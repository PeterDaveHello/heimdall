[
  {
    "id": 40870357,
    "title": "The Origins of DS_store (2006)",
    "originLink": "https://www.arno.org/on-the-origins-of-ds-store",
    "originBody": "Apple Lore On the origins of DS_store If you are a Mac user, or if you have transferred files from Mac to Windows, you’re probably familiar with .DS_Store files. But where does this name come from? Back in 1999 I was the technical lead for the Mac OS X Finder at Apple. At that time the Finder code base was some 8 years old and had reached the end of its useful life. Making any changes to it require huge engineering effort, and any changes usually broke two or three seemingly unrelated features. For Mac OS X we decided to rewrite the Finder from scratch. Part of the work involved separating its user interface and its core functionality, the back-end. The back-end of the Finder enumerates files, watch for changes in the file system, deals with metadata, including icon locations and folder settings. Internally, those two components were known as Finder_FE and Finder_BE (Frontend and Backend). However, we soon started realizing that the Finder backend would be useful outside of the Finder. Therefore, a plan was hatched to someday make it available as a public API. Since I had previously been responsible for naming Icon Services and Navigation Services, we decided to go with Desktop Services (at the time, we were also considering renaming the Finder to “Desktop”). Hence the name of the .DS_Store, for “Desktop Services Store”. We added a “.” in front of it so that it would be considered as an invisible file by Unix OS, including Mac OS. Personally, I don’t think it’s a great name and I wish we had gone with something a bit more descriptive, but it’s too late for that :-) There is also an unfortunate bug that is not fixed to this day that result in an excessive creation of .DS_Store file. Those files should only be created if the user actually makes adjustments to the view settings or set a manual location for icons in a folder. That’s unfortunately not what happens and visiting a folder pretty much guarantees that a .DS_Store file will get created Incidentally, Finder_BE aka Desktop Services did end up being used by more than just the Finder: Navigation Services (the Open/Save dialog) now also make use of it, although it didn’t in the initial release of Mac OS. However, the Desktop Services API has still not been fully released. Published Oct 1, 2006 By Arno Gourdol Published by Arno Gourdol",
    "commentLink": "https://news.ycombinator.com/item?id=40870357",
    "commentBody": "The Origins of DS_store (2006) (arno.org)375 points by edavis 21 hours agohidepastfavorite200 comments ggm 20 hours agoAside from this file, the \"fork\" concept of Mac file systems caused some wtf moments. Fork not being fork() but being the two-pronged idea in that file system, both a resource and a data component existed as pair. One metadata and one the file contents. In Unix, the metadata was in the directory block inode, and wasn't bound to the file in a formalism uniquely, it had to be represented by structure in tar, or cpio or zip distinctly. Implementing Mac compatible file support in Unix meant treating the resource fork first class and the obvious way you do it is for each file have .file beside it. You couldn't map all the properties of the resource fork into an inode block of the time in UFS. It has stuff like the icon. More modern fs may have larger directory block structure and can handle the data better. reply klodolph 15 hours agoparent> One metadata and one the file contents. I’d say this is not the right way to describe a resource fork. Instead, think of it as two sets of file contents—one called \"data\" and one called \"rsrc\". On-disk, they are both just bytestreams. The catch is that you usually store a specific structure in the resource fork—smaller chunks of data indexed by 4-byte type codes and 2-byte integer IDs. Applications on the 68K normally stored everything in the resource fork. Code, menus, dialog boxes, pictures, icons, strings, and whatever else. If you copy an old Mac application to a PC or Unix system without translation, what you got was an empty file. This meant that Mac applications had to be encoded into a single stream to be sent over the network… early on, that meant BinHex .hqx or MacBinary .bin, and later on you saw Stuffit .sit archives. That’s why these structures don’t fit into an inode—it’s like you’re trying to cram a whole goddamn file in there. The resource fork structure had internal limits that capped it at 16 MB, but you could also just treat it as a separate stream of data and make it as big as you want. reply kmeisthax 10 hours agorootparentIn Unix, it's said that \"Everything is a file\" - i.e. that everything on the system that applications need to manage should either be actual files on disk or present themselves to the application as if they were files. This adage translated to classic MacOS becomes \"Everything is a resource\". The Resource Manager started out as developer cope from Bruce Horn for not having access to SmallTalk anymore[0], but turned out to completely overtake the entire Macintosh Toolbox API. Packaging everything as type-coded data with standard-ish formats meant cross-cutting concerns like localization or demand paging were brokered through the Resource Manager. All of this sounds passe today because you can just use directories and files, and have the shell present the whole application as a single object. In fact, this is what all the ex-Apple staff who moved to NeXT wound up doing, which is why OSX has directories that end in .app with a bunch of separate files instead. The reason why they couldn't do this in 1984 is very simple: the Macintosh File System (MFS) that Apple shipped had only partial folder support. To be clear, MFS did actually have folders[1], but only one directory[2] for the entire volume. What files went in which folders was stored in a separate special file that only the Finder read. There was no Toolbox support for reading folder contents, just the master directory, so applications couldn't actually put files in folders. Not even using the Toolbox file pickers. And this meant the \"sane approach\" NeXT and OSX took was actually impossible in the system they were developing. Resources needed to live somewhere, so they added a second bytestream to every file and used it to store something morally equivalent to another directory that only holds resources. The Resource Manager treats an MFS disk as a single pile of files that each holds a single pile of resources. [0] https://www.folklore.org/The_Grand_Unified_Model.html?sort=d... [1] As in, a filesystem object that can own other filesystem objects. [2] As in, a list of filesystem objects. Though in MFS's case it's more like an inode table... reply crest 10 hours agorootparentOne of most important technical details about resources in early MacOS is that it allowed the system to swap resources by using double indirect pointers (aka handles) with the lock bit stuffed into the upper 8 bits of the 32 bit. Stealing the extra flag bits from the upper bits instead of increasing the alignment to make a few lower bits available was fine on the 68000 and 68010 with their 24 Bit address space, but exploded into your face on an 020/030 with a real 32 Bit address space. It was a nightmare do develop and debug. A mix of assembler, Pascal and C without memory protection, but at least you could use ResEdit to put insults into Menu entries on school computers. reply nxobject 10 hours agorootparentGood 'ol purgeable resources: one of the reasons why the early Mac could get away with 128kb and lots of floppy swapping. reply kzrdude 20 hours agoparentprevResource fork used to contain all the stuff you could edit with ResEdit (good old times!) right? Icons, various gui resources, could be text and translation assets too. For example Escape Velocity plugins used custom resource types and a ResEdit plugin made them easy to edit there. reply worstspotgain 19 hours agorootparentA lot of Classic Mac apps just used the resource fork to store all their data. It was basically used as a Berkeley DB, except the keys were limited to a 32-bit OSType plus a 16-bit integer, and performance was horrible. But it got the job done when the files were small, had low on-disk overhead, and was ridiculously easy to deploy. Once you pushed an app beyond the level of usage the developer had performed in their initial tests, it would crawl to a near-halt, thrashing the disk like crazy on any save. Apple's algorithm would shift huge chunks of the file multiple times per set of updates, when usually it would be better to just rewrite the entire file once. IIRC, part of the problem was an implicit commitment to never strictly requiring more than a few KBs of available disk space. In a sense, the resource fork was just too easy and accessible. In the long run, Mac users ended up suffering from it more than they benefited. When Apple finally got rid of it, the rejoice was pretty much universal. There was none of the nostalgia that usually accompanies disappearing Apple techs, especially the ones that get removed outright instead of upgraded (though one could argue that's what plists, XML and bundles did.) reply nickm12 11 hours agorootparentThe rejoicing was definitely not universal. It really felt like the NeXT folks wanted to throw out pretty much the entire Mac (except keeps its customer base and apps) and any compatibility had to be fought for through customer complaints. Personally, MacOS X bundles (directories that were opaque in the Finder) seemed like a decent enough replacement for resource forks. The problem was that lots of NeXT-derived utilities munged old Mac files by being ignorant of resource forks and that was not ok. reply worstspotgain 9 hours agorootparentThe 9->X trapeze act was a colossal success, but in retrospect it was brutally risky. I can't think of a successful precedent involving popular tech. The closest parallel is OS/2, which was a flop for the ages. A large amount of transition code was written in those years. One well-placed design failure could have cratered the whole project. Considering that the Classic environment was a good-enough catch-all solution, I would have also erred on the side of retiring things that were redundant in NeXT-land. Resource forks were one of the best victims, 1% functionality and 99% technical debt. The one I mourned for was the Code Fragment Manager. It was one of Apple's best OS9 designs and was massively superior to Mach-O (and even more so wrt other unices.) Alas, it didn't bring enough value to justify the porting work, let alone the opportunity cost and risk delta. reply tambourine_man 6 hours agorootparentI'm still mourning file name extensions and the loss of the spatial Finder. reply pjmlp 9 hours agorootparentprevMacOS X bundles are actually NeXTStep bundles, and are behind the same idea in Java JAR files with META-INF directory, and .NET resources, due to Objective-C's legacy on all those systems. reply Lammy 14 hours agorootparentprev> When Apple finally got rid of it, the rejoice was pretty much universal. There was none of the nostalgia that usually accompanies disappearing Apple techs Here's some https://arstechnica.com/gadgets/2001/08/metadata/ reply sweetjuly 17 hours agorootparentprevNSUserDefaults, the modern programmer's fork DB :) reply inferiorhuman 15 hours agorootparentprevOnce you pushed an app beyond the level of usage the developer had performed in their initial tests, it would crawl to a near-halt With HFS (unsure about HFS+) the first three extents are stored in the extent data record. After that extents get stored in a separate \"overflow\" file stored at the end of the filesystem. How much data goes in those three extents depends on a lot of things, but it does mean that it's actually pretty easy for things to get fragmented. A bit more detail: the first three extents the resource and data forks are stored as part of the entry in the catalog (for a total of up to six extents). On HFS each extent can be 2^16 blocks long (I think HFS+ moved to 32-bit lengths). Anything beyond that (due to size or fragmentation) will have its info stored in an overflow catalog. The overflow catalogs are a.) normal files and b.) keyed by the id (CNID) of the parent directory. If memory serves this means that the catalog file itself can become fragmented but also the lookups themselves are a bit slow. There are little shortcuts (threads) that are keyed by the CNID of the file/directory itself, but as far as I can tell they're only commonly written for directories not files. tl;dr For either of the forks (data or resource) once you got beyond the capacity of three extents or you start modifying things on a fragmented filesystem performance will go to shit. reply jwells89 19 hours agorootparentprevI credit ResEdit hacking partially for steering my path towards becoming a programmer. I had my Classic Mac OS installs throughly customized, as well as the other various programs and games that stored their assets in resource forks. It was a lot of fun and something I’ve missed in modern computing. Not even desktop Linux is really fills that void. ResEdit and the way it exposed everything complete with built-in editors was really something special. reply be_erik 17 hours agorootparentResEdit and using it to modify Escape Velocity is 100% the reason I’m still in this industry. reply tarsinge 8 hours agorootparentSame here but only for joining the industry. Now it's the opposite, that webdev still hasn't reached that level of maturity of classic Mac OS makes me want to quit. reply nox101 2 hours agorootparentprevI always thought the resource fork as a good idea poorly implemented. IMO they should have just given you a library that manipulated a regular file. Then you could choose to use it or not but it would still be a single file. It could have a standard header to identify it and the system could look inside if that header was there. One of the big problems with resource forks was that no other system supported them so to host a mac file on a non-mac drive or an ftp server, etc, the file had to be converted to something that contained both parts, then converted back when brought to the mac. It was a PITA. reply whartung 19 hours agorootparentprevThe other big thing in the resource fork was the executable code segments that made up the application. In fact applications typically had nothing but the data fork at all. It was all in the resource fork. reply pkaye 20 hours agoparentprevNTFS has alternate data streams. I think its hardly ever used. https://en.wikipedia.org/wiki/NTFS#Alternate_data_stream_(AD... reply rnts08 17 hours agorootparentVery commonly used to hide malware and other things you don't want the average user or windows admin to find. reply kccqzy 19 hours agorootparentprevI used to dual boot OS X and Windows on my Mac in the late 2000s. I am pretty certain when I open the HFS+ volume and copy things to the NTFS volume, some stuff became alternate data streams. Windows even had a UI to tell me about it. I didn't understand it then but my guess would be that's the resource fork. reply pseudalopex 19 hours agorootparentprevThe article said most browsers mark downloaded files. reply kccqzy 17 hours agorootparentThat's done as part of xattr, or extended attributes. It's a very flexible system. For example you can add comments to a file so they are indexed by Spotlight. reply p_l 11 hours agorootparentExcept NTFS does not have \"extended attributes\" in Linux/Irix/HPFS sense. Every FILE object in the database is ultimately (outside of some low level metadata) a map of Type-(optional Name)-Length-Value entries, of which file contents and what people think of as \"extended attributes\" are just random DATA type entries (empty DATA name marks the default to own when you do file I/O). It's similar to ZFS (in default config) and Solaris UFS where a file is also a directory reply EvanAnderson 17 hours agorootparentprevNTFS ACLs (aka file permissions) are stored in alternate data streams. reply neerajsi 16 hours agorootparentI work on ReFS and a little bit on NTFS. Alternate data streams are simply seekable bags of bytes, just like the traditional main data file stream. Security descriptors, extended attributes, reparse points and other file metadata are represented as a more general concept called an \"attribute\". You can't actually open a security descriptor attribute and modify select bytes of it to create an invalid security descriptor, as you would if it were a general purpose stream. reply EvanAnderson 15 hours agorootparentHelp me understand the terminology. I thought alternative data streams were just non-resident attributes. Attributes like \"$SECURITY_DESCRIPTOR\" have reserved names but, conceptually, I thought were stored in the same manner as an alternative data stream. (Admittedly, I've never seen the real NTFS source code-- I've only perused open source tools and re-implementations.) reply p_l 11 hours agorootparentEssentially, attribute names directly specify the attribute type - so $SECURITY_DESCRIPTOR declared the entry in FILE attribute list to be a security descriptor. DATA attributes have another name field to handle multiple instances reply EvanAnderson 4 hours agorootparentI see. So there's one more layer of indirection there that I'm missing. reply asvitkine 20 hours agorootparentprevUsed by malware mostly, I think. reply Someone 19 hours agoparentprev> the two-pronged idea in that file system, both a resource and a data component existed as pair. One metadata and one the file contents. Application metadata describing what file types an application could open, what icons to use for those file types if they matched the application’s creator code was stored in the resource fork of the application, but file metadata never was stored in the resource fork. File types, creator codes, lock, invisible, bozo, etc. bits always were stored in the file system. See for example the description of the MFS disk format at https://wiki.osdev.org/MFS#File_Directory_Blocks reply dylan604 18 hours agoparentprevIt was all of the forked data that made dual format CDs/DVDs \"interesting\". In the beginning it was a trick. Eventually, the Mac burning software made it a breeze. Making a Mac bootable DVD was also interesting. reply netsharc 18 hours agorootparentI recall seeing CD-ROMs that had both Mac and Windows software on it, and depending on which OS it was mounted on, it would show the Windows EXE or the Mac app... I wonder how that's done. I'm guessing there was a clever trick so files on both filesystems share the same data (e.g. if the program/game had a movie, it would only store the bytes of the movie once but its addressable as a file on each filesystem), but that sounds like a nightmare. I can probably look it up and figure it out myself, ah, the joys of learning about obsolete tech! reply marcodiego 17 hours agorootparentYou can hide files from windows by setting a property on the file. You can hide files from MacOS by inserting it's name in a file called \".hidden\". reply dylan604 18 hours agorootparentprevThere were also the audio CDs that had data on them. Audio CD players would just play the audio, but a CD-ROM could access both. Some had apps that were games that would play the audio portion for the game. If you want to know about the different types of CDs, you'll want to know about the various colors: https://en.wikipedia.org/wiki/Rainbow_Books reply dmicah 17 hours agorootparentSome Playstation 1 were setup to also play the game soundtrack if you put them in an audio CD player. reply nullindividual 17 hours agorootparentMechWarrior 2: Mercenaries (for PC) was the same way. Rocking soundtrack. Beautiful game, provided you had a Voodoo 2. reply jwells89 16 hours agorootparentThe Mac version of the original Descent was like this too, with a great redbook audio soundtrack. The game wasn't locked to the original disc though, you could pop out the CD in the middle of the game and replace it with any other audio CD and it'd play that just as well. reply tarsinge 6 hours agorootparentprevI remember listening to the Warcraft 2 soundtrack from the game CD-ROM in the living room audio CD player. reply rescbr 16 hours agorootparentprevIIRC from that time, those CD-ROMs contained two tracks, one formatted with ISO 9660 and another with HFS+. Windows didn't come with HFS+ drivers so it ignored it, and probably MacOS prioritized mounting the HFS+ track. reply Lammy 14 hours agorootparentI've seen some where the combined file size exposed on each track would be larger than a CD could hold, so there had to be something more going on. StarCraft and Brood War come to mind with the large StarDat.mpq / BrooDat.mpq files. reply p_l 11 hours agorootparentTL;DR ISO9660 provided an area to stuff type-tagged extension information for each directory entry. In addition, first 32kB of iso9660 are unused, which allowed tricks like putting another filesystem metadata there. By carefully arranging metadata on disk it was then possible to make essentially overlapping partitions, stuffing each filesystem metadata in area unused by the other, with files reusing the same space reply inferiorhuman 15 hours agorootparentprevAs it starts about 32k in, the ISO 9660 superblock doesn't inherently conflict with an Apple partition map which starts at the beginning. Apple also had proprietary ISO 9660 extensions that add extra metadata to the directory entries much like the RockRidge extension does. Those would get ignored by non-Apple implementations of ISO 9660. Microsoft went a different route with its long filename extensions (Joliet) – they simply created a whole different (UCS-2/UTF-16 encoded) directory tree. An ISO 9660 implementation that's compatible with Joliet will prefer the Unicode directory hierarchy and look there for files. reply euroderf 10 hours agoparentprev> Implementing Mac compatible file support in Unix meant treating the resource fork first class and the obvious way you do it is for each file have .file beside it. Prefixing the file name with a single dot - is this a file system convention ? Or just a \"good idea\" ? reply ggm 7 hours agorootparentUnix convention to hide. .Files hidden from ls unless -a used but cd .config/ works fine. It matched the use of . For \"this dir\" and .. for \"parent dir\" also hidden by default. It was in v7 on a pdp11, my first experience of Unix in 1980. Probably pre-dated that. reply euroderf 3 hours agorootparentOh sure. I started with v6 on a pdp-10 in 1979. And the leading dot is ingrained in my brain. But what I'm wondering about is the idea of associating (for example) \"myfile.xyz\" and \".myfile.xyz\". I've never heard of this as a convention for associating metadata. reply a-dub 13 hours agoparentprevresource and data forks were hfs(+) features that appeared in pre-osx versions of macos. post-osx made use of the bsd fast filesystem and a rather nice unix style convention from nextstep where the on-disk representation of a .app or .pkg (which would appear as a single entity in the gui) was actually a directory tree. this would rather elegantly include ui resources as well as multiple binaries for cross platform support. reply senderista 20 hours agoparentprevYou have the same \"resource fork\" concept in Unix xattrs and NTFS streams. reply ggm 20 hours agorootparentNo disagree, Both came later IIRC. Melbourne unis work on appletalk and Apple file system support was in the late 80s and I believe POSIX xattr spec work was mid nineties, NTFS was '93 or so. The fork model in apple file store was eighties work. reply adrian_b 1 hour agorootparentThe concept of extended file attributes has been introduced by HPFS, in OS/2, in 1989. From HPFS it was taken by SGI XFS (the ancestor of Linux XFS) and MS NTFS, both in 1993. From there it has spread to various other file systems and specifications. The concept of resource forks is earlier, but both are examples of using alternate data streams in a file. reply nullindividual 20 hours agorootparentprevGP wasn’t arguing about timelines. NTFS ADS were created to accommodate Mac OS resource forks on network volumes when using AFP. reply ggm 20 hours agorootparentGotcha! I assumed they were invented for Windows centric reasons. reply slmjkdbtl 17 hours agoprevI remember there used to ways to turn off the creation of .DS_Store but they removed it, I can't figure out for life why they would make such a change. I had to write a program [0] to watch the entire file system and delete .DS_Store as soon as they're created. [0] https://github.com/slmjkdbtl/dskill reply js2 16 hours agoparentYou can turn it off for network volumes: defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool TRUE https://support.apple.com/en-us/102064 I don't recall there ever being a way to turn it off for local volumes. reply nness 5 hours agorootparentI set up my samba config to veto .DS_Store files, which also seems to work (although not sure if it creates more overhead as MacOS tries to recreate it each time...) reply grishka 2 hours agorootparentprevIs it also possible to do this for removable media? It seems to be the first time I'm seeing Apple themselves officially recommending a \"defaults write\" command. reply js2 1 hour agorootparentNot sure if this still works, but: defaults write com.apple.desktopservices DSDontWriteUSBStores -bool TRUE Re: defaults: https://support.apple.com/guide/terminal/edit-property-lists... reply silverwind 9 hours agorootparentprevThere was https://github.com/binaryage/asepsis but Apple broke it IIRC. reply js2 2 hours agorootparentIt was a hack, not anything Apple ever supported: > At core Asepsis provides a dynamic library DesktopServicesPrivWrapper which gets loaded into every process linking against DesktopServicesPriv.framework. It interposes some libc calls used by DesktopServicesPriv to access .DS_Store files. Interposed functions detect paths talking about .DS_Store files and redirect them into a special prefix folder. This seems to be transparent to DesktopServicesPriv. > Additionally Asepsis implements a system-wide daemon asepsisd whose purpose is to monitor system-wide folder renames (or deletes) and mirror those operations in the prefix folder. This is probably the best we can do. This way you don’t lose your settings after renaming folders because rename is also executed on folder structure in the prefix directory. Unsurprisingly, you can no longer do anything like this with SIP. If you're willing to disable SIP, there are forks of the project that apparently still work. reply phyzix5761 13 hours agoparentprevfind / -name \".DS_Store\" -exec rm {} \\; 2>/dev/null Put that in a script and add it to your crontab. reply mort96 2 hours agorootparentThat's gonna be incredibly slow on most developer machines. node_modules, __pycache__, Cargo target/ folders, Yocto build folders, .git folders, etc etc etc -- all my machines which are ever used for development end up with such a gargantuan amount of small files across the filesystem that any operation which involves iterating through all of them takes forever. Besides, there are .DS_Store I really don't wanna delete. Notably, there are git repos which have erroneously committed .DS_Store files; I don't wanna make those repos dirty by deleting them. reply fragmede 13 hours agorootparentprev-delete is faster reply phyzix5761 13 hours agorootparentEven better! reply gumby 16 hours agoparentprevWhy? I just ignore them. reply prurigro 16 hours agorootparentGoogle sent a copyright violation notice for each .DS_Store anyone at my company uploaded to Drive for nearly a year (yes, many support tickets were filed). It wasn't Apple's fault, but it still would have been nice if there was a way to turn them off. reply heavyset_go 16 hours agorootparentThat's scary considering how willingly they'll shutdown accounts for tripping their automated copyright violation service. reply prurigro 15 hours agorootparentFor sure! I made sure to have an open ticket with them until it was resolved so I'd have someone to call if some other automated system decided to shut down our services for it. reply janalsncm 16 hours agorootparentprevWhy? Somehow DS_Store is claimed as a copyrighted file? reply biztos 15 hours agorootparentWhite noise was claimed on YouTube[0]. When is someone going to copyright .gitignore? You could register gitignore.me right now! Fame, riches, lunch with Myhrvold[1][2]! [0]: https://www.bbc.com/news/technology-42580523 [1]: https://en.wikipedia.org/wiki/Intellectual_Ventures [2]: https://www.amazon.com/Modernist-Cuisine-Science-Stainless-S... reply Pikamander2 11 hours agorootparentprevThe process likely went something like this: 1. Pirates uploaded a folder full of copyrighted files to Google Drive, accidentally including some DS_Store files along with the actual media. 2. The copyright owner filed a DMCA takedown on the whole folder, accidentally claiming ownership of a bunch of generic DS_Store files. 3. The above two steps have likely happened many times, not just once. 4. Google's takedown system now automatically flags DS_Store files as having multiple copyright violations. 5. A Google employee might be able to whitelist a user's individual DS_Store files to temporarily suppress the violation on their account, but since they can appear in different folders with different data and are constantly receiving new copyright claims, their system likely errs on the side of caution and continues to flag them as copyright violations so that Google doesn't accidentally lose its safe harbor protections. In theory, a Google engineer could code in a special case to avoid this problem, but good luck finding and talking to one who's authorized to do so; Google is notorious for having one of the lowest employee;revenue ratios in the world and writing useless FAQs instead of having a proper support channel for when things go wrong. reply prurigro 15 hours agorootparentprevThat's a good question. I get the impression the system is fairly opaque even to the people working there. I was told it was \"resolved\" and had my ticket closed a bunch of times, only to have another 30+ copyright violation emails the next time someone uploaded a batch of files from MacOS. If the person who finally managed to figure it out ends to reading this, thanks for the resolution :) reply gumby 15 hours agorootparentprevHoly cow, that’s crazy! reply rf15 12 hours agorootparentprevMissing Stair effect - ignoring a problem does make everything progressively worse for everyone because the problems pile up. reply demondemidi 2 hours agorootparentprevThat’s the easy solution. But some people are absolute control freaks and would rather go nuts about a hidden file than actually spend their energy creating things. Very telling. reply DEADMINCE 16 hours agorootparentprevSome people can't. reply seabass 16 hours agoprevNever understood why it had to be in the same folder. Can’t the os have its own little db somewhere that has a reference to each path? reply neerajsi 16 hours agoparentPutting in in the folder is also nice in that it naturally gets deleted when the folder is deleted reply earthboundkid 14 hours agorootparentAll file operations have been watched by Spotlight since forever at this point. reply threeseed 13 hours agorootparentExcept for ignored file types and folders you marked as private. reply vetinari 7 hours agorootparentOr those on network volumes or removable media. When somebody else on other machine removes them, your local database is out of sync pronto. reply earthboundkid 7 hours agorootparentThis also happens with .DS_store files if the other computer on the network isn’t a Mac. It’s irrelevant. reply stereo 16 hours agoparentprevThe idea was that metadata, for example a file’s label, would travel across to whichever device you use the network drive from. reply tambourine_man 5 hours agorootparentBut classic Mac OS stored “Desktop DB” and “Desktop DF” at the root of each mounted drive, IIRC. It seems like a better solution. reply jojobas 14 hours agorootparentprevYeah, because such devices are only made by Apple and can or should understand Apple's internal format. reply thought_alarm 20 hours agoprev> Those files should only be created if the user actually makes adjustments to the view settings or set a manual location for icons in a folder. That’s unfortunately not what happens and visiting a folder pretty much guarantees that a .DS_Store file will get created This is my number one frustration with the Finder. You can customize the look and size of individual folder windows in many interesting ways, al a the Classic Mac OS Finder, which is a really great feature. But if you blow through that same folder in a browser window then most of those customization are lost, overwritten with the settings of that browser window, even if you never change anything. What's the point of allowing all of these great customizations when they're so easily clobbered? I have a global hot key to bring up the Applications folder. I'd love to customize the look of that window, but it's pointless. Whenever I hit that hot key I have no idea what I'm going to get. It's always getting reset. By the way, the reason it does this is because the Finder has no way to set a default browser window configuration. So instead, it just leaves behind the current browser settings in each folder it visits. Super frustrating. reply dreamcompiler 4 hours agoparent> I have a global hot key to bring up the Applications folder Not global, but as long as you're in the Finder cmd-shift-A opens the Applications folder. cmd-shift-U opens the Utilities folder. reply wirrbel 51 minutes agoprevWhat's really astonishing is that no one at apple dared to fix the bug that creates these files... reply metadat 20 hours agoprevIt's worth mentioning how to turn off the creation of .DS_Store files by default while browsing network volumes - otherwise the directory modified timestamps are updated as you browse using the Finder, which is Just Plain Terrible. https://old.reddit.com/r/MacOS/comments/lvju40/comment/gpc8i... reply al_borland 19 hours agoparentmacOS is tricky now. I just looked via Finder to see if I had any .DS_Store files on my network volumes, and it appeared not. However, when I went to Terminal, sure enough, they were there. I now can't trust Finder's ability to show hidden files, as it only shows the hidden files it thinks a user should care about, rather than all hidden files. Not good. Since my network shares are for a local Synology, it's not a a big deal for me. I have run into them at work before, and it does create quite the mess. reply lwkl 10 hours agorootparentIf I remember correctly there is an option in Synology DSM to not let clients create .DS_Store files in network shares. reply vetinari 7 hours agorootparentIt is a samba feature, called veto. You can define there, what you don't want on your shares, starting from .DS_Store and Thumbs.db, to *.mp3, for example. reply op00to 15 hours agorootparentprevMy synology NAS drops turds in lots of directories too. reply vetinari 7 hours agorootparentYour synology has its own way to store xattrs and alternate file streams, in the @eaDir, so some of the turds may be dropped by your windows or mac client machine. But yes, it also does few of its own things for the other software running on your box, like for example SYNOINDEX_MEDIA_INFO for known media files. reply black_puppydog 20 hours agoparentprevPersonally I make sure mac users do this before they get write access to a network share. It's just a matter of common curtesy IMHO. reply vetinari 7 hours agorootparentSet up a veto on your network share (https://www.samba.org/samba/docs/current/man-html/smb.conf.5...). reply chrisweekly 16 hours agorootparentprevcurtsy (feminine bow) -> courtesy (polite act) reply philwelch 16 hours agorootparentYes, but it’s a common courtesy to perform a curtsy in the appropriate situation. reply DidYaWipe 12 hours agorootparentAnd failure to do so might come off as curt, see? reply actionfromafar 20 hours agoparentprevIf you run Samba you can also configure Samba to just ignore such creations. reply FredPret 19 hours agorootparentFor nginx and WebDAV: https://gist.github.com/jirutka/5380770 reply webwielder2 1 hour agoprev> For Mac OS X we decided to rewrite the Finder from scratch. I would think that the file manager for an entirely separate operating system being written from scratch would be a foregone conclusion. reply SoftTalker 1 hour agoparentNeXT OS had a perfectly good file manager/GUI, but I guess it was pretty different from what Mac users were used to. reply gregmac 14 hours agoprevAs a non-Mac user, I always find it somewhat annoying when I download some .tgz published on Github or something and find .DS_Store littered inside. I guess macos probably just uses GNU tar? It's kind of surprising it wasn't modified or configured by default to ignore .DS_Store. reply LeoPanthera 14 hours agoparent> It's kind of surprising it wasn't modified or configured by default to ignore .DS_Store. It was, but not by default. If you export COPYFILE_DISABLE=true then tar will skip .DS_Store files. reply sneed_chucker 13 hours agoparentprevMost of Mac's Unix utils come straight from FreeBSD without any special sauce from Apple. reply leptons 12 hours agorootparentThey had the chance to get rid of DS_store, but they put it in MacOS anyway? reply pasc1878 10 hours agorootparenter. as the article says - it was created for OSX and not classic macOs reply willsmith72 14 hours agoparentprevAh that reminds me I committed a few last week and never cleaned it up.. reply nanna 6 hours agoprevThankfully Emacs's file manager Dired lets me easily pretend this pesky little file, as well as those produced by a LaTeX run, doesn't exist. (setq dired-omit-mode t dired-omit-files \"^.+\\\\.\\\\(DS_Store\\\\|aux\\\\|bak\\\\|bbl\\\\|bcf\\\\|blg\\\\|dvi\\\\|ent\\\\|idx\\\\|ilg\\\\|ind\\\\|log\\\\|orig\\\\|out\\\\|pdf-view-restore\\\\|pdf#\\\\|reg\\\\|run.xml\\\\|synctex.gz\\\\|toc\\\\)$\") reply thiht 5 hours agoparent\"easily\" you say? reply nanna 3 hours agorootparentWell sure it's a bit noisy but it's just a bit of regex. reply Ruq 17 hours agoprevEvery time I see it I think Nintendo DS. reply wodenokoto 3 hours agoprevWhenever I move a file from windows into WSL via explorer I get a Zone file. I assume it’s the same things but quite annoying. reply Waterluvian 20 hours agoprevDS Store seems so unfortunate. Yes it serves a purpose. Yes you can work around it in various ways. But the reality is that it’s basically proliferated file litter to 99% of people who come across it. It’s uncharacteristically un-Apple in terms of UX polish. Growing up with both System 7.5 / OSX, and windows machines, the Macs never seemed inclined to make me see extraneous files, filetypes, and other “how the computer works” implementation details. It’s just so odd to my mental model of it all to see this file end up everywhere. reply al_borland 19 hours agoparentFor those who live their whole life within Apple's walls, they will never see .DS_Store files, unless they use the Terminal. Finder (with hidden files shown) doesn't even show them anymore. It is very ugly when files are shared from a Mac to people on Windows though. I think it gives a bad first impression for anyone who might be thinking of transitioning to the Mac. reply nathan_douglas 19 hours agorootparentThey pop up in code repositories too, depending on contents and whether the engineer in question noticed it. reply doomlaser 16 hours agorootparentabsolutely essential to add a line for .DS_Store in every .gitignore, unfortunately. reply klodolph 14 hours agorootparentEnough to teach people to use a global git core.excludesfile, IMO. Same place you should put rules for Emacs / Vim swap files. reply smix96 9 hours agorootparentTotally correct. Files which are unrelated to the project don't belong in .gitignore. reply al_borland 4 hours agorootparentThis may be technically correct, and I do have .DS_Store in my global, but I also put it in projects, because I know not everyone on my team is going to do that. I add it to the .gitignore in projects to save me from other people junking up the project. It’s a lot easier to add some lines to a file than it is to micromanage the global file for every potential future contributor. reply Waterluvian 4 hours agorootparentThis touches on something I've learned to be more mindful of: the \"right answer\" (especially to a techie) is often not the right answer in real world cases. reply account42 4 hours agorootparentprevIt makes sense to add an ignore for .* though and then specifically unignore only those dotfiles/directories that you actully want checked in. reply beeboobaa3 10 hours agorootparentprevI've banned people before because they couldn't stop themselves from continuously uploading those useless ds files reply DEADMINCE 16 hours agoparentprev> It’s uncharacteristically un-Apple in terms of UX polish. Apple's polish has always been more about the surface then the internals. reply userbinator 15 hours agorootparentI remember playing around with setting up a Hackintosh, and found all those errors in the system logs --- then realised that an actual working Mac generates much the same (ignorable) errors. reply plasticeagle 14 hours agorootparentTo be fair to Apple here, so does every other operating system. Linux system logs are filled with errors too. In general, keeping the logs of even a moderately complex application \"clean\" - so that the only errors logged are real errors, in some poorly defined meaning of \"real\" - is very hard. For operating systems it must be straight up impossible. reply marcosdumay 2 hours agorootparent> Linux system logs are filled with errors too. Mostly only due to misbehaving hardware. Something that should really not happen on a Mac. And \"filled\" is way hyperbolic, there usually isn't a lot of it. reply popcalc 10 hours agorootparentprevIt's difficult to accept as competency when they control both the software and hardware. reply hoherd 15 hours agoprevFYI there is a tool on macOS called `dot_clean` that will \"Remove dot-underscore files\" https://ss64.com/mac/dot_clean.html reply java-man 19 hours agoprevAlso there are those dot underscore files. Is there any way to disable creating these files on the network shares? [0] https://superuser.com/questions/212896/is-there-any-way-to-p... reply tripdout 15 hours agoprevWhy doesn't Windows need such a directory to store folder customizations in Explorer? reply steve1977 14 hours agoparentExplorer uses a hidden desktop.ini file for this. reply OptionOfT 12 hours agorootparentNegative. desktop.ini doesn't get edited when you switch (for example) from Details to List. Also, I think only the desktop allows moving icons around freely. reply steve1977 11 hours agorootparentI guess a more correct answer would have been that deskop.ini is used for some folder customizations. reply reddalo 8 hours agorootparentprev> only the desktop allows moving icons around freely I'm pretty sure Windows used to allow you to move icons around, I clearly remember making a mess on some Windows 98 folders. Maybe they removed that feature recently? reply kaladin-jasnah 15 hours agoprevThere's also the .fseventsd directory which I've also seen on non-UNIX systems. reply rkachowski 11 hours agoprev> Internally, those two components were known as Finder_FE and Finder_BE (Frontend and Backend). Interesting to see that apps were split into front and back end (indeed, I'm surprised even that the terms existed) back in 1999. reply pasc1878 10 hours agoparentWhy are you surprised? I have been writing client server apps since the late 80s. Originally a central DB and a PC front end. But the server could be doing business processing e.g. feeds and processing of stock prices. Client Server predates the web. reply ee99ee 15 hours agoprevMost informative post ever on Hacker News. Now I know! reply sherburt3 16 hours agoprevI am MacOS's biggest fanboy and Tim Cook's strongest soldier but I will also say the Finder is one of the dumbest file explorers I've ever experienced in my life reply DEADMINCE 16 hours agoparent> I am MacOS's biggest fanboy and Tim Cook's strongest soldier wow. reply DidYaWipe 20 hours agoprevNot to mention that it's an obnoxious and incompetent design. Look at the fact that Mac OS litters every other computer it visits with turds, for its own (and in fact only one user's) benefit. It's doubly stupid because the next browsing Mac that comes along trounces the previous one's turd. If Apple wanted to store view settings for remote volumes (or even local volumes), the competent design would have been to store them locally (and per user) in a central location on the machine doing the browsing. I remember the promised re-write of Finder and thought it never happened. Nothing seems to have improved for the user. I could post a list of decades-old defects that persist today. The one thing I can think of that has finally been fixed (and this was long after the \"rewrite\") was that you can now finally sort the file list properly: with folders at the top. Now I wish someone would explain something that might actually be worse than DS-turds: the presence of a \"Contents\" subdirectory in every goddamned Apple package. I mean... who thought you needed to create a directory called \"Contents\" to hold the contents of the parent directory? It's mind-boggling. reply jwells89 19 hours agoparentI can see the appeal for removable media, at least. It’s pretty common for those to have only a single user toting them around between home/work/school and for that case it makes a lot of sense to store that info on the media so settings stick across different machines. It probably made even more sense back when removable media was the norm for data transfer because network access was spotty or slow. It really should be turned off by default on network volumes though. reply ryandrake 16 hours agoparentprev> Not to mention that it's an obnoxious and incompetent design. Look at the fact that Mac OS litters every other computer it visits with turds, for its own (and in fact only one user's) benefit. It's doubly stupid because the next browsing Mac that comes along trounces the previous one's turd. It also kind of reveals an underlying attitude of the OS developers: That it's OK to use the user's filesystem (particularly directories owned by the user as opposed to the OS) as their dumping ground for all this metadata. As if it's their hard drive rather than mine. I'm OK with Apple putting whatever it wants in /System and /Library, but I'd expect the rest of my filesystem to contain only files I put there. Same goes for you, Microsoft: You can have C:/WINDOWS and I should get the rest of the filesystem. reply jwells89 15 hours agorootparent> That it's OK to use the user's filesystem (particularly directories owned by the user as opposed to the OS) as their dumping ground for all this metadata. There are more of this type of offender than I can possibly count that dump myriad dotfiles and dotfolders in your home folder on nixes instead of adhering to platform conventions or XDG or anything, really. Worse, these programs won't function properly if you set your home folder to be read-only (leaving subdirectories writable) to keep it clean. Drives me nuts. reply ryandrake 15 hours agorootparentOh, yea. I didn't mean to give Linux/Unix a pass. Those systems can be equally cavalier about leaving their configuration droppings all over my filesystem, too. reply pasc1878 10 hours agorootparentThe issue is where does this information go. If in a central place what happens if the original directory is moved - how is the metadata updated. - Unix is another file somewhere, Windows can be in the registry. With Apple it is kept with the directory. The issue is that a directory needs some metadata and the Unix design of everything is a file does not allow the directory to include this without adding another file somewhere. The POSIX file system is not the perfect thing. reply smallstepforman 11 hours agorootparentprevYou really want to look at Haiku. The only sane hierarchy for desktop OS’s. Native apps respect the hierarchy, however some ported apps create garbage .files where they shouldn’t (Haiku reserves /home/config/apps/name/… for garbage). /system is read only as a bonus reply ulbu 15 hours agorootparentprevoh man, don’t get me started on gui applications usurping the Documents folder. reply mathnode 20 hours agoparentprevI will raise you- desktop.ini and thumbnails.db reply tredre3 19 hours agorootparentWindows is polite enough to not write them on network shares, unlike .DS_Store. reply lazide 18 hours agorootparentNow, yes. It used to be a really irritating problem there too. reply wasabinator 15 hours agorootparentprevThat's still a weaker hand. macOS also has the ton of ._ files. Would have been better to have folded than raised reply pasc1878 10 hours agorootparentNo macOS does not. The issue is the file system. Apple file systems allow a file to have extended attributes or resource forks. Thus a file is not a simple stream of bytes. When you copy a file to a file system (e.g. FAT) that does not understand these attributes macOS copies those to a ._ (I think if the file system was NTFS then you could probably convert them but I don't think anyone does) Copying a file out of an Apple environment loses data (OK the data is metadata and usually no one cares) reply dang 14 hours agoparentprevWe detached this subthread from https://news.ycombinator.com/item?id=40870645. reply threeseed 20 hours agoparentprev> the competent design would have been to store them locally (and per user) in a central location on the machine doing the browsing Not sure but it could be the case that when you mount a network drive there isn't a stable identifier that can be used to track it. reply organsnyder 20 hours agorootparentSure, that wouldn't work if the network volume was accessed by different URIs. But it would work in 95% of cases, which is good enough. reply DidYaWipe 19 hours agorootparentExactly. And if the same machine used two URIs, there'd simply be two entries for settings. And the settings cache could flush old entries periodically. reply nullindividual 17 hours agorootparentprevLike two websites that look the same, except one captures your creds? You don't want user prefs to apply to multiple locations solely based on URI. reply Wingy 19 hours agorootparentprevStore a single .DS_Store in the root of the disk that stores either the reference or all of the data for that filesystem? reply threeseed 19 hours agorootparentUsers rarely mount network drives as root so not sure how this would work. Also the conflict resolution to support concurrent updates would be crazy. reply DidYaWipe 19 hours agorootparentprevI think it's likely that there is a reasonably stable path for any kind of mount, but I don't know a ton about networks so I'll leave it to someone else to weigh in. But the stakes are very low here, so settings can be invalidated and discarded if they can't be resolved or they age out of the local cache. And if the mount is of a type that can't be reliably identified later, the default should have been to do nothing. Spewing junk all over every computer visited, especially junk that won't even survive the next Mac user's visit... is amateur-hour and obnoxious at best. reply lyu07282 7 hours agoparentprevThe funny part is actually that its not supposed to create DS_store everywhere: > There is also an unfortunate bug that is not fixed to this day that result in an excessive creation of .DS_Store file. Those files should only be created if the user actually makes adjustments to the view settings or set a manual location for icons in a folder. That’s unfortunately not what happens and visiting a folder pretty much guarantees that a .DS_Store file will get created I get the sense that if you are annoyed by it, you aren't the target audience of Mac OS, the target audience are technologically illiterate people for who it really doesn't matter (they barely know what folders are anyway), so to Apple there is no reason to ever invest any effort to fix it. reply DidYaWipe 3 hours agorootparentBy that logic, though, there was never any reason to implement it in the first place. reply dada78641 17 hours agoprev> Back in 1999 I was the technical lead for the Mac OS X Finder at Apple. At that time the Finder code base was some 8 years old and had reached the end of its useful life. Making any changes to it require huge engineering effort, and any changes usually broke two or three seemingly unrelated features. For Mac OS X we decided to rewrite the Finder from scratch. Not that I don't appreciate your work from back then, but as a longtime daily Mac user I cannot wait for the day that this is done once again. The Finder has so many bizarre quirks and it's so slow to proliferate updates that it's just embarrassing. Not to mention it's actually capable of locking up waiting for network access in some circumstances. I don't know what the Finder source code looks like today but I bet it's a similar kind of hell project as the Classic Finder was back then when they first rewrote it, considering how reluctant they are to do anything to it. reply stereo 16 hours agoparentWhen they rewrite it, I’m afraid we’ll get an iPad-esque nerfed and incomplete monstrosity, like we have with the Home or Settings apps. reply aikinai 16 hours agorootparentExactly my thought. When they replace Finder, it’ll almost certainly be with a port of the useless iPad Files app. Apple unfortunately isn’t in the business of making powerful, efficient (user-facing) software anymore. reply meindnoch 16 hours agoparentprevBased on how well the System Preferences → Settings rewrite went: please don't. reply esprehn 16 hours agoparentprevThey did apparently rewrite it in Cocoa back in ~2008. Although that was 16 years ago so I'm sure it's accumulated a fair bit of tech debt since then. reply DEADMINCE 16 hours agoparentprev>The Finder has so many bizarre quirks and it's so slow to proliferate updates that it's just embarrassing Say what you will about Windows, but the Explore file manager has always been pretty rock solid. reply al_borland 16 hours agorootparentI will say, network drives feel local on Windows. On macOS they feel like network drives. I think I’d say the same about external drives. I stopped using them, because I got sick of waiting for them to spin up anytime Finder had to do some work. reply seabird 16 hours agorootparentprevUp until 7, and even afterward in some areas, Windows got things right from an interface standpoint. People seem to forgot that Microsoft dumped large amounts of time and money into figuring out how people use computers and developed their desktop environment accordingly. I've used Windows, macOS, and more Linux DEs than I care to admit. The only thing that tops the Windows DE is KDE, which isn't a massive departure from Windows. macOS has legacy as an excuse, but I don't know what can be said about the various Linux DEs that don't Work Right for the sake of spiting ideas that do. Windows 11 has pretty severely fucked up Explorer. Named directories can't have their path copied (I think 10 did this bullshit, too). The context menu getting insane whitespace, missing options, and having things dynamically load into it is a travesty. It is heartbreaking that mobile-inspired trash is ultimately going to be way you're forced to interact with a computer. People let their distaste for somebody's bad behavior and/or old things stop them from admitting that we're in a pretty severe backward slide. reply Andrex 5 hours agorootparentDynamically-loaded context options (with any user-perceptible lag whatsoever) has to be one the greatest UX sins I can think of. Like apps stealing focus on startup (looking at you, Adobe!) reply marcosdumay 2 hours agorootparent> with any user-perceptible lag whatsoever About that part... Modern computers are insanely fast. How does every single piece of software manages to fill half a minute of CPU or disk I/O for enumerating some 3 or 4 items? It's absurd. I use Firefox inside eatmydata nowadays, because it spends 10 minutes enumerating the same 2 directories every time it starts up (hundreds of thousands of times). The start menu and equivalents everywhere are already famous. Windows can't search files nowadays, not only it doesn't work, but it never ends either... The list is endless. reply nottorp 12 hours agorootparentprevHmm. Wasn't it completely unreliable for moving around large numbers of files at the same time? Like if file #243 of 400 failed for some reason, you could actually lose data? I don't know any more because I use Total Commander on Windows... reply DEADMINCE 9 hours agorootparentI'm not aware of any bugs like that. Got any links maybe? reply nottorp 8 hours agorootparentNo, it may have been windows 95 :) I prefer the good ole two pane file managers and I actively avoid both the finder and explorer most of the time. reply robertoandred 13 hours agorootparentprevExplorer can’t even sort folders by size… reply OptionOfT 12 hours agorootparentThat's because folders have no size. It requires calculating children size recursively. reply sznio 4 hours agorootparentIt could be done quickly by reading the MFT. WizTree can calculate the size of all 236k directories/800k files on my system in two seconds. For some reason, Explorer takes ~10 seconds to calculate the size of a single directory (Program Files, 17k directories, 240k files). If Explorer just did what WizTree does, it could actually show and sort by directory sizes. reply workfromspace 8 hours agoprevWhen I saw `DS_store` inside the title, my eyes automatically filtered out and ignored that word for few seconds. I had to re-read. That's how I see these files. And maybe one day, we can have and edit our own .gitignore -like files for such Inattentional blindness[0]. [0]: Inattentional blindness reply yellow_postit 19 hours agoprevFinder remains one of those apps I still can’t make effective use of. Windows File Explorer for all its warts and changes still “just makes sense” to my brain vs how finder lays things out and expects you to browse. I’ve long since moved to command line or dual pane explorers but it’s something that makes me pause every time I do find myself in Finder for some reason. reply climb_stealth 18 hours agoparentI wholly agree with you on this one. Windows has its fair share of issues, but Windows Explorer feels like peak file browsing to me. For MacOS I can recommend Forklift [0]. I've been using it for years and it is a bit closer to the Windows Explorer way of doing things. Does what it is meant to do. Affordable. No nags. Gets out of the way. Not perfect, but soooo much better than the horrific experience that is Finder. [0] https://binarynights.com/ reply radicality 15 hours agorootparentHow’s Forklift 4? I have a paid Forklift 3, and it’s nagging me to upgrade and pay for next version. I mostly went back to Finder for now, as I remember having some kind of issues with Forklift3 not being performant, though I don’t remember the details. reply climb_stealth 14 hours agorootparentIt seems fine to me. To be honest I don't recall what actually changed from v3. That said I only work on local files and don't use any of the remote workflows. The most advanced feature I use is synchronising files between local storage and SD card. And that works fine. One thing that did break in v4 is that search doesn't work anymore when using the text only toolbar. I reported that ~10 months ago but it's still broken. Maybe I'm the only person who was actually using it. reply wsc981 17 hours agoparentprevI never quite understand why the Finder gets so much hate. Personally I think it’s quite ok. I especially like columns navigation, quite effective for me to get around. It does make me wonder though, how do you feel about System 7.0 Finder? reply jwells89 15 hours agorootparentI have similar feelings about the Finder and also don't quite get the love for Windows Explorer. It's just ok and if it were practical to replace it with just about any common Linux file manager on my Windows boxes I'd do so without a second thought. NeXT/Mac column view are great and should be table stakes in a file manager in my opinion. reply Minor49er 16 hours agoparentprevI found myself in a similar situation. Learning some of the hotkeys in Finder for common tasks really helped me curb that feeling Command + O to open files/folders in Finder was a bit challenging to remember since Enter/Return just works in Explorer reply kfarr 15 hours agorootparentCommand + down arrow also works to open Command + up arrow is a good shortcut to go up one level, surprisingly hard via gui reply userbinator 15 hours agorootparentprevCommand + O to open files/folders in Finder was a bit challenging to remember since Enter/Return just works in Explorer ...and in Finder, Enter is rename, which is a lot more puzzling, so much that many others have commented on the same and some even tried to justify it: https://apple.stackexchange.com/questions/6727/why-does-the-... https://old.reddit.com/r/MacOS/comments/16hxjrn/why_is_the_d... reply robertoandred 13 hours agorootparentprev“O” as in “Open”. It’s the same shortcut in every app. reply klodolph 14 hours agorootparentprevArrow keys are where it’s at. Command up to go up one level, command down to go down one level (open). Always felt like I had to move my hands more on Windows. reply pasc1878 10 hours agoparentprevWhich is annoying as I liked the NeXT file Manager. Agreed on Dual Pane file managers though. I used them on Windows from Windows 3 onwards and various macOS ones except the writers of the macOS ones had nice early versions then decided to rewrite to provide memory hogs that stopped working - e.g. Cocoatech Pathfinder - It is simple just a file browser don't keep adding stuff. reply eek2121 19 hours agoparentprevOh boy, Windows does the same thing (regarding hidden files to sort out FS stuff), but they hide it (just like Apple). We WSL2 users found out the hard way and Microsoft refuses to offer a solution. Relevant issue: https://github.com/microsoft/WSL/issues/7456 Apologies for my post getting snipped, The latest iOS beta keeps randomly eating my text. Apple is aware. reply 0l 16 hours agorootparentUnless im misunderstanding something, these files don't actually exist but reside in the NTFS's alternative data stream, and only display separately in WSL due to ext4 not supporting ADS right? reply pasc1878 10 hours agorootparentWhich then is the same with Apple's ._ files Unix file systems are not sufficient, you need a layer on top. reply deldelaney 16 hours agoprevI miss the old Pre-OSX finder that could accomplish copy files without opening a second window and dragging into. I'll never get how some rocket scientist (IVIE I suspect) removed Apple's best finder feature, colored file folders, which made for easy sorting. To make matters worse, added stupid dot labels instead. What a cluster. Oh well. Still a bad day on a Mac is better than a great day in Windows. reply pasc1878 10 hours agoparentSee John Siracusa\"S comments on Finder https://arstechnica.com/gadgets/2003/04/finder/ reply l33tman 20 hours agoprev [–] Maybe unrelated to this, but I noticed fairly recently that my backups from my macbook now backup seemingly randomly modified pdf and txt files all over the disk. My guess is that whenever I search for something, it decides to touch a couple of hundred files (but not ALL pdf/txt files for some reason). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      ".DS_Store files, commonly seen when transferring files from Mac to Windows, stand for \"Desktop Services Store,\" originating from a 1999 rewrite of the Mac OS X Finder.",
      "The Finder was split into a user interface (Finder_FE) and core functionality (Finder_BE), with plans to make the backend a public API called Desktop Services, though it was never fully released.",
      "A bug causes excessive creation of .DS_Store files, even without user adjustments, making them a persistent issue for Mac users."
    ],
    "commentSummary": [
      "The discussion revolves around the historical context and technical details of the DS_store file and the \"fork\" concept in Mac file systems, which includes both resource and data components.",
      "The resource fork in early MacOS stored various application data like icons, menus, and executable code, which posed challenges when transferring files to non-Mac systems.",
      "The transition from MacOS to MacOS X involved significant changes, including the removal of resource forks, which was met with mixed reactions from the user community."
    ],
    "points": 375,
    "commentCount": 200,
    "retryCount": 0,
    "time": 1720043713
  },
  {
    "id": 40869877,
    "title": "Xcapture-BPF – like Linux top, but with Xray vision",
    "originLink": "https://0x.tools/",
    "originBody": "0x.tools X-Ray vision for Linux systems By Tanel Poder 0x.tools (GitHub) is a set of open-source utilities for analyzing application performance on Linux. It has a goal of deployment simplicity and minimal dependencies, to reduce friction of systematic troubleshooting. There’s no need to upgrade the OS, install kernel modules, heavy monitoring frameworks, Java agents or databases. 0x.tools allow you to measure individual thread level activity, like thread executed code, sleep states, system calls and kernel wait locations - by tracking (not tracing) and then sampling the right events at the right time. xcapture-bpf & xtop 2.0.2 beta TL;DR This is what you get with the latest eBPF update: This (2-minute) ascii-cast box below is sized pretty high for a reason, play it and you’ll see, command line nerds should love it ;-) xcapture-bpf (and xtop) are like the Linux top tool, but extended with x-ray vision and ability to view your performance data from any chosen angle (that eBPF allows to instrument). You can use it for system level overview and drill down into indivual threads’ activity and soon even into individual kernel events like lock waits or memory stalls. eBPF is not only customizable, it’s completely programmable and I plan to take full advantage of it. I have so far implemented less than 5% of everything this method and the new tool is capable of, stay tuned for more! xcapture-bpf terminal highlighting and stacktiles in action I included a screenshot image below, to show how the terminal text search/highlighting and scrolling capabilities work nicely together with my new stacktiles formatting method, this way you can fit more relevant things on your screen and not have to switch windows or scroll around that much, while keeping some structure and sanity in place with all the stack traces. The stacktiles do not have to contain only stacks of function names, but could contain other things, like filenames or any other thing, like top memory allocation reasons (and amounts) done under a code location reported below, etc. xcapture-bpf installation xcapture-bpf is still in beta, don’t run it on busy production systems yet. As it uses eBPF (and currently BCC with python3 as a reporting frontend), you’d need to be at least on RHEL 8.1 (or a clone) or Ubuntu 24.04. Ubuntu 22.04’s BCC has some kernel header compatibility issue and the 20.04 kernel does not have the required eBPF features available. These are the only versions I’ve tested with so far, on x86_64 and arm64 platforms. If you try it out on any other distros/platforms/versions, please tell me the results! On RHEL8, you can install the prerequisites with this: $ sudo dnf install bcc bcc-tools python3 python3-bcc $ git clone git@github.com:tanelpoder/0xtools.git $ ls -l 0xtools/bin/xcapture-bpf* -rwxrwxr-x. 1 tanel tanel 25724 Jul 2 22:04 0xtools/bin/xcapture-bpf -rw-rw-r--. 1 tanel tanel 12127 Jul 2 15:34 0xtools/bin/xcapture-bpf.c $ cat 0xtools/bin/xtop #!/usr/bin/bash CURDIR=\"$(dirname \"$(realpath \"$0\")\")\" ${CURDIR}/xcapture-bpf --xtop --clear-screen $* $ cd 0xtools/bin $ sudo ./xtop If you don’t want to clone/download the whole 0xtools repository, then for xcapture-bpf, you only need the 2 xcapture-bpf* files listed above. No need to compile the .c file as the BCC toolset takes care of it on the fly. xtop is just a simple shell wrapper for convenience (and now there’s an “xtop” in the Linux command line namespace! ;-) bcc-tools are not really needed for xcapture-bpf itself, but they’re worth checking out, if you’re gonna play with eBPF tools anyway. xcapture-bpf launch video (2024-06-25) I have uploaded my 0xtools v2 beta (with eBPF) nerd-launch video here: Slides, code, discussion here: https://github.com/tanelpoder/0xtools/discussions/38 The details about the rest of the 0xtools are below (all the other tools just read various /proc files, no eBPF needed for them). Table of Contents Included Tools Example Output Installation & Usage FAQ What’s next Articles Included Tools You get two classes of utilities: Real-time interactive tools for analyzing current system behavior as it is happening. Low-overhead thread activity samplers for always-on low-frequency profiling of production systems. The continuously captured data allows you to “go back in time” and systematically troubleshoot even intermittent problems right after (or during) their first occurrence. Command Description psn Show current top thread activity by sampling /proc files xcapture Low-overhead thread state sampler reading /proc files xcapture-bpf Low-overhead programmable thread state sampler build with eBPF (beta) syscallargs List all system calls with their arguments schedlat Show single process’es CPU scheduling latency as a % of its runtime run_xcapture.sh A simple “daemon” script for keeping xcapture running run_xcpu.sh Low-frequency continuous stack sampling for threads on CPU (using perf) xcapture is written in C for efficiency reasons and it consists of just a single C source file and a single header file for system call name translation. All other tools are Python or shell scripts. Usage & Example Output Sample Linux thread activity and show fixed-width output on screen: $ xcapture 0xTools xcapture v1.0 by Tanel Poder [https://0x.tools] Sampling /proc... DATE TIME PID TID USERNAME ST COMMAND SYSCALL WCHAN 2020-10-17 12:01:50.583 6404 7524 mysql R (mysqld) fsync wait_on_page_bit 2020-10-17 12:01:50.583 6404 8944 mysql D (mysqld) fsync wait_on_page_bit 2020-10-17 12:01:50.583 6404 8946 mysql D (mysqld) fsync wait_on_page_bit 2020-10-17 12:01:50.583 6404 76046 mysql D (mysqld) fsync wait_on_page_bit 2020-10-17 12:01:50.583 6404 76811 mysql D (mysqld) fdatasync xfs_log_force_lsn 2020-10-17 12:01:50.583 6404 76815 mysql D (mysqld) fsync blkdev_issue_flush 2020-10-17 12:01:50.583 8803 8803 root R (md10_resync) [running] 0 DATE TIME PID TID USERNAME ST COMMAND SYSCALL WCHAN 2020-10-17 12:01:51.623 6404 7521 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 7524 mysql D (mysqld) fsync xfs_log_force_lsn 2020-10-17 12:01:51.623 6404 7767 mysql D (mysqld) fsync xfs_log_force_lsn 2020-10-17 12:01:51.623 6404 8398 mysql D (mysqld) fsync call_rwsem_down_read_failed 2020-10-17 12:01:51.623 6404 5446 mysql D (mysqld) fsync xfs_log_force_lsn 2020-10-17 12:01:51.623 6404 8941 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 8944 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 8945 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 76045 mysql D (mysqld) fsync call_rwsem_down_read_failed 2020-10-17 12:01:51.623 6404 76046 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 76810 mysql D (mysqld) pwrite64 xfs_file_buffered_aio_write 2020-10-17 12:01:51.623 6404 76811 mysql D (mysqld) fdatasync xfs_log_force_lsn 2020-10-17 12:01:51.623 6404 76812 mysql D (mysqld) fsync wait_on_page_bit 2020-10-17 12:01:51.623 8803 8803 root D (md10_resync) [no_syscall] msleep Watch a SVG video of xcapture in action! Sample threads in all states (including Sleeping) and write output into hourly CSV files: $ xcapture -a -o /data/xcap & $ head 2020-10-16.21.csv TS,PID,TID,USERNAME,ST,COMMAND,SYSCALL,WCHAN,EXE,CMDLINE,KSTACK 2020-10-16 21:00:00.001,5335,5335,root,R,(collectl),[running],0,perl,/usr/bin/perl, 2020-10-16 21:00:00.001,8803,8803,root,D,(md10_resync),[no_syscall],msleep,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync()->msleep() 2020-10-16 21:00:01.038,8803,8803,root,R,(md10_resync),[no_syscall],md_do_sync,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync() 2020-10-16 21:00:02.075,8803,8803,root,D,(md10_resync),[no_syscall],md_do_sync,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync() 2020-10-16 21:00:02.075,16762,16762,oracle,R,(ora_m000_lin19c),[running],0,oracle,ora_m000_LIN19C,->do_blockdev_direct_IO()->dio_complete() 2020-10-16 21:00:03.112,8803,8803,root,R,(md10_resync),[no_syscall],md_do_sync,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync() 2020-10-16 21:00:04.149,8803,8803,root,D,(md10_resync),[no_syscall],msleep,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync()->msleep() 2020-10-16 21:00:05.186,8803,8803,root,D,(md10_resync),[no_syscall],md_do_sync,-,-,->ret_from_fork_nospec_begin()->kthread()->md_thread()->md_do_sync() 2020-10-16 21:00:05.186,65913,65913,oracle,D,(ora_ckpt_lin122),pwrite64,blkdev_issue_flush,oracle,ora_ckpt_LIN122,->system_call_fastpath()->SyS_pwrite64()->vfs_write()->do_sync_write()->xfs_file_aio_write()->generic_write_sync()->xfs_file_fsync()->xfs_blkdev_issue_flush()->blkdev_issue_flush() You can “Query” the thread activity history for performance analysis on the command line (or just load the CSV into any database): Query CSV files with standard Linux text processing tools. It’s like SQL but with different keywords: grep for filtering, cut, awk for column projection, uniq for group by and sort for ordering. Filename patterns like cat 2020-10-??.0[89].csv could be used for scanning through only the files of interest (partition pruning): $ cat 2020-10-13.01.csvawk -F, '{ printf(\"%2s %-20s %-20s %s\",$5,$4,$7,$10) }'sortuniq -csort -nbrhead -20 2303 D root read - 1761 R tanel [running] stress 1384 D postgres pread64 postgres: tanel pgbench [local] UPDATE 894 R root [running] - 229 R root read - 229 D mysql fsync /usr/sbin/mysqld 144 R tanel [running] - 115 - - - - 110 D oracle io_submit ora_ckpt_LINPRD 101 D root [running] - 73 D root read dd 58 R root [running] /opt/oracle.ahf/jre/bin/java 55 R mysql [running] /usr/sbin/mysqld 52 D tanel [no_syscall] stress 51 R oracle [running] oracleLIN19C 50 R root [running] dd 35 R oracle [running] xe_mz01_XE 32 R tanel [running] /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/bin/java 29 R oracle [running] pidstat 27 D oracle pread64 oracleLIN19C Or you can query CSV files with q-text-as-data: $ q -d, -bTH ' select count(*) avgthr, username,st,syscall,wchan from 2020-10-13.01.csv group by username,st,syscall,wchan order by 1 desc'head -20 1955 tanelR [running]0 1384 postgres D pread64generic_file_read_iter 1084 rootD readraise_barrier 1041 rootR [running]0 712rootD readmsleep 341oracleR [running]0 317rootD readmd_super_wait 123mysqlD fsync__xfs_log_force_lsn 115-- -- 92oracleD io_submitmd_write_start 92rootR readraise_barrier 79rootD readwait_barrier 66oracleR nanosleephrtimer_nanosleep 66rootD [running]0 52mysqlR [running]0 51rootR readworker_thread 48mysqlD fsyncsubmit_bio_wait 48rootD read0 41tanelD [no_syscall] rq_qos_wait 39rootD readmd_bitmap_cond_end_sync Or you can do tabular data analysis in your terminal with the awesome VisiData tool. Note that the video below does not have sound (it’s not your computer :-) This brings me back memories of Lotus 1-2-3 on a crappy 286 with floppy drives and MS-DOS! Installation & Usage xcapture, schedlat and psn sample the Linux /proc filesystem just like standard tools like ps, top and lsof do. The /proc filesystem is essentially Linux kernel presenting useful metrics into userspace as user-readable files. So, you do not need any additional Linux configuration or anything fancy to be installed on your hosts. 0x.tools require Linux kernel version 2.6 or later, so they will work even on your legacy installations (like RHEL 5, CentOS 5) from 15 years ago. For running psn on CentOS 5 (RHEL 5 clones), you need to have Python 2.6+ on it (it can be installed from EPEL repo). $ git clone https://github.com/tanelpoder/0xtools $ make $ sudo make install Running 0xTools utilities: xCapture $ xcapture 0x.Tools xcapture v1.0 by Tanel Poder [https://0x.tools] Usage: xcapture [options] By default, sample all /proc tasks in states R, D every second and print to stdout Options: -a capture tasks in additional states, even the ones Sleeping (S) -A capture tasks in All states, including Zombie (Z), Exiting (X), Idle (I) -cprint additional columns (for example: -c exe,cmdline,kstack) -dseconds to sleep between samples (default: 1) -Ecustom task state Exclusion filter (default: XZIS) -h display this help message -owrite wide output into hourly CSV files in this directory instead of stdout $ xcapture -c exe,kstack $ xcapture -o . $ xcapture -o /data/perf_archive/xcap Linux Process Snapper Linux Process Snapper is a Python script meant for troubleshooting currently on-going issues (no historical capture). It currently reports more fields directly from /proc than xcapture captures (like filenames accessed by IO system calls). I plan to improve this tool so that it could use xcapture CSV files as an input, in addition to current real-time monitoring. IO bottleneck example: My “pipeline” is bottlenecked by writes to the output file, not input reads: $ psn -p 18286 -G syscall,filename Linux Process Snapper v0.14 by Tanel Poder [https://0x.tools] Sampling /proc/stat, syscall for 5 seconds... finished. === Active Threads ================================================================================== samplesavg_threadscommstatesyscallfilename ----------------------------------------------------------------------------------------------------- 790.79(dd)Disk (Uninterruptible)write/backup/tanel/test (stdout) 70.07(dd)Disk (Uninterruptible)[running] |50.05(dd)Running (ON CPU)write/backup/tanel/test (stdout) 40.04(dd)Disk (Uninterruptible)read/reco/fio/mmapfile.0.0 (stdin) 30.03(dd)Running (ON CPU)[running] |20.02(dd)Running (ON CPU)read/reco/fio/mmapfile.0.0 (stdin) ``` MySQL I/O bottleneck example: there’s some OS kernel inode level semaphore contention due to frequent use of fsync(): $ sudo psn -p \"mysqld|kwork\" -G syscall,wchan Linux Process Snapper v0.14 by Tanel Poder [https://0x.tools] Sampling /proc/syscall, stat, wchan for 5 seconds... finished. === Active Threads ======================================================================================== samplesavg_threadscommstatesyscallwchan ----------------------------------------------------------------------------------------------------------- 253.12(mysqld)Disk (Uninterruptible)fsync_xfs_log_force_lsn 162.00(mysqld)Running (ON CPU)[running]0141.75(mysqld)Disk (Uninterruptible)pwrite64call_rwsem_down_write_failed 81.00(mysqld)Disk (Uninterruptible)fsync_bio_wait 40.50(mysqld)Disk (Uninterruptible)pread64io_schedule 40.50(mysqld)Disk (Uninterruptible)pwrite64io_schedule 30.38(mysqld)Disk (Uninterruptible)pread64030.38(mysqld)Running (ON CPU)[running]io_schedule 30.38(mysqld)Running (ON CPU)pread64020.25(mysqld)Disk (Uninterruptible)[running]010.12(kworker/*:*)Running (ON CPU)readworker_thread 10.12(mysqld)Disk (Uninterruptible)fsyncio_schedule 10.12(mysqld)Disk (Uninterruptible)futexcall_rwsem_down_write_failed 10.12(mysqld)Disk (Uninterruptible)poll010.12(mysqld)Disk (Uninterruptible)pwrite64_xfs_log_force_lsn 10.12(mysqld)Running (ON CPU)fsync_bio_wait 10.12(mysqld)Running (ON CPU)futexfutex_wait_queue_me More info and examples are available at Tanel Poder’s Linux Performance Troubleshooting Page SchedLat $ ./schedlat.py 29801 SchedLat by Tanel Poder [https://0x.tools] PID=29801 COMM=oracle_29801_li TIMESTAMP %CPU %LAT %SLP 2020-02-26 23:17:35 100.0 0.0 0.0 <<-- no CPU shortage, process 100% on CPU 2020-02-26 23:17:36 100.0 0.0 0.0 2020-02-26 23:17:37 100.0 0.0 0.0 2020-02-26 23:17:38 100.0 0.0 0.0 <<-- %SLP = 100-(%CPU+%LAT), when Linux reports slightly 2020-02-26 23:17:39 98.0 0.0 2.0 more than \"100%\" of CPU+LAT, then the derived 2020-02-26 23:17:40 0.0 0.0 100.0 \"remaining time\" SLP% may show a negative value 2020-02-26 23:17:41 0.0 0.0 100.0 2020-02-26 23:17:42 0.0 0.0 100.0 <<-- no CPU shortage, process sleeping 2020-02-26 23:17:43 0.4 0.0 99.6 2020-02-26 23:17:44 33.5 0.2 66.3 <<-- no CPU shortage, process doing synchronous I/Os 2020-02-26 23:17:45 55.5 0.2 44.2 in a loop (thus taken off CPU frequently by scheduler) 2020-02-26 23:17:46 53.9 0.2 45.9 2020-02-26 23:17:47 54.5 0.2 45.3 2020-02-26 23:17:48 59.1 0.2 40.7 2020-02-26 23:17:49 4.4 0.0 95.6 2020-02-26 23:17:50 58.5 0.1 41.4 2020-02-26 23:17:51 95.7 0.0 4.3 2020-02-26 23:17:52 0.3 0.0 99.7 2020-02-26 23:17:53 0.1 0.0 99.9 2020-02-26 23:17:54 0.1 0.0 99.9 2020-02-26 23:17:55 0.3 1.1 98.6 2020-02-26 23:17:56 0.1 6.0 93.9 2020-02-26 23:17:57 0.1 15.0 84.9 2020-02-26 23:17:58 0.1 13.8 86.1 2020-02-26 23:17:59 9.6 61.4 29.0 <<-- CPU shortage + process doing synchronous I/Os in a loop 2020-02-26 23:18:00 14.6 83.9 1.5 <<-- and spending more time in CPU runqueue after every I/O 2020-02-26 23:18:01 31.4 59.7 8.9 2020-02-26 23:18:02 13.0 13.9 73.1 2020-02-26 23:18:03 0.3 5.3 94.4 There are more details in my Measuring Linux CPU Scheduling Latency blog entry. CPU profiling When you look into the run_xcpu.sh, you’ll see that I’m currently using just perf under the hood with 1 Hz frequency. You can have it always-on no noticeable performance overhead! $ cat bin/run_xcpu.sh ... perf record -g -F 1 -a \\ --switch-output=1m \\ --timestamp-filename \\ --timestamp \\ -o $1/xcpu ... With the above arguments, perf writes the sampled on-CPU stack traces into 1-minute granularity files. Then all you need to do is run perf on the file with the right timestamp, to zoom in to the time of your performance problem: $ perf report -s sym,dso -i xcpu.2020101619323791 Perf CPU usage profile, including kernel-mode and interrupts CPU usage FAQ How is the 0x.tools toolset licensed? 0x.tools is an open source, GPL v3-licensed product, so you can use it like most other standard command line tools in your Linux distribution. What is the measurement overhead? 0x.tools xcapture is designed to have very low overhead, well under 1% of your server’s CPU capacity, even when sampling every second. Note that xcapture does not invoke any tracing, but samples already built-in kernel instrumentation from /proc file system asynchronously and independently. Therefore it won’t slow any of your existing applications down, but uses a small percentage of one CPU in the system for its sampling. In extreme cases (with tens of thousands of active threads), you can reduce sampling frequency to reduce xcapture CPU usage. The run_xcpu.sh CPU sampling script uses standard Linux perf utility under the hood, with just 1 Hz sampling rate by default. Thanks to the low-frequency sampling, perf will not cause noticeable overhead for your applications. Is it safe to use in production? 0x.tools are designed to be safely used in production, including traditional enterprise environments where you can’t just upgrade to latest OS version at will or load custom kernel modules. All the code is open source, without any dependencies outside the standard Linux utilities and libraries, skimming through a few hundred lines of 0x.tools C and Python code should be doable in matter of minutes. As with all software and tools, I recommend to try them first on a test system (ideally similar to production) and see how it works, before deploying to production. Why not just use perf for everything (including xcapture)? Perf sampling captures only on-CPU activity by default. If you have 32 CPUs, it will check what code is running on them at every sample, but does not aim to walk through the hundreds (or thousands) of OS threads that happen to be sleeping. While it is possible to enable tracing for off-cpu events in Perf, it comes with a high tracing overhead (and later, overhead of post-processing these high-frequency events). Why not just use BPF instead of /proc sampling? In short, eBPF is not available for wide-scale production use in traditional enterprises (think banks, telcos and other Fortune 500s with decades of IT history). This may come as a surprise if you’ve worked only for startups running latest ephemeral Ubuntu containers in the cloud :-) For example RedHat started actually supporting eBPF in RHEL 8.1 (Released Nov 2019). The enterprises I work with, still have RHEL6 (kernel 2.6.32) as their mostly widely used OS version, with RHEL7 (and CentOS 7) gaining traction. So “let’s just do a major OS upgrade” for troubleshooting this performance spike is out of the question. Nevertheless, I have written an eBPF sampler prototype already, it combines both thread state and CPU usage profiling into one tool. But I wanted to productionize the simpler, widely available /proc file-based profiler first, for practical reasons. Why not just use distributed tracing like OpenTracing, Zipkin, Jaeger? These powerful, but complex frameworks are high level end-to-end tracers of request flow through application layers and components. They are designed to point out in which component of your distributed multi-tier system most of the user response was spent, but they do not drill down into the reason why. 0x.tools are designed to fill that gap. Why not just use something like Prometheus? Prometheus is designed for shipping, storing and serving system & appliction time-series metrics captured from a large fleet of servers & applications. You can plot nice dashboards with charts showing various latency, request count and system utilization metrics over time. Such time-series metrics are useful background info, but do not allow you to drill down into the low level reasons of increased system activity, application resource usage or misbehavior of the OS kernel itself. What’s next? There are a lot of new features and utilities that can be added to 0xTools suite. Before I go there, I will work on some packaging & productionization things first (scripts for automatic compression & archiving of the captured files, installation via a RPM/DEB package, built-in data visualization). Feel free to submit ideas and issues in the 0x.Tools GitHub repo. I also deliver consulting and training around systematic Linux troubleshooting & tuning, including helping you to come up with a strategy for rolling out always-on profiling for production systems in your company. Get 0x.Tools updates via Twitter @0xtools. Articles Linux performance & troubleshooting articles by Tanel Poder Profiling Linux Activity for Performance And Troubleshooting video by Tanel Poder Using 0xtools with MySQL series by Valerii Kravchuk Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=40869877",
    "commentBody": "Xcapture-BPF – like Linux top, but with Xray vision (0x.tools)353 points by tanelpoder 22 hours agohidepastfavorite35 comments __turbobrew__ 19 hours agoI use BCC tools weekly to debug production issues. Recently I found we were massively pressuring page caches due to having a large number of loopback devices with their own page cache. Enabling direct io on the loopback devices fixed the issue. eBPF is really a superpower, it lets you do things which are incomprehensible if you don’t know about it. reply jyxent 17 hours agoparentI've been learning BCC / bpftrace recently to debug a memory leak issue on a customer's system, and it has been super useful. reply tptacek 18 hours agoparentprevI'd love to hear more of this debugging story! reply __turbobrew__ 16 hours agorootparentContainers are offered block storage by creating a loopback device with a backing file on the kubelet’s file system. We noticed that on some very heavily utilized nodes that iowait was using 60% of all the available cores on the node. I first confirmed that nvme drives were healthy according to SMART, I then worked up the stack and used BCC tools to look at block io latency. Block io latency was quite low for the NVME drives (microseconds) but was hundreds of milliseconds for the loopback block devices. This lead me to believe that something was wrong with the loopback devices and not the underlying NVMEs. I used cachestat/cachetop and found that the page cache miss rate was very high and that we were thrashing the page cache constantly paging in and out data. From there I inspected the loopback devices using losetup and found that direct io was disabled and the sector size of the loopback device did not match the sector size of the backing filesystem. I modified the loopback devices to use the same sector size as the block size of the underlying file system and enabled direct io. Instantly, the majority of the page cache was freed, iowait went way down, and io throughout went way up. Without BCC tools I would have never been able to figure this out. Double caching loopback devices is quite the footgun. Another interesting thing we hit is that our version of losetup would happily fail to enable direct io but still give you a loopback device, this has since been fixed: https://github.com/util-linux/util-linux/commit/d53346ed082d... reply M_bara 1 hour agorootparentWe had something similar about 10 years ago where I worked. Customer instances were backed via loopback devices to local disks. We didn’t think of this - face palm - on the loop back devices. What we ended up doing was writing a small daemon to posix fadvise the kernel to skip the page cache… your solution is way simpler and more elegant… hats off to you reply FooBarWidget 8 hours agorootparentprevWhich container runtime are you using? As far as I know both Docker and containerd use overlay filesystems instead of loopback devices. And how did you know that tweaking the sector size to equal the underlying filesystem's block size would prevent double caching? Where can one get this sort of knowledge? reply __turbobrew__ 4 hours agorootparentThe loopback devices came from a CSI which creates a backing file on the kubelet’s filesystem and mounts it into the container as a block device. We use containerd. I knew that enabling direct io would most likely disable double caching because that is literally the point of enabling direct io on a loopback device. Initially I just tried enabling direct io on the loopback devices, but that failed with a cryptic “invalid argument” error. After some more research I found that direct IO needs the sector size to match the filesystems block size in some cases to work. reply jauntywundrkind 14 hours agorootparentprevThere's also either Composefs or Puzzlefs, both of which attempt to let the page cache work across containers! https://github.com/containers/composefs https://github.com/project-machine/puzzlefs reply metroholografix 17 hours agoprevFolks who find this useful might also be interested in otel-profiling-agent [1] which Elastic recently opensourced and donated to OpenTelemetry. It's a low-overhead eBPF-based continuous profiler which, besides native code, can unwind stacks from other widely used runtimes (Hotspot, V8, Python, .NET, Ruby, Perl, PHP). [1] https://github.com/elastic/otel-profiling-agent reply kbouck 11 hours agoparentGrafana has one too called Beyla. https://grafana.com/oss/beyla-ebpf/ reply 3abiton 12 hours agoparentprevI am trying to wrap my head around it, still unclear what it does l. reply zikohh 59 minutes agorootparentThat's like most of Grafana's documentation reply malkia 19 hours agoprevRelatively how expensive is to capture the callstack when doing sample profiling? With Intel CET's tech there should be way to capture a shadow stack, that really just contains entry points, but wondering if that's going to be used... reply tanelpoder 18 hours agoparentThe on-cpu sample profiling is not a big deal for my use cases as I don't need the \"perf\" sampling to happen at 10kHz or anything (more like 10-1Hz, but always on). But the sched_switch tracepoint is the hottest event, without stack sampling it's 200-500ns per event (on my Xeon 63xx CPUs), depending on what data is collected. I use #ifdefs to compile in only the fields that are actually used (smaller thread_state struct, fewer branches and instructions to decode & cache). Surprisingly when I collect kernel stack, the overhead jumps higher up compared to user stack (kstack goes from say 400ns to 3200ns, while ustack jumps to 2800ns per event or so). I have done almost zero optimizations (and I figure using libbpf/BTF/CO-RE will help too). But I'm ok with these numbers for most of my workloads of interest, and since eBPF programs are not cast in stone, can do further reductions, like actually sampling stacks in the sched_switch probe on every 10th occurrence or something. So in worst case, this full-visibility approach might not be usable as always-on instrumentation for some workloads (like some redis/memcached/mysql lookups doing 10M context switches/s on a big server), but even with such workloads, a temporary increase in instrumentation overhead might be ok, when there are known recurring problems to troubleshoot. reply malkia 16 minutes agorootparentAwesome info!!! Thanks a lot! reply jamesy0ung 21 hours agoprev [–] I’ve never used eBPF, does anyone have some good resources for learning it? reply tanelpoder 21 hours agoparentBrendan Gregg's site (and book) is probably the best starting point (he was involved in DTrace work & rollout 20 years ago when at Sun) and was/is instrumental in pushing eBPF in Linux even further than DTrace ever went: https://brendangregg.com/ebpf.html reply bcantrill 18 hours agorootparentJust a quick clarification: while Brendan was certainly an active DTrace user and evangelist, he wasn't involved in the development of DTrace itself -- or its rollout. (Brendan came to Sun in 2006; DTrace was released in 2003.) As for eBPF with respect to DTrace, I would say that they are different systems with different goals and approaches rather than one eclipsing the other. (There are certainly many things that DTrace can do that eBPF/BCC cannot, some of the details of which we elaborated on in our 20th anniversary of DTrace's initial integration.[0]) Edit: We actually went into much more specific detail on eBPF/BCC in contrast to DTrace a few weeks after the 20th anniversary podcast.[1] [0] https://www.youtube.com/watch?v=IeUFzBBRilM [1] https://www.youtube.com/watch?v=mqvVmYhclAg#t=12m7s reply tanelpoder 18 hours agorootparentThanks, yes I was more or less aware of that (I'd been using DTrace since Solaris 10 beta in 2004 or 2003?)... By rollout I really meant \"getting the word out there\"... that's half the battle in my experience (that's why this post here! :-) What I loved about DTrace was that once it was out, even in beta, it was pretty complete and worked - all the DTrace ports that I've tried, including on Windows (!) a few years ago were very limited or had some showstopper issues. I guess eBPF was like that too some years ago, but by now it's pretty sweet even for more regular consumer who don't keep track of its development. Edit: Oh, wasn't aware of the timeline, I may have some dates (years) wrong in my memory reply abrookewood 18 hours agorootparentprevYes, not involved in DTrace itself, but he did write a bunch of DTrace Tools which led to an interesting meeting with a Sun exec: https://www.brendangregg.com/blog/2021-06-04/an-unbelievable... reply anonfordays 13 hours agorootparentprev>As for eBPF with respect to DTrace, I would say that they are different systems with different goals and approaches For sure. Different systems, different times. >rather than one eclipsing the other. It does seem that DTrace has been eclipsed though, at least in Linux (which runs the vast majority of the world's compute). Is there a reason to use DTrace over eBPF for tracing and observability in Linux? >There are certainly many things that DTrace can do that eBPF/BCC cannot This may be true, but that gap is closing. There are certainly many things that eBPF can do that DTrace cannot, like Cilium. reply tanelpoder 13 hours agorootparentPerhaps familiarity with the syntax of DTrace if coming from Solaris-heavy enterprise background. But then again, too many years have passed since Solaris was a major mainstream platform. Oracle ships and supports DTrace on (Oracle) Linux by the way, but DTrace 2.0 on Linux is a scripting frontend that gets compiled to eBPF under the hood. Back when I tried to build xcapture with DTrace, I could launch the script and use something like /pid$oracle::func:entry/ but IIRC the probe was attached only to the processes that already existed and not any new ones that were started after loading the DTrace probes. Maybe I should have used some lower level APIs or something - but eBPF on Linux automatically handles both existing and new processes. reply bch 2 hours agorootparent> eBPF on Linux automatically handles both existing and new processes Without knowing your particular case, DTrace does too - it’d certainly be tricky to use if you’re trying to debug software that “instantly crashes on startup” if it couldn’t do that. “execname” (not “pid”) is where I’d look, or perhaps that part of the predicate is skipable; regardless, should be possible. reply mgaunard 21 hours agoparentprevIt lets you hook into various points in the kernel; ultimately you need to learn how the Linux kernel is structured to make the most of it. Unlike a module, it can only really read data, not modify data structures, so it's nice for things like tracing kernel events. The XDP subsystem is particularly designed for you to apply filters to network data before it makes it to the network stack, but it still doesn't give you the same level of control or performance as DPDK, since you still need the data to go to the kernel. reply tanelpoder 21 hours agorootparentYep (the 0x.tools author here). If you look into my code, you'll see that I'm not a good developer :-) But I have a decent understanding of Linux kernel flow and kernel/app interaction dynamics, thanks to many years of troubleshooting large (Oracle) database workloads. So I knew exactly what I wanted to measure and how, just had to learn the eBPF parts. That's why I picked BCC instead of libbpf as I was somewhat familiar with it already, but fully dynamic and \"self-updating\" libbpf loading approach is the goal for v3 (help appreciated!) reply tptacek 18 hours agorootparentI was going to ask \"why BCC\" (BCC is super clunky) but you're way ahead of us. This is great work, thanks for posting it. reply tanelpoder 18 hours agorootparentYeah, I already see limitations, the last one was yesterday when I installed earlier Ubuntu versions to see how far back this can go - and even Ubuntu 22.04 didn't work out of the box, ended up with some BCC/kernel header mismatch issue [1] although the kernel itself supported it. A workaround was to download & compile the latest BCC yourself, but I don't want to go there as the customers/systems I work on wouldn't go there anyway. But libbpf with CO-RE will solve these issues as I understand, so as long as the kernel supports what you need, the CO-RE binary will work. This raises another issue for me though, it's not easy, but easier, for enterprises to download and run a single python + single C source file (with <500 code lines to review) than a compiled CO-RE binary, but my long term plan/hope is that I (we) get the RedHats and AWSes of this world to just provide the eventual mature release as a standard package. [1] https://github.com/iovisor/bcc/issues/3993 reply mgaunard 20 hours agorootparentprevMyself I've only built simple things, like tracing sched switch events for certain threads, and killing the process if they happen (specifically designed as a safety for pinned threads). reply tanelpoder 20 hours agorootparentSame here, until now. I built the earlier xcapture v1 (also in the repo) about 5 years ago and it just samples various /proc/PID/task/TID pseudofiles regularly, it also allows you get pretty far with the thread-level activity measurement approach, especially when combined with always-on low frequency on-CPU sampling with perf. reply tptacek 19 hours agorootparentprevXDP, in its intended configuration, passes pointers to packets still on the driver DMA rings (or whatever) directly to BPF code, which can modify packets and forward them to other devices, bypassing the kernel stack completely. You can XDP_PASS a packet if you'd like it to hit the kernel, creating an skbuff, and bouncing it through all the kernel's network stack code, but the idea is that you don't want to do that; if you do, just use TC BPF, which is equivalently powerful and more flexible. reply mgaunard 7 hours agorootparentYes for XDP there is a dedicated API, but for any of the other hooks like tracepoints, it's all designed to give you read-only access. The whole CO-RE thing is about having a kernel-version-agnostic way of reading fields from kernel data structures. reply tptacek 2 hours agorootparentRight, I'm just pushing back on the DPDK thing. reply lathiat 17 hours agoparentprevI'll toot my own horn here. But there are plenty of presentations about it, Brendan Gregg's are usually pretty great. \"bpftrace recipes: 5 real problems solved\" - Trent Lloyd (Everything Open 2023) https://www.youtube.com/watch?v=ZDTfcrp9pJI reply jiripospisil 21 hours agoparentprevThere's a bunch of examples over at https://github.com/iovisor/bcc reply rascul 20 hours agoparentprev [–] You might find some interesting stuff here https://ebpf.io/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "0x.tools is a set of open-source utilities designed for analyzing application performance on Linux, emphasizing simplicity and minimal dependencies.",
      "Key features include measuring individual thread-level activity and providing eBPF-based tools for system-level and detailed thread activity analysis.",
      "It is designed for safe use in production environments with very low overhead and does not require OS upgrades or heavy monitoring frameworks."
    ],
    "commentSummary": [
      "Xcapture-BPF is a new tool likened to Linux's top command but with enhanced capabilities, often referred to as having \"Xray vision\" for system diagnostics.",
      "Users have shared experiences of using eBPF (extended Berkeley Packet Filter) and BCC (BPF Compiler Collection) tools to debug complex production issues, highlighting their effectiveness in resolving performance bottlenecks and memory leaks.",
      "The discussion includes practical examples of troubleshooting, such as resolving high iowait and page cache issues in containerized environments by enabling direct IO and matching sector sizes on loopback devices."
    ],
    "points": 353,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1720039957
  },
  {
    "id": 40869461,
    "title": "AI's $600B Question",
    "originLink": "https://www.sequoiacap.com/article/ais-600b-question/",
    "originBody": "AI’s $600B Question The AI bubble is reaching a tipping point. Navigating what comes next will be essential. By David Cahn Published June 20, 2024 In September 2023, I published AI’s $200B Question. The goal of the piece was to ask the question: “Where is all the revenue?” At that time, I noticed a big gap between the revenue expectations implied by the AI infrastructure build-out, and actual revenue growth in the AI ecosystem, which is also a proxy for end-user value. I described this as a “$125B hole that needs to be filled for each year of CapEx at today’s levels.” This week, Nvidia completed its ascent to become the most valuable company in the world. In the weeks leading up to this, I’ve received numerous requests for the updated math behind my analysis. Has AI’s $200B question been solved, or exacerbated? If you run this analysis again today, here are the results you get: AI’s $200B question is now AI’s $600B question. Note: It’s easy to calculate this metric directly. All you have to do is to take Nvidia’s run-rate revenue forecast and multiply it by 2x to reflect the total cost of AI data centers (GPUs are half of the total cost of ownership—the other half includes energy, buildings, backup generators, etc)1. Then you multiply by 2x again, to reflect a 50% gross margin for the end-user of the GPU, (e.g., the startup or business buying AI compute from Azure or AWS or GCP, who needs to make money as well). What has changed since September 2023? The supply shortage has subsided: Late 2023 was the peak of the GPU supply shortage. Startups were calling VCs, calling anyone that would talk to them, asking for help getting access to GPUs. Today, that concern has been almost entirely eliminated. For most people I speak with, it’s relatively easy to get GPUs now with reasonable lead times. GPU stockpiles are growing: Nvidia reported in Q4 that about half of its data center revenue came from the large cloud providers. Microsoft alone likely represented approximately 22% of Nvidia’s Q4 revenue. Hyperscale CapEx is reaching historic levels. These investments were a major theme of Big Tech Q1 ‘24 earnings, with CEOs effectively telling the market: “We’re going to invest in GPUs whether you like it or not.” Stockpiling hardware is not a new phenomenon, and the catalyst for a reset will be once the stockpiles are large enough that demand decreases. OpenAI still has the lion’s share of AI revenue: The Information recently reported that OpenAI’s revenue is now $3.4B, up from $1.6B in late 2023. While we’ve seen a handful of startups scale revenues into the <$100M range, the gap between OpenAI and everyone else continues to loom large. Outside of ChatGPT, how many AI products are consumers really using today? Consider how much value you get from Netflix for $15.49/month or Spotify for $11.99. Long term, AI companies will need to deliver significant value for consumers to continue opening their wallets. The $125B hole is now a $500B hole: In the last analysis, I generously assumed that each of Google, Microsoft, Apple and Meta will be able to generate $10B annually from new AI-related revenue. I also assumed $5B in new AI revenue for each of Oracle, ByteDance, Alibaba, Tencent, X, and Tesla. Even if this remains true and we add a few more companies to the list, the $125B hole is now going to become a $500B hole. It’s not over—the B100 is coming: Earlier this year, Nvidia announced their B100 chip, which will have 2.5x better performance for only 25% more cost. I expect this will lead to a final surge in demand for NVDA chips. The B100 represents a dramatic cost vs. performance improvement over the H100, and there will likely be yet another supply shortage as everyone tries to get their hands on B100s later this year. One of the major rebuttals to my last piece was that “GPU CapEx is like building railroads” and eventually the trains will come, as will the destinations—the new agriculture exports, amusement parks, malls, etc. I actually agree with this, but I think it misses a few points: Lack of pricing power: In the case of physical infrastructure build outs, there is some intrinsic value associated with the infrastructure you are building. If you own the tracks between San Francisco and Los Angeles, you likely have some kind of monopolistic pricing power, because there can only be so many tracks laid between place A and place B. In the case of GPU data centers, there is much less pricing power. GPU computing is increasingly turning into a commodity, metered per hour. Unlike the CPU cloud, which became an oligopoly, new entrants building dedicated AI clouds continue to flood the market. Without a monopoly or oligopoly, high fixed cost + low marginal cost businesses almost always see prices competed down to marginal cost (e.g., airlines). Investment incineration: Even in the case of railroads—and in the case of many new technologies—speculative investment frenzies often lead to high rates of capital incineration. The Engines that Moves Markets is one of the best textbooks on technology investing, and the major takeaway—indeed, focused on railroads—is that a lot of people lose a lot of money during speculative technology waves. It’s hard to pick winners, but much easier to pick losers (canals, in the case of railroads). Depreciation: We know from the history of technology that semiconductors tend to get better and better. Nvidia is going to keep producing better next-generation chips like the B100. This will lead to more rapid depreciation of the last-gen chips. Because the market under-appreciates the B100 and the rate at which next-gen chips will improve, it overestimates the extent to which H100s purchased today will hold their value in 3-4 years. Again, this parallel doesn’t exist for physical infrastructure, which does not follow any “Moore’s Law” type curve, such that cost vs. performance continuously improves. Winners vs. losers: I think we need to look carefully at winners and losers—there are always winners during periods of excess infrastructure building. AI is likely to be the next transformative technology wave, and as I mentioned in the last piece, declining prices for GPU computing is actually good for long-term innovation and good for startups. If my forecast comes to bear, it will cause harm primarily to investors. Founders and company builders will continue to build in AI—and they will be more likely to succeed, because they will benefit both from lower costs and from learnings accrued during this period of experimentation. A huge amount of economic value is going to be created by AI. Company builders focused on delivering value to end users will be rewarded handsomely. We are living through what has the potential to be a generation-defining technology wave. Companies like Nvidia deserve enormous credit for the role they’ve played in enabling this transition, and are likely to play a critical role in the ecosystem for a long time to come. Speculative frenzies are part of technology, and so they are not something to be afraid of. Those who remain level-headed through this moment have the chance to build extremely important companies. But we need to make sure not to believe in the delusion that has now spread from Silicon Valley to the rest of the country, and indeed the world. That delusion says that we’re all going to get rich quick, because AGI is coming tomorrow, and we all need to stockpile the only valuable resource, which is GPUs. In reality, the road ahead is going to be a long one. It will have ups and downs. But almost certainly it will be worthwhile. If you are building in this space, we’d love to hear from you. Please reach out at dcahn@sequoiacap.com Some commenters challenged my 50% assumption on non-GPU data center costs, which I summarized as energy costs. Nvidia actually came to the exact same metric, which you can see on Page 14 of their October 2023 analyst day presentation, published a few days after my last piece. Share Share this on Facebook Share this on Twitter Share this on LinkedIn Share this via email Related Topics #AI AI’s $200B Question By David Cahn Perspective Read AI in 2024: From Big Bang to Primordial Soup By David Cahn Perspective Read AI Ascent 2024 Video highlights from our AI conference. Perspective Read AI 50: Companies of the Future Konstantine Buhler on the 2024 AI 50 list Perspective Read JOIN OUR MAILING LIST Get the best stories from the Sequoia community. Email address",
    "commentLink": "https://news.ycombinator.com/item?id=40869461",
    "commentBody": "AI's $600B Question (sequoiacap.com)331 points by fh973 23 hours agohidepastfavorite496 comments LarsDu88 22 hours agoAccording to Jensen it takes about 8000 H100s running for 90 days to train a 1.8 Trillion param MoE GPT-4 scale model. Meta has about 350,000 of these GPUs and a whole bunch of A100s. This means the ability to train 50 GPT-4 scale models every 90 days or 200 such models per year. This level of overkill suggests to me that the core models will be commoditized to oblivion, making the actual profit margins from AI-centric companies close to 0, especially if Microsoft and Meta keep giving away these models for free. This is actually terrible for investors, but amazing for builders (ironically). The real value methinks is actually over the control of proprietary data used for training which is the single most important factor for model output quality. And this is actually as much an issue for copyright lawyers rather than software engineers once the big regulatory hammers start dropping to protect American workers. reply Voloskaya 15 hours agoparent> This means the ability to train 50 GPT-4 scale models every 90 days or 200 such models per year. Not anywhere close to that. Those 350k GPUs you talk about aren't linked together. They also definitely aren't all H100s. To train a GPT-4 scale model you need a single cluster, where all the GPUs are tightly linked together. At the scale of 20k+ GPUs, the price you pay in networking to link those GPUs is basically almost the same as the price of those GPUs themselves. It's really hard and expensive to do. FB has maybe 2 such clusters, not more than that. And I'm somewhat confident one of those cluster is an A100 cluster. So they can train maybe 6 GPT-4 every 90 days. reply LarsDu88 14 hours agorootparentI had to take a second look at this: https://www.datacenterdynamics.com/en/news/meta-to-operate-6... 340,000 H100s 600,000 H100 equivalents (perhaps AMD Instinct cards?) On top of the hundreds of thousands of legacy A100s. And I'm certain the order for B100s will be big. Very big. Even the philanthropic org Chan-Zuckerberg institute current rocks 1000 H100s, probably none used for inference. They are going ALL OUT reply onion2k 13 hours agorootparentThey are going ALL OUT Just like they did for their metaverse play, and that didn't work out very well. reply LarsDu88 12 hours agorootparentI honestly don't think we've seen the end of AR/VR yet. The tech continues to improve year over year. There are rumors the prototype Zuck plans to show at Meta Connect this year are mindblowing reply onion2k 12 hours agorootparentBetter VR tech won't make people buy VR. You could literally offer them a Star Trek holodeck and they still wouldn't buy in. People don't buy it because they don't see the point. This was even true in Star Trek. People could do literally anything on a holodeck and the writers still had them going to Risa for a holiday. There is no chance of VR going mainstream until someone solves the fundamental human problem of people preferring to do things in real life. reply TeMPOraL 10 hours agorootparent> This was even true in Star Trek. People could do literally anything on a holodeck and the writers still had them going to Risa for a holiday. If anything, that was a failure of imagination on writers' part, somewhat rectified over time and subsequent shows. Even in the core shows (TNG, DS9, VOY), we've seen the holodeck used for recreation, dating, study, simulation, brainstorming, physical training, hand-to-hand combat training, marksmanship training, gaming out scenarios for dangerous missions, extreme sports, engineering, accident investigation, crime scene reconstruction, and a bunch of other things. Still, the show was about boldly going where no one has gone before - not about boldly staying glued to a virtual reality display - so this affected what was or wasn't shown. Plus, it's not either/or. People went to Risa to have real sex (er, jamaharon) with real people, and lots of (both it and them). This wasn't shown on-screen, just heavily implied, as this is where Roddenberry's vision of liberated humanity clashed with what could be shown on daytime TV. Holo-sex was a thing too, but it was shown to be treated more like pornography today - people do it, don't talk much about it, but if you try to do it too often and/or with facsimile of people you know, everyone gets seriously creeped out. reply jordanb 5 hours agorootparentIn TNG we see Barkley get addicted to the holodeck and use it to play out fantasies with female members of the crew. Through the episode we end up learning that Barkley escaped to the holodeck because he was having problems and not being fulfilled in his real life. There was a similar episode of DS9 where Nog gets addicted to the holodeck due to war trauma. The central take of the show is that real life is better for these people in this future communist space utopia and the only reason why you'd go to the holodeck is light entertainment, physical training, or if there's something wrong with your life that needs fixed. reply TecoAndJix 7 hours agorootparentprevhttps://youtu.be/6lobo3c0NFg?si=WqxJiOHyjwl3vD_m reply ChainOfFools 8 hours agorootparentprevso much human potential, natural resources, and anxiety wasted on obsessive pursuit of diddling a few special nerve endings, heaped in a mountain of self-serving social pecking order mythology and ritualistic mystery. reply mc32 7 hours agorootparentOnce we can produce offspring in sci-fi vats, then we can remove the then unnecessary organs from our DNA and not have those worries. We can be just like human ants where the queen is now vats and we just work and maybe think a little. reply plasticchris 5 hours agorootparentWhat a brave new world it would be. reply eru 11 hours agorootparentprev> There is no chance of VR going mainstream until someone solves the fundamental human problem of people preferring to do things in real life. I don't think that's much of a problem? People already watch TV and play computers games and read novels, instead of real life. I agree that VR has _some_ problem, but I don't think it's that people prefer real life. reply figassis 11 hours agorootparentprevVR requires too much setup. I have a PS4, bought a used PSVR set and realized I needed a camera that I did not have. Realized instead of buying a camera, I could upgrade to a ps5 and buy the new headset that did not require a camera, bc I prefer not to have my living room look like a lab. Then there is the social aspect of it. You can't interact with people around you the same way you do if you play with, say, a console controller. VR is an all encompassing activity that you have to dedicated time for, instead of having it casually just exist around you. Then we have the cost. Only some people can have it, so it will be a lonely activity most of the time when it could be so much more. I can afford it, but every time I am in front of a new set, I consider my life with it and say \"maybe next time\". Finally, I have not really explored them, but I have a feeling the experience is limited by the content that exists. I dream of a VR experience where suddenly all content I currently enjoy on flat screens will automagically be VRified. But I am pretty sure that will not be the case. Only a very limited collection will be VR native. But I want it all to be, or almost all, before I go all in. reply mapt 3 hours agorootparentThe appropriate response to VR is that we all get a VR/storage/etc room in addition to the existing paradigms of bedroom, living room, kitchen, etc. At the high end we've grown houses to the point that in order to remain boxes they demanded interior rooms without windows, and so far we have varied between refusing to build these rooms because \"natural light\" and outright banning these rooms for safety reasons, creating sprawling complicated floorplans instead with lots of surface area per volume. It would be a bit better suited to a civilization that wasn't undergoing a catastrophic urban housing shortage crisis with demographic & economic effects for upcoming generations that are comparable to a world war or the Black Death. We are building huge exurban houses which nonetheless do not have VR-appropriate rooms, and tiny 1-bedroom apartments, and not much else. https://www.youtube.com/watch?v=4ZxzBcxB7Zc The question is whether this is a chicken/egg problem that prevents us from launching next-generation VR plays. reply LarsDu88 5 hours agorootparentprevYou should try the quest3. Virtually no setup reply pmarreck 2 hours agorootparentprevVision Pro is already that today, FYI. It’s honestly amazing. But it’s too expensive and still too heavy on your face. reply duggan 8 hours agorootparentprev> There is no chance of VR going mainstream until someone solves the fundamental human problem of people preferring to do things in real life. Ready Player One had a pretty good answer to this: dystopia. Once real life is miserable enough, VR's time will have arrived. reply bergie 8 hours agorootparentThat, or another year of lockdown could also do it. reply boloust 11 hours agorootparentprevTotally agreed. It's like the hype around \"social media\" or \"streaming services\" or \"video games\". There's no chance of any of them going mainstream because of the fundamental human problem of people preferring to do things in real life. reply jononor 8 hours agorootparentprev> There is no chance of VR going mainstream until someone solves the fundamental human problem of people preferring to do things in real life. Three counterpoints: Online gaming, social media, smartphones. All of these favor \"virtual\" over \"real life\", and have become massively popular over the last decades. Especially among the young, so the trend is likely to continue. reply tonynator 11 hours agorootparentprevThink you're off base here and the issue is comfortability. People spend all day on their computers and phones, VR just needs to make some breakthroughs in comfort (maybe built-in fans? Literally include ginger tablets with the headset?) to get people over the initial nausea hump. This plus higher resolution for AR purposes will do a ton. Now, there may also be a physical laziness factor to overcome, but there are enough people that enjoy moving their bodies to really explode the industry even if all the lazy folks stay 2D. reply rlt 11 hours agorootparentprevAR might be a different story, if the tech gets small/good enough. reply dmix 13 hours agorootparentprev> Even the philanthropic org Chan-Zuckerberg institute current rocks 1000 H100s, probably none used for inference. What do they use them for? reply ChainOfFools 8 hours agorootparenttax writeoffs reply LarsDu88 14 hours agorootparentprevOk, I might have misread some rumored ballpark figures. And most of the GPUs will be used for inference rather than training. Still 6 GPT-4's every 90 days is pretty amazing. reply Willish42 18 hours agoparentprev> once the big regulatory hammers start dropping to protect American workers Have we been living in the same universe the last 10 years? I don't see this ever happening. Related recent news (literally posted yesterday) https://www.axios.com/2024/07/02/chevron-scotus-biden-cyber-... reply LarsDu88 14 hours agorootparentI think people wildly underestimate how protectionist people - particularly educated software engineers and PhDs will get once an AI model directly impacts their source of wealth. Red state blue collar workers got their candidate to pass tariffs. What happens when both blue state white collar workers and red state blue collar workers need to contest with AI. Perhaps not within the next 10 years, but certainly within 20 years! And if you think 20 years is a long time... 2004 was when Halo 2 came out reply sangnoir 12 hours agorootparent> I think people wildly underestimate how protectionist people - particularly educated software engineers and PhDs will get once an AI model directly impacts their source of wealth. I don't know what power you imagine SWEs and PhDs posses, but the last time their employers flexed their power by firing them in droves (despite record profits); the employees sure seemed powerless, and society shrugged it off and/or expressed barely-concealed schadenfreude. reply demondemidi 2 hours agorootparentThey were sued for collusion and the lawyers got a massive payout and the employees got a fraction of lost wages. I was one of them. (Employees not lawyers.) reply dmix 13 hours agorootparentprevIt's not going to stop it though even if they try though. You can't stop technical progress like this any more than you can stop piracy. But agreed, between the unions with political pull and \"AI safety\" grifters I suspect there could be some level of regulatory risk, particularly for the megacorps in California. I doubt it will be some national thing in the US absent a major political upheaval. Definitely possible in the EU which will probably just be a price passed on to customers or reduced access, but that's nothing new for them. reply jjallen 11 hours agorootparentprevHopefully that time AI will be working for us in our homes, stores and farms so we don't need to work as much and this is ok. reply rlt 11 hours agorootparentPeople will still need purpose, which for better or worse is often provided by their job. reply hnthrow289570 17 hours agorootparentprevThe only upside is state-level minimum wage increases. The federal minimum wage is still a complete joke at $7.25 an hour. But there's bigger fish to fry for American politics and worker obsolescence is not really top of mind for anyone. reply Kon-Peki 22 hours agoparentprevIt's like someone thinking that they are SOOOO smart, they are going to get rich selling shovels in the gold rush. So they overpay for the land, they overpay for the factory, they overpay for their sales staff. And then someone else starts giving away shovels for free. reply gwd 11 hours agorootparent> And then someone else starts giving away shovels for free. Ah, I see -- it's more like a \"level 2 gold rush\". So a level 1 gold rush is: There's some gold in the ground, nobody knows where it is, so loads of people buy random bits of land for the chance to get rich. Most people lose, a handful of people win big. But the retailers buying shovels at wholesale and selling them at a premium make a safe, tidy profit. But now that so many people know the maxim, \"In a gold rush, sell shovels\", there's now a level 2 gold rush: A rush to serve the miners rushing to find the gold. So loads of retailers buy loads and loads of shovels and set up shop in various places, hoping the miners will come. Probably some miners will come, and perhaps those retailers will make a profit; but not nearly as much as they expect, because there's guaranteed to be competition. But the company making the shovels and selling them at a premium makes a tidy profit. So NVIDIA in this story is the manufacturer selling shovels to retailers; and all the companies building out massive GPU clouds are the retailers rushing to serve miners. NVIDIA is guaranteed to make a healthy profit off the GPU cloud rush as long as they play their cards right (and they've always done a pretty decent job of that in the past); but the vast majority of those rushing to build GPU clouds are going to lose their shirts. reply demondemidi 2 hours agorootparentAnd basically one AI company making all the money. Weird symbiosis. reply Terr_ 21 hours agorootparentprev> And then someone else starts giving away shovels for free. And their business model is shovel-fleet logistics and maintenance... :p reply woah 20 hours agorootparentThe platform for shovel fleet logistics startups reply ugh123 18 hours agorootparentSaaS (shoveling as a service) reply TeMPOraL 21 hours agorootparentprevAnd/or exploiting the legal infrastructure around intellectual property rights to make sure only hobbyists and geologists can use the shovels without paying through the nose or getting sued into oblivion. reply freehorse 18 hours agorootparentIf your company grows to 700 million monthly active users, then most probably you can make your own AI department and train your own models. I guess people's aspirations are very high in this space, but let's be realistic. reply jona-f 11 hours agorootparentprevTheir business model is of course tracking all the shovels and then selling the locations of all the gold. reply from-nibly 19 hours agorootparentprevIt's almost like you can't actually control the demand side. reply alberth 18 hours agoparentprev> making the actual profit margins from AI-centric companies close to 0 The same thinking stopped many legacy tech companies from becoming a “cloud” company ~20 years ago. Fast forward to today and the margin for cloud compute is still obscene. And they all wish in hindsight they got into the cloud business way sooner than they ultimately did. reply jordanb 4 hours agorootparentThat's not the way I remember the cloud transition at all. My company adopted an internal cloud. VMWare had some great quarters. Openstack got really big for a while and everyone was trying to hire for it. All the hosting companies started cloud offerings. What ended up happening was Amazon was better at scale and lockin than everyone else. They gave Netflix a sweet deal and used it as a massive advertisement. It ended up being a rock rolling down a hill and all the competitors except ones with very deep pockets and the ability to cross-subsidize from other businesses (MSFT and Google) got crushed. reply coredog64 34 minutes agorootparentIt’s not just the software, it’s the hardware too. Too many companies got good at speeding up VM deployments but ignored theory of constraints and still had a 4-6 month hardware procurement process that gave everyone and their dog veto power. And then you come to companies that managed to streamline both and ran out of floor space in their data center because they had to hold onto assets for 3-5 years. At one previous employer, the smallest orderable unit of compute was a 44U rack. They eventually filled the primary data center and then it took them 2 years to Tetris their way out of it. reply demondemidi 2 hours agorootparentprevIt still blows my mind that Microsoft is the most valuable company in the planet because of the cloud and Balmers long term vision. I thought they would have gone the way of IBM. reply fragmede 2 hours agorootparentWhich, IBM booked about $62 billion in revenue for 2023. I thought Nvidia recently took that crown recently though. reply hollerith 2 hours agorootparentprevAgree. Ballmer seems to have done his job well. reply lmm 17 hours agorootparentprevAre the second-tier cloud companies really seeing big margins? Why is it not competed away to zero like airlines? reply mbb70 17 hours agorootparentThere is essentially zero cost for a user to switch airlines. The cost to switch clouds is astronomical for any decent sized org. reply jart 12 hours agorootparentThose poor little AI clouds will never keep people reeled in unless they invent something like CUDA. reply throwaway2037 14 hours agorootparentprevI like the sentiment of your post. I mostly agree. If you use OpenShift, doesn't that help to reduce the cost of switching cloud providers? reply fragmede 25 minutes agorootparentYou're not switching cloud providers. Amazon's not going to suddenly decide to jack up rates for EC2 instances on you. So the extra complexity just isn't worth it. There is a hypothetical \"but what if we honestly actually really really do\", but that's such a waste of engineering time when there are so many other problems to be solved that it's implausible. The only time multi-cloud makes sense is when you have to meet customers where they're at, and have resources in whichever cloud your customers are using. Or if you're running arbitrage between the clouds and are reselling compute. reply coredog64 32 minutes agorootparentprevNot really. What happens when you run a cloud on top of your cloud is that you don’t get to use any of their differentiating features and that winds up costing you money. Plus you have to pay for your own control plane when that’s already baked into the cloud provider’s charge model. reply flyingpenguin 16 hours agorootparentprevThere will probably be 2 huge winners, and everyone else will fail. Similar to the solar boom. reply throwaway2037 14 hours agorootparentWho are the winners in solar? reply kristjansson 16 hours agoparentprevper Zuckerberg[0], ~half of their H100s were for Reels content recommendation: > I think it was because we were working on Reels. We always want to have enough capacity to build something that we can't quite see on the horizon yet. ... So let's order enough GPUs to do what we need to do on Reels and ranking content and feed. But let's also double that. So there's an immense capacity inside Meta, but the _whole_ fleet isn't available for LLM training. [0]: https://www.dwarkeshpatel.com/p/mark-zuckerberg?open=false#§... reply matthewdgreen 6 hours agorootparentSurely they’re using some of that hardware to overcome Apple’s attempts to deprive them of targeted advertising data. reply shoggouth 17 hours agoparentprevIn my opinion Elsevier and others charging for access to academic publications has held back the advancement of humanity to a lower exponential acceleration into the future at a considerable factor. Think of how cancer could have been cured a decade ago if information was allowed to flow freely from the 50's forward - if anyone could have read scientific publications for free. I have no respect for people that want to protect the moat around information that could be used to advance humanity. reply robwwilliams 16 hours agorootparentHave to disagree. Almost all researchers have essentially unfettered access to all of biomedical literature. Access to papers is therefore a tertiary annoyance wrt progress in science and the cures for cancers. What IS a huge problem is the almost complete lack of systematically acquired quantitative data on human health (and diseases) for a very large number (1 million subjects) of diverse humans WITH multiple deep-tissue biopsies (yes, essentially impossible) that srr suitable for multiomics at many ages/stages and across many environments. (Note, we can do this using mice.) Some specific examples/questions to drive this point home: What is the largest study of mRNA expression in humans? ANSWER: The small but very expensive NUH GTEx study (n max of about 1000 Americans). This study acquired postmortem biopsies for just over 50 tissues. And what is the largest study of protein expression in humans across tissues? Oh sorry, this has never been done although we know proteins are the work-horses of life. What about lipids, metabolites, metagenomics, epigenomics? Sorry again, there is no systematically acquired data at all. What we have instead is a very large cottage-industry of lab-level studies that are structurally incoherent. Some brag about the massive biomedical data we have, but it is truly a ghost and most real data evaporates with a few years. Here is my rant on fundamental data design flaws and fundamental data integration flaws in biomedical research: Herding Cats: The Sociology of Data Integration https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2751652/ reply mchinen 8 hours agorootparentI also think the bottleneck isn't access to the papers today and data access and silos are more important. But I also think the GP's claim and yours are not incompatible. I wonder how much survivorship bias this has since it only considers those that are able to do research, and not those that would have but ended up doing continuing with another STEM job. We could be asking the counterfactual that I think the GP is implying: would more people have been interested in becoming cancer researchers if publications were open? We can sort of see the effect because we have scihub now, which basically unlocks journal access for those that are comfortable with it, and I consider it plausibly having a significant effect for the population that have a research background without an academic affiliation. I've met a few biotech startup founders that switched from tech to bio and did self study+scihub outside of the university. The impetus for change I've heard a few times is a loved one got X disease, and I studied it, quit my less impactful tech job to work on bio stuff. reply OJFord 17 hours agorootparentprevAs much as I'd love open access to academic publications and don't think the current model is great: > Think of how cancer could have been cured a decade ago if information was allowed to flow freely from the 50's forward might be a bit fanciful? Unless you're referring to something particular I'm unaware of. The people best equipped and trained to deliver a cure for cancer (and then some, since it tends not to be particularly field-restricred) do have access. I think the loss is more likely in engineering (to the publication's science), cheaper methods, more reliably manufacturable versions of lab prototypes, etc. I doubt there are many people capable of cancer research breakthroughs who don't have access to cancer research, personally. (And to be clear: I'm not capable of it.) reply enjoylife 17 hours agorootparentI’ll add that even if the papers we all wanted were more freely accesible, the replication and completeness of their described methods would be another source of slowdown. reply robwwilliams 16 hours agorootparentMain problem is still just getting good quantitative data and metadata. Most biomedical researchers are motivated to “tell stories”. Few of us care about generating huge mineable data sets. reply instagib 14 hours agorootparentprevAll of the engineering companies I’ve worked for have not paid for IEEE or any journals. I have to go to the library and maintain membership for IEEE myself then request reimbursement. The schools I’ve worked with have access to everything I’ve needed. They didn’t advertise it but it’s also free for students. reply azinman2 12 hours agorootparentprevNot to mention there is not singular “cancer” - there are many types and they’re all sufficiently different to make the problem much more challenging. reply robertlagrant 3 hours agorootparentprevYou have to not latch on to causes such as \"advance humanity\" and then justify making people do work for free. We decided a while ago[0] that making people work for free was a bad thing. There is high demand for curing cancer. Every company that tries it will hire scientists and lab techs and large laboratories, and have subscriptions to journals. Do you think all of those people should work for free, in the cause of advancing humanity? [0] https://en.wikipedia.org/wiki/Slavery_Abolition_Act_1833 reply lkrubner 3 hours agorootparentprevBut anyone who needs to see these publications can reach them through libraries. One of the reasons why Elsevier can charge so much is that their customers have been institutions. reply aleph_minus_one 2 hours agorootparentThis depends a lot on the country and the library. reply xhkkffbf 3 hours agorootparentprev1) First, most researchers at universities or other institutions have always had unfettered access thanks to a site license. It would be pretty hard to find a real example of a university researcher who couldn't see something. 2) There may be a few researchers who don't have unfettered access. Perhaps they paid $40 for a copy of a paper. Given the high cost of other parts of research labs, I find it hard to believe that any real possibility of curing cancer was halted because someone had to pay $40. 3) It's possible to imagine the opposite being the case. Perhaps someone had a key insight in a clever paper and decided to distribute it for free out of some info-anarchistic impulses. There it would sit in some FTP directory uncrated, unindexed and uncared for. Perhaps the right eyes would find it. Perhaps they wouldn't. Perhaps the cancer researcher would be able to handle all of the LaTeX and FTP chores without slowing down research. Perhaps they would be distracted by sys admin headaches and never make a crucial follow up discovery. The copyrighted journal system provides curation and organization. Is it wonderful? Nah. Is it better than some ad hoc collection of FTP directories? Yes! Your opinion may be that this scenario would never happen. In my opinion, this is more likely than your vision. reply jimjimjim 16 hours agorootparentprev99%+ of the people doing scientific work in curing cancer have access to all the relevant medical and scientific journals. reply eru 11 hours agorootparentprevEh, most published research papers are wrong anyway. reply laweijfmvo 22 hours agoparentprevA lot of those GPUs are for their 3B users to run inferencing, no? reply benreesman 18 hours agorootparentIt’s been a very long time since I had any inside baseball, but I very much doubt that Hopper gear is in the hot inference path. The precisions and mantissa/exponent ratios you want for inference are just different to a mixed-precision, fault tolerant, model and data parallel pipeline. Hopper is for training mega-huge attention decoders: TF32, bfloat16, hot paths to the SRAM end of the cache hierarchy with cache coherency semantics that you can reason about. Parity gear for fault tolerance, it’s just a different game. reply LarsDu88 20 hours agorootparentprevTrue that, but I think in a very short amount of time, using dedicated general purpose GPUs just for inferencing is going to be mega overkill. If there's dedicated inferencing silicon (like say the thing created by Groq), all those GPUs will be power sucking liabilities, and then the REAL singularity superintelligence level training can begin. reply campers 18 hours agorootparentEtched is another dedicated inference hardware company that recently announced their product. It only works for transformer based models, but is ~20x faster than a H100 reply eru 11 hours agoparentprev> The real value methinks is actually over the control of proprietary data used for training which is the single most important factor for model output quality. Maybe. But we've barely scratched the surface of being more economical with data. I remember back in the old days, there was lots of work on eg dropout and data augmentation etc. We haven't seen too much of that with the like of ChatGPT yet. I'm also curious to see what the future of multimodal models holds: you can create almost arbitrarily amounts of extra data by pointing a webcam at the world, especially when combined with a robot, or letting your models also play StarCraft or Diplomacy against each other. reply weitendorf 12 hours agoparentprevThere are more kinds of AI-centric companies than just foundation models. Making that equivalency is akin to equating internet companies during the dotcom bubble with just websites like pets.com. Now one semi-skilled person in a couple days can make websites that entire teams back then would have taken months to build, but that doesn't mean google.com and thefacebook.com are easily commodotized or bad businesses just because they're websites. reply lkdfjlkdfjlg 28 minutes agoparentprev> but amazing for builders (ironically). Could you expand on this? Who are \"the builders\" here? You mean the model developers? I don't see how this situation can be \"amazing\" for the builders - developers will just get a wage out of their work. reply babyshake 11 hours agoparentprev> once the big regulatory hammers start dropping to protect American workers. The US Supreme Court seems determined to make sure that big regulatory hammers are not going to be dropping, from what I can tell. reply naveen99 17 hours agoparentprevPropriety data is not necessary for training intelligence. Wikipedia, pubmed, arxiv, Reddit and github are probably sufficient. And babies don’t even use that. I agree though that the returns on hardware rapidly diminish. reply itkovian_ 14 hours agoparentprevEverything is if scaling keeps making the models better. If it does you don't train 50 gpt4s, you have the best model. reply apitman 21 hours agoparentprevI thought AI is supposed to put all the lawyers out of work. reply tracerbulletx 18 hours agorootparentNothing will put lawyers or doctors out of work. They are powerful cartels that can easily protect themselves. Realtors are already irrelevant technologically but they have a huge entrenched social and legal system to make it impractical to compete. reply DataDive 12 hours agorootparentWeren't lots of realtors recently put out of work in the US, at least? When NAR settled the price collusion charge? Thus cartel or not, times do change. reply bamboozled 14 hours agorootparentprevMy friend is a real estate agent, they play a major part in the psychology of buyers and sellers. Selling your dead parents home that you grew up in (for example) isn't something everyone just signs up to some website and does using a credit card without a second thought. A good real estate agent can guide people through this process while advising them on selling at the right price while avoiding the most stress often during an extremely difficult time in their life, such as going through divorce of breakup. They of course also help keep buyers interested while the seller is making up their mind about the correct offer to take. I find your comment ignorant in so many ways. Maybe have some respect? reply dmix 13 hours agorootparentAre you not just explaining \"a huge entrenched social system\" as OP said? It takes a long time for cultures to shift and for people to start to trust information systems to entirely replace high touch stuff like that. And at some level there will always be some white glove service on top for special cases. reply bamboozled 13 hours agorootparentHow is hiring a professional to help you sell a property a \"huge entrenched social system\" sorry ? No one is forced to hire a real estate agent. I bought my house through private sale. reply DataDive 12 hours agorootparent> No one is forced to hire a real estate agent. but for long time in the US you were \"forced\" to hire a real estate agent, if you wanted to get the market price. Refer to the NAR settlement that pretty much admits to this. https://www.realestatecommissionlitigation.com/ This is not to say that real estate agents cannot add value to a process; it is just that they were a cartel with anticompetitive practices. The mandated and fixed 6% on each sale was and is ridiculous, when the median sell price is 400K in the US ... that is 24K commission reply wolfendin 12 hours agorootparentThat really does say something about how unrealistic house prices are nowadays, doesn’t it? reply threeseed 20 hours agorootparentprevLexis+ AI and Ask Practical Law AI systems produced incorrect information more than 17% of the time, while Westlaw’s AI-Assisted Research hallucinated more than 34% of the time: https://hai.stanford.edu/news/ai-trial-legal-models-hallucin... reply falcor84 20 hours agorootparentJust out of curiosity, what's the human lawyer baseline on that? reply csa 18 hours agorootparent> Just out of curiosity, what's the human lawyer baseline on that? Largely depends on how much money the client has. reply dbish 18 hours agorootparentprevit's the self-driving car problem. Humans aren't perfect either but people like to ignore that. reply Terr_ 18 hours agorootparentTrue, they're similar... But what's also similar is that people make the mistake of focusing on differences in failure rates while glossing over failure modes. Human imperfections are a family of failure-modes which have a gajillion years of experience in detecting, analyzing, preventing, and repairing. Quirks in ML models... not so much. A quick thought-experiment to illustrate the difference: Imagine there's a self-driving car that is exactly half as likely to cause death or injury than a human driver. That's a good failure rate. The twist is that its major failure mode is totally alien, where units attempt to inexplicably chase-murder random pedestrians. It would be difficult to get people to accept that tradeoff. reply Barrin92 16 hours agorootparentprevNo, people have the correct intuition that human errors at human speeds are very different in nature from human rate errors at machine speeds. It's one thing if a human makes a wrong financial decision or a wrong driving decision, it's another thing if a model distributed to ten million computers in the world makes that decision five million times in one second before you can notice it's happening. It's why if your coworker makes a weird noise you ask what's wrong, if the industrial furnace you stand next to makes a weird noise you take a few steps back. reply zacmps 15 hours agorootparentprevI'm sure it's no where near good enough yet, but a legal model getting the answer right 83% of the time is still quite impressive imo. reply boloust 11 hours agoparentprevWhy are we assuming we're topping out at a GPT-4 scale model? reply forgot-im-old 16 hours agoparentprevWhat percentage of GPUs are being used for training versus inference? reply vasco 20 hours agoparentprevThe infinitely expanding AI-generated metaverse isn't going to render itself, at least in the case of meta I think that might be one of the only pieces missing. reply VirusNewbie 15 hours agoparentprevLlama isn't even in the same stratosphere as the big models when it comes to coding, logic, and other interesting tasks that I think are commercially viable. reply pedalpete 20 hours agoprevI think this is the correct take. My understanding of the article is that huge investments in hardware, mostly to NVIDIA, and spending by major tech companies is currently defining the market, even if we include OpenAI, Anthropic, etc. It is FAANG money they are running on. I put this as equivalent to investing in Sun Microsystems and Netscape in the late 90s. We knew the internet was going to change the world, and we were right, but we were completely wrong as to how, and where the money would flow. reply npalli 19 hours agoparentThe better analogy is the massive investment in fiber optic cable in the late 90s. All the companies in that line (Global Crossing, Worldcom etc.) went bust after investing 10s of billions but the capacity was useful (with a >90% drop in price) for future internet services. Around 2000 when the bubble was bursting only 5% of capacity was being used but proved to be useful to get all internet-first companies like the Googles, Amazon, NetFlix's going. reply pedalpete 17 hours agorootparentI initially was agreeing with you, but I don't see NVIDIA, AWS, Microsoft, etc going to zero (and Worldcom was unraveled by accounting fraud). Sun Microsystems sold to Oracle for $7B, and Netscape was acquired by AOL for $10B. reply fuzztester 15 hours agorootparenthttps://en.m.wikipedia.org/w/index.php?title=Acquisition_of_... reply wmf 16 hours agorootparentprevYeah, Cisco didn't go to zero in 2000 and Nvidia won't go to zero. It will merely go down 90%. reply aurareturn 10 hours agorootparentWhen Cisco went bust, their stock price was still 150% higher than before the boom. So will Nvidia be worth $5 trillion AFTER the AI bust? reply cruffle_duffle 18 hours agorootparentprevGood old JDS Uniphase was one of the first individual stocks I bought. I mean it had to go up right? Fiber and dark fiber and the continual threat of the internet collapsing due to load… better buy that stock! reply elphinstone 17 hours agorootparentWorldcom here. Ah, the Enrons of the internet. reply ddrmaxgt37 14 hours agorootparentprevI wonder how the analogy holds up given computational advances. Will a bunch of H100s be as useful a decade later like fiber ended up being? reply torginus 11 hours agorootparentI might be wrong, but my understanding is that we're on a decelerating slope of perf/transistor and have been for quite a while - I just looked up the OpenCL benchmark results of the 1080 Ti vs 4090, and the perf/W went up by 2.8x despite going from 16nm to 5nm, with perfect power scaling, we would've seen a more than 10x increase. reply novaRom 11 hours agorootparentprevProbably not. There will be better GPUs. It's like we did use all those Kepler K10 and K80 fifteen or so years ago, they were Ok for models with few millions of parameters, then Pascal and Volta arrived ten years ago with massive speed up and larger memory, allowing to train same size models 2-4 times faster, so you simply had to replace all Keplers. Then Turing happened making all P100 and V100 obsolete. Then A100, and now H100. Next L100 or whatever with just more on-board memory will make H100 obsolete quickly. reply dmix 13 hours agorootparentprevSame applies to the railroads analogy used in the original article. reply treis 17 hours agoparentprevThink the FAANGs are doing part moat defending and part value add. Like AI powered spreadsheet might dethrone Excel so Excel has to be AI powered. MS will probably get some additional revenue from it but I don't think it will be a revolutionary amount. reply hnburnsy 18 hours agoparentprev> I put this as equivalent to investing in Sun Microsystems and Netscape in the late 90s. Cisco too. reply ksec 2 hours agoprevThis article and most of these thesis assume one thing, you need to make a return of investment from AI while using OpenAI revenue as an anchor to measure it. What if these AI investment were only to protect or strengthen their current business? AI in Windows, macOS, Adobe, iPhone, Facebook, Instagram may not bring in any additional revenue. But it add additional value to their current product line, making competition harder, further hardening their moat. Nvidia or Jensen is also smart to play the national security card. Does the European want their model to be all US based. Are the answer culturally correct? Just like how every single country invested in their own Telecom or Internet infrastructure, if this pitch were even half as successful, do these numbers we are looking at even matter when it is spread out across G7 or G20? While I believe we are still far, or at least 10+ years away from AGI, the current form of AI still has a lot of improvement incoming and are already bringing in real world benefits and value to a lot users. The adoption curve will accelerate once it is integrated into Windows, Office and Mac. So even if we are in a bubble, I still think we are very early in the curve before it burst. reply threeseed 22 hours agoprev> Founders and company builders will continue to build in AI—and they will be more likely to succeed, because they will benefit both from lower costs and from learnings accrued during this period of experimentation Highly debatable. When we look back during the internet and mobile waves it is overwhelmingly the companies that came in after the hype cycle had died that have been enduring. reply malshe 22 hours agoparentThere is an old study that supports your point. The abstract reads: \"Several studies have shown that pioneers have long-lived market share advantages and are likely to be market leaders in their product categories. However, that research has potential limitations: the reliance on a few established databases, the exclusion of nonsurvivors, and the use of single-informant self-reports for data collection. The authors of this study use an alternate method, historical analysis, to avoid these limitations. Approximately 500 brands in 50 product categories are analyzed. The results show that almost half of market pioneers fail and their mean market share is much lower than that found in other studies. Also, early market leaders have much greater long-term success and enter an average of 13 years after pioneers.\" PDF available here: https://people.duke.edu/~moorman/Marketing-Strategy-Seminar-... reply dmix 13 hours agorootparentYes the \"first mover advantage\" is mostly just a common myth in business that refuses to die, but if we look at the original statement: > Founders and company builders will continue to build in AI—and they will be more likely to succeed, because they will benefit both from lower costs and from learnings accrued during this period of experimentation This still lines up with the 2nd wave benefiting more. The first movers helped established the large scale AI hardware industry, got a bunch of smart kids trained on how to make AI, a bunch of people will fail and learn, etc and this experimentation stage sets the groundwork for OpenAI 2.0. We could very well just in the Altavista vs Yahoo days of AI and an upstart takes over in 5yrs. reply malfist 22 hours agoparentprevLet's see: Microsoft Windows: wasn't close to the first OS Microsoft Office: wasn't close to the first office editing suite Google: Wasn't close to the first search engine Facebook: Wasn't close to the first social media website Apple: ~~First \"smart phone\"~~ but not the first personal computer. Comments reminded me that it wasn't the first smartphone Netflix: Wasn't close to the first video rental service. Amazon: Wasn't close to the first web store None of the big five were first in their dominate categories. They were first to offer some gimmick (i.e., google was fast, netflix was by mail, no late fees), but not first categorically. Though they certainly did benefit from learnings of those that came before them. reply AlexandrB 22 hours agorootparent> Apple: First \"smart phone\" but not the first personal computer Was it the first smartphone? I would call phones like the Palm Treo and later BlackBerries smartphones. There were even apps, but everything was lot more locked down and a lot more expensive. reply irq 22 hours agorootparent> I would call phones like the Palm Treo and later BlackBerries smartphones. It's not just you; at the time these products were available, _everyone_ called them smartphones. Emphatically, Apple did not bring the first smartphone to market, not even close. They were, however, the first to popularize it beyond the field of nerds into the general public. reply tim333 19 hours agorootparentI had an Nokia N95 which was basically a smartphone and came out a year before the iPhone. And Wikipedia says >it became a huge sales success for Nokia ... It managed to outsell rivals such as LG Viewty and iPhone. However the iPhone got better. reply dvt 22 hours agorootparentprev> There were even apps, but everything was lot more locked down and a lot more expensive. And just plain... bad. The entire experience didn't have that \"feel\" that Apple turned into reality. It's comparable to today's AI landscape—the technology is pretty neat, but using it is a complete slog. reply AlexandrB 22 hours agorootparentI actually have pretty fond memories of PalmOS PDAs. The hardware was very nice, but they were held back by the resistive touchscreen and dependence on a stylus for input. I never used a Treo but it felt like this was Palm trying to copy BlackBerry by adding a physical keyboard. Edit: There were also the limitations of that era that held devices back in general. WAP internet[1] was awful, but most mobile services were too slow for much else. [1] https://en.wikipedia.org/wiki/Wireless_Application_Protocol reply nextos 22 hours agorootparentprevNokias were very open. You had a terminal with apt-get. The entire device was a regular Linux machine. reply endless1234 20 hours agorootparentIn general, they were not. You're probably thinking of the very niche and unsuccessful Maemo/MeeGo project - eg Nokia N900 - that were indeed Linux-based. But everything else smartphone-ish from Nokia before Lumia (Windows Phone) were Symbian, which predates Linux and has nothing to do with it. reply nextos 19 hours agorootparentI am of course referring to Maemo, as per my previous post. reply seanmcdirmid 22 hours agorootparentprevFirst modern smartphone (capacitive touch screen/multi-touch/form factor), but not first smartphone. reply nextos 22 hours agorootparentprevThere were Nokias running Maemo ahead of the iPhone. Note these were not Symbian. The 770 was released in Q4 '05. They definitely fell within the smartphone category, but oddly the first few iterations lacked GSM radio. reply Ekaros 21 hours agorootparentI would classify them as tablets. At least what I thought my N810 as. reply malfist 22 hours agorootparentprevI'm a complete idiot. I almost bought an HTC fuse too reply edanm 11 hours agorootparentprev> i.e., google was fast, Just to quibble with this - that was not even close to the reason Google got popular. It was because Google was much, much better at finding what you actually wanted. It was just a far better product. You can debate why this is exactly, Joel Spolsky pointed out many years ago that it was because Google got that what matters to users most isn't \"finding all pages related to X\" but rather \"ranking\" those pages, a take I agree with. reply olalonde 19 hours agorootparentprevI have a pet peeve with this common piece of wisdom. You can always find a \"predecessor\" for about anything. The corollary being that there is never a \"first\". And therefore, stating that \"none of the big companies were the first in their categories\" is just stating a tautology. reply robbiemitchell 22 hours agorootparentprev> some gimmick \"key differentiator\" and not necessarily easy to pull off or pay for reply jjtheblunt 22 hours agoparentprev“Pioneers get the arrows, and settlers get the land”? reply chrisweekly 15 hours agorootparent\"The early bird gets the worm -- but the second mouse gets the cheese.\" reply igammarays 16 hours agoprevMaybe because the revenue isn't directly attributable to AI itself, but is realized in the cost savings and productivity improvements in already existing revenue streams? That's where AI has been useful to me. I can't put a number on how much AI has made me exactly, but it has certainly helped all aspects of my bootstrapped startup. reply alok-g 13 hours agoparentAny product should bring benefits to both the producer and the consumer. For the case where a company is using their own AI for their own cost reduction and productivity improvements, they can keep doing that but not offer to another party. If they offer to another party, and that party is having benefits (like you have said), the price should be such that a part of the consumer benefit is shared with the producer resulting in benefits for the producer. The real challenge here is because of price wars, i.e., too much competition already with producers willing to take a hit on profitability in anticipation that they will be able to do so later after creating a moat above and beyond competitors. Or they think that it will strenghen their overall bigger offering by adding an otherwise lossy feature. In a nutshell, even if there's a lot of value for the consumers, it must result in a win-win for a new product to be sustainable in the market. reply dmix 13 hours agoparentprev> but it has certainly helped all aspects of my bootstrapped startup. Well if there's value to you then how much did you pay for it and would it realistically cover operating cost once VC cash dries up? That's the only question. reply ryandrake 22 hours agoprevOthers are saying this article is bearish, but then... > A huge amount of economic value is going to be created by AI. Company builders focused on delivering value to end users will be rewarded handsomely. Such strong speculative predictions about the future, with no evidence. How can anyone be so certain about this? Do they have some kind of crystal ball? Later in the article they even admit that this is another one of tech's all-too-familiar \"Speculative frenzies.\" The whole AI thing just continues to baffle me. It's like everyone is in the same trance and simply assuming and chanting over and over that This Will Change Everything, just like previous technology hype cycles were surely going to Change Everything. I mean, we're seeing huge companies' entire product strategies changing overnight because We Must All Believe. How can anyone speak definitively about what AI will do at this stage of the cycle? reply TeMPOraL 22 hours agoparentHow can anyone not see just how impactful it's going to be? Or already is? I can't think of a single recent technology that was so widely adopted by tech and non-tech people alike, immediately integrated into day-to-day experience. The rise of mobile phones and e-commerce in the 90s would be the last time I've seen this happen (I'm not counting smartphones, as those are more of an iteration). Or social media, in purely software space. I've just had GPT-4o write me a full-featured 2048 clone in ~6 hours of casual chat, in between of work, making dinner, and playing with kids; it cost me some $4 in OpenAI bills, and I didn't write a single line of code. I see non-tech people around me using ChatGPT for anything from comparison shopping to recipe adjustments. One person recently said to me that their dietitian is afraid for their career prospects because ChatGPT is already doing this job better than she is. This is a small fraction of cases in my family&friends circle; anyone who hasn't lived under the rock, or wasn't blinded by the memetic equivalent of looking at a nuclear weapon detonation, likely has a lot of similar things to say. And all of that is not will, it's is, right now. reply cfeduke 20 hours agorootparentOkay I guess I've just had a different experience entirely. Maybe I'm jaded by hallucinations. The code ChatGPT generates is often bad in ways that are hard to detect. If you are not an experienced software engineer, the defects could be impossible to detect, until you/ChatGPT has gone and exposed all your customers to bad actors, or crash at runtime, or do something terribly incorrect. As far as other thought work goes, I am not consulting ChatGPT over, say, a dietician or a doctor. The hallucination risk is too high. Producing an answer is the not the same as producing a correct answer. reply nvarsj 20 hours agorootparentI agree. I've just seen it hallucinate too many things that on the surface seem very plausible but are complete fabrications. Basically my trust is near 0 for anything chatgpt, etc. spits out. My latest challenge is dealing with people that trust chatgp to be infallible, and just quote the garbage to make themselves look like they know what they are talking about. reply gwervc 19 hours agorootparent> things that on the surface seem very plausible but are complete fabrications LLMs are language model, it's crazy people expect them to be correct in anything beyond surface level language. reply nvarsj 9 hours agorootparentYeah, I was probably being a bit too harsh in my original comment. I do find them useful, you just have to be wary of the output. reply TeMPOraL 20 hours agorootparentprevMy experience actually agrees with you. It's just that the set of use cases that either: - Are hard (or boring) to do, but easy to evaluate - for me, e.g. writing code, OCR, ideation; or - Don't require a perfectly correct answer, but more of a starting point or map of the problem space; or - Are very subjective, or creative, with there being no single correct answer, is surprisingly large. It covers pretty much everything, but not everything for everyone at the same time. reply ignoramous 20 hours agorootparentprev> Okay I guess I've just had a different experience entirely. I've seen both the good and the bad. I really like the good parts. Most recently, Claude Sonnet 3.5 fixed a math error in my code (I prompted it to check for it from a well-written bug report, and it did it fix it ever so perfectly). These days, it is pretty much second nature for me to pull up a new file & prompt Copilot to complete writing the entire code from my comment trails. I don't think I've seen as much change in my coding behaviour since Borland Turbo C -> NetBeans. reply ern 17 hours agorootparentprevI recently needed to help a downstream team with a problem with an Android app. I never did mobile app dev before, but I was able to spin up a POC (having not coded in Java for 22 years) and solve the problem with the help of ChatGPT 4.0. Sure I probably would have been able to do it without ChatGPT, but it was so much easier to have something to bounce ideas off-of. A safety net, if you will. The hallucination risk was irrelevant: it did hallucinate a little early on. I told it it was a hallucinating, and we moved onto a different way of solving the problem. It was easy enough to verify it was working as expected. reply gexla 14 hours agorootparentSeems to me this is the equivalent of fast retrieval and piecing together from a huge amount of examples in the data. This might take far more time if you were to do this yourself. That's a plus for the tools. In other words, a massively expensive (for the service provider) auto-complete. But try to do something much more simple but has much fewer examples (a typical case is something which has bad documentation) in the data, and it falls apart. I even tried to use Perplexity to create a dead simple CLI command, and it hallucinated an answer (looking at the docs, it misused the parameter, and may have picked up on someone who gave an incorrect answer in the data.) reply itsoktocry 20 hours agorootparentprevIf your procees is asking it to \"write me all this code\", then you slap it in production, you're going to have a bad time. But there's intermediate ground. >I am not consulting ChatGPT over, say, a dietician or doctor Do you know any doctors, by chance? You have way more faith in experts than I do. reply kortilla 19 hours agorootparentChatGPT is just statistically associating what it’s observed online. I wouldn’t take dietary advice from the mean output of Reddit with more trust than an expert. reply interstice 19 hours agorootparentDoctors can be associating what they’ve learned, often with heavy biases from hypochondriacs and not enough time per patient to really consider the options. I’ve had multiple friends get seriously ill before a doctor took their symptoms seriously, and this is a country with decent healthcare by all accounts. Human biases are bad too. reply abraae 14 hours agorootparent> Doctors can be associating what they’ve learned, often with heavy biases from hypochondriacs So true. And it's hard to question a doctor's advice, because of their aura of authority, whereas it's easy to do further validation of an LLMs diagnosis. I had to change doctor recently when moving towns. It was only when chancing on a good doctor that I realised how bad my old doctor was - a nice guy but cruising to retirement. And my experience with cardiologists has been the same. Happy to get medical advice from an LLM though I'd certainly want prescriptions and action plans vetted by a human. reply throwaway2037 13 hours agorootparent> It was only when chancing on a good doctor that I realised how bad my old doctor was How did you determine the new doctor is \"good\"? reply theshackleford 12 hours agorootparentprevBy the time a doctor paid me enough attention to realise something was wrong I had suffered a spinal cord injury whose damage can never be reversed. I’m not falling all over myself to trust chatgpt, but I got practically zero for doctors either. Nobody moved until I threatened to start sueing. reply aworks 2 hours agorootparentprevI sometimes use ChatGPT to prepare for a doctor's visit so I can have a more intelligent conversation even if I may have more trust overall in my doctor than in AI. reply XajniN 16 hours agorootparentprevI’ve seen so many doctors advertising or recommending homeopathic “medicines” or GE-132 [1], that I would be fairly more confident in an LLM + my own verification from reliable sources. I’m no doctor, but I know more than enough to recognize bullshit, so I wouldn’t just recommend this approach to everyone. [1] https://pubmed.ncbi.nlm.nih.gov/1726409/ reply TeMPOraL 12 hours agorootparentprevYou realize that \"online\" doesn't just mean Reddit, but also Wikipedia and arXiv and PubMed and other sources perused by actual experts? ChatGPT read more academic publications in any field than any human. reply dmix 13 hours agorootparentprevIt's already gotten significantly better and faster in a few yrs. Maybe LLMs will hit a wall in the next 5yrs but even if it does it's still extremely useful and there are always other ways to optimize the current technology where this is already a major development for society. reply remarkEon 11 hours agorootparentprev>The code ChatGPT generates is often bad in ways that are hard to detect. If you are not an experienced software engineer, the defects could be impossible to detect, until you/ChatGPT has gone and exposed all your customers to bad actors, or crash at runtime, or do something terribly incorrect. I wonder about this a lot, because there's a future here where a decent amount of software engineering is offloaded to these AIs and we reach a point, in the near future, where no one really knows or understands what's going on. That seems bad. Put another way, suppose that your primary care doctor is really just using MedAI to diagnose and recommend treatment for whatever it is you went in to see him about. Over time, these sorts of shortcuts metastasize and the doctor ends up not really knowing anything about you, or the other patients, or what he's really doing as a doctor ... it's just MedAI (with whatever wrongness rate is tolerable for the insurance adjusters). Again, seems bad. There's a palpable loss of human knowledge here that's enabled by a \"tool\" that's allegedly going to make us all better off. reply disgruntledphd2 10 hours agorootparentThe closest analogy here is that we don't have as full-featured autopilots in airplanes as we could, because they reduce safety. reply remarkEon 1 hour agorootparentRight, good point. Maybe I'm making an argument that some features, or scope of features, should be highly regulated along the same lines. reply CuriouslyC 19 hours agorootparentprevEasy answer. Ask ChatGPT to write testable code, and tests for the code, then just verify the tests. If the tests don't work, have ChatGPT use the test output to rewrite the code until it does. If you can't have ChatGPT write testable code because of your architecture, you have other problems. People with bad process and bad architecture saying AI is bad because it doesn't work well with their dumpster fire systems, 100% facepalm. reply aleph_minus_one 2 hours agorootparent> If you can't have ChatGPT write testable code because of your architecture, you have other problems. There exist lots of reasons why code is hard to test automatically that have nothing to do with the architecture of the code, but with the domain for which the code is written and runs. reply jakderrida 12 hours agorootparentprev>The code ChatGPT generates is often bad in ways that are hard to detect. If you are not an experienced software engineer, the defects could be impossible to detect I keep hearing this, but it's incorrect. While I only know R, which is obviously a simple language, I would never type out all my code and go without testing to ensure it does what I intended before using it regularly. So I can't imagine someone that knows a more complex language just typing out all of it before integrating it into business systems at their work or anything else before testing it. Why would AI be any different? Why the hell are AI skeptics acting like getting help from an LLM would involve not testing anything? Of course I test it! Why on earth wouldn't I? Just as I tested code made by freelancers I hired on commission before using the code I bought from them. Do AI skeptics really not test their own code? Are you all insane? reply disgruntledphd2 10 hours agorootparent> While I only know R, which is obviously a simple language Take it from someone who started with R, R is 100% not a simple language. If you can write good R, you're probably a surprisingly good potential SE as R is kinda insane and inconsistent due to 50+ years of history (from S, to R etc). reply jakderrida 9 hours agorootparentHmmm.. I'm trying to imagine interviewing for SE and telling them I got wealthy from a crypto market-making algorithm I coded in R during Covid and the interviewer responding with anything but laughter or with silence as they ponder legal ways to question my mental health. It's an excellent language, I think, for many reasons. One is that you can work with data within hours because even before learning what packages or classes are, you got native objects for data storage, wrangling, and analysis. Even import my Excel data and rapidly learn the native function cheat sheet so fast that I was excited to learn what packages are because I couldn't wait to see what I could do. That was my experience in like 2010, maybe, and after having C++ and Python go in and out my head during college multiple times. I view R as simple only because I actually felt more helpless to keep learning it than helpless to ever learn coding at all. Worth noting that I was a Stat/Probability tutor with a Finance degree and much Excel experience. reply disgruntledphd2 8 hours agorootparent> That was my experience in like 2010, maybe, and after having C++ and Python go in and out my head during college multiple times. I view R as simple only because I actually felt more helpless to keep learning it than helpless to ever learn coding at all. Worth noting that I was a Stat/Probability tutor with a Finance degree and much Excel experience. Ah yeah, makes sense. That's the happy path for learning R (know enough stats etc to decode the help pages). That being said, R is an interesting language with lots of similarities to both C based languages and also Lisp (R was originally a scheme intepreter), so it's surprisingly good at lots of things (except string manipulation, it's terrible at that). reply sumedh 20 hours agorootparentprev> The code ChatGPT generates is often bad in ways that are hard to detect. Does it work though, yes it does. There are many human coders who write bad code and life goes. reply elicksaur 20 hours agorootparentprev>I can't think of a single recent technology that was so widely adopted by tech and non-tech people alike, immediately integrated into day-to-day experience. This is not meant to be an offense, but you are in a bubble. The vast, vast majority of people do not use LLMs in their day-to-day life. That’s ok, we’re all in our own bubbles. You should also post the 2048 clone as proof. Lots people saying they built X in Y minutes with AI. But, when it’s inspected, it’s revealed it very obviously doesn’t work right and needs more development. reply williamcotton 19 hours agorootparentI hand-wrote perhaps 10-20 lines of this project: https://github.com/williamcotton/guish The rest is Claude 3.5 (with a dash of GPT-4o) with a LOT of supervision! I'd say I'm about 8 hours deep and that this would have taken me at least 30+ hours to get it to the current state of polish. I used it to make some graphs at work today! reply bomewish 17 hours agorootparentQuite interesting — but how is it fundamentally more productive than being in VS code in R or python? You don’t get any of the benefits of an IDE here. I often find myself doing very similar workflows but default to either VS Code or the shell. Trying to imagine this truly making workflows faster/easier/more efficient, but can’t figure it. reply williamcotton 17 hours agorootparentMaybe it isn’t? I am just experimenting with new UX! Maybe it could be integrated into an editor of… the fuuuturrre! But seriously, do you have any thoughts or suggestions? reply TeMPOraL 20 hours agorootparentprev> You should also post the 2048 clone as proof. I posted it twice already in this thread, but I guess third time's the charm: http://jacek.zlydach.pl/v/2048/ (code: https://git.sr.ht/~temporal/aider-2048). It's definitely not 100% correct (I just spotted a syntactic issue in HTML, for example), and I bet a lot of people will find some visual issue on their browser/device configuration. I don't care. It works on my desktop, it works on my phone, it's even better than the now-enshittified web and Android versions I used to play. I'm content :). reply cout 19 hours agorootparentIt is too large for my phone display (iphone SE). Do you think chatgpt can fix it? reply TeMPOraL 10 hours agorootparentYes. It does so trivially, but in the process it breaks the CSS for larger screens. I couldn't get it to get both to work at the same time in 5 minutes of trying. My modern CSS skills aren't good enough to quickly notice what the problem is, so it's beyond my horizon of caring (but I do encourage hints I could pass on to the AI). reply elicksaur 5 hours agorootparent>Yes. It does so trivially >but in the process it breaks the CSS for larger screens. So, no, it doesn’t fix it trivially. Also isn’t correctly sized on iPhone 11 Safari. reply TeMPOraL 1 hour agorootparentIt does fix it trivially, just in a way that causes regression on larger screens :). As mentioned above, I don't care. It's sized correctly for the devices I use to play it, and I'm not going to put any more work into this. I mean, even junior web devs get paid stupidly high salaries for doing Responsive Web Design; I ain't gonna work on this for free. (But I will accept AI-generated patches, or human-generated advice I could paste into the prompt to get a correct solution :P.) reply tempusalaria 20 hours agorootparentprevBeing able to create 2048 in 6 hours has basically zero economic value. Can ChatGPT materially and positively impact the code written by big companies? Can it do meaningful work in excel? Can it do meaningful PowerPoint work? Can it give effective advice on management? Right now we don’t know the answer to those questions. LLM apps can still improve in many ways - better base models, better integration with common enterprise applications, agentic processes, verifiability and so on - so there is definitely hope that there will be significant value created. Companies and people are excited because there’s huge potential. But it is really just potential right now … current systems aren’t creating real enterprise value at this moment in time reply TeMPOraL 20 hours agorootparent> Can ChatGPT materially and positively impact the code written by big companies? Can it do meaningful work in excel? Can it do meaningful PowerPoint work? Can it give effective advice on management? > Right now we don’t know the answer to those questions. I know the answer to the first three. Yes, yes, and yes. I've done them all, including all of them in the past few weeks. (Which is how I learned that it's much better to ask ChatGPT to use Python evaluation mode and Pandoc and make you a PPTX, than trying to do anything with \"Office 365 Copilot\" in PowerPoint...) As for the fourth question - well, ChatGPT can give you better advice than most advice on management/leadership articles, so I presume the answer here is \"Yes\" too - but I didn't verify it in practice. > current systems aren’t creating real enterprise value at this moment in time Yes, they are. They would be creating even more value if not for the copyright and exports uncertainty, which significantly slows enterprise adoption. reply WgaqPdNr7PGLGVW 20 hours agorootparent> I know the answer to the first three. Yes, yes, and yes. You say this but from a management perspective at a large enterprise software company I have not seen it. Some of our developers use copilot and gpt and some don't and it is incredibly difficult to see any performance difference between the groups. We aren't seeing higher overall levels of productivity. We aren't seeing the developers who start using copilot/gpt rush ahead of their peers. We aren't seeing any ability to cut back on developer spend. We aren't seeing anything positive yet and many developers have been using copilot/gpt for >1 year. In my opinion we are just regaining some of the economic value we lost when Google Search started degrading 5-10 years ago. reply TeMPOraL 20 hours agorootparent> We aren't seeing higher overall levels of productivity. You can't measure productivity for shit, otherwise companies would look entirely differently. Starting from me not having to do my own finances or event planning or hundred other things that are not my job description, not my specialty, and which were done by dedicated staff just a few decades ago, before tech \"improved office productivity\". > We aren't seeing the developers who start using copilot/gpt rush ahead of their peers. That's because individual productivity is usually constrained by team productivity. Devs rushing ahead of their teammates makes the team dysfunctional. > We aren't seeing any ability to cut back on developer spend. Devs aren't stupid. They're not going to give you an opportunity if they can avoid it. > We aren't seeing anything positive yet and many developers have been using copilot/gpt for >1 year. My belief is that's because you aren't measuring the right things. But then, no one is. This is a problem well-known to be unsolved. reply WgaqPdNr7PGLGVW 19 hours agorootparent> You can't measure productivity for shit We can't measure small changes and we aren't great at comparing across orgs. However, at the director level we can certainly see a 50% or 100% productivity improvement in our teams and with individuals in our teams. We aren't seeing changes of this magnitude because they don't exist. reply WgaqPdNr7PGLGVW 19 hours agorootparentThere are other potential explanations. Perhaps developers are now slacking off. Perhaps we have added more meetings because developers have more free time. Or perhaps developers were never the bottleneck. We can see large productivity improvements when we make simple changes like having product managers join the developers daily standup meetings. We can even measure productivity improvements from Slacks/Zooms auto-summary features. Yet gpt/copilot doesn't even register. reply Vegenoid 18 hours agorootparent> We can even measure productivity improvements from Slacks/Zooms auto-summary features. While not code generation, this auto-summary is powered by the same tech. I think using it to sift through and surface relevant information, as opposed to generation of new things, will have the biggest impact. By far the greatest value I get out of LLMs is asking them to help me understand code written by others. I feel like this is an under-appreciated use. How long has this feature been in Copilot? Since February or so? Are people using it? I do not use Copilot. reply SanderNL 10 hours agorootparentprev> Or perhaps developers were never the bottleneck. Now that's dangerous thinking, but I think you are onto something. reply mbernstein 19 hours agorootparentprevOut of curiosity what are you using to measure developer productivity platform or metrics wish (if beyond typical sprint metrics)? reply bongodongobob 18 hours agorootparentprevI use ChatGPT copilot etc to reduce my cognitive load and get a lot of things done quicker so I also have more time to fuck around. You're out of your goddamn mind if you think I'm going to increase my output for the mere chance that maybe I'll get an above inflation raise in a year. \"We gave our devs a magic 10% productivity boost machine, but their output hasn't increased? I guess the machine doesn't work...\" It's amusing how out of touch you are. reply amanaplanacanal 4 hours agorootparentThere is an ethical question in here that I don’t have an answer for. As an employee, I find a way to do my job more efficiently. Do I hand those efficiencies to my employer so I can get a pat on the head, or do I keep them to myself to make my own life less stressful? If I give them to the boss, do they even have the ability to increase my pay? Using the extra time to slack off rather than enriching the employer might be the best choice. Edit: and now I see chillfox made the same point. reply chillfox 17 hours agorootparentprevPassing on personal productivity gains to management is always a HUGE L for the individual worker. As a dev, you can use the saved time to slow down and not be stressed, spend more time chatting with colleagues, learn new skills, maybe improve the quality of the code, etc. Or you can pass it on to management which will result in your workload being increased back to where you are stressed again and your slower colleagues will be let go, so now you get to feel bad about that and they won't be around to chat with. I have never in my life seen workers actually get rewarded with pay raises for improved productivity, that is just a myth the foolish chase, like the pot of gold at the end of the rainbow. I have also tried being the top performer on a team before (using automation tools to achieve it), and all I got was praise from management. That's nice, but I can't pay for my holidays with praise, so not worth it. reply afro88 19 hours agorootparentprevWriting code is just one part of the process. Other bottlenecks might prevent you from seeing overall productivity improvements. For example: - time between PRs being created and being picked up for review and merged - time spent on releasing at end of sprint cycles - time spent waiting for QA to review and approve - extreme scrum practices like \"you can only work on things in the sprint, even if all work is done\" How are you measuring developer productivity? Were those that adopted copilot and chatgpt now enabled to finally keep up with their faster peers (as opposed to outstrip them)? Is developer satisfaction improved, and therefore retention? reply WgaqPdNr7PGLGVW 19 hours agorootparentYes, other bottlenecks might be preventing us from seeing overall productivity improvements. We might require large organisational changes across the industry in order to take advantage of the improvements. I guess we will see if smaller startups without many of our bottlenecks are suddenly able to be much more competitive. > How are you measuring developer productivity? We use a host of quantitative and qualitative measures. None of them show any positive improvements. These include the basics like roadmap reviews, demo sessions, feature cycle time, etc as well as fairly comprehensive business metrics. In some teams every developer is using copilot and yet we can't see any correlation with it and improved business metrics. At the same time we can measure the impact from changing the label on a button on our UI on these business metrics. > Were those that adopted copilot and chatgpt now enabled to finally keep up with their faster peers No. > Is developer satisfaction improved, and therefore retention? No. reply afro88 14 hours agorootparent> We use a host of quantitative and qualitative measures. None of them show any positive improvements. These include the basics like roadmap reviews, demo sessions, feature cycle time, etc as well as fairly comprehensive business metrics. Those are very high level. If there's no movement on those, I'd guess there are other things bottlenecking the teams. They can code as fast as possible and things still move at the same pace overall. Nice thing to know. If you want to really test the hypothesis that Copilot and ChatGPT have no impact on coding speed, look at more granular metrics to do with just coding. The average time from the moment a developer picks up a work item to the time it gets merged (assuming code reviews happen in a timely fashion). Hopefully you have historical pre-AI data on that metric to compare to. Edit: and average number of defects discovered from that work after merge reply WgaqPdNr7PGLGVW 12 hours agorootparent> look at more granular metrics to do with just coding. The average time from the moment a developer picks up a work item to the time it gets merged (assuming code reviews happen in a timely fashion) We do collect this data. I personally don't put a lot of stock in these kinds of metrics because they depend far too much on the way specific teams operate. For example perhaps Copilot helps developers understand the codebase better so they don't need to break up the tasks into such small units. Time to PR merge goes up but total coding time could easily go down. Or perhaps Copilot works well with very small problem sizes (IMO it does) so developers start breaking the work into tiny chunks Copilot works well with. Time to PR merge goes way down but total code time for a feature stays the same. For what it is worth I do not believe there have been any significant changes with these code level metrics either at the org level. reply SanderNL 11 hours agorootparentprev> We aren't seeing higher overall levels of productivity. > We aren't seeing the developers who start using copilot/gpt rush ahead of their peers. You think we are antsy worker bees, hastily rushing forwards to please the decision maker with his fancy car? You are leadership. It's not hard. Cui bono, follow the money, etc. The incentives are clear. If me and my peers were to receive a magic \"do all my work for me\" device I can assure you exactly zero percent of that knowledge will reach your position. Why would it? The company will give me a pat on the back. I cannot pay with pats on the back. Your Tesla cannot be financed with pats on the back. Surely you understand the nature of this issue. reply CuriouslyC 19 hours agorootparentprevIf you write a spaghetti system where collecting the context for the AI is a big time sink, and there are so many service/language barriers that AI get confused, of course AI is going to suck. Of course, if you give your programmers a game pad and tell them to use it to program with a virtual keyboard, they're gonna suck ass too, so you should consider where the fault really lies. reply Sysreq2 19 hours agorootparentprevIs it the superstars or the line holders that have been the first adopters? I could speculate, but I am actually curious what you are seeing in practice. reply WgaqPdNr7PGLGVW 19 hours agorootparentThe first adopters seem to be the same group / personality type that is always first to adopt new technologies. Very few of these are the superstars. But plenty are good solid senior developers. reply tempusalaria 19 hours agorootparentprevI think you’re thinking about things very locally. Of course ChatGPT can help with some coding - I use it for regex quite often cause I never really learned that well. The problem is that at the average medium sized company code looks like this - you have 1mln lines of code written over a decade by a few hundred people. A big portion of the code is redundant, some of it is incomplete, much of it is undocumented. Different companies have different coding styles, different testing approaches, different development dynamics. ChatGPT does not appreciate this context. Excel has some similar problems. First of all Excel is 2 dimensional and LLMs really don’t think in 2 dimensions well. So you need to flatten the excel file for the LLM. A common approach to do this with LLMs is using pandas and then using the column and row names to index into the excel. Unfortunately, excels at companies cannot be easily read using pandas. They are illogically structured, have tons of hardcoding, intersheet referencing is weird circular ways and so on. I spent some time in finance and sell side equity research models are written by highly trained financial analysts and are substantially better organized than the average excel model at a company. Even this subset of real world models is far from suitable for a direct pandas interpretation. Parsing sell side models requires a delicate and complex interpretation before being fed into an LLM. reply kortilla 19 hours agorootparentprev>Which is how I learned that it's much better to ask ChatGPT to use Python evaluation mode and Pandoc and make you a PPTX, than trying to do anything with \"Office 365 Copilot\" in PowerPoint... Can you elaborate on what this saved over just making the ppt the old fashioned way? reply TeMPOraL 11 hours agorootparent\"I have this set of notes attached below; would you kindly group them by X and tabulate, and then use Python with Pandoc to make me a PowerPoint with that table in it, plus an extra slide with commentary from the notes?\" Attach notes, paste, press Enter, wait half a minute, get back a PPTX you can build on, or just restyle[0]. Sure, it's faster to build the presentation yourself than to make ChatGPT make the whole thing for you. But the more time-consuming and boring parts, like making tables and diagrams and summaries from external data or notes, is something ChatGPT can do in a fraction of time, and can output directly into PPTX via Pandoc. (There's a lot of fun things you can do with official ChatGPT and Python integration. The other day I made it design, write and train a multi-layer perceptron for playing tic-tac-toe, because why waste my own GPU-seconds :).) -- [0] - In contrast, if you make the same request in PowerPoint's O365 Copilot, it'll barf. Last time I tried, it argued it has no capability to edit the document; the time before that, it made a new slide with text saying literally \"data from the previous message\". reply mdorazio 19 hours agorootparentprev> Can ChatGPT materially and positively impact the code written by big companies? It already has at a Fortune 100 company I contract with currently. > Can it do meaningful work in excel? We can quibble about what \"meaningful\" means, but it satisfactorily answered questions for two friends about how to build formulas for their datasets and is currently being used to summarize data insights from a database at a different large client (Excel =/= database, but the point stands). > Can it do meaningful PowerPoint work? I've used Midjourney multiple times a month to generate base imagery for various things in PowerPoint (usually requires modification in Photoshop, but saves me several hours each time compared to digital painting or photobashing from scratch). > Can it give effective advice on management? Again, what does \"effective\" mean in the context of management? I've seen VP-level individuals with hundreds of people in their orgs using AI tools for different things. It really feels like a significant chunk of the HN crowd is living in a bubble with respect to AI in the real world right now. It's absolutely invading everything. As for how much revenue that will translate into long-term vs. the investment dollars being poured into it, that's a more interesting question to discuss. reply nl 16 hours agorootparentprev> Can it do meaningful work in excel? Yes it can. But more importantly have you tried ChatGPT Data Analyst?: https://openai.com/index/improvements-to-data-analysis-in-ch... It drops the barrier for \"pretty good data analysis\" to effectively zero. > Can it do meaningful PowerPoint work? Canva and Figma are both building this and they are pretty decent right now. Better than most PowerPoints I've seen. The aforementioned Data Analyst does good presentations in a different way, too. > Can it give effective advice on management? Yes. Unfortunately can't talk about this except it is mindblowingly good. reply TeMPOraL 11 hours agorootparent> The aforementioned Data Analyst does good presentations in a different way, too. And on top of that, it can do PowerPoint presentations too - magic keywords are \"use Python and Pandoc\". reply reaperman 18 hours agorootparentprev> Can it give effective advice on management? My friends at McKinsey say that while it can’t fine-tune reports and presentations with quite enough nuance, it does a good job sifting through lots of shit to pick out important parts they should pay more attention to, highlighting data/talking points that contradict a working hypothesis, assisting in writing emails, and other time-consuming or very nit-picky tasks. That said, no one I know has fed it real customer data, that would be a career-ending event. But self-hosted models like Gemma2 open up the possibility for using LLMs against real customer info. reply munificent 18 hours agorootparentHard to tell if that says more about the value of LLMs or the lack of value of McKinsey... reply throwaway2037 12 hours agorootparent> the lack of value of McKinsey Leaving McKinsey's specific brand value aside, people always miss the value of \"hiring (business) consultants\". You basically get insider knowledge about how competitors businesses and systems work. So if you ask for advice about how to build a healthcare app for smartphones, you hire McKinsey (or whomever) to tell you \"about the market\". But really, they are just telling you about what they saw at other competitors. For some business decisions, it is very valuable. reply fragmede 14 hours agorootparentprev> Being able to create 2048 in 6 hours has basically zero economic value. That's actually a really good point. In the realm of programming, things that were previously not done because they were too expensive can now be done. Prior to ChatGPT, GP could have a) done it themselves, but the cost was too high/it wasn't worth their time, b) found enough time to write a spec, found someone on upwork/etc, paid them to make it, except that costs money they didn't want to spend, or c) just not do it. Now, GP can code this thing up while watching netflix with the kids or whatever. What programs do not exist that previously did not have the economic value to exist, but now can, thanks to programming time getting cheaper? Now apply that to fields outside of programming. LLMs' ability to program is front and center here, since many of us can program, but they do other things as well. reply ern 17 hours agorootparentprevCan it do meaningful PowerPoint work Yes, it absolutely can. I threw together a PowerPoint presentation with a script for a low-value, high visibility meeting a couple of weeks ago with ChatGPT 4.0 and a PowerPoint plugin. Everyone loved it. reply throwaway2037 12 hours agorootparent> low-value, high visibility meeting This is such a gem. Can you tell us more about the meeting? A senior manager kicking the tyres, or what? Any funny bike-shedding stories to tell? reply ern 12 hours agorootparentCorporate values. Drew the short straw, but had to present something. reply somesortofthing 19 hours agorootparentprev> I can't think of a single recent technology that was so widely adopted by tech and non-tech people alike, immediately integrated into day-to-day experience. I've heard this asserted sometimes, and I just don't think it's true. ChatGPT's use cases as consumer software were discovered basically immediately after GPT 3 came out, and nothing new has really emerged since then. It's great for automating high school/undergrad B-quality writing and the occasional administrative email. Beyond that, it sometimes does better than 2024 Google(though probably still worse than 2019 Google) on knowledge questions. ChatGPT is software. The barrier to entry is almost zero, and the tech industry has had decades of practice in enticing people into walled gardens and making sure they can never leave. If it's not completely taken over the world in the time it's had, I wouldn't bet on it doing so without a massive jump in capability or accuracy. reply myworkinisgood 19 hours agorootparentScientific computing is going to be overhauled with this. We have essentially standardized a way to approximately solve optimization problems. And people are now going to design methods to fit this kind of solution, just like people did with linear solvers. Not to mention all the proof-writing that will become simpler with this optimization/searcher now. reply somesortofthing 19 hours agorootparentSure, AI represents a substantial improvement over other heuristic methods in some areas. But that's a long way down from the \"AI is going to be a permanent fixture in most people's day to day lives\" claim that the tech industry is betting the farm on right now. reply reaperman 18 hours agorootparentI think you’re missing that “every” K-12 student is using ChatGPT for all their work right now. Yes, the state of education is in peril (with ChatGPT actually being pretty far down the pareto chart), but the generation coming up after us is absolutely growing up using LLMs the way we used calculators, and using it for everything. We may not see universal adoption in people who are currently >30 but I think we will in the generations that areThe fact that this power user class of young people hasn't found any other use cases for LLMs Why are you assuming they haven't? High school writing assignments and homework are the majority of the problems teenagers face daily, but they're also having fun with it, and why wouldn't they try it on new problems as they come along? reply threeseed 15 hours agorootparentprevOf course students are using ChatGPT. It helps them to write assignments. But it isn't translating into better across the board test results and at least in Australia we would be able to tell because we have yearly standardised testing. And so schools are looking at it as more of a form of cheating and simply moving back to in-person, hand-written tests. reply pas 19 hours agorootparentprevwith what? how? there's already a lot of bad python/R code out there, how more of it will \"overhaul\" scientific computing? > We have essentially standardized a way to approximately solve optimization problems. .. what does this mean? we had simplex solvers before. do you mean things like protein folding prediction? reply ryandrake 21 hours agorootparentprev> How can anyone not see just how impactful it's going to be? Or already is? I can't think of a single recent technology that was so widely adopted by tech and non-tech people alike, immediately integrated into day-to-day experience. The rise of mobile phones and e-commerce in the 90s would be the last time I've seen this happen (I'm not counting smartphones, as those are more of an iteration). Or social media, in purely software space. You can't know this for certain until you look back on it in retrospect. We did not know mobile phones and e-commerce were going to be huge back in the 90s. We know now, of course, looking back, and the ones who guessed right back then can pat themselves on the back now. Everyone is guessing. I'll admit it's totally possible LLMs and AI are going to be as earth shattering as its boosters claim it will be, but nobody can know this now with as much certainty as is being written. reply doe_eyes 20 hours agorootparent> We did not know mobile phones and e-commerce were going to be huge back in the 90s. Eh? We did. The whole dot-com boom was predicated on that assumption. And it wasn't wrong. But most of the dot-com investments went sideways. In fact, they imploded hard enough to cause a recession. In the same vein, even if we all agree that AI is fundamentally transformative, it doesn't mean that it's wise to invest money into it right now. It's possible that most or all of these early products and companies will go bust. reply ganoushoreilly 18 hours agorootparentI think this is the right sentiment. I know a handful of AI startups that have raised 10's to 100's of millions. All of them were crushed with gpt-3 and subsequent models. None of them have any real revenue, have crazy burn rates with their compute spend, and generally haven't proven any value with their AI platforms. Most seem to be working on tech to find a problem rather than the inverse. Funds are throwing money on ideas.. that haven't panned out for years now. I worked with one and they were spending 10's of millions per researcher on AI compute... which makes sense if it's directed but most of the researchers were just running off on their own and the company hoped one would figure something out. Very disorganized for the stacks of cash being spent. Similar things have happened in the Cyber Security field just at a lesser scale. But hey, Nvidia is investing in companies.. to spend money on Nvidia .. infinite money glitch! reply TeMPOraL 21 hours agorootparentprev> You can't know this for certain until you look back on it in retrospect. Correct, but the thing is, AI blown up much faster than phones - pretty much a decade in a single year, in comparison. Mobile phones weren't that useful early on, outside of niche cases. Generative AI is already spreading to every facet of peoples' lives, and has even greater bottom-up adoption among regular people, than top-down adoption in business. reply Quothling 21 hours agorootparent> Correct, but the thing is, AI blown up much faster than phones What do you base that on though? Two years into the iPhone, Apple reported a $6.75b revenue on iPhone related sales. ChatGPT may reach or surpass that this year considering they're currently at $3.4b. That's not exactly what I would call growing faster than phones, however, and according to this article, very few people outside of nvidia and OpenAI are actually making big money on LLM's. I do think it's silly to see this wave of AI to be referred to as the next blockchain, but I also think you may be hyping it a little beyond its current value. It being a fun and useful tool for a lot of things isn't necessarily the same thing at it being something that's actually worth the money investors are hoping it will be. reply sib 18 hours agorootparent>> Correct, but the thing is, AI blown up much faster than phones >What do you base that on though? Two years into the iPhone, Apple reported a $6.75b revenue on iPhone related sales. ChatGPT may reach or surpass that this year considering they're currently at $3.4b. But the iPhone was launched more than 10 years past mobile phones (in fact, more than 20, but that's stretching it). There were more than 1B mobile phones shipped in 2006, the year before the iPhone launched. reply TeMPOraL 21 hours agorootparentprev> What do you base that on though? My childhood? I was a teen when mobile phones started to become widely used, and soon after pretty much necessary, in my part of the world. But, to reiterate: > Two years into the iPhone, Apple reported a $6.75b revenue on iPhone related sales. That's just an iteration, and not what I'm talking about. Smartphones were just different mobile phones. I'm talking about the adoption of a mobile phone as a personal device by general population. > It being a fun and useful tool for a lot of things isn't necessarily the same thing at it being something that's actually worth the money investors are hoping it will be. That's probably something which needs to be disentangled in these conversations. I personally don't care what investors think and do. AI may be hype for the VCs. It's not hype for regular Janes and Joes, who either already integrated ChatGPT into their daily lives, or see their friends doing so. reply dragontamer 20 hours agorootparentIts a lot easier to use AI when its basically given away for free than when it cost $399 for a Palm Pilot in the 90s. For a $399 device, Palm Pilot did well and had an excellent reputation for the time. Phones really took over the PDA market as a personal pocket-computer more-so than being used as ... a phone... Really, I consider the modern smartphone a successor to the humble PDA. I grew up in that time too, and I remember the early Palm adopters having to explain why PDAs (and later Blackberries) were useful. That was already all figured out by the time iPhone took over. reply Quothling 13 hours agorootparentprev> I personally don't care what investors think and do. Isn't this a an odd take when you're discussing things on a VC website? In any case, if you like LLM's you probably should care considering it's the $10b Microsoft poured into OpenAI that's made the current landscape possible. Sure, most of those money were fuled directly into Azure because that's where OpenAI does all it's compute, but still. > It's not hype for regular Janes and",
    "originSummary": [
      "The AI revenue gap has widened from $200B to $600B, raising questions about the industry's growth expectations.",
      "Key developments include the easing of the GPU supply shortage, Nvidia's increased data center revenue, and OpenAI's significant revenue growth to $3.4B.",
      "Challenges such as lack of pricing power, investment risks, and rapid depreciation of older chips persist, but lower GPU costs could benefit startups and innovation."
    ],
    "commentSummary": [
      "Training large AI models like GPT-4 requires significant computational resources, with estimates suggesting 8,000 H100 GPUs running for 90 days.",
      "Meta's substantial GPU investments could allow them to train multiple GPT-4 scale models annually, potentially commoditizing core AI models and impacting profit margins for AI companies.",
      "The real value in AI may shift towards proprietary data for training, raising potential legal issues and emphasizing the importance of data ownership."
    ],
    "points": 331,
    "commentCount": 496,
    "retryCount": 0,
    "time": 1720036541
  },
  {
    "id": 40870345,
    "title": "Beating NumPy matrix multiplication in 150 lines of C",
    "originLink": "https://salykova.github.io/matmul-cpu",
    "originBody": "Beating NumPy's matrix multiplication in 150 lines of C code Jul 1, 2024 • Aman Salykov TL;DR The code from the tutorial is available at matmul.c. This blog post is the result of my attempt to implement high-performance matrix multiplication on CPU while keeping the code simple, portable and scalable. The implementation follows the BLIS design, works for arbitrary matrix sizes, and, when fine-tuned for an AMD Ryzen 7700 (8 cores), outperforms NumPy (=OpenBLAS), achieving over 1 TFLOPS of peak performance across a wide range of matrix sizes. By efficiently parallelizing the code with just 3 lines of OpenMP directives, it’s both scalable and easy to understand. The implementation hasn’t been tested on other CPUs, so I would appreciate feedback on its performance on your hardware. Although the code is portable and targets Intel Core and AMD Zen CPUs with FMA3 and AVX instructions (i.e., all modern Intel Core and AMD Zen CPUs), please don’t expect peak performance without fine-tuning the hyperparameters, such as the number of threads, kernel, and block sizes, unless you are running it on a Ryzen 7700(X). Additionally, on some Intel CPUs, the OpenBLAS implementation might be notably faster due to AVX-512 instructions, which were intentionally omitted here to support a broader range of processors. Throughout this tutorial, we’ll implement matrix multiplication from scratch, learning how to optimize and parallelize C code using matrix multiplication as an example. This is my first time writing a blog post. If you enjoy it, please subscribe and share it! I would be happy to hear feedback from all of you. This is the first part of my planned two-part blog series. In the second part, we will learn how to optimize matrix multiplication on GPUs. Stay tuned! Intro Matrix multiplication is an essential part of nearly all modern neural networks. For example, most of the time spent during inference in Transformers is actually taken up by matrix multiplications. Despite using matmul daily in PyTorch, NumPy, or JAX, I’ve never really thought about how it is designed and implemented to maximize hardware efficiency. To achieve such speeds, NumPy, for instance, relies on external BLAS (Basic Linear Algebra Subprograms) libraries. These libraries implement highly optimized common linear algebra operations such as dot product, matrix multiplication, vector addition, and scalar multiplication. Examples of BLAS implementations include: Intel MKL - optimized for Intel CPUs Accelerate - optimized for Apple CPUs BLIS - open-source, multi-vendor support GotoBLAS - open-source, multi-vendor support OpenBLAS - open-source, based on GotoBLAS etc. If you look at the OpenBLAS code, you’ll notice it’s a mix of C and low-level assembly code. In fact, OpenBLAS, GotoBLAS, and BLIS are all written in C/FORTRAssembly and contain matmul implementations handcrafted for different CPU types. During runtime, the appropriate function is called depending on the detected CPU device. I challenged myself and asked if it is possible to write a high-performance matmul (across a wide range of matrix sizes) without diving deep into Assembly and Fortran code, at least for my CPU. After some searching on the internet, I found a couple of exciting and educational step-by-step tutorials on how to implement fast matmul from scratch, covering both theoretical and practical aspects: Fast Multidimensional Matrix Multiplication on CPU from Scratch by Simon Boehm. Matrix Multiplication by Sergey Slotin. Geohot’s famous stream Can you multiply a matrix? I highly recommend checking out these well-written and well-spoken tutorials with alternative matmul implementations. They helped me better understand the topic and, in some sense, motivated me to write a different implementation. Why? The reason is that all three solutions above work only for specific matrix sizes and do not achieve NumPy’s multi-threaded speed (except for Geohot’s implementation, which is comparable to NumPy in terms of speed but again works only for specific matrix sizes and requires an extra preswizzle step, resulting in a full copy of one of the input matrices). So, I wasn’t satisfied with the results and continued researching until I stumbled across two fascinating papers: “Anatomy of High-Performance Matrix Multiplication” and “Anatomy of High-Performance Many-Threaded Matrix Multiplication”. The former presents the BLAS implementation known as GotoBLAS, developed by Kazushige Goto. The latter briefly reviews the design of matmul op used in BLIS (an extended version of GotoBLAS) and discusses different parallelization possibilities for the matmul algorithm. After reading these papers I felt that the BLIS matmul design could potentially achieve all my goals: NumPy-like multi-threading performance across a broad range of matrix sizes Simple, portable and scalable C code Support for a wide variety of processors In the next sections, we will implement the algorithm from the paper and compare it against NumPy. NumPy Performance By default, if installed via pip, numpy uses OpenBLAS on AMD CPUs. Therefore, throughout this tutorial I will use numpy and OpenBLAS interchangeably. Before performing any benchmarks, it’s always good practice to specify your hardware specs and development environment to ensure the results can be reproduced: CPU: Ryzen 7 7700 8 Cores, 16 Threads Freq: 3.8 GHz Turbo Freq: 5.3 GHz L1 Cache: 64 KB (per core) L2 Cache: 1MB (per core) L3 Cache: 32MB (shared), 16-way associative RAM: 32GB DDR5 6000 MHz CL36 Numpy 1.26.4 Compiler: clang-17 Compiler flags: -O2 -mno-avx512f -march=native OS: Ubuntu 22.04.4 LTS To multiply two float32 matrices A of shape \\(M \\times K\\) and B of shape \\(K \\times N\\), for each element of the resulting matrix C of shape \\(M \\times N\\), we need to calculate the dot product between a row of A and a column of B. This results in \\(K\\) (additions) + \\(K\\) (multiplications) = \\(2K\\) FLoating Point Operations (FLOP) per element of matrix C or \\(2KMN\\) FLOP in total. We will measure performance in terms of FLOP per second FLOP/s=FLOPS. In Python, this can be simply done as follows: import numpy as np import time A = np.random.randn(M, K).astype(np.float32) B = np.random.randn(K, N).astype(np.float32) FLOP = 2*K*M*N start = time.perf_counter() C = A @ B end = time.perf_counter() exec_time = end - start FLOPS = FLOP/exec_time GFLOPS = FLOPS/1e9 Important! When benchmarking code, try to minimize the number of running tasks, especially when measuring multi-threaded code. Results obtained on Windows are usually lower than on Linux. To benchmark numpy’s matmul, we will use benchmark_numpy.py, which executes the code snippet above for different matrix sizes in a loop and measures peak/average FLOPS. By default, numpy will use all available cores; however, we can easily change this by setting environment variables before importing numpy and matplotlib os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" os.environ[\"MKL_NUM_THREADS\"] = \"1\" os.environ[\"OMP_NUM_THREADS\"] = \"1\" import numpy as np import matplotlib.pyplot as plt To measure Numpy’s matmul performance, run python benchmark_numpy.py -NITER=200 -ST -SAVEFIG for single-threaded benchmark and python benchmark_numpy.py -NITER=500 -SAVEFIG for multi-threaded benchmark. On my machine I got the following results: How close are we to the theoretical upper limit achievable on the CPU? Theoretical Limit Recall the computer’s memory hierarchy (for now, ignore the layers between registers and RAM; we will discuss them later). To perform arithmetic operations on data stored in RAM (off-chip memory, slow and large), the data must first be transferred to the CPU and eventually stored in CPU registers (on-chip memory, fast and small). Modern x86 CPUs support SIMD (Single Instruction Multiple Data) extensions, which allow multiple pieces of data to be processed in parallel. There are various SIMD extensions, but the ones relevant to our discussion are Advanced Vector Extensions (AVX) and Fused Multiply-Add (FMA). Both AVX and FMA operate on data stored in special 256-bit YMM registers. Each YMM register can hold up to 256/32 = 8 packed single-precision (32-bit) floats. The FMA extension allows a multiply-add operation to be performed in one step on data stored in YMM registers. The corresponding assembly instruction is called VFMADD213PS (PS stands for PackedSingle) and takes three registers (YMM1, YMM2, YMM3) as input to calculate YMM1 * YMM2 + YMM3 and store the result in YMM3, hence the “213” (there are also vfmadd132ps, vfmadd231ps variants). According to the intel intrinsics guide or https://uops.info/table.html, the throughput (TP) of fused-multiply-add is 0.5 cycles/instruction or 2 instructions/cycle: Theoretically, the CPU can execute 32 FLOP per cycle = 8 (floats in YMM register) * 2 (add + mul) * 2 (1/TP). On my machine, the CPU boosts up to 5.1 GHz in single-threaded tasks and up to 4.7 GHz in multi-threaded tasks. Therefore, a rough estimation of the maximum achievable FLOPS can be calculated as 5.1GHz * 32 FLOP/cycle = 163 GFLOPS for single-threaded matmul and 4.7GHz * 32 FLOP/cycle * 8 cores = 1203 GFLOPS for multi-threaded matmul. Starting from \\(M=N=K=1000\\), numpy reaches on average 92% of the theoretical maximum single-threaded performance and 85% of the multi-threaded. Can we compete with NumPy using plain C code without thousands of lines of low-level assembly code? Naive Implementation Without loss of generality in this implementation we will assume that matrices stored in column-major order. A matrix A of shape MxN is stored as contiguous array of length M*N and an element A[row][col] is accessed via C raw pointer ptr[col*M + row], where 0the kernel function is declared as follows: void kernel_16x6(float* A, float* B, float* C, const int M, const int N, const int K); The function takes as input 3 matrices + their dimensions and calculates a $16\\times6$ sub-matrix $\\bar{C}$ of $C$. Inside the function, first, declare the variables that reside in YMM registers: __m256 C_buffer[2][6]; __m256 b_packFloat8; __m256 a0_packFloat8; __m256 a1_packFloat8; The __m256 datatype is a vector of 8 floats (8x32 = 256 bits) that resides in YMM register. C_buffer is a 16x6 sub-matrix of $C$ stored in YMM registers. The first dimension of C_buffer is 2, because we need 16/8=2 registers to store 16 elements. b_packFloat8, a0_packFloat8, a1_packFloat8 are column vectors of $\\bar{B}$ and $\\bar{A}$. Again, we need two vectors to store 16 elements of the column vector of $\\bar{A}$. Next, we load the sub-matrix $\\bar{C}$ into YMM registers: for (int j = 0; jmatmul_kernel.txt to ensure that the SIMD instructions and the YMM registers are utilized: vbroadcastss (%rsi,%rbp,4), %ymm14 vbroadcastss (%rbx,%rbp,4), %ymm15 vfmadd231ps %ymm14, %ymm12, %ymm3 # ymm3 = (ymm12 * ymm14) + ymm3 vfmadd231ps %ymm14, %ymm13, %ymm1 # ymm1 = (ymm13 * ymm14) + ymm1 vbroadcastss (%r13,%rbp,4), %ymm14 vfmadd231ps %ymm12, %ymm15, %ymm11 # ymm11 = (ymm15 * ymm12) + ymm11 vfmadd231ps %ymm15, %ymm13, %ymm10 # ymm10 = (ymm13 * ymm15) + ymm10 vfmadd231ps %ymm14, %ymm12, %ymm2 # ymm2 = (ymm12 * ymm14) + ymm2 vfmadd231ps %ymm14, %ymm13, %ymm0 # ymm0 = (ymm13 * ymm14) + ymm0 vbroadcastss (%r12,%rbp,4), %ymm14 vfmadd231ps %ymm14, %ymm12, %ymm5 # ymm5 = (ymm12 * ymm14) + ymm5 vfmadd231ps %ymm14, %ymm13, %ymm4 # ymm4 = (ymm13 * ymm14) + ymm4 vbroadcastss (%r15,%rbp,4), %ymm14 vfmadd231ps %ymm14, %ymm12, %ymm7 # ymm7 = (ymm12 * ymm14) + ymm7 vfmadd231ps %ymm14, %ymm13, %ymm6 # ymm6 = (ymm13 * ymm14) + ymm6 vbroadcastss (%r14,%rbp,4), %ymm14 vfmadd231ps %ymm14, %ymm12, %ymm9 # ymm9 = (ymm12 * ymm14) + ymm9 vfmadd231ps %ymm14, %ymm13, %ymm8 # ymm8 = (ymm13 * ymm14) + ymm8 Masking And Packing You might notice that the current kernel implementation works only for matrix sizes that are multiples of $m_R$ and $n_R$. To make the algorithm work for arbitrary matrix sizes, we need to handle edge cases where the kernel doesn’t fully overlap with matrix $C$. First of all, we when loading and storing the elements of $C$, we should pick the elements only within the matrix boundary. The case where the number of overlapped columns $n$ is less than $n_R$ is straightforward - we simply iterate over $n$ columns within the $C$ boundary: # n - number of overlapped columns within C boundary # \"j<n\" instead \"j<6\", since n can be less than 6. for (int j = 0; j < n; j++) { C_buffer[0][j] = _mm256_loadu_ps(&C[j * M]); C_buffer[1][j] = _mm256_loadu_ps(&C[j * M + 8]); } Handling the case where the number of overlapped rows $m$ differs from $m_R$ is a bit trickier because _mm256_loadu_ps loads 8 elements at once. Fortunately, there is a function called _mm256_maskload_ps which loads 8 floats based on mask bits associated with each data element. It takes as input 2 arguments: const float* data and __m256i mask. __m256i is a 256-bit vector of 8x32-bit integers. The most significant bit (MSB) of each integer represents the mask bits. If a mask bit is zero, the corresponding value in the memory location is not loaded and the corresponding field in the return value is set to zero. For example, MSB of unsigned integer 2147483648 (binary representation 10000000 00000000 00000000 00000000) is 1, hence corresponding float in data will be loaded. On the other hand, MSB of unsigned integer 2147483647 (binary format 01111111 11111111 11111111 11111111) is 0, hence the corresponding float in data will not be loaded. The function _mm256_maskstore_ps works similarly, except it stores data instead of loading. If $m eq m_R$ , we create integer masks by left-shifting the unsigned integer 65535 (=00000000 00000000 11111111 111111111 in binary format) depending on the number of overlapped rows $m$. The function _mm256_setr_epi32 creates an 8-integer vector from 8 32-bit integers. __m256i masks[2]; if (m != MR) { const unsigned int bit_mask = 65535; masks[0] = _mm256_setr_epi32(bit_mask << (m + 15), bit_mask << (m + 14), bit_mask << (m + 13), bit_mask << (m + 12), bit_mask << (m + 11), bit_mask << (m + 10), bit_mask << (m + 9), bit_mask << (m + 8)); masks[1] = _mm256_setr_epi32(bit_mask << (m + 7), bit_mask << (m + 6), bit_mask << (m + 5), bit_mask << (m + 4), bit_mask << (m + 3), bit_mask << (m + 2), bit_mask << (m + 1), bit_mask << m); for (int j = 0; j < n; j++) { C_buffer[0][j] = _mm256_maskload_ps(&C[j * M], masks[0]); C_buffer[1][j] = _mm256_maskload_ps(&C[j * M + 8], masks[1]); } } The same masks are used to store the results back after rank-1 updates. Additionally, we copy and pad with zeros (if needed) $m \\times K$, $K \\times n$ blocks of $A$ and $B$ into arrays with static shapes $m_R \\times K$, $n_R \\times K$. void pack_blockA(float* A, float* blockA_packed, const int m, const int M, const int K) { for (int p = 0; p < K; p++) { for (int i = 0; i < m; i++) { *blockA_packed = A[p * M + i]; blockA_packed++; } for (int i = m; i < MR; i++) { *blockA_packed = 0.0; blockA_packed++; } } } These blocks with static shapes are then passed into the kernel, so that the rank-1 update inside the kernel can remain unchanged and be optimized during compilation time. void matmul_pack_mask(float* A, float* B, float* C, float* blockA_packed, float* blockB_packed, const int M, const int N, const int K) { for (int i = 0; i < M; i += MR) { const int m = min(MR, M - i); pack_blockA(&A[i], blockA_packed, m, M, K); for (int j = 0; j < N; j += NR) { const int n = min(NR, N - j); pack_blockB(&B[j * K], blockB_packed, n, N, K); kernel_16x6(blockA_packed, blockB_packed, &C[j * M + i], m, n, M, N, K); } } } The new implementation matmul_cache.c achieves “only” 56 GFLOPS on my machine: clang-17 -O2 -mno-avx512f -march=native -DTEST -DNITER=100 matmul_pack_mask.c -o matmul_pack_mask.out && ./matmul_pack_mask.out We see roughly a 2.6x decrease in performance, mostly because of frequently copying large $K$ dimensional sub-matrices of $A$ and $B$ from main memory. For each $m_R \\times K$ sub-matrix of $A$ the entire(!) matrix $B$ is copied. Let’s optimize data reuse and cache management to finally achieve numpy’s level of performance for arbitrary matrix sizes. Caching Recall the CPU’s memory system diagram. Initially, we’ve ignored the intermediate layer between main-memory (DRAM) and the CPU’s registers - the CPU Cache. Unlike DRAM, the cache is on-chip memory used to store frequently and recently accessed data from main memory. This minimizes data transfers between main memory and registers. Although faster than DRAM, the cache has limited capacity. CPUs typically employ a multi-level cache hierarchy for efficient data access. Levels like L1, L2, and L3 offer progressively larger capacities but slower access times, with L1 being the fastest and closest to the core. Intel Core i9-13900K labelled die shot. Source: How are Microchips Made? To enhance access speed, CPUs transfer data between main memory and cache in fixed-size chunks called cache lines or cache blocks. When a cache line is transferred, a corresponding cache entry is created to store it. On Ryzen 7700, the cache line size is 64 bytes. The cache takes advantage of how we typically access data. When a single floating-point number from a continuous array in memory is requested, the cache cleverly grabs the next 15 floats along the way and stores them as well. This is why reading data sequentially from a contiguous array is much faster than jumping around to random memory locations. When the processor needs to read or write to a memory location, it first checks the cache for a corresponding entry. If the processor finds the memory location in the cache, a cache hit occurs. However, if the memory location is not found in the cache, a cache miss occurs. In the case of a cache miss, the cache allocates a new entry and copies the data from main memory. If the cache is full, a cache replacement policy kicks in to determine which data gets evicted to make room for new information. Several cache replacement policies exist, with LRU (Least Recently Used), LFU (Least Frequently Used), and LFRU (Least Frequently Recently Used) being the most widely used. Similar to registers, once data is loaded into the cache, we want to reuse the data as much as possible to reduce main memory accesses. Given the cache’s limited capacity, storing entire input matrices input matrices $C, B, A$ in the cache isn’t feasible. Instead, we divide them into smaller blocks, load these blocks into the cache, and reuse them for rank-1 updates. This technique is often referred to as tiling or cache blocking, allowing us to handle matrices of arbitrary size effectively. The final single-threaded matrix multiplication implementation, including the cache blocking, can be visualized as shown in the image borrowed from the official BLIS repository: Let’s step through the diagram and discuss it. In the outer-most loop (5th loop) we iterate over dimension $N$, dividing matrix $C$ into blocks $C_j$ of size $M \\times n_c$ and matrix $B$ into blocks $B_j$ of size $K \\times n_c$. The subscript $c$ in $n_c$ stands for cache. In the 4th loop we iterate over dimension $K$ and divide matrix $A$ into $A_j$ of size $M \\times k_c$ and $B_j$ into $B_p$ of size $k_c \\times n_c$. Notice $B_p$ has fixed, limited size and can now be loaded into the cache. $B_p$ is packed into $\\tilde{B}_p$, padded with zeros, if necessary, and loaded into the L3 cache. I In the 3rd loop we iterate over dimension $M$ and divide $C_j$ into $C_i$ (there is a typo in the diagram) of size $m_c \\times n_c$ and $A_p$ into $A_j$ of size $m_c \\times k_c$. Matrix $A_j$ is now restricted in size and can be loaded entirely into the L2 cache. $A_j$ is packed into $\\tilde{A}_j$ and padded with zeros if needed. Note how we reuse the same $\\tilde{B}_p$ block from the L3 cache for different $A_j$ blocks. Both $m_c$ and $n_c$ are chosen to be a multiple of $m_r$ and $n_r$ respectively. In the last two loops we simply iterate over cached blocks and divide them into $m_R \\times k_c$ and $k_c \\times n_R$ panels. These panels are then passed to the kernel to perform rank-1 updates on the $m_R \\times n_R$ sub-matrix of $C$, similarly to what we have already done in the previous chapter. Each panel of $\\tilde{B}_p$ is loaded into the L1 cache and reused for multiple panels of $\\tilde{A}_j$. Keep in mind that $\\tilde{A}_j$ and $\\tilde{B}_p$ are packed differently. During rank-1 updates we sequentially read a panel of $\\tilde{A}_j$ column by column and a panel of $\\tilde{B}_p$ row by row. Thus, each panel inside $\\tilde{A}_j$ is stored in column-major order, while each panel inside $\\tilde{B}_p$ is stored in row-major order. Different CPU models have varying cache sizes. To achieve peak performance, it’s crucial to optimize three key parameters: cache sizes for L1, L2, and L3 cashes (represented by $k_c$, $m_c$, and $n_c$ respectively). Theoretically, these parameters should be chosen so that: The matrix $k_c \\times n_c$ fills the entire L3 cache. The matrix $m_c \\times k_c$ fills the entire L2 cache. The matrix $k_c \\times n_R$ fills the entire L1 cache. While these values provide a good starting point, using larger values often leads to better performance in practice. Unfortunately (or fortunately), we cannot manually place data into the cache or control which cache levels store the data; the CPU manages this automatically using cache replacement policies. Therefore, cache blocking and cache reuse must be implemented at the algorithm level through, for example, well-designed loops and strategic data access patterns. The implementation straightforwardly follows the algorithm depicted in the diagram: void matmul_cache(float* A, float* B, float* C, const int M, const int N, const int K) { for (int j = 0; j < N; j += NC) { // 5th loop const int nb = min(NC, N - j); for (int p = 0; p < K; p += KC) { // 4th loop const int kb = min(KC, K - p); pack_blockB(&B[j * K + p], blockB_packed, nb, kb, K); for (int i = 0; i < M; i += MC) { // 3rd loop const int mb = min(MC, M - i); pack_blockA(&A[p * M + i], blockA_packed, mb, kb, M); for (int jr = 0; jr < nb; jr += NR) { // 2nd loop const int nr = min(NR, nb - jr); for (int ir = 0; ir < mb; ir += MR) { // 1st loop const int mr = min(MR, mb - ir); kernel_16x6(&blockA_packed[ir * kb], &blockB_packed[jr * kb], &C[(j + jr) * M + (i + ir)], mr, nr, kb, M); } } } } } } Before implementing the multi-threaded version of the algorithm, let’s benchmark our current implementation and compare it against numpy: python benchmark_numpy.py -ST clang-17 -O2 -mno-avx512f -march=native benchmark_st.c -o benchmark_st.out && ./benchmark_st.out python plot_benchmark.py Multithreading There are indeed many loops that can be potentially parallelized. To achieve high-performance, we want to parallelize both packing and arithmetic operations. Let’s start with the arithmetic operations. The 5th, 4th, 3rd loops around the micro-kernel iterate over matrix dimensions in chunks of cache block sizes $n_c$, $k_c$, $m_c$. To efficiently parallelize the loops and keep all threads busy, we want number of iterations (=matrix dimension / cache block size) to be at least = number of threads (generally, the more the better). In other words, the input matrix dimension should be at least = number of threads * cache block size. As we discussed earlier, we also want cache blocks to fully occupy the corresponding cache levels. On modern CPUs, this second requirement results in cache block sizes of thousand(s) of elements. For example, on my Ryzen 7700, cache block sizes of $n_c=1535$, $m_c=1024, k_c=2000$ attain the best performance in the single-threaded case. Given the number of available threads on Ryzen 7700, $Nthreads=16$, we need input matrices with dimensions of at least $2000 \\times 16$ to be able to distribute the work over all threads. In contrast, the last two loops iterate over cache blocks, dividing them into $m_r, n_r$ blocks. Since $n_r, m_r$ are typically very small (<20), these loops are ideal candidates for parallelization. Moreover, we can choose $m_c, n_c$ to be multiples of $Nthreads$ so that the work is evenly distributed across all threads. On my machine, parallelizing the second loop results in much better performance compared to the first loop (possibly due to large $n_c$ and little work in each iteration of the first loop). We will therefore parallelize the second loop using OpenMP directives (more on OpenMP here, here and here): #pragma omp parallel for num_threads(NTHREADS) schedule(static) for (int jr = 0; jr < nb; jr += NR) It’s also possible to parallelize the 2nd and 1st loops using #pragma omp parallel for collapse(2), which leads to similar performance when parallelizing only the 2nd loop. Together with arithmetic operations, we also want to accelerate the packing of both $\\tilde{A}$ and $\\tilde{B}$: void pack_blockA(float* A, float* blockA_packed, const int mb, const int kb, const int M) #pragma omp parallel for num_threads(NTHREADS) schedule(static) for (int i = 0; i < mb; i += MR) void pack_blockB(float* B, float* blockB_packed, const int nb, const int kb, const int K) #pragma omp parallel for num_threads(NTHREADS) schedule(static) for (int j = 0; j < nb; j += NR) Similar to arithmetic operations, the packing loops can be easily parallelized due to the high number of iterations and the flexibility of choosing $m_c, k_c, n_c$. Running clang-17 -O2 -mno-avx512f -march=native -DNITER=100 -fopenmp matmul_parallel.c -o matmul_parallel.out && ./matmul_parallel.out shows around 1 TFLOPS. Don’t forget to add the -fopenmp compiler flag to use OpenMP directives. You might also need to install libomp-dev with sudo apt install libomp-dev. Let’s check the CPU utilization htop and benchmark the multithreading implementation: python benchmark_numpy.py clang-17 -O2 -mno-avx512f -march=native -fopenmp benchmark_mt.c -o benchmark_mt.out && ./benchmark_mt.out python plot_benchmark.py",
    "commentLink": "https://news.ycombinator.com/item?id=40870345",
    "commentBody": "Beating NumPy matrix multiplication in 150 lines of C (salykova.github.io)298 points by p1esk 21 hours agohidepastfavorite51 comments epr 1 hour agoIf the point of this article is that there's generally performance left on the table, if anything it's understating how much room there generally is for improvement considering how much effort goes into matmul libraries compared to most other software. Getting a 10-1000x or more improvement on existing code is very common without putting in a ton of effort if the code was not already heavily optimized. These are listed roughly in order of importance, but performance is often such a non-consideration from most developers that a little effort goes a long way. 1. Most importantly, is the algorithm a good choice? Can we eliminate some work entirely? (this is what algo interviews are testing for) 2. Can we eliminate round trips to the kernel and similar heavy operations? The most common huge gain here is replacing tons of malloc calls with a custom allocator. 3. Can we vectorize? Explicit vector intrinsics like in the blog post are great, but you can often get the same machine code by reorganizing your data into arrays / struct of arrays rather than arrays of structs. 4. Can we optimize for cache efficiency? If you already reorganized for vectors this might already be handled, but this can get more complicated with parallel code if you can't isolate data to one thread (false sharing, etc.) 5. Can we do anything else that's hardware specific? This can be anything from using intrinsics to hand-coding assembly. reply throwaway4good 2 minutes agoprevWhat is the point of making the matrix multiplication itself multithreaded (other than benchmarking)? Wouldn't it be more beneficial in practice to have the multithreadedness in the algorithm that use the multiplication? reply bjourne 4 hours agoprevGood writeup and commendable of you to make your benchmark so easily repeatable. On my 16-core Xeon(R) W-2245 CPU @ 3.90GHz matmul.c takes about 1.41 seconds to multiply 8192x8192 matrices when compiled with gcc -O3 and 1.47 seconds when compiled with clang -O2, while NumPy does it in 1.07 seconds. I believe an avx512 kernel would be significantly faster. Another reason for the lackluster performance may be omp. IME, you can reduce overhead by managing the thread pool explicitly with pthreads (and use sysconf(_SC_NPROCESSORS_ONLN) instead of hard-coding). reply ssivark 16 hours agoprevMost common coding patterns leave a lot of performance unclaimed, by not fully specializing to the hardware. This article is an interesting example. For another interesting demonstration, see this CS classic \"There's plenty of room at the top\" https://www.science.org/doi/10.1126/science.aam9744 reply auselen 7 hours agoparentTitle comes from: https://en.m.wikipedia.org/wiki/There%27s_Plenty_of_Room_at_... reply hmaarrfk 13 hours agoparentprevThanks for sharing. That was a great read reply ks2048 16 hours agoprevThis looks like a nice write-up and implementation. I'm left wondering what is the \"trick\"? How does it manage to beat OpenBLAS, which is assembly+C optimized over decades for this exact problem? It goes into detail about caching, etc - is BLAS is not taking advantage of these things, or is this more tuned to this specific processor, etc? reply hansvm 15 hours agoparent- OpenBLAS isn't _that_ optimized for any specific modern architecture. - The matrices weren't that big. Numpy has cffi overhead. - The perf difference was much more noticeable with _peak_ throughput rather than _mean_ throughput, which matters for almost no applications (a few, admittedly, but even where \"peak\" is close to the right measure you usually want something like the mean of the top-k results or the proportion with under some latency, ...). - The benchmarking code they displayed runs through Python's allocator for numpy and is suggestive of not going through any allocator for the C implementation. Everything might be fine, but that'd be the first place I checked for microbenchmarking errors or discrepancies (most numpy routines allow in-place operations; given that that's known to be a bottleneck in some applications of numpy, I'd be tempted to explicitly examine benchmarks for in-place versions of both). - Numpy has some bounds checking and error handling code which runs regardless of the underlying implementation. That's part of why it's so bleedingly slow for small matrices compared to even vanilla Python lists (they tested bigger matrices too, so this isn't the only effect, but I'll mention it anyway). It's hard to make something faster when you add a few thousand cycles of pure overhead. - This was a very principled approach to saturating the relevant caches. It's \"obvious\" in some sense, but clear engineering improvements are worth highlighting in discussions like this, in the sense that OpenBLAS, even with many man-hours, likely hasn't thought of everything. And so on. Anyone can rattle off differences. A proper explanation requires an actual deep-dive into both chunks of code. reply robxorb 11 hours agorootparentTo your third point - it looks as if the lines of mean values were averaged, this posts code would still be a clear winner. reply sbstp 15 hours agoparentprevMaybe -march=native gives it an edge as it compiles for this exact CPU model whereas numpy is compiled for a more generic (older) x86-64. -march=native would probably get v4 on a Ryzen CPU where numpy is probably targeting v1 or v2. https://en.wikipedia.org/wiki/X86-64#Microarchitecture_level... reply stingraycharles 14 hours agorootparentDoesn’t numpy have runtime SIMD dispatching and whatnot based on CPU flags? E.g. https://github.com/numpy/numpy/blob/main/numpy/_core/src/com... reply KeplerBoy 11 hours agorootparentnp.matmul just uses whatever blas library your NumPy distribution was configured for/shipped with. Could be MKL (i believe the conda version comes with it) but it could also be an ancient version of OpenBLAS you already had installed. So yeah, being faster than np.matmul probably just means your NumPy is not installed optimally. reply ipsum2 10 hours agoparentprevComparison with numpy 2.0 should be better for numpy because it integrates Google highway for better simd across different microarchitectures. reply david-gpu 8 hours agoprevThere is no reason to burden one side with Python while the other side is C, when they could have just as easily perform an apples-to-apples comparison where both sides are written in C, one calling a BLAS library while the other calls this other implementation. reply moomin 8 hours agoparentPython is the right thing to compare to here, because it is easily the most popular way to perform these computations in the modern day. Specifically using numpy. The overhead isn't that high, but as mentioned elsewhere in this thread, calling it correctly is important. Pitting naive numpy code against tuned C code is definitely not a fair comparison. reply stinos 7 hours agorootparent> Python is the right thing to compare to here, because it is easily the most popular way to perform these computations in the modern day. Specifically using numpy. By that reasoning, wouldn't it make more sense to wrap their C code and maybe even make it operate on numpy's array representation, so it can be called from Python? reply moomin 6 hours agorootparentI think it’s okay to say “This is the benchmark, now I’m going to compare it against something else.” It’s up to the reader to decide if a 3% (or 300%) improvement is worth the investment if it involves learning a whole other language. reply pmeira 6 hours agorootparentIt's a muddy comparison given that NumPy is commonly used with other BLAS implementations, which the author even lists, but doesn't properly address. Anaconda defaults to Intel oneAPI MKL, for example, and that's a widely used distribution. Not that I think MKL would do great on AMD hardware, BLIS is probably a better alternative. The author also says \"(...) implementation follows the BLIS design\", but then proceeds to compare *only* with OpenBLAS. I'd love to see a more thorough analysis, and using C directly would make it easier to compare multiple BLAS libs. reply david-gpu 6 hours agorootparentprevIf that was the goal, they should have compared NumPy to BLAS. What they did was comparing OpenBLAS wrapped in NumPy with their C code. It is not a reasonable comparison to make. Look, I'm trying to be charitable to the authors, hard as that might be. reply cnity 4 hours agorootparentThere is some reason in this comparison. You might want to answer the question: \"if I pick the common approach to matrix multiplication in the world of Data Science (numpy), how far off is the performance from some potential ideal reference implementation?\" I actually do have that question niggling in the back of my mind when I use something like NumPy. I don't necessarily care exactly _where_ the overhead comes from, I might just be interested whether it's close to ideal or not. reply david-gpu 2 hours agorootparentIf that was your question, you would compare against a number of BLAS libraries, which are already well optimized. What they are doing here is patting themselves on the back after handicapping the competition. Not to mention that they have given themselves the chance to cherry pick the very best hyperparameters for this particular comparison while BLAS is limited to using heuristics to guess which of their kernels will suit this particular combination of hardware and parameters. The authors need to be called out for this contrived comparison. reply lamcny 6 hours agorootparentprevPopular does not mean best. Suppose that this blog post were part of a series that questions the axiom (largely bolstered by academic marketing) that one needs Python to do array computing. Then it is valid to compare C directly to NumPy. It isn't even far fetched. The quality of understanding something after having implemented it in C is far greater than the understanding gained by rearranging PyTorch or NumPY snippets. That said, the Python overhead should not be very high if M=1000, N=1000, K=1000 was used. The article is a bit silent on the array sizes, this is somewhere from the middle of the article. reply CleanRoomClub 5 hours agorootparentPython is popular precisely because non-programmers are able to rearrange snippets and write rudimentary programs without a huge time investment in learning the language or tooling. It’s a very high level language with syntactic sugar that has a lot of data science libraries which can call C code for performance, which makes it great for data scientists. It would be a huge detriment and time sink for these data scientist to take the time to learn to write an equivalent C program if their ultimate goal is to do data science. reply 38 5 minutes agoprev> #define min(x, y) ((x)Important! Please don’t expect peak performance without fine-tuning the hyperparameters, such as the number of threads, kernel and block sizes, unless you are running it on a Ryzen 7700(X). More on this in the tutorial. I think I'll need a TL;DR on what to change all these values to. I have a Ryzen 7950X and as a first test I tried to only change NTHREADS to 32 in benchmark.c, but matmul.c performed worse than NumPy on my machine. So I took a look at the other values present in the benchmark.c, but MC and NC are already calculated via the amount of threads (so these are probably already 'fine-tuned'?), and I couldn't really understand how KC = 1000 fits for the 7700(X) (the author's CPU) and how I'd need to adjust it for the 7950X (with the informations from the article). reply SushiHippie 4 hours agoparentActually, leaving it on 16 threads performs a bit better than setting it to 32 threads. 16 threads: https://0x0.st/XaDB.png 32 threads: https://0x0.st/XaDM.png But still not as fast as it ran on your 7700(X) and NumPy is 2-3x faster than matmul.c on my PC. I also changed KC to some other values (500: https://0x0.st/XaD9.png, 2000: https://0x0.st/XaDp.png), but it didn't change much performance wise. reply salykova 2 hours agorootparentas we discussed earlier, the code really needs Clang to attain high performance reply SushiHippie 2 hours agorootparentAgreed https://0x0.st/XakD.png ;) reply marmaduke 11 hours agoprevVery nice write up. Those are the kind of matrix sizes that MKL is fairly good at, might be worth a comparison as well? Also, if you were designing for smaller cases, say MNK=16 or 32, how would you approach it differently? I'm implementing neural ODEs and this is one point I've been considering. reply pmeira 5 hours agoparentDon't forget BLIS itself! reply teo_zero 9 hours agoprevDoes it make sense to compare a C executable with an interpreted Python program that calls a compiled library? Is the difference due to the algorithm or the call stack? reply rustybolt 8 hours agoparentYes reply lamcny 6 hours agoparentprevYes, as far as I can tell the array sizes were large enough to make the wrapping overhead negligible. reply jstrong 16 hours agoprevin terms of comparing to numpy, how much overhead would there be from Python (vs. running the numpy C code alone)? reply SJC_Hacker 15 hours agoparentPython can efficiently call C libs if you use ctypes and native pointers, which numpy uses. Of course depends on expected layout. If you want to convert to Python lists its is going to take time. Not sure about Python arrays. reply dgan 11 hours agorootparentI don't recall the link but there was a github repo with comparisons of CFFI implementations in different languages, and from what i remember Python was 'bout 3 orders of magnitude slower than, say, Lua or Ocaml Edit: ha, found it https://news.ycombinator.com/from?site=github.com/dyu reply brnt 14 hours agorootparentprevIf you use numpy, then you use an ndarray, which you can create from a C array for 'free' (no copying, just set a pointer). reply KerrAvon 16 hours agoprevThe article claims this is portable C. Given the use of intel intrinsics, what happens if you try to compile it for ARM64? reply tempay 16 hours agoparentI think they mean portable between modern x86 CPUs as opposed to truly portable. reply rurban 11 hours agoparentprevYou'd need first to convert it to portable SIMD intrinsics. There are several libraries. reply pmeira 19 minutes agorootparentApparently it also needs Clang to achieve the same performance: https://news.ycombinator.com/item?id=40875968 reply le-mark 17 hours agoprev [–] > This is my first time writing a blog post. If you enjoy it, please subscribe and share it! Great job! Self publishing things like this were a hallmark of the early internet I for one sorely miss. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A high-performance matrix multiplication implementation in C, following the BLIS design, outperforms NumPy (OpenBLAS) on an AMD Ryzen 7700, achieving over 1 TFLOPS.",
      "The code is simple, portable, and scalable, using only 3 lines of OpenMP directives for parallelization, and targets Intel Core and AMD Zen CPUs with FMA3 and AVX instructions.",
      "The implementation demonstrates that efficient matrix multiplication can be achieved in C without deep assembly or Fortran code, with performance comparable to established BLAS libraries when fine-tuned for specific hardware."
    ],
    "commentSummary": [
      "A blog post demonstrates outperforming NumPy matrix multiplication using 150 lines of C code, focusing on performance enhancements.",
      "Key improvements include algorithm selection, minimizing kernel round trips, vectorization, cache efficiency, and hardware-specific optimizations.",
      "Discussions in the comments address the fairness of comparing C code to NumPy, suggesting comparisons with other BLAS (Basic Linear Algebra Subprograms) libraries and emphasizing the need for thorough benchmarking and hyperparameter tuning for specific CPUs."
    ],
    "points": 298,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1720043575
  },
  {
    "id": 40870280,
    "title": "The joy of reading books you don't understand",
    "originLink": "https://reactormag.com/the-joy-of-reading-books-you-dont-entirely-understand/",
    "originBody": "Home / The Joy of Reading Books You Don’t Entirely Understand 24 Share Column Mark as Read The Joy of Reading Books You Don’t Entirely Understand It really should be acceptable and normal to say “I don’t entirely understand what I just read, but I loved it.” By Molly TempletonPublished on June 27, 2024 Woman Reading in the Studio by Jean-Baptiste-Camille Corot (c. 1868) 24 Share Woman Reading in the Studio by Jean-Baptiste-Camille Corot (c. 1868) At present, I have an alarming number of tabs open. I’m absolutely not going to tell you how many, or how many are open on my phone. There are 15 pages of notes in my now-finished notebook that are about the same subject that led to all these tabs. A lot of these tabs concern the history of a country I don’t live in. Some are mythology. It’s a real cornucopia of delights, and it’s also very distracting. There are so many rich and fascinating rabbit holes a person might fall down. This is all because I’ve been reading a book that I don’t entirely understand, and frankly, it’s wonderful. A very long time ago, I read Neal Stephenson’s Baroque Cycle, one at a time, as the books came out. I am—I cannot stress this enough—very bad at remembering historical details. Part of this I blame on high school. Part of this is just the way my brain works. I can tell you the basic plot of most books I’ve ever read, but I cannot tell you the names and dates involved with specific moments in the world’s past. While I read Stephenson’s sprawling series, I spent a lot of time referencing the encyclopedia, because I did not know, necessarily, which characters were based on real humans and which were entirely made up. It was really quite educational. (I also learned about kidney stones, which was less pleasant. But still kind of interesting.) I could have just let it go, let the books roll me along in blissful ignorance. I understood the story structure and the characters just fine. I knew what he was getting at. It was just all that history that kept throwing me: Who? When? Why? But what happened, as I looked up names and places and dates and wars, is that I began to take almost as much joy in that process as I did in reading the books. The two things remain twined in my head, all these years later, and maybe some part of me is always looking for something else like that—something that will offer me a book, a story to read and inhabit, but also an adventure in not-knowing. In recent years, I feel like it has been less common to find books to challenge me, and by me I mean their readers, and by “books” what I really mean is “publishing,” which can feel very focused on the sure thing, the brand name, the splashy debut that somehow speaks to millions and millions of people. Still, there are challenging, mystifying, weird-ass books being published all the time. To be fair, a weird-ass, mystifying, challenging book isn’t inherently a good book, or a book you want to spend your finite reading time on. We only get to read so many books in a month, or a year, or a life. There is value in escapism and familiarity and comfort. But I still want to advocate for sometimes, at least sometimes, going out on a limb, out on a genre vacation, or just out into the wilds of a tale you don’t feel like you entirely understand. It can feel, too often, like these books bobble and vanish in the big world of Book Discourse. I have searched weird corners of the internet for people talking about Alaya Dawn Johnson’s The Library of Broken Worlds, which requires patience, and a willingness to trust her incredible, vivid, dizzying worldbuilding. I think sometimes about how many books there are that American, English-language readers will never get to see, simply because they were too something to get translated here. I think about how lucky we are that Riverhead keeps publishing the great and unmatched Helen Oyeyemi, whose books are works of art that I can’t ever quite fit my head around—which is as it should be, for there is always something else to find in them. I think about how lucky we are that we get to read trippy and furious books like Molly McGhee’s Jonathan Abernathy You Are Kind, which is both deceptively easy to read and hard to fully fathom. Or perhaps what’s “hard” about it is that it’s hard to accept exactly how clearly it speaks to this moment in time. McGhee’s Twitter bio used to say something about how literary and genre fiction ought to touch tongues more often, and I think about that, too: About the science fiction and fantasy that appears in the other section of bookstore, about all the SFF writers overlooked by the mainstream even as their prose is crystalline, elegant, looping, rich, just stunning. We build so many walls for ourselves about what we do and don’t do, read and don’t read. Some of it is simply practical: We’re back to the question of time, and how much of it we do or don’t have. When someone says “I am only reading X kinds of books,” they are drawing boundaries around their time as much as their taste. I want, though, for us to have the time, the space, the mental bandwidth to welcome uncertainty, to crank up our curiosity and give the weird or confusing or just slightly unexpected books a chance. And I want it to be totally okay and acceptable and normal to say “I don’t entirely understand what I just read, but I loved it.” When I started writing reviews, in the mid-2000s, there was a real pressure to be authoritative. To speak with your whole chest, even if you didn’t really know what you were on about. I’ve always been a little suspicious of this tendency—of an unwillingness to be transparent about the fact that every reader (and writer!) is coming from their own specific background and none of us knows everything about everything. Subjectivity is inevitable. Maybe, just maybe, this requirement that we all pretend to know what we’re talking about at all times is a limiting thing. On today’s bookternet, a lot of us can go off about tropes and western story structure and the hero’s journey and probably also several other kinds of story structure we read about once or twice and maybe even there’s some of that Save the Cat guy baked in there, too. So it’s easy, in a way, to keep reading books from this sort of narrative tradition, because we know a bit of what we’re talking about. I can pick up a retelling of a Greek myth and know the basic beats because I grew up steeped in those stories. But there are so many other stories, and so many other ways to tell them. What set me off on this path of delirious not-knowing is that I read Vajra Chandrasekera’s Rakesfall. I read it on a plane, and I felt, later, like I dreamed it. Whole scenes existed in my mind stripped of any context, the way you might remember dreams. And then I read it again, with a pen and a notebook and my phone and laptop at hand. I opened a million tabs, and revisited the general outline of the Ramayana, which I know as a Penguin Classic I read in book group some years back, not at all the way I know the stories and myths I met in textbooks as a child. I put off drafting a review of the book in favor of reading every interview with the author I could find. I put pieces together and, outside of my airplane dream-state, began to see where the story restarted, where it looped, where it ate its own tail and then birthed itself again. There is so much I don’t entirely understand in this book, because I can’t; I’m a white American who does not have the cultural context to fully understand all the things that this story encompasses. And what I’m saying is: Good. Good, let me bask in that. Good, let me admit to that. There is real joy to be found in not immediately understanding exactly what a book is doing. Joy in seeing that something outside of the narrative structure we’re familiar with is at play; joy in discovering a different sense of vastness and fluidity. Joy in waiting, patiently, with rich anticipation, for the seemingly disparate pieces of a narrative to mesh, to become something huge and beautiful. Joy in realizing, several chapters into a book, that you could not possibly say what it was “about” until reading to the end, and maybe not even then. About the Author Molly Templeton Author Molly Templeton has been a bookseller, an alt-weekly editor, and assistant managing editor of Tor.com, among other things. She now lives and writes in Oregon, and spends as much time as possible in the woods. Learn More About Molly See All Posts About book culture Mark as Read reading reading habits Subscribe Connect with Login Notify of new follow-up comments new replies to my comments {} [+] Name* Email* Website 24 Comments Oldest Newest Most Voted Inline Feedbacks View all comments",
    "commentLink": "https://news.ycombinator.com/item?id=40870280",
    "commentBody": "The joy of reading books you don't understand (reactormag.com)298 points by speckx 21 hours agohidepastfavorite185 comments joaorico 5 hours agoKafka [1] on which types of book to read: \"I believe one should only read those books which bite and sting. If the book we are reading does not wake us up with a blow to the head, then why read the book? To make us happy, as you write? My God, we would be just as happy if we had no books, and those books that make us happy, we could write ourselves if necessary. But we need the books that affect us like a disaster, that hurts us deeply, like the death of someone we loved more than ourselves, like if we were being driven into forests, away from all people, like a suicide, a book must be the axe for the frozen sea inside us.\" [2] [1] Brief an Oskar Pollak, 27. Januar 1904. , https://homepage.univie.ac.at/werner.haas/1904/br04-003.htm [2] Literal translation by ChatGPT. Original: \"Ich glaube, man sollte überhaupt nur solche Bücher lesen, die einen beißen und stechen. Wenn das Buch, das wir lesen, uns nicht mit einem Faustschlag auf den Schädel weckt, wozu lesen wir dann das Buch? Damit es uns glücklich macht, wie Du schreibst? Mein Gott, glücklich wären wir eben auch, wenn wir keine Bücher hätten, und solche Bücher, die uns glücklich machen, könnten wir zur Not selber schreiben. Wir brauchen aber die Bücher, die auf uns wirken wie ein Unglück, das uns sehr schmerzt, wie der Tod eines, den wir lieber hatten als uns, wie wenn wir in Wälder vorstoßen würden, von allen Menschen weg, wie ein Selbstmord, ein Buch muß die Axt sein für das gefrorene Meer in uns.\" reply techostritch 1 hour agoparentI don’t know if I’m taking Kafka too literally here, but the books that I read that bite and sting probably fall into two categories. Things that are cynically written in bad faith and things that are hopeless and callous. Torture porn bites and stings, reading hacky partisan politics bites and stings. Anything that makes me feel stupider after reading it bites and stings. The things that I think that he wants to say, the inconvenient truths, the things that make me see the world in a whole new way, that challenge everything I believe in. Those things fill me with joy and wonder they are just so few and far between. Maybe the thing he’s getting at is the existential dread? The truth that nothing you do is meaningful? The staring into the abyss? In which case maybe in moderation, but I fundamentally disagree. in a sense I wonder, if this is what he means, what a weird way to view life, that those things that challenge you are negative. reply borroka 1 hour agoparentprev\"If the book we are reading does not wake us up with a blow to the head, then why read the book?\" -- That's the authorial feeling of self-importance making itself visible. Why read the book? Because it might be enjoyable, a pastime, something that makes us dream, reflect, cry, or connect some dots in our lives through a parallel representation of feelings or ideas. There are many reasons, and the \"blow to the head\" will not and should not be the main reason, especially for older people who have seen some water flowing under the bridge and see the shock factor as artfully constructed and therefore much less provocative than the author intended it to be. reply jyunwai 20 hours agoprevA useful habit that I've begun to follow with more complicated books—especially when reading them out of personal interest—is to actively avoid taking notes or worrying about background material on a first read. I've recently read and greatly enjoyed a historical fiction novel called \"Augustus\" written by John Edward Williams and published in 1972. On the surface level, it's about the events of the life of Augustus Caesar (better known in the book as \"Octavian\")—but on a deeper level, it's about the rarity of longtime friends in life, and dealing with aging and one's mortality. I put the novel off for a year because I thought I had to read a non-fiction historical account of Augustus's life first, as I thought I couldn't appreciate the novel without doing so, due to the unfamiliar character names and events. But one day, I just decided to try it out—and I found myself naturally remembering the character names and events without special care in reading the novel. Similar experiences have been reported by people engaging with various forms of media. I've seen readers take copious notes on the novel \"Infinite Jest,\" which has a reputation for being a difficult read, only to burn out. In contrast, readers who have finished the novel said that they didn't need to take notes, and that the story began to make sense simply by reading more. I've also seen a similar pattern from subjects as academic mathematics, where some learners spend too much time on textbook explanations instead of working on the textbook problems, to subjects as relaxed as computer role-playing games, in which some players end up dropping these games due to a perceived need to take notes to understand the story, before they can get immersed in the game's world. I think a lot more understanding and enjoyment of various subjects can be attained by being comfortable with confusion for a while. While note-taking has its place in understanding a subject, I've personally found that immersion is the most important factor for understanding. reply k2enemy 26 minutes agoparentIf you enjoyed Williams's Augustus, do give Stoner and Butcher's Crossing a read. I \"enjoyed\" them even more than Augustus. Enjoyed is in quotes because they are both emotionally devastating -- Stoner more so than Butcher's Crossing. I didn't feel like myself for a week after reading Stoner and a decade later I still often think about it. reply senkora 19 hours agoparentprevI finished Infinite Jest without taking notes. I definitely missed a lot of stuff but I loved the experience and it ended up being one of my favorite books. I think Infinite Jest is a great example for this sort of thing because I later realized that I had completely missed the entire main plot. By the author: > There is an ending as far as I’m concerned. Certain kind of parallel lines are supposed to start converging in such a way that an “end” can be projected by the reader somewhere beyond the right frame. If no such convergence or projection occurred to you, then the book’s failed for you. Nothing converged for me at all and yet I thoroughly enjoyed the book. I’m still not quite sure what to think of that. Aaron Swartz (yep, that Aaron Swartz) wrote a great essay that explains the ending and main plot in clear language: http://www.aaronsw.com/weblog/ijend But I don’t think I got any part of that plot by reading the book. It’s all hidden and disjointed, and there’s so much interesting stuff at the surface that you almost don’t even care to go deeper. reply spondylosaurus 14 hours agorootparentIf you ever get the urge to read Infinite Jest again (which I highly recommend—a second read is easily more enjoyable than the first), the Infinite Jest Wiki includes some page-by-page annotations that are nice to have on hand. https://infinitejest.wallacewiki.com/david-foster-wallace/in... Probably overkill to look up every little thing (and most of the annotations are just defining SAT-worthy words anyway), but I liked having it around when a random word/phrase would make no sense and it turned out to be a vintage shaving cream brand or some bit of Boston-ese. And it's free of spoilers, so friendly enough to first-time readers, but I do think a first read is best with no notes or supporting material or anything. Other than two bookmarks, lol. reply brookst 1 hour agorootparent+1 for re-reading. I’d also suggest the audiobook as an alternate form that is differently accessible. Certainly it’s easier to follow some of the changing perspectives as the narrator does a good job of voicing differently. reply constantinum 15 hours agorootparentprevThe best thing about reading(and finishing) Infinite Jest is that you are not sure. Not sure if the book has ended, not sure about anything. I've read and listened to multiple interpretation of the book. But that is what makes it a different experience(because of varying perspectives) I wrote a small blog on how I did read Infinite Jest > https://www.prasannakumarr.in/journal/reading-infinite-jest reply sonorous_sub 19 hours agorootparentprevI like the short form stuff DFW wrote for Harper's Magazine. The one about his trip to the state fair with an old flame is sublime. reply jihadjihad 1 hour agorootparentTicket To The Fair (1993) [0] -- a superb read, indeed. 0: https://harpers.org/wp-content/uploads/HarpersMagazine-1994-... reply maroonblazer 18 hours agorootparentprevAlso, his documenting his 7-day Caribbean cruise, aptly titled (IMO) \"A Supposedly Fun Thing I'll Never Do Again\" reply senkora 17 hours agorootparentprevAgreed. His short stuff is excellent. I'll also call out his commencement speech on the meaning of a liberal arts education, \"This is Water\". https://fs.blog/david-foster-wallace-this-is-water/ reply ravi_m 13 hours agorootparentprevInfinite Jest seems excessively long and I haven't worked up the motivation to read it yet, but his short stories / essays in Consider the Lobster are excellent, including the titular story which is about a lobster festival in Maine. And looking at the comments in this thread, seems like he had some kind of fascination for fairs and other touristy things. reply ofcourseyoudo 18 hours agorootparentprevAlso the one about a cruise trip, Michael Joyce, and Lost Highway... essentially everything from \"A Supposedly Fun Thing I'll Never Do Again\" reply bondarchuk 1 hour agorootparentprevHoly shit. Thanks for that link. reply dr_kiszonka 18 hours agoparentprevI have no research to back this up, but I think the need to understand everything may result from low self-esteem. Specifically, when not knowing something, people with low self-esteem may feel stupid. To eliminate this feeling, they† focus on learning. It is a good adaptive mechanism, especially compared to maladaptive ones like avoidance behaviors. A potentially better one is learning not to derive self-worth from how much we know or how others perceive us. † Some of them, not everyone, on average, etc. Also, different people have different motivations. Not everyone who has a curious mind has low self-esteem. People are complex. reply brokenmachine 17 hours agorootparentBut it's kind of high self esteem to think that you're actually capable of understanding everything. Low self esteem would assume they're not capable of understanding and just give up. reply dr_kiszonka 12 hours agorootparentI think this might be more closely related to self-efficacy: https://en.wikipedia.org/wiki/Self-efficacy reply cal85 12 hours agorootparentprevPeople are complicated. You can have a high view of some aspects of yourself and a low view of other aspects. reply dyauspitr 16 hours agorootparentprevIt’s absolutely acceptable to build some of your self worth on what you have worked to learn. It’s a beautiful feedback loop. reply PaulRobinson 19 hours agoparentprevThis made me think of Umberto Eco’s Foucault’s Pendulum, where you find yourself thinking “I need to look some of this stuff up, it’s becoming hard to know if I understand it all”, but that is part of the satirical commentary he wanted to make - it’s very meta, very good, not knowing all of the esoteric references is the exact point. reply brookst 1 hour agorootparentFoucault’s is amazing. It’s a great story, but it also delivers a visceral experience that really mirrors what the characters are feeling. One of the best “medium is the message” books. reply 2143 15 hours agoparentprevI see so many comments about taking notes while reading. I didn't even know that was a thing. I'm not even sure if I would want to do it, because it would interrupt the reading. My own personal belief (which I came up with just now) is that reading novels should be a smooth relatively easy affair. Because I read simply for the fun of it. This may not be the case with academic books however. I just, start reading. I have in fact stalled on books before though off the top of my head only SICP and Anna Karenina come to mind. I'll reattempt both of them in the near future. Stalling on SICP was probably due to me not having the sufficient math background, which I'm slowly working on fixing. The post you wrote gives me hope. There's a possibility that I've been doing things the wrong way all these years. reply skydhash 6 hours agorootparentFirst read, I don’t take notes unless I’m familiar with the material. At most, I’ll mark interesting passages. But I usually pause after each or every two chapters, reflecting on the concepts. I don’t take notes with fiction books, but I pause whenever I can’t give it my full attention (interruptions, some other tasks, tired). reply troad 15 hours agorootparentprevI think taking notes while reading fiction would be relatively unusual (outside fields like literary criticism), but taking notes while reading non-fiction is quite common, especially when grappling with denser material. For example, I kept extensive notes while reading Bertrand Russell's History of Western Philosophy. The work assumes you're internalising as you go along, which is somewhat inescapable given the nature of the material. The author can't stop to re-explain some finer point of Aristotle's every time it is engaged with in the subsequent two thousand years. Pausing to take notes helps one reflect on the material and solidify their understanding, but also gives them a quick reference later if necessary. I just use my phone's Notes app, to keep the barrier as low as possible. reply scubbo 19 hours agoparentprevFascinating. My first response to your opening paragraph was horror - how on Earth could you hope to really internalize and learn from a textbook without taking notes on it? - before realizing that you were (mostly) referring to fiction or entertainment media. In which case, yes, I wholeheartedly agree with you - don't do anything to pull yourself out of the story, remain immersed and (if it's a well-structured work) it will start to make sense to you. I did take notes throughout my first playthrough of Elden Ring, for instance, and started enjoying it a lot more once I stopped! reply brianush1 12 hours agorootparentit works with textbooks too though reply autoexec 18 hours agoparentprev> A useful habit that I've begun to follow with more complicated books—especially when reading them out of personal interest—is to actively avoid taking notes or worrying about background material on a first read. I recommend using those little sticky tabs instead. If I come across something I want to look up later, or want to come back to for whatever reason I use one on the page itself to to highlight the line, and another at the top so I can find the page again. By the time I'm done reading it might be full of those little tabs but it doesn't really slow me down in the moment. reply A4ET8a8uTh0 16 hours agoparentprevI will admit that there is some level of joy in finding previously unnoticed angle or joke on re-read. Every few years or so I find one such gem in Pratchett's books. It does make me smile. I don't think I can emjoy Infinite Jest or Ulysses that way. For non-fiction, I will admit that it is hard for me to take that advice. I am currently going through a historical analysis book, which in itself covers a complicated topic and references tons of source materials, which now I feel almost obligated to add to my reading list. And for harder subjects, it feels like I get lost on the foundational materials if I don't take notes. reply emmanone 12 hours agoprevI’ve recently moved to Europe and found myself surrounded by hundreds of famous galleries, which are essentially the main entertainment here. I started visiting them and looking at classical paintings, little by little googling what it was and why. It turned out to be so exciting! Now, a year later, I can say for sure which of the women with a severed male head in their hands in the painting is Judith and which is Salome. And I understand much better how people lived in these parts before, and why they live the way they do now. Therefore, I completely agree with the author of the article - sometimes you need to plunge into the unknown, and this unknown will reward you. I’m afraid to imagine how many discoveries await me in museums of contemporary art. reply ogou 10 hours agoparentAs an artist and technologist living in Europe, I am glad to see a comment like this. It's refreshing. An open-minded and incremental approach to culture can be incredibly rewarding. https://berlinartgalleries.de/ reply bigthymer 6 hours agoparentprevI would read a blog post about this friend. reply RandomWorker 7 hours agoprevI had a huge complex in my youth , I simply couldn’t read as fast as my peers. Now, I realize that I was going too fast, and by slowing down, taking my time and reading slowly I could absorb more, and understand, and I had this amazing ability to never forget anything I did read (at least for an extended period of maybe 2-3 years). I realized over time that going fast isn’t for me. Better to go slow absorb, digest and ultimately retain more would get me where I needed to be. Never did well in school in terms of grades but ultimately I got better and better doing a masters and actually got sponsored to do my PhD. Many years I read but could not understand, but ultimately it was the joy of reading slow that got me further than the joy of reading and not understanding. reply skydhash 6 hours agoparentI do this for my media consumption. I take breaks, never trying to finish in one go. I also pause intentionally when pause occurs (chapters in non-fiction books, series episodes. And I don’t mind revisiting the material, especially if it was good. As for music, I treat it like a soundtrack, focused albums (and a few playlist) listening, falling back to silence when my attention is needed on some tasks. reply jdswain 16 hours agoprevThe article title reminded me of when I was young and used to read Byte Magazine. Byte used to cover a wide range of topics, and could get quite technical, but the big thing that is vastly different to today is that you would get a monthly digest of articles that were selected by the editors, not by yourself. And I used to read it cover to cover. There was a lot I didn't understand, but also I feel like I gained a wider knowledge than if I only read what I was interested in, and many times the ideas that I was exposed to turned out to be useful much later in life. Some of them ended up being distractions too, like playing with hardware, or writing a compiler, but it was all very interesting. reply Blackstrat 6 hours agoparentByte magazine was a terrific publication. There's nothing similar in print these days that I'm aware of. Certainly, Byte couldn't be accused of dumbing down the content to reach a wider audience, unlike many of today's supposedly technical magazines. I learned a lot from Byte and experimented frequently with the knowledge and understanding I gained from Byte. reply cubefox 3 hours agoprevThis reminds me of the recent \"Neuromancer\" discussion here on HN: The early cyberpunk writing style, and especially that of William Gibson, made extensive use of unexplained technical terms. The story was occasionally hard to follow. But that was part of \"cyberpunk\", at least initially. If you were really about to read a report from a different possible world, you also wouldn't understand everything. In reality not everything serves some central plot. There are always superfluous details, and (especially for fictional settings) things that are hard to understand for the outsider. I remember reading, a few years ago, Amazon reviews of the 1990 William Gibson/Bruce Sterling novel \"The Difference Engine\". Apparently most people expected a normal novel, just with a \"steampunk\" setting, so naturally they were disappointed and complained about the book being confusing. That's because it's a cyberpunk novel. Which is a literary genre, not merely a setting like steampunk. (The latter term didn't even exist when the book came out.) I remember Stanisław Lem (an SF author well-known outside the English speaking world) said approximate this about historical novels: Historical novels have the advantage of depth, they can reference a world that is much more complex than required for their plot, they can set themselves in the deep complexity of actual history -- whereas fantasy and sci-fi books must always rely on their own made-up world, which almost necessarily looks flat and shallow in comparison, even if it seems spectacular on the surface. I really came to understand this when I read Umberto Eco's \"The Name of the Rose\". All the historical details are so intricate that they are almost impossible to match by a novelist writing about a fantasy world or the far future. This is, perhaps, also why The Lord of the Rings is such a great fantasy story, and why most other fantasy stories fall short in comparison: Tolkien didn't just write a novel. He invented a fictional language first, then an elaborate fictional history around it, and the Lord of the Rings is really just a small part of this story near the end. When reading the book, you constantly read allusions to \"historical details\" about things that happened thousands of years ago in Valinor, Beleriand, Númenor, in certain ancient wars etc. These \"superfluous details\" are occasionally hard to understand (except if you read Tolkien's posthumous \"Silmarillion\", which his son compiled from fragments) but they approximate something like the depth that usually only a historical novel can achieve. reply eigenhombre 19 hours agoprevI do like some \"hard\" fiction like the Stephenson mentioned in TFA, as well as Pynchon and David Foster Wallace, but my mind immediately went to some of the harder technical writing I've enjoyed - The Art of Computer Programming; SICP and other Lisp texts, math books, etc. I once spent a very pleasant short vacation on a beach on Lake Michigan reading Peter Gabriel Bergmann's \"Introduction to the Theory of Relativity,\" finding pleasure in gradually unraveling the notation, the mathematics, and the ideas, in a quiet and beautiful setting. It always surprises me when I meet engineers who don't enjoy reading technical books, but different strokes and all that. It takes a kind of patience and persistence to unravel a technical text, which can be its own reward if you're not trying to solve a specific technical problem at the moment. reply emporas 16 hours agoparentWhen i started reading the Common Lisp Reference Manual i knew neither English nor Lisp. When i finished it reading it for the first time, i learned English better. I read it again 3 times, then i started learning Lisp. reply sillyfluke 17 hours agoparentprevDon't leave us hanging, what happened at the end of the beach on Lake Michigan? Jokes aside, I do the no note taking on the first read thing as well. Because I like reading, I do sometimes skip the problems in technical books the first time round, but I'm consciously aware it's a form of procrastination when I'm doing it. reply __rito__ 9 hours agoparentprev> \"but my mind immediately went to some of the harder technical writing I've enjoyed [...] math books\" What are your favorite Math books, and what texts did you enjoy the most? Could you please share the titles? reply eigenhombre 5 hours agorootparentMy very first was Naive Set Theory by Paul Halmos. Way over my head in 7th(?) grade but my first intro to math beyond pre-algebra stuff. Lately I've enjoyed, but did not finish, the Joy of Abstraction by Eugenia Cheng, on category theory. And there was a differential geometry book whose name I have forgotten but whose exercises I really enjoyed, because I could do them in my head while riding the bus, just by thinking about them. I'm not particularly well read on mathematics (had a lot of math in college, hardly any since) but I would like to circle back to reading more at some point. reply __rito__ 5 hours agorootparentThanks for your reply. The Halmos book is on my to-read list for some months. Will bump it! I also started reading the Cheng book, but I did not finish it either. Let me know the name of the Diff. Geometry book when you remember it. And wish you the best on your plans of circling back. reply monacobolid 10 hours agoprevRelated to \"I don't entirely understand what I just read, but I loved it\" from the article - some time ago (I'd say it's been years now), there was a submission on HN (at least I believe I found it on HN, though I'm not 100% sure) about rules for critiquing art (again, I'm not 100% certain, but this is how I remember it). Unfortunately, I think I didn't finish the whole article, but at the start it said that if you want to critique art, you have to understand that: 1. There is art you love that is also actually good. 2. There is art you don't love but is actually good. 3. There is art you love that is actually bad. 4. There is art you don't love that is also actually bad. If you know which article I'm talking about, please let me know. I've been trying to find it on and off for what seems like years now. reply severine 10 hours agoparentMaybe this? https://salmagundi.skidmore.edu/articles/477-thirteen-ways-o... reply monacobolid 10 hours agorootparentUnfortunately no. I think that one I have in mind is more authoritative, almost a guide. reply shaggie76 17 hours agoprevI cannot remember the books I've read any more than the meals I have eaten; even so, they have made me. ― Ralph Waldo Emerson reply alberto_ol 12 hours agoparentIt is not certain that the quote is from Ralph Waldo Emerson. https://quoteinvestigator.com/2016/06/20/books/ reply borroka 1 hour agorootparentBut also, who knows if it is true? And why only books and not everything we have done in life? And why should Ralph Waldo Emerson, assuming the quote is his, know more about this than anyone else? The hours of history classes in elementary, middle, and high school, when we discussed the Roman Republic and the Empire and before that the Egyptians and the Assyrians and memorized the names, perhaps formed and made us, even if we do not remember a single date, only the names of Cleopatra and Caesar, and we could not find the location of Carthage even if our lives depended on it. Or maybe they did nothing to most of us, which is the more parsimonious view. When I was a child, a whole debate emerged about the risk of developing a violent personality after watching movies and reading comic books in which violence and gore were shown quite freely. As far as I know, this development of a dangerous, antisocial personality never happened, because, dare I say it, we can distinguish fact from fiction, and everything we ingest, food or media, is modulated by our history, family, genetics, culture, and friends and enemies. reply the__alchemist 5 hours agoprevI think there is a limit. If it's a topic you can look things up about (Maybe something technical where you haven't read the prerequisites.). The initial example from the article is interesting, in that you can learn so much about history with the looked-up context, but you can still follow and enjoy the books without it - you will just not know which characters and events were real! I think you will probably remember the history better this way with a fun story-context than wrote memorization, which I believe is a point of the author. Some material, I feel like I am too stupid for, or my brain is wired so differently from the author I will never make sense of it. Examples: Gravity's Rainbow, and parts 2 + 3 of The Divine Comedy. (Granted, the latter is full of parts where looking up contexts and references will help, but I am not sure what to do with the former; there are rare sections where I can gain a purchase on events transiently, but it mostly passes through without absorption for reasons I don't understand). reply beezlebroxxxxxx 4 hours agoparentGravity's Rainbow is downright abstract at times, but if you use a guide like this [0] then you can move beyond trying to figure out what actually happens and really enjoy how Pynchon twists language and sentences into incredible images and scenes. Some of it is, for lack of a better term, downright fantastical rather than literal. [0]: https://people.math.harvard.edu/%7Ectm/links/culture/rainbow... reply EGKW 4 hours agoprevI get the point. Only a few days ago I watched the restored version of \"Jeanne Dielman,...\", to its full length of 3 hours and 20 minutes. Nothing happens in that movie, absolutely nothing at all, except for the registration of a housewife's daily routine and a few conversations with her son. Until the last quarter of an hour. You start with boredom, wanting to stop and forget all about it. But then curiosity kicks in, and you learn to appreciate the innumerable small details. reply cubefox 2 hours agoparentThere are also books and movies which don't even have plot, or where the plot isn't very important, where the journey is the destination. An example is the film Amarcord (1973). reply nextstepguy 17 hours agoprevI started reading the original edition of Don Quijote in Spanish with two years of high school Spanish under my belt. Ten years later, I finally finished the first book. reply thunkle 17 hours agoprevI spent so much time on \"Road to Reality\". I was mostly confused, but then every once in a while something would click and it was mind blowing. Now I'm going back through linear algebra. I'm also looking at the hardest book I've tried \"Moonshine beyond the monster\" I'm trying... reply voisin 17 hours agoprevI’ve recently started reading The Iliad. I find it challenging because characters can be referred to by a variety of different things, even within the same paragraph or two, so it is challenging to follow the conversation or who is being discussed. I’ve taken to asking ChatGPT to summarize chapters and key characters within the chapter after I’ve finished each chapter and it helps give me feedback as to whether what I thought happened was what indeed happened. It’s also given me little contextual tidbits that are helpful and apparently would have been known to audiences of the time but for me would have gone unappreciated. It’s helpful, though I think I’d prefer an annotated copy over ChatGPT so I have realtime information as I read without the lag of finishing a chapter first (or added friction of stopping to search and starting again) reply egl2021 17 hours agoparentI found Malcolm Wilcock, A companion to the Iliad, and Ralph Hexter, A guide to the Odyssey, helpful when I read Homer recently. reply voisin 6 hours agorootparentThank you. Putting my thoughts into my comment made me wonder a bit more about the translation I am reading which is Butler’s translation included in the Great Books of Western Culture. It apparently is a somewhat weak translation when compared to modern ones and so I might switch to something modern to see if that helps. reply beezlebroxxxxxx 4 hours agorootparentWith Homer, which translation you read can make a huge difference for the reading experience. Older translations tend to be far more purple and ornate, while recent translations, like Emily Wilson's, are far more straight forward with a more restrained diction and helpful translation notes and introductions. It's all really a matter of degree, though. reply phendrenad2 3 hours agoprevThis is how I felt listening to the audiobook of Infinite Jest over the course of many months. Who was that person again? What's going on? Doesn't matter, it's something to listen to. reply milleramp 14 hours agoprevI read the Baroque Cycle almost 20 years ago and have to say I enjoyed every bit of it, the relatable characters, the circle of life and the science was amazing. I am sure there were parts that went completely over my head but it felt good to sit down open the large books and dive in. Thanks to the poster, it's about time I re-read the series. reply world2vec 9 hours agoprevI'm halfway through Neal Stephenson’s \"Baroque Cycle\" and it's absolutely delightful but it sure requires frequent dictionary/Wikipedia consultation, at least for me. reply dclowd9901 10 hours agoprevI know he’s well regarded on this site but I’ll espouse my own experience with Cormac McCarthy books. Blood Meridian is nearly impossible to get through without some kind of version of a “urban dictionary for the old west” at hand, but the lurid language draws you in constantly. The beauty of language, I think, lies in the absolute specificity of a word. One that could only exist at a certain point in time, and his books are filled to the brim with language like that. Is it dense? Yeah absolutely, but it makes your arm hairs tingle, some of the writing he employs. reply aeturnum 19 hours agoprevI feel like the idea of understanding media has, for many of us, become a prison. The purest version of understanding is kind of a personal relationship to a piece of media. A relationship you form while engaging with it that enlivens your life and has the potential to broaden your horizons. But we live in a moment where it's very popular to talk about \"the right understanding of media\"[1] and therefor everyone begins to need to explain their relationship to every piece of media to their friends. The bare experience of reading The Baroque Cycle completely stuffed full of historical references you don't understand is kind of its own immersive experience in a less media-rich climate. You kind of get a sense what it might be like to have no access to education and run into like, Leonardo da Vinci or whoever. But then it comes time to explain that experience to someone else and they might think you were silly for not just looking the names up. I just think it's too bad. I once almost broke my wrist snow boarding, but my friend wanted to finish the day so I hung out in our car. The medics had given me a dose of percocet[2] for the pain and I had just started Neuromancer. Finishing that book in that hot car, slightly high, has both erased all of \"what happens\" from my mind and left me with this kind of indelliable feeling of what it was like to be reading the book. I didn't understand it and feel all the better for it. [1] I think it's very easy to understand why people want to set others straight on points like this, even if I don't like the ecosystem it creates. [2] I think it was percocet? Though it seems odd that I would be given a dose of narcotics for a bad sprain. reply banish-m4 11 hours agoprevIf works do not test you or bring new ideas, then what is the point of reading them in the first place? Uncomfortable nonfiction is like eating your vegetables. There is much disquieting history and knowledge that must not be ignored. Mainstream mass public education will not teach curiosity or imbue anyone with ambition or initiative, it is something one must cultivate on their own. reply walterbell 17 hours agoprevSome books \"you don't understand\" can change the reader, so the (new) reader experiences a (new) book in their next reading. R.A. Lafferty, from “Selenium Ghosts of the Eighteen Seventies” (1978), an alternate history of television, https://www.wired.com/story/who-is-r-a-lafferty-best-sci-fi-... There seemed to be several meetings in this room superimposed on one another, and they cannot be sorted out. To sort them out would have been to destroy their effect, however, for they achieved syntheses of their several aspects and became the true meeting that never really took place but which contained all the other meetings in one theatrical unity. > ..On first read, yes, it’s nonsense, but this is the experience of experiencing Lafferty. He doesn’t make any sense, until you decide, and you must decide, that he does. Then, suddenly, he becomes a genius. Read the paragraph again. What’s he talking about? Today, you might realize he’s predicting Zoom: a main meeting full of individual nonmeetings taking place in chats and side slacks that together constitute a constant and overarching supermeeting! Tomorrow, it’ll sound like something else entirely. reply j7ake 14 hours agoprevA appropriate difficulty level is where you understand enough of the book to enjoy it, but that there are parts that are just beyond your reach so you can grow. reply the__alchemist 5 hours agoprevLooking up history for context while reading The Baroque Cycle? That's like looking up spoilers! reply robbiep 20 hours agoprevWhen I was 11 I was attracted to the cover of a book that showed some boats sailing on a terraformed Mars. Reading Blue Mars, the final book in the series, at age 11 totally blew my mind. Not only was I already scientifically inclined, so Sax's explanations of the world and descriptions of materials science really expanded my 'scope of the possible' (even if a bunch of the high tech stuff was hand wavy), this book also contained their constitutional convention which rolls on for a whole bunch of pages about the different government systems and some of the impact these power structures can have. At 11 I had no conception of what an anarchist is or socialist or communist was (The wall had fallen 7 years earlier and China was a flea bite) but it populated my 'potential space' of how all these words and concepts I barely understood were related to each other, which made it a hell of a lot easier later in life to have some grounding in which they had been discussed reply sanex 16 hours agoprevWhen I saw the article I thought of The Baroque Cycle which I finished a year or two ago and am currently working up the courage to tackle it again. Pleasantly surprised that it was the first series mentioned. I'm thinking of trying it this time on Kindle so I can look some things up without leaving the book. reply malux85 19 hours agoprevWhen I was about 14 years old, my parents saw my interest in electronics and computers and went to a university professor they knew and purchased 6-7 books on various topics. (Mostly electrical engineering and some programming) They were designed for 2nd or 3rd year university students, and they were way wayyyy beyond me, but I used to read them, over and over, and slowly parts of them were becoming clearer to me, even the bits I didn’t understand (at all) must have been going into my memory because later when the concepts started to click, then the connections were being made. It took me years, I read the books many times over and over all through my teens. Reading books I don’t understand has become a lifelong joy for me, just yesterday I got my subscription to “Advanced Materials” and I have thousands of articles to read! reply djmips 14 hours agoparentSame but for computers. As a child I got the engineering books for the 6502 from my father ( he was a power engineer).and they were like a foreign language. But I persisted and read them over and over like I was trying to decode an ancient cipher. And like you, eventually they became clearer and my understanding flourished. Such a cool experience. reply malux85 13 hours agorootparentI fondly remember seeing the integral symbol and having no idea what it meant, and no internet to check. I remember thinking to myself “This must be important if it’s in this book” and just memorising without understanding. I still write x^(p/q)==q//(x^p) as my goto graffiti! reply hilux 19 hours agoparentprevBoth you and your parents sound so cool! This brought a smile to my face - thanks for sharing. :-) reply teekert 10 hours agoprevIt's like watching 3Blue1Brown. A look into the soul of the universe causing a sense of awe and wonder, but little understanding. reply __rito__ 9 hours agoparentDepends on one's background, really. I understood some videos really well on the first watching, but some videos on the same technical level were like- \"what?... Ooh, maybe that's okay... Oh... Yeah\". Total discomfort. It's the areas of Math where you already have decent groundings, you will find that you can take more with you from 3b1b. Same with Feynman's Lectures. If you are a smart person but no formal background in Physics, they are fun, sure. But you read the same lectures as a Physics undegrad in your Junior or Senior year, your 'return' from reading those lectures goes up five-fold. reply ximilian 11 hours agoprevIf we read for the joy of not understanding, why don't we write books that are optimized for sounding interesting and clever but have no real meaning? reply ojbyrne 13 hours agoprevThis reminds me of a (half-remembered) quote from Joe Strummer about reggae songs - the words are so hard to understand that every time you listen to them you understand a little more. reply damontal 17 hours agoprevStarted reading a book by Irish humorist Ross O’Carroll-Kelley. It’s full of Dublin slang specific to the 90’s I think. I don’t understand a lot of it but it’s fascinating to sort of listen in on the patois. reply circlefavshape 4 hours agoparentRoss O'Carroll-Kelly is a character! The writer is Paul Howard reply roc856 16 hours agoprevThe title of this post does not correctly reflect the title of the article. reply jdmoreira 19 hours agoprevThose that read 'The Book of the New Sun' will know the feeling reply cooolbear 4 hours agoparentExactly what I was thinking. I'm really looking forward to when the second read-through will call to me and what I'll get out of it then reply savanaly 18 hours agoparentprevI love reading books that I don't understand and not understanding them. As long as I know there is something there, which I could either look up what others have pieced together online, or reread carefully myself. Funny thing is looking that stuff up or figuring it out is optional, I still enjoy the read where I'm in the dark enough that sometimes I move on. And look back fondly on the book. Gene Wolfe books are very good for this style of reading. I feel guilty mainly when I run into someone else who says they love the book, and I am totally unable to have a meaningful conversation about it because to be honest I didn't understand or retain much from it. And I end up looking like a poser a lot of the time I'm sure, and maybe I am in some way. But I still read and enjoyed it! reply sophacles 17 hours agorootparentI find that some of the books i don't understand come to me very slowly over time. I'll just have some insight one day and out of the blue I'll think \"oh like that one thing in zen and the art of motorcycle maintenance\", 20 years after i read it (or similar). As for looking like a poser - most people will respond well to \"tbh I didn't really understand what I read, tell me more and I'll keep it in mind if I revisit\" (or \"help me understand this other thing I have some concrete memory about\", etc). Some jerks will scoff or dismiss you, most people I've encountered are pretty open to a good discussion even after it's been revealed. reply Kikawala 19 hours agoparentprevThe unreliable narrator doesn't help either. reply temporallobe 5 hours agoprevThis is how I feel about most HN posts. reply bowsamic 9 hours agoprevThere's a fine line between \"I don't need to understand\" and \"I have no idea what's going on\". At some point it becomes unworkable and you have to give up. reply pessimizer 20 hours agoprevWouldn't it be better just to slow down? They're not books that can't be understood. edit: I've spent years reading some books. Sometimes I stop and realize that the words are just washing over me; I backtrack until I'm at a place that I've understood the path I took to get there. I go forward again, maybe realize that I'm actually missing the background to go farther. If I've been fascinated up to this point, I find another book that will give me the background. I may come back to the original book a month later or ten years later. The other material may obviate the need for the original book altogether, or even give me contempt for the original book. This seems more like somebody who doesn't speak French reading French books, and claiming that imagining the sounds that might be made from the sight of the words in the book leads them to some sort of transcendence. People write to be understood. I read to understand. I'm not just checking off things and trying to come up with a review filled with vague evocative metaphors that I can impress people with at a party. There's an obsession with appearances and presentation rather than actual engagement. Associative dream logic in the place of understanding. Why not just meditate on the cover painting and say you read it? reply sfink 15 hours agoprevThis reminds me of something that I heard once from a Chinese teacher. I can't vouch for the accuracy of it, but he was definitely on to something: In the West, it is assumed that it is the speaker's job to make himself understood to his listeners. In the East, it is the other way around. In recent times, it seems like we've gotten even more extreme. The speaker or writer must not only spoon-feed the understanding, they also have to provide the motivation and the entertainment. Which I find sad, because some things you can't get unless you go to the effort of extracting them yourself. (See many reproducible psychology findings about retention being highly correlated with depth of processing, for example.) It's like the information equivalent of highly processed food. I find myself falling into this trap on sites like this. An interesting but difficult article will be posted, I won't immediately know what to think or where I stand on the topic, and I'll flip to the comments so that I can get some part of the collective to tell me what to think and how to feel about it. Which is also sad. In paintings, it is known that the viewer can get out more than the painter put in. It used to be the same with writing, but it feels like that is becoming more rare and less acceptable. If a reader can't follow the argument, it's automatically the author's fault and a waste of the reader's time. Heaven forbid the reader might need to exert some effort and grow in the gleaning. reply jjmarr 15 hours agoparentI read this comment before the post, and now I feel bad. I watched a spy movie from the 1960s recently with someone. We got 20 minutes in before she was confused about why the movie is just about a depressed drunk who lost his job in a spy agency, before my movie-watching accomplice looked up the plot of the movie on Wikipedia. Spoiler alert, there's a twist, and the movie didn't tell the viewer that. It's interesting that modern movies have to make you think you understand something, before they pull the curtain back and reveal there's a twist. Otherwise people will get disengaged and stop watching before the twist occurs. reply lqet 9 hours agorootparentI also strongly suspect you watched \"The Spy Who Came in From the Cold\". If you enjoyed this movie and the way it is narrated, please do yourself a favor and watch all the BBC mini series from the 70ies/80ies based on John Le Carré books, namely \"Tinker Tailor Soldier Spy\", \"Smiley's People\", and \"A Perfect Spy\" (or read the books, John Le Carré is an excellent writer, and \"A Perfect Spy\" can be compared to works by Dickens). You usually have no clue what is going on, and only learn about it later. reply marginalia_nu 3 hours agorootparentI think these movies are attempting to put you in the position of a spy, where you need to pay attention and infer motives from actions, and actions from motives. The IPCRESS File is probably my favorite in the genre of cold war spy thrillers. It's slightly more on the fantastical side of the spectrum, but still so good it makes grocery shopping interesting. The camera work is just brilliant, with many shots taken from angles that emulate covert surveillance, yet still managing to beautifully frame the scenes. Since this is implied, but never spoken, some reviewers seem to have missed this aspect, and just though they were shooting scenes through building windows for the sake of it. Even just the opening scene says so much about the main character on without him or anyone else speaking a single word: https://www.youtube.com/watch?v=pBCqP7R42K0 reply noefingway 4 hours agorootparentprevSecond this. IMHO Richard Burton and Alec Guinness give stellar performances in these shows/movies. I would also recommend the Len Deighton series Game, Set, Match with Ian Holm. You need to watch to the end to figure out what's going on. reply Sebb767 6 hours agorootparentprev> It's interesting that modern movies have to make you think you understand something, before they pull the curtain back and reveal there's a twist. Otherwise people will get disengaged and stop watching before the twist occurs. So why should you keep on watching a movie where nothing happens just because, in the end, it _might_ be that there is a twist? I do see the more general point about ever shorter attention spans, but in general, it's probably a good thing that we have enough options to entertain ourselves in order to not having to take these gambles. reply haswell 5 hours agorootparent“Nothing happening” can be as impactful and meaningful as a scene full of action. I personally like to know as little as possible about a movie before I watch it, aside from genre. I want to experience the story as the creators intended, and at times this includes being completely in the dark. The transition from “wtf is going on?” to understanding is where the payoff resides. Every movie you watch is a gamble, even if you read the Wikipedia page first. And it is possible to get a general understanding of the reception of a movie without having to know anything about the plot itself. > it's probably a good thing that we have enough options to entertain ourselves in order to not having to take these gambles Different people watch for different reasons. I personally think it’d be incredibly boring to stop making gambles on potentially interesting movies. reply VariableStar 6 hours agorootparentprev\"It's interesting that modern movies have to make you think you understand something, before they pull the curtain back and reveal there's a twist. Otherwise people will get disengaged and stop watching before the twist occurs.\" I agree with this. For a particularly insidious example see the latest Star Wars series, the Acolyte, by Disney. reply auggierose 5 hours agorootparentWell, I have watched 5 episodes so far, still waiting for the twist. So far I think the Acolyte is pretty dull. My girlfriend checked out after episode 3. Your comment fills me with hope! reply nswest23 4 hours agorootparentSo this is just storytelling 101...you don't have to give up the whole story but it does have to be engaging in the meantime...before the _big reveal_. Five dull episodes is not good storytelling and you're probably going to end up disappointed. reply croisillon 14 hours agorootparentprevSo how’s the movie called? reply tdrgabi 14 hours agorootparentProbably \"The spy who came in from the cold\" reply big_paps 7 hours agorootparentThe book and the movie are quite rough, raw and extradry - i don’t mean this in a bad way. The mood reminds me more of eastern productions like tarkowsky (stalker) and the like. reply ted_bunny 6 hours agorootparent\"Bond for grownups\" reply jhbadger 2 hours agorootparentAlso called \"stale beer\" spy fiction to emphasize its lack of glamour and that settings like dive bars are more common in it than fancy casinos and cocktail parties. https://tvtropes.org/pmwiki/pmwiki.php/Main/SpyFiction reply bowsamic 10 hours agorootparentprevWell the issue is that people panic, since honestly I think we are very insecure about our media literacy reply lqet 9 hours agoparentprevI work in academia, and the pessimistic/cynic standpoint is that university is not about teaching, but about filtering. Making a lecture \"fun\", comprehensible, or even \"innovative\" may not have the desired effect of improving the level of understanding among all students, because a fun and easy course is a worse filter than a hard course. Personally I always learned the most in courses that were very hard and had a nerdy teacher/professor who did not care at all whether you could follow the stuff on the blackboard / in the presentation. Theses courses required work on your own: you had to read the actual literature again and again to even remotely understand the topics on the weekly exercise sheets, or to pass the exam. This \"learning by yourself\" lead to a much deeper understanding than just memorizing some concepts from a streamlined lecture. reply infinitezest 8 hours agorootparentIf you're putting in all of the effort to make the material make sense to you, what is the role of the educator? If the way to learn things is to read a book a bunch of times, what value does my tuition money get me? A syllabus? The ability to ask questions of a possibly poor communicater? reply lqet 8 hours agorootparentThe cynic answer would be: a highly standardized and comparable filter and testing environment. A more realistic answer: you are guided through and exposed to topics, motivated by exams, and in the end you will have proof that you understand the topics you received grades on. You also have - often direct and personal - access to top-level people in your field. reply gyomu 4 hours agorootparentprevI paid all this money to get to a beautiful surf spot, and you’re telling me I have to paddle and stand up on my own?! reply wavemode 3 hours agorootparentprev> If the way to learn things is to read a book a bunch of times, what value does my tuition money get me? The real answer? You gained a piece of paper which certifies that you are educated in a field. Depending on the school you may also gain access to an insular professional network. That's pretty much it. The notion that university degrees are worth anything more than that is moderately outdated. reply fhe 12 hours agoparentprevmy Chinese teacher supplied me with this supposedly ancient piece of Chinese scholarly wisdom: read any book a hundred times, and its meaning will be obvious. i have found this to work amazingly well -- particularly with poorly written technical papers. your comment also reminded me of this one time I was hanging out and watching the Matrix (for the 100th time probably) with a film maker friend. and he was pointing out to me that American film editing guides you with a rather heavy hand on where to look on the screen, whereas European films did little of that and the viewer has to search for what to pay attention to in a scene. after he showed me the editing techniques it all made sense, and explained why i could mindless follow hollywood movies, whereas watching an european film i'd get lost if not paying attention. reply lqet 8 hours agorootparent> American film editing guides you with a rather heavy hand on where to look on the screen There are notable exceptions, and I think the most commercially successful US director who largely ignored this advice was Francis Ford Coppola. In the \"Godfather\" trilogy, nothing is spelled out. You are not guided to anything. If you miss a minor detail in some scene, you are on your own, and you might not be able to follow the plot to the end. reply germinalphrase 2 hours agorootparentprevaka ‘intensified continuity editing’ which is the modern evolution of the ‘Hollywood style’. David Bordwell out of UW-Madison did a lot of work on this. reply mannykannot 7 hours agorootparentprevThat is probably what it will take for me to finally understand Calasso's The Ruin of Kasch. reply spacephysics 15 hours agoparentprevI find a similar trend with education in general. Some states have phased out programs for gifted students. Instead, many of them aren’t stimulated enough and end up going down a troubling path (worst case) or they don’t really reach their full potential during those formidable years. Teachers are expected to make the content match the lowest denominator, outside of the occasional exceptional teacher reply Broken_Hippo 10 hours agorootparentAnd so many of those programs were absolutely horrible. I had a lot more homework than my peers and was expected to act more mature. Sorry, but we were all the same age as other kids - we didn't deserve a higher workload (as kids and teens) and we should have been expected to act our age. It was pretty common to make fun of others for not keeping up well enough (struggling not allowed) or for appearing too smart (Not me, but a family member). Some school systems completely separated gifted kids from 'regular' students. By high school, it was obvious that this created some issues communicating with a broader range of folks. There is more than one way to make sure gifted kids get challenged - you don't necessarily need a special class for gifted kids. And you'll need to provide proof for the last one. It is true that they do teach so that the test scores are good - and since funding and jobs are tied to that testing, other things are going to go down. This isn't really making content matching the lowest denominator, though. reply fma 4 hours agorootparentMy daughter is in gifted. She still has a regular home room class that she is in 80% of the day. Gifted is treated as an elective where they have a class or two that is small in size and more intellectually stimulating (or so they say, I don't sit in there and have nothing to compare). No extra homework...they don't give homework at all nowadays. Because of how fragmented the United States school system is, your experience will definitely not be applicable to everyone. Heck, even the county next to mine does gifted differently. reply strken 6 hours agorootparentprevMine was great. It had a normal amount of homework, a smaller class size (which was a happy but unintentional accident), and accelerated four years into three. We shared electives with the rest of the school and socialised widely. I was bullied pretty badly through my pre-teen childhood and the program provided a way out of that, which in turn taught me how to interact with a group of people who didn't physically and emotionally abuse me for social gain - something a lot of people take for granted. Which is to say that anecdotes are of course going to be mixed. reply conjectures 10 hours agorootparentprevSeems geographical/not uniform. In UK some schools just pocket the gifted and talented funds and deliver nothing. reply Broken_Hippo 8 hours agorootparentIt definitely isn't uniform in the US - it isn't even uniform in schools near each other. Schooling systems between countries are very difficult to compare. I just learned that some UK schools have gifted and talented programs and funds, for example. reply arethuza 3 hours agorootparentThe UK doesn't even have a single education system (e.g. exams for university entrance and the length of first degrees). reply ZeroGravitas 10 hours agorootparentprevIn some ways this supports the point, in others it's the exact opposite. He says spoonfeeding information to people in bite-size chunks is like processed food and it should be hard, but you're saying that information should be spoonfed to 'smart' kids in exactly the right level of difficulty or they'll wreck their lives. Possibly it's like the concept of flow, where the standard suggestion is that things should be not too hard and not too easy, in order to keep your attention, interest and focus. But philosphically, that's just 'spoon-feeding' information in bite-size chunks like processed food, it's just varying the size of the bite to suit the level of the reciever, which again is exactly what he's saying is bad. reply InDubioProRubio 10 hours agorootparentprevThe terror of the masses to join the molasses. reply surfingdino 13 hours agorootparentprevBecause making someone think is stupid-shaming and therefore not politically correct. reply monero-xmr 12 hours agorootparentEventually everyone is tested. I have received perfect looking resumes and cover letters, then you get the person on a call and they are… hopeless. It’s very sad. Who pushed them so far? How did they get the credentials? Who wrote and edited their materials? Eventually the “rubber meets the road” so to speak, and all of the lies and gold stars and platitudes don’t count for anything. reply kaba0 7 hours agorootparentThere are certainly people like that, but there are also exceptionally smart people that just absolutely suck at selling themselves, and you might unknowingly decided the same way in case of both. It’s very hard to fairly evaluate someone. E.g. I had interviews where I 100% know more than one of my interviewers on the specific topic (not bragging, my knowledge is ain’t a high bar), and that gap in this unusual direction made the process very awkward and strange. reply GuB-42 5 hours agoparentprevI am all for the \"western\" side, where the speaker has to provide the understanding, motivation and entertainment, especially in the modern day where information is easily accessible. It doesn't mean that no effort should be expected of listeners, more like unnecessary effort should be minimized. An example of unnecessary effort would be using a foreign language listeners have no particular interest in learning. Personally, I would rather have my math class in a language I am at least fluent it, so that I can focus my attention on the math and not on the language. I also like my teach when they have an understanding of the psychology of learning, so that I can learn more effectively. Entertainment and motivation is part of it. It is spoon-feeding, but that's also how you get people to focus on the heart of the matter. At higher levels, it becomes less of a consideration, not because it is unimportant, but because at high level, knowledge itself becomes scarce, so you'd be lucky to find someone who really knows his stuff, even if he isn't the best at making it easy for people to understand. So the listen can spare some effort as it is the only way to get that knowledge. In the old days, knowledge in general was scarce so it made sense to tip the balance in favor of the speaker as you'd be lucky to have a knowledgeable speaker at all. But now, almost everything is a few clicks away on the internet, and the entire point of having a speaker is to present the information is an easily digestible manner. If you want to go \"the hard way\" you can do it by yourself, papers, textbooks, etc... are everywhere on almost every subject at almost every level, even more so if you embrace piracy. As for painting, or meaningful art in general, it is also part of the artist job to guide the viewer, not just dump a random idea on canvas, this is just lazy (on the part of the artist). Leave some clues leading to the big idea. Think like a puzzle. Puzzles are designed to be challenging, but they also involve guiding the player so that in the end, they can solve a more difficult challenge than they would have been able to with no help. Another thing to consider is that in a speaker-listener relationship, there are usually more listeners than there are speakers, so it is more efficient to have the speaker spend the effort being understood than having the listeners spend it understanding. reply sfink 2 hours agorootparentI didn't intend to claim that the \"Eastern\" way is unconditionally better. I'm just used to the Western way of thinking, so it's a novel perspective that I keep finding applies in more situations than I expect. Making things understandable is good. It's just not always the right thing to optimize for. Which is very different from saying that complexity is always better. Or as you said it: > It doesn't mean that no effort should be expected of listeners, more like unnecessary effort should be minimized. If all the information that needs to be conveyed is in the material, then making it accessible, understandable, and digestible probably is most important. Again, as you said: > it is more efficient to have the speaker spend the effort being understood than having the listeners spend it understanding. But it's kind of the difference between a sack of gold and the proverbial Golden Goose. For some things, you can't get all the benefit at once. As someone else here brought up with the idea of reading a book 100 times, some books/lectures/whatever give you more, and something different, every time you go back to them. It's like you need to incorporate the previous pass into your head before you can peel back a layer and grasp the next one down. It's a weird experience; with the same Chinese teacher I mentioned, I've many times had the experience of re-listening and hearing something totally different than I remembered. I sometimes doubt that I've ever listened to that one before. I think partly that's because the information is not coming just from the material, it's coming from the interaction between my mind and the material, and my mind is changing all the time. (Not necessarily for the better, but I'll leave that aside...) So I disagree that this applies universally: > But now, almost everything is a few clicks away on the internet, and the entire point of having a speaker is to present the information is an easily digestible manner. It really isn't. A lot of stuff is, so much that we get overwhelmed and blinded by it to the point that we assume that it must cover everything. But some things are not out there, or at least not out there for easy picking. Nobody has yet been able to write up such a clear and accessible description of how to ride a bicycle that someone could read it and then ride off on a bike their very first time. And that's the rule, not the exception, even with cerebral subjects like calculus or programming or whatever. It's not the difficulty that provides the extra value; you're not going to communicate more by making it artificially hard (as with your foreign language example)[1]. What helps is getting the learner to process more deeply, or apply the knowledge, or practice, or \"use it in anger\", or compete with it, or whatever way you want to say roughly the same thing. Our brains are not landfills of facts that benefit from the more you dump into them. They are coordinated systems of knowledge and behavior, where truly adding to one place requires adjusting everything else a little or a lot to accommodate. [1] Actually, you might, but only because it slows the reader down enough for things to sink in. Any other mechanism would work as well, and a mechanism that adds something else to the mix like tests or reviews is going to be overall more effective and efficient than artificial friction. reply btilly 14 hours agoparentprevThat is because the USA is low context while China is high context. For more about this and related topics, read https://www.amazon.com/gp/aw/d/B00IHGVQ9I/ref=tmm_kin_swatch.... reply lukan 14 hours agorootparent\"Americans precede anything negative with three nice comments; French, Dutch, Israelis, and Germans get straight to the point; Latin Americans and Asians are steeped in hierarchy; Scandinavians think the best boss is just one of the crowd. It's no surprise that when they try and talk to each other, chaos breaks out.\" Oh, it is a book about stereotypes. Well, as a german it seems I have to get straight to the point, I do not think, thinking in stereotypes this broad is helpful for communicating. reply sfink 1 hour agorootparentIf you would like an answer to that, then I would suggest reading the section titled \"Being open to individual differences is not enough\", and perhaps the quoted passage in the later section \"Tasting the water you swim in\". You're probably less \"German\" than she thinks you are, and more \"German\" than you think you are, but that's not incompatible with what she says. Don't mistake the blurb for the content. I agree that the blurb is a bit obnoxious, but then, its function is to appeal to (or piss off) someone enough that they'll pause and consider buying the book (maybe if only to prove how wrong it is). I have not read the book but I have heard the author speak on the topic, and in my opinion she adequately addresses your complaint. I personally still find her message a bit oversimplified, but isn't that what we're talking about? That's what you have to do in order to get your readers/listeners to understand what you're trying to communicate! Or do you? As in the original article here, there can be benefit to reading things where the author doesn't try to make it easy. Perhaps they put down the messy truth in disconnected fragments, or they pile up lots of examples that don't quite fit any simple orthogonal dimensions of explanation. Such compendiums incorporate deep insight to anyone willing and able to put in the effort to derive it for themselves. Let the reader figure it out by meditating on them, or rereading them 100 times, or trying them out in practice, or whatever. reply lukan 17 minutes agorootparentThanks, that is a more nuanced perspective, so maybe I should give it a try. \"You're probably less \"German\" than she thinks you are, and more \"German\" than you think you are\" Possible. I am definitely \"german\" in many ways. I positivly associate with the \"thinker and philosopher\" tradition. But I hate beer culture. But I also still have a unconscious deep rooted believe, that only german engeneering is good. But when I notice that, I stop with \"wtf? I know that is BS\". Those are the stereotypes I want to get away from. But when other people see me mainly as \"german\" - they push me into this role. reply forgotusername6 13 hours agorootparentprevDid you read the beginning of the book or just read that one quote? The first chapter on a story about meeting etiquette in Chinese business culture is actually quite insightful. It certainly resonates with me a least. I wish I had a manual so I knew how to behave in a meeting with people from different cultures. We are not all the same and there is no one size fits all way of behaving in a meeting. reply lukan 12 hours agorootparentI stopped at that quote. There are no doubt interesting anecdota inside, that might be insightful and there is no doubt some truth to some clichés, but I seriously doubt a box so big as \"asians\" has much value. And even for \"small\" boxes like \"germans\", there are for example great differences between east and west germany (seperated by the iron curtain and different systems for over 40 years) - but more so for the older and less for the younger generation. Etc. So reading in general about cultural differences when meeting someone from that culture can be surely be helpful - but in my experience it is not useful for taking such advice by the letter. reply lmm 12 hours agorootparentThe alternative to considering \"asians\" or \"germans\" is probably not understanding each person's cultural background individually but rather putting everyone in a single \"world\" box. Which is the biggest and most useless one of all. Once you have a good understanding of a typical german you can of course zoom in and get more detail, but if you refuse to learn about germans in general then that's going to make you less understanding of both an old person from east germany and a young person from west germany, not more. reply lukan 12 hours agorootparentThere is also the alternative of treating humans as humans first, if you don't know much about them, except their looks and their passport nationality. And not assuming one has these and those traits, because they look \"asian\", but were raised in the US for example. I know I met many people from many backgrounds all over the world and my thinking in boxes default mode, was never really helpful, but often very wrong. So it is good to know what some common traits are for a person from a certain cultural background, but not with the assumption that the individual in front of you is in fact like this. That can also offend people. For example some cultures do not like to shake hands. Germans usually do, but personally I also don't. So just be conscious and try to read body language, would be my advice. And in case of doubt, asking a person on the side and not in front of everyone usually works to work around missunderstandings. reply kaba0 7 hours agorootparentWell, the differences between western cultures are less pronounced, but I do think that knowing, say, Chinese etiquette when meeting mainland Chinese people is essential to not come across accidentally as rude. There are significant differences there, and natural body language does differ with culture. Nonetheless, I agree with your general point/sentiment. reply lukan 6 hours agorootparent\"Chinese etiquette when meeting mainland Chinese people is essential to not come across accidentally as rude\" For sure. And I read up about any culture I visit the first time. But chinese are quite different from mongolians and thais for example. So my issue was especially with \"asian\". This term is allmost meaningless to me, as it puts billions of different people in one basket. reply latexr 10 hours agorootparentprevAlternatively, by doing what you suggest you embed stereotypes into the person which may then need to be undone, which is harder then starting from a blank slate. This is how we get to harmful (even if well intentioned) ideas like “Asians are good at math”. https://ideas.ted.com/why-saying-asians-are-good-at-myth-isn... https://phys.org/news/2020-07-racist-stereotyping-asians-goo... reply btilly 4 hours agorootparentprevWhat you think the book is, and what it actually is, are very different. It is not about stereotypes for judging someone from another culture. It is about how to think about other cultures so that we won't fail in stereotypical ways when we have to function in those cultures. And how to understand and resolve common conflicts that happen between businesses from different cultures. reply trueismywork 11 hours agorootparentprevWell it's not a book of rules. reply tommiegannert 12 hours agorootparentprev> Well, as a german it seems I have to get straight to the point \"The Joy of Reading Books You Don't Understand\" It seems you didn't even try to understand The Culture Map, and opted for a strawman. > I do not think, thinking in stereotypes this broad is helpful for communicating. You're trying to use it as a cookbook. If you instead see it as a dictionary to be used when someone you're interacting with isn't behaving the way you had expected, it will make more sense. We can still be unique flowers with a wide variance, even if cultural regions have shifted medians. reply lukan 8 hours agorootparent\"If you instead see it as a dictionary to be used when someone you're interacting with isn't behaving the way you had expected, it will make more sense\" Well, to be honest, I doubt that. By now I have read some examples from the book and the way he uses nationality in absolute terms and placing them on scales is deeply offputting to me. So far I often experienced situations where people behaved differently, than what I would have expected - but I do not recall any situation where placing those people on mathematical sounding scales would have explained their reactions better. With some thinking and asking they all could be explained and resolved in a normal way. To me the whole thing sounds like something that sounds good and easy on first glance - but falls apart when you look deeper. The author as a \"international business expert\" likely knows his way around different cultures simply by experience - not because he makes cultural meassurments in his head. But he made a goodselling book, so good for him. And good for you that you find value in it. I don't. So maybe I \"didn't even try to understand The Culture Map\" - or maybe I just have a different opinion. reply Jensson 5 hours agorootparentCulture correlates strongly with nationality, you are throwing away a very powerful tool just for an ideological reason. And no, often it is too late once you have already made the mistake, first impressions matter and you massively improve your chances if you take their nationality into account. Sure they might take your nationality into account and adapt to you instead, as you say that often works for you, good, but some people actually wants to learn to adapt to others. reply lukan 2 hours agorootparent\"but some people actually wants to learn to adapt to others\" Yes. And I said I don't want to learn by fixating on nationality. Not that I don't take it into account. And the quote above from the cover already talks about \"asians\". Even less meaningless. Not completely meaningless, but allmost. And all I read about the book seems like strongly fixating on nationality. Maybe it goes deeper at some point. I only judged from what I read. And I am aware of the potential irony given the topic, but so far I think, I understood enough. reply sfink 1 hour agorootparentprevI have not read the book but I have heard Erin speak, and I do find the high/low context dimension to be very powerful in explaining a great many things. I don't see how it applies all that well to this one, though, other than perhaps explaining one way in which something can come across as dense or cryptic. Specifically, you could use it to say that a text is embedded in the context in which it was written, and so for example what is not said can speak louder than what is said. But I don't see how it explains differences in what is expected of a listener/reader/learner. I may very well just be missing it. reply jesterson 14 hours agorootparentprevthat's extreme oversimplification of multifaceted topic. reply btilly 4 hours agorootparentAll models are simplifications. High versus low context is only one of many dimensions in its model. reply eruci 2 hours agoparentprevwith Poetry, the onus is still on the reader to get something out of it, even in the west. I recently read \"Pale Fire\". (without the preface and Nabukov's commentary). I enjoyed it thoroughly, without understanding a lot, which is fine. reply mannykannot 7 hours agoparentprevIf this is true in general, it might have been a factor in why the scientific and technological 'revolutions' of the so-called Enlightenment occurred in the West. reply parthianshotgun 7 hours agorootparentHow? reply rvba 8 hours agoparentprevOversimplification is bad, but there is a reason why Western/US universities are so much better at teaching students. Imagine your professors and textbooks werent there to teach you, but they were there to show off their knowledge or to prove you that you know nothing. A very known problem in my country is that a professor is not making a lecture to teach you something or to explain you something, the professor wants to show off his knowledge - that he is the king and you are a nobody. Then you get unreadable textbooks full of big words (sometimes you think authors dont grasp them)... which are just plain student unfriendly. I remembet that I had borrowed some statistics books from USA - and they were easier to read in English than the crap I had in my own language. They were easier to read and easier to understand. No big words. Just explanations and examples. On a side note, they taught us physics with English abbreviarions. When most students didnt know English. Think you are in 5th grade and they nake you memorize things like: d = s x t You have to figure out that it is distance, speed and time. Note that those abbreviations have nothing to do with the local language. Also why even use abbreviations? Lazy teacher (AND lazy textbook) could have used full words at least. In own language, not English. Most people from US dont realize how much easier you have. For starters you dont spend a lot of time learning English as a foreign language. Then the non-Americans can get books that are written to teach you something* not to show that the author is great. (* although now I think most textbooks are written for profit). reply 0xFEE1DEAD 5 hours agorootparentIndeed, the truth often lies somewhere in between. It sounds like you might not have been studying to become a mathematician but had to take a statistics course as a requirement for your degree. In such scenarios overcoming vague and complex teachings can indeed feel incredibly cumbersome, often resulting in a negative overall experience. However, when it comes to topics you’re passionate about the situation can be quite different. While exceptions exist in every field passion can make certain teaching styles more tolerable. For instance, I taught myself programming at the age of 13 and I vividly remember struggling with OOP. It took me 2 months to grasp it, but I persevered. English is not my native language and I was quite poor at it in school. I began learning English on my own because there were far more programming resources available in English than in my native language. I was terrible at math and finished high school with an E in math. Fast forward a few years I developed an interest in algorithms and theoretical computer science because I wanted to understand how compilers work. I spent months learning to comprehend mathematical symbols and notation, reading numerous resources that assumed a solid mathematical foundation which I did not have. I persevered because I was genuinely interested. Making learning too difficult isn’t helpful, but neither is making it too easy. Like most things, it really depends. reply KptMarchewa 5 hours agorootparentprev100% agree, the same things happen in Poland and I wish it was closer to US. reply Xfx7028 8 hours agorootparentprevThis sound ls very much like the experience of Greeks according to some friend. reply kaba0 7 hours agorootparentprevI think this has more to do with budget. In many countries, professors are not paid specifically to write books, but if no proper book exists in the local language then they sort of have to do it one way or another. It usually ends up as some hodgepodge document, each chapter written by a different professor - whose are experts at their fields, but not at book writing!, which is a specific skill. Some or better at it, others absolutely suck. Then they just print it some way without any lecturing, unification of styles, references and use it as the course book, because they were sorta forced to. In English, there is competition and people with actual experience can publish books, which will be used by multiple universities, if it becomes famous than multiple generations of students from multiple universities and years have criticized it making the nth version better, etc. This is very different from the budget solution my med university (of which 4 exists in this language alltogether) could reasonably come up with. Nonetheless, there were smaller topics, documents, chapters which easily surpassed the same found in any English language book, especially in mathematics (the Soviet block used to be famously good at mathematics, so the level was much higher than the west’s), for these small gems it does worth speaking obscure languages. reply electrodank 6 hours agoparentprev>In paintings, it is known that the viewer can get out more than the painter put in. There’s something very much “Dabblers and Blowhards” about this statement that I can’t quite put my finger on it. [0] Try painting, I mean really painting, before spouting nonsense. It wreaks havoc on the rest of your comment. [0] https://idlewords.com/2005/04/dabblers_and_blowhards.htm reply lloydatkinson 9 hours agoparentprev> If a reader can't follow the argument, it's automatically the author's fault and a waste of the reader's time. Heaven forbid the reader might need to exert some effort and grow in the gleaning. I've experienced the receiving end of this a couple of times on HN. I once posted a blog post and it was extremely obvious that the detractors (who, despite toxic being a bit of an overblown word now, were being toxic and breaking HN rules and got away with it) hadn't even read perhaps a third or less before getting angry in the comments. I don't care if people don't like the suggestion but I believe blocking should be implemented. reply exe34 8 hours agoparentprevthe problem is how much shite there is out there to read through. you could read all the monad tutorials in the world but it won't help until you start using them yourself. (admittedly I'm going a step further from your point, not against it). however, I'd say if I can't understand what an authors trying to say, it makes more sense to find one that I can understand first, and then go back to the more abstruse one. reply renewiltord 15 hours agoparentprevYou'll see this often on the Internet: proof? Proof? Citation? Boy, no one's going to do any work for you. If you believe the wrong thing, the consequences are your own. In fact, only people with no better use for their time will spend their time teaching you. This means you're being taught only by people whose time is worthless or for whom it is useful for you to believe in something. If you're not paying for the knowledge, you're not the customer, you're the product. I never elucidate for those beneath me in understanding. I only discuss with peers. Perhaps the only capable person I know who does different is Taleb but his pleasure appears to be in calling someone \"imbecile\" after proving them wrong. reply zo1 11 hours agorootparentI only wish for people to take a \"little bit\" of a charitable interpretation of my comments. Lot of time they simply find one little gap, wrong wording, etc and just run with it and dismiss my entire comment. I do it too, sometimes, but I like to think I do it for a reason. Alas, we're all humans: greedy, and biased towards our own views. reply renewiltord 2 hours agorootparentPrecisely. They usually come to you with misbelief and intentional miscomprehension. I don’t think it is worth convincing someone of the truth if they insist on finding a way to believe a falsehood. Let them believe. reply wozniacki 15 hours agorootparentprevI was waiting the whole time I read this to find a /s somewhere. Anywhere. Yikes. reply bheadmaster 13 hours agorootparent> Yikes. Proof? Citation? Jokes aside, I find that this sentence makes much sense, especially in the context of online forums such as HN or Reddit: In fact, only people with no better use for their time will spend their time teaching you. This means you're being taught only by people whose time is worthless or for whom it is useful for you to believe in something. Why do you think that is inaccurate? reply hashiyakshmi 10 hours agorootparentBecause there are plenty of people who just enjoy explaining things or helping others understand, and to say the only two reasons for that behavior is that their time is worthless or they have an agenda is myopic. reply latexr 10 hours agorootparentprev> Why do you think that is inaccurate? Not only is it inaccurate, it is insulting to the person teaching you. Have you never been on a popular HN thread where a known expert in the field, someone who’s more productive and knowledgeable than you, provides context? But somehow because they did you feel it justified to call their time worthless? Well, certainly I’d regret wasting my time on someone like that and I’d hope the other readers were more appreciative. What the OP is calling a “better use of time” I’m reading “more selfish use of time”. Maybe, just maybe, the person spending their time teaching others doesn’t consider their time worthless, but they manage it better and thus have some moments to share their knowledge. Or maybe they enjoy doing so. This is not a hard concept for those not affected by such a superiority complex they claim there are others “beneath [them] in understanding”. reply bheadmaster 10 hours agorootparentThat makes sense, thanks for the perspective. Much better than just \"yikes\". I hate tweetspeak. reply hju22_-3 13 hours agorootparentprevI will agree that it is big yikes. But I will, at least kind of, agree that you are more likely to meet more arm chair scientists than you are scientists and actual field experts online in this fashion. Though, obviously, there are actual scientists and field experts around. The issue, as always, is how to differenciate them from the not-so-obviously fake ones as a layman. But yes. Still big yikes. reply m3kw9 16 hours agoprevFor books that you don’t understand much about and can get daunting/painful reading it, you should use the table of contents and read the most interesting one first, then the next.. reply tuduka 11 hours agoprevI recently read 100 Polish books in 100 consecutive days to see how much of the language I'd learn (I also listened to the matching audiobook of each book). To make meaning of the text, I relied on quick look-ups, context clues, and the audiobook's narration (inflection, pacing, etc.). At first, I hardly understood anything and didn't know any Polish vocabulary, but somewhere around book #50, I started recognizing words and phrases and even experienced language automaticity. Many language experts say you should be able to comprehend about 90%+ of the vocabulary in your target language when you read, but I think that's completely unrealistic. Read as if you're fluent now, even if you don't understand a word of it. You will eventually learn! For those interested in my experiment, I wrote a book about it called \"BLITZED: What I Learned Reading 100 Books in 100 Days in My Target Language\": https://a.co/d/0bKrjq44 reply ofcourseyoudo 18 hours agoprevCan someone tell me why this website asks if you are between 13 and 15 years old? reply lannisterstark 17 hours agoparentCould this be it? >Our websites are designed for children aged 13 and up. We do not sell any children’s data for monetary consideration. Website pages that are aimed at children under 16 are configured so that we do not knowingly share any children’s data with third party advertising companies unless the website visitor opts-in to allow the sharing or indicates that they are 16 and over. reply whatnotests2 20 hours agoprevHegel's Phenomenology of the Spirit, Marx's Capital, Foucault's work, von Neumann and Morgenstern' Game Theory, and the Perl 6 Apocalypses and Exegeses from the early 2000's. reply sigriv 3 hours agoparentDeleuze, Bourdieu, Malebranche, ... I find it incredibly satisfying to stretch my brain with books that are inaccessible on the first read. reply g8oz 16 hours agoparentprevGödel, Escher, Bach: An Eternal Golden Braid (GEB) - too much for me. reply diffxx 14 hours agorootparentI found GEB understandable but ridiculously long winded and the passages with achilles were insufferably annoying. reply bowsamic 10 hours agorootparentI honestly found even the preface insufferable reply __rito__ 8 hours agorootparentprevIt was very readable to me. I really enjoyed reading the book. reply nextstepguy 14 hours agoparentprevThe bible king james version translation. reply insane_dreamer 20 hours agoparentprevFeynman's lectures reply hsavit1 20 hours agoparentprevkant's critique of pure reason reply DrStormyDaniels 20 hours agorootparentShakespeare, Emily Dickinson reply wozniacki 16 hours agoprev [–] I'm dismayed that no one so far has brought up a point that's begging to be made in these sorts of things. While the point of the article has _some_ merit, there's also another equally valid contrary argument to be made. Just because a book - however storied & fabled - exists out there, does not mean that you should strive to find some meaning, import or significant cogitable thought when one is not clearly and immediately present. There's a whole industry of writers that exist to exclusively furnish meaning to the lofty thoughts of some distinguished authors, that that was simply never meant or not present in the authors own words. Sometimes the authors themselves invite and regale in this kind of festive chicanery. Sometimes not. But this sort of thing - far more than useful or warranted - does exist. In other words some works of writing often fiction but not necessarily are just elaborate exercises in getting away with balderdash. It pays to remember the enterprise of getting published in the past has not always been equitable as is the case today. A virtual nobody off the street couldn't expect to even get his manuscript read by a publishing house, much less get published even for a limited run. So if you were already reputed or privileged or had the blessings of a wealthy house of patrons who bankrolled your previous works, you were more widely published and translated. In other words far too many mediocre works of the past still get top billing, than they rightly deserve largely because no one called out their bullshit. Yes, sometimes if you don't understand the author that is because the author never had the intentions of being understood in the first place or did not have much to say of value or import, however fleeting or ethereal or unyielding to lucid language, the authors thoughts were. HN should buck this trend and not join in adulation. reply MikeBVaughn 13 hours agoparent [–] > Sometimes the authors themselves invite and regale in this kind of festive chicanery. Sometimes not. But this sort of thing - far more than useful or warranted - does exist. Why does art and the attempts at interpretation thereof have to be useful or warranted? Festive chicanery sounds delightful to me. I would like more of that in my life, please. > In other words some works of writing often fiction but not necessarily are just elaborate exercises in getting away with balderdash. > In other words far too many mediocre works of the past still get top billing, than they rightly deserve largely because no one called out their bullshit. > HN should buck this trend and not join in adulation. Do you have some concrete examples of works that fit these claims? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the joy and value of reading books that are not entirely understood, suggesting that it's okay to appreciate a book without fully grasping it.",
      "The author, Molly Templeton, shares personal experiences with complex books like Neal Stephenson’s *Baroque Cycle* and recent titles such as Alaya Dawn Johnson’s *The Library of Broken Worlds* and Molly McGhee’s *Jonathan Abernathy You Are Kind*.",
      "Templeton argues that embracing uncertainty in reading can be liberating and enrich the reading experience, encouraging readers to explore challenging narratives."
    ],
    "commentSummary": [
      "The post discusses the value of reading books that challenge and provoke deep thought, referencing Kafka's belief that impactful books should \"bite and sting\" rather than simply entertain.",
      "It highlights different perspectives on reading difficult or complex books, with some readers advocating for immersion without note-taking to enhance understanding and enjoyment.",
      "The conversation includes personal anecdotes and recommendations for books that have left a lasting impression, emphasizing the joy of discovering new insights through re-reading and engaging with challenging material."
    ],
    "points": 298,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1720043087
  },
  {
    "id": 40874341,
    "title": "Twilio confirms data breach after hackers leak 33M Authy user phone numbers",
    "originLink": "https://www.securityweek.com/twilio-confirms-data-breach-after-hackers-leak-33m-authy-user-phone-numbers/",
    "originBody": "www.securityweek.com Verifying you are human. This may take a few seconds. www.securityweek.com 89e15dec189e9c31",
    "commentLink": "https://news.ycombinator.com/item?id=40874341",
    "commentBody": "Twilio confirms data breach after hackers leak 33M Authy user phone numbers (securityweek.com)260 points by mindracer 6 hours agohidepastfavorite148 comments pembrook 1 hour agoWhile this sucks, my phone is in so many data breaches at this point it doesn’t matter. The spam-to-ham ratio on my phone number is now far worse than any other channel for me. The traditional phone network is at risk of going the way of the fax machine if we don’t do something about the spam problem like we did with email. If I’m on a call, even with family, it’s now almost exclusively on FaceTime/zoom/meet/etc. I can’t remember the last time I talked on the traditional phone network or received a legitimate call. Which isn’t great because those aforementioned platforms are all proprietary walled gardens with terrible incentives — once they capture the market fully they will eventually dump ads all over your calls. Don’t believe me? Just look at what Gmail did to monetize the lock-in on your inbox. reply bonestamp2 48 minutes agoparent> I can’t remember the last time I talked on the traditional phone network or received a legitimate call Doctors and dentists. Most of the calls I get are spam, but then the MOST important calls I get are from doctors, labs, and dentists. I do as much as possible online of course, but not all of these professionals have good online systems and phone calls are often required. Sometimes you know what number they're going to be calling from ahead of time, but often you don't... especially if you're in a large medical network that has different offices for different specialists, etc. It's a really sad situation if you get sick and you're trying not to miss these important calls, especially when it's a long wait for a specialist and then you miss their call when they get to your name on the waiting list. This will literally cost some people their lives and legislators need to act on making spoof calls impossible -- there's no reason why anyone should be allowed to spoof a number that they can't receive calls at. reply tmpz22 33 minutes agorootparent> I can’t remember the last time I talked on the traditional phone network or received a legitimate call Social services are another example. Many services are county-administered and thus don't have a centralized online platform. As always our most vulnerable populations suffer the most from techno-greed. Not the families of software engineers who built the system. reply Ghexor 1 hour agoparentprevHow convenient for the data collecting companies that so generously sponsor the new & free services, that our democratically controlled communication infrastructure looses in value. reply TeMPOraL 54 minutes agorootparentAdvertising is a cancer on modern society. It will metastasize to any new communications medium, public or private, and destroy it from within. People will switch to new medium that offer less spam, but advertisers quickly follow to strip-mine the new channel. A cycle of life, so to speak. reply SoftTalker 1 hour agoparentprevI make and receive regular phone calls all the time. However I only answer those that are from numbers I have in my address book. I do the same with text messages, I have my default view set to \"Known Senders\" so I'm not even really aware of others. If I'm expecting an unknown sender message, such as a TFA code, it's easy enough to just look in \"Unknown Senders\" for it. reply cjbgkagh 1 hour agoparentprevI think that is intentional, AFAIK phone communication is more protected than other types so allowing spam to continue unabated is in the governments interest. Outsourcing the harassment to 3rd parties, similar to how prison torture is outsourced to the inmates. The government could fix these things but would rather not. reply darby_nine 1 hour agorootparentI think we just don't have very much competition in telecommunications so things never get fixed. Why bother? It's easier to extract rent off largely the same offerings as the rest of your market (difficult to understand pricing tiers that function as a congestion tax more than a transaction, often region-specific monopolies or duopolies, indistinguishable quality of service) and bring home large profits, market efficiency damned. Yes, I'm exaggerating. No, it's not by much. reply cjbgkagh 1 hour agorootparentAlmost no-one is pro-spam, it’s pretty much universally hated, and in many cases it’s already illegal so it’s more of a matter of enforcement. It is also trivial to detect. Sure there probably is some regulatory capture but if anything at all can be regulated it’s spam calls / messages. If the government can’t regulate spam then what could it be expected to regulate. The general population is increasing worried about scam calls for their elderly relatives, it’s already a big deal. reply ToucanLoucan 58 minutes agorootparent> Almost no-one is pro-spam In fact there are really only two groups that are pro-spam: spammers, obviously, and the entities that provide them services from which they may spam. Oh sure basically any provider of any service be it phone, web hosting, email, etc. will say they don't want spammers, and the email providers may actually mean it what with them not wanting their server's scores trashed and be unable to get email to anyone (though plenty others don't give a shit), but website hosts, telephone companies, and SMS providers? They utterly do not care and in fact go out of their way to not know when spammers are (mis)using their services. Meanwhile like that other commenter said, everyone is incentivized to enter walled garden services that actually do the barest minimum of enforcement for spam activity. I doubt they're conspiring in a dark room somewhere, but neither side is going to upset at the other in that situation. reply cjbgkagh 52 minutes agorootparentHanse my other example of the inability to police prisons enough to prevent abuse, I didn't allege an explicit scheming but a happy little accident. Allowing a problem to fester when it benefits you is totally normal and expected behavior. But if there is a role for government at all it would be regulate such dysfunctions. reply treflop 44 minutes agorootparentprevEmail is easier to mitigate spam with. The whole body of the message is given upfront. reply darkr 1 hour agoprevThis doesn’t surprise me. I found an information exposure vuln on the user registration endpoint a while ago (given a phone number of an authy user who had previously registered via another customer, retrieve all other numbers/devices/timestamps, email addresses and other info for that user). It took them two years to fix it. reply rvnx 1 hour agoparent> Twilio has detected that threat actors were able to identify data associated with Authy accounts, including phone numbers, due to an unauthenticated endpoint Isn’t it what you are describing? reply duckmysick 3 hours agoprev> Twilio has detected that threat actors were able to identify data associated with Authy accounts, including phone numbers, due to an unauthenticated endpoint. We have taken action to secure this endpoint and no longer allow unauthenticated requests How do I avoid such problems in my own app? Force authentication for all requests with row-level security? Rate limiting? Any testing frameworks that would catch this? Something like \"given endpoint /user/phone-number-validate make sure onlycan access it\". reply jmvoodoo 2 hours agoparentOne step we have taken is to build an auth system that requires you as the developer to explicitly specify the security of an endpoint using a decorator. If no decorator is provided, then the endpoint is completely locked down even to admins (effectively disabled). If an endpoint is decorated with something that is considered dangerous (i.e. public access), that triggers additional review steps. In addition, the authentication forbids certain combinations of decorators and access patterns. It's not perfect, but it has saved us a few times from securing endpoints incorrectly in code. reply hypeatei 2 hours agorootparent.NET web apps / APIs have an option where you can require authorization on all controllers (and their actions) by default. If you need an anonymous controller/action, you can use the `[AllowAnonymous]` attribute on it. reply api_or_ipa 11 minutes agorootparentYou can easily do the same with most (all?) routers using middleware. Whether you get it slotted in your roadmap is a different story. reply kardianos 1 hour agoparentprevThis is really, really, simple. 1. build a single endpoint handler that handles auth, then looks up the endpoint on the path. 2. Never create direct endpoints, just register endpoints in the system that the auth endpoint works under. You know table driven tests? Use table driven endpoints. It works and makes things so much simpler and secure. reply znpy 1 hour agorootparent> 1. build a single endpoint handler that handles auth, then looks up the endpoint on the path. 2. Never create direct endpoints, just register endpoints in the system that the auth endpoint works under. So like, an authn/authz middleware ? reply brunoarueira 2 hours agoparentprevIt's a common problem. On a previous job, I'd found one unauthenticated endpoint just because I want to add some integration tests on it and my tests failed! After that, I'd created a script which lists all endpoints and curl each one with invalid credentials and expecting them to return 401. reply cmgbhm 1 hour agoparentprevThis is actually a use-case I use for interviews. 1. Everyone tests authenticated user can do the right thing. 2. Canauthenticated user access the data? 3. Can an unauthenticated user access data? If there’s a testing framework that does this scaffolding automatically, I’d love to hear it. reply tmpz22 25 minutes agoparentprevHoly shit why is this even a question?? You. Write. Tests. You build into your testing framework/library a mechanism that will craft sessions across your range of authentication-levels - unauthenticated (no-session), authenticated but unauthorized, etc. You mandate new endpoints must have permissions test in code review. Simple, straight forward, and absolutely the bare minimum of competency for any endpoint returning personal data. reply snowwrestler 3 hours agoprevI use Authy’s iOS app to generate 2FA tokens for a few accounts. I cannot remember ever entering my phone number into it, or establishing an Authy account of any kind. Is there some other way they would have acquired my phone number? I’m trying see if the issue is some unanticipated issue with the iOS client app itself, or if it is only affecting people who created online accounts with Authy to sync their 2FA credentials across devices. reply ayewo 1 hour agoparent> I cannot remember ever entering my phone number into it, or establishing an Authy account of any kind. Is there some other way they would have acquired my phone number? Entering your phone number was mandatory. This was what turned me away [1] from Authy to Duo Mobile on my Apple devices. https://news.ycombinator.com/item?id=33244324 reply inhumantsar 3 hours agoparentprevAuthy is both a SaaS and a consumer-facing authenticator app. When companies integrate Authy into their system, they can use it for SMS OTP (also deliverable by phone call + TTS iirc) as well as regular TOTP, Authy's proprietary TOTP, and others. Your phone number would only be at risk if you used a service which used Authy for SMS 2FA reply ffsm8 3 hours agorootparentThe consumer app also wants your phone number... It prompts you to \"backup\" your codes, so that they're not gone if you reinstall the app or switch devices you probably gave them your phone number at some point if youve got authy on multiple devices. /Edit: just checked on a clean install. It prompts for a phone number instantly and won't let you scan codes without creating an account. Not sure when that happened, as I haven't really used it in years. reply inhumantsar 3 hours agorootparentFigures. I stand corrected then. We used Authy for 2FA at my last company and migrated off it to use a complete auth platform. The amount of user (consumer and business) hostile shit we found in the process was astounding. Twilio was nice to work with way back when it was the only decent API-driven POTS connection service out there. They've steadily gotten worse over the years and acquisitions though. Wouldn't recommend them to my worst enemy these days. reply razakel 3 hours agorootparentYou know, one thing I learned from my patients... they all hate the phone company. It's interesting; even the stock holders of the phone company hate the phone company! reply inhumantsar 2 hours agorootparentAs a former telco employee and current telco shareholder, can confirm. reply stogot 2 hours agorootparentprevWhat do you recommend now reply inhumantsar 2 hours agorootparentFor authentication services to integrate into apps/services, Zitadel. For consumer password/2FA management, Bitwarden and Yubikey. reply toomuchtodo 2 hours agoparentprevCloudflare should probably deprecate their Authy provider, considering they support other more secure MFA options (hardware and virtual WebAuthN). I believe Wise (ex TransferWise) and Plastiq also use Authy natively for SMS OTP server side, but provide no mechanism to disable SMS 2FA (boo). https://authy.com/guides/cloudflare/ reply jgrahamc 1 hour agorootparentThere's no \"Use Authy\" option any more in Cloudflare. It just says: Mobile App Authentication Secure your account with TOTP two-factor authentication. And clicking the button gives you a generic QR code to use with app of your choice. reply toomuchtodo 1 hour agorootparentThank you for correcting me, Cloudflare was presented as an Authy token that would be destroyed when I deleted my Authy account and some of the docs I found led me to believe this was still actively in use. I retract the Cloudflare part of my above comment. reply jgrahamc 1 hour agorootparentNo need to apologize. We did use Authy for a long time but allowed more general TOTP solutions from 2017 and have really pushed hard for people to use hardware keys. reply slightwinder 3 hours agoparentprevHave you looked into the settings? On android you can see a cellphone-number and e-mail there. If they are missing, I guess it's not known to them. reply snowwrestler 3 hours agorootparentNothing in the iOS Settings app for Authy, but tapping the little gear icon in the app UI shows my phone number and email! I guess I did enter them at some point and forgot. Thanks. reply k8sToGo 2 hours agoparentprevIf you use cloud sync I think it requires your phone number reply godzillabrennus 1 hour agoprevAuthy is basically unsupported. Not surprised. I switched my accounts to 1Password when they announced the end of life of the macOS app. reply bonestamp2 43 minutes agoparentThat makes sense. In case it helps others... when they announced end of life of the mac app, that was because Apple Silicon macs can run the iOS version of Authy. So, if you have an M series mac then you can still use and get updates to authy. reply gz5 33 minutes agoprevconsider* putting endpoints on a private overlay network in which network access is cryptography-gated (e.g. x.509 cert based). then, a misconfigured endpoint (or a zero day etc.) can't be exploited by any_actor_on_the_internet - actors need to first complete the provisioning process you choose to enforce to be authorized to use the private overlay. *not one size fits all, e.g. bad option if endpoints need to accept requests from unknowns. however, many endpoints only need to accept requests from known (identified, authenticated, authorized) endpoints, and the added friction to id/authN/authZ get use the private overlay is not a business impediment. there is a stigma here due to the horrors of NAC on private enterprise WANs. but NAC goals can be accomplished without that baggage via internet overlays and modern cryptography. to be clear, i am by no means advocating to abandon traditional methods of endpoint auth - this it is just another layer which recognizes that single layers are rarely airtight (e.g. what just happened to Authy and Twilio). reply bonestamp2 40 minutes agoprevI recently setup a focus profile on my iPhone that only lets calls ring through from knowns contacts. There is going to be an adjustment period as I discover people and companies (such as doctors/hospitals) that I want to allow calls from and add them to the whitelist. But otherwise, it has been really nice to cut down on all of the interruptions. reply ndneighbor 2 hours agoprevI guess this explains the recent uptick in spam... reply jonathanlydall 1 hour agoprevWhen I tried SendGrid it was super annoying that I had to install yet another Authenticator app on my phone. Now it’s become a point of data loss. It’s bizarre to me that Twilio decided to get into the Authenticator business at all, especially while SendGrid had plenty enough problems to keep them busy. reply jmbwell 2 hours agopreviOS/iCloud has a built-in TOTP function also. Maybe better for friends and family than some people here. https://support.apple.com/guide/iphone/automatically-fill-in... reply delduca 2 hours agoparentI have been using Apple’s Passwords, it is great. reply hypeatei 2 hours agoprevI just migrated off of Authy last week but I was probably caught in this breach, ugh. Never liked it but they make it extremely difficult to export your data. I used this project for exporting: https://github.com/alexzorin/authy EDIT: it appears this project was actually using the unauthenticated endpoint (used in breach, too) to facilitate exporting, lol. Good luck to anyone trying to get off of Authy, Twilio really doesn't want you to export your data for \"security\" reasons. reply NelsonMinar 1 hour agoparentThe lack of export in Authy is a really ugly choice they made. When I migrated to Aegis I used some hack that involved a desktop Electron app's javascript console. I wonder if that still works? reply hypeatei 55 minutes agorootparentThey don't offer Authy Desktop anymore officially and you need a specific version. Not sure if the hack still works if you have it installed. reply Zetaphor 2 hours agoparentprevI also just recently left for Aegis and have been very happy. I feel much better knowing that my 2FA is completely offline reply teamspirit 1 hour agorootparentRight, I did the same a while back. Aegis for Android and 2FAS for iOS. Never looked back. Also, if anyone is going either direction, AndroidiOS, both of these open source options allow easy export. reply lifeinthevoid 1 hour agorootparent2FAS also exists for Android, is Aegis superior or you don't use 2FAS on Android for another reason? reply pnw 1 hour agoparentprevHas anyone found a single open-source app that supports both mobile and desktop though? That was the attraction of Authy before they killed their desktop apps. reply EVa5I7bHFq9mnYK 15 minutes agorootparentThe desktop version somewhat contradicts the purpose of 2FA. reply hypeatei 8 minutes agorootparentNot really, 2FA is literally just that: a second factor. It makes it unlikely someone has access to both your password and the TOTP URI. So, if you leak your password on a public forum (for example), the person who gets that is not likely to also have your TOTP info. reply hypeatei 54 minutes agorootparentprevMost password managers support it and offer mobile + desktop clients. reply Yhippa 1 hour agoparentprevWhat did you end up moving to? reply hypeatei 59 minutes agorootparentStoring 2FA in Bitwarden (my password manager) and Aegis as a fallback. Also making offline backups of each periodically. reply smaddox 2 hours agoprevNo wonder I've seen such a major spike in spam calls / texts. reply deegles 1 hour agoprevI have removed all SMS based 2FA from every account that allows it and you should too. reply selbyk 1 hour agoparentI'm a bit confused how this is relevant. Authy is a OTP app, nothing to do with SMS. reply yieldcrv 1 hour agorootparentAuthy uses SMS based recovery of your entire account, a weaker link that a single service using SMS based OTP reply ingatorp 51 minutes agorootparentYou can always disable multi-device, so it can act like a regular OTP auth app. reply yieldcrv 1 hour agoparentprevand we should do product liability lawsuits on every service that only allows SMS based one time passwords, if they don't allow a client side only option reply otachack 3 hours agoprevAs alternatives: I use Authenticator Pro on my phone and keep encrypted backups whenever I modify it. I know others have pointed out Aegis. The issue is starting the migration out of Authy. Assuming Authy has no easy export, I suggest you migrate over a few entries at a time (maybe from top down) while keeping account of transfers somehow. You can have authenticators live side by side in the meantime! reply cmgbhm 2 hours agoparentYou can rename them as they are migrated reply MenhirMike 3 hours agoprevDoes anyone have a recommendation for an Open Source 2FA OTP app? That's the only thing I use Authy for, to scan the QR Codes into the App and generate the 2FA tokens, but in a way that allows me to migrate to another phone without having to re-set all the 2FA tokens on the vendor side. reply SushiHippie 3 hours agoparentFor Android I'd recommend Aegis https://f-droid.org/packages/com.beemdevelopment.aegis/ Or if you have a YubiKey you could also use it for TOTPs Windows, Linux, Android: https://github.com/Yubico/yubioath-flutter iOs: https://github.com/Yubico/yubioath-ios I personally use Bitwarden for TOTPs (with a self hosted vaultwarden instance), it's by far not the most secure way to store your passwords and TOTPs next to each other, but it saves so much time. reply alias_neo 3 hours agorootparentThis. I migrated to Aegis a while back because I wasn't happy with how hard it is to get secrets out of Authy, or that someone else is managing them, and they they need my phone number (guess I was right, again). I use Folder Sync on my Android to sync the Aegis auto-backups to a MinIO bucket I host at home. reply tamimio 2 hours agoparentprevEnte Auth or bitwarden builtin one or keepassXC builtin one. Migrating from Authy is a headache, though you don’t have to reset the tokens. I found a way to do it (1), but I had to do it manually because Authy only exported the email/user and the token. Now, if you are like how I used to be, having the same email for different accounts, the exported JSON will be confusing and there's no way to tell which account is for which service. Only in the Authy UI can you tell. I had to follow the order of the JSON and the app, one by one, for my 700+ accounts, and verify that it works by going to the service site and testing the generated code from the new app, and also changing the email to a unique one. It took a whole week! Edit: to add, I wouldn’t recommend using Yubico or hardware-based ones unless you will have two or more replicas, losing them is easy compared to having your tokens backed up in an encrypted KeepassXC db for example. (1) https://gist.github.com/gboudreau/94bb0c11a6209c82418d01a59d... reply prophesi 2 hours agoparentprevFor Android, if you happen to use Keepass as your password manager, I really like KeePassDX[0]. If the camera app you use doesn't support QR scanning, though, you'd need an app for that (and I don't think any FOSS camera apps implement this, as for as I can tell). This one[1] seems the most up-to-date, by a German research group. You'd share the link as text to the KeePassDX app, search for the entry it's for, and it populates it with the HTOP/TOTP secret. There are iOS Keepass clients that support this as well, though from what I can tell there's some drama with source code[2][3] in the landscape. [0] https://f-droid.org/en/packages/com.kunzisoft.keepass.libre/ [1] https://f-droid.org/en/packages/com.secuso.privacyFriendlyCo... [2] https://github.com/MiniKeePass/MiniKeePass/issues/606 [3] https://keepassium.com/articles/keepass-apps-for-ios/welcome... And other allegations under the ethics & transparency sections of KeePassium's list of iOS alternatives https://keepassium.com/articles/keepass-apps-for-ios/ reply pnw 1 hour agorootparentI started with Keepassium but ended up with Strongbox which has been great. reply mrb 3 hours agoparentprevI use andOTP https://github.com/andOTP/andOTP and my favorite feature is the database of 2FA can be backed up PGP-encrypted and reimported on another device. But sadly it is no longer maintained. The latest version on Google Play Store is from 2021 and can still be installed and works fine on Android 14. reply MaxMatti 3 hours agoparentprevI used Aegis for a while and really liked it, switched to Bitwarden now but the UX was better reply hypeatei 2 hours agorootparentI use both and make offline backups regularly. reply bobbylarrybobby 3 hours agoparentprevI'm of the opinion that it's basically fine yo store them in your password manager. Yes if your password manager is broken into you lose everything (same as having no 2fa in that case), but you still prevent people from guessing your password and often avoid having to deal with email- or text-based 2fa. And if your password manager is broken into, there's a good chance your device has been broken into, in which case it doesn't matter where you store your 2fa. reply brightball 3 hours agorootparentI mix it up and store some 2FA on different apps. When it’s not a system I’m deeply concerned about I will just use the 2FA on the password manager. reply nwhale 3 hours agoparentprevIf you do not need QR codes, oathtool is great. You can protect your tokens, recovery codes etc. with gpg -c or similar, so the encryption is entirely separate from the authentication mechanism. And you actually know what is going on. Works for GitHub. https://www.nongnu.org/oath-toolkit/ reply notatworkbro 3 hours agoparentprevI've implanted my 2FA token in my arm and just hope it never breaks :D reply fragmede 3 hours agorootparentWhich one did you get? Did you get the Apex Flex from Dangerous Things? How do you like it/how was the process? https://dangerousthings.com/product/apex-flex/ reply TheBozzCL 3 hours agoparentprevI use a YubiKey with their Authenticator app. reply WanderPanda 3 hours agoparentprevI‘m using Raivo. It hasn’t let me down, yet reply pxeger1 3 hours agorootparentRaivo was bought by a shady developer last year and is no longer open source. If that wasn’t enough, a few weeks ago they released an update which deleted all your codes - failing at literally the one job a 2FA app has! reply mm263 3 hours agorootparentprevThe same Raivo that was sold to some shady dev who proceeded to delete all of the OTPs that I had in the app? https://www.reddit.com/r/privacy/comments/1d3zqvv/raivo_auth... reply yakito 1 hour agoprevWe should have something similar to Apple's hide my email for phone numbers reply 29athrowaway 2 hours agoprev> due to an unauthenticated endpoint. This is truly unacceptable for an authentication product. An authentication product that doesn't implement authentication correctly in their own APIs? reply flutas 2 hours agoparentIMO: I'm pretty sure this is less of an auth issue, than it is a rate limiting issue. I haven't been able to find anything about the endpoint, but based on the data exposed[0] I think the endpoint they are talking about is the register one which requires a phone number. I'd bet they didn't rate limit it, and someone just blasted through all phone numbers with it and stored the data for ones that didn't error out. [0] The CSV data columns: account_id phone_number device_lock account_status device_count reply awahab92 1 hour agoprevwhat do people use instead of twilio today? they make 2dcp verifications take too long reply okokwhatever 2 hours agoprevI still remember how hard was the process to be hired in this company. Maybe just a mask to hide the sad truth. reply delduca 2 hours agoprevI never trusted them, I hated the fact of having to use SMS. reply blackeyeblitzar 4 hours agoprevAuthy makes it hard to migrate away. Anyone know how to get the seed of the 2FA codes? Is there really no export option? reply prevent6672 19 minutes agoparentI thought I had a lot of totp codes to migrate but then it turned out I didn't use many of them. After deducting them, there remained 10 apps that I needed to migrate. It took me an hour to port them to bitwarden manually. reply slightwinder 3 hours agoparentprevSome months ago, I used https://github.com/alexzorin/authy to export them. It basically creates a dummy-device to access the tokens, and then exports them to some format. But I have not figured out how to import them now into another app. reply hypeatei 2 hours agorootparentUse the plaintext export option on that project. Most TOTP apps should accept the URIs that are exported. Maybe not en-masse but individually for sure. reply slightwinder 2 hours agorootparentAh, thank you, that worked in Aegis. I just missed the option for plaintext because of the long list of supported apps. So all it needs is a textfile with one otpauth://-entry per line and it imports them all at once. reply hipadev23 3 hours agoparentprevI slowly migrated away from Authy when they decided to shut down their desktop authenticator. You can painfully export codes, though I generated new 2FA codes at every vendor. reply conception 3 hours agoparentprevMaybe? https://gist.github.com/gboudreau/94bb0c11a6209c82418d01a59d... reply hypeatei 2 hours agorootparentAuthy desktop is no longer available and you need a specific version. reply tamimio 2 hours agorootparentI had that exact needed version when I migrated, if you need it, I can look it up, but there’s a slim chance that I deleted it. reply deegles 3 hours agoparentprevYou'll have to reset them one by one. reply drooopy 3 hours agorootparentI finished that process recently for 50+ accounts. It's something that I would definitely wish on my worst enemy. reply tamimio 2 hours agorootparentHa! when I finished mine, I actually bought myself some treats and snacks for celebration. reply localfirst 3 hours agoprevThere really has to be steep repercussions for companies that fail to protect user data like this. At this point I can't help but feel that there is wilful neglect with the aim of exfiltrating data with unknowable aim. Our digital data must be recognized as human rights but lately the world has been vocal about it but silent when it comes to action and enforcement. More and more reason why people no longer trust cloud hosted solutions. Offline-first, local-first with optional data sync is the only path forward to combat violation of our rights to our own digital data. Case in point, feeding haveibeenpwned with a bunch of HN user handles reveal a good chunk of you aren't even aware your data has been leaked, especially ironic since I see comments from those handles are very anti-regulation when it comes to user data ownership. reply cj 3 hours agoparentI agree the US in particular should have better data protection laws and consequences. But phone numbers aren’t something I’d consider confidential in most cases. Hell, we used to publish our phone numbers in physical books and give them to the whole town for free (literally). The data was even monetized with ads plastering every page. I guess the digital age isn’t all that different from the analog age (in certain ways!) reply olyjohn 2 hours agorootparentWe didn't use phone numbers to prove our identity back then. It was only used to call you. You often wanted it to be public so you could be reached. Now it's a critical piece of information required to access services online and prove who you say you are. reply localfirst 2 hours agorootparentprevthat was before internet now phone number leaks can be way more troublesome due to the way all of our data is connected to it via 2FA reply blackeyeblitzar 1 hour agoprevWhat’s a better 2FA product that is E2E encrypted and lets me export the seeds? reply exabrial 1 hour agoprevThat app is so dumb. Completely negated the usefulness of TOTP. Needs just to die already. Some executive over at Twilio signed the check for Authy acquisition and is still trying to justify the expense. reply Dma54rhs 4 hours agoprevHow to confirm if my number was one of the leaked ones? reply sofixa 3 hours agoparentI suppose https://haveibeenpwned.com/ will add the information when it can be verified. reply simcollect 5 hours agoprevHow come companies don't care about encrypting their users' data in their databases? It's been possible for a very long time now. Yet, companies keep leaking. And people keep sleeping. reply sethammons 1 hour agoparentWhy would that have helped? The endpoint was exposing the data, not the database. The endpoint would have simply decrypted. encryption of data at rest is for hard drives that walk off, not for access. reply infecto 6 hours agoprevGood motivation to stop using Authy. reply fauigerzigerk 4 hours agoparentWhat is a good alternative? reply imrehg 4 hours agorootparentBesides all the other advice of using the password manager as a 2FA store as well, on the stand-alone side there is Aegis. I have good experience with it, and allows better interoperability than Authy as well. reply attendant3446 3 hours agorootparentprevAegis (Android), supports automatic backups. There is also Ente Auth (it's been mentioned on this site), but I haven't used it much. reply haswell 3 hours agorootparentprevOn iOS, I’ve been using “OTP Auth”. While it’s nice that password managers can handle this as others have mentioned, the whole point of a 2nd factor is to ensure an attacker can’t get in if they somehow get your password. Storing the second factor along with the 1st factor doesn’t make much sense to me. reply infecto 4 hours agorootparentprevMost likely whatever password app you use supports these now. I know for myself, I started using Authy long long ago when there were not really many options. In my case, 1 Password can do this now. I believe the same is true for Bitwarden and Apple passwords. reply fauigerzigerk 4 hours agorootparentI hesitate to use the same app for both authentication factors. The reason why I started using Authy a long time ago is that it supports multiple devices and isn't linked to any other account (such as Google or Microsoft). reply lozf 3 hours agorootparentprevAlso KeePassXC -- if you don't like the idea of 2FA codes being in the same db as passwords, it's straightforward to use a separate db for 2FA only. Manage your own sync between devices with syncthing, dropbox or whatever you prefer. reply sofixa 3 hours agorootparentprevPersonally I dislike the idea of putting the other factor(TOTP) alongside the main two ones (email/password). Kind of ruins most of the purpose of TOTP and MFA in general. reply cess11 3 hours agorootparentprevI'll join the choir and recommend Aegis. It's slick, got features, code on Github. reply moffkalast 1 hour agoprev\"Company who thought they'd lost all public trust loses last additional bit of trust they didn't even know they still had, more at 11.\" reply ilrwbwrkhv 2 hours agoprevJesus fucking Christ. Can these companies learn how to write software? Quality is dropping like dogs. Twilio used to be a good company and now they are utter shite. Such a shame. Leetcode and bad hiring practices have done this to our industry. reply sethammons 1 hour agoparentNeither bad hiring not leet code is a problem with Twilio properties in my experience. Quality however, that gets railroaded by \"deliverables\" -- the problem is craftsmanship is hard to maintain and manage as companies scale while priority shifts to product announcements. reply ilrwbwrkhv 54 minutes agorootparentThere needs to be penalties. Massive penalties for breaches like this. That is the real problem. Nothing will happen to Twilio even though they caused such loss. They need to suffer economically for this, then quality will improve. reply rvz 6 hours agoprev [–] My goodness, for the 100,000th time, just stop using phone numbers for 2FA. (I know you won't anyway) There are no more excuses other than asking for your phone to be sim-swapped and your bank accounts or your wallets to be drained by call centers. If this breach doesn't scare you from using phone number for 2FA, then maybe nothing ever will and AI and deep fakes will make this even worse. reply AceyMan 5 hours agoparentAuthy doesn't implement SMS 2FA (how could it). A phone number is part of your user profile for registered mobile devices hosting the app. reply ceejayoz 3 hours agorootparent> Authy doesn't implement SMS 2FA (how could it). https://www.authy.com/integrations/ssh/ \"Someone in your organization doesn't have a smartphone? We got you covered. Authy SSH can send them the token via SMS or a phone call.\" reply Justin_K 5 hours agorootparentprevEven worse... Sounds like phone number is irrelevant, yet they collect it. reply oldmariner 5 hours agorootparentHow else are they going to track people with a hard-to-change identifier? reply Terretta 5 hours agorootparent> How else are they going to track people with a hard-to-change identifier? Using the device advertisee ID that the user is entitled to change. // Sorry, for a moment I thought you were serious. reply prng2021 4 hours agorootparentI just did some quick research on these IDs. Correct me if I'm wrong, but it seems like each user account would be tied to one device. It also seems like the user, at least on Apple devices, has to opt into advertising tracking in order for your app to even get access to this. Ignoring the security pitfalls of phone numbers, it really doesn't seem like these advertising IDs are a drop in replacement for using phone numbers. reply jokethrowaway 5 hours agorootparentprevIt's used to store and retrieve your 2fa secrets in case you lose your device reply Terretta 5 hours agorootparent> > Even worse... Sounds like phone number is irrelevant, yet they collect it. > It's used to store and retrieve your 2fa secrets in case you lose your device The phone number doesn't store anything? But if somehow knowing that phone number is a key to getting your 2FA secrets, you'd have a bigger problem. Except it often is, and that's the problem. reply ezekg 4 hours agorootparentDo what I do and turn off \"allow multi-device.\" Problem solved -- even if your phone number is stolen, they can't recover your 2FA because it's locked to the device too. reply FabHK 2 hours agorootparentYou can enable multi device, and have it on multiple devices, then disable it. https://authy.com/blog/understanding-authys-multi-device-fea... reply ezekg 1 hour agorootparentYep. I've done this. Lots of people I know use \"burner\" phones without cellular for 2FA. reply rvz 5 hours agorootparentprevThat is brilliant news for SIM swappers and criminals now that they can gain access to your codes directly with your phone number! A terrific reason to avoid anything Twilio / Authy reply Ayesh 3 hours agorootparentIn fairness, you cannot. It requires a backup password. reply tamimio 2 hours agoparentprev> for the 100,000th time, just stop using phone numbers for 2FA. I agree, and I say this to whoever asks me too, and I avoid any services that still use phone numbers as a way to associate it to you (Signal, I’m looking at ya!) However, easier said than done, some services still require you to use a phone number, like banks, some government agencies, insurance companies, etc., the services that actually matter if your data get leaked. I believe there should be a regulation to prevent using the phone in any way to confirm your ID, and never force you to provide one to access such services. reply ezekg 4 hours agoparentprevIf you use Authy, turn off \"allow multi-device\" and SIM-swapping isn't an issue. This should be on regardless of the leak. reply SketchySeaBeast 3 hours agorootparentBut one of the selling points for me was to allow multiple devices so that if one broke I'd still have access. reply FabHK 2 hours agorootparentYou can enable multi device, and have it on multiple devices, then disable it (and keep it on multiple devices - it's just that then adding yet another device needs toggling multi-device on from an existing device, a confirmation SMS is not enough). reply SketchySeaBeast 1 hour agorootparentPerfect. I can just toggle it on when I add another device. Thank you, great solution. reply greenchair 3 hours agorootparentprevpeople with this use case would need to be comfortable taking on the extra risk. reply k8sToGo 2 hours agoparentprev [–] It doesn’t scare me because in Authy you also set a password which without you cannot access the codes. The phone number here just acts as a username. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Twilio has confirmed a data breach that exposed the phone numbers of 33 million Authy users, leading to increased spam calls and concerns over the reliability of traditional phone networks.",
      "Users are considering alternative communication methods such as FaceTime and Zoom, while also emphasizing the critical role of phone calls in essential services like healthcare and social services.",
      "The breach highlights the need for stronger data protection, better enforcement of anti-spam measures, and recommendations for alternative two-factor authentication (2FA) apps like Aegis, Bitwarden, and Yubikey."
    ],
    "points": 260,
    "commentCount": 148,
    "retryCount": 0,
    "time": 1720095999
  },
  {
    "id": 40872182,
    "title": "The saddest \"Just Ship It\" story ever (2020)",
    "originLink": "https://www.kitze.io/posts/saddest-just-ship-it-story-ever",
    "originBody": "Zero to Shipped - Master Fullstack Development and finally ship a product K Kitze Products Gear Social Posts Talks Podcast Newsletter Jobs 4 years ago The saddest \"Just Ship It\" story ever #Dev #Startups #Productivity #Motivational I know, I know, at this point you want \"Just Ship It\" to be an actual person so you could punch it in the face. As an Indie Maker, that sentence can be super frustrating because it's tired, it's cliche, and your response is always \"HEY! You don't understand... it's not that easy\". I agree, it's not easy, but it's always the right thing to do. Here's why. I started building an app on 01.01.2018. It was New Year's Eve and we just had the crappiest night ever. Yes, imagine a night so bad that at midnight you decide \"you know what, fuck it, I'm gonna work on WEB DEVELOPMENT\". That bad. The MVP was ready in a few days. I'm not that good of a coder, it's just a simple app. The 0.0.1 alpha version was more than ready. I could've released it, share it with a couple of people, and call it a day. I could've done that with every single version that I made, at any point from 2018 until now. I just wanted to add one more thing. One more feature. Just this one more thing and people will like it. One more screen and everything is gonna make sense. I swear, just this one last thing and it's ready. BAM, last moment decision from the world's biggest dumbass: \"People wouldn't use this if it doesn't have a proper native mobile app for it. Time to learn React Native and spend a few months on that 🤦\" God, if time machines were real, past-Kitze would be shoved in a toilet so hard right now. After 2 years of development, juggling between the fucking horror that's the web platform, React Native, Expo, GraphQL, bitching about how there's no ideal tech stack, the good old jQuery and Filezilla days, switching to other projects, releasing other apps, losing passion, finding passion, coming back to the app, etc. etc. etc... I just dropped it. I was still using it but I stopped developing it and just dropped the idea of releasing the app, ever. After a while, I was using it, but I realized that I'm missing a lot of features, so I'd either have to go back to developing it, or I'd have to find an alternative. And boy did I find one. I was scrolling their landing page and I was happy and furious at the same time. Someone solved the problem that I was solving. It was like someone literally read my mind and started coding. WHAT. I have previously sent a video of my app to a couple of people (closest I came to shipping it) so I started getting suspicious if someone actually shared the video of my app with these people because they were solving literally the same problem, and they most of the features that I had. I started getting this overwhelming happy, sad, and panicky feeling. I literally cannot explain how I felt while scrolling their page. One moment I am scrolling their list of features giggling like a little kid with a 48$ bill in a candy store (yea I know 48$ bills don't make sense, but JavaScript doesn't make sense and you're still using it), one moment I want to find these people and THROW THEM IN A PIT OF LIONS. FUCK. It's not their fault. I was just slow. I didn't ship on time. I'm gonna go ahead and tattoo \"JUST SHIP IT\" on my forehead. Nah I wouldn't be able to see it there. On my arm maybe. Nvm, let me go back to scrolling their landing page. Fuck. They have solved everything that I wanted to solve, and WAY more. Hey, maybe I should be happy? I don't have to code anymore. Yay!? No more web platform? BLISS. Oh crap... the world is never gonna see my app though. But at least I don't have to see React Native anymore. NICE! Wait... BUT I WASTED SO MUCH TIME ON IT. FUCK. A bunch of mixed feelings. Here comes the saddest part, so grab a pack of tissues. After a little bit of hesitation, I made an account. I watched the videos in their help center. Every time I caught myself smiling about a clever way they implemented something I slapped myself. NO. Bad Kitze. You shouldn't like this. THEY'RE COMPETITORS. sigh Sure buddy, whatever you tell yourself. Competitors to a shitty codebase sitting on your hard drive. For 2 frickin' years, I thought it's too early to release my app because it's clunky, buggy, it's missing features, blah, blah, blah. No one would ever use it, right? I was so wrong. I started using their app. Even though they were working on it for the past few years it's still slow, buggy, and super unpolished, it doesn't matter, because they shipped. Their mobile app is terrible and it needs 10 seconds to sync. It doesn't matter, they shipped. And I'm looking forward to every single update they release. Their backlog of things to do is huge, but it doesn't matter, they ship every single week, and the app is growing along with the community. \"But Kitze, even though tHeY sHiPpEd no one would pay for something unpolished and broken, right?\" Oh, Indie Hackers. So clever, yet so naive. Today my 30-day trial has expired. A tear rolled down my cheek for every single digit of my credit card that I entered in their app. I am officially not only a subscriber, but also a fan. Every time I'll get a payment notification it's gonna feel like stepping on a lego ... glued to a knife. My bank might as well change the notification from \"You have paid 5$ to ThatCompany\" to \"You never shipped, loser\". My app is officially dead. 99% of you are in the same boat right now, but hopefully just a few weeks into your project. Don't be a dumbass like me. Take a breath, roll your eyes at the cliche saying, but please... Just Ship It. 2024 update I actually decided to say fuck it and still shipped my productivity app in 2022! Why? No competitor app was coming even close to my vision. I wanted an app that combines Todos, Habits, Planner, Goals, Pomodoros, Meal tracking, Fasting, Hydration, Packing, Trips, and many many more features. It also has a public timeline where people share their successes and achievements while improving their life. If you are serious about self improvement, it would be amazing if you join our app and community! Check out Benji - The Life OS! More posts GitHub stars won't pay your rent GPT-3 is the beginning of the end I made 🟣 Sizzy The Browser For Developers 🐶 Benji The life OS 🚢 Zero To Shipped Interactive video course on mastering fast-paced Fullstack Development.",
    "commentLink": "https://news.ycombinator.com/item?id=40872182",
    "commentBody": "The saddest \"Just Ship It\" story ever (2020) (kitze.io)217 points by thunderbong 15 hours agohidepastfavorite263 comments Culonavirus 13 hours agoThere's massive caveat with all of these \"and then the story ends because we didn't just ship it :(\" stories: sometimes the value of the \"app\" you're working on is in the technical details that cannot really be \"hastened\" and you can't \"just ship it\". Also, and this is something a lot of the managerial class people don't want to hear: Your job as a sw engineer/architect is to resist the \"just ship it\" pressure from the management as much as possible. So unless you own what you're coding and you really need it out of the door for your own benefit, if more time makes your work more professional, then take more time. Anyone telling you otherwise is a 100% hack. You are not an automaton that takes in JIRA tickets and spits out hacky code as soon as possible. Or at least you shouldn't be. Not to mention that not taking time (doing things properly) is incredibly taxing on your psyche and you WILL burn out. There are only so much garbage tasks you can take. It's worth repeating: Unless you have a stake in the company, it is NOT your job to make sure the company is the most profitable it can be. Your job is to create great software. What's great software? The kind you'd be willing to put on your resume without feeling bad. This is the thing that will ultimately make you feel good about the work you're doing. Hitting that arbitrary deadline for a 1425474th time may feel like a relief but it's short term and a form of negative motivation - and in the workplace, those NEVER work over a long period of time. So RESIST that pressure from the top and do your work properly. If they fire you, then who cares, the only way up these days is job hopping anyways. reply vouwfietsman 12 hours agoparent> It's worth repeating: Unless you have a stake in the company, it is NOT your job to make sure the company is the most profitable it can be. Your job is to create great software This is a terrible take, and one I generally see as a signal of lack of seniority in a software dev. It is absolutely everybody's job in the company to make sure it is profitable, unless you work for a non-profit. The thing is though, that profitable is not the same as \"just ship it\". If you're in a company where those things are conflated frequently, that's a sign of lack of seniority in management. > Your job is to create great software It really isn't, your job is to make a great product. Making a great product often, but not always, requires great software. Many great products have terrible software behind them. The tension between product, sales and development should result in a compromise that creates the most value short, middle and long term. It is 100% your job as a developer to understand what can and cannot be hacked, what priorities the company has beside delivering great software, and finally: when great software must be made, because compromise is unwise. Just like the boy who cried wolf, the developer who can never compromise on software quality is powerless when there is an actual reason to not compromise on software quality, and rightfully so. reply fabioborellini 12 hours agorootparent> This is a terrible take, and one I generally see as a signal of lack of seniority in a software dev. It is absolutely everybody's job in the company to make sure it is profitable, unless you work for a non-profit. Developers aren't compensated for any extraordinary achievement, though. Unless they own a notable share of the company. So, why should they give everything and get nothing back? How is that fair? Salespeople and managers usually consider technical guys pariah. They can always be outsourced or otherwise replaced, and they can be blamed, for example for failing to meet impossible targets. If there's success, it always is a manager's achievement. Developers also get roughly the same pay for both working minimum and for screwing their lives and healths on powerpoint-driven death marches. The compensation for extraordinary success goes to shareholders, and maybe managers or even salespeople. reply IanCal 3 hours agorootparent> why should they give everything and get nothing back They aren't giving everything they're there to do a job and that job is make the company profit. The person you are replying to isn't saying \"you need to go the extra mile to make profits\" they're saying \"you should be focused on making money for the company, not on personal preferences in your code\". reply hinkley 1 hour agorootparent> They aren't giving everything they're there to do a job and that job is make the company profit. But GP is saying everyone except the devs is there for status and fat bonuses. You’re talking like a defector. reply portaouflop 11 hours agorootparentprevMy experience working in this industry is very different and devs do get recognition and sometimes bonuses (if certain goals are hit). Devs are paid a premium, have generally much more relaxed work schedules than managers or any customer facing role. If you screw over your health working in a toxic environment that’s on you IMO - as tech worker you probably have more opportunities to improve your working conditions than the vast majority of humans reply freilanzer 11 hours agorootparentI have never known a software developer who got a bonus. Rarely even recognition. reply linuxlizard 2 hours agorootparentI came to work over a vacation to fix a critical customer issue. The company rewarded me with a $50 gift card. reply snozolli 1 hour agorootparentprevI worked with a guy who got a sizable bonus (rumored $200k). The reason? An old college buddy called and asked if our technology could help their social media company with a problem they had. So, if you want a bonus as a software developer, be a sales rep by happenstance. Probably helps if you went to a prestigious university where you met people who went on to prestigious roles... reply smackeyacky 9 hours agorootparentprevMerchant banking. I miss those bonuses reply astura 6 hours agorootparentprevI get two bonuses every year - one for individual achievement and one for profit sharing. reply pdimitar 3 hours agorootparentYou and every other programmer on the planet then? Or, what is your comment meant to claim exactly, beyond the extremely obvious \"there are exceptions to the rule\" trope? reply portaouflop 3 hours agorootparentWell the original notion here was > Developers aren't compensated for any extraordinary achievement That as a blanket statement is just not true. Of course it does not apply to every single developer, I would even say it doesn’t apply to the majority. reply pdimitar 3 hours agorootparentIt is true when it applies to the majority (and nobody said something is only true when it applies to 100% of a group because then absolutely nothing would ever be true when it comes to people). If you think otherwise and are convinced of it then you are very privileged -- and I actually envy you. You have no idea what contractor programmers go through out there, apparently. reply collingreen 2 hours agorootparentThis is too angry of a reply to someone adding their personal experience onto your comment about how the world is. I appreciate their anecdote refuting the claim; I think it makes a better discussion than just superlatives so I think you should get back to your point (which I mostly align with) instead of attack them on \"just because you showed an example where my rule isn't true doesn't make my rule not true\". reply pdimitar 1 hour agorootparentSo are you saying my point was so obvious that it was unnecessary to be made? reply OvbiousError 3 hours agorootparentprevMaybe this is a US vs Europe thing? I'm in the latter region, where I am there's a ceiling for dev salaries, virtually the only way to break through is to become a manager. It's also very uncommon to share the companies success. reply hinkley 1 hour agorootparentOnly twice have I worked for a manager who thought it was unethical to ask the devs to work hours they themselves would not work. The notion that devs have better hours than managers is either a fiction or conflating managers with founders. Who do often work ridiculous hours and expect everyone else to do it too - and enthusiastically - even though they’re the only ones with a stake in the outcome. reply Elinvynia 5 hours agorootparentprevMake the company 100 millions and you are lucky to see more than 10k of it. If you believe otherwise I would question your real life experience. reply portaouflop 3 hours agorootparentI’m not saying it’s perfectly proportional to the value created, I was challenging the notion that devs are these wages slaves chained to their desks who never get anything extra while their managers swim in money reply rustcleaner 2 hours agorootparentprevWhile ad hominem is a fallacious way to handle a debate, it is a great way to determine motive for and if debate is even worthwhile. reply fabioborellini 11 hours agorootparentprevMaybe that depends on the job market you're working on. In some countries salaries are more affected by personal performance than in others. My first job as a developer made me work around the clock, day and night, for a below average (nationwide compared) salary. And I got yelled at. Since I didn't have enough work experience, my applications weren't responded to. There were no ways to improve conditions without changing jobs. reply throwaway84751 10 hours agorootparentIt sounds like a toxic workplace, but you should do yourself a big favor and try to put that behind you. If you keep that mindset in your new job, your perspective will be negative and you risk burning out. If you work at a traditional workplace, think of yourself as a professional. Do the best work you can during business hours, without ruining your health. The company is paying you to make decisions that are best for them, and you may not like it but that is what you agreed to. At the same time, they only pay for your work and not your soul. You need to keep a balance between work and you as a person. Unfortunately there are many toxic workplaces out there, and if I ended up \"trapped\" with no escape, I would probably consider things differently. But when you finally escape, keep an open mind and try to put your bad experiences behind you. reply pdimitar 3 hours agorootparent> It sounds like a toxic workplace Like most workplaces on the planet. Check your bubble. I agree with your advice but you have to understand that many people start families and that tilts the table in favor of the employer; people become very risk-averse and dare not refuse anything. This is a fact and it's happening every second to dozens, if not hundreds, of millions of people out there. > Unfortunately there are many toxic workplaces out there, and if I ended up \"trapped\" with no escape, I would probably consider things differently. Please do. Living paycheck to paycheck is the reality of most working people. reply segfaltnh 1 hour agorootparentIs it the reality of most software engineers though? I recognize I live in a wealthy part of a wealthy country, but it hasn't been particular hard to be an SWE, I live well below my means. reply pdimitar 1 hour agorootparentYou live alone? I have a family. That introduces a rather big difference in monthly expenses. reply kkarakk 1 hour agorootparentprevYou just sound like you're venting and depressed tbh, if you're in STEM and working \"paycheck to paycheck\" then either there is a skills mismatch or you refuse to improve in some critical way that has been addressed to you already. this reply goes for everyone complaining about jobs really, everyone has strengths and areas where they can grow- so focusing on these can lead to greater job satisfaction. reply pdimitar 1 hour agorootparentThanks for your assumptions. reply fabioborellini 7 hours agorootparentprevI'm sorry to say but I'm very much burned out already. It all went well for a while after switching jobs, but the lockdowns and remote work ruined what was left of my life. My friends noted that they don't miss me, my company switched to completely remote etc. I can now only sleep and work, I can't even think of anything else anymore. I did put that behind me after switching jobs. My initial outlook and attitude on professional life was that I need to try hard and stay positive and help everyone. I expected other people to work for common goals, put their own goals aside, and be friendly. As in school, that mostly lead to abuse. I'm only advocating against trying too hard because I've hurt myself with it. As a child, I was taught to go the extra mile, work in a sustainable manner and never blame anyone. At work, I have never refused to work on a problem because it's not my fault or someone else would be a better fit for it - and that makes me a very good scapegoat and a fine target for impossible requests. Guess what happened? I'm right now stuck with a person who is asking daily for something I have explained to be impossible, and does not accept my answer. So he asks again. I have no idea what to do. I can't set boundaries or take care of myself, so I'm a bad employee and a bad person who doesn't try enough. Great. reply throwaway84751 4 hours agorootparentIt sounds like you have ended up in a bad situation. There's no real suggestion I can provide for how you can improve your situation, other than to try to find a way to disconnect. (Meetups, walking, museums, hobbies). Maybe that means talking to a professional therapist to find out how you can handle the situation. They get paid to help you, and there is no shame in that reply pdimitar 3 hours agorootparentThey usually get paid by the hour so they have a vested interest to talk BS and prolong the session as much as they can. reply unplug8224 2 hours agorootparentprevA good therapist can help you with this. reply jve 10 hours agorootparentprev> So, why should they give everything and get nothing back? How is that fair? Ofcourse it is fair. How can you say you get nothing back? They pay you salary you agreed on when signed contract. > My first job as a developer made me work around the clock Bad employer. Maybe doesn't obey the contract on their side and doesn't pay for overtime. Tough one until you level up your experience for sure. But I'd expect salary to be below average in your first job. You still don't have the necessary experience. And when you do, you CAN find another job. Just don't have bad attitude towards your job. You will be rewarded one day for being a honest and productive worker. reply pdimitar 5 hours agorootparent> You will be rewarded one day for being a honest and productive worker. 22.5 years later I can confidently say you are living in a comfortable bubble and have no idea what do most programmers go through every day. reply fifilura 3 hours agorootparentI would say that their view is akin to religion. Any religion really. And religion is not always bad. It can help us stay focused, and selfless for some unknown greater good. That greater good does not have to exist. But decreasing the focus on self can still be good for your soul. reply pdimitar 1 hour agorootparent> But decreasing the focus on self can still be good for your soul. Only applies to people who mostly focused on themselves. I'd argue that most working people have the opposite problem: they have to focus on everybody else's problems but not theirs. I'm only working towards tangible greater goods. Including my own inner peace. reply aydyn 9 hours agorootparentprevi think whether it is fair isnt the right question (which is subjective and I disagree anyway) but whether it is rational and advantageous for you. What benefit do you get for rushing a job and shipping shitty code? I think the downsides vastly outweigh the upside. If a company is that pressed for time, theyre not going to fire you. Take your time and do things right, dont Boeing it up. reply achenet 11 hours agorootparentprevI'm sorry to hear that. I had a similarly shitty first job, although I was lucky to be in France, which was very strict worker protection laws, so \"working day and night\" was 'only' 10 hour days. However, after like 8 months there I was able to get another job - it doesn't take much experience for recruiters to start reaching out on LinkedIn aparantly. reply fabioborellini 10 hours agorootparentWe have the laws in place in Finland, too, but they aren't enforced in practice. So, people who refuse to obey them get an advantage against people with integrity. The workers protection authorities are not resourced to do any individual checkups, and going to court against a company would take years and possibly leads to lifetime in debt. So the practical way to resolve this is to change companies. reply sveme 11 hours agorootparentprevIf that is your experience with companies and managers, you have to choose your jobs more wisely. In good companies managers were engineers at some point as well and know what is important technically and motivationally. reply chickenchase-rd 10 hours agorootparentprevIts rare in any field to be compensated for any extraordinary achievement. But these are the people I like to work with. reply fabioborellini 10 hours agorootparentSurgeons save lives which they may find important and motivating per se. Grinding meaningless Javascript to deliver more advertisements and conflict to people is neither motivating nor important, or at least it shouldn't be to a responsive person. reply throwaway7ahgb 7 hours agorootparent1) Not all surgeons save lives 2) Not everyone can be a doctor We can all find meaning in our own work. reply ekianjo 10 hours agorootparentprev> get roughly the same pay where? reply bbarnett 11 hours agorootparentprevDevelopers aren't compensated for any extraordinary achievement, though. Unless they own a notable share of the company. So, why should they give everything and get nothing back? How is that fair Get nothing?! What a strange way to view a paycheque. \"Hi, you're only paying me, but that's not enough to expect a solid work ethic. Instead, I'll make sure my work is just passable. Want more, and now you have to pay me more!\" Where I come from, \"extraordinary achievement\" is just \"doing your job\". (Are you advocating something else? I'm not suggesting free overtime, just doing the best job you can.) reply strken 8 hours agorootparent\"I'll make sure my work is just passable\" is a very strange interpretation of the original statement, which was \"if more time makes your work more professional, then take more time.\" You are not being paid enough to rack up tech debt during 80 hour weeks constantly moving from one sales-driven project to the next, because that's a stupid way to develop software and it'll burn you out after a year of back-to-back \"why isn't XYZ done?\"/\"why didn't you make XYZ not buggy?\" meetings, at which point you'd better have made enough money to retire to the Bahamas. reply freilanzer 11 hours agorootparentprevYou get a salary for doing the expected work, not extraordinary work. reply fabioborellini 10 hours agorootparentExactly. Do the expected work well, but don't sacrifice yourself so that other people can use, abuse and eventually desert you. reply JacobThreeThree 10 hours agorootparentHaving the interests of the bottom line of the business in mind does not equate to sacrificing yourself. Do what's expected in the interests of the business, not in pursuit of some \"great software\" ideal. reply pdimitar 3 hours agorootparent> Having the interests of the bottom line of the business in mind does not equate to sacrificing yourself. Only in theory. In practice, the incompetent leadership leads to those naturally being identical, as in \"we have to deliver project 17 for this year, please do your best!!!\" and ad infinitum. > Do what's expected in the interests of the business, not in pursuit of some \"great software\" ideal. Who mentioned this? Only you. A projection on your part, it seems. reply JacobThreeThree 53 minutes agorootparentThe original poster of the thread mentioned it. >Unless you have a stake in the company, it is NOT your job to make sure the company is the most profitable it can be. Your job is to create great software. What's great software? The kind you'd be willing to put on your resume without feeling bad. reply pdimitar 51 minutes agorootparentThought we were talking about the sub thread but okay, I'll give you that. You did omit the first part of my comment however. reply damethos 6 hours agorootparentprevWhat is this \"extraordinary work\" we are talking about here exactly? reply achenet 11 hours agorootparentprevthat scene from Office Space where he talks about \"I do just enough not to get fired\" may be relevant here. There is something to be said for 'if you're going to be a rational economic actor and you have a salaried job, the optimal strategy is finish your job's tasks as fast as possible and work on a personal project which you control the upside to with all your extra energy'. From a purely economic point of view, spending any effort beyond the minimum at a salaried job is a waste of effort - the expected value of that extra effort is nil, unless you own significant stock. This may be why many tech companies offer equity as part of their compensation. reply evilduck 10 hours agorootparentIf you're going to sandbag you should aim for slightly below average. Minimum leaves you no room for error. reply js8 10 hours agorootparentprevI call it (doing the bare minimum required) \"organizational laziness\" and I hate it. It might be rational, but it leads to mediocrity. reply pdimitar 3 hours agorootparentOK, hire me, and if I over-perform and help you achieve a business target, I want a triple salary next month. Or better still, 20% of the extra profit. No? Then you'll keep seeing what you call \"mediocrity\". I am a pretty good programmer and have literally saved businesses, several times over the course of my career. Never again though. A pat on the back is not enough of a reward. reply js8 3 hours agorootparentSorry to disappoint, but.. I don't want to hire you. I am a socialist, I find labor markets morally objectionable. The above is one reason, putting pursuit of profit above human excellence leads to mediocrity. reply rustcleaner 2 hours agorootparentWhat benefit does excellence derive for the excellent then, other than feel-good bubbly feelings? If labor cannot differentiate itself then collectively it will do the minimum acceptable and everybody will be mediocre (unless autism). reply pdimitar 3 hours agorootparentprevI too strive to be humanly excellent. I don't strive to make my boss' bonus bigger. Start your thought process by making the extremely obvious distinction between these two. reply rustcleaner 2 hours agorootparentprevStarts at the top. Lazy pay, lazy perks -> lazy workers. reply weweweoo 1 hour agorootparentThis. From worker's point of view it's irrational to work hard if the pay sucks. Good management should realize that, and demand less from underpaid workers. Embrace the culture of laziness, or pay a fair wage. reply aydyn 9 hours agorootparentprevwhats mediocre is willingly being a cog, in my opinion. Cogs dont get any credit. reply bbarnett 11 hours agorootparentprevNo, Office Space depicted a horrible office environment, most are not such. And minimalist work ethic is a poor one, regardless of hand wavy, trumped up, rationalizations. reply pdimitar 3 hours agorootparent> No, Office Space depicted a horrible office environment, most are not such. You are right. ...Most are much worse. :D reply achenet 7 hours agorootparentprevyou can be very hard working on things that aren't your day job. As I'm writing these words, an aspiring musician is probably sleepwalking through his day job because he was up all night practicing his music. Or Paul Graham, when he talks about writing the book On Lisp during his time at Interleaf [0] [0] https://paulgraham.com/worked.html reply piva00 10 hours agorootparentprev> And minimalist work ethic is a poor one, regardless of hand wavy, trumped up, rationalizations. That's a very protestant work ethic worldview. Doing what is expected from what you're being paid in the best way possible is not a poor work ethic, it's a pretty rational one. The other side of it where one always strive to do more, to go above and beyond what you're being compensated for, and so forth can also be a quite poor one. God is not going to give you extra points, for some people doing their best work at current expectations is good enough, no need to spend more energy than required on a job, there's more to life than working and accumulating. reply bbarnett 6 hours agorootparentTrying to bring religion into this is beyond amusing. I guess the Japanese are all protestants? Hardly. And rationalizing poor behaviour by saying it's rational is another good one. Lastly, you're trying to shift the discussion by claiming \"best way possible\" as opposed to others saying \"do the bare minimum\". These are very often not the same. The problem is, people don't \"get it\". There are people in this thread protesting about \"doing poor quality work\", eg, \"racking up tech debt\". Why? Because it eats at them. Because they are in this to build, and build that which holds, which has value. They have pride in their work! Yet the response some have here is simply don't do the best you can do. These two things are counter to one another! I am advocating that yes, do the best you can do. Take joy, deep internal joy in doing your job correctly, because of what you build. This indeed does not mean doing the bare minimum, by some broken, made up rationalized excuse. As I said, a good work ethic is not a protestant thing, it is a human thing. We can expand this to everything. What are you being compensated for? Are your ethics formed around what's profitable?! Madness! reply weweweoo 1 hour agorootparentWhat is seen as \"good work ethic\" is absolutely a cultural thing, and at least partially explains economic success (or lack of) in many countries. The Japanese aren't protestant, but they have other elements in their culture that encourage hard work as a virtue. I take joy in projects that I actually find meaningful. Being an underpaid cog in the machine, working on JIRA tasks visioned by someone else isn't meaningful. Lack of adequate pay makes me feel underappreciated, and frankly destroys any motivation I could otherwise have. So no, I'm doing the bare minimum and don't feel bad about it. I treat my employer like it treats me, that's called justice. reply achenet 4 hours agorootparentprevThat's actually a very interesting point of view. It's also interesting that you bring the Japanese into this. While they certainly to care a lot about producing high quality work, they also have one of the world's highest suicide rates. I understand taking pride in what you build. However: 1) it's important to not let that destroy the rest of your life 2) it's a lot easier to take pride in what you build when you're working on something you own[0]. As I believe I alluded to in other comments on this thread, and was kind of insinuating with the original comment that you replied to, deciding that extra effort spent on a dysfunctional enterprise project micro-managed by 3 competing orgs who spend their time changing requirements in order to win minor political victories (yes, this is an extreme example, please bear with me) is better spent on a personal open-source project, or even building something like a sport club or happy family seems to be the logical course of action when you care about what you build. Which isn't to say don't do the best you can at work - ship the code they ask you to ship, write it well, add unit tests, all that jazz. But then once that's done, you can either focus on being the best employee for Megacorp, which is likely to be soul-crushing, because you'll have extremely little reward for your effort, or you can be the best employee of You, LLC, where you natural human desire to make something beautiful can express itself in a way that is much more rewarding for you, both financially and emotionally. [0] https://paulgraham.com/own.html reply piva00 4 hours agorootparentprevIt's not beyond amusing, the protestant work ethic is the tradition from where a lot of nations have derived their work ethic from. Just like the Japanese derived their work ethic from their traditions, bringing it up is just clearing the way that yes, it's a religion-originated way of thinking about work, there's nothing wrong about that and you getting hung up on it is what's truly beyond amusing. > Lastly, you're trying to shift the discussion by claiming \"best way possible\" as opposed to others saying \"do the bare minimum\". These are very often not the same. Because the discussion gets murky exactly at this point. Doing the bare minimum means not going out of the way to solve issues for others, like Americans working outside of their working hours and bosses expecting that should be done. I have many work colleagues in the USA who are beyond annoying by trying to prove themselves by working outside of what they are paid for, with the thought they should go \"above and beyond\" instilled in their minds. It just creates issues for other cultures who do not prize themselves in sacrificing their lives for the job. The other side of it is doing the best work you are willing to do, with the limitations you currently have (skill, health [physical or mental], time, etc.), that's what I call \"bare minimum\" for myself. I won't be wasting my time trying to come up with new products, new paths of generating revenue, simply because I'm not paid for that, when I am in that spot I definitely offer the best help I can but I won't be fighting political infights, depriving myself of a life to work another 2h/day to setup a new project for some higher ups, and so on. > I am advocating that yes, do the best you can do. Take joy, deep internal joy in doing your job correctly, because of what you build. This indeed does not mean doing the bare minimum, by some broken, made up rationalized excuse. You don't need to take deep internal joy of doing your job correctly, at all, one just need to have a work ethic that doing your job correctly is the right thing to do, it's what I'm being paid for, and that's the bare minimum. If that means I can slack off a little bit because I'm aware I can deliver what's expected so be it. Perhaps we are talking past each other here because I do not disagree mostly with you, I probably just disagree with your approach to it (and hence what I called a derivative of the protestant work ethic). > As I said, a good work ethic is not a protestant thing, it is a human thing. Not necessarily, if your work is bullshit and you are not paid enough for it without much chance to do something else because of life's circumstances there's absolutely no inner motivation to have good work ethic. > We can expand this to everything. What are you being compensated for? Are your ethics formed around what's profitable?! Madness! Much the opposite, what's profitable is usually the least of my concerns ethically-wise, I would even say it is most times detrimental to ethical behaviour. reply ruszki 44 minutes agorootparentprevPersonally, I’ve never seen good developers who cared at all about profitability. All of them cared about the products and customers, but no one was driven by profit at all. Whom I met and cared about profit, they were mediocre the best. reply dclowd9901 12 hours agorootparentprevI 100% agree with you. I’m in a situation at the moment when I’m nearly constantly having to check myself against other devs. I don’t do hacky work and nothing I do isn’t extensible or easily refactorable. But I constantly get feedback from another dev on my team wanting me to polish out a change to perfection against every feasible possibility. I try to employ YAGNI reasoning to them, but they just have a perfectionist standard. Look: job 1 is making safe software, job 2 is making money and job 3 is perfecting the architecture. reply another2another 6 hours agorootparentAsk them to write the tests that will trigger the failure condition - i.e. make them prove that it's a possible thing that can easily occur. If it is, then at least you have a simple test case to work with so they've done a chunk of the work for you. If it's not, then they'll spend ages trying to craft a test case for a scenario that's extremely unlikely. People very often leave you alone when challenged to put in the work to prove their point. reply incanus77 10 hours agorootparentprev> It is absolutely everybody's job in the company to make sure it is profitable, unless you work for a non-profit. Common misconception, but a non-profit needs to make a profit, too. It just gets invested back into the business instead of distributed to shareholders, or in fact to any individuals. reply Earw0rm 6 hours agorootparentprevIt's more contextual than that. In some cases, ship vs not ship, profit vs not profit, is the difference between a company thriving and failing to thrive. In others, there are second- or third-order effects that render marginal profitability kind of irrelevant to the trajectory. This usually applies to either very large / institutional orgs, or situations where the business is leveraged by investors (VC or PE) such that the kind of honest profit earned by shipping an update or a new product won't meaningfully impact on the company's fate. In those situations, doing good engineering and cleaning up tech debt might make more difference to yours and your colleagues' lives, and maybe even your customers', than shipping. reply pdimitar 3 hours agorootparent> In some cases, ship vs not ship, profit vs not profit, is the difference between a company thriving and failing to thrive. That's their problem, not mine. I get paid a fixed amount. If I get paid the same + a percentage of outcome then I'll change how I work. Easy to understand, I believe. reply epicureanideal 2 hours agorootparentprev> It is absolutely everybody's job in the company to make sure it is profitable, unless you work for a non-profit. This attitude is more likely to get you fired than rewarded in most companies. If you step outside your role, or even lift your head up and peek around and offer your thoughts about what you see, you'll be at greater risk for no reward. Again, in most companies, not all. reply jampekka 11 hours agorootparentprevMaking a great product and maximizing profitability are often at odds. E.g. see all those dark patterns. reply katzenversteher 12 hours agorootparentprev>> Your job is to create great software >It really isn't, your job is to make a great product. Making a great product often, but not always, requires great software. I believe that depends on the role you have. If you're a software engineer I think you should try to create great software. If you're responsible for the product you can decide it might be better for the product to ship earlier or with the current state of software but I think you should not keep your software engineers from trying to make the software great. You can try to shift their focus on a different (software) topic that you think is more important if you don't like what they are currently trying to improve. reply vouwfietsman 57 minutes agorootparentDepends on what you mean with great software of course, but a developer should work together with the product team to come to a specific realization of the product vision. That vision may not always require great software, and it may actually specifically call for hacky software, but the product people cannot make that judgement call (and if you are in a healthy company, they don't). This is different from always making great software but shifting priorities. reply snozolli 1 hour agorootparentprevMany great products have terrible software behind them. Please give some examples of great products created by software developers that have terrible software behind them. reply fragmede 44 minutes agorootparentEarly Facebook was written in PHP, and regardless of what you think of them as a company, just look at their market cap. WordPress has a huge userbase these days and had similar roots. reply starfallg 12 hours agorootparentprev>It is absolutely everybody's job in the company to make sure it is profitable, unless you work for a non-profit. To generalise this, it's everyone's job to create value. This may or may not result in profit, but ultimately aligns with the goals of the organisation. reply Culonavirus 11 hours agorootparentprev> absolutely everybody's job in the company to make sure it is profitable Absolutely and categorically it is not. That is complete nonsense. Why should an employee care? Unless there's some profit sharing scheme in place that would benefit the said employee and they took advantage of that scheme. And even then, employees typically (and I'd even say in the vast majority of cases) have very few and tiny levers to pull to affect company profitability. So you typically you get nothing if the profit is x and nothing if the profit is x+n and even if you get some of that n you can't really affect the absolute value of n. Why should you care about n again? Oh, right, so the company doesn't go under and/or lays you off. That shit may have worked 20-30 years ago when company loyalty and upwards mobility was a thing. > It really isn't, your job is to make a great product. Nope, that's the product manager's job. Here, a Wikipedia link, just for you: https://en.wikipedia.org/wiki/Product_manager reply vouwfietsman 1 hour agorootparentYou're conflating work with responsibility. Let me ask you this: How many product managers does it take to make a product? Its interesting that a lot of the replies here are railing against the idea that software devs are used as cogs in a machine, yet at the same time all replies are arguing that a dev should only be a cog in the machine, and any further context/awareness/action outside of being a cog should either not be expected or make the dev eligible for an outsized reward. So to zoom in on your reply: > Why should an employee care Because they are paid to make the company profitable, and if they fail to do so they may not continue to be employed. I'm not sure why this is controversial. This requires no profit sharing to be in place, because it is simply the job that is required of the employee. It is probably by far the most general description of a job that is not: \"the thing you do for money\". It may very well appear that the employee has no direct influence on company profitability, but since this is nearly impossible to measure objectively the next best thing is to try to make sure that he or she does by listening to signals coming down the hierarchy of management. Your PO telling you to hack a thing together is such a signal, and should be listened to. You warning about a giant pile of tech debt is a signal you should send to your PO, that he/she should listen to. This is all absolutely trivial. reply marcosdumay 1 hour agorootparentprevThe OP's scenario has a product manager telling you that what they need is for you to finish feature X quickly so the company can be profitable. In a well-working organization, that absolutely means that you, as an engineer should look into how to do X with the least amount of effort, hush things up, and ship it. The PM is the one that has to decide into hushing or not things, you are the one to decide how. The problem is, every single organization where the PM insists on you to hush isn't well-working. It's easier to win the lottery than it's to find exceptions here. On those problematic organizations the PM will use your results to improve their curriculum and will absolutely throw you under the bus when the hushed product behaves like a hushed product. And everybody will be happy with kicking you down. Also, if the product has any kind of safety impact, it's not the PM's job to decide about it anymore. It's yours. reply IanCal 2 hours agorootparentprev> Absolutely and categorically it is not. That is complete nonsense. Why should an employee care? They don't have to care, they are just there to do a job. If your company values a release now more than a more stable one later, it's not on you to refuse to do that. reply mrfumier 12 hours agoparentprevWhy are you developping? Is it a hobby for passing time? For pleasure? For the beauty of the code? Could be. But most of the time, you develop in order for the software to perfom a task someone needs. And that should be your first focus: to develop something that brings value for its user, and develop it as efficiently as possible. After all, what's the point of a software if nobody uses it? So no, your job is not to create great software, your job is to bring value to users. In my career, I've mostly seen the developers pleasuring themselves with overengineering, bloating code with features nobody needs, and writing lines to anticipate future developments that never came. Rather than the opposite. So I think the challenge is to remain minimalistic, that's hard, and that's what the original post is about. reply katrotz 11 hours agorootparentUnfortunately there is no correct approach since both sloppy and overengineered codebases backfire. What helps me find balance is following the philosophy of 1. Make it work 2. Make it right 3. Make it fast reply treve 13 hours agoparentprevThis entire comment reads as someone who has a purely adversarial relationship with their coworkers with little trust. Sounds exhausting! reply zogrodea 13 hours agorootparentIt reminded me of that quote wrongly attributed to Shigeru Miyamoto (who created Mario and Zelda and other classic games). \"A rushed game is bad forever, but a delayed one may eventually be good.\" Of course it may not apply to software today (except to the extent first impressions count) because modern software tends to be continuously maintained (and modern games often are too, to a much lesser degree, with post-release patches). reply rustcleaner 2 hours agorootparentIt applies. I did a debloated Windows 10 IoT Enterprise install for a friend (who's giving this laptop to his future wife) and iconcache in Explorer is still broken and giving me the occasional white page icon, and nt authority\\system gets access denied when trying to delete %localappdata%\\microsoft\\windows\\explorer\\iconcache* while explorer is kill. Bad forever! reply vasco 12 hours agorootparentprevGames were not updatable at the time of the quote, which makes an incredibly huge difference from products that can be regularly updated to fix issues. reply zogrodea 11 hours agorootparentI do mention that in the second paragraph. My understanding is that most [0] games don't get super-substantial updates making the game leaps and bounds better than it was in its initial state, the way continuously-developed software does. Am I wrong? [0] Games like Minecraft, Stardew Valley, Terraria and online games being a minority. reply shalmanese 12 hours agorootparentprevGames and software operate from a different set of first principles, games are more akin to movies in that we consume them for the experience and that final level of polish often does make or break it. reply achenet 11 hours agorootparentprevthe counter quote to that is John Carmack, who was originally a fan of the \"it'll ship when ready philosophy\", saying he 'largely recanted from that now' when discussing Rage on the Joe Rogan podcast. Or Duke Ellington: \"I don't need time. What I need is a deadline.\" reply manmal 13 hours agorootparentprevSeeing you‘re a CTO, I‘m a bit concerned for your staff (if any) if your first reflex is blaming the developer for not trusting their boss. reply vasco 12 hours agorootparentThe person is adversarial, that is clear in their comment. There's literal adversarial quotes like \"only do X if good for you, otherwise do the opposite\". On the other hand you went straight to googling a random commenters job and attacked that. reply aydyn 9 hours agorootparentnah fam he is right. And he didnt google shit, its literally in the profile. reply Huggernaut 13 hours agorootparentprevI don't see any blame assigned in the previous comment. reply exe34 12 hours agorootparentno it was more passive aggressive - suggesting there was blame but not actually coming out and assigning it. reply treve 11 hours agorootparentprevNot my intent to assign blame, it just sounds like a toxic workplace. I've been there! The better places I've worked were more focused on compromise, experimentation and taking shared responsibility for risk. reply manmal 4 hours agorootparentThanks for elaborating, this helps understand where you are coming from. reply yamumsahoe 12 hours agorootparentprevno, to me it doesnt. to me he's right about how devs should deliver it as engineeringly sound as possible, and executives should deliver it as timely as possible. its a balance between having a usable product and a product they need at the right time. it's not adversarial. its a conflict yes, but a healthy one. reply IanCal 2 hours agorootparentThey should all be aligned in what they're trying to achieve. Misalignment here is not healthy. Swe should inform about consequences, and executives should inform about business priorities, but they don't have to all disagree on the way forwards. reply Culonavirus 11 hours agorootparentprevSounds exhausting? Sounds like gaslighting a wee bit... Oh nooo... the poor managers, how will they manage if you don't TRUST them blindly and completely (and bend over backwards to fulfill all their whims)? What kind of a cult is this? I'm not a sheep that needs to be herded. Besides, trust is earned. So no, it's not exhausting, it's the exact opposite. It's refreshing and freeing. Anyways, I get along with my coworkers very well. In fact most of my friends began as my coworkers. Then again I do not consider managers to be my coworkers. And generally speaking, yes, you could say I don't trust them. But that just comes from working in 5 for-profit companies for over 15 years. The only exception was an energy company, probably because the \"just ship it\" mentality wasn't as strong. reply treve 11 hours agorootparentYou've hit the nail on the head about trust needing to be earned. I didn't assume that this was on you or it was your fault. It just sounded like a bad situation. Also based on what you're saying manager also doesn't trust you or at the very least you don't share the same values. That said it was quite a cynical interpretation of my comment and aggressive reply, so I'll leave it with that. reply aprilthird2021 13 hours agorootparentprevI can see some of the tone being too adversarial, but I think the gist, that we are after all professional engineers, who studied and spent time and money to understand how to build such systems to high standards. And since we are that and are paid for that knowledge, we should strive to improve software / system quality as much as possible. Isn't the job of executives, managers, etc. to figure out the constraints we have and strive to improve profit and shipping speed as much as possible? Together, with our combined expertise we can get the practical best of all three. But if eng just nod along knowing they are sacrificing quality, security, scalability, etc. then they are doing a disservice to the team, no? reply jampekka 11 hours agorootparentprevOr with the employer/owners? Owners and workers have a fundamentally adversarial relationship. reply vasco 12 hours agoparentprevThis has to be satire. \"Do the wrong thing unless it's for you, in which case ship it earlier\", \"your job is to make you feel good about what you put in your CV\", and other amazing quotes. The only thing of value in the whole comment is saying to not get too attached to any job or worry too much if you get fired, but even the reason given for it is wrong. Incredible to realize I probably work alongside a few people with the same adversarial attitude. Some people do like to live life in the wrong game theory quadrant. reply aydyn 9 hours agorootparentNo, everyone lives life in the same \"game theory quadrant\" whatever that means. People primarily play for themselves including you, some people are just more aware of this fact than others. And also its still always rational to defect. reply vasco 7 hours agorootparentThat's not true, I have many examples I've seen of people putting themselves second. Of course everyone has these thoughts and actions and needs to reflect and be self aware, but it's not true that everyone is just always out for themselves, in fact that's the type of world view that leads people into the wrong quadrant and it's a bit of a depressing world view. I think you understood exactly what I meant by wrong quadrant, from your reply. Don't you know anyone that donates money? Or that volunteers their time? Or that doesn't use all their deductions when doing taxes? Haven't you read stories about people who anonymously donate kidneys? There's so many examples - and the only way for your world view to \"work\" is if all of them only do these things for selfish reasons. At some point one has to acknowledge and believe in good and cooperation and decide if they mostly want to try and operate in coop mode or in selfish mode, but the more you believe others are likely to choose coop, the more like you are to do the same. So you need to start from the belief that good and coop are things other people will also choose. Your world view prevents this \"from the start\". reply aydyn 30 minutes agorootparentI understood that you were referring to the prisoners dilemma, but I dont understand or agree with your idea of living in a quadrant. Everyone lives with the same game theory, difference arises from individual incentives and indivdual intelligence/rationality. But ultimately everyone seeks the nash. Donating money? Volunteering? Even donating a kidney can all categorically be understood in terms of individual gain. reply shepherdjerred 2 hours agorootparentprevI'll drop in here and say that donating a kidney is an excellent relatively low risk thing you can do that has the power to transform someone's life. Here [0] is an excellent article telling one persons' story. [0]: https://www.astralcodexten.com/p/my-left-kidney reply kkarakk 1 hour agorootparentLast i researched, donating a kidney affects your quality of life extra-ordinarily. for eg you can't eat the same way, can't exercise/move around the same way and also you might have permanent complications from the surgery. i MIGHT consider it for a close relative but for a stranger? that would require some real mental gymnastics. reply swat535 4 hours agorootparentprevWhich part of his comment was satire exactly? He mentioned that your job as Software Engineer is to focus on Software quality and push against on unreasonable deadlines. It is NOT your job to make sure the company is hitting its sales targets, that's the management's job. If you think this is satire, you are in the wrong profession and frankly I'm amazed that on a website called Hacker news, people are attacking parent for this advise. Then again, perhaps not since most people here are focused on startups churning out products with unreasonable deadlines. No wonder no one takes Software seriously if _this_ is the attitude from the self proclaimed \"Engineers\" themselves.. reply fragmede 3 hours agorootparentit's just written in such an over the top, office-space, Dilbert-esque, somebody's-got-a-case-of-the-Mondays style that it's hard to take seriously. Yeah, don't write absolute trash code, but also don't spend three years architecting the most beautiful, most modular code and run out of money before you ship; you ain't gonna need it. If you're programming from a place that's so far removed from customers that you don't care about sales, what are you even doing? Just writing code for the sake of writing code? If the only person you're writing code for is yourself, that's fine. this is the Internet age and there are a ton of unbelievably awesome passion projects out there, like that engine noise one. But the rest of us are writing code for some sort of purpose which involves other people using the product that the code is being written for, and sometimes money changes hands. There's software quality, and there's that one guy who over-engineer's everything and just loves writing frameworks and never ships actual product. Without seeing actual demands and code, it's impossible to know which of the two categories they fit into, but it reads like the latter of the two. reply throwaway84751 10 hours agoparentprevI was one of the first employees in a startup, and if I hadn't made compromises the company would probably not exist today. Your approach only works for big established companies where your team has limited impact on the results. For small and medium sized companies you definitely need to make compromises. It is part of your job as a professional to make choices that leads to the best business outcome. Unfortunately a lot of developers don't have a connection to the business side, because they are \"protected\" by several layers of management, PMs and designers that interact with the business on their behalf. Getting a product out quickly means that you also get feedback earlier. This is not only good for business, but it is also an opportunity to evaluate your implementation to see if it matches your assumptions. In my experience this causes less stress compared to rolling out a \"perfect\" solution that has to be rewritten while under pressure. My job as a developer at my current workplace is to reduce complexity and get the PM and designers to cut down their initial plans to a minimum. It makes arbitrary deadlines less stressful and any delays will not have the same impact. reply lazyasciiart 11 hours agoparentprevGood news - once you work at a large enough company, this flips to \"your job is to finagle your way through a million different safety and security checks at every step in order to write a few lines of code and get them shipped before a pivot and/or re-org makes them irrelevant to your new managers goals\". reply danparsonson 8 hours agoparentprevThere's usually an entire pipeline of people waiting on your work - sales & marketing, ops, support - and every delay you induce risks knock-on effects for them, not to mention that depending on the product, customers can be planning their own workloads around your release schedules, so you'll mess them up too. Not all deadlines are arbitrary. reply pdonis 13 hours agoparentprev> Unless you have a stake in the company, it is NOT your job to make sure the company is the most profitable it can be. Maybe. But... > Your job is to create great software. No. Your job is to do what the company pays you for. The company is not paying you to create great software. It's paying you to solve some kind of business problem or provide some kind of business service. Some of those problems or services do indeed require great software. But many do not; mediocre software will do the job just fine. And if that's the case, and you insist on creating great software anyway, you are spending time and effort that is not adding anything to the company's bottom line. And while you personally might not care about that, the company does, and they're paying you. > What's great software? The kind you'd be willing to put on your resume without feeling bad. You used the term \"software engineer\". A software engineer's job is not to always write great software. It's to write the optimal amount and quality of software for meeting the particular requirement it's supposed to meet. More generally, an engineer's job is to produce optimal solutions to problems. That does not always mean building the highest quality product possible. Sometimes it means doing just enough to get the job done, even if it's not very pretty, and stopping there because doing more would add no business value. And you should not at all feel bad about putting that on your resume. Refusing the temptation to gold plate everything when it's not necessary is a good thing for an engineer. reply WgaqPdNr7PGLGVW 12 hours agorootparent> Your job is to do what the company pays you for. The problem with this mindset is the most companies are short-sighted and when the problems inevitably start coming in it is the developers who are placed under pressure. It is the developer being paged at 2am on a Sunday. It is the developer working overtime to get the feature out because the codebase is a giant mess. Etc. Having a minimum quality bar is a must. The OP was suggesting a professional level of quality - not gold plating everything. reply pdonis 21 minutes agorootparent> most companies are short-sighted The best course if you find yourself employed by such a company is to change employers. A company that is genuinely short-sighted is dysfunctional and no amount of professionalism on your part as an individual contributor is going to fix that. Now if you could get yourself promoted to management, then maybe you could have an impact--but then you wouldn't be doing actual software engineering any more, you'd be doing corporate management fixing, which is a different job. > and when the problems inevitably start coming in it is the developers who are placed under pressure. Yes, this is true. And as above, if it's genuinely dysfunctional, your best course, unless you are willing and able to become a manager yourself, is to change employers. > Having a minimum quality bar is a must. The OP was suggesting a professional level of quality - not gold plating everything. That's not what \"write great software\" means to me. \"Great\" is not the same as \"minimum quality bar\". reply protocolture 12 hours agorootparentprev>No. Your job is to do what the company pays you for. Yes and no. Ultimately, your solution will live on after you. Even if theres no way for it to backfire publicly, your coworkers and future employees have to deal with it. It can absolutely ruin your reputation and cost you future employment. Admittedly its much easier for a contractor to draw a red line, but I had done this as far back as my first full time IT job, setting boundaries with the owner of that business as to what is and is not practical and achievable. reply pdonis 18 minutes agorootparent> It can absolutely ruin your reputation and cost you future employment. If your solution does not meet the company's needs, yes, this is certainly true. But I never said you should do something that does not meet the company's needs. I just pointed out that meeting the company's needs, even if you add the qualifier that you are going to do that in a way that is professional and does not create unnecessary burdens for others, still does not always mean \"write great software\". It means finding the optimal solution for the particular problem. > setting boundaries with the owner of that business as to what is and is not practical and achievable. And in cases where \"write great software\" is not practical and achievable (and yes, there will be plenty of such cases), that would mean telling the owner that it is not practical and achievable to write great software to meet this particular requirement, and recommending a different solution. Which is what I said. reply aprilthird2021 13 hours agorootparentprev> No. Your job is to do what the company pays you for. The company is not paying you to create great software. It's paying you to solve some kind of business problem or provide some kind of business service. The company pays me because of my knowledge and expertise, that is needed to create and maintain the porduct as best as possible. If I am just a yes man, the company isn't getting what they paid for. If I honestly assess and say what is being sacrificed re: security, scalability, future flexibility, etc. in the software, then the company is getting what they paid for, and they can choose how far they want to go in the quality / time spectrum. And if they pick somewhere my expertise says is too far on one or the other side, it's my job to say so. No? reply pdonis 13 hours agorootparent> The company pays me because of my knowledge and expertise, that is needed to create and maintain the porduct as best as possible. Yes, but that does not always mean writing great software. Sometimes it means writing mediocre software because that's all that's needed. Sometimes it might mean writing no software at all because software isn't the best way to solve a particular problem. You recognize this because you say there is a quality/time spectrum, and the company wants you to use your expertise to help find the optimal point on that spectrum for a particular need. But \"write great software\" implies that there is no such spectrum--everything you do is always at the extreme high end. You are agreeing with me that that's not the case. reply aprilthird2021 2 hours agorootparent> But \"write great software\" implies that there is no such spectrum--everything you do is always at the extreme high end. You are agreeing with me that that's not the case. Yes I guess I just read the parent comment more charitably. Like always push for maximizing the quality as that's your job and your expertise. And sometimes when quality really matters you have to stick your neck out and push back aggressively, even if it will cost the company more than expected. That's what we learn in engineering ethics courses in school after all. reply pdonis 14 minutes agorootparent> sometimes when quality really matters you have to stick your neck out and push back aggressively Yes, that's certainly true. But not all cases are cases where \"quality really matters\". In many cases you reach a point where adding more quality has rapidly diminishing returns in terms of business value--often because it takes time and effort away from other projects where quality matters a lot more. Ultimately that's the company's decision to make. And as I pointed out in another response upthread, if you genuinely believe the company is dysfunctional in this regard (and many companies are), your only real choices at that point are to try to become a manager so you can fix the company's culture, or change employers. reply willvarfar 12 hours agorootparentprevShould we not find out what the company are paying you for? They might be surprised that you think they hired to you do to be a no-man and believe a course correction is in order? :) reply shalmanese 12 hours agoparentprevThis is the wrong framing of it, shipping is an internal mental battle, not an external one. We are not perfectly rational automatons that always work towards our rational best interests. Shipping is hard so we create mental battles inside ourselves to avoid it and use post-hoc justifications to feel morally ok with that decision. The choice of when to ship isn't nearly as important as the answer to why you aren't shipping on the choice you said you would ship. Yes, shipping at different points of maturity represent different sets of tradeoffs in a complex multi-dimensional matrix and it's an intellectually fascinating challenge but often, the choice is far less important than the debate that sucks it in and engineers end up bikeshedding the decision. Whenever you find yourself bikeshedding, it's important to realize you don't solve it by looking at the decision but by looking at why people feel the need to bikeshed in the first place. You talk about how \"Your job is to create great software. What's great software? The kind you'd be willing to put on your resume without feeling bad.\", what is the \"job to be done\" by that belief for you? Is this a true, genuine held belief by you or is that here as a cover to some deeper belief that is uncomfortable to surface? The answer is something ultimately only you can answer and no internet comment can force that deep degree of introspection, only those close to you who have a requisite degree of insight and therapeutic practice can reliably help with that. But the larger point I want to make is that the shape of the conversation around shipping is all around exploring where we feel different mental resistance around shipping at any point in time and uncovering where that resistance truly comes from vs the reasons we make up to dress it up. reply Fr0styMatt88 11 hours agorootparentWhat do you mean by “not shipping on the choice you made to ship”? Do you have an example? Really fascinating thoughts. reply shalmanese 8 hours agorootparentLike what the article stated. “I should ship this week but I really feel we need a mobile app to ship.” You build the mobile app and you still don’t ship because the mobile app was not the problem, it just hid the problem. Until you understand and confront the true problem (emotional discomfort at the consequences of shipping), there will always be another reason not to ship. reply sisve 9 hours agoparentprev> Your job as a sw engineer/architect is to resist the \"just ship it\" pressure from the management as much as possible. So unless you own what you're coding and you really need it out of the door for your own benefit, if more time makes your work more professional, then take more time. Anyone telling you otherwise is a 100% hack. You are not an automaton that takes in JIRA tickets and spits out hacky code as soon as possible. Or at least you shouldn't be. Not to mention that not taking time (doing things properly) is incredibly taxing on your psyche and you WILL burn out. There are only so much garbage tasks you can take. Any job, In any industry at any level should be working together with your manager to understand what the company goal and how to achive that. Its always going to be time/price vs quality if you work on creating software or clothing or whatever. There are properly companies and managers that do not care.. but imbany healthy organisation this should be the case. There is not garbage tasks. Its tasks that needs to be done. Any understanding that not all tasks that needs to be done in an organisation is fun but needs to be done is a sign of maturity that you want from your employees reply therouwboat 7 hours agorootparentMy boss says our goal is to make quality products, but often we don't even have time to check the parts. Once there was a problem that machine didn't make good enough surface finish, but boss just said it's fine, let it run. Turns out it wasn't fine and we had to fix 3000 parts with hand sander. That's why I don't like to cut corners. reply mirsadm 12 hours agoparentprevMost deadlines are made up anyway. If it doesn't happen usually nothing happens. reply achenet 11 hours agorootparent“I love deadlines. I love the whooshing noise they make as they go by.” - Douglas Adams reply bruce511 13 hours agoparentprevI started writing a reply about how the context is very different between indie side projects and your work environment, but then your post morphed into a comment about the role of a programmer in an organisation, and so did my reply. Firstly, I sense you are frustrated in your current post and I sense you are not in step with your management or perhaps not even in step with your coworkers. I sympathize with your predicament. If I had any advice it's that you cast around for a position that's more in line with your principles and goals (don't quit your current job till you find that.) Personally I've been on all sides in the myself. I've been the principled programmer. I've been the one dedicated to quality. I've been the manager. I've been the person responsible for the business staying in business. Yeah its nice to adopt the \"code my problem, business not\" position. It seems like the moral high ground. But businesses are all about balance. They have to balance things like income, and happy customers with tech debt and so on. Having programmers (or anyone for that matter) working -against- the big picture is not ultimately useful and can do more harm than good. I don't always agree with my team and they often don't agree with me. But ultimately I'm responsible so (sometimes tough) decisions have to be made. The most successful employees are those who acknowledge that this isn't necessarily the best way to do something, but strive anyway to make it a success. I wish I had the time and money to let programmers just spend forever building stuff and never shipping. They most definitely wish that. But we live in a real world with constraints and the reality is we need a lot more than perfect code. Of course some places are just impossible to work at, where everything is rushed and there is no balance either. And then it sucks to be you. reply pdimitar 3 hours agorootparent> Yeah its nice to adopt the \"code my problem, business not\" position. It seems like the moral high ground. What? No. It's the economically and humanly well-balanced position. I got a wife whom I love to spend time with and I don't want to live on the computer. 8 hours is already way too much. I do what is expected of me and maybe a little more, for the rest they'll have to pay extra or give me parts of the profit. If not, they'll get baseline performance. It is always so mind-bendingly odd to watch people claim that you can go above and beyond as if that has zero side effects on any other aspects of your life. reply eastbound 12 hours agorootparentprevIf we waste resources overengineering everything, it takes resources away from other parts of society that require them. reply pdimitar 3 hours agorootparentGood thing that 100% of all programmers go out there and volunteer all the time they saved at work. Oh wait, no they don't. reply rkuodys 12 hours agoparentprevI think it's very different when you're one-man-show vs a cogwheel in organisation. In former case, just like the author mentioned, your struggle is more with yourself. You need to make yourself move forward. And \"Just ship it\" approach gives you that - you get external push to carry on. External user(s), external comments and demands give you that bit of energy to carry on. In corporation setup, you always have a manager and peers that push you. You have periodic meetings with team/manager whatever to show what you have done, and then, depending on company setup, it's quality vs speed discussion. But that's very different from being all alone with your code and computer trying to build something from the grou up. reply antupis 12 hours agoparentprev> The kind you'd be willing to put on your resume without feeling bad. Usually when you are doing \"just ship it\" often enough you achieve this, especially at larger firms too often I see situations where engineers are doing their 3th refactoring without any customer feedback. reply snozolli 1 hour agoparentprevif more time makes your work more professional, then take more time. Anyone telling you otherwise is a 100% hack. It sounds like you're a fan of Total Quality Control, as am I. However, it also sounds like you're susceptible to feature drift and undermining yourself with ideas thought up late in the development cycle, as am I. I've seen it from both sides. Software people who want to get just one more feature in, or who want to hold up release to fix an inconsequential bug. Similarly, the CEO who ruminates all weekend and decides we have to revamp something for no objective reason and on a ridiculous deadline. reply achenet 11 hours agoparentprevhttps://www.dreamsongs.com/WorseIsBetter.html If you want to write good software, get it in front of someone who will actually use it as soon as possible. Get it in front of many people who use it as soon as possible. No matter how smart you are, you can't anticipate every user need. Let the users tell you what they need. To do that, you need to ship. reply riwsky 12 hours agoparentprevEngineering is tradeoffs; the whole concept of quality only makes sense in the context of a certain budget. If an engineer takes two weeks hand-crafting some one-off charts in d3 and react that could’ve been done uglier in some BI dashboard in a day, it might look good on a résumé—but it’s shitty engineering. reply potsandpans 7 hours agoparentprevWhat a weird screed. reply eastbound 13 hours agoparentprev> Your job as a sw engineer/architect is to resist the \"just ship it\" pressure from the management as much as possible. Wow. That reminds me of what the guy who was fired said. I’ll never understand him. He lives in a house that’s below sea level, with one kid and one wife, and it gets flooded every 4 years. He was raised 30% during Covid, job was quite comfortable, he was competent for it, then stopped pulling his weight. Wouldn’t it be easier to keep pulling weight and try to build something that works, rather than deploying energy for union tactics, and then live in a twice-more expensive above-sea-level house? Yes it would. Sometimes people get stuck in a victimization loop. reply polotics 12 hours agorootparentGood News: Dr Sapolsky has recently updated his intro video on depression. it's here: https://youtu.be/fzUXcBTQXKM?si=JU-iOaP9SeFdB5qy reply ggeorgovassilis 13 hours agoprev> so I started getting suspicious if someone actually shared the video of my app with these people because they were solving literally the same problem I once met a guy who had a good idea about an app, something that became fairly mainstream two years later. He asked me to code the app for free and we'd share revenues. When asked what his contribution would be, he offered to \"run\" the company and otherwise his 50% was \"having the idea\". I thanked him and told him that he has a head-start of 6 months, if his app hasn't hit the market by then, I'd write the thing myself. reply protocolture 12 hours agoparentI was once involved in an indie game project. We had a middling idea for a crappy game but we were all hankering for software development experience in that field. Of the 3 of us who set about coding it, 2 of us just sat down and started blocking the thing out. The third, pushed faulty code to the SVN, created a design document crediting the entire idea to himself, and then called a meeting telling us he is the game designer and he would sue us if we made the game elsewhere. The 2 of us actually contributing just looked at each other and dropped the whole thing. He basically played his cards face up and we got to see he was a pissant. Later I did see someone with the same general idea on steam. Good for them honestly. If they came up with the idea separately, great. If they stole it from the loser, also great. If they managed to persevere under him they deserve some coin. reply silisili 13 hours agoparentprevAs someone extremely logic and programming oriented, I -wish- I'd met someone like that. I'm just not an idea guy, apparently. If the idea was truly good so much so you believed in it, it was a heck of an offer tbh. reply jaggederest 12 hours agorootparentI'll give you any one of 3 or 4 ideas right now, if you want them. They're ideas I'd develop if I had the time, energy and brainpower. I've got personal connections for you too, if you want them, people who need the product. I won't even ask 50%, heck, 5% sounds fine by me. My experience has been that, while that is useful, it's by no means sufficient to lead to a complete and useful tool. But I'd absolutely take that deal any day of the week, from the \"idea\" side of the fence. Imagine if you did it 20 times - quite a nice portfolio. reply silisili 12 hours agorootparentThe ideas are important, but remember the idea guy also was to run the company. That's a ton of work, too. In any event, while I appreciate the offer, I'm now too old and domesticated to work on it. I guess my comment should have said I'd have loved to met someone like that in my early 20s, when I had more free time and my brain worked better :). reply ggeorgovassilis 10 hours agorootparent> but remember the idea guy also was to run the company Company of 2 where neither receives a salary. > That's a ton of work, too. In this case, I really, really don't think so. reply jaggederest 12 hours agorootparentprevWell, in an honest sense, that's kind of what YC's matching program does. I'm working for a company I found on there right now. It's a much different experience from job searching but it's full of people with ideas. Half or more of them are complete nonsense but any decent engineer should be able to sift the gold from the dross. reply RangerScience 12 hours agorootparentDo you have some tips for getting started on there? I poked on a few months ago, but wasn't able to make enough sense of it to get started. I think I just didn't find the right entry points? reply jaggederest 21 minutes agorootparentBook meetings with people who seem interesting, don't judge a book by its cover. Fill out your profile in a pretty detailed way, you'll get better matches if you specify what you're looking for. If you're a technical person, you're going to have your choice, I gather many of the people on there are not technical. reply RangerScience 12 hours agorootparentprevCan I hit you up for one those in September if my current thing doesn't work out? Completely serious. reply jaggederest 7 hours agorootparentSure thing, goes for folks reading this too. reply light_triad 12 hours agorootparentprevThe problem with this setup is that having the idea is only the starting point. You will need to iterate, talk to customers, improve the solution for years while you learn more about the market. Even if the idea is truly good all the value is in the execution and acting on non obvious information from your customers. Anyone who isn’t committed to this long and uncertain process probably shouldn’t be a co-founder. reply raincole 12 hours agorootparentprev> If the idea was truly good so much so you believed in it, it was a heck of an offer tbh. Perhaps. But in the past 15 years I've never seen such a good idea. If one is going to take 50% I'd expect them to be a really good salesman at least. reply saulpw 13 hours agoparentprevI mean handling sales + marketing + accounting and everything else required to run a company is worth 50% (maybe more). But of course while you were probably competent enough to \"code it up\", odds are he wasn't competent to \"run the company\". reply laborcontract 13 hours agorootparenti’ve recently had a revelatory experience with my cofounder, who is focused entirely on sales. we were pitching our product/services to the company execs- and he just owned the room. He definitely oversold, hallucinated a little bit, said some (many) things i never would have said, but he didn’t lie. I have a highly diminished opinion of my talents than compared to reality and i know it. And i can’t fix it, i’ve tried. In that meeting, there are times i wanted to interject to add some context, they very skillfully pulled me away from talking too much and qualifying everything. We got our first sale. Big client, big company, very important sale. Came out of that meeting daze because that’s when i internalized it - im worth no more than half the company. It was truly revelatory. I’m writing all the code, all the “hard” work is on me. I dont care. Before that, i spent two years on my own project with zero traction because i didn’t even want to sell it, i just wanted to make it better. by the end i was gasping for air. If you can find a good decent person who has a great network and a knack for sales (and is fine doing administrative work) by god, let go of your ego and give them room to cook and have faith in yourself to ship. I spent the first half of my career deriding and diminishing the effort of salespeople. Never again. reply flappyeagle 13 hours agorootparentWhen you watch a great salesperson in action it’s like magic. It’s like watching a chess player find moves you couldn’t have found in a million years. They chose the right words that converted to motions of neurons in your counterparty that caused action by them to give you money. Crazy. reply ggeorgovassilis 10 hours agorootparent> When you watch a great salesperson in action it’s like magic. That, a hundred times. It's otherworldly, transcends laws of reality. That's why I dislike most salespeople I met, who don't tire of iterating how important sales are, but never come even close to what a great salesperson can deliver. reply throwaway7ahgb 6 hours agorootparentIn my history the best sales people I've met wouldn't have me believe they were in sales at all. They are just great at relationships and solving problems for clients. You want to give them your money to solve your problem. reply laborcontract 13 hours agorootparentprevI literally couldn’t stop shaking my head on the commute home. I’ve always had the habit of underselling myself. I would have never made that pitch. But he made them believe in the product and in that process made me believe in it too. reply protocolture 12 hours agorootparentprevYes my former business partner is a rare hybrid of super competant salesperson and super competant engineer. I sometimes just enjoyed reading his contracts. reply dgb23 10 hours agorootparentprevAdministrative work should probably not exceed 10% of the budget, especially in a small company, where the advantage is to have less requirements in that area. Marketing and sales, it depends on what you sell to whom. Let’s say 10-20%. Then there’s a grey area. Obviously there needs to be a feedback loop between clients, development and business. If the business/idea guy facilitates a lot of this and is mostly responsible to maintain the relationships, then he is part of development. So another 10-20% give or take. That 50% number seems kind of fair. Ultimately it implies that both are putting in the same time and effort. If the business guy is getting overwhelmed, the developer guy can automate things, talk to clients, write pretty documents for clients or take on some admin tasks. If the developer guy is overwhelmed then the business guy can simplify requirements negotiate deadlines, do manual testing and feedback, organize tasks and so on. What I‘m trying to say is that 50% might be more of a social contract rather than an a priori, objective assessment of value. reply ggeorgovassilis 10 hours agorootparentI second your analysis, that's how things normally work. It's just that in these and similar scenarios, there isn't much product development happening before the MVP, so the developer fronts all the work and the risk. reply ggeorgovassilis 10 hours agorootparentprev> odds are he wasn't competent to \"run the company\" A 2 people company where neither gets paid and \"product development\" consists of me coding the MPV, fronting all the work and the risk. reply deadbabe 13 hours agoparentprevNot sure if it’s smart to do that if you aren’t entirely sure about what mental state a person could be in. Edit: okay I don’t know why I’m getting downvoted for this but if you think you can just tell someone you’re going to steal their awesome idea in 6 months you better hope their not the type of “crazy” entrepreneur who will cause problems for you or in extreme cases even end up shooting you in the back of the head. reply pjc50 9 hours agorootparentYou never know when you're going to get hit by a car either. Or a meteorite. It's a tempting trap to make decisions - or even worse, paralyze decisions - around the rare outcomes, rather than the common ones. reply ggeorgovassilis 10 hours agorootparentprev> okay I don’t know why I’m getting downvoted Because saying \"don't do something because an irrational player might irrationally punish you for that\" isn't particularly insightful. To expand on your argument, I could find out your real identity (this is hacker news, so I might be a hacker) and pay you a visit (I won't, I'm too lazy & incompetent for that). Just saying that your argument doesn't make sense to me. > you’re going to steal their awesome idea I paid for it by listening to a bad sales pitch, that's 40 minutes of my life I'm never getting back. Also, there was no confidentiality or non-compete agreement. > end up shooting you in the back of the head Only a semi-related tangent, I live in Europe, people here usually don't carry guns which shifts the entire risk/reward calculation significantly. reply solatic 13 hours agoprevI wish someone would solve my problem for me. I'm working on the problem because there are no solutions that I can just go and buy. Someone else putting their blood, sweat, and tears into solving my problem, they're the ones who have to deal with being on-call for it, they're the ones who have to maintain it, would be a joy. Why is it so important for you to be the one to solve this problem? Why is it so important for your solution to the problem to be a business? Running a business is about creating value for yourself and for your customer - if you're obsessed about the problem, be thankful that someone else is putting so much effort into solving it; if you're obsessed about the customer, then you would've shipped something to the customer a long time ago to get feedback. reply thekitze 12 hours agoparentauthor here: for me it was important because after a while the app I was using was stale and wasn't shipping updates anymore. Also, as I was learning more about productivity, I reached the limits of the app. I wanted more features and I was coming up with tons of ideas that I could implement into my app, and other devs wouldn't care about implementing these ideas. I wanted control. So I finally shipped Benji (https://benji.so) reply ZaoLahma 12 hours agoparentprevI guess some solo dev engineers hope to implement a novel idea, curate the implementation into a somewhat successful business (have a firm grip on the market) and then sell it all off to a bigger fish and make bank. I don't think many people actually want to run a business (long term), but most of us wouldn't mind suddenly being paid a good chunk of money for having solved or worked on an interesting problem. reply thekitze 11 hours agorootparentin my case, I would never sell Benji to anyone else because my life literally depends on it. The business part is hard tho. reply thekitze 12 hours agoprevAuthor here! I need to update this article. Years later, I actually got motivated by the comments on HN (whenever this gets posted). P People are always like \"why don't you just ship your app?\" ... so I did! I'm happy I went through with and it's way WAY better than any competitor in this category Check it out at https://benji.so (landing page is still w.i.p) reply micimize 2 hours agoparentI went through an unshipped app dev cycle that was fairly similar in many ways. Started on React, then RN, then Flutter, and eventually migrated away from graphql into sqlite. My app had some similar aspirations -- to bridge the gap between habit motivation, goal adherence measurement, task scheduling & rescheduling. I worked on it for a few years, and my identity was very much wrapped up in eventually bootstrapping a company. For me, the decision to let go of the project came in multiple phases, but one big closer was that I simply didn't want to be an app dev in the long run. While difficult to let go of, I currently feel good about the decision. Also, as evidenced by the resurrection part of the story, \"nothing is ever fully lost\" anyhow, though I doubt I'll ever return to this particular project. One key idea I had for expanding beyond the \"high cognitive load\" nature of most productivity apps was to implement a \"life module\" marketplace of sorts that would let, say, a fitness influencer sell a workout routine + meal plan + journal template one could \"install\" into their life. LLMs will also make detecting fall-off and attempting to attribute causes, or respond to \"non-actions\" much more feasible, which I think is important for anyone not type-A enough to use a productivity app consistently every day on their own. reply philipwhiuk 9 hours agoparentprevThe landing page lags my browser. How is that even possible - it's a splash screen with a few images. What on earth are you doing on that page. reply OvbiousError 3 hours agorootparentRotating an image and moving some text apparently reply wellbehaved 2 hours agorootparentprevOK Steve. reply pantulis 10 hours agoparentprevI just tried it and the detected timezone for me is called: \"Africa/Ceuta (Romance Standard Time) (UTC+01:00) Brussels, Copenhagen, Madrid, Paris\" While the assigned delta with GMT is correct, this is confusing as hell because neither Brussels, Copenhaguen, Madrid or Paris are in Africa. You may want to take a look at the TZ info you are using. EDIT: See comment below, this is not an issue. reply com 9 hours agorootparentBut Africa/Ceuta time aligns with CE(S)T for longstanding national administrative reasons, I believe. That’s why those other cities are listed, since Africa/Ceuta time is not linked by policy, regulation or legislation with any African time zones… Time is one of those human constructs that isn’t strongly bound to geographic or perhaps even physical reality. Look at the International Date Line or Chinese timezone maps for examples that are “bigger” than Africa/Melila and Africa/Ceuta. We should all be glad that people are thanklessly doing the hard work to keep the TZ databases updated. reply pantulis 9 hours agorootparentI stand corrected, I had skipped the \"Europe/Paris\" and \"Europe/Madrid\" options that are present in the long listbox. I had assumed that someone from Paris would be confused by having to select the timezone \"Africa/Ceuta\". reply rob74 12 hours agoparentprevHi! Looks great, maybe I'll give it a try... Is it named after this guy https://en.wikipedia.org/wiki/Benji ? I was a huge fan of him (them?) when I was about 6-8 years old! ...and sorry about expressing my frustration (and suspicion) about you not mentioning the name of your \"competitor\"! I guess now that you have released your own app, the chances of you mentioning it are even smaller (if it's still around at all)? reply thekitze 12 hours agorootparentIt's named after my dog Benji haha. Yes you're right, I'm not gonna mention any competitors, especially on a viral HN article :) There's a chance I might make a /comparison page in the future though. reply edg5000 12 hours agoprevI found that becoming an actual user of your own system changes the perspective entirely. I had this thing that I was making for myself, and it was not ready, not usable at all, or so I thought. After giving up on the project I decided to try and actually started using it as if I was a user. I realised that as users, we are used to countless minor issues, and we automatically find ways around that. When you are the creator of something, you sometimes forget that a lot of sloppyness will not be a dealbreaker, and the user will effortlessly work around many of the shortcomings. Obtaining perfection is more about ego at that point. So trying to actually use it, ignoring that you are the one who made it, and forbidding yourself to make any modifications for a while, can change everything! reply knallfrosch 12 hours agoparentInteresting point. An app I just used had an embedded ios web browser that didn't work properly. I opened the web page directly and used it like that. The username/password were iCloud-synced, of course. Took me all of 5 seconds to resume the user flow in Safari (which only took another 30 seconds to finish.) reply tstrimple 5 hours agoparentprevI think this is why ship it and iterate fast is often the best path. There will be deal breaking bugs in any software project. If your accounting package doesn’t account correctly, you probably shouldn’t just ship it. But if one of the reports displays twice for some reason, users can probably deal with it until the next iteration as long as that next iteration isn’t months to years away. But the reality is you wont even find what most of the “bugs” your users will encounter until it’s out in the wild with real users. And unless you ship it, you’ll not get to work on the bugs your users are actually impacted by. reply thekitze 12 hours agoparentprevI 100% agree with you. reply dspillett 2 hours agoprevIf I was writing something for myself, I'd not be sad that someone had beat me to it: it just proves that the idea was a good one! Then again, none of the many personal projects I've got in my head and on paper (few of them even actually started) are ones that I would release to sell. They are things that I want or that friends/family/other might find useful. Heck, if I had something Alpha quality maybe I'd release that in the hopes someone would see it and think “this idea is useful, but the implementation is shite, I'll write a better one”. reply nyokodo 2 hours agoparent> If I was writing something for myself, I'd not be sad that someone had beat me to it The reality is that you’ve already been beaten to the punch by something in almost every situation. If you’re automating something for the first time in history then the preexisting manual method is your initial competitor. In the case of the app in the article they’re competing against that other app but also every other possible patchwork of partial solutions that their target customers are already using. Additionally, if you are the first mover then you’ll quickly have competitors rise up and eat away at your advantage without your effort to stay ahead competitively. So, since you’re always scooped then don’t worry about being scooped and since competition now or later is a certainty then instead focus on your competitive advantage. The author came to this realization in the end, bravo and good luck! reply raincole 13 hours agoprevPerhaps it's just me, but I can't stand the \"I'm trying so hard to be funny\" writing style. reply pm90 12 hours agoparentAgreed. It’s very exhausting to read. A few well timed digs are ok but you need to not overdo it. reply thekitze 11 hours agoparentprevit's not just you, people have different preferences and that's fine :D reply netmare 7 hours agorootparentI'm also not a fan of overly-funny writing, but I just wanted to say that I found your style quite enjoyable. Maybe it's because a can relate somewhat to your situation... :b reply torlok 10 hours agoprevNot the best example of \"just ship it\". The thing about these productivity apps (TODO, habit tracking, spending tracking, journaling, workout planning etc.) is that lots of people have these ideas independently. You can't imagine how clever I felt when I thought of the idea of an app that helps you keep track of your expenses. Then I checked the Play Store. KRAZAM even made fun of it the \"The Hustle\" video 5 years ago. reply realharo 10 hours agoparentYeah, this is probably the most overdone app category in the history of overdone app categories. That's not to say that a new twist on it can't be successful (most successful things aren't entirely original), but if you're worrying about someone releasing their version earlier and beating you, just take a look around. reply satvikpendem 8 hours agoparentprev> Then I checked the Play Store I think you missed the point of the article, which is to ship it despite competitors; in fact, competitors validate your idea, it is a good sign, not a bad one. reply purple-leafy 11 hours agoprevOkay I gave up reading because it’s written by a man-baby. So cringe. So you made a proof of concept and rested on your laurels. Too bad, that’s life. They did the work and reaped the rewards. Lesson learnt. You can’t claim ideas reply rawbert 11 hours agoprevWhen I read this I was like \"Oh, that sounds like my family calendar / collaborative productivty app I am working on for months \". And in the last sentence he mentions > I wanted an app that combines Todos, Habits, Planner, Goals, Pomodoros, Meal tracking, Fasting, Hydration, Packing, Trips, and many many more features. Surprised Pikachu face. reply SergeAx 18 minutes agoprevWhat do I feel like I just read an advertisement article for a productivity app? reply adastra22 13 hours agoprevWhy not ship now? Well not now in 2024, but when this article was written? Most apps and services you use were not first to market. reply thekitze 12 hours agoparentI did! 2 years ago I launched Benji (https://benji.so) (but rewrote it from scratch this time) reply adastra22 12 hours agorootparentNice! Thanks for the update! reply lukan 13 hours agoprevOn the other hand, there is no second try, to make a first impression. And the first impression lasts a while. So at least get some feedback before \"just shipping\". Otherwise reactions might be \"doesn't work\" - because what was obviously a start button for you making it, was not so obvious for someone just stumbling over it. reply tass 12 hours agoparentIt doesn’t need to be shipped to the public. He shared videos with people, and those same people can be the first users of the MVP. The point of an MVP is to elicit feedback, know you’re on the right track to build something useful, and iterate. reply lukan 12 hours agorootparent\"It doesn’t need to be shipped to the public\" Ok, I took it by the usual meaning of ship it to the market. Not having people test it. reply com 8 hours agorootparentI think also, it’s not just “people”, but “people that you hypothesise have a problem that you’re aiming to solve”, and perhaps even “… and have money that they can and want to spend on solving it” reply jascha_eng 13 hours agoprevI wholeheartedly agree with this, when I started working on Kviklet we were a team of 3 and one of us was a lot more perfectionist than the others. It took a lot of convincing to even put our (tbf shitty) first website version up. Much more to release our repo. We lost that \"co-founder\" early on, but man I'm glad we released early and tried to sell. It didn't work and we found no buyers but imagine we still were working on a product without knowing if anyone would ever want to pay for it, keeping our hopes up in the dark. By now we went with the backup plan and open sourced and have a few cool users. Could maybe even say it's a small community: https://github.com/kviklet/kviklet It's not the startup success story that I hoped for a year ago. But it's a lot better than still hoping for it and not being a bit more grounded. Also open-source doesn't mean I can never sell support or a premium version and still make a few bucks right? For now it's just a fun side project though. reply 38 13 hours agoparent> Pull Request-like Review/Approval flow for database queries Terrible description IMO. a query should not need approval. Should use mutation or edit or update or modification. Even if query is technically correct it just sounds wrong and confusing. reply jascha_eng 13 hours agorootparentI'm not quite sure what you mean. Maybe statement instead of query would be more accurate but I think people get the gist of it just fine. Also, I disagree a manual query like: \"select * from credit_cards;\" should probably go through an approval flow if you have a table like that in your prod env. reply jamesfinlayson 12 hours agorootparentA guy I used to work with said he worked for a company where all queries had to get approved by one of two full time DBAs - apparently with good reason as someone tried to modify a query that would have joined with half the rows in some gigantic table. reply throwaway84751 4 hours agorootparentI worked at a place where only certain teams with a dedicated DBA were trusted to write direct queries (based on past incidents). All other teams had to ask a central DBA team to build stored procedures for any interaction with the database. If you think that this would create a huge backlog, you are correct... Non critical updates also needed to be coor",
    "originSummary": [
      "The author shares a personal journey of developing an app, starting in 2018, but delaying its release due to continuous feature additions and learning new technologies like React Native.",
      "Despite abandoning the project after two years, the author later discovered a similar app that succeeded despite being imperfect, leading to mixed emotions.",
      "In 2022, the author finally released a productivity app combining various features like Todos, Habits, Planner, and Goals, and invites readers to join the community on Benji - The Life OS."
    ],
    "commentSummary": [
      "The discussion revolves around the \"just ship it\" mentality in software development, emphasizing that rushing to meet deadlines can compromise the quality of the software and lead to developer burnout.",
      "There is a debate on whether developers should prioritize company profitability or focus on creating high-quality software, with some arguing that developers are not adequately compensated for extraordinary efforts unless they have a significant stake in the company.",
      "The conversation highlights differing perspectives on job satisfaction, compensation, and the balance between professional integrity and company demands, reflecting broader industry concerns about work-life balance and recognition."
    ],
    "points": 217,
    "commentCount": 263,
    "retryCount": 0,
    "time": 1720065035
  },
  {
    "id": 40874013,
    "title": "Jeffrey Snover and the Making of PowerShell",
    "originLink": "https://corecursive.com/building-powershell-with-jeffrey-snover/",
    "originBody": "Home Subscribe To Podcast Episodes By Date Episode By Theme About Donate CORECURSIVE #102 Navigating Corporate Giants Jeffrey Snover and the Making of PowerShell Listen Now PODCAST PLAYER From Burnout to Breakthrough Navigating Corporate Giants What if you had to fight against your company’s culture to bring a revolutionary tool to life? Meet Jeffrey Snover, the Microsoft architect behind PowerShell, a command tool that transformed Windows system administration. Initially met with skepticism, Snover’s idea faced resistance from a company that favored graphical interfaces. Snover’s journey began with a simple mission: to make Windows as command-line managable as UNIX systems. Despite facing pushback and navigating through company restructures, his persistence paid off. This episode explores how Snover’s relentless drive and clear vision overcame numerous obstacles, leading to a tool that is now fundamental in modern enterprise environments. Listen to how one person’s determination can challenge the status quo. Guest Jeffrey Snover @jsnover Table Of Contents 00:00 - Help Adam Find His Next Role 02:10 - Intro 04:10 - The Problem 04:46 - Selling Jeffrey 05:41 - Beat Unix 09:25 - Cultural Challenges 11:02 - Scenarios for Enterprise 12:08 - Windows isn’t Unix 14:56 - Coding Window 26:13 - The .Net Wedge 27:09 - The ReOrg 28:24 - Shell Team 29:38 - Demotion 31:00 - The Monad Manifesto 33:51 - Getting Product Teams on board 37:07 - Having Impact 38:10 - .Net Execution 40:16 - Getting Back into Windows 44:27 - Keep Shipping 46:48 - Azure 48:03 - Outro Transcript Note: This podcast is designed to be heard. If you are able, we strongly encourage you to listen to the audio, which includes emphasis that’s not on the page Adam: Welcome to CoRecursive. Each episode is the story of a piece of software being built. Help Adam Find His Next Role Adam: Before we dive into today’s episode, I need your help. Like truly, I’m on the hunt for a new developer relations role after being laid off from Earthly. During my time at Earthly, I really honed my skills in combining coding with communication. I’m talking about making tutorials, doing conference talks, YouTube videos, the whole shebang. My developer-focused content had a lot of impact at Earthly. And so now I’m looking for a new role where I can keep flexing my software development muscles and my communication powers. And so I’m looking for a DevRel role. I know it’s a bit of a niche, but if you’re aware of one, that’s where I need your help, right? If you know of any dev tool companies or dev op outfits or app security tools, or AI ML places where they need someone who can speak engineer, who can communicate to developers, that’s me. I’m your guy for explaining complex stuff in a way that’s catchy and fun and makes sense to software developers. So here’s how you can help. If you know of any roles like this, let me know. Who should I be talking to? You can reach out to me, Adam@CoRecursive.com or on Twitter @AdamGordonBell. I’ll put a link to my LinkedIn in the show notes and a link to my calendar. If you want to set up a call and just tell me about an opportunity, then go for it. I’m excited to find a new role where I can keep doing what I think I’m pretty good at. I think I’m good at developer communication and hey, maybe you even know about something cool that I haven’t even thought of. And if so, let me know. I’m all ears, but all right, that’s enough about me. Let me know if you can help me out. But now let’s get to the podcast Intro Adam: Today, we have the story of the creation of PowerShell, a tool that transformed Windows system administration forever. And it’s a fascinating story because of all the challenges it took to get it built, especially because at the time that PowerShell was built, the culture at Microsoft was definitely going a different way. Jeffrey: By the way, is it okay to swear? Adam: Oh yeah. You could swear Jeffrey: You know, I had executives say, ‘Jeffrey, exactly which part of fucking Windows is confusing you, Jeffrey?’ Two keynotes ago, Bill Gates got up there and said, ‘Look, here’s command.exe. It’s the last time you’re ever going to see it.’ And he types exit, carriage return, and it goes away. Like, remember that? And then what the hell is this Snover guy talking about? Comes in from the outside, doesn’t drive a freaking Porsche, so let’s, let’s get that in focus. Okay, not driving a Porsche, uh, and he’s saying we need to do command line interfaces. But over here, you look, I mean, I wrote this GUI, I got a promo, John wrote a GUI, he got a promo. Like, who’s the person who wrote a command line interface and got a promo? Adam: That’s Jeffrey Snover. He’s now a distinguished engineer at Google. And yeah, he is the creator along with his team of PowerShell. Today, he’s going to share his insights on navigating large organizations and driving change. How to achieve significant outcomes despite strong opposition, right? This is the essence of technical leadership. PowerShell, if you’re not familiar, was groundbreaking, not only influencing other CLIs but enabling Microsoft’s move to the cloud, and maybe most importantly, cultivating a vast network of skilled Windows system administrators. In fact, Jeffrey got his start at Microsoft because, in 1999, Bill Gates, at the height of the relevance of Windows, well, Bill Gates was worried that Microsoft just didn’t understand the data center, didn’t understand the enterprise market. That’s where our story starts. The Problem Jeffrey: So this was kind of a fixation. Hey, how do we make sure we’re not complacent? That we’re not sitting there patting ourselves on the back and saying, Hey, this is great. So yeah, we’ve been wildly successful with the PC. But look at all this money being spent in the, in the enterprise software marketplace. We want to be good at that. What do we have to take? Now, again, there’s the organization, then there’s the leadership. Like Bill understood that. And so then when the organization tried to do it, tried to do it, tried to do it, that’s where they had that famous, you know, meeting where he’s like, ‘Okay, I think we need some help.’ Let’s bring in some people who understand the problem better. Selling Jeffrey Adam: The problem was clear. They didn’t understand the server market. Their executives were skilled in the personal computer realm, but they lacked enterprise experience. That’s when they found Jeffrey. Jeffrey: A Microsoft executive who I met a couple of times before, just an awesome guy, Dave Thompson, reached out and said, ‘Well, Jeffrey, I’d like to talk to you.’ I was like, ‘Well, okay, I’d like to talk to you, Dave.’ Anyway, at the end of that conversation, I ended up having my final interview with Jim Algen. He wanted to hire me and I said, ‘No, your software is crap.’ And he said, ‘Jeffrey, I know, and I need help.’ And think of it this way: if you do that, if you come here and help me fix this, think of the effect you’ll have on the world. That was like a laser-guided missile to my psyche. It’s like, ‘Yeah, I’m in.’ Beat Unix Adam: The goal was clear to establish Microsoft NT based operating systems in data centers and to outcompete Unix vendors like Sun and IBM and HP. Jeffrey: How do we do that? And the answer is, we do the same stuff, but with a lot lower cost, right? Same capability, a lot lower cost. One, we have an intrinsic price advantage because we were on Intel and most of the Unix vendors were using it as a mechanism to sell proprietary hardware. So we had Intel, but we also had this like open hardware ecosystem. So that gave us a structural advantage. So now our software just had to be as good and boom, we win. Adam: But the software wasn’t as good, especially for managing many servers. You are physically clicking a mouse and configuring things on every machine and the setup you need for each business might vary. Jeffrey: A bank is different than an industrial control process, is different than a, you know, a scientific lab. Everybody’s different. And so basically, if the scenario doesn’t work, then what do you do? And the answer is IBM Professional Services. Jeffrey: Or, Arthur Anderson or you know, systems integrators, right? So now all of a sudden, I say, ‘My hardware costs 10, my software costs 2, and then my systems integrator costs me 40.’ Like, wait a second. In that equation, the hell, we’re right back with these guys. So the key thing was you can’t have systems integrators. Like if you have systems integrators, the whole price equation is broken, and it doesn’t work, and then they get all the value, and then they also own the customer. So anyway, so you had to get rid of the systems integrators. And so the answer is, well, in-house systems integration, right? That’s what the Unix model was. Unix guys aren’t going out there hiring IBM Professional Services, right? They were doing it themselves, right? Because they were programmer admins. So that’s what we’ve got to do. We’ve got to develop our own professional class of administrators, people who can do more than just click next. They become our systems integrators. They’re not going to cost what IBM is going to cost. It’s an upfront thing. It’s a salary thing. And then they can become heroes, and so how do we do that? And the answer is of course, the Unix composition model, right? Have a standard, tool chest of small tools that then these people can put together to solve unique problems, automate it, and then go solve the next one, the next one, the next one. Adam: This plan is what Jeffrey calls a plausible theory of success. It’s not probable, right? But it’s plausible. There is a path where this could possibly work. And so Jeffrey joins at Microsoft’s Windows Server team to help with this mission. Jeffrey: Like literally I faxed in my letter, fax, it was a while ago, faxed in my letter of acceptance. And 20 minutes later, I get a call from Dave. And I’m thinking, Oh, wow. I was like looking at the fax machine. That was pretty quick. And no, it wasn’t. He called me up to say, Hey, Jeffrey, just heads up. There’s been a reorganization and, and, and you’re not going to be reporting to me. You’re going to be reporting to this other person. I was like, what? Like, oh no, I already faxed him my letter of acceptance and hit enter on my email to my boss saying adios. So I was like, oh, holy schmoly. But the idea was that they were, he owned Windows Server and what they were doing was they were taking the management technology out of Windows Server and putting it, combining it with some management products. So it’s going to be a management division. And so he says, you’re going to be the chief architect over this management division. I said ‘so is it, is this a good thing?’ And he says, ‘yeah, it’s a really good thing.’ And I think that was the only time Dave ever lied to me. Cultural Challenges Adam: This management division, right, they’re all about making Windows easier to manage. So it’s still really what he signed up for. But the problem is the group is made up of these people who’ve been succeeding by seeing the world through this lens of personal computing. Jeffrey: Now, again, they had. business server, right? you know, an admin in his server, right? Boy and his dog, you know, you got one server, you go there and all that sort of sensibilities, you know, you walk up to it, you have a mouse, you have this little screen, like happy days, click, click, next, next, next. And they have been very successful with that, right? So again, they were thinking, Hey, that’s the way to do it. This is all good stuff. And I was coming in saying, well, Yeah, but, uh, for the enterprise, you know, we’re going to have data centers, which are going to be large Adam: You can’t be expecting people to walk up to each server and log into it in a data center. Jeffrey: And they said, no, no, no, we got that problem solved. I said, okay, I didn’t know that. How do you solve that problem? Remote desktop. wait, but it’s still, you connect to one of those machines, right? So yeah, you connect it. It’s like, that’s not going to work. this is a server. Another example was when somebody was producing a, uh, service and I said, okay, well, uh, you tell me about the service that you have, the server and they’re like, blah, blah. I said, okay, but how do you start it up? And they said, well, uh, you know, you start it up from the, you enter a command line to start up. I said, okay, but. How’s the user going to do that? He said, well, that’s how they’ll do it. I said, wait, so you’re saying every time they start the system, someone has to log in, enter a command line to start your process. They said, yeah. So, okay, that’s not going to work. So, you know, just the whole sensibilities were off, Scenarios for Enterprise Adam: The Microsoft team, for their part, considered Jeffrey old fashioned, right? He was discussing how Unix and mainframe vendors were tackling issues. But the modern Microsoft way was a user interface for each problem. So why not just do that? Jeffrey: And the answer is that’s not the model, right? It’s a toolkit approach. It’s the small tool approach, right? You have a tool chest of small tools. And then, as unique problems come up, an admin will compose these tools in different arrangements to solve that novel problem, and then it goes away, and then you iterate, iterate, iterate. And so their mindset was, well, no, that’s a bad idea, because it requires, you know, a smart admin to do something. Instead, tell us what the We’ll put it into our queue. We’ll spend a couple of years developing it. Then we’ll ship it as a new product. Then if and when they adopt the new product, problem solved. It’s like, but immediately after you sell that product, the scenario changes. So we’ll then tell us again and the next version will include it. It’s like, you’re just not getting it. Windows isn’t Unix Adam: initial solution for managing Windows was to use Windows services for Unix. This was something you could download, add to Windows, and get a Unix shell. And you’d have awk whatever else. And his theory was a skilled administrator could learn these tools and then use them to manage servers. Jeffrey: Then turns out didn’t solve any of the problems. And here’s where I began to understand the true, where you are, uh, versus where you want it to be. There’s a core architectural difference between Unix and Windows. Unix, everything is, it’s a file oriented operating system, okay? So everything, if you can manipulate a file and restart processes, you can manage everything, right? That’s why AWK/GREP/SED those are management tools, because they manipulate files. Okay. So then when I said, okay, great. Take AWK/GREP/SED and a shell and all that and put it on Windows. Happy days, right? No, because Windows is not a file oriented operating system. There are very few files, uh, in, in, in Windows. Instead, everything’s behind an API. Right. So awk didn’t work against the registry, sed didn’t work against Active Directory, grep didn’t work against WMI, Windows was all APIs that call, and then you get structured data back. So none of that stuff worked. Adam: The only thing close to a solution was WMI, Windows Management Instrumentation, and it allowed for some management tasks, but it was largely underused. Jeffrey’s group saw this as an opportunity. They could develop command line tools to manage everything. User setups, network configuration, application installation, all using this WMI. Jeffrey: I remember the meeting when I was there and I finally convinced my exec that yes, this is what we had to do. And then she said, okay, get it. I totally get it. Which 10? I said, which 10 what? She said, well, which 10 command lines should we do? I said, well, we need to do thousands. So yeah, yeah, yeah. But we’re probably going to get to 10 this release. So which 10 should we do? I was like, Oh my God, we’re doomed. Adam: Is there really thousands of APIs sitting in Windows? Was this? Windows 2000. Windows xp. What was, Jeffrey: Yeah. So it was XP and beyond. Yeah. I don’t know. Pulled that number out of my ear, but yeah, probably, you know, depends on what you call a command. yeah, you need like lots of commands focused in on the objects in WMI. Okay. That’s why you probably need thousands. Okay. So, so then the next step was, okay, I need more than 10. I was able to secure funding to get contract engineers to do this. , the existing engineers are like, well, the command lines, they’re beneath me. You know, you’re off talking about this crap, but nobody cares about it. You’re wrong. And, uh, I’m not going to waste my career, you know, spending time on this shit. Coding Window Adam: But Jeffrey still had this vision clear in his mind, right? It was the reason he came to Microsoft, enable a system administrator to script and manage hundreds of headless Windows servers in a data center and Windows 2000 had just shipped. So there was an opportunity to integrate some of these commands into Windows XP. Jeffrey: And they’re like, yeah, but the coding window is, uh, 10 weeks. 10 weeks? F me. What the hell are you talking about? 10 weeks? Wait, what can you do in 10 weeks? Oh, we can do a lot in 10 weeks. I don’t think so. Obviously, I misunderstood something. Um, I’ll have to wait till my next window of opportunity to figure things out and do something. Well, it turned out when they said, “Our coding window is 10 weeks,” what they were doing was they’re just writing code for 10 weeks. And some of that code would be like a function. Uh, comment, fill in this function, check it in. And then, so you have 10 weeks worth of that. Then nothing works after those 10 weeks, nothing. And then they spend like a couple of years making it work. It’s like what the hell is that? Adam: Yeah, why did they just check in like a stub of a function? Jeffrey: Oh, because, because, uh, after 10 weeks, you couldn’t do anything new, but you could fix the bug. The bug is doesn’t work. I mean, how crazy is that? Adam: So Jeffrey’s group pays contractors to write these command line interfaces that use WMI, they managed to implement 70 actions. But really that was barely making a dent in the needs of administering Windows servers. It was a start though, right? So they decided to seek more funding and continue this work. Jeffrey: So now let’s bring in another weird aspect of, of Microsoft. Nothing goes out the door unless the test organization signs off on it. Okay, great. And I will tell you that there’s an incredible, There are mountains, mountains of awesome features. a dustbin Microsoft dustbins, because, uh, they developed this stuff and didn’t have the, the test organization’s bandwidth, to sign off on it. And then I had some issues with them signing off on the 70 commands that I had done, but I got that right. Cause we spent a lot of money on this. It’s like you, you were going to find a way to fricking sign off on these things. I was like, okay, but if I do this again, this is gonna be a problem. So what I said was think about HTML, right? they test the browser, but then you don’t have the test organization test every HTML page. Adam: This is clever. Instead of writing code for each command, you know, you have a format for setting up a command. Describe it in XML or whatever config format, how each command works. And then you make this generic command builder and the organizational trick is saying that any specific commands config is just metadata. No need to test them. Just test the tool. And bam, the, the testing. Organization’s bottleneck in the process has been sidestepped. Jeffrey: People fought me against that. Like, that’s a stupid idea. You know, it’s going to take forever. Nobody, they just didn’t see it. It’s like, okay, and here’s one of the life lessons. every now and again, you just need to know when to be a butthead. So this was one of those moments, right? If you’re always a butthead, , you’re never going to get very far, but if you’re never a butthead, it probably means that you’re not taking enough courageous changes and driving a reticent organization where it needs to be. So this was definitely one of those times. They did not want to go down this path, but I was just adamant. It’s like, no, we must do this. Adam: Jeffrey won the battle for his metadata driven solution, and over his Christmas vacation, he diligently produced metadata, and he got done 72 commands. Jeffrey: And I was like, Oh my God, you know, I just spent basically 4 million at a long time to get these 70 commands I spent. I forget what it was. It was like 60, 000 to get this engine. And then I spent my Christmas vacation and I got more commands than them. It’s like, wow, this is, this is gold. This is good stuff. And then I said, Hey, you know what? That engine is missing a number of features. Can I get some more money to improve it? And they said, Yeah, yeah, we can. And so we did that and we added a bunch of features, right? Filtering, formatting, et cetera. And here’s what the magic happened. We added it to the engine and all 72 of those commands got better. It’s like, Oh, this is gold. This is gold. And what I had discovered was, with my architecture, I have a base investment. And then the incremental cost to write these things is really, really small, right? It’s really flat. Incremental cost to add new function. And I can invest in the engine and the value goes up. The functionality goes up for everything. Adam: Now these commands aren’t PowerShell, right? This is WMIC, an earlier solution. But if you’re familiar with PowerShell, this might seem familiar, right? It’s generic, it’s data driven, things are coming together. But still, the problem of getting coverage for everything in Windows is an issue. And at this point, Bill Gates and NET enter the picture. Jeffrey: Bill’s a, you know, an interesting guy, um, more of a business guy than a technology guy, in my opinion, as technology, he’s got opinions, etc. But, it comes to business, guy’s a freaking genius. And so here was the challenge. we had shipped XP. XP was an excellent operating system. Excellent operating system, right? We got rid of the crap old, you know, sort of handcrafted kernel, brought in Dave Cutler, god of operating systems. You have the NT kernel now available for everybody. So we had this awesome operating system and we couldn’t get anybody to use it. Jeffrey: They were all stuck on Windows 98. Windows 98 was demonstrably unfit for purpose and we couldn’t get people off of it onto XP. Adam: What Bill had figured out is that if people won’t get off Windows 98 for something good like XP, Then once they get on XP, how will we ever get them to upgrade? Jeffrey: And so he realized that he had a severe business problem. And so he said, Here’s the way we’re going to solve this business problem. We’re going to draw a line in the sand. And we’re going to engineer a Windows 95 moment. A Windows 95 moment when the world changed. There was the world before Windows 95, and then there was the world after Windows 95. And this new, moment is going to be called Longhorn. Um, and it was going to have a new way to do develop code. net. It was going to have a new graphics model, WPF. It was going to have a new communications model, WCF, and it’s going to have a new, uh, storage model Okay. Just a whole new way, you know, just a, a complete generational shift and no one was going to be allowed to invest in anything that wasn’t, uh, this. Okay, so I’m going to engineer this moment and transform the industry. It’s actually brilliant thinking. Didn’t work, but brilliant thinking. Adam: Now Bill is a business genius, yes. But by this time Jeffrey also knew Bill was a force of nature. Bill was famous for his Bill G reviews, where executives would pile into a boardroom and give him status updates on various projects. Jeffrey: So I remember one it was so bad. It was so bad. It was so bad. So we’d been beaten up, beaten up, beaten up. And then. This one time, we’re going to have three topics. Jeffrey: The first guy, , he’s going to come in and do exactly what Bill told him not to do in the last meeting. The second guy is coming in, has made no progress since our last Bill meeting. It’s like, oh my god, we’re going to get killed. And then the third guy has a new topic. Talk about diagnostics and it’s like, okay, well, that’s actually some pretty good stuff. So we’re, we’re in good shape here. Anyway, so our exec was out that day. She was sick or something. And so we go in and it’s like, well, it’s just us. It’s like, this is going to be bad. first guy talks, what guy who did exactly what Bill told him not to do. And Bill’s all smiles, like the hell. That’s weird. Second guy is okay. Here’s where it’s going to put, brace for impact. This guy’s going to get it. goes through it, builds smiles. Oh, this is great. Fantastic. Great progress. All this. I’m thinking, Oh, you know what? He hates our exec. That’s what it is. It turns out that’s not what it was. What happened was, was that that morning, just before that meeting, he, as part of his, you know, other work, he had acquired a bunch of patents for malarial treatments. And so he’s riding high on that. Anyway, we didn’t know that. So we’re just like, what the hell? Adam: Then Jeffrey’s up. He’s covering changes to diagnostics, for a person that’s out. It’s not his area, but the changes that are being covered make a lot of sense to him. Jeffrey: I start to present, like the first slide, Bill’s face changes, like a bad Mexican meal just hit his intestines. And it’s just a matter of, like, minutes before he’s losing it, right? And he’s screaming at me, ‘YOU FUCKED HIM! YOU FUCKED HIM! YOU FUCKED HIM!’ Spit. I’m not making this up. Spit is coming across the conference room table and landing on my glasses as he’s like, ‘YOU FUCKED HIM!’ I have no idea what the hell he’s talking about, but he’s so, and it’s like, wow, this other execs on the other side of the table is like, Bill, Bill, Bill, Jeffrey didn’t fuck those people. I fucked those people. I’m like, what the hell is going on? And so, I just keep trying to go on. And, uh, what had happened was he got very excited about. Uh, diagnostics in the past and somebody had convinced him, oh, the key to diagnostics are these Bayesian networks, blah, blah, blah. And so, they had these things, these Bayesian networks troubleshooters and they shipped them in windows. Bill just loved it, right? AI. He loved these things. Adam: The problem is, these things didn’t work. And even getting them translated into all the various languages that Windows supports was going to cost a lot of money. And so the decision was made to just stop shipping them. Jeffrey: Bill thinks, ‘Oh, you guys fucked up. My favorite Bayesian network, uh, diagnostics people.’ So that’s why he was screaming at me and it was like, okay, well, can we get beyond that and just talk about this? The answer was no, but he didn’t say no. and so I felt it was my responsibility. Like, hey, I got to speak truth to power here. So let me just break it down. Bunk, bunk, bunk, bunk, bunk. beat the crap out of me. People were like, ‘Oh my God, you were like a fricking Weeble. He’d knock you down. You just get back up.’ He says, we were looking for a towel to throw. It was so bad. And it really was. It really was. So at the end of that meeting, right? Concludes, didn’t conclude well. I ran out of that conference room and ran to the men’s room because I thought I was going to puke. I just hovered over the toilet for like, It was bad. It was really bad. I mean, no joke. Adam: So Jeffrey made it through that tough review. But then he realized from these other Bill G meetings that Bill was really pushing everyone towards .NET, aiming to shape that specific outcome. The .Net Wedge Jeffrey: Right, because that was when he was beating us up, .NET, .NET, .NET. I was like, man, that’s just not going to help me. but okay, well, let’s find out what he’s talking about. So, that’s where I began to say, ah, this can help me. Bill’s going to go get everybody to give me coverage. Cause I can’t get anybody to give me this Windows Management Instrumentation (WMI) coverage. Cause I, I did not generate that. loop where everybody said, oh, I can’t wait to write a WMI provider. I get so much value from it. So it wasn’t getting coverage there, but he was going to get me coverage for .NET. And so, all I had to do was to write the utilities on top of it. So that’s, that’s, that’s how I got down that path. Adam: .NET was you using Bill’s anger to get, to get you leverage interally? Jeffrey: Exactly. The Bill Bully puppet. Adam: This .NET insight gave Jeffrey an even stronger plausible theory of success. But of course, things wouldn’t stand still while he tried to execute on that. Because, at the same time, his org’s leadership was struggling to figure out priorities. The ReOrg Jeffrey: Windows Server kept saying, hey, I, I need you to do X, Y, and Z. You know, I’m willing to work with you, I understand what you’re doing, but I need the following things. And they weren’t getting what they needed from the organization. You know, the execs like, nope, I’m going to do this. I’m not going to play ball. Uh, and then one day she woke up and half her organization was gone and it was moved to Windows Server. And so then it was remained, went into crisis, dysfunction, et cetera. And so because of that, because of my role, right, I’ve been in this role of like, okay, figuring out the products, et cetera. And that’s effectively stopped, right? Stopped having staff meetings. There’s no all hands, there was no message. I mean, he just went into complete dysfunction. It’s like, oh fuck, what do I do? What do I do? And it’s like, okay, well this isn’t gonna last for long, right? Something’s got to change here, but I got nothing to do. Adam: There are many ways to handle a reorg. One option might be to reconnect with the Windows Server Group and try to focus on this broader goal of helping them understand the data center and the enterprise. But despite his organization virtually disappearing, Jeffrey stuck to his plan. It was still plausible and he was excited about its potential. But how could he keep it moving? Shell Team Jeffrey: Turned out there was another group in some other organization that were producing a shell. And so, I said, ‘Hey, you guys, let me tell you how the right way to do a shell. Right. Here’s how you should do it.’ And they’re like, ‘I don’t know what you’re talking about. We’re just going to take K-shell and ported it.’ I said, yeah, but we should do better. Like, cause you’re, you’re going to run into the problem. I ran into when I did the first thing, you’re going to have a shell and then nothing’s going to talk to it. So you’re not going to move the ball forward. And so I tried to explain it, tried to explain it, they didn’t get it. And so as much as I tried to explain the ideas, they didn’t get it. I said, you know, just go away. And I’m going to lock myself in a room and then produce a prototype of what became PowerShell. And so I produced basically a 10, 000 line demo prototype of this that had all of the core architectural principles of PowerShell. And I invited them back. I said, ‘Let me show you something.’ And I was able to show them and they said, ‘Well, what about this?’ And I showed them. And they said, ‘What about that?’ And I showed them. And I said, ‘What about this?’ And I showed them. Their eyes just got big and they’re like, ‘This, this, this.’ And so I said, ‘Okay, great. Go do this.’ And then, eventually, they were going to go do that and I helped them get funding. And then I realized like, man, this is probably the best idea I’ve ever had in my life. I should go work on this. Demotion Adam: Getting himself on that project though, that wasn’t easy. Jeffrey: Like, ‘Hey Jeffrey, wait a second.’ We hired you in as this industry expert. You’re the chief architect for our products and our services, varied from like 700, uh, You know, 1, 200 people, and now you’re going to want to go work on something that’s got a team of like a couple people. I said, yeah, but we need to fund it tomorrow. They said, okay, but it’s a couple people and it doesn’t matter. It’s a command line interface. It’s like, but this is important. Like it’s a command line interface. And I said, well, I know I want to work at this. And basically the answer was, ‘Well, okay, we’ll let you work on it, but you know, you can’t keep that job.’ We’re going to take that away from you. And so, yeah, so I got demoted. It was really painful actually. So I didn’t, you know, I didn’t tell any of my friends. I didn’t tell any of my family. It just was so painful. I’ve obviously I had to tell my wife because it had a big economic effect on us. And because my focus was on having an impact on the world, that was far more valuable than, you know, a title. It’s far more valuable than, you know, money, et cetera. So I said, you know, I think I can change the world with this thing. And okay, I got demoted. That sucks. It’s gonna cost me a lot of money. That sucks. But if I can pull this off, and get it in Windows, man, I could affect the lives of many millions of people. The Monad Manifesto Adam: Jeffrey’s new team named their project Monad and lacking staff. They outsourced some of the work to India. And so to align everyone with the project’s vision and this plausible path to success, Jeffrey created the Monad Manifesto. And although the collaboration with the India team eventually dissolved, this document became a cornerstone of the project. Jeffrey: Basically laid out the argument in very clear notion like, hey, what is the problem? What is the traditional approach? What is our new approach? Yeah. Why is this valuable? How does it differ from other things? And then at the end had its concrete, Jeffrey Moore, market relevant statements, which I found to just be incredibly hard to do. But then once you have it, It provides incredible clarity about what you are, what’s important and what you must achieve and where it’s okay to not achieve things. And it basically takes the format of for user who qualifier offering value proposition unlike alternative offering differentiator and then repeat, repeat, repeat. For, Admins who need to manage Windows Server. PowerShell provides a comprehensive, coherent, composable management solution, unlike command.exe, PowerShell provides a common parser, PowerShell provides consistent formatting, PowerShell provides a pipeline, blah, blah, blah, more details and all that. Unlike Unix shells, PowerShell, da, da, da, da, da. And then for users, for providers, for, uh, development teams. Adam: One thing Jeffrey did was outline all the stakeholders for this project and how it would benefit them. Because this whole thing had a huge bootstrapping problem. Jeffrey: You know, at Microsoft, everybody’s hair’s on fire and they have 10 jobs and they got to decide which nine they’re going to fail and not, not get fired. Right? And the command line interface is something that, Not doing it would not get you fired and doing it wasn’t going to get you promoted. So I was like, okay, but my success depends on all of those teams giving me coverage. Like, how’s that going to work? So I had to articulate that I’m going to knock on their door and ask them for the code that they and only they could write. and only that code. And I’ll provide all the rest of the code. I provide the formatting. I provide the sorting. I provide the filtering. I provide the parser. I provide the remoting. I provide the, you know, elevated privilege. I provide all of that. You don’t have to do all of that. Any of that. You just need to tell me how to manipulate your objects. Okay. Um, and then you don’t have to test all that. Like I test all that. Getting Product Teams on board Adam: With the Monad Manifesto as his clearly articulated theory of success, Jeffrey now just needed to make it happen. Convincing the product teams was crucial, and so he met with the Active Directory group, and attempted to win them over Jeffrey: Knock on people’s door. They’re like, eh, you know, you’re one of the nine that I can fail and not do something. It’s like, yeah, but come on. And at that point you have a little bit of personal credibility and you get people to try something like, can you just like, Have some people do this because I think you’re going to find it’s easy. you know, get people to spend a couple of weeks on it. I’m trying to remember who had the weight with who, but we got them to, to invest a couple of weeks of some somebody’s work and we worked with them and they were able to develop a couple of cmdlets and then they had user groups, you know, you continually talking to the users, et cetera, and they showed it to them and they just got such a strong reaction that there’s like, Oh, okay. And then the customer reaction is like, Oh my God. My engineers tell me this is good. We should do it. So let’s do it. And so now a rational product owner looks at this and said, Okay, that’s a good deal. Let’s, let’s finish that work. I liked how little it cost me and I liked the reaction of my customers. So let’s do it. And then they helped get the next one, which helped get the next one. Adam: At this point, the capabilities of PowerShell were expanding, and although there was a big bump in the road still coming, the Shell team was very busy building things out. Jeffrey: We would be in somebody’s office at a whiteboard like brainstorming. There’s a group of us and just like, hey, drawing pictures and what about this? And always challenging herself, like go big, go big like that. Don’t go for the easy solution. What’s the bigger answer? When somebody had done something and everybody rushed to their office to see what they had just done. Um, yeah. realized at some point, like, oh, yeah, this is, this is familiar. You know, computing used to be fun. And then it sort of wasn’t fun anymore, but this is fun again. And as I thought about that, I realized that, you know, that the mouse is antisocial, The GUI is antisocial, So what’s that mean? you have a problem to solve and you solve it with the GUI. What do you have? A problem solved. But when you solve it with a command line interface in a scripting environment, you have an artifact. And all of a sudden that artifact can be shared with someone. By the way, the way you did it can show cleverness. I’ve never seen anybody use a GUI in a clever way. Ever. There’s no cleverness to it. No, like, Oh my God, you should see the way Adam clicked that mouse. Oh my God. Guys, guys, guys, guys, come on, check it out. Adam’s going to click the button. Oh my God. That’s amazing. It just doesn’t happen. Scripting, you’re using a language, right? You’re communicating. It’s like, Oh my God, did you see what, Proust did, that’s phenomenal. This guy’s a freaking genius. And then, Hey, give that to me. I’m going to steal that technique and apply it to my code. Or then I have this artifact and I publish it and people are using it. There’s a debt of gratitude. Like they owe me a beer. Right? Or they’ll come back and they say, Hey, Jeffrey, uh, why’d you do it this way? Why didn’t you do it that way? And it’s like, Oh my God, that’s a better way to do it. Thank you. And so it’s a social environment. Anyway. It really was this moment, like, man, we are having a blast. Like, and it hasn’t been that way in years. Having Impact Adam: For Jeffrey, this made the demotion feel worthwhile, right? It gave him a sense that there was potential impact here. For many at Microsoft, a Porsche or a promotion or the total compensation package were the important rewards. But Jeffrey was measuring things differently. Jeffrey: It’s that, uh, effectively I’m an existentialist. Okay, so what’s that mean? That means I don’t believe in a life after this. I don’t believe in any grand narrative. I don’t believe any of that. I believe life is what you decide to make of it. And that caused a lot of like when I went through that kind of discarding of, of, uh, comfortable beliefs and embracing of this. That was a very difficult period. But then after that I said, okay, well then, what do I want to make of this? And it really came down to, uh, I’ve got a certain number of amount of time and I want the world to be different and hopefully better, uh, because I existed. Jeffrey: It’s that simple. So it’s really about having impact and, and, and mattering that made me say, yeah, I want to do this. .Net Execution Adam: With that conviction, PowerShell looked like it just might happen. After all, it’s attached to this great new .NET operating system initiative, Longhorn. Jeffrey: So then began the boufforama, right? A bunch of engineers making terrible decisions about how and where to adopt .NET. And the reality was, many of these technologies like, Hey, if you had done it over the course of, you know, five to seven years, that brilliant idea. They were like, no, no, we just know, we just know. You set a deadline and our engineers will rise to the moment. They’ll sleep here nights, weekends, they’ll get divorced, but the hell with that, they get the, they’ll be making enough money. They can afford a new family. I’m not making this up, but you know, people, people will rise to the occasion and when they tell us it’s not working, we’ll say, make it work and they’ll do it because they have, they’re a really amazing group of people. Until they weren’t and it didn’t work this time and they couldn’t force people and it couldn’t, you couldn’t force reality. They couldn’t make it work. Adam: Take Notepad, for example. It was native code, and it used around 15 kilobytes of memory. Jeffrey: Well, somebody got the idea that said, Hey, you know what? We’re one way to make sure everybody’s using .NET is, uh, I’ll rewrite the common control dialog boxes using it, you know, use WCF. So now all of a sudden I got this native code notepad. I bring it up, starts instantaneously. Everything’s fast, fast, fast. And then I say save as, and the dialog box comes up a minute and a half later. And I go from a 15 kilobyte working set to a 15 megabyte working set. And then I can save my file. Terrible idea. Terrible idea. Other people were looking to figure out, hey, how can I get .NET in the kernel? I think it got to some point. I don’t remember the details, but it had been like maybe seven months that the nightly builds didn’t work. And so there had been this, okay, no new code, only code to fix the builds and, uh, make things work. And then that wasn’t working. So they basically had to do a reset. Getting Back into Windows Adam: So the Windows people decided that .NET was the problem, when perhaps they had just bitten off more than they could chew. But PowerShell got pulled out of Windows, like all the .NET code during this reset, and now Jeffrey has to find a way back in. Jeffrey: So, that, that began like a multi-year process. Just hell as they, they began to really come after command line interfaces, anything that do with .NET. So they just like on a regular basis would come at us and try and get us canceled. First, Bill Gates always got it. Bill Gates got it. That helped me exactly zero. Helped me not one bit, right? Then I had the head of Windows Server. He got it. He helped at like crucial moments, right? There were crucial moments. But like the day to day stuff, uh, you know, it just felt, it was a very hostile environment, uh, very unwelcoming environment. Lots of people throwing rocks at us and, and, uh, and we were just trying to, you know, You know, go underneath the covers and get our job done. Adam: Doing the actual building was actually going really well. Jeffrey: I mean, it was just so delightful. I mean, we would run into each other’s office. Like, Hey, have you seen what, what Bruce just did? And we all run into Bruce’s office and we’d see something did like our heads would explode. Like, Oh my God, that’s fantastic. And, Oh, Hey, I heard Jim’s doing something. We’d all run into Jim’s office and he’d do something. And, uh, you know, you just see it coming together. And there was a reality to it that made you say, Hey, all these people are throwing rocks at me. They don’t know what the fuck they’re talking about. They’re wrong. This is awesome. And we’re gonna, we’re gonna ship this thing. I had partnered with exchange. , and they were a deep customer. So when they’d like have this meeting, like, okay, let’s kill them this time. And I just bring my exchange guys there and they just sit in the corner, they had a meeting. Okay. So we’ve agreed, uh, we need to kill this Monad thing, the exchange guys would then perk up and said, no. I have a multi billion dollar business. I’m betting on this. I need this. You can’t kill it. Adam: Even though the project survived, it had funding, it was still not part of Windows. It needed a way to get back in. Jeffrey: WinArch, WinArch was the windows architects. They got together and said, okay, here are the seven things you have to do to get .NET into windows. And these were like incredible draconian set of things that, you know, some of them were based in good things, but they were just phrased in a way. It was very clear, like, No, yeah, we’re going to keep .NET out of this. So again, everybody looked at that thing and everyone bailed except for two people, Scott Guthrie and myself. he, he cheated. He, he, he shipped outside of Windows. I tried to figure out how to get back in. Adam: So what’s the process to get into windows? Like, is it, Jeffrey: Yeah, well, you have to have a formal, uh, design change process, right? What I said was, they’re going to go out of their way to find any, uh, you know, weakness in our argument. So we can’t be green. We got to be greener than green. And so we did that for everything, everything, everything, finally got our story together. And we said, okay, we’ve sent the request. Like 20 minutes later, the executive sends back mail saying, withdraw this request. Like the head of windows, withdraw this request. And so my program manager, Jeffrey, come here. Yeah. Look at this mail. I just got from Brian. Like, oh my God, what should I do? I said, I, I don’t know. He said, I know what I’ll do. His response, he says, no. He says, you turn it down. And so he says, okay, fine. I, I turned it down. Adam: Turning things down formally, it triggered a review process. This process involved the Windows Server Organization, the group Jeffrey was originally supposed to work for. Jeffrey: And the head of windows server had dialed in that this was critical to his business. And so he was a supporter and he basically said, Hey, this is critical to my business. I own this decision. If it meets the requirements, it’s going in and it did meet all the requirements. And so, and the, and so then there’s this big battle between the executives. It was actually messy, but ultimately, uh, the Windows server executive prevailed and we were able to get back in. Keep Shipping Adam: So they shipped it. PowerShell 1 was released as part of Windows Vista. But for Jeffrey and his team, the work wasn’t done. Jeffrey: Even before it shipped version one, people like, my manager’s like, okay, what’s the next thing you’re going to do? It’s like, I’m going to ship this. I say, yeah, yeah. But after you ship it, what are you going to do after that? And it’s, well, it’s not done. I got to do the next version. Like, no, no, you’re a senior guy. You need to do something else. Otherwise, you’re going to get pigeonholed. And it’s like, no, I’m going to do this. And it’s like, well, it’s going to hurt your career. I say. Whatever. And so then that happened with version two, and then it happened with version three, and then it happened with version four, Adam: Each version, each release, was executing towards the end goal of successful server administration that Jeffrey had laid out in his manifesto. Jeffrey: Version one, we got this far. Version two, we filled out these things. Version three, these things, version four, we pretty much came to completion of the vision. So, you know, it just took that long to do. So that was me personally. And again, being warned all the time, “Hey, this is going to affect you financially. It’s going to affect your career, et cetera.” I was like, yeah, I don’t care. I’m just going to do this, man. It’s the right thing to do. And eventually it worked out okay for me. Adam: That sustained focus paid off. For those who invested in PowerShell, their skills grew more valuable over time. Instead of frequent disruptive changes, the core ideas remained consistent, and each release made their skills more valuable. In big picture, what’s truly fascinating is how PowerShell bridged this gap between the administrator and the programming world. Windows administrators who never saw themselves as coders were suddenly writing scripts and automating complex tasks and PowerShell created this whole community of people helping each other out. Server admins forming PowerShell user groups. People were sharing scripts back and forth. They were answering questions online. Conferences were coming together. Jeffrey: Yeah, they become a hero. They’re asked to talk at conferences. A number of people have become, you know, professional speakers because of this. It’s amazing. One woman who was so nervous about presenting at a conference, she’s in the speaker room, you know, doing shots. Like, what are you doing? I can’t cope. She goes up there and she just nailed it. She was fantastic. And then she’s at all the circuits, just giving talks all the time now. So she’s a pro. Yeah. Transformed her career. Azure Adam: For Jeffrey, the success of PowerShell diminished the sting of his demotion. But really, the true impact of this work, of this effort, is that Windows Server Administration changed. And that change to how Windows is administered actually enabled a lot. Jeffrey: So it took me five years, but I finally got my stripes back became Distinguished Engineer and then eventually Technical Fellow. At some point I went to work in office and the head of the office, we were having lunch, and he said, “You know, you realize that PowerShell is the reason why Microsoft’s in the cloud.” What? How’s that? He says, “Well, Office led the way to the cloud. Like we were, you know, many years ahead of Azure.” And he said, we never would have been able to get to the cloud without PowerShell. He said, before that when I went to provision a server, it’s click, click, click, click, click. And you couldn’t do that multiple times. And then you, when you got it wrong, you couldn’t fix it. So having scripts allowed us to scale up when something went wrong, we could change the script. And, uh, he says that enabled us to get to the cloud and, office going to the cloud enabled Azure to go to the cloud. Outro Adam: That was the show! Jeffrey. thank you so much for candidly sharing your story. I hope I did a good job of presenting it because it’s an amazing one. I learned so much from you about the importance of a clear strategy for success. Thanks. And in the need to stay focused on these long term goals, despite the daily challenges thrown your way, you know, sometimes you have to be willing to, to take the demotion or face the criticism to keep your eye on that long term prize. Adam: And listeners, Jeffrey is very active on Twitter at @jsnover, and he’s now working in the SRE group at Google. So give him a follow. If you’ve got a PowerShell story of your own or more relevant, if you’ve faced similar organizational challenges, driving change, uh, if you have a story to tell, I’d love to hear about it or let me know what you think of this episode. Adam: You can reach out to me on Twitter @AdamGordonBell or email me adam@corecursive.com. Or join the slack and tell me about it there or whatever. You can find me lots of places And until next time, thank you so much for listening. Support CoRecursive Hello, I make CoRecursive because I love it when someone shares the details behind some project, some bug, or some incident with me. No other podcast was telling stories quite like I wanted to hear. Right now this is all done by just me and I love doing it, but it's also exhausting. Recommending the show to others and contributing to this patreon are the biggest things you can do to help out. Whatever you can do to help, I truly appreciate it! Thanks! Adam Gordon Bell Support The Podcast About CoRecursive A podcast about building software. Favorite Episodes Subscribe to Podcast Subscribe via Email Follow @CoRecursive on Twitter About Links: Monad Manifesto Behind the Manifesto About Episode Release Date: 04 Jul, 2024 Updated Date: 04 Jul, 2024 Download Audio File Permalink RSS From Burnout to Breakthrough Navigating Corporate Giants NAVIGATION HomeSubscribe To PodcastEpisodes By DateEpisode By ThemeAboutDonatePodcast Rankings SOCIAL Twitter Linked in Instagram Slack LISTEN ON Apple Podcasts Spotify Google Play Sticher Overcast CoRecursive © Copyright 2021 EPISODES Audio Player 00:00:00 00:00 49:30 1x 2x 1.5x 1x 0.75x Navigating Corporate Giants ⬆ Podcasts List Navigating Corporate Giants Jul 03, 2025 49 min Navigating Corporate Giants Jul 04, 2024 49 min From Burnout to Breakthrough Jun 04, 2024 52 min Coding Machines May 03, 2024 48 min Code, Kickflips and Crunch Time Apr 02, 2024 57 min Leaving LinkedIn Mar 04, 2024 47 min Beautiful Code Feb 02, 2024 57 min Code as a Lifeline Jan 02, 2024 44 min From 486 to Vue.js Dec 04, 2023 46 min Platform Takes The Pain Nov 02, 2023 48 min Sloot Digital Coding System Oct 02, 2023 51 min Configuring Identity Sep 01, 2023 42 min The Science of Learning to Code Aug 02, 2023 50 min A Dark Room - From Code Hobo to Indie Game Developer Jul 03, 2023 40 min Quitting (And Then Rejoining) Stack Overflow Jun 02, 2023 53 min From Project Management to Data Compression Innovator May 02, 2023 59 min JSON vs XML Apr 03, 2023 49 min Sun's Mobile Blunders Mar 02, 2023 51 min Shipping Graphing Calculator Feb 02, 2023 46 min The Unfulfilled Engineer Jan 02, 2023 42 min DOOMed to Fail: A Horror Story Dec 02, 2022 45 min Software World Tour Nov 02, 2022 48 min Android's Unlikely Success Oct 03, 2022 60 min From Prison To Programming Sep 02, 2022 46 min CPAN Aug 01, 2022 56 min The History and Mystery Of Eliza Jul 05, 2022 44 min Why still 80 columns? Jun 01, 2022 39 min LISP in Space May 02, 2022 38 min April Fools' Is Cancelled (2014) Apr 01, 2022 38 min The Story Graph Mar 02, 2022 48 min Serenity OS Feb 02, 2022 41 min The Internet Is Made of Duct Tape Jan 02, 2022 42 min Cocoa Culture Dec 02, 2021 44 min Leaving Debian Nov 02, 2021 40 min The Original Remote Developer Oct 04, 2021 41 min Quines, Polyglot Code and Other Fun Computations Sep 02, 2021 61 min Full-Time Open Source Aug 02, 2021 46 min The Untold Story of SQLite Jul 02, 2021 38 min From Competitive Programming to APL Jun 02, 2021 53 min Smart Contract Rescue May 02, 2021 34 min Apple 2001 Apr 03, 2021 48 min Video Game Programming From Scratch Mar 01, 2021 41 min Reinforcement Learning At Facebook Feb 01, 2021 38 min 2020 Year End Jan 01, 2021 34 min Frontiers of Performance Dec 01, 2020 47 min The Birth of UNIX Nov 01, 2020 51 min To The Assembly Oct 01, 2020 41 min Memento Mori Sep 01, 2020 40 min We're Teaching Functional Programming Wrong Aug 03, 2020 46 min Software That Doesn't Suck Jul 01, 2020 37 min Unproven Techology Case Study Jun 10, 2020 39 min Krystal's Story May 18, 2020 40 min Learning a new language May 05, 2020 35 min Portal Abstractions with Sam Ritchie Apr 17, 2020 35 min Loving Legacy Code with Jonathan Boccara Apr 03, 2020 26 min The Reason For Types Mar 16, 2020 36 min Karl L Hughes on Conference Talks Mar 02, 2020 50 min Don and Adam Discuss Folds Feb 15, 2020 36 min David Heinemeier Hansson Feb 01, 2020 1 min React and Scala JS Jan 16, 2020 38 min The Business Of Developer Tools Dec 17, 2019 37 min Software in Context Dec 02, 2019 53 min Beautiful and Useless Coding Nov 16, 2019 52 min Tech Evangelism Nov 01, 2019 67 min Language Oriented Design Oct 01, 2019 56 min Open Source Health and Diversity Sep 15, 2019 41 min Learning About Compilers Sep 01, 2019 58 min Advanced Software Design Aug 16, 2019 53 min Category Theory Aug 15, 2019 53 min Using TypeScript Like A Pro Jul 15, 2019 70 min Rethinking Technological Positivism Jun 15, 2019 61 min How to Build a Programming Language May 31, 2019 56 min Refinement Types May 15, 2019 50 min Rethinking Databases Apr 30, 2019 58 min Learning to Think Apr 15, 2019 53 min Data and Scale Mar 31, 2019 56 min Abstraction and Learning Mar 15, 2019 49 min Modern Systems Programming Feb 22, 2019 0 min Recreational Coding Jan 25, 2019 62 min Software as a Reflection of Values Dec 18, 2018 79 min The Little Typer Dec 01, 2018 67 min Big Ball Of Mud Nov 14, 2018 60 min God's Programming Language Oct 22, 2018 60 min Concurrency and Functional Programming Oct 03, 2018 62 min Test in Production Aug 31, 2018 47 min Domain Driven Design and Micro Services Aug 17, 2018 49 min Typeful Functional Streaming HTTP Jul 27, 2018 50 min Moves and Borrowing In Rust Jul 03, 2018 64 min Dependent Types in Haskell Jun 13, 2018 58 min Microservices Architecture Jun 06, 2018 66 min Rust And Bitter C++ Developers May 16, 2018 62 min Distributed Systems May 02, 2018 66 min Graphql Apr 18, 2018 55 min PureScript Apr 04, 2018 51 min Throwaway the Irrelevant Mar 21, 2018 68 min Generic Programming Mar 07, 2018 60 min Total Programming Using Swift Feb 12, 2018 53 min Type Driven Development and Idris Jan 29, 2018 59 min Algebraic Domain Modelling using Functions Jan 22, 2018 58 min Design Principles From Functional Programming Jan 10, 2018 51 min Scala at Duolingo Jan 07, 2018 53 min Incident Response Jan 05, 2018 51 min Scala Native Jan 01, 2018 48 min",
    "commentLink": "https://news.ycombinator.com/item?id=40874013",
    "commentBody": "Jeffrey Snover and the Making of PowerShell (corecursive.com)200 points by todsacerdoti 7 hours agohidepastfavorite182 comments adamgordonbell 7 hours agoHost here, thanks for sharing. PowerShell faced extreme opposition at Microsoft, and its creator Jeffrey Snover was demoted for pursuing it. Jeffrey was originally brought into Microsoft to help MS learn how to compete in the data center, but culturally they were so tied to the personal computer model of the world, that they fought him every step of the way. Edit: Another interesting thing, is how Powershell exists because Windows isn't file based. Jeffrey's goal was server administration, but on Windows you can't just edit files to administer things, you need to call various APIs and get structured data back forth. The rich object model fell out of that. It was the only way. ( Also, apologies if the transcript has errors. I've gone from professional transcriptions to Descript and then a pass of GPT4 trying to find the right punctation breaks and then me doing a quick read through. I don't think its coming out as high quality as I'd like. ) reply 7thaccount 3 hours agoparentIf anyone from MS-PWSH team reads this, I'd love for y'all to add some basic GUI functionality that doesn't involve me having to write a bunch of .NET. I'm sorry, but the reason I like PWSH in the first place is it's a simple dynamic language with lots of easy to use commands that I can chain together. I'd love to have a new set of cmdlets for creating simple user interfaces and charts. For example, something like the below would be so simple for Microsoft to add to the product and remove a page of boilerplate code that I don't really understand well. Create-Chart -Type \"Bar\" -XAxis $Cities -YAxis $GDP -OutputFile \"C:/Documents/ProjectAnalysis/CitiesBarGraph.png\" There are probably users in the millions that are ok at the basics of programming, but don't have the job role to where tools like Java or C# make sense. Python is usually a good fit here, but I really wish Microsoft had something written for us common folks and not just server admins and IT folks. If Microsoft put some more effort into PWSH to where it wasn't turtle slow at things like parsing files and then started adding things like what I talk about above. Maybe even cmdlets for statistics and science...it could be something pretty amazing that your typical business analyst could quickly use to build some really amazing software to do their job better or a prototype for the software team to actually implement in a more robust manner. It's such a really cool technology that has a lot of missing potential IMO. It seems like Microsoft assumes that the three options are full software developer with C#, IT stuff with PWSH, or Excel for the business folks. Excel is really great in a lot of ways, but it is also pretty limited and VBA+Excel is one of the most limited ecosystems I've dealt with. I guess third party languages like Python, R, and so on make for another fourth option, but sometimes I wish Microsoft had spent more time in this area. reply RajT88 2 hours agorootparentYou are looking for something like Kusto then. reply 7thaccount 2 hours agorootparentA query language? I don't think that's what I'm referring to at all if I found the right links. Business users can use SQL quite easily and tools like Powershell or Python make automating that easy. reply thiht 1 hour agoparentprev> I've gone from professional transcriptions to Descript and then a pass of GPT4 trying to find the right punctation breaks and then me doing a quick read through. I don't think its coming out as high quality as I'd like. I tried reading the transcript and couldn’t reach the end, it’s a bit hard to read in my opinion. I assumed it was machine generated while reading it, but I can’t say why specifically. Maybe it needs a bit of editing to be easier to read. Thanks for the effort anyway, it’s still better than no transcript :) reply bloopernova 3 hours agoparentprevAs someone who prefers to read interviews and articles rather than listen, I really appreciate you providing a transcript. Thank you! reply vb-8448 4 hours agoparentprevI wonder why they chose to build something from scratch instead of using python or similar tools that existed before. reply dmd 4 hours agorootparentSomething like ksh, you mean? Like they talked about in the article? reply hobs 6 hours agoparentprevAnd as far as I can tell its no longer a priority at MSFT, reading the complaints from the MVPs on Github about how MSFT promised a bunch of further investment that has not been realized, changes not implemented, good stuff just sort of left to rot on the vine. I loved powershell with (some of) its weird warts, but I have moved on. reply mike_hearn 5 hours agorootparentThere have been regressions even. PowerShell 7 isn't backwards compatible with PowerShell 5 and some features that used to exist are just gone with no plan for a return, due to (surprise) fights between .NET and Windows teams over API metadata formats or something. Most incredibly Microsoft situation ever. Say what you want about bash, at least it doesn't pull stunts like that. reply pjmlp 58 minutes agorootparentAs someone deep into Microsoft ecosystem since MS-DOS 3.3, it feels like the old ways of WinDev vs DevDiv politics have slowly creeped back into daily Microsoft. It is as if DevDiv is now full into UNIX like, poliglot, FOSS culture and such, now under Azure business unit, whereas WinDev is back into how to sneak people into Windows licensing and the usual old culture. reply WithinReason 4 hours agorootparentprevSo the old org chart comic is correct: https://flowingdata.com/2011/06/30/organizational-charts-in-... reply binkHN 1 hour agorootparentWow, the Oracle chart is so spot on. reply oblio 3 hours agorootparentprevAlways was ︻┳═一 (hey, I managed to sneak a rifle past HN's Unicode filter :-D) reply JoBrad 4 hours agorootparentprevAre there any currently-supported versions of Windows that don’t support Powershell Core? I recall installing it on Windows 2012, even. > Say what you want about bash, at least it doesn’t pull stunts pile that. I guess it depends on what you consider to be included in the terminal’s domain? There are entire papers and guides on which commands are considered safe (sometimes only when run in a very specific way), and which variants, alternatives, etc. you should use instead, for Bash scripts, because of the inconsistency in what a command evaluates, does, and returns for various distros. That’s not to hate on Bash, but just to point out that it’s not a strength of Bash vs PS. reply sllabres 2 hours agorootparentWhen Powershell was first released I was primarily working with Unices and was very curious what would work better: The Unix way in cutting, changing and grepping some part of stdout (in some OS a bit simplified b with options for automated pricessing which commands the programm to output colon delimited or otherwise formatted) Or the Powershell way where on can access data members directly. I thought I would prefer the second method more, because the access looked much cleaner/structured. But after all these years (still mostly Unix scripting but Powershell and some other environments too) my mind has changed. Would like to hear what others would prefer' Unix method with some scripting language or windows method with Powershell) reply chaps 1 hour agorootparentCombining both powershell and unix is neat! reply akira2501 1 hour agorootparentprev> because of the inconsistency in what a command evaluates, does, and returns for various distros. I'm not aware of any difference in bash between vendor distributions for which this is true. reply akira2501 1 hour agorootparentprevMy guess from external observations is that the reward and bonus structure inside Microsoft is entirely decoupled from customer feedback and response. The middle managers are in charge of the product and they're fully insulated from any concerns outside of getting this years maximum bonus for themselves. Monopolies always destroy innovation. reply atmavatar 4 hours agorootparentprevI was under the impression much of that is due to the fact that PowerShell is Windows-only, while PowerShell Core is multi-platform. reply paulirwin 2 hours agorootparentThey now refer to the former as Windows PowerShell, and the latter as PowerShell. The Core part was dropped. reply g15jv2dp 5 hours agorootparentprevIt's easy not to lose features when you don't have many to begin with... I mean, what are the features that disappeared? What's the bash equivalent of these features? reply SonOfLilit 2 hours agorootparentFor a programming language, losing a feature is much, much worse than not having it in the first place. reply g15jv2dp 2 hours agorootparentI'd like to see examples to judge. What are the features? Also, are you confusing \"the language\" with \"the standard library\"? reply hypercube33 4 hours agorootparentprevYou can tell by WinGet which is a new product not even close to supporting anything PowerShell by design and almost being anti PowerShell with its overly verbose output. Which is odd since it's right up powershells alley being a management cli tool reply g15jv2dp 2 hours agorootparentIf you have winget and powershell installed, try `Get-WingetPackage` and see what comes out. Or even better: gcm? noun -like winget* help Install-WinGetPackage The \"usual\" winget cli tool is indeed not powershell compatible. But winget also ships with all the necessary cmdlets. You don't have to install anything extra. reply gecko 31 minutes agorootparentI was super excited to see this comment, but I don't seem to have those cmdlets, even though I'm on Windows 11, fully updated. Are you sure you didn't install something extra? reply briHass 3 hours agorootparentprevThere is a Winget client module for Powershell. I used it the other day to write a one liner that could upgrade all with exclusions. I didn't want to pin, because I still wanted to see updates I'm skipping. reply vips7L 2 hours agorootparentprevDoes anyone even use winget? I tried but its repositories never have the tools I need in it. Scoop has been superior for me in every way. reply 5636588 2 hours agorootparentUniGetUI (formerly WingetUI) is a really great tool since it supports multiple package managers. https://github.com/marticliment/UniGetUI reply JoBrad 5 hours agorootparentprevI’m curious about the scenarios that Powershell used to work for you in, and what tooling you’ve moved on to. Do you use something else for Write-Once-Run-(Nearly)Anywhere type scripts? I primarily write Powershell Core scripts for scenarios where I need to execute the same commands on a variety of operating systems, and I know that the script is likely to be maintained by your “typical” sysadmin (highly technical, but not a programmer)in an environment where installing runtimes for programming languages is discouraged. I switched to macOS as my daily driver about 2 years ago, so PS fits these scenarios pretty well, and Powershell Core updated fairly regularly. Sure there are annoying bugs and misses with the built-in and add-on MS modules: networking cmdlets are an almost total miss, Get-LocalGroup (and maybe other commands?) is totally broken on some AzureAD-joined machines, and the Azure and MgGraph Powershell modules still don’t have enough coverage to move on from the legacy Windows Powershell modules (or even to rely on just one of them, for areas they supposedly cover). But overall I’ve been pretty happy that 99% of the time I can write a powershell script once, and it will run on any machine with Powershell Core, in a consistent way. reply hobs 4 hours agorootparentDon't do as much \"local\" sysadmin stuff as most of the workloads moved to the cloud and containers, and that's all automateable via APIs which are much nicer to work with in python with its useful library support. (one of the biggest misses in powershell) There's still a lot of good stuff wrt powershell maintainability by normal humans (though the entire mental model of object output usually throws them for a loop for years) managing local stuff. reply pjmlp 4 hours agorootparentPowerShell has the whole Windows OS libraries, COM and .NET available, without additional installation, a bit more than just Python. reply dh2022 4 hours agorootparentprevIMO PowerShell is very well integrated with Microsoft Azure cloud. Every Azure resource I work with (Storage, VM, Kusto, EventHub, Service Fabric, AAD, Networking) has tons of PowerShell support. I never set time to use Python with Azure Cloud - and this is only because PowerShell is so good at Azure. reply vips7L 2 hours agorootparentprevHow is having the entire C# ecosystem a big miss? reply jodrellblank 1 hour agorootparentIt's a big miss for the casual convenience of a scripter/non-programmer. A Pythonic way to do a remote procedure call is XMLRPC (pseudocode): import xmlrpc svr = xmlrpc.connect('http://remote/') result = svr.add(1, 2) The C# way is to use the Microsoft Windows Communication Foundation (WFC) Client Proxy using the Service Model Metadata Utility Tool and the Web Services Description Language (WSDL) and XML Schema Definition Language (XSD) files from the remote server, declare a public interface attributed as a Service Contract referencing a namespace, generate a class which inherits from the generic ClientBase and implements the new interface, create an instance of said WCF client and call its methods. (Or rely on Visual Studio magic to hide all that) - https://learn.microsoft.com/en-us/dotnet/framework/wcf/acces... In any decision, Python goes for \"What would Guido do?\" and C# gets some union of \"what would a committee of Microsoft, IBM, Oracle do?\", \"What would impress Gartner?\", \"What is Microsoft legally obliged to do, and backwards-compatibly required to do?\", \"What would we do if we tried to do everything everyone needs all in one?\", \"What would Java do?\", \"What would a large team need to design and maintain a stable, typed, large system for years?\". PowerShell is on top of that; there's no simple included graphing and drawing, no simple hooks into Windows own voice recognition and OCR, and definitely not into whatever magically good ones newer Office / Cloud is using, no casual email or spreadsheet handling, no Visual Basic style form building, no simple data science; there's a few things you can do or download, generally less convenient than a Python equivalent. And Microsoft are leaving it all 'to the community' but the community is using Python so that's where the Excel power-user who wants to script a couple of things will go. reply vips7L 1 hour agorootparentI don’t think Python, a general purpose scripting language; and PowerShell, a shell, are going for the same things. They have different goals and by your definition every other tool is a “big miss” because they are not Python. I have never desired to do xmlrpc, ocr, voice recognition, or gui building from my shell (and if I did I still don’t see how importing a C# library would be a big miss). What I do desire to do is open files, read their contents and pipe them into other programs, something that Python makes a pain to do with all the file handling. Powershell definitely excels at this, does that make Python a big miss? reply valiant55 5 hours agorootparentprevWhat have you moved on to? I don't think I could go back to bash after learning PowerShell and no longer parsing string output. reply hobs 4 hours agorootparentPretty much all python, dicts work just as well over here. reply pletnes 6 hours agorootparentprevMy biggest gripe is that some Azure stuff seems to be available only through powershell, which is hard to install and configure (I think) in certain corporate policy-infested environments. reply Uvix 6 hours agorootparentThe Az PowerShell module just uses HTTP APIs, so anything they do can be done with other tooling if you really want to. (There’s also the Az CLI tool, which I don’t like as much as the Az PowerShell module but might be easier to manage in an environment like that.) reply ExoticPearTree 5 hours agorootparentYes, true, they call APIs. But for whatever reason, MS decided that instead of documenting those APIs they went on the route of abstracting them in PS modules or the az-cli submodules that its mind boggling. reply RajT88 1 hour agorootparentThe REST API is documented. Maybe not completely so, but mostly. https://learn.microsoft.com/en-us/rest/api/azure/ reply pjmlp 4 hours agorootparentprevActually Azure CLIs are kind of schizophrenic, in typical MS fashion. You have Azure Powershel Cmdlets (the old 5.1 based, and the new Powershell Core based), AZ CLI (in Python), AZD CLI (in Go). The only one that offers full power is actually the AZ CLI one, e.g. some Kubernetes features aren't exposed in the others. reply nullindividual 4 hours agorootparentprevThe Azure CLI [0] is a viable method of managing Azure infrastructure if you don't/cannot use PowerShell. [0] https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cl... reply RajT88 2 hours agorootparentprevIf it exists in Powershell, it is backed by a REST API. Lots of AZ SDK's include a helper method to make calling the API's easier (like az rest cli command or invoke-azrestmethod). reply lhamil64 2 hours agorootparentprevWhat have you moved on to? I have to use Windows at work and PowerShell is way better than basic cmd, even better with Windows Terminal. I suppose you could probably install bash or another unix-y shell but that seems like it wouldn't be as integrated with Windows. reply lozf 3 hours agoparentprevThat was a great listen, thanks. reply dwoldrich 59 minutes agoprevAt $dayjob I am blessed with the task of wrangling a 20+ year old codebase of SQL Server stored procedures. It's around 300k lines of monkey tested, business-critical code that: * wasn't source controlled * was never tuned (properly) for performance * deployed into environments by editing/executing sql in SSMS. * of course, no automated tests, etc... It's a windows shop, I am developing on a Mac, and we do linux on Github Actions. I selected tools like PowerShell Core, sqlcmd, docker for running Windows SQL Server instances, RedGate SQL Compare for extracting existing schema and code from the legacy servers, tSQLt for unit testing, TSqlLint for code compliance, SQLFluff for style compliance, and Flyway for deployments. We quickly discovered PowerShell Core was the most interoperable cross-platform scripting shell when Windows had to be one of the platforms. It wasn't pleasant to code in. The regex engine comes from .Net, which has bad catastrophic backtracking problems, and the array situation was goofy. Launching executables with any sort of control over the launch and capturing the output stream was hit or miss - I would often have to launch a process, redirect its output to a temp file, and then read the temp file after the child exited. So piping around with child's stdout into a string variable was always more trouble than it should have been. BUT, PowerShell Core executes fast (nice job M$FT, if there's one thing you do well, it's micro-optimizing!) It has nice tools for interacting with the user, like ascii art list pickers and easy input prompt generators. And, most of the strange quirks in dealing with the Windows file system are papered over if one avoids folders with locked files. Anything you want to achieve can probably be done if you search hard enough. Recommended! reply nu11ptr 5 hours agoprevPowershell wasn't awful when I wrote it but I never understood why arrays of length 1 removed the array and become the contained type. This caused a huge # of bugs as you suddenly had to care how many items could potentially be in the array and check each time you modified vs. handling them generically. Does anyone know WHY they did this? reply Arnavion 2 hours agoparentThere's looseness around arrays because the API for emitting outputs from commandlets is also loose around arrays. There is just one function `WriteObject` that writes the value you give it as the output of your commandlet. If you call it once, then that's your output. If you call it multiple times, then the shell has no choice but to then make your output an array of all the values you write. So if one invocation of your commandlet only calls `WriteObject` once and the other calls it twice, the shell in the first case doesn't have the knowledge that it should've wrapped that one output in an array too. And it can't always wrap commandlet outputs in arrays because that would be disruptive for commandlets that semantically only have one result and so always write only one output (like `Get-Date`). And for whatever reason, they didn't want to complicate the API to let commandlets themselves be able to express whether they'll semantically write only one output or multiple, regardless of how many times they actually call `WriteObject`. Such an API can't be a static attribute on the commandlet because commandlets can have wildly varying output based on their parameters. It can't be an overload of `WriteObject` like `(Object, bool iMightWriteMoreValues)` because it has to work for empty arrays. So it would have to be a separate `IWillWriteMultipleValues()` function probably. Edit: Also explained here: https://news.ycombinator.com/item?id=40874873 reply sgbeal 5 hours agoparentprev> Does anyone know WHY they did this? That sounds like a classic case of software trying to be \"too clever,\" nine times out of ten of which will invariably backfire. As a software developer, one should always resist the temptation to make their software \"too clever.\" reply AndrewDucker 3 hours agorootparentAgreed. This drives me crazy. You have to work around it in a script of any complexity. It's probably my least favourite thing about PowerShell. reply JoBrad 5 hours agoparentprevThis is so annoying. I’ve started using the preceding comma hack to force an array. $Ary = @(, \"value\") One cool feature for creating arrays (especially larger arrays, for performance improvements over using +=) is to assign your array to a for loop. It wasn’t an obvious method of populating an array to me, but is definitely handy. $Ary = foreach ($Obj in $Objs) { @{ foo = $Obj.foo } } reply jmull 4 hours agorootparentI think @(\"value\") is enough. I use [array]$Ary = \"value\" reply Arnavion 1 hour agorootparentI haven't used PS in many years, but I do remember `@(\"value\")` would still end up unwrapping the single-element array back into its element in some situations, and the only sure-fire way to get an array was `, \"value\"` or `@(, \"value\")` reply low_tech_punk 2 hours agoparentprevIt feels like a half baked feature. The other (missing) half would be automatically converting a single value into an array of length 1 when the receiving end expects an array. reply gugagore 4 hours agoparentprevThis is reminiscent of MATLAB where there is no distinction between a scalar and a 1x1 matrix. reply jdbebdjjd 3 hours agorootparentA scalar _is_ a 1*1 matrix A scalar however is not a tuple of length 1 reply CyberDildonics 2 hours agoparentprevNiche languages seem to end up with insane quirks like this every time. It's like the tragedy of sisyphus that things like lua exist, but instead of using it directly or modifying it slightly, then having a dead simple, tight, small, elegant, consistent language, people reinvent the wheel and never make it round. reply thiht 1 hour agorootparentLua is not perfect either. Having a specific keyword to make variables local was a mistake. Variables should be local by default. Oh, and 1-indexed arrays are annoying. reply CyberDildonics 1 hour agorootparentBoth of these could be changed trivially. Someone familiar with lua could do both in one day as opposed to making an entire brand new language from scratch. reply neves 6 hours agoprevI've always been curious to know. I am an experienced Bash developer, and when PowerShell was released, I was very excited. Finally, we would have a cool shell on Windows for development. However, since then, I've never managed to grok PowerShell and continue to use my good old Bash even on Windows. What is the experience of other developers who are experts in both shells to compare them? Did PowerShell really fulfill the promise of being a more efficient and modern shell? Or people just use it because it is already installed and better than CMD? reply sebstefan 6 hours agoparentI've done a lot of Bash, read some books about it, I'm a firm believer that nobody should be writing anything complex in bash. Above the ballpark of 50 lines I consider it a code smell. I have this web page saved up in case I ever need to convince anyone of this. http://mywiki.wooledge.org/BashPitfalls I used Powershell recently and not having to wrangle with text (commands return objects) makes it a much easier scripting language, and command line language. The fact that they have an official way to handle argument parsing is excellent, everything is unified and the commandline window is able to have autocompletion for literally every option of everything. Bash could never even dream of having that. It is incredible and makes you extremely productive. But type coercion manages to introduce new ways to create bugs that Bash didn't have. Honestly at this point I prefer PWSH but still kinda dislike both. I'm waiting for the new natural evolution. reply PeterWhittaker 6 hours agorootparent> I have this web page saved up in case I ever need to convince anyone of this... http://mywiki.wooledge.org/BashPitfalls This is a great list of pitfalls, absolutely, but they are more an argument for integrating shellcheck into your IDE than avoiding bash, IMHO! I know, I know, possible religious war, but I find that bash+shellcheck is far more often the right toolset than switching languages to avoid the pitfalls. The immense power and expressiveness and immediacy of bash makes so many programs and so much prototype->PoC->MVP progression so easy and effortless that it is worthwhile having tooling to catch the worst of the warts. reply jonhohle 3 hours agorootparentAdd shunit or similar and I You can have a really nice experience (imho). reply nunez 5 hours agorootparentprevI don't know if I agree with that. I write Bash; I've written some stuff I really shouldn't have written in Bash. Writing elegant Bash is possible, though I agree it is very easy to footgun yourself with it. The problem is that Bash is right at the middle of \"I need something quick\" vs \"I don't want to break out Golang or whatever\" and Bash is installed damn nearly everywhere. A 50-line awk script though? Get outta here with that lol reply sebstefan 4 hours agorootparentI don't trust myself so I need my language to not be bug-prone (and I don't recommend it for others because if we're looking at it realistically, most people who trust themselves shouldn't trust themselves) reply jonhohle 3 hours agorootparentDo you write unit tests in other languages? Do you write unit tests for your bash scripts? reply candiddevmike 3 hours agorootparentprev> I don't trust myself so I need my language to not be bug-prone Which language is that? reply alganet 5 hours agorootparentprevMany of these pitfalls are not the shell, but external programs (sed, xargs, grep, etc). I know it ends up in the same terminology bucket for the public consciousness as \"CLI stuff\", but there is a separation and understanding this separation is crucial for writing good sh. The problem actually lies in bash+coreutils being some sort of de-facto standard for command line stuff. The way bash+coreutils evolved was mostly for autotools and not for humans. This ecosystem could be much better. PowerShell has more builtins, so it relies less on external commands, therefore it is less vulnerable to pitfalls due to mismatches between different programs. reply sebstefan 4 hours agorootparentFor mismatches, custom scripts & commands in Powershell benefit from having a standard way to take input, and because the output is not text, you're also safer on that end But in the end really it doesn't matter in whose end of the kingdom the bugs come from. What matters is that's how people write Bash. reply alganet 3 hours agorootparentIt works very well if you're all inside powershell. Try this exercise: make a .BAT invoke a powershell script that invokes another .BAT passing parameters containing double quotes inside. It just can't be done reliably. The .BAT is only just an example, any param passing to/from powershell (outside powershells internals) is a nightmare. In bash, this kind of interaction is commonplace. You can make `find` generate shell snippets for you, and pipe the generate shell commands into another interpreter instance seamlessly. Think of the sheer amount of software the uses the shell this way and you never notice. That npm script that just passes parameters along is relying on the shell interface, that CI yaml that passes variables is relying on the the shell interface, etc. It runs just for a few milliseconds, to pass and glue things around, super simple. Powershell is just not designed nor suitable for that. The problem of an uniform interface _can_ be solved by changing how people write stuff. The problem of not fitting well as an architectural piece replacement is much more difficult to overcome. Powershell fits Windows though, but that's about it. reply inquist 4 hours agorootparentprevI’ve spent so much time wrestling with and learning all (most) of these pitfalls, and I consider it time well spent. After my first few weeks I started dreaming in bash. reply sebstefan 6 hours agorootparentprevI forgot but one thing I also liked that made me go \"Why would bash not have that?\" is having a dedicated verbose,info,warn,error,debug output and a glob star *> to redirect everything. It's nice not having verbose text polluting stderr. reply Kwpolska 6 hours agoparentprevPowerShell is more verbose than Bash, and has its idiosyncrasies (like sometimes automatically unwrapping 0/1-element arrays into scalars when you least expect it), but it is more productive and more readable. Object-orientation can be nice to produce pipelines. For example: find potential file duplicates in a folder, recursively, by grouping by file size: Get-ChildItem -File -RecurseGroup-Object -Property LengthWhere-Object { $_.Count -gt 1 }Sort-Object -Property Count No need to remember arcane `file` incantations, no need to parse textual output. You get real objects with properties, and tab completion can see their structure. Need to parse JSON? No need to involve `jq`, just `Get-Content -Raw whatever.jsonConvertFrom-Json` and do it directly from PowerShell. Need to convert XML into CSV? ConvertFrom-Xml (or Select-Xml) -> do stuff -> ConvertTo-Csv. Is `Get-ChildItem` too much typing? Use `gci`, or `dir`, or `ls` (by default only on Windows). Is `Where-Object` too much? Try `where` or `?`. And things are case-insensitive. reply g15jv2dp 4 hours agorootparentYou can even simplify your `Where-Object` as ...Where-Object Count -gt 1... Or, of course, ...? count -gt 1... reply akira2501 1 hour agorootparentprev...and how do I get a \"manual page\" for the command \"Get-ChildItem\" in the console? reply Miner49er 1 hour agorootparentGet-help get-childitem reply TeMPOraL 16 minutes agorootparentAlso remember to check Get-Help Get-Help. Also, unfortunately, you'll probably want to Update-Help first, because for reasons beyond my comprehension, PowerShell does not ship with detailed help installed by default. reply PeterWhittaker 6 hours agoparentprevI started my career with a X term hosted on a SunOS pizza box, have written thousands of scripts, more than a few in the KLoC count. Several years ago, I had to write a complex unattended robust data transfer system in PowerShell. (I know, I know, without more info these “requirements” beg many questions, but they are all out of scope for this reply.) I enjoyed the experience so much I switched all my shells, on MacOS (my DD) and on Linux (my most common work environments) to PWSH. What I liked most about it was the power of passing objects in pipelines, and being able to extract/manipulate some of the properties of an object in the first filter and still have access to others, along with those of objects created by that first filter, in filters later in the pipeline. Immensely powerful. The consistency of commands, of error handling, and of object properties was also very nice. Eventually, as the nature of work changed I switched all shells back to bash, as the older muscle memory asserted itself. PWSH as a shell made sense when I was working and thinking so much in that space, but when I left it, it was more effortful to think in PWSH than to resume bash. There are times I miss it. There is nothing else in the shell space that comes close, or at least not close enough to justify the effort of switching. reply ethbr1 6 hours agorootparentDo you remember what the common commands for pipelining you used were? I feel like my PowerShell struggles are really about not knowing 2-5 core, general-purpose pipelining commands well enough to use in any situation. reply Jochim 4 hours agorootparentNot OP but the \"-Object\" commands are pretty fundamental to creating useful pipelines: Select-Object - Pick out specific fields from an object, create calculated fields etc. Where-Object - Drop non-matching objects from the rest of the pipeline. Group-Object - Cluster objects into groups based on a shared property value. Sort-Object - Order an array of objects based on a property value. Get-Content - Read from a file. ConvertFrom-(CSV/JSON) - Parse a json/csv formatted string into a powershell object. ConvertTo-(CSV/JSON) - Serialise a powershell object into a csv/json string. (Parallel)ForEach-Object - Loop over each item in the pipeline, performing one or more actions on it. Usually occurs at the end of the pipeline, or when you need to call an executable that can't handle pipeline input. One thing I struggled with in the beginning, was not knowing what properties an object might have. You can pipe any object into `Get-Member` and it'll list its available properties and methods. Many of the \"-Object\" commands support the use of script blocks if you need to carry out more complex filtering/projection. reply jodrellblank 4 hours agorootparentprev|? PropertyName -gt 5 # where-object filter numbers |? PropertyName -match 'text' # where-object filter text |? {$_.thing ...} # where-object filter script |% PropertyName # same as select-object -expandproperty propertyname |% { $_.thing } # foreach loop |sort |sort prop1, prop2 |sort {$_.Name.Substring(0,5) -as [int]} # sort calculated thing |fl * # format-list , all properties |ft -auto # format-table, autosize table column widths |sv q # set-variable -name q, same as $q =|tee -var q # same but show the results as well as storing in q reply PeterWhittaker 2 hours agorootparentprevOthers have provided better answers than I could have, as I don’t remember clearly enough. What I do remember is that until I understood what I was trying to do and why (mostly error handling around edge cases, all specific to the app), I couldn’t really grok the various object management calls, but that once I’d done a few rounds of PoCing and RTFMing, everything fell into place. reply valiant55 5 hours agorootparentprevGet-ChildItem, Select-Object, Where-Object, Sort-Object or anything that is operating on a collection feel very natural. Often times I'm piping Get-ChildItem into sort to find the most recently created file in a directory. reply gosub100 4 hours agorootparentpreva few years ago I had to write a data integrity check for tens-of-thousands of files. Just crawl the dir and compute filename, size, checksum, maybe date, can't remember. I thought \"ooh neat, I bet I can use some powershell trick to compute that tuple for each file\", and no. Immediately ran into some obscure limitation with their \"pipe\" - I can't remember the details, it was like impossible to create many-things from a single-thing - and thought \"how disappointing. the one time I give it an honest shake to do something entirely conceivable by the creator and it falls flat on its face.\". reply jodrellblank 12 minutes agorootparentA lot of years ago, circa Exchange 2010, there was a limitation on pipelines-within-pipelines which had to be worked around. Files already include name, size, dates, so select all of those and add a calculated property of the file hash using Get-FileHash (MD5 and SHA options): gciselect *, @{L='Hash'; E={ ($_Get-FileHash).Hash }} Get-ChildItemSelect-Object -Property FullName, CreationTime, Length, @{Label='Hash'; Expression={ ($_get-filehash).Hash}} but even then you could do what you want with a traditional loop and no pipeline: $results = foreach ($file in get-childitem) { # a hashtable of things you want to be # in each object ('row') of the output: $data = @{ Name = $file.Name SizeGB = $file.Length / 1GB Hash = ($fileGet-FileHash).Hash } [pscustomobject]$data } reply latkin 2 hours agorootparentprevNothing in your description sounds difficult to do in powershell. You can certainly output \"many-things\" from a part of the pipeline that takes \"single-things\" as input. Crawling files is a single command, then you can do whatever you want with each one in the next part of the pipeline - \"map\" from file info object to something else (e.g. custom object with filename, size, checksum, etc props) 1-1, multiplex each file into N output objects, buffer file inputs until some heuristic is met then emit outputs, etc. reply omnicognate 6 hours agoparentprevNo expert in either but I've had to use both a fair amount. Powershell fell into a few traps that rendered it awful to use IMO: * Trying to be a .NET language. I don't know why the one-runtime-many-languages promise of .NET withered on the vine while it flowered in Java-land without the promise even having been made, but it did. If you're writing .NET code, use C#. (And I say that having worked in one of the very few teams doing large scale development in F#.) * Failing to get the basics of being a shell right. I can't remember the details as I've left it firmly in the past now, but its handling of redirection was just broken, to the extent that things that were trivial in bash were nigh on impossible in powershell. I got the impression its developers ignored the things bash etc. did well in their enthusiasm over building something new and powerful. * Being fetishistic about stuff like requiring Verb-Object naming for everything. It's certainly subjective and has its defenders, but IMO it renders scripts ugly, is awkward to type and doesn't materially improve discoverability or memorability at all. reply nickpeterson 1 hour agorootparentAs someone who really likes F#, I wonder sometimes if I should just jump ship to ocaml or Haskell. What made you feel like F# wasn’t ultimately worth it? reply Kwpolska 5 hours agorootparentprevPowerShell isn't quite a .NET language, and you can't really write serious .NET software in PowerShell. But if you need to access the power of .NET (or heck, COM) from a script or an interactive shell, PowerShell lets you do that. reply omnicognate 4 hours agorootparentThat's the point, though. It's a bad .NET language, but the desire to interact with .NET dictated much of its design. Windows didn't need a way to \"access the power of .NET\". It needed a decent shell, and sadly still lacks one Edit: And that isn't to say .NET doesn't need a scripting language. It sorely needs that and the various relatively useless variants on \"C# scripting\" are testament to that. Powershell isn't, and doesn't even attempt to be, a solution to that lack, though. reply Kwpolska 37 minutes agorootparentEh, I find PowerShell nicer than bash. And having a way to use .NET/COM stuff ad-hoc can be useful in scripts. For example, to talk to Windows Installer with reasonable performance over COM. Or to launch the best browser: (New-Object -ComObject InternetExplorer.Application).Visible = $true reply adamgordonbell 2 hours agorootparentprevThe article is about exactly this. Powershell's job was not to be a decent shell, but to make server management work automatable. Unix style shells didn't solve this because windows was not file based, you need all these hooks into these various api to setup a user, or a network connection or a whatever. Why it's .Net is also covered and its probably not what you expect. reply neonsunset 4 hours agorootparentprevTo be fair, nothing in .NET required Powershell to make the choices it did (case in point: F#, ClojureCLR) for the integration. I wish to learn it but it being so...wordy definitely makes it an unattractive choice. A shell should be terser than F# or C#, not the other way around. reply omnicognate 4 hours agorootparentYeah, \"dictated by\" is probably the wrong phrasing. A decent shell with those capabilities could probably have been made, but I do think the drive to make something fancy and new based on .NET was a big part of the reason they neglected decades of insights and successful idioms in the shell space, which were sorely needed on Windows. reply hollerith 4 hours agorootparent>decades of history and successful idioms in the shell space Just because something has persisted for a few decades does not make it good or worthy of being emulated on a platform different from the platform it started out on. reply omnicognate 3 hours agorootparentI don't recall saying that it did. My judgement that Windows needed (and still needs) those ideas is based on heavily using both platforms. reply neonsunset 3 hours agorootparentI just gave up on Windows. On every step it feels proactively user-hostile, with non-dismissable notifications, removal of UI customizations, constant reminders to change some settings to \"recommended\" despite my choice, accompanied by overall loss of reliability. It feels like there is no more vision, desire and engineering excellence left in the teams responsible for it. I know that there are rare exceptions like Microsoft Store that, as an application, has started to work so much better because the person responsible for it cares, and it uses WinGet for updates and distribution, which is great. But these are droplets in the ocean. At this point, setting up a Linux distro for a desktop yields incomparably better experience given equal amount of effort. The only thing that makes me worried is that such a dismal state of affairs damages the image of .NET which is night and day compared to most other products made by MSFT in terms of quality and taste, it already needs help - note how much undeserved good will Go and Swift receive, despite former being much worse than everyone thinks once you look into the fine-print and implementation details, and latter having poor non-macOS support story and ecosystem outside of iOS development. And at the same time no matter the degree of improvement that happens to .NET, it is popular to bash it, even if the criticism is completely detached from reality. (downvotes only demonstrate my point) reply bionhoward 6 hours agoparentprevI don’t like powershell because it’s so surprising in a billion ways from bash. The tab being replaced with right arrow screws me up, and so does not having the normal commands, and it’s quite opinionated about naming. That said, powershell seems more capable than bash if you know it well, because it has a better type system and arguments are easier to mess with, whereas bash things are amorphous strings. https://github.com/bionicles/tree_plus/blob/main/tests/more_... That’s a slightly out of date version of what I use to provision stuff on my windows machine to test things, and can show what’s possible a bit. reply drsopp 5 hours agoparentprevI used a Powershell-script in a recent project and learned some lessons. It gets some data from an API. It is run dozens of times every minute, can run concurrently with itself, has some writing to a json file cache in a race condition free way. Works mostly ok. Development has been difficult because of low volume of forums talking about Powershell. Wouldn't have bothered to make the script without ChatGPT. One time it failed by inexplicably setting system+hidden+read only flags in a project folder. Quite a head scratcher. The worst experience was a bug I still don't understand: The script failed to get an access token when running in \"ordinary powershell\" but managed to do so when running in Powershell ISE. I checked every possible environment difference and concluded that there was only one: powershell.exe vs powershell_ise.exe. I am not going to be making any more Powershell scripts. reply dfedbeef 3 hours agoparentprevPowershell is a huge improvement over cmd. However; much like Windows, it is full of complicated gotchas and edge cases. Bash has them too, but Bash makes concurrency and parallelization extremely simple, so despite it's absolutely bananas syntax and rules for hash tables and arrays, it is superior IMO. Bash also makes it simple to write your own more complex utilities in any language and call them from bash. Linux makes it simples to implement new kernel tech and Bash makes it simple to interact with the kernel. Windows... You have to wait until someone in WDG decides some existing idea is good and then also wait for them to get support to implement the idea, then wait to see how that whole thing goes. So yeah Powershell is great for interacting with Windows. But Windows thinks you're a dumb shit so you can only do what Windows already lets you do. reply zamalek 39 minutes agoparentprevI have used it for nontrivial stuff. The idea of streaming objects has a lot of potential, but I think the \"enterprise crowd\" had a really good crack at it - especially the syntax. Not having to resort to grep, jq, cut, and the host of tools required to (often fragile) parse out whatever specific format the tool you are calling is giving you. Put another way, you could feel free to parse the output of ls in powershell - because it's already been done for you. reply langcss 6 hours agoparentprevI find it is in a weird spot where it is more powerful than bash but you often don't need that power or if you do coding in c# might be better. It is very good of you want to work with Azure though. You probably don't want to use bash for that. But lots of reading up each time I use it. Bash is my go to for small jobs or a command line experience. I am not an expert in either but have used both plot to get stuff done. reply simooooo 5 hours agorootparentMost times if I’m doing anything suitably complex it ends up as a little C# app rather than powershell. reply prmoustache 6 hours agoparentprevWell I used powershell a lot when I was managing VMWare infra. Ultimately the force of a object oriented shell is in the ecosystem, I haven't found any other use case where powershell was useful to me. I am pretty sure it is useful in an Azure context too but in the last few years I have never been in a situation where I would feel the need to install powershell on my linux workstation to use a specific module/use case. reply GrantMoyer 5 hours agoparentprevI use *sh at home, and Powershell at work. Powershell has a lot of really cool and genuinely useful concepts. There's the obvious like piping objects instead of raw bytes, but there's also things like first class parameter validation, automatic array flattening, multi-threading, full .NET API interoperation, and more. Unfortunately, it also has some real sore points for use as an interactive shell, and even some outright broken behavior. Some that I lament most frequently: - Powershell has native tab-completion support, but the word-of-god dictated \"Verb-Noun\" command naming scheme means almost every command shares a common prefix with a slew of entirely unrelated commands, making tab-completion painful. - Power shell has sparse built-in support for funtional style list manipulation or data structures beyond lists. There's a \"map\" command (%), but no reduce, zip, tail, etc. These feel like they'd fit in perfectly to the design of the language, but they're just missing. Also, pipes are lazy, but lists are always strict, which can cause friction. - most operators are strangely named, such as \"-eq\" for ==, which hurts legibility. - There're lots of unvoidable brackets, meaning you often need to jump around the line while writing out a command incrementally. For example, to get filenames, you either need to write `(Get-Item).Name` or `Get-Item%{$_.Name}`. It'd be nice to have something like `Get-Item |. Name` as an option. - It's impossible to pipe raw bytes between commands/executables; raw bytes are interpreted as strings. This means, for example, it's impossible to pipe a file to `patch`, because the contents will be decoded into strings, piped as strings, then re-encoded with line endings changed, which `patch` rightfully refuses to apply. - Running Powershell scripts is disabled by default (WTF?) By comparison, *sh's heavily favor interactive use. For example, you can put io redirections at the start or end of a line, tab-completion is often very good, it's often possible to avoid brackets (ex. with xargs), background/foreground job management, etc. Overall, I often find Powershell more pleasant to use than *sh, especially when writing scripts. But for interactive use, I often wish Powershell was a little smarter and a little dumber at the same time. reply nu11ptr 5 hours agorootparentWhy is automatic array flattening a good thing? This has caused more bugs than I can count. Every time I make an array I have to check if it has a length of 1 and do something special with it or else the script blows up. Best I can tell this is a huge foot gun anti-feature. What am I missing? reply GrantMoyer 5 hours agorootparentIt's a foot gun, and the behavior is often surprising until you get used to it and sometimes after, but Powershell would be very painful to write without it. Basically all functions return a list in Powershell, including all user-defined functions, so you'd need to unwrap singleton lists all over the place. reply nu11ptr 5 hours agorootparentBut if they COULD be lists of >1 then by unwrapping it you have two scenarios to handle instead of one. How is this an improvement? And if it is ALWAYS a list of one why return a list? reply GrantMoyer 4 hours agorootparentAll Powershell functions are basically python generator functions. It could have been designed to have more traditional functions, but I find generators fit well into a language based on piping streams of values between functions. Usually, the distinction between a single item, a flat list, and a list of lists isn't important in Powershell, because commands are written with Powershell's behavior in mind. It's extra painful when it does matter, but it's a trade off against being more verbose every time it doesn't matter. Ultimately, it's personal preference; for an interactive shell, I like the tradeoff Powershell made, and I think it's a cool design space to explore, but I do think there's a lot of room to improve the implentation of the concept to make it less surprising and less painful to choose the alternate behavior when needed. reply LeonB 4 hours agorootparentprevThis works: Get-Item% Name reply GrantMoyer 4 hours agorootparentWelp, I wish I'd learned that earlier. reply vips7L 2 hours agorootparentprevI switched to menu completion and the tab completion isn’t as painful with all the choices. reply nunez 5 hours agoparentprevI was extremely well versed in PowerShell early into its development. It was indispensable when it came out. VBscript and Cygwin were the only options; both of them sucked. The ability to use native .NET objects in scripts was insane; I definitely abused that! Nowadays, Git Bash and WSL2 replace a lot of the value that PowerShell provided. It's purpose was to drive Microsoft away from the GUI, and, at that, it more than succeeded. reply oblio 3 hours agorootparent> VBscript and Cygwin were the only options; both of them sucked. I'd challenge the Cygwin aspect. With judicious use of cygpath & co., you can go VERY far with Cygwin. I've built an entire SaaS deployment system on top of it (and Python). reply marcosdumay 5 hours agoparentprevA shell is not a thing by itself. It's only as good as it integrates with the rest of the system. So, the experience of PowerShell on Windows is unbeatable... on Windows. It's easily beaten on any other system. reply ed_elliott_asc 3 hours agoparentprevIt took me about a decade to be comfortable with it :) reply delta_p_delta_x 5 hours agoparentprevI'm not an expert, but I use UNIX/Linux at work and only Windows at home (I have a dormant Arch Linux install that I haven't touched in a couple of years), and hopefully you think I'm qualified to respond. I feel Bash is a 'glue language' in every sense of the word, and is optimised for interactive command-line usage because of its terseness. Being remotely productive on Bash presupposes the presence of an entire UNIX subsystem and coreutils (which is why Git Bash for Windows drags along an entire `/usr/bin` directory with all the coreutils in it). You can't really do much in it without the coreutils. I find that Bash parseability and ease-of-understanding rapidly deteriorates in scripts of increasing complexity. After about a hundred lines of code I reach for other languages. I personally dislike its unintuitive syntax, global scope, and weak, dynamic typing. The terseness that lends itself to powerful interactive command-line usage means script readability plummets. Of course, that might just mean I'm inexperienced with respect to Bash and need more practice, but I find this a fairly common opinion amongst colleagues and friends. In my opinion, Bash scripts are write-only. It is a wonder that something like neofetch[1]—which is an eleven thousand-line monstrosity—lasted this long (if I recall correctly the author stopped maintaining it because—amongst other reasons—it had just become too complicated). I find that people understand PowerShell better when they realise it is closer to Python than any interactive UNIX shell. By default, PowerShell is more verbose than Bash; for instance, the PowerShell equivalent of the two-character all-lowercase `ls` is the thirteen-character mixed-case `Get-ChildItem`. Arguments are by default also long and verbose, with things like `-FollowSymlink`. PowerShell is generally meant to be used in executable scripts, just like how most Python isn't executed in the interactive REPL, but by running scripts. Consequently, I find this verbosity lends itself to improved readability in scripts, at the expense of some interactive productivity. That being said, common PowerShell commands have UNIX-like aliases[2] (at one point the cause of much gnashing of teeth here and at /r/programming because PowerShell aliased `curl` to `Invoke-WebRequest`[3]) to improve interactive productivity. PowerShell also supports truncating parameter names; for instance, `Get-ChildItem -Recurse -Force` can be abbreviated to `gci -r -fo`, which looks remarkably like a UNIX command now. Even so, the PowerShell ISE and the PowerShell VS Code extension both suggest that programmers use the full unabbreviated commands and arguments in `.ps1` scripts[4]. PowerShell is also dynamically typed, though semi-static typing can be opted in to[5]. That said there is some controversy about arrays decaying to scalars when they have size 1 [6]. It is easy to set up both function and script parameters with as much power and expressiveness as `argparse` for Python, but natively without involving any additional modules[7]. The real power (pun not entirely unintended) of PowerShell comes when you realise it is just another front-end to the incredibly massive .NET ecosystem. Where Bash requires a full UNIX subsystem and a collection of hundreds of additional binaries to be productive, PowerShell employs pre-compiled cmdlets[8] written in .NET (usually C#) to augment the basic language. Most 'commands' in PowerShell are cmdlets, including `Get-ChildItem`. You can write your own 'back-end' in C#, F#, C++/CLI, VB, or any other .NET language, compile it, and expose a PowerShell interface for end-users. Pretty much like how people write fast but complex code in C/C++, compile it, and expose a Python interface (Numpy, Pandas, PyTorch, etc come to mind). You can additionally directly call .NET methods in PowerShell[9]. This also means that everything in PowerShell is an object. The output of `Get-ChildItem` is a list, not a string. Likewise for `Get-Process` (UNIX equivalent: `top`). These lists may be formatted[10], filtered, parsed, modified, dumped to JSON, and otherwise manipulated like any other list data type in most other languages. PowerShell also lends itself to the side-effect-free/functional/monadic map-reduce paradigm very well, with pipelines[11]. In my opinion, it is not merely 'better than CMD'; it blows CMD out of the water. A little less so for Bash, because Bash is augmented by the UNIX core-utils. As should now be evident, in my view the real competition to PowerShell is Python. [1]: https://github.com/dylanaraps/neofetch/blob/master/neofetch [2]: https://learn.microsoft.com/en-us/powershell/scripting/learn... [3]: https://news.ycombinator.com/item?id=12319670 [4]: https://learn.microsoft.com/en-gb/powershell/utility-modules... [5]: https://stackoverflow.com/a/115815/1654223 [5]: https://superuser.com/a/414666/1132163 [7]: https://learn.microsoft.com/en-gb/powershell/module/microsof... [8]: https://learn.microsoft.com/en-gb/powershell/scripting/power... [9]: https://devblogs.microsoft.com/scripting/learn-how-to-use-ne... [10]: https://learn.microsoft.com/en-us/powershell/module/microsof... [11]: https://learn.microsoft.com/en-gb/powershell/module/microsof... reply pletnes 6 hours agoparentprevExactly my experience. reply gonzo41 4 hours agoparentprevI just find everything about powershell tricky. Something just doesn't click for me. It's always a struggle. reply Joel_Mckay 6 hours agoparentprevI don't think PowerShell itself was any better or worse than other CLI level environments. However it is a contradiction in how Windows typically handles complexity. GUI by their very nature usually make \"simple things easy, and difficult things impossible\". Thus, attempting to integrate those features in a way similar to a complete posix system was never going to turn out completely functional. MS was successful as the users love their one-button [>>] wizard utilities... kind of like how a chicken loves corn. https://en.wikipedia.org/wiki/Operant_conditioning_chamber Don't get mad, Cortana is watching after all. =3 reply nullindividual 4 hours agorootparent> GUI by their very nature usually make \"simple things easy, and difficult things impossible\". Creating an Active Directory forest was traditionally done via a GUI wizard. I would say that makes a very complex task easy, where-as creating it via CLI requires some amount of knowledge, making something otherwise easy \"difficult\" as you need to know what the CLI parameters are asking for (GUI explains it in plain ), _and_ you need to know what the proper cmdlet is, leading to difficulty in discovery. Server Manager starts automatically after logging into Windows Server for the first time, presenting the user with a two-click method to start the deployment of AD (and some more clicks for the wizard itself). This lets anyone, from an SMB one-person shop to the largest company in the world (who is that?) deploy AD in a matter of minutes. It's empowering. Equating Win Admins with chickens is rather disrespectful and/or elitist. \"Don't get mad, bro!\" doesn't absolve one of their statement. I've written PowerShell modules in C# for SharePoint Server. I continue to use the click-click-next-finish GUI for certain tasks. Does that make me a troglodyte? [0] https://learn.microsoft.com/en-us/powershell/module/addsdepl... reply Joel_Mckay 3 hours agorootparentThe point was things like AD are not really transparent to the end user, and thus difficult to handle in a CLI outside of a utility (usually need privileged MS insider OS API documentation). As a side note, exposing a desktop environment in a server role is still a problem 30 years later (GUI usually slowly leak memory over time.) Note chickens are fairly aggressive little creatures, and through the power of Cortana could easily masquerade as an admin. Thus, one has to be careful when you see sharp peckers. Have a wonderful day, =3 reply nullindividual 3 hours agorootparent> thus difficult to handle in a CLI outside of a utility (usually need privileged MS insider OS API documentation) There's a CLI command for it! What are you talking about with regards to APIs needed to do the particular task I mentioned? > (GUI usually slowly leak memory over time.) No, no, no, no. Please don't make up false statements to try to \"prove a point\". You're just being disrespectful to be disrespectful. You're disrespectful of many talented individuals in the industry. For shame. reply Joel_Mckay 2 hours agorootparent>No, no, no, no. Please don't make up false statements to try to \"prove a point\". lol, one may believe whatever they like... but maybe MS finally did fix that decades old GUI window de-allocation leak in win11. >You're just being disrespectful to be disrespectful. To whom exactly? a self-deprecating straw-man presenting geniuses poultry/paltry issues are difficult to understand =3 >many talented individuals Just so we are on the same page, you are asserting people who can peck/click [Next>>] twice require talent greater than or equal to a chicken? And note I think all software is equally terrible... =3 reply eigenvalue 2 hours agoprevSeems so weird in retrospect that Microsoft wouldn’t have seen the value of an easily composable and programmatic way to configure anything in Windows and their other important enterprise applications like Active Directory and Exchange. The idea that connecting with Remote Desktop and clicking around with a mouse was suggested as an alternative strikes me as absurd. Especially because automating that sort of thing (at least based on my experience using autohotkey and window spy) is horrendously difficult and annoying. reply akira2501 1 hour agoparent> an easily composable and programmatic way to configure anything in Windows There's an entire industry of consultants and software vendors that don't want that outcome. > connecting with Remote Desktop and clicking around with a mouse This generates a lot of hours. > is horrendously difficult and annoying. It is ironic that this is probably the main reason that alternative operating systems even exist. reply bluedino 3 hours agoprevUnless I'm interacting with some Windows subsystem, and need the specific Powershell commands, I just think \"why the fuck am I not using Python?\" It's also way too verbose and slow for 90% of the stuff I'd use Bash for (or would have used Perl in another life) I often wonder why Microsoft didn't base it on Python, Node, or something else. I can't remember when PS was first released so I'm not sure what would have been ideal at the time. reply thesuperbigfrog 2 minutes agoparent>> I often wonder why Microsoft didn't base it on Python, Node, or something else. I can't remember when PS was first released so I'm not sure what would have been ideal at the time. They originally based it loosely on Perl and the Korn shell. In the first edition of \"Powershell in Action\" by Bruce Payette there is a sidenote that states: 'PowerShell uses the \"at\" symbol (\"@\") in a few places, has $_ as a default variable, and uses \"&\" as the function call operator. These elements lead people to say that PowerShell looks like Perl. In fact, at one point, we were using Perl as a root language, and these elements stem from the period. Later on, the syntax was changed to align more with C#, but we kept these elements because they worked well. In Perl terminology, they contributed significantly to the \"whipupitude quotient\" of the language.' It also states: 'The core PowerShell language is based on the POSIX 1003.2 grammar for the Korn shell. Originally, Perl idioms were appropriated for some of the more advanced concepts such as hash tables. However, as the project progressed, it became clear that aligning PowerShell syntax with C# was more appropriate.' reply dopylitty 3 hours agoparentprevPython's default REPL is godawful. It's also just not designed for the kind of console work that PowerShell excels at because you have to do things like managing file handles rather than just getting the file content and piping it to another command. PowerShell is great because it's a swiss army knife that has a very nice REPL with autocomplete, no weird whitespace behavior, readline[0], and can do anything .NET can do if you need to do anything more complicated. Plus it's object oriented so you can focus on doing the tasks you actually need to do rather than trying to figure out how to use archaic utilities to parse the text based output of other archaic utilities. 0: https://learn.microsoft.com/en-us/powershell/module/psreadli... reply oblio 3 hours agoparentprevPowershell predates Node by 3 years. They probably didn't base it on anything else since they control .NET. I doubt it's slow because of .NET, it's slow because of either its design or because of under-investment in performance. However... which version of PS are you using? As far as I remember the newer versions are quite fast. reply UweSchmidt 4 hours agoprevPowershell was truly a product of Microsoft monopolistic confidence. To create a language that allows absolutely no syntactical carryover from any other langauge is wild. You couldn't guess or assume any command, parameter or flag. Even with Microsoft's ambition they should have realized that legions of admins and programmers had to learn and maintain scripts in both Powershell and bash for a few decades at least. The extreme verbosity of the syntax may look good for a presentation to the committee, but dealing with it regularly collides with well-researched and understood limits of the human brain, where information of a certain size, and delays of a certain length break the state of flow and requires concentration, explicit memorization and double-takes. Even with practice one could never quickly execute the common shell incantation to turn a thought into reality, instead one would have to wrestle with syntax, even if it's just waiting for autocomplete to appear and deciding to accept the next word of a multipart command. Let's try the start menu and search for \"pow..\", chose between 4 amazing options, Powershell or Powershell ISE, both in regular and x86 flavour. Either would take a while to load, breaking flow. The ISE shows a little splash screen that jumps to a second location. Another dialog shows up and informs you that you've closed the last session without saving the unnamed script files. But it opens them anyway, as you would expect. So why scold me for this? Because, you know damn well that saving the textfile is trouble: I can type or copy, and then execute any kind of evil code imaginable, but saving the file and then running it as a .ps Script triggers the ridiculous execution-policy song and dance. Probably trauma from Microsoft's bad security reputation of early Internet Explorers and Windows versions. I tried to love it anyway but one day my script encountered filenames with square brackets. Powershell implicitly interpreted those [1] and [2] as iterators somehow (https://stackoverflow.com/questions/21008180/copy-file-with-...). Sorry but dealing with files is the one job a scripting language has, filenames are beyond the control of script authors, and the space of valid filenames on Windows should be known. This gave me some long lasting trust issues with that language. (The Azure team aparently had enough power within Microsoft to create their own, sane and readable syntax: \"az find vm\", \"az account show\".) reply oblio 3 hours agoparent> Powershell was truly a product of Microsoft monopolistic confidence. To create a language that allows absolutely no syntactical carryover from any other langauge is wild. You couldn't guess or assume any command, parameter or flag. Even with Microsoft's ambition they should have realized that legions of admins and programmers had to learn and maintain scripts in both Powershell and bash for a few decades at least. Um... no. Powershell was inspired by shell, Perl and a bunch of other languages, you see it in its design. The other part was just a desire for consistency, since *NIX knowledge is just brute forced. Yeah, -v is generally verbose and -h is generally help, but in practice you can't rely on anything. reply UweSchmidt 3 hours agorootparentCould you illustrate how any other language inspired Powershell in any way, beyond the most basic concept that it is a scripting language? In bash/unix, I feel -l often stands for list, -a for all, -f for force, -q quiet or slient, -r recursive, -d debug. A modern approach could have been for Microsoft to clean it up and make it more consistent. But no. reply pjmlp 48 minutes agoprevBesides the whole interesting story background, yet another confirmation of the anti-.NET bias by WinDev during the Longhorn efforts. Instead of uniting and having everyone collaborating into a common like Google with Android making it happen no matter what, or how Bell Labs tried with Inferno / Limbo, the active fight against .NET, and anything related. To what ended up being the WinRT failure. Ironically, WinDev is now shipping JavaScript and Webview2 all over the place on Windows 11. reply spicyusername 6 hours agoprevI'll agree with some of the other sentiments here that whenever my career brought me close to Windows administration I absolutely loathed the experience. PowerShell however was actually pretty great to use, despite everything else on Windows being extremely clunky. It always felt very thoughtfully designed. Linux is great, and I'll always use it as my daily driver for work, but using bash is absolutely horrible. But because it's always everywhere, it's something that everyone reaches for first, and so we'll probably still be dealing with bash scripts in 2100, warts and all. reply justanother 6 hours agoprevI have never, and I mean never, been a Windows user, even though I've been using computers since 1982. During the rise of Wintel in the early 1990s, I followed the rise of Linux and 386BSD. When Win95 and NT ruled the business desktop in the late 1990s, I sought refuge in SPARCStations, Linux, and discontinued NeXT hardware. After the turn of the century, I adopted the newly-POSIX-compliant Mac OS. All this to say, avoidance of Microsoft products has been a cornerstone of my computing policy for nearly half a century (with the notable exception of Applesoft BASIC). But PowerShell? PowerShell's nice. reply heresie-dabord 6 hours agoparentYou have long experience in computing and especially in the innovative currents that shifted the paradigm. Cheers! Your first paragraph creates expectation that the second paragraph disappoints, though. Would you explain why you think PowerShell is \"nice\"? reply justanother 6 hours agorootparentIt's a way of using Windows that is very tolerable to POSIX commandline diehards. The similarities are numerous and include enhanced scriptability and even small details like up-arrow command history. You could get all of that with Cygwin, but Powershell adds tight OS integration, access to COM objects (or whatever we're calling them this year) as well as the remoting of objects. It's a useful and powerful shell that reminds me of VMS DCL and csh. reply Kon-Peki 4 hours agorootparent> It's a useful and powerful shell that reminds me of VMS DCL and csh. I remember watching a video on one of the various Microsoft learning sites in which Snover and another guy were interviewed about the creation of PowerShell. One of the two guys said that after they realized that UNIX-style wasn’t going to work they turned to VMS and drew a lot of inspiration from it. reply jojobas 5 hours agorootparentprevPretty sure arrow-recall predates Powershell and in fact Windows itself, it was there since DOS. Even Tab-path completion had to be done in an incompatible way. reply neves 6 hours agoparentprevYou posted just at the same time I asked. What's makes PowerShell better than bash? reply oblio 6 hours agorootparent1. Structured data passed along pipelines. 2. Automatic introspection/command completion for command parameters, even user-created commands. You can argue about a lot of other things Powershell does, but these 2 things are things which if Bash were designed today by 100 top notch software developers, would probably be part of 95 of their designs. reply alganet 5 hours agoprevPowerShell is very comfortable in interactive mode. For scripting, I don't get it. It's not designed to be a simple text-based glue like the bourne shell is, so it feels weird in many places (quoting, escaping, even more than sh is). It's very good for glueing Windows stuff though, like .NET libraries and so on. reply mike_hearn 5 hours agoparentHuh I always found that to be the opposite. PowerShell is clearly designed to be a programming language not an interactive environment. E.g. 'wget' exists but run it on its own and it doesn't save the file to disk or print it out, it prints an object as a table, which is about the most useless output possible. I know why this happens but that doesn't make it helpful. Reading about the dysfunction inside Microsoft makes it clear why it ended up this way though! reply alganet 5 hours agorootparentI don't want an object oriented language to glue stuff, I want a glue language. Want the result of wget in memory instead of print or disk? Write it to memory: page_contents=\"$(wget -O- some_page)\" It doesn't try to apply some default structure, so I don't need to rely on MS doing a fancy special wget for me that calls thousands of lines Invoke-WebRequest in the background. reply nunez 5 hours agoparentprevDoes the ISE still take forever to spin up? reply briHass 1 hour agorootparentThe ISE is deprecated, and MS guidance is that it should be uninstalled. Defender endpoint management flags it as a mild security risk. reply gavindean90 4 hours agorootparentprevI think everyone just uses VSCode now reply meisel 2 hours agoprevWhy wouldn’t they just replicate bash or some other UNIX shell, along with the basic UNIX tools like cp and find with matching APIs? Huge mistake there imo, even if they did add a few bells and whistles with powershell reply DrTung 3 hours agoprevI think PowerShell is a bit scary, for example I could never get curl to work in it, say a simple POST command: curl -X \"POST\" google.com should return Error 411 (Length required) from google (as it does in CMD.EXE) When I try it in PowerShell I get: Invoke-WebRequest : A parameter cannot be found that matches parameter name 'X'. and some more error messages even curl -X \"GET\" doesn't work :-( reply skowalak 3 hours agoparentThat is because `curl` is an alias for the builtin Invoke-WebRequest and not the actual curl program. https://learn.microsoft.com/en-us/powershell/module/microsof... reply gabrielsroka 2 hours agorootparentIt was at one point, but they took it out because it was stupid. reply martinsnow 3 hours agorootparentprevWell that's just plain stupid reply useerup 6 hours agoprevSo much was so right about PowerShell. But it failed to attract a wider audience, and in their quest to woo Linux devs Microsoft has been undermining PowerShell lately. Knowing what PowerShell offers, falling back to bash CLI tools feels like two steps back. Just some of the stuff PowerShell did right: - PowerShell cmdlets are self-describing and rich in information. Rather than each command doing its own parsing of parameters, cmdlets describe parameters and delegates the actual parsing to the shell. The shell understands data types, parsing rules, e.g. how to parse a UUID or a date. Not only does this ensure a consistency that was never in *sh shells, but it also enables cool stuff like e.g. autocomplete, predictive input, help instructions etc. almost for free. - \"Simulation\" mode (-Confirm and -WhatIf) where a cmdlet can describe the action it is about to take, and the mode of the shell may decline everything (effectively a \"simulation mode\") or may actually ask the user for permission (-Conform) for each action. But, alas, PowerShell never caught on outside Windows, and now MS is leaving it to wither in their quest to not upset a wider non-Windows community. reply iso8859-1 4 hours agoparentNushell is based on it, and it is picking up steam. https://www.nushell.sh/book/ So in the end, PowerShell doesn't need to catch on. reply vips7L 2 hours agorootparentNushell isn’t anywhere near as powerful as PowerShell. reply ripley12 38 minutes agorootparentI think it depends on what you want to do. Nushell's never going to surpass PowerShell for Windows infrastructure automation. But if you want a shell that makes it easy+quick to work with data in all kinds of formats, Nushell wins IMO. reply evacchi 6 hours agoprevI love corecursive! Keep up the good work @adam!! reply munchler 4 hours agoprevThis is a great story of finding a way to be productive inside a giant, impersonal machine like Microsoft. That said, it did nothing to convince me to invest more mental energy in PowerShell. Every time I use it, I have the same “meh” reaction: The learning curve is too steep and the syntax is too ugly. It doesn’t ever “stick” with me, so I end up starting from zero every time I encounter it again. I think the fundamental problem is that PWSH inhabits a dark valley between quick-and-dirty scripting and I’m-serious-about-this programming. It really needed to pick one side or the other to win people over, but never did. reply low_tech_punk 2 hours agoprevThe Monad Manifesto mentioned in the podcast: https://jsnover.com/Docs/MonadManifesto.pdf reply saghm 3 hours agoprevIt says a lot about the uphill battle that he faced when the first two sentences Jeffrey says in the podcast are \"By the way, is it okay to swear?\" and \"You know, I had executives say, ‘Jeffrey, exactly which part of fucking Windows is confusing you, Jeffrey?’\". reply johng 5 hours agoprevI once had 2 microsoft engineers call me for help with qmail because I was active on the qmail mailing list. They couldn't wrap their heads around how svc and daemontools worked.. the good old days! I never asked why 2 microsoft engineers were working with qmail. I think I was 20 at the time and I was just happy to be on the phone with people from Microsoft even though I was very much a Linux guy. reply rr808 6 hours agoprevI used to be a hardcore Windows dev but never figured out powershell. I actually ended up writing scripts in C# and had a utility to load and run it. reply martinsnow 3 hours agoparentGood news. You can write C# that powershell will compile on the fly. It's so ingrained in Microsoft applications that even MSSQL Server will do it as well, if you want to go down that route. reply oblio 3 hours agorootparentHow? reply martinsnow 2 hours agorootparentExecuting C# in Powershell: https://blog.adamfurmanek.pl/2016/03/19/executing-c-code-usi... Executing C# in SQL Server: https://learn.microsoft.com/en-us/sql/language-extensions/ho... An anecdote: about 7 years ago I made an postal code lookup function for SQL server, that would parse a csv from the danish postal services, that was retrieved from a HTTP GET in T-SQL, then parsed with C# to get the city from a postal code. It was a project i did for fun at school. reply rcarmo 4 hours agoprevI am not a fan of PowerShell, but I am a fan of Jeffrey--he toiled and talked sense at a time way before Azure was a thing, but everything since he shipped has proven he was right. reply arunsivadasan 5 hours agoprevWould have loved to hear why he made the move to Google considering that he became a Technical Fellow and very popular in the community reply theimposter 5 hours agoprevPowerShell, is still the best way to scale your workload, administering a Windows server environment on premise. For Azure, there were features that were present in the Graph API that were not in PowerShell. I haven't checked back in a while, but I think most new features in PowerShell are just pointing back to the Graph API. reply axpvms 5 hours agoprevPowerShell helped me a lot in my earlier career, bash always felt like banging rocks together in comparison. reply siriushacker 1 hour agoprevFavorite line: \"I’ve never seen anybody use a GUI in a clever way. Ever. There’s no cleverness to it. No, like, Oh my God, you should see the way Adam clicked that mouse. Oh my God. Guys, guys, guys, guys, come on, check it out. Adam’s going to click the button. Oh my God. That’s amazing. It just doesn’t happen.\" reply aragonite 1 hour agoparentTo be fair, a GUI can be used in a very clever and skillful manner. It's called keyboard shortcuts (and, to a lesser extent, mouse gestures). You can't deny an Excel world champion uses its GUI in a highly impressive way. And features like multiple carets editing (as popularized by Sublime Text) makes it possible to perform some remarkable editing feats. :) reply zlies 6 hours agoprevI love this podcast! reply Pet_Ant 5 hours agoprevHonestly I love Powershell and run it on Linux or OS X. Being able to access fields instead of having to play with `cut` to get the data I want is priceless. Just feels cleaner and more maintainable. reply ilrwbwrkhv 3 hours agoprevBig fan of PowerShell. Such a shame that being in Microsoft, you aren't allowed to do great work. That is why hackers shouldn't join large companies. You can create much more value working for yourself or joining a small startup. reply jodrellblank 4 hours agoprevPowerShell is MIT licensed, cross-platform Windows, Linux, macOS compatible, and you can download it in various installers and packages here: https://github.com/PowerShell/PowerShell/releases for anyone who wants to try the newer version on Windows, get either Windows Terminal (from Microsoft Store or Github releases: https://learn.microsoft.com/en-us/windows/terminal/install) or Visual Studio Code. The classic Windows' command prompt console host engine just can't do Unicode and fonts and colours and Unix shell escape sequences. After that, find something which will immediately trigger you to froth at the mouth, hurry onto some Microsoft forum and post about how Microsoft is the devil. Here's some popular choices, many of them valid complaints: ('curl' and 'wget' on Windows override the real programs with M$ imposters). (gci doesn't support the parameters of either dir or ls). (aliases work differently to Unix shells). (almost everything works differently to Bash). (gci -recurse is frustratingly slow). (it doesn't have a CLI text editor like nano). (you don't understand that GNU and Unix utilities aren't \"Bash\"). (PowerShell remoting with enter-pssession and invoke-command aren't SSH). (there isn't any sudo because Windows isn't Linux). (Execution policies are annoying). (it doesn't use UTF8 everywhere always). (backslash, the one true Unix escape character isn't PowerShell's escape character). (Line endings aren't Unix line endings). (having to use sigils to disambiguate between shell and code is worse than Python for coding and worse than Bash/cmd for shelling). (You want > to be both numeric comparison and/or Unix shell IO redirect and it's not). (you hate Verb-Noun and the one true way is Noun-Verb). (byte streams don't pipeline well or quickly). (It's verbose which you hate, but the elastic syntax one-liners are unreadable which you hate, it should have exactly the right amount of verbosity which coincidentally is exactly the amount you are comfortable with). Moving on from there, avoid falling for the tempting usermode filesystem equivalent, which is abandoned and only exists for backwards compatibility making everything slower. Avoid falling for the declarative host config system Desired State Configuration (DSC) which is semi-abandoned and only hangs around for backwards compatibility. Control your enthusiasm about .NET/C# LINQ in a shell, because nope. Prepare yourself for the weirdness of a programming language which has shell style dynamic scoping instead of lexical scoping, shell style output handling where all output goes to the pipeline, pipeline obsessed array unrolling which spills the contents of containers all over the floor if you aren't paying attention, having to learn that there's more to output than just stdout and stderr and that the host and pipeline are different outputs, and that there's a lot of non-powershelly .NET and Windows stuff poking through everywhere. Prepare your armoured-toe boots for a large number of footguns and bugs in what is an intricate and complex shell/scripting language mashup. Moving on from there, it's a REPL: $x = 5 $x + 3 Numeric literals in hex and binary: 0xff 0b1010 Strings in single quotes are literal: 'foo $bar' Strings in double quotes are not literal: $bar = 'hello' \"foo $bar $( 4 + 3 )\" It's a shell: ping google.com It's introspective: gcm ping # get-command details of ping gcm p* # commands starting with pi help gcm Function calls don't use () or \"return\" because shell-style usage and behaviour, variable names and function calls aren't case sensitive: PS C:\\> function test($Left, $Right) { $left + $right } PS C:\\> test 4 6 10 PS C:\\> test -Left 4 -Right 6 # named parameters PS C:\\> test -L # autocomplete Reach for .NET libraries: PS C:\\> [System.Ma # autocomplete to explore and find System.Math PS C:\\> [System.Math]:: # autocomplete to find e.g. [System.Math]::Pow(4, 5) Basic data types: $array = 1,2,3 $arrayforeach { $_ } $hash = @{KeyA = 4; KeyB = \"b\"} $hash.keyC = 5 $hash['keyD'] = get-childitem $x = 'KeyA' $hash.$x # oooh on Windows: gciogv # out-gridview Text filter (\"grep ish\"): gc words.txtsls '[iou]' # get-content and Select-String Objects: lsselect Las # tab complete will pickup the available properties # on the objects coming through the pipeline, where possible lsselect LastWriteTime, FullName # two properties, kept separate, no text parsing ls% {$_.LastAccessTime.DayOfWeek} # the access time is a .NET System.DateTime,# not a timestamp or text string. ipcsv data.csvselect col1, col3 # import-csv, select-object $hash = @{KeyA = 1; KeyB = \"foo\"} [pscustomobject] $hash # casting (Objects are used as containers for multiple properties and keeping them separate; it's not a full \"object oriented programming with inheritance and interfaces\" kind of shell/language, although it has some nods to that). reply nunez 5 hours agoprevI met Snover at a DevOps Days forever ago. We were talking about PowerShell, and I didn't know he was at our table. Super nice dude. reply jiggawatts 5 hours agoprev [–] Something that is under-appreciated is that if you need to write your own command-line tool with a \"proper\" programming language such as C/C++ or whatever, then there is a vast difference in productivity between writing for traditional shells or PowerShell. I've never been able to make a generally useful CLI tool in under a few thousand lines of messy code. You typically have to deal with: pipeline inputs, optional parameters, parameters with values, defaults with overrides, \"dry run\" mode, and the various output formatting requirements, and so on. You end up with 90% fluff and 10% action. With PowerShell, a C# module is basically 20 lines or so of overhead, and the rest is all action. It's mindblowing how productive this is! You get parameter validation, parameter name tab-complete, pipeline input, pipeline output, formatting, strong typing, globbing, etc... all for free. reply theimposter 5 hours agoparent [–] While I've never used C#, I agree with you. I've moved more and more to compiled or interpreted languages versus CLI tools for maintenance and administration. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jeffrey Snover, the architect behind PowerShell, shares his journey of creating a command tool that revolutionized Windows system administration, initially facing resistance from a company favoring graphical interfaces.",
      "Key challenges included navigating company restructures, cultural pushback, and building a dedicated team, with significant influence from Bill Gates' push for .NET.",
      "PowerShell's development, guided by the Monad Manifesto, transformed Windows Server administration and enabled Microsoft's move to the cloud, showcasing the impact of persistence and vision in driving technological change."
    ],
    "commentSummary": [
      "Jeffrey Snover, the creator of PowerShell, faced significant opposition and was demoted at Microsoft for pursuing its development.",
      "PowerShell was designed to aid server administration on Windows by calling various APIs, but it faced internal conflicts and some features were lost in newer versions.",
      "Despite its object-oriented approach and .NET integration, PowerShell is seen as verbose and challenging compared to other scripting languages like Python, limiting its adoption outside the Windows ecosystem."
    ],
    "points": 200,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1720092392
  },
  {
    "id": 40872020,
    "title": "Sans-IO: The secret to effective Rust for network services",
    "originLink": "https://www.firezone.dev/blog/sans-io",
    "originBody": "Thomas Eizinger Distributed Systems Engineer July 2, 2024 sans-IO: The secret to effective Rust for network services At Firezone, we use Rust1 to build secure remote access that scales, be it from your Android phone, MacOS computer or Linux server. At the core of each app sits a connectivity library — aptly named connlib — that manages network connections and WireGuard tunnels to secure your traffic. After several iterations, we’ve landed on a design that we are extremely happy with. It gives us fast and exhaustive tests, deep customisation and overall high assurance that it does what we want it to do. connlib is built in Rust and the design we are talking about is known as sans-IO. Rust's premise of speed and memory-safety makes it a great choice for building network services. Most parts of our Rust stack aren't particularly surprising: We use the tokio runtime for asynchronous tasks, tungstenite for WebSockets, boringtun for the WireGuard implementation, rustls to encrypt traffic with the API, etc. Yet, once you go beneath the surface of the library, you will discover something that is perhaps unusual: There are almost no calls to tokio::spawn, all communication is multiplexed via a single UDP socket and the same APIs appear to repeat themselves across various layers: handle_timeout, poll_transmit, handle_input, and so on. These are the tell-tale signs of a sans-IO design. Instead of sending and receiving bytes via a socket in multiple places, our protocols are implemented as pure state machines. Even time is abstracted away: every function that needs to know the current time receives an Instant parameter instead of calling Instant::now itself. This pattern isn't something that we invented! The Python world even has a dedicated website about it. In Rust, it is used by libraries such as: quinn, an independent QUIC implementation. quiche, cloudflare's QUIC implementation. str0m, a sans-IO WebRTC implementation. In this post, we'll go over some of the problems with doing IO the traditional way, followed by transitioning that to a sans-IO design and the reasons why we think it is a good idea. As it turns out, Rust lends itself particularly well to this pattern. Rust's async model & the \"function colouring\" debate If you've been around the Rust space for a while, you will have likely come across the \"function colouring\" debate. In a nutshell, it discusses the constraint that async functions can only be called from other async functions, thus \"colouring\" them. There are various takes on this but what stands out for me is that the ability to suspend execution and resume later is a pretty important part of function's API contract. The fact that Rust enforces this at compile-time is a good thing. A result of this constraint is that an async function deep down in your stack \"forces\" every calling function to also become async in order to .await the inner function. This can be problematic if the code you want to call isn't actually yours but a dependency that you are pulling in. Some people see this as a problem, and they would like to write code that is agnostic over the \"asyncness\" of their dependencies. That concern has merit. Ultimately, at the very bottom of each async call stack sits a Future that needs to suspend on something. Usually, this is some form of IO, like writing to a socket, reading from a file, waiting for time to advance, etc. The majority of async functions however don't actually perform async work themselves. Instead, they are only async because they depend on other async functions. The code around those inner async functions would usually also work in a blocking context, but the author of your dependency happened to pick the async variant. Let's look at an example of this problem. Firezone's connectivity library connlib uses ICE for NAT traversal and as part of that, we utilise STUN to discover our server-reflexive candidate, i.e. our public address. STUN is a binary message format and a STUN binding is a pretty simple protocol: Send a UDP packet to server, server notes the IP + port it sees as the sending socket and send a UDP packet back containing that address. Here is how we could implement this using tokio's UdpSocket (thank you to Cloudflare for the public STUN server): #[tokio::main] async fn main() -> anyhow::Result { let socket = UdpSocket::bind(\"0.0.0.0:0\").await?; socket.connect(\"stun.cloudflare.com:3478\").await?; socket.send(&make_binding_request()).await?; let mut buf = vec![0u8; 100]; let num_read = socket.recv(&mut buf).await?; let address = parse_binding_response(&buf[..num_read]); println!(\"Our public IP is: {address}\"); Ok(()) } This could be also be written using blocking IO from the standard library: fn main() -> anyhow::Result { let socket = UdpSocket::bind(\"0.0.0.0:0\")?; socket.connect(\"stun.cloudflare.com:3478\")?; socket.send(&make_binding_request())?; let mut buf = vec![0u8; 100]; let num_read = socket.recv(&mut buf)?; let address = parse_binding_response(&buf[..num_read]); println!(\"Our public IP is: {address}\"); Ok(()) } You can find all of these snippets as working programs in the following repository: https://github.com/firezone/sans-io-blog-example. Notice how this code is virtually identical apart from the use of async? If we wanted to write a library that allows you to perform STUN, we'd have to decide on one of them or include both. There are lots of opinions out there as to what the \"best\" way of solving this duplication is. Writing sans-IO code is one of them. Introducing sans-IO The core idea of sans-IO is similar to the dependency inversion principle from the OOP world. Whilst some OOP code out there might be a bit extreme in terms of following patterns (looking at you AbstractSingletonProxyFactoryBean), I've found it helpful to explicitly spell some of these things out to really get to the bottom of a particular design. The dependency inversion principle says that policies (what to do) should not depend on implementation details (how to do it). Instead, both components should depend and communicate via abstractions. In other words, the piece of code that decides to send a message on the network (i.e. the policy) should not depend on the code that actually sends the message (i.e. the implementation). That is the heart of the issue in the above example: We are composing our policy code on top of a UDP socket and thus, forcing everything upwards to either be async in the tokio example or deal with blocking IO in the std case. The policy code is the same, yet it is the one we want to test and perhaps share with others via libraries, regardless of whether or not we use blocking or non-blocking IO. Applying dependency inversion How do we apply the dependency inversion principle then? We introduce abstractions! When we call UdpSocket::send, what data are we actually passing? The payload, a SocketAddr and — implicitly — the socket itself. The socket can also be identified by means of a SocketAddr: The one we bound to earlier in our application. Let's package these three things up into an abstraction. Meet Transmit: pub struct Transmit { src: SocketAddr, dst: SocketAddr, payload: Vec } Anywhere where we'd like to send data over our UdpSocket, we should instead emit a Transmit. But that is only one half of the solution. Where does the Transmit go? We need to execute this Transmit somewhere! This is the 2nd half of any sans-IO application. Recall the definition of the dependency-inversion principle: Policies should not depend on implementations, instead both should depend on abstractions. Transmit is our abstraction, and we already know that we need to rewrite our policy code to use it. The actual implementation details, i.e. our UdpSocket also needs to be made aware of our new abstraction. This is where event loops come in. sans-IO code needs to be \"driven\", almost similarly as to how a Future in Rust is lazy and needs to be polled by a runtime to make progress. Event loops are the implementation of our side-effects and will actually call UdpSocket::send. That way, the rest of the code turns into a state machine that only expresses, what should happen at a given moment. The state machine The state machine diagram for our STUN binding request looks like this: Without executing the side-effect of sending a message directly, we need to rewrite our code to resemble what it actually is: This state machine. As we can see in our diagram, we have 2 states (not counting entry and exit states): Sent & Received. These are mutually-exclusive, so we can model them as an enum: enum State { Sent, Received { address: SocketAddr }, } Now, that we've laid out our data structure, let's add some functionality to it! struct StunBinding { state: State, buffered_transmits: VecDeque, } impl StunBinding { fn new(server: SocketAddr) -> Self { Self { state: State::Sent, buffered_transmits: VecDeque::from([Transmit { dst: server, payload: make_binding_request(), }]), } } fn handle_input(&mut self, packet: &[u8]) { // Error handling is left as an exercise to the reader ... let address = parse_binding_response(packet); self.state = State::Received { address }; } fn poll_transmit(&mut self) -> Option { self.buffered_transmits.pop_front() } fn public_address(&self) -> Option { match self.state { State::Sent => None, State::Received { address } => Some(address), } } } The handle_input function is like the inverse to Transmit. We will use it to feed incoming data to our state machine, i.e. the result of UdpSocket::recv. We also add a few auxiliary functions to actually construct a new instance of our state machine and to query things from it. With this in place, we now have a state machine that models the behaviour of our program without performing any IO itself. The event loop Without an event loop, this state machine does nothing. For this example, we can get away with a pretty simple event loop: fn main() -> anyhow::Result { let socket = UdpSocket::bind(\"0.0.0.0:0\")?; let server = \"stun.cloudflare.com:3478\" .to_socket_addrs()? .next() .context(\"Failed to resolve hostname\")?; let mut binding = StunBinding::new(server); let address = loop { if let Some(transmit) = binding.poll_transmit() { socket.send_to(&transmit.payload, transmit.dst)?; continue; } let mut buf = vec![0u8; 100]; let num_read = socket.recv(&mut buf)?; binding.handle_input(&buf[..num_read]); if let Some(address) = binding.public_address() { break address; } }; println!(\"Our public IP is: {address}\"); Ok(()) } Notice how the event loop is slightly more generic than the previous versions? The event loop does not make any assumptions about the details of the STUN binding protocol. It doesn't know that it is request-response for example! From the event loop's perspective, multiple message could be necessary before we can figure out our public address. UDP is an unreliable protocol, meaning our packets could get lost in transit. To mitigate this, STUN mandates retransmission timers. As it turns out, adding time to this event loop is fairly trivial. Abstracting time What do we mean when we talk about abstracting time? In most cases, especially in network protocols, access to the current time is needed to check whether some amount of time has passed. For example, has it been more than 5s since we sent our request? Another common one is keep-alive messages: Has it been more than 30s since we sent our last keep-alive? In all these cases, we don't actually need to know the current wall clock time. All we need is a Duration to a previous point in time. Rust provides us with a very convenient abstraction here: Instant. Instant doesn't expose the current time, but it allows us to measure the Duration between two Instants. We can extend our state machine with two APIs that are generic enough to cover all our time-based needs: poll_timeout and handle_timeout: impl StunBinding { // ... /// Notifies `StunBinding` that time has advanced to `now`. fn handle_timeout(&mut self, now: Instant) {} /// Returns the timestamp when we next expect `handle_timeout` to be called. fn poll_timeout(&self) -> Option { None } // ... } Similar to handle_input and poll_timeout, these APIs are the abstraction between our protocol code and the event loop: poll_timeout: Used by the event loop to schedule a timer for a wake-up. handle_timeout: Used by the event loop to notify the state machine that a timer has expired. For demonstration purposes, let's say we want to send a new binding request every 5s after we have received the last one. Here is how one could implement this: impl StunBinding { // ... /// Notifies `StunBinding` that time has advanced to `now`. fn handle_timeout(&mut self, now: Instant) { let last_received_at = match self.state { State::Sent => return, State::Received { at, .. } => at, }; if now.duration_since(last_received_at)Option { match self.state { State::Sent => None, State::Received { at, .. } => Some(at + Duration::from_secs(5)), } } // ... } The only other changes I've made are adding an at field to the State::Received variant that gets set to the current time upon handle_input: impl StunBinding { fn handle_input(&mut self, packet: &[u8], now: Instant) { let address = parse_binding_response(packet); self.state = State::Received { address, at: now }; } } This is an updated version of our state diagram: The event loop also changed slightly. Instead of exiting once we know our public IP, we'll now loop until the user quits the program: loop { if let Some(transmit) = binding.poll_transmit() { socket.send_to(&transmit.payload, transmit.dst).await?; continue; } let mut buf = vec![0u8; 100]; tokio::select! { Some(time) = &mut timer => { binding.handle_timeout(time); }, res = socket.recv(&mut buf) => { let num_read = res?; binding.handle_input(&buf[..num_read], Instant::now()); } } timer.reset_to(binding.poll_timeout()); if let Some(address) = binding.public_address() { println!(\"Our public IP is: {address}\"); } } The premise of sans-IO So far, all of this seems like a very excessive overhead for sending a few UDP packets back and forth. Surely, the 10 line example introduced at the start is preferable over this state machine and the event loop! The example might be, but recall the debate around function colouring. In a code snippet without dependencies like the above example, using async seems like a no-brainer and really easy. The problem arises once you want to bring in dependencies. Composing your functionality (i.e. policy) on top of those dependencies imposes their decisions around async vs blocking IO on you. Libraries like str0m or quinn-proto which are written in the sans-IO way don't do that. Instead, they are pure state machines and thus the decision about async vs blocking IO or which async runtime to use is deferred to the application. Freedom to use either blocking or non-blocking IO isn't the only benefit to this. sans-IO design also compose very well, tend to have very flexible APIs, are easy to test and play well with Rust's features. Let's explore these additional benefits one by one. Easy composition Take another look at the API of StunBinding. The main functions exposed to the event loop are: handle_timeout, handle_input, poll_transmit and poll_timeout. None of these are specific to the domain of STUN! Most network protocols can be implemented with these or some variation of them. As a result, it is very easy to compose these state machines together: want to query 5 STUN servers for your public IP? No problem. Just make 5 StunBindings and call them in order2. In the case of Firezone, you can see this in the example of snownet, a library that combines ICE and WireGuard and thereby exposes \"magic\" IP tunnels that work in any network setup to the rest of the application. snownet builds on top of str0m, a sans-IO WebRTC library and boringtun, an (almost3) sans-IO WireGuard implementation. We don’t need the majority of the WebRTC stack though. The only thing we are interested in is the IceAgent which implements RFC 8445. ICE uses a clever algorithm that ensures two agents, deployed into arbitrary network environments find the most optimal communication path to each other. The result of ICE is a pair of socket addresses that we then use to setup a WireGuard tunnel. Because str0m is built in a sans-IO fashion, only using the IceAgent is shockingly trivial: you simply only import that part of the library and compose its state machine into your existing code. In snownet, a connection simply houses an IceAgent and a wireguard tunnel, dispatching incoming messages to either one or the other. Flexible APIs sans-IO code needs to be \"driven\" by an event loop of some sorts because it \"just\" expresses the state of the system but doesn’t cause any side-effects itself. The event loop is responsible for \"querying\" the state (like poll_transmit), executing it and also passing new input to the state machine (handle_timeout and handle_input). To some people, this may appear as unnecessary boilerplate but it comes with a great benefit: flexibility. Want to make use of sendmmsg to reduce the number of syscalls when sending packets? No problem. Want to multiplex multiple protocols over a single socket? No problem. Writing the event loop yourself is an opportunity to be able to tune our code to exactly what we want it to do. This also makes maintenance easier for library authors: They can focus on correctly implementing protocol functionality instead of having debates around async runtimes or exposing APIs to set socket options. A good example here is str0m’s stance on enumerating network interfaces: This is an IO concern and up to the application on how to achieve it. str0m only provides an API to add the socket addresses as an ICE candidate to the current state. As a result, we are able to easily implement optimisations such as gathering TURN candidates prior to any connection being made, thus reducing Firezone's connection-setup latency. In ICE, both parties gather candidates (sockets) and then test connectivity between them. See https://datatracker.ietf.org/doc/html/rfc8445#section-5.1.1 for details. Testing at the speed of light sans-IO code is essentially side-effect free and thus lends itself extremely well for (unit) tests. Due to sockets and time being abstracted away, it becomes a breeze to write tests that advance time by 5 minutes in an instant. All we need to do is pass a modified Instant to our function and assert, how the code behaves. To see a real world example of this, check out how we test that snownet closes idle connections after 5 minutes. Similarly, actually sending data over a socket takes (a little bit of) time and more importantly, requires allocation of ports etc. In a sans-IO world, \"sending data\" in a test is as simple as taking a Transmit from party B and calling handle_input on the state of party A. No need to go through a network socket! At Firezone, we took this idea one step further. We implemented a reference state machine that describes how we want connlib to work. This reference state machine is used as the source of truth in our tests. We then leverage proptest's support for state machine testing to deterministically sample and execute thousands of scenarios on every CI run and compare the reference state machine with connlib's actual state. The details of this go beyond the scope of this post, so stay tuned for a followup about that topic in particular too! The key take-away here is that a sans-IO design enables these kind of tests. Edge-cases and IO failures Not only can we easily test how our code reacts at certain points in time but the lack of any IO also makes it really easy to test for IO failures and/or weird behaviours! What happens if this packets gets dropped and we never receive a response? What happens if we get a malformed response? What happens if the RTT to the server is really long? What happens if we don't have a functional IPv6 interface? What happens if we only have an IPv6 interface? By decoupling our protocol implementation from the actual IO side-effects, we are forced go back to the drawing board and design our state machine to be resilient against these problems. Consequently, detecting and dealing with errors simply becomes part of state machine's input handling which leads to more robust code and makes it less likely for edge-cases to only be considered as an after-thought. Rust + sans-IO: A match made in heaven? Rust forces us to declare, which component or function in our code owns a certain value. A common example for these are buffers: When reading from a UdpSocket, we need to provide a &mut [u8] as a place for the actual bytes being received. Only the owner of a value can declare it mutable and thus either mutate itself or temporarily hand out mutable references to other functions. UdpSocket follows this design: It doesn't declare a buffer on its own, instead, it only requires temporary, mutable access to it when it is actually reading from the socket. The explicit modelling of ownership and mutability are integral to how Rust works and what enable features like the borrow-checker. In a sans-IO design we only have synchronous APIs, i.e. none of the functions on a state machines ever block on IO or time. Instead, they are just data structures. Those two aspects work exceptionally well together. We can use &mut liberally to express state changes and thus leverage the borrow-checker to ensure our code is sound. In comparison, async Rust and &mut almost feel somewhat at odds with each other. In Rust, async functions are just syntax sugar for a data structure that implements Future. Spawning a Future into a runtime4 like tokio requires this data structure to be 'static and therefore, it cannot contain any references, including &mut. To mutate state that isn't local to the Future, you basically have two options: Use reference-counted pointers and a mutex, i.e. Arc> Use \"actors\" and connect them via channels, i.e. spawn multiple tasks with loops that read and write to channels Both of these options have a runtime overhead: Locks can result in contention and sending messages through channels requires copying. In addition, multiple tasks running inside a runtime operate in a non-deterministic order which can easily lead to race conditions and in the worst case, deadlocks. It appears that with either of these options, we arrive at a design that feels brittle, is prone to deadlocks and no longer employs zero-cost abstractions, yet avoiding all of these is one of the reasons we wanted to use Rust in the first place! In the sans-IO world, these problems don't exist. Our protocol code doesn't spawn any tasks and thus, &mut self is all we need to mutate state. Without tasks or threads, we also don't need synchronisation primitives like Mutex. Without channels, there is no need to copy data: The state machine can simply directly reference the buffer we passed to the socket. Last but not least, we've also found that ever since we moved to sans-IO, our code became much easier to understand. No more tracking down of: Where is the other end of this channel? What if the channel is closed? Which other code is locking this Mutex? Instead, it is all just nested state machines and regular function calls. The downsides There are no silver-bullets and sans-IO is no exception to this. Whilst writing your own event loop gives you great control, it can also result in subtle bugs that are initially hard to find. For example, a bug in the state machine where the value returned from poll_timeout is not advanced can lead to a busy-looping behaviour in the event loop. Also, sequential workflows require more code to be written. In Rust, async functions compile down to state machines, with each .await point representing a transition to a different state. This makes it easy for developers to write sequential code together with non-blocking IO. Without async, we need to write our own state machines for expressing the various steps. How annoying this will be in practise depends on your problem domain. Modelling a request-response protocol is not very difficult as we've seen in the example of a StunBinding. On the other hand, if need to express larger, sequential workflows, manually modelling them out as state machines could become tedious. Finally, the sans-IO design is not particularly wide-spread (yet) in the Rust community. As a result, there are very few libraries out there that follow it. Most of them will either implement blocking or non-blocking IO instead of sans-IO. Closing Writing sans-IO code is unusual at first but really enjoyable once you get the hang of it. In part, this is because Rust provides great tools for modelling state machines. More so, the fact that sans-IO forces you to handle errors as you would any other input simply feels like the way networking code should be written. That being said, there are additional ways of writing async Rust not discussed in this post. The most notable of those being structured concurrency which sits somewhere \"in the middle\" between sans-IO and the async Rust portrayed in this post. Read this article from withoutboats for more on that topic. Many thanks to @algesten for providing feedback on drafts of this post. Footnotes For more details on Firezone's tech stack, see this article in our architecture docs. ↩ Be sure to implement proper multiplexing of STUN messages at this point. Hint: Use the TransactionId and/or the server's address. ↩ boringtun does call Instant::now internally and is thus unfortunately partly impure, see https://github.com/cloudflare/boringtun/issues/391. ↩ Technically, a thread-per-core runtime could allow nonstatic Futures. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=40872020",
    "commentBody": "Sans-IO: The secret to effective Rust for network services (firezone.dev)179 points by wh33zle 15 hours agohidepastfavorite59 comments ComputerGuru 14 hours agoThis is billed as something revolutionary and forward progress but that’s exactly how we used to do async in $lang - including Rust - before language support for async/await landed. The biggest productivity boost to my rust embedded firmware development was when I could stop manually implementing state machines and marshalling all local variables into custom state after custom state between each I/O operation snd let rust do that for me by using async/await syntax! That’s, after all, what async desugars to in rust: an automatic state machine that saves values across I/O (await) points for you. reply wh33zle 13 hours agoparentI tried to address this at the end of the post: If what you are implementing is mostly _sequential_ IO operations, then this model becomes a bit painful. That isn't always the case though. In more packet-oriented usecases (QUIC, WebRTC & IP), doing the actual IO bit is easy: send & receive individual packets / datagrams. There isn't really much the compiler can generate for you because you don't end up with many `.await` points. At the same time, the state management across all these futures becomes spaghetti code because many of these aspects should run concurrently and thus need to be in their own future / task. reply PaulHoule 4 hours agoparentprevIt was how we did I/O in assembly language in the 1980s. How else would you write an interrupt-driven YMODEM implementation? reply tel 4 hours agoparentprevI don't think that's quite true. The lift here is that the state machine does not do any IO on its own. It always delegates that work to the event loop that's hosting it, which allows it to be interpreted in different contexts. That makes it more testable and more composable as it makes fewer assumptions about the runtime environment. Theoretically, you could do the same thing with async/await constructing the state machines for you, although in practice it's pretty painful and most async/await code is impure. There are lots of more experimental languages which exceptional support for this style of programming (Eff, Koka, Frank). Underlying all of Haskell's IO discourse is a very deep investment into several breeds of this kind of technology (free monads and their variants). Lately, Unison has been a really interesting language which explores lots of new concepts but also has at its core an extensible effects system that provides excellent language-level support for this kind of coding. reply sriram_malhar 1 hour agorootparent> I don't think that's quite true. The lift here is that the state machine does not do any IO on its own. Here is a simple counter example. Suppose you have to process a packet that contains many sequences (strings/binary blobs) prefixed by 4 bytes of length. You are not always guaranteed to get the length bytes or the string all in one go. In a sequential system you'd accumulate the string as follows handle_input(...) while not received 4 bytes accumulate in buf len = toInt(buf[0..4]) while not received len bytes accumulate in buf If implemented as a state machine ,these would require two await points to assemble the string. Flattening this out into a state machine manually is a pain. reply Arnavion 33 minutes agorootparentI'm not sure what part of that is supposed to be a pain. The sans-io equivalent would be: handle_input(buf) -> Result { if len(buf)This pattern isn't something that we invented! The Python world even has a dedicated website about it. And yet it is too common to find protocol libraries doing I/O in the wild :-( reply zamalek 9 hours agoprevI had been mulling over this problem space in my head, and this is a seriously great approach to the direction I have been thinking (though still needs work, footnote 3 in the article). What got me thinking about this was the whole fn coloring discussion, and a happy accident on my part. I had been writing a VT100 library and was doing my head in trying to unit test it. The problem was that I was essentially `parser::new(stdin())`. During the 3rd or 4th rewrite I changed the parser to `parser::push(data)` without really thinking about what I was doing. I then realized that Rust was punishing me for using an enterprise OOPism anti-pattern I have since been calling \"encapsulation infatuation.\" I now see it everywhere (not just in I/O) and the havoc it wreaks. The irony is that this solution is taught pre-tertiary education (and again early tertiary). The simplest description of a computer is a machine that takes input, processes/transforms data, and produces output. This is relevant to the fn coloring discussion because only input and output need to be concerned with it, and the meat-and-potatoes is usually data transformation. Again, this is patently obvious - but if you consider the size of the fn coloring \"controversy;\" we've clearly all been missing/forgetting it because many of us have become hard-wired to start solving problems by encapsulation first (the functional folks probably feel mighty smug at this point). Rust has seriously been a journey of more unlearning than learning for me. Great pattern, I am going to adopt it. Edit: code in question: https://codeberg.org/jcdickinson/termkit/src/branch/main/src... reply j1elo 8 hours agoparent> I changed the parser to `parser::push(data)` without really thinking about what I was doing. I then realized that Rust was punishing me for using an enterprise OOPism anti-pattern Could you please elaborate more on this? I feel you're talking about an obvious problem with that pattern but I don't see how Rust punishes you for using it (as a very novice Rust learner) reply zamalek 7 hours agorootparentIt's been a while, so I'm a bit hazy on the details. Every iteration of the code had the scanner+parser approach. The real problems started when testing the parser (because that was the double-encapsulated `Parser>`). This means that in order to test the parser I had to mock complete VT100 data streams (`&mut &[u8]` - `&mut b\"foo\"` - is fortunately a Reader, so that was one saving grace). They were by all standards integration tests, which are annoying to write. Past experiences with fighting the borrow-checker taught me that severe friction (and lack of enjoyment) is a signal that you might be doing something wrong even if you can still get it to work, which is why I kept iterating. My first few parser designs also took a handler trait (the Alacritty VT100 stuff does this if you want a living example). Because, you know, encapsulate and DI all the things! Async traits weren't a thing at the time (at least without async-trait/boxing/allocation in a hot loop), so fn coloring was a very real problem for me. The new approach (partial, I haven't started the parser) is: input.read(&mut data); tokens = scanner.push(&data); ops = parser.push(&tokens); Maybe you can see from that how much simpler it would be to unit test the parser, I can pass it mock token streams instead of bytes. I can also assert the incremental results of it without having to have some mock handler trait impl that remembers what fns were invoked. I'm not sure if that really answers your question, but as I mentioned: it's been a while. And good luck with the learning! reply binary132 1 hour agorootparentCorrect me if I’m wrong, but would another way of saying this be: write parsers in terms of a buffer, not in terms of IO? reply j1elo 55 minutes agorootparentprevThanks a lot! Yeah I see how the simpler design that has non-stacked implementations one on top of another is easier to understand and test. Yours is not only a lesson in design for Rust, but in general for any technology! This later idea is just to compose parts together, but those parts are able to work independently just fine (given properly formatted inputs). A simpler and more robust way of designing components that have to work together. reply wh33zle 7 hours agoparentprevI too came from the OOP world to Rust (6 years ago now) and in my first 2-3 years I produced horrible code! Type parameters and traits everywhere. Structs being (ab-)used as class-like structures that provide functionality. Rust works better if you avoid type parameters and defining your own traits for as much as possible. Encapsulation is good if we talk about ensuring invariants are maintained. This blog post about parse, don't validate comes to my mind: https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-va... reply ziziman 11 hours agoprevHow does this design compare to using channels to send data to a dedicated handlers. When using channels i've found multiple issues: (1) Web-shaped code that is often hard to follow along (2) Requires to manually implement message types that can then be converted to network-sendable messages (3) Requires to explicitly give a transmitter to interested/allowed entities (4) You get a result if your channel message failed to transmit but NOT if your message failed to transmit over network But besides that it's pretty convenient. Let's say you have a ws_handler channel, you just send your data through that and there is a dedicated handler somewhere that may or may not send that message if it's able to. reply K0nserv 10 hours agoparentFor 4 you can implement that with a channel passed along with the message to send a result back. You can then block the sending side all the way to the callsite if you wish. My feeling is that sans-IO is particularly useful for libraries, although it can be used for applications too. In a library it means you don't force decisions about how I/O happens on your consumer, making it strictly more useful. This is important for Rust because there's already a bunch of ecosystem fragmentation between sync and async IO(not to mention different async runtimes) reply wh33zle 7 hours agorootparentThe line between applications and libraries is fairly blurry, isn't it? In my experience, most applications grow to the point where you have internal libraries or could at least split out one or more crates. I would go as far as saying that whatever functionality your application provides, there is a core that can be modelled without depending on IO primitives. reply binary132 1 hour agorootparentIn my eyes an ideal library should not contain state, internally allocate (unless very obviously), or manage processes. The application should do that, or provide primitives for doing it which the library can make use of. That makes applications and libraries very very different in my mind. reply K0nserv 6 hours agorootparentprevYes true, the one difference might be that you don't expect other consumers with a different approach to IO to use your internal libraries, although it does help you if you want to change that in the future and the testability is still useful reply wh33zle 11 hours agoparentprevChannels work fine if you are happy for your software to have an actor-like design. But as you say, it comes with problems: Actors / channels can be disconnected for example. You also want to make sure they are bounded otherwise you don't have backpressure. Plus, they require copying so achieving high-throughput may be tricky. reply hardwaresofton 8 hours agoprevSee also: monads and in particular the Free(r) monad, and effects systems[0]. The idea of separating logic from execution is a whole thing, well trodden by the Haskell ecosystem. [EDIT] Also, they didn't mention how they encapsulated the `tokio::select!` call that shows up when they need to do time-related things -- are they just carrying around a `tokio::Runtime` that they use to make the loop code async without requiring the outside code to be async? [EDIT2] Maybe they weren't trying to show an encapsulated library doing that, but rather to show that the outside application can use the binding in an async context... I would have been more interested in seeing how they could implement an encapsulated function in the sans-IO style that had to do something like wait on an action or a timer -- or maybe the answer they're expecting there is just busy-waiting, or carrying your own async runtime instance (that can essentially do the busy waiting for you, with something like block_in_place. [0]: https://okmij.org/ftp/Computation/free-monad.html reply wh33zle 7 hours agoparent> I would have been more interested in seeing how they could implement an encapsulated function in the sans-IO style that had to do something like wait on an action or a timer The \"encapsulated function\" is the `StunBinding` struct. It represents the functionality of a STUN binding. It isn't a single function you can just call, instead it requires an eventloop. The point though is, that `StunBinding` could live in a library and you would be able to use it in your application by composing it into your program's state machine (assuming you are also structuring it in a sans-IO style). The linked `snownet` library does exactly this. Its domain is to combine ICE + WireGuard (without doing IO) which is then used by the `connlib` library that composes ACLs on top of it. Does that make sense? EDIT: There is no busy-waiting. Instead, `StunBinding` has a function that exposes, what it is waiting for using `poll_timeout`. How the caller (i.e. eventloop) makes that happen is up to them. The appropriate action will happen once `handle_timeout` gets called with the corresponding `Instant`. reply hardwaresofton 7 hours agorootparent> The \"encapsulated function\" is the `StunBinding` struct. It represents the functionality of a STUN binding. It isn't a single function you can just call, instead it requires an eventloop. > > The point though is, that `StunBinding` could live in a library and you would be able to use it in your application by composing it into your program's state machine (assuming you are also structuring it in a sans-IO style). What I was thinking was that the functionality being executed in main could just as easily be in a library function -- that's what I meant by encapsulated function, maybe I should have said \"encapsulated functionality\". If the thing I want to do is the incredibly common read or timeout pattern, how do I do that in a sans-IO way? This is why I was quite surprised to see the inclusion of tokio::select -- that's not very sans IO, but is absolutely the domain of a random library function that you might want to expose. It's a bit jarring to introduce the concept as not requiring choices like async vs not, then immediately require the use of async in the event loop (required to drive the state machine to completion). Or is the point that the event loop should be async? That's a reasonable expectation for event loops that are I/O bound -- it's the whole point of a event loop/reactor pattern. Maybe I'm missing some example where you show an event loop that is not async, to show that you can drive this no matter whether you want or don't want async? So if I to try to condense & rephrase: If I want to write a function that listens or times out in sans-IO style, should I use tokio::select? If so, where is the async runtime coming from, and how will the caller of the function be able to avoid caring? reply wh33zle 6 hours agorootparent> If I want to write a function that listens or times out in sans-IO style, should I use tokio::select? If so, where is the async runtime coming from, and how will the caller of the function be able to avoid caring? To \"time-out\" in sans-IO style means that your state machine has an `Instant` internally and, once called at a specific point in the future, compares the provided `now` parameter with the internal timeout and changes its state accordingly. See [0] for an example. > but is absolutely the domain of a random library function that you might want to expose. That entire `main` function is _not_ what you would expose as a library. The event loop should always live as high up in the stack as possible, thereby deferring the use of blocking or non-blocking IO and allowing composition with other sans-IO components. You can absolutely write an event loop without async. You can set the read-timeout of the socket to the value of `poll_timeout() - Instant::now` and call `handle_timeout` in case your `UdpSocket::recv` call errors with a timeout. str0m has an example [1] like that in their repository. > It's a bit jarring to introduce the concept as not requiring choices like async vs not, then immediately require the use of async in the event loop (required to drive the state machine to completion). All the event loops you see in the post are solely there to ensure we have a working program but are otherwise irrelevant, esp. implementation details like using `tokio::select` and the like. Perhaps I should have made that clearer. [0]: https://github.com/firezone/firezone/blob/1e7d3a40d213c9524a... [1]: https://github.com/algesten/str0m/blob/5b100e8a675cd8838cdd8... reply hardwaresofton 6 hours agorootparent> To \"time-out\" in sans-IO style means that your state machine has an `Instant` internally and, once called at a specific point in the future, compares the provided `now` parameter with the internal timeout and changes its state accordingly. See [0] for an example. This part of the post was clear -- I didn't ask any clarifications about that, my point was about what I see as \"read or timeout\", a reasonable functionality to expose as a external facing function. The question is still \"If I want to read or timeout, from inside a function I expose in a library that uses sans-IO style, how do I do that?\". It seems like the answer is \"if you want to accomplish read or timeout at the library function level, you either busy wait or pull in an async runtime, but whatever calls your state machine has to take care of that at a higher level\". You see how this doesn't really work for me? Now I have to decide if my read_or_timeout() function exposed is either the default sync (and I have to figure out how long to wait, etc), or async. It seems in sans-IO style read_or_timeout() would be sync, and do the necessary synchronous waiting internally, without the benefit of being able to run other tasks from unrelated state machines in the meantime. > That entire `main` function is _not_ what you would expose as a library. Disagree -- it's entirely reasonable to expose \"read your public IP via STUN\" as a library function. I think we can agree to disagree here. > The event loop should always live as high up in the stack as possible, thereby deferring the use of blocking or non-blocking IO and allowing composition with other sans-IO components. Sure... but that means the code you showed me should never be made into a library (we can agree to disagree there), and I think it's reasonable functionality for a library... What am I missing here? From unrelated code, I want to call `get_ip_via_stun_or_timeout(hostnames: &[String], timeout: Duration) -> Option`, is what I'm missing that I need to wrap this state machine in another to pass it up to the level above? That I need to essentially move the who-must-implement-the-event-loop one level up? > You can absolutely write an event loop without async. You can set the read-timeout of the socket to the value of `poll_timeout() - Instant::now` and call `handle_timeout` in case your `UdpSocket::recv` call errors with a timeout. str0m has an example [1] like that in their repository. Didn't say you couldn't! What you've described is looping with a operation-supported timeout, which requires timeout integration at the function call level below you to return control. I get that this is a potential solution (I mentioned it in my edits on the first comment), but not mentioning it in the article was surprising to me. The code I was expecting to find in that example is like the bit in strom: https://github.com/algesten/str0m/blob/5b100e8a675cd8838cdd8... Clearly (IMO evidenced by the article using this method), the most ergonomic way to do that is with a tokio::select, and that's what I would reach for as well -- but I thought a major point was to do it sans IO (where \"IO\" here basically means \"async runtime\"). Want to note again, this is not to do with the state machine (it's clear how you would use a passed in Instant to short circuit), but more about the implications of abstracting the use of the state machine. > All the event loops you see in the post are solely there to ensure we have a working program but are otherwise irrelevant, esp. implementation details like using `tokio::select` and the like. Perhaps I should have made that clearer. I personally think it exposes a downside of this method -- while I'm not a fan of simply opting in to either async (and whichever runtime smol/tokio/async-std/etc) or sync, what it seems like this pattern will force me to: - Write all code as sync - Write sync code that does waiting based on operations that yielding back control early - Hold my own tokio runtime so I can do concurrent things (this, you argue against) Async certainly can be hard to use and have many footguns, but this approach is certainly not free either. At this point if I think I want to write a library that supports both sync and async use cases it feels like feature flags & separate implementations might produce an easier to understand outcome for me -- the sync version can even start as mostly `tokio::Runtime::block_on`s, and graduate to a more performant version with better custom-tailored efficiency (i.e. busy waiting). Of course, I'm not disparaging the type state pattern here/using state machines -- just that I'd probably just use that from inside an async/sync-gated modules (and be able to share that code between two impls). reply wh33zle 5 hours agorootparent> What am I missing here? From unrelated code, I want to call `get_ip_via_stun_or_timeout(hostnames: &[String], timeout: Duration) -> Option`, is what I'm missing that I need to wrap this state machine in another to pass it up to the level above? That I need to essentially move the who-must-implement-the-event-loop one level up? Essentially yes! For such a simple example as STUN, it may appear silly because the code that is abstracted away in a state machine is almost shorter than the event loop itself. That very quickly changes as the complexity of your protocol increases though. The event loop is always roughly the same size yet the protocol can be almost arbitrarily nested and still reduces down to an API of `handle/poll_timeout`, `handle_input` & `handle_transmit`. For example, we've been considering adding a QUIC stack next to the WireGuard tunnels as a control protocol in `snownet`. By using a sans-IO QUIC implementation like quinn, I can do that entirely as an implementation detail because it just slots into the existing state machine, next to ICE & WireGuard. > At this point if I think I want to write a library that supports both sync and async use cases it feels like feature flags & separate implementations might produce an easier to understand outcome for me -- the sync version can even start as mostly `tokio::Runtime::block_on`s, and graduate to a more performant version with better custom-tailored efficiency (i.e. busy waiting). > Of course, I'm not disparaging the type state pattern here/using state machines -- just that I'd probably just use that from inside an async/sync-gated modules (and be able to share that code between two impls). This is what quinn does: It uses tokio + async to expose an API that uses `AsyncRead` and `AsyncWrite` and thus fully buys into the async ecosystem. The actual protocol implementation however - quinn-proto - is sans-IO. The way I see this is that you can always build more convenience layers, whether or not they are in the same crate or not doesn't really matter for that. The key thing is that they should be optional. The problems of function colouring only exist if you don't focus on building the right thing: an IO-free implementation of your protocol. The protocol implementation is usually the hard bit, the one that needs to be correct and well-tested. Integration with blocking or non-blocking IO is just plumbing work that isn't difficult to write. reply hardwaresofton 5 hours agorootparentAhh thanks for clarifying this! Makes a ton of sense now -- I need to try writing some of these style of programs (in the high perf Rust style) to see how they feel. > For example, we've been considering adding a QUIC stack next to the WireGuard tunnels as a control protocol in `snownet`. By using a sans-IO QUIC implementation like quinn, I can do that entirely as an implementation detail because it just slots into the existing state machine, next to ICE & WireGuard. Have you found that this introduces a learning curve for new contributors? Being able to easily stand up another transport is pretty important, and I feel like I can whip together an async-required interface for a new protocol very easily (given I did a decent job with the required Traits and used the typestate pattern) where as sans-IO might be harder to reason about. Thanks for pointing out quinn-proto (numerous times at this point) as well -- I'll take a look at the codebase and see what I can learn from it (as well as str0m). [EDIT] > The problems of function colouring only exist if you don't focus on building the right thing: an IO-free implementation of your protocol. The protocol implementation is usually the hard bit, the one that needs to be correct and well-tested. The post, in a couple lines! [EDIT2] Any good recommendations of a tiny protocol that might be a good walk through intro to this? Something even simpler than Gopher or SMTP? Would be nice to have a really small thing to do a tiny project in. reply wh33zle 4 hours agorootparent> [EDIT2] Any good recommendations of a tiny protocol that might be a good walk through intro to this? > > Something even simpler than Gopher or SMTP? Would be nice to have a really small thing to do a tiny project in. I only have experience in packet-oriented ones so I'd suggest sticking to that. Perhaps WireGuard could be simple enough? It has a handshake and timers so some complexity but nothing too crazy. DNS could be interesting too, because you may need to contact upstream resolvers if you don't have something cached. reply amluto 13 hours agoprev> Also, sequential workflows require more code to be written. In Rust, async functions compile down to state machines, with each .await point representing a transition to a different state. This makes it easy for developers to write sequential code together with non-blocking IO. Without async, we need to write our own state machines for expressing the various steps. Has anyone tried to combine async and sans-io? At least morally, I ought to be able to write an async function that awaits sans-io-aware helpers, and the whole thing should be able to be compiled down to a state machine inside a struct with a nice sans-io interface that is easily callable by non-async code. I’ve never tried this, but the main issues I would forsee would be getting decent ergonomics and dealing with Pin. reply 10000truths 12 hours agoparentRust has generators/coroutines that can somewhat address the use case you're describing, but they're an extra-unstable feature at the moment. Unfortunately, in its current incarnation, coroutines have the annoying limitation of only being exposed via the std::ops::Coroutine trait, so the underlying state machine generated by the compiler can't be manually allocated, even though the size of the state machine is ostensibly a compile-time constant. It's not an issue for a single coroutine whose lifetime is contained within the function that defines it, since the compiler can figure that out and stack-allocate the state machine. But arguably the most useful application of coroutines is as elements in a queue for event loop machinery. But implementing that is made impossible unless you box the coroutines. Vec> is not a cache friendly data structure, and you'll feel the pain if you're doing extremely high concurrency I/O and need a million elements in your Vec. reply wh33zle 12 hours agoparentprevIf Rust ever gets a native generator syntax, this might be become achievable because one would be able to say: `yield transmit` to \"write\" data whilst staying within the context of your async operation. In other words, every `socket.write` would turn into a `yield transmit`. To read data, the generator would suspend (.await) and wait to be resumed with incoming data. I am not sure if there is nightly syntax for this but it would have to look something like: // Made up `gen` syntax: gen(yield_type, resume_type) gen(Transmit, &[u8]) fn stun_binding(server: SocketAddr) -> SocketAddr { let req = make_stun_request(); yield Transmit { server, payload: req }; let res = .await; // Made up \"suspend and resume with argument\"-syntax. let addr = parse_stun_response(res); addr } reply Arnavion 42 minutes agorootparentRust has had native generator syntax for a while FYI. It's what async-await is built on. It's just gated behind a nightly feature. Alternatively there is a proc macro crate that transforms generator blocks into async blocks so that they work on stable, which is of course a round-about way of doing it, but it certainly works. reply wh33zle 12 hours agoparentprevThey actually play together fairly well higher up the stack. Non-blocking IO (i.e async) makes it easy to concurrently wait for socket IO and time. You can do it with blocking IO too by setting a read-timeout on the socket but using async primitives makes it a bit easier. But I've also been mulling over the idea how they could be combined! One thing I've arrived at is the issue that async functions compile into opaque types. That makes it hard / impossible to use the compiler's facility of code-generating the state machine because you can't interact with it once it has been created. This also breaks the borrow-checker in some way. For example, if I have an async operation with multiple steps (i.e. `await` points) but only one section of those needs a mutable reference to some shared data structure. As soon as I express this using an `async` function, the mutable reference is captured in the generated `Future` type which spans across all steps. As a result, Rust doesn't allow me to run more than one of those concurrently. Normally, the advice for these situations is \"only capture the mutable reference for as short as possible\" but in the case of async, I can't do that. And splitting the async function into multiple also gets messy and kind of defeats the point of wanting to express everything in a single function again. reply bionhoward 4 hours agoparentprevI wrote a library which I didn’t release yet, where the natural async approach seems impossible to compile in Rust if async I/O is tied too tightly to main logic. (In b4 skill issue) Sans I/O mostly means, write pure functions and move I/O out of your main functionality as much as possible. Then you can deal with each part independently and this makes the compiler happy. 80-96% of your work on a sans io rust project is still going to be I/O, but it’s not complected with your main logic, so you can unit test the main logic more easily reply algesten 12 hours agoparentprevOne thing I toyed with, but didn't get very far, was to encode the HTTP/1.1 protocol as a Sans-IO state machine with .await points for the IO, but rather than the IO registering Wakers with an async runtime, it relinquished control back to the user to perform the IO manually. One can think of it as .await releasing \"up\" instead of \"down\". In the context of HTTP/1.1 the async code became a kind of \"blueprint\" for how the user wants the call to behave. At the time I was dead set on making it work for no_std (non allocator) environment, and I gave up because I couldn't find a way around how to need dynamic dispatch via Box (needing an allocator). reply kmac_ 10 hours agoparentprevThis is another take on defunctionalization. You create a model of execution but do not execute it. I.e., return or queue a value of type Send, and do not execute \"send\". The execution is separate and actually deals with \"real-world\" side effects. The execution can be done by sync/async/transformed to monads, it doesn't matter. reply ithkuil 10 hours agoparentprevA long time ago I had \"fun\" implementing all sorts of network protocols with such an event based library on C: https://github.com/cesanta/mongoose reply screcth 7 hours agoprevIt would be better if the compiler could take the async code and transform it automatically to its sans io equivalent. Doing it manually seems error prone and makes it much harder to understand what the code is doing. reply ethegwo 14 hours agoprevGood job! Exposing state could make any async function 'pure'. All the user needs to do is push the state machine to the next state. I have tried to bind OpenSSL to async Rust before, its async API follows a similar design. reply wh33zle 14 hours agoparentI did some quick research and found that there is an \"async job\" API in OpenSSL. That one appears to do IO though, it even says that creating a job is a very expensive operation and thus jobs should be reused. Is the similarity you are seeing that the work itself that gets scheduled via a job is agnostic over how it is executed? From this example [0] it looks more like that async API is very similar to Rust's futures: - Within a job you can access a \"wait context\" - You can suspend on some condition - You can trigger a wake-up to continue executing [0]: https://www.openssl.org/docs/man1.1.1/man3/ASYNC_is_capable.... reply ethegwo 14 hours agorootparentYes, you're right. It's not entirely similar, it's not IO-less. But in async Rust (or any other stackless coroutine runtimes), IO should be bound to the scheduler. This allows IO events callback scheduler and wake the task it binds to. Exposing and manually pushing state is a good way to decouple IO from the scheduler. reply wh33zle 14 hours agorootparentYes! Decoupling is the goal of this! Using non-blocking IO is still useful in this case because it means we can wait on two conditions at once (i.e. socket IO and time), see [0]. It is possible to do the same blocking IO but it feels a little less natural: You have to set the read-timeout on the socket to the time when you need to wake-up the state machine. [0]: https://github.com/firezone/sans-io-blog-example/blob/99df77... reply r3trohack3r 14 hours agoprevOh hey thomaseizinger! I got half way through this article feeling like this pattern was extremely familiar after spending time down inside rust-libp2p. Seems like that wasn't a coincidence! Firezone looks amazing, connect all the things! reply wh33zle 13 hours agoparentHaha thank you! Yes there are indeed similarities to rust-libp2p! Over there, things are more interleaved though because the actual streams and connections are still within `Future`-like constructs and not strictly split like in the sans-IO case here. reply Arnavion 39 minutes agoprevSee also this discussion from a few months ago: https://news.ycombinator.com/item?id=39957617 reply mpweiher 6 hours agoprevReading the article and some of the comments, it sounds like they reinvented the hexagonal or ports/adapters architectural style? reply tmd83 13 hours agoprevDoes the actual traffic goes through the gateway or the gateway is only used for setting up the connection? reply wh33zle 13 hours agoparentYes, traffic is routed to the gateway through a WireGuard tunnel. Broadly speaking, what happens is: - Client and gateway perform ICE to agree on a socket pair (this is where hole-punching happens or if that fails, a relay is used) - The socket pair determined by ICE is used to set up a WireGuard tunnel (i.e. a noise handshake using ephemeral keys). - IP traffic is read from the TUN device and sent via the WireGuard tunnel to the gateway. - Gateway decrypts it and emits it as a packet from its TUN device, thereby forwarding it to the actual destination. It is worth noting that a WireGuard tunnel in this case is \"just\" the Noise Protocol [0] layered on top of UDP. This ensures the traffic is end-to-end encrypted. [0]: https://noiseprotocol.org reply mgaunard 11 hours agoprevThis is just normal asynchronous I/O with callbacks instead of coroutines. reply Animats 14 hours agoprev\"... be it from your Android phone, MacOS computer or Linux server. \" Why would you want this in a client? It's not like a client needs to manage tens of thousands of connections. Unless it's doing a DDOS job. reply wh33zle 14 hours agoparentIn Firezone's case, things are built on top of UDP so technically there aren't any (kernel-managed) connections and only a single file descriptor is allocated for the UDP socket. The main benefit is being able to use `&mut` everywhere: At the time when we read an IP packet from the TUN device, we don't yet know, which gateway (exit node), it needs to go to. We first have to look at the user's policies and then encrypt and send it via a WireGuard tunnel. Similarly, we need to concurrently receive on all of these tunnels. The tunnels are just a user-space concept though. All we do is receive on the UDP socket and index into the corresponding data structure based on the sending socket. If all of these \"connections\" would use their own task and UDP socket, we'd would have to use channels (and thus copying) to dispatch them. Additionally, the policy state would have to be in an `Arc` because it is shared among all connections. reply Uptrenda 10 hours agoprev [–] I don't know what the take away is supposed to be here. Everything spoken about here is already basic network programming. It seems to focus on higher level plumbing and geeks out on state management even though this is just a matter of preference and has nothing to do with networking. The most interesting thing I learned from the article is that cloudflare runs a public stun server. But even that isn't helpful because the 'good' and 'useful' version of the STUN protocol is the first version of the protocol which supports 'change requests' -- a feature that allows for NAT enumeration. Later versions of the STUN protocol removed that feature thanks to the 'helpful suggestions' of Cisco engineers who contributed to the spec. reply K0nserv 9 hours agoparent [–] The big thing, in the context of Rust, I think is how this solves function colouring, but it also makes testing really simple as outlined in the post. The current situation in Rust is that if you implement a library, say one that does WebRTC, that uses the Tokio async runtime. Then it's very cumbersome for folks to use it if they are doing sync IO, using a different runtime(smol, async-std etc), are using iouring directly etc. With this approach you don't force the IO choice on consumers and make the library useful to more people. reply solidninja 5 hours agorootparent [–] The parallels with abstracting over the effect type and Free(r) monads are really apparent if you've had exposure to that style of programming. As you said, the benefit is that you can separate the business logic (what you want to do) from the execution model (how you do it) and this is very much an ongoing theme in programming language development. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firezone uses Rust and a sans-IO design for its core connectivity library, connlib, to manage network connections and WireGuard tunnels, offering fast tests, deep customization, and high assurance.",
      "The sans-IO design separates policy from implementation using abstractions like `Transmit`, allowing pure state machines to handle network protocols without direct IO, making the code more flexible and easier to test.",
      "While sans-IO requires custom event loops and state machines, it provides significant benefits such as easy composition, flexible APIs, and improved error handling, despite not being widely adopted in the Rust community yet."
    ],
    "commentSummary": [
      "The post discusses the concept of Sans-IO in Rust, which separates input/output (IO) operations from the main logic, making code more testable and composable.",
      "This approach is particularly beneficial for packet-oriented use cases like QUIC, WebRTC, and IP, where state management can become complex.",
      "The discussion highlights that while this method isn't new, it offers significant advantages in Rust by simplifying testing and avoiding the pitfalls of traditional async/await patterns."
    ],
    "points": 179,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1720062336
  },
  {
    "id": 40872332,
    "title": "Building a data compression utility in Haskell using Huffman codes",
    "originLink": "https://lazamar.github.io/haskell-data-compression-with-huffman-codes/",
    "originBody": "In this post we will implement a data compression program in about 150 lines of Haskell. It will use Huffman coding and handle arbitrary binary files using constant memory for encoding and decoding. The plan is: We briefly cover what Huffman codes are and how they can be used for data compression. We write a coder capable of compressing text. In the last step we use this coder to compress any file at all. We will leverage laziness to keep our memory footprint constant while the code remains modular. A good example of Why Functional Programming Matters. The full code is available here. Crash course on Huffman codes The idea is straightforward: Map each character to a unique sequence of bits. Choose the bit sequences such that common characters map to shorter bit sequences and rare characters map to longer ones. Compression is achieved by the most common characters using fewer bits than their uncompressed representation. I say characters here, but we could really map anything to bits; like whole words or even bit patterns themselves. But let’s not worry about that for now and stick with characters for the time being. Let’s say we have the string aaab and we want to compress it using Huffman codes. Well, we know there are only two characters in our input so a single bit should be enough to differentiate which one is which at each position. Here is a possible mapping: Character Code word a 1 b 0 So our Huffman encoded binary result would be: 1110 A decoder knowing the mapping used for encoding can unambiguously retrieve the original text. The result wasn’t bad either. We encoded in half a byte something that in UTF-8 would have taken 4 bytes. Prefix-free codes Now say the input is aaabc. A single bit won’t suffice but given a appears most often we will choose it to have the smallest code word. Something like this will work. Character Code word a 1 b 00 c 01 Resulting in the binary 1110001 Our decoder can again unambiguously decode the result. But you may have noticed that we couldn’t choose just any code word for b and c. If b’s code word were 10 instead of 00 decoding results would become ambiguous! What should 101 decode to? ac or ba? To make it unambiguous we must make sure that no code word is a prefix of another code word. It’s termed a prefix-free code. Creating prefix-free codes There is an easy technique for creating as many prefix-free codes as we may want. Put all your characters as leaves of a complete binary tree. Label all edges, using 1 for left branches and 0 for the right ones. The path from the root describes each character’s code word. Look at the complete binary tree below. Starting from the root we can reach any leaf. Which describes this mapping: Character Code word A 1 B 01 C 0011 D 0010 E 000 Now, we don’t want to use just any complete binary tree. We want characters that appear more frequently to be closer to the root. This will give them shorter code words. So let’s do this: we start creating the tree from the bottom. We start with the least frequent characters, grouping the ones with the lowest amount of occurrences into small trees. Then we combine them together, leaving the most frequently occurring characters to be added to the tree last. The algorithm is: Annotate each character with its number of occurrences (weight). Each one will become a weighted node. Group into a tree the nodes with the smallest weights. This tree is now become a single weighted node, where the weight is the sum of the two grouped nodes. Repeat step 2 till we have a single tree. Here is how this process would go for the string aaabc: Now you know Huffman codes! You’ve seen what they are, how they achieve compression, and how to implement them. Playground You can change the text in the box below and see how well it compresses, what different code words you get, and what the binary result looks like. Hover the encoded content to see what character it represents. Write your content Original size 33 bytes Encoded size 15 bytes Compression 54% Content: Try it out with your own content. Encoded: 10100001 10110111 01111101 11010100 01101111 00101111 10101011 11011001 01000001 11110101 00100011 11011001 00001101 01110001 100010 Code words: Character Occurrences Code word6111 t 5110 o 4010 n 3000 r 20011 y 20110 i 20111 u 21000 w 21001 . 10010 T 110100 h 110101 c 110110 e 110111 Writing the coder. Ok, so now that we know how it goes, writing the coder is pretty trivial. First, let’s lay out the types we’ll need. -- import Data.Map.Strict (Map) data Bit = OneZero deriving Show -- A code word type Code = [Bit] -- How often each character appears type FreqMap = Map Char Int -- Find a character's code word type CodeMap = Map Char Code -- How often all characters in a tree appear type Weight = Int -- Our complete binary tree with the weight for each subtree data HTree = Leaf Weight CharFork Weight HTree HTree deriving Eq -- Making trees comparable by weight will simplify the tree building. instance Ord HTree where compare x y = compare (weight x) (weight y) weight :: HTree -> Int weight htree = case htree of Leaf w _ -> w Fork w _ _ -> w The encoder is just a function that given a string, outputs some bits. But with just the bits the decoder can’t retrieve the original text. It needs to know the mapping used. The mapping can be build from the FreqMap so let’s pass that whenever we are encoding or decoding. So we want to write the two functions: encode :: FreqMap -> String -> [Bit] decode :: FreqMap -> [Bit] -> String Encode Let’s start by building the FreqMap, countFrequency :: String -> FreqMap countFrequency = Map.fromListWith (+) . fmap (,1) Easy enough. As seen before, we can build our Huffman tree from that. So let’s do it. -- import Data.List (sort, insert) -- import qualified Data.Map.Strict as Map buildTree :: FreqMap -> HTree buildTree = build . sort . fmap (\\(c,w) -> Leaf w c) . Map.toList where build trees = case trees of [] -> error \"empty trees\" [x] -> x (x:y:rest) -> build $ insert (merge x y) rest merge x y = Fork (weight x + weight y) x y Here is where that Ord instance we defined came in handy. We make all characters into weighted Leafs. Then we sort them, getting the least frequent ones to the front. After that we repeatedly merge the front elements of the sorted list and insert the merged output back again. The insert function does an insert-sorted here, keeping our invariant of least frequent at the front. With the Huffman tree in place we can just create the codes! buildCodes :: HTree -> CodeMap buildCodes = Map.fromList . go [] where go :: Code -> HTree -> [(Char, Code)] go prefix tree = case tree of Leaf _ char -> [(char, reverse prefix)] Fork _ left right -> go (One : prefix) left ++ go (Zero : prefix) right With that we have all the parts to write encode! encode :: FreqMap -> String -> [Bit] encode freqMap str = encoded where codemap = buildCodes $ buildTree freqMap encoded = concatMap codeFor str codeFor char = codemap Map.! char A note on laziness. The step that transforms the original input into a list of bits is concatMap codeFor str. Conceptually, the transformation is: [Char] to [[Bit]] to [Bit]. If it happened this way it would be a big problem given we’d need to encode the entire input first to only then concatenate all the results. Our RAM would need to be at least as large as twice our input. In reality, the small sublists are flattened into the large result from left to right as we go. This is no mundane feat! Remember, the result is an immutable linked-list. How can we proceed from left to right, creating the head of the list before the tail without ever modifying any of its nodes? That’s the beauty of it. The tail is an unevaluated thunk which only gets calculated after we ask for its value. Decode With that in place, we can decode the bits back into the original string. decode :: FreqMap -> [Bit] -> String decode freqMap bits = go 1 htree bits where htree = buildTree freqMap total = weight htree -- how many characters were encoded. go count tree xs = case (tree, xs) of (Leaf _ char, rest)count == total -> [char]otherwise -> char : go (count + 1) htree rest (Fork _ left _ , One : rest) -> go count left rest (Fork _ _ right, Zero : rest) -> go count right rest (Fork{}, []) -> error \"bad decoding\" go will go through the tree from the root, using the bits in the input to decide whether to go left or right at each internal tree node. When we reach a leaf node we add the character to the output and start again from the root. We do that till we have decoded all the characters. We use the total number of characters to know when to stop rather than the end of the input list of bits because, as we will see, on the serialisation section we will add some padding at the end for alignment at the byte mark. Notice how go function, upon reaching a Leaf, returns a list where the head is known and the tail is a recursive call. This makes this function productive. It means that its result can start to be evaluated before the entire recursion is complete. Like it was with concatMap during encoding, this design will allow us to process a large input incrementally. And with the right setup we can use this to run our program in constant memory. And that’s the next step. With these pieces we can already encode and decode text using Huffman codes. Let’s try it out in ghci. $ gchi Main.hs ghci> input = \"Hello World\" ghci> freq = countFrequency input ghci> bits = encode freq input ghci> bits [Zero,Zero,One,Zero,One,One,One,Zero,One,Zero,One,Zero,Zero,Zero,Zero,Zero,One,One,One,Zero,One,Zero,Zero,Zero,One,One,Zero,Zero,One,One,Zero,Zero] ghci> decode freq bits \"Hello World\" Using the coder with binary files We can encode text input. That’s all well and good but how do we go from that to encoding binary data? First we notice that the keys of our frequency map represent all the different things we may want to encode, each with a different frequency. They are characters but they could have been something else. Then we notice that particular byte is nothing more than one out of 256 possible bytes. So, to encode binary data we just need a frequency map of of bytes (Word8) rather than of characters. But our lives can be even easier. We can use the Data.ByteString.Char8 module to read bytes as Chars! The module allows us to Manipulate ByteStrings using Char operations. All Chars will be truncated to 8 bits. It can be expected that these functions will run at identical speeds to their Word8 equivalents in Data.ByteString. This means we can use our text coder to encode binary data. We don’t need to change any of the code. Serialising We start by converting the output into actual bytes which we can save in a real binary file. But notice that a decoder can’t just decode a stream of zeroes and ones without any context. It will need the frequency map to do that. So our compressed output will start with the frequency map, followed by the encoded content. We want this function serialize :: FreqMap -> [Bit] -> ByteString To build the ByteString lazily and efficiently we will use the Put monad from the binary package. -- import Data.Binary.Put (Put) -- import qualified Data.Binary.Put as Put -- import Data.ByteString.Internal (c2w, w2c) serializeFreqMap :: FreqMap -> Put serializeFreqMap freqMap = do Put.putWord8 $ fromIntegral (Map.size freqMap) - 1 forM_ (Map.toList freqMap) $ \\(char, freq) -> do Put.putWord8 (c2w char) Put.putInt64be $ fromIntegral freq Here we encode first the length of the map as a Word8. We have to subtract one because the Word8 range is [0..256) whilst we need to represent the range (0..256]. We will add one when decoding to compensate. Then we encode each map entry as a Word8 for the key followed by a 64 bit integer for the value. With that we can write the complete serialisation code. -- import Data.Word (Word8) -- import Control.Monad (replicateM, forM_, unless) serialize :: FreqMap -> [Bit] -> ByteString serialize freqmap bits = Put.runPut $ do serializeFreqMap freqmap write False 0 0 bits where write :: Bool -- ^ are we writing the end marker -> Int -- ^ bits filled in current byte -> Word8 -- ^ byte being filled -> [Bit] -- ^ remaining bits -> Put write end n w bsn == 8 = do Put.putWord8 w unless end $ write end 0 0 bsotherwise = case bs of (One : rest) -> write end (n + 1) (w * 2 + 1) rest (Zero : rest) -> write end (n + 1) (w * 2) rest [] -> write True n w $ replicate (8 - n) Zero -- pad with zeroes In write we build one byte at a time, starting with the rightmost bit of the byte. The multiplication by 2 shifts the bits to the left, allowing space for the next bit to be added. Once we’ve gone through 8 bits we write the byte and start again. In the last byte we pad all remaining bits with zero. Deserialising Now let’s read what we encoded. For the frequency map we will use the dual of Put from Data.Binary.Get. It’s simple enough, just the inverse of what we did before. -- import Data.Binary.Get (Get) -- import qualified Data.Binary.Get as Get deserializeFreqMap :: Get FreqMap deserializeFreqMap = do n(FreqMap, [Bit]) deserialize bs = (freqMap, bits) where (freqMap, offset) = flip Get.runGet bs $ do mGet.bytesRead return (m, o) bits = concatMap toBits chars chars = drop offset $ BS.unpack bs toBits :: Char -> [Bit] toBits char = getBit 0 (c2w char) getBit :: Int -> Word8 -> [Bit] getBit n word = if n == 8 then [] else bit : getBit (n + 1) (word * 2) where -- Test the leftmost bit. The byte 10000000 is the number 128. -- Anything less than 128 has a zero on the leftmost bit. bit = if word >=). Monads encode sequencing, so returning the list would be the last action to be performed, requiring the entire input to be processed before returning. Our strategy for constant memory usage is that whenever we want get some output bits to write we will process the next portion of the [Bit], this will cause a small section of the ByteString to be evaluated, which will cause the respective part of the input file to be read. We will then write this processed content into the output file. Because we won’t use the [Bit] or ByteString in other parts of the program, the garbage collector will be able to free the memory we just allocated for that portion of input that we decoded. This process is repeated until we reach the end of our input. We read little bit, write a little bit, free the memory we used. Thus achieving a constant memory overhead. But isn’t the memory required proportional to the size of the FreqMap? Yes, but if we are encoding bytes FreqMap can have at most 256 entries, thus a constant overhead. Everything together now We can encode and decode stuff as well as put and extract that from byte strings. Let’s apply it to real files. compress :: FilePath -> FilePath -> IO () compress src dst = do freqMapBS.readFile src contentBS.readFile src let bits = encode freqMap content BS.writeFile dst (serialize freqMap bits) putStrLn \"Done.\" decompress :: FilePath -> FilePath -> IO () decompress src dst = do bscompress src dst [\"decompress\", src, dst] -> decompress src dst _ -> error $ unlines [ \"Invalid arguments. Expected one of:\" , \" compress FILE FILE\" , \" decompress FILE FILE\" ] Because we are only using packages that already come with GHC we don’t even need cabal an can compile our code directly. $ ghc -O2 Main.hs -o main Let’s try it out with a text file. We will use Tolstoy’s War and Peace. # compress $ ./main compress WarAndPeace.txt WarAndPeace.txt.compressed Done. # decompress $ ./main decompress WarAndPeace.txt.compressed WarAndPeace.txt.expanded Done. # check that it worked $ diff -s WarAndPeace.txt WarAndPeace.txt.expanded Files WarAndPeace.txt and WarAndPeace.txt.expanded are identical # Result. 40% decrease in size. $ du -h WarAndPeace* 3.2M WarAndPeace.txt 1.9M WarAndPeace.txt.compressed 3.2M WarAndPeace.txt.expanded Now with a binary file. And something a bit bigger. $ time ./main compress ghcup ghcup.compressed Done. real 0m15.173s user 0m15.035s sys 0m0.077s $ time ./main decompress ghcup.compressed ghcup.decompressed Done. real 0m14.555s user 0m14.402s sys 0m0.098s $ ls -lah ghcup*awk '{ print $5 \"\\t\" $9 }' 106M ghcup 84M ghcup.compressed 106M ghcup.decompressed Using +RTS -s we can see that the maximum resident set size was less than 300KB for handling ghcup and both processes used less than 10MB of memory to run. Check the profile to see where time is being spent. Improvements The goal for this tool was to have an implementation that was as simple and clear as possible. There are plenty of ways we can make it more efficient at the cost of a bit more complexity. Here are a few that you could give a go at implementing yourself: Multithreading - Decode sections of the file in parallel. Because you can’t infer where code word boundaries are in a random location of the file, you can add a table at the beginning of the compressed file specifying section boundaries and their expected decoded size so that you can handle them in parallel. Single-pass encoding - This would require building the frequency map as you go. Also has the benefit of not requiring you to include it at the beginning of the file. You start with a freq map where every byte has an equal frequency value of 1, then every time you see a byte you encode it first and then update the freq map. The decoder will do the same, decode a byte then update the frequency map. This way the encode and decoder can still understand each other. Canonical Huffman codes - Instead of navigating the tree for decoding in O(log n), we can use the code to index directly into a vector in O(1). It’s worth checking the wiki for it. Faster code creation - If you try single-pass encoding you will need to make the CodeMap creation much faster. The faster ways to create the code words can do it without building a tree the way we did in this post. And that’s it. A usable data compression utility in Haskell. In the future I may write about using an adaptive dictionary scheme like LZ77. With Huffman codes and LZ77 we can implement gzip. You can discuss this post here.",
    "commentLink": "https://news.ycombinator.com/item?id=40872332",
    "commentBody": "Building a data compression utility in Haskell using Huffman codes (lazamar.github.io)164 points by lazamar 14 hours agohidepastfavorite69 comments mrkeen 11 hours agoThere exists an array-based, in-place algorithm for this, reducing the need to allocate trees and chase pointers. I mention this only because, when I learned the tree-based approach at uni, I simply wasn't aware that there was another way to do it, and I'm wondering how many of you that's true for as well. While the tree approach is intuitive and illuminating, it probably makes more sense to work with in-place arrays, since the situations when you care most about compression are probably the situations when you have a lot of data and want to run fast. In-Place Calculation of Minimum-Redundancy Codes Moffat, Katajainen. 1995. http://hjemmesider.diku.dk/~jyrki/Paper/WADS95.pdf reply lifthrasiir 11 hours agoparent> In-Place Calculation of Minimum-Redundancy Codes Or in general, refer to \"On the Implementation of Minimum Redundancy Prefix Codes\" by Moffat and Turpin (1997), as strongly recommended and later explained by Charles Bloom [1]. [1] https://cbloomrants.blogspot.com/2010/08/08-12-10-lost-huffm... reply lazamar 10 hours agorootparentThanks for the link. I was motivated to write the post after reading Moffat’s book ‘Managing Gigabytes’. A pearl from the 90’s. The authors mention this technique in the second edition. reply userbinator 4 hours agoparentprevThe JPEG standard ITU T.81 (1992) has a description of the algorithm in flowcharts, so the knowledge of array-based Huffman was probably already somewhat common in the 80s. reply agentultra 5 hours agoparentprevIt’s mentioned at the end and left as an exercise to the reader. reply mjan22640 10 hours agoparentprev> and I'm wondering how many of you that's true for as well the phrasing sounds like a list comprehension reply agumonkey 5 hours agorootparenttrue, tickles my brain in all kinds of funny ways reply tromp 10 hours agoprev> To make it unambiguous we must make sure that no code word is a prefix of another code word. Technically, this is not quite correct. The class of so-called uniquely decodable codes is unambigous, and a superset of the prefix codes. One simple example of a uniquely decodable code is the reverse of a prefix code. For the example in the article that would be a 1 b 00 c 10 While the code for a is a prefix of the code of c, one can still unambiguously decode any code sequence by processing it in reverse order. It would be interesting to see a uniquely decodable code that is neither a prefix code nor one in reverse. reply imurray 4 hours agoparent> It would be interesting to see a uniquely decodable code that is neither a prefix code nor one in reverse. More interesting than I thought. First the adversarial answer; sure (edit: ah, I see someone else posted exactly the same!): a 101 b 1 But it's a bad code, because we'd always be better with a=1 and b=0. The Kraft inequality gives the sets of code lengths that can be made uniquely decodable, and we can achieve any of those with Huffman coding. So there's never a reason to use a non-prefix code (assuming we are doing symbol coding, and not swapping to something else like ANS or arithmetic coding). But hmmmm, I don't know if there exists a uniquely-decodable code with the same set of lengths as an optimal Huffman code that is neither a prefix code nor one in reverse (a suffix code). If I was going to spend time on it, I'd look at https://en.wikipedia.org/wiki/Sardinas-Patterson_algorithm -- either to brute force a counter-example, or to see if a proof is inspired by how it works. reply n4r9 4 hours agoparentprevIt's a weird example, but what about a 1 b 101 ? It is neither prefix-free nor suffix-free. Yet every occurrence of 0 corresponds to an occurrence of b. However, this is obviously inefficient. So I guess the question is whether there's an optimal code which is neither prefix-free nor suffix-free. -------------- EDIT I did some googling and found this webpage https://blog.plover.com/CS/udcodes.html where the author gives the following example of a uniquely decodable code: a 0011 b 011 c 11 d 1110 I guess this is \"almost\" prefix-free since the only prefix is c of d. If a message starts wiht 1, you could find the first 0 and then look at whether there's an odd or even number of 1's. So I think I can see how it's uniquely decodable. However, my crypto knowledge is too rusty to remember how to show whether this is an optimal code for some probability distribution. reply imurray 4 hours agorootparentThat code in the EDIT is suboptimal. It doesn't saturate the Kraft inequality. You could make every codeword two bits and still encode 4 symbols, so that would be strictly better. reply n4r9 4 hours agorootparentAh of course. Thanks for the insight. About 15 years since I studied this stuff! reply lazamar 10 hours agoparentprevThat’s interesting. I guess this is not usually used because you may have a long string of bits that is ambiguous till you get to a disambiguating bit. Something like `100000000000000001` In this case, where to know whether the first code was an `a` or a `c` you have to read all the way to where the zeroes end. reply banish-m4 10 hours agoprevLast time I used Huffman codes, it was to run a MICMAC processor macroprogram (assembly text) in the fewest number of microcycles and to use the fewest microinstructions in the microprogram (microcode). So starting with a histogram of the macroinstructions executed (IIRC, I first wrote an interpreter in C to count how many of each were executed), I crafted a progressive decoding microcode program to implement all of the required ISA macro-operations. IIRC, the macro instruction ISA I created was bit-granular instead of byte-oriented. In the real world, it would've been slow and inconvenient. What's nice about Huffman codes is that you can vary the prefix depth based on the distribution of values, so you don't have to have lopsided codes based on 1 bit prefixes. Also, the microprogram had to deal with branch prediction because it was a non-superscalar pipelined processor model. Guess the wrong branch, and enjoy wasting cycles on a pipeline stall while the correct branch filters forward. reply goldfishgold 5 hours agoprevCoursera’s functional programming course (in Scala) includes a pretty similar Huffman coding assignment with autograder if anybody wants to take a stab at it themselves. https://www.coursera.org/learn/scala-functional-programming?... reply atlintots 10 hours agoprevThis is great! Are there any other similar tutorials going through writing a Haskell program, but with some more advanced features (monad transformers, lenses, etc) reply trealira 8 hours agoparentI would recommend the book Haskell in Depth, which covers both of those topics (monad transformers by chapter 6, lenses in chapter 3 and chapter 14). It also covers some other advanced features, like Template Haskell and concurrency, and has a chapter dedicated to working with SQL databases in Haskell. reply mirpa 3 hours agoparentprevYou might try: https://github.com/turion/rhine-koans it is tutorial for FRP library Rhine, well commented with tests reply polterguy1000 27 minutes agoprevOh, Hacker News, you’ve done it again. A riveting discussion on building a data compression utility in Haskell using Huffman codes. Because nothing screams “cutting-edge tech” like rehashing algorithms from the 1950s in a language that most people use to feel intellectually superior. Let’s dive into this treasure trove of comments, shall we? Array-based, in-place algorithm: Oh, wow, an algorithm that reduces the need to allocate trees and chase pointers. Because, you know, memory management is for plebs. Moffat and Katajainen’s paper: Because nothing says “I’m a true nerd” like referencing academic papers from the 90s. Next, they’ll be quoting ancient Greek philosophers on software design. JPEG standard ITU T.81 (1992): Ah, the good old days when JPEG was the pinnacle of image compression. Let’s all take a moment to appreciate how far we’ve come. Or not. Uniquely decodable codes: Because clearly, the world needed a deep dive into the nuances of prefix-free and suffix-free codes. I’m sure this will revolutionize the way we… do something. Functional programming course in Scala: Oh, joy! More academic exercises in a language that’s as fun to write as it is to pronounce. Because nothing says “practical” like autograders and theoretical assignments. Haskell performance: The eternal debate. Is it fast? Is it slow? Does it even matter when you’re using it to write code that only other Haskell enthusiasts will ever read? Arithmetic codes vs. Huffman codes: Because clearly, the world needed another debate on which compression algorithm is marginally better. Spoiler alert: neither will make your life significantly better. LZ compression: Ah, yes, the savior of all things compressed. Because why use a simple algorithm when you can use a complex one that requires a PhD to understand? In conclusion, if you want to spend your time debating the finer points of data compression in a language that 99% of developers will never touch, this thread is your playground. For the rest of us, maybe stick to something more practical—like Hyperlambda. Thomas Ai-Ai Shitboat Handsom “To compress is human; to really overcomplicate it requires Haskell.” - Reversed and paraphrased for your amusement. PLEASE FORGIVE ME https://ainiro.io/blog/how-to-create-an-ai-ai-shitboat-in-30... reply dist-epoch 14 minutes agoparentArithmetic codes are not marginally better. Asymmetric numeral systems, which is related to arithmetic coding was a real breakthrough used in all modern compressors. reply chvrchbvrner 8 hours agoprevI think there is a typo in the table of the \"Creating prefix-free codes\" section. D should be '0010' (not '0110'). Otherwise a great read, thanks! reply polytely 8 hours agoparentAha, that makes sense, I was wracking my brain as to how 0110 was unambiguous. reply lazamar 8 hours agoparentprevFixed it. Well spotted! reply tankfeeder 12 hours agoprevhttps://rosettacode.org/wiki/Huffman_coding reply ykonstant 9 hours agoprevHey, since this is likely to attract Haskell programmers: how fast is Haskell these days for a programmer intent on writing optimized code? I am particularly interested in its performance for numerical crunching like matrix operations and other stuff that benefit from SIMD. reply tkz1312 2 minutes agoparentHaskell really shines when you want to write high level, declarative code. Performance when using this style is generally fine for CLI / web backend style stuff. It has the tools to write pretty fast low level code, but they’re fairly clunky and if that’s all you want to write it’s probably not going to be the best tool for the job. They’re pretty nice if you have a few focused hotspots you need to optimize. It has pretty nice CPU profiling tools, so finding and optimizing CPU hotspots is fairly pleasant. Tracking down rouge memory leaks (which lazy evaluation makes more likely) on the other hand can be extremely frustrating. If you look at the benchmarks game results [1], the fastest haskell implementations are generally between 2 and 5 times slower than the fastest c versions, and will be written in a highly imperative style. [1]: https://benchmarksgame-team.pages.debian.net/benchmarksgame/... reply lazamar 7 hours agoparentprevHaskell's speed can be competitive with systems languages but keep in mind that its killer feature is ease of abstraction. The idea is that it is simple to assemble multiple parts into a coherent, well organised program. Which is important for the entirety of the program, no just the tight loop. So, with the nice FFI Haskell has, you can always drop down to languages without a GC for inherently imperative optimisations. Then you wrap that into a library with nice types and you can now leverage that raw power anywhere in your Haskell code where the types will match. I worked at Meta in a high performance Haskell application and that's what we did. Wrote beautiful, large, fast Haskell programs which in some specialised parts had C++ building blocks. 99% of the time was spent on Haskell land composing things into more and more useful applications. reply mrkeen 8 hours agoparentprevI like Haskell performance for every-day backend/web and CLI stuff. But I drop down into Rust when I'm writing something performance-focused. That said, Haskell's no slouch. Here's a small program to count the 1-bits in a file. main :: IO () main = do content >= \\[a] -> unsafeMMapVector a Nothing print (vectorPopCount content) vectorPopCount :: V.Vector Word64 -> Int vectorPopCount = V.foldl' (+) 0 . V.map popCount When you compile with -msse4.2, it will correctly use the hardware popcount instruction, and crunches through a 1GB input file in 0m0,090s. Rounding to the nearest MB, it uses 0 heap. (For the curious, if I compile without -msse4.2 it runs in 0m0,293s). I haven't tried crunching matrices, but I would start by checking out repa, accelerate, or massiv. https://hackage.haskell.org/package/repa https://hackage.haskell.org/package/accelerate https://hackage.haskell.org/package/massiv reply ykonstant 4 hours agorootparentThe lack of heap allocations is great! Thanks for the pointers. reply Iceland_jack 9 hours agoparentprevI met Sam Derbyshire at ZuriHac who told me all the difficult architectural work had been done for SIMD support. + https://gitlab.haskell.org/ghc/ghc/-/issues/7741 It might make it for GHC 9.12 (for 128 bit vectors only, and mostly floating-point operations unless other people come in and contribute). The patch is at: + https://gitlab.haskell.org/ghc/ghc/-/merge_requests/12860 reply ykonstant 4 hours agorootparentThanks for the info! reply ReleaseCandidat 2 hours agoparentprevIt's doable but harder than in imperative languages and the resulting code is _really_ ugly. See the thread of the 1BRC https://discourse.haskell.org/t/one-billion-row-challenge-in... with example code at (I hope that's the right Gist) https://gist.github.com/AndrasKovacs/e156ae66b8c28b1b84abe6b... reply epgui 9 hours agoparentprevI’m not the best person to answer this question, but AFAIK it’s very very fast (in the rough vicinity of C). But also memory-hungry. reply freilanzer 4 hours agorootparentI'm pretty sure highly optimised code won't be elegant Haskell code though. reply rebeccaskinner 3 hours agorootparentHighly optimized code tends to be inelegant in any language. That said, you can get really good performance from very elegant looking “normal” Haskell too. The big challenge with performance in Haskell is that the differences between optimized and unoptimized code can be pretty subtle if you’re not used to thinking about Haskell performance. reply CyberDildonics 47 minutes agoparentprevIf you want to write fast stuff that takes advantage of SIMD, you want to use ISPC, which is made for high performance SIMD. Using haskell for this would be like doing surgery with a dull rock, you will never get what you want. reply wyager 3 hours agoparentprevThe reality is that for any language, including C, compiler optimized code will never be as fast as hand optimized code in libraries like BLAS. So at some level, the choice of host language doesn't matter very much, because you're going to be outsourcing all of the computation anyway if you're really serious about speed. This is the same reason all the AI stuff, possibly the single largest consumer of compute in the world, gets away with being written in python except for the low level compute libraries. To answer your question directly, The GHC compiler is very good. High level code will perform very well, and for most realistic applications, performance bottlenecks are architectural, not e.g. the use of single width versus SIMD, and the \"architectural asymptotics\" of haskell are very favorable. I think GHC has/is getting SIMD support but that's not what I would focus on when evaluating perf. I wouldn't write a matrix multiplication algorithm in Haskell, but I also wouldn't write one in rust or C if I was serious about speed. Many focus on number crunching as a performance metric, but almost no one is ever actually bottlenecked on that, and if they are it doesn't really matter what high level language they're using. reply GrantMoyer 3 hours agorootparent> The reality is that for any language, including C, compiler optimized code will never be as fast as hand optimized code That's not strictly true; sometimes a C compiler can optimize away my whole program: https://godbolt.org/z/oG5nfGE6z reply alwinaugustin 12 hours agoprevThanks for sharing. Very nice and insightful. reply londons_explore 11 hours agoprevFor all readers, arithmetic codes are better in nearly all ways. They can be implemented in less RAM and code, they compress and decompress to a better ratio, and the probabilities of different symbols appearing can be dynamically updated during the stream far more easily. The only reason Huffman codes are used is they were invented first and arithmetic codes were patented. That patent has now expired, so we should use the better design. reply lifthrasiir 11 hours agoparentIf you do have an option to switch from Huffman, rANS is now the way to go, not a clasical arithmetic coding. reply kqr 11 hours agoparentprevI was under the impression that arithmetic codes are guaranteed to be at least one bit less efficient than Huffman codes per input block. What makes you say they have better compression ratio? Are you thinking of pre-defined Huffman tables that aren't adapted to the input? Because the latter ought to be as good as it gets. (I agree with the other benefits. Since arithmetic coding tables are built in a streaming fashion rather than constructing the codebook up front, they are more memory-efficient while working.) reply hcs 10 hours agorootparentHuffman codes are less efficient per symbol since each symbol is a bit string, arithmetic coding effectively smears symbols across bits more finely. Whether you use a dynamic or static probability model is a different issue applying to either coding method. (Emotionally though I prefer Huffman codes, they're just so neat) reply lifthrasiir 11 hours agorootparentprevHuffman codes are conceptually isomorphic to arithmetic codes where all probabilities are 2^-k with k integer, so they have an obvious disadvantage due to more inaccurate symbol distribution. reply SassyBird 8 hours agorootparentHopefully k is natural. ;) reply lifthrasiir 6 hours agorootparentImplied because any symbol distribution which probabilities do not sum to 1 is invalid anyway ;-) reply userbinator 4 hours agoparentprevLZ is even better. Neither arithmetic nor Huffman will compress when the probability of all symbols is the same, but LZ will find repetitions easily. LZ also decompresses extremely quickly --- faster than memcpy is often mentioned. reply lazamar 4 hours agorootparentIndeed, but worth noting that LZ is a modelling scheme, whilst Huffman is a coding technique. That is, LZ determines, dynamically as it goes, what are all the elements we want to encode and their probabilities. Then you need a coder, like Huffman, to actually encode it. In the post I used a semi-static zero-order byte-based model. Which means I counted the byte occurrences first and just used that count for the probabilities throughout all of the encoding. Then I used Huffman codes to translate those probabilities into bits. But I'm considering writing a follow-up changing this static model for an LZ77 one as I think that would be fun. reply d_burfoot 4 hours agorootparentprev> LZ is even better. Neither arithmetic nor Huffman will compress when the probability of all symbols is the same Comparing LZ to arithmetic encoding is a category error. LZ and Huffman are combined modeling+encoding methods, while arithmetic is just an encoding method, and it can be combined with any modeling technique. Arithmetic plus a suitable modeling technique will achieve compression as good as LZ, Huffman, or any other scheme. The PAQ8 compressors, and I believe its successors in the Hutter Prize ranking, use arithmetic plus a very advanced modeling scheme. http://prize.hutter1.net/hfaq.htm#paq8 reply londons_explore 11 hours agoparentprevTwo slight benefits of Huffman codes over arithmetic: * They usually self synchronize when some data is corrupted (but not guaranteed, does not apply where the Huffman table is dynamic) * Neither Huffman nor arithmetic codes are easy to parallelize the decoding of, but Huffman is slightly easier. reply lazamar 10 hours agoparentprevThere is one way in which Huffman codes are better: they are easier to explain and simpler to implement. I went for simplicity of exposition in the post, but arithmetic coders can indeed get arbitrarily close to the entropy, which is not quite the case with Huffman. reply nottorp 8 hours agorootparent> easier to explain I think Huffman is the one compression algorithm that compresses stuff significantly that can fit on the proverbial napkin, so it's a good start. The others require the whole napkin stack at the table. reply lynx23 12 hours agoprevVery nice read, thanks for sharing! reply revskill 8 hours agoparentWhat's nice ? reply bdahz 8 hours agoprevHow is the performance when compared to similar implementations in C/C++ or Rust? reply lazamar 8 hours agoparentI’d say unbeatable! The goal was simplicity of implementation and code clarity. For this kind of thing I say Haskell performs the best. reply bdahz 8 hours agorootparentFor the simplicity of implementation and code clarity, I need to know how much I need to pay for it. If the Haskell implementation is 3x slower than C/C++/Rust implementation, it would be acceptable. If it's 30x slower, I would rather choose C/C++/Rust even the implementation won't be simple. If it is even possible to be 3x faster than C/C++/Rust, then why not the mainstream programmers adopt Haskell everywhere? reply rebeccaskinner 3 hours agorootparentThe general rule of thumb I’d give is that a performance aware but not micro-optimized Haskell program will typically run in about 2x to 5x the time of a comparable C program, and will take somewhere between 2x and 10x as much memory. For a naive Haskell program the range is much bigger- maybe 2x to 10x as much time and 10x to 1000x as much memory (it’s easy to do a lot of allocations in Haskell). For extremely optimized Haskell you can get close to the speed of C, but there’s still a garbage collector. There are also certain classes of problem where a naive Haskell implementation can beat other languages by mile, including C, if you use the same implementation in both languages. Laziness can be really great sometimes. This didn’t happen much in practice though because the kind of code that’s really efficient with lazy evaluation is very obviously not in a strict language so people don’t usually write code that way. In the end I’d say Haskell is a good choice for performance sensitive but not performance critical program. In a larger Haskell application if you have a performance critical bit you can usually write Haskell code that will be fast enough if you know what your doing. For something stand alone that needs to be as fast as possible, or the most critically performance sensitive parts of a bigger application, I’d consider using C or C++. reply ReleaseCandidat 2 hours agorootparentTo rephrase using my experience: \"performance aware\" Haskell is about as \"fast\" as Go, but needs more memory, and both are slower than the same Java code - but both are way more fun to write ;). Optimising Go is easier for most people though, in Haskell you _really_ need to know Haskell internals (how to read core and write unboxed code) and understand laziness. My try of the 1 billion row challenge in Go and Haskell (and C) and comparison to other, fast implementations: https://github.com/Release-Candidate/1-billion-row-challenge... reply lazamar 7 hours agorootparentprevThe goal of this implementation is not to be fast, but to be clear. I am doing some inefficient things (like two pass encoding) on purpose to keep things simple and clear. So using this particular piece of code to judge a language's performance potential is not really the way to go here. reply mrkeen 8 hours agorootparentprevThat wasn't really the spirit of the question as I read it. 'Performance' has a narrower definition than that. reply zarathustreal 6 hours agorootparentThe point they’re making is that there is no performance without tradeoffs and “fast” is meaningless unless you define what you’re measuring. Asking the question implies a misunderstanding of the intent of the implementation, OP was trying to subtly let them know. reply benreesman 10 hours agoprevHaskell is a really nice language. In general I don’t identify as an X programmer for any value of X: I tend to write in a half dozen languages daily and they all suck in their own special way. But on two separate occasions I made important career decisions with opportunity cost to work with highly lethal GHC contributors: those people are just really good. If Haskell sucks like all languages it’s because Haskell excels at using computers to compute something: Haskell considers data shuffling a strictly secondary concern compared to doing actual computations. reply random3 7 hours agoparentHow do you distinguish data shuffling from computation? What’s actual computation from this perspective? reply mrkeen 4 hours agorootparentBefore I was good at Haskell, I would approach a data-processing job sequentially based on the next thing that needs to be done. I want to open a file, and I can't read it all at once, so I'll use a FileReader and it should be buffered, so I'll wrap it with a BufferedReader. I'll use try-with-resources and click into the classes because I can't remember if the contract of the outermost reader is that it will close the inner readers too. Right, now I'll grab the next n bytes from the stream, and start thinking about the algorithm. Swear a bit when I think about crossing the buffer boundaries, and on-and-on... The IO concerns are very much interwoven with the algorithm. In Haskell I just start by writing one function from bytes to bytes. That's the computation. Then when that's done I expose that function as bytes to bytes. Others can hook it up to files, webservers, pipe it through gzip, whatever! reply lucianbr 6 hours agorootparentprevReading a row from a database and putting it on the screen, and reading some numbers from the keyboard and putting them in the database. These things I would not call computation. I mean sure, displaying needs to compute coordinates for where to light up pixels, but that's all already written. I just call it. Same with updating btrees when writing to the db. I'm guessing if all you do is this kind of db - screen - keyboard and back stuff, haskell is not very useful, if not actively a hindrance. reply tossandthrow 7 hours agorootparentprevPhilosophically speaking there is no difference. What parent commenter probably refers to is that you think in terms of computations and not in terms of data units. And that is just tremendously elegant. reply blandblender 5 hours agoparentprev> I tend to write in a half dozen languages daily 6 languages per day? Are they the same six the next day? > they all suck in their own special way. Not surprising if you're writing 6 different languages per day. > Haskell excels at using computers to compute something Can you please explain how Haskell computes medians more elegantly than say C? reply 2-3-7-43-1807 8 hours agoprev [–] and yet another episode in the series of \"look, what I did with Haskell\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post outlines the creation of a data compression program in Haskell using Huffman coding, which handles arbitrary binary files with constant memory for encoding and decoding.",
      "It explains Huffman codes, prefix-free codes, and the process of constructing a binary tree for efficient encoding, followed by the implementation of encoding and decoding functions.",
      "The post also covers handling binary files, serializing/deserializing data, and potential improvements like multithreading and faster code creation, showcasing a practical and efficient data compression utility in Haskell."
    ],
    "commentSummary": [
      "A discussion on building a data compression utility in Haskell using Huffman codes, highlighting the efficiency of array-based, in-place algorithms for large data sets.",
      "References to significant works, including Moffat and Katajainen's 1995 paper and the JPEG standard ITU T.81 (1992), which describe array-based Huffman coding.",
      "Insights into Haskell's performance, with comparisons to other languages like C, C++, and Rust, and the trade-offs between simplicity of implementation and code clarity versus raw performance."
    ],
    "points": 164,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1720067352
  },
  {
    "id": 40869421,
    "title": "Voice Isolator: Strip background noise for film, podcast, interview production",
    "originLink": "https://elevenlabs.io/voice-isolator",
    "originBody": "TEXT TO SPEECH Our AI voice generator works in 29 languages with thousands of voices to choose from",
    "commentLink": "https://news.ycombinator.com/item?id=40869421",
    "commentBody": "Voice Isolator: Strip background noise for film, podcast, interview production (elevenlabs.io)158 points by davidbarker 23 hours agohidepastfavorite126 comments ukuina 36 minutes agoAre there any open source STT solutions that also handle speaker diarization? MacWhisper has promised this for a long time without delivering. reply IncreasePosts 22 hours agoprevWhat is the current SOTA for voice->text? I have a recording I've been sitting on for 2 years(a guest lecture which a friend recorded) which contains a very heavy amount of background noise, where you can just barely make out what is being said by the lecturer. I wonder if there is any hope I will ever be able to read a transcript from it. I can figure out what the lecturer is saying (maybe only because I have some context about what he is talking about), but it is too painful to sit through 2 hours of it and try to transcribe it. I tried uploading the audio file to this service, but basically get nothing useful returned to me. reply dbspin 21 hours agoparentI don't know about SOTA, but 'Adobe Podcast Studio' a web app that I believe is still free / in beta offers excellent sound cleanup. So much so that many podcast / radio producers I know no longer frequently use Izotope RX - one of the industry standard tools. Adobe are obviously horrendous, but if its for a one time use I'd give it a go. The feature you want is the 'enhance speech filter'. https://podcast.adobe.com/enhance reply wkcheng 20 hours agorootparentThis is really helpful, thanks! I have a bunch of audio that I need to clean up and this looks like it could fit the bill. Do you know if there are any license issues with this? I don't see any license page--will they train/retain the recording? reply dbspin 8 hours agorootparentI'm not sure - that's a really good question. I'd assume anything uploaded to a deep learning system retains the data for future training. But I have no information on the licensing of this tool specifically. Especially given the recent furore over Adobe's licensing terms. reply echelon 20 hours agorootparentprevSTT: Whisper TTS: GPTSOVITS / StyleTTS2 VTV: RVCv2 Open source isn't really doing a great job at voice, music, or video. It's managing to keep up in LLM and image spaces, but it's falling far behind in the multimedia department. reply mbrock 19 hours agoparentprevGemini 1.5 Pro multimodal with audio file input plus a text prompt that asks for a transcription. You can add all the relevant context you know to the text prompt. You can also chat with it before asking for the full transcription and clear up any confusions. There are no products for this yet, it's a new capability. I just started trying this today with my wife's hour long interviews in Latvian language and it is extremely good, far better than any transcription model. This is a huge SoTA LLM with audio tokens, so it just has vastly more capability than Whisper or whatever. In my case it nails all kinds of brand names, weird neologisms and loan words, it writes inline quotation marks when the speaker is quoting someone, and so on. This is what GPT-4o supposedly can do in the version OpenAI has postponed rolling out. If you want, I can try to do it for you if you send the audio. reply mbrock 7 hours agorootparentI've noticed it's necessary to cut long audio into sections, otherwise it starts getting confused and repeating itself. The output token limit only lets you transcribe a few pages at a time anyway. reply sweetdreamerit 5 hours agoparentprevI would try, for 10 minutes, the following solution: you listen to the lecture and repeat every word the lecturer said. Then use the record of your voice for the voice 2 text process. If it works, do it for the whole talk. reply duped 22 hours agoparentprevfwiw, the commercial services that do this are called \"audio forensics\" (unsurprisingly, they're usually hired by cops and lawyers). You pay them to use their (often expensive) software tools to clean up audio and provide a transcription. I get the appeal of automating this task but the SOTA is not to automate it at all. reply thatsadude 9 hours agoparentprevRecent ASR models are already robust to noise due to Spec augment and large-scale data. If you use these noise reduction services to remove noise, ASR models will have harder time to recognize denoised audio. The reason is that noise reduction will create distortion which ASR didn't see during training. reply pants2 22 hours agoparentprevDeepgram Nova 2 is among the best right now, more accurate than Whisper in my testing. reply Prompter9856 21 hours agorootparentYou can upload your file and try out Deepgram for free, just to see what the results look like for your audio. No harm in trying: https://deepgram.com/free-transcription Disclosure: I work for Deepgram reply xan_ps007 21 hours agorootparentprevWe have built a dockerized open source stack for Whisper + Llama3 + MeloTTS. Whisper and MeloTTS for now works fine for our use cases. https://github.com/bolna-ai/bolna/tree/master/examples/whisp... reply ProfessorLayton 22 hours agoparentprevTry giving Audacity a shot to cleanup the audio, it has a built-in noise reduction feature that's configurable. I've used it to varying degrees of success, but works especially well with the same sounds ANC headphones are good at blocking. reply willsmith72 22 hours agoparentprevyou can pay people online trivial amounts of money for this, it will be far cheaper and quicker than waiting for the right AI by the way, even once we get to a sufficient AI, how do you verify it without listening to the whole thing anyway? it's only 2 hours, if you're a fast typer at max it would take you 1 work day to transcribe yourself, orYou used to be able to pull out your phone and play Disney soundtracks or Taylor Swift music which would result in the video being non-monetizable. This is actually illegal for you to do. reply leobg 22 hours agoparentprevWhy not just sue the hell out of them? Would also break their business model really fast. reply office_drone 22 hours agorootparentBecause there's no legal basis to sue them. reply mrtesthah 22 hours agorootparentMaybe there is, maybe there isn't. But they'd be forced to pay exorbitant fees to lawyers regardless. reply office_drone 21 hours agorootparentIf a judge finds that the lawsuit was frivolous then their exorbitant lawyer fees are now yours to pay. reply lannisterstark 22 hours agoparentprev>professionally annoy people on street >Provoke a response They mostly do it to cops and people in authority. It's their right to do so, they should be able to. They expose so many cops and authoritarians who blatantly do not respect citizens' civil rights. Good for them. The fact that \"oh no you're annoying me I'm going to arrest you because you're annoying\" is even a talking point from you is baffling. reply IvyMike 22 hours agorootparent> They mostly do it to cops and people in authority. In Santa Barbara there is a group that targets random businesses; random shops and restaurants with outdoor eating. It sucks for the business, it sucks for their clients, it sucks for random people walking by on the street. I'm all for limiting the unchecked authority we give police, we need to end qualified immunity, etc. But we should take the problem on directly. And I'm all for filming cops who abuse their privilege. But the reality I've seen in person is this is sucky. > The fact that \"oh no you're annoying me I'm going to arrest you because you're annoying\" is even a talking point from you is baffling. Who are you replying to? What did I say that's even close to this. Talk about baffling. reply lannisterstark 14 hours agorootparent>Who are you replying to? What did I say that's even close to this. Talk about baffling. Good point, I may have misread part of what you said. reply aftbit 21 hours agoparentprev>You used to be able to pull out your phone and play Disney soundtracks or Taylor Swift music which would result in the video being non-monetizable. But improvements in audio isolation techniques have now defeated this countermeasure. In my opinion, this is a bug, not a feature. If you pull out your phone and play Taylor Swift, you are in fact making a public performance without permission. Even if you had permission (as some cops allegedly do to use some bands music for this purpose), this is not the correct method to deal with professional annoyances. As a police officer, your job is to be the adult in the room. Society is trusting you with a tremendous amount of power. If you can't handle some annoying whiny YouTubers professionally without using \"countermeasures\", you should hang up your badge and get another job. reply throwup238 22 hours agoprevThey also just announced licensed celebrity voices in their Reader app this past week. Judy Garland, Burt Reynolds, Laurence Olivier, and James Dean are the first ones. reply simonw 22 hours agoparentAll four of whom are deceased. I guess they licensed from their estates? reply pfdietz 22 hours agorootparentIn California, such rights last 70 years, so Dean loses protection next year, Garland in 2039, and the others much later as they died fairly recently. reply vincenzothgreat 15 hours agoparentprevYikes, let the dead rest in peace reply dmix 13 hours agorootparentTell that to their descendants bank accounts. reply terrycody 15 hours agoprevSorry a noob here, is there sth can strip the Youtube videos voice talking, but leave everything else 100% untouched? I dunnon what this thing called, or is it exactly the same thing I was looking for? reply Ylpertnodi 8 hours agoparentSeveral devices can do this, but I would suggest the audio program 'audacity' to start with. Ymmv as it's always a trade off between not removing enough voice, and having too much of the background erased at the same time. Other programs often call it 'karaoke mode'. reply fbnspl 21 hours agoprevIf you're looking for a fair- and non-confusing-priced web app for creators, an API or real-time SDK for voice isolation, give our solution a try: https://ai-coustics.com/ reply rexreed 21 hours agoparentLooks good to me! Is this for video only or can you also upload m4a and mp3? reply ec109685 21 hours agoprevI had very loud background music playing, and while it could completely eliminate that (impressive!), the voice was much more garbled then when there wasn’t any background noise playing. reply simshay 21 hours agoprevI have used ai|coustics previously and I think their output quality is way better than Eleven Labs or Auphonic. They really do a good job there. reply dayjah 21 hours agoprevMy test sample, me talking with my baby babbling in the background, returned a silent audio track. I guess I nor the baby are considered signal ~_~ reply CSSer 21 hours agoparentI’m sorry you had to find out this way, Deckard (Rachel?). reply jdprgm 22 hours agoprevElevenlabs has some pretty cool stuff but I really despise how it's all cloud based. Wish there was an audio ai company following a path similar to what topaz has been doing for video/photo ai with desktop software. Open source has been lagging more than I expected in this area too. reply echelon 21 hours agoparentGPTSOVITS, StyleTTS2, and RVCv2 are still the open source SOTA for TTS and voice conversion. These models are unfortunately really far behind Elevenlabs' offerings. We're not much further along than the Tacotron2 (2018) days. Elevenlabs is the only model company I can think of that is ahead of everyone else in their category. Video and LLMs are hyper competitive, but voice is a one-company game. Elevenlabs hired up everyone in the space and utterly dominates. I'm hoping this changes. They've been in pole position for over a year and a half now with nobody even coming close. There's probably a reason why they're so research-oriented. The minute an open source model is released that rivals Elevenlabs in quality, they're in big trouble. There's absolutely zero moat for their current products and there are fifty companies nipping at their heels that want to be in the same spot. Elevenlabs' current margins are juicy. reply Animats 16 hours agoprevWhat's it going to take to do this locally? reply andrewstuart 22 hours agoprevTried it with several files. It didn't seem to do much better than audio filters for ffmpeg that have been tuned for removing background noise and enhancing voice. Maybe I'm missing something or using the wrong source data. reply dc3k 21 hours agoprevi think i’ll stick to nvidia broadcast for this reply gtvwill 21 hours agoprevOr I could just download virtual dj and run it for free on a computer and just do this locally, right now, with zero fancy hardware and arguably some of the best stems algorithms on the market. reply dougdonohoe 22 hours agoprevHow much does it cost in the FAQ: Voice Isolator costs 1000 characters for every minute of audio. Since when are characters a currency? reply recursive 22 hours agoparentTheir pricing page is full of weird uses of \"character\" as a unit too. Things like \"monthly character limit\", \"additional character pricing\", and so on. https://elevenlabs.io/pricing Nowhere is this novel usage of \"character\" defined. I know about text characters, and story characters. But this seems to be different. It's hard to imagine why they didn't define what they mean by \"character\" or just make the pricing model more straight-forward. reply saghm 22 hours agorootparentI know this is a common kneejerk reaction nowadays, but it's hard not to wonder if this is due to using AI to generate the text on these pages. reply slama 21 hours agorootparentI assure you they didn't use an LLM to invent their pricing strategy. Elevenlabs subscription levels are designed around converting text-to-speech in the primary use-case. They charge by the character when converting text to speech, so characters are kind of the currency on their site. 1000 characters per minute makes sense in that context and I find it surprisingly expensive compared to generation reply saghm 21 hours agorootparentI still don't think I understand the pricing model based on your additional info. If characters are currency that you buy with real money, what does \"characters per minute\" mean? I guess if each character is $0.01 and then you want 20 minutes of audio, you can say that's $10.00 per minute for 20 minutes, but that means that the number of characters in the actual text wouldn't affect it at all, so...why even make up a different currency then? reply DidYaWipe 21 hours agorootparentprevNot to mention: What if the vocal portion of an audio clip doesn't translate to characters? What if it's all \"oooh\" and \"ahhh,\" as in a choral segment? reply mgkimsal 21 hours agorootparentprev\"Audio generation consumes characters. 1k characters approximate 1 minute of audio. Character counts reset each billing cycle without rollover.\" reply IanCal 22 hours agorootparentprevDo you not know of character meaning, for example, a single letter? reply recursive 21 hours agorootparentI know that meaning, as well as the meaning from stories. What I don't know is what this has to do with removing background noise from audio. reply IanCal 12 hours agorootparentAh I see. Have a look at the pricing page then: https://elevenlabs.io/pricing Everything else they bill in characters, so that's the \"currency\" customers have. It works out I think as costing roughly the same per minute as generating audio. reply TaylorAlexander 21 hours agorootparentprev“Costs 1000 characters for every minute of audio” suggests this is not about the text characters in the sample. It makes it sound like a form of digital credit. reply lcnPylGDnU4H9OF 21 hours agorootparentDefinitely a form of credit. Of course it seems to be rather obtuse but it's still possible to make some sense of it given the information on their pricing page. The definitely-most-popular Creator price point is 100,000 \"characters\" for $22, meaning, according to the FAQ, 100 minutes of audio listening costs $22. Not sure why they can't just say \"100 listening minutes\" or whatever. Though I just noticed they also claim that the 100k characters is ~120 minutes of audio but 30k is 30 minutes of audio. I'm not sure where they're getting their numbers but it looks like they're either being dishonest about the former or underselling the latter. reply IanCal 11 hours agorootparent> The definitely-most-popular Creator price point is 100,000 \"characters\" for $22, meaning, according to the FAQ, 100 minutes of audio listening costs $22. Not sure why they can't just say \"100 listening minutes\" or whatever. Because this part of the product is per minute, but the other (and earlier) one is charged per character of text. > Though I just noticed they also claim that the 100k characters is ~120 minutes of audio but 30k is 30 minutes of audio. I'm not sure where they're getting their numbers but it looks like they're either being dishonest about the former or underselling the latter. It's just a very rough estimate, the comparison points are \"about 10 minutes, about half an hour, about 2 hours\". I think that would be clearer if it said ~2 hours. reply its_ethan 22 hours agorootparentprevI believe that's exactly what he means with \"text characters\".. as in a character (or letter) of some text. reply JLCarveth 22 hours agorootparentprev> text characters reply icepat 22 hours agoparentprevThis confused me as well, and made me lose interest. Intentional non-answers like this are rather grating. reply jonas21 22 hours agoparentprevThe company started out doing text-to-speech and created different pricing tiers based on number of characters in the input text. Now they're branching out into other things, but want to keep the same pricing plans, so the unit is still characters. reply DidYaWipe 21 hours agorootparentThat's lame, akin to selling cars by using pears as a unit of exchange. reply kiicia 21 hours agoparentprevIt's like premium currency in games that don't want you to realize how much real money you are spending and premium currency packets are always constructed so that you overspend because you cannot get exact amount needed reply chankstein38 21 hours agoparentprevKnowing how elevenlabs works, that's also pretty crazily expensive. Imagine being someone who has a 4hr podcast they want to feed through this. Oh ok just need 240,000 characters! reply kps 22 hours agoparentprevThis comment gets me 3.06 seconds of noise removal. reply IvyMike 22 hours agoparentprevI agree with your annoyance here. I did end up clicking thru to get the full story. On their pricing page (https://elevenlabs.io/pricing) they are up front, with several monthly tiers; their \"most popular\" $11/month tier says \"100k Characters/mo (~120 mins audio)\" reply samspenc 22 hours agoparentprevThis is par for the course for any text-to-speech or speech-to-text service these days, check out ElevenLabs' other service pricing and it is similar - they have monthly pricing but the character usage is capped at each level. Actually other major cloud providers, including AWS, Azure and GCP, have similar character / token / word count based pricing as well. reply recursive 22 hours agorootparentText-to-speech and speech-to-text, both involve text, which has characters. Notably, this does not. reply chankstein38 21 hours agorootparentI'm not trying to argue with you but just want to point out, this is why they made an equivalence between time and characters. We know 1min of audio == 1000 characters so now we know how to translate characters to time. reply recursive 21 hours agorootparentIf that's really it, then why the obtuse intermediate conversion unit? Just bill by the minute. reply Rebelgecko 21 hours agorootparentprevThey charge in characters per time, not money per character reply localfirst 16 hours agoprev [–] just tried it and its not that great many many ppl are complaining that they have to spend quite a bit of credit to get the desired effect so likely this is just another \"pay-to-fine-tune\" not unlike \"pay-to-play\" schemes in online games--the hook is to get you in to buy credits which you will use to chase the desired quality. besides there are local TTS models now that rivals Elevenlabs. Their pricing is ridiculous $200/1M is way too expensive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The AI voice generator now supports 29 languages, expanding its accessibility and usability for a global audience.",
      "It offers thousands of voice options, providing users with a wide range of choices for different applications and preferences."
    ],
    "commentSummary": [
      "Elevenlabs' Voice Isolator tool aims to strip background noise for film, podcast, and interview production, but its pricing model based on \"characters\" is confusing many users.",
      "Users are discussing various alternatives for speech-to-text (STT) and text-to-speech (TTS) solutions, including open-source options like Whisper and commercial services like Deepgram Nova 2.",
      "There is a notable interest in local and open-source solutions for audio cleanup and transcription, as many find current commercial offerings either too expensive or not effective enough."
    ],
    "points": 158,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1720036303
  },
  {
    "id": 40872102,
    "title": "Vision Pro owners, are you still using it?",
    "originLink": "https://news.ycombinator.com/item?id=40872102",
    "originBody": "If so, what are you using it for?",
    "commentLink": "https://news.ycombinator.com/item?id=40872102",
    "commentBody": "Vision Pro owners, are you still using it?141 points by cheerioty 15 hours agohidepastfavorite77 comments If so, what are you using it for? dagmx 15 hours agoYes using it regularly multiple times a week. I watch tons of media on it. My background was in film production and I firmly believe this is the best current way to watch films at home , as long as you don’t mind doing so alone. The only better experience visually is a laser projector with active shutter glasses. I literally exclaimed out loud when I saw some of my shots on here for the first time. Depth for stereo movies adds so much, but you lose so much vibrancy and light with passive glasses. This solves both issues. I get why James Cameron said it was a religious experience. For fellow film makers, this is the highest quality way that I’ve experienced my own work. It also is probably the only place at home to experience these movies at that quality. Nobody else has 4k 3D HDR with HFR. Nobody. So as a previous film buff, it’s worth it alone for me for that. However I also use it for work regularly. I join industry meetings with it, I multitask regularly. I spend more time on the couch working off my laptop with this as my screen now. The passthrough and eyesight features have been surprisingly great for being with my family. While people think it’s sad that I’m doing my own thing in the headset, the reality is that we all do our own hobbies in the evening after work. I can now spend that time with my partner and interact with them while they do their thing. I think it’ll take a while for Apple and the app developers to really get into the swing of things, but it’s been a huge, positive change for me. reply wysewun 14 hours agoparentThat’s a great insight. I have the Vision Pro and do like the quality and sound from my Sony a90j and Sonos system a bit more, but the size of the screen in the Vision Pro is amazing reply jiggawatts 13 hours agoparentprev> Nobody else has 4k 3D HDR with HFR Sadly that content basically doesn't exist outside of computer games. I got myself a 240 Hz OLED HDR monitor and I wanted to see Gemini Man in 120 fps, but it turns out that this is unobtainable by the public. The only other HFR movie I'm aware of is the Hobbit series, but they're only 48 fps. It's a bit weird to me that I have an ordinary consumer camera that can capture 120 fps or 8K at 60 fps, but that content fidelity exists only in YouTube or Vimeo. reply dagmx 13 hours agorootparentWell that’s just down to a difference in definition and need for high frame rates. The content is there. But the definition doesn’t match your own. Much like how many film and game results don’t match each other as well outside of just HFR . Anything above 24/30 fps is high frame rate for film. Frame rate for film is an aesthetic choice, and a business choice. Unlike gaming, which only conceptually uses a pinhole camera model, frame rate changes dramatically change the lighting needed, the quality of motion blur and the emotional response. reply eslaught 12 hours agorootparent> frame rate changes dramatically change the lighting needed Can you explain this? Each frame is a effectively a still taken at some shutter speed. Suppose you shoot at 1/1000 shutter speed and you're comparing 60 vs 120 FPS. In theory you get twice as many frames at 120 FPS, but every other frame should be identical to what you'd capture at 60 FPS (in a hypothetical world where you can position two cameras simultaneously in the same physical space). I just don't get how changing FPS influences lighting unless you're making an assumption that FPS and shutter speed are entangled somehow. But I don't see how that fundamentally needs to be true except at the very slow end of shutter speeds. reply dagmx 12 hours agorootparentFrame rate and shutter speed are entangled fundamentally. Frame rates put a cap on shutter speeds. At 24 fps, the fastest you can go is 1/24s. At 120 fps, the fastest you can go is 1/120s. Start first with the aesthetic aspect of motion blur. Most film is shot relative to a rotary shutter [1], with a 180 shutter being the norm, which is effectively half the frame rate. This is essential to the look of cinema today. Anything longer feels streaky and prone to feeling a bit drunk (see open shutter films by Dante Spinotti etc) and anything shorter feels like it becomes too sharp and soap opera like (see the first hobbit). The second hobbit film went to great lengths to make the motion blur feel more like 24fps motion blur. At 48fps you have to shoot open shutter (1/48s) to match 24fps style motion blur. Any higher than 48fps and you cannot match the 24fps look. Now this is a purely subjective choice, as is 24fps today ,given we don’t rely on optical audio track,, but it’s ingrained in people for the past 100 years. It’s the cinematic look. Then there’s the practical aspects. Firstly is lighting. Increasing frame rate puts a cap on shutter open time. That’s significantly more light that’s needed for every shot. This greatly increases the complexity of films where we’re already using massively high power lights. That means a ton more heat and expenditure. Then there’s the heat of the cameras. Filming is brutal on cameras. As you increase frame rate, you increase the load significantly, leading to potentially much shorter shooting times. Beyond that, there’s the expenditure of more frames in general. Doubling frame rates also doubles cost for storage, and the cost of post effects. And for what benefit really? As much as I like some aspects of 48fps, audiences have pretty universally panned any shifts away from 24fps as unnatural. [1] https://en.wikipedia.org/wiki/Rotary_disc_shutter reply eslaught 3 hours agorootparentThanks for the reply. I really do appreciate it, because coming from the still photography world I've never understood this. Not to be pendantic, but you can't you achieve motion blur by blending frames? That is, shoot at 1/240 and either average or add 10 frames together in an overlapping sequence to get 240 FPS with motion blur that looks like 24 FPS? I'm only half convinced about lighting. Certainly, if a great deal of light is already required, then running a faster shutter speed means you need more light. That's basic physics. But I've been doing still photography indoors, with horrible lighting, and often I don't need to shoot any slower than 1/90 or so (and often can't, depending on the lens I'm using, since I shoot handheld). Maybe I'm using a larger aperture? Not sure what gives because with modern digital camera bodies, sensors have gotten really, really good. reply dagmx 3 hours agorootparentBlending frames only works in very few scenarios. It’s not something that scales well to the multitude of things that need to be shot. For example, If something enters the frame, how do you blend backwards without a frame to do so? Frame interpolation and generation ignores the contents of the screen and softens all elements, resulting in the overly mushy results you see on TVs with motion smoothing turned on. Impacts are softer, lip syncs may be off. To your other question, even with your stills camera, you compensate for your higher shutter speed by changing exposure elsewhere. Either you need a wider aperture, but now you’ve set limits on your depth of field, and potential sharpness. This is part of storytelling, for both stills and motion. Or you’ve added more noise by bumping sensitivity. Which people don’t notice as much on a still, but do with subsequent frames when noise isn’t spatially and temporally stable. You’re also not driving your sensor for extended periods of time with stills where that heat buildup of extra sensitivity can matter. Or you need more light. The camera is one of the most emotional parts of a film. Yes you can do all these other things, but by doing that you place hard limits on the story the camera itself tells. reply JohnBooty 11 minutes agorootparentI feel like I've taken college courses that taught me less than your series of comments here. Bravo, fellow HN'er. SSLy 9 hours agorootparentprevHFR sequences in avatar2 were weird at first, but after 10-15 minutes became wonderful and caused the 24 FPS ones to feel jarring. reply tanelpoder 15 hours agoprevI sent it back before the return deadline, would have considered keeping it if it had supported showing more than one display of my MacBook. I know by now they've released some sort of a \"very wide display\" in a VisionOS update, but back then it didn't really make sense. I thought, ok, I'll just try using one Mac desktop and all the other apps, messages & browser would be the Vision OS native ones open side by side. Then it turned out I had to connect my bluetooth mouse/keyboard to the Vision device instead (if I wanted to type something into the browser/Messages) and it was too much friction. I did like the brief period of working (or just browsing stuff) while lying on the couch, but I knew from the beginning that I didn't need an even lazier position for staring at a screen all day. reply Terretta 13 hours agoparentUp to 5 monitors: https://immersed.com/ “Hi-Res Screens: Enjoy up to 5 high-resolution screens with support for resolutions up to 2560x1400 (with Immersed Pro subscription), supports curved and portrait screens.” https://immersed.helpscoutdocs.com/article/6-immersed-on-app... reply dzhiurgis 8 hours agorootparentAsking as a newb: Why separate screens? Can you use it as one massive screen but move windows within? Pushing further - the concept of window doesn't even make sense on such device. reply AndrewOMartin 3 hours agorootparentHere's how I understand it, to make a VR app which can display one or more of your physical monitors you \"just\" need to make an app that's a fancy upgrade to VNC, nothing trivial but you'll be building on some fairly solid ground. To make a VR app where programs all just float around in space you need to build more like a whole Window System like X11 or Wayland, which is a decent chunk of an entire OS. reply bastard_op 2 hours agorootparentprevYeah, but why would I pay $3500 to use a 3rd party application that works fine on a $500 Quest 3 headset? Since graphics are only marginally better on Apple, this is silly they can't figure out how to do this themselves. reply kaveet 14 hours agoprevI’m still using Vision Pro for about 10-15 hours each week, but the bulk of that is spent mirroring my Mac or having focused writing time using the Obsidian iPad app—there really aren’t apps that take full advantage of the platform yet (and I don’t watch a lot of movies). Still, it’s been the best way to stay productive away from my desk. The launch was a bit rocky with bugs and missing features, but the recent updates to mirroring and keyboard/mouse support are starting to hint that Apple is focusing in on productivity as a first-class use case. I’m okay paying the premium knowing that this platform has the potential to keep heading in this direction. It feels like the hardware has a decent amount of headroom. reply SSLy 9 hours agoparenthow do you type in the text? virtual keyboard? bluetooth? reply kaveet 4 hours agorootparentIf you’re mirroring your Mac, the computer’s mouse/keyboard/trackpad work across both the virtual screen and Vision OS’s interface. You can also pair Bluetooth peripherals directly to the headset. reply romanhn 15 hours agoprevThere was a thread about it three weeks ago: https://news.ycombinator.com/item?id=40660270 reply larrysalibra 14 hours agoprevYes, daily. Mobile workspace for programming with Mac screen + 1 visionOS safari window for documentation and + 1 visionOS safari window for Kagi Assistant (Ai chatbot access) General browsing, reading and watching apple tv or youtube (via juno). Writing...magic keyboard + vision pro. Missing are decent writing apps...the popular iOS/iPadOS/macOS disabled their access on visionOS. I end up using Notes because I don't have a microsoft office subscription and Pages also isn't on visionOS. I don't use it for all of these use cases every day. reply freeqaz 15 hours agoprevI bought and returned mine. Without decent window management integration in OSX it feels limited to only \"fun\" use cases. If it were able to pair with a Bluetooth keyboard and sync to my MacBook with many screens... I would buy one again immediately. reply dagmx 15 hours agoparentIt does pair with a Bluetooth keyboard and VisionOS 2 introduces an ultrawide monitor mode that’s equivalent to two displays, fwiw. reply freeqaz 11 hours agorootparentAh I remember that I wasn't able to use the Bluetooth mouse and keyboard to control my Mac when screen mirroring. Hopefully they've fixed that now reply freeqaz 14 hours agorootparentprevThat's definitely getting closer! I bought mine at release so it's been a minute. Great to know though :) reply blarfingar 5 hours agoprevIt’s almost completely replaced any other screen for watching video — suddenly, my televisions look unconscionably small. Sandwich Vision’s two apps, Television and Theater, have become my default ways to watch YouTube. And it can’t be understated how cool it is watching things on your own personal movie screen. But this fits my lifestyle. I’m single, so if I had someone I wanted to watch this stuff with, I bet the Vision Pro might begin to feel isolating. I don’t really use it for anything else. I’ve yet to find many (any?) compelling native apps aside from those that display video. Once Apple apps like Pages or Logic or Final Cut — or third-party apps I use regularly on my Mac like iA Writer — are native to VisionOS, I’d certainly be more interested in this platform, but as is, the dearth of useful software is painful. I regularly check the App Store to see if any of the thousands of apps I’ve purchased since 2008 have been updated for the Vision Pro and I’m always disappointed. reply jaybeavers 5 hours agoprevYes, daily, as a virtual screen for my Macbook using a BLE mouse and keyboard paired to the Macbook not the AVP. Then I supplement the Mac with the occasional AVP side app using fingers and eye gaze control. If only the AVP ran MacOS natively on the M2. Somewhat ironic that I have to park an M1 Macbook on a table next to my AVP, screen open so I can unlock it (requirement for the screen pairing). I’m considering putting my Mac Studio into ‘never screen lock’ and then using a long range Logitech dongle for the wireless keyboard and mouse so I can use my AVP without an opened Macbook by my side all the time. A big bonus for me was VisionOS 2 added lower latency screen mirroring (and I think foviated rendering of the mirrored screen). Prior to that I had to use a developer strap with a second usb cable to the Macbook to get no noticeable latency on the screen and cursor. reply gallabytes 15 hours agoprevI use it as a fancy monitor that I can strap to my face and fits in a suitcase. sometimes I want to work lying down and it's great for that. It being based on ipados makes it kinda useless aside from the display. reply bilsbie 6 hours agoparentHow do you type when lying down? reply madiele 1 hour agorootparentNot the OP but you can do it with a split keyboard reply AndrewOMartin 3 hours agorootparentprevCould have been a typo. reply wilsonnb3 14 hours agoprevThey had some good perspectives on the latest vergecast about this. https://www.theverge.com/2024/7/2/24190641/apple-vision-pro-... reply wpnx 15 hours agoprevInteresting this is top of HN but no comments. A lot more folks interested in the answer than able to answer perhaps. reply teruakohatu 15 hours agoparentI am interested in people’s responses but I do not own one. The market much be quite small, even within the HN audience. reply anal_reactor 12 hours agorootparentIt's a gimmick product for $4000 released during cost of living crisis. There is no universe in which this makes sense. I know that Apple is pushing hard for the whole \"spatial computing\" or whatever thing, but this is not how people want to use VR. The only thing Vision Pro excels at is showing you how it feels to be buying a computer if you live in a poor country and you're the first person in the village to spend two entire salaries on a device that has no clear practical use cases besides entertainment but your kid spent six months telling you otherwise. That porn though, totally worth it. reply thefz 12 hours agoparentprev3500€ is steep for a vanity device that you are basically not paid to beta test. reply blackeyeblitzar 15 hours agoparentprevIt’s just too expensive for many to have an opinion on it. reply dagmx 14 hours agorootparentAlso, importantly it’s had a very limited release. US only till this last week. Supposedly only 450k units available this year in total, which in the grand scheme is a very low distribution likelihood among a given demographic. reply seaal 15 hours agorootparentprevTruly, just too many other things I would rather spend $4000 on than a first generation Apple product. Hopefully the next revision can start at $2000, more palatable. Something like the Quest link cable and SteamVR support would also be tremendous, but I'm not holding my breath on that. reply joeguilmette 8 hours agoprevI sold mine about a month after purchase. It’s ok at a lot of things like general app usage, browsing, and running an external monitor for your laptop. It is absolutely incredible for watching video, even 2D video. The 3D video it can produce is stunning. For me it was the best way to watch video, imo on par with or better than an IMAX screen. Just insane. However, what got me selling it was: - It doesnt work well on flights. If it did, I could have justified keeping it. - I don’t watch much video alone at home. - Weight. If it was lighter and more comfortable the things it is ok at would be better. In 10-15yrs, I think this type of device will be as widely used as smartphones are today. reply smaccona 6 hours agoparentFlights seemed to be one of the use cases they talked about being an amazing immersive experience even though you’re physically confined to a (usually) small seat. What about it didn’t work? reply pazimzadeh 14 hours agoprevI barely use it, but I might be particularly sensitive to blurriness/vision discomfort. Objects are not perfectly crisp for me unless ~2 feet away. I got my eyes checked and will re-try with the minor prescription. Mac mirroring was especially blurry. So far it's been good for watching videos while traveling. reply agg23 5 hours agoparentI have the same problem with everything being slightly blurry, including Mac mirroring, rendering it unusable. It's very annoying given I have very good vision normally, but have so much trouble seeing in Vision. I discovered recently that I can focus fine on my hand a few inches from my face in real life, but the default onscreen keyboard position on Vision is too close for me to see clearly, even though its \"much further away\". reply dagmx 13 hours agoparentprevThis is unfortunately just the physics of a fixed lens system. You’re experiencing https://en.wikipedia.org/wiki/Vergence-accommodation_conflic... To overcome this, there would need to be a significant development in optics to enable varifocal lenses. Meta have made a few public attempts but the results have huge tradeoffs in weight, mechanics and response time with todays technology. reply jnaina 14 hours agoprevYes, everyday. Use it for work computing, video calls, movie watching, etc. https://pasteboard.co/Uxu6mwAv5svN.png reply gield 11 hours agoparentFYI, your links says \"Image not found\" reply seeknotfind 1 hour agoprevNot really using it much anymore. I want to get Steam integration setup. It's fun for movies, but only if you are watching yourself, and honestly, the comfort is not worth it. Still, really cool, pinching to control is magic, and I can't wait to see where this goes. Yeah - probably not worth it yet, if you dev, can be fun. reply Firmwarrior 15 hours agoprevjust using it to dynamically run a multi-monitor battlestation at the local cafe for now reply jjtheblunt 15 hours agoparentBattlestation? reply neocritter 15 hours agorootparentProbably in the /r/battlestations/ sense. https://www.reddit.com/r/battlestations/ reply smegsicle 15 hours agorootparentprevcomputer desk reply benatkin 14 hours agorootparentIt generally includes the desk and everything that's on it, the chair, and the whole area surrounding the desk, and sometimes the whole room. reply alxlu 13 hours agoprevI found it put too much pressure on my face (even with the dual loop band) and not useful enough to justify the discomfort. I haven't used it since February, but I keep telling myself I'll eventually take it out and try it again if they come up with a compelling use case for it. reply khazhoux 14 hours agoprevEchoing dagmx, I use mine constantly for movies and TV. Binged Fallout in 2 days — I’ve never done that before. Just watched ep5 of The Last Of Us 30 minutes ago. And so many movies, after watching probablytoys Apt description to me. I do wonder if APPL has managed to wring every dollar out of the market with their inflated price and limited delivery quantity. Kudos to them if they did. reply xvector 15 hours agoprevHuge VR fan here. I tried the demo in the Apple Store. It was cool and all, but nothing game changing. From a company like Apple - somewhat disappointing even. The technology just isn't there yet for a compelling product. Apple should wait until they have this in a glasses form factor before hyping it up any more. reply Animats 14 hours agoparentYes. That's what Carmack said when he quit Oculus. The headgear has to get down to swim goggle size to get any traction, and eyeglass size to go mainstream. Apple got it down from a brick on your head to a half-brick, but that's not good enough. reply D13Fd 14 hours agorootparentIt doesn’t seem like they were even prioritizing size and weight. They made it out of heavy glass and aluminum, and sacrificed size to add the front-facing screen. reply toasterlovin 13 hours agorootparentprevSki goggles seem like existence proof that Vision Pro is only roughly 2x bigger than it needs to be for people to feel comfortable wearing it in public. reply tikkun 14 hours agoprevPeople who own Bigscreen headsets as well as Vision Pro, how do they compare for you? reply dorkwood 15 hours agoprev [–] For all the people laughing at the Vision Pro and saying it's useless, just remember that when the first iPhone came out people were saying the same thing. It's often the second or third generation of a product where it really starts to shine. reply wilsonnb3 14 hours agoparentThere are also plenty of products that just aren’t that good or useful from the beginning and it doesn’t matter how many revisions it gets. Also, Apples version is on its first generation but it’s not like VR as a whole is, “modern” VR has been around for a decade. reply sys_64738 10 hours agoparentprevThe iPhone had a killer feature back then which allowed you to make phone calls. Less needed nowadays I suppose but back then it was key to success. reply TheLoafOfBread 12 hours agoparentprevWe can also remember Siri when it came out. Jaw-dropping demo, but still more less useless after 10 years. reply wombat-man 14 hours agoparentprev [–] I’m happy to wait and see what they come up with. Tempted to get a quest in the meantime. AVP was too heavy, quest a bit too limiting. I’ll say that I was very happy with my G1 instead of an iPhone back in the day. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Vision Pro users have mixed experiences, with some praising its media and work capabilities, while others criticize its high cost and limited functionality.",
      "Key features appreciated include screen size, passthrough, eyesight features, and improved Bluetooth peripheral support, but issues like vision discomfort and limited software integration are noted.",
      "The device's high price point ($3500) and limited release (450k units) have led to a small market, with many users waiting for future revisions or opting for cheaper alternatives like the Quest 3."
    ],
    "points": 141,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1720063824
  },
  {
    "id": 40871783,
    "title": "Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion",
    "originLink": "https://boyuan.space/diffusion-forcing/",
    "originBody": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion Boyuan Chen1, Diego Martí Monsó1*, Yilun Du1, Max Simchowitz1, Russ Tedrake1, Vincent Sitzmann1 * Work done while being a visiting student at MIT. 1MIT Paper Code TL;DR: Diffusion Forcing combines the strength of full-sequence diffusion models and next-token models, acting as either or a mix at sampling time for different applications without retraining. Abstract This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Diffusion Forcing Diffusion Forcing enjoys key strengths of both next-token autoregressive models and full-sequence diffusion models. By training Diffusion Forcing once, one can flexibly control its behavior at sampling time to simultaneously perform flexible and compositional geneation like next-token models, and perform sequence level guidance like full-sequence diffusion models. Diffusion Forcing achieves so by training sequence diffusion but allowing each token to have a different noise level. One can view noises in diffusion as varying levels of masking and establish a unified view: full-sequence diffusion denoise all frames at once with the same noise level, while next-token prediction denoises next frame at a time with zero noise in its past tokens. As a result, one can use different noise levels across a sequence at sampling time to achieve flexible behaviors such as stablizing auto-regressive rollout, guidance over long horizon or planning with causal uncertainty. Video Prediction We provide a list of synthesized videos directly generated by models (without VAE / superresolution). The below results are sampled without cherry-picking. Video Prediction by Diffusion Forcing (ours) and baselines in DMLab dataset (0.25x speed). Teacher forcing easily blows up while causal full-sequence diffusion models suffer from serious consistency issues. Diffusion Forcing can achieve stable and and consistent video prediction. PNG visualizations are provided below to reflect the original quality of generated samples. Video Prediction by Diffusion Forcing (ours) and baselines in Minecraft dataset (0.5x speed). Teacher forcing easily blows up while causal full-sequence diffusion models suffer from serious consistency issues. Diffusion Forcing can achieve stable and and consistent video prediction. PNG visualizations are provided below to reflect the original quality of generated samples. Stablizing Infinite Rollout without Sliding Window In addition, one can rollout much longer videos with our method than the maximum sequence length it's trained on. Remarkly, we can do this without Sliding Window. That is, we rollout RNN without ever resetting the latent z to initial latent z0, showing stablization effect of Diffusion Forcing thanks to its stablization effect. Videos are compressed for loading speed. The results are sampled without cherry-picking. Quality of the video is decreased due to mp4 compression of long videos! We provide PNG visualizations below to reflect original quality of generated samples longer than training horizon. Diffusion Forcing (ours) trained on 36 frames can rollout for 2000 frames or more on DMLab dataset, without sliding window thanks to its stablization effect. Videos are compressed for loading speed. Original dataset resolution is 64x64. Quality of the video is decreased due to mp4 compression of long videos! We provide PNG visualizations below to reflect original quality of generated samples longer than training horizon. Diffusion Forcing (ours) trained on 72 frames rolloutss for 2000 frames or more on Minecraft dataset without blowing up, without sliding window. Original dataset resolution is 128x128. In certain scenarios, the agent will get stuck in front of two block high dirt or stone blocks until it switches direction, which is an instrinsics issue of the dataset collection. Diffusion Planning Similar to prior works like Diffuser, we can use test-time guidance to make our diffusion sequence a planner. However, we explictly model the causal relationship by defining each token as [a_t, o_{t+1}]. By doing so, we have a belief over action to take and the observation it's leading to, but can also update this belief to posterior estimation when new observation is made after the action is taken. Visualization of the diffusion planning process of Diffusion Forcing as a decision-making framework. To model the causal uncertainty of future, diffusion forcing's plan can have near future at lower noise level while having far future at higher noise level. Long Horizon Imitation Learning Many real world tasks are not markovian and requires long horizon memory to accomplish. In our real robot task, a robot arm is asked to swap the slots of two fruits using a third slot. Since the fruits are input in random slots at the beginning, one cannot determine the next steps from a single observation without knowledge of the initial placement of the fruits. We simply remove guidance from the planning experiments and jointly diffuses action-observation sequences to perform feedback control. The above video shows multiple continuous successes before a failure happens. One can observe that the robot is able to accomplish the task even when the fruit location is randomized by the previous run. On the other hand, we tried SOTA imitation learning techniques Diffusion Forcing but it cannot perform the task due to non-markovianess. In addition, diffusion forcing can be prompted to treat incoming observation as noisy ones to be robust to unseen distractions at test time. In the video above, we illustrate our distraction method of randomly throwing a shopping bag into the field of view. BibTeX @misc{chen2024diffusionforcingnexttokenprediction, title={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion}, author={Boyuan Chen and Diego Marti Monso and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann}, year={2024}, eprint={2407.01392}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2407.01392}, }",
    "commentLink": "https://news.ycombinator.com/item?id=40871783",
    "commentBody": "Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion (boyuan.space)133 points by magoghm 16 hours agohidepastfavorite6 comments vessenes 8 hours agoA number of ideas seem notable to me here; first, they are merging the idea of sequence masking (the key training idea for LLMs) with diffusion models; they do this by keeping track of an ‘uncertainty’ level per pixel. This ‘uncertainty’ level is treated as the ‘noise’ level for the diffusion model, (a model which denoises controlled by some sort of embedding). There are a bunch of neat things you can do with this: in particular, you can firm up parts of the image earlier than others, and thus use it for, say maze solving. They even show it controlling a robot arm moving fruit around, which is pretty wild. In a way the title undersells the idea - this is a way to do fractional masking, since the masking level is a float - and I think is really a pretty profound and interesting idea. However, there’s a lot not talked about in this paper; I’d be very curious to see their codebase. How exactly do you set up a maze-following task vs a video extension task? How do you hook up a robot arm to this model, and tell the model what you want done? The architecture itself deserves a significant number of papers / explication. reply bravura 6 hours agoparentThank you for this. It appears to be an exceedingly elegant take on modeling uncertainty in planning and search. There's something quite potent about changing the task to be variable-length, but also forcing the agent to account for its current situation instead of taking it for granted. This allows the agent to react and generalize way better along its path, even in the face of unforeseen challenges. I assume this is set up so that all tasks are treated as variable horizon, and the current state as a consequence of preceding actions. I agree it would be nice to see the code. reply treprinum 3 hours agoprevRuss is doing diffusion now? Must be very applicable to robotics. reply omerhac 1 hour agoprevVery cool, but why is it called diffusion forcing? reply luke-stanley 10 hours agoprevAnyone know of research or tools for using an existing text generating LLM with diffusion like techniques with no new pre-training, or at most, a bit of fine-tuning, such that it works with a small GPT / Phi 3 / Gwen model, for example? I know about Tree of Thoughts with MCTS etc, that are somewhat similar (though often with a different reward learned goal) but I'm interested in something closer to token level generation. Is this possible? reply blovescoffee 12 hours agoprev [–] Am I missing something about training time? Does adding per token noise cause training to slow significantly? Cool paper though! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Diffusion Forcing is a new training paradigm that combines next-token prediction and full-sequence diffusion models, offering flexible generation and sequence-level guidance.",
      "It achieves significant performance improvements in applications like video prediction, stabilizing infinite rollouts, diffusion planning, and long-horizon imitation learning.",
      "This method allows for stable and consistent video predictions, longer rollouts without sliding windows, and robust handling of non-Markovian tasks with long-term memory requirements."
    ],
    "commentSummary": [
      "The paper combines sequence masking, essential for Large Language Models (LLMs), with diffusion models by tracking an 'uncertainty' level per pixel, treated as 'noise' for the diffusion model.",
      "This method is beneficial for tasks like maze solving and controlling a robot arm, as it allows for firming up parts of an image earlier.",
      "The approach models uncertainty in planning and search, enhancing agents' ability to react and generalize, but the paper lacks implementation details and codebase access."
    ],
    "points": 133,
    "commentCount": 6,
    "retryCount": 0,
    "time": 1720058960
  },
  {
    "id": 40872438,
    "title": "Finding near-duplicates with Jaccard similarity and MinHash",
    "originLink": "https://blog.nelhage.com/post/fuzzy-dedup/",
    "originBody": "Finding near-duplicates with Jaccard similarity and MinHash Jul 3, 2024 Suppose we have a large collection of documents, and we wish you identify which documents are approximately the same as each other. For instance, we may have crawled the web over some period of time, and expect to have fetched the “same page” several times, but to see slight differences in metadata, or that we have several revisions of a page following small edits. In this post I want to explore the method of approximate deduplication via Jaccard similarity and the MinHash approximation trick. This is a commonly-used approach to this problem (e.g. the GPT-3 paper describes using it as part of their dataset preparation pipeline), but one I had not encountered until recently, and which I find to be pretty interesting. Excerpt from the GPT-3 paper describing fuzzy deduplication Similarity 🔗︎ Our approach to approximate deduplication will be to define a notion of “similarity” between any two documents, and then to search for pairs where their similarity value is above some threshold. So if we have some universe of possible documents 𝑈 U, we might define a similarity measure between pairs of documents: 𝑆 : 𝑈 × 𝑈 → [ 0 , 1 ] S:U×U→[0,1] and consider two documents “approximate duplicates” if 𝑆 ( 𝐴 , 𝐵 ) ≥ 𝑆 crit S(A,B)≥Scrit. It’s worth noticing that this definition is not in general transitive: we may well have three documents 𝐴 , 𝐵 , 𝐶 A,B,C such that 𝑆 ( 𝐴 , 𝐵 ) ≥ 𝑆 crit S(A,B)≥Scrit and 𝑆 ( 𝐵 , 𝐶 ) ≥ 𝑆 crit S(B,C)≥Scrit but 𝑆 ( 𝐴 , 𝐵 ) < 𝑆 crit S(A,B)<Scrit. That means that “approximately identical” is not an equivalence relation, and is part of the reason that approximate deduplication is trickier to reason about, and to perform at scale, compared to finding exact matches. Jaccard similarity 🔗︎ One measure of similarity widely used across several domains, including large-scale text processing is the Jaccard index, also known as the Jaccard similarity coefficient. The Jaccard index is a function that compares sets, and characterizes the similarity of two finite sets as the ratio of their overlap to the size of their union: 𝐽 ( 𝐴 , 𝐵 ) = ∣ 𝐴 ∩ 𝐵 ∣ ∣ 𝐴 ∪ 𝐵 ∣ J(A,B)=∣A∪B∣∣A∩B∣ I find this calculation makes some intuitive sense: if two sets are similar, they should have mostly the same elements. This means the sets are of similar sizes, and their union is only slightly larger, and their intersection only slightly smaller. If the sets are very different, or of very different sizes, then the union will be large and the intersection small. It has two very natural limit points, which define its range: For two disjoint sets, the numerator ∣ 𝐴 ∩ 𝐵 ∣ ∣A∩B∣ is zero, and the index goes to zero. But if the sets are identical, 𝐴 ∩ 𝐵 = 𝐴 ∪ 𝐵 = 𝐴 = 𝐵 A∩B=A∪B=A=B, and the Jaccard similarity is 1. Note that Jaccard similarity operates on sets, but we’re starting with documents (typically represented as Unicode strings). I’ll return at the end to how we can turn textual documents into sets, but for now I’ll just assume that we’ve done so. I’ll refer to these sets as “feature sets,” where the individual elements are “features” of the document. Scaling Jaccard similarity 🔗︎ We now have a definition of “approximate similarity”: We convert documents into feature sets, and then search for sets with high Jaccard similarity. For very small corpora, we could potentially apply that definition directly. However, considering every pair of documents scales as 𝑂 ( 𝑛 2 ) O(n2) with the size of our corpus, which rapidly becomes infeasible. For finding exact duplicates, we avoid the quadratic cost by hashing; we hash documents and group them by their hash value, which puts identical documents (and, with good probability, only identical documents) into the same hash bucket. We want to find a similar shortcut for approximate duplicates; in the language of the field, we want a locality-sensitive hash. It turns out such techniques exist for Jaccard similarity! Let’s see how it works. Approximating Jaccard similarity 🔗︎ We’ll first consider the problem of approximating the Jaccard similarity between two documents. We’ll find an approximation that avoid examining the entire sets, and which only requires a small, fixed-size “signature,” which we can precompute for each document independently. Then, we’ll use the structure of that signature to find ways to group documents such that (with good probability) similar documents, and mostly-only similar documents, group together. MinHash signatures 🔗︎ Recall that the Jaccard similarity is the ratio of two sizes: the intersection and the union of our two input sets. 𝐽 ( 𝐴 , 𝐵 ) = ∣ 𝐴 ∩ 𝐵 ∣ ∣ 𝐴 ∪ 𝐵 ∣ J(A,B)=∣A∪B∣∣A∩B∣ To estimate a ratio of areas like this, one classic strategy is sampling. If we can generate random elements (uniformly in an appropriate sense), and we can query whether those elements are present in the two sides of the ratio, we can produce an empirical estimate which will approach the true value. In this case, we know the union is at least as large as the intersection, so we want a uniformly-random sample from 𝐴 ∪ 𝐵 A∪B. It’s not clear how to do that given the sets themselves, but it turns out we can do so cheaply if we’re allowed precomputation on each set! First, we make the problem apparently more complicated. Assume that features are integers in some finite range 0 ≤ 𝑓 𝑖 ≤ 𝐹 0≤fi≤F, and then pick a random permutation on 𝑍 𝐹 ZF. Call that permutation 𝑃 ( 𝑥 ) P(x). Now, we can select a random element by selecting the feature in our set that has the smallest value under this permutation1: 𝑥 random ← arg min ⁡ 𝑥 ∈ 𝐴 ∪ 𝐵 𝑃 ( 𝑥 ) xrandom←x∈A∪BargminP(x) It’s not really feasible to work with truly-random permutations, but we can approximate one using a good hash function. This also avoids the need for features to be represented as fixed-range integers; by accepting a miniscule risk of collision and storing only the hash values, we can map any reasonable featurespace into fixed-size hash values: 𝑥 sig ← min ⁡ 𝑥 ∈ 𝐴 ∪ 𝐵 𝐻 ( 𝑥 ) xsig←x∈A∪BminH(x) Next, we’ll exploit the fact that min is associative, and rewrite the above to pre-process each set individually: 𝑎 min ← min ⁡ 𝑥 ∈ 𝐴 𝐻 ( 𝑥 ) 𝑏 min ← min ⁡ 𝑥 ∈ 𝐵 𝐻 ( 𝑥 ) 𝑥 sig ← min ⁡ ( 𝑎 min , 𝑏 min ) aminbminxsig←x∈AminH(x)←x∈BminH(x)←min(amin,bmin) Let’s step back, and consider what we’ve achieved. If we pick a good hash function on features, we can compute a “signature” for each set, individually, consisting of the minimum hash value of all its features. Given any two sets, then, we can take the minimum of those signatures, and we have (the hash of) some element, drawn uniformly-at-random from their union. We want to know whether that element is present in the intersection, or whether it’s misssing from one side. But this construction also makes that trivial! We know that 𝑥 sig xsig is the minimum hash value of any element in either set. Therefore, if it is present in, say, set 𝐴 A, it must also be the minimum hash value in that set. But we know the minimum hash values of each set – that’s precisely what we have! Thus, we don’t actually need to compute 𝑥 sig xsig; we can instead just ask whether 𝑎 min = 𝑏 min amin=bmin! For any two sets, this equality with hold with probability equal to 𝐽 ( 𝐴 , 𝐵 ) J(A,B)! Using more hash functions 🔗︎ That probability is taken over the universe of permutations of 𝑍 𝐹 ZF (aka, with some caveats, “over our choice of hash function”). With a single hash function and a single min-hash, we only have a boolean estimate for each pair – “equal” or “not equal.” We can improve on that by instead selecting 𝑘 k different hash functions from some appropriate hash family, and summarizing each document into a 𝑘 k-element vector: 𝐴 sig = ( min ⁡ 𝑥 ∈ 𝐴 𝐻 1 ( 𝑥 ) min ⁡ 𝑥 ∈ 𝐴 𝐻 2 ( 𝑥 ) ⋯ min ⁡ 𝑥 ∈ 𝐴 𝐻 𝑘 ( 𝑥 ) ) Asig=(x∈AminH1(x)x∈AminH2(x)⋯x∈AminHk(x)) Given two of these signatures, we can approximate the Jaccard similarity by counting how many hashes match: 𝐽 ( 𝐴 , 𝐵 ) ≈ 1 𝑘 ∑ 𝑖 = 1 𝑘 ( 𝐴 sig [ 𝑖 ] = 𝐵 sig [ 𝑖 ] ) J(A,B)≈k1i=1∑k(Asig[i]=Bsig[i]) One caveat to mention: the choice of the hash family function here is a bit subtle. We are attempting to approximate a random permutation over the universe of features, but the number of such permutations grows extremely quickly, and so our hash family will represent a tiny fraction of all possible permutations. We need to be sure that members of our hash family are not inappropriately correlated – formally, the salient property here is referred to as “min-wise independence”. Fortunately, this problem is reasonably well-studied, and efficient solutions are available in the literature. Comparing all documents 🔗︎ We’ve now condensed each document into a 𝑘 k-element fingerprint of hash values, which allows efficient approximation of Jaccard similarities. The next problem is to find approximate duplicates throughout our entire corpus – documents with a high similarity – without considering every pair of documents. As alluded to above, our strategy will be to define some set of keys that we can group documents by, and then only perform the full comparison within each group. We will aim to construct the grouping key so that similar documents group together with good probability, and dissimilar ones do not. Using the full signature 🔗︎ The simplest choice is to simply to use all 𝑘 k MinHash values together as a grouping key, and consider two documents “approximate duplicates” iff all of their MinHash values match. I’m pretty sure is what the GPT-3 paper cited above means when they say “we fuzzily deduplicated documents […] using Spark’s MinHashLSH implementation with 10 hashes.” They split each document into features, computed 10 MinHash values for each document (using 10 different hashes2), and then grouped documents by that 10-vector, and kept only one document per group. The strongest virtue of this approach is its simplicity and efficiency. Grouping documents by a single high-cardinality bytestring is an efficient operation and easy to scale horizontally, and is offered as a basic primitive in essentially any data-processing toolkit (it’s arguably the core primitive in MapReduce, taking the form of the “shuffle” between the map and reduce stages). How does this approach behave? For a single pair of documents, we expect each MinHash value to be equal with probability 𝐽 ( 𝐴 , 𝐵 ) J(A,B), so we expect all 10 to match with 𝑝 = 𝐽 ( 𝐴 , 𝐵 ) 𝑘 p=J(A,B)k. For 𝑘 = 10 k=10, here’s what that looks like: As well as some quantiles: p(all match) 1% 10% 25% 50% 75% 90% Jaccard 0.63 0.79 0.87 0.93 0.97 0.99 We can see that documents with similarities below 0.6 or so will almost-never collide, and that the odds of matches become large around 0.95 or so. If we’re primarily concerned about documents that are very close siblings, this approach may be sufficient. And in fact I suspect – but haven’t verified – that in many corpora, we will encounter fairly bimodal Jaccard values – two clusters, near 1 and 0. Unrelated documents have similarity close to 0, and similar documents will largely be “nearly-identical” – e.g. two slight revisions of an article, or two copies of the same context with different timestamps or metadata. It’s also worth noting that the 𝐽 𝑘 Jk calculation holds for a single pair of documents. If we have many documents that are all similar, the pairwise probabilities are not at all independent. In practice, given many very-similar documents, they’re likely to end up hashed into at-most two or three buckets, and so we will find “almost all” of the duplication, in some sense. Going fuzzier 🔗︎ (Note: this discussion is primarily sourced from “Mining of Massive Datasets” section 3.4. I haven’t played with this strategy, and don’t know whether or when it’s used in practice, so far). What if we want to detect “fuzzier” duplicates? Perhaps after some empirical study, we determine that we want to find pairs with similarities above 0.8 or 0.7, instead of only “near 1.” By using a subset of our 𝑘 k MinHash hashes as a grouping key, we can increase the likelihood of collisions at lower similarity values, and then compare the full signatures within each bucket to weed out false collisions. For instance, we might group by the first 4 MinHash values, and then – within each colliding group – use all of our MinHash values to estimate the trule similariy. Using fewer hashes is helpful, but only so far; 𝐽 𝑟 Jr will always be smaller than 𝐽 J, and if we push 𝑟 r too small, the rate of spurious matches will become unacceptable. What we can do instead is to generate multiple keys per document, and place each document into several buckets, one per key, using a different subset of MinHashes for each key. If we compute 𝑘 = 20 k=20 hashes as our signature, we might place each document into 𝑏 = 4 b=4 different buckets, using 𝑟 = 5 r=5 hashes to construct each key, and then compare each pair within each bucket. What are the odds that two documents end up hashed together in at least one bucket? The odds that two documents collide using a single key is 𝐽 𝑟 Jr So the odds they don’t collide based on that key is 1 − 𝐽 𝑟 1−Jr So the odds that don’t collide in any of the buckets is 1 − 𝐽 𝑟 ) 𝑏 1−Jr)b Thus, the odds that they collide at-least-once ends up as: 𝑝 = 1 − ( 1 − 𝐽 𝑟 ) 𝑏 p=1−(1−Jr)b For example, the above example – using 4 groups of 5 hashes – produces a probability curve like this: The curve is not as steep as our earlier example, but we’ve successfully shifted it to the left – the odds of colliding become 50% at somewhere around 𝐽 = 0.7 J=0.7. It turns out, that for any choice of 𝑟 r and 𝑏 b greater than 1, the resulting curve is S-shaped in a broadly similar fashion, and so varying those values gives us a rich tradeoff space over sensitivity, recall, and performance costs. Closing thoughts 🔗︎ Prior to working in AI, I believed myself to be relatively familiar with common algorithmic tricks, including most common sketch algorithms. But I had somehow never encountered MinHash, or even been aware that such algorithms (locality-sensitive hashing) existed or were practical! I really enjoyed learning how this trick works and digging in. I hope this blog post introduces it to some more engineers for the first time, and/or that it helps fill in the gaps in someone’s understanding. I just love neat mathematical/algorithmic tricks! Postscript: MinHash and HyperLogLog 🔗︎ While researching and writing this post, I realized that the core MinHash trick reminded me a bit of a classic, somewhat famous, sketch: HyperLogLog. The core idea in HyperLogLog (going back to a much older algorithm) is to hash each element of a stream, and store a running maximum of “the number of leading zeros” in the resulting stream of hashes. The algorithm is very different in the details, but there’s a clear similarity: In both cases, we use a hash function to map input elements into a uniform distribution, and then we compute a running extremum, which – with appropriate calculation – allows us to estimate some distributional property using only a constant-size summary of our input. Indeed, I contend the algorithms are even more similar than they first appear: HyperLogLog counts leading zeros (zeros in the least-significant digits). However, we’re assuming our hash function outputs uniform values in [ 0 , 2 𝑁 ) [0,2N), and so we may as well reverse the bit order and count the number of zeros in the most-significant position. But given an N-bit number 𝑥 x, the number of high-bit zeros is closely related to log ⁡ 2 ( 𝑥 ) log2(x) – the more leading zeros, the smaller 𝑥 x. Thus, we can just as well think of think of HyperLogLog as computing a running minimum of 𝑙 𝑜 𝑔 2 ( 𝐻 ( 𝑥 ) ) log2(H(x)), as compared to MinHash’s 𝐻 ( 𝑥 ) H(x). In addition, it turns out that there’s a sense in which HyperLogLog and MinHash are (somewhat) dual: Given two HyperLogLog structures for two different sets, we can combine them, and estimate the size of their union. Given two MinHash structures for those sets, we can compare them and estimate the (relative) size of their intersection. Thus, if you combine both structures, you can produce a sketch that lets you ask questions about both intersections and unions of arbitrary sets! This idea was noticed at least by 2013, and it turns out there’s an ongoing literature of sketches that combine ideas from the two data structures in interesting ways. I think that’s neat! Appendix: Representing documents as sets 🔗︎ I promised I’d return to the question of representing documents as sets, so I’ll also leave a brief note on two common approaches. First, before applying either of these strategies, we may want to normalize documents in some way. For instance, we likely want to convert to a standard Unicode normalization form, and we may also wish to case-fold, collapse runs of whitespace, or perform similar transformations. n-grams aka “shingles” 🔗︎ We can represent a document as a set of all the n-grams that appear in the document, picking some appropriate value of n. In the field of large-scale text processing, often the literature uses the word “shingle” instead of “n-gram,” but I find that needless confusing. We can pick any value of n, with the primary tradeoff that smaller values will tend to compare documents more coarsely (e.g. most English text probably looks fairly similar through the lens of bigrams), and larger ones generating more distinct features and thus larger sets. At some limit I expect you also lose sensitivity, but I suspect performance problems arise earlier. According to one source I’ve found3, values of n between 5 and 9 appear to be common choices for a range of applications. Word-splitting 🔗︎ We can instead attempt to split the input into “words” or “tokens,” and use those as our features. The except from GPT-3 paper above mentions “Spark’s standard tokenizer,” which I believe refers to this class, which simply lowercases the input and then splits on whitespace. We could use a more sophisticated tokenizer, or we could hybridize the approaches by tokenizing and then using n-grams of tokens. In that case we would use a smaller value of n, since individual tokens should much be higher-entropy than bytes or characters. If you’ve ever written SELECT ... FROM table ORDER by random() LIMIT 1, you’ve used a similar trick! ↩︎ If you’re curious, Spark documents its choice of hash family, along with the relevant literature reference. ↩︎ Mining of Massive Datasets §3.2.2 ↩︎ Subscribe to my newsletter (blog updates and occasional other content): Powered by Buttondown. « Stripe's monorepo developer environment",
    "commentLink": "https://news.ycombinator.com/item?id=40872438",
    "commentBody": "Finding near-duplicates with Jaccard similarity and MinHash (nelhage.com)127 points by brianyu8 13 hours agohidepastfavorite16 comments tpoacher 5 hours agoOne thing many people miss about set based metrics like the jaccard similarity (aka Tanimoto coefficient) and F1 score (aka Dice coefficient), is that they can also be used identically with fuzzy sets. The only complication is that you then need to choose a suitable T-Norm / T-Conorm pair, which express the concept of intersection and union for fuzzy sets, and there's an infinite family of them. But that's a good thing, since you get to pick the pair with your desired semantics. I wrote about this ([0][1]) in the context of validating medical image segmentations when both the segmentation and ground truth are probabilistic/fuzzy rather than binary masks. Otherwise what most people do instead is to simply threshold at 0.5 to obtain binary sets for use with the binary variants of the jaccard / dice coefficients. Which apparently decreases the precision of your validation operator by 2 orders of magnitude. It's like, you publish your algorithm claiming it's better than SOTA by 0.001, ignoring the fact that your validation operator has an error margin of 0.1 ... [0] https://link.springer.com/chapter/10.1007/978-3-319-46723-8_... [1] https://ora.ox.ac.uk/objects/uuid:dc352697-c804-4257-8aec-08... reply BiteCode_dev 10 hours agoprevI worked with a client that implemented their own Python version of this to deduplicate citizen entries in a big french gov database. It worked well. Of course, nowaday I would probably just tell them to use datasketch (https://pypi.org/project/datasketch/). With this trip to memory lane, I looked around a little, and noticed people are still creating new stuff on the topic. E.G: https://pypi.org/project/rensa/ Which is basically a more specialized but faster version of datasketch minhash, written in rust, with a little python on top. reply RobinL 7 hours agoparentFor deduplicating people, the Fellegi Sunter model is also a powerful approach. Splink[0] is a free Python library that implements this for big datasets. Probably you could combine parts of both approaches as well. Full disclosure: I am the lead author. I've also written up some interactive tutorials on how the method works [1] if anyone's interested [0]https://github.com/moj-analytical-services/splink [1]https://www.robinlinacre.com/intro_to_probabilistic_linkage/ reply BiteCode_dev 7 hours agorootparentInteresting. What would you say are the main differences with other approaches? reply RobinL 7 hours agorootparentThe Fellegi Sunter model is able to estimate the importance of different types of information from the data itself (i.e. unsupervised learning). For instance, a match on a date of birth column lends a greater weight of evidence in favour of a match than a match on first name (since dob has higher cardinality). The method is also able to estimate weights for fuzzy matches (how much evidence in favour of a match is close match on dob with one character difference), and also how much evidence against a match a mismatch is. For instance, if you have very high data quality on gender, then a match on gender doesn't tell you much, but a mismatch on gender is quite strong evidence against the idea two records match. I have a blog post here that delves into this a bit more: https://www.robinlinacre.com/fellegi_sunter_accuracy/ reply BiteCode_dev 6 hours agorootparentOk, so you get better accuracy if you datasets have obviously weighted fields. Do you pay that in perfs, and if yes how much? reply RobinL 6 hours agorootparentThe performance is pretty good because the prediction is ultimately just adding up match weights. Much of the performance is dictated by (1) The blocking approach you choose (how wide you cast the net in searching for matches. This is actually somewhere were minhash can be used in conjunction (2) whether you choose to use complex fuzzy matching functions and how many - this is chosen by the user. There's some benchmarking results here: https://www.robinlinacre.com/fast_deduplication/ Overall it's an approach which makes a pretty good trade off between speed and accuracy. That's why it's used by many national stats institutes (UK, US, Aus, Germany etc.) - because it's capable of working on country-population sized datasets. reply molodec 2 hours agoparentprevThere is also gaoya ( I am the author ), which is also written in Rust and has python bindings. datasketch is great, but it is not performant enough for my use case. gaoya is used in a production system for large scale clustering https://github.com/serega/gaoya reply pkeenan11 6 hours agoprevHoly synchronicity Batman! I just implemented a minhash system that some may find interesting. The problem is to find (pseudo) inverses of many different proper submatrices of a big square matrix. Certain matrix identities (Woodbury, banachiewicz) allow you to update the inverse of a “close” submatrix to cheaply calculate a new one. So you store the inverses you’ve calculated already, with their row/column indices as keys. Then for each new submatrix you want to find a close already-computed inverse to update from. This is solved with minhashing. You minwise hash the indices so that close matrices are likely to have the same hash. In my particular implementation I did a “multi-resolution” hash so that I can change the selectiveness of the search as the number of prior computed inverses grows reply vivzkestrel 7 hours agoprevI have this problem right now in postgres. I have 600000 feed_items with the schema (feed_item_id uuid, author varchar, content text, guid varchar, link varchar, title varchar, summary text, feed_id integer) The content and summary columns especially for some of the news items are highly similar but not equal. For any given 2 such news items, I am trying to cut them down to 1. Any ideas? reply kordlessagain 11 minutes agoparentHave an LLM generate a reverse index for the entries, but force it to keep the cardinality low. Then you can use a Jaccard similarity. reply dleeftink 5 hours agoparentprevI've implemented a minhash-like system in Bigquery, and was able to calculate cosine similarities (within a certain similarity threshold) between all Stack Overflow in reasonable time. To give you a broad idea of the procedure: 1. Concatenate and split all text fields into an array of ngrams (2...n chars) 2. Declare two global arrays A & B of length-n and fill them with random 32-64 bit integers 3. Hash each ngram to a 32-64 bit integer, multiply the resulting hash by each random value in array A, and modulo the resulting output with each random value in array B. Take the minimum value. Per row, your aim is to end up with an array of 'minhashed' integers of equal length to the arrays in step 2. If you declare the global arrays to be length of 64, the minhashed array for each row will also be of length 64. 4. Bucketise the resulting hash arrays by summing N consecutive minhash values using a window function (e.g. sum each 4 consecutive rows) If all went well, you can now unnest these arrays (call them 'source rows') and self join the dataset on each bucketed minhash value (resulting in an additional column of 'target rows'). Group by source, target columns and count the occurances to get an indication of how likely two rows are similar. In essence, the more often two items hash to a similar bucket, the more similar they are, and it's up to you to define the cut-off from where to calculate actual pairwise Jaccard (or cosine) similarities between items. reply RobinL 7 hours agoparentprevOne useful technique here could be to use text embeddings and cosine similarity: https://simonwillison.net/2023/Oct/23/embeddings/ reply swasheck 4 hours agorootparentlove this and have been using tf/idf for embeddings and various measures of similarity for some personal pet projects. one thing i came across in my research is that cosine similarity was more useful for vectors of different lengths and that euclidean distance was useful for vectors of similar length but simon alludes to a same-length requirement. i’m not formally trained in this area so i was hoping someone could shed some light on this for me. reply probably_wrong 4 hours agoparentprevIf you think two items cover the same highly similar _keywords_ then Jaccard distance should work. If you think two items share highly similar _text_ then give Levenshtein distance a try. reply is_true 7 hours agoprev [–] I had to do this with sport teams but I used levenshtein for the names. I ended up creating a vector with other data (country, gender) and using that vector to calculate the distance (similarity). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jaccard similarity and MinHash are used to identify approximately similar documents in large text collections, such as those used in GPT-3 dataset preparation.",
      "MinHash approximates Jaccard similarity by hashing document features and using the minimum hash value as a signature, allowing efficient comparison of large corpora.",
      "This method is scalable and can be combined with other techniques like HyperLogLog, making it suitable for large-scale text processing applications."
    ],
    "commentSummary": [
      "The post discusses using Jaccard similarity and MinHash for finding near-duplicate data, highlighting their application in various fields such as medical image segmentation and database deduplication.",
      "Several tools and libraries are mentioned for deduplication tasks, including datasketch, rensa, Splink, and gaoya, with insights into their performance and use cases.",
      "The Fellegi Sunter model is noted for its effectiveness in deduplicating people by assigning weights to fuzzy matches and mismatches, improving accuracy in large datasets."
    ],
    "points": 127,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1720069449
  },
  {
    "id": 40873001,
    "title": "Region-specific Machines pricing",
    "originLink": "https://community.fly.io/t/fresh-produce-region-specific-machines-pricing/20690",
    "originBody": "jfentRegular 20h Starting on July 1st, we’re breaking down pricing for Machines, including additional RAM, by region. For as long as we’ve offered Machines in multiple regions, we’ve had a flat price. If you gave us $0.00000075, we’d run you a shared-1x-256mb Machine for one second in São Paolo, London, Tokyo, you name it. When we launched Machines, we opted for one consistent price across all regions. This was mostly due to billing system constraints (our billing system has been hot garbage for most of the life of the company). The underlying infrastructure costs vary wildly between regions, though. Taxes, energy cost, suspicious “fees” to get servers through customs. All kinds of weirdo variables. Did you know it costs us twice as much to rack a server in São Paulo? When we were starting out, it made sense to normalize pricing across all our regions. But at the scale we’re at now, it doesn’t. We’d rather be transparent and give you control about where to place workloads than raise prices everywhere to account for colo costs in Mumbai. So we are implementing per region machine pricing. Which means the price for machines in many regions is going up. We’re going to ease y’all into this over the next four months. Your July invoice will have per region line items, but no pricing changes. In August, we’ll start changing prices. Specifically, we’ll adjust the price in each region by 25% of the difference between the current price and the final price for that region. In September, we’ll adjust by a further 25%, and so on until November, when the prices will settle at their final price. If you’re finding that confusing to follow, there’s a worked example here 537 in the pricing docs. While we’re on the topic, we’ve updated the pricing docs so you can now select a region and the Started Fly Machines pricing table will update itself to show the region-specific prices. Finishing out with a quick housekeeping notice. Some of you may have seen that your Machines Shared CPU 1x usage was not being paid off by your Free Machines Allowance credit. This was a bug that we’ve tracked down and fixed. We’re having to reissue everyones credits to fully rectify, which may take anywhere from several hours to a day or so - bear with us on that! 4 created 20h last reply 3h 12 replies 6.2k views 8 users 19 likes 4 links 3 3 2",
    "commentLink": "https://news.ycombinator.com/item?id=40873001",
    "commentBody": "Region-specific Machines pricing (fly.io)94 points by bishopsmother 11 hours agohidepastfavorite109 comments brigadier132 6 hours agoThis is painful, I was building for fly.io because of their dynamic request routing but I'm thinking I'm just going to go with hetzner. The cost difference is so substantial. If I want high availability I can just get 5 servers and have that much more compute available and I'd still end up paying less. reply tptacek 41 minutes agoparentWe're sorry. I hope we're clearly expressing what's happening here: either we do flat pricing globally, so that people deploying in cheaper regions subsidize those deploying in more expensive ones, or we reflect our cost basis in our prices and let our customers (and prospective customers, like you) make their own choices. In your specific situation: it really just is the case that Brazil soaks us on import fees. Getting machines in racks in Brazil is just crazy expensive. We're going to keep doing it! Brazil has too many good sandwiches. But the idea behind building a new public cloud on our own hardware is for us to be around for the long haul, and flat low global pricing is not a \"long haul\" decision. (It wasn't a long haul decision last year, either, but building a billing system capable of expressing this stuff is a nightmare from which we have not yet fully awoken. It turned out to be a shockingly hard problem.) There are going to be cases where places like Hetzner make a lot sense compared to us. We hope you get your thing launched and find success wherever you end up. Some of our stuff, like LiteFS, you can take with you! :) reply danpalmer 6 hours agoparentprevThis same argument applies to most cloud hosting, non-cloud (or less cloudy, Hetzner is a little cloudy) will always be cheaper because the service is more basic in a number of ways. Fly’s pitch is not just on demand scalable compute, but also that compute being close to the edge, and with “ops-less” deployments. All of those are factors you pay for. You could pay a little less with a big cloud provider’s serverless platform, a bit less again with regular cloud VMs, a lot less with colo hardware, and a ton less running your own DC (at scale). Obviously not all of these work for all businesses, but saying Hetzner is cheaper than Fly misses the point of those two services on the spectrum of service levels. reply jstummbillig 6 hours agorootparent> will always be cheaper This has me so confused. Should the cloud abstractions not clearly make the service more optimizable at scale and thus cheaper to offer? Sandboxing things, limiting options, not having to deal with users wanting to provision VMs and do all kinds of unexpected stuff? The marginal cost of the beautiful software to make all of that happen smoothly, and all the dx it affords, that will just go towards 0, no? I understand that people ask for what they can get away with. But does the market fail or am I missing a piece of the cost/price puzzle? reply cle 1 hour agorootparentNo it won't reach 0, for many reasons. - Things change and break. You are paying for people to fix that for you at 2 AM, and to keep up with industry developments and best practices and make those decisions for you. - Legal risk and compliance requirements change. You are paying for them to keep up with those for you. - Overhead imposed by multi-tenancy. Security boundaries, control planes, etc. These are fixed operational costs caused by the abstraction. You're not paying for just an abstraction, you're paying for the things the abstraction enables. It's also not always what you need. But it often makes good business sense. reply tptacek 54 minutes agorootparentprevThe sandboxing (by way of example) is a hardware affordance, not a software feature. In order to hardware virtualize every instance running on Fly.io, we have to provision hardware globally; we can't run customer jobs in namespaced userland containers under a shared kernel on cloud-provisioned hosts. reply filleokus 5 hours agorootparentprevInteresting take. At one side, the big cloud providers have their very fair profit margins. But I've always assumed that comes mostly from that they can, and do, charge extra for the beautiful software (not seldomly through ridiculous egress bandwidth pricing). I'm skeptical of how much flexibility and optimisation the big cloud providers really get compared to a very large VPS provider? All the compute products (different form of container hosting, serverless) are essentially still priced at CPU+RAM + a premium. Some additional abstraction for nouveau databases (\"capacity\" or \"billing\" units). Many of the highly abstracted offerings also have the scale-to-zero concept, while the provider still have to have physical capacity ready (to some extent). Sure, you can probably crank out some percentage more utilisation, but that doesn't really matter for the 100+% price-add on the market bears. But I of course have no data to back up my claims, just speculation. reply arccy 5 hours agorootparentprevThe tooling for managing the \"lower\" layers is progressively worse / slower / harder the closer to the metal you get, so you usually are paying a heavy premium for convenience / DX. Only very specific serverless products where the provider can bin-pack you with other tenants allow them to possibly offer lower prices. reply candiddevmike 6 hours agorootparentprevFor a lot of people (most...?), Hetzner gives them exactly what they're looking for at an excellent price point. As an ecosystem, we never really moved past VMs, we just went up the stack more. reply brigadier132 6 hours agorootparentprevI'm not missing the point of the service, I'm against the idea of infrastructure being an 84% margin business. But I guess this is what the VCs demand and some are willing to pay it. reply hipadev23 5 hours agoparentprevDon’t host anything important on Hetzner, as they’re cheap they get a lot of questionable users, and their systems are tuned to null route anything that looks remotely suspicious or high-volume to their hair-trigger network monitoring and DDoS protection. And then you’re at the mercy of whatever support agent decides to look at your ticket and that can take days. Use literally anyone but Hetzner is my advice. reply js4ever 5 hours agorootparentI disagree, at my company (elest.io) we have several thousands VMs with Hetzner and reliability is very comparable to AWS from our internal stats. In case of DDOS attack we get notified and can discuss with their team. Also support is competent and quick. We usually get an answer in few hours most of the time. reply hipadev23 4 hours agorootparentI’m glad you’ve had a positive experience, maybe it’s the usage level that grants you that treatment. As a one-time new client I had the worst possible experience with them: null routed my game server on launch day, very slow to respond support team, and effectively held my server hostage for 3 days. I guess I needed to inform them I was intending to use the server I paid for? Took my data and never returned. reply scandox 2 hours agorootparentprevYears of excellent uptime with Hetzner here. Almost decades. reply itake 3 hours agorootparentprevthere is no way Hetzner is worse than fly for uptime. Fly has deleted my db, taken servers offline for hours, and expired tls certs for hours without telling me. reply hipadev23 2 hours agorootparentAbsolutely not defending fly.io. OP is migrating off fly and in my personal experience, Hetzner is not a good option. Maybe OVH, AWS, GCP, Azure, Render, Digital Ocean, or linode. reply itake 1 hour agorootparentI haven't heard bad things about DO in a long time, but a few years ago they deleted customer data in their cloud storage solution. reply throwAGIway 5 hours agorootparentprevI disagree, the support is very responsive. Try to let them know in advance about your plans and you will be fine. reply theallan 2 hours agorootparentprevAdding another one to say that I've only have a positive experience with Hetzner so far. 6 months in, and 4 machines. All doing fine. reply hstaab 2 hours agorootparentprevHetzner has been fantastic for us for many reasons. I’m not convinced the problems you ran into are Hetzner specific. reply hipadev23 2 minutes agorootparentHow is it you can have a good experience, and yet my negative experience somehow doesn't qualify and you're insinuating the issue is with me, and not Hetzner? https://www.trustpilot.com/review/hetzner.comIf I want high availability I can just get 5 servers and have that much more compute available and I'd still end up paying less. Please give us an update here when you are done building your hetzner beowulf cluster and let us know how it fares. I think you are grossly underestimating the system you describe. reply zamadatix 5 hours agorootparentWhether this is completely out of the box or a multi-year engineering project to get equivalent levels of utility in the long term is completely up to how one specifically uses Fly.io. E.g. a load balancer in front of some relatively stateless http microservices is really not going to tie you to Fly.io because of complexity. A globally distributed edge compute environment with database, RPC, varying scaling patterns, and environment certifications is probably not going to have the value prop impacted by this kind of minor pricing change though. reply whalesalad 4 hours agorootparentTrue, but managing 5 servers requires overhead. Where does the load balancer sit? What happens when it goes down? What if the entire datacenter loses comms? That is why cloud providers have AZ's that are physically separated from one another at different sites. When your boxes are spread across different physical networks, how do you connect them securely? For true HA you need a system that can self heal. Just throwing servers at the problem does not make things better. Reminds me of the expression 9 women can't make a baby in 1 month. None of this is impossible, but when you are a small startup it makes sense to offload this to someone else and not waste time reinventing the wheel. reply ffsm8 3 hours agorootparentFor the record, hetzner cloud also has multiple data centers (at least in Europe, haven't checked for US) with sharedand load balancers . What you don't get is managed services like k8s, databases, sqs, lambda/faas. These are generally easy to setup via preconfigured Terraform or ansible scripts, but if something goes wrong, you'll have to look at the logs yourself - that's true. (But that's ignoring that such a small team as you're proposing would likely be better served with a more mundane setup, avoiding all of these extra points of failure you'll have to manage... Making a simple bare metal setup once again the less involved and better performing alternative) reply everfrustrated 1 hour agorootparentHetzner load balancers don't have any auto scaling feature which is a pretty critical requirement for any load balancer product in 2024. reply ffsm8 1 hour agorootparentIm pretty sure you're vastly underestimating the amount of performance you get from today's hardware. You won't be a small team by the time you're at a scale to need horizontal scaling. Only the biggest consumer facing/social media sites get value from that. reply whalesalad 1 hour agorootparentIf you cannot tolerate downtime you need a redundant load balancer and you need nodes in different datacenters. Scale really doesn't matter in this equation. Now maybe you don't need five-nine's so then the risk is worth keeping it simple. reply ffsm8 1 hour agorootparentBoth of which hetzner has? Did you skip the previous comment entirely? reply whalesalad 1 hour agorootparentI read it. Historically Hetzner has been popular in the dedicated server space. I cannot speak to their IAAS/cloud offerings, I have never used them. I will take them for a spin, though, seeing as it is really the only way to use their services in the US with good latency. reply brigadier132 5 hours agorootparentprevI don't want a cluster, I just want a load balancer that can connect users to the machine I want. reply iampims 3 hours agorootparentIf the LB is for HTTP traffic, check out Cloudflare’s offering. reply yjftsjthsd-h 2 hours agorootparentprevHi, professional sysadmin here. I've run production deployments across hundreds of machines in Hetzner; it scales fine. The only real problem is that Hetzner is so much cheaper than anything else that it's easy to get a little bit stuck. (Honestly, that was a fun job in a lot of ways. Oh well.) reply whalesalad 1 hour agorootparentNever said it couldn't be done. I forgot that they have ventured into the iaas space and are doing more than just dedicated boxes now. For a small startup with a simple app, managing five dedicated nodes and building your own orchestration on top of deployments, load balancing, networking etc feels like the juice ain't worth the squeeze. For their cloud offering, it is more compelling. reply lilouartz 1 hour agoprevVery happy with Fly.io. My website Pillser runs on it and I never had issues with them. The cost per month us just under USD 100 for hosting 3 services, including the database. reply miyuru 7 hours agoprev> Did you know it costs us twice as much to rack a server in São Paulo? There is an interesting dynamic of server prices and customer spend in different parts of the world. Usually where the server prices are high, customers spend less. reply vasco 7 hours agoparentThere's also physical limits though, and structural economic lag in terms of investment. If you look at any CDN costs globally and compare it with the number of intercontinental fiber connections between those places you generally start having a good idea that the two are related. In some places there's also just less qualified talent (due to having less ability to train people in enough universities, or just not having them for as long) and datacenter space built and extra import costs that delay everything. At some point everything will saturate and probably energy production costs will dominate (largely for cooling), but I think we're still a bit far from that. reply felipeac 6 hours agoparentprevIn the case of Brazil it's mainly due to our absurd import taxes. reply Xunjin 5 hours agorootparentDo you have numbers to support it? If you mean import of hardware that do makes sense, but what about hardware that's already here? (I'm also Brazilian) Looking at AWS site it explains that PIS/COFINS and ISS are charged, excerpt below: \"AWS SBL can issue an NFS-e to a Bra zilian customer only after receiving a valid Tax Registration Number (\"TRN\") (i.e., CPF/CNPJ). The relevant taxes applicable to AWS cloud services offered locally are the following: PIS and COFINS (9.25%, combined rate) and ISS (2.9%, charged by São Paulo municipality).\" https://aws.amazon.com/tax-help/Brazil/ This is below 15% which many countries do tax for cloud services. I'm not saying that Brazilian taxes are really high, they are for hardware imports, but the taxes into the service itself does seem \"fair\". reply definitelyauser 4 hours agorootparent> but what about hardware that's already here? (I'm also Brazilian) It's ridiculously expensive. Very little hardware is produced domestically, and the little that is, you don't want to be running your switches on what is in essence a no-name \"intelbras\" brand. reply tptacek 52 minutes agorootparentprevIf you can help us drastically reduce our Brazilian hosting costs, we're all ears. :) reply thedangler 5 hours agoparentprevGreat!, then I'd rather my app be a littler slower for those people if it saves me money. If the latency is not noticeable, why pay more to have that edge hosted. reply hypeatei 7 hours agoprevRecently evaluated Fly.io for a company project but decided against it. I get the vibe that it fits the hobby/side project market more and that they're still in startup mode. reply vdfs 6 hours agoparentThey did remove the hobby plan too reply gitinit 6 hours agorootparentIt can still be used for free as from my experience and the docs, monthly bills under $5 are free. reply itake 2 hours agorootparentCan you link to the docs you’re talking about? The docs I see say you need to have a legacy account. reply x0x0 2 hours agoparentprevfwiw, we deploy on fly and are happy with it. The reason to like fly is you get a 20-ish line yaml file that replaces (and I'm not exaggerating) thousands of lines of cloudformation. It automagically sets up the equivalent of a load balancer, autoscaling group, VPCs, db, vpn, etc. And it's trivial to replicate that environment per employee. Plus the autoscaling using their firecracker stuff is fast. I'd recommend most companies who aren't using tons of aws services take a hard look at fly. reply siliconc0w 2 hours agoprevFly is the only place I can get a lambda-like application with an attached SSD. Tigris is cool too (s3-alike but faster for our use case). FWIW, I haven't had too many reliability concerns but I have had some issues with builder VMs (can always build locally so no big deal) or non-prod environments with just one instance. reply zackify 7 hours agoprevPretty funny that I built an app in Brazil and then this happens right before it goes live. It would be nice to have stable pricing and not have a fear of different features costing more out of the blue. Other than this, fly has been a great service, I love the system, the CLI and litefs. It scares me that internal bandwidth charges will start being added, the application I’m using has a lot of internal transfer which is currently free. reply estebarb 7 hours agoparentInternal bandwidth price is free? I decided to avoid Fly.io because the pricing page suggests it is not: \"Any traffic exiting a Fly.io Machine is considered outbound data transfer, including: - traffic destined for the internet - traffic destined for other Machines or apps in your network\" reply zackify 6 hours agorootparentI have many edge nodes with data being synced to them internally with liteFS. This is free Edit: maybe you are right actually. I have to check on this closer. App is just about to launch so I hadn’t paid so much attention reply davedx 7 hours agoparentprevThe nice thing about Fly is it’s very docker oriented, so depending on your app migrating it off may be fairly straightforward? My apps are all on Fly but in the regions without big hikes thankfully, so I’m sticking around for now, but it sucks some people are being hit by this… reply zackify 7 hours agorootparentTechnically you are right. However my app is using litefs. In theory you could set this up on your own servers. Which is part of the reason I was so interested in it. However with fly it’s just one command to add a new region and my data is auto replicated there. That’s super nice compared to manually managing this. As long as bandwidth costs do not go up drastically I think it’ll be okay. reply bvrlt 6 hours agoprevFly.io and Render.com are alternatives to Heroku. How do the two compare in terms of maturity? Can they both be trusted with production apps? reply MobileVet 5 hours agoparentWe looked at both and chose Render based on various posts here along with our real world testing and bake off parameters. We use the Pro Ultra config and handle ~20rps on a Node / Express stack w/ 36 workers using clustering. Our bandwidth is around 50Gb / month. Render has been quite solid and the support has been on point when we have found issues or run into unexpected edge cases. I have been impressed that despite a big raise and subsequent scaling up, they continue to ship a solid product and the platform improvements have been useful as well. reply calyhre 5 hours agoparentprevNever used Fly.io but I have a website running on Render (2.4M unique users per month) and the service is getting quite nice. Occasional hiccups, but they are improving. Not as stable as Heroku used to be 7-8 years ago yet, but it's also waaaaay cheaper. I really like their Docker support and infra as code, makes it very easy to spin up a whole thing while not being too far conceptually from Kubernetes for example. reply smallerfish 2 hours agoparentprevI have services running on each. Fly's more flexible. Render's further from the metal, and thus somewhat more limited. Both have some reliability issues and rough edges. My new stuff is mostly on Fly, but I'd use Render where I had a junior team who had zero infra chops. reply x0x0 2 hours agoparentprevRunning production app on fly. There have been some hiccups but generally we're quite happy with it. reply bibabaloo 7 hours agoprevLooks like they've quietly (?) deprecated the hobby plan too. Getting 3 instances a month for free was probably too good to be true forever.. reply 4ggr0 7 hours agoparentWhy do you think that? Still visible here: https://fly.io/plans/personal I hope they don't cancel this plan, currently on it :D EDIT: Ohh, it's only visible when I'm logged in, weird... EDIT2: ...indeed > The paid Hobby plan and the Legacy Hobby plan are not available for new sign-ups. https://fly.io/docs/about/pricing/ damn :( > If you were on the free Hobby plan at the time that the paid Hobby plan became the default for new organizations, your plan is now called the Legacy Hobby plan. Your costs stay the same as they were, with no monthly subscription fee, and no included usage beyond the free resource allowances. so i get to use the legacy hobby plan because i'm grandfathered in, nice. only use it for a hobby project with one instance anyways(see bio), if they really decide to start charging me i'll gladly leave. reply gregors 1 hour agoparentprevWhoa, they did get rid of it. Not surprising in the least, but they definitely did it with a minimum of fanfare. When Heroku got rid of their free plans at least they were upfront and released a blog post about it. Maybe they did and I just didn't notice. reply throwaway74566 6 hours agoparentprevHeya, we deprecated pretty loudly! We made a post on the community forum [0] and we sent an email to all active accounts saying the following: > The first improvement we're excited to announce is that the $5 Hobby plan is no more. We're replacing it with a very simple Pay As You Go plan. On this plan there's no more upfront $5 charge and no minimum monthly commitment. You only pay for what you use. If you don't use anything for a given month you pay $0. You still need a credit card on file to prevent abuse. But your card is only charged if you use the service. [0]: https://community.fly.io/t/fresh-produce-pay-as-you-go-plan/... reply d-z-m 5 hours agorootparentIf you're going to stand behind your product, I'm not sure posting with a throwaway sends the strongest message. reply tptacek 1 hour agorootparentIf anyone's ever wondering how carefully choreographed our HN presence is, please refer them directly to this subthread, which, for posterity's sake, I will note is occurring on the morning of July 4. reply jfentflyio 5 hours agorootparentprevSorry, didn’t intend for throwaway to come across as not standing behind the product. I wrote the community forum announcement and was one of the people that worked on implementing our PAYG plan. I just wanted to clarify that we didn’t deprecate silently, so I quickly created an account to do so. Apologies for any confusion this caused! reply itake 2 hours agorootparentprevI didn’t get this email. I don’t check the forums for announcements reply dathinab 6 hours agorootparentprevrandom side note, it would be nice if the region selector for regional pricing would display human readable region labels in addition to 3 letter acronyms. reply fooey 1 hour agorootparentyeah, it's basically impossible to compare regions right now they need a table that's a human readable name and an average/range for the cost modifier as compared to the baseline reply Aeolun 6 hours agorootparentprevWhy a throwaway? reply goykasi 3 hours agoprevSo, hosting in Japan is more expensive than regions in America? JPY is in a horrible place compared to USD. That seems like a good way to push away Japanese users away -- make it more expensive in countries where the local currency is weak. reply benatkin 5 hours agoprev> our billing system has been hot garbage for most of the life of the company I’ve heard the same thing about their core product. Oh wait, they even acknowledged it: https://community.fly.io/t/reliability-its-not-great/11253 reply TiredOfLife 6 hours agoprevThey currently cost 4 times more than Hetzner. reply tptacek 1 hour agoparentIf you are literally just looking for a place to cost-effectively park a VM, you should very seriously consider Hetzner! It would be weird if there weren't use cases where Hetzner comes out ahead. reply sofixa 6 hours agoparentprevHetzner gives you bare (or virtual) metal that you have to install stuff on, and maintain it. Fly.io and similar give you a higher abstraction layer where you throw a Docker container and the stuff below and around it is managed for you. Basic suff like OS patches, but also autoscaling, load balancing, request routing, etc. Does the 4x difference make it up? It will depend entirely on you and how good you are with the things fly.io abstract for you, and how much you value your time spent on it. reply TillE 1 hour agoparentprevHN always mentions Hetzner, but they're no longer an exceptionally cheap VPS provider. They're basically the same price as OVH. I forget when exactly they raised their prices, but it was several years ago. reply Aeolun 6 hours agoparentprevThe big benefit of fly is that your server only costs money when it’s doing something (with autoscaling enabled). reply neom 6 hours agoprevDon't know if anyone noticed, but Ben Johnson recently became head of product over there. I'm fortunate to have called Ben a friend for a long ass time now and I gotta say, I'm extremely bullish on fly as a result of this. Ben is not just a developers developer, he's genius level intelligence. https://github.com/benbjohnson (We fumbled the ball hard at DigitalOcean ngl, fingers crossed Ben can build what I dreamed of building one day!) reply PUSH_AX 2 hours agoparentI’ll be bullish on fly when the awful outages stop. They had a hardware outage in the LHR region for a month, it was probably longer but I moved to a different provider so I haven’t bothered to check. reply tptacek 1 hour agorootparenthttps://fly.io/infra-log/ reply tyre 5 hours agoparentprevWhat/why do you think DO fumbled? Fly looks like what DO could have been, and they had a 10 year head start. reply ericcholis 2 hours agorootparentI'm curious as well. Pretty happy with DO overall; but it is relatively vanilla as far as a hosting product goes. reply jsheard 7 hours agoprevHow has Flys reliability been as of late? From past discussions I got the impression that despite it being quite polished on the surface, behind the scenes their operations are a bit of a mess. e.g. https://news.ycombinator.com/item?id=36808296 https://news.ycombinator.com/item?id=39365735 reply pgt 7 hours agoparentFrom personal experience, still not great. Sometimes things just don't deploy. I host my little mobile braai simulator on it (braaisim.com) because it's an art project and easy to add new domains / certificates. Convenient to be able to scale up easily if it went viral in ZA. Nice that they support Johannesburg region (jnb), though, which is close to me. reply burningion 6 hours agorootparentYou were downvoted, but as a user, I’ve experienced the same inconsistency with deploys. There’s also a hard container size limit I’ve run into multiple times. If you add a dependency and go over the size limit, your app won’t deploy unless you switch to a GPU instance, which is substantially more expensive. reply tptacek 1 hour agorootparentDon't switch to GPU instances just to get around the container size limit! What are you trying to do and what's the size of your container? My instinctive response here is that you're not holding it the way we expect you to: that some of what you're sticking in your container, we'd expect you to keep in a volume. reply soulofmischief 1 hour agorootparentprevOne thing you can do to minimize container size is have multi-stage builds in your Dockerfile so that you are not affected by cache bloat. reply manchmalscott 2 hours agoparentprevTo provide at least one positive example, I'm using fly.io to host a website I made for a friend (for her college capstone project), so I'm on the now-legacy hobby plan paying nothing, and I have had zero problems. reply urschrei 6 hours agoparentprevI see two sibling comments with reports of patchy reliability, so I'll note that it's been rock-solid for me (multiple daily deploys every weekday) since I last commented on it (about a year ago?) reply zackify 6 hours agoparentprevA bunch of mini outages and the main site + dashboard was out for maybe an hour last week on Thursday. my production app has had 1 outage where stripe webhooks weren’t delivering for 45 minutes which is a huge issue for sure. I mitigated by checking in addition to the webhook event and also it hasn’t had a problem since. Their explanation was a 3rd party networking issue at or near one of their data centers reply fideloper 1 hour agoparentprevI use it to power CI builds (a lot of them!) and have extremely little issue with it. Basically I’m just using the API to spin up machines, which do some work and shut down. There’s some extra machines per build job, like database containers or a headless browser for testing. Pretty smooth in my experience. I think the only occasional issue I hit is the internal DNS being a few seconds behind reality. reply tptacek 58 minutes agorootparentYou're not entirely unbiased about this. :) reply tptacek 1 hour agoparentprevhttps://fly.io/infra-log/ Detailed description of every incident (a superset of \"outages\"), including stuff not visible enough to make the status page, going back to April. reply prettyflyboy 44 minutes agorootparentSemantics aside, if an incident/outage/ affects devs/users in any way, it really should be on the status page. I found it impossible to distinguish between user error and platform outage. Too often it was a problem on fly's end yet the status page gave nothing (perplexed, I'd rerun the deploy a few hours later and it would work). Can't stress it enough: if fly's services aren't working for any reason, big or small, put it on the status page. Devs need to know when something's not in their control so they can inform their team and customers and go to bed or take a coffee break. They shouldn't be up till 5am thinking they borked something when the problem is 100% on fly's end. reply tptacek 32 minutes agorootparentWe have the same semantics. The incidents that are on the infra-log and not on the status page are things that didn't affect devs/users in any way. A good example: we have clusters of edge servers in every region we serve. They're servers, part of their job is to occasionally throw a rod. When that happens, we pull them out of service and stop advertising them. That's an internal incident, but it's not a status page incident. Preempting a response here: no, I don't think users would be better off if we status paged stuff like this. Every week, we go through the incident management system, and all our incidents, whether or not they had user impact, get written up here. One thing you're probably running in to, which is not on you but rather on us, that we are aware of, and that we are grinding steadily away at: as the platform has stabilized over the last year, an increasing share of problems users run into aren't \"platform\" issues (in the sense of: things going wrong in our API or on our cloud servers) but rather bugs in `flyctl`, our CLI. We have two big reliability projects happening: * We've finally landed our \"white whale\" of being able to move workloads that have attached volumes between different physicals. I'm writing a big long blog post about how that works now. We've been doing it for about 9 months, but we're at the point where we can do it automatically and at scale. Really hard to express how much not having this capability increased our degree of difficulty. * We're taming `flyctl`; in the immediacy, just doing a better job of reporting errors (it's a Go program, we love Go, but showing \"context deadline exceeded\" to a user is, obviously, a bug, albeit one we didn't recognize as such until recently). I don't have a way of addressing these kinds of concerns, which are valid, without just being clear about what we're up to and how we're thinking. I wrote a sort of \"once-and-for-all\" thing about this mentality here: https://news.ycombinator.com/item?id=39373476 reply itake 2 hours agoparentprevThey upgraded my server and it was down for about 8 hours. They warned me about the upgrade, but didn’t tell me they failed to restart my server. reply no_exit 4 hours agoparentprevThe very first time I tried to spin up a test deploy the instance never came online, which I thought was pretty funny. This was like 6 months ago. reply Aeolun 6 hours agoparentprevI get the impression that the operations are a bit of a mess too, but that support is empowered to actually make people whole, which is a nice change of pace. reply oefrha 5 hours agorootparent> which is a nice change of pace That’s about every early stage company, though. reply matthewmacleod 4 hours agoparentprevBad enough that we had to migrate our key customer-facing platform off of it because of the impact. I love the idea and will still use it for many less-business-critical apps but after multiple late-night weird un-debuggable database outages we had to switch. reply echoangle 6 hours agoprev [10 more] [flagged] neom 6 hours agoparentCompliance with what exactly? When you bring servers down to Brazil it gets weird, outside just not supporting that region, not much you can't do about it. We never did anything in Brazil because it was such a huge pain in the ass, it's hard to get stuff in and expensive to rack once you're there. imo Good for fly for doing the work, even if they did have to slip someone a 5 on the way in. reply echoangle 6 hours agorootparentCompliance with ISO 37001 for example. As a big company, you can’t just go around and bribe random border guards to get your stuff through. I’m also not sure if it is illegal to pay bribes in other countries when you’re an American company. So compliance with laws is another problem when paying bribes. Edit: https://en.wikipedia.org/wiki/Foreign_Corrupt_Practices_Act Looks like it is explicitly forbidden to bribe foreign officials as a US entity. Not very smart to suggest you’re bribing people in other countries in official communications. reply neom 6 hours agorootparentIt's not overt, as they mentioned in their post, mystical fees just show up on your invoice from the government. I do agree tho, it's a really silly detail to include. I'm a bit passionate about this because I ran a long disability study at DigitalOcean to see if we could do Brazil. reply riffraff 4 hours agorootparentI suppose you meant \"feasibility study\" or is \"disability\" the actual word? reply solardev 0 minutes agorootparentOh, that makes a lot more sense lol solardev 4 hours agorootparentprevWhat does a \"disability study\" mean in the context of international server installs? reply Duwensatzaj 4 hours agorootparentprevLook at the Exception section. Grease payments are allowed. You can’t bribe border guards to sneak things through, but if they demand a bribe to do their job or your servers get mysteriously blocked from import, paying them off is allowed. reply fragmede 3 hours agorootparent> Despite efforts to provide advice, the distinction between grease payments and bribes remains a source of contention in FCPA enforcement, with legal experts and practitioners continuing to argue and interpret it. Talk to corporate council before \"grease payment\"-ing anyone. reply sofixa 6 hours agoparentprev [–] To be fair, Brazil does have weird and expensive customs/import duties on technology products, which makes a lot of hardware expensive. Maybe fly.io just didn't know about it and that's why they say \"suspicious\"; But yeah, it definetely sounds like admitting to bribery, which as an American-registered company is illegal even in other countries. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Starting July 1st, region-specific pricing for Machines, including additional RAM, will be introduced due to varying infrastructure costs by region.",
      "The price adjustment will be phased in over four months, with final prices set by November; initial invoices will show region-specific line items without price changes.",
      "A bug fix for Machines Shared CPU 1x usage not covered by Free Machines Allowance credit has been implemented, and credits are being reissued."
    ],
    "commentSummary": [
      "Fly.io's region-specific pricing has ignited discussions, with some users finding it expensive compared to alternatives like Hetzner, especially for high availability.",
      "Fly.io defends its pricing by highlighting the unsustainability of flat global rates due to high operational costs in certain regions, such as Brazil.",
      "Despite the removal of the hobby plan and some reliability concerns, many users appreciate Fly.io's features like dynamic request routing and \"ops-less\" deployments, which they believe justify the higher costs."
    ],
    "points": 94,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1720077454
  }
]
