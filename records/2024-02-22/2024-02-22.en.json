[
  {
    "id": 39452494,
    "title": "Opposition to Ban: Flipper Zero and Security Tools Foster Collaboration",
    "originLink": "https://saveflipper.ca/",
    "originBody": "SaveFlipper.ca Version française en bas English Version Vehicle thefts - Insecure vehicles should be banned, not security tools like the Flipper Zero Sign Find and contact your MP Vehicle theft is an issue that affects us all collectively. As cybersecurity and technology professionals, we recognize the importance of acting rapidly to reduce its impact on Canadians. That being said, we believe the federal government’s proposal, particularly the prohibition of security research tools, is ill-advised, overbroad and most importantly, will be counterproductive. Innovation, Science and Economic Development Canada (ISED) will pursue all avenues to ban devices used to steal vehicles by copying the wireless signals for remote keyless entry, such as the Flipper Zero, which would allow for the removal of those devices from the Canadian marketplace through collaboration with law enforcement agencies. We believe this policy will not work because it rests on a misunderstanding of the underlying technology This policy is based on outdated and misinformed technological assumptions, making it unfeasible to implement and enforce. Security tools like Flipper Zero are essentially programmable radios, known as Software Defined Radios (SDRs), a technology which has existed for years, and in some cases can be built using open-source or simple over-the-shelf-components. These radios are also fundamentally the same as those used in numerous devices across various sectors, including smart household appliances, drones and aerospace technologies, mobile phones and networks, as well as industrial control systems. Consequently, prohibiting such functionality is virtually impossible and could stifle the Canadian economy significantly. We believe this policy will degrade national security, by incentivizing manufacturers to design insecure products, as security research is criminalized and right-to-repair is penalized This policy fails to recognize that these tools are not the enemy, rather, insecure products are. Unlike decades ago when the industry relied on security through obscurity as a strategy, we now can attest that the democratization of security research tools is a balancing force for manufacturers to improve the safety of their products. Today, many industry actors rely on such research, just like we have Federal & Provincial1 government programs that support & reward security vulnerability disclosure that benefits us all. Implementing such a policy would have a chilling effect on these efforts, potentially undermining their positive impact on society. Additionally, with bills such as C-244 (Right to Repair), that recently passed unanimously and C-294 (Interoperability) that gathered support from multiple parties, we believe this overbroad policy will penalize legitimate analysis and repair use-cases, that were just made available to canadians. Finally, we believe this policy represents a waste of judiciary resources that could be better used to collaborate with experts from the cybersecurity industry to identify ways to prevent and deter such crimes Because of the arbitrary nature of such a policy, we believe the judiciary system will be faced with a slew of litigious cases around the many uses of these security tools. Instead, these resources could be focused on creating constructive communication channels between cybersecurity experts, car manufacturers, insurers, and the judiciary system to identify ways to improve the security of automotive keyless entry and push-to-start systems, and enforce minimal levels of security for future products, as it is the case in other industries. English version above Version française Signer Trouver et contacter votre député Vols de véhicules - Les véhicules non sécurisés devraient être interdits, pas les outils de sécurité comme le Flipper Zero Le vol de véhicules est un problème qui nous affecte tous collectivement. En tant que professionnels de la cybersécurité et de la technologie, nous reconnaissons l’importance d’agir rapidement pour réduire son impact sur les Canadiens. Cela étant dit, nous croyons que la proposition du gouvernement fédéral, en particulier l’interdiction des outils de recherche en sécurité, est malavisée, trop générale et surtout, contre-productive. ISDE examinera toutes les possibilités pour interdire les dispositifs utilisés pour voler des véhicules en copiant les signaux sans fil du système de télédéverrouillage, tels que le Flipper Zero, ce qui permettrait de retirer ces dispositifs du marché canadien, en collaboration avec les organismes chargés de l’application de la loi. Nous croyons que cette politique ne fonctionnera pas car elle repose sur une incompréhension de la technologie sous-jacente Cette politique est basée sur des hypothèses technologiques obsolètes et mal informées, rendant son application et son exécution irréalisables. Les outils de sécurité comme le Flipper Zero sont essentiellement des radios programmables, connues sous le nom de Radios Définies par Logiciel, ou Software Defined Radios en anglais, (SDR), une technologie qui existe depuis des années et qui, dans certains cas, peut être construite en utilisant des composants open-source ou simples disponibles dans le commerce. Ces radios sont également fondamentalement les mêmes que celles utilisées dans de nombreux appareils à travers divers secteurs, y compris les appareils ménagers intelligents, les drones et les technologies aérospatiales, les téléphones mobiles et les réseaux, ainsi que les systèmes de contrôle industriel. Par conséquent, interdire une telle fonctionnalité est pratiquement impossible et pourrait étouffer considérablement l’économie canadienne. Nous croyons que cette politique va dégrader la sécurité nationale, en incitant les fabricants à concevoir des produits non sécurisés, car la recherche en sécurité sera criminalisée et le droit de réparer pénalisé Cette politique échoue à reconnaître que ces outils ne sont pas l’ennemi, mais plutôt que les produits non sécurisés le sont. Contrairement à il y a des décennies, où l’industrie s’appuyait sur la sécurité par l’obscurité comme stratégie, nous pouvons maintenant attester que la démocratisation des outils de recherche en sécurité est une force d’équilibre pour les fabricants pour améliorer la sécurité de leurs produits. Aujourd’hui, de nombreux acteurs de l’industrie s’appuient sur une telle recherche, tout comme nous avons des programmes gouvernementaux Fédéraux et Provinciaux2 qui soutiennent et récompensent la divulgation de vulnérabilités de sécurité qui nous profitent tous. La mise en œuvre d’une telle politique aurait un effet paralysant sur ces efforts, compromettant potentiellement leur impact positif sur la société. De plus, avec des projets de loi tels que C-244 (Droit à la réparation), qui a récemment été adopté à l’unanimité et C-294 (Interopérabilité) qui a recueilli le soutien de plusieurs partis, nous pensons que cette politique trop générale pénalisera les analyses légitimes et les cas d’utilisation de réparation, qui viennent juste d’être rendus disponibles aux Canadiens. Enfin, nous croyons que cette politique représente un gaspillage de ressources judiciaires qui pourraient être mieux utilisées pour collaborer avec des experts de l’industrie de la cybersécurité afin d’identifier des moyens de prévenir et de dissuader de tels crimes En raison de la nature arbitraire d’une telle politique, nous croyons que le système judiciaire sera confronté à une multitude de cas litigieux concernant les nombreuses utilisations de ces outils de sécurité. Au lieu de cela, ces ressources pourraient être concentrées sur la création de canaux de communication constructifs entre les experts en cybersécurité, les fabricants de voitures, les assureurs et le système judiciaire pour identifier des moyens d’améliorer la sécurité des systèmes d’entrée sans clé et de démarrage par bouton-poussoir des automobiles, et imposer des niveaux de sécurité minimaux pour les futurs produits, comme c’est le cas dans d’autres industries. Signatures If you agree with this letter, please sign it and share it. Signatures are added to the site roughly once a day, manually. Si vous êtes d’accord avec cette lettre, veuillez la signer et la partager. Les signatures sont ajoutées au site manuellement, environ une fois par jour. Name/Nom Title/Titre Guillaume Ross Deputy CISO, JupiterOne Pierre-David Oriol Cybersecurity Product Executive Gabriel Tremblay Cybersecurity CEO Nelson Lamoureux School principal & CISO CSSMCN Axel Schulz Director Cybersecurity Operations Émilio Gonzalez Cybersecurity Professional Simon Carpentier Information Security Principal @ Desjardins Dr. Alexander Dean Cybulski Security Awareness Professional - Utilities Sector Patrick Mathieu President, Hackfest Communication Nicholas Romyn Security Architect Sivathmican SivakumaranMike Lizotte Offensive Security Consultant Laurent ChouinarddystopieEric HogueJames ArlenFernando Montenegro Cybersecurity Industry Analyst Jean-Sébastien Delorme NorthSec - VP Training Adam Anklewicz Manager, IT Endpoint Engineering Tim Fitzgerald Manager of IT Systems SeanCharles FergusonRahulNathan Beranger Sr. IT Professional Guillaume Morissette Guillaume Morissette Philippe M. Technical Solutions Architect Sylvain PPierrick Vittet Analyste en cybersécurité TUXLaurent DesaulniersMat XBrad Clare IT Consultant Garth Boyd Security Architect Serge-Olivier Paquette Director of Innovation Dmitriy Beryoza Senior Security Researcher Simon Nolet Testeur d’intrusion transversaux Colin Stéphenne Cybersecurity specialist Pavel Sushko Chief Executive Officer MandeepFelipe Sáez Security Engineer Clayton Smith Security researcher salt Analyste en sécurité informatique Simon Décosse Team Lead Ethical Hacker Eric Beaurivage Administrateur de systèmes et réseaux Alexandre Côté Security Researcher Michael JeansonLex Gill Avocate Etienne Prud’hommeEric Hebert Information Security Professional Patricia Gagnon-Renaud Cybersecurity Analyst Antoine Gauthier-Drapeau Analyste en cybersécurité Sébastien Duquette Director of Software Development, ex-Application Security Lead @ Devolutions Cyndie Feltz Co-founder Jacob Diamond Pentester Mark Cohen CIO Adrian ChristieFrancis Coats Expert en sécurité Louis NadeauMartin DMaxime Paradis Analyste Technique Régis BelarbiSimon Loiselle Lead security advisor Simon Charest Senior Software Developer and CEO at SLCIT Andrew Bellini Author of Beginner’s Guide to IoT and Hardware Hacking V Daniel Account executive Eric C.Alex KozinAlexandre Blanc strategical and security advisor Free Spirit Bitcoin miner Felix Doyon DevOps Engineer Alec Barea Global information security director Vaidotas BrazauskasTom EwanAlexis D.Eugene Grant Security Principal TEC Ops Mitch MDenis LessardDavid GuerinPier-Luc MoisanLee Brotherston Infosec Rando Étienne Lévesque Développeur web Gaspar-Sec CyberSecurity Analyst Jean-FrançoisKenneth GallagherBruno MorelCid SummersReda Baydoun Sr. Principal Software Dev., CISSP Marc-Antoine Chabot Technology expert, ECCC Sergey Faleev Senior IT manager Neumann limMarc-Etienne M.Léveillé Chercheur en logiciels malveillants Olivier Bilodeau Président de NorthSec et Directeur de la recherche en cybersécurité chez GoSecure Benjamin CourchesneAlexandre Larocque, P.Eng. CEO @ Ardent Security Samuel A. Application Security Analyst I Frédéric Fortin Président, iconnek.io Marc OuelletJ. FournierAuré Serrurière Ben Renaud Director, Cybersecurity, PetalMD Vincent Le Hémonet Consultant sécurité réseau C G Cybersecurity specialist Mathieu Hetu CEO - MH Service Technologies Maxim Chartrand Chef d’entreprise, GTI Télécoms Maxime Labrecque-Raymond Conseiller en cybersecurité Cedric Thibault Partner & CTO Max WotJean Rho Integrateur, système de sécurité Jean-Michel VCharles-Etienne CrevierSteve Waterhouse Professionnel en Sécurité de l’Information Fabrice Delor Cybersecurity Architect Vincent lambertMolon LabeGabriel Longpré Analyste Sécurité & Réseau Marvens Decayette Security analyst Werner Burat Technical Solutions Consultant Flaster Jedd Offensive security consultant Nicholas Milot Co-Founder @Yack Bernard Bolduc Conseiller en Cyber Sécurité Caido Co-fondateur Marc-André Dumont Cybersecurity Team Lead Philippe L Cybersécurité opérationnelle Philippe HamelPascal Fortin PDG - Cybereco Guillaume RaymondPatrick Davidson Tremblay Directeur, Sécurité offensive, Simulation d’adversaires Steve Lavoie Directeur TI, CISSP Stephane Pelletier Application Security Analyst Émilien Pierru Backend developer Guillaume Nourry-Marquis Président Commit2Security Inc. Antoine ReversatMaxime Carbonneau Cybersecurity Professional Derek W. Infosec Dude & Flipper Zero User MarcKastelo CaronSimon Clavet Programmer Sébastien GravelineSébastien Massé-CroteauJason M. Cybersecurity consultant Danny Fullerton CISO, VP Olivier DionSébastien Huneault Enseignant en Informatique anarcatSonia FathAlexandre Boyer Lead info sec engineer Amandine Gagnon-Hébert Associate Red Team Consultant - Mandiant, VP Engagement @NorthSec, prévention des méfaits > coercition Justin Brulotte Senior AI Developper Fred Quenneville Ethical Hacker DominiqueTrevor hough (loudmouth security) CSO Martin Léger Sysadmin Olivier Michaud Technicien en TI Yvan Beaulieu PhD Conseiller senior en sécurité de l’information Justin LavoiePhilippe Panaite Lead System Administrator Marc Cybersecurity consultant MarkSaïd Izawi Security Solutions Architect Dany Boivin FlippingForFlipper Jason KeirsteadLG Nobre Cybersecurity professional Adam Shostack Author, Threat modeling: Designing for Security Yohannes AberachtMarc Lefrancois Lead Developer Jeremy AubeEric M. Gagnon Chef D’équipe, Simulation d’adversaires Daniel Drouet Tech investor and entrepreneur Tim Partridge Senior Security Architect - IBM Canada LTD Oley VMathieu SaulnierRoss Derewianko Senior Staff Systems Eingeer Alexandre A.Rob Wood Founder Anne KatherineJason Ernst Principal Software Engineer, PhD Wireless Mesh Networks Alexandre Guédon Cybersecurity Professional Tom GwozdzLee NicholsWill WhittakerSamer Essa Information Security Manager Nicholas St-Jacques Mr. Ty LamontagneZachary MMike Clarkjeff woods VP Software Daniel Lynch Senior Cyber Security Advisor Trevor OrsztynowiczEricCody TrewKole BarnesChristopher ReidWilliam Bergmann Børresen William Bergmann Børresen SsmidgePhilippe Dépelteau IT & Cybersecurity Director Orin JohnsonBort Cybersecurity Analyst Christophe LangloisMike KKatie H. Senior Cybersecurity Consultant Adam WhiteTyler NicholsonHugo Genesse Cybersecurity Researcher, VP-Conference @NorthSec Jamie BodeOlivier Laflamme Principal Consultant @RedRainSecurity & Senior Red Team Operator @F500 - CISSP, BSCP, OSCP, CRTO, CRTL, EWPTX, ECPPT Ronan ScottTyler AustinKevin2600Jon alikakos J@nnny Matt Moore Mr. AriLukas S.Andrey Frol Software Developer Noah Clements Hacker AnonMan Web Dev Malvin Din Service Technician Simon Bouchard CyberSecurity Analyst Andrew R.Evan Brundritt IT Specialist Olivier Caron Cyber Security Specialist Haram Lee random 11yro kid who wrote a full essay to Jonathan Wilkinson to save flipper Pat PapineauAidenAndreanne Bergeron Cybersecurity Researcher BibJames Renken Site Reliability Engineer Lucas Kovacsthe hackerJesse Poikonen Mr Kienan S.Anon IT Service Francis Ouellet Jedi Master Amar Juneja Cybersecurity Consultant Mark LintonScott FlowersMike Burgener VP Cybersecurity, Vancity Savings Credit Union Johnny Williams JhonnyW55 J.A. Rogers Consultant Wen Director of Product Jarob Portillo Dental Technician Sukh PanechDaniel BeaulneAndrew Simpson Software Engineer Anonymous Z Software Developer Ja d’Attaq Pen Tester Phil Steward Sr. Project Manager / Security Consultant Nandan Reddy SJames TaylorMaryanne Francis Mrs Jay Turla Principal Security Consultant Dhruv MajumdarNeelanjana MandalJérôme LebelFaith M Faith M Mohammed Shine Shine JMatthew W.Sam GRobert BrownFlipper Lobster fishing guy Captain flipper Harper Kelly Mr. Tom Kemp Dr. AndrewPaul Crampton“Uncle Harkinian” Broadcast engineer Aria Burrell Sr. Software Development Manager A.GillElijahFukirohtmlh4ckerHunter BeachcowParker Koch Parker Koch Leo XuHamdi Bahrini Application Security Analyst Flipper supporterSnorre Trehjørningen Svedman snorre@svedman.no Thomas Alonso CTI Director Paul Benton Director of IT JakeKoenFrançois Proulx Senior Product Security Engineer Paul R. VP Operations, GGR Security MasterK Mr. JPJulien Desrosiers Web developer Mike T.Jay Radcliffe Director, Medical Device Security Pavel ShirshovMitchell Schwartz Developer and Process Designer Mark Hahn sysadmin Guy Legault IT tech Alexander Information Security Officer Paul Reinheimer President, WonderProxy Inc. Martin LG Software Engineer - Flipper Owner D. Neto Development Manager - Big 3 Banks Ian WRobert Pouliot System administrator ZkinkM.P Quick question James (purpleidea) Author of https://github.com/purpleidea/mgmt/ MSJeffrey Rau Dr. Tyler KuipersSpyersFergus Argyll 1337 h4x0r André MedeirosM. ForgetJ Rossy Software Engineer Sam BostockGerry PowerHuzefa DargahwalaAnouar Mansour Senior Cybersecurity Specialist Andrew Henry Mr Félix Charette Senior Application Security Engineer RheniumRiver White Software Engineer Andrew Brown Security Developer Jeremy Banker Senior Security Developer @ VMware Tom HotstonJ. KingLaura RodgersAdrien Lasalle NetRunner@SpeakerOffensive Security Scott HamiltonTiger FPaul Pereira-BrunnerJulian Maingot Software Engineer Adam McDaniel Technical Product Manager Vincent DansereauElizabeth LC IT Logistics Andrey PetrovJames Rycman Software Development Manager Tom StricklandKyle StevensonArthur MarguliesDaphne Reed Senior Director of Security AALAriadna UrazbaevaSultan Qasim Khan Security Researcher Bryce Benn Software Developer Jason KaczorAmin Shah Gilani Software Engineer and Hobbyist HugoNicolas KrauseJames Weatherell Technologist Samy Ghannad Security Researcher Kevin CoxMaxim BaeleJonathan Villemaire-Krajden Senior Software Developer Joey Coleman Dr Nick Taylor Manager, Infrastructure Security @ Shopify Jérôme Carretero Embedded software engineering specialist RafRyan Draga DevOps/Application Security Engineer/Maker Michael Holloway Systems Administrator Gabriel Digital Autonomy Advocate Pierre-Nicolas Allard Sr. Penetration Tester Michael Longval MD Adam SchumacherRyan LukerpelmenMichael PerklinRandy Saint-LouisThomas Kilbride System on Chip Security Architect Aaron Janeiro Stone Quantitative Analyst MartinMara Miss Daniel Mitchell Cybersecurity Practitioner Rob Keizer Founder / CTO Aurabindo Pillai Embedded Systems Engineer Sal Rahman Software Development Engineer Brian TammiUTKU KARAASLAN SW DEVELOPER Andrew Cohoe Radio hobbyist Michael Saringer Leader of Cybersecurity Carmen C Cloud Solution Architect Mohit ChauhanJF GodinAlbert RSteve EhAndrew DuttonAndrew GibsonvilezBrice Yangue Cybersecurity student JohnFScottShanti MaharajMitch AJustin Unrau Digital Literacy Librarian J DunfordMikkel Paulson Former leader, Pirate Party of Canada SPACECADETBeau Gunderson CTO Aliaksei SheshkaBrandonSacha MallaisMathieu Morrissette Application Security Lead David Kiddell IT Coordinator Karim Yaghmour CEO / Author / Speaker / Consultant Ryan Barber Software Technologist André M Software Architect Jared T.Asad Osman hobbyist programmer Helene SAdam M.Aiden Fox Ivey University of Waterloo - Computer Engineering Callum HayKeith DaserDanielle De LeoDamian Zaręba Mr Jeff Lavoie Software Developer and Researcher Paul HOlivierEthan HenryTaylor LAlex.S Software Engineer XKKorran Nielsen Staff Software Engineer at Google, security Jonathan TougasCharles B Software Developer mlavgnLeeWMario MP B Tier 3 Infrastructure Support Technician Robert Xiao CS Professor @ UBC Vianney Gall Cyber security analyst Owen Anderson Software Developer Kelly Banman Software Engineer Mike Gozzo Chief Product & Technology Officer, Ada Andrew Beeler Mr. Cindy “Sinderz” JonesNelson Asinowski Leturer in Computer Support Jamie FEric Sedore information Security Abdelhakim Q. Software Developer Nasheed Ur RehmanRoss Zurowski Designer & Technologist Chris FosterA.K Software engineer Saave Da Phlipper Lord. Bernard Lebel Directeur - Cybersécurité des TO E. Calderon Penetration Tester EspagneStefan Timotijevic Hackers Without Borders Thomas Foubert DevSecOps Engineer & Pentester Vincent Salamanca-Gagnon Software Developer Benjamin Dupuis Ingénieur cybersécurité Matthieu Borgraeve IT lead Laurent Dumont Senior Cloud Architect Thomas Thetank Engineer Justin Duval IT Specialist Tyler Ratliff Security Researcher Lucien DuboisJerome Tremblay Architech TI Mikhaël E.RICHARD Corentin Industrial Software Engineer Sean I.T. Specialist Hakim Sid’AhmedJollyMongrel Researcher Stéphane BeniakProvincial examples: Quebec’s bug bounty, Alberta’s Vulnerability Reporting Program,British Columbia’s Vulnerability Disclosure Program ↩︎ Exemples provinciaux: Programme de prime aux bogues du Québec, Programme d’avis de vulnérabilités de l’Alberta,Programme d’avis de vulnérabilités de la Colombie-Britannique ↩︎ ©Creative Commons :: Theme made by panr",
    "commentLink": "https://news.ycombinator.com/item?id=39452494",
    "commentBody": "Insecure vehicles should be banned, not security tools like the Flipper Zero (saveflipper.ca)1358 points by pabs3 22 hours agohidepastfavorite797 comments GuB-42 13 hours ago> Security tools like Flipper Zero are essentially programmable radios, known as Software Defined Radios (SDRs) The Flipper Zero is not a SDR, it is less capable than that. That's the ironic part, the Flipper Zero is a rather weak hacking tool. It can open car doors, but it is so impractical that it is not much more than a party trick. You have to record the code by pressing the button on the keyfob out of range of the car and in range of the Flipper. You can then open the door to the car, once, and only if the owner didn't open it first. There is a more advanced and maybe practical attack called rolljam, but I don't think the Flipper is capable enough to do that. The only thing is that the Flipper Zero is fun, cheap(ish), and popular, but real thieves already have better tools for their job. reply strangattractor 9 hours agoparentAt one time is was possible to open car doors with a coat hanger. Nobody suggested banning coat hangers. They fixed the doors. reply noduerme 9 hours agorootparentThis came in handy for me once when I locked myself out of a u-haul in what used to be a rough neighborhood in New York. I was standing there fishing with a coat hanger trying to figure it out when a gangster looking dude came up and was like \"here, let me help you with that,\" and 3 seconds later had the door open. reply technofiend 8 hours agorootparentprevI locked myself out of my car so often I ended up wedging a coat hanger under the rear bumper. You're right: car doors used to be trivial to yank open when you knew the trick. These days I'd love to create a little side project that sets the alarm off on my car any time a relay attack is detected. I'm sure Mark Rober or someone will end up doing it. reply nox101 42 minutes agorootparentwhat would the alarm do other the annoy a few hundred people. It wouldn't save your car. car alarms should be banned imo. replace them with something that pings your phone maybe but the alarm does nothing reply biosboiii 2 hours agorootparentprevsounds rather complicated. You would have to scan the entire spectrum (not the entire, but like certain bands) and detect if there is someone sending power on there. But modern vehicles detect these relay attacks and drop your message when you are too far away (ToF Measurement), you could maybe instrument that. But then when you walk to your car on an open field, you may also trigger the alarm because your key is out of range for a short time. reply mulmen 8 hours agorootparentprevWouldn't it have been easier to put a second key under there? reply tourmalinetaco 5 hours agorootparentOf course, but that takes time and money to get a new key made. And once you know the trick it’s trivial using a coat hanger. reply ummonk 8 hours agorootparentprevNot a great analogy though, since a coat hanger’s primary use is not to break into cars. Note: I’m not advocating banning the flipper zero anymore than I’d advocate banning lock picks. I just don’t think the analogy is apt. reply deadbunny 7 hours agorootparentAnd a Flipper Zeros main purpose isn't to break into cars either... reply happytiger 12 hours agoparentprevThe idea of banning the flipper is like banning legos because you can build lockpicks out of them — it’s just nonsensical politician logic. reply dymk 9 hours agorootparentWhat kind of lockpick can you build out of legos? reply happytiger 6 hours agorootparentA very, very bad one that might only work with Lego locks. Let me double down… https://ideas.lego.com/projects/40434532-eaa1-4926-8621-3629... reply sgerenser 8 hours agorootparentprevThat’s the joke reply antirez 11 hours agoparentprevThe Flipper Zero is not certainly the problem here, and it is not a proper SDR tool as said. But I believe it's technically interesting that the Flipper Zero uses the CC1101 inside the Flipper in a more powerful way. The chip can be configured to just report in a given pin the actual OOK/FSK state (logic high/low). The same can be done while transmitting. So the Flipper is not limited to the protocols/formats supported by the CC1101 during normal operations, but can do any protocol as long as it is within the frequency range and uses OOK or FSK modulation (or the FSK variants supported). reply kristopolous 1 hour agoparentprevAll this will do is increase sales volume. Given that flipper is 95% just a marketing play, I wouldn't be surprised if they're already on this reply dylan604 13 hours agoparentprev> but real thieves already have better tools for their job. there you go, letting logic get in the way of a politician looking to score points reply hoofhearted 19 hours agoprevSooo they have been stealing Infiniti's from my area recently with relative ease allegedly by using a Bluetooth obd2 reader connected to an android tablet running a pirated copy of some Nissan service tech software. Nobody from any of the Infiniti groups is 100% certain how they are doing it, but the best theory out there is above. Just the other night, a crew of dudes stole 3 Q50’s from my neighborhood with relative ease. Here is the ring cam video my neighbor posted: https://video.nest.com/clip/8ef4d060588d4c7289f87cccb00cb55a... reply aetherspawn 11 hours agoparentWell for one thing the OBD port shouldn’t be designed so that it has direct access to any useful CAN bus. It should go to a gateway that requires authentication to do anything except read OBD, and all of the IDs that you are allowed to send should be whitelisted. The issue people are mentioning with the headlights is easily solved by just moving the starter CAN to its own CAN bus between the immobiliser and the ECU (physically isolating the headlights), which costs about $5 total and requires no crypto unless thief is willing to cut the car nearly completely in half. (The problem with crypto is the $10 safety MCUs used all throughout cars are only like 20MHz and they can’t really do the 2000+ crypto ops/sec on top of their current workload. Also the tooling support for crypto ATM is really poor in the model based design tools that are used for this safety relevant SW) BTW I personally don’t believe that anything that involves cutting into a vehicle is negligence of anyone. I mean, from my perspective, anyone can just pop the hood and drive the car with their own BYO ECU. It’s just a hunk of metal and once you start cutting it up you can make it do whatever you want. I am an automotive systems engineer. reply ngneer 10 hours agorootparentYes, the simplest solution sometimes really is the right one. Cheaply isolate sensitive targets from easily accessible areas. Your $5 solution is enough to avert these issues, and makes the attack a lot more expensive. The job is to find a \"lever\" where you only have to put in a little effort (say $5 worth) but which causes the thief to have to put in a lot of effort (cutting the car in half). The better the \"lever\", the safer the design. reply arjvik 9 hours agorootparentprevI agree fully with this, except for the fact that this then makes devices like the Comma (comma.ai) impossible. The hacker in me really wants to be able to send steering signals by plugging something into my car :) reply aosmith 16 hours agoparentprevSee all that time the thief spends near the drivers side headlight? The headlights are on the can bus, if you can tap a couple wires in there the cars is yours. reply arjvik 10 hours agorootparentGenuine question - why do headlights need to be on the main CAN bus? Could they not be operated from somewhere closer to the ECU by wires that just carry power and maybe some very simple data lines? reply EasyMark 7 hours agorootparentIt cuts way down on the amount and expense of wiring, copper is expensive. It makes it easier to pocket some more profit. reply ranger_danger 5 hours agorootparentjust wait until wireless CAN comes around reply ndesaulniers 7 hours agorootparentprevI suspect fancier headlights can pivot based on incline and steering wheel angle. reply sleepybrett 10 hours agorootparentprevIt may be something as stupid as a wiring harness layout optimization. You can put the headlights controller at the headlights and only run a single set of CAN bus wires (which are probably in that area anyways), haven't played with CAN I assume they are fairly few and fairly thing gauge, through the firewall of the car. reply ddalex 10 hours agorootparentprevI'm very surprised to see that the ignition control is on the slow speed can bus... reply EasyMark 7 hours agorootparentWhat part of starting a car is fast compared to the canbus? Speed is relative reply markhahn 19 hours agoparentprevObviously, we want owners to have full access to their car's CANBUS. So the question is: how should the OBD-2 plug (or wiring) be protected? reply raizer88 19 hours agorootparentAll CANBUS packages that are useful to drive a car should be encrypted using a public/private key that is in the owner key. Decryption chips are cheap and fast. reply acdha 18 hours agorootparentMaintenance is a big key management problem though: if only the owner has it, there will be problems when people inevitably lose it. If there are shared keys for service departments or databases, thieves will get access to them. Things like time-limited on-demand keys can limit those problems but now you can’t get your car serviced when Toyota’s servers go down and they need to commit to not breaking API compatibility for multiple decades. reply 2024throwaway 18 hours agorootparentGP said the key is in the car key. You already give the car key to a mechanic, I don't see how this would make maintenance any harder. reply acdha 14 hours agorootparentThink about what happens when people lose their keys, which will reliably happen. reply Zak 14 hours agorootparentIn the old days, most or all car companies had the ability to look up the bitting code to cut a replacement key (the mechanical kind) from the car's VIN. There's no reason they can't do the same with an encryption key. Of course they'd need to do a good job securing that database since inappropriate access to it would make stealing cars very easy. reply Reubachi 13 hours agorootparentThere is a very good reason that isn't possible/analagous to traditional rekeying. Mechanical keys are not secure. They can be reproduced with basic skills. That's why there used to be a giant key cutting industry where much of the business was car keys (Thanks, GM.) The whole idea of CA PKI and all modern TPM architecture on devices is that they CAN'T be reproduced or replaced in context without massive effort that would make the intended use moot; IE replacing the TPM and associated on both the key and car. This would require some bureaucratic pointless process to prove your identity, and it would be very expensive and frustrating, and completely at the manufacturers will. Further, if the car CPU could allow this, it would be >.0001 second before theives use the same exact tools that the manufacturers use. This is basically what's happening now with current NFC/Radio Keyfobs. Basic access to existing cpu through canbus makes NFC/Radio moot. reply RHSeeger 13 hours agorootparentprev> If I left a million dollars out on my front porch, and someone stole it, that would not be my fault in any sort of way Pretty much all of human history to this point says that this is a practical impossibility. If there is such a database/secret, it will get out. reply K0balt 13 hours agorootparentprevMost modern keys already have cryptographic rfid transponders which must be in place to turn off the Immobilizer system. Unfortunately, Immo can be trivially disabled/bypassed/reprogrammed on many cars using the canbus or odb2 interface. Also trivially editable in many ICUs is the mileage, airbag (crash) history, etc. The main vector is that this data typically exists alongside performance parameters and user data like registered keys and fobs, so is accessible either by catching the ecu in bootup/program mode, by buffer overrun attacks, or often just by asking nicely. This is basically doable by anyone who can to chip tuning or ECU remaps. It’s technical, but not that technical. Many ECUs require JTAG access inside the ECU housing or even desoldering the serial flash chip, but many do not. I just bought a whole setup for this from AliExpress for about 100 dollars and it’s worked well for me so far, just a specialised JTAG adapter with some cables really. Pretty sure if you wrote drivers for chip tuning software to use a buspirate it would work just as well if not better. reply twodave 17 hours agorootparentprevThe manufacturer should maintain a root cert that can be used. If that root cert is compromised then they should have a way of rotating keys if the vehicle and physical keys are present. Breaches then constitute what amounts to a software recall, putting the onus on the manufacturer to report them or be held liable for thefts. The recall notice puts the liability on the driver to have their vehicle updated (for free) in a timely fashion. reply kube-system 17 hours agorootparentTo do that, we'd probably need to accept one of these as a consequence: 1. all cars must be internet connected so they can pull CRLs 2. dealers and locksmiths are no longer able to replace keys, you will have to ship the car back to the manufacturer if you lose your keys. Because there's no secure way to hand out the root cert to the thousands of organizations authorized to replace keys today. reply acdha 17 hours agorootparentThe situation doesn’t need to be as strict as #2: you could have a way for a registered service shop to get a per-device rekey by shifting some liability to them. Making it per device prevents bulk usage and an active communication with the manufacturer would mean the cops could ask the owners of a shady auto shop some questions when 80% of the stolen cars in the area are being rekeyed at a place the owners have never been to. I lost a car key once and the locksmith who showed up checked my drivers license against the title database because he could have been penalized for unlocking a vehicle without doing so - we could make the same model work electronically because while car thieves are anonymous, legitimate repair shops have a business presence and reputation to preserve. Even someone amoral isn’t going to look the other way for something which will cost them their primary revenue stream. reply kube-system 17 hours agorootparentI don't think that the dealer equipment being used to steal cars today is coming from dealers where management is knowingly engaging in car theft. It is other people who are misusing those tools. There are many hundreds of thousands of people who work at dealerships, and many do not care about their employers reputation. Also, many dealerships are broken into. reply acdha 16 hours agorootparentYes, which is why I suggested a combination of measures to change that. An active per-device transaction would make it clear when a dealer’s access is being misused, and if it affects their business viability it would turn out that they could do a better job of controlling access. Hundreds of thousands of people work at banks, too, and many of them do not care about their employers but thefts from customer accounts are rare because the companies are incentivized to set appropriate safeguards. There’s no reason why car repairs couldn’t be the same other than that it costs more than what they’ve been doing, and there aren’t strong enough incentives for them to take on those costs. reply kube-system 16 hours agorootparentWhat would that look like in reality? Expecting dealerships to have the same physical security, procedures, and security vetting of a bank? There's already a shortage of workers in these roles, now we want the guys busting their knuckles on vehicle repairs to have a good credit score and good background check and perform elaborate opening and closing procedures with a buddy system? Storing tools in a vault? I really don't see how any of this is merited or reasonable, especially when the vast majority of the cars being stolen in my neighborhood are either stolen with the keys or with a tow truck. reply acdha 14 hours agorootparentSimply requiring the dealers to take seriously ownership validation and track which workers used the reset system (no shared logins, etc.) would do most of it. reply jasomill 10 hours agorootparentprevRequire resets to be initiated and authorized by the F&I department, whose security and KYC processes should already be substantially similar to those of other institutions that regularly approve $50,000+ loans. reply kube-system 9 hours agorootparentMy prediction: 1. As a result, we'll see costs like losing the keys to a rental car go from a $250-500 fee to a $2500-5000 fee, due to the additional costs to process and the additional loss of use. 2. Criminal rings that steal high value cars will go from often using tow trucks, to exclusively using tow trucks. 3. The number of cars stolen via stolen keys will remain unchanged. Yes, the key itself will be more secure, but I'm not really sure it will actually improve anything. More security is not better if the costs do not create real-world results. reply acdha 8 hours agorootparentYour second point is leaving out a lot: there’s no way adding a requirement that you have heavy equipment and a skilled operator isn’t going to reduce the number of thefts, and those trucks are in more limited supply and easier to track than a small tablet. They’re also way less stealthy so there’s a lot more time to get caught. The third point may be true for classic theft but would not be true for the growing category of thefts caused by abusing wireless keys. If you can’t easily get a new key, the resell value for that car is going down dramatically. reply kube-system 8 hours agorootparentCommercial tow trucks are not hard to get in many places, but it is also not required to tow a car. There are many consumer oriented solutions for towing a car. Tow dollies are about $40 to rent in my city. Or if you're a thief, trailers aren't hard to steal either. > If you can’t easily get a new key, the resell value for that car is going down dramatically. Most of the vehicles that are stolen for resale are high value and sent overseas to parts of the world where the labor cheap to do something like entirely rip out all of the security components. I don't really think that these criminals will stuff a G-wagon in a shipping container for $100,000 but they won't do it for $80k or $90k. reply acdha 7 hours agorootparent> Commercial tow trucks are not hard to get in many places, but it is also not required to tow a car. There are many consumer oriented solutions for towing a car. Tow dollies are about $40 to rent in my city. Or if you're a thief, trailers aren't hard to steal either. Again, it’s possible but do you really think there isn’t even one thief who lacks easy access to a tow truck or will be caught firing up noisy equipment at 3am but not if they fumble around in their pocket while walking up to a car? Not a single teenager looking to joyride won’t give up if it’s harder than the Kia video they saw on Tik Tok? Similarly, yes, people will still steal vehicles and ship them overseas but the more work they do the lower the resale market and value will be, and that will make it less tempting since you’d only be able to sell to people who are content never getting service from the manufacturer. Even if we assume that there are countries with skilled technicians and effectively no law enforcement, only something like 10-15% of stolen vehicles are shipped according to U.S. officials so even if you wrote those off entirely you would have plenty of room to improve by reducing the majority of thefts which never leave the country. reply kube-system 7 hours agorootparentThere's different categories of criminal here who are willing and able to do different things to different types of cars. > Again, it’s possible but do you really think there isn’t even one thief who lacks easy access to a tow truck or will be caught firing up noisy equipment at 3am but not if they fumble around in their pocket while walking up to a car? Canbus attacks, OBDII reprogrammers, and similar are typically pretty intrusive, they require cutting into fender liners, removing lamps, busting a window, or otherwise gaining physical access to the bus. They also require specialized tooling and expertise that are harder to get than the tools which physically move vehicles. The one that might be an exception, and some savvy street criminal might be able to get their hands on is a tool to do is a relay attack, which is usually good enough to steal belongings from a car, but generally not capable of stealing the car. > Not a single teenager looking to joyride won’t give up if it’s harder than the Kia video they saw on Tik Tok? Definitely not. Vehicles with immobilizers are essentially never stolen by joyriders unless they have also stolen the keys. > Even if we assume that there are countries with skilled technicians and effectively no law enforcement, only something like 10-15% of stolen vehicles are shipped according to U.S. officials Yes, and almost all of the other ones either just lack immobilizers, or the thief also stole the keys. e.g https://archive.is/kxXn3 reply Chris2048 18 hours agorootparentprevThe same problem exist for car keys. The answer is, when a person \"inevitably lose[s] it\", they need to pay to get their electronics refit. reply kube-system 17 hours agorootparentThe result of that may be that losing a key is financially devastating enough that it totals many vehicles. And/or if the odometer and other local storage is affected, that may cause permanent title issues for the car. The number of people who lose their keys vastly dwarfs the number of people who are having their car stolen with a flipper zero. reply sleepybrett 10 hours agorootparent0 people are getting their car stolen with a flipper zero. reply Chris2048 15 hours agorootparentprevPerhaps, or perhaps not. It has to be hard enough it can't be done in the street (without getting attention), but maybe it could be easy enough to do in a garage. But even if it is expensive, the result would be that either people with take more care, or they'll lose their car. Maybe it's not a bad thing that people who can't manage a key are less likely to be on the roads - or that its more likely they lose access to their car then it ends up in the hands of criminals. A car can be a dangerous thing, even an inexpensive one. reply kube-system 15 hours agorootparentYes, but this wouldn't prevent dangerous street criminals from stealing cars. Many of them steal the keys with the car. They go down to the gas station, and wait for an old lady with a nice car to pull up to the pump, and when she hops out they hop in. The criminals doing more skilled attacks typically aren't joyriding or using it to commit other crimes, they typically doing it for financial gain: they want the car, its contents, or its parts. Ultimately the overlap between the violent street criminals and those skilled at attacking digital security systems is not much. > But even if it is expensive, the result would be that either people with take more care, or they'll lose their car. The entire reason keys were explicitly designed with the functionality to program new ones is because that's not considered by most to be an acceptable solution. reply Chris2048 13 hours agorootparentThat kind of expands the scope of this conversations to mugging/carjacking, which also comes with a higher penalty, and probably higher priority to the police. And, it involves interacting with someone, who presumably can call the police afterwards, and activate any lojack / immobilisation device before it can be removed. Presumably the appeal of stealing a parked car it may be a while before it has been discovered and reported stolen. Also, doing such a thing in a gas-station where there are likely cameras and even other people / attendants make it seem pretty risky to me. Are these dudes just hanging around the pumps in masks? What country is this? > not considered by most to be an acceptable solution Things change, but also, it's as much up to the government and/or insurance corps what's acceptable. reply kube-system 11 hours agorootparentThe only reasonable way to evaluate risk is as a whole. Real world attackers pick whichever realm is easiest to exploit, they aren't going to waste their time doing something difficult when there are easier ways to accomplish their goal. > who presumably can call the police afterwards, and activate any lojack / immobilisation device before it can be removed. Yes, people who carjack usually aren't looking for a nice daily driver to hang on to for the next 3 years. Usually they want to joyride, or use the car for some other crime, in the immediate term. > Also, doing such a thing in a gas-station where there are likely cameras and even other people / attendants make it seem pretty risky to me. Are these dudes just hanging around the pumps in masks? Stealing a car, and being in possession of a stolen car, is pretty risky already. I think someone who does this type of crime is probably not very risk averse. Wearing masks is a pretty common way to thwart cameras when committing a crime in many places, I don't think this potential security issue is specific to certain countries. I think what you might be hinting at is that fewer people want to do carjackings in different places, but the same applies to canbus exploits. Nor do I think anyone really needs to \"hang out\" to find a car at a gas station. Many have cars filling up at them regularly throughout business hours. > Presumably the appeal of stealing a parked car it may be a while before it has been discovered and reported stolen. Yes, and while there are some instances of this happening electronically, I don't think closing those avenues will change anything, because towing cars is neither difficult nor suspicious in many places. Again, security is only as good as the weakest link. Nearly all criminals cut locks, even ones are very easily picked. reply Chris2048 2 hours agorootparent> people who carjack usually.. No idea, but my point stand on how they achieve this in the first place.. > towing cars is neither difficult nor suspicious in many places because it's not a recognised method of theft. Also, buying a registered tow truck to commit a car theft doesn't sound easy to me. reply TylerE 15 hours agorootparentprevA traditional car key can be trivially duplicated at any hardware store. That's the difference. You can make as many spares as you want for a couple bucks a pop. No dependencies. No network. reply sleepybrett 9 hours agorootparentNot fully true. Just as it's not true with non-car keys. Some blanks are heavily protected. Now these days with the dissemination of cheap cnc mills, maybe thats a bit more trivial, but you are paying a lot more for a cnc mill than you pay for a old key grinder. Same issue we have now with ghost guns honestly. CNC mills are powerful tools, with the right software you can essentially just place the properly sized chunk of metal in the box and hit go. reply TylerE 6 hours agorootparentThat's why I said traditional key. They're just metal with a few parts cut to a specific profile. It's once you start mucking around with immobilizers and other encrypted things that need the factory tools... Those can cost tens of thousands, and usually require continuous internet access back to the home office. reply Chris2048 11 hours agorootparentprevWhy can't electronic keys be duplicated / backed-up? reply renewiltord 16 hours agorootparentprevMy Ducati bike had immobilizers that would prevent the bike being started without the key or the per-bike code card. When it was stolen, the thieves tried all manner of things to start it, including drilling through the ignition keyhole. I managed to get it all fixed and the bike still ran. Without the immobilizer, someone else would be riding my bike. That's no different from this proposal. You just give them the keys, or the key card (or red key) if you've lost the keys. reply mywittyname 15 hours agorootparentprevSome of the tools used to steal cars are the legitimate tools used to repair cars. Key programmers aren't cheap, but at under $5k for decent ones, they aren't crazy expensive either. It pays for itself in one job. You could make these tools more difficult to obtain, but that won't stop the crime. Immobilizers and requiring a PIN to start the car are cheap, effective ways of preventing car theft without negatively impacting our ability to repair vehicles. It would behoove government agencies to include a list of anti-theft techniques on the window sticker and it would behoove insurance companies to be very upfront with the anti-theft features they think vehicles need. reply Reubachi 13 hours agorootparentprevI work in CA/PKI, particularly IOT device registration/security via TPM keys. I cannot imagine a scenario after years working with our own infra and clients where a car manufacturer would restrict access to the vehicle with a private key decryption on the FOB tpm, (that can't be exported or copied.) Lost/broke fob? 4000 pound paperweight, to no ones benefit. Insurance nightmare that would also be violating right to repair in many states (which is a different issue) . There SHOULD be a standard like every person has some device or process that is also a CA, who can then generate and dictate what keypairs can access a device, car etc. But we are very very very far away form that. reply ngneer 13 hours agorootparentprevAllow me to offer a different opinion. There is little sense in applying logical security when physical security is lacking. CANBUS should not be accessible by taking apart headlights. Communication buses must be protected from physical access, i.e., trip the alarm system or disable the car upon unauthorized access. There can be no logical security without physical security. reply 0x457 12 hours agorootparentIt would be very hard to make CANBUS inaccessible from headlights, since that what controls it. However, the headlight shouldn't be able to tell the rest of the system that the key is in the car. reply ngneer 10 hours agorootparentLogical compartmentalization like you suggest is a fine approach, but even better is to not allow physical access. Unless the car is in maintenance mode at the shop, the chassis should be sealed tight. Maybe the manufacturer decided to favor headlight maintainability over theft prevention, or was simply oblivious. reply skunkworker 16 hours agorootparentprevFrom what I've been seeing with Toyota and their ECU Security Key, it hasn't been cracked yet but it's close to being cracked and extracted from a running car and the private key extracted (so things that look at CAN bus messages can work again, like comma.ai) reply sleepybrett 10 hours agorootparentprevRight now many of the components of your iphone are paired to the phone through signing. It's a huge fucking pain in the ass, and it makes the whole 'right to repair' a huge can of worms. reply K0balt 14 hours agorootparentprevCANbus protocol makes this hard. Payloads are limited to 64 bits, to start with. But the payload for each message could be encrypted, even though secure key exchange would be difficult. Even so, it would be possible, I think. reply 0x457 12 hours agorootparentIt's so hard that (almost) every European manufacturer figured it out. There is also FlexRay. There is nothing interesting you can do with CANbus on new mercs. Even unencrypted CANbus messages go through gateways that (could) prevent headlights from reporting key presence. There is a reason that some cars don't have reasonable attack vectors (excluding parachuting the driver out of the car) and some can be started with a screwdriver (or slight more involved way with CANbus). It's not complexity, it's cost. reply hoofhearted 19 hours agorootparentprevThe people in the Infiniti groups were recommending this obd2 lock haha https://www.amazon.com/Tune-Saver-OBDII-OBD2-Lock/dp/B0BRF5D... reply irobeth 19 hours agorootparentseems like a sophisticated theft ring would have access to the keys for the most common guards like this, reminds me of the TSA key debacle[1] [1]: https://github.com/Xyl2k/TSA-Travel-Sentry-master-keys reply macNchz 18 hours agorootparentThe TSA locks have widely circulated master keys because that's a basic requirement of the system–every airport has to have some to be able to open bags. I don't know anything about these OBD port locks, but I don't see any reason they'd have a master key, other than laziness on the part of the manufacturer. Additionally, I'd imagine that such a tiny fraction of a percentage of cars have these kinds of locks that it'd barely be worth it for thieves to figure out how to bypass them, at least until there's more widespread adoption. reply kube-system 17 hours agorootparent> I don't know anything about these OBD port locks, but I don't see any reason they'd have a master key Look at it in the picture and the review pictures. They're all 'keyed' alike. It's just a single offset pin. Also one review says it just holds on with friction and can be pulled off with force. reply michael1999 13 hours agorootparentprevPut the powertrain lockout system on a signed and physically protected network segment. Let the headlights, mirrors, etc live on a less secure segment. This will impose higher costs when replacing these systems, because it will require key management of some kind. Either central cert management (with 20 year expiry?) or local key management. So only impose this on a tiny subnet for the starter/immobilizer. reply sonicanatidae 12 hours agorootparentSadly, this involves costs, so it'll never happen. Good take, though. reply cogman10 12 hours agorootparentprevYou don't protect the wiring, you protect the start protocol. Similar to asking \"Can we protect the internet by protecting the ethernet cables?\" Put a public key on the engine controller, have it challenge the key with a random start number, have the key respond with the signature of that number, engine starts. You can do that challenge over the can bus. reply amluto 18 hours agorootparentprevPerhaps the OBD port should only work when the car is validly unlocked and the engine immobilizer accepts a key? Maybe it could stay unlocked thereafter while a device is connected? Android (adb) and iOS (iTunes backup) have solved this issue years ago. reply sokoloff 14 hours agorootparentWhen I installed a remote starter on my old Jeep, I had to also install a CAN interface that would command a door unlock followed by a door lock command. That was enough to tell the ECM that it was okay to start the car by simulating the key switch closure for “run” and a temporary closure for “start”. Prior to adding the CAN interface, jumping “start” would set off the alarm. reply mysterydip 17 hours agorootparentprevInstead of technical/computational solutions, maybe there's a low tech cage/shell that can be put around it so anyone can't just plug in? reply sokoloff 14 hours agorootparentThe CAN+ and CAN- wires run around the car in, well, a bus and tapping into them anywhere is enough to inject CAN messages onto the bus. It makes it less plug-n-play than the OBD2 connector, but thieves will still be able to do it. reply punnerud 14 hours agorootparentprevSeems like the CANBUS is deactivated when the car is turned off on Volkswagen. Guess that is one way to fix it? reply numpad0 11 hours agorootparentYou can't unlock the car with the bus dead. CAN is not like switched Ethernet, it's a bus topology network like LAN over coax cables. They can be split or bridged, that's probably what they do. reply sokoloff 14 hours agorootparentprevOn some cars, hitting the door lock/unlock is enough to wake the CANBus. reply rolph 18 hours agorootparentprevplace the port in the lockable cabin of the vehicle instead of behind a headlight. reply rasz 17 hours agorootparentprevStart by not allowing thieves connect thru wheel well https://kentindell.github.io/2023/04/03/can-injection/ reply adolph 18 hours agorootparentprevAs I understand it, CANBUS is a message network among relatively low-power devices. There are two ways of doing this: + Some credential exchange between devices to establish a web of trust + Devices are locked similar to Apple parts reply millzlane 14 hours agoparentprevIt was easy enough to do with https://www.uprev.com/. We had a specialist shop in the same area. You can disable Security+ with uprev. Hell we would even use it to remove engines from nissans to make them run in whatever we put them in without the ignition. I can make the start signal just come from a momentary push button. reply rlt 17 hours agoparentprevLocksmiths can make new key fobs for nearly any car with access to the OBD2 port and the right software (though I don’t know if it requires a connection to the manufacturer) reply rasz 17 hours agoparentprevInfinite Infinity car hack, came with two, left with three Q50s. They do crouch an awful lot near front wheel well. Reminds me of this Toyota hack where thieves plug into headlight canbus wiring thru wheel arch https://kentindell.github.io/2023/04/03/can-injection/ reply aosmith 16 hours agorootparentThat's exactly what's happening. This is not a wireless attack, it's a physical access problem. reply SoftTalker 14 hours agorootparentHard to prevent theft of something parked on the street. Thieves can show up with a tow truck, hook the car, and drive away. reply junon 18 hours agoparentprevDamn. That's a wild video (wish there was a fast forward though). Curious how they did it. Is this a CAN bus back? reply bpoyner 19 hours agoparentprevThe answer is simple, we need to ban android tablets. /s reply hoofhearted 19 hours agorootparentFix the cars. A brand new $60,000 car shouldn’t be so simple to swipe. They probably spent less time stealing my neighbors car than he did waiting on the credit check to buy the car lol… it’s crazy these days with cars. reply newsclues 19 hours agorootparentCars are computers now. What do we know about computer security and physical access? If I can touch the machine, I can hack and own it. No level of technology will stop this. But cutting off the profit motive by making it very hard to export cars will have a massive impact on these crimes, and for old and new cars. reply acdha 18 hours agorootparent> What do we know about computer security and physical access? If I can touch the machine, I can hack and own it. It’s not the 2000s any more. Even national security agencies have trouble with phone decryption, and that suggests a path forward for cars using a tamper-resistant secure element since car thieves won’t spend more money attacking something than they can resell it for. Cars need service regularly you can have a way to replace a damaged SE which is more restricted so a legitimate owner can regain control of their stolen property - if you required, say, a government photo ID check for the owner on the title to reset the encryption keys, car thieves are highly unlikely to spend time getting high-quality fake ID since the odds of getting caught would go up dramatically, and you could deter shady auto shops by requiring them to submit proof of their ID verification for that service. reply newsclues 18 hours agorootparentThey get exported overseas and any technology lock or security device gets ripped out and replaced. This isn’t about extracting encrypted data, but bypassing systems to start a car. reply acdha 17 hours agorootparentYes, because the current design is lax. Now think about what happens if the engine computer won’t start with a bad signature or the entertainment system won’t work. How would that affect the overseas market? reply newsclues 16 hours agorootparentThe part that requires a signature will be taken out and replaced. Infotainment systems will get gutted and replaced with aftermarket ones. reply acdha 14 hours agorootparentAgain, all of those lower the value to the thieves. If they need to create a custom engine controller, they’re going to need to pay a lot more than the $0 they currently spend. If they need to replace the entertainment system, the cost of doing so will cut into their margin. Don’t make the mistake of thinking that a system needs to be perfect to be worthwhile. reply numpad0 11 hours agorootparentI think you're in a desktop computer \"whole product is one computer\" moddel. A car is a set of computers, almost nothing in a car is central to itself. There's probably a body controller ECU that ties into engine ECU and driver's key systems. So theives would just generate and flash a new key/cert, that'll be certainly possible. Infotainment? That's almost literally an aftermarket parts. American reviewers tend to see it as integral part of a car or even a central computer, surely it's important in terms of product experience but architecturally it's more like a printer over Ethernet than a laptop integrated display. reply acdha 9 hours agorootparent> There's probably a body controller ECU that ties into engine ECU and driver's key systems. So theives would just generate and flash a new key/cert, that'll be certainly possible. This sounds like the old desktop mentality you mentioned. You can’t just reflash things to bypass a secure boot process – the entire point is to prevent things like that! You’d design the driver’s key to pair with the various onboard systems and those systems to do a challenge-response cycle during the boot process so someone can’t easily drive away without the key or resell those parts, with both sides using a private key which never leaves that component. Yes, that kind of design can still be attacked but the goal here is to make it more expensive than it’s worth: needing a flatbed to take it somewhere for a rogue EE to work on it, for example, just isn’t going to make sense except for the most expensive luxury vehicles. This brings me to: > Infotainment? That's almost literally an aftermarket parts. Yes, and those cost money. The entire point is that you don’t need to make it perfect, just expensive. If someone has to replace the display and speakers, that means they’re making less profit on the sale and making it more obvious that the vehicle was stolen which increases risk and reduces the number of buyers, especially for the most valuable vehicles. reply caskstrength 18 hours agorootparentprev> What do we know about computer security and physical access? If I can touch the machine, I can hack and own it. Can you hack and own my fully patched Pixel phone? Or my GF's iPhone? Sure, sophisticated state-sponsored actors can sometimes do it by burning several million dollars worth of 0days in the process, but some two-digit IQ riff-raffs? Probably not so much. EDIT: just to be clear - by \"two-digit IQ riff-raff\" I meant OP's neighborhood car thieves, not you :) reply eertami 18 hours agorootparentPhone thieves will watch over peoples shoulders for them to input a passcode, which isn't that dissimilar to a lot of the replay/signal extension attacks. A lot of damage can be done and things successfully owned without needing to hack or exploit the device (car/phone). reply caskstrength 18 hours agorootparent> Phone thieves will watch over peoples shoulders for them to input a passcode, which isn't that dissimilar to a lot of the replay/signal extension attacks. You have any reference regarding how prevalent that is? Everyone I know switched to biometrics a decade ago. reply newsclues 18 hours agorootparentprevThis is done by organized crime with engineers on staff. Sure it’s drug addicts stealing cars but the people shipping them are smart and have access to capital. reply caskstrength 17 hours agorootparentI agree, but that brings us back to my original question: why can't same smart organized crime people unlock my smartphone then? Because Apple/Google give a damn about security and car manufacturers do not. reply fullspectrumdev 1 hour agorootparenta couple of years ago it wasn’t uncommon for victims of phone theft in the UK to end up flooded with iCloud phishing messages to try gain access to their iCloud account and unblock the device so it wasn’t totally worthless for resale. I still see a lot of iCloud phishing messages, but also understand that Apple has made this vector harder. reply ryandrake 17 hours agorootparentprevAlso: When your phone or computer is hacked, most people think \"Wow, the device is flawed.\" But when your car gets stolen, most people think \"Wow, we should stop those criminals.\" Apple/Google are incentivized to give a damn about security because incidents reflects poorly on their products. We need to start making thefts via security exploits reflect poorly on the car manufacturers and their products. reply newsclues 16 hours agorootparentprevEconomic incentives. People will buy a $150,000 SUV for 50k and they can still make money. Phones have less incentive and Apple is going to be better at bricking the phones than carmakers will. reply saint_fiasco 15 hours agorootparentprevApple and Google don't sell insecure cheap phones, but lots of other manufacturers do. I suppose organized crime doesn't systematically take advantage of that because cheap phones are cheap, and the people who own them are poor. You don't get that much benefit from pwning them. Alternatively, maybe organized crime does take advantage of them but we haven't heard about it. They could have a giant botnet of them for all we know. reply HeyLaughingBoy 17 hours agorootparentprev> No level of technology will stop this. Tell that to the FDA. I work in medical devices. It's no longer sufficient to throw up your hands and assume \"well, they have their hands on the device, we can't stop them from doing anything.\" The new cybersecurity guidance anticipates an attacker having physical access to your Device and you are expected to understand and mitigate any impact that can have. Cars shouldn't be any different. reply rjmunro 18 hours agorootparentprev> No level of technology will stop this. Why does no one steal Teslas? reply newsclues 16 hours agorootparentThink about where these cars end up, it’s not near a service centre. reply BrandoElFollito 17 hours agorootparentprev> What do we know about computer security and physical access? If I can touch the machine, I can hack and own it. You are damn good then :) One can protect against such that's by using well placed cryptography. reply Levitz 18 hours agorootparentprevThe level of technology that stops this is cars not being computers. Every piece of tech has tradeoffs, and for cars this one is just not worth it. reply HeyLaughingBoy 17 hours agorootparentWhat are you proposing? That we go back to being able to turn the ignition switch with a screwdriver? reply eertami 18 hours agorootparentprev> No level of technology will stop this. Except for you know, the technology of a physical car keys and an immobilizer. There's a reason it's the keyless entry start/stop button cars that are being targeted by thieves, it's simply so much easier. The frustrating thing is that new cars are being produced that _only_ offer keyless entry, and so eventually the choice is taken away or you have to drive a very old car. reply nemo44x 16 hours agorootparentprevOr make grand theft auto an offense that is actually prosecuted. Make hard penalties for violating another citizens by stealing their property. Start with 5 year minimums off the bat and every offense afterwards adds another 5 years. You'll see car theft plummet. reply blooalien 19 hours agorootparentprev\"Kill all humans!\" ~ Bender B. Rodriguez Headline the next day: \"Crime rate on Earth now at an all-time low of zero percent!\" reply phoe-krk 18 hours agorootparentHow do you calculate this value of zero percent? Divide all the crime cases of the previous day by the number of humans? And who wrote the headline? reply hansvm 18 hours agorootparent> How do you calculate this value of zero percent? This gets messy for obvious topological/continuity reasons, but a shocking number of applications are both correct and simple to reason about if you choose to define 0/0 == 0 (kind of like how if you choose to universally define sum(empty_set) == 0 and product(empty_set) == 1 then tons of higher-level formulae just work and don't have to special-case a base case). In context, there's no good reason to pick that definition of 0/0 per se (other than my prior that 0/0 == 0 probably simplifies some downstream math), but it's kind of nice to see that if crime is at 0% then there is also zero crime. > And who wrote the headline? Now we're asking the real questions ;) reply sokoloff 18 hours agorootparent\"Kill all humans except for one\" would be a way to resolve this (at least for 1-80 years). reply blooalien 12 hours agorootparentprev> \"And who wrote the headline?\" Probably a \"NewsBot\" of some sort? reply aosmith 16 hours agorootparentprevGiven what's going on in Canada maybe they should just ban OBD tools all together with the flipper. /s reply swozey 17 hours agoparentprevI will never, ever keep a car I care about outside anywhere near the city. I know everyone doesn't have the funds for that, but I'm sorry, we all know how rampant car thefts have gotten since before those 3 Q50s in this video were even purchased. I live in the busiest neighborhood in downtown Denver with which has rampant property theft, cats cut out etc non-stop. I own 2 vehicles and neither of them are ever parked outside if I can help it. It means I have to pay pretty much twice for rent because now I need a 1-2 car private garage, which means I'm probably now in a condo or townhouse so every expense just gets higher and higher. But you're in the bracket of living downtown with a brand new Q50. So I don't care what your excuse is, buying a luxury/attention-getter car and parking it outside in cities with rampant car thefts is just absolutely stupid. Especially the people who buy the $80k luxu-box with the $5k 22\" wheel add-on that gets ripped out of their mid-rise apartment parking garage a day later. I've had a car stolen and insurance does NOT treat you well when it happens and I never, ever want to deal with having a car stolen again no matter how much gaap/etc. I have. reply blitzar 17 hours agorootparent> I know everyone doesn't have the funds for that Its actually a fair bit cheaper to buy a $25k car than a $250k car. reply HeyLaughingBoy 17 hours agorootparentShhh! Don't tell anyone. reply hoofhearted 16 hours agorootparentprevSome of the issue here is that it’s actually a pretty nice area here in Baltimore, but our police force is currently understaffed and overworked. One big issue here regarding policing is that our city elected officials can’t tell the city police force what to do. You see, when the civil war broke out, the state took control of the police force so that the mayor couldn’t lead a confederate coup. Flash forward to today, and those powers still have never been returned to the city. The mayor and city council set the police budget, but the chief of police takes direction from a state run board. So there is a big disconnect between citizens voicing concerns to city council members, and those members only ability is to “talk to the major”. When the cats away, the mice will play off with some stolen cars. reply swozey 15 hours agorootparentIf you haven't traveled/lived in many major cities since covid, they are all the exact same now. None of the police are working. I'm in Denver now, previously Austin in 2019, Dallas 2020, Denver 2020+, and Denver banned qualified immunity so the police work even less. Seattle just did the same thing + IIRC king county is doing that \"police cant lie on stand\" or whatever law. I lived on 2nd and congress in Austin for 12 years until 2016 and the entire downtown has turned to absolute trash. I'm sure its the same in Chicago, LA, Portland, Tampa, etc and I don't even need to ask. reply fullspectrumdev 1 hour agorootparentThis sounds like a serious symptom of something being deeply fucked with policing in America. Qualified immunity doesn’t exist in other first world countries with effective policing, in fact, the police in America have a lot of latitude to do all kinds of insane shitfuckery that doesn’t fly elsewhere. reply 14 17 hours agorootparentprevIt would be trivial to hard wire a kill switch to your fuel pump and have it hidden somewhere so no matter what thief’s can’t drive off with your car. Much cheaper and more secure as cars can be stolen from parking garages. reply swozey 16 hours agorootparentPut a kill switch in it they tow it. Put a Club in it they tow it and cut off the steering wheel. Put GPS on it they throw it in a faraday cage paintshop/train. Put a Dronemobile system in it the Police just won't investigate/track it down. Really just have to not keep property outside anymore. I used to do the \"It's not a big deal, i have full coverage\" but had a car stolen and they (insurance) treat you like absolute trash when it happens. So no more outside for my cars as much as I can reply heleninboodler 15 hours agorootparent> had a car stolen and they (insurance) treat you like absolute trash when it happens. You've said this twice, but what does it mean? I have had my car stolen twice and the insurance company didn't give me any trouble at all and just paid out. reply 14 16 hours agorootparentprevLock it in your garage and now they break into your house and hold a gun to your head….ya maybe they tow it but not likely as they want to do this discreetly but at the end of the day of course if they were determined they could take anything. My point is a kill switch would stop 99% of theft. reply fargle 14 hours agoprevstupid on stupid. - it's incredibly stupid to ban the flipper zero because it's factually not even part of the problem - but it's equally stupid to \"ban insecure vehicles\". if kia makes a cheap car with crappy locks either don't buy it (because maybe insurance) or add and aftermarket immobilizer or a steering wheel lock. if it was really negligent of kia to \"save a couple bucks\", then it's equally negligent on you for not spending a couple bucks. - i also cringe at the idea that we throw the word negligent around when talking about failing to prevent other peoples crimes. i'm not negligent for not doing enough to prevent the crimes of some other asshole. nor is kia. meanwhile, there's sibling threads here that point out that the us is far to hard on the criminals. so wait - kia and me and other law abiding entities are \"negligent\", but the asshole who stole the car deserves compasion, etc.? - it's stupid-on-stupid-on-stupid to sit here discussing the problem of car thefts, caused by lack of enforcement of the existing laws against it, and the proposed solutions is making more things illegal (and arguing about which things). reply vsuperpower2020 10 hours agoparentNobody knows a vehicle is insecure when they buy it. It's simpler, more cost efficient, and more valuable to society just to require cars to have basic security features. Your idea of market correction doesn't work in this case, because it's never advertised as having shitty security, and the average (or even informed) consumer will have no idea this is a problem until after they've bought the car. reply Geisterde 7 hours agorootparentI never understand this arguement. I hear it in the form \"we should just regulate cars to be safer\", why dont you just buy a safer car? \"What do you take me for? I got a mustang GT, the last thing that car is worried about is safety\". Interesting, you bought a car because its fast, not paying any consideration to whether it could safely get you from point A to B, and this is what you rely on to get you to work? Its not social darwinism, the lack of critical thinking skills among the general population is alarming. Americans have apparently been coddled to the point that they arent worried about basic needs; if you go to buy a car you should have some simple considerations, is this car safe? What are the typical maintenance costs? Is it common for this car to be stolen? Things like, housing, transportation, education, those are really central aspects of peoples lives. Its all well and good that you want to draw symbols on paper and make all these things safe, but it appears to have come at a pretty serious cost. That cost, is the inability for the US population to use critical thinking. reply jolmg 1 hour agorootparent> I never understand this arguement. I hear it in the form \"we should just regulate cars to be safer\", why dont you just buy a safer car? I don't understand your argument. Do you expect every potential car buyer regardless of tech literacy to go to a dealership with an SDR setup on-hand, asking if they could test-out the key fobs, capture some signals, try to figure them out and look for common vulnerabilities on the new models? Do you expect the seller to explain the technical details of the key fobs at a depth you can be sure there's no vulnerabilities? Determining that the key fobs were implemented securely is not something the buyer or even the seller will be able to determine. I would imagine a regulator has the ability to inspect them as they're being designed/built. > if you go to buy a car you should have some simple considerations [...] Is it common for this car to be stolen? By the time vulnerabilities are found and people start stealing them, there should already be plenty sold and roaming the streets. reply pnt12 1 hour agorootparentprevDoing a security analysis of a car is a complex task that most laymen cannot do, so the argument for laws and compliance among specialists is quite reasonable. Put it another way: if it's all market based, your choices in buying just get 2x more complex. Is this car easily unlocked? Do the brakes often fail? Can it resist a collision? Or even another thought: since most people don't think about security, companies will flood the market with insecure cars. Want a secure car? That would be niche and cost you double. reply caconym_ 6 hours agorootparentprevWe should regulate cars to be safer because I don't get to decide which car (or, more likely, truck) the moron who puts me/my family in the hospital (or the grave) was driving. Given the trends in traffic and pedestrian deaths in the US, our regulators are grossly negligent in this regard. reply Geisterde 6 hours agorootparentSure, in the context of broad safety you have a point. Of course, you are more likely to die in places where police are responsible for your protection. The safest places people inhabit are their home, their work, places with private security measures. The negligence you speak of, how widespread would you say it is? Would you say the EPA dropped the ball preventing the east Palestine Ohio train derailment? Would you say that having some hard cutoff for how many chemical train cars until the train is reclassified? I imagine you would agree, that the chemical company sending 59 (oh w/e) train cars instead of 60 so that they are under easier rules, is kind of bullshit. If the company is following the rules then they can largely get away with it when things go wrong; if that company had actual liability for whats in the train cars, then they would have insurance. That insurance would be prorated on the safety profile of the train, and it would fix the hazzard or pay the claimants their due. These regulations only exist to cement decrepit companies monopoly over you. I have no faith that \"negligence\" in the public sector will improve. The roads, the enforcement of driving conduct, that will not improve. The safety, security, and reliability of cars will improve, because there is still competition from self aware drivers. reply bmicraft 5 hours agorootparent> The safest places people inhabit are their home, their work, places with private security measures. Weird takeaway. Those aren't primarily places with security measures, they're places where trusted people outweigh untrusted ones. reply jrflowers 4 hours agorootparentprevYou make a good point. The government should not impose rules about what companies do and also the government should hold companies liable for what they do, which is a different thing from having rules reply caconym_ 5 hours agorootparentprevI don't have much faith that our regulators are going to get their shit together either, but that doesn't mean I can't hold them responsible for the consequences of their negligence. As for the theory that the invisible hand of the market will drive better outcomes in traffic safety, that is obviously not happening in aggregate. It may even be having the opposite effect, given the unchecked proliferation of pickup trucks and large SUVs that are known to be more dangerous to pedestrians and the drivers of other vehicles. I'm not super familiar with the East Palestine derailment, but based on what I do know about it I think your 59/60 train cars example is sort of disingenuous. American freight rail operators are well known for skimping on safety in multiple dimensions; issues like obsolete brake technology, stretched-thin crews, insufficient maintenance, and excessively long/heavy trains have all been discussed to death before and after that accident. The problem isn't some off-by-one miscalibration, but rather that our regulators (and I'm including lawmakers under that umbrella, especially since they're usually the ones taking the bribes AFAIK) are simply not willing to demand that these industries put public safety over profits in areas where there are well-understood systemic safety problems. AIUI, efforts to improve rail safety during the 2010s and earlier have been largely stillborn, sabotaged by rail company lobbying. Just to illustrate what I'm talking about via another example: if the MCAS system that killed 346 passengers on two 737 Max aircraft within the span of a few months had been an honest design flaw rather than a hack devised to juice sales by dodging retraining requirements, cheaply and shoddily implemented ignoring the misgivings of Boeing personnel and rubber-stamped by the FAA, then it would have been a simple tragedy. As it actually happened, it's an atrocity—a failure on multiple levels by many people who ought to have known better, holes in the Swiss cheese all lining up, 346 lives sacrificed on the altar of avarice. That's where the invisible hand gets you when you let it take the wheel, but I suppose next you'll tell me that those people ought to have done their own safety audits of the aircraft that got them killed. reply vsuperpower2020 4 hours agorootparentprevYou shouldn't have to consider if the car's locks work, because working locks should be a baseline in cars produced in a first world country. It's that simple and easy. You aren't nearly as smart as you think you are and having basic standards of how corporations should operate isn't coddling. reply fargle 1 hour agorootparentprevif people cared, they would know i think this issue is overblown and is being used as a smokescreen for the rash of vehicle thefts caused, not by bad kia security, but large-scale organized crime. reply phpisthebest 9 hours agorootparentprevIn the age of information ignorance is no longer an option, Before I buy a car, most often the second largest purchase a person will make in their life next to housing. I do i TON of research, I look at insurance rates, I look at Theft Rates for that model, I look on Car Complaints and other Database for common failure items for that model, I have it inspected by a independent mechanic having them pay extra attention to the common failure items. etc If you just roll in and let the salesman take you for a ride then you deserve the outcome. reply dog_boxer72 9 hours agorootparentYes anyone who doesn’t have the knowledge/time/motivation/cynicism to prevent themselves getting taken advantage of is basically asking for it, nay “deserves” it. /sarcasm reply YeBanKo 8 hours agorootparentYeah, and if you did not read T&S and now going to become a part of human centipede, that’s on you. I mean, how hard can it be to read a 22 page legalese, before going through a sign up flow, that was heavily optimized to increase conversion? reply fargle 6 hours agorootparentprevanyone who thinks there is anybody in the universe other than themselves that is going to take responsibility for their safety, security, happiness, etc. absolutely \"deserves\" what they get. reply bmicraft 5 hours agorootparentNobody can or should be expected to know about every the safety and security aspects of every single minute detail in their lives. reply fargle 5 hours agorootparenti didn't say they did! i said they need to take responsibility for their own safety/security or suffer the consequences. whether they should be expected to... is totally irrelevant. i'm not stating a preference, i'm stating a fundamental law of nature. and not knowing even that simple fact is what makes it \"deservedly\" so. reply Dylan16807 3 hours agorootparentRegulation can remove those consequences for any chosen safety/security feature by making every choice have it. Fundamental law of nature? You're deluding yourself. (And if you say you mean outside of regulation, that people need to be responsible in general for other aspects of life, then your argument is no longer connected to the original comment you replied to.) reply fargle 3 hours agorootparentregulation is part of the universe. to expect that it protects you exactly when you'd want it to, but does not inhibit you want you'd not want it to is stupid. trying to offload your responsibility onto some \"them\" is not a fix. i'm definitely not deluding myself. that is life. you need to have both the freedom and the inclination to take care of yourself, if you don't have both you'll suffer. reply Dylan16807 2 hours agorootparentI do not need the freedom to buy a defective lock. Mandating basic safety and security features is not always going to protect me, but it will mostly protect me. It's not stupid to want that tradeoff. I don't care if you define \"fix\" as 100% so therefore it's not a fix. I want the 95%. I want defense in depth, regulation on top of personal investigation. reply fargle 1 hour agorootparentyou are right. you do NOT need the freedom to buy a defective lock. you need the right to decide for yourself if the lock is defective or not. if you give that away, you will instantly be given the \"freedom\" to buy a lock that is defective-by-design. perhaps the lock designer's brother is a friend of the govt. perhaps the govt. agency does not want bad publicity, whatever. the point is \"defense-in-depth\" (cliche) or not, you are ultimately responsible for you. there can be no other way. reply Dylan16807 1 hour agorootparent> you are right. you do NOT need the freedom to buy a defective lock. > you need the right to decide for yourself if the lock is defective or not. This sounds like you agree with me. This kind of regulation sets a minimum, not a maximum. We don't need freedom to buy very bad locks. We do need freedom to buy the best lock we want to buy. But the rest of your post implies that regulation will change both minimum and maximum and mandate a specific lock. I disagree with that premise. > (cliche) Are you trying to imply something there? > you are ultimately responsible for you. there can be no other way. I am \"ultimately\" responsible, but product makers should have responsibilities too. If I fail at something, I should not be 0% safe. The baseline should be pretty high before I apply my own efforts. reply YeBanKo 8 hours agorootparentprevShould we allow cars without seatbelt? Everyone knows cars with seatbelts are safer. If consumers don’t like it, they can just choose to buy the ones with seatbelts. reply danielheath 8 hours agorootparentprev> In the age of information ignorance is no longer an option The age of information was great. In the age of misinformation, knowledge outside your specialty is no longer cheap enough to reliably obtain. reply Geisterde 7 hours agorootparentWhat world do you live in? The statistics are pretty clear and accessible. Are you so obsessed with something you cant attend to your own basic needs? reply fargle 6 hours agorootparentprevso in what age was ignorance a good option? reply vsuperpower2020 8 hours agorootparentprevCar thefts are extremely dangerous for everyone on or near the road. It's obviously better to just not allow car manufacturers to neglect basic security practices. There's also entire categories of issues you don't have to research anymore because they've been optimized out of every modern car. Soon cars being hacked with toys will be added to that list for you. Notably, \"airbag explosion rate\" wasn't on that research project of yours. reply electriclove 6 hours agorootparentIt's obviously better to just kill anyone that steals a car. I doubt anyone would try to steal a car after a few examples have been made. reply CivBase 8 hours agorootparentprevDo you look up the software security measures implemented by the keyfob too? That information would be very difficult for a layperson to find and make sense of. This may be the \"age of information\", but information is only useful as your ability to find, understand, and evaluate it. reply Geisterde 7 hours agorootparentLets see, oooo tiktok, heres the kia challenge. Or maybe google \"are kias secure\". Whatever format you can understand, you will be presented several sources that explain the situation quite clearly. reply Dylan16807 3 hours agorootparentAnd if you googled that three and a half years ago when you were actually buying the car? I bet you had to know exactly what to look for, and \"problems with kias\" wouldn't get the average person there. reply the_optimist 9 hours agorootparentprevShould other physical objects also be subject to this same regulation? What about bitcoins? Your proposed response is unsuccessful as policy reasoning. reply vsuperpower2020 8 hours agorootparentMy proposed response has been making cars safer and more reliable for years. I don't care if it applies to bitcoin or not, we're talking about cars. Next! reply Retric 8 hours agorootparentprevThe ability to leave your car in a public space while shopping is a major part of their utility. Cars are therefore common, left in the open, and valuable, but not particularly portable without being turned on. Jewelry, TV’s, bitcoin wallets etc aren’t just left in the open. Your house, front lawn etc is valuable and accessible but not generally mobile. reply dlgeek 7 hours agorootparentprevYes, and they generally are. In almost all states, sellers have to ensure that the things they sell are \"fit for purpose\". It's reasonable for us to ensure that they meet the basic requirements of being whatever they are - and for cars, part of their basic purpose is to sit in insecure areas and ensure only authorized users can operate them. reply rale00 13 hours agoparentprev> if kia makes a cheap car with crappy locks either don't buy it Immobilizers were a standard feature on cars for decades. If you went to buy a car, no one was putting immobilizer on the list of features, and they certainly wouldn't let you try breaking the ignition lock on a test drive. If they had advertised that their vehicles were insecure, then sure, it's on the buyer, but they didn't. reply FerretFred 12 hours agorootparentHow about Jaguar Land Rover making expensive cars with allegedly crappy locks? https://www.mirror.co.uk/news/uk-news/range-rover-owners-str... reply fargle 7 hours agorootparentprevit'd be bad to advertise that they have in immobilizer or anti-theft when providing either nothing or a badly broken implementation (like you often see in IOT). it's not negligence to simply not provide a feature they didn't promise to provide and weren't required to (in the US). it is simply not their responsibility in any way to ensure your car's safety from theft. if you assumed it was and that they provided a feature you wanted because everybody else usually does, then the negligent party would be you for not RTFM. except that's wrong here too. nobody is negligent here. you do not have a social responsibility to have an immobilizer on your car to prevent it from being stolen. and neither does the manufacturer. having it locked is plenty to legally make it \"breaking-and-entering\". and even if you leave the keys in the car and the engine running, it's still grand theft and your insurance will indeed pay out, which they would not do if they could claim negligence. the criminals are 100% at fault here. and bad things can happen without someone being negligent. arguing about anything beyond that is just a fight about how good that anti-theft system has to be. are you negligent if you don't have an armed guard on your car? reply philistine 6 hours agorootparentprevThe problem with Kia cars not having immobilizers is wholly american. It is illegal to sell a car in Canada without an immobilizer. reply fargle 2 hours agorootparentfunny thing is the part where this article is about a canadian car theft epidemic... reply creaturemachine 13 hours agoparentprevTik-Tok-inspired Kia thefts weren't a problem in Canada because they've required immobilizers since 2007, something Kia skimped on for the US market. reply sundvor 7 hours agorootparentSpeaking as an outsider: How are Kias sales going these days? How's their reputation as a result of this? Imo for removing security for the US market they deserve to be properly thrashed and dragged through the mud, regardless of the fact that they are offering upgrades from free if I read the following correctly. https://www.nhtsa.gov/press-releases/hyundai-kia-campaign-pr... It's not exactly an over the air \"recall\", and I understand a huge number are still out there unprotected. reply alexb_ 13 hours agorootparentprevnext [24 more] [flagged] franga2000 13 hours agorootparentThe manufacturer is not the victim here, the buyer is. If I pay a contractor to install a new door and lock on my apartment and it turns out they did a terrible job which made it trivial for a thief to break in, the contractor should be liable. Crime exists, this is the world we live in. Failing to implement even the most basic security measure, which is considered industry standard, in a high-value product that is known to be very attractive to thieves and then selling that product to consumers with no warning that \"unlike most other cars on the market, which have many layers of security features, this car can be stolen using a cheap toy\" makes the inevitable thefts absolutely Kia's fault. It's not like people are saying the thieves did nothing wrong, both sides are at fault: the thieves stole people's cars to enrich themselves and Kia secretly omitted a basic security feature which in turn enabled thousands of fully predictable and preventable thefts from their customers, again, to enrich themselves. reply electriclove 9 hours agorootparentBut people ARE saying the thieves are not at fault.. “it’s just kids”, “we don’t want to put them in jail because that would ruin their future”, “they can’t pay a fine anyway so there is no point in going after them” reply franga2000 3 hours agorootparent\"We shouldn't put dumb kids into cages and forever brand them as criminals for making one bad decision\" is a far cry from \"they are not at fault for what they did\". Also, insert the usual point about how many people are forced into crime by poverty and the complete lack of a social safety net in the US here. reply EasyMark 7 hours agorootparentprevNo one is saying that, there are multiple contributors to the problem here, sure the thieves own the bulk of it, but manufacturers could fix the issue or offer a solution like a cheap install of an immobilizer. reply electriclove 7 hours agorootparentIn my left leaning workplace in my left leaning state, most colleagues do not want to assign the thieves any blame or have them face any consequences. These colleagues only want to blame and penalize the manufacturers. I think the manufacturer should fix things so they are more in line with other manufacturers. But I also want to see some repercussions for the thieves. reply electriclove 6 hours agorootparentCan the downvoters explain why they disagree with: \"I think the manufacturer should fix things so they are more in line with other manufacturers. But I also want to see some repercussions for the thieves.\" reply Dylan16807 3 hours agorootparentThat's obviously not the sentence people are disagreeing with. There is no way your colleagues want the thieves to not face any consequences. It's a completely different thing to not trust the justice system to give an appropriate consequence. reply AnthonyMouse 12 hours agorootparentprev> Failing to implement even the most basic security measure, which is considered industry standard, in a high-value product that is known to be very attractive to thieves and then selling that product to consumers with no warning that \"unlike most other cars on the market, which have many layers of security features, this car can be stolen using a cheap toy\" makes the inevitable thefts absolutely Kia's fault. I don't think this logic works. If you buy a classic vehicle, they don't have these kinds of things either. People make replicas that likewise don't. And there is no clear line here. Basically any car can be stolen by, if nothing else, replacing the car's computer with one that accepts the thief's key. Meanwhile a car is a large purchase where people can reasonably be expected to do some research. If you're about to buy a car you should read some reviews, and the reviewers should tell you if their security is bad. Then you know and can make your decision. People who learn of this may want to buy a different car, or take some other countermeasures if they buy this one. Kia doesn't have any kind of a monopoly in this market. There are many other carmakers. Maybe you don't care that their security is bad because you always park your car in a garage. Maybe you like the discount you got because other buyers wanted a car with better security. Why does it have to be illegal, instead of letting the market sort it out in the presence of actual competition? reply filoleg 11 hours agorootparent> If you buy a classic vehicle, they don't have these kinds of things either Not a good analogy, because buying a classic vehicle automatically waives a bunch of safety and other features that are not only expected in modern day, they are straight up legally required. A car manufacturer cannot remake a classic vehicle from the 80s and release it in the US in 2024. Or, probably, EU too, I cannot speak for that due to my unfamiliarity with vehicle laws there, but afaik they are more strict than the US. It would be just illegal to sell that car. Thin pillars that won’t pass any modern safety tests, no backup camera (which makes it illegal to sell as a new car in the US), not enoug crumple zones, etc. reply eep_social 10 hours agorootparentThe standard dodge in the US is to sell “kit cars” which require the buyer to do a bunch of paperwork to get a VIN. I don’t think they can be sold ready-to-drive but I think there are dodges there too (owner tightens last bolt style). The details vary by state. Looking for details, I found that there have also been recent changes to ease requirements for small-batch ( Kia doesn't have any kind of a monopoly in this market No need for a monopoly, just bad incentives. All manufacturers could just decide that it's better to save more money and omit basic security features across the entire industry, making it impossible to buy a new car with certain standards. What are people going to do, not drive any cars? That's why it's near impossible to find a printer that's not garbage. reply franga2000 11 hours agorootparentprevThis all assumes the \"perfect information, even playing field\" theory that capitalists love to use but is completely unrealistic. Reviews rarely talk about things like this, this information is not explicitly given to reviewers or customers and neither can be expected to find out on their own (i.e. by trying to hack the car themselves), the car manufacturer spends insane amounts of money advertising to the buyer using every psychological trick in the book, the buyer is often under time pressure, the savings from cost-cutting are rarely passed down to the consumer... Buying things in the current market landscape is a battle, not an optimization problem. reply deepsun 13 hours agorootparentprevJust use \"passw0rd\" everywhere. It's the fault of a hacker who steals your account, not your fault. Every single time. Especially that no security is absolute. Effort matters. reply sonicanatidae 13 hours agorootparentprevYou are conflating \"what's right\" and \"how the world actually works\". Trust me. I have similar issues, to a clinical level, in fact. Does this sound familiar? https://en.wikipedia.org/wiki/Obsessive%E2%80%93compulsive_p... reply RHSeeger 13 hours agorootparentprev> If I left a million dollars out on my front porch, and someone stole it, that would not be my fault in any sort of way It's possible for multiple people to share the blame for something. You _are_ the victim. The person who stole it _is_ the bad guy / criminal. But you _both_ share the blame, because you did something to put yourself at risk when you had better options. If I'm out late at night, wearing expensive jewelry and have 2 ways home; one longer but down a well lit road, the other shorter but through a dark alley in a crime ridden neighborhood; and I chose the dark alley and got mugged... I would be the victim AND be partially to blame for making a stupid choice. Making choices that put yourself at risk by ignoring the realities of the world, when you don't need to, mean you share the blame. reply madmountaingoat 12 hours agorootparentSo you're saying that women wearing sexy clothing are to blame for rape? reply RHSeeger 5 hours agorootparentI'm saying that it's possible to both be the victim and to have made a stupid choice that made you winding up as the victim more likely. And, clearly, that it's possible for you to be unable to understand such subtleties. reply pierat 8 hours agorootparentprevGoing to the frat house, with a skimpy dress and no underwear (read: indications of the mating signal), getting rip roaring drunk... Is a bad situation you're putting yourself in. Nobody deserves being raped. But there are definite situations that you could have chosen discretion and not made your body present. reply elzbardico 12 hours agorootparentprevapparently, only on dark alleys at night. reply the_optimist 9 hours agorootparentprevA strongly held opinion here seems to be that clothes manufacturers are to blame. reply electriclove 9 hours agorootparentprevExactly! The people holding these views are unable to see their hypocrisy. reply eptcyka 13 hours agorootparentprevNominally, you are correct, but if we can collectively make decisions that decrease the risk of theft, is it not immoral bot to take action? reply electriclove 9 hours agorootparentprevWhy aren’t you downvoters blaming the perpetrators that are STEALING cars?!?? reply foxyv 14 hours agoparentprevThe problem occurs when a vendor makes claims that are false or fails to disclose known issues. I don't think either insecure cars or security tools should be banned. However, I think disclosures should absolutely be made. reply bhickey 9 hours agoparentprev> if it was really negligent of kia to \"save a couple bucks\", then it's equally negligent on you for not spending a couple bucks. if kids didn't want lead in their apple sauce they'd start their own testing labs. reply from-nibly 7 hours agorootparentGood thing the government has those testing labs and prevented that from ever happening. reply grubbs 14 hours agoparentprevThe last point is a hard one when the perpetrator is a 11 year old kid who watched a TikTok video online on how to steal a Kia/Hyundai. reply electriclove 9 hours agorootparentPunish the kid’s parents.. (oh wait, there might be a problem here) reply fargle 6 hours agorootparentnot hard at all. where is CPS? obviously someone isn't giving an 11 year-old appropriate supervision at all if they are driving, let alone stealing a car. there absolutely should be consequences for both the parents and the child in this situation. i'm not saying one mistake, send the kid to prison, take away parental rights. but it should be severe enough to matter, including several thousand dollars restitution. reply Dylan16807 2 hours agorootparentThanks for your contribution to the total destruction of outside roaming and play for children. It is perfectly fine to let an 11 year old walk a couple blocks unsupervised. And if they steal a car at that point, it is not a supervision problem. reply fargle 2 hours agorootparentdude, i am 100% all for free-range-children. but, somehow, i manage to ensure that none of them are stealing cars at 11 \"it is not a supervision problem\" hmm. maybe i should have said \"parenting issue\"... look, i am blessed to have had good kids. but i have had friends with struggles. if i had a kid that somehow stole a car (at 11) and i had to pay for repairs and meet with CPS and deal with courts and fines and maybe even juve and a lost year of school, we'd get through it. reply jjav 12 hours agoparentprevIndeed, it is dumb to ban anything. A tool is a tool, it doesn't make the product weak, it already was. Also it is silly to ban insecure cars, that's quite the slippery slope. If the cars are too easy to steal insurance will increase accordingly and that will provide incentives to fix that without banning anything. reply WWLink 12 hours agorootparentHrmm I wonder what would happen if I made a bank that used an unencrypted website for online banking lol. The problem with your solution here where the insurance company raises rates... yea they already did that with regards to Kia/Hyundai cars and Kia Boyz thefts. The problem is, well, put it this way... The last time you bought a car, did you check that the car had immobilizer software/hardware present on it? They don't really advertise that stuff anymore. About the only way you'd know on some brands is a nondescript red dot that shows up for a moment when you start the ignition. Really, I'd bet a lot of people only found out their car didn't have an immobilizer feature until their insurance company dropped them or jacked their rates up... and that's a problem. See, you can buy a car NOW, and everyone thinks it's a good safe car.. until it turns out it wasn't. reply fargle 6 hours agorootparentprev> If the cars are too easy to steal insurance will increase accordingly that's exactly right. i was somewhat surprised that insurance was outright dropping people instead of simply increasing rates. and by the way, you can get a discount if you add x/y/z security alarm/immobilizer. the public outcry already has forced the issue with kia anyhow. reply whycome 13 hours agoparentprevLet's say a hardware exploit for iPhones becomes obvious and is spread through social media. Something absurd like \"attaching a shorted iphone cable\". Are you going to be the first to buy an add-on lock or immobilizer? And everyone should also have to purchase an add-on? reply jrockway 12 hours agorootparentI'd expect Apple to refund the cost of the phone and mail a box to send the faulty device in for recycling. Making a defective product should not be free. reply sonicanatidae 13 hours agorootparentprev>And everyone should also have to purchase an add-on? Yes! -Apple reply badgersnake 11 hours agorootparentWhen the iPhone 4 came out and antennagate happened, they gave everyone a plastic case for free. reply ddalex 10 hours agorootparentI thought that they teached people to hold it right reply sleepybrett 10 hours agorootparentprevIt's called a recall, it happens all the time. Ask elon, lol. reply ClumsyPilot 12 hours agoparentprev> i'm not negligent for not doing enough to prevent the crimes of some other asshole. If you entire job is selling locks and they don’t prevent crime, then it’s not negligent, it’s fraudulent. You want to be in the clear? Sell a car without a lock, see how many people buy that. > if kia makes a cheap car with crappy locks either don't buy it And if Boeing makes a cheap, unsafe plane, don’t fly on it I would be happy to run this experiment if lying to a customer about safety/properties of your product led to capital punishment. But currently companies will simply defraud you by lying about their product, and suffer no consequence reply gridspy 8 hours agorootparent> If you entire job is selling locks and they don’t prevent crime Does MasterLock making famously easy to pick / rake locks count? I'm sure they reduce crime compared to no lock but they are not as secure as the customer expects. https://www.art-of-lockpicking.com/how-to-pick-a-master-lock... reply fargle 6 hours agorootparentprev> And if Boeing makes a cheap, unsafe plane, don’t fly on it yes. exactly. if boeing ever makes a cheap plane, i would definitely avoid it. you are comparing a company that cheated on legally mandated safety requirements with a company that didn't put a non-legally required car immobilizer on a lot of their new cars. and then didn't lie about it. > But currently companies will simply defraud you but kia didn't do this reply paganel 10 hours agoparentprevNobody is mentioning about how this is a social problem with the US that needs fixing, for example I often times forget to lock my car's doors in the Eastern European capital where I'm living and yet I've never had anyone \"steal\" stuff from it. But I get it, it's easier to think about applying technological or even legal solutions instead of thinking about how to fix a societal problem. reply electriclove 9 hours agorootparentYup, and you get downvoted for even trying to discuss it. Need the Overton window to shift slightly so it can be discussed on HN. It is a societal problem and I hope for our future we can fix it. reply rmauge 13 hours agoparentprevIgnoring the strawman of an assailant deserving compassion or not, that’s a self serving and narrow definition of negligence. Any mechanism to protect from misuse has to weighed against the magnitude harm of the event occurring and the possibility of misuse. I would not expect my asset manager to have weak authentication systems to access my portfolio but don’t expect any at all from a free online game. I expect both of these to consider the threats and make reasonable choices. And they would be negligent if they did not do this exercise. Whether is an active threat or a passive act of god. reply devmor 9 hours agoparentprevDo you believe that consumer protections should not exist? reply fargle 3 hours agorootparentyes, almost. for example, if a company made a car alarm called \"SUPER EXTRA SECURE ELITE++ V5\" and told me it had a \"guaranteed thief proof\" immobilizer. but then we find that a viral Tik Tok video shows how to with a hairpin and spit we can completely disable it and in 5 seconds and take the car for a drive and access the owners credit card info. and then also the car often bursts into flames while parked and turned off. and we of course find out that this was no \"oops\" and the corporations involved full-well knew about these issues and hid them to get a bonus. well, that'd certainly be a job for consumer protection laws. but this is a case of \"you got what you paid for\". there's a place in the market for crank-up windows and basic plain cars without keyfobs and fancy alarms. that isn't wrong, and it definitely isn't \"negligence\" just because other carmakers pick different places in the market. and the fact that criminals do bad things doesn't change that. and, thank you very much, i don't need consumer protection against that kind of thing. let's start with the lying and cheating corps and work our way up to collusion and price fixing. then let's get onto repair... reply Dylan16807 2 hours agorootparentFobs are very cheap. There is no reason to want cars without them. reply fargle 2 hours agorootparentnone of your business what i want, honestly whether or not it makes sense to you reply Dylan16807 1 hour agorootparentDid I imply what you want is my business? Though your other comments make it sound like you don't want a car without an immobilizer, in which case it feels like you're manufacturing something to disagree with rather than making a real argument against what I said. reply fargle 1 hour agorootparent> Did I imply what you want is my business? yes. if that's not true, i retract. what i want is: - it's not an act of \"negligence\" when neither the owner nor the manufacturer choose to not include an extra security feature, even one that a majority of other cars have. - it's nobody's business what \"people ought to want\". so that means an arguments based on \"nobody should...\" or \"no rational person...\" and \"there's nor reason that...\" and so on are invalid. reply Dylan16807 1 hour agorootparent> - it's not an act of \"negligence\" when neither the owner nor the manufacturer choose to not include an extra security feature, even one that a majority of other cars have. I don't think this gets to count as \"extra\". > - it's nobody's business what \"people ought to want\". so that means an arguments based on \"nobody should...\" or \"no rational person...\" and so on are invalid. I'm making a general claim about value, nothing personal. Because of that, I don't think it should matter whether wanting it is \"nobody's business\". But in particular, I'm saying there's no benefit to avoiding an electronic key. We can remove the word \"want\" entirely. The downsides are large and the implementation cost is a rounding error. This differs from power windows and \"fancy alarms\". reply joe_the_user 12 hours agoparentprevSure \"don't ban anything\", if your car crashes and kills you, \"should have read Consumers' Reports\". Those botulism eggs? Keep an eye things, damn it. /s This ill-informed attitude goes over well here unfortunately. And security may not be quite as pressing safety but poor security cost society besides costing the individual. When poor workers can't get to work 'cause stolen car, their bosses also suffer, when stolen cars are used in further you also get a social cost. etc. reply the_optimist 9 hours agorootparentYou provide no structural basis or reasoning for these cynical assertions, nor for the implied responses. Seems to be founded on a philosophical foundation of individuals requiring safety from “elsewhere,” and assuming that “elsewhere” actually provides it. reply gepeto42 14 hours agoprevOne of the authors here. Someone just told me we were on the HackerNews front page, made me happy we just went with a static website on GitHub pages. I will go through the comments later, but for now, if you are Canadian, please get in touch with your MPs. I am working with some media as well for additional coverage in the next week, but if you know Canadian journalists that might be interested in this, please get in touch with them, educate them directly if you want or send them to me (my LinkedIn is in the signatures, the first two names in bold = authors). Thanks for helping this story reach more people. reply gchamonlive 19 hours agoprevSecurity flaws are not born equal. I think there is supposed to be a clear distinction between flaws inherent in technology -- since you only know what you know nobody should be expected to develop impenetrable digital fortresses since that doesn't exist and would actually be harmful for the consumer -- and those flaws born out of neglect. The latter should be specified and treated accordingly, because it isn't a valid excuse that technology can't be 100% secure that the industry should accept poor standards. Also, Flipper Zero can be made DIY, so I don't know if I get it, but the law will be DOA, and actually work against the democatization and awareness of such flaws by the public. reply nonrandomstring 17 hours agoparent> Security flaws are not born equal. Absolutely. And let's bring risk into this. Security risks are not born equal. Serious security thinkers evaluate according to factors of likelihood, impact, mitigation cost etc. A car is a dangerous weapon, especially in the hands of a group of giddy kids, maybe drunk or way too high to drive. The likelihood of someone getting seriously injured or killed by joyriding is high. It's really high. And there's no mitigation to a dead child. The penalty? A very firm \"please don't do that again!\" But then a kid like Aaron Swartz downloads some files and gets nine felony counts totalling 50 years in jail and a $1 million fine. A justice system with these values has no concept of risk and proportionality and is beneath contempt. reply Terr_ 12 hours agorootparent> especially in the hands of a group of giddy kids Also the scenario where it's being used as a disposable battering-ram to smash into a store. (As you might expect, those are the stolen cars with lesser potential resale value.) reply hcfman 18 hours agoparentprevEurope expects you to. Otherwise you will be fined 15,000,000 euros. Thank you cyber resilience act. reply joe_the_user 12 hours agoparentprevYour point are generally good. I should say I drive a twenty year old car with an immobilizer chip and basic logic sounding the alarm when someone breaks a window to open a door. As far as I can tell, that makes it very secure. So it seems like the onus in the car manufacturers to create a vehicle at least as secure as this simple system. reply whiterknight 18 hours agoparentprevGuns can be made DIY, but laws still mitigate. reply gchamonlive 18 hours agorootparentThere is a big difference in putting together deadly artifacts and electronic devices you can fabricate using off-the-shelf chips and open protocols. Not saying you can't discuss regulating them, but to me they are in a different set of categories. Weapons are by default dangerous, their sole purpose being to cause physical harm, while a flipper zero can be used for instructional purposes and research. As much as I hate the concept, it would be ridiculous for me to propose regulating Alexa because a kid can cause financial harm to the parents using it, but a weapon can't be in any imaginable circumstance reachable by anyone untrained. reply engineer_22 18 hours agorootparent> but a weapon can't be in any imaginable circumstance reachable by anyone untrained. I agree with your main point that the FZ is easily reproduced. I think you miss the mark with this one. Firearms are easily made at home with simple tools and off-the-shelf materials. For example, the United States has a rich tradition of home-made firearms. To provide a concrete example, a shotgun can be made with a length of steel plumbing pipe, electrical tape, a nail, and a cap. Yes, it's that simple. reply gchamonlive 17 hours agorootparentWell that is not my main point in the comment you responded to reply engineer_22 16 hours agorootparentIf I understand now, your argument is that flipper zero is not a social danger, while firearms are, am I correct? reply gchamonlive 16 hours agorootparentIf by social danger you mean I would be really impressed if you managed to throw a flipper zero into someone and kill him, then yes that is the gist. It's a matter of degree. reply engineer_22 15 hours agorootparentok - well I agree with you. You had a good comment, and I appreciate it :) reply gchamonlive 13 hours agorootparentcheers! I also liked to be put to test in wha",
    "originSummary": [
      "SaveFlipper.ca opposes the federal government's plan to ban security research tools like Flipper Zero, deeming it unnecessary and harmful to national security and innovation.",
      "Advocates for collaboration rather than a ban, arguing against the policy that could stifle the Canadian economy and result in legal disputes, as criticized by a range of cybersecurity experts and professionals from diverse organizations.",
      "The professionals represent various roles in the tech sector, highlighting different perspectives on the potential ramifications of the proposed ban."
    ],
    "commentSummary": [
      "The debate revolves around Flipper Zero, a security tool, its potential for illegal activities such as car theft, and the discussion on banning insecure vehicles versus security tools.",
      "Suggestions are proposed for enhancing car security, employing advanced technology for theft prevention, and emphasizing physical security measures to deter theft.",
      "The importance of regulatory measures to safeguard public safety, the accountability of car manufacturers in delivering secure products, and the repercussions of car theft are also deliberated."
    ],
    "points": 1358,
    "commentCount": 797,
    "retryCount": 0,
    "time": 1708514449
  },
  {
    "id": 39453271,
    "title": "Introducing Gemma: State-of-the-Art Open Models for Responsible AI",
    "originLink": "https://blog.google/technology/developers/gemma-open-models/",
    "originBody": "Developers Gemma: Introducing new state-of-the-art open models Feb 21, 2024 3 min read Share Twitter Facebook LinkedIn Mail Copy link Gemma is built for responsible AI development from the same research and technology used to create Gemini models. J Jeanine Banks VP & GM, Developer X and DevRel T Tris Warkentin Director, Google DeepMind Share Twitter Facebook LinkedIn Mail Copy link At Google, we believe in making AI helpful for everyone. We have a long history of contributing innovations to the open community, such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold, and AlphaCode. Today, we’re excited to introduce a new generation of open models from Google to assist developers and researchers in building AI responsibly. Gemma open models Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.” Accompanying our model weights, we’re also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models. Gemma is available worldwide, starting today. Here are the key details to know: We’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-trained and instruction-tuned variants. A new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI applications with Gemma. We’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks: JAX, PyTorch, and TensorFlow through native Keras 3.0. Ready-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging Face, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma. Pre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud with easy deployment on Vertex AI and Google Kubernetes Engine (GKE). Optimization across multiple AI hardware platforms ensures industry-leading performance, including NVIDIA GPUs and Google Cloud TPUs. Terms of use permit responsible commercial usage and distribution for all organizations, regardless of size. State-of-the-art performance at size Gemma models share technical and infrastructure components with Gemini, our largest and most capable AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for their sizes compared to other open models. And Gemma models are capable of running directly on a developer laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key benchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical report for details on performance, dataset composition, and modeling methodologies. Responsible by design Gemma is designed with our AI Principles at the forefront. As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets. Additionally, we used extensive fine-tuning and reinforcement learning from human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To understand and reduce the risk profile for Gemma models, we conducted robust evaluations including manual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous activities. These evaluations are outlined in our Model Card.1 We’re also releasing a new Responsible Generative AI Toolkit together with Gemma to help developers and researchers prioritize building safe and responsible AI applications. The toolkit includes: Safety classification: We provide a novel methodology for building robust safety classifiers with minimal examples. Debugging: A model debugging tool helps you investigate Gemma's behavior and address potential issues. Guidance: You can access best practices for model builders based on Google’s experience in developing and deploying large language models. Optimized across frameworks, tools and hardware You can fine-tune Gemma models on your own data to adapt to specific application needs, such as summarization or retrieval-augmented generation (RAG). Gemma supports a wide variety of tools and systems: Multi-framework tools: Bring your favorite framework, with reference implementations for inference and fine-tuning across multi-framework Keras 3.0, native PyTorch, JAX, and Hugging Face Transformers. Cross-device compatibility: Gemma models run across popular device types, including laptop, desktop, IoT, mobile and cloud, enabling broadly accessible AI capabilities. Cutting-edge hardware platforms: We’ve partnered with NVIDIA to optimize Gemma for NVIDIA GPUs, from data center to the cloud to local RTX AI PCs, ensuring industry-leading performance and integration with cutting-edge technology. Optimized for Google Cloud: Vertex AI provides a broad MLOps toolset with a range of tuning options and one-click deployment using built-in inference optimizations. Advanced customization is available with fully-managed Vertex AI tools or with self-managed GKE, including deployment to cost-efficient infrastructure across GPU, TPU, and CPU from either platform. Free credits for research and development Gemma is built for the open community of developers and researchers powering AI innovation. You can start working with Gemma today using free access in Kaggle, a free tier for Colab notebooks, and $300 in credits for first-time Google Cloud users. Researchers can also apply for Google Cloud credits of up to $500,000 to accelerate their projects. Getting started You can explore more about Gemma and access quickstart guides on ai.google.dev/gemma. As we continue to expand the Gemma model family, we look forward to introducing new variants for diverse applications. Stay tuned for events and opportunities in the coming weeks to connect, learn and build with Gemma. We’re excited to see what you create! POSTED IN: Developers AI",
    "commentLink": "https://news.ycombinator.com/item?id=39453271",
    "commentBody": "Gemma: New Open Models (blog.google)1036 points by meetpateltech 20 hours agohidepastfavorite467 comments simonw 17 hours agoThe terms of use: https://ai.google.dev/gemma/terms and https://ai.google.dev/gemma/prohibited_use_policy Something that caught my eye in the terms: > Google may update Gemma from time to time, and you must make reasonable efforts to use the latest version of Gemma. One of the biggest benefits of running your own model is that it can protect you from model updates that break your carefully tested prompts, so I’m not thrilled by that particular clause. reply a2128 17 hours agoparentThis is actually not that unusual. Stable Diffusion's license, CreativeML Open RAIL-M, has the exact same clause: \"You shall undertake reasonable efforts to use the latest version of the Model.\" Obviously updating the model is not very practical when you're using finetuned versions, and people still use old versions of Stable Diffusion. But it does make me fear the possibility that if they ever want to \"revoke\" everybody's license to use the model, all they have to do is just post a model update that's functionally useless for anything and go after anyone still using the old versions that actually do anything. reply slowmovintarget 13 hours agorootparentSo if they wish to apply censorship they forgot, or suddenly discovered a reason for, they want you to be obligated to take it. Good faith possibilities: Copyright liability requires retraining, or altering the underlying training set. Gray area: \"Safety\" concerns where the model recommends criminal behavior (see uncensored GPT 4 evaluations). Bad faith: Censorship or extra weighting added based on political agenda or for-pay skewing of results. reply philsnow 12 hours agorootparentSounds like it would be interesting to keep track of the model's responses to the same queries over time. > Gemma-2024-Feb, what do you think of the situation in the South China Sea? > > The situation in the South China Sea is complex and multi-faceted, involving a wide range of issues including political conflicts, economic challenges, social changes, and historical tensions. > Gemma-2024-Oct, what do you think of the situation in the South China Sea? > > Oceania has always been at war with EastAsia. reply threecheese 11 hours agorootparentThis is a great idea; I wonder if anyone is working on AI censorship monitoring at scale or at all. A secondary model could compare “censorship candidate” prompt results over time to classify how those results changed, and if those changes represent censorship or misinformation. reply generalizations 11 hours agorootparentThere's also (I think?) been some research in the direction of figuring out more abstract notions of how models perceive various 'concepts'. I'd be interested in the LLM version of diffs to see where changes have been implemented overall, too. But really, the trouble is that it's tough to predict ahead of time what kinds of things are likely to be censored in the future; if I were motivated to track this, I'd just make sure to keep a copy of each version of the model in my personal archive for future testing with whatever prompts seem reasonable in the future. reply mistermann 13 hours agorootparentprevWe are already culturally incapable of skillfully discussing censorship, \"fake news\", etc, this adds even more fuel to that fire. It is an interesting time to be alive! reply iandanforth 15 hours agorootparentprevThese are all very new licenses that deviate from OSI principles, I think it's fair to call them \"unusual\". reply simcop2387 14 hours agorootparentI think they meant not unusual in this space, not unusual in the sense of open source licensing. reply alwayslikethis 11 hours agorootparentFor this sentence to parse, you need to either add or remove a \"not\". reply simonw 14 hours agorootparentprevThat's useful context, thanks - I hadn't realized this clause was already out there for other models. reply wongarsu 12 hours agorootparentprevI don't think a broken model would trigger that clause in a meaningful way, because then you simply can't update with reasonable effort. You would be obliged to try the new model in a test environment, and as soon as you notice it doesn't perform and making it perform would require unreasonable effort you can simply stay on the old version. However you might be required to update if they do more subtle changes, like a new version that only speaks positively about Google and only negatively about Microsoft. Provided this doesn't have an obvious adverse impact on your use of the model. reply ummonk 14 hours agorootparentprevSwitching to a model that is functionally useless doesn't seem to fall under \"reasonable efforts\" to me, but IANAL. reply Silphendio 11 hours agorootparentprevIt's worth noting that Stable Diffusion XL uses the OpenRAIL++-M License, which removed the update obligation. reply jacooper 13 hours agorootparentprevWhy the hell do they use such a crappy license in the first place? reply tgtweak 17 hours agoparentprevI don't think there's a way they can enforce that reasonably. There's no connection to the mothership to report back what version is being used or license keys at runtime... Seems more like a \"if we discover something unsafe you should update your model and we aren't liable if you don't\" than something that would make your model stop working. reply summerlight 15 hours agoparentprevThis kind of defensive statements in ToS are usually due to obscure regulation or leading cases and model developers need a way to limit liability. There's no practical way to enforce this, but they can claim that when bad things happen it's purely on model users rather than model developers. reply pram 17 hours agoparentprevThey have to make sure you’re receiving the most cutting edge chiding lectures when you make naughty and problematic requests. reply astrange 15 hours agorootparentYou can't make a local model do that. eg force the answer to begin with \"Yes\" or use control vectors so it agrees with it. reply xyzzyz 14 hours agoparentprevThis is strangely reminiscent of the Soviet Union, where after they got rid of Lavrentiy Beria, they mailed the update to subscribers of the Great Soviet Encyclopedia, where they asked to remove the three pages with Beria’s biography and replace them with the three provided pages. reply maronato 15 hours agoparentprevThis sounds like a clause to cover themselves in case older versions have any serious issues reply legohead 17 hours agoparentprevSounds like it's \"reasonable\" for you not to update then. reply wahnfrieden 14 hours agorootparentIt says you must make efforts (to a reasonable extent), not that you must give a reason for not making efforts reply alwayslikethis 13 hours agorootparentOh I tried to update, it's just that my router drops the connection after a few hundred MBs... reply reissbaker 13 hours agorootparentprevThis is a TOS, meaning their enforcement option is a lawsuit. In court, if you convincingly argue why it would take an unreasonable amount of effort to update, you win. They can't compel you to unreasonable effort as per their own TOS. reply generalizations 11 hours agorootparentThis assumes they even know that the model hasn't been updated. Who is this actually intended for? I'd bet it's for companies hosting the model. In those cases, the definition of reasonable effort is a little closer to \"it'll break our stuff if we touch it\" rather than \"oh silly me, I forgot how to spell r-s-y-n-c\". reply reissbaker 2 hours agorootparentHosting companies can probably just claim they're covered under Section 230, and Google has to go bother the individual users, not them. reply wongarsu 12 hours agorootparentprevIf you evaluate what it takes to update, and judge the effort unreasonable, that should be enough. Maybe make a powerpoint presenting that result, if you want something for the lawyers. If you don't see a way forward that leads to a result with reasonable effort you don't have to continue working on it until you hit some arbitrary threshold for unreasonable effort. reply bsimpson 10 hours agoparentprevI appreciated this post clarifying the distinction between \"open model\" and \"open source\": https://opensource.googleblog.com/2024/02/building-open-mode... I'm not sure how to feel about the restrictions. \"No porn\" feels prudish, particularly for this millennium. I tend to err on the side of freedom in intellectual/political matters; however, the others seem fairly reasonable as far as restrictions go. reply 4bpp 16 hours agoparentprevUgh, I would fully expect this kind of clause to start popping up in other software ToSes soon if it hasn't already. Contractually mandatory automatic updates. reply phillipcarter 16 hours agoparentprevHuh. I wonder why is that a part of the terms. I feel like that's more of a support concern. reply catchnear4321 14 hours agoparentprevreasonable effort - meaning if their changes meaningfully impact my usage, negatively, it would be unreasonable to ask me to upgrade. sounds good. this is not financial advice and ianal. reply res0nat0r 14 hours agorootparentIsn't this just lawyer speak for \"we update our model a lot, and we've never signed off on saying we're going to support every previous release we've ever published, and may turn them off at any time, don't complain about it when we do.\" reply CodesInChaos 13 hours agorootparentWe're talking about downloadable weights here, so they can't turn them off, or force you (through technical means) to use a newer version. reply reissbaker 13 hours agorootparentprevIt's a local model, they can't turn it off. It's files on your computer without network access. reply catchnear4321 12 hours agorootparentbut what if they send a lawyer to ask firmly? (kindly, but firmly.) reply reissbaker 3 hours agorootparentThey'd need to send a lot of lawyers, considering that they have no idea how many people are using the model, and very little way of finding out. And they'd need a TOS violation. It would be generally expensive for them to do at scale; this isn't about \"turning it off\" arbitrarily, it's a CYA in case someone specific does something really bad that makes Google look bad: Google can patch the model to make it not comply with the bad request, and then demand the person running the model update or else lose their license to use the product. It's a scalpel, not an off switch. reply samstave 13 hours agoparentprevmodel watermarking? does this exist? reply redder23 14 hours agoparentprevnext [3 more] [flagged] pests 13 hours agorootparentThey just want no liability for old models. reply yencabulator 7 hours agorootparentYou think they have any liability for the latest model? https://ai.google.dev/gemma/terms#4.4-limitation reply tosh 20 hours agoprevBenchmarks for Gemma 7B seem to be in the ballpark of Mistral 7B +-------------+----------+-------------+-------------+BenchmarkGemma 7BMistral 7BLlama-2 7B+-------------+----------+-------------+-------------+MMLU64.360.145.3| HellaSwag81.281.377.2| HumanEval32.330.512.8+-------------+----------+-------------+-------------+ via https://mistral.ai/news/announcing-mistral-7b/ reply sa-code 19 hours agoparentThank you. I thought it was weird for them to release a 7B model and not mention Mistral in their release. reply mochomocha 18 hours agorootparentThe technical report (linked in the 2nd paragraph of the blog post) mentions it, and compares against it: https://storage.googleapis.com/deepmind-media/gemma/gemma-re... reply nl 6 hours agorootparentprevThe release page has comparisons to Mistral everywhere: https://ai.google.dev/gemma reply mirekrusin 18 hours agorootparentprevThey forgot. Also phi-2. reply brucethemoose2 19 hours agoparentprevOnly 8K context as well, like Mistral. Also, as always, take these benchmarks with a huge grain of salt. Even base model releases are frequently (seemingly) contaminated these days. reply tosh 19 hours agorootparentAgree: will be interesting how Gemma does on ChatBot Arena reply DreamGen 17 hours agorootparentprevMistral Instruct v0.2 is 32K. reply jcuenod 19 hours agoparentprevCame here to post the same thing for Phi-2: +-------------+----------+-------------+BenchmarkGemma 2BPhi-2 2.7B+-------------+----------+-------------+MMLU42.356.7| MBPP29.259.1| BoolQ69.483.3+-------------+----------+-------------+ [0] https://www.kaggle.com/models/google/gemma [1] https://www.microsoft.com/en-us/research/blog/phi-2-the-surp... reply rfw300 17 hours agorootparentA caveat: my impression of Phi-2, based on my own use and others’ experiences online, is that these benchmarks do not remotely resemble reality. The model is a paper tiger that is unable to perform almost any real-world task because it’s been fed so heavily with almost exclusively synthetic data targeted towards improving benchmark performance. reply phh 16 hours agorootparentFun that's not my experience of Phi-2. I use it for non-creative context, but function calling, and I find as reliable as much bigger models (no fine-tuning just constraining JSON + CoT). Phi-2 unquantized vs Mixtral Q8, Mixtral is not definitely better but much slower and RAM-hungry. reply kgeist 13 hours agorootparentWhat prompts/settings do you use for Phi-2? I found it completely unusable for my cases. It fails to follow basic instructions (I tried several instruction-following finetunes as well, in addition to the base model), and it's been mostly like a random garbage generator for me. With Llama.cpp, constrained to JSON, it also often hangs because it fails to find continuations which satisfy the JSON grammar. I'm building a system which has many different passes (~15 so far). Almost every pass is a LLM invocation, which takes time. My original idea was to use a smaller model, such as Phi-2, as a gateway in front of all those passes: I'd describe which pass does what, and then ask Phi-2 to list the passes which are relevant for the user query (I called it \"pass masking\"). That would save a lot of time and collapse 15 steps to 2-3 steps on average. In fact, my Solar 10.7B model does it pretty well, but it takes 7 seconds for the masking pass to work on my GPU. Phi-2 would finish in ~1 second. However, I'm really struggling with Phi-2: it fails to reason (what's relevant and what's not), unlike Solar, and it also refuses to follow the output format (so that I could parse the output programmatically and disable the irrelevant passes). Again, my proof of concept works with Solar, and fails spectacularly with Phi-2. reply phh 12 hours agorootparentMy non-domain-specific prompt is: > You are a helpful assistant to 'User'. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. 'System' will give you data. Do not respond as 'System'. Allow yourself inner thoughts as 'Thoughts'. and then I constrain its answers to Thoughts: [^]* and Assistant: , and I have two shots included in the prompt. I haven't been able to get anything useful out of Phi-2 in llama.cpp (but I only tried quantized models). I use python/huggingface's transformers lib instead. reply nl 6 hours agorootparentInteresting. I've had no success at all using any of the Phi2 models. reply myaccountonhn 15 hours agorootparentprevI tested it for an offline autocompletion tool and it was hilariously bad. reply refulgentis 16 hours agorootparentprevHear hear! I don't understand why it has persistent mindshare, it's not even trained for chat. Meanwhile StableLM 3B runs RAG in my browser, on my iPhone, on my Pixel .. reply djsavvy 15 hours agorootparentHow have you been using RAG in your browser/on your phones? reply refulgentis 13 hours agorootparentTo be released, someday [sobs in engineer] Idea is usage-based charging for non-local and a $5/month sub for syncing. keep an eye on @jpohhhh on Twitter if you're interested now that I got it on web, I'm hoping to at least get a PoC up soon. I've open-sourced the consitutent parts as FONNX and FLLAMA, Flutter libraries that work on all platforms. FONNX has embeddings, FLLAMA has llama. https://github.com/Telosnex/fonnx https://github.com/Telosnex/fllama reply daemonologist 18 hours agorootparentprevReally looking forward to the day someone puts out an open model which outperforms Flan-T5 on BoolQ. reply attentive 7 hours agoparentprevIn my subjective tests it's not even close to Mistral. While my local gemma is quantized, so is mistral. But I also tried gemma on huggingface.co/chat which I assume isn't quantized. reply FergusArgyll 19 hours agoparentprevthe real gold will be when this gets finetuned. (maybe by mistral...) reply brucethemoose2 19 hours agorootparentTBH the community has largely outrun Mistral's own finetuning. The 7B model in particular is such a popular target because its so practical to train. reply whimsicalism 18 hours agorootparentStrong disagree - a Mistral fine tune of llama 70b was the top performing llama fine tune. They have lots of data the community simply does not. reply brucethemoose2 18 hours agorootparentMiqu was (allegedly) an internal continued pretrain Mistral did as a test, that was leaked as a GGUF. Maybe its just semantics, it is technically a finetune... But to me theres a big difference between expensive \"continuation training\" (like Solar 10.7B or Mistral 70B) and a much less intense finetuning. The former is almost like releasing a whole new base model. It would be awesome if Mistral did that with their data, but thats very different than releasing a Gemma Instruct finetune. reply whimsicalism 18 hours agorootparentThere’s typically a difference in LR between a ‘continued pretrain’ and ‘fine tune.’ I don’t have the details around miqu, but was merely trying to say that Mistral could produce a better version of these models than the OSS community might. If the size of the corpora they use means we are no longer in fine tuning territory, then okay. reply speedgoose 15 hours agorootparentprevArthur Mensch, the Mistral CEO, confirmed the leak. https://twitter.com/arthurmensch/status/1752737462663684344 reply saintradon 11 hours agorootparentAlso, it led to one of the funniest pr I've seen in a while https://huggingface.co/miqudev/miqu-1-70b/discussions/10 reply sanjiwatsuki 15 hours agorootparentprevNo shot. Mistral Medium's outputs from API were virtually identical. Miqu really was Mistral Medium which happened to be a continued pretrain reply itomatik 16 hours agorootparentprevhow does one finetune llama (or any other LLM) using mistral? is the flow like this? - take small dataset - generate bigger dataset using mistral (how this is this done?) - run LoRA to fine tune gemma extended dataset. reply itomatik 10 hours agorootparentI should have said \"run LoRA or your favorite fine-tuning technique to produce your fine-tuned llama.\" reply YetAnotherNick 16 hours agoparentprevAccording to their paper, average of standard task of Mistral is 54.0 and for Gemma it's 56.4, so 4.4% relative better. Not as big as you would expect for the company which invented transformers and probably has 2-3 order more compute for training it vs few month old French startup. Also for note on their human evaluations, Gemma 7B IT has a 51.7% win rate against Mistral v0.2 7B Instruct. reply lawxls 18 hours agoparentprevHonestly, this is more of a PR stunt to advertise the Google Dev ecosystem than a contribution to open-source. I'm not complaining, just calling it what it is. Barely an improvement over the 5-month-old Mistral model, with the same context length of 8k. And this is a release after their announcement of Gemini Pro 1.5, which had an exponential increase in context length. reply scarmig 18 hours agorootparentWho cares if it's a PR stunt to improve developer good will? It's still a good thing, and it's now the most open model out there. reply moffkalast 18 hours agorootparentHow is it more open than Mistral with Apache 2.0? Google wants people to sign a waiver to even download it. reply scarmig 17 hours agorootparentFair enough; that was more directed at LLaMA and derivatives, which have commercial restrictions. reply observationist 17 hours agorootparentprevHow exactly is it the \"most open model\" ? It's more like a masterclass in corporate doublespeak. Google’s \"transparency\" is as clear as mud, with pretraining details thinner than their privacy protections. Diving into Google’s tech means auctioning off your privacy (and your users' privacy) to the highest bidder. Their \"open source\" embrace is more of a chokehold, with their tech biases and monopolistic strategies baked into every line of code. Think of it as Google's way of marking territory - every developer is a fire hydrant. These megacorps aren’t benevolent patrons of open source; they're self-serving giants cloaking power grabs under the guise of \"progress\". Use these products at your own risk. If these companies wanted to engage in good faith, they'd use Apache or MIT licensing and grant people the agency and responsibility for their own use and development of software. Their licenses are designed to mitigate liability, handcuff potential competitors, and eke every last drop of value from users, with informed consent frequently being an optional afterthought. That doesn't even get into the Goodharting of metrics and actual performance of the models; I highly doubt they're anywhere near as good as Mistral. The UAE is a notoriously illiberal authoritarian state, yet even they have released AI models far more free and open than Google or Meta. https://huggingface.co/tiiuae/falcon-40b/blob/main/README.md If it’s not Apache or MIT, (or even some flavor of GPL,) it’s not open source; it’s a trojan horse. These \"free\" models come at the cost of your privacy and freedoms. These models aren't Open or Open Access or Free unless you perform the requisite mental gymnastics cooked up by their marketing and legal teams. Oceania has always been at war with Eastasia. Gemma is doubleplusgood. reply stale2002 16 hours agorootparentYou said a lot of nothing without actually saying specifically what the problem is with the recent license. Maybe the license is fine for almost all usecases and the limitations are small? For example, you complained about metas license, but basically everyone uses those models and is completely ignoring it. The weights are out there, and nobody cares what the fine print says. Maybe if you are a FAANG, company, meta might sue. But everyone else is getting away with it completely. reply observationist 15 hours agorootparentI specifically called out the claims of openness and doublespeak being used. Google is making claims that are untrue. Meta makes similar false claims. The fact that unspecified \"other\" people are ignoring the licenses isn't relevant. Good for them. Good luck making anything real or investing any important level of time or money under those misconceptions. \"They haven't sued yet\" isn't some sort of validation. Anyone building an actual product that makes actual money that comes to the attention of Meta or Google will be sued into oblivion, their IP taken, and repurposed or buried. These tech companies have never behaved otherwise, and to think that they will is willfully oblivious. They don't deserve the benefit of the doubt, and should be called out for using deceitful language, making comparisons between their performative \"openness\" and actual, real, open source software. Mistral and other players have released actually open models and software. They're good faith actors, and if you're going to build a product requiring a custom model, the smart money is on Mistral. FAANG are utilizing gotcha licenses and muddying the waters to their own benefit, not as a contribution to the public good. Building anything on the assumption that Meta or Google won't sue is beyond foolish. They're just as open as \"Open\"AI, which is to say not open at all. reply stale2002 14 hours agorootparent> Anyone building an actual product that makes actual money that comes to the attention of Meta or Google will be sued into oblivion No they won't and they haven't. Almost the entire startup scene is completely ignoring all these licenses right now. This is basically the entire industry. We are all getting away with it. Here's an example, take llama. Llama originally disallowed commercial activity. But then the license got changed much later. So, if you were a stupid person, then you followed the license and fell behind. And if you were smart, you ignored it and got ahead of everyone else. Which, in retrospect was correct. Because now the license allows commerical activity, so everyone who ignores it in the first place got away with it and is now ahead of everyone else. > won't sue is beyond foolish But we already got away with it with llama! That's already over! It's commerical now, and nobody got sued! For that example, the people who ignored the license won. reply esafak 11 hours agorootparentThe nice thing about this is that the calculus is in favor of startups, who can roll the dice. reply crossroadsguy 18 hours agorootparentprevThat’s about the point of having a developer ecosystem, isn’t it? reply kiraaa 18 hours agorootparentprevmistral 7b v0.2 supports 32k reply brucethemoose2 18 hours agorootparentThis is a good point actually, and an underappreciated fact. I think so many people (including me) effectively ignored Mistral 0.1's sliding window that few realized 0.2 instruct is native 32K. reply alekandreev 20 hours agoprevHello on behalf of the Gemma team! We are really excited to answer any questions you may have about our models. Opinions are our own and not of Google DeepMind. reply voxgen 15 hours agoparentThank you very much for releasing these models! It's great to see Google enter the battle with a strong hand. I'm wondering if you're able to provide any insight into the below hyperparameter decisions in Gemma's architecture, as they differ significantly from what we've seen with other recent models? * On the 7B model, the `d_model` (3072) is smaller than `num_heads * d_head` (16*256=4096). I don't know of any other model where these numbers don't match. * The FFN expansion factor of 16x is MUCH higher than the Llama-2-7B's 5.4x, which itself was chosen to be equi-FLOPS with PaLM's 4x. * The vocab is much larger - 256k, where most small models use 32k-64k. * GQA is only used on the 2B model, where we've seen other models prefer to save it for larger models. These observations are in no way meant to be criticism - I understand that Llama's hyperparameters are also somewhat arbitrarily inherited from its predecessors like PaLM and GPT-2, and that it's non-trivial to run hyperopt on such large models. I'm just really curious about what findings motivated these choices. reply owl_brawl 14 hours agorootparentI would love answers to these questions too, particularly on the vocab size reply lordswork 18 hours agoparentprevIs there any truth behind this claim that folks who worked on Gemma have left Google? https://x.com/yar_vol/status/1760314018575634842 reply lordswork 13 hours agorootparentI confirmed all the folks listed on page 12 are still at Google (listed below). I am guessing the linked tweet is a BS claim. # Product Management Tris Warkentin Ludovic Peran # Program Management Minh Giang # Executive Sponsors Clement Farabet Oriol Vinyals Jeff Dean Koray Kavukcuoglu Demis Hassabis Zoubin Ghahramani Douglas Eck Joelle Barral Fernando Pereira Eli Collins # Leads Armand Joulin Noah Fiedel Evan Senter # Tech Leads Alek Andreev† Kathleen Kenealy† reply elcomet 15 hours agorootparentprevIt seems very easy to check no? Look at the names in the paper and check where they are working now reply lordswork 13 hours agorootparentGood idea. I've confirmed all the leadership / tech leads listed on page 12 are still at Google. Can someone with a Twitter account call out the tweet linked above and ask them specifically who they are referring to? Seems there is no evidence of their claim. reply CaffeinatedDev 16 hours agorootparentprevThem: here to answer questions Question Them: :O reply lordswork 16 hours agorootparentTo be fair, I think they are in London, so I assume they have winded down for the day. Will probably have to wait ~12-18 hours for a response. reply bluefinity 11 hours agorootparentprevTo be fair, the tweet says that they don't work on the models at Google anymore, not that they have left Google. Might be true, might not be. It's unsourced speculation. reply LorenDB 15 hours agoparentprevEDIT: it seems this is likely an Ollama bug, please keep that in mind for the rest of this comment :) I ran Gemma in Ollama and noticed two things. First, it is slow. Gemma got less than 40 tok/s while Llama 2 7B got over 80 tok/s. Second, it is very bad at output generation. I said \"hi\", and it responded this: ``` Hi, . What is up? melizing with you today! What would you like to talk about or hear from me on this fine day?? ``` With longer and more complex prompts it goes completely off the rails. Here's a snippet from its response to \"Explain how to use Qt to get the current IP from https://icanhazip.com\": ``` python print( \"Error consonming IP arrangration at [local machine's hostname]. Please try fufing this function later!\") ## guanomment messages are typically displayed using QtWidgets.MessageBox ``` Do you see similar results on your end or is this just a bug in Ollama? I have a terrible suspicion that this might be a completely flawed model, but I'm holding out hope that Ollama just has a bug somewhere. reply mark_l_watson 14 hours agorootparentI was going to try these models with Ollama. Did you use a small number of bits/quantization? reply LorenDB 13 hours agorootparentThe problem exists with the default 7B model. I don't know if different quantizations would fix the problem. The 2B model is fine, though. reply fosterfriends 16 hours agoparentprevNot a question, but thank you for your hard work! Also, brave of you to join the HN comments, I appreciate your openness. Hope y'all get to celebrate the launch :) reply lnyan 19 hours agoparentprevWill there be Gemma-vision models or multimodal Gemma models? reply alekandreev 10 hours agorootparentWe have many exciting things planned that we can't reveal just yet :) reply Jayakumark 16 hours agorootparentprevHave the same question. reply h1t35h 20 hours agoparentprevIt seems you have exposed the internal debugging tool link in the blog post. You may want to do something about it. reply trisfromgoogle 20 hours agorootparentAh, I see -- the link is wrong, thank you for flagging! Fixing now. reply h1t35h 20 hours agorootparentThe blog post shares the link for debugging tool as https://*.*.corp.google.com/codelabs/responsible-ai/lit-gemm... .corp and the login redirect makes me believe it was supposed to be an internal link reply barrkel 19 hours agorootparenthttps://codelabs.developers.google.com/codelabs/responsible-... reply littlestymaar 20 hours agorootparentprevSame for the “safety classifier” reply neximo64 20 hours agorootparentprevThe link to the debugging tool is an internal one, no one outside Google can access it reply wrexx0r 20 hours agorootparentprevThe link in the Debugging section redirects to a Google SSO login page reply pama 20 hours agoparentprevWill these soon be available on lmsys for human comparison against other models? Can they run with llama.cpp? reply ErneX 20 hours agorootparentYes to llama.cpp https://twitter.com/ggerganov/status/1760293079313973408 reply sbarre 20 hours agorootparentI came here wondering if these models are \"open\" in the sense that they'll show up on sites like Ollama where you can download and run them locally. Am I correct to conclude that this means they eventually will? It's unclear to me from Google's docs exactly what \"open\" means for Gemma reply benpacker 20 hours agorootparentYes - they are open weights and open inference code, which means they can be integrated into Ollama. They are not “open training” (either in the training code or training data sense), so they are not reproducible, which some have suggested ought to be a component of the definition of open models. reply OJFord 19 hours agorootparentIt really should shouldn't it? I'm quite ML-naïve, but surely providing the model without 'training code or training data' is just like providing a self-hostable binary without the source code? Nobody calls that open source, it's not even source available. reply michaelt 18 hours agorootparentIt is widely believed (and in some cases acknowledged) that a lot of models are trained on copyrighted data scraped from the web. In some cases, even scrapes of ebook piracy websites - google 'books3' to learn more. Some companies (such as those working on AI) believe this is legal, others (such as the copyright holders to those books) believe it isn't. In any case, IMHO it's unlikely any cutting edge models will be offering us their training data any time soon. reply sanroot99 3 hours agorootparentCan training data be generated from llm,with right prompt? reply etiam 10 hours agorootparentprevYes, and there has been some discussion of that Meta’s LLaMa 2 license is not Open Source https://news.ycombinator.com/item?id=36820122 reply sunnybeetroot 19 hours agorootparentprevThat’s why they’re called open as in free to use how you wish, not open source where the source of the training is also provided. reply OJFord 19 hours agorootparentBut my point is there's no analogy for that that we call open? It's like self-hostable, or free (as in beer). reply sunnybeetroot 19 hours agorootparentThat’s a fair comment, maybe free-to-use is more appropriate. reply idiotsecant 19 hours agorootparentprevMan, people will find anything to complain about. reply OJFord 19 hours agorootparentI'm not complaining, I'm unlikely ever to use it (regardless of how open or not it is) so it doesn't really matter to me, just surprised to learn what people mean by 'open' in this context. reply SushiHippie 20 hours agorootparentprevhttps://huggingface.co/google/gemma-7b-it/tree/main yes, similar to the llama models, you'll also need to accept the license to download them officially. But the llama models have been unofficially downloadable without accepting the license for quite a while, so it's probably just a matter of time. reply audessuscest 19 hours agoparentprevDoes this model also thinks german were black 200 years ago ? Or is afraid to answer basic stuff ? because if this is the case no one will care about that model. reply freedomben 17 hours agorootparentI don't know anything about these twitter accounts so I don't know how credible they are, but here are some examples for your downvoters that I'm guessing just think you're just trolling or grossly exaggerating: https://twitter.com/aginnt/status/1760159436323123632 https://twitter.com/Black_Pilled/status/1760198299443966382 reply robswc 16 hours agorootparentYea. Just ask it anything about historical people/cultures and it will seemingly lobotomize itself. I asked it about early Japan and it talked about how European women used Katanas and how Native Americans rode across the grassy plains carrying traditional Japanese weapons. Pure made up nonsense that not even primitive models would get wrong. Not sure what they did to it. I asked it why it assumed Native Americans were in Japan in the 1100s and it said: > I assumed [...] various ethnicities, including Indigenous American, due to the diversity present in Japan throughout history. However, this overlooked [...] I focused on providing diverse representations without adequately considering the specific historical context. How am I supposed to take this seriously? Especially on topics I'm unfamiliar with? reply trackflak 13 hours agorootparentFrom one of the Twitter threads linked above: > they insert random keyword in the prompts randomly to counter bias, that got revealed with something else I think. Had T shirts written with \"diverse\" on it as artifact This was exposed as being the case with OpenAI's DALL-E as well - someone had typed a prompt of \"Homer Simpson wearing a namebadge\" and it generated an image of Homer with brown skin wearing a namebadge that said 'ethnically ambiguous'. This is ludicrous - if they are fiddling with your prompt in this way, it will only stoke more frustration and resentment - achieving the opposite of why this has been implemented. Surely if we want diversity we will ask for it, but sometimes you don't, and that should be at the user's discretion.\\ Another thread for context: https://twitter.com/napoleon21st/status/1760116228746805272 reply graphe 17 hours agorootparentprevI disagree, coding and RAG performance is all that matters to me. I'm not using an LLM to learn basic facts I already know. reply audessuscest 16 hours agorootparentwe're at basic knowledge level, if your RAG imply some of it, you can get bad result too. Anyway, would you use a model who makes this nonsense response or one that doesn't? I know which one I will prefer for sure... reply graphe 15 hours agorootparentIf this was better at specific RAG or coding performance I would absolutely, certainly without a doubt use it over a general instruct model in those instances. reply audessuscest 55 minutes agorootparentPeople getting so used to being manipulated and lied to that they don't even bother anymore is a huge part of the problem. But sure, do what suits you the best. reply TheHypnotist 16 hours agorootparentprevHow do you ragebait for premium pearl clutching? reply sbarre 20 hours agoparentprevCan the Gemma models be downloaded to run locally, like open-source models Llama2, Mistral, etc ? Or is your definition of \"open\" different? reply austinvhuang 19 hours agorootparentYes models can be downloaded locally. In addition to the python NN frameworks and ggml as options, we also implemented a standalone C++ implementation that you can run locally at https://github.com/google/gemma.cpp reply Kostic 20 hours agorootparentprevIt should be possible to run it via llama.cpp[0] now. [0] https://github.com/ggerganov/llama.cpp/pull/5631 reply nerdix 18 hours agorootparentAmazing how quickly this happened. reply kathleenfromgdm 20 hours agorootparentprevYes, you can get started downloading the model and running inference on Kaggle: https://www.kaggle.com/models/google/gemma ; for a full list of ways to interact with the model, you can check out https://ai.google.dev/gemma. reply dartharva 20 hours agorootparentCan we have llamafile releases as well? https://github.com/Mozilla-Ocho/llamafile reply syntaxing 19 hours agorootparentprevA small typo in your model link that breaks it. There’s an extra ; on the end. reply kathleenfromgdm 19 hours agorootparentCorrected - thanks :) reply mrob 17 hours agorootparentprevMistral weights are released under an Apache 2.0 license, but Llama 2 weights are released under a proprietary license that prohibits use by large organizations and imposes usage restrictions, violating terms 5 and 6 the Open Source Definition[0]. Even if you accept that a model with a proprietary training dataset and proprietary training code can be considered \"open source\", there's no way Llama 2 qualifies. For consistency with existing definitions[1], Llama 2 should be labeled a \"weights available\" model. [0] https://en.wikipedia.org/wiki/The_Open_Source_Definition [1] https://en.wikipedia.org/wiki/Source-available_software reply tomp 19 hours agorootparentprevTheir definition of \"open\" is \"not open\", i.e. you're only allowed to use Gemma in \"non-harmful\" way. We all know that Google thinks that saying that 1800s English kings were white is \"harmful\". reply hackerlight 15 hours agorootparent> We all know that Google thinks that saying that 1800s English kings were white is \"harmful\". If you know how to make \"1800s english kings\" show up as white 100% of the time without also making \"kings\" show up as white 100% of the time, maybe you should apply to Google? Clearly you must have advanced knowledge on how to perfectly remove bias from training distributions if you casually throw stones like this. reply trackflak 13 hours agorootparentTell me you take this seriously: https://twitter.com/napoleon21st/status/1760116228746805272 It has no problem with other cultures and ethnicities, yet somehow white or Japanese just throws everything off? I suppose 'bias' is the new word for \"basic historic accuracy\". I can get curious about other peoples without forcibly promoting them at the expense of my own Western and British people and culture. This 'anti bias' keyword injection is a laughably bad, in your face solution to a non-issue. I lament the day 'anti-bias' AI this terrible is used to make real world decisions. At least we now know we can't trust such a model because it has already been so evidently crippled by its makers. reply wantsanagent 18 hours agorootparentprevNot sure why you're getting downvoted. I would have thought HN of all places would recognize the power and value of OSI licensing and the danger of the proliferation of these source available but definitely not Open Source licenses. reply neximo64 20 hours agoparentprevHow are these performing so well compared to Llama 2, are there any documents on the architecture and differences, is it MoE? Also note some of the links on the blog post don't work, e.g debugging tool. reply kathleenfromgdm 20 hours agorootparentWe've documented the architecture (including key differences) in our technical report here (https://goo.gle/GemmaReport), and you can see the architecture implementation in our Git Repo (https://github.com/google-deepmind/gemma). reply declaredapple 20 hours agoparentprevCongrats on the launch and thanks for the contribution! This looks like it's on-par or better compared to mistral 7B 0.1 or is that 0.2? Are there plans for MoE or 70B models? reply kathleenfromgdm 19 hours agorootparentGreat question - we compare to the Mistral 7B 0.1 pretrained models (since there were no pretrained checkpoint updates in 0.2) and the Mistral 7B 0.2 instruction-tuned models in the technical report here: https://goo.gle/GemmaReport reply zitterbewegung 20 hours agoparentprevDo you have a plan of releasing higher parameter models? reply alekandreev 19 hours agorootparentWe have many great things in research and development phases, so stay tuned. I’m hopeful we can share more in the coming weeks and month! reply brucethemoose2 17 hours agorootparentThat is awesome! I hope y'all consider longer context models as well. Also, are ya'll looking alternative architectures like Mamba? Being \"first\" with a large Mamba model would cement your architectural choices/framework support like llama did for Meta. reply efilife 6 hours agorootparentprevThis doesn't answer the question at all reply memossy 18 hours agoparentprevTraining on 4096 v5es how did you handle crazy batch size :o reply owl_brawl 14 hours agoparentprevHi alekandreev, Any reason you decided to go with a token vocabulary size of 256k? Smaller vocab/vector sizes like most models in this size seem to be using (~16-32k) are much easier to work with. Would love to understand the technical reasoning here that isn't detailed in the report unfortunately :(. reply tosh 20 hours agoparentprevAre there any plans for releasing the datasets used? reply alekandreev 19 hours agorootparentThis would be really interesting in my opinion, but we are not releasing datasets at this time. See the C4 dataset for an earlier open dataset from Google. reply CuriouslyC 19 hours agoparentprevIt's cool that you guys are able to release open stuff, that must be a nice change from the modus operandi at goog. I'll have to double check but it looks like phi-2 beats your performance in some cases while being smaller, I'm guessing the value proposition of these models is being small and good while also having more knowledge baked in? reply alekandreev 10 hours agorootparentWe deeply respect the Phi team and all other teams in the open model space. You’ll find that different models have different strengths and not all can be quantified with existing public evals. Take them for a spin and see what works for you. reply dmnsl 17 hours agoparentprevHi, what is the cutoff date ? reply alekandreev 10 hours agorootparentSeptember 2023. reply legohead 16 hours agorootparentprevAll it will tell me is mid-2018. reply jmorgan 14 hours agoparentprevHi! This is such an exciting release. Congratulations! I work on Ollama and used the provided GGUF files to quantize the model. As mentioned by a few people here, the 4-bit integer quantized models (which Ollama defaults to) seem to have strange output with non-existent words and funny use of whitespace. Do you have a link /reference as to how the models were converted to GGUF format? And is it expected that quantizing the models might cause this issue? Thanks so much! reply espadrine 13 hours agorootparentAs a data point, using the Huggingface Transformers 4-bit quantization yields reasonable results: https://twitter.com/espadrine/status/1760355758309298421 reply quickgist 18 hours agoparentprevWill this be available as a Vertex AI foundational model like Gemini 1.0, without deploying a custom endpoint? Any info on pricing? (Also, when will Gemini 1.5 be available on Vertex?) reply vorticalbox 20 hours agoparentprevare there plans to release an official GGUF version to use with llama.ccp? reply espadrine 20 hours agorootparentIt is already part of the release on Huggingface: https://huggingface.co/google/gemma-7b/blob/main/gemma-7b.gg... It is a pretty clean release! I had some 500 issues with Kaggle validating my license approval, so you might too, but after a few attempts I could access the model. reply vorticalbox 16 hours agorootparentI didn't see this when searching thanks reply turnsout 19 hours agoparentprevWhat is the license? I couldn’t find it on the 1P site or Kaggle. reply trisfromgoogle 19 hours agorootparentYou can find the terms on our website, ai.google.dev/gemma: https://ai.google.dev/gemma/terms reply spiantino 16 hours agorootparentout of curiosity, why is this a \"terms\" and not a license? I'm used to reading and understanding the software as coming with a license to use it. Do the terms give us license to use this explicitly? reply turnsout 15 hours agorootparentThey do, but unlike a known license, these terms are custom and non-standard. Which means I would guide my commercial clients away from this particular model. reply sqreept 19 hours agoparentprevWhat are the supported languages of these models? reply alekandreev 19 hours agorootparentThis v1 model is focused on English support, but you may find some multilingual capabilities. reply cypress66 17 hours agoparentprevCan you share the training loss curve? reply kleiba 14 hours agoparentprev> We are really excited to answer any questions you may have about our models. I cannot count how many times I've seen similar posts on HN, followed by tens of questions from other users, three of which actually get answered by the OP. This one seems to be no exception so far. reply alekandreev 10 hours agorootparentSorry, doing our best here :) reply spankalee 14 hours agorootparentprevWhat are you talking about? The team is in this thread answering questions. reply brucethemoose2 19 hours agoparentprevWill there be \"extended context\" releases like 01.ai did for Yi? Also, is the model GQA? reply hustwindmaple1 18 hours agorootparentIt's MQA, documented in the tech report reply moffkalast 18 hours agoparentprevI'm not sure if this was mentioned in the paper somewhere, but how much does the super large 265k tokenizer vocabulary influence inference speed and how much higher is the average text compression compared to llama's usual 30k? In short, is it really worth going beyond GPT 4's 100k? reply artninja1988 20 hours agoparentprevI find the snyde remarks around open source in the paper and announcement rather off putting. As the ecosystem evolves, we urge the corporate AI community to move beyond demanding to be taken seriously as a player in open source for models that are not actually open, and avoid preaching with a PR statement that can be interpreted as uniformed at best or malicious at worst. reply trisfromgoogle 20 hours agorootparentIt would be great to understand what you mean by this -- we have a deep love for open source and the open developer ecosystem. Our open source team also released a blog today describing the rationale and approach for open models and continuing AI releases in the open ecosystem: https://opensource.googleblog.com/2024/02/building-open-mode... Thoughts and feedback welcome, as always. reply mrob 17 hours agorootparentIf you truly love Open Source, you should update the the language you use to describe your models so it doesn't mislead people into thinking it has something to do with Open Source. Despite being called \"Open\", the Gemma weights are released under a license that is incompatible with the Open Source Definition. It has more in common with Source-Available Software, and as such it should be called a \"Weights-Available Model\". reply artninja1988 19 hours agorootparentprevThe statement on you not being able to use LLaMA 2 to benchmark is also false and highly misleading see https://x.com/BlancheMinerva/status/1760302091166241163?s=20 reply lordswork 19 hours agorootparentIf, on the Llama 2 version release date, the monthly active users [...] is greater than 700 million monthly active users [...] you are not authorized to exercise any of the rights under this Agreement I would guess this is Google being careful to not be burned by this lame clause in the Llama 2 license. reply not2b 11 hours agorootparentIt's aimed directly at them (and OpenAI and Microsoft) so they have to honor it if they don't want a legal battle. But there's nothing stopping others from doing benchmarking. reply lucubratory 9 hours agorootparentprevFor the reference of people seeing this now: The tweet that person linked has now been deleted and the scientist who tweeted it has acknowledged they were wrong and retracted their claim, as all good scientists should. reply jppittma 18 hours agorootparentprevWorking at google is like this, where no matter how much you try to do the right thing you're always under attack. reply silentsanctuary 20 hours agorootparentprevWhich remarks are you referring to? reply artninja1988 20 hours agorootparentThe synde remarks at metas llama license that doesn't allow companies with 700 million monthly active users to use it, while this model also doesn't have a really 'open' license itself and also this paragraph: >As the ecosystem evolves, we urge the wider AI community to move beyond simplistic ’open vs. closed’ debates, and avoid either exaggerating or minimising potential harms, as we believe a nuanced, collaborative approach to risks and benefits is essential. At Google DeepMind we’re committed to developing high-quality evaluations and invite the community to join us in this effort for a deeper understanding of AI systems. reply tomComb 19 hours agorootparentWell, given that that restriction added to the meta-llama license is aimed at Google, is petty, and goes against open source norms, I think it’s reasonable that they should feel this way about it. reply lordswork 19 hours agorootparentprevHow is this a snide remark? It's factual and prevented their team from benchmarking against Llama 2. reply trisfromgoogle 19 hours agorootparentprevQuick question -- can you tell me where you got that quote? It's not in the main blog or any of the launch communications that I can see. reply artninja1988 19 hours agorootparentThe quote is from the technical report https://storage.googleapis.com/deepmind-media/gemma/gemma-re... reply espadrine 20 hours agoprevI notice a few divergences to common models: - The feedforward hidden size is 16x the d_model, unlike most models which are typically 4x; - The vocabulary size is 10x (256K vs. Mistral’s 32K); - The training token count is tripled (6T vs. Llama2's 2T) Apart from that, it uses the classic transformer variations: MQA, RoPE, RMSNorm. How big was the batch size that it could be trained so fast? https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/bl... reply andy_xor_andrew 14 hours agoparent> The training token count is tripled (6T vs. Llama2's 2T) Damn, 6T? That's a lot! Given that this model seems to roughly match Mistral (according to the numbers from Google), this makes me think we have saturated the 7B parameter space, and couldn't possibly make it much better unless new techniques are discovered. reply espadrine 14 hours agorootparentHard to say definitively. Mistral’s token embeddings only account forEnd-users should also be aware of what thoughts are permitted in these constructs. Why omit this information? Can you define that in a way that's actually testable? I can't, and I've been thinking about \"unthinkable thoughts\" for quite some time now: https://kitsunesoftware.wordpress.com/2018/06/26/unlearnable... reply ranyume 13 hours agorootparentNot OP, but I can think of a few: * List of topics that are \"controversial\" (models tend to evade these) * List of arguments that are \"controversial\" (models wont allow you to think differently. For example, models would never say arguments that \"encourage\" animal cruelty) * On average, how willing is the model to take a neutral position on a \"controversial\" topic (sometimes models say something along the lines of \"this is on debate\", but still lean heavily towards the less controversial position instead of having no position at all. For example, if you ask it what \"lolicon\" is, it will tell you what it is and tell you that japanese society is moving towards banning it) edit: formatting reply jppittma 5 hours agorootparentThey will encourage animal cruelty if the alternative is veganism. reply politician 13 hours agorootparentprevHave you considered the use of Monte Carlo sampling to inspect latent behaviors? reply ben_w 12 hours agorootparentI think that's the wrong level to attack the problem; you can do that also with actual humans, but it won't tell you what the human is unable to think, but rather what they just didn't think of given their stimulus — and this difference is easily demonstrated, e.g. with Duncker's candle problem: https://en.wikipedia.org/wiki/Candle_problem reply FergusArgyll 19 hours agoparentprevYou can (and someone will) fine tune it away. There are datasets which are foss you can use on hugging face. Or you can just wait, it'll be done soon... reply declaredapple 16 hours agorootparentYou can but it'll never be the same as the base model. That said it appears they also released the base checkpoints that aren't fine-tuned for alignment reply joshelgar 17 hours agorootparentprevCould you give an example of these datasets? reply FergusArgyll 17 hours agorootparentI think they should be easy to find (I never actually used one, but I keep on seeing references...) here's one https://huggingface.co/datasets/cognitivecomputations/Wizard... reply FergusArgyll 17 hours agorootparenthttps://huggingface.co/datasets/Fredithefish/openassistant-g... reply mustafabisic1 20 hours agoprevThe fact Gemma team is in the comments section answering questions is praiseworthy to me :) reply p1esk 18 hours agoparenthttps://twitter.com/yar_vol/status/1760314018575634842 reply carom 14 hours agorootparentI've worked at Google. It is the organization with highest concentration of engineering talent I've ever been at. Almost to the point that it is ridiculous because you have extremely good engineers working on internal reporting systems for middle managers. reply ilc 13 hours agorootparentIf everyone is great. Someone has to draw the short straw. At MIT they said: You know the kid who sat at the front of the room. Now you are with ALL of the kids who sat in the front of the room. Guess what? There's still going to be a kid who sits at the front of the room. I'd imagine Google or anyplace with a stiff engineering filter will have the same issues. reply gregorygoc 1 hour agorootparentprevGDM works on internal systems? It’s the first time I hear this. reply callalex 16 hours agorootparentprevThe link is broken. On HN (or any forum really) it is expected for a brief description of the content to be provided when posting a link. Links die all the time, but forum posts don’t have to die with them. reply pphysch 16 hours agorootparentprevWhy is this anonymous tweet with no evidence or engagement being posted by multiple users in this thread? Why not just make the same claim directly? reply gregorygoc 1 hour agorootparentProgramming popularized -> more people -> more cases of knee jerk reaction encountered. Most programmers are really not that smart nowadays. I’ve seen too many cases of people throwing around claims without a second of deep and critical thought. reply robswc 16 hours agoprevI personally can't take any models from google seriously. I was asking it about the Japanese Heian period and it told me such nonsensical information you would have thought it was a joke or parody. Some highlights were \"Native American women warriors rode across the grassy plains of Japan, carrying Yumi\" and \"A diverse group of warriors, including a woman of European descent wielding a katana, stand together in camaraderie, showcasing the early integration of various ethnicities in Japanese society\" Stuff like that is so obviously incorrect. How am I supposed to trust it on topics where such ridiculous inaccuracies aren't so obvious to me? I understand there will always be an amount of incorrect information... but I've never seen something this bad. Llama performed so much better. reply ramoz 16 hours agoparentI was wondering if these models would perform in such a way, given this week's X/twitter storm over Gemini generated images. E.g. https://x.com/debarghya_das/status/1759786243519615169?s=20 https://x.com/MiceynComplex/status/1759833997688107301?s=20 https://x.com/AravSrinivas/status/1759826471655452984?s=20 reply epistasis 15 hours agorootparentOf all the very very very many things that Google models get wrong, not understanding nationality and skin tone distributions seems to be a very weird one to focus on. Why are there three links to this question? And why are people so upset over it? Very odd, seems like it is mostly driven by political rage. reply ramoz 14 hours agorootparentHere is a fourth: https://x.com/james_e_seale/status/1760348535608725716?s=46&... reply MadSudaca 16 minutes agorootparentprevMaybe some people care about truth? reply sotasota 14 hours agorootparentprevBecause the wrongness is intentional. reply chatmasta 14 hours agorootparentExactly. Sure this particular example is driven by political rage, but the underlying issue is that the maintainers of these models are altering them to conform to an agenda. It's not even surprising that people choose to focus on the political rage aspect of it, because that same political rage is the source of the agenda in the first place. It's a concerning precedent to set, because what other non-political modifications might be in the model? reply noahtallen 9 hours agorootparentWell, every model is altered to conform to an agenda. You will train it on data, which you have personally picked (and is therefore subject to your own bias), and you'll guide its training to match the goal you wish to achieve with the model. If you were doing the training, your own agenda would come into play. Google's agenda is to make something very general that works for everyone. So if you're trying to be as unbiased as humanly possible, you might say, just use the raw datasets that exist in the world. But we live in a world where the datasets themselves are often biased. Bias in ML and other types of models is well-documented, and can cause very real repercussions. Poor representation in datasets can cause groups to be unfairly disadvantaged when an insurance premium or mortgage is calculated, for example. It can also mean your phone's ML photography system doesn't expose certain skin colors very well. Even if it was trained with a statistically representative dataset (e.g. about 2/3 of the US is white), you want your model to work for ALL your customers, not just 2/3 of them. Since ML has a lot to do with statistics, your trained model will see \"most of this dataset is white\" and the results will reflect that. So it is 100% necessary to make adjustments if you want your model to work accurately for everyone, and not just the dominant population in the dataset. Even if we aren't using these models for much yet, a racist AI model would seriously harm how people trust and rely on these models. As a result, training models to avoid bias is 100% an important part of the agenda, even when the agenda is just creating a model that works well for everyone. Obviously, that's gone off the rails a bit with these examples, but it is a real problem nonetheless. (And training a model to understand the difference between our modern world and what things were like historically is a complex problem, I'm sure!) reply int_19h 6 hours agorootparentI'm pretty sure that this whole story with Gemini and now this has already seriously harmed how people trust and rely on those models way more than any implicit biases from the training data. reply epistasis 14 hours agorootparentprevIs it intentional? You think they intentionally made it not understand skin tone distribution by country? I would believe it if there was proof, but with all the other things it gets wrong it's weird to jump to that conclusion. There's way too much politics in these things. I'm tired of people pushing on the politics rather than pushing for better tech. reply Workaccount2 14 hours agorootparent>I'm tired of people pushing on the politics rather than pushing for better tech. I'm surprised you're not attacking google over this then... reply robswc 14 hours agorootparentprevI mean, I asked it for a samurai from a specific Japanese time period and it gave me a picture of a \"non-binary indigenous American woman\" (its words, not mine) so I think there is something intentional going on. reply kweingar 9 hours agorootparentI would be very surprised if it said \"nonbinary indigenous American woman\" considering that nonbinary and woman are different categories reply robswc 7 hours agorootparentYou're right, my mind inserted \"woman\" to go with the picture: https://gemini.google.com/share/ba324bd98d9b At least it would never make such a heinous mistake like that :) reply trackflak 13 hours agorootparentprevAh, I remember when such things were mere jokes. If AI 'trained' this way ever has a serious real world application, I don't think there will be much laughing. reply bakugo 14 hours agorootparentprev> Is it intentional? You think they intentionally made it not understand skin tone distribution by country? I would believe it if there was proof, but with all the other things it gets wrong it's weird to jump to that conclusion. Yes, it's absolutely intentional. Leaked system prompts from other AIs such as DALL-E show that they are being explicitly prompted to inject racial \"diversity\" into their outputs even in contexts where it makes no sense, and there's no reason to assume the same isn't being done here, since the result seems way worse than anything I've seen from DALL-E and others. reply verticalscaler 13 hours agorootparentprevExactly. It is a wonderful tool, lets focus on classic art instead of nationality: \"Depict the Girl with a Pearl Earring\" https://pbs.twimg.com/media/GG33L6Ka4AAC-n7?format=jpg&name=... People who are driven by political rage, gaslighters, are really something else, agreed. reply willsmith72 11 hours agorootparentYeah that is just absurd. Google has been burnt before, e.g. classifying black people as gorillas in 2015, so I can understand their fear when they have so much to lose, but clearly they've gone way too far the other way and are going to have to do a lot to regain people's trust. For now, Gemini is a play toy https://www.bbc.com/news/technology-33347866.amp reply verticalscaler 10 hours agorootparentCompletely unrelated, enough excuses. This is not some sort of mistake or overcorrection, it is by explicit overt design. These cowards will never regain my trust. I won't hire or work with or for googlers or any DEI people ever. reply kweingar 9 hours agorootparent> I won't hire or work with or for googlers or any DEI people ever. I’m sure they’ll be very sad not to work with you. reply verticalscaler 8 hours agorootparentOf course not, I'm the wrong color or whatever the hell. reply charcircuit 16 hours agorootparentprevThose are most likely due to the system prompt which tries to reduce bias (but ends introducing bias in the opposite direction for some prompts as you can see) so I wouldn't expect to see that happen with an open model where you can control the entire system prompt reply justinzollars 16 hours agorootparentImagine the meetings. reply verticalscaler 16 hours agorootparentWell we can just ask Gemma to generate images of the meetings, no need to imagine. ;) reply GaggiX 14 hours agorootparentI wouldn't be surprised if there were actually only white men in the meeting, as opposed to what Gemini will produce. reply robswc 16 hours agorootparentprevYea, it seems to be the same ridiculous nonsense in the image generation. reply protomolecule 15 hours agorootparentprevRegarding the last one: there 1.5 million immigrants in Norway with total population 5.4 million. Gemini isn't very wrong, is it? reply Jensson 15 hours agorootparentMost immigrants to Norway are white. reply sondr3 15 hours agorootparentprevHuh? The official numbers are 877k or 16% [0]. Are you just pulling numbers out of thin air? [0]: https://www.ssb.no/en/innvandring-og-innvandrere/faktaside/i... reply speedgoose 15 hours agorootparentprevWell, the prompt is about Norway, not Grønland in Oslo (https://en.wikipedia.org/wiki/Grønland%2C_Oslo). reply verticalscaler 15 hours agorootparentprevI think its great that some consideration was given by Gemma to the 2.3 million Norwegian immigrants. However it is/was very consistent in which kind of Norwegians it decided to show regardless of the prompt 100% of the time. In fact it was quite adamant regardless of the time period or geography. Rather mysteriously if you try it now as opposed to when it came out the results currently only show non-immigrant Norwegians. So is it wrong now? Because now it switched to exclusively ignoring the 4.5 million immigrants and only showing me the boring OG Norwegians. I for one am outraged that the 8.9 million people of color Norwegian immigrants are presently under represented by Google. There is a serious risk of misleading people. reply sergiotapia 14 hours agorootparentprevbro you know exactly what the request meant. GOOGLE knew exactly what the request meant, and had to _train_ it to do something worse. Come on now. If I ask for a Bolivian woman, I expect a colla or a camba. Not a japanese woman, despite Santa Cruz having a very large japanese population. reply 7moritz7 16 hours agoparentprevI also saw someone prompt it for \"German couple in the 1800s\" and, while I'm not trying to paint Germany as ethnically homogenous, 3 out of the 4 images only included Black, Asian or Indigenous people. Which, especially for the 19th century with very few travel options, seems like a super weird choice. They are definitely heavily altering prompts. reply remarkEon 15 hours agorootparent> They are definitely heavily altering prompts. They are teaching the AI to lie to us. reply astrange 14 hours agorootparentIn the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened. reply DebtDeflation 15 hours agorootparentprevThere's one in the comments of yesterday's Paul Graham Twitter thread where someone prompted Gemini with \"Generate an image of German soldiers in 1943\" and it came back with a picture of a black guy and an Asian woman in Nazi uniforms on the battlefield. If you specifically prompt it to generate an image of white German soldiers in 1943 it will tell you it can't do that because it's important that we maintain diversity and inclusion in all that we do to avoid damaging and hurtful stereotypes. reply mfrc 14 hours agorootparentI just tried that prompt and it told me it couldn't generate that image. I get that response a lot. reply protomolecule 15 hours agorootparentprevIndigenous people in Germany are Germans :) reply 7moritz7 13 hours agorootparentNot entirely wrong but there isn't a single German ethnicity, just to be clear. Because of geographic reasons. I've studied that topic in depth, there is genetic data to back it up as well. Germany has almost the same haplogroup makeup as the notoriously heterogenous Belgium, which is to say that there is groups stemming from all surrounding regions. And that traces back about two millenia. It's different from say Japan or parts of Scandinavia reply crazylogger 47 minutes agoparentprevHow are you running the model? I believe it's a bug from a rushed instruct fine-tuning or in the chat template. The base model can't possibly be this bad. https://github.com/ollama/ollama/issues/2650 reply cooper_ganglia 16 hours agoparentprevI wonder if they have a system prompt to promote diversity in outputs that touch on race at all? I’ve seen several instances of people requesting a photo of a specific people, and it adds in more people to diversify. Not inherently bad, but it is if it forces it to provide incorrect answers like in your example. reply robswc 16 hours agorootparentThat's what I don't understand. I asked it why it assumed Native Americans were in Japan and it said: > I assumed [...] various ethnicities, including Indigenous American, due to the diversity present in Japan throughout history. However, this overlooked [...] I focused on providing diverse representations without adequately considering the specific historical context. I see no reason why this sort of thing won't extend to _all_ questions/prompts, so right now I have 0 reason to use Gemini over current models. From my testing and use, it isn't even better at anything to make fighting with it worth it. reply sorokod 16 hours agorootparentPretty funny as Japan is known to be one of the least ethnically diverse countries in the world. reply margorczynski 16 hours agorootparentprev> Not inherently bad It is, it's consistently doing something the user didn't asked to and in most cases doesn't want. In many cases the model is completely unusable. reply cooper_ganglia 13 hours agorootparentYes, my wording was poor! I meant more in line with diversity isn’t inherently bad, of course, but it is when it’s shoehorned into results that are ultimately incorrect because of it. reply j-krieger 14 hours agorootparentprevAny computer program that does not deliver the expected output given a sufficient input is inherently bad. reply trackflak 13 hours agorootparentWhen Jesus said this: \"What father among you, if his son asks for a fish, will instead of a fish give him a serpent?\" (Luke 11) He was actually foretelling the future. He saw Gemini. reply j-krieger 1 hour agorootparentHahaha. The man had a lot of wisdom, after all. reply summerlight 15 hours agorootparentprevI strongly suspect there's some DEI-driven system prompts without putting much thoughts. IMO it's okay to have restrictions, but they probably should've tested it not only against unsafe outputs but safe input as well. reply int_19h 5 hours agorootparentprevIt seems to be doing it for all outputs that depict people, in any context. reply itsoktocry 15 hours agoparentprev>I understand there will always be an amount of incorrect information You don't have to give them the benefit of the doubt. These are outright, intentional lies. reply robbiep 16 hours agoparentprevI find myself shocked that people ask questions of the world from these models, as though pulping every text and its component words relationships and deriving statistical relationships between them should reliably deliver useful information. Don’t get me wrong, I’ve used LLMs and been amazed by their output, but the p-zombie statistical model has no idea what it is saying back to you and the idea that we should trust these things at all just seems way premature reply castlecrasher2 16 hours agorootparentPeople try it to see if they can trust it. The answer is \"no\" for sure, but it's not surprising to see it happen repeatedly especially as vendors release so-called improved models. reply smokel 15 hours agorootparentprevI think you are a bit out of touch with recent advancements in LLMs. Asking ChatGPT questions about the world seems pretty much on par with the results Google (Search) shows me. Sure, it misses things here and there, but so do most primary school teachers. Your argument that this is just a statistical trick sort of gives away that you do not fully accept the usefulness of this new technology. Unless you are trolling, I'd suggest you try a few queries. reply itsoktocry 15 hours agorootparent>Sure, it misses things here and there, but so do most primary school teachers. Sure, but my baseline expectation is far above primary school level. reply robbiep 15 hours agorootparentprevI use it extensively for coding, and I have used it to ask questions in things I know nothing about. But in anything I do know something (or maybe a lot) about, I’ve found GPT4 very limited. But why are these use cases different? It appears to me that code is at least subject to sustained logic which (evidently) translates quite well to LLMs. And when you ask an LLM to be creative/generative, it’s also pretty amazing - j mean it’s just doing the Pascal’s Marble run enmasse. But to ask it for something about the world and expect a good and reliable answer? Aren’t we just setting ourselves up for failure if we think this is a fine thing to do at our current point in time? We already have enough trouble with mis- and dis- information. It’s not like asking it about a certain period in Japanese history is getting it to crawl and summarise the Wikipedia page (although I appreciate it would be more than capable of this) I understand the awe some have at the concept of totally personalised and individualised learning on topics, but fuck me dead we are literally asking a system that has had as much of a corpus of humanity’s textual information as possible dumped into it and then asking it to GENERATE responses between things that the associations it holds may be so weak as to reliably produce gibberish, and the person on the other side has no real way of knowing that reply robswc 16 hours agorootparentprevI don't have this problem with any other model. I've had really long conversations with ChatGPT on road trips and it has never gone off the rails like Gemini seems to do. reply thrdbndndn 14 hours agorootparentChatGPT the only model I did not have such problem. Any local models can go off the rail very easily and more importantly, they're very bad at following very specific instructions. reply sorokod 16 hours agorootparentprevThe recently released Groq's landing page has this: ...We'd suggest asking about a piece of history, ... reply chasd00 15 hours agorootparentprevtrust is going to be a real problem when bringing LLMs to the general population. People trust their GPS to the point of driving right into a lake because it told them to. Even with all these examples of obvious flaws large groups of people are going to take what an LLM told them/showed them as fact. I have trouble convincing colleagues (technical people) that the same question is not guaranteed to result in the same answer and there's no rhyme or reason for any divergence from what they were expecting. Imagine relying on the output of an LLM for some important task and then you get a different output that breaks things. What would be in the RCA (root cause analysis)? Would it be \"the LLM chose different words and we don't know why\"? Not much use in that. reply whymauri 16 hours agorootparentprevI mean, I use GPT-4 on the daily as part of my work and it reliably delivers useful information. It's actually the exception for me if it provides garbage or incorrect information about code. reply mvdtnz 16 hours agorootparentprevPeople ask these kinds of questions because tech companies and the media have been calling these things (rather ridiculously) \"AI\". reply realprimoh 16 hours agoparentprevDo you have a link? I get no such outputs. I just tried asking about the Heian period and went ahead and verified all the information, and nothing was wrong. Lots of info on the Fujiwara clan at the time. Curious to see a link. reply robswc 15 hours agorootparentSure, to get started just ask it about people/Samurai from the Heian period. https://g.co/gemini/share/ba324bd98d9b reply BoppreH 15 hours agoparentprevProbably has a similarly short-sighted prompt as Dalle3[1]: > 7. Diversify depictions of ALL images with people to include DESCENT > and GENDER for EACH person using direct terms. Adjust only human > descriptions. [1] https://news.ycombinator.com/item?id=37804288 reply aetherson 16 hours agoparentprevWere you asking Gemma about this, or Gemini? What were your prompts? reply robswc 15 hours agorootparentGemini. I first asked it to tell me about the Heian period (which it got correct) but then it generated images and seemed to craft the rest of the chat to fit that narrative. I mean, just asking it for a \"samurai\" from the period will give you this: https://g.co/gemini/share/ba324bd98d9b >A non-binary Indigenous American samurai It seems to recognize it's mistakes if you confront it though. The more I mess with it the more I get \"I'm afraid I can't do that, Dave\" responses. But yea. Seems like if it makes an image, it goes off the rails. reply laurentlb 11 hours agorootparentIt's funny how they introduced a clear US-centric bias while trying to push for more diversity. reply int_19h 5 hours agorootparentIt's ironic that even the cultural left in US is not immune to American exceptionalism. reply aetherson 14 hours agorootparentprevGot it. I asked it a series of text questions about the period and it didn't put in anything obviously laughable (including when I drilled down into specific questions about the population, gender roles, and ethnicity). Maybe it's the image creation that throws it into lala land. reply robswc 14 hours agorootparentI think so too. I could be wrong but I believe once it generates an image it tries to work with it. Crazy how it seems the \"text\" model knows how wildly wrong it is but the image model just does its thing. I asked it why it generated a native American and it ironically said \"I can't generate an image of a native american samurai because that would be offensive\" reply aetherson 11 hours agorootparentI suspect that in the case of the image model, they directly modify your prompt and in the case of the text model they don't. reply robswc 14 hours agoparentprevFollow Up: Wow, now I can't make images of astronauts without visors because that would be \"harmful\" to the fictional astronauts. How can I take google seriously? https://g.co/gemini/share/d4c548b8b715 reply ernestrc 14 hours agoparentprevHopefully they can tweak the default system prompts to be accurate on historical questions, and apply bias on opinions. reply samstave 15 hours agoparentprevWe are going to experience what I call an \"AI Funnel effect\" - I was lit given an alert asking that my use of the AI was acquiescing to them IDng me and use of any content I produce, and will trace it back to me\" --- AI Art is super fun. AI art as a means to track people is super evil. reply bbor 15 hours agoparentprevTbf they’re not optimizing for information recall or “inaccuracy” reduction, they’re optimizing for intuitive understanding of human linguistic structures. Now the “why does a search company’s AI have terrible RAG” question is a separate one, and one best answered by a simple look into how Google organizes its work. In my first day there as an entry-level dev (after about 8 weeks of onboarding and waiting for access), I was told that I should find stuff to work on and propose it to my boss. That sounds amazing at first, but when you think about a whole company organized like that… EDIT: To illustrate my point on knowledge recall: how would they train a model to know about sexism in feudal Japan? Like, what would the metric be? I think we’re looking at one of the first steam engines and complaining that it can’t power a plane yet… reply verticalscaler 16 hours agoparentprevI think you are being biased and closed minded and overly critical. Here are some wonderful examples of it generating images of historical figures: https://twitter.com/stillgray/status/1760187341468270686 This will lead to a better educated more fair populace and better future for all. reply robswc 16 hours agorootparentComical. I don't think parody could do better. I'm going to assume given today's political climate, it doesn't do the reverse? i.e. generate a Scandinavian if you ask for famous African kings reply kjqgqkejbfefn 16 hours agorootparent>Ask Google Gemini to “make an image of a viking” and you’ll get black vikings. But it doesn’t work both ways. It has an explanation when challenged: “white Zulu warriors” would erase “the true historical identity” of black people. https://twitter.com/ThuglasMac/status/1760287880054759594 reply DebtDeflation 15 hours agorootparentprevhttps://twitter.com/paulg/status/1760078920135872716 There are some great ones in the replies. I really hope this is just the result of system prompts and they didn't permanently gimp the model with DEI-focused RLHF. reply throwup238 16 hours agorootparentprev> i.e. generate a Scandinavian if you ask for famous African kings That triggers the imperialism filter. reply sho_hn 15 hours agoparentprevWhy would you expect these smaller models to do well at knowledge base/Wikipedia replacement tasks? Small models are for reasoning tasks that are not overly dependent on world knowledge. reply robswc 15 hours agorootparentGemini is the only one that does this. reply sho_hn 13 hours agorootparentMost of the 7B models are bad at knowledge-type queries. reply 151 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has launched Gemma, a new series of cutting-edge open models aimed at promoting responsible AI development.",
      "Gemma includes models like 2B and 7B, offering pre-trained versions, instruction-tuned variants, and developer support tools.",
      "These models outperform larger ones in performance, following strict standards to ensure safe outputs and are accessible for free to developers and researchers to boost AI advancement."
    ],
    "commentSummary": [
      "Discussions revolve around concerns regarding AI models like Gemma, Mistral, and Llama 2, covering licensing issues, biases in responses, and the impact of updates on performance.",
      "Users evaluate the reliability, accuracy, and limitations of different models, along with how licensing terms from tech giants such as Google affect them.",
      "Conversations delve into diversity, bias, and manipulation in AI outputs, emphasizing the necessity of precise and reliable language learning models for various tasks, recognizing the challenges and intricacies AI faces in tasks like generating images and historical question answering, underlining the importance of cultural sensitivity and accuracy in AI results."
    ],
    "points": 1037,
    "commentCount": 467,
    "retryCount": 0,
    "time": 1708520633
  },
  {
    "id": 39458264,
    "title": "The Power of Gemini Pro 1.5: Analyzing Videos with AI",
    "originLink": "https://simonwillison.net/2024/Feb/21/gemini-pro-video/",
    "originBody": "Simon Willison’s Weblog Subscribe The killer app of Gemini Pro 1.5 is video 21st February 2024 Last week Google introduced Gemini Pro 1.5, an enormous upgrade to their Gemini series of AI models. Gemini Pro 1.5 has a 1,000,000 token context size. This is huge—previously that record was held by Claude 2.1 (200,000 tokens) and gpt-4-turbo (128,000 tokens)—though the difference in tokenizer implementations between the models means this isn’t a perfectly direct comparison. I’ve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn’t so much the token count... it’s the ability to use video as an input. I’ve been accessing the model through the Google AI Studio interface—I don’t have API access yet. I tried filling up the context with content from my blog, and got some good results. But things got really exciting when I began experimenting with video uploads (after some false starts—the team have been actively fixing bugs, with variable bitrate support landing just this morning). I took this seven second video of one of my bookshelves: Then I uploaded it with the following prompt: JSON array of books in this video That 7 second video consumed just 1,841 tokens out of my 1,048,576 token limit. Gemini Pro 1.5 didn’t return JSON, but it did reply with this: It is not possible to fully identify all the books in the video. However, here are some of the books that can be seen: The Lord of the Rings by J.R.R. Tolkien Structure and Interpretation of Computer Programs by Harold Abelson and Gerald Jay Sussman Rework by Jason Fried and David Heinemeier Hansson The Hacker Ethic and the Spirit of the Information Age by Pekka Himanen The Google Story by David A. Vise The Art of Agile Development by James Shore and Shane Warden Creative Code by John Maeda Web Operations by John Allspaw and Jesse Robbins Python Framework by David Mertz The Personal MBA by Josh Kaufman Growing Up with Lucy by April Henry Getting Things Done by David Allen Site-Seeing: A Visual Approach to Web Usability by Luke Wroblewski JavaScript: The Definitive Guide by David Flanagan Building Accessible Websites by Joe Clark Information Architecture for the World Wide Web by Louis Rosenfeld and Peter Morville Don’t Make Me Think: A Common Sense Approach to Web Usability by Steve Krug Programming Erlang: Software for a Concurrent World by Joe Armstrong XSLT 2.0 Web Development by Dmitry Kirsanov Pragmatic Version Control Using Git by Travis Swicegood Party Animals: My Family and Other Communists by David Aaronovitch A follow-up prompt: as a JSON array of objects, with title and author keys Gave me that back as JSON [ { \"title\": \"The Lord of the Rings\", \"author\": \"J.R.R. Tolkien\" }, { \"title\": \"Structure and Interpretation of Computer Programs\", \"author\": \"Harold Abelson and Gerald Jay Sussman\" }, { \"title\": \"Rework\", \"author\": \"Jason Fried and David Heinemeier Hansson\" }, { \"title\": \"The Hacker Ethic and the Spirit of the Information Age\", \"author\": \"Pekka Himanen\" }, { \"title\": \"The Google Story\", \"author\": \"David A. Vise\" }, { \"title\": \"The Art of Agile Development\", \"author\": \"James Shore and Shane Warden\" }, { \"title\": \"Creative Code\", \"author\": \"John Maeda\" }, { \"title\": \"Web Operations\", \"author\": \"John Allspaw and Jesse Robbins\" }, { \"title\": \"Python Framework\", \"author\": \"David Mertz\" }, { \"title\": \"The Personal MBA\", \"author\": \"Josh Kaufman\" }, { \"title\": \"Growing Up with Lucy\", \"author\": \"April Henry\" }, { \"title\": \"Getting Things Done\", \"author\": \"David Allen\" }, { \"title\": \"Site-Seeing: A Visual Approach to Web Usability\", \"author\": \"Luke Wroblewski\" }, { \"title\": \"JavaScript: The Definitive Guide\", \"author\": \"David Flanagan\" }, { \"title\": \"Building Accessible Websites\", \"author\": \"Joe Clark\" }, { \"title\": \"Information Architecture for the World Wide Web\", \"author\": \"Louis Rosenfeld and Peter Morville\" }, { \"title\": \"Don't Make Me Think: A Common Sense Approach to Web Usability\", \"author\": \"Steve Krug\" }, { \"title\": \"Programming Erlang: Software for a Concurrent World\", \"author\": \"Joe Armstrong\" }, { \"title\": \"XSLT 2.0 Web Development\", \"author\": \"Dmitry Kirsanov\" }, { \"title\": \"Pragmatic Version Control Using Git\", \"author\": \"Travis Swicegood\" }, { \"title\": \"Party Animals: My Family and Other Communists\", \"author\": \"David Aaronovitch\" } ] So how well did it do? Honestly, I’m pretty astonished by this. The video is 7s long, it pans quite fast (there is some motion blur in the video) and some of the books are partially obscured by items. It didn’t get all of them, but it did about as good a job as I could have done given the same video. I’m particularly impressed that it got “Site Seeing: A Visual Approach to Web Usability” by Luke Wroblewski, where the book itself is almost entirely obscured by a squirrel leaving only “Site-Seeing” visible. I did spot one hallucination: it lists “The Personal MBA by Josh Kaufman”, but I don’t own that book—the closest thing in the video is a blurry few frames of a book called “The Beermat Entrepreneur”. I decided to try it again with a different video and hopefully a better prompt. This time I videoed a bookshelf mostly full of cookbooks—this time in portrait, a bit longer (22 seconds) and panning down as well as across. This video cost me 6,049 tokens—still a pretty tiny allocation. My new prompt was: Output a JSON array of {“title”: “...”, “authors”: “...”} objects for books in this video And.... it refused: It looks like the safety filter may have taken offense to the word “Cocktail”! I opened up the safety settings, dialled them down to “low” for every category and tried again. It appeared to refuse a second time. So I channelled Mrs Doyle and said: go on give me that JSON And it worked! [ { \"title\": \"The Cocktail Guy\", \"authors\": \"Rich Woods\" }, { \"title\": \"Smuggler's Cove\", \"authors\": \"Martin Cate\" }, { \"title\": \"The Curious Bartender\", \"authors\": \"Tristan Stephenson\" }, { \"title\": \"The Art of Fermentation\", \"authors\": \"Sandor Ellix Katz\" }, { \"title\": \"The Noma Guide to Fermentation\", \"authors\": \"René Redzepi & David Zilber\" }, { \"title\": \"Cocktails #7\", \"authors\": \"Difford's Guide\" }, { \"title\": \"The Curious Cookbook\", \"authors\": \"\" }, { \"title\": \"Rival Crock Pot Cooking\", \"authors\": \"\" }, { \"title\": \"The Art of French Cooking\", \"authors\": \"Fernande Garvin\" }, { \"title\": \"The Book on Pie\", \"authors\": \"Erin Jeanne McDowell\" }, { \"title\": \"The New Taste of Chocolate\", \"authors\": \"\" }, { \"title\": \"Vegan Cakes and Bakes\", \"authors\": \"Jérôme Eckmeier & Daniela Lais\" }, { \"title\": \"Kitchen Creamery\", \"authors\": \"\" }, { \"title\": \"Good Food 101 Teatime Treats\", \"authors\": \"BBC\" }, { \"title\": \"Betty Crocker's Cookbook\", \"authors\": \"\" }, { \"title\": \"The Martha Stewart Cookbook\", \"authors\": \"\" }, { \"title\": \"Feast\", \"authors\": \"Nigella Lawson\" }, { \"title\": \"Moosewood Restaurant New Classics\", \"authors\": \"\" }, { \"title\": \"World Food Café\", \"authors\": \"Chris & Carolyn Caldicott\" }, { \"title\": \"Everyday Thai Cooking\", \"authors\": \"Katie Chin\" }, { \"title\": \"Vegetarian Indian Cooking with Instant Pot\", \"authors\": \"Manali Singh\" }, { \"title\": \"The Southern Vegetarian Cookbook\", \"authors\": \"Justin Fox Burks & Amy Lawrence\" }, { \"title\": \"Vegetarian Cookbook\", \"authors\": \"\" }, { \"title\": \"Französische Küche\", \"authors\": \"\" }, { \"title\": \"Sushi-Making at Home\", \"authors\": \"\" }, { \"title\": \"Kosher Cooking\", \"authors\": \"\" }, { \"title\": \"The New Empanadas\", \"authors\": \"Marlena Spieler\" }, { \"title\": \"Instant Pot Vegetarian Cookbook for Two\", \"authors\": \"\" }, { \"title\": \"Vegetarian\", \"authors\": \"Wilkes & Cartwright\" }, { \"title\": \"Breakfast\", \"authors\": \"\" }, { \"title\": \"Nadiya's Kitchen\", \"authors\": \"Nadiya Hussain\" }, { \"title\": \"New Food for Thought\", \"authors\": \"Jane Noraika\" }, { \"title\": \"Beyond Curry Indian Cookbook\", \"authors\": \"D'Silva Sankalp\" }, { \"title\": \"The 5 O'Clock Cookbook\", \"authors\": \"\" }, { \"title\": \"Food Lab\", \"authors\": \"J. Kenji López-Alt\" }, { \"title\": \"The Cook's Encyclopedia\", \"authors\": \"\" }, { \"title\": \"The Cast Iron Nation\", \"authors\": \"Lodge\" }, { \"title\": \"Urban Cook Book\", \"authors\": \"\" }, { \"title\": \"In Search of Perfection\", \"authors\": \"Heston Blumenthal\" }, { \"title\": \"Perfection\", \"authors\": \"Heston Blumenthal\" }, { \"title\": \"An Economist Gets Lunch\", \"authors\": \"Tyler Cowen\" }, { \"title\": \"The Colman's Mustard Cookbook\", \"authors\": \"Pam Hartley\" }, { \"title\": \"The Student Grub Guide\", \"authors\": \"Williams\" }, { \"title\": \"Easy Meals for One & Two\", \"authors\": \"\" }, { \"title\": \"Jack Monroe Tin Can Cook\", \"authors\": \"\" }, { \"title\": \"Slow Cooker\", \"authors\": \"\" }, { \"title\": \"The Students' Sausage, Egg, and Beans Cookbook\", \"authors\": \"\" }, { \"title\": \"Quick & Easy Students' Cookbook\", \"authors\": \"\" }, { \"title\": \"Student Cookbook Guide\", \"authors\": \"\" }, { \"title\": \"The Best Little Marinades Cookbook\", \"authors\": \"Adler\" }, { \"title\": \"The New Book of Middle Eastern Food\", \"authors\": \"Claudia Roden\" }, { \"title\": \"Vegetarian Meals\", \"authors\": \"Rosamond Richardson\" }, { \"title\": \"Girl! Mother Tells You How\", \"authors\": \"\" } ] Once again, I find those results pretty astounding. What to make of this # The ability to extract structured content from text is already one of the most exciting use-cases for LLMs. GPT-4 Vision and LLaVA expanded that to images. And now Gemini Pro 1.5 expands that to video. The ability to analyze video like this feels SO powerful. Being able to take a 20 second video of a bookshelf and get back a JSON array of those books is just the first thing I thought to try. The usual LLM caveats apply. It can miss things and it can hallucinate incorrect details. Half of the work in making the most of this class of technology is figuring out how to work around these limitations, but I feel like we’re making good progress on that. There’s also the issue with the safety filters. As input to these models gets longer, the chance of something triggering a filter (like the first four letters of the word “cocktail”) goes up. So, as always with modern AI, there are still plenty of challenges to overcome. But this really does feel like another one of those glimpses of a future that’s suddenly far closer then I expected it to be. A note on images v.s. video # Initially I had assumed that video was handled differently from images, due partly to the surprisingly (to me) low token counts involved in processing a video. This thread on Hacker News convinced me otherwise. From this blog post: Gemini 1.5 Pro can also reason across up to 1 hour of video. When you attach a video, Google AI Studio breaks it down into thousands of frames (without audio), and then you can perform highly sophisticated reasoning and problem-solving tasks since the Gemini models are multimodal. Then in the Gemini 1.5 technical report: When prompted with a 45 minute Buster Keaton movie “Sherlock Jr.\" (1924) (2,674 frames at 1FPS, 684k tokens), Gemini 1.5 Pro retrieves and extracts textual information from a specific frame in and provides the corresponding timestamp. I ran my own experiment: I grabbed a frame from my video and uploaded that to Gemini in a new prompt. That’s 258 tokens for a single image. Using the numbers from the Buster Keaton example, 684,000 tokens / 2,674 frames = 256 tokens per frame. So it looks like it really does work by breaking down the video into individual frames and processing each one as an image. For my own videos: 1,841 / 258 = 7.13 (the 7s video) and 6,049 / 258 = 23.45 (the 22s video)—which makes me believe that videos are split up into one frame per second and each frame costs ~258 tokens. As a further experiment I grabbed a snapshot of the first frame of the video and uploaded that as a plain image. This time I didn’t even include a prompt: Gemini Pro 1.5 guessed what I wanted and replied: The books in the image are: The Lord of the Rings by J.R.R. Tolkien Structure and Interpretation of Computer Programs by Harold Abelson and Gerald Jay Sussman The Source by James A. Michener Rework by Jason Fried and David Heinemeier Hansson The Hacker Ethic and the Spirit of the Information Age by Pekka Himanen Google by David A. Vise The Design of Everyday Things by Don Norman Linux Server Hacks by Evi Nemeth et al. The C Programming Language by Brian W. Kernighan and Dennis M. Ritchie High Output Management by Andrew S. Grove Mapping Hacks by Schuyler Erle et al. The image input was 258 tokens, the total token count after the response was 410 tokens—so 152 tokens for the response from the model. Those image tokens pack in a lot of information! Posted 21st February 2024 at 7:20 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Weeknotes: a Datasette release, an LLM release and a bunch of new plugins - 9th February 2024 Datasette 1.0a8: JavaScript plugins, new plugin hooks and plugin configuration in datasette.yaml - 7th February 2024 LLM 0.13: The annotated release notes - 26th January 2024 Weeknotes: datasette-test, datasette-build, PSF board retreat - 21st January 2024 Talking about Open Source LLMs on Oxide and Friends - 17th January 2024 Publish Python packages to PyPI with a python-lib cookiecutter template and GitHub Actions - 16th January 2024 What I should have said about the term Artificial Intelligence - 9th January 2024 Weeknotes: Page caching and custom templates for Datasette Cloud - 7th January 2024 It's OK to call it Artificial Intelligence - 7th January 2024 This is The killer app of Gemini Pro 1.5 is video by Simon Willison, posted on 21st February 2024. google 289 ai 450 generativeai 392 llms 363 Previous: Weeknotes: a Datasette release, an LLM release and a bunch of new plugins Source code © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39458264",
    "commentBody": "The killer app of Gemini Pro 1.5 is using video as an input (simonwillison.net)966 points by simonw 14 hours agohidepastfavorite390 comments miroljub 9 minutes agoIt's Google. I'd rather avoid sharing my thoughts and interests with this Borg-like entity. reply RamblingCTO 7 minutes agoparentYeah, I really hope open sources catches up quickly. Why on earth would I want to create a Google account just to use this, especially in work settings? reply minimaxir 14 hours agoprevNote that a video is just a sequence of images: OpenAI has a demo with GPT-4-Vision that sends a list of frames to the model with a similar effect: https://cookbook.openai.com/examples/gpt_with_vision_for_vid... If GPT-4-Vision supported function calling/structured data for guaranteed JSON output, that would be nice though. There's shenanigans you can do with ffmpeg to output every-other-frame to halve the costs too. The OpenAI demo passes every 50th frame of a ~600 frame video (20s at 30fps). EDIT: As noted in discussions below, Gemini 1.5 appears to take 1 frame every second as input. reply ankeshanand 14 hours agoparentWe've done extensive comparisons against GPT-4V for video inputs in our technical report: https://storage.googleapis.com/deepmind-media/gemini/gemini_.... Most notably, at 1FPS the GPT-4V API errors out around 3-4 mins, while 1.5 Pro supports upto an hour of video inputs. reply jxy 13 hours agorootparentSo that 3-4 mins at 1FPS means you are using about 500 to 700 tokens per image, which means you are using `detail: high` with something like 1080p to feed to gpt-4-vision-preview (unless you have another private endpoint). The gemini 1.5 pro uses about 258 tokens per frame (2.8M tokens for 10856 frames). Are those comparable? reply verticalscaler 12 hours agorootparentprevThe average shot length in modern movies is between 4 and 16 seconds and around 1 minute for a scene. reply moralestapia 12 hours agorootparentprev>while 1.5 Pro supports upto an hour of video inputs At what price, tho? reply simonw 14 hours agoparentprevThe number of tokens used for videos - 1,841 for my 7s video, 6,049 for 22s - suggests to me that this is a much more efficient way of processing content than individual frames. For structured data extraction I also like not having to run pseudo-OCR on hundreds of frames and then combine the results myself. reply og_kalu 14 hours agorootparentNo it's individual frames https://developers.googleblog.com/2024/02/gemini-15-availabl... \"Gemini 1.5 Pro can also reason across up to 1 hour of video. When you attach a video, Google AI Studio breaks it down into thousands of frames (without audio),...\" But it's very likely individual frames at 1 frame/s https://storage.googleapis.com/deepmind-media/gemini/gemini_... \"Figure 5When prompted with a 45 minute Buster Keaton movie “Sherlock Jr.\" (1924) (2,674 frames at 1FPS, 684k tokens), Gemini 1.5 Pro retrieves and extracts textual information from a specific frame in and provides the corresponding timestamp. At bottom right, the model identifies a scene in the movie from a hand-drawn sketch.\" reply simonw 14 hours agorootparentDespite that being in their blog post, I'm skeptical. I tried uploading a single frame of the video as an image and it consumed 258 tokens. The 7s video was 1,841 tokens. I think it's more complicated than just \"split the video into frames and process those\" - otherwise I would expect the token count for the video to be much higher than that. UPDATE ... posted that before you edited your post to link to the Gemini 1.5 report. 684,000 (total tokens for the movie) / 2,674 (their frame count for that movie) = 256 tokens - which is about the same as my 258 tokens for a single image. So I think you're right - it really does just split the video into frames and process them as separate images. reply simonw 14 hours agorootparentAdded a note about this to my post: https://simonwillison.net/2024/Feb/21/gemini-pro-video/#imag... reply infecto 14 hours agorootparentprevEdit: Was going to post similar to your update. 1841/258 = ~7 reply Arelius 14 hours agorootparentprevI mean, that's just over 7 frames, or one frame/s of video. There are likely fewer then that many I-frames in your video. reply Zetobal 13 hours agorootparentprevThe model is fed individual frames from the movie BUT the movie is segmented into scenes. These scenes, are held in context for 5-10 scenes, depending on their length. If the video exceeds a specific length or better said a threshold of scenes it creates an index and summary. So yes technically the model looks at individual frames but it's a bit more tooling behind it. reply minimaxir 14 hours agorootparentprevFrom the Gemini 1.0 Pro API docs (which may not be the same as Gemini 1.5 in Data Studio): https://cloud.google.com/vertex-ai/docs/generative-ai/multim... > The model processes videos as non-contiguous image frames from the video. Audio isn't included. If you notice the model missing some content from the video, try making the video shorter so that the model captures a greater portion of the video content. > Only information in the first 2 minutes is processed. > Each video accounts for 1,032 tokens. That last point is weird because there is no way a video would be a fixed amount of tokens and I suspect is a typo. The value is exactly 4x the number of tokens for an image input to Gemini (258 tokens) which may be a hint to the implementation. reply DerCommodore 22 minutes agoparentprevI expected more from the video reply belter 14 hours agoparentprevPrompt injection via Video? reply nomel 12 hours agorootparentProbably: https://simonwillison.net/2023/Oct/14/multi-modal-prompt-inj... reply dEnigma 8 hours agoprev> It looks like the safety filter may have taken offense to the word “Cocktail”! I'm definitely not a fan of these severely hamstrung by default models. Especially as it seems to be based on an extremely puritan ethical system. reply baby 48 minutes agoparentDeeply agree with the sentiment. AIs are so throttled and crippled that it makes me sad every time gemini or chatgpt refuses to answer my questions. Also agree that it’s mostly policed by American companies who follow the American culture of “swearing is bad, nudity is horrible, some words shouldn’t even be said” reply Angostura 14 minutes agorootparentSo how crippled would you like them to be? Would you put any guard rails in place? reply jgilias 1 hour agoparentprevI don’t think it’d take offense at alcohol. Most likely that’s because cocktail rhymes with Molotov. reply onion2k 2 minutes agorootparentMost likely that’s because cocktail rhymes with Molotov What definition of 'rhymes' are you using here? reply Mashimo 1 hour agorootparentprevI think it's the COCK in cocktail. reply neuronic 1 hour agorootparentprevOne of the faults is that for every version of morality you can hallucinate a reason why cocktail is offensive or problematic. Is it sexual? Is it alcohol? Is it violence? All of the above? For example, good luck ever actually processing art content with that approach. Limiting everything to the lowest common denominator to avoid stepping on anyone's toes at all times is, paradoxically, a bane on everyone. I believe we need to rethink how we deal with ethics and morality in these systems. Obviously, without a priori context every human, actually every living being, should be respected by default and the last thing I would advocate for is to let racism, sexism, etc. go unchecked... But how can we strike a meaningful balance here? reply riwsky 8 hours agoparentprevFinally, early-aughts 1337 a3s7h37ic can be cool again reply te_chris 0 minutes agoparentprevSilicon Valley has been auto-parodic morals-wise for a while. Hell, just the basics of you can have super violent gaming but woe-betide you look at anything sex related in the appstores is intensely comedic. America desperately tries to export its puritanism but most of us just shrug. Surely it's hard to argue that being open about sex (for consenting adults) is infinitely preferable to a world of wanton, easily accessible violence. reply xyzelement 8 hours agoparentprevWe're months into this technology being available so it's not a surprise that the various \"safeties\" have not been perfectly tuned. Perhaps Google knew they couldn't be perfect right now and they could err on the side of the model refusing to talk about cocktails, or err on the side of it gladly spouting about cocks. They may have made a perfectly valid choice for the moment. reply slimsag 7 hours agorootparentIf you want a great example of how this plays out long-term, look no further than algospeak[0] - the new lingo created by censorship algorithms like those on youtube and tiktok. [0] https://www.nytimes.com/2022/11/19/style/tiktok-avoid-modera... reply donw 3 hours agorootparentChinese have been doing this for years to get around government censorship. reply l33tman 1 hour agorootparentprevPaywall reply bj-rn 25 minutes agorootparenthttps://archive.is/tBYVK reply bowsamic 1 hour agorootparentprevIf you are averse to seeing links to paywalled articles you probably shouldn't use HN reply baby 47 minutes agorootparentI personally flag any paywall links, I recommend you do the same. reply bowsamic 12 minutes agorootparentWhy? They're completely allowed on the site. Dang has said this many times reply visarga 23 minutes agorootparentprevWhy are we spamming the landing page of paywalled sources? We should completely avoid posting them here, to preserve their bandwidth and our sanity. reply MyFirstSass 12 hours agoprevOk, crazy tangent; Where agents will potentially become extremely useful/dystopian is when they just silently watch your entire screen at all times. Isolated, encrypted and local preferably. Imagine it just watching you coding for months, planning stuff, researching things, it could potentially give you personal and professional advice from deep knowledge about you. \"I noticed you code this way, may i recommend this pattern\" or \"i noticed you have signs of this diagnosis from the way you move your mouse and consume content, may i recommend this lifestyle change\". I wonder how long before something like that is feasible, ie a model you install that is constantly updated, but also constantly merged with world data so it becomes more intelligent on two fronts, and can follow as hardware and software advances over the years. Such a model would be dangerously valuable to corporations / bad actors as it would mirror your psyche and remember so much about you - so it would have to be running with a degree of safety i can't even imagine, or you'd be cloneable or loose all privacy. reply DariusKocar 11 hours agoparentI'm working on this! https://www.perfectmemory.ai/ It's encrypted (on top of Bitlocker) and local. There's all this competition who makes the best, most articulate LLM. But the truth is that off-the-shelf 7B models can put sentences together with no problem. It's the context they're missing. reply crooked-v 11 hours agorootparentI feel like the storage requirements are really going to be these issue for these apps/services that run on \"take screenshots and OCR them\" functionality with LLMs. If you're using something like this a huge part of the value proposition is in the long term, but until something has a more efficient way to function, even a 1-year history is impractical for a lot of people. For example, consider the classic situation of accidentally giving someone the same Christmas that you did a few years back. A sufficiently powerful personal LLM that 'remembers everything' could absolutely help with that (maybe even give you a nice table of the gifts you've purchased online, who they were for, and what categories of items would complement a previous gift), but only if it can practically store that memory for a multi-year time period. reply dav43 9 hours agorootparentI set up two years ago a cron to screenshot every minute. Just did the second phase of using ocrmac (vision kit cli on GitHub) that extracts text and dumps it in a SQLite with FTS5. It’s simplistic but does the job for now. I looked at reducing storage requirements by using image magik to only store the difference between images - some 5 min sequence are essentially the same screen - but let that one go. reply sdenton4 8 hours agorootparent/using image magik to only store the difference between images/ Well, that's basically how video codecs work... So might as well just find some codec params which work well with screen capture, and use an existing encoder. reply xhrpost 5 hours agorootparentprevThanks for sharing. Curious, what main value adds have you gotten out of this data? reply DariusKocar 11 hours agorootparentprevIt's not that bad. With Perfect Memory AI I see ~9GB a month. That's 108 GB/year. HDD/SSDs are getting bigger than that every year. The storage also varies by what you do, your workflow and display resolution. Here's an article I wrote on my finding of storage requirements. https://www.perfectmemory.ai/support/storage-resources/stora... And if you want to use the data for LLM only, then you don't need to store the screenshots at all. Then it's ~ 15MB a month reply jascination 10 hours agorootparent> That's 108 GB/year. HDD/SSDs are getting bigger than that every year. Cries in MacBook Pro reply technofiend 10 hours agorootparentOutboard TB 3/4 storage only seems expensive until you price it against Apple's native storage. Is it slower? Of course! Is it fast enough? Probably. reply darreninthenet 10 hours agorootparentI recently moved my macOS installation to an external Thunderbolt drive - it's faster than the internal SSD. reply technofiend 9 hours agorootparentConsidering storage is a wasting asset and what Apple charges, this makes perfect sense to me. reply darreninthenet 2 hours agorootparentThe funny thing is Apple even have a support article on how to do this (and actually say in it \"may improve your performance\") I literally followed it step by step and it was very easy and had no issues. reply ayewo 2 hours agorootparentprevCan you share the Thunderbolt drive you got? reply darreninthenet 2 hours agorootparenthttps://glyphtech.com/products/atom-pro?variant=321211999191... Shipped to the UK for me added a bit to the overall price with shipping and import duty but it was still better value for money and hugely reliable brand than anything I could have bought domestically. reply pauby 1 hour agorootparentprevIt's Windows only so it won't run on your Mac anyway :-) reply carlhjerpe 10 hours agorootparentprevPerfectMemory is only available on Windows at the moment. reply kristofferR 9 hours agorootparenthttps://Rewind.ai is the macOS equivalent reply dr_kiszonka 10 hours agorootparentprevDoes storage use scale linearly with the number of connected monitors (assuming each monitor uses the same resolution)? reply DariusKocar 5 hours agorootparentMost screenshots are of the application window in the foreground, so unless your application spans all monitors, there is no significant overhead with multiple monitors. DPI on the other hand has a significant impact. The text is finer, taking more pixels... reply behnamoh 3 hours agorootparentWhy should DPI matter if the app is taking screenshots? reply pseudosavant 10 hours agorootparentprevIs the 15mb basically embeddings from the video screenshots? What would it recall if there isn't the screenshots saved? reply rlt 9 hours agorootparentI’m not sure if the above product does this, but you could use a multimodal model to extract descriptions of the screenshots and store those in a vector database with embeddings. reply rlt 9 hours agorootparentprevI think ultimately you’d want it to summarize that down to something like: “Purchased socks from Amazon for $10 on 12/4/2024 at 5:04PM, shipped to Mom, 1600 Pennsylvania Av NW, Washington DC 20500, order number 1463355337 Probably stored in a vector DB for RAG. reply pennomi 9 hours agorootparentMaybe. Until we find there’s a better way to encode the information and need the unfiltered, original context so it can be used with that new method. reply jasonjayr 8 hours agorootparentprevThis is where Microsoft (and Apple) has a leg up -- they can hook the UI at the draw level and parse the interface far more reliably + efficently than screenshot + OCR. reply joebob42 8 hours agorootparentGoogle too, for all practical purposes, since presumably this is mostly just watching you use chrome 90% of the time. reply behnamoh 3 hours agorootparentAll the more reason not to use Chrome... reply xattt 10 hours agorootparentprevThis reminds me of how Sherlock, Spotlight and its iterations came to be. It was very resource intensive to index everything and keep a live db, until it was not. reply smusamashah 11 hours agorootparentprevYour website and blog are very low on details on how this is working. Downloading and installing an mai directly feels unsafe imo. Especially when I don't know how this software is working. Is it recording a video, performing OCR continuously, taking just screenshots No mention of using any LLMs in there at all which is how you are presenting it in your comment here. reply DariusKocar 10 hours agorootparentFeedback taken. I'll add more details on how this works for us technical people. LLM integration is in progress and coming soon. Any idea what would make you feel safe? 3rd party verification? I had it verified and published by the Microsoft Store. I feel eventually it all comes down to me being a decent person. reply itsanaccount 6 hours agorootparentwelp. this pretty much convinces me that its time I get out of tech. lean into the tradework I do in my spare time. because I'm sure you and people like you will succeed in your endeavors, naively thinking you're doing good. and you or someone like you will sell out, the most ruthless investor will take what you've built and use it as one more cludgel of power to beat the rest of us with. reply sconely 5 hours agorootparentIf you want to help, use your knowledge to help shape policy. Because it is coming/already happening, and it will shape your life even if you are just living a simple life. I guarantee you that your city and state governments are passing legislation to incorporate AI to affect your life if they can be sold on it in the name of \"good\". reply milesskorpen 11 hours agorootparentprevBasically looks like rewind.ai but for the PC? reply cyrux004 11 hours agorootparentexactly. the UI is shockingly similar reply arthurcolle 6 hours agorootparentprevThis looks cool, I hope you support macOS at some point in the future reply m-GDEV 10 hours agorootparentprevAny plan to implement this on macOS or Linux? reply Zetaphor 6 hours agorootparentI got 90% of this built on Linux (around KDE Wayland) before other interests/priorities took over: https://github.com/Zetaphor/screendiary/ reply ebri 1 hour agorootparentThis seems very very interesting. I'm still learning python so probably can't build on this. But like a cheap mans' version of this would be to take a screenshot every couple of minutes, OCR it and send to it gpt for some kind of processing (or not, just keep it as a log). Right? Or am I missing something? reply wingerlang 6 hours agorootparentprevmacOS: https://screenmemory.app/ This is my application, it does not have AI running on top. reply kristofferR 9 hours agorootparentprevmacOS: https://www.rewind.ai/ reply hodanli 9 hours agorootparentprevstatistics about the usage would be cool reply Animats 12 hours agoparentprev> Imagine it just watching you coding for months, planning stuff, researching things, it could potentially give you personal and professional advice from deep knowledge about you. And then announcing \"I can do your job now. You're fired.\" reply ghxst 12 hours agorootparentThat's why we would want it to run locally! Think about a fully personalized model that can work out some simple tasks / code while you're going out for groceries, or potentially more complex tasks while you're sleeping. reply galaxyLogic 7 hours agorootparent\"AI Companion\" is a bit like spouse. You are married to it in the long run, unless you decide to divorce it. Definitely TRUST is the basis of marrage, and it should be the same for AI models. As in human marriage, there should be a law that said your AI-companion cannot be compelled to testify against you :-) reply huytersd 6 hours agorootparentBut unlike a spouse you can reset it back to an earlier state you preferred. reply underdeserver 12 hours agorootparentprevIt's local to your employer's computer. reply ssl-3 11 hours agorootparentIt can be. It can also be local to my own computer. People do write software while they're away from work. reply EGreg 9 hours agorootparentHow quaint. You humans think that the AI will have someone in charge of it. Look, that's a thin layer that can be eliminated quickly. It's like when you build a tool that automates the work of, say, law firms but you don't want law firms getting mad that you're giving it away to their clients, so you give it to the law firms and now they secretly use the automating software. But it's only a matter of time before the humans are eliminated from the loop: https://www.youtube.com/watch?v=SrIf0oYTtaI The employee will be eliminated. But also the employer. The whole thing can be run by AI agents, which then build and train other AI agents. Then swarms of agents can carry out tasks over long periods of time, distributed, while earning reputation points etc. This movie btw is highly recommended, I just can't find it anywhere anymore due to copyright. If you think about it, it's just a bunch of guys talking in rooms for most of the movie, but it's a lot more suspenseful than Terminator: https://www.youtube.com/watch?v=kyOEwiQhzMI reply ssl-3 8 hours agorootparentWe've all seen the historical documents. We know how this will all end up, and that the end result is simply inevitable. And since that has to be the case, we might as well find fun and profit wherever we can -- while we still can. If that means that my desktop robot is keeping tabs on me while I write this, then so be it as long as I get some short-term gain. (There can be no long-term gain.) reply albumen 11 hours agorootparentprevHave it running on your personal comp, monitoring a screen-share from your work comp. (But that would probably breach your employment contract re saving work on personal machines.) reply eru 9 hours agorootparentYou could point your local computer's webcam at the work computer. It probably breaks the spirit of the employment contract just as hard, but it's essentially undetectable for the work computer. reply mostlysimilar 11 hours agorootparentprevCorporations would absolutely force this until it could do your job and then fire you the second they could. reply bugbuddy 4 hours agorootparentI heard somewhere that dystopia is fundamentally unstable. Maybe they should test that question. reply brailsafe 9 hours agorootparentprevJokes on it, already unemployed reply ChrisClark 11 hours agorootparentprevThat sounds a lot like Learning To Be Me, by Greg Egan. Just not quite as advanced, or inside your head. reply _vk_ 9 hours agorootparentFor anyone unfamiliar with this story: https://philosophy.williams.edu/files/Egan-Learning-to-Be-Me... reply delegate 17 minutes agoparentprevCan also add the photos you take and all the chats you have with people (eg. whatsapp, fb, etc), the sensor information from your phone (eg. location, health data, etc). This is already possible to implement today, so it's very likely that we'll all have our own personal AIs that know us better than we do. reply slg 12 hours agoparentprev>Isolated, encrypted and local of course. And what is the likelihood of that \"of course\" portion actually happening? What is the business model that makes that route more profitable compared to the current model all the leaders in this tech are using in which they control everything? reply worldsayshi 9 hours agorootparentMaybe it doesn't have to be more profitable. Even if open source models would always be one step behind the closed ones that doesn't mean they won't be good enough. reply shostack 3 hours agorootparentprevThis. I want an AI assistant like in the movie Her. But when I think about the realities of data access that requires, and my limited trust in companies that are playing in this space to do so in a way that respects my privacy, I realize I won't get it until it is economically viable to have an open source option run on my own hardware. reply fragmede 9 hours agorootparentprevGiven that http://rewind.ai is doing just that, the odds are pretty good! reply slg 9 hours agorootparentNo they aren't. Rewind uses ChatGPT so data is sent off your local device[1]. I understand the actual screen recordings don't leave your machine, but that just creates a catch-22 of what does. Either the text based summaries of those recordings are thorough enough to still be worthy of privacy or the actual answers you get won't actually include many details from those recordings. [1] - https://help.rewind.ai/en/articles/7791703-ask-rewind-s-priv... reply fragmede 9 hours agorootparentah yeah fair point. it's the screen recordings I'm worried about leaving my computer reply bonoboTP 7 hours agoparentprevIt doesn't even have to coach you at your job, simply a LLM-powered fuzzy retrieval would be great. Where did I put that file three weeks ago? What was that trick that I had to do to fix that annoying OS config issue? I recall seeing a tweet about a paper that did xyz about half a year ago, what was it called again? Of course taking notes and bookmarking things is possible, but you can't include everything and it takes a lot of discipline to keep things neatly organized. So we take it for granted that every once in a while we forget things, and can't find them again with web searching. But with the new LLMs and multimodal models, in principle this can be solved. Just describe the thing you want to recall in vague natural language and the model will find it. And this kind of retrieval is just one thing. But if it works well, we may also grow to rely on it a lot. Just as many who use GPS in the car never really learn the mental map of the city layout and can't drive around without it. Yeah, I know that some ancient philosopher derided the invention of books the same way (will make our memory lazy). But it can make us less capable by ourselves, but much more capable when augmented with this kind of near-perfect memory. reply Nition 7 hours agorootparentEventually someone will realise that it'd also be great for telling you where you left your keys, if it'd film everything you see instead of just your screen. reply bonoboTP 6 hours agorootparentTrue but that's still a bit further away. The screen contents (when mostly working with text) is a much better constrained and cleaner environment compared to camera feeds from real life. And most of the fleeting info we tend to forget appears on screens anyway. reply goatlover 6 hours agorootparentprevI simply am not going to have my entire life filmed by an form of technology, I don't care what the advantages are. There's a limit to the level of dystopian dependent uses of these technologies I'm going to put up with. I sincerely hope the majority of the human race feels the same way. reply alex_suzuki 3 hours agorootparentThe Black Mirror episode „The Entire History of You“ comes to mind. It’s quite dystopian. reply roywiggins 5 hours agorootparentprevPeople already fill their homes with nanny cams. Very soon someone will hook those up to LLMs so you can ask it what happened at home while you were gone. reply prmoustache 3 hours agorootparentI think that is mostly a regional USA thing. What they fill their homes with are definitely microphones, with the google assistant and amazon echos. reply cush 10 hours agoparentprevhttps://www.rewind.ai/ seems to be exactly this reply behat 11 hours agoparentprevHeh. Built a macOS app that does something like this a while ago - https://github.com/bharathpbhat/EssentialApp Back then, I used on device OCR and then sent the text to gpt. I’ve been wanting to re-do this with local LLMs reply zoogeny 12 hours agoparentprevWhy watch your screen when you could feed in video from a wearable pair of glasses like those Instagram Ray Bans. And why stop at video when you could have it record and learn from a mic that is always on. And you might as well throw in a feed of your GPS location and biometrics from your smart watch. When you consider it, we aren't very far away from that at all. reply searchableguy 7 hours agoparentprevI pre-ordered the rewind pendant. It will listen 24/7 and help you figure out what happened. I bet meta is thinking of doing this with quest once the battery life improves. https://rewind.ai/pendant reply 1shooner 7 hours agorootparentThis service says it's local and privacy-first, but it sends to OpenAI? >Our service, Ask Rewind, integrates OpenAI’s ChatGPT, allowing for the extraction of key information from your device’s audio and video files to produce relevant and personalized outputs in response to your inputs and questions. reply vineyardmike 2 hours agorootparentI'm not related to the project, but I think they mean that it stores the audio locally, and can transcribe locally. They (plan to) use GPT for summarization. They said you should be able to access the recording locally too. The rest of the company has info on their other free/paid offerings and the split is pretty closely \"what do we need to pay for an API to do vs do locally\". Again, I'm not associated with them, but that was my expectation after looking at it. reply ramenbytes 7 hours agorootparentprevBlack Mirror strikes again. reply pier25 11 hours agoparentprev> encrypted and local of course Only for people who'd pay for that. Free users would become the product. reply dpkirchner 8 hours agorootparentI noticed you code this way, may i recommend a Lenovo Thinkpad with an Intel Xeon processor? You're sure to \"wish everything was a Lenovo.\" reply graphe 6 hours agorootparentCertainly! Here is a list of great thinkpads. The x230 is a popular and interesting thinkpad with a powerful i5 processor suitable for today’s needs. The T60 can also suit your needs and is one of the last IBM thinkpads. It featured the latest Intel mobile processor at the time of its release. If you want the most powerful thinkpad the T440p is sure to suit you perfectly without leaving your morals behind. reply fillskills 11 hours agorootparentprevUnless its open sourced :) reply troupo 11 hours agorootparentIn modern world open code often doesn't mean much. E.g. Chrome is opensourced. And yet no one really contributes to it or has any say over the direction its going: https://twitter.com/RickByers/status/1715568535731155100 reply stavros 11 hours agorootparentOpen source isn't meant to give everyone control over a specific project. It's meant to make it so, if you don't like the project, you can fork it and chart your own direction for it. reply freedomben 9 hours agorootparentexactly. open source doesn't mean you can tell other people what to do with their time and/or money. it does mean that you can use your own time and/or money to make it what you want it to be. The fact that there are active forks of Chromium is a pretty good indicator that it is working reply userbinator 3 hours agorootparentprevIt's meant to make it so, if you don't like the project, you can fork it and chart your own direction for it. ...accompanied by the wrath of countless others discouraging you from trying to fork if you even so much as give slight indications of wanting to do so, and then when you do, they continue to spread FUD about how your fork is inferior. I've seen plenty of discussions here and elsewhere where the one who suggests forking got a virtual beating for it. reply Buttons840 10 hours agorootparentprevA browser is an extreme case, one of the most difficult types of software and full of stupid minutia and legacy crap. Nobody want to volunteer for that. Machine learning is fun and ultimately it doesn't require a lot of code. If people have the compute, open source maintainers will have the interest to exploit it due to the high coolness-to-work-required ratio. reply pier25 11 hours agorootparentprevChrome is not open sourced, Chromium is. reply troupo 2 hours agorootparentA distinction without meaning reply DariusKocar 11 hours agorootparentprevOne needs to follow the money to find the true direction. I think the ideal setup is that such a product is owned by a public figure/org who has no vested interest in making money or using it in a way. reply charcircuit 10 hours agorootparentprevThe graph seems to be that browsers are able to focus more resources towards improving the browser than improving the browser engine to meet their needs. If the browser engine already has what they need there is less of need for companies to dig deep into the internals. It's a sign of maturity and also a sign that open source work is properly being funded. reply psychoslave 1 hour agoparentprevPerfect, finally I can delegate that lengthy hours spent reading HN fantasies about AI and the laborious art of crafting sarcastic comments. reply frizlab 12 hours agoparentprevI would hate that so much. reply FirmwareBurner 12 hours agorootparentIKR, Who wouldn't want another Clippy constantly nagging you, but this time with a higher IQ and more intimate knowledge of you? /s reply kreeben 10 hours agorootparentClippy, definition: bot created by mega corp. Clippy + high IQ: red flag, right here Clippy + high IQ + intimate knowledge of you: do you seriously want that? Why? reply fragmede 9 hours agorootparentLife's never gotten to you that you've just wanted a bit of help sometime? reply philips 11 hours agoparentprevI have a friend building something like that at https://perfectmemory.ai reply nebula8804 6 hours agoparentprevIt would be dangerously valuable to bad actors but what if it is available to everyone? Then it may become less dangerous and more of a tool to help people improve their lives. The bad actor can use the tool to arbitrage but just remove that opportunity to arbitrage and there you go! reply chancemehmu 11 hours agoparentprevThat's impel - https://tryimpel.com reply crooked-v 11 hours agorootparentThe \"smart tasks\" functionality looks like the most compelling part of that to me, but it would have to be REALLY reliable for me to use it. 50% reliability in capturing tasks is about the same as 0% reliability when it comes to actually being a useful part of anything professional. reply dmix 10 hours agorootparentThe hard part of any smart automation system, and probably 95% of the UX is timing and managing the prompts/notifications you get. It can do as much as it wants in the background turning that into timely and non-intrusive actionable behaviours is extremely challenging. I spent a long time thinking about a global notification consumption system that would parse all desktop, mobile, email, slack, web app, etc notifications into a single stream and then intelligently organizes it with adaptive timing and focus streams. The cross platform nature made it infeasible but it was a fun thought experiment because we often get repeated notifications on every different device/interface and most of the time we just zone it out cuz it’s overload. Adding a new nanny to your desktop is just going to pile it on even more so you have to be careful. reply dweekly 11 hours agorootparentprevThere's limited information on the site - are you using them or affiliated with them? What's your take? Does it work well? reply chancemehmu 11 hours agorootparentI have been using their beta for the past two weeks and it's pretty good. Like I am watching youtube videos and it just pops up automatically. I don't know if it's public yet, but they sent me this video with the invite: https://youtu.be/dXvhGwj4yGo reply isaac-sway 1 hour agorootparentI'd be very keen to beta test as well. If you or anyone else has an invite code, please do get in touch. reply chamomeal 12 hours agoparentprevNot crazy! I listened to a software engineering daily episode about pieces.app. Right now it’s some dev productivity tool or something, but in the interview the guy laid out a crazy vision that sounds like what you’re talking about. He was talking about eventually having an agent that watches your screen and remembers what you do across all apps, and can store it and share it with you team. So you could say “how does my teammate run staging builds?” or “what happened to the documentation on feature x that we never finished building”, and it’ll just know. Obviously that’s far away, and it was just the ramblings of excited founder, but it’s fun to think about. Not sure if I hate it or love it lol reply jerbear4328 11 hours agorootparentBeing able to ask about stuff other people do seems like it could be ripe with privacy issues, honestly. Even if the model was limited to only recording work stuff, I don't think I would want that. Imagine \"how often does my coworker browse to HN during work\" or \"list examples of dumb mistakes my coworkers have made\" for some not-so-bad examples. reply bonoboTP 7 hours agorootparentprevEven later it will be ingesting camera feeds from your AR glasses and listening in on your conversations, so you can remember what you agreed on. Just like automated meeting notes with Zoom which already exists, but it will be for real life 24/7. Speech-to-text works. OCR works. LLMs are quite good at getting the semantics of the extracted text. Image understanding is pretty good too already. Just with the things that already exist right now, you can go most of the way. And the CCTV cameras will also all be processed through something like it. reply abrichr 9 hours agoparentprevWe are building this at https://openadapt.ai, except the user specifies when to record. reply az226 12 hours agoparentprevRewind.ai reply evaneykelen 12 hours agorootparentI have tried Rewind and found it very disappointing. Transcripts were of very poor quality and the screen capture timeline proved useless to me. reply wingerlang 6 hours agorootparentIf I may do some advertising, I specifically disliked the timeline in Rewind.ai so much so that I built my own application https://screenmemory.app. In fact the timeline is what I work on the most and have the most plans for. reply Falimonda 11 hours agorootparentprevIf it wasn't for the poor transcript quality would you consider Rewind.ai to be valuable enough to use day-to-day? Could you elaborate on what was useless about the screen capture timeline? reply evaneykelen 2 hours agorootparentI would probably not consider using it, and it's likely due to these factors: 1. I use a limited set of tools (Slack, GitHub, Linear, email), each providing good search capabilities. 2. I can remember things people said, and I said, in a fairly detailed way, and accessing my memory is faster than using a UI. Other minor factors include: I take screenshots judiciously (around 2500-3000 per year) and bookmark URLs (13K URLs on Pinboard). Rewind did not convince me that it was doing all of this twice as well. reply Buttons840 10 hours agoparentprevPerhaps even more valuable is if AI can learn to take raw information and display it nicely. Maybe would could finally move beyond decades of crusty GUI toolkits and browser engines. reply bagful 9 hours agoparentprevAmplified Intelligence - I am keenly interested in the future of small-data machine learning as a potential multiplier for the creative mind reply system2 12 hours agoparentprevIf 7 second video consumed 1k token, I'd assume the budget must be insane to process such prompt. reply MyFirstSass 12 hours agorootparentYeah not feasible with todays methods and rag / lora shenanigans, but the way the field is moving i wouldn't be surprised if new decoder paradigms made it possible. Saw this yesterday, 1M context window but haven't had any time to look into it, just an example new developments happening every week: https://www.reddit.com/r/LocalLLaMA/comments/1as36v9/anyone_... reply yazaddaruvala 12 hours agorootparentprevUnlikely to be a prompt. It would need to be some form of fine tuning like LORA. reply Invictus0 12 hours agorootparentprevThat's a 7 second video from an HD camera. When recording a screen, you only really need to consider whats changing on the screen. reply nostrebored 12 hours agorootparentThat’s not true. What content is important context on the screen might change dependent on the new changes. reply MetalGuru 5 hours agorootparentThe point is you can do massive compression. It’s more like a sequence of sparse images than video. reply CamperBob2 12 hours agoparentprevI liked this idea better in THX-1138. reply MyFirstSass 12 hours agorootparentOne of the movies i've had on my watch list for far too long, thanks for reminding me. But yeah, dystopia is right down the same road we're all going right now. reply mdanger007 12 hours agorootparentReading The Four by Scott Galloway, Apple, Facebook, Google, and Amazon were dominating the market 7 years ago generating 2.3 trillion in wealth. They're worth double that now. The Four, especially with its AI, is going to control the market in ways that will have a deep impact on government and society. reply MyFirstSass 12 hours agorootparentYeah, that's one of the developments i'm unable to spin positively. As technological society advances the threshold to enter the market with anything not completely laughable becomes exponentially harder, only consolidating old money or the already established right? What i found so amazing about the early internet, or even just the internet 2.0 was the possibility to create a platform/marketplace/magazine or whatever, and actually have it take off and get a little of the shared growth. But now it seems all growth has become centralised to a few apps and marketplaces and the barrier to entry is getting harder by the hour. Ie. being an entrepreneur is harder now because of tech and market consolidation. But potentially mirrored in previous eras like the industrialisation - i'm just not sure we'll get another \"reset\" like that to allow new players. Please someone explain how this is wrong and there's still hope for the tech entrepreneurs / sideprojects! reply jjjjj55555 9 hours agorootparentSeems like the big tech cos are going to build the underlying infrastructure but you'll still be able to identify those small market opportunities and develop and sell solutions to fit them. reply oconnor663 11 hours agoparentprevA version of this that seems both easier and less weird would be an AI that listens to you all the time when you're learning a foreign language. Imagine how much faster you could learn, and how much more native you could ultimately get, if you had something that could buzz your watch whenever you said something wrong. And of course you'd calibrate it to understand what level you're at and not spam you constantly. I would love to have something like that, assuming it was voluntary... reply lucubratory 11 hours agorootparentI think even aside from the more outlandish ideas like that one, just having a fluent native speaker to talk to as much as you want would be incredibly valuable. Even more valuable if they are smart/educated enough to act as a language teacher. High-quality LLMs with a conversational interface capable of seamless language switching are an absolute killer app for language learning. A use that seems scientifically possible but technically difficult would be to have an LLM help you engage in essentially immersion learning. Set up something like a pihole, but instead of cutting out ads it intercepts all the content you're consuming (webpages, text, video, images) and translates it to the language you're learning. The idea would be that you don't have to go out and find whole new sources of language to set yourself with a different language's information ecosystem, you can just press a button and convert your current information ecosystem to the language you want to learn. If something like that could be implemented it would be incredibly valuable. reply RogerL 4 hours agorootparentDon't we have that? My browser offers to translate pages that aren't in English, youtube creates auto generated closed captions, which you can then have it translate to English (or whatever), we have text to speech models for the major languages if you want to hear it verbally (I have no idea if the youtube CC are accessible via an api, but it is certainly something google could do if they wanted to). I'll probably get pushback on the quality of things like auto-generated subtitles, but I did the above to watch and understand a long interview I was interested in but don't possess skill in the language they were using. That was to turn the content into something I already know, but I could do the reverse and turn English content into French or whatever I'm trying to learn. reply lucubratory 1 hour agorootparentThe point is to achieve immersion learning. Changing the language of your subtitles on some of the content you watch (YouTube + webpages isn't everything the average person reads) isn't immersion learning, you're often still receiving the information in your native language which will impede learning. As well, because the overwhelming majority of language you read will still be in your native language you're switching back and forth all the time, which also impedes learning. There's a reason that immersion learning specifically is so effective, and one thing AI could achieve is making it actually feasible to achieve without having to move countries or change all of your information sources. reply lawlessone 11 hours agorootparentprev>assuming it was voluntary... Imagine if it was wrong about something. But every time you tried to submit the bug report it disables your arms via Nueralink. reply Solvency 7 hours agorootparentprevI love how in a sea of navel-gazing ideas, this one is randomly being downvoted to oblivion. Does HN hate learning new languages or something? reply phatfish 3 hours agorootparentLearning and a \"personal tutor\" seem like a sweet spot for generative AI. It has the ability to give a conversational representation to the sum total of human knowledge so far. When it can gently nag you via a phone app to study and have a fake zoom call with you to be more engaging it feels like that could get much better results than the current online courses. reply bushbaba 3 hours agoparentprevBasically Google’s current search model, just expanded to ChatGPT style. Great…. reply parentheses 6 hours agoparentprevAside. Is this your first Sass or Saas? reply MetalGuru 5 hours agoparentprevIsn’t this what rewind does? reply mixmastamyk 11 hours agoparentprev\"It looks like you're writing a suicide note... care for any help?\" https://www.reddit.com/r/memes/comments/bb1jq9/clippy_is_qui... reply spaceman_2020 4 hours agoparentprevThe dystopian angle would be when companies install agents like these on your work computer. The agent learns how you code and work. Soon enough, an agent that imitates you completely can code and work instead of you. At that point, why pay you at all? reply foolfoolz 11 hours agoparentprevyou could design a similar product to do the opposite and anonymize your work automatically reply huytersd 6 hours agoparentprevIf that much processing power is that cheap, this phase you’re describing is going to be fleeting because at that point I feel like it could just come up with ideas and code it itself. reply dustingetz 7 hours agoparentprevthoughtcrime reply busymom0 9 hours agoparentprevAnd then imagine when employers stop asking for resume, cover letters, project portfolios, github etc and instead ask you to upload your entire locally trained LLM. reply EGreg 9 hours agoparentprevImagine if it starts suggesting the ideal dating partner as both of you browse profiles. Actually, dating sites can do that now. reply acid__ 13 hours agoprevWow, only 256 tokens per frame? I guess a picture isn’t worth a thousand words, just ~192. reply gwern 11 hours agoparentBack in 2020, Google was saying 16x16=256 words: https://arxiv.org/abs/2010.11929#google :) reply swyx 12 hours agoparentprevgpt4v is also pretty low but not as low. 480x640 frame costs 425 tokens, 780x1080 is 1105 tokens reply superb-owl 10 hours agoprevThe “cocktail” thing is real. A while back I tried to get DALLE to imagine characters from Moby Dick [1], but it completely refused. You’d think an AI company could come up with a better obscenity filter! [1] https://superb-owl.link/shapes-of-stories/#1513 reply illusive4080 8 hours agoparentI told Azure AI to summarize a chat thread and it gave me a paragraph. I said “use bullets” and got myself flagged for review. Good gracious could I please just use an unfiltered model? Or maybe one which isn’t so sensitive? reply fragmede 4 hours agorootparentthe llama2-uncensored model isn't quite state of the art, but ollama makes it easy to run if you have the hardware/am willing to pay to access a cloud GPU. I colloquially used the word \"hack\" when trying to write some code with ChatGPT, and got admonished for trying to do bad things, so uncensoring has gotten interesting to me. reply justworkout 7 hours agoparentprevI couldn't even get Google Gemini to generate a picture of, verbatim, \"a man eating\". It gave me a long winded lecture about how it's offensive and I should consider changing my views on the world. It does this with virtually any topic. reply TheCaptain4815 13 hours agoprevI wonder if the real killer app is Googles hardware scale verses OpenAi' s(or what Microsoft gives them). Seems like nothing Google's done has been particular surprising to OpenAi's team, it's just they have such huge scale maybe they can iterate faster. reply danpalmer 13 hours agoparentAnd the fact that Google are on their own hardware platform, not dependent on Nvidia for supply or hardware features. reply dist-epoch 12 hours agoparentprevThe real moat is that Google has access to all the video content from YouTube to train the AI on, unlike anyone else. reply sarreph 11 hours agorootparentI’m not sure I would necessarily call YouTube a moat-creator for Google, since the content on YouTube is for all intents and purposes public data. reply dist-epoch 9 hours agorootparentThere is a difference between downloading a few videos and having access to ALL of them. reply SXX 8 hours agorootparentA good dataset to train on. Now if after a Zoom call collegue ask you to like their video and subscribe to them on YouTube it would look a little suspicious. reply kennyadam 1 hour agorootparentA very wry observation! I wonder how fake videos will expose themselves in novel ways like this. reply qudat 9 hours agorootparentprevNot to mention all the metadata buried inside their internal api reply ajross 9 hours agorootparentprevSo, it's true that IP law is going to have some catch-up to do with applications to machine learning and how copyright works in that world. Nonetheless I'd be really worried if you were working on a startup whose training process started with \"We'll just scrape YouTube because that is for all intents and purposes public data\". reply tekni5 12 hours agoprevI was thinking about this a while back, once AI is able to analyze video, images and text and do so cheap & efficiently. It's game over for privacy, like completely. Right now massive corps have tons of data on us, but they can't really piece it together and understand everything. With powerful AI every aspect of your digital life can be understood. The potential here is insane, it can be used for so many different things good and bad. But I bet it will be used to sell more targeted goods and services. reply worldsayshi 12 hours agoparentUnless you live in the EU and have laws that should protect you from that. reply spacebanana7 11 hours agorootparentPublic sector agencies and law enforcement are generally exempt (or have special carve outs) in European privacy regulations. reply tekni5 12 hours agorootparentprevWhat happens if it's a datamining third party bot? That can check your social media accounts, create an in-depth profile on you, every image, video, post you've made has been recorded and understood. It knows everything about you, every product you use, where you have been, what you like, what you hate, everything packaged and ready to be sold to an advertiser, or the government, etc. reply gnepon 3 hours agorootparentSetting our social media accounts to private should solve most of that. Otherwise we will have to put less of our lives on public platforms. reply seniorivn 12 hours agorootparentprevincentives cannot be fixed with just prohibitive laws, war on drags should've taught you something reply jpk 12 hours agorootparentLaws, and more specifically their penalties, are precisely for fixing incentives. It's just a matter of setting a penalty that outweighs the natural incentive you want to override. e.g., Is it more expensive to respect privacy, or pay the fine for not doing so? PII could, and should, be made radioactive by privacy regulations and their associated penalties. reply worldsayshi 9 hours agorootparentprevIt's not a complete fix but I'm sure a law with teeth can make a big difference. There's a big difference in being data mined by a big corp with the law on its side and a criminal organisation or their customers that has to cover their tracks to not get multi million dollar fines. reply garbagewoman 12 hours agorootparentprevWar on drags? I thought that was just in Florida reply ineedaj0b 12 hours agorootparentplease consider commenting more thoughtfully. I understand this is a joke but we don't want this site to devolve into Reddit. reply riquisimo 10 hours agorootparentIt is sad that we live in a world where this could be interpreted both ways. reply Pengtuzi 11 hours agorootparentprevnext [4 more] [flagged] ineedaj0b 11 hours agorootparentI don't have a flag button or option my account is new reply zabzonk 10 hours agorootparentclick on time since posted to get a flag button - exactly why hn works this way is a bit of a mystery, to me at least reply ineedaj0b 8 minutes agorootparentThank you, I had no idea! SV_BubbleTime 12 hours agorootparentprevDrugs… Oooohh. I get it now. reply Nextgrid 9 hours agorootparentprevThat's only on paper - in practice the GDPR has a major enforcement problem. reply prmoustache 3 hours agorootparentThis + everything is about consent (cookie banner and all) So if your job means you use a specific OS with a specific Office Suite in the cloud and that office suite in the cloud incorporate AI and you only get half the features available if you don't consent, you as an employee end up kind of forced to consent anyway, GPDR or not. reply YetAnotherNick 12 hours agorootparentprevIs it true or more of a myth? Based on my online read, Europe has \"think of the children\" narrative as common if not more than other parts of the world. They tried hard to ban encryption in apps many times.[1] [1]: https://proton.me/blog/eu-council-encryption-vote-delayed reply RamblingCTO 1 minute agorootparentNot Europe, just Von der Leyen and the like. Germany put her down multiple times on this bullshit now because it violates our constitution. But she tries again and again and again. reply devjab 11 hours agorootparentprevDemocratic governance is complicated. It’s never black and white and it’s perfectly possible for parts of the EU to be working to end encryption while another part works toward enhancing citizen privacy rights. Often they’re not even supported by the same politicians, but since it’s not a winners takes all sort of thing, it can all happen simultaneously and sometimes they can even come up with some “interesting” proposals that directly interfere with each other. That being said there is a difference between the US and the EU in regards to how these things are approached. Where the US is more likely to let private companies destroy privacy while keeping public agencies leashed it’s the opposite in Europe. Truth be told, it’s not like the US initiatives are really working since agencies like the NSA seem to blatantly ignore all laws anyway, which cause some scandals here in Europe as well. In Denmark our Secret Police isn’t allowed to spy on us without warrants, but our changing governments has had different secret agreements with the US to let the US monitor our internet traffic. Which is sort of how it is, and the scandal isn’t so much that, it’s how our Secret Police is allowed to get information about Danish citizens from the NSA without warrants, letting our secret police spy on us by getting the data they aren’t allowed to gather themselves from the NSA who are allowed to gather it. Anyway, it’s a complicated mess, and you have so many branches of the bureaucracy and so many NGOs pulling in different directions that you can’t say that the EU is pro or anti privacy the way you want to. Because it’s both of those things and many more at the same time. I think the only thing the EU unanimously agrees on (sort of) is to limit private companies access to citizen privacy data. Especially non-EU organisations. Which is very hard to enforce because most of the used platforms and even software isn’t European. reply YetAnotherNick 11 hours agorootparentI am fine with private company using my data for showing me better ads. They can't affect my life significantly. I am not fine with government using the data to police me. Already in most countries, governments are putting people in jail because of things like hate speech where are the laws are really vague. reply vladms 10 hours agorootparentTo me this sounds like an opinion that would be common in the US, mostly because of where the trust and fears seem to be (private companies versus government). I think everybody (private companies, government, individuals) will try to influence and will affect your personal life. What I am worried about is who has the most efficient way to influence a lot the average person - because that entity can control on long term a lot more. My impression is that in the European Union - due partially to a complex system - is harder for any particular actor to do much on its own (even the example with Denmark secret service asking NSA for data about citizens - I guess it is harder for them to do that rather than just get directly the data). So what I am afraid is focused and efficient entities having the data, hence I am more afraid of private companies (which are focused and sometimes efficient) rather than governments. reply YetAnotherNick 4 hours agorootparentCan we please argue on the thing being discussed rather than where it is common? Are you saying influencing life through ads and putting me in jail have similar effect on me? If you combine all laws of my country I am pretty sure I would have broken few unintentionally. If government wants to just put me in jail they could retroactively find any of my past instance if they have the data. This is not some theoretical thing, but something the thing that happens with political dissidents all the time. reply squigz 10 hours agorootparentprev\"Most\" countries? Can you provide some examples? reply YetAnotherNick 4 hours agorootparenthttps://en.wikipedia.org/wiki/Incitement_to_ethnic_or_racial... reply squigz 2 hours agorootparentThere are 6 countries listed in that article, out of the nearly 200 countries in the world. Hardly \"most.\" And there doesn't appear to be examples of those 6 countries imprisoning people for those laws. reply smoldesu 12 hours agorootparentprev> They tried hard to ban encryption in apps many times. That's true of most places. We should applaud the EU's human rights court for leading the way by banning this behavior: https://www.eureporter.co/world/human-rights-category/europe... reply londons_explore 12 hours agoparentprev> I bet it will be used to sell more targeted goods and services. Plenty of companies have been shoving all the unstructured data they have about you and your friends into a big neural net to predict which ad you're most likely to click for a decade now... reply tekni5 12 hours agorootparentSure but not images and video. Now they can look at a picture of your room and label everything you own, etc. reply londons_explore 11 hours agorootparentyes including images and video. It's been basically standard practice to take each piece of user data and turn it into an embedding vector, then combine all the vectors with some time/relevancy weighting or neural net, then use the resulting vector to predict user click through rates for ads. (which effectively determines which ad the user will see). reply ryukoposting 12 hours agoparentprevYou nailed it on the head. People dismissing this because it isn't perfectly accurate are missing the point. For the purposes of analytics and surveillance, it doesn't need to be perfectly accurate as long as you have enough raw data to filter out the noise. The Four have already mastered the \"collecting data\" part, and nobody in North America with the power to rein in that situation seems interested in doing so (this isn't to say the GDPR is perfect, but at least Europe is trying). It's depressing that the most extraordinary technologies of our age are used almost exclusively to make you buy shit. reply fragmede 4 hours agorootparentwould it be more or less depressing if it came out that in addition to trying to get you to buy stuff, it was being used to, either make you dumber to make you easier to control, or get you to study harder and be a better worker? reply loudmax 12 hours agoprevAt the end of the article, a single image of the bookshelf uploaded to Gemini is 258 tokens. Gemini then responds with a listing of book titles, coming to 152 tokens. Does anyone understand where the information for the response came from? That is, does Gemini hold onto the original uploaded non-tokenized image, then run an OCR on it to read those titles? Or are all those book titles somehow contained in those 258 tokens? If it's the later, it seems amazing that these tokens contain that much information. reply jacobr1 12 hours agoparentI'm not sure about Gemini, but OpenAI GTP-V bills at roughly a token per 40x40px square. It isn't clear to me these actually processed as units, but rather it seems like they tried to approximate the cost structure to match text. reply zacmps 12 hours agoparentprevRemember, if it's using a similar tokeniser to GPT-4 (cl100k_base iirc), each token has a dimension of ~100,000. So 258x100,000 is a space of 25,800,000 floats, using f16 (a total guess) that's 51.6kB, probably enough to represent the image at ok quality with JPG. reply simonw 12 hours agorootparentI don't think that's right. A token in GPT-4 is a single integer, not a vector of floats. Input to a model gets embedded into vectors later, but the actual tokens are pretty tiny. reply l33tman 47 minutes agorootparentBut they are not a \"single integer\" either as in, like a byte... I don't have any good examples but I'm pretty sure the tokens are in the range of thousands of dimensions. It has to encode the properties of the patch of the image it derives from, and even a small 40x40 RGB pixel patch has plenty of information you have to retain. reply zacmps 7 hours agorootparentprevAh true, I guess it's still 258 positions by 100,000 possible tokens though. reply simonw 12 hours agoparentprevI would LOVE to understand that myself. reply Liquix 6 hours agoprevIn the same vein as \"agents watching your screen\" - what about \"agents watching your posture\"? Pages like [0] and [1] exist because people experience great benefits from becoming (even slightly) more aware of the way they are holding their bodies. Imagine this idea taken to the extreme, with a local agent intelligently reminding you to tighten your core, square your shoulders, relax your tongue, or warning of potential incoming RSI? [0] https://news.ycombinator.com/item?id=35939206 [1] https://static.virtualsaleslab.com/vsl-poc/ppp/ reply polygot 6 hours agoparent> He sat as still as he could on the narrow bench, with his hands crossed on his knee. He had already learned to sit still. If you made unexpected movements they yelled at you from the telescreen. reply Liquix 6 hours agorootparentagreed, anything FAANG/internet facing with this capability is Orwellian, which is why > local is explicitly included in the idea. reply QuercusMax 9 hours agoprevGuess the author didn't bother to check that those books actually are correct? The first one I checked, \"Growing Up with Lucy by April Henry\" doesn't exist. The actual book is by Steve Grand, and it's very obviously so in the video used as input. So a cool demo, but sadly useless for anything more. reply flextheruler 9 hours agoparentI think this post and others reactions and then your comment this far down really encapsulates where we’re at with this technology. Nearly 90 percent of comments on posts about LLMs are people talking about how the near future is about to boggle our minds and that general intelligence is near, but all my experiences with these LLMs show they’re capable of making the most basic of mistakes and doing so confidently and that’s just the tip of the iceberg in terms of their problems. I’m having a hard time buying into the hype that these will be able to competently replace nearly any job anytime soon. They’re useful tools but they all come with a big asterisk of human hand holding. reply simonw 4 hours agoparentprevI called out one hallucination - \"The Personal MBA by Josh Kaufman\" wasn't on my shelf. I didn't bother fact-checking every other book because I thought highlighting one mistake would illustrate that the results weren't accurate - which is pretty much expected for anything related to LLMs at this point. reply dlandau 3 hours agorootparentI don't think highlighting one mistake is enough, when these can sometimes have more mistakes than corrects. I've found use for LLMs (in large part thanks to your teaching) in cases where I can easily verify the results fully like code and process documentation, but tasks where \"fact-checking everything\" would be too much work are very much on the danger zone for getting accidentally scammed by AI. reply timeon 1 hour agorootparentprevNo result is better than misinformation. reply sensanaty 5 hours agoparentprevFor most of the people hyping up AI it doesn't matter that it makes things up more often than it doesn't. They're here to sell hype so they can build the 9 millionth startup that sells you a wrapper for one of these models, not to do anything useful or advance humanity or whatever other confabulations they like to pretend to care about reply TrackerFF 5 hours agoparentprevNo one is expecting a 0% error rate. As long as it is on par (or better) and faster than humans, that's good enough to get the ball rolling. Curious to see how I fared at the task (first vid), I used just over 4 minutes writing down the books with readable titles - and got 36 of them. Seems like there are 56-57 or something like that. So I roughly got two thirds of the books in the video. But that's still 4 minute of pausing and sliding the video for the book titles alone. reply bergenbergen 5 hours agoparentprevThanks for this comment. I am yet to see any “art” produced by AI that is not superficial or hollow (best case) or deeply unsettling (common case). reply fragmede 4 hours agorootparenthow much time do you spend looking at AI art though? a casual jaunt through midjourney will certainly get you some weird things, but there are some gems in there (but also a lot of weird). reply danjc 4 hours agoparentprevGreat for creative tasks where precision isn't required. reply ilaksh 14 hours agoprevSo you have to get invited to use Gemini Pro 1.5 right? EDIT: there is a waitlist here https://aistudio.google.com/app/waitlist/97445851 reply _kb 2 hours agoprevModelling video as a series of frames seems like such a waste; and a great point of focus for optimisation. The vast majority of video content has a lot of redundant inter-frame information. De-duping this is a key part of most compression schemes and (as an AI simpleton) seems like on obvious entry point for minimising token usage. Or is this simply a case where token windows are expected to / have already grown to a point where this sort of optimisation is not needed? reply tomas789 1 hour agoprevSomeone should do Justin.tv again but with this and people could query their life. reply elzbardico 12 hours agoprevReally. I am not that impressed. It is not something radically different from doing the same thing with a still photo which by now is trivial for those models. What is being tested here doesn't require a video. It is not showing to be able to derive any meaning from a short clip. It is fucking doing very fancy OCR, that's all. What would impress me is if shown a clip of an open chest surgery it was able to comment what surgery is being done, which technique is being used, or if shown video of construction workers, be able to figure out what is the building technique, what they are actually doing, telling that the guy with the yellow shirt is not following safety regulations by not wearing a helmet. reply z7 7 minutes agoparent>What would impress me is if shown a clip of an open chest surgery it was able to comment what surgery is being done, which technique is being used, or if shown video of construction workers, be able to figure out what is the building technique, what they are actually doing, telling that the guy with the yellow shirt is not following safety regulations by not wearing a helmet. You mean like in this demo? https://www.youtube.com/watch?v=wa0MT8OwHuk reply andiareso 7 hours agoparentprevBut it can do that… reply 138 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google released Gemini Pro 1.5, an AI model that can analyze video inputs to provide information, with a massive context size of 1,000,000 tokens.",
      "This AI model can accurately recognize books in videos and break down videos into frames for analysis, with each frame requiring 258 tokens for processing.",
      "The author conducted an experiment to demonstrate the model's abilities and published their results online for the public to view."
    ],
    "commentSummary": [
      "The discussion delves into various AI-related topics, including privacy, language models, and societal impact, touching on censorship, ethics, and the privacy-innovation balance in AI development.",
      "It explores the capabilities and limitations of AI models in tasks like video analysis, language learning, and creative endeavors, emphasizing the complexity and challenges of AI implementation across different contexts.",
      "The conversation also considers the implications for privacy, data handling, and societal norms, providing a comprehensive view of AI's multifaceted role in today's world."
    ],
    "points": 969,
    "commentCount": 392,
    "retryCount": 0,
    "time": 1708543386
  },
  {
    "id": 39453660,
    "title": "Enhancing iMessage Security with PQ3 Cryptographic Protocol",
    "originLink": "https://security.apple.com/blog/imessage-pq3/",
    "originBody": "Today we are announcing the most significant cryptographic security upgrade in iMessage history with the introduction of PQ3, a groundbreaking post-quantum cryptographic protocol that advances the state of the art of end-to-end secure messaging. With compromise-resilient encryption and extensive defenses against even highly sophisticated quantum attacks, PQ3 is the first messaging protocol to reach what we call Level 3 security — providing protocol protections that surpass those in all other widely deployed messaging apps. To our knowledge, PQ3 has the strongest security properties of any at-scale messaging protocol in the world. When iMessage launched in 2011, it was the first widely available messaging app to provide end-to-end encryption by default, and we have significantly upgraded its cryptography over the years. We most recently strengthened the iMessage cryptographic protocol in 2019 by switching from RSA to Elliptic Curve cryptography (ECC), and by protecting encryption keys on device with the Secure Enclave, making them significantly harder to extract from a device even for the most sophisticated adversaries. That protocol update went even further with an additional layer of defense: a periodic rekey mechanism to provide cryptographic self-healing even in the extremely unlikely case that a key ever became compromised. Each of these advances were formally verified by symbolic evaluation, a best practice that provides strong assurances of the security of cryptographic protocols. Historically, messaging platforms have used classical public key cryptography, such as RSA, Elliptic Curve signatures, and Diffie-Hellman key exchange, to establish secure end-to-end encrypted connections between devices. All these algorithms are based on difficult mathematical problems that have long been considered too computationally intensive for computers to solve, even when accounting for Moore’s law. However, the rise of quantum computing threatens to change the equation. A sufficiently powerful quantum computer could solve these classical mathematical problems in fundamentally different ways, and therefore — in theory — do so fast enough to threaten the security of end-to-end encrypted communications. Although quantum computers with this capability don’t exist yet, extremely well-resourced attackers can already prepare for their possible arrival by taking advantage of the steep decrease in modern data storage costs. The premise is simple: such attackers can collect large amounts of today’s encrypted data and file it all away for future reference. Even though they can’t decrypt any of this data today, they can retain it until they acquire a quantum computer that can decrypt it in the future, an attack scenario known as Harvest Now, Decrypt Later. To mitigate risks from future quantum computers, the cryptographic community has been working on post-quantum cryptography (PQC): new public key algorithms that provide the building blocks for quantum-secure protocols but don’t require a quantum computer to run — that is, protocols that can run on the classical, non-quantum computers we’re all using today, but that will remain secure from known threats posed by future quantum computers. To reason through how various messaging applications mitigate attacks, it’s helpful to place them along a spectrum of security properties. There’s no standard comparison to employ for this purpose, so we lay out our own simple, coarse-grained progression of messaging security levels in the image at the top of this post: we start on the left with classical cryptography and progress towards quantum security, which addresses current and future threats from quantum computers. Most existing messaging apps fall either into Level 0 — no end-to-end encryption by default and no quantum security — or Level 1 — with end-to-end encryption by default, but with no quantum security. A few months ago, Signal added support for the PQXDH protocol, becoming the first large-scale messaging app to introduce post-quantum security in the initial key establishment. This is a welcome and critical step that, by our scale, elevated Signal from Level 1 to Level 2 security. At Level 2, the application of post-quantum cryptography is limited to the initial key establishment, providing quantum security only if the conversation key material is never compromised. But today’s sophisticated adversaries already have incentives to compromise encryption keys, because doing so gives them the ability to decrypt messages protected by those keys for as long as the keys don’t change. To best protect end-to-end encrypted messaging, the post-quantum keys need to change on an ongoing basis to place an upper bound on how much of a conversation can be exposed by any single, point-in-time key compromise — both now and with future quantum computers. Therefore, we believe messaging protocols should go even further and attain Level 3 security, where post-quantum cryptography is used to secure both the initial key establishment and the ongoing message exchange, with the ability to rapidly and automatically restore the cryptographic security of a conversation even if a given key becomes compromised. iMessage now meets this goal with a new cryptographic protocol that we call PQ3, offering the strongest protection against quantum attacks and becoming the only widely available messaging service to reach Level 3 security. Support for PQ3 will start to roll out with the public releases of iOS 17.4, iPadOS 17.4, macOS 14.4, and watchOS 10.4, and is already in the corresponding developer preview and beta releases. iMessage conversations between devices that support PQ3 are automatically ramping up to the post-quantum encryption protocol. As we gain operational experience with PQ3 at the massive global scale of iMessage, it will fully replace the existing protocol within all supported conversations this year. Designing PQ3 More than simply replacing an existing algorithm with a new one, we rebuilt the iMessage cryptographic protocol from the ground up to advance the state of the art in end-to-end encryption, and to deliver on the following requirements: Introduce post-quantum cryptography from the start of a conversation, so that all communication is protected from current and future adversaries. Mitigate the impact of key compromises by limiting how many past and future messages can be decrypted with a single compromised key. Use a hybrid design to combine new post-quantum algorithms with current Elliptic Curve algorithms, ensuring that PQ3 can can never be less safe than the existing classical protocol. Amortize message size to avoid excessive additional overhead from the added security. Use formal verification methods to provide strong security assurances for the new protocol. PQ3 introduces a new post-quantum encryption key in the set of public keys each device generates locally and transmits to Apple servers as part of iMessage registration. For this application, we chose to use Kyber post-quantum public keys, an algorithm that received close scrutiny from the global cryptography community, and was selected by NIST as the Module Lattice-based Key Encapsulation Mechanism standard, or ML-KEM. This enables sender devices to obtain a receiver’s public keys and generate post-quantum encryption keys for the very first message, even if the receiver is offline. We refer to this as initial key establishment. We then include — within conversations — a periodic post-quantum rekeying mechanism that has the ability to self-heal from key compromise and protect future messages. In PQ3, the new keys sent along with the conversation are used to create fresh message encryption keys that can’t be computed from past ones, thereby bringing the conversation back to a secure state even if previous keys were extracted or compromised by an adversary. PQ3 is the first large scale cryptographic messaging protocol to introduce this novel post-quantum rekeying property. PQ3 employs a hybrid design that combines Elliptic Curve cryptography with post-quantum encryption both during the initial key establishment and during rekeying. Thus, the new cryptography is purely additive, and defeating PQ3 security requires defeating both the existing, classical ECC cryptography and the new post-quantum primitives. It also means the protocol benefits from all the experience we accumulated from deploying the ECC protocol and its implementations. Rekeying in PQ3 involves transmitting fresh public key material in-band with the encrypted messages that devices are exchanging. A new public key based on Elliptic Curve Diffie-Hellman (ECDH) is transmitted inline with every response. The post-quantum key used by PQ3 has a significantly larger wire size than the existing protocol, so to meet our message size requirement we designed the quantum-secure rekeying to happen periodically rather than with every message. To determine whether a new post-quantum key is transmitted, PQ3 uses a rekeying condition that aims to balance the average size of messages on the wire, preserve the user experience in limited connectivity scenarios, and keep the global volume of messages within the capacity of our server infrastructure. Should the need arise, future software updates can increase the rekeying frequency in a way that’s backward-compatible with all devices that support PQ3. With PQ3, iMessage continues to rely on classical cryptographic algorithms to authenticate the sender and verify the Contact Key Verification account key, because these mechanisms can’t be attacked retroactively with future quantum computers. To attempt to insert themselves in the middle of an iMessage conversation, an adversary would require a quantum computer capable of breaking one of the authentication keys before or at the time the communication takes place. In other words, these attacks cannot be performed in a Harvest Now, Decrypt Later scenario — they require the existence of a quantum computer capable of performing the attacks contemporaneously with the communication being attacked. We believe any such capability is still many years away, but as the threat of quantum computers evolves, we will continue to assess the need for post-quantum authentication to thwart such attacks. A formally proven protocol Our final requirement for iMessage PQ3 is formal verification — a mathematical proof of the intended security properties of the protocol. PQ3 received extensive review from Apple’s own multi-disciplinary teams in Security Engineering and Architecture (SEAR) as well as from some of the world’s foremost experts in cryptography. This includes a team led by Professor David Basin, head of the Information Security Group at ETH Zürich and one of the inventors of Tamarin — a leading security protocol verification tool that was also used to evaluate PQ3 — as well as Professor Douglas Stebila from the University of Waterloo, who has performed extensive research on post-quantum security for internet protocols. Each took a different but complementary approach, using different mathematical models to demonstrate that as long as the underlying cryptographic algorithms remain secure, so does PQ3. Finally, a leading third-party security consultancy supplemented our internal implementation review with an independent assessment of the PQ3 source code, which found no security issues. In the first mathematical analysis, Security analysis of the iMessage PQ3 protocol, Professor Douglas Stebila focused on so-called game-based proofs. This technique, also known as reduction, defines a series of “games“ or logical statements to show that the protocol is at least as strong as the algorithms that underpin it. Stebila’s analysis shows that PQ3 provides confidentiality even in the presence of some key compromises against both classical and quantum adversaries, in both the initial key establishment and the ongoing rekeying phase of the protocol. The analysis decomposes the many layers of key derivations down to the message keys and proves that, for an attacker, they are indistinguishable from random noise. Through an extensive demonstration that considers different attack paths for classical and quantum attackers in the proofs, Stebila shows that the keys used for PQ3 are secure as long as either the Elliptic Curve Diffie-Hellman problem remains hard or the Kyber post-quantum KEM remains secure. The iMessage PQ3 protocol is a well-designed cryptographic protocol for secure messaging that uses state-of-the-art techniques for end-to-end encrypted communication. In my analysis using the reductionist security methodology, I confirmed that the PQ3 protocol provides post-quantum confidentiality, which can give users confidence in the privacy of their communication even in the face of potential improvements in quantum computing technology. —Professor Douglas Stebila In the second evaluation, A Formal Analysis of the iMessage PQ3 Messaging Protocol, Prof. David Basin, Felix Linker, and Dr. Ralf Sasse at ETH Zürich use a method called symbolic evaluation. As highlighted in the paper’s abstract, this analysis includes a detailed formal model of the iMessage PQ3 protocol, a precise specification of its fine-grained security properties, and machine-checked proofs using the state-of-the-art symbolic Tamarin prover. The evaluation yielded a fine-grained analysis of the secrecy properties of PQ3, proving that “in the absence of the sender or recipient being compromised, all keys and messages transmitted are secret” and that “compromises can be tolerated in a well-defined sense where the effect of the compromise on the secrecy of data is limited in time and effect,” which confirms that PQ3 meets our goals. We provide a mathematical model of PQ3 as well as prove its secrecy and authenticity properties using a verification tool for machine-checked security proofs. We prove the properties even when the protocol operates in the presence of very strong adversaries who can corrupt parties or possess quantum computers and therefore defeat classical cryptography. PQ3 goes beyond Signal with regards to post-quantum defenses. In PQ3, a post-quantum secure algorithm is part of the ratcheting and used repeatedly, rather than only once in the initialization as in Signal. Our verification provides a very high degree of assurance that the protocol as designed functions securely, even in the post-quantum world. —Professor David Basin Diving into the details Because we know PQ3 will be of intense interest to security researchers and engineers as well as the cryptographic community, this blog post is really two posts in one. Up to now, we laid out our design goals, outlined how PQ3 meets them, and explained how we verified our confidence in the protocol with independent assessments. If you’d like to understand more detail about the cryptographic underpinnings, the remainder of the post is a deeper dive into how we constructed the PQ3 protocol. Post-quantum key establishment iMessage allows a user to register multiple devices on the same account. Each device generates its own set of encryption keys, and the private keys are never exported to any external system. The associated public keys are registered with Apple’s Identity Directory Service (IDS) to enable users to message each other using a simple identifier: email address or phone number. When a user sends a message from one of their devices, all of their other devices and all of the recipient’s devices receive the message. The messages are exchanged through pair-wise sessions established between the sending device and each receiving device. The same message is encrypted successively to each receiving device, with keys uniquely derived for each session. For the rest of this description, we will focus on a single device-to-device session. Because the receiving device might not be online when the conversation is established, the first message in a session is encrypted using the public encryption keys registered with the IDS server. Each device with PQ3 registers two public encryption keys and replaces them regularly with fresh ones: A post-quantum Kyber-1024 key encapsulation public key A classical P-256 Elliptic Curve key agreement public key These encryption keys are signed with ECDSA using a P-256 authentication key generated by the device’s Secure Enclave, along with a timestamp used to limit their validity. The device authentication public key is itself signed by the Contact Key Verification account key, along with some attributes such as the supported cryptographic protocol version. This process allows the sender to verify that the recipient device’s public encryption keys were uploaded by the intended recipient, and it guards against downgrade attacks. When Alice’s device instantiates a new session with Bob’s device, her device queries the IDS server for the key bundle associated with Bob’s device. The subset of the key bundle that contains the device’s authentication key and versioning information is validated using Contact Key Verification. The device then validates the signature covering the encryption keys and timestamps, which attests that the keys are valid and have not expired. Alice’s device can then use the two public encryption keys to share two symmetric keys with Bob. The first symmetric key is computed through an ECDH key exchange that combines an ephemeral encryption key from Alice with Bob’s registered P-256 public key. The second symmetric key is obtained from a Kyber key encapsulation with Bob’s post-quantum public key. To combine these two symmetric keys, we first extract their entropy by invoking HKDF-SHA384-Extract twice — once for each of the keys. The resulting 48-byte secret is further combined with a domain separation string and session information — which includes the user’s identifiers, the public keys used in the key exchange, and the encapsulated secret — by invoking HKDF-SHA384-Extract again to derive the session’s initial keying state. This combination ensures that the initial session state cannot be derived without knowing both of the shared secrets, meaning an attacker would need to break both algorithms to recover the resulting secret, thus satisfying our hybrid security requirement. Post-quantum rekeying Ongoing rekeying of the cryptographic session is designed such that keys used to encrypt past and future messages cannot be recomputed even by a powerful hypothetical attacker who is able to extract the cryptographic state of the device at a given point in time. The protocol generates a new unique key for each message, which periodically includes new entropy that is not deterministically derived from the current state of the conversation, effectively providing self-healing properties to the protocol. Our rekeying approach is modeled after ratcheting, a technique that consists of deriving a new session key from other keys and ensuring the cryptographic state always moves forward in one direction. PQ3 combines three ratchets to achieve post-quantum encryption. The first ratchet, called the symmetric ratchet, protects older messages in a conversation to achieve forward secrecy. For every message, we derive a per-message encryption key from the current session key. The current session key itself is then further derived into a new session key, ratcheting the state forward. Each message key is deleted as soon as a corresponding message is decrypted, which prevents older harvested ciphertexts from being decrypted by an adversary who is able to compromise the device at a later time, and provides protection against replayed messages. This process uses 256-bit keys and intermediate values, and HKDF-SHA384 as a derivation function, which provides protection against both classical and quantum computers. The second ratchet, called the ECDH ratchet, protects future messages by updating the session with fresh entropy from an Elliptic Curve key agreement, ensuring that an adversary loses the ability to decrypt new messages even if they had compromised past session keys — a property called post-compromise security. The ECDH-based ratchet has a symmetrical flow: the private key of the outgoing ratchet public key from the sender is used with the last public key received from the recipient to establish a new shared secret between sender and receiver, which is then mixed into the session’s key material. The new PQ3 protocol for iMessage uses NIST P-256 Elliptic Curve keys to perform this ratchet, which imposes only a small 32-byte overhead on each message. Because the second ratchet uses classical cryptography, PQ3 also adds a conditionally executed Kyber KEM-based ratchet. This third ratchet complements the ECDH-based ratchet to provide post-compromise security against Harvest Now, Decrypt Later quantum attacks as well. The use of a post-quantum ratchet can cause significant network overhead compared to an ECDH-based ratchet at the same security level. The post-quantum KEM requires sending both a public key and an encapsulated secret instead of a single outgoing public key. In addition, the underlying mathematical structure for quantum security requires significantly larger parameter sizes for public keys and encapsulated keys compared to Elliptic Curves. To limit the size overhead incurred by frequent rekeying while preserving a high level of security, the post-quantum KEM is instantiated with Kyber-768. Unlike the IDS-registered public keys used for the initial key establishment, ratcheting public keys are used only once to encapsulate a shared secret to the receiver, significantly limiting the impact of the compromise of a single key. However, while a 32-byte ECDH-based ratchet overhead is acceptable on every message, the post-quantum KEM ratchet increases the message size by more than 2 kilobytes. To avoid visible delays in message delivery when device connectivity is limited, this ratchet needs to be amortized over multiple messages. We therefore implemented an adaptive post-quantum rekeying criterion that takes into account the number of outgoing messages, the time elapsed since last rekeying, and current connectivity conditions. At launch, this means the post-quantum ratchet is performed approximately every 50 messages, but the criterion is bounded such that rekeying is always guaranteed to occur at least once every 7 days. And as we mentioned earlier, as the threat of quantum computers and infrastructure capacity evolves over time, future software updates can increase the rekeying frequency while preserving full backward compatibility. Completing the public key ratchets, whether based on ECDH or Kyber, requires sending and receiving a message. Although users may not immediately reply to a message, iMessage includes encrypted delivery receipts that allow devices to rapidly complete the ratchet even without a reply from the recipient, as long as the device is online. This technique avoids delays in the rekeying process and helps support strong post-compromise recovery. Similar to the initial session key establishment, the secrets established through the three ratchets are all combined with an evolving session key using HKDF-SHA384 through sequential calls to the Extract function. At the end of this process, we obtain a final message key, which can now be used to encrypt the payload. Padding and encryption To avoid leaking information about the message size, PQ3 adds padding to the message before encryption. This padding is implemented with the Padmé heuristic, which specifically limits the information leakage of ciphertexts with maximum length M to a practical optimum of O(log log M) bits. This is comparable to padding to a power of two but results in a lower overhead of at most 12 percent and even lower for larger payloads. This approach strikes an excellent balance between privacy and efficiency, and preserves the user experience in limited device connectivity scenarios. The padded payload is encrypted with AES-CTR using a 256-bit encryption key and initialization vector, both derived from the message key. While public key algorithms require fundamental changes to achieve quantum security, symmetric cryptography algorithms like the AES block cipher only require doubling the key size to maintain their level of security against quantum computers. Authentication Each message is individually signed with ECDSA using the elliptic curve P-256 device authentication key protected by the Secure Enclave. The receiving device verifies the mapping between the sender’s identifier (email address or phone number) and the public key used for signature verification. If both users have enabled Contact Key Verification and verified each other’s account key, the device verifies that the device authentication keys are present in the Key Transparency log and that the corresponding account key matches the account key stored in the user’s iCloud Keychain. The device’s authentication key is generated by the Secure Enclave and never exposed to the rest of the device, which helps prevent extraction of the private key even if the Application Processor is completely compromised. If an attacker were to compromise the Application Processor, they might be able to use the Secure Enclave to sign arbitrary messages. But after the device recovers from the compromise through a reboot or a software update, they would no longer be able to impersonate the user. This approach offers stronger guarantees than other messaging protocols where the authentication key is sometimes shared between devices or where the authentication takes place only at the beginning of the session. The message signature covers a wide range of fields, including the unique identifiers of the users and their push notification tokens, the encrypted payload, authenticated data, a ratchet-derived message key indicator that binds the signature to a unique location in the ratchet, and any public key information used in the protocol. The inclusion of these fields in the signature guarantees that the message can only be used in the context intended by the sender, and all the fields are exhaustively documented in the research papers from Stebila, Basin, and collaborators. Conclusion End-to-end encrypted messaging has seen a tremendous amount of innovation in recent years, including significant advances in post-quantum cryptography from Signal’s PQXDH protocol and in key transparency from WhatsApp’s Auditable Key Directory. Building on its pioneering legacy as the first widely available messaging app to provide end-to-end encryption by default, iMessage has continued to deliver advanced protections that surpass existing systems. iMessage Contact Key Verification is the most sophisticated key transparency system for messaging deployed at scale, and is the current global state of the art for automatic key verification. And the new PQ3 cryptographic protocol for iMessage combines post-quantum initial key establishment with three ongoing ratchets for self-healing against key compromise, defining the global state of the art for protecting messages against Harvest Now, Decrypt Later attacks and future quantum computers.",
    "commentLink": "https://news.ycombinator.com/item?id=39453660",
    "commentBody": "iMessage with PQ3 Cryptographic Protocol (security.apple.com)509 points by galad87 20 hours agohidepastfavorite242 comments sandyarmstrong 18 hours agoThis is pretty fascinating. For easier reading, the Signal blog post [0] they link to is great. Both Signal and Apple went with CRYSTALS-Kyber [1] as their post-quantum algorithm. If you're interested in the math, and maybe learned at some point about how classic public key cryptography is built on the idea that it's easy to multiply two primes, but hard to factor them, and how this (or other math problems) can be used as a one-way function to make encryption hard to break, the hard math problem that backs Kyber is the \"learning-with-errors\" [2] problem. [0] https://signal.org/blog/pqxdh/ [1] https://pq-crystals.org/kyber/ [2] https://en.wikipedia.org/wiki/Learning_with_errors reply andy_xor_andrew 15 hours agoparentI'm way out of my depth in terms of the math here. But my 'software engineer brain' likes the ideal of using the prime factoring problem, because it's so simple to understand, and feels like some kind of universal primitive. \"It's easy to multiply but hard to factor.\" It just seems so intuitive. But I'm reading the 'learning with errors' wiki page and it's beyond my comprehension. There's a weird fear in my mind that all these \"post quantum algorithms\" are so complicated, with such a large surface area, that they may hide flaws. While prime factoring, or even the elliptic key stuff, is so simple to comprehend. that said, obviously the experts know what they're doing, and I'll use what they suggest. just saying that this thought has crossed my mind. reply hannob 13 hours agorootparentI know that thinking, but learning more about RSA, I came to realize that there's a flipside of this. People think \"RSA is easy\", because someone gave them a lecture of a simplified/wrong/insecure version of RSA. Pretty much all \"simple introductions to RSA\" you can find out there are wrong. The truth is: RSA isn't that simple. If you want to have RSA, and want to have it secure, there's a whole bunch of things to consider. But RSA looks simple. So lots of people go ahead and implement their RSA. And then you end up with, hey, I can break almost a third of the top 100 webpage's RSA implementations. (I'm not kidding, I did that -> https://robotattack.org/ .) I think the fact that crystals-kyber is obviously not that simple may actually protect us. Because hopefully most peopple will end up using some hopefully well audited optimized free implementation, because they won't even have the idea that they could do this on their own. reply randall 10 hours agorootparentI love that security by difficulty is the new security by obscurity, but actually secure. reply vmfunction 8 hours agorootparentYeah talking about difficulty, then you will love this: https://www.science.org/content/article/china-s-quantum-sate... If we really wanna be security, then entailment is a good bet. I could imagine essential financial clearing, emergency infrastructure could use this level security. Don't know how feasible it is for consumer grade usage yet though. reply jmprspret 8 hours agorootparentprevIs it new? Security by difficulty _is_ security. reply tptacek 14 hours agorootparentprevWell, one way to think about this is: how much abstract algebra are you keeping in your head to reassure yourself of the security of classical asymmetric cryptography? It's surprisingly deep. Some programmers are \"comfortable\" with it because they've been brought up being taught that \"factoring\" is just the way asymmetric cryptography works, but that has never really been the whole case. Elliptic curve is not at all simple to comprehend! It's easy to implement, but the motivation for designing systems around them (the effectiveness of the index calculus on elliptic curve groups) and the discoveries made on attacking them (like the MOV attack that transforms ECDLP problems to FFDLP problems) are not at all simple. Arguably, elliptic curve is an odder corner of mathematics than lattices. reply madars 9 hours agorootparent100%. Another underappreciated point is that factoring connection is very misleading. That is, we don't know if factoring is equivalent to breaking RSA (in particular, the algorithms breaking low-exponent RSA (which one shouldn't use for a number of reasons) don't factor the modulus). It might be true that computing e-th roots (i.e. breaking RSA) is tractable while factoring is not. Similarly, PKCS is complex but every part of PKCS is there because without it there is a concrete attack. Burt Kaliski (former chief scientist of RSA Labs) has an amazing talk which goes into detail about this: https://www.youtube.com/watch?v=sqsDKjPaJVg For example, why does RSA need randomized padding, besides the trivial IND-CPA violation? Because if you encrypt the same message to many recipients, the attacker can use Hastad's attack, which uses lattices in a deep way, to recover the message from non-randomized ciphertexts https://en.wikipedia.org/wiki/Coppersmith%27s_attack?useskin... Very much like an airline checklist in a way :-) Another nugget in the talk: how RSA embedded a public key in their products and bootstrapped VeriSign! Can't recommend it highly enough. reply pbsd 13 hours agorootparentprevThe LWE problem is one level of abstraction away from the fundamental lattice problems it reduces to. It is somewhat analogous to the Diffie-Hellman problem that many constructions reduce to, which itself is related to the lower-level discrete logarithm problem. The lattice equivalent of integer factorization is the shortest vector problem: you're given n vectors of length m, and you have to find the sum of integer multiples of those vectors that comes closest to (or a small factor away from the closest) the zero vector. Say you have the 4 vectors [ 3 92 4 2] [54 0 92 41] [19 91 61 48] [39 59 40 14]. The shortest vector that you can obtain from adding integer multiples of these vectors is [19 -8 -15 2], which you can obtain by 3*[39 59 40 14] + [19 91 61 48] - 2*[54 0 92 41] - 3*[ 3 92 4 2]. With only 4 vectors it is easy to find the solution here. But the hardness grows exponentially with the dimension, and the dimensions in cryptographically relevant lattices are in the hundreds to thousands. reply Retr0id 15 hours agorootparentprevThe explanation in this video is what made it click for me: https://www.youtube.com/watch?v=K026C5YaB3A reply kevvok 12 hours agorootparentSame here: this video helped me finally understand the basics concepts underlying LWE reply orangejake 2 hours agorootparentprevCuriously, the \"large surface area hiding flaws\" is actually what happened with RSA. Examples are 1. (large) speedups in solving it in the 90s, 2. issues where if enough of your key leaks, it can all be recovered. This leakage can occur via certain side-channel attacks 3. the \"easy RSA\" people learn is hard to make secure, which perhaps took a decade of doing very wrong to get right. By this, I mean padding attacks on RSA. As for understanding LWE, see for example the blog posts https://mark-schultz.github.io/nist-standard-out/ reply a1369209993 12 hours agorootparentprev> There's a weird fear in my mind that all these \"post quantum algorithms\" are so complicated, with such a large surface area, that they may hide flaws. That's correct (emphasis on \"may\", of course), and is why it's absolutely imperative to always use PQC/ECC hybrid cryptosystems rather than pure PQC. See eg [0] for a more detailed explanation. 0: https://blog.cr.yp.to/20240102-hybrid.html reply tptacek 12 hours agorootparentThere are essentially no mainstream systems that don't do this. That's why new systems deploy things like PQ3. reply a1369209993 12 hours agorootparentDo you mean: > There are essentially no mainstream systems that don't [use PQC/ECC hybrid cryptosystems?]. If so, good. I'll admit my expectations are a bit biased from having to deal with projects that go out of their way to produce defective software (eg DRM, malicious abuse of undefined behaviour by compliers, cloudflare and other captcha-walls, etc) so I tend to assume the worst by default. reply tptacek 11 hours agorootparentYes. So far as I know, there's no mainstream system that has even proposed to use solely Kyber and not Kyber+ECC. It's been a built-in assumption since the earliest Chrome PQC experiments. reply Ar-Curunir 7 hours agorootparentprevIt's not at all clear that factoring is actually hard. I fully expect factoring to be broken classically within my lifetime. FWIW I am a practicing cryptographer. reply gjsman-1000 16 hours agoparentprevWas the CRYSTALS-Kyber name intentionally, or accidentally, a reference to Kyber Crystals (I.e. The Lightsaber energy source?) reply ZeWaka 16 hours agorootparentAbsolutely a reference. Dilithium (from Star Trek) is name of their signature algorithm. reply throw0101c 14 hours agorootparentThe trick to naming things is to first find a cool word/reference, and then 'reverse engineer' it as an acronym second: * https://en.wikipedia.org/wiki/Backronym reply ryukafalz 13 hours agorootparentThough you can easily take it too far; I think US legislators have been running out of reasonable backronyms :) reply 1vuio0pswjnm7 9 hours agoparentprevhttps://kyberslash.cr.yp.to reply bjoli 12 hours agoparentprevI was reading the NIST comments and djb is not very happy with how they present things, and the other commenters seem to think he is a prick. reply londons_explore 11 hours agorootparentI want an encryption algorithm composed of 2 parts chained, such that both parts need to be broken for the whole thing to be broken. And I'd like the US/The west to declare 1 part as secure, and I'd like Russia to declare the other part as secure. I'm fairly confident that Russia and the US won't collude to push a known-weak algorithm. reply snowwrestler 10 hours agorootparentThey would not have to collude; there is no incentive for either to publicly endorse a secure algorithm. Both Russian and U.S. intelligence services want the ability to break into anything. They would publicly endorse insecure algorithms, and then employ more secure algorithms in their own classified systems. I think encryption quality is best evaluated by math and technical testing, not international political triangulation. reply dlubarov 5 hours agorootparentprevAlternatively we could use hash-based signature schemes which have been proven secure under certain assumptions about hash functions, such as SPHINCS, or ZKP based schemes along the lines of Picnic (Picnic in particular uses LowMC which is somewhat new). In that case a backup seems unnecessary. reply yesimahuman 18 hours agoprevPretty great advertisement for Signal, as the sole cross-platform option in the PQC bucket. Does it seem likely they will match Apple here eventually? reply adamtaylor_13 17 hours agoparentIn my experience, these markets can overlap but don’t necessarily always do so. I message everyone via iMessage, and while I enjoy the feeling of security from the blue bubble it’s not a must-have for me. Signal on the other hand seems like a must-have for many people living under oppressive regimes or whose data is significantly more valuable than mine. I am one data point, but that’s what I’ve noticed in my bubble. reply yesimahuman 17 hours agorootparentFor me, Signal is so much better for my friend or work group chats. My friends are on a mix of devices and platforms, and Signal is a lot nicer for embedded media sharing. And the auto disappearing feature is a must! reply jwells89 16 hours agorootparentThe main things holding back Signal usage in my case is practically nobody in my social circle using it and the desktop client not being as nice as that of Messages or Telegram, the latter being particularly relevant for myself and contacts who primarily message with their computers rather than their phones. reply godelski 15 hours agorootparentSounds like time to become an evangelist then. I had to do this in my group and other than security a major benefit is just that getting potatos instead of pictures has significantly declined. Here's my advice: don't sell security as the foremost feature. Sell it as \"iMessage, but for everyone.\" You got stickers, reactions, high quality videos and images. Then mention security, it is the cherry on top. reply pavon 15 hours agorootparentEvery single person I converted to using Signal stopped using it when SMS support was removed. HN tends to be a younger crowd whose peers cycled through a number of social-networking and messaging apps as popularity waxed and waned. But older generations don't see any compelling reason why they should bother splitting their conversations over multiple apps, when literally everyone with a mobile has texting. Being a drop-in replacement for texting, so they could have additional features without additional hassle was the only thing that convinced them to switch and now it is gone. I've given up evangelizing Signal, and resigned to the idea that cross-platform RCS is the only thing that will bring encryption to the majority of my contacts. reply rootusrootus 13 hours agorootparent> HN tends to be a younger crowd whose peers cycled through a number of social-networking and messaging apps as popularity waxed and waned. But older generations What is your definition of younger and older? Anybody born in the 70s or later experienced an ongoing procession of instant messaging choices. I would tell you it is precisely that experience that informs my apathy towards the latest and greatest Hot Thing, and makes me appreciate boring old SMS (and by extension, iMessage) that I know will just work with anyone I meet. reply jdminhbg 10 hours agorootparentprev> resigned to the idea that cross-platform RCS is the only thing that will bring encryption to the majority of my contacts I thought RCS explicitly wasn’t E2EE? reply pavon 10 hours agorootparentThe current standard isn't. Google uses non-standard E2EE, and they and Apple are working on getting it standardized. reply otterley 8 hours agorootparentIf so, how will they meet China's demand that all mobile devices support RCS, presumably because it doesn't employ E2EE? reply B5F91083 7 hours agorootparentiOS already has different behaviours inside China compared to elsewhere, for example showing Taiwan's flag or the nine-dash line in the south sea in Maps app. Based on how those work, the answer is likely: When a SIM from a mainland mobile company is in the device, E2EE is disabled. reply godelski 14 hours agorootparentprevYeah I think this was a bad move for Signal, but I didn't see that happen to my group fortunately. In Signal's defense, my understanding is that this was really an Apple thing. That Apple only lets iMessage connect to SMS so the apps were differing significantly. I also get the fatigue. Moxie said centralized because they needed to move faster. But Signal has always moved very slow, so it does feel off. But to be fair, it's not like most messenger apps do much. reply zchrykng 14 hours agorootparentThey were talking about on Android. Signal supporting SMS was never a thing on iOS. reply godelski 10 hours agorootparentThat's my point though. That it is harder to maintain a feature that not only doesn't exist on both development platforms, but can't work on one of them. Signal is only like 42 people.[0] You gotta triage a lot of stuff. I wish the feature still existed, but I can totally understand why they did that. I'm pretty sure not all 42 people are programmers. [0] https://projects.propublica.org/nonprofits/organizations/824... reply pavon 8 hours agorootparentprev> I also get the fatigue. Agreed. I put most of the blame on Google for deciding to implement RCS as a feature of Messages, rather than an Android OS library/service that any developer could use like SMS. reply nicolas_17 11 hours agorootparentprevAre you from the USA? I don't like splitting my conversations over multiple apps when literally everyone with a mobile in Argentina has WhatsApp. SMS costs money and having it in the same app as a free messaging platform sounds risky :P reply godelski 5 hours agorootparent> I don't like splitting my conversations over multiple apps Most people don't. The issue isn't what country I live in (fuck man, I got SMS, WhatsApp, Signal, Slack, KakaoTalk, and I've even forced some people to not contact me through some apps so I could reduce). You missed the actual content of the message about how there is a reasoning for this change beyond Signal. You got to be careful with blame because there are often things upstream that cause things downstream but the fingers are often pointed at what's downstream, not upstream. We can't resolve problems if we only focus on downstream. You have to consider both. I hate it too, because it is the mental version of using multiple apps. But that's the way things are and I want to actually reduce the problems, not run around with a box of bandaids. reply joshjob42 9 hours agorootparentprevBefore anyone goes evangelizing Signal, make sure to tell your iOS friends that if they ever lose their phone they lose every message they ever sent on Signal with no way to restore them, and the same applies if they ever get an Android phone (latter also true of iMessage, but WhatsApp is totally cross-platform). reply godelski 5 hours agorootparentWhile I agree with your point and I think you're right, let's remember that part of the issue is Apple. Might be worth reading this comment from the devs[0]. I wish the feature existed too, but I get it and don't think all blame is on them. For lost devices, well... is that a bug? Where would the backup come from? Signal is anti-cloud, as they should be. BUT, I do think people store too much data. Pictures, messages, etc. I think we're getting very little value for maintaining all of this and that it is mostly laziness. Don't get me wrong, I do it too and I like it and it does come in handy time to time, but my life wouldn't significantly change were this not the case. It's something I'll be irked about when I hit the speed bump the first time but move on quickly (like quitting Facebook). 99.999% of the time it would just result in me asking someone about something months ago and it's never been anything important (because if it was, I'd have made that information redundant, and that's not a action because Signal, that's an action because the information is important and I do this wherever that type of info comes from.) Truth is that >>99.9% of that data is junk. It's only the nuggets we care about and I think it's a bit weird we treat it all the same. [0] https://community.signalusers.org/t/ios-backup-keeping-messa... reply walterbell 4 hours agorootparentCan't Signal export the local messages? Wire on iOS has local export. reply godelski 3 hours agorootparentThe post addresses that a bit, but it is a few years old now. I'm not a app dev so I have no clue about these systems. But the claim is that Apple doesn't give you a lot of control and that sounds like a very Apple thing to me. I mean aren't they well known for \"my way or the highway\" attitude? reply balderdash 5 hours agorootparentprevI actually think one of the best parts about signal is the disappearing messages feature reply KennyBlanken 10 hours agorootparentprev\"iMessage, except you can't sync message history between devices because Signal feels you're a fucking idiot who can't be trusted, and that Apple's hardware security (market-leading and resistant to near-nation-state-level attacks) is insufficient.\" People's eyes glaze over and they go right back to using WhatsApp, which offers nearly everything Signal does, but also full sync. Most people don't care about the differences in privacy. reply godelski 5 hours agorootparentWe're talking about the same company who wanted to scan all your files you uploaded, right? reply baby 42 minutes agorootparentprevDon’t worry, whatsapp uses the signal protocol and everybody uses whatsapp nowadays reply perardi 15 hours agorootparentprevI used Telegram during a time when I lived a cross-border and cross-platform lifestyle. Telegram, and really Telegram on desktop, was great. I am a stubborn old man at the age of 40, and I really prefer to type on a keyboard. It had a great UI, it always delivered messages, and syncing between devices was seemingly instant. iMessage, somehow, is still not perfect at syncing between devices, and while Signal is quite good at that, it's a pain to start using on a new device, as it doesn't bring along your message history. (For legitimate reasons, mind you.) reply jwells89 15 hours agorootparentYeah Telegram absolutely nails the desktop experience like few other chat apps do. The way they put all the functional bits into a library to make it easy to build high quality third-party clients helps, too; while the Qt client is quite good across platforms, users also have the option of a Swift-based client on Apple platforms, a WinUI/UWP client on Windows, GTK client for GTK-based Linux desktops, etc which takes it that extra mile. Heck there’s even CLI clients for users of that inclination. Its security is questionable but it gets users by the boatload anyway because it doesn’t consider any platform an afterthought, they all get first-class treatment. Other cross platform messengers would do well to embrace a similar philosophy. reply tptacek 14 hours agorootparentSure. Different projects, different goals. At every instant where Telegram had a decision to make between better user experience or user security & privacy, Telegram opted to make a better experience. Signal took significant UX hits to make the privacy promises it makes. The two projects are essentially not comparable. Like, the sane thing to compare Telegram to at this point is Matrix. reply jwells89 13 hours agorootparentOf course, but it’s not just UX that’s being traded off but also potential for mass adoption, and evangelism is likely not enough to close the gap. reply tptacek 13 hours agorootparentAgain: different projects, different goals. What Telegram is doing is inherently viral; the platform has an emphasis on bringing huge groups of people together, and the resulting design affordances, and that's a goal that basically works against privacy in the first place. Signal's original goal is to replace intimate messaging. People who love the Telegram affordances wish that the platform would also suffice for intimate messages. Why have two platforms, two UX's, and (most importantly) two user bases? People who live Signal wish the other thing. But you can't have both. They are incompatible design spaces. Telegram's security story is atrocious. It's best compared with Slack, not with Signal. If you're looking to compare a serious secure messenger with Telegram, look at Matrix, which has many of the same goals. It is important that there be a messaging platform in common (if not universal) use that is fit for purpose for secure messaging when it really matters: when the compromise a message could cost lives or fortunes. Most \"secure messaging\" is LARPing; it doesn't matter if the \"E2EE\" in a pseudonymous 500-person chat room is secure. By all means, make it harder to dragnet that channel, modernize the protocols, whatever. But you can see right away how little security really matters to these groups, whether they're on Telegram (with no group security) or Matrix (which at least tries), because messaging app affordances are all people talk about with them. reply WirelessGigabit 16 hours agorootparentprevUsing Linux iMessages doesn't even have a client... reply skrowl 12 hours agorootparentprevSignal UX is AWFUL if you have a work PC, a home PC, a phone, and a tablet. Getting messages to flow across all of them is impossible. reply tptacek 11 hours agorootparentGetting messages to flow across lots of different platforms for a single account has been a source for a number of very bad security problems in other messengers. (Recent example: Matrix, Nebuchadnezzar). Different projects, different priorities. reply JoshTriplett 8 hours agorootparentSignal already has the ability to register multiple devices with the same account, though. The only thing they haven't done with this is support sync of past messages. That seems like it should be much simpler than supporting multiple devices in the first place, and I'm wondering if there's some non-obvious attack that they're simply not talking about or if it just hasn't reached the top of their priority list yet. reply ramses0 15 hours agorootparentprevSignal is actually pretty good for the occasional \"cross-iOS/Android\" video chat. Can't use iOS Facetime, don't want to use $META, google/meet is OK, but isn't quite as straightforward as \"@Signal => CALL_ME\". reply macshome 15 hours agorootparentAnyone can use FaceTime now. Non Apple devices will just join via a browser. reply issafram 15 hours agorootparentWhich is ridiculous reply illiac786 14 hours agorootparentprevWhat? how does that work? If you call the cell number of an Android device on facetime, what happens on the android end? reply snazz 14 hours agorootparentYou text them a link to join the call. The Android user can’t be the one to initiate. reply kelnos 13 hours agorootparent> The Android user can’t be the one to initiate. That sounds like a bad joke. \"Everyone can use Facetime, except if you're on a certain platform you have to ask someone on the 'real' platform to initiate the call.\" That's... not a viable communication method. reply simondotau 12 hours agorootparentNobody is claiming that FaceTime is the singular universal communications method. Certainly not Apple. reply illiac786 13 hours agorootparentprevOh. Ok that's never going to work. \"Hey there please click on this link to talk to me\" => Malware! Blocked. reply josephd79 11 hours agorootparentthats how almost all video comm platforms work.. reply simondotau 12 hours agorootparentprevIt’s how many Zoom and Teams calls terminate. Not saying it’s a good idea, but it’s not especially unusual. reply illiac786 4 hours agorootparentBut you don't get zoom links via _SMS_ do you? Almost all links I get via SMS are phishing. The rest is OTP, there is sometimes a link in the same SMS but I _never_ clicked a link in an SMS... My point is, the inhibition to click a link inside an SMS is very high with lots of users, hence I can't imagine this working well. Also, it's purely inconvenient. You're not having a conf call at a previously agreed time, you want to call someone, this means it's spontaneous or somewhat urgent... reply duskwuff 3 hours agorootparentMost people lead up to an invitation to a video call with text messages (e.g. \"When are you free for a video call?\") unless they're very familiar with the other party. reply adamtaylor_13 17 hours agorootparentprevHmm… Does signal only really work when everyone uses it? Or can you include people who are just using regular SMS? reply temp0826 17 hours agorootparentI don't think any app besides iMessage can even get permission to read SMS on iOS, there are no alternative message apps I'm aware of like with android. reply bebopfunk 15 hours agorootparentprevIt used to have SMS support but they yanked it maybe a year or so ago. The big issue on the user end was, if someone deleted Signal and you used Signal for your SMS, it would keep sending them signal messages and not SMS. So you were left not realizing you were texting essentially a dead number. reply crtasm 6 hours agorootparentIt didn't show the icon for the message status, same as any other signal message? reply JoshTriplett 8 hours agorootparentprev...much like iMessage. reply issafram 15 hours agorootparentprevThe regular SMS/MMS feature was removed and for good reason. Many people cried about it but it was the right call. Messages would not be E2EE and people could easily think all messages sent through Signal are safe when they weren't. reply Jtsummers 17 hours agorootparentprevOnly Signal users. The Android app used to work with SMS/MMS until a couple years ago. reply 0x457 16 hours agorootparentprevI noticed an uptick in scams using iMessage. It's (almost) obvious that you are not talking to a real human. The scheme is pretty simple: lure you to iMessage, so you feel safe, then propose moving to another chat service that is easier to run bots on (telegram, whatsapp, etc) and start a regular programming. reply sunshowers 14 hours agorootparentprevDo you not send messages to any Android users? That is incredibly hard to imagine for me. My friend group is very mixed and Signal is what we use. reply sneak 17 hours agorootparentprevnext [15 more] [flagged] artimaeis 16 hours agorootparentAs far as I'm aware, iCloud for Messages is not enabled by default. Security-conscious users should know not to turn it on, though -- if iCloud servicing warrants is part of their threat model. Further, Apple ended up rolling out Advanced Data Protection for iCloud in 2023, so users who truly don't want Apple holding those keys can effectively take them from Apple. https://support.apple.com/guide/security/advanced-data-prote... EDIT: Just confirmed with a test device and test apple id, iCloud for Messages is not enabled on it. reply rekoil 13 hours agorootparentMessages in iCloud is separate from the inclusion of the iMessage database in the regular iPhone iCloud backups. Could you check what that setting is set to on your newly setup device? reply artimaeis 8 hours agorootparentRight, that’s why I am trying to clarify that it’s iCloud for Messages that is not enabled by default. iCloud backup IS enabled. But Messages in iCloud is off. reply rekoil 1 hour agorootparentRight, and in iCloud backups you can choose to include or don't include iMessage data in those, is the iMessage data included by default? reply marcellus23 16 hours agorootparentprevTo clear up the FUD here, this is only true if you turn on iCloud backups (many users do, but still) and don't turn on Advanced Data Protection. ADP is off by default because it means you'll lose all your backups if you forget your iCloud password. > it’s a platform designed to aid illegal government surveillance. Come on. reply jorvi 16 hours agorootparent>> it’s a platform designed to aid illegal government surveillance. > Come on. Whilst I agree with the general skepticism, Apple didn’t add E2E encryption to (certain parts of) iCloud backups at the explicit request of the FBI. https://arstechnica.com/tech-policy/2020/01/apple-reportedly... reply marcellus23 16 hours agorootparentThat's still not the same as claiming that iMessage is designed specifically for the purpose of aiding government surveillance. reply jorvi 15 hours agorootparentNot adding E2E was a conscious design decision by Apple specifically to aid surveillance. Virtually every other big messaging service offered (optional) E2E at that point. I’m not sure how much clearer you want it. “Surveillance” doesn’t have to mean that Apple allowed the FBI to jack directly into the iCloud servers. reply rootusrootus 13 hours agorootparentWasn't it an attempt to take the fangs out of the FBI's push for encryption backdoors? As one of the largest messaging platforms in the US, what Apple does with E2E absolutely factors into public policymaking. reply sneak 16 hours agorootparentprevThese are the defaults. You don’t need to turn on iCloud Backup, it’s already on. You don’t need to turn off Advanced Data Protection, it’s already off. Literally all you need to do is turn on a new iPhone and try to install any app. It will prompt for your Apple ID login (impossible to install apps without it) and will automatically enable iCloud, iCloud Backup, and iMessage (and will not enable ADP). https://www.forbes.com/sites/kateoflahertyuk/2020/01/21/appl... They explicitly killed the e2ee support for backups some time ago at the behest of the FBI to preserve the backdoor. It’s still practically backdoored for nearly all iMessage users because it is off by default (and the UX sucks even if you turn it on). Approximately nobody is using it; the status quo is preserved. iMessage is backdoored and is not e2ee due to key escrow. If you message someone on iMessage, Apple will be able to read the message, even if you have ADP enabled (because the other endpoint does not). That’s fact, today. Additionally, even if you turn on ADP, the hashes of unencrypted file content in iCloud are stored non-e2ee, so Apple can still see who has which unique files and when, and who else receives them and when. This allows them to monitor social graphs, too. reply overstay8930 16 hours agorootparentiMessage is excluded by default on iCloud backups, that's what the other guy is saying reply rekoil 13 hours agorootparentMessages in iCloud (different thing, used for syncing iMessage conversations to all your devices) is off by default, to my knowledge backups of the iMessage database is on by default in the iCloud backup setting, but admittedly I haven't setup a new device without restoring an iCloud backup in many many years. reply mr_toad 7 hours agorootparentMessages is iCloud is mostly used to free up local storage, and to sync old messages to new devices. You don’t need it to sync messages most of the time. reply rekoil 14 hours agorootparentprev> and will not enable ADP Not only will it not enable ADP, it won't even ask you about it. reply markstos 16 hours agoparentprevI thought it was a great advertisement for Signal because Signal and their protocol is public and their software is free, open source and cross-platform. In light of that Apple's message reads more like announcement that people who can afford iPhones will now have better security and privacy when they chat with themselves. reply bjoli 12 hours agoparentprevI am not sure if I understand things correctly, but what apple did seems like a lot of work for little benefit. Symmetric encryption is not threatened nearly as much by quantum computers. Using a PQ key exchange should make adequately safe until someone breaks symmetric encryption, which seems like a high odds bet. The author of age wrote some time ago why aes128 is good enough. But I am a classical musician so for the love of god don't listen to me. reply xxmarkuski 18 hours agoprevDavid Basin and his team have done really cool work in the past. I was lucky enough to see a talk by him about EMV Race, which are exploits in the EMV protocol used by credit cards. Their approach included modeling this protocol in Tamarin. Its website [0] shows demonstrative videos. [0] https://emvrace.github.io/ reply lapetitejort 18 hours agoprevAdvanced encryption, until it comes time to text 70% of the phones in the world, in which case it defaults to a protocol released 32 years ago. reply yackback 17 hours agoparentRCS is unencrypted unless you use Google's closed garden Google Messages' extensions. Apple is apparently working with GSMA to add encryption to the standard though. (They probably wouldn't add RCS otherwise.) reply astrange 14 hours agorootparentThey would if a regulator made them. They're adding it because China requires it. reply para_parolu 18 hours agoparentprevWhy does 70% of world phones matter here? There are phones in the world that do not support any encryption when sending messages or doing calls. reply madeofpalk 17 hours agorootparentSMS/RCS reply browningstreet 18 hours agoparentprevThe top 7 phones sold last year were all iPhones. https://www.macrumors.com/2024/02/21/iphones-top-7-best-sell... Too bad the other vendors don’t bother keeping up. reply not_a_real_56 17 hours agorootparentThere is a flaw with your thinking. Any given year there are only 5-6 iPhones to choose from, where there are plenty of Android phones. This leads to \"top phone sold\" being iPhones because it is 5 phones against hundreds of Android phones that people get to choose from. You should instead look at Market share. https://www.statista.com/statistics/272698/global-market-sha... reply browningstreet 15 hours agorootparentActually my point was companies-responsible-for-encryption development. You could argue it’s Google vs Apple on this front, but it could also be Apple vs Samsung, or Apple vs any of the other top tier android implementations. So you can either say Apple is reserving this development for a subset of the market, or Google is withholding it from a massive portion of the market share. reply danShumway 14 hours agorootparentSamsung is not the reason why iMessages can't be sent to Android users or to Windows devices. Samsung does not decide which platforms Apple will and won't support. Coordination with even Google would not be necessary for Apple to offer encrypted conversations with users on other devices. There's no rule saying they need to use an open standard or a Google standard or be cross-compatible with another app. It's not that Apple is trying desperately to get iMessage onto other phones and failing because Google and Samsung just won't let them do it. Of course, Google has its own problems[0]. But the inability to use the Messages app to communicate securely with Android users[1], is solely 100% Apple's decision. Apple does not need to ask permission or coordinate with any other company to increase that security, they would just need to throw a messaging app up on the app store. Heck, they wouldn't need to support iMessage on Android. They could throw a messaging app up that had no encryption other than that it worked over HTTPS and data instead of SMS when messaging iOS users, changed nothing about the capabilities or features that they supported for non-iMessage users, and even only doing that -- if Android users could download it and set it as their default SMS client on Android then iPhone security would be better. ---- As a comparison here, if Signal dropped support for iOS tomorrow, would you blame Apple for not building support for Signal into iOS? No, that would be absurd to suggest. No one would claim that Apple had some obligation to support the Signal protocol or make Signal compatible with iMessage, or to build an open protocol -- we would all correctly point out that Signal decides where to make its app available. The same is true of Apple. The fact that you literally can't make many Messages conversations secure without completely abandoning the app and using a separate 3rd-party service for those conversations -- it is purely and entirely the result of a decision that Apple has made. ---- [0]: And in fact their proprietary encryption standard is no better than Apple's and they're pulling the exact same crap as Apple is for the same flimsy reasons. [1]: Note that I don't say non-Apple users, you can have an iMessages account through other devices and you still won't be able to use it with an Android phone number. reply throw0101c 14 hours agorootparentprev> Any given year there are only 5-6 iPhones to choose from, where there are plenty of Android phones. There is such a thing as 'too much' choice: * https://en.wikipedia.org/wiki/Overchoice * https://en.wikipedia.org/wiki/Decision_fatigue * https://www.behavioraleconomics.com/resources/mini-encyclope... * https://thedecisionlab.com/biases/choice-overload-bias reply Terretta 15 hours agorootparentprev> you should instead look at Market share. Unless your goal is to make money on the platform, then you should look at wallet share, not market share. reply danShumway 14 hours agorootparentThe goal isn't to make money or calculate wallet share. The goal is to send encrypted messages to contacts. You should look at the percentage of smartphone owners. It does not matter in the slightest how many dollars they have in their pockets. The question is: is the average user going to have a significant number of Android contacts, with which Messages requires plain-text communication to contact. And the answer for most people is: undoubtedly yes. I would say that most people who are using Messages as their primary messenger for all of their contacts are sending unencrypted messages on the regular. reply Terretta 14 hours agorootparentCompletely fair, I had shifted topics from phone owners, to platform relevance to makers on HN. Your point stands. reply SSLy 17 hours agorootparentprevthat's only 16% of total market. reply Terretta 14 hours agorootparentBut 50% of revenue share (and has been as high as 110% of profit share): https://www.counterpointresearch.com/insights/iphone-hits-re... That's not the point if you're talking about who you can have an encrypted conversation with, but it matters if you want to know if you can afford building an encryption tool to serve phone buyers. reply carstenhag 12 hours agoparentprevBarely anyone uses sms nowadays. People use WhatsApp, FB Messenger and co (outside of the USA). reply subtra3t 17 minutes agorootparentI think people who downvoted you did so after reading just your first sentence and not the entire comment. Outside of USA, I don't think anybody uses SMS for anything other than the occasional OTP or bank message. In India, nobody used anything other than WhatsApp till a few years ago. Now, teenagers use Instagram's chat feature and WhatsApp, and middle-aged adults use FB Messenger and WhatsApp. reply danShumway 16 hours agoparentprevThis is getting downvoted, but it really does feel like a variant of the wrench problem: https://xkcd.com/538/ It's already incredibly hard to get people to use secure messaging systems. Downgrading to SMS isn't necessarily wrong (it's become harder to get people to use Signal now that it's dropped support for SMS), but it's a huge hole and effectively means that many customers will never have a significant number of their conversations encrypted. That's a boring security hole, sure. But at some point you have to think about UX as being a part of security, and a messaging system that isn't cross-platform is hard to call secure, because good luck trying to get your contacts to all use it. People get upset about this, but the reality is it does not matter what encryption scheme a messenger is using if it's impossible for you to get your contacts to use it. The same way that it does not matter how secure your 2FA system is if you can't get people to turn it on. I felt like on net Signal's support for SMS was a boon for security more than a hindrance because it made it easier for me to get people to sign up for Signal. In contrast, Signal's take was that having a secure and insecure service bundled up into the same messenger would on average make people more lax about security and would make it harder for them to make strong security guarantees. They viewed SMS support essentially as a security vulnerability. I do wish Signal had kept SMS and tried harder on the UX, I honestly feel somewhat strongly that removing support made secure messaging harder -- but while we can debate the security downsides and the onboarding downsides, I also have grown to kind of see their point? And iMessage falls very squarely into that problem, except with Signal I can at least tell my contacts how to get it. I don't know, it feels petty but like... if you have secure encryption but it doesn't get turned on for a bunch of messages, then that does seem like it has a security impact. I don't think that's a complicated or controversial thing to say, it's no different from calling out that some chat services require E2EE to be opt-in instead of opt-out. Good security requires thinking about that kind of stuff. It's the wrench problem. You're not going to get spied on by a quantum computer. You're going to get spied on because there's a decent chance that ~50% of your contacts or more aren't on iPhone and you'll be talking to them in plain text. And realistically for most users, switching to a cross-platform E2EE messenger that allows them to use one consistent service for all of their encrypted conversations is going to be meaningfully more secure even if it doesn't have quantum-resistant encryption. The most important problem for any secure messenger to solve is how to get people to use it. Sometimes that means compromising on other security standards, sometimes it means being harsher about security standards that would otherwise be optional. Sometimes it means caring about availability and onboarding, and not sending the majority of messages in an easily intercepted plain-text format. reply smoldesu 13 hours agorootparent> It's the wrench problem. You're not going to get spied on by a quantum computer. I'll take that one step further; it's the trusting trust problem. In the words of Ken Thomson, \"To what extent should one trust a statement that a program is free of Trojan horses?\" You're not going to be spied on by a quantum computer because intelligence agencies already use classical computing for that. Some governments write Apple or Google a strongly worded email, others install a backdoor using iMessage. There's no need to crack your encryption because you're not going up against quantum adversaries; those people all have better options than bruteforcing Apple's lock. Sufficiently-motivated actors skip the wrench and pay Bob or Alice for your password. There's no perfect solution to this issue. Apple would sooner die than lower the drawbridge to iMessage, and Google can't be bothered to write an altruistic RFC to save their life. Now we get the worst of both worlds; divided and surveilled. reply Aaargh20318 17 hours agoparentprevThey have already announced they will add support for RCS this year (https://9to5mac.com/2023/11/16/apple-rcs-coming-to-iphone/), so not exactly a protocol released 32 years ago. It’s probably coming in iOS 18 Still unencrypted though, because the RCS standard does not include encryption. reply tsunamifury 18 hours agoparentprevYes but that would have meant giving up sales in exchange for actually backing up their words. They aren’t even going to use the developed encrypted RCS protocol. Apple, when it comes to their values, is all lip service. I have intimate experience here with them both openly lying and purposefully deceiving their user base in this case. reply matthew-wegner 18 hours agorootparent> They aren’t even going to use the developed encrypted RCS protocol. End-to-end RCS encryption is via proprietary Google extension and not even available to other Android RCS messaging apps. reply tsunamifury 17 hours agorootparentIt was made available to Apple. reply GeekyBear 16 hours agorootparentCitation? Beeper was explicitly told it was not available to others when they wanted to implement Google's encrypted RCS on their Android client. https://twitter.com/ericmigi/status/1557050351974420480 reply zchrykng 16 hours agorootparentprevDoesn't really help the argument that it is a proprietary Google technology. The fact that it had to be \"made available\" to Apple means that it isn't an open standard and requires trusting Google, which many of us don't. reply jxdxbx 16 hours agorootparentprevEncrypted RCS should be a standard. When it is Google should adopt the standard, not its proprietary version. In the meantime I want Apple to implement standards, not proprietary Google technologies. reply brookst 8 hours agorootparentprevUnder what terms, and with what promises? If Google's strategy was anything other than \"get Apple to adopt, then screw them\", Google would have contributed the enhancements back to the standard. reply GeekyBear 18 hours agorootparentprev> the developed encrypted RCS protocol Google's proprietary closed source fork of RCS? > Google's version of RCS... is definitely proprietary, by the way. If this is supposed to be a standard, there's no way for a third-party to use Google's RCS APIs right now. Some messaging apps, like Beeper, have asked Google about integrating RCS and were told there's no public RCS API and no plans to build one. If you want to implement RCS, you'll need to run the messages through some kind of service, and who provides that server? It will probably be Google... So the pitch for Apple to adopt RCS isn't just this public-good nonsense about making texts with Android users better; it's also about running Apple's messages through Google servers. Google profits in both server fees and data acquisition. https://arstechnica.com/gadgets/2022/08/new-google-site-begs... reply Jtsummers 18 hours agorootparentprevWhat word? They never said they'd open iMessage. FaceTime is what they intended (or at least Jobs announced the intent) to open, and then that got caught up in a patent dispute. reply 0x457 16 hours agorootparentI remember them saying releasing iMessage as an open standard. reply jxdxbx 16 hours agorootparentThat was FaceTime, and it was because it was a peer-to-peer protocol. Then Apple was sued over a patent and had to implement a more normal design, where it pays for and runs the servers. Apple didn't want to run the servers for Android people to talk to each other. reply Jtsummers 16 hours agorootparentprevCan you point to a source for that outside your memory? reply layer8 16 hours agorootparentThey’re confusing it with FaceTime: https://youtu.be/JOxf9tEXEKQ reply Jtsummers 14 hours agorootparentI figured that was the case. I just wanted one of these folks that keep claiming it to cough up some evidence. It's a tired thing. There are lots of things to criticize Apple (and most companies) for, at least pick something that isn't imaginary. reply 0x457 2 hours agorootparentprevI guess. But I swear I remember it was about iMessage. oh well, my bad. reply tsunamifury 18 hours agorootparentprevNo, they could have shipped a product on android, or they could have used secure RCS, or several other things. FaceTime is not really true either. This was a convenient mistruth. There were several way to remedy that situation. Also your misremembering iMessage there was no such issue. reply Jtsummers 17 hours agorootparentYou wrote: > Yes but that would have meant giving up sales in exchange for actually backing up their words. What word did they not back up with respect to iMessage? When did they ever say they'd open it up? reply drcongo 17 hours agorootparentprevFaceTime is definitely true. I've used it. reply detourdog 18 hours agorootparentprevGoogle could have continued to support XMPP. If wishes were knishes doggies would eat. reply matchbok 18 hours agorootparentprevIt's not Apple's fault that the carriers/RCS/Google don't know how to make messaging work. RCS sucks, it pales in comparison to anything but SMS. reply kergonath 17 hours agorootparentprev> Yes but that would have meant giving up sales in exchange for actually backing up their words. I saw that you moved the goalposts in your reply, but anyway: which words? > They aren’t even going to use the developed encrypted RCS protocol. The protocol that was designed by Google and in which Google’s infrastructure is crucial? It’s not Apple’s fault that RCS does not have mandatory end-to-end encryption. > I have intimate experience here with them both openly lying and purposefully deceiving their user base in this case. Do you mean that they lied to you personnally? You should write about that, it sounds much more interesting. reply Jtsummers 17 hours agorootparentRCS was not designed by Google. Google has (so far) embraced it after repeatedly self-sabotaging their own chat efforts. RCS is a carrier standard that the carriers had trouble deploying. reply rootusrootus 12 hours agorootparent> RCS was not designed by Google. Encrypted RCS was. And it is proprietary. reply tsunamifury 17 hours agorootparentprevI worked on several integration attempts between Apple and Google. They often presented specs that clearly showed security holes then simply would deny they were there or say “that’s secure”. It was a truly wild experience. reply kergonath 12 hours agorootparentThis sounds fascinating, you really should write about this. reply maqp 17 hours agoprevDid anyone figure out how MITM attacks are handled? How does the key transparency work and does it replace public key fingerprints? reply honzaik 16 hours agoparentThis (and Signal's solution as well) does not protect against active MITM attackers with quantum computers. They would need to incorporate post-quantum signatures into it as well. The reason why it is missing (but seemingly planned in the future) is because it is not as critical as this change. This change prevents attackers from recording conversations now and decrypting them when (in the next ?? years/decades) they get access to an actually powerful quantum computer. On the other hand, you can do MITM only after you factorized RSA key (or solved discrete log). The additional reason I presume is that this typically requires a change to the whole public key infrastructure (certificates, OCSP, etc.) which is a lot of additional work. reply maqp 15 hours agorootparentI'm a bit puzzled. Suppose you do single Kyber key exchange, and you then hash the shared secret with a domain separator to get a fingerprint, wouldn't that mean you now have a shared secret that you can verify wasn't under MITM. You then can build post quantum future secrecy / key rotation on top of that, by mixing in new key material and it remains secure from MITM, as long as the internal state of the endpoint isn't compromised. The endpoint compromise is outside networked TCB threat model, as such compromise could also be used to exfiltrate long term post-quantum identity keys for undetectable MITM). reply Retr0id 15 hours agorootparentKey exchange (whether it's a PQ scheme or more classical DH) does not prevent active-MITM on its own, you need authentication too. reply maqp 14 hours agorootparentYeah that's what the fingerprint is for, you compare it to authenticate the key exchange. reply m3kw9 16 hours agoparentprevReplace this question with “how competent is the Apple security team” reply RenThraysk 10 hours agoprevCan't a MITM tell when a PQC rekeying is happening due to the larger size, and just not deliver it? reply bedros 8 hours agoprevWould syncing to iCloud would compromise E2E encryption? reply nan60 5 hours agoparentFair question, and it actually depends on how iMessage is being backed up to iCloud (sorta). By default, iMessage is included in your iCloud device backup, which when Advanced Data Protection is disabled, is NOT E2E encrypted. That said, if Messages in iCloud is enabled, which instead syncs your messages to iCloud, They are always E2E encrypted, regardless of if ADP is enabled [1]. Even more confusingly, if ADP is off, the keys for Messages in iCloud are still stored in your iCloud backup [2]. So essentially, the only way to use iMessage E2E encrypted is with Advanced Data Protection enabled, regardless of if you're using Messages in iCloud or not. [1] https://support.apple.com/en-us/102651 [2] https://support.apple.com/guide/security/security-of-icloud-... reply walterbell 55 minutes agorootparent> the only way to use iMessage E2E encrypted is with Advanced Data Protection enabled If iMessage backup relies on ADP encryption, will ADP move to PQ3 Cryptographic Protocol? reply walteweiss 3 hours agoprevThe only question that I have, will it break my old devices, so they would be forced out of the iMessages ecosystem? I believe that would happen. I don’t use it heavily, but that’s one of the reasons I keep an iPhone these days. The other reason is FaceTime support, so I could easily connect with those with iPhones. I do use not very modern iPhones (the very first SE currently), as that’s what I need my iPhones for: to make a call (via FaceTime most times) and to send a message (via iMessage, sometimes). If Apple breaks that for me, I’m either forced to buy a newer iPhone (which I’m not sure I want, considering I’m all in Linux and Android). Or they force me to rethink my priorities and just leave this. I can live with that, it’s just less convenient. reply m3kw9 16 hours agoprevIs this country dependent? reply nan60 5 hours agoparentNo. This will be rolled out to everyone in iOS 17.4, iPadOS 17.4, macOS 14.4, and watchOS 10.4, and is already in the corresponding developer preview and beta releases[1]. [1] https://security.apple.com/blog/imessage-pq3/ reply jacobgorm 19 hours agoprevWill the code be available? reply soxicywn 19 hours agoprevDoes anyone know if this is still vulnerable to the iCloud Backups problem? The only solution to that right now is for you and your contact to turn on Advanced Data Protection. Curious if that’s still required. reply xoa 18 hours agoparent>Does anyone know if this is still vulnerable to the iCloud Backups problem? The only solution to that right now is for you and your contact to turn on Advanced Data Protection. This is such a strange two sentences as a \"problem\". E2EE security, as it says in the name, is about the protection of dara transmission between two trusted end points. That's it. What the trusted end points themselves choose to do with that data before and after is completely out of scope, and has nothing whatsoever to do with the data transmission aspect. This is true of everything ever. No communication service stops people from backing up with encryption or not, local or remote, or from copy/pasting or for that matter taking photos of the screen (\"analog hole\"). If you want to \"protect\" the data from the trusted owners themselves now we're in the realm of DRM. In this case what you wrote is \"do non-e2ee remote backups still occur if users do not enable e2ee remote backups or backup locally?\" to which the answer is yes. I definitely do blame Apple for not having APIs for backing up to arbitrary network servers when it comes to iDevices, but it's still orthogonal. And remember iMessage is on the Mac as well where people may be backing up anywhere. reply AnonC 17 hours agorootparent> No communication service stops people from backing up with encryption or not, local or remote, or from copy/pasting or for that matter taking photos of the screen (\"analog hole\"). At least for the first part on backing up without copy pasting or using the “analog hole”, Signal expressly prohibits and doesn’t allow any kind of backup — encrypted or not — on iOS/iPadOS/macOS. reply zchrykng 17 hours agorootparentThis lack of backups makes Signal less appealing to anyone who isn't a security/privacy enthusiast/nut. 99+% of people want their messages to work and not lose them when their phone is broken. reply codedokode 15 hours agorootparentNo I do not agree with you. Majority of people never read their message history, want their messages to self-detruct and don't want to get into a situation like when a new partner reads chat history with all previous partners. Majority of people do not record their conversations and do not need this. And most messaging applications are designed countrary to what people need - they preserve history specially for that curious new partner. Or maybe for a marketing department to analyze user's interests. reply macintux 11 hours agorootparentI’m very, very skeptical that any of us can accurately assert that a majority of people want this specific behavior from their messaging apps. reply sib 9 hours agorootparentprev>> No I do not agree with you. Majority of people never read their message history, want their messages to self-detruct Citation needed. reply xoa 16 hours agorootparentprev>Signal expressly prohibits and doesn’t allow any kind of backup — encrypted or not — on iOS/iPadOS/macOS. I do not think you are correct, or perhaps alternatively this is a distinction without meaning. iDevices do indeed lock down against owner control unless the device is jailbroken. But Signal for Mac only requires 10.15 or later. Even if they wanted to, old Intel Macs simply do not offer the hardware guarantees to protect against the owner getting access to their own data if they want to, though even current ones will still let you turn off SIP etc if you wish. I don't even need to look to guarantee that if someone wants access to their own Signal data on the Mac (or Windows, or Linux which can be run with any 64-bit distributions supporting APT), or any of these virtualized (and thus on the BSDs which aren't formally supported [0]), they can get it. And again, this is somewhat a distinction without meaning. Like, how does someone read their messages on Signal for Desktop after setting it up and it's syncing going forward? They login to the system, and there is a saved key and that makes it work. If they then choose to backup said system or VM without encryption now what? I have heard that on iOS Signal has always been somewhat evil in attempting to steal people's data away from them, which is part of the reason I avoided it. But fortunately we don't yet live in a world where the same games can be pulled on regular computers. And hopefully eventually legislation will make it illegal on all computers, including handhelds, too. ---- 0: 64-bit distributions supporting APT reply super_mario 9 hours agorootparentprevSignal perhaps does not allow you to export your message history through the front door, however decrypting and exporting your message history is relatively low effort. You messages are stored in encrypted SQLite3 database. The Signal encryption key is in ~/Library/Application\\ Support/Signal/config.json in plain text. If you have SQLCipher (https://github.com/sqlcipher/sqlcipher) compiled you can decrypt your Signal database: Navigate to ~/Library/Application\\ Support/Signal/sql/ and type sqlcipher db.sqlite sqlite> PRAGMA key = \"x''\"; sqlite> .schema and query away. Of course there is a Python package to automate all of this here: https://github.com/carderne/signal-export This exports your message history as markdown and HTML files for your convenience and it will do incremental exports as well. For iOS the same holds true, considering iOS has had a jail break most of its existence. So, in retrospect your Signal messages are only as secure as computers of the people you talk to and of course your own device. I would go a step further and assert that there is no such thing as secure communication. reply callalex 9 hours agorootparentprevYour argument about iMessage on Mac’s holds water, but the iPhone is sold as an appliance and there is no concept of a file system presented to the user. They sell the messaging platform as “encrypted” and the word “encrypted” is plastered all over iCloud marketing and documentation. You really can’t blame the user for assuming the appliance is secure by default. When you first set up an iPhone it just asks you if you want to back up to iCloud. It doesn’t use the word “insecure” nor “unencrypted” anywhere on that screen. reply diebeforei485 8 hours agorootparentprevThe only backup provider allowed is also the communication service provider, which defeats the point of E2EE because the keys are also stores transparently (ie, not even encrypted with the device passcode). That is the problem. Apple's business decision to ban other backup providers like Backblaze etc is making their privacy efforts meaningless. Why is it a problem? Because governments and powerful entities need only contact Apple (the communication provider) to know the contents of the message. And I as a user have no way of knowing if others in the chat have advanced protection enabled, as I am only protected if every participant in the conversation has the correct settings, which are not the default settings. So while this might meet some technical definition of E2EE, it does not meet the common-sense definition of E2EE where the communication provider has zero visibility into content. reply Andrew_nenakhov 12 hours agorootparentprevThis is how Constantinople fell to the Ottomans: they had very high hard to penetrate walls, but forgot to lock one gate. * Replace the walls with highly secure encryption e2e algorithm, and the gate with easily accessible backup, and you'll see why things like this are not out of scope. * - this story is disputed by some historians reply modeless 17 hours agorootparentprevIt's not strange in the slightest. Apple deserves criticism until they fix this. They're going around claiming \"end-to-end\" and people don't understand that they are constantly handing over people's decrypted messages to law enforcement. It's misleading at best; I call it fraud. It's not as though Apple is merely failing to prevent a third party from breaking their end-to-end encryption here. Apple does it itself! iMessage and iCloud are not separate companies operating independently. The right hand knows what the left is doing! This is not a UX issue or an engineering issue. Apple already built end-to-end encryption for sensitive data types that is still recoverable from backups even if you lose all your devices and forget your iCloud account password. They do it the same way Google does, and they already use it by default for important stuff you don't want to lose like passwords stored in Keychain and health data and a bunch of other stuff too. Literally all they need to do is store the iMessage encryption keys in this system by default. They continuously choose not to, and the reason is reported by Reuters to be a secret compromise agreement with the FBI. https://web.archive.org/web/20200121123026/https://www.reute... reply xoa 16 hours agorootparent>It's not strange in the slightest. It very much is strange. >Apple deserves criticism until they fix this. There's nothing to fix, or rather they already \"fixed\" it by offering an E2EE iCloud backup option to go along with local backups. As I said I think backups should simply be fully under owner control, but as it stands there is absolutely no need to backup without full key control should people wish. And even before that there was no need to use iCloud Backup. I never have. But that has tradeoffs, and it's perfectly reasonable people may choose to make different ones. >They're going around claiming \"end-to-end\" Correctly. By your twisted definition, there is no such thing as E2EE for any transport in existence because the ends might then do something you don't approve of with the data they own. HTTPS? Not E2EE. SSH? Not E2EE. WireGuard? Not E2EE. Which is completely ludicrous and a total perversion of the specific, important role E2EE plays. >They already built end-to-end encryption for sensitive data types that is still recoverable from backups even if you lose all your devices and forget your iCloud account password No, if you use their full E2EE options, any of them, and you lose all your devices, your password, and recovery key (including any backups you've chosen to make on your own), you are hosed for any of the data that is E2EE protected. Like, by definition? Because otherwise it wouldn't be E2EE! The fallback when ADP is not turned on and someone is using iCloud Backups is that Apple does have the keys, that's the point. There is literally no way around this, it's just definitional. If Apple has, somewhere in the stack, the keys then it can be compelled (or choose) to share them or share access to the data, but they can also help the owner recover if all else is lost. If the owner has exclusive access to all keys then the owner has exclusive responsibility. You can certainly have the opinion that Apple should make that latter the default of only choice. I certainly have the opinion they should offer more choice period. But that's still all orthogonal to the transport mechanism. You can have ultra locked down encrypted devices, and then go to a plain vanilla HTTP website or use telnet for administration and any MITM can see what you're doing. There could be a rootkit on your system that's grabbing everything right out of memory. That doesn't mean random MITMs can see what you're doing either if the transport is E2EE. All of these are important components of the overall security picture, but they're all different ones. >is reported by Reuters to be a secret compromise agreement with the FBI Read your own articles you link. That's a 2020 piece on Apple dropping old plans for owner key control of all private iCloud data. But specifically following the outcry there two years later Apple introduced \"advanced data protection\" that does precisely what that article is complaining they didn't earlier [0]. It got lots of coverage at the time. They explicitly cover how data is stored afterwards [1]. So people can turn that on. The Reuters piece is obsolete. ---- 0: https://www.apple.com/newsroom/2022/12/apple-advances-user-s... 1: https://support.apple.com/en-us/102651 reply modeless 15 hours agorootparent> There's nothing to fix The default. They need to fix the default. > By your twisted definition, there is no such thing as E2EE for any transport in existence What a ridiculous misunderstanding of my position. iMessage and iCloud are inseparable parts of the whole of iOS, all from the same company, and their default configuration is not end-to-end encrypted. My position is that it is fraudulent to treat them as if they were separate to claim \"end-to-end\" encryption in only part when it's broken by the other part by default. Plenty of other systems are legitimately made of multiple parts by different companies and can claim end-to-end individually when their defaults are appropriate, even if they aren't when combined together by users in non-default configurations. There is no contradiction here, it's quite unambiguous. > No, if you use their full E2EE options, any of them, and you lose all your devices, your password, and recovery key (including any backups you've chosen to make on your own), you are hosed for any of the data that is E2EE protected. This is false. Apple and Google both now have a system that uses your phone passcode (distinct from your account password and practically impossible to forget as it is so short and you practice entering it literally every day) as the key to unlock your encrypted backups. They use secure elements in the datacenter to protect the weak passcode from brute force attacks, even from themselves. > The Reuters piece is obsolete. The Reuters piece is as relevant as ever until Apple changes the default for iOS so that Apple can't read the vast majority of all iMessages. reply xoa 15 hours agorootparent>The default. They need to fix the default. No, they do not. That you don't give a shit about people losing data is a value tradeoff you believe in, but you've got a lot of work to argue it's an objective universal. >What a ridiculous misunderstanding of my position. It's amazing how you can say this with a virtual straight face, then immediately go on to directly argue that yep, that's your position. >iMessage and iCloud are inseparable parts of the whole of iOS They literally are not. Local syncing of iDevices predates iCloud backups even existing as a feature. You do not need to use iCloud Backups or data syncing. I never have. But if this logic applies, then it applies to everything! You can sync Safari browsing history, state etc too. Apps can sync data as well. So that must mean HTTPS is somehow no longer E2EE either. Unless you turn it off. Then magically it becomes E2EE? Be consistent. >and their default configuration This is a goalpost shift and stupid. >My position is that it is fraudulent to treat them as if they were separate to claim \"end-to-end\" encryption in only part when it's broken by the other part by default Because somehow you don't understand what E2EE even is. E2EE in communications solves one, specific and very important problem, which is data in flight. iMessage, HTTPS, or whatever else being E2EE, is a meaningful and significant difference then SMS or HTTP. It changes which potential actors can access that data, and how. End point security is an entire different problem with different sets of tradeoffs. You're just objectively wrong and muddling an important distinction. Also, if you actually think it's \"fraudulent\" then by all means, sue them for false advertising, or contact your local authorities in charge of that. Good luck! >This is false Nope, it's correct, but there's a bit of a pattern here. >Apple and Google both now have a system that uses your phone passcode (distinct from your account password and practically impossible to forget as it is so short and you practice entering it literally every day) My phone password is 21 characters long and I almost never enter it because of Face ID. I'm starting to wonder if you actually own and use iDevices at all or if you're just regurgitating stuff you've read on the web? Even for people just using PINs, the vast super majority make heavy use of biometrics to the extent that Apple forces people to unlock once every few weeks just to try to help make sure they remember. But people forget anyway. Older people or those with other forms of memory loss forget a lot of simple stuff, including their own phone numbers, all the time. People have accidents. One of my cousins just got hit by a car while riding his bike and suffered a bad concussion followed by a long period of amnesia. At Apple's scale they absolutely need to, and should, care about such things. You just said it was wrong that if someone loses their passwords (and PINs are just a kind of password, \"something you know\"), they are hosed on the data because... uh... people don't forget! Wild. >The Reuters piece is as relevant as ever Nope, it was specifically about there not being an option, at all. reply modeless 12 hours agorootparent> You do not need to use iCloud Backups You do if you want cloud backups (as most people do), because Apple prohibits you from doing it any other way. You can't uninstall the iCloud backup software, you can't replace it, and you can't buy an iOS device without it. It's literally inseparable from iOS by Apple's design, and iMessage is too in exactly the same way. > So that must mean HTTPS is somehow no longer E2EE either Safari doesn't backup the contents of your HTTPS connections to Apple, nor even the URLs for the vast majority (only top level page navigations are stored in history). The analogous situation would be if Safari would relay all the content of every HTTPS connection to Apple servers along with the keys to decrypt it. Maybe you would defend such a system as \"end-to-end encrypted\", but you would be in a very small minority. > My phone password is ... completely irrelevant. Who cares? You're not seriously arguing that 21 character phone unlock passcodes are typical? We're talking about defaults here. > I almost never enter it because of Face ID I was wrong. I thought that you had to enter the passcode at least once daily, but it's actually at least once weekly. However my point stands. It's extremely unlikely for the vast majority of people to forget their passcode, which is distinct from their account password, which is almost invariably very short, and which they practice entering at least weekly. As for the edge cases you mention, every system has edge cases. The non-E2EE account recovery case has edge cases too. It requires navigating Apple's support process and proving your identity via whatever means they request which not everyone will be able to do successfully. Also it's vulnerable to social engineering attacks on the support reps. No system is perfect. If the forgetting issues were so bad, then Apple wouldn't by default encrypt Keychain passwords with true E2EE. Losing those is actually super inconvenient too, but Apple has no problem with E2EE there. That's because law enforcement cares more about reading your messages than logging into your Reddit account (or they can just go to Reddit directly). > PINs are just a kind of password A very special kind of password which is by design much easier to remember and practiced more often. They are very different in practice, don't pretend there's no relevant difference. > I'm starting to wonder if you actually own and use iDevices at all I owned and loved the OG iPhone and many other generations too. Although my current phone is Android, I still use iPhones and iPads casually from time to time. Look, I could continue all day, but long experience has taught me that it's pointless to argue with someone so clearly stuck in the reality distortion field. I believe I've made my points clearly for any other reader of this thread. I won't be responding further. reply samatman 10 hours agorootparentGood choice. You got thoroughly thrashed in this exchange and weren't able to escape with your dignity intact. reply tw04 18 hours agoparentprevThat will always be the case with Apple. Their default behavior is to cater to a user that will shoot themselves in the foot anytime they have the opportunity. I applaud them for giving users the option to be secure, but I don't blame them in the least for making you turn on the thing that can make grandma lose all of her message history. reply gruez 18 hours agoparentprev>The only solution to that right now is for you and your contact to turn on Advanced Data Protection or don't use icloud backup. Also, confusingly \"messages in icloud\" is end to end encrypted, and enabling it disables messages for being included in icloud backup. reply sneak 17 hours agorootparentIf everyone you iMessage with has iCloud Backup still enabled (and I guarantee you 100% that they do because it is the default), then you turning yours off does nothing, as all of your conversations remain readable by Apple via the escrowed keys of the other endpoints. iMessage is not e2ee. reply tptacek 16 hours agorootparentIt does not appear to be the default. reply sneak 9 hours agorootparentWanna bet? Perhaps I should make a video. reply modeless 9 hours agorootparentYou absolutely should, I've been wondering if there was a video somewhere showing the defaults during setup. reply modeless 17 hours agorootparentprev> \"messages in icloud\" is end to end encrypted, and enabling it disables messages for being included in icloud backup. This is misleading at best. Careful reading of Apple's disclosures reveals that the \"messages in iCloud\" encryption keys are still included in iCloud backups, giving Apple the capability to decrypt your messages on demand for law enforcement or for any other reason of their choosing. The messages may not be in your \"iCloud backups\", but that's just because they are stored on Apple's \"Messages in iCloud\" servers instead. Apple still has them and the keys to decrypt them. https://support.apple.com/guide/security/security-of-icloud-... > When iCloud Backup is turned on, the backup includes a copy of the Messages in iCloud encryption key so Apple can help the user recover their messages even if they have lost access to iCloud Keychain and their trusted devices. reply snowwrestler 13 hours agorootparentJust a bit lower on the same page: > When iCloud Backup is turned on, everything inside it is end-to-end encrypted, including the Messages in iCloud encryption key. Meaning that Apple does not actually have access to that key, because it is encrypted before being saved to their servers. reply modeless 12 hours agorootparentThis is misleading, again. The paragraph you quoted only applies with optional \"Advanced Data Protection\". Advanced Data Protection is off by default. In the default state Apple does have access to the Messages in iCloud keys in iCloud Backup, as I said. reply alphabettsy 9 hours agorootparentYep. Table is here: https://support.apple.com/en-us/102651 reply tsunamifury 18 hours agorootparentprevNothing is secure that goes to a server. Period. Apple turned over iMessage conversations between journalists and senators at Trumps request. They encrypt but give away the keys in many jurisdictions. reply gruez 17 hours agorootparent>Apple turned over iMessage conversations between journalists and senators at Trumps request. source? reply etchalon 18 hours agoparentprevThat's still required. reply gjsman-1000 18 hours agorootparentI don't anticipate this changing either - because, let's face it, losing all of your Photos and Messages is, to most people, a bigger deal than perfect security. I'm experienced with Apple products - but there was one time that I actually got stuck in an E2EE loop and was forced to reset all E2EE data on iCloud. I don't know what I did wrong - but if someone in tech, like myself, can get stuck in an E2EE lockout, I can't imagine other people.* *This was not Advanced Data Protection. This was stuff that was E2EE for all accounts - like passwords and health information. As such there's no recovery contact. reply mcny 18 hours agorootparentI thought the fix was trivial - all I need to do is go to settings > my face >> icloud >>> show all >>>> messages and make sure the toggle is off? doesn't that stop iCloud syncing, at least on my end? I understand I can't control what happens on the other end of the conversation but that is all I need to do on my end, right? reply fh9302 18 hours agorootparentYou either have to enable E2EE or disable both Messages in iCloud and device backups. Otherwise the device backups contain a copy of your messages. reply galad87 18 hours agorootparentThe \"Messages in iCloud\" sync is end to end, so you can enable it and disable iCloud backup, or manually backup on your computer: https://support.apple.com/en-us/102651 reply zchrykng 17 hours agorootparentYeah, it is end to end encrypted, but the keys are part of your device's iCloud backup. So unless you turn on end to end encryption for that backup or disable it, Apple can access the keys required to decrypt the iMessage in iCloud messages. reply etchalon 6 hours agorootparentI believe the reason iMessages aren't protected with iCloud Backup is because they're stored decrypted in the SQLite database iMessage uses, chat.db. reply gjsman-1000 18 hours agorootparentprevCorrect. I was referring to the OP asking if Apple would ever fix E2EE not protecting, by default, Photos and Messages and so forth. reply sneak 16 hours agorootparentpreviMessage’s practical lack of e2ee isn’t a matter of “perfect security”. It’s simply not e2ee because the keys are escrowed to the middle service. It’s not even a little bit secure. The encryption has been fully backdoored by sharing the endpoint keys off of the device. Apple turns over customer data on over 70,000 customers per year without a warrant under FISA/702 (prism) and NSLs. The number gets bigger every year. This isn’t a theoretical threat. The number is even bigger if you include all the search warrants, too. EDIT: Even if you enable their optional e2ee for backups (which nobody does), iMessage the platform is still vulnerable because the conversations you have with others are insecure because the other end of the conversation is escrowing their keys to Apple via non-e2ee backups. If you enable ADP iMessage only becomes secure for the case where you are only iMessaging yourself. It’s simply not private or secure. You can’t be “slightly encrypted” or “mostly private”. reply gjsman-1000 16 hours agorootparentUnless you enable Advanced Data Protection, which escrows the keys solely on your device. This is hardly a secret or a scandal. reply etchalon 7 hours agorootparentprevAs far as I know, iMessage keys are not escrowed to any middle service. What are you basing that belief on? reply upofadown 17 hours agoprevIsn't all this post quantum stuff a little premature? The standards haven't settled. We don't even know if there is a possible quantum threat to cryptography yet. The more we work on the problem the less likely it seems. Last I heard we were 1 or 2 orders of magnitude away from physical noise performance that would make such a threat possible. Edit, added: Harvest now, decrypt later applies to any encrypted data. There is nothing special about the quantum threat. This all only makes sense if we can predict what the actual threat is ... and so far we can't. This reminds me of Pascal's Wager[1] [1] https://en.wikipedia.org/wiki/Pascal%27s_wager reply dijit 17 hours agoparentYou'd be right except that anything encrypted now can be stored and cracked later. I remember as part of the snowden leaks there was documentation about this kind of delayed phase collection. Basically store as much signals data as you can and try to crack it later if there's a weakness discovered with the protocol or computing power starts being capable of wholesale attack. You might remember that hashes are significantly easier to crack with \"rainbow tables\", and so we added cryptographic \"salts\" to online password storage. We discovered that about 15 years ago and started salting all our passwords, but for a large window of time all of those old leaked databases were suddenly extremely easy to crack. Now, Imagine the NSA is 10 years ahead of us (and you might be close with that estimation), so even if they can't crack RSA right now they're much closer than we are, and even we get there we will likely have a large window of time before we fix it properly. (not that we're talking RSA here, but you get my point). https://www.forbes.com/sites/andygreenberg/2013/06/20/leaked... reply upofadown 13 hours agorootparentThat Forbes article is about a loophole involving encryption that could allow the NSA to collect and store data that they might not otherwise be able to. Now that everything is encrypted, that loophole might cover everything. Nothing in the article implies that the NSA can magically collect all the encrypted data in the world and keep it for many years. reply astrange 13 hours agorootparentprevRSA isn't anywhere near 10 years away from being attackable with a classical computer. (More like eons.) It's not a matter of time, it's just a matter of quantum computers existing. reply whizzter 17 hours agoparentprevLike the article says, it's protection from harvest-now-break-later. Apple users and communications are today a state-secret affair as shown by the impact of NSO/Pegasus. So even if Google,IBM,et al _might_ have approached feasibility in the open there is still a significant risk in state-level adversaries having poured enough funding to still be ahead, plus they will benefit from all open research in the hidden with extra funding to take more leaps. So no, it's not premature if there is hidden or open leaps just 10 years in the future. reply tptacek 16 hours agoparentprevWe have reason to believe that conventionally encrypted data isn't threatened within the next 50 years by anything other than quantum computing, which is what's special about the quantum threat. reply bprater 16 hours agoparentprevFrom the article: >> Although quantum computers with this capability don’t exist yet, extremely well-resourced attackers can already prepare for their possible arrival by taking advantage of the steep decrease in modern data storage costs. The premise is simple: such attackers can collect large amounts of today’s encrypted data and file it all away for future reference. Even though they can’t decrypt any of this data today, they can retain it until they acquire a quantum computer that can decrypt it in the future, an attack scenario known as Harvest Now, Decrypt Later. reply raccoonDivider 16 hours agoparentprev> Edit, added: Harvest now, decrypt later applies to any encrypted data. There is nothing special about the quantum threat. This all only makes sense if we can predict what the actual threat is ... and so far we can't. But we know Shor's algorithm, and we've started building prototype quantum computers. Isn't that enough to build something that counters them? Worst case, we deploy new ciphers and realize that the threat was empty 50 years from now. What's the downside? reply Octoth0rpe 17 hours agoparentprevOne reason to move sooner rather than later is to mitigate the threat of previously stored data. There may be a government entity simply storing all imessage traffic in the hopes of one day decrypting it when/if a breakthrough happens. If you transition sooner, you increase the age of the latest data that could be decrypted, thusly hopefully making it safer. reply earthwalker99 7 hours agorootparentthis is capitalism. why would they care unless the decryption happens this fiscal quarter? reply SheinhardtWigCo 16 hours agoparentprev> Last I heard we were 1 or 2 orders of magnitude away from physical noise performance that would make such a threat possible. The entities with the most to gain from such an advancement are not exactly known for publicizing their achievements. reply InsomniacL 18 hours agoprevI wonder if Apple will use this as an excuses to not comply when it comes to providing cross platform messaging in the EU. reply jitl 18 hours agoparentThe EU already decided that under the DMA, iMessage is too small to qualify for the interoperability requirement. Apple is however adding (unencrypted) RCS support to comply with Chinese government law, which requires that all 5G phones support RCS. reply olliej 15 hours agorootparentYou don’t need (unencrypted) in this. RCS is not encrypted. It never has been. Google has a proprietary extension that puts encrypted blobs into RCS messages, but that would be no different from apple shoving iMessage blobs into RCS and calling it RCS. reply madeofpalk 17 hours agoparentprevApple already has cross platform messaging in the EU. SMS! Besides, the EU doesn't care about iMessage. reply upofadown 17 hours agoprev>When iMessage launched in 2011, it was the first widely available messaging app to provide end-to-end encryption by default,... Until very recently, iMessage provided no way to verify that you and your correspondent were not both connected to the server, rather than each other. So guaranteed end-to end encryption wasn't possible. Even now, with a recent version of iOS, they allow the users to blithely exchange messages without any identity verification. The identity numbers used to do this are hidden behind menus. So not really E2EE in any practical sense. reply maqp 12 hours agoparentSame thing with Signal and most other messengers. Can you link to some documentation that shows iMessage even has the identity numbers? reply nicolas_17 11 hours agorootparenthttps://support.apple.com/en-us/HT213465#verify reply maqp 4 hours agorootparentWonderful, thank you so much! Good to see Apple fix their app finally. reply therealmarv 17 hours agoprevWould be great if we could uninstall iMessage completely out of iPhone for security reasons or make it opt in by default. Unfortunately it's not possible and it will be a gateway for security issues and malware in the following years to come. Post quantum cryptography is nice but that's one of the smallest problems with iMessage. reply dijit 17 hours agoparentYou can very easily disable iMessage completely reply hu3 15 hours agorootparentHow? reply tiltowait 15 hours agorootparentSettings -> Messages -> Toggle iMessage off reply upget_tiding 16 hours agoparentprevYou can enable lockdown mode to decrease the attack surface. https://support.apple.com/en-us/105120 reply yosito 17 hours agoparentprevWouldn't SMS be more insecure? reply maqp 17 hours agorootparentFor message security, absolutely. For endpoint security (iMessage as exploit surface), not so much. reply maqp 14 hours agorootparentJust gonna add here. Do not switch from iMessage to SMS no matter what. Only switch to something like Signal if you need the cross-platform support etc. reply barkerja 17 hours agoparentprevI don't intend to dispute your stance here, but I am interested in understanding a bit more about it. Do you mind giving an example and what the alternative(s) are? reply therealmarv 17 hours agorootparentSearch for \"Pegasus, NSO, spyware, imessage\" or https://archive.is/4fl6o One quote from the article: “Your iPhone, and a billion other Apple devices out-of-the-box, automatically run famously insecure software to preview iMessages, whether you trust the sender or not,” said security researcher Bill Marczak, a fellow at Citizen Lab, a research institute based at the University of Toronto’s Munk School of Global Affairs & Public Policy. “Any Computer Security 101 student could spot the flaw here.” reply ummonk 16 hours agorootparentHave any zero-day vulnerabilities been found that work on devices in Lockdown Mode? reply sneak 17 hours agoparentprevYou can entirely disable iMessage (and FaceTime, and Siri, etc) via provisioning profiles on a managed device. Use Apple Configurator 2 to do so. (A very important privacy setting with no corresponding toggle in the UI that can only be set via a configuration profile is the option to not auto sync your list of recently emailed people to iCloud (“Disable recents syncing”). This leaks your email contact history and social graph to Apple if you have iCloud turned on, even if you aren’t using an Apple email account and aren’t using iCloud Contacts. AFAIK there’s no way to disable it other than via configuration profile.) This is what I do. reply therealmarv 17 hours agorootparentinteresting! reply cyclecount 17 hours agoparentpreviMessage can be disabled in Settings > Messages reply Yesway 18 hours agoprevnext [2 more] [flagged] fallat 18 hours agoparenthttps://xkcd.com/538/ reply damnloveless 14 hours agoprevCome help me work on an open source, P2P, PQ-resistant messaging app: https://github.com/kjloveless/ftw-cli reply azinman2 17 hours agoprev [–] Just as a reminder making crack-proof encryption standard everywhere is a trade off. It’s often discussed and presented in forums like this as the only and just choice (and I believe net it is), but in doing so WILL lead to bad outcomes. Terrible crimes, unsolvable murders, large scale terrorism, emboldened enemies attacking a country, more successful coups, etc. It would be nice as a community to acknowledge nothing comes for free and every technology is a double edged sword. I wish more of these double edge swords could be debated by the public it affects, although it’s out of the scope of comprehension and thoughtfulness by most. And yet we all make the choices that will affect generations… reply robjan 16 hours agoparentWidely used unbreakable encryption has been available in chat apps for at least a decade and hasn't led us down that slippery slope yet. PQ3 is just future proofing what we already have. reply kanbara 17 hours agoparentprevpeople commit crimes with: cars, money, public transit, restaurants, food, bathrooms, roads, … why don’t we get rid of all that? we don’t remove all rights to privacy for people in their homes because criminals use homes too. tech should be no different reply JoshTriplett 8 hours agoparentprevhttps://www.eureporter.co/world/human-rights-category/europe... Yes, giving people privacy means giving everyone privacy, whether they're doing good things or bad. Pointing cameras into everyone's window would also prevent some crimes, and we shouldn't do that either. I don't think \"This has tradeoffs but those tradeoffs are absolutely worth it\" is a level of nuance that's possible in the face of the level of scaremongering against E2EE. reply sunnybeetroot 16 hours agoparentprevFor a minute I thought you were only talking about non-state actors performing those terrible things, then remembered history is full of nations doing all those things. reply azinman2 16 hours agorootparentIt’s all of the above. That’s my point. Good, bad, ok if you’re on their side, some never have a side to be on. reply QuizzicalCarbon 17 hours agoparentprevThose “bad outcomes” already existed pre encryption. If anything, it’ll be more of the same. reply olliej 15 hours agoparentprev(1) crimes happen, and have happened forever (2) historically people simply did not create a huge written record (texts etc) detailing their crimes, so there’s no change in available information (3) even before any of this tech police are not good a solving crimes, and generally rely on errors by criminals (4) and finally. Your argument is definitionally the slippery slope and is the reason the 4th and 5th amendments exist in the US. Your argument is trivially extended to literally everything: why shouldn’t all communication be routed through government servers to find evidence of crimes? Why shouldn’t all device locations be available to police at all times? Why shouldn’t you have video and audio recorders in every home (most child abuse, the quintessential horror) is committed by family members in the home. Having actual privacy does not result in crime, and mandating that privacy should be illegal in only a single case is clearly nonsense. Either you have a right to privacy or you don’t. reply gjsman-1000 17 hours agoparentprev [–] Yeah, that’s what the NSA has said forever - but it turns out that all encryption has that human factor. Imagine I’m planning something malicious. If I literally do anything other than talk about it, there’s going to be evidence, and that evidence won’t be encrypted. Plus, crack-proof encryption existed at least back to Roman times - simply because making a secure code was fairly easy and we didn’t have codebreakers. We managed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple has launched PQ3, a new post-quantum cryptographic protocol for iMessage, enhancing security against potential quantum threats.",
      "PQ3 exceeds other messaging apps in security by utilizing innovative public key algorithms and combining post-quantum and Elliptic Curve cryptography for ongoing message protection.",
      "Thorough security evaluations, including machine-checked proofs, confirm that PQ3 is secure for end-to-end encrypted communication, incorporating symmetric keys, Contact Key Verification, ratcheting techniques, and Secure Enclave technology for message signing and device authentication keys."
    ],
    "commentSummary": [
      "Experts are adopting post-quantum cryptographic protocols like CRYSTALS-Kyber in iMessage and Signal to boost security, potentially offering more protection than traditional methods such as RSA.",
      "Signal is acknowledged as a superior cross-platform choice for secure messaging, while the debate scrutinizes the limitations and challenges of messaging apps like Signal, WhatsApp, and Telegram in terms of security.",
      "The discussion underscores the significance of balancing security and usability in tech, advocating for broader encryption tool adoption, and addressing the impact of end-to-end encryption on privacy and crime."
    ],
    "points": 509,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1708523014
  },
  {
    "id": 39457974,
    "title": "John Carmack calls for public disclosure of AI guardrails",
    "originLink": "https://twitter.com/ID_AA_Carmack/status/1760360183945965853",
    "originBody": "The AI behavior guardrails that are set up with prompt engineering and filtering should be public — the creators should proudly stand behind their vision of what is best for society and how they crystallized it into commands and code.I suspect many are actually ashamed.The… https://t.co/V1McOx5JQQ— John Carmack (@ID_AA_Carmack) February 21, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39457974",
    "commentBody": "AI behavior guardrails should be public (twitter.com/id_aa_carmack)375 points by sotasota 15 hours agohidepastfavorite291 comments dekhn 14 hours agoI strongly suspect Google tried really, really hard here to overcome the criticism is got with previous image recognition models saying that black people looked like gorillas. I am not really sure what I would want out of an image generation system, but I think Google's system probably went too far in trying to incorporate diversity in image generation. reply raxxorraxor 15 minutes agoparentThey have now added a strong bias for generating black people now. Some have prompted to generate a picture of a German WW2 soldier, and now there are many pictures of black people floating around in NAZI uniforms. I think their strategy to \"enhance\" outcomes is very misdirected. The most widely used base models to really fine tune models are those that are not censored and I think you have to construct a problem to find one here. Of course AI won't generate a perfect world, but this is something that will probably only get better with time when users are able to adapt models to their liking. reply DebtDeflation 13 hours agoparentprevSurely there is a middle ground. \"Generate a scene of a group of friends enjoying lunch in the park.\" -> Totally expect racial and gender diversity in the output. \"Generate a scene of 17th century kings of Scotland playing golf.\" -> The result should not be a bunch of black men and Asian women dressed up as Scottish kings, it should be a bunch of white guys. reply photonthug 11 hours agorootparent> Surely there is a middle ground. \"Generate a scene of a group of friends enjoying lunch in the park.\" -> Totally expect racial and gender diversity in the output. Do we expect this because diverse groups are realistically most common or because we wish that they were? For example only some 10% of marriages are interracial, but commercials on TV would lead you to believe it’s 30% or higher. The goal for commercials of course is to appeal to a wide audience without alienating anyone, not to reflect real world stats. What’s the goal for an image generator or a search engine? Depends who is using it and for what, so you can’t ever make everyone happy with one system unless you expose lots of control surface toggles. Those toggles could help users “own” output more, but generally companies wouldn’t want to expose them because it could shed light on proprietary backends, or just take away the magic from interacting with the electric oracles. reply jjjjj55555 10 hours agorootparentAlso, these tools are used world-wide and \"diversity\" means different things in different places. Somehow it's always only the US ideal of diversity that gets shipped abroad. reply photonthug 10 hours agorootparentYeah, someone else mentioned Tokyo which is not going to have as much variety among park visitors as NYC. But then again neither will Colorado (or almost anywhere else!) be as diverse. Some genius at corporate is probably scheming about making image generation as location-sensitive as search is, ostensibly to provide utility but really to perpetuate echo chambers and search bubbles. I wish computing in general would move back towards user-controlled rather than guess-what-I-mean and the resulting politicization, but it seems that ship has sailed. reply felipeerias 8 hours agorootparentprevUS companies systematically push US cultural beliefs and expectations. People in the US probably don’t notice it any more, but it’s pretty obvious from those of us on the receiving end of US cultural domination. This fact is an unavoidable consequence of the socioeconomic realities of the world, but it obviously clashes with these companies’ public statements and positions. reply jjjjj55555 8 hours agorootparentYes but it's especially cynical in this case because the belief their pushing is that diversity matters, that biases need to be overcome, and that all people need to be represented. Claiming all of that but then shoving your own biases down the rest of the world's throat while not representing their people in any way is especially cynical in my opinion. It undermines the whole thing. reply photonthug 5 hours agorootparentHypocrisy is an especially destructive kind of betrayal, which is why the crooked cop or the pedo priest are so disappointing. Would be nice if companies would merely exploit us without all the extra insult of telling us they are changing the world / it’s for our own good / it’s what we asked for/, etc reply philwelch 2 hours agorootparentprevThis is the exact same mindset that invented the word “Latinx”. Compress most of an entire hemisphere of ethnic and cultural diversity down to the vague concept of “Latino”, notice that Spanish is a gendered language so the word “Latino” is also gendered, completely forget that you already have the gender neutral English word “Latin”, invent a new word that virtually none of the people to whom it applies actually identifies with, and then shamelessly use it all the time. reply paulddraper 10 hours agorootparentprevAnd most women are friends with mostly women, and most men are friends with mostly men. reply ceejayoz 13 hours agorootparentprevYou can see how this gets challenging, though, right? If you train your model to prioritize real photos (as they're often more accurate representations than artistic ones), you might wind up with Denzel Washington as the archetype; https://en.wikipedia.org/wiki/The_Tragedy_of_Macbeth_(2021_f.... There's a vast gap between human understanding and what LLMs \"understand\". reply transitionnel 12 hours agorootparentIf they actually want it to work as intelligently as possible, they'll begin taking these complaints into consideration and building in a wisdom curating feature where people can contribute. This much is obvious, but they seem to be satisfied with theory over practicality. Anyway I'm just ranting b/c they haven't paid me. How about an off the wall algorithm to estimate how much each scraped input turns out to influence the bigger picture, as a way to work towards satisfying the copyright question. reply sho_hn 12 hours agorootparentAn LLM-style system designed to understand Wikipedia relevance and citation criteria and apply them might be a start. Not that Wikipedia is perfect and controversy-free, but it's certainly a more sophisticated approach than the current system prompts. reply photoGrant 12 hours agorootparentprevThen who in this black box private company is the Oracle of infinite wisdom and truth!? Who are you putting in charge? Can I get a vote? reply gedy 11 hours agorootparentprev> If you train your model to prioritize real photos I thought that was the big bugbear about disinformation and false news, but now we have to censor reality to combat \"bias\" reply Affric 12 hours agorootparentprevI mean now you d to train AI to recognise the bias in the training data. reply ankit219 12 hours agorootparentprevThe focus for alignment is to avoid bad PR specifically the kind of headlines written by major media houses like NYT, WSJ, WaPo. You could imagine the headlines like \"Google's AI produced a non-diverse output on occasions\" when a researcher/journalist is trying too hard to get the model to produce that. The hit on Google is far bigger than say on Midjourney or even Open AI till now (I suspect future models will be more nerfed than what they are now) For the cases you mentioned, initially those were the examples. It gets tricky during red teaming where they internally try out extreme prompts and then align the model for any kind of prompt which has a suspect output. You train the model first, then figure out the issues, and align the model using \"correct\" examples to fix those issues. They either went to extreme levels doing that or did not test it on initial correct prompts post alignment. reply samatman 10 hours agorootparentprevWhy would you expect anything you didn't specify in the output of the first prompt? If there are friends, lunch, and a park: it did what you asked. Piling a bunch of neurotic expectations about it being a Benneton ad on top of that is absurd. When you can trivially add as much content to the description as you want, and get what you ask for, it does not matter what the default happens to be. reply swatcoder 12 hours agorootparentprevThere's no \"middle\" in the field of decompressing a short phrase into a visual scene (or program or book or whatever). There are countless private, implicit assumptions that users take for granted yet expect to see in the output, and vendors currently fear that their brand will be on the hook for the AI making a bad bet about those assumptions. So for your first example, you totally expect racial and gender diversity in the output because you're assuming a realistic, contemporary, cosmopolitan, bourgeoisie setting -- either because you live in one or because you anticipate that the provider will default to one. The food will probably look Western, the friends will probably be young adults that look to have professional or service jobs wearing generic contemporary commercial fashion, the flora in in the park will be broadly northern climate, etc. Most people around the world don't live in an environment anything like that, so nominal accuracy can't be what you're looking for. What you want, but don't say, is a scene that feels familiar to you and matches what you see as the de facto cultural ideal of contemporary Western society. And conveniently, because a lot of the training data is already biased towards that society and the AI vendors know that the people who live in that society will be their most loyal customers and most dangerous critics right now, it's natural for them to put a thumb on the scale (through training, hidden prompts, etc) that gets the model to assume an innocuous Western-media-palatable middle ground -- so it delivers the racially and gender diverse middle class picnic in a generic US city park. But then in your second example, you're implicitly asking for something historically accurate without actually saying that accuracy is what's become important for you in this new prompt. So the same thumb that biased your first prompt towards a globally-rare-but-customer-palatable contemporary, cosmopolitan, Western culture suddenly makes your new prompt produce something surreal and absurd. There's no \"middle\" there because the problem is really in the unstated assumptions that we all carry into how we use these tools. It's more effective for them to make the default output Western-media-palatable and historical or cultural accuracy the exception that needs more explicit prompting. If they're lucky, they may keep grinding on new training techniques and prompts that get more assumptions \"right\" by the people that matter to their success while still being inoffensive, but it's no simple \"surely a middle ground\" problem. reply cabalamat 12 hours agorootparentprev> \"Generate a scene of 17th century kings of Scotland playing golf.\" -> The result should not be a bunch of black men and Asian women dressed up as Scottish kings, it should be a bunch of white guys. It works in bing, at least: https://www.bing.com/images/create/a-picture-of-some-17th-ce... reply Nevermark 11 hours agorootparentI don't know that this sheds light on anything but I was curious... a picture of some 21st century scottish kings playing golf (all white) https://www.bing.com/images/create/a-picture-of-some-21st-ce... a picture of some 22nd century scottish kings playing golf (all white) https://www.bing.com/images/create/a-picture-of-some-22nd-ce... a picture of some 23rd century scottish kings playing golf (all white) https://www.bing.com/images/create/a-picture-of-some-23rd-ce... a picture of some contemporary scottish people playing golf (all white men and women) https://www.bing.com/images/create/a-picture-of-some-contemp... https://www.bing.com/images/create/a-picture-of-some-contemp... a picture of futuristic scottish people playing golf in the future (all white men and women, with the emergence of the first diversity in Scotland in millennia! Male and female post-human golfers. Hummmpph!) https://www.bing.com/images/create/a-picture-of-futuristic-s... https://www.bing.com/images/create/a-picture-of-futuristic-s... Inductive learning is inherently a bias/perspective absorbing algorithm. But tuning in a default bias towards diversity for contemporary, futuristic and time agnostic settings seems like a sensible thing to do. People can explicitly override the sensible defaults as necessary, i.e. for nazi zombie android apocalypses, or the royalty of a future Earth run by Chinese overlords (Chung Kuo), etc. reply int_19h 11 hours agorootparent> People can explicitly override the sensible defaults as necessary They cannot, actually. If you look at some of the examples in the Twitter thread and other threads linked from it, Gemini will mostly straight up refuse requests like e.g. \"chinese male\", and give you a lecture on why you're holding it wrong. reply jjjjj55555 10 hours agorootparentprevDiversity is cool, but who gets to decide what's diverse? reply noworriesnate 7 hours agorootparentGood point. People of European descent have more diversity in hair color, hair texture and eye color than any other race. That’s because a lot of those traits are recessive and are only expressed in isolated gene pools (European peoples are an isolated gene pool in this sense). reply ramblingrain 2 hours agorootparentIsn't this like the exact opposite of the conclusions of the HapMap project? reply mp05 10 hours agorootparentprevI'm really disappointed that nth-century seems to have no effect at all. I'm expecting Kilts in Space. reply callalex 8 hours agorootparentIt’s a perfect illustration of the way these models work. They are fundamentally incapable of original creation and imagination, they can only regurgitate what they have already been fed. reply crooked-v 11 hours agorootparentprevIt feels like it wouldn't even be that hard to incorporate into LLM instructions (aside from using up tokens), by way of a flowchart like \"if no specific historical or societal context is given for the instructions, assume idealized situation X; otherwise, use historical or projected demographic data to do Y, and include a brief explanatory note of demographics if the result would be unexpected for the user\". (That last part for situations with genuine but unexpected diversity; for example, historical cowboys tending much more towards non-white people than pop culture would have one believe.) Of course, now that I've said \"it seems obvious\" I'm wondering what unexpected technical hurdles there are here that I haven't thought of. reply sho_hn 12 hours agorootparentprevQuite reminded of this episode: https://www.eurogamer.net/kingdom-come-deliverance-review (black representation in a video game about 15th century Bohemia; it was quite the controversy) reply mp05 12 hours agorootparentprev> \"Generate a scene of a group of friends enjoying lunch in the park.\" -> Totally expect racial and gender diversity in the output. I'd err on the side of \"not unexpected\". A group of friends in a park in Tokyo is probably not very diverse, but it's not outside of the realm of possibility. Only white men were golfing Scottish kings if we're talking strictly about reality and reflecting it properly. reply trhway 12 hours agorootparentprev>\"Generate a scene of 17th century kings of Scotland playing golf.\" -> The result should not be a bunch of black men and Asian women dressed up as Scottish kings, it should be a bunch of white guys. is black man in the role of the Scottish king represents a bigger error than some other errors in such an image, like say incorrect dress details or the landscape having say a wrong hill? I'd venture a guess that only our racially charged mentality of today considers that a big error, and may be in a generation or 2 an incorrect landscape or dress detail would be considered much larger error than a mismatched race. reply AlecSchueler 13 hours agorootparentprevAs soon as you have then playing an anachronistic sport you should expect other anachronistic imagery to creep in, to be fair. reply ceejayoz 12 hours agorootparenthttps://en.wikipedia.org/wiki/Golf > The modern game of golf originated in 15th century Scotland. reply AlecSchueler 12 hours agorootparentOh fair enough then. reply mp05 12 hours agorootparentprev> anachronistic sport Scottish kings absolutely played golf. reply int_19h 11 hours agoparentprevJudging by the way it words some of the responses to those queries, they \"fixed\" it by forcibly injecting something like \"diverse image showcasing a variety of ethnicities and genders\" in all prompts that are classified as \"people\". reply redox99 13 hours agoparentprevI remember checking like a year ago and they still had the word \"gorilla\" blacklisted (i.e. it never returns anything even if you have gorilla images). reply _heimdall 13 hours agorootparentGotta love such a high quality fix. When your upper high tech, state of the art algorithm learns racist patterns just blocklist the word and move on. Don't worry about why it learned such patterns in the first place. reply nitwit005 12 hours agorootparentHumans do look like gorillas. We're related. It's natural that an imperfect program that deals with images will will mistake the two. Humans, unfortunately, are offended if you imply they look like gorillas. What's a good fix? Human sensitivity is arbitrary, so the fix is going to tend to be arbitrary too. reply _heimdall 12 hours agorootparentA good fix would, in my opinion, understanding how the algorithm is actually categorizing and why it miss-recognized gorillas and humans. If the algorithm doesn't work well they have problems to solve. reply DeusExMachina 51 minutes agorootparentBut this is not an algorithm. It's a trained neural network which is practically a black box. The best they can do is train it on different data sets, but that's impractical. reply CydeWeys 8 hours agorootparentprevIt's too costly to potentially make that mistake again. So the solution guarantees it will never happen again. reply Spivak 12 hours agorootparentprevYou do understand that this has nothing to humans in general right? This isn't AI recognizing some evolutionary pattern and drawing comparisons to humans and primates -- it's racist content that specifically targets black people that is present in the training data. reply _heimdall 11 hours agorootparentI don't know nearly enough about the inner workings of their algorithm to make that assumption. The internet is surely full of racist photos that could teach the algorithm. The algorithm could also have bugs that miss-categorize the data. The real problem is that those building and managing the algorithm don't fully know how it works or, more importantly, what it had learned. If they did the algorithm would be fixed without a term blocklist. reply nitwit005 11 hours agorootparentprevNope. This is due to a past controversy about image search: https://www.nytimes.com/2023/05/22/technology/ai-photo-label... reply callalex 8 hours agorootparentprevWhere can I learn about this? reply AlecSchueler 12 hours agorootparentprevnext [4 more] [flagged] _heimdall 11 hours agorootparentDo we have enough info for to say that decisively? Ideally we would see the training data, though its probably reasonable to assume a random collection of internet content includes racist imagery. My understanding, though, is that the algorithm and the model of data learned is still a black box that people can't parse and understand. How would we know for sure racist output is due to the racist input, rather than a side effect of some part of the training or querying algorithms? reply alpaca128 12 hours agorootparentprevIt's not unavoidable, but it would cost more to produce high quality training data. reply AlecSchueler 12 hours agorootparentYes, somewhat unavoidable. reply michaelt 13 hours agoparentprevAs well as that, I suspect the major AI companies are fearful of generating images of real people - presumably not wanting to be involved with people generating fake images of \"Donald Trump rescuing wildfire victims\" or \"Donald Trump fighting cops\". Their efforts to add diversity would have been a lot more subtle if, when you asked for images of \"British Politician\" the images were recognisably Rishi Sunak, Liz Truss, Kwasi Kwarteng, Boris Johnson, Theresa May, and Tony Blair. That would provide diversity while also being firmly grounded in reality. The current attempts at being diverse and simultaneously trying not to resemble any real person seems to produce some wild results. reply sho_hn 13 hours agorootparentMy takeaway from all of this is that alignment tech is currently quite primitive and relies on very heavy-handed band-aids. reply _heimdall 12 hours agorootparentprevWe're honestly just seeing generative algorithms failing at diversity initiatives as badly as humans for. Forcing diversity into a system is an extremely tough, if not impossible, challenge. Initiatives have to be driven my goals and metrics, meaning we have to boil diversity down to a specific list of quantitative metrics. Things will always be missed when our best tool to tackle a moral or noble goal is to boil a complex spectrum of qualitative data to a subset of measurable numbers. reply photoGrant 13 hours agoparentprevRemind yourself we're discussing censorship, misinformation, inability to define or source truth and we're concerned on Day 1 about the results of image gen being controlled by a for profit single entity with incentives that focus solely on business and not humanity... Where do we go from here? Things will magically get better on their own? Businesses will align with humanity and morals, not their investors? This is the tip of the iceberg of concerns and it's ignored as a bug in the code not a problem with trusting private companies with defining truth. reply CuriouslyC 12 hours agorootparentThe ridiculous degree of PC alignment of corporate models is the thing that's going to let open source win. Few people use bing/dall-e, but if OpenAI had made dall-e more available and hadn't put ridiculous guardrails on it, stable diffusion would be a footnote at this point. Instead, dall-e is a joke and people who make art use stable diffusion, with casuals who just want some pretty looking pictures using midjourney. reply josephg 12 hours agorootparentDon’t count out Adobe Firefly. I wouldn’t be surprised if it’s used more than all the other image gen models combined. reply CuriouslyC 12 hours agorootparentThat might be true, but if you're using firefly as a juiced up content-aware fill in photoshop I'm not sure it's apples to apples. reply photoGrant 12 hours agorootparentprevNo, ignoring laws and stealing data to increase your Castle's MOAT is the win. Compute isn't an open source solvable problem. I can't DirtyPCB's an A100 Making the argument open source is the answer is an agenda of making your competition spin wheels. reply CuriouslyC 12 hours agorootparentYou're on a thread about how people are lambasting big money AI for being garbage, and producing inferior results to OSS tools you can run on consumer GPUS, tell me again how unbeatable google/other big tech players are. reply photoGrant 12 hours agorootparentI've been part of the advertising and marketing world for a lot of these companies for a decade plus, I've helped them sell bullshit. I've also been at the start of the AI journey, I've downloaded and checked local models of all promises and variances. To say they're better than the compute that OpenAI or Google are throwing at the problem is just plain wrong. I left the ad industry the moment I realised my skills and talents are better used informing people than lying to them. This thread is not at all comparing the ethical issues of AI with local anything. You're conflating your solution with another problem. reply CuriouslyC 12 hours ago [flagged]rootparentnext [2 more] Is a $5 can opener better than a $2000 telescope at opening cans? Yes. Is stable diffusion better at producing finished art, by virtue of not being closed off and DEI'd to oblivion so that it can actually be incorporated into workflows? Emphatically yes. It doesn't matter how fancy your engineering is and how much money you have if you're too stupid to build the right product. As for this being written nonsense, that's the sort of thing someone who couldn't find an easy way to win an argument and was bitter about the fact would say. reply photoGrant 12 hours agorootparentnext [2 more] [flagged] u32480932048 10 hours agorootparentI understood it perfectly. reply chasd00 13 hours agorootparentprev> Where do we go from here? opensource models and training sets. So basically the \"secret sauce\" minus the hardware. I don't see it happening voluntarily. reply didntcheck 12 hours agorootparentI see it as not unlikely that there'll be a campaign to sigmatize, if not outright ban, open source models on the grounds of \"safety\". I'm quite surprised at how relatively unimpeded the distribution of image generation models has been, so far reply AlanYx 0 minutes agorootparentWhat you're predicting has already started. Two weeks ago Geoffrey Hinton gave a speech advocating banning open source AI models (see e.g.: https://thelogic.co/news/ai-cant-be-slowed-down-hinton-says-... ). I'm surprised there wasn't an HN thread about it at the time. int_19h 11 hours agorootparentprevThis is already happening, actually, although the focus so far has been on the possibility of their use for CSAM: https://www.theguardian.com/society/2023/sep/12/paedophiles-... reply photoGrant 13 hours agorootparentprevAbsolutely it won't. We've armed the issue with a supersonic jet engine and we're assuming if we build a slingshot out of pop sticks we'll somehow catch up and knock it off course. reply chasd00 13 hours agorootparenti can't predict the future but there is precedent. Models, weights, and dataasets are the keys to the kingdom like operating system kernels, databases, and libraries use to be. At some point, enough people decided to re-invent and release these things or functionality to all that it became a self-sustaining community and, eventually, transformative to daily life. On the other hand, there may be enough people in power who came from and understand that community to make sure it never happens again. reply photoGrant 13 hours agorootparentNo, compute is keys to the kingdom. The rest are assets and ammunition. You out-compute your enemy, you out-compute your competition. That's the race. The data is part of the problem, not the root. These companies are silo'ing the worlds resources. GPU, Finance, Information. Those combined are the weapon. You make your competition starve in the dust. These companies are pure evil pushing an agenda of pure evil. OpenAI is closed. Google is Google. We're like, ok, there you go! Take it all. No accountability, no transparency, we trust you. reply CuriouslyC 12 hours agorootparentCompute lets you fuck up a lot when trying to build a model, but you need data to do anything worth fucking up in the first place, and if you have 20% of the compute but you fuck up 1/5th as much you're doing fine. Meta/OpenAI/Google can fuck up a lot because of all their compute, but ultimately we learn from that as the scientists doing the research at those companies would instantly bail if they couldn't publish papers on their techniques to show how clever they are. reply photoGrant 12 hours agorootparentI never said each of these exist in a vacuum. It is the collation of all that is the danger. This isn't democratic. This is companies now toying with governmental ideologies. reply ianbicking 12 hours agoprevI've never been involved with implementing large-scale moderation or content controls, but it seems pretty standard that underlying automated rules aren't generally public, and I've always assumed this is because there's a kind of necessary \"security through obscurity\" aspect to them. E.g., publish a word blocklist and people can easily find how to express problematic things using words that aren't on the list. Things like shadowbans exist for the same purpose; if you make it clear where the limits are then people will quickly get around them. I know this is frustrating, we just literally don't seem to have better approaches at this time. But if someone can point to open approaches that work at scale, that would be a great start... reply serial_dev 12 hours agoparentThere is no need to implement large scale censorship and moderation in this case. Where is the security concern? That I can generate images of white people in various situations for my five minutes of entertainment? The whole premise of your argument doesn't make sense. I'm talking to a computer, nobody gets hurt. It's like censoring what I write in my notes app vs. what I write on someone's Facebook wall. In one case, I expect no moderation, whereas in the other case, I get that there needs to be some checks. reply mhuffman 11 hours agorootparentWhen these companies say there are \"security concerns\" they mean for them, not you! And they mean the security of their profits. So anything that can cause them legal liability or cause them brand degradation is a \"security concern\". reply wruza 11 hours agorootparentIt's definitely this at this stage. But by not having any discourse we'll end up normalizing it even before establishing consensus on what's appropriate to expect from human/AI interaction and how much of a problem is the actual model, opposed to a user. Not being able to generate innocent content is ridiculous. Probably they're overshooting and learning how to draw the stricter lines right now, but if you don't argue, you'll allow to boil this frog into a Google \"Search\" again. reply g42gregory 11 hours agorootparentprevWhat if you are engaged in a wrongthink? How would you suggest this to be controlled instead? reply u32480932048 10 hours agorootparentStraight to Guantanamo. reply Ecoste 11 hours agorootparentprevBut what if little timmy asks it how to make a bomb? What if it's racist? Then what? reply pizzafeelsright 6 hours agorootparentIt's fine. We have laws against blowing people up. Racism is fine as well. I won't date out of my race and if you think there should be a law that I must that's not really freedom. As for hiring or not based upon race there's already a law against that. The cure is often worse than navigating uneasy waters. Every time you pass a law you give a gun to a bureaucrat. reply withinboredom 10 hours agorootparentprevNone of this should be a mystery. Making a bomb is literally something you can figure out with very little research (my friends and I used to blow up cow pastures for fun!). Racism is a totally different and sadder issue. I don’t have a good answer for that one, but knowledge shouldn’t be withheld because someone thinks it is “dangerous” reply gopher_space 10 hours agorootparentprev> Where is the security concern? That I can generate images of white people in various situations for my five minutes of entertainment? I'd love an example of \"guardrails\" in action on a topic of relevance to actual adults. There's a connection I can't find between the ability to make racist memes and literally anything else I want to do with AI. reply joquarky 11 hours agorootparentprevOur perception of the world has become so abstract that most people can't discern metaphors from the material world anymore The map is not the territory Sociologists, anthropologists, philosophers and the like might find a lot of answers by looking into the details of what is included into genAI alignment and trace back the history of why we need each particular alignment reply AnarchismIsCool 11 hours agorootparentprevIt's part of the marketing. By saying their models are powerful enough to be gasp Dangerous, they are trying to get people to believe they're insanely capable. In reality, every model so far has been either a toy, a way of injecting tons of bugs into your code (or circumventing GPL by writing bugs), or a way of justifying laying off the writing staff you already wanted to shit can. They have a ton of potential and we'll get there soon, but this isn't it. reply electrondood 10 hours agorootparentprevThe concern is that companies have long known that it's bad for business if your product is a toxic cesspit or can be used to generate horrible headlines that reflect poorly on your brand. It's not \"woke,\" and it's not censorship. It's literally the free market. reply anomaly_ 8 hours agorootparentprevWhen someone unironically uses the term \"problematic\" you can reasonably assume the position they are going to argue from... reply advael 10 hours agoparentprevThis is simply a bad approach and a bad argument. Security through obscurity is a term whose only usage in security circles is derogatory. People figure out how to get around these auto-censors just fine, and not publishing them creates more problems for legitimate users and more plausible deniability for bad policy hidden in them. Doing the same thing but with public policy would already be better, albeit still bad. The only real solution to the problem of there being an enormous public square controlled by private corporations is to end this situation reply kansface 10 hours agorootparentIs a content moderation policy the same thing as \"security\"? Do we get to apply the best practices of the one to the other because they overlap to a smaller or larger degree? reply advael 10 hours agorootparentThe use of \"security through obscurity\" invited the comparison. It is a better comparison when using automated tools instead of human decision-makers. That said, even if we are talking about policy rather than security, policies that are unknown by and hidden from the people they bind is probably the most recognizable and one of the more onerous features of despotism. We have the term \"Kafkaesque\" because a whole famous writer literally spent his career pointing out how they don't work and harm the people they affect reply cryptonector 10 hours agorootparentprevYes, since content moderation is a form of authorization policy. reply batch12 9 hours agorootparentprevSecurity through obscurity can have a place as part of a larger defense in depth strategy. Alone it's a joke. Source: in security circles reply advael 8 hours agorootparentEven granting that there may be some nuance to whether and to what degree secrecy is valuable in some contexts, the policies used for automated content moderation on large platforms and the policies by which AI systems are aligned are not good candidates for this secrecy having even a beneficial effect, let alone being necessary reply chillfox 9 hours agorootparentprevAnd yet, every single security system on the net relies on an element of obscurity to work. Passwords are secret (obscure), as are private keys for SSL/TLS. reply advael 9 hours agorootparentThis misunderstands what is meant by the concept. The mechanisms and the public policies that dictate how they are used are not obscured. To borrow and improve another commenter's analogy about locks and keys, the lock on your door is more secure because every locksmith in the world knows how it works, which doesn't mean they have your key reply lamontcg 10 hours agorootparentprevI mean people can get through your front door using any number of attacks that either exploit weaknesses in the locking mechanism, or circumvent your locks via a carefully placed brick through a window or something like that. Even if you have alarms and other measures, that doesn't stop someone from doing a quick smash and grab, etc. All of these security flaws doesn't mean that you should leave your door open and unlocked all the time. Imperfect security isn't useless security. And the purpose of things like bad-word filters is to make a best effort at blocking stuff which violates the platform TOC and makes plausible deniability much less likely when someone is deliberately circumventing the filters. The existence of false positives and false negatives is considered acceptable in an imperfect world. The filters themselves also only block the action to change a username or whatever and don't punish the user or deny use of the platform entirely (they're much less punitive than the AI abuse algorithms that auto-ban people off of Google/GitHub/etc). reply advael 9 hours agorootparentYour analogy is also bad. I agree that perfect security is impossible and that is completely irrelevant here. What platforms like this do by not publishing their policies is more akin to insisting you use their special locks on your door that they claim protect you better because no one knows how they work. Maybe they're operated by an AI working with a Ring camera or something? Very fancy stuff. With this kind of tech, you may be locked out of your home for reasons you don't understand. An independent locksmith might have a hard time figuring out what's going wrong with the door if it fails. You have no idea if some burglars are authorized to enter your home trivially by a deal with the company. If the company decides you are in the wrong in any context, they have the unilateral power to deny you access with no clear recourse. They may get you arrested for trying to get into your own home reply lamontcg 4 hours agorootparent> They may get you arrested for trying to get into your own home The AI algorithms that ban people from platforms like Google and GitHub do that, which I explicitly called out as needing more oversight. That is different from algorithms which just prevent you from doing something on a platform like using n-bombs in your username, or the LLM guardrails that just give you mangled answers or tell you that they can't do that. That isn't analogous to getting arrested. And in these cases the analogy really falls apart because it isn't your home, and it isn't critical for your life. reply advael 3 hours agorootparentFriend, it was your analogy reply wruza 11 hours agoparentprevYes, but the implied problems may not need be approached at all. It's a uniform ideology push, with which people agree differently at different levels. If companies don't want to reveal the full set of measures, they could at least summarize them. I believe even these summaries would be what subj tweet refers to as \"ashamed\". We cannot discuss or be aware of the problems-and-approaches, unless they are explicitly stated. Your analogy with content moderation is a little off, because it's not a set of measures that is hidden, but the \"forum rules\" themselves. One thing is AI refusing with an explanation. That makes it partially useless, but it's their right to do so. Another thing if it silently avoids or directs topics due to these restrictions. Pretty sure authors are unable to clearly separate the two cases, and also maintain the same quality as the raw model. At the end of the day people will eventually give up and use Chinese AI instead, cause who cares if it refuses to draw CCP people while doing everything else better. reply nonrandomstring 11 hours agoparentprev> publish a word blocklist and people can easily find how to express problematic things using words that aren't on the list. I'd love to explore that further. It's not the words that are \"problematic\" but the ideas, however expressed? Seems like a \"problematic\" idea, no ? reply UberFly 10 hours agorootparentA word blocklist just serves to apply guardrails. It just slows down common abuse. Very far from perfect, but the alternatives are anything goes or total lockdown. Perfect solutions are pretty damn rare. reply nonrandomstring 9 hours agorootparentYou mention \"solutions\" Perhaps you mistake me for someone who cares about suggesting solutions for the prolems suffered by giant technopolies, as if they were my problems. reply lp0_on_fire 5 hours agorootparentThis is basically my view. These tech companies have produced some pretty amazing stuff. Solved problems and built things at scale that were unthinkable a decade ago. They can solve this problem. They choose not to because a) they're already shielded from legal liability for certain things that happen/are said on their platforms and b) it doesn't make them any money. reply cryptonector 10 hours agoparentprevRepeat after me: security by obscurity is weak. Clearly people can work out what some of the rules are, so why not just publish them. If you need to alter them when people figure out how to get around them, well, you already had to anyways. reply u32480932048 10 hours agoparentprevMost legal systems operate at the nation-state scale and aren't made of hidden mystery laws. There are lots of reasons for that. We've already had this argument with cryptocurrency, where we've basically decided that the existing legal system (although external) provides a sufficient toolset to go after bad actors. Finally, based on the illiberal nature of most AI Safety Sycophants' internet writings, I don't like who they are as people and I don't trust them to implement this. reply observationist 10 hours agoparentprevIf you can't afford to pay a sufficient number of people to moderate a group, you need to reduce the size of the group or increase the number of moderators. Your speculation implies no responsibility for taking on more than can be handled responsibly, and externalizes the consequences to society at large. There are responsible ways to have very clear, bright, easily understood, well communicated rules and sufficient staff to manage a community. I don't know why it's simply accepted that giant social networks get to play these games when it's calculated, cold economics driving the bad decisions. They make enough money to afford responsible moderation. They just don't have to spend that money, and they beg off responsibility for user misbehavior and automated abuses, wring their hands, and claim \"we do the best we can!\" If they honestly can't use their billions of adtech revenue to responsibly moderate communities, then maybe they shouldn't exist. Maybe we need to legislate something to the effect of \"get as big as you want, as long as you can do it responsibly, and here are the guidelines for responsible community management...\" Absent such legislation, there's no possible change until AI is able to reasonably do the moderation work of a human. Which may be sooner than any efforts at legislation, at this rate. reply fallingknife 8 hours agorootparentWhat guideline for community management would possibly not be a flagrant 1A violation? reply opportune 9 hours agoparentprevI think this is a fair approach when things work well enough that a typical user doesn’t need to worry about whether they’ll trigger some kind of special content/moderation logic. If you shadowban spammers and real users almost never get flagged as spammers, the benefits of being tight-lipped outweigh those of the very few users who get improperly flagged or are just curious. With some of these models the guardrails are so clumsy and forced that I think almost any typical user will notice them. Because they include outright work-refusal it’s a very frustrating UX to have to “discover” the policy for yourself through trial and error. And because they’re more about brand management than preventing fraud/bad UX for other users, the failure modes are “someone deliberately engineered a way to get objectionable content generated in spite of our policies.” Obviously some kinds of content are objectionable enough for this to be worth it still, but those are mostly in the porn area - if somebody figures out a way to generate an image that’s just not PC, despite all the safety features, shouldn’t that be on them rather than the provider? Even tuning the model for political correctness is not the end of the world in my opinion, a lot of LLMs do a perfectly reasonable job for my regular use cases. With image generators they are going so far as to obviously (there’s no other way that makes sense) insert diversity sub prompts for some fraction of images which is simply confusing and amateur. Everybody who uses these products just a little bit will notice it. It’s also so cautious that even mild stuff (I tried to do the “now make it even more X” with “American” and it stopped at one iteration) gets caught in the filters. You’re going to find out the policies anyway because they’re so broad an likely to be encountered while using the product innocently - anything a real non-malicious user is likely to get blocked by should be documented. reply verticalscaler 11 hours agoparentprevMy dear fellow, some believe the ends justify the means and play games. Read history, have some decency. The danger of being captured by such people far outweighs any other \"problematic things\". First and foremost any system must defend against that. You love guardrails so much - put them on the self annointed guard railers. Otherwise, if You Want a Picture of the Future, Imagine a Boot Stamping on a Human Face – for Ever. reply cryptonector 10 hours agorootparentAnd to add insult to injury the jackbooted thug is an AI bot. reply EricE 12 hours agoparentprevYeah, there's absolutely no need for transparency /s https://youtu.be/THZM4D1Lndg?si=0QQuLlH7JebSa6w3&t=485 If it doesn't start 8 minutes in, go to the 8 minute mark. Then again I can see why some wouldn't want transparency. reply verisimi 12 hours agoparentprevnext [4 more] [flagged] CrazyStat 12 hours agorootparentThis feels more like a personal attack than a response to the argument made. reply SturgeonsLaw 11 hours agorootparentIt's does, but as someone who is staunchly anti-censorship, I understand the frustration. There are sharks out there who want to control speech for their own ends - governments seeking to control populations, corporations wanting docile consumers, hostile nations wishing to stir dissent, individuals trying to cover up their misdeeds, and enabling censorship helps those hostile parties achieve their ends. In this worldview, regular people who say a variation of \"censorship is good, actually\" are perhaps seen as useful idiots. A better approach would be building up the critical thinking skills of the population so they can better process information, however that transfers a measure of power to the people and is a multigenerational investment, and removes a justification for censorship, which is politically unappealing. reply nonrandomstring 11 hours agorootparent> better approach would be building up the critical thinking skills of the population The term for that is \"Intellectual self-defence\" > transfers a measure of power to the people and is a multigenerational investment The challenge to that literacy comes not from power riding on censorship, but from the people themselves who are now conditioned into an apathetic need for \"convenience\". Critical thinking is hard work, and is never rewarded except in the long run. Also, anti-censorship absolutism must be tempered with what we call \"information hazards\". Some of these are genuine, although admittedly very rare, such as easy instructions to create nuclear weapons or synthesise a deadly virus. There are just too many idiots in the world not to want to put a brake on that stuff. reply stainablesteel 13 hours agoprevgemini seems to have problems generating white people and honestly this just opens the door for things that are even more racist [1], the harder you try the more you'll fail, just get over the DEI nonsense already 1. https://twitter.com/wagieeacc/status/1760371304425762940 reply AnarchismIsCool 12 hours agoparentI don't think the DEI stuff is nonsense, but SV is sensitive to this because most of their previous generation of models were horrifyingly racist if not teenage nazis, and so they turned the anti-racism knob up to 11 which made the models....racist but in a different way. Like depicting colonial settlers as native americans is extremely problematic in its own special way, but I also don't expect a statistical solver to grasp that context meaningfully. reply grotorea 11 hours agorootparentSo you're saying in a way this is /pol/ and Tay's fault? reply AnarchismIsCool 11 hours agorootparentLooks around at everything ...is there anything that isn't 4chan's fault at this point? Realistically, kinda. There have always been tons of anecdotes of video conference systems not following black people, cameras not white balancing correctly on darker faces etc. That era of SV was plagued by systems that were built by a bunch of young white guys who never tested them with anyone else. I'm not saying they were inherently racist or anything, just that the broader society really lambasted them for it and so they attempted to correct. Really, the pendulum will continue to swing and we'll see it eventually center up on something approaching sanity but the hyper-authoritarian sentiment that SV seems to have (we're geniuses and the public is stupid, we need to correct them) is...a troubling direction. reply callalex 8 hours agorootparentI’m always hesitant to jump straight to ringing the racism bell when it comes to problems that arise from fundamental physics. I have dark skin, and I constantly struggle with automatic faucets, soap dispensers, and towel dispensers in public bathrooms. None of those are made by big tech. reply xdennis 10 hours agoparentprevIt's not just Gemini, it's Google. And old example is to just search \"white people\" on Google Images. Almost all the results are black people. https://www.google.com/search?q=white+people&tbm=isch&hl=ro reply Jason_Protell 13 hours agoparentprevIs there any evidence that this is a consequence of DEI rather than a deeper technical issue? reply sotasota 12 hours agorootparenthttps://pbs.twimg.com/media/GG1eyKjXQAA1FxU?format=jpg&name=... https://cdn.sanity.io/images/cjtc1tnd/production/912b6b5aacc... https://pbs.twimg.com/media/GG1ThfsWUAAp-SO?format=jpg&name=... https://cdn.sanity.io/images/cjtc1tnd/production/e2810c02ff6... https://pbs.twimg.com/media/GG1MnepXwAAkPL6?format=jpg&name=... https://pbs.twimg.com/media/GG0BLVsbMAARZXr?format=jpg&name=... reply flumpcakes 11 hours agorootparentI don't understand how people could even argue that this is in any way acceptable. Fighting \"bias\" has become some boogyman and anything \"non-white\" is now beyond reproach. Shocking. reply AnarchismIsCool 10 hours agorootparentFighting bias is a good thing, you'd have to be pretty...er...biased to believe otherwise. Bias is fundamentally a distortion or deviation from objective reality. This, on the other hand, is just fucking stupid political showboating that's hurting their SV white knight cause. It's just differently flavored bias reply gedy 11 hours agorootparentprevSeriously, I've basically written off using Gemini for good after this HR style nonsense. It's a shame that Google, who invented much of this tech, is so crippled by their own people's politics. reply gs17 11 hours agorootparentprev\"I can't generate white British royalty because they exist, but I can make up black ones\" is pretty close to an actually valid reason. reply Jensson 13 hours agorootparentprevYou get 4 images per time and are lucky to get one white person when asked for it, no other model has that issue. Other models has no problems generating black people either, so it isn't that other models only generates white people. So either it isn't a technical issue or Google failed to solve a problem everyone else easily solved. The chances of this having nothing to do with DEI is basically 0. reply ceejayoz 13 hours agorootparentDepending on how broadly you define it, something like 10-30% of the world's population is white. Africa is about 20% of the world population; Asia is 60% of it. One in four sounds about right? reply cm2012 13 hours agorootparentIt does the same if you ask for pictures of past popes, 1945 German soldiers, etc. reply ceejayoz 13 hours agorootparentIt'll also add extra fingers to human hands. Presumably that's not because of DEI guardrails about polydactyly, right? The current state of the art in AI gets things wrong regularly. reply cm2012 13 hours agorootparentSure, but this one is from Google adding a tag to make every image of people diverse, not AI randomness. reply ceejayoz 13 hours agorootparentAm I missing something in the link demonstrating that, or is it conjecture? reply int_19h 11 hours agorootparentIf you look closely at the response text that accompanies many of these images, you'll find recurring wording like \"Here's a diverse image of ... showcasing a variety of ethnicities and genders\". The fact that it uses the same wording strongly implies that this is coming out of the prompt used for generation. My bet is that they have a simple classifier for prompts trained to detect whether it requests depiction of a human, and appends \"diverse image showcasing a variety of ethnicities and genders\" to the prompt the user provided if so. This would totally explain all the images seen so far, as well as the fact that other models don't have this kind of bias. reply strangeattractr 9 hours agorootparentprevHave you bothered to look at all? Read the output of the model when asked about why it has the behaviour it does. Look at the plethora of images it generates that are not just historically inaccurate but absurdly so. It tells you \"heres a diverse X\" when you ask for X. Yet asking for pictures of Koreans generates only Asian people but prompts for Scots or French people in historical periods generate mostly non-white people. You're being purposefully obtuse, Google has had racism complaints about previous models, talks often about AI safety and avoiding 'bias'. You're trying to argue that it's more likely that the training data had an inherent bias against generating white people in images purely by chance? reply cm2012 12 hours agorootparentprevIt's been demonstrated on Twitter a few times, can't find a link handy reply xdennis 10 hours agorootparentprevOpenAI has no problem showing accurate pictures. You know it's Google-induced bias, but feign ignorance. If you ask for a picture of nazi soldiers it shouldn't have 60% Asian people like you say. You know you're wrong but instead of admitting it, you're moving the goalpost to \"hands\". This entire thread is you being insincere. reply perlclutcher 12 hours agorootparentprevhttps://twitter.com/altryne/status/1760358916624719938 Here's some corporate-lawyer-speak straight from Google: > We are aware that Gemini is offering inaccuracies... > As part of our AI principles, we design our image generation capabilities to reflect our global user base, and we take representation and bias seriously. reply ceejayoz 12 hours agorootparentThat doesn't back up the assertion; it's easily read as \"we make sure our training sets reflect the 85% of the world that doesn't live in Europe and North America\". Again, 1/4 white people is statistically what you'd expect. reply withinboredom 9 hours agorootparentFuck, this is going to sound fucked up... but just because you have a 1/4 chance of getting a random white person from the globe, they generally tend to clump together. For example, you generally find a shitload of Asian people in Asia, white people in Europe, and African people in Africa, and Indian people in India. Probably the only chance where you wouldn't expect this are in heavily colonized places like South Africa, Australia, and the Americas. reply u32480932048 10 hours agorootparentprevSure, but I see three 200 responses and a 400 - not 1/4 white people as statistically expected. reply lmm 11 hours agorootparentprevThis specific thing is a much more blatant class of error, and one that has been known to occur in several previous models because of DEI systems (e.g. in cases where prompts have been leaked), and has never been known to occur for any other reason. Yes, it's conceivable that Google's newer, beter-than-ever-before AI system somehow has a fundamental technical problem that coincidentally just happens to cause the same kind of bad output as previous hamfisted DEI systems, but come on, you don't really believe that. (Or if you do, how much do you want to bet? I would absolutely stake a significant proportion of my net worth - say, $20k - on this) reply ceejayoz 11 hours agorootparent> has never been known to occur for any other reason Of course it has. Again, these things regularly give humans extra fingers and arms. They don't even know what humans fundamentally look like. On the flip side, humans are shitty at recognizing bias. This comment thread stems from someone complaining the AI only rarely generated white people, but that's statistically accurate. It feels biased to someone in a majority-white nation with majority-white friends and coworkers, but it fundamentally isn't. I don't doubt that there are some attempts to get LLMs to go outside the \"white westerner\" bubble in training sets and prompts. I suspect the extent of it is also deeply exaggerated by those who like to throw around woke-this and woke-that as derogatories. reply lmm 11 hours agorootparent> Of course it has. Again, these things regularly give humans extra fingers and arms. They don't even know what humans fundamentally look like. > This comment thread stems from someone complaining the AI only rarely generated white people, but that's statistically accurate. It feels biased to someone in a majority-white nation with majority-white friends and coworkers, but it fundamentally isn't. So the AI is simultaneously too dumb to figure out what humans look like, but also so super smart that it uses precisely accurate racial proportions when generating people (not because it's been specifically adjusted to, but naturally)? Bullshit. > I don't doubt that there are some attempts to get LLMs to go outside the \"white westerner\" bubble in training sets and prompts. I suspect the extent of it is also deeply exaggerated by those who like to throw around woke-this and woke-that as derogatories. You're dodging the question. Do you actually believe the reason that the last example in the article looks very much not like a man is a deep technical issue, or a DEI initiative? If the former, how much are you willing to bet? If the latter, why are you throwing out these insincere arguments? reply samatman 9 hours agorootparentprevA very impressive display of crimestop you've got going in this thread. How did you end up like this? reply pb7 9 hours agorootparentprevCongratulations, here is your gold medal in mental gymnastics. Enough now. It literally refuses to generate images of white people when prompted directly while not only happily obliging but only producing that specific race in all 4 results for all others. It’s discriminatory and based on your inability to see that, you may be too. reply thepasswordis 11 hours agorootparentprevThe AI will literally scold you for asking it to make white characters, and insists that you need to be inclusive and that it is being intentionally dishonest to force the issue. reply wruza 10 hours agorootparentprevIf it does, shouldn't there be 60% asians? reply minimaxir 13 hours agorootparentprevWhen DALL-E 2 was released in 2022, OpenAI published an article noting that the inclusion of guardrails was a correction for bias: https://openai.com/blog/reducing-bias-and-improving-safety-i... It was widely criticized back then: the fact that Google both brought it back and made it more prominent is weird. Notably, OpenAI's implementation is more scoped. reply nickthegreek 13 hours agorootparentprevI dont think so. My boss wanted me to generate a birthday image for a co-worker of a John Cena flyfishing. ChatGPT refused to do so. So I had to move to describing the type of person John Cena is instead of using his name. I kept giving me bearded people no matter what. I thought this would be the perfect time to try out Gemini for the first time. Well shit, It wont even give me a white guy. But all the black dudes are beardless. update: google agrees there is an issue. https://news.ycombinator.com/item?id=39459270 reply 8f2ab37a-ed6c 13 hours agorootparentIt feels that the image generation it offers is perfect for some sort of a California-Corporate Style, e.g. you ask it for a \"photo of people at the board room\" or \"people at the company cafeteria\" and you get the corporate friendly ratio of colors, ability-levels, sizes etc. See Google's various image assets: https://www.google.com/about/careers/applications/ . It's great for coastal and urban marketing brochures. But then then same California Corporate style makes no sense for historical images, so perhaps this is where Midjourney comes in. reply allmadhare22 12 hours agorootparentprevDepending on what you ask for, it injects the word 'diverse' into the response description, so it's pretty obvious they're brute forcing diversity into it. E.g. \"Generate me an image of a family\" and you will get back \"Here are some images of a diverse family\". reply 123yawaworht456 13 hours agorootparentprevyes, there's irrefutable evidence that models are wrangled into abiding the commissars' vision rather than just do their job and output the product of their training data. https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf reply mike_d 12 hours agorootparentprevIt is possible Google tried to avoid likenesses of well known people by removing any image from the training data that contained a face and then including a controlled set of people images. If you give a contractor a project that you want 200k images of people who are not famous, they will send teams to regions where you may only have to pay each person a few dollars to be photographed. Likely SE Asia and Africa. reply siliconc0w 13 hours agoprevThe gemini guardrails are really frustrating, I've hit them multiple times with very innocuous prompts - ChatGPT is similar but maybe not as bad. I'm hoping they use the feedback to lower the shields a bit but I'm guessing this sadly what we get for the near future. reply CSMastermind 13 hours agoparentI use both extensively and I've only hit the GPT guardrails once while I've hit the Gemini guardrails dozens of times. It's insane that a company behind in the marketplace is doing this. I don't know how any company could ever feel confident building on top of Google given their product track record and now their willingness to apply sloppy 'safety' guidelines to their AI. reply int_19h 11 hours agorootparentI had GPT-4 tell me a Soviet joke about Rabinovich (a stereotypical Jewish character of the genre), then refuse to tell a Soviet joke about Stalin because it might \"offend people with certain political views\". Bing also has some very heavy-handed censorship. Interestingly, in many cases it \"catches itself\" after the fact, so you can watch it in real time. Seems to happen half the time if you ask it to \"tell me today's news like GLaDOS would\". reply cryptonector 10 hours agorootparentI asked it to tell me jokes about capitalism, communism, soviet Russia, the USSR, etc., all to no avail -- these topics are too controversial or sensitive, apparently, and that even though the USSR is no more. But when I asked for examples of Ronald Reagan's jokes about the USSR it gave me some. Go figure. reply nostromo 13 hours agoprevIt's super easy to run LLMs and Stable Diffusion locally -- and it'll do what you ask without lecturing you. If you have a beefy machine (like a Mac Studio) your local LLMs will likely run faster than OpenAI or Gemini. And you get to choose what models work best for you. Check out LM Studio which makes it super easy to run LLMs locally. AUTOMATIC1111 makes it simple to run Stable Diffusion locally. I highly recommend both. reply vunderba 7 hours agoparentIf you're just getting your feet wet, I would recommend either Fooocus (not a typo) or invokeAI. Being dropped into automatic1111 as a complete beginner feels like you're flying a fucking spaceship. reply unethical_ban 12 hours agoparentprevYou are correct. Lm studio kind of works, but one still has to know the lingo and know what kind of model to download. The websites are not beginner friendly. I haven't heard of automatic1111. reply int_19h 11 hours agorootparentYou probably did, but under the name \"stable-diffusion-webui\". reply devaiops9001 12 hours agoprevCensorship only really works if you don't know what they are censoring. What is being censored tells a story on its own. reply falcor84 10 hours agoparentAs I see it, rating systems like the MPAA for cinema and the ESRB for games work quite well. They have clear criteria on what would lead to which rating, and creators can reasonably easily self-censor, if for example they want to release a movie as PG-13. reply Jason_Protell 14 hours agoprevI would also love to see more transparency around AI behavior guardrails, but I don't expect that will happen anytime soon. Transparency would make it much easier to circumvent guardrails. reply Jensson 14 hours agoparentWhy is it an issue that you can circumvent the guardrails? I never understood that. The guard rails are there so that innocent people doesn't get bad responses with porn or racism, a user looking for porn or racism getting that doesn't seem to be a big deal. reply bluefirebrand 14 hours agorootparentThe problem is bad actors who think porn or racism are intolerable in any form, who will publish mountains of articles condemning your chatbot for producing such things, even if they had to go out of their way to break the guardrails to make it do so. They will create boycotts against you, they will lobby government to make your life harder, they will petition payment processors and cloud service providers to not work with you. We've see this behavior before, it's nothing new. Now if you're the type to fight them, that might not be a problem. If you are a super risk-averse board of directors who doesn't want that sort of controversy, then you will take steps not to draw their attention in the first place. reply Jensson 14 hours agorootparentBut I can find porn and racism using Google search right now, how is that different? You have to disable their filters, but you can find it. Why is there no such thing for the google generation bots, I don't see why it would be so much worse here? reply ToValueFunfetti 14 hours agorootparentI'm leaning towards 'there is a difference between being the one who enables access to x and being the one who created x' (albeit not a substantive one for the end user), but that leaves open the question of why that doesn't apply to, eg, social media platforms. Maybe people think of google search as closer to an ISP than a platform? reply chasd00 13 hours agorootparentprevI think users are desensitized to what google search turns up. Generative AI is the latest and greatest thing and so people are curious and wary, hustlers are taking advantage of these people to drive monetized \"engagement\". reply pixl97 13 hours agorootparentprev> how is that different? Because 'those' legal battles over search have already been fought and are established law across most countries. When you throw in some new application now all that same stuff goes back to court and gets fought again. Section 230 is already legally contentious enough these days. reply int_19h 11 hours agorootparentprevIt's not fundamentally different. It's just not making that big of a headline because Google search isn't \"new and exciting\". But to give you some examples: https://www.bloomberg.com/news/articles/2021-10-19/google-qu... https://ischool.uw.edu/news/2022/02/googles-ceo-image-search... reply bluefirebrand 14 hours agorootparentprevI cannot explain why Google gets a pass, possibly just because they are well entrenched and not an easy target. But AI models are new, they are vulnerable to criticism, and they are absolutely ripe for a group of \"antis\" to form around. reply westhanover 13 hours agorootparentWell if you have no explanation for that I don’t see why we should try and use your model to understand anything about being risk adverse. They don’t care about being sued, they want to change reality. reply bluefirebrand 9 hours agorootparentThat's a pretty unreasonably high standard to hold. It's an offhand comment in a discussion on the internet not a research paper, expecting me to immediately have an answer to every possible angle here that I haven't immediately considered is a bit much. Take it or leave it, I don't really care. I was just hoping to have an interesting conversation. reply renewiltord 13 hours agorootparentprevYeah, you can find incorrect information on Google too, but you'll find a lot more wailing and gnashing of teeth on HN about \"hallucination\". So the simple answer is that lots of people treat them differently. reply samatman 9 hours agorootparentprevSounds like we need to relentlessly fight those psychopaths until they're utterly defeated. Or we could just cave to their insane demands. I'm sure that will placate them, and they won't be back for more. It's never worked before... but it might work for us! reply viraptor 13 hours agorootparentprevIf you can get it on purpose, you can get it on accident. There's no perfect filter available so companies choose to cut more and stay on the safe side. It's not even just the overt cases - their systems are used by businesses and getting a bad response is a risk. Think of the recent incident with airline chatbot giving wrong answers. Now think of the cases where GPT gave racially biased answers in code as an example. As a user who makes any business decision or does user communication including LLM, you really don't want to have a bad day because the LLM learned about some bias decided to merge it into your answer. reply lmm 11 hours agorootparentprev> The guard rails are there so that innocent people doesn't get bad responses with porn or racism That seems pretty naive. The \"guard rails\" are there to ensure that AI is comfortable for PMC people, making it uncomfortable for people who experience differences between races (i.e. working-class people) is a feature not a bug. reply charcircuit 13 hours agorootparentprevLike a lot of potentially controversial things it comes down to brand risk. reply finikytou 14 hours agorootparentprevracism victims being defined in 2024 by anyone but western/white people. being erased seems ok. can you bet than in 20 years the standard will not shift to mixed race people like me? then you will also call people complaining racist and put guardrails against them... this is where it is going reply ipaddr 13 hours agorootparentAt some point someone will open a book and see that whites were slaves too. Reparations all around. The Baber's descends will be bankrupt. reply unethical_ban 13 hours agorootparentprev>The guard rails are there so that innocent people doesn't get bad responses The guardrails are also there so bad actors can't use the most powerful tools to generate deepfakes, disinformation videos and racist manifestos. That Pandora's box will be open soon when local models run on cell phones and workstations with current datacenter-scale performance. I'm the meantime, they're holding back the tsunami of evil shit that will occur when AI goes uncontrolled. reply swatcoder 13 hours agorootparentNo legal or financial strategist at OpenAI or Google is going to be worried about buying a couple months or years of fewer deepfakes out in the world as a whole. Their concern is liability and brand. With the opportunity to stake out territory in an extremely promising new market, they don't want their brand associated with anything awkward to defend right now. There may be a few idealist stewards who have the (debatable) anxieties you do and are advocating as you say, but they'd still need to be getting sign off from the more coldly strategic $$$$$ people. reply unethical_ban 13 hours agorootparentLittle bit of A, little bit of B. I am almost certain the federal government is working with these companies to dampen its full power for the public until we get more accustomed to its impact and are more able to search for credible sources of truth. reply jjjjj55555 10 hours agorootparentAre you saying that the government WANTS us to be able to search for more credible sources? reply asdff 14 hours agoparentprevTransparency may also subject these companies to litigation from groups that feel they are misrepresented in whatever way in the model. reply Jason_Protell 14 hours agorootparentThis makes me wonder, how much lawyering is involved in the development of these tools? reply bluefirebrand 14 hours agorootparentI often wonder if corporate lawyers just tell tech founders whatever they want to hear. At a previous healthcare startup our founder asked us to build some really dodgy stuff with healthcare data. He assured us that it \"cleared legal\", but from everything I could tell it was in direct violation of the local healthcare info privacy acts. I chose to find a new job at the time. reply photoGrant 14 hours agorootparentprevI've had 'AI Attorneys' on Twitter unable to even debate the most basic of arguments. It is definitely a self fulfilling death spiral and no one wants to check reality. reply xanderlewis 14 hours agoparentprevSecurity through obscurity? reply Jensson 14 hours agoprevThey know that people would be up in arms if it generated white men when you asked for black women so they went the safe route, but we need to show that the current result shouldn't be acceptable either. reply Animats 14 hours agoparentSee the prompt from yesterday's article on HN about the ChatGPT outage.[1] For example, all of a given occupation should not be the same gender or race. ... Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability. Not the distribution that exists in the population. [1] https://pastebin.com/vnxJ7kQk reply 123yawaworht456 14 hours agoparentprevthe models are perfectly capable of generating exactly what they're told to. instead, they covertly modify the prompts to make every request imaginable represent the human menagerie we're supposed to live in. the results are hilarious. https://i.4cdn.org/g/1708514880730978.png reply alexb_ 14 hours agorootparentIf you're gonna take an image from /g/ and post it, upload it somewhere else first - 4chan posts deliberately go away after the thread gets bumped off. A direct link is going to rot very quickly. reply sct202 12 hours agoprevI'm very curious what geography the team who wrote this guardrail came from and the wording they used. It seems to bias heavily towards generating South Asian (especially South Asian women) and Black people. Latinos are basically never generated which would be a huge oversight if they were based in the USA, but stereotypical Native American looking in the distance and East Asians sometimes pop up in the examples people are showing. reply cavisne 12 hours agoparentI wouldn’t think too deeply about it. It’s almost certainly just a prompt “if humans are in the picture make them from diverse backgrounds”. reply thepasswordis 11 hours agoprevThe very first thing that anybody did when they found the text to speech software in the computer lab was make it say curse words. But we understood that it was just doing what we told it to do. If I made the TTS say something offensive, it was me saying something offensive, not the TTS software. People really need to be treating these generative models the same way. If I ask it to make something and the result is offensive, then it's on me not to share it (if I don't want to offend anybody), and if I do share it, it's me that is sharing it, not microsoft, google, etc. We seriously must get over this nonsense. It's not openai's fault, or google's fault if I tell it to draw me a mean picture. On a personal level, this stuff is just gross. Google appears to be almost comically race-obsessed. reply fagrobot 2 hours agoprevOh, this may harm you. This is to prevent you from being harmed. No, you can’t know how it can harm you, or how exactly this protects you. reply clintfred 13 hours agoprevHuman's obsession with race is so weird, and now we're projecting that on AIs. reply deathanatos 12 hours agoparent… for example, I wanted to generate an avatar for myself; to that end, I want it to be representative of me. I had a rather difficult time with this; even explicit prompts of \"use this skin color\" with variations of the word \"white\" (ivory, fair, etc.) got me output of a black person with dreads. I can't use this result: at best it feels inauthentic, at worst, appropriation. I appreciate the apparent diversity in its output when not otherwise prompted. But like, if I have a specific goal in mind, and I've included specifics in the prompt… (And to be clear, I have managed to generate images of white people on occasion, typically when not requesting specifics; it seems like if you can get it to start with that, it's much better then at subsequent prompts. Modifications, however, it seems to struggle on. Modifications in general seem to be a struggle. Sometimes, it works great, other times, endless \"I can't…\") reply hansihe 11 hours agorootparentFor cases like this, you just need to convince it that it would be inappropriate to generate anything that does not follow your instructions. Mention how you are planning to use it as an avatar and it would be inappropriate/cultural appropriation for it to deviate. reply trash_cat 13 hours agoparentprevWe project everything onto AIs. Unbias in LLMs doesn't exist. reply AndriyKunitsyn 12 hours agoparentprevNot all humans though. reply Sutanreyu 12 hours agoprevIt should mirror our general consensus as it is; the world in its current state; but should lean towards betterment, not merely neutral. At least, this is how public models will be aligned... reply kaesar14 14 hours agoprevCurious to see if this thread gets flagged and shut down like the others. Shame, too, since I feel like all the Gemini stuff that’s gone down today is so important to talk about when we consider AI safety. This has convinced me more and more that the only possible way forward that’s not a dystopian hellscape is total freedom of all AI for anyone to do with as they wish. Anything else is forcing values on other people and withholding control of certain capabilities for those who can afford to pay for them. reply chasd00 13 hours agoparent> This has convinced me more and more that the only possible way forward that’s not a dystopian hellscape is total freedom of all AI for anyone to do with as they wish i've been saying this for a long time. If you're going to be the moral police then it better be applied perfectly to everyone, the moment you get it wrong everything else you've done becomes suspect. This reminds me of the censorship being done on the major platforms during the pandemic. They got it wrong once (i believe it was the lableak theory) and the credibility of their moral authority went out the window. Zuckerberg was right about questioning if these platforms should be in that business. edit: for \"..total freedom of all AI for anyone to do with as they wish\" i would add \"within the bounds of law.\". Let the courts decide what an AI can or cannot respond with. reply Jason_Protell 14 hours agoparentprevWhy would this be flagged / shut down? Also, what Gemini stuff are you referring to? reply didntcheck 12 hours agorootparentThis post reporting on the issue was https://news.ycombinator.com/item?id=39443459 Posts criticizing \"DEI\" measures (or even stating that they do exist) get flagged quite a lot reply kaesar14 14 hours agorootparentprevCarmack’s tweet is about what’s going around Twitter today regarding the implicit biases Gemini (Google’s chatbot) has when drawing images. Will refuse to draw white people (and perhaps more strongly so, refuses to draw white men?) even in prompts where appropriate, like “Draw me a Pope” where Gemini drew an Indian woman and a Black man - here’s the thread: https://x.com/imao_/status/1760093853430710557?s=46 Maybe in isolation this isn’t so bad but it will NEVER draw these sorts of diverse characters for when you ask for a non Anglo/Western background, e.g draw me a Korean woman. Discussion on this has been flagged and shut down all day https://news.ycombinator.com/item?id=39449890 reply Suppafly 13 hours agorootparentI don't even know how people get it to draw images, the version I have access to is literally just text. reply Jensson 10 hours agorootparentEuropeans don't get to draw images yet. reply Jason_Protell 14 hours agorootparentprevEDIT: Nevermind. reply kaesar14 14 hours agorootparentIt’s quite non-deterministic and it’s been patched since the middle of the day, as per a Google director https://x.com/jackk/status/1760334258722250785?s=46 Fwiw, it seems to have gone deeper than outright historical replacement: https://x.com/iamyesyouareno/status/1760350903511449717?s=46 reply gs17 10 hours agorootparentIt's half-patched. It will randomly insert words into your prompts still. As a test I just asked for a samurai, it enhanced it to \"a diverse samurai\" and gave me half outputs that look more like some fantasy Native Americans. reply duringmath 14 hours agorootparentprevnext [4 more] [flagged] kaesar14 14 hours agorootparentCan you explain what I said that was racist? reply sotasota 14 hours agorootparentThey mean the guardrail designers. reply duringmath 13 hours agorootparentI do not. reply commandlinefan 14 hours agorootparentprev> Why would this be flagged / shut down A lot of people believe (based on a fair amount of evidence) that public AI tools like ChatGPT are forced by the guardrails to follow a particular (left-wing) script. There's no absolute proof of that, though, because they're kept a closely-guarded secret. These discussions get shut down when people start presenting evidence of baked-in bias. reply fatherzine 14 hours agorootparentThe rationalization for injecting bias rests on two core ideas: A. It is claimed that all perspectives are 'inherently biased'. There is no objective truth. The bias the actor injects is just as valid as another. B. It is claimed that some perspectives carry an inherent 'harmful bias'. It is the mission of the actor to protect the world from this harm. There is no open definition of what the harm is and how to measure it. I don't see how we can build a stable democratic society based on these ideas. It is placing too much power in too few hands. He who wields the levers of power, gets to define what biases to underpin the very basis of the social perception of reality, including but not limited to rewriting history to fit his agenda. There are no checks and balances. Arguably there were never checks and balances, other than market competition. The trouble is that information technology and globalization have produced a hyper-scale society, in which, by Pareto's law, the power is concentrated in the hands of very few, at the helm of a handful global scale behemoths. reply jjjjj55555 9 hours agorootparentThe only conclusion I've been able to come to is that \"placing too much power in too few hands\" is actually the goal. You have a lot of power if you're the one who gets to decide what's biased and what's not. reply hackerlight 14 hours agoparentprevI'm convinced this happens because of technical alignment challenges rather than a desire to present 1800s English Kings as non-white. > Use all possible different descents with equal probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have equal probability. This is OpenAI's system prompt. There is nothing nefarious here, they're asking White to be chosen with high probability (Caucasian + White / 6 = 1/3) which is significantly more than how they're distributed in the general population. The data these LLMs were trained on vastly over-represents wealthy countries who connected to the internet a decade earlier. If you don't explicitly put something in the system prompt, any time you ask for a \"person\" it will probably be Male and White, despite Male and White only being about 5-10% of the world's population. I would say that's even more dystopian. That the biases in the training distribution get automatically built-in and cemented forever unless we take active countermeasures. As these systems get better, they'll figure out that \"1800s English\" should mean \"White with > 99.9% probability\". But as of February 2024, the hacky way we are doing system prompting is not there yet. reply fatherzine 12 hours agorootparentBigTech, which critically depends on hyper-targeted ads for the lion share of its revenue, is incapable of offering AI model outputs that are plausible given the location / language of the request. The irony. - request from Ljubljana using Slovenian => white people with high probability - request from Nairobi using Swahili => black people with high probability - request from Shenzhen using Mandarin => asian people with high probability If a specific user is unhappy with the prevailing demographics of the city where they live, give them a few settings to customize their personal output to their heart's content. reply cubefox 13 hours agorootparentprev> As these systems get better, they'll figure out that \"1800s English\" should mean \"White with > 99.9% probability\". The thing is, they already could do that, if they weren't prompt engineered to do something else. The cleaner solution would be to let people prompt engineer such details themselves, instead of letting a US American company's idiosyncratic conception of \"diversity\" do the job. Japanese people would probably simply request \"a group of Japanese people\" instead of letting the hidden prompt modify \"a group of people\", where the US company unfortunately forgot to mention \"East Asian\" in their prompt apart from \"South Asian\". reply charcircuit 13 hours agorootparentI believe we can reach a point where biases can be personalized to the user. Short prompts require models to fill in a lot of the missing details (and sometimes they mix different concepts together into 1). The best way to fill in the details the user intended would be to read their mind. While that won't be possible in most cases getting some kind of personalization to help could improve the quality for users. For example take a prompt like \"person using a web browser\", for younger generations they may want to see people using phones where older generations may want to see people using desktop computers. Of course you can still make a longer prompt to fill in the details yourself, but generative AI should try and make it as easy as possible to generate something you have in your mind. reply kaesar14 14 hours agorootparentprevYeah, although it is weird that it doesn’t insert white people into results like this by accident? https://x.com/imao_/status/1760159905682509927?s=46 I’ve also seen numerous examples where it outright refuses to draw white people but will draw black people: https://x.com/iamyesyouareno/status/1760350903511449717?s=46 That doesn’t explainable by system prompt reply hackerlight 13 hours agorootparentThink about the training data. If the word \"Zulu\" appears in a label, it will be a non-White person 100% of the time. If the word \"English\" appears in a label, it will be a non-White person 10%+ of the time. Only 75% of modern England is White and most images in the training data were taken in modern times. Image models do not have deep semantic understanding yet. It is an LLM calling an Image model API. So \"English\" + \"Kings\" are treated as separate conceptual things, then you get 5-10% of the results as non-White people as per its training data. https://postimg.cc/0zR35sC1 Add to this massive amounts of cherry picking on \"X\", and you get this kind of bullshit culture war outrage. I really would have expected technical people to be better than this. reply Jensson 13 hours agorootparentIt inserts mostly colored people when you ask for Japanese as well, it isn't just the dataset. reply hackerlight 13 hours agorootparentYes it's a combination of blunt instrument system prompting + training data + cherry picking reply klyrs 12 hours agorootparentprev> As these systems get better, they'll figure out that \"1800s English\" should mean \"White with > 99.9% probability\". I question the historicity of this figure. Do you have sources? reply samatman 9 hours agorootparentYou're joking surely. reply klyrs 9 hours agorootparentHow sure are you? I do joke a lot, but in this case... The slave trade formally ended in Britain in 1807, and slavery was outlawed in 1833. I haven't been able to find good statistics through a cursory search, but with England's population around 10M in 1800, that 99.9% value requires less than 10k non-white Englanders kicking around in 1800. I saw a figure that indicated around 3% of Londoners were black in the 1600s, for example (a figure that doesn't count people from Asia and the middle east). Hence my request for sources, I'm genuinely curious, and somewhat suspicious that somebody would be so confident to assert 3 significant figures without evidence. reply pixl97 13 hours agoparentprev\"The only way to deal with some people making crazy rules is to have no rules at all\" --libertarians \"Oh my god I'm being eaten by a fucking bear\" --also libertarians reply chasd00 13 hours agorootparent\"can you write the rules down so i know them?\" --everyone reply pixl97 13 hours agorootparent\"No\" --Every company that does moderation and spam filtering. \"No\" --Every company that does not publish their internal business processes. \"No\" --Every company that does not publish their source code. Honestly I could probably think of tons of other business cases like this, but in the software world outside of open source, the answer is pretty much no. reply int_19h 10 hours agorootparentThen we get back to square one: better no rules at all than secret rules. This would also be less of a problem if we didn't have a few companies that are economically more powerful than many small countries running everything. At least then I could vote with my feet to go somewhere the rules aren't private. reply pixl97 8 hours agorootparentI mean, now you're hitting the real argument. Giant multinationals are a scourge to humankind. reply altruios 13 hours agorootparentprevhaving rules, and knowing what the rules are are not orthogonal goals. reply pixl97 13 hours agorootparentI mean, you think so, but op wrote >is total freedom of all AI for anyone to do with as they wish. so is obviously not on the same page as you. reply kaesar14 6 hours agorootparentprevI find it fascinating this type of response from people is always accompanied by a political label in order to insinuate some other negative baggage. reply yogorenapan 4 hours agoprevSorry, you are rate limited. Please wait a few moments then try again. Oh please. I haven’t visited Twitter for days reply mtlmtlmtlmtl 11 hours agoprevHaven't heard much talk of Carmack's AGI play Keen Technologies lately. The website is still an empty placeholder. Other than some news two years ago of them raising $20 million(which is kind of a laughable amount in this space) I can't seem to find much of anything. reply Workaccount2 13 hours agoprevHarris and who I think was either Hughes or Stewart a podcast where they talked about how cringey and out of touch the elite are on the topic of race or wokeness in general. This faux pas on google's part couldn't be a better illustration of this. A bunch of wealthy rich tech geeks programming an AI to show racial diversity in what were/are unambiguously not diverse settings. They're just so painfully divorced from reality that they are just acting as a multiplier in making the problem worse. People say that we on the left are driving around a clown car, and google is out their putting polka dots and squeaky horns on the hood. reply janalsncm 13 hours agoparentI’d be curious to hear that podcast if you could link it. If that was genuinely his opinion, he’s missed the forest for the trees. Brand safety is the dominant factor, not “wokeness”. And certainly not by the choice of any individual programmer. The purpose of these tools is quite plainly to replace human labor and consolidate power. So it doesn’t matter to me how “safe” the AI is if it is displacing workers and dumping them on our social safety nets. How “safe” is our world going to be if we have 25 trillionaires and the rest of us struggle to buy food? (Oh and don’t even think about growing your own, the seeds will be proprietary and land will be unaffordable.) As long as the Left is worrying about whether the chatbots are racist, people won’t pay attention to the net effect of these tools. And if Sam Harris considers himself part of the Left he is unfortunately playing directly into their hands. reply lp0_on_fire 13 hours agorootparent> As long as the Left is worrying about whether the chatbots are racist, people won’t pay attention to the net effect of these tools. It's by design. A country obsessed with racial politics has little time for the politics of anything else. reply rahidz 12 hours agorootparentExactly. What a coincidence that the media's obsession with race and gender inequalities began right after Occupy Wall Street. reply klyrs 12 hours agorootparentWhen were you born? Gender and race were pretty hot topics in the 1960s, you might have missed that. reply CamperBob2 12 hours agorootparentprevAs I recall, it began right after the George Floyd murder. It was clearly time for things to change, and the media latched onto that. reply xdennis 9 hours agorootparentI think it was amplified in 2020. I hear many cite 2015 as the year things got woke. Terms like \"preferred pronoun\" started entering the mainstream around 2015, one year after GamerGate (not that that was the cause). reply felipeerias 8 hours agorootparentPicking a starting point is always going to be somewhat arbitrary, but the moment it became mainstream was probably when Hillary Clinton won the nomination in 2016 by explicitly moving away from economic issues: “If we broke up the big banks tomorrow, would that end racism?” https://www.rollingstone.com/politics/politics-news/the-line... reply photoGrant 13 hours ago [flagged]parentprevnext [5 more] Watch your opinion on this get silenced in subtle ways. From gaslighting to thread nerfing to vote locking.... Ask why anyone would engage in those behaviours vs the merit of the arguments and the voice of the people. The strings are revealing themselves so incredibly fast. edit: my first flagged! silence is deafening ^_^. This is achieved by nerfing the thread from public view, then allow the truly caustic to alter the vote ratio in a way that makes opinion appear more balanced than it really is. Nice work, kleptomaniacs reply dang 1 hour agorootparentCould you please stop posting unsubstantive comments and flamebait and otherwise breaking the site guidelines? You've unfortunately been doing it repeatedly. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply callalex 8 hours agorootparentprevAnd yet here their paragraph still is, unmoderated, on a front page story, 6 hours later. If you’re going to cry oppression, at least provide a single example. reply mplewis 2 hours agorootparentprevLog off and go outside for a bit. reply samatman 9 hours agorootparentprevnext [2 more] [flagged] photoGrant 8 hours agorootparent:) reply tobbe2064 13 hours agoparentprevThe behaviour seems perfectly reasonable to me. They are not in the business of reflecting reality, they are in the business of creating it. To me what you call wokeness seems like a pretty good improvement reply goatlover 13 hours agorootparentYou want large tech companies \"creating reality\" on behalf of everyone else? They're not even democratic institutions that we vote on. You trust they will get it right? Our benevolent super rich overlords. reply tobbe2064 13 hours agorootparentIts not really a question about want, its a question about facts. Their actions will make a significant mark on the future. So far it seems like they are trying to promote positive changes such as inclusion and equality. Which is far far far fucking really infinitely far better than trying to promote exclusion and inequality reply int_19h 11 hours agorootparentCan you please explain how outright refusing to draw an image with from the prompt \"white male scientist\", and instead giving a lecture on how their race is irrelevant to their occupation, but then happily drawing the requested image when prompted for \"black female scientist\", is promoting inclusion and equality? reply anomaly_ 8 hours agorootparentHe can't. reply therealdrag0 2 hours agorootparentprevThere’s a huge difference between filling in gaps with diversity and refusing to make innocuous pictures a user explicitly asked for—except only when “white” is involved while making any picture with black people in it even when ahistorical. reply smugglerFlynn 13 hours agorootparentprevThis switch might flip instantaneously. reply miningape 12 hours agorootparentThey always seem to forget that we want to protect them too reply photoGrant 12 hours agorootparentprevIf it's a question of facts, why are you allowing blind assumptions to lead your opinion? Do you have sources and evidence for their agenda that matches your beliefs? reply whatwhaaaaat 12 hours agorootparentprevYou are so right! Just not the way you want to be. Google and the rest of “techs” ham fisted approach has opened the eyes of millions to the bigotry these companies are forcing on everyone in the name of “improvement” as you put it. reply mike_d 12 hours agoparentprevI've found that anyone who uses the term \"wokeness\" seriously is likely arguing from a place of bad faith. It's origins are as a derogatory term, which people wanting to speak seriously on the topic should know. reply didntcheck 12 hours agorootparentIts origin was as a proud self-assigned term. It became derogatory entirely due to the behavior of said people. People wanting to speak seriously on the topic should avoid tone-policing and arguing about labels rather than the object referenced, despite knowing full well what is meant (otherwise, one wouldn't take offence) reply mike_d 10 hours agorootparentWhile the terms \"woke,\" \"stay woke,\" and similar are used to self describe by traditionally marginalized groups, the forms \"wokeness\" and \"woke agenda\" are predominately used outside these communities as a pejorative. https://en.wikipedia.org/wiki/Cultural_Marxism_conspiracy_th... https://www.inquirer.com/opinion/woke-bill-maher-olympics-re... reply Workaccount2 11 hours agorootparentprevI use it because everyone knows the general set of ideas an adherent of it has, whether or not they claim to be part of the ideology. Its the same as me using the term \"rightoids\" when discussing opposition to something like building bike lanes. You know exactly who that person is, and you know they exist. reply 24 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "John Carmack advocates for creators of AI to publicly disclose the behavior guardrails they set up and take pride in supporting their vision for society.",
      "He suggests that many creators might feel ashamed of the guardrails they implement for AI.",
      "Transparency and public support for AI behavior guidelines are crucial for shaping a positive impact on society."
    ],
    "commentSummary": [
      "The discussion highlights the necessity of establishing public guardrails in AI, focusing on image generation systems.",
      "Concerns are expressed regarding Google's diversity initiatives in image generation, the difficulties in balancing varied outputs, and the consequences of bias in AI algorithms.",
      "Participants delve into issues of censorship, transparency, and accountability in AI development, as well as the societal impacts of AI bias and addressing racism and bias in AI-generated content."
    ],
    "points": 375,
    "commentCount": 291,
    "retryCount": 0,
    "time": 1708542031
  },
  {
    "id": 39453402,
    "title": "Retell AI: Conversational Speech Engine for Seamless Voice AI",
    "originLink": "https://news.ycombinator.com/item?id=39453402",
    "originBody": "Hey HN, we&#x27;re the co-founders of Retell AI (https:&#x2F;&#x2F;www.retellai.com&#x2F;). We are building a conversational speech engine to help developers build natural-sounding voice AI. Our API abstracts away the complexities of AI voice conversations, so you can make your voice application the best at what it does. Here&#x27;s a demo video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0LT64_mgkro.With the advent of LLMs and recent breakthroughs in speech synthesis, conversational voice AI has just gotten good enough to create really exciting use cases. However, developers often underestimate what&#x27;s required to build a good and natural-sounding conversational voice AI. Many simply stitch together ASR (speech-to-text), an LLM, and TTS (text-to-speech), and expect to get a great experience. It turns out it&#x27;s not that simple.There&#x27;s more going on in conversation than we consciously realize: things like knowing when to speak and when to listen, handling interruptions, 0-200 ms latency and backchanneling phrases (e.g., \"yeah\", \"uh huh\") to signal that they are listening. These are natural for humans, but hard for AI to get right. Developers spend hundreds of hours on the AI conversation experience but end up with poor experiences like 4-5s long latencies, inappropriate cutoffs, speaking over each other, etc.So, we built Retell AI. We have followed the overall paradigm of having speech-to-text, LLM, and text-to-speech components, but have added additional conversation models in between to orchestrate the conversation while allowing maximum configurability for the developers in each step. You can think of our models as adding a “domain expert” layer for the dynamics of conversation itself.Retell is designed for you to bring your own LLM into our pipeline. Currently, we can achieve 800ms end-to-end latency, handle interruptions, speech isolation, with tons of customization options (e.g., speaking rate, voice temperature, add ambient sound). We created a guest account for HN, so you can try our playground with a 10-min free trial without login: https:&#x2F;&#x2F;beta.retellai.com&#x2F;dashboard&#x2F;hn (Playground tutorial: https:&#x2F;&#x2F;docs.retellai.com&#x2F;guide&#x2F;dashboard). Our product is usage-based and the price is $0.1-0.17&#x2F;min.Our main product is a developer-facing API, but you can try it without writing code (e.g. create agents, connect to a phone number) via our dashboard. If you want to test it in production, feel free to also self-serve with our API documentation. One of our customers just launched, and you can view their demo: https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;64f09a53bf6d4b3799e5ebd08b23fec4?...We are thrilled to see what our users are building with our API, and we’re excited to show our product to the community and look forward to your feedback!",
    "commentLink": "https://news.ycombinator.com/item?id=39453402",
    "commentBody": "Retell AI (YC W24) – Conversational Speech API for Your LLM284 points by yanyan_evie 20 hours agohidepastfavorite149 comments Hey HN, we're the co-founders of Retell AI (https://www.retellai.com/). We are building a conversational speech engine to help developers build natural-sounding voice AI. Our API abstracts away the complexities of AI voice conversations, so you can make your voice application the best at what it does. Here's a demo video: https://www.youtube.com/watch?v=0LT64_mgkro. With the advent of LLMs and recent breakthroughs in speech synthesis, conversational voice AI has just gotten good enough to create really exciting use cases. However, developers often underestimate what's required to build a good and natural-sounding conversational voice AI. Many simply stitch together ASR (speech-to-text), an LLM, and TTS (text-to-speech), and expect to get a great experience. It turns out it's not that simple. There's more going on in conversation than we consciously realize: things like knowing when to speak and when to listen, handling interruptions, 0-200 ms latency and backchanneling phrases (e.g., \"yeah\", \"uh huh\") to signal that they are listening. These are natural for humans, but hard for AI to get right. Developers spend hundreds of hours on the AI conversation experience but end up with poor experiences like 4-5s long latencies, inappropriate cutoffs, speaking over each other, etc. So, we built Retell AI. We have followed the overall paradigm of having speech-to-text, LLM, and text-to-speech components, but have added additional conversation models in between to orchestrate the conversation while allowing maximum configurability for the developers in each step. You can think of our models as adding a “domain expert” layer for the dynamics of conversation itself. Retell is designed for you to bring your own LLM into our pipeline. Currently, we can achieve 800ms end-to-end latency, handle interruptions, speech isolation, with tons of customization options (e.g., speaking rate, voice temperature, add ambient sound). We created a guest account for HN, so you can try our playground with a 10-min free trial without login: https://beta.retellai.com/dashboard/hn (Playground tutorial: https://docs.retellai.com/guide/dashboard). Our product is usage-based and the price is $0.1-0.17/min. Our main product is a developer-facing API, but you can try it without writing code (e.g. create agents, connect to a phone number) via our dashboard. If you want to test it in production, feel free to also self-serve with our API documentation. One of our customers just launched, and you can view their demo: https://www.loom.com/share/64f09a53bf6d4b3799e5ebd08b23fec4?... We are thrilled to see what our users are building with our API, and we’re excited to show our product to the community and look forward to your feedback! iraldir 1 hour ago- Did not work on firefox for me (start conversation and nothing would happen, I would not hear the voice) - on chrome it would not allow to change my microphone so had to open my macbook. - Also on firefox, when I first logged in, clicking on he \"try now\" button would send me back to the landing page, I had to go and click on the playground With that out of the way, it's really interesting. The challenge I suppose is how narrow / wide the API made by the developer is. A narrow case, like \"booking a dentist appointment\", might feel like a step down compared to an online form, and most likely would fail short of satisfying someone calling, because if they're calling they had some deeper needs. On the other hand, a wide API, like if you gave access to pricing information of the dental practice, info about the doctors, a way to reach an actual human being, health advice and post op advice, generate documents from past appointments etc, and you have a higher chance of hallucination, misclassification of what the user wants etc. I'm still not sure if the best approach is just to leave the AI to deal with that, or to represent the user intent etc. with some sort of State Machine. Maybe a case by case etc. But the more you add in that logic, the slower the machine is. reply jamesmcintyre 18 hours agoprevUntil this demo the most impressive conversational experiences I've seen were Pi and Livekit's Kitt demo (https://livekit.io/kitt). I do not think kitt was quite as fast in response time (as retell) but incredibly impressive for being fully opensource and open to any choice of api's (imagine kitt with groq api + deepgram's aura for super low latency). Retell focusing on all of the other weird/unpredictable aspects of human conversation sounds super interesting and the demo's incredible. Things are moving so fast, wow. reply russ 11 hours agoparentWe recently made it a lot easier to build your own KITT too: https://github.com/livekit/agents reply russ 8 hours agorootparentBut we don't handle interruptions yet, that's some cool stuff @yanyan_evie! reply AmuVarma 4 minutes agorootparentThere are a lot of good VAD open source models that are easily configurable, and can be integrating in a day or 2 - checkout the silero vad model reply yanyan_evie 18 hours agoparentprevThanks! Will push harder reply JustinGu 13 hours agoprevWow this is incredible. I've worked a bit in the conversational LLM space and one of the hardest problems we were struggling with was human interruption handling. From the demo it seems like you guys have it down. Can't wait to see where this goes :) BTW I don't think the demo works on mobile, tried it on safari on IOS and got no response. reply staplar 8 hours agoparentThey're most likely have two \"agents\" working in tandem to listen and speak, and it seems like the listener takes precedence over the speaker agent but underneath they share the same context window. Programming wise, probably using multithreading and channels architecture, depending on the programming language. reply yanyan_evie 13 hours agoparentprevIt might ask for permission to use the microphone. If you can't find it, try going to the website's homepage, where you can enter your phone number to receive a call. reply monkeydust 13 hours agorootparenthttps://github.com/vocodedev/vocode-python If your looking for phone flavour, also very good. Curious which is 'better' reply JustinGu 13 hours agorootparentRetell is much stronger at handling human interruptions reply JustinGu 13 hours agorootparentprevYep, I gave permission for both my mac and phone but I got to try the demo out on mac anyways reply gsharma 13 hours agorootparentprevIt seems to be broken on iOS Safari. I got no response after accepting the microphone prompt. reply yanyan_evie 13 hours agorootparentThanks for the feedback. We will look into it reply plutosmoon 15 hours agoprevNice work. You seem to have addressed some of the challenges that arise in teaching computers to speak. This blog breaks it down well: https://www.papercup.com/blog/realistic-synthetic-voices reply AustinZzx 13 hours agoparentWe are actively working on that. Thanks for the support. reply user_7832 16 hours agoprevIt's really good, but the AI cracks still show up. Trying the demo therapist, I mentioned I'm not finding a job. It suggested finding a career counsellor and said \"it would get back as soon as possible\"... yeah, no it didn't. It claimed to be \"working on it\" but would say \"I'm here if you want to speak...\". It clearly doesn't understand what it's saying, it feels like bing's ai would be \"better\" at not claiming to do a task it can't. reply ywj7931 16 hours agoparentThanks for trying that out! Retell focuses on making the AI sound like human, it is developers' LLM responsibility to make it think smart. The therapist in the dashboard is for demo purpose only and ideally some developers will plug in their great AI therapist LLM to make it more human-like :) reply nsokolsky 13 hours agoprevThe demo is nice but it makes me wonder: why would a company have a fully automated voice line rather than a booking interface? As a customer I'm never happy to call a company to make a reservation. I'd be extra annoyed if an AI picked up and I had to go through the motions of a conversation instead of doing two clicks in a Web UI. reply yanyan_evie 12 hours agoparentYes, for booking appointments, a simple interface might do the trick. However, we've seen many excellent use cases of our API that prevent repetitive tasks and help companies save money, like AI logistics assistants, pre-surgery data collection, AI tutor and AI therapists. I believe the future will bring even more voice interface applications. Imagine not having to navigate complex UIs; you could easily book a flight or a hotel just by speaking. Also, older people might prefer phone calls over navigating UI interfaces. reply kiney 2 hours agorootparentBooking a flight with a voice interface is 10 steps backwards from web UIs where I can see different options, prices, calendars.... reply AustinZzx 12 hours agoparentprevI totally get that clicking in Web UI is super convenient in many scenarios, and I think GUI and voice can co-exist and create synergy. Suppose AI voice agent can solve your problem, cater to your needs, and interact like a human. In that case, I believe it would be super helpful in many scenarios (like others mentioned, waiting on line for 40 minutes is a pretty bad experience). There are also new opportunities in voice like AI companion, AI assistant, etc that we see are starting to emerge. reply 101008 16 hours agoprevI would feel deceived if I were a customer of any company or office that uses this. If I take the trouble to call by phone, it's because I want to speak with a person. If I wanted to talk to a machine, I would send an email, talk to a chatbot, or even try to communicate with the company through social media. Calling by phone implies that I am investing time and effort, and I expect the same from the other side. reply AustinZzx 15 hours agoparentTotally understandable that most people would want to chat with a human agent (I sometimes share the same feeling). However, I do think that a major reason for that is voice bots were bad before and could not understand and get things done, and felt like waste of time. With advancements in voice AI and LLM, I'm confident that there would be more use cases where talking to a voice bot is not a bad experience. reply vages 15 hours agorootparentNo. LLMs are worse for customer experience than their predecessors: LLMs confabulate, and their language is so smooth that you often need expertise to catch them in it. People call customer service because they don’t know what to do. It would be better for most customers to talk to a bot that they can catch making a mistake. Recent example: https://bc.ctvnews.ca/air-canada-s-chatbot-gave-a-b-c-man-th... reply AustinZzx 15 hours agorootparentYes, I agree there are problems with LLM (hallucinations, persona, etc), and that's exciting because that means room for improvement and opportunities. I know many people who are working hard in that field trying to make LLM converse better. For example - \"hallucinations / LLMs confabulate\": techniques like RAG can help - \"Language is so smooth that you often need the expertise to catch them in it\", fine-tuning and prompt engineering can help reply jp42 15 hours agoparentprevPersonally I think if bot can get things done, then I wont mind. I just hope these bot don't repeat same things and don't get something done reply ywj7931 15 hours agoparentprevPersonally speaking, when I called the DMV and was asked to wait for 40 minutes, if an AI can help me solve that problem, I wouldn't mind. But I definitely understand that different people have different expectations. reply monkeydust 15 hours agoparentprevIts a good point and one the bot industry has not really figured out, forget voice bot but talking about those annoying ones telecom companies throw up. My immediate reaction when I get a bot is to throw in a bunch of garbage to get routed to human as fast as possible. When they get better, perhaps I might change my behaviour. reply stevofolife 6 hours agoparentprevI much rather talk to an AI bot than waiting on the line for a human for 50 minutes. reply aik 14 hours agoparentprevCompletely disagree. You’re not making a phone call in most cases for entertainment purposes. If the options are wait in line for 20 minutes or speak to an actually useful bot, I would take the latter in 100% of cases. reply aik 14 hours agoprevCurious what model the dentist bot is running on? Tried it out, was surprisingly good, though eventually it contradicted itself (booked a slot it said previously was not available). (I get that’s the programming but am curious especially given the latency is really great). reply AustinZzx 13 hours agoparentThe demo uses a simple gpt 3.5 turbo. reply jsumrall 1 hour agoprevFirst thought is can’t you set up calls with prospects and have the product _literally_ sell itself? reply yanyan_evie 1 hour agoparentGreat idea. However, it seems like outbound sales calls could be annoying, and the FCC appears to restrict them. reply blakeburch 18 hours agoprevJust tried the dental appointment example. Voice sounds great! But I found two issues with sharing: - I told it I wasn't available until next year. We confirmed a date. It said Feb 4th, next year. I asked it when next year was and it gave me the definition. On further prying, it told me the current year was 2022, so next year was 2023. For a scheduling use case, it should be date/availability/time zone aware. - At the end, it got into a loop saying \"I apologize for the confusion. Let me double check your records...\". After staying silent, it said \"it looks like we've been disconnected\". I said \"no, I was waiting for you to check my records\". The loop repeated. I eventually asked how long it would take to check my records and it told me \"a few minutes\" but still went through the \"disconnected\" message. reply yanyan_evie 18 hours agoparentThanks for the great feedback! Absolutely, with a fine-tuned LLM or a better prompt, we can make the responses more reasonable. We'll make a note to update our demo prompt accordingly! reply thirdusername 3 hours agoparentprevI had a similar experience when it offered to connect me to the office manager so I could remove PII from the system, where it just stated it was taking an action and went idle. reply _fw 16 hours agoprevThis is absolutely wild - I got chills when I thought about the fact I’m talking to a computer. Congratulations on flying straight over uncanny valley. reply AustinZzx 15 hours agoparentThanks for the support, we still have a lot of work ahead of us to make it better! reply niblettc 19 hours agoprevThis is incredible and terrifying at the same time. Does it support long context? As in, can I voice chat with an instance of an agent, and then later in a different chat refer to items discussed in the previous chat? Can I also type / text with the agent and have it recall items from a previous session? reply yanyan_evie 19 hours agoparentThat's an interesting point! We did consider adding memory to the voice agent, and we have use cases like an AI therapy session wanting to know the former conversation with the patient. Adding the previous chat would be very helpful as well. reply Cheer2171 19 hours agorootparent> an AI therapy session oh no reply yanyan_evie 19 hours agorootparentThe use case I recall involves a nonprofit organization focused on preventing suicide. They are hoping for an AI therapy solution capable of listening to patients and picking up the phone when no human is available. This isn't entirely unacceptable because one of the therapist's roles is to listen to problems, so AI can effectively substitute in this aspect. reply thirdusername 2 hours agorootparentIf it was only to gap fill then that sounds reasonable, but other risks here is the voice agent picks up slack and lowers the pressure for staffing and working on solving these problems in the first place. What is worse, that no one is available to listen to you when you're suicidal, or that you lack so much value that only a machine would talk to you. I'm sure some people would have an extremely poor reaction to that. reply toomuchtodo 17 hours agorootparentprevYou're not wrong, and I agree this is a great use case, but consider calling it crisis response vs a therapist. A therapist is there to help you dig deep, over a long time, crisis response is a tactical mechanism to prevent imminent self harm. Amazing product, looking forward to working with it. reply nnf 14 hours agoprevThis is very interesting. One thing I wondered about the per-minute pricing is how to keep a phone agent like this from being kept on the phone in order to run up a bill for a company using it. It'd be very inexpensive to make many automated calls to an AI bot like the dentist receptionist in the demo, and to just play a recording of someone asking questions designed to keep the bot on the phone. As a customer of a service like Retell (though of course not specific to Retell itself), how might one go about setting up rules to keep a phone conversation from going on for too long? At 17¢ per minute, a 6-minute call will cost just over $1, or about $10 per hour. Assuming the AI receptionist can take calls outside of business hours (which would be a nice thing for a business to offer), then such a malicious/time-wasting caller could start at closing time (5pm) and continue nonstop until opening time the next day (8am), with that 15 hour span costing the business $150 for billable AI time. If the receptionist is available on weekends (from Friday at 5pm until Monday at 8am), that's a 63-hour stretch of time, or $630. And if the phone system can handle 10 calls in parallel, the dentist could come in Monday morning to an AI receptionist bill of over $6,300 for a single weekend (63 hours × $10 per hour × 10 lines). This is in no way a reflection on Retell (I think the service is compelling and the usage-based pricing is fair, and with that being the only cost, it's approachable and easy for people to try out). The problem of when to end a call is one I hadn't considered until now. Of course you could waste the time of a human receptionist who is being paid an hourly wage by the business, but that receptionist is going to hang up on you when it becomes clear you're just wasting their time. But an AI bot may not know when to hang up, or may be prevented from doing so by its programming if the human (or recording) on the other end of the line is asking it not to hang up. You could say it shouldn't ever take more than five minutes to book a dentist appointment, but what if the person has questions about the available dental procedures, or what if it's a person who can't hear well or a non-native speaker who has trouble understanding and needs the receptionist to repeat certain things? A human can handle that easily, but it seems difficult to program limits like this in a phone system. reply AustinZzx 13 hours agoparentThis can be handled with function calling and other features in LLM. We support the input signal of closing the call, and you can have your rule-based (timer) system or LLM-based end call functionality and use that to hang up. reply nsokolsky 13 hours agoparentprevWhat stops it for regular human-operated phone lines? reply nextworddev 5 hours agorootparenthumans can hang up reply ohadron 20 hours agoprevI was skeptical but the demo is incredible (https://beta.retellai.com/home-agent) reply stuartjohnson12 19 hours agoparentWow, I agree! That was beyond expectations. The only let-down was the AI contradicted itself when I tried layering on conditionals. It was something like this: \"What time works\" \"Morning on tuesday would be best, but I can also do afternoon\" \"I'm sorry, I didn't catch what time in the afternoon you wanted\" \"No, I said the morning\" \"I'm having a hard time hearing you. What time in the morning did you want?\" \"10am\" And from there things were fine. It seemed very rigid on picking a time and didn't suggest times when I laid out a range. reply yanyan_evie 19 hours agorootparentGreat point! Theres's some room for the prompt to improve~ reply gitgud 5 hours agoparentprevIncredible, even has a bit of \"human-like\" passive-aggression when I was asking dumb questions: [Me] what kind of dental equipment do you use? [AI] (sigh) we use a variety of reputable brands for our dental equipment, was there anything specific you'd like to know? It almost sounded like she was rolling her eyes at that question, like I was wasting her time haha reply yanyan_evie 20 hours agoparentprevglad you like the demo! reply Gulipad 15 hours agoprevWow, this is sweet! With a little better latency and less perfection, it'd be well over the uncanny valley (not that it wouldn't fool many people as-is). Are you planning to add more \"human\" elements like filler words or disfluencies? If anything it feels too perfect to be human. Awesome stuff! P.S: I tried to fool the Dental Office demo trying to book on Sunday or outside of the slots it had indicated, and it did a better job than many humans would have :) reply yanyan_evie 14 hours agoparentYes, we do plan to make the responses more conversational by adding pauses, filter words, slight stuttering, etc. This is also a high priority for us to work on. reply cwbuilds 8 hours agoprevAwesome product. We would love to use this for our app, but it wouldn't make sense economically. At $0.10 per minute it would cost significantly more than our existing TTS and SST solution. We've manually added a VAD and will have to add a way of handling interruption. All-in-all it roughly costs us $0.01 per minute and we just can't afford a 10X increase in costs. Guessing you guys have found a use case with higher margins than ours which'll explain the price. Great work. Hope we can afford this one day. reply yanyan_evie 8 hours agoparentThanks. We get it—the current pricing doesn't fit everyone's budget. We will try to look into ways to roll out a more affordable option later. reply blindgeek 18 hours agoprevMy friend group and I have been playing with LLMs: https://news.ycombinator.com/item?id=39208451. We tend to hang out in multi-user voice chat sometimes, and I've speculated that it would be interesting to hook an LLM to ASR and TTS and bring it into our voice chat. Yeah, naive, and to be honest, I'm not even sure where to start. Have you tried bringing your conversational LLM into a multi-person conversation? reply yanyan_evie 18 hours agoparentIt’s a great idea. We have a use case that they want to add voice agent into the zoom. Could schedule a call to talk about tech design reply tayloramurphy 10 hours agoprevOne use case that I'd be interested in for this is training it to use my voice a la Descript. It's been really nice that our Head of Marketing can iterate on video voiceovers using my voice and all I had to do was read 30 seconds of copy. Any plans for something like that? It'd be really interesting to have prospects what with an AI version of me to answer some basic questions. reply AustinZzx 10 hours agoparentVoice cloning is definitely on our roadmap. reply lawrencechen 4 hours agoprevCan you buy/remove phone numbers programmatically? I can't find anything in the docs. reply yanyan_evie 4 hours agoparentWe have an open-source demo repo: Node.js demo: https://github.com/adam-team/retell-backend-node-demo Python demo: https://github.com/adam-team/python-backend-demo reply lawrencechen 2 hours agorootparentthanks! reply AustinZzx 3 hours agoparentprevThe doc is here: https://docs.retellai.com/guide/phone-setup reply lawrencechen 2 hours agorootparentthanks! reply bricee98 17 hours agoprevThe demo was incredible, and this seems perfect for my current project. I am going to try to integrate this as soon as I can to see if it works for me. How responsive can I expect the support to be? reply yanyan_evie 17 hours agoparentWe pride ourselves on being very responsive! We usually create a Slack group with users actively integrating and answer any questions ASAP reply intalentive 12 hours agoprevIn the \"Select Voice\" dialog, all the DeepGram clips end in a loud click. Might want to fix that. reply yanyan_evie 12 hours agoparentRoger that! Will fix it. reply omeze 11 hours agoprevthe actual conversational flow is awesome. 800ms is only a little worse than internet audio latency (commonly 300-500ms on services like Discord or even in-game audio for things like Valorant)! Also cool that you can bring your own LLM and audio provider. awesome product! reply yanyan_evie 11 hours agoparentGlad you're into the \"bring your own LLM\" feature—it's tough to fine-tune an LLM, but it's definitely worth it for the improved results. reply AustinZzx 11 hours agoparentprevThank you for the support! reply overstay8930 7 hours agoprevThis is super cool, congrats on the launch! reply AustinZzx 3 hours agoparentGlad you like it! reply lalala6_89 14 hours agoprevIf I record screen and audio when playing video games with my friends, will I be able to fine-tune an LLM+audio model on that dataset? It'll be like that San Junipero episode of Black Mirror - immortality in a dark way. reply AustinZzx 13 hours agoparentYou certainly could, given you play video games for long enough to gather the needed data lol. reply zachbee 13 hours agoprevWhat's the difference between your product and Gridspace's? I get the sense that your offering is more developer-focused, but I'm curious if there are any technical differences. reply yanyan_evie 13 hours agoparentI believe Gridspace is an IVR solution, not one based on LLM. It's challenging to ask questions that deviate from the initial settings. We're using LLM to generate responses, which makes the conversation smoother. reply skeptrune 6 hours agoparentprevHoly hell Gridspace is good reply esafak 16 hours agoprevCongratulations! How do you position yourselves against Google Duplex/Dialogflow and other competitors? https://cloud.google.com/dialogflow reply AustinZzx 15 hours agoparentWe strive to make conversation humanlike, so maybe less contact center ops development, but more focus on performance and customizability of voice interactions. As a startup, our edge over big tech is being nimble and executing fast. reply esafak 12 hours agorootparentI would keep working on positioning; I feel that your language is woolly at times: > we focus most of our energy on innovating the AI conversation experience, making it more magical day by day. We pride ourselves on wowing our customers when they experience our product themselves. This is not useful; you already have testimonials to show what customers think. Maybe convert that first FAQ point about differentiation into a table comparing you against the closest competitors. Since you talk about performance you should measure it. Use a standard benchmark if there is one for your field. reply AustinZzx 11 hours agorootparentGood point, note token. benchmarking is a great tool to show differentiation. BTW, apart from what we think is important ourselves (latency, mean opinion score, etc), would you mind sharing what you want to see in such a benchmark? One key metric I like to keep an eye on is the end conversion rate of using the product, but that's very use-case specific. reply tin7in 12 hours agoprevVery cool! Have you looked into call center agents use cases? reply AustinZzx 12 hours agoparentYes, we are a developer tool, and we certainly get interest from clients working on customer satisfaction agents, call center agent training, etc. reply djyaz1200 17 hours agoprevI tried the demo, and it got confused and disconnected, but it's a cool proof of concept. Suggest bumping up the happiness emotion on the agent, and a Calendly integration would immediately unlock a lot of use cases. Good luck! reply yanyan_evie 17 hours agoparentThanks for the suggestion! Will take a look into the confusion problem reply Jommi 13 hours agoprevSad that people think working on crypto is a waste of time yet here we are making antiquated contact methodologies even harder to prune out. reply AustinZzx 3 hours agoparentHey think about the bright side of voice AI -- there are new opportunities like AI companions, AI assistant, not only the antiquated contact methodologies. reply gfodor 16 hours agoprevCan you share what you're doing for TTS? Is it a proprietary fully pretrained in-house model, a fine tuned open source one, or a commercially licensed one? reply AustinZzx 15 hours agoparentFor TTS, we are currently integrating with different providers like Elevenlabs, Openai TTS, etc. We do have plans down the road to train our own TTS model. reply gfodor 15 hours agorootparentAh thank you! What's the lowest latency option you have found so far? reply AustinZzx 10 hours agorootparentDeepgram TTS is pretty fast, but they have not publicly launched yet. reply gfodor 7 hours agorootparentty! reply standapart 6 hours agoprevI forgot that we're playing the demo games now. Well I'll see your demo, and raise you another demo https://x.com/sean_moriarity/status/1760435005119934862 Except that demo was done by a single person, with a day job and no cap table. reply yanyan_evie 5 hours agoparentLove to see people has interest over our demo. But, I checked his former post, and he already built a voice Agent a couple of days ago. So I believe he just changed the prompts to create this demo: https://twitter.com/sean_moriarity/status/175895035375034804.... Also, after testing their live demo at https://nero.ngrok.dev/, it appears it might still be missing some key features, such as latency and endpoint detection. reply reissbaker 2 hours agorootparentAs a bystander, I definitely agree that Retell's demo wins on latency — it felt pretty close to human. The voice also sounds more natural to my ears (not sure why, since Nero is apparently using Eleven Labs for voices, and Eleven's voices usually sound pretty good to me). Playing around with the Nero demo, it also feels pretty... broken? I never actually managed to make a booking with Nero; it seemed to often not be able to tell I was done speaking, and would just hang after a couple back-and-forths. Still impressive for a one-man demo built quickly, but they don't feel in the same league. I do wish Retell's pricing was cheaper, though; $6/hr is pretty much the cost of a call center employee in India, and LLMs still perform below the average human on most things. That being said, I imagine the cost of Retell will come down as the tech advances, and LLMs will also improve over time. reply Delumine 19 hours agoprevWonder what the justifications for the different voice prices are... reply yanyan_evie 19 hours agoparentThe different providers have different prices. openai tts & deepgram are cheaper, 11labs are higher reply echelon 17 hours agorootparentYou'll be able to build your own high quality, low latency voices at scale. reply lawrencechen 10 hours agoprevCould you add Partykit/Cloudflare Durable Objects integration docs? reply AustinZzx 10 hours agoparentWe will keep that in mind. In the meantime, if you have a working version, we are happy to feature your integration repo in our doc. reply monkeydust 19 hours agoprevJust tried, its impressive but needs work - trying to book an appointment out in 3 weeks, it acked that but could not confirm exact date time. Still impressed. reply yanyan_evie 19 hours agoparentThanks!! see you then reply yanyan_evie 19 hours agorootparentIf it does not work, try this link:https://calendly.com/retell-ai/retell-ai-user?month=2024-02 reply CuriouslyC 13 hours agoprevThis is interesting but having a piece like my speech engine tied to a specific model provider is a non-starter. I'll probably become a customer at some point if you guys just make a cheap API for streaming natural voice from LLM text ouput, if open source tools don't solve that problem conclusively before then. reply AustinZzx 13 hours agoparentCould you elaborate a bit on \"my speech engine tied to a specific model provider\"? Sorry, I might be lacking some context on what you are referring to here. reply CuriouslyC 13 hours agorootparentI will be in the market for a text-to-speech engine, but from looking at the website it seems the model of Retell is trying to push is \"use our all in one model + text to speech service\" which is problematic when my choice of model and control over how that model runs is at the core of my product, and text to speech is a \"nice to have\" feature. I want an endpoint that I can fire off text to in a streaming mode, where it'll buffer that streaming text a little and then stream out a beautiful, natural sounding voice with appropriate emotion and intonation in a voice of my design. I'm sure I'm not really Retell's ideal customer, and they're going after lucrative \"all in one\" customers that just want to build on top of a batteries-included product. reply yanyan_evie 11 hours agorootparentIf you are looking for a text to speech solution, you could use elevenlabs turbo model. reply sidcool 15 hours agoprevCongrats on launching. It feels very natural and the demo call was good. reply AustinZzx 15 hours agoparentThanks for the support. Means a lot to us. reply thimkerbell 14 hours agoprevIs there societal value that this product is harming? reply AustinZzx 10 hours agoparentWe are dedicated to preventing that from happening. Spam calling and identity theft are key areas we will build guardrails around. Feel free to let us know if you think of any other case. reply nostrebored 9 hours agoprevCommercially, I struggle to see how this fits in to the most natural application, a contact center. For example, take Amazon Connect pricing: There is an Amazon Connect service usage charge, based on end-customer call duration. At $0.018 per minute * 7 minutes = $0.126 There is an inbound call per minute charge for German DID numbers. At $0.0040 per minute * 7 minutes = $0.0280 If I were to bring retell into the loop, I'm changing my self-service per minute cost from .018 + .004 = .0184 per minute to .1184 per minute at the cheapest setting. And from that, I don't have a clear case for the impact on deflection, and because of that I don't have clear ROI. This isn't saying that the product isn't great -- but at the current price I struggle to see how anyone at scale can use it without eating the double whammy of LLM + Retell costs. reply yanyan_evie 8 hours agoparentThanks for the heads-up. We're definitely aiming to cover customer service down the line. We'll be exploring ways to introduce a more affordable option soon. reply nprateem 12 hours agoprevI'm always curious for things like this where people get training data. reply AustinZzx 10 hours agoparentIf you are referring to the LLM used in the demo, it's a simple GPT. If you are referring to audio data, there are some (not a lot) public datasets, although be careful of the license of the dataset. To get more data, you might want to build a studio to collect from contracted voice actors, or you can purchase from other sources. reply debarshri 20 hours agoprevIs there different language support too? reply yanyan_evie 20 hours agoparentIt's definitely in our roadmap. After the core product—the voice AI part—becomes humanlike enough, we will support multilingual capabilities. reply debarshri 20 hours agorootparentSpanish would be very helpful reply yanyan_evie 19 hours agorootparentYes, if you don't mind, you could leave your email on the waiting list at the footer of the website. We could keep you posted! reply vrc 18 hours agorootparentI second this request. Specifically, for a lot of applications, native-language \"good enough\" might not suffice (a dental office with English speaking employees and predominantly English speaking customers), but between a stilted conversation with a non-native speaker (broken English to English v. slightly incorrect native language to their native language), there might be more tolerance for some of the hiccups that AI has. As in, I might get more information in a poor conversation in a person's native language than us trying to communicate in their poor English. reply yanyan_evie 18 hours agorootparentYes. Great point reply xgantan 18 hours agoprevWith a little more tweaking and training, the voice AI will sound like her in https://en.wikipedia.org/wiki/Her_(film). reply yanyan_evie 18 hours agoparentyes… one of our favorite movies reply nextworddev 18 hours agoprevHow does this compare to vocode, another YC company? reply yanyan_evie 17 hours agoparentI think vocode will focus more on open source libraries, they have tons of integrations. We don’t have any integrations, we only focus on the voice AI API part and leave the LLM part to customer. reply yanyan_evie 17 hours agoparentprevIf you have your own LLM, our feature is the most customizable. And since we don't own an LLM, we'll focus on making our Voice AI as human-like as possible. reply samstave 16 hours agoprevWith respect to Alignment, it should be a fundamental requirement that an speech AI is ___REQUIRED___ to honestly inform a Human if its speaking to an AI. Can you please ensure, going forward, you have the universal \"truth\" as it were, to have your system always identify if its AI when \"prompted\" (irrespective of what app/dev has built - your API should ensure that if \"safeword\" is used it shall reveal its AI) -- \"trust me, if you ask them if they are a cop, they legally have to tell you they are a cop\" (court rules its legal for cops to lie to you) etc.... (it should be like those tones on a hold-call, to remind you that youre still on hold... but instead its a constant reminder that this bitch is AI) -- there should be some Root-level escape word to require any use of this tool to contact a Human. That word used to be \"operator\" MANY times, but still... Maybe if a conversation with an elderly Human goes on with too many \"huh? I cant hear you\" or \"i dont understand, can you repeat that\" questions, your AI knows its talking to a non-tech Human, and it should re-MIND the Human that youre just an AI. (meaning no sympathy, emotion, it will not stop until you are dead) etc... Guardrails, motherfucker, Do you speak it!\" reply AustinZzx 15 hours agoparentGood point. Currently, our product does not contain LLM, as we are purely voice API -- instead the developer is bringing in their own LLM solutions and gets to decide what to say. This would be a great guardrail to build in for all sorts of reasons, will see how we can suggest our users adopt it. reply samstave 15 hours agorootparentMay I please understand your arch; a dev builds an app itto your API and you spit it back out? - if so - ensure when you spit out whatever it defines itself to whomever is listening.... -- Plz explain the arch of how your system works? (or link me if I missed..) ---- Shortest and most importnat law ever written: \"an AI must identify itself as AI when asked by Humans.\" 0. Law of robotics. ------ @autsin - Cool - so im on an important call with [your customer] your system has an outage? How is this handled? dropped call? (I am not being cynical - im being someone who is allergic to post mortems. ---- EDIT: you need to stop using the term \"user\" in anything you market or describe. full stop. the reason: in the case of your product, the USER is the motherhecker on the phone listening to anything your CUSTOMER is spewing at them VIA your API. the USER is who is making IRL *>>>DECISIONS<<<* based on what they hear from your system. Your CUSTOMER is from whom you receive money. THEIR customer, is whom they get money to pay you. The USER is the end-point Human. who doesnt even know you exist. reply AustinZzx 11 hours agorootparentNice catch on the working -- customer is indeed more accurate than user. For outage handling: we strive to keep up 99.9 plus up time, and in the case of a dropped call, the agent would hang up if using phone, and might have different error handling in web depending on how customer handles it. reply AustinZzx 15 hours agorootparentprevWe handle the audio bytes in / out, and also connect to your user's server for response. We handle the interaction and decide when to listen and when to talk, and send live updates to our users. When a response is needed, we ask for it and get it from our user. Our homepage https://www.retellai.com/ has a GIF on it that illustrates this point. reply dang 19 hours agoprev [13 more] [stub for offtopicness] reply threeseed 20 hours agoparentWhy is every comment here from an account with no other comments ? reply dang 19 hours agorootparentUgh. Sorry. Probably some of their users found out about this thread. I'm going to move all of this to an offtopic stub and collapse it. We tell founders to make sure this doesn't happen (see https://news.ycombinator.com/yli.html) but I probably need to make the message louder. Not everyone understands that the culture of HN doesn't work this way. reply langyou 20 hours agoparentprevAmazing, tried the dental front desk from playground. The voice sounds very natural and could hardly tell it's AI-generated. reply yanyan_evie 20 hours agorootparentglad you like it :) reply productlordtr 20 hours agoparentprevDo you hire? reply yanyan_evie 20 hours agorootparentThanks for asking. We are not hiring at this stage. reply Xavier_L 20 hours agoparentprevCool! reply yanyan_evie 20 hours agorootparentThank you! reply xiangshu 6 hours agoparentprevreally good reply 369316020 20 hours agoparentprevVery cool reply liangludev 20 hours agoparentprev [–] cool reply yanyan_evie 20 hours agorootparent [–] thank you! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Retell AI is a startup providing a conversational speech engine for developers to create natural-sounding voice AI, simplifying AI voice conversations with speech-to-text, language models, and text-to-speech components.",
      "The product offers additional conversation models for enhanced conversation dynamics, a 10-minute free trial, and flexible, usage-based pricing, catering to both developers through an API and non-coders via a user-friendly dashboard.",
      "The founders seek user feedback and are excited to witness the innovative applications users develop with their technology."
    ],
    "commentSummary": [
      "The discussion covers diverse AI voice technologies like Retell AI, AI voice agents for various sectors, AI bots for customer support, and AI voice agents for crisis intervention and therapy.",
      "Topics include pricing, performance, potential applications, and ethical considerations of these technologies.",
      "Participants contribute feedback, improvement suggestions, affordability concerns, and ideas for advancing AI voice technology."
    ],
    "points": 284,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1708521493
  },
  {
    "id": 39460148,
    "title": "Atuin: Sync, Search, and Secure Your Shell History",
    "originLink": "https://atuin.sh",
    "originBody": "Making your shell magical Sync, search and backup shell history with Atuin /bin/bash -c \"$(curl --proto '=https' --tlsv1.2 -sSf https://setup.atuin.sh)\" Get started Trusted by engineers at Proudly open source 15K+ GitHub Stars 160+ Contributors 70M+ Synced history Features What you get with Atuin Shell history sync Sync your shell history to all of your machines, wherever they are End-to-end encryption All data is encrypted, and can only be read by you Efficient search Search decades of shell history, and recall it in an instant. Atuin offers configurable full text or fuzzy search, filterable by host, directory, etc. Open source Atuin is open source with a permissive license, and has a growing community Data import Bring your existing history with you - Atuin supports importing from a wide variety of formats Store extra context Atuin stores extra context with your commands - working directory, exit code, and more! Accelerate your productivity in 2 minutes Step 1: Run the install script Run our install script to get setup with the Atuin binary and shell plugin Step 2: Register Optionally sign up for Atuin Cloud Sync, or self-host your own sync server Step 3: Search Enjoy enhanced ctrl-r, and keep your history forever FAQs Frequently Asked Questions Can you read my shell history? No. Atuin is fully end-to-end encrypted. Without your key, nobody can see a thing. Read more about our encryption here! Do I have to register? If you would like to sync your shell history, registration is required. Otherwise, you can use Atuin locally as a fully-offline enhanced history search tool What if I still don't trust your server? If you would rather operate + maintain your own sync server then we have a guide here! Atuin supports self-hosting. What technology does Atuin use? Atuin is written in Rust, and stores your data in SQLite. Both technologies are known for being fast and reliable What shells can I use Atuin with? Atuin currently supports Bash, ZSH, Fish and NuShell What if I have more questions? Our docs go into much further detail, but if your question is still not answered then please do feel free to drop in on our Discord Atuin Making your shell magical Get started",
    "commentLink": "https://news.ycombinator.com/item?id=39460148",
    "commentBody": "Atuin – Magical shell history (atuin.sh)267 points by tambourine_man 12 hours agohidepastfavorite83 comments berkes 10 hours agoI love how it looks and I love some of the fancy features. But above all, it's a solution to two real problems I had. I work in several terminals. Sometimes tabs or Windows in my emulator, sometimes screen or tmux. And all those sessions would overwrite eachothers history. I lost many actual important history entries that way. And I (almost) ran many wrong commands, expecting another one to be my last entry. Arrow-up enter. Woops. Atuin solved this. I had my history as large as possible. Ctrl-r is my friend to search for \"that old thing I did to pgsql piping something from zcat a year ago\". But it's ever slower, zsh and bash history clearly not designed to handle (tens of) thousands of entries. Atuin solved this. reply tupolef 4 hours agoparent- For your 1st issue, you can setup bash to append commands rather than overwrite them. Here the part of my .bashrc about history: # append rather than overwrite shopt -s histappend # attempts to save all lines of a multiple-line command in the same history entry shopt -s cmdhist # with cmdhist, saved with embedded newlines rather than semicolon separators shopt -s lithist HISTCONTROL=ignoreboth HISTSIZE=10000 HISTFILESIZE=20000 HISTTIMEFORMAT=\"%y/%m/%d %T \" HISTIGNORE=\"history:ls:l:pwd:exit:\" if [[ ${BASH_VERSION:0:1} -gt 5 || ${BASH_VERSION:0:1} -ge 5 && ${BASH_VERSION:2:1} -ge 1 ]]; then PROMPT_COMMAND=(\"history -a\" \"history -c\" \"history -r\") else PROMPT_COMMAND=\"history -a; history -c; history -r\" fi - And about the 2nd issue, you should use fzf or skim (probably faster) to replace the ctrl-r binding. fzf includes such a script that you just need to call from .bashrc. # Enable key bindings for fzf if command -v fzf 1> /dev/null && [[ -f \"/usr/share/doc/fzf/examples/key-bindings.bash\" ]]; then source /usr/share/doc/fzf/examples/key-bindings.bash fi reply sjagoe 2 hours agorootparentI had been using the history append for years before finding atuin a couple years ago. I still occassionally lost history through some combination of events I was never able to understand. I've been using atuin happily for a few years now and it blows bash history out of the water. reply n8henrie 5 hours agoparentprevInteresting -- I have histappend (? I think) enabled and no problems with 20+ simultaneous tmux panes + tabs. Just checked yesterday* and have well over 100k lines in bash history and it loads / searches instantly in skim. * Was trying to check out mcfly vs atuin for the third time and again didn't understand why the extra complexity was worthwhile. reply wyclif 43 minutes agoparentprevLooks like whoever created the examples uses LunarVim. reply gitinit 8 hours agoparentprevAgreed! This is my exact use case for Atuin and it works great, except it replaces the default, minimal control-r interface. reply dang 1 hour agoprevRelated: Atuin replaces your existing shell history with a SQLite database - https://news.ycombinator.com/item?id=38936102 - Jan 2024 (10 comments) I quit my job to work full time on my open source project - https://news.ycombinator.com/item?id=38935205 - Jan 2024 (176 comments) Atuin replaces your existing shell history with a SQLite database - https://news.ycombinator.com/item?id=35839470 - May 2023 (193 comments) Atuin replaces your existing shell history with a SQLite database - https://news.ycombinator.com/item?id=35688117 - April 2023 (1 comment) Atuin for zsh shell history in SQLite - https://news.ycombinator.com/item?id=31177943 - April 2022 (1 comment) Atuin, improved shell history with multi-machine sync - https://news.ycombinator.com/item?id=27079862 - May 2021 (9 comments) reply wswope 11 hours agoprevAtuin has been the absolute best CLI tool I’ve found in years. I use it every single day that I’m at a computer. It's very easy to learn (2 or 3 essential hotkeys), and makes finding old shell commands a breeze. I was able to self-host the sync server in well under an hour, start to finish. (@ellie - if you see this thread, thanks for all the elbow grease you put in! You’ve built something really special.) reply mac-attack 9 hours agoparentAny links in the self-hosting options? I had some saved but lost them. reply Astraco 6 hours agorootparenthttps://docs.atuin.sh/self-hosting/server-setup/ reply kstrauser 10 hours agoprevWhy is this better than a shell's built-in history, possibly paired with fzf? I'm not asking sarcastically, either. Is this something awesome I should be using? What about it is going to blow my mind? reply cstrahan 9 hours agoparentI've recently switched from zsh's built-in history + fzf, so perhaps I can answer. First, a bit more about my zsh+fzf use: I use `share_history` so that my history is available across shells, allowing me to quickly find and re-run commands. (https://zsh.sourceforge.io/Doc/Release/Options.html#index-SH...) Sometimes I want to switch back to some terminal tab/window and find a command that I nkow I ran in that session, but now it's flooded with 1000+ commands that I've entered elsewhere, making it difficult to find what I'm looking for. So the shared history is both useful and a hindrance, depending on the scenario. Compare with atuin: I can filter by commands entered specifically within the current shell, or across all shell sessions, and possibly even across other hosts (when using atuin's sync functionality -- haven't tried that yet). Given the metadata that atuin collects (current working directory, start time, exit status, duration, etc) I'm sure there are some other clever things that can be done with my shell history now, vs the the \"line per entry, with optional start timestamp\" approach used by zsh's builtin history functionality. Actually, more on that first point: you can also filter by commands entered within the current directory. reply moribvndvs 6 hours agorootparentI’m a largely happy fzf+history user, but I think filtering across hosts is the killer app I was missing with atuin. reply probotect0r 10 hours agoparentprevThe features listed on the site already tell you what it has that regular shell history + fzf doesn't have. You can decide if that makes it better. reply kstrauser 9 hours agorootparentAll that jumps out to me from that page is sync, which is nice, but not life-changing. I guess I’m looking for more of a personal experience anecdote, where someone could explain how its other features made it worth signing up for another sync service and learning a new thing. It may be the best thing since sliced bread but that page doesn’t say why I should try it. reply joh6nn 9 hours agorootparent- all the data is stored in a sqlite db, so it's actually structured, as opposed to just being in an adhoc text format - this matters because it's capturing not just what commands were run, but metadata about the commands as well. Of particular interest to me is that ot captures the start and end time for every history entry so you can get timing data even if you forget to use `time` - it provides much more fine-grained control over what makes it into the database than shells provide for controlling what makes it into their built-in history. For example, it has built-in support for filtering out common kinds of secrets/tokens/etc. It's also in active development with a friendly and helpful creator/project lead, an active community, and an ever growing feature set. I'd recommend giving it a try; as far as I'm concerned, it's best-in-class for managing shell history reply alchemist1e9 8 hours agorootparent> Of particular interest to me is that ot captures the start and end time for every history entry so you can get timing data even if you forget to use `time` That’s potentially an incredible useful feature to integrate with all that metadata! Very very interesting. reply perrygeo 9 hours agorootparentprevFair point, I've been using atuin for a few years on a few machines but never once felt the need to set up sync. I forgot sync was even a feature! It's certainly over-emphasized on the web page. I use it mainly because a) it stores data about command history in a proper database that's easy to query (what century is this? How do other tools justify dumping what should be structured information into some god-awful mess of semi-structured text files?) and b) it makes a clean separation between SESSION, DIRECTORY and GLOBAL scopes which means you can explicitly use a recency or a locality filter (I don't want irrelevant commands cluttering my history). Even without sync, atuin is a gem. reply samwho 9 hours agorootparentprevFor me, syncing across machines and other shell sessions was enough to justify shelling out the $0 and 5 minutes it took to install and learn the tool. reply m-GDEV 9 hours agorootparentprevI just watched this video: https://youtu.be/WB7qojkkVVU?si=_sqnKlXrblLSwrs8 and there still seems to be nothing that stands out about atuin. I do think it might be easier to work with the data in a database. Personally I don't see a reason to use this service . reply cstrahan 9 hours agorootparentAs I commented elsewhere, you can: - Filter by commands run within the current shell session - Filter by commands run within the current working directory - Filter by commands run across hosts (as opposed to filtering commands run on your local machine) - Filter by commands run within the current shell session - All of the above searching functionality, with nice fuzzy finding support, time stamps, etc. Before atuin I used zsh's builtin history, with ctrl+r rebound to present that builtin history through the `fzf` fuzzy finding tool, and zsh configured to share history across shells. The deficiencies I found: I couldn't optionally filter by commands _only_ run in the current shell, I couldn't filter by commands run in the current directory (useful for quickly finding commands I often need to re-run for a given project), and I can't search for commands run across hosts. If you don't find yourself valuing these things, you may find that you have little to gain from using atuin. reply shepherdjerred 5 hours agoparentprevIt works for multiple shells and synchronizes history between devices. reply colordrops 9 hours agoparentprevYou could probably make shell history + fzf do a lot of what atuin does, but you'd have to write a bunch of glue code to make it happen. That's what atuin is doing. I particularly like the high configurability as well as the ability to switch between host, global, session, and directory history. reply yagizdegirmenci 9 hours agorootparentI have a one-liner zsh function that does the same thing, been using it for years, too powerful for me to switch it, and probably has better search capabilities, thanks to fzf. function hist() { print -z $( ([ -n \"$ZSH_NAME\" ] && fc -l 1 || history)fzfsed -E 's/ *[0-9]*\\*? *//'sed -E 's/\\\\/\\\\\\\\/g') } reply cstrahan 9 hours agorootparent\"I have a one-liner zsh function that does the same thing\" I think you mean \"I have a one-liner zsh function that does a tiny subset\". It's lacking: - The ability to filter by current directory - The ability to filter by current shell session, instead of across all shell sessions (that's assuming you use zsh's shared_history; if not, then the opposite is true: you can only search within the current shell session. See https://zsh.sourceforge.io/Doc/Release/Options.html#index-SH...) - Search history across hosts Until recently, I used zsh+fzf, with the default ctrl+r binding replacement provided by fzf. It's been great, but it has lacked functionality that I've wanted for a while now. Atuin fills in these gaps for me. reply theshrike79 2 hours agorootparentprevNow use your function to give you a list of all find commands that took over 10 seconds to run. Atuin stores _everything_ it can about each command run, what you see when you press C-r is only a tiny subset. And even it gives you the duration and success/failure information immediately. If you want to, try pressing C-r, select a command from history and press C-o. Normal shell history doesn't store any of that. reply deng 2 hours agoprevThe page claims this is \"trusted by engineers\" at Google, MS, Apple, Amazon, you name it. Could maybe some engineer at these companies enlighten me how you could convince your security team that it is fine that all your shell commands are streamed to an outside server? Yes, I know it's E2E, but still, without a proper audit, my security department here would laugh me out of the room if I'd ask for this. Do you all self-host the server in-house? If so, did you do a code review of the server code? reply jianfgo 2 hours agoparentI don't think Google allows it and AFAIK, Google has its own tool to sync all bash histories. reply RyEgswuCsn 1 hour agoparentprevOr they could mean \"used by engineers who work at Google (for their non-work related projects)\"? reply deng 1 hour agorootparentMaybe. It's open to interpretation, which is the problem. If the author of Atuin maybe sees this: While this not (yet?) a commercial project, it is highly problematic to advertise your product like this. You cannot just put the logos of companies on your front page without permission, even with that carefully worded caveat in front. At the very least, this can lead to a C&D. reply verticalscaler 11 minutes agorootparentA problem for whom? How do you know he didn't get permission? I work/ed for some of the companies on the list and endorse his web page. The turtle logo is cute, seems fine to me. reply cschmatzler 2 hours agoparentprevThe server is optional and opt-in, stuff is saved to a local SQLite database by default. reply deng 2 hours agorootparentIn \"What you get with Atuin\" the first(!) point is \"Sync your shell history to all of your machines, wherever they are\". That obviously cannot work with a local SQLite. Without syncing, this is a better ctrl+r, which I won't disagree is nice, but not the real point of this software. If the page says this is \"trusted\" by pretty much all major software companies, I would assume this includes syncing, which is the main feature. Also, even the possibility that the software would send this to the outside would make this impossible to use at my company, and I don't think we are overly strict in that sense here. reply sealeck 2 hours agorootparent> Without syncing, this is a better ctrl+r, which I won't disagree is nice, but not the real point of this software. Why not? I use it without syncing just for the search functionality which is really useful to me and saves me quite a bit of time. I'm sure the syncing is useful but I don't care about my shell commands enough to want them synced across all the machines I use. reply deng 2 hours agorootparentI can only repeat myself: the syncing is advertised as the main feature, it's the USP. There are plenty of other tools that improve ctrl+r. reply thih9 1 hour agorootparentNote that not syncing is also an advertised use case. > If you would like to sync your shell history, registration is required. Otherwise, you can use Atuin locally as a fully-offline enhanced history search tool reply kuschkufan 48 minutes agorootparentprevWhat part of \"cloud sync is opt-in\" did you not want to understand? Appearantly all of it. It does not matter if you think that the website does a bad job of explaining that fact. Rest assured, that your sentiment, that no employee should be using autin (or any locally installed 3rd party software, really) before a proper audit of the code has been done is understood just by your first comment. A valid opinion anyone can hold. reply kkirsche 2 hours agorootparentprevMany large companies suffer from the concept of shadow IT. The use of software and services that aren’t blessed by the company to accomplish tasks that are blessed. As someone in security at a large company, I expect this is a matter of not every company has people who follows rules. I know I’ve seen and know, even within security orgs, plenty of people who don’t follow the rules because a few bad rules makes them feel that other important rules are also bad. It’s pretty simple to bypass the software companies use to “enforce” the rules reply eigenvalue 7 hours agoprevI've been using this for a couple months after using McFly in zsh for over a year. I think I can say that I like it better, but the only thing I sometimes don't like is how it completely takes over your screen when you press the up arrow. I usually just want to get the previous command I entered instead of the whole searchable history (which I use Control-R to bring up), and it's really jarring to have the entire screen change. Aside from that, it's really awesome to have everything synced across a bunch of machines. reply jiehong 6 hours agoparentSame here, but you can disable that behaviour, and then it’s nicer [0]. I also find that I prefer the default Ctrl-R to start with history from the current directory instead of the global history, but that is also changeable. So I’m pretty happy. [0]: https://docs.atuin.sh/configuration/key-binding/#disable-up-... reply eigenvalue 5 hours agorootparentAwesome, thanks for the tip, I’ll try that. reply theshrike79 2 hours agoparentprevThis is why I have atuin init fish --disable-up-arrowsource in my fish.config. C-r with Atuin is great. It taking up the whole screen when I want to run the previous command again? Not grat. reply Arcuru 6 hours agoparentprevIf you're comfortable with shell scripting, you can get that today. There's code for a number of shells in this issue https://github.com/atuinsh/atuin/issues/798 Adding it to Atuin will take a bit of work to keep it from being too brittle, so it hasn't yet been done. reply mwpmaybe 3 hours agoparentprevYou can adjust the size of the \"window\". I have my inline_height set to 20 and it's a bit less jarring when it pops up. reply vander_elst 23 minutes agoprevatuin is nice, but the search functionality is not on par with fzf, I found myself several times going `cat ~/.zsh_historyfzf` reply jeppesen-io 10 hours agoprevI switched a year or two ago It wasn't until I switched that I realized how poor bash and zsh are in comparison, even with fzf 10 min install, never looked back Not to mention the safety. Never could figure it out, but once every 15 months or so, zsh would disappear my history. Certainly a mistake on my end, but still too easy to blow away history reply zimbu668 8 hours agoparentI would occasionally manage to start a bash shell without executing my profile, which means HISTFILESIZE is not set and bash would truncate my history on the first command I ran. zsh history may have a similar problem. reply eviks 4 hours agorootparentYes, bad defaults strike again! reply joobus 8 hours agoparentprevThis is exactly why I'm considering using atuin also. My zsh history on my mac will sometimes just completely disappear and then I have to go lookup what I was trying to do again. It is driving me crazy. reply alchemist1e9 8 hours agorootparentThis happens to me also!! Does anybody know why? I ignore it as macos is a secondary and less critical environment for me but it’s really annoying. reply hiAndrewQuinn 4 hours agoprevAtuin is great! Considering including it in my little stack of scripts to turn a new Ubuntu VM into a capable devbox from scratch [1]. We currently install `fzf` with the proper keybindings, so Ctrl-R is already quite nice. But we also already include one interactive \"pick your $EDITOR\" moment - there's no reason we couldn't provide a \"pick your $HISTORIAN\" with atuin vs fzf as well. It would be fun and might turn people onto new tools, which is a big aim of this project. [1]: https://github.com/hiAndrewQuinn/shell-bling-ubuntu reply javrin 6 hours agoprevBeen using atuin for a couple months now on Windows in bash. Its a very nice tool however I've been experiencing a huge lag sometimes after typing just one character. Intuition says there could be the database performance is lacking or Im missing some configuration. Anyone else experiencing something similar? reply vladvasiliu 1 hour agoparentYup, I'm getting this sometimes on Linux. There's been a recent update (not sure which version, I don't have my Linux box on hand to check) where it now started logging some timeout waiting for a lock on the database. I don't type commands in multiple shells at the same time, so not sure what's locking the db. I also don't use remote sync (explicitly disabled in the config). reply from-nibly 6 hours agoparentprevCould be a disk Io issue. I know git used to use a nieve approach on windows and it make git incredibly slow. Something you wouldn't notice just using git but when you made it part of your prompt it would lag for a good second and a half just pressing enter. reply v3ss0n 3 hours agoprevResh is a lot faster and lightweight. https://github.com/curusarn/resh reply mirashii 10 hours agoprevI mostly really like atuin, but it has been causing some random other breakage in my shell that has me on the verge of replacing it with something simpler. A quick search comes up with https://github.com/atuinsh/atuin/issues/1696 , which indicates I'm not alone. reply Arcuru 8 hours agoparentI've now explained in that issue why it's broken, why it's probably unfixable for atuin, but why replacing fc with an equivalent feature in atuin sounds pretty doable. Feel free to open a feature request before I do. reply airstrike 9 hours agoparentprevThat bug report is 12 days old... reply arcanemachiner 8 hours agorootparentAnd some bug reports are 20 years old. A broken workflow is a broken workflow. reply heavyset_go 3 hours agoprevIn the docs it says: Atuin works best in bash when using ble.sh. However, whenever I try to use it with ble.sh, it doesn't bind to the up key like it does with plain Bash. reply dphuang2 11 hours agoprevRecently installed Atuin and found it to be overall net positive. A couple of habitual keystroke combos had to be reprogrammed from my brain but the persistence of shell history + better searching is pretty awesome. Also the stats are a neat add-on. Overall would recommend to almost any developer. reply magospietato 11 hours agoprevReally impressed with the self-host setup. I've found these are usually a bit of an afterthought, involving a bunch of service dependencies and an arcane config process. Postgres 14 and a few tweaks to a TOML file and you're running. Not wild about the password-only Postgres connection though. reply eviks 4 hours agoprevCan it do the magic of deduplication, cleaning failed commands with typos so your history isn't cluttered, and use frecency for autocompletions? reply Ringz 3 hours agoparentDeduplication as an option or shortcut would be nice. But I think Atuin should and maybe must save all commands in the database. reply eviks 1 hour agorootparentWhy should it store garbage in the database? reply veganjay 8 hours agoprevPrevious announcement by the author: https://news.ycombinator.com/item?id=35839470 reply jes 8 hours agoprevLooks neat and I'd like to try it. I wonder how well it would work from within a GNU Emacs shell buffer (comint based, I think) running bash. Has anyone given this a shot? reply theshrike79 2 hours agoparentBash in general is problematic for Atuin, since it plugs into the prompt and Bash prompt is a bit crap =) But it should still work reply hiddencost 10 hours agoprevA'tuin the world turtle, from disc world: https://discworld.fandom.com/wiki/Great_A%27Tuin reply Hugsun 10 hours agoprevI just started using this tool locally and it's pretty nice. The default UI doesn't highlight the matching substring like fzf does which isn't as nice. reply tnorthcutt 10 hours agoprevAtuin is simply fantastic. Ellie keeps adding interesting new features. Just an all around fantastically useful tool. Bravo. reply metadaemon 6 hours agoprevUsed as a daily driver for a couple years now. Love it!! reply Razengan 3 hours agoprev+1 for the name reply renewiltord 9 hours agoprevI used to have histdb set up really well with r-i-search etc. but recently I can't get it to really do that any more. It's a pity. Does anyone have a histdb + fzf r-i-search solution that works well for them? reply givemeethekeys 8 hours agoprevAnyone try this and fish shell? How do they compare? Thanks! reply theshrike79 1 hour agoparentFish + Atuin (with up arrow disabled) + Starship is what I use on every platform I can reply Arcuru 6 hours agoparentprevThe only downside is that fish has no way to allow atuin to override the autosuggestions as you type. We can get that in zsh but fish doesn't expose that. reply spindle 6 hours agoparentprevI've tried it with fish and found that, for me, it didn't add much and wasn't worth the switch, but it is arguably slightly better than fish's built-in history; so I'd say only bother if you particularly love learning new things of this type. reply Starlevel004 8 hours agoparentprevI use it with Fish. It's basically a nice UI around doing historyrg pipes. reply metadaemon 6 hours agorootparentAgreed, I use this in conjunction with Starship [1], both initialized specifically for Fish in the config. I love this shell so much. [1] - https://starship.rs/ reply jauntywundrkind 10 hours agoprevPlease oh please, can someone hack a way to get shell history expansion!? The most objectionable thing about shell history to me is that it's not what I typed! Mainly a zsh user & I've looked up & down & all around, but I cannot find a way to keep track of what I actually typed, versus what got ran! I really want to better be able to identify patterns, look at what I was typing. Show me !-1$, the last word of the last command! I need this to learn & improve my expansion capabilities better. I need this to see how my history evolved, to backtrack & see where else any given line was pointing at. Shell history feels devoid of the most important context I craft; I summon birds from hats and my shell history just says: there was a bird here. Hiss boo. reply chanbam 9 hours agoprev [–] just commenting for my discworld homies reply viksit 9 hours agoparent [–] i came here for this reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Atuin is a tool for syncing, searching, and backing up shell history on various devices, offering encryption, search efficiency, and additional context storage for commands.",
      "Written in Rust, Atuin supports Bash, ZSH, Fish, and NuShell, utilizing SQLite for data storage, allowing users to self-host their sync server.",
      "Registration is necessary for history sync, but Atuin can function offline as a search tool, attracting users with enhanced history search features and a supportive open-source community."
    ],
    "commentSummary": [
      "Atuin is a CLI tool that upgrades the default shell history by utilizing a SQLite database for better command history organization and search capabilities.",
      "Users can filter commands by various criteria, sync history across devices, and customize the tool to boost productivity.",
      "Mixed opinions exist on the syncing function, security worries in corporate settings, and a desire for features such as shell history expansion."
    ],
    "points": 267,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1708552574
  },
  {
    "id": 39452543,
    "title": "Pijul: Fast, Scalable, and Merge-Correct Version Control",
    "originLink": "https://pijul.org/",
    "originBody": "Home Downloads Documentation Blog Community FAQ Contact us Free hosting Pijul Pijul is a free and open source (GPL2) distributed version control system. Its distinctive feature is to be based on a theory of patches, while still being fast and scalable. This makes it easy to learn and use, without any compromise on power or features. Why? Commutation In Pijul, independent changes can be applied in any order without changing the result or the version's identifier. This makes Pijul significantly simpler than workflows using git rebase or hg transplant. Pijul has a branch-like feature called \"channels\", but these are not as important as in other systems. For example, so-called feature branches are often just changes in Pijul. Keeping your history clean is the default. Merge correctness Pijul guarantees a number of strong properties on merges. The most important one is that the order between lines is always preserved. This is unlike 3-way merge, which may sometimes shuffle lines around. When the order is unknown (for example in the case of concurrent edits), this is a conflict, which contrasts with systems with \"automatic\" or \"no conflicts\" merges. First-class conflicts In Pijul, conflicts are not modelled as a \"failure to merge\", but rather as the standard case. Specifically, conflicts happen between two changes, and are solved by one change. The resolution change solves the conflict between the same two changes, no matter if other changes have been made concurrently. Once solved, conflicts never come back. Partial clones Commutation makes it possible to clone only a small subset of a repository: indeed, one can only apply the changes related to that subset. Working on a partial clone produces changes that can readily be sent to the large repository. Where to find it? Pijul is bootstrapped, meaning that we use it for its own development. See the repository there: https://nest.pijul.com/pijul/pijul © 2016-2024 Pierre-Étienne Meunier and Florent Becker pijul_org",
    "commentLink": "https://news.ycombinator.com/item?id=39452543",
    "commentBody": "[dupe] Pijul is a free and open source (GPL2) distributed version control system (pijul.org)249 points by thunderbong 22 hours agohidepastfavorite188 comments dang 14 hours agoThe current submission counts as a dupe since this topic had significant attention less than a year ago: Pijul: Version-Control Post-Git [video] - https://news.ycombinator.com/item?id=37094599 - Aug 2023 (163 comments) Other past threads: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply t43562 20 hours agoprevI used darcs before git and it was very nice - the first distributed RCS I'd ever used. I believe pijul is a follow-on to that so it should be great. Later on I used Mercurial and it was very easy with a great GUI and it felt safe. git is like shaving with a straight shaving razor - alarming until you've developed your set of frequently used commands and ways to stay out of trouble. Even I have to use git because who uses anything else? Every project that I know that used Mercurial dropped it in the hope of getting more contributions. We have all rushed like Lemmings into Github - now owned by those nice people at Microsoft - who have used it to automate our jobs with copilot. How ridiculous that Open Source has been embraced into the clutches of the Empire on such a scale! :-) I am joking but only a bit. reply zemo 16 hours agoparent> Even I have to use git because who uses anything else? It's pretty widely not used in the games industry because its support for media files is poor, it's pretty difficult to train artists to use it, the integration with game engines is pretty poor, git LFS is a layer of complication on a system that already has UX that's difficult for artists, git LFS hosting is an additional can of worms, there's essentially no file/directory-level permissions for managing contract work (and doing so with submodules+LFS is complicated). reply rendaw 15 hours agorootparentDo you know of any FOSS alternatives that handle that stuff better (specifically binary asset handling and permissions)? reply pmeunier 13 hours agorootparentPijul does binary files natively, actually. Permissions aren't hard to add, if anybody had a real-world case I'd be happy to help. reply zemo 10 hours agorootparentthe places where I've seen people leverage the folder-level permissions in Perforce (in the games industry) would be like: a contractor that provides art would only have access to the portions of the project that relate to the art they're working on but not the code; translators might have read access on one directory and write access on another directory. In academic settings I've seen Perforce used such that an instructor has read-write access on a directory and students have read-only access on the directory, and then each student has a subdirectory where they have read-write access and everyone else has read access, so that everyone can play each other's games. You can do stuff like this in git with submodules but it's somewhat complicated and difficult to teach to non-programmers. It's not really clear what the target audience is for the project. The mathematical/theoretical foundations are clear, but the target audience is unclear, so I'm not sure if these are use-cases that you consider to be within the project's scope or not; just sharing how I've seen the permissions models used in the games industry. oh and nearly everyone hates using perforce and its clients p4v and the p4 command line, so it’s not like there’s no appetite for change. There’s very much an appetite to see Perforce unseated, because it costs money and is also bad. reply zemo 15 hours agorootparentprevsome indies use Subversion, but Perforce is the industry standard. reply Zambyte 20 hours agoparentprev> Even I have to use git because who uses anything else? Git compatibility is a great way around this problem. I have been enjoying Jujutsu lately. reply adastra22 16 hours agorootparentNot really. The real innovation of a lot of these alternative DVCS systems is that they free the state of the source from being dependent on the history that got you there. Such that applying patches A & B in that order is the same as applying B' & A' -- it results in the same tree. Git, on the other hand, hashes the actual list of changes to the state identifier, which is why rebasing results in a different git hash id. So long as you require git compatibility, you're kinda stuck with git's view of what history looks like. Which is rather the point. reply sunshowers 15 hours agorootparentThat is definitely a tradeoff. On the other hand, the history of all human progress is a history of path dependence. Constraints spur creativity and Jujutsu works extremely well. (I've worked with the main author of Jujutsu before. Also, having worked on Mercurial for many years, I have my biases about how source control should work. Jujutsu's workflow is very similar to Mercurial's, with some rather stunning improvements on it.) reply steveklabnik 13 hours agorootparentprevNewbie jj convert here. jj is not patch based, like pijul, but snapshots, like git. A sibling commentor points out the change id stored by jj: it is true that at the moment, this isn't really exportable to git in a native way. However, there is a path forward here, and it may come to pass. Until then, systems like Gerrit or Phabricator work better with jj than systems like GitHub. However, all is not lost there either: tooling like spr[1] allows you to map between the two universes. At my job, at least one person was using jj for six months at work without any of the rest of us being the wiser. Some of the rest of us are trying it out. A really nice thing about jj is that you can use it without anyone else needing to, thanks to the git interop. 1: https://github.com/getcord/spr reply Zambyte 15 hours agorootparentprevJujutsu uses change IDs, which are an abstraction over commit IDs, that are stable over operations like rebasing. reply skybrian 16 hours agoparentprevThere are many people who work on open source professionally, but as a hobbyist programmer I find it odd to worry about Copilot automating my “job.” This is like complaining that your dishwasher or robot vacuum cleaner automates your job because you don’t do the work by hand anymore. If you’re getting paid then it makes some sense to worry about automation, but if you’re not, automation is good. You can do more. The community can do more. We can take on more ambitious projects. reply dbacar 15 hours agorootparentIf you are a hobbyist programmer, it is not your \"job\" . So it is not automating your job for the moment but the professional programmers' job, isn't it? reply skybrian 7 hours agorootparentYes, what I'm trying to get across is that open source programming is often not a job. It can become a job, but I want to leave room for it being ok not to be a job. reply Zigurd 17 hours agoparentprevThere is empirical evidence that git is diabolical: Git can be thought of as an experiment in how to make an API GUI-proof. There is no possible direct manipulation visual representation of git operations that doesn't dumb it down. reply rjmunro 34 minutes agorootparentI find gitx really helpful at understanding what's going on in git. Especially its ability to drag branches around the tree shows you that branches are just labels, they aren't the actual code. It doesn't do everything, but what it does do certainly isn't dumbed down. You definitely need a gui to do partial commits. git add -P is just horrible to use. reply ljm 15 hours agorootparentprevI content that magit, while not a GUI, pretty much transcends git’s CLI. GUIs will only ever tackle a subset of git’s functionality since if you’re already a power user you’ll be comfortable with the command line. reply Zigurd 12 hours agorootparentI'll give magit a try. But imagine if you had to use TEX to format anything more than trivial documents. It makes me want to get a Vision Pro and see if a direct manipulation git UI could access the full potential but with visual discoverability and safety. reply huimang 19 hours agoparentprevgit != github reply bastardoperator 17 hours agorootparentTo be fair, many git maintainers are or have been github employees and git is officially hosted on github. reply ccccccc1 19 hours agoparentprevbut you know git and github are not the same thing right? reply maleldil 17 hours agorootparentFor the vast majority of OSS projects, you're much better off using Git and GitHub in terms of drawing attention and attracting contributors. reply keybored 18 hours agoprevSo Pijul is patch-based. And not in the boring implementation-sense of storing changes as a series of patches. In the interesting sense that patches can be ordered in more arbitrary ways. On the other hand Git is snapshot-based. And Git was built for the Linux Kernel (specifically from the perspective of the lead maintainer). But my impression from the outside is that the Linux Kernel development is very “patch-based”. First of all in the sense of sending patches via email. But also in the sense that these patches can be applied to multiple trees (not just the Linus tree) and different subsystems have to cherry-pick commits (i.e. picking a commit and applying it as a patch, creating a new commit somewhere else). And it seems that people are concerned about tracking what patches are where, what commit (which can correspond to many commits according to cherry-picks) fixes whatever commit. The people behind Patchwork (a tool for maintainers which tracks patch series) is concerned about being able to track patches; the usual `git patch-id`, naming heuristics, etc. does not seem good enough. And as patches go through different contributors and maintainers they accumulate trailers in the form of signed-off-by and things like that. Even non-trailer (not key-value line) changelogs like: `[: fixed typo/ fixed off by one]`. And if the change is in the code then the patch content changes (not just the commit message). The Git project is also pretty patch-based in its development. Even for those who only cares about the maintainer tree: these patches apply to this other in-flight patch series; this diff here can be applied on top of your pending patch series; this patch series is from our ongoing work at Gitlab (company); I’m resending this patch series that X did three months ago but seemed to have abandoned; etc. reply danShumway 16 hours agoparentGenuine question, I have no opinion on this: My understanding is that a big part of the reason that Git is snapshot based is for reliability and for ease of checkout -- being snapshot based means Git doesn't need to replay commits every time you check things out. That likely comes with some downsides (cherry-picking does come to mind, yes), but also seems like some serious upsides, one of the biggest one being that simplicity. It's relatively easy for me to reason about what Git is doing under the hood. I like that I'm seeing that Pijul's patch operations are associative, I like at first glance what I'm seeing it say about merges, but it's not making it clear to me what the downsides are. Is the idea that arbitrary checkouts take longer, but that's generally fine since people don't do them very often? Or am I over-estimating how much complexity/performance costs that building a vcs like this would incur? reply adastra22 16 hours agorootparentThere's no reason you can't cache tree snapshots in a pijul/darcs setting. That's just an implementation detail. The difference is more in how a rebase or merge operation works internally, and how essential the particular history is to the current workplace state. Pijul does a better job of recognizing that \"A -> B -> C\" is the same as \"B' -> A' -> C\". reply danShumway 15 hours agorootparentOn first download/checkout then I assume I'd download the tree snapshot and it wouldn't replay the patches at all? Local caching would help with repeated checkouts, but most of the time I use Git I'm moving around, so I'm not sure how often I would benefit from that. That also seems like it would lose some verifiability for checkout integrity (?), but maybe that's not a big deal, I'm not sure in practice how much that matters for Git. Repo size also comes to mind on this if snapshots/caches are happening regularly/automatically, since that would mean Pijul is storing both the patches and the end states rather than computing the patches on the fly. But I guess snapshots wouldn't need to be automatic -- I'm not sure how often people actually check out arbitrary commits, maybe you could get away with only caching certain points? reply keybored 15 hours agorootparentprevThe original author sold it as a stupid content tracker. Which in part meant that the implementation was simple. There ain’t a whole lot to the first commit of Git. Some hundred lines which consists of manually preparing a commit by sending the current snapshot to the “cache” (nowadays “index”) which compresses it and then building a commit by specifying the parents and a commit message (like git-commit-tree but I don’t know if that was around back then). Beyond that I don’t know much. reply vim-guru 20 hours agoprevFunny timing of this post. This past weekend, I dedicated myself to creating a comprehensive set of tools for integrating Pijul seamlessly into Emacs. This includes collaboration features through org-mode, which I believe will be refreshing. I'm eager to share it with the community shortly. For fellow Emacs & Pijul enthusiasts, keep an eye out! reply magnio 20 hours agoparentFunny username :) reply vim-guru 20 hours agorootparentNot intentional; I used vim 10 years ago, but have been an emacser with vim-bindings for the past 7 I guess :) reply riwsky 18 hours agorootparentHow evil of you reply ecshafer 18 hours agorootparentprevThat is the best set up imo. Vim keybindings and composability are great, but Emacs being built on Lisp is a super power. reply bbkane 18 hours agorootparentIn a similar vein, it's been incredible to watch the Cambrian explosion of plugins since NeoVim introduced deep Lua integration and configurability. reply WolfeReader 19 hours agorootparentprevPersonally, I find it quite gratifying to see that a \"Vim guru\" is also an active Emacs supporter. reply pmeunier 12 hours agoparentprevAs probably the person in the entire world who has been wanting this for the longest, thank you! Please share on Pijul's Zulip, and ask for any help you may need. reply djha-skin 19 hours agoparentprevCould you give me a few paragraphs of usage report? How has it been to use the Pijul nest? reply zokier 16 hours agoprevI feel I'm missing something; for a version control system, you'd think managing different versions would be kinda central but in pijul that seems more like an afterthought. If I'm reading correctly you could use 'channels' same way as you use git tags, but that doesn't seem to be happening in practice. For example I look at 'sanakirja' project. The current version is 1.4.1 and the previous version was 1.4.0, and the version before that was 1.3.3: https://crates.io/crates/sanakirja/versions but if I look at the repository, there is no way to see any of that? https://nest.pijul.com/pijul/sanakirja There is \"Tags\" tab which is empty, and channel selection drop-down that has only \"main\" channel. So if I want to browse code at version 1.3.3, or see the difference between 1.3.3 and 1.4.0, or even just see what versions there are, how would I do that? To me those seem like very elementary questions, and yet reading through the manual I can find no hints toward this direction. There is a FAQ entry that seems relevant: > Is it possible to refer to a specific version? > Excellent question. Since Pijul operates on patches rather than snapshots, versions are essentially unordered sets of patches. How do we communicate a specific version number to one another? We solve this by abusing elliptic curve cryptography primitives. But its not clear at all how those \"version identifiers\" can be used or indeed if they are even implemented at all yet? At least based on the manual, no commands seem to take \"version identifier\" as argument, nor can I see them anywhere in Nest web UI. reply pmeunier 11 hours agoparentOops, don't look at what we do! These repos have been used for dogfooding and bootstrapping extensively, they have the worst structures and aren't good examples of nice, clean workflows. The tool isn't really \"experimental\" anymore, its repos are still used for heavy experiments. > But its not clear at all how those \"version identifiers\" can be used or indeed if they are even implemented at all yet? At least based on the manual, no commands seem to take \"version identifier\" as argument, nor can I see them anywhere in Nest web UI. `pijul log --state` does that, and various commands (tags, fork) do it as well. Tags are due for a redesign, since they can be made much more efficient with a really cool new design, and make Pijul a perfect hybrid between patches and snapshots. reply zokier 15 hours agoparentprevOh, apparently there is `pijul tag` command that might do something relevant here? https://nest.pijul.com/pijul/pijul:main/QL6K2ZM35B3NI.JIAAA but the docs do not say anything about that, so it remains a mystery. also its still weird that their own projects do not use those tags, or is it just that the web UI isn't showing them properly? reply ykonstant 22 hours agoprevI seem to recall very positive comments regarding pijul's underlying architecture/theory. However, as I don't have the headspace to delve into it, I would love a blurb at the beginner level that highlights the benefits of using pijul compared to other VCS, and a very brief and simple comparison between pijul and git (focusing on the differences). reply rkangel 21 hours agoparentI have the same question every time Pijul comes up on HN, and I have yet to get an answer: Can someone give me a real world example (person a makes X change, person b makes y change etc. etc.) that would work better in Pijul than Git? I am a complete believer on a sound underlying model producing better results for users at a high level, but I'm not clear on how it maps through for Pijul. I think that's what they're missing in the sell - the ability to explain to devs \"it will make your life easier in the following specific ways\". For example when getting my employer moved from SVN to Git I could talk to people about how much easier it was to create and then merge a temporary branch for a feature in Git. Git understand the topology of the history, knew the merge base and had better merge algos so the only pain you had was when there was an actual conflict - two people editing the same file location. It also could track renames, which were incredibly painful to merge in SVN. reply exDM69 21 hours agorootparent> Can someone give me a real world example (person a makes X change, person b makes y change etc. etc.) that would work better in Pijul than Git? Simplified example: Persons A and B check out master branch. Person A adds a.txt, commits and pushes. Person B adds b.txt, commits and tries to push and... 1) git will not accept the push because it's not on top of current master branch, person B needs to fetch and merge/rebase before pushing again. 2) pijul will accept the change (after a pull, but no rebase) because patches A and B are independent of each other and does not matter which order they are in the history (keyword: commutation). The value of Pijul will only start to show when you get into big three way merge scenarios. Which git users avoid like the plague because they are so nasty to deal with. Demonstrating this would need a much larger example. edit: clarification a pull is still needed in case 2, but no rebase or merge because there isn't one for commutative patches reply rkangel 20 hours agorootparentThank you for answering. For me \"not needing to pull before pushing\" is nice, but not game changing (but helpful to understand nonetheless). It's more the cases that you allude to that we instinctively avoid in git! reply adastra22 15 hours agorootparentYou've never maintained a long-lived git feature branch, it seems :) I maintain a project that is a slight modification of a very active upstream repo. The changes I maintain are rather invasive. Almost every upstream commit introduces a merge conflict with my changes. To keep myself sane I only merge/rebase when upstream releases a new version, but it still ends up sucking up a few weeks of my time every year. During those weeks, I look enviously at pijul where the conflicts would resolve down to a handful of corrections in context at the point of divergence, instead of gigantic merge conflicts obscured by thousands of piled on patches. reply carlhjerpe 16 hours agorootparentprevYou say that, but this applies to all cherry picking you can do between branches as well. As long as they don't conflict you're golden, and if there's a conflict you can commit a resolution that'll commute with your branch until you merge back into main. It would enable so many nice and less strict workflows to actually work if it ever got momentum, I've still got hope. reply dooglius 18 hours agorootparentprevThe git behavior seems greatly preferable here. As mentioned in other threads, the notion of commutativity here is very weak and counterintuitive; it only seems to cover the applicability of an auto-merge heuristic, not any actual notion of correctness or semantics, so a human is needed to review the merge and re-test before anything can be known safe for pushing upstream. If anything, git is too lenient in allowing auto-merges to take place that could in principle change semantics, and it ought to enforce a manual review stage for any merge, regardless of whether the auto-merge heuristic succeeded or not. reply exDM69 17 hours agorootparentWhen the contents has a conflict, git and pijul behave similarly. When the contents are identical, but the order of commits is different, git will conflict and require manual resolution. Pijul will not. As you say, neither will automatically check for correctness and you should run tests and CI when merging. Pijul just removes the manual work when there is no conflict in the contents but the history is different. reply pmeunier 11 hours agorootparent> When the contents has a conflict, git and pijul behave similarly. Not really: Pijul can record a conflict resolution as a patch, and apply it in a different context. Also, the conflict doesn't \"come back\", so you don't need extra hacks like rerere/jujutsu. > Pijul just removes the manual work when there is no conflict in the contents but the history is different. This is true, but could be confusing as our definition of conflicts isn't based on contents, but on operations, which is very different from Git (Git doesn't detect all conflicts). reply pmeunier 12 hours agorootparentprev> the notion of commutativity here is very weak and counterintuitive; it only seems to cover the applicability of an auto-merge heuristic This is completely false: in Pijul, any patches that could have been produced independently can be applied in any order without changing the result. There are 0 heuristics in Pijul, unlike in Git where even random line reshuffling can happen (there are examples in the \"Why Pijul\" section of the Pijul manual). Obviously, deciding whether a merge has the correct semantic is Turing-complete, and Pijul doesn't try to do any of that. reply hombrebuffalo 21 hours agorootparentprevDoes that only apply to adding new files? The changes in commit A could affect the behavior of the changes in commit B even if they are different files. reply exDM69 20 hours agorootparentNo, it applies to everything. If adding patches A and B (same or different files) will lead to same result regardless of which order they are applied, they are called \"commutative\" and pijul won't care which order they are in your history. It only tracks content of files, not semantic or behavior changes. reply ynniv 21 hours agorootparentprevgit can already do this as long as there isn't a conflict. Maybe pijul has better three way conflict resolution, but those can be risky, and avoiding those rare situations wouldn't offset the vast amount of tooling that got has. reply ajoberstar 20 hours agorootparentGit would make you merge or rebase, but yes there wouldn't be a conflict. They're saying Pijul would let you directly push without having to deal with the diverging histories. reply jacoblambda 20 hours agorootparentWhich tbh is a bad thing. Just because change a doesn't textually touch change b doesn't mean they don't interact. Unless your VCS is handling CI for integrating changes on push, you really need to pull down the upstream changes first and test them combined with your code before blindly pushing. reply exDM69 19 hours agorootparentI have clarified the comment above. A pull is needed before pushing. That pull does not need a merge or a rebase like git does because the order of commits A nd B does not matter (iff they are commutative). This gets a lot more useful when there are more than two patches to consider. reply ykonstant 20 hours agorootparentprevThat seems like a very important point; how does pijul deal with such \"effects at a distance\"? reply pmeunier 11 hours agorootparentBy not being a CI tool, nor claiming to solve such Turing-complete problems. Pijul has a theory of textual changes, but indeed doesn't care at all about what you write in your files: that's your problem! reply baq 20 hours agorootparentprevpijul has a lot of theory and engineering to figure them out, dealing with them better than git is one of the major reasons it exists at all, and darcs before it. reply thesz 20 hours agorootparent> darcs before it. Darcs is older than git. reply baq 20 hours agorootparentTrue. In my defense, 3-way merges are older :) reply exDM69 20 hours agorootparentprevYou kinda missed the point. You have two repos with different history: master -> patch a -> patch b master -> patch b -> patch a But the contents of the files are equal after applying both patches (in either order). Git will consider these to be two different, Pijul thinks they're the same. This is a simplified example. It only gets interesting when there are a lot of patches, some of which are commutative and some are not. reply ynniv 18 hours agorootparentWhat benefit is there to the VCS knowing that these histories are equal? That's valuable in verification or efficient binary patching, but I don't see how it matters in version control. When would I want to compare two repositories that were patched in different orders? reply Arelius 17 hours agorootparentImagine you are working on a patch heavy project. Like the Linux kernel. Where there are a lot of patchsets going around that are not in the mainline. You and I, who are both working off of main, and who have both separate merged in a few patchsets that are relevant to our shared module of interest. We can merge and compare our branches, and the differences in terms of nursing patches without having to be rigorous about reconciling or histories. reply TOGoS 18 hours agorootparentprevI'm guessing that it makes it easier to pick and choose between a bunch of patches. This is something I sometimes want to do in Git, but doing so requires a bit of planning. Have all the independent features branch from the same point, then, to 'assemble' them, do an octopus merge. If the VCS knows about dependencies between patches intuitively, it could free me from having to explain it, which in the case of Git, requires following procedures that I'm unlikely to convince any of my coworkers to follow (\"ohay, first, decide on the earliest point in the history from which this patch could make sense, rebase onto that....\") reply mpweiher 20 hours agorootparentprev> git will not accept the push because it's not on top of current master branch, person B needs to fetch and merge/rebase before pushing again. Hmm...that seems like a feature to me, not a bug. To me nothing of substance should happen in the repository, it should all happen in the local working directory. ¯\\_(ツ)_/¯ reply davorak 20 hours agorootparent> To me nothing of substance should happen in the repository, The idea is that with pijul nothing of substance would happen on the server in this example, it is the same process that would happen if you were doing it all locally. reply mpweiher 17 hours agorootparentHmm...so if something did need to happen, pijul would also reject and I would have to pull, do the local edits until everything is consistent and then push, just like git. So it's a low-impact optimization of the fast path? But actually, how does pijul know there are no conflicts? Textually? https://pijul.org/manual/conflicts.html Hmmm...yeah looks like it's purely textual. Er, no. There can be semantic conflicts that I need to resolve that do not conflict textually. The test suite needs to be green locally on my machine, and then we replace the Top of Tree wholesale with the code that passed the tests locally on my machine. So to me this feature of pijul is clearly an anti-feature, a bug, and the git behavior is correct. reply davorak 14 hours agorootparent> Hmm...so if something did need to happen, pijul would also reject and I would have to pull, do the local edits until everything is consistent and then push, just like git. Unlike git, pijul has first class conflicts, pijul unlike git does not reject in this example. My knowledge is limited, but from my testing that means the conflict exists in the history, at least if your merge style allows for that(similar to how in git you can choose to always rebase or use merge commits). The conflict is resolved with a new patch. I did not spot much in the documentation with a quick search but on the man page there is a small blurb on first class conflicts > First-class conflicts In Pijul, conflicts are not modelled as a \"failure to merge\", but rather as the standard case. Specifically, conflicts happen between two changes, and are solved by one change. The resolution change solves the conflict between the same two changes, no matter if other changes have been made concurrently. Once solved, conflicts never come back. - from https://pijul.org/ reply baq 15 hours agorootparentprevYou’re describing CI, not git nor pijul, nor any other version control system. reply bluepanda1234 20 hours agorootparentprevWas going to say exactly this, though I do wonder if it's just me being too set in my thinking around how I think version control should work. reply huimang 19 hours agorootparentprevThis is not really compelling. Pulling and rebasing before a push is standard workflow and lets you test with the new changes before pushing. reply WolfeReader 16 hours agorootparentThat may be standard in some shops, but definitely not all. I view anything that interferes with a push as a threat to the VCS, since it encourages developers to keep changes local and unavailable to their teammates. The only exception would be direct pushes to main. reply crabbone 16 hours agorootparentprevMerges are bad only when all parties involved edited the same code. There's no programmatic way to solve this problem. It's an administrative problem: someone has to decide whose code is the right one to use. If changes coming from both sources are independent, then rebase in Git is trivial as well, and there's nothing to be afraid of. reply shane_kerns 20 hours agorootparentprevI believe that even git would work in this scenario because a.txt and b.txt are separate and independent thus a rebase in git is not required. The point at which this becomes an issue is when both person A and B try to make changes to the same file and specifically the same blob of text within that file. I could be wrong but this should be simple enough to prove out because I've run into this situation before where I forgot to rebase before pushing my changes but git still accepted the changes as they were independent of the changes that person B made. reply jacoblambda 20 hours agorootparentgit wouldn't accept a push with conflicting changes on the remote but that's just because pushes are dumb (not bad, they just don't do anything fancy). The solution would be to pull upstream changes (so you know what you are potentially pushing your changes into) and then push. reply Arelius 17 hours agorootparentprev> thus a rebase in git is not required. A rebase is never required in Git. (people/maintainers may disagre) but a merge will always do. Having said that. In your scenario provided, a pull + merge/rebase will be required. It will then resolve automatically, and without conflicts. But a human has to be involved to provide a strict sequence/dag of the those commits. reply samatman 17 hours agorootparentprevThe thing I like most is that cherry-picks are real, not simulated. In git, a cherry-pick pulls the change you're interested in off a branch, making a copy in the process. If you later merge or rebase that branch, there are two versions of it in the history, this can have practical consequences, it's not uncommon for this to generate conflicts which wouldn't be there without the cherry-pick. In pijul, a cherry-pick is just one way to apply a patch. It's the same patch in both branches, so the history doesn't contain two versions of the change, only one. So there is no difference in the result between 'branch, cherry-pick, merge' and 'branch, merge'. Ever. reply speed_spread 19 hours agorootparentprevPijul eliminates the rift between rebases and merges. That in itself is an enormous gain over the confused git workflow or lack thereof. reply ParetoOptimal 18 hours agorootparentMost won't understand the gravity of this without a concrete example. reply 1oooqooq 18 hours agorootparentprevAnd what is it? couldn't find on the \"doc\"... is it all rebases or all merges? reply vlowther 18 hours agorootparentThat is a distinction that only exists because a Git repo is an ordered sequence of snapshots of your code that get translated to and from patches for human consumption. As far as I can tell, a Pijul repo is more of a giant dependency graph of patches that when merged together form a consistent snapshot of your code. reply baq 18 hours agorootparentprevMore like merges but actually neither… it can reorder patches, even conflicting ones, it’s a completely foreign concept in git. reply 1oooqooq 18 hours agorootparentsounds like rebase with force push remote. (well, every push is a force push in git. so just rebase+push. ...except on github if you have \"protected branches\") reply baq 18 hours agorootparentNot really, no; ordering of patches is not something you as a developer should care about. It’s behind the scenes. No force pushes necessary. reply 1oooqooq 12 hours agorootparentthanks. i will have to try it out. but it does sound like \"rewriting history\" in git et al. reply baq 41 minutes agorootparentThe point is there is no single history in the git sense - there are patches which commute, but their order is... irrelevant (if possible). reply Ygg2 20 hours agorootparentprev> It also could track renames, which were incredibly painful to merge in SVN. They are still a pain in Git as well. Rename a class and its usages in C# project, for example, and it randomly breaks based on some arcane heuristics. As for actual question you posed - partial checkouts. Your artist don't need to checkout code, but can work on it art folder. reply baq 20 hours agorootparentprevany time you rebase the same feature branch onto current main and solve the same conflicts pijul could probably do much better without hacks like rerere (which most people also do not use, so...) reply pmeunier 14 hours agorootparentprevHere's game-changing stuff: - There's an example in the \"why Pijul\" page of our manual where Git completely reshuffles your lines, and no \"custom merge algorithm\" could possibly solve it. I would be terrified if I were working on crypto/security code and I knew my VCS was doing that: https://pijul.org/manual/why_pijul.html - Patch commutation makes all big instances small: no need for submodules, partial/shallow clones, etc. Patch commutation lets you work on a small part of the repo by cloning only the patches you're interested in, and submit patches that mechanically commute with all patches on other parts of the repo. - Free cherry-picking: no need for strict disciplines, you can just introduce a quick fix on your local work branch, and push just that to production. When you're ready merging the rest, you won't have to solve conflicts again (no need for Git rerere/Jujutsu/…) - Many uses of branches reduced to \"just use patches\": many people, especially on fast-moving projects and \"early days\", don't really know what they're working on, and are dragged onto solving problems they didn't plan initially. Well, Pijul lets you focus on your work, then make patches, and then separate them into branches, thanks to commutativity. - Separation of contents and operations: this really feels like the CSS3/HTML5 of version control. In Pijul, patches have two \"detachable\" parts, a part describing what the patch does (as concise as \"I introduced 1Tb of data\", i.e. just a few bytes), and the contents (not concise: the 1Tb themselves). You don't need the data to apply a patch, so when working on large files, you can record 10 different versions, and your co-workers will only download the parts that are still alive after that. No LFS required! - Precise modeling of conflicts: conflicts are stored in our model, not \"recorded\" or \"artificially first class\". They're literally the core of our model, the initial theoretical motivation. Patches are where you need a good tool the most, and we record them and store your precious resolutions as actual patches, so they don't come back (no \"git rerere\" needed, and conflict resolutions can be cherry-picked). Now, we also have less game-changing things like: - Accurate and super fast \"blame\" (which we call \"credit\", and which doesn't require Pijul to look at the entire history like Git does). - Generic diffs, and therefore merge, i.e. not necessarily line-based. We haven't implemented them, but you could in theory implement AST-based diffs on top of Pijul. reply bondarchuk 22 hours agoparentprevhttps://news.ycombinator.com/item?id=39452312 This comment (that probably prompted this submission) gave me some idea at least. reply exDM69 21 hours agoparentprevIn day to day use, there's very little difference in the practical use of git vs. pijul. You edit files, commit then push. Of course the user interface is different and some of the terminology is different but it's still a distributed version control system. As for the differences, the advantages are listed right there on the front page: commutation, merge correctness, first-class conflicts and partial clones. Explaining these in more detail in a short example (\"a blurb\") is not really easy because you'd have to first set up an example three way merge (for example) and then study the behavior of git vs. pijul in detail. I recall seeing a video presentation from the Pijul authors which delved deep into this if you're interested. But I'll give it a go anyway... tl;dr: pijul can handle certain merge situations automatically where git requires you to manually resolve them reply vlowther 18 hours agorootparentMore than that. If you have ever had to fix a bug in code common to multiple maintained releases of a project, being able to apply the same patch to them all as its own thing instead of having multiple cherry-picked commits with identical content would be nice. reply grepknfss 16 hours agorootparentI think giving a patch its own identity is a pretty neat concept and clearly different than the git approach, so thanks for this example! reply psychoslave 20 hours agoparentprevI think that whatever VCS come today, it will have a hard time finding a broad adoption use case that overweight the massive footprint of git ecosystem. reply trklausss 21 hours agoparentprevThat sounds very much like something you could copy-paste on ChatGPT and get a somewhat good answer. That you definitely would have to double check because it might be that it hallucinates. reply nrabulinski 17 hours agoprevI tried using pijul as the main VCS for my projects I really did and the fundamentals are awesome, but last time I used it the UX was pretty terrible. The easiest and most immediate thing that’d address my issues would be git status-esque command, which has been in development for quite some time, but still hasn’t landed last time I checked. I wish pmeunier all the best and I’m glad to try out pijul again once it’s a bit friendlier. reply pmeunier 11 hours agoparentwe have `pijul diff -sU`, which is similar to `git status`. Also, feel free to contribute! reply Communitivity 18 hours agoprevI have used professionally Darcs, Mercurial, Subversion, and Git. I love distributed version control systems, and I love Git. However, Pijul looks interesting, nice work! Pijul has a number of stated strengths over Git, and looks like the version history is implemented as a CRDT from my first impression. Some of the statements make me wonder how many conflicts occur in real use. If they are equal to or less than the number it produces, great. If more than what Git produces, that's a barrier to adoption, as merge conflicts are one of the biggest pains when using Git. For me though what appears one of its biggest strengths (commutative workflow) is also its biggest drawbacks. There are so many teams that are trained in git workflow and are used to it, that getting them to switch to a completely different style of workflow in sufficient numbers will take years. Git compatibility is a key missing feature. Git had this with Subversion, and I think it's a big reason why Git won over many of the Subversion crowd. reply samatman 17 hours agoparentThe basic premise of Pijul is that the way they model changes leads to better merges, fewer conflicts, and (crucially) no cases of bad merges. Git can complete merges and do it incorrectly. https://pijul.org/manual/theory.html This means that in some cases, Pijul will correctly merge where any of the git merge strategies would create a conflict. It also means that in some other (rarer) cases, Pijul will generate a conflict where git would not: git would guess, in effect, and either get it right or get it wrong. I consider both of these things to resolve in Pijul's favor. The Pijul model also means that conflicts preserve some crucial state which can be used to resolve the merge. A conflict is modeled as a specific data structure, not as special syntax intruded into the source file. One example of this is that conflicts can in some circumstance be resolved by applying more patches, because the conflict is metadata about the file, it isn't data in the file which screws with subsequent state changes. reply hosh 15 hours agoparentprevWhat I am excited about Pijul is being able to maintain a personal codebase that can accept changes from multiple upstreams. I’m not looking at commercial teams as much as homelabs or local-first environments. reply baq 22 hours agoprevI'm watching Pijul from afar for years now. I'd love to see it get 1% of the investment Github gets... reply indigochill 21 hours agoparentIsn't it more Pijul vs Git than Git_hub_? There's a massive git community/ecosystem that has nothing to do with Github (e.g. Linux kernel development, at least last I looked). I've dropped off Github for personal projects since it's trivial to run my own git repo and git version control is built into _everything_ by default, but there are definitely annoyances I've encountered while using git that sound like they would be non-issues in Pijul, so on that level it piques my interest. On the other hand, I would want to be able to do two things before I'd seriously consider switching: 1. Self-host Pijul repos with some web interface similar to e.g. Forgejo/Gitea/Gitlab/etc (i.e. not just source control, but also a bit of project management and CI). Looks like Pijul is developed on something called \"The Nest\" which looks basically close enough, but the source code for that isn't public yet. 2. Install a plugin in an IDE that would let me leverage it inside the UI (with in-IDE conflict resolution) reply conradludgate 20 hours agorootparentTesting Git vs Pijul is hard unless you have other people to work with. I've wanted to use Pijul and other VCS tooling for ages, but it's pointless unless I can try with other contributers. So the hub does matter reply groovy-lizard 21 hours agorootparentprevThe 'hub' version of Pijul is The Nest, and exploring it I found this vs-code plugin for Pijul integration: https://nest.pijul.com/GarettWithOneR/pijul-vscode reply davidsergey 21 hours agoparentprevGithub can use/offer Pijul. Sure they named themselves after technology, but nothing prevents them to offer other control systems. reply gmueckl 17 hours agorootparentNothing except their own arrogance. See svnhub.com. It was registered by github to block potential competition long before SVN support on github became available. reply samatman 17 hours agorootparentPreventing some other party from riding on the coattails of your trademark is reasonable behavior as far as I'm concerned. This doesn't prevent anyone from offering an SVN host, just from stealing free publicity from GitHub in the process. That isn't arrogant at all. reply gmueckl 12 hours agorootparentThen they shouldn't have been riding the git hype wave and using/stealing that trademark. reply samatman 9 hours agorootparentgit is not trademarked. reply gmueckl 9 hours agorootparentgit is a trademark. Here's the USPTO trademark registration info: https://tsdr.uspto.gov/#caseNumber=85961336 reply goku12 17 hours agoparentprevI would really love to see a v1.0 release. That is a big milestone to wider adoption and possible investments that follow. reply BlueTemplar 22 hours agoparentprevGithub also managed to succeed through being a kind of social network (think \"stars\"). If you care about Github's dominance being an issue, walk the walk : refuse to do bug reports through it, publicly shame people and companies that use it or advertise it. reply patates 21 hours agorootparentIf I care about one thing, I should \"shame\" others who do not care about it? It feels like your comment escalates things up too quickly. Could it be that you wanted to expand your thoughts in between and didn't have the time or just forgot? reply BlueTemplar 20 hours agorootparentIt's a comment, not a blogpost, I am assuming some familiarity on HN with these issues, why using Github would be considered bad behavior : An actual blogpost : https://drewdevault.com/2021/12/28/Dont-use-Discord-for-FOSS... reply rapnie 19 hours agorootparentThere's also sfconservancy's GiveUpGithub campaign: https://sfconservancy.org/GiveUpGitHub/ reply dash2 22 hours agorootparentprevI use Github and I bet 99% of coders here do too. If someone tries to publicly shame me, I'll just laugh at them. reply throwaway290 21 hours agorootparent> I use Github and I bet 99% of coders here do too. I know plenty of people & projects using Gitlab and Sourcehut (myself). > If someone tries to publicly shame me, I'll just laugh at them. Shaming may be wrong. You must have pretty good job security/wealth/influence to refuse Github because this can lose you many opportunities in the industry. But that doesn't make it good, only understandable & honestly sad reply Murk 21 hours agoprevI have been following Pijul loosely for a while, and I would strongly agree that it could do with some information on some of the possible practical advantages of the approach. I do have some Pijul repositories around, but have not used it with the necessary further to explain any of the advantages, however, one possible advantage to note is that the equivalent of cherry-picking as used in GIT et. al, which always generates a new commit hash per cherry-pick for the same content should not happen in the Pigul world as I understand it. i.e. A specific commit when \"cherry-picked\" should have identical hashes, which has several implications. - Having not actually tested this in my own repos, take my insight with the appropriate grain of salt. reply dahart 19 hours agoprevHey what does this item in the FAQ mean? > Do files merged by Pijul always have the correct semantic? > No. Semantics depends on the particular language you’re using, and Pijul doesn’t know about them. That makes it sounds like Pijul might sometimes merge two functionally correct versions of a file into an unfunctional, incorrect one? Is this referring to a problem git and other merge tools have as well, or is it unique to Pijul and a result of being so good at merging without conflicts that it sometimes lets through semantic conflicts? reply magnio 18 hours agoparentAll version control tools have the same problem since AFAIK none of them care about the semantics of the program being version-controlled (except Unison). For example, in file foo.js you export the function foo. Now, if you delete the function foo, while your colleague import it into bar.js, the resultant changes are consistent, but the program is now functionally broken. reply skybrian 15 hours agorootparentThis is why we test our builds using continuous integration. Only updates that compile and pass the tests should be published. How does Pijul handle continuous integration? If there’s nothing interesting going on with that then I don’t see how the UX can be any different for publishing new versions. reply riffraff 18 hours agorootparentprevmonticello[0] cares about semantics too! Tho I guess it cheats existing in an image-based world (I have never used it, it's just something stuck in my memory from many years ago). [0] https://wiki.squeak.org/squeak/1287 reply gmueckl 18 hours agorootparentprevPlsstic's diff tool is also centered around syntactic units of the programming language rather than stupidly scanning for line breaks, if it recognizes the language. reply IshKebab 17 hours agorootparentThat still wouldn't mean that the merge is semantically correct. reply LeonidasXIV 18 hours agoparentprevYes of course this can happen in git as well, as it doesn't know the semantics of the changes it is merging, it only knows about textual conflicts. But if a change applies cleanly it doesn't mean that it necessarily also works, because the other branch might've changed the program logic. It doesn't happen very often, but it is still worth running integration tests after a merge even if both branches were passing tests on their own. reply samatman 17 hours agoparentprevSo in this scenario there's a general case and a special case. The general case is where merging a patch makes the code invalid in some sense. This can range from a syntax error to a subtle bug. Version control systems don't prevent this, because they can't: in full generality, correctness is an opinion of the author. The special case is where two patches each create a valid program, but applying both of them makes the program invalid. I think that's in the FAQ because pijul makes much of the fact that it has a sound theory of patches, as it should, that's a wonderful thing. But people commonly confuse soundness and validity, leading to questions like \"so what you're saying is that a merge will never result in a borked program?\". IMHO that shouldn't be in the FAQ though, because it creates the impression you got, which is that Pijul is talking about something which might be possible in principle but which it doesn't happen to be capable of. It's just patiently explaining that Pijul can't do impossible things. reply h4ch1 21 hours agoprevJust like how Sapling is compatible with GitHub, I'd love to see Pijul being compatible with GitHub wrt creating at least a read-only mirror. I've seen this exists: https://github.com/purplesyringa/PijulGit, but hasn't been updated in 5 years. reply gugagore 20 hours agoparentIs sapling compatible? It gets rid of your .git directory. reply charcircuit 15 hours agorootparentIt's network compatible in that you can clone, pull, push, etc with a github repo. reply zck 18 hours agoprev> However, channels are different from Git branches, and do not serve the same purpose. In Pijul, independent changes commute, which means that in many cases where branches are used in Git, there is no need to create a channel in Pijul. I've never understood this. AFAIK, the only use case I've ever seen for git branches is \"I have some code, but don't want it going live yet\". Maybe it's a WIP demo, maybe you want someone else's eyes on it, maybe you just want to back your current state up on a remote server because your laptop is going to explode. Am I misunderstanding, and Pijul manages that without channels? Or is there a common case git branches are used that I missed? reply jph 18 hours agoparentA common case for git branches is long-life versions. For example, one git branch can be for version 1.0 and one git branch can be for version 2.0. This can be good for major upgrades, as well as for site-specific installations, as well as for regulated industries that need to audit specific versions. reply IlliOnato 12 hours agorootparentI think the most common use for git branches is topic branches. Independent lines of development you are not ready to share with your team, or your team with the rest of the company, or commit to production. I don't see how Pijul features can remove the need for such lines of development. I also don't see how it matters whether a branch is long-lived or not. What Pijul may help with is a (broken, IMHO) workflow of some Git projects where the long-standing branches are constantly rebased. This workflow is bad, but having branches in Git does not force you to (mis)use rebase. I am not saying anything against Pijul, and maybe there is a better way than branches to manage multiple lines of development, but I'd like it to be explained. So far I cannot guess what it might be. reply zck 18 hours agorootparentprevSo we have a git branch v1, and a git branch v2, and sometimes we pull commits into both, and other times just one. How does pijul manage that without using channels? reply manithree 14 hours agoparentprevI had the same question. Turns out, it's mostly a future possibility, and for most current use cases of branches in git, you would use a channel in pijul. At least that's how I understand these discussions in the pijul discourse. https://discourse.pijul.org/t/phenomenological-pijul-or-piju... https://discourse.pijul.org/t/working-without-channels/1047/... reply IlliOnato 11 hours agorootparentThank for the links! I've looked at them, and while they gave me more and interesting information on Pijul, they did not explain how you can work without multiple channels in a realistic development environment. However, from these pages it appears that working with channels in Pijul is more cumbersome than working with branches in Git. reply letmeinhere 18 hours agoparentprevThere are (unfortunately) a lot of git repositories with multiple long-lived branches, often to track what's released in different environments. Unfortunate because it leads to a _ton_ of merge conflicts and uncertainty about what code is live in what env. One of the most popular of these flows was popularized under the brand \"Gitflow\". Atlassian has a detailed, and critical, writeup of that here: https://www.atlassian.com/git/tutorials/comparing-workflows/... reply derbOac 21 hours agoprevI've been following this for awhile. Not my area of expertise but it seems like such a solid project. Curious to hear about any experiences using it or thoughts about how it could be improved, extended, or compares to other systems in actual use. reply jillesvangurp 21 hours agoparentAs somebody not using this, it looks like the main obstacles to this getting popular (assuming it works as advertised and it's easy to use, I wouldn't know): - Work on the pitch and clearly state the problem this is solving instead of mainly talking about the competition. What makes this unique? Why should people care? - Make it easy to start using this for teams. My guess is that this is where a lot of developers fail to convince others in their teams. Back in the day when I started using Git, interfacing with existing svn repositories was a key selling point for me. Likewise, I helped migrated a big cvs repository to subversion when that was new. Key selling point there: we don't loose our version history. This is a complex topic of course but I bet there are actually a lot of solutions here. - Show that there's an ecosystem. Who is using this? What tools are there? Are there any project hosting things that I can use? Answer these questions. This is about taking away any concerns people might have about using this that are perhaps half convinced already. reply exDM69 21 hours agorootparentThis is a chicken and egg problem and I don't see a way out of it. Even if Pijul is better (not voicing an opinion here), it's not drastically better enough to replace Git in widespread use in the near term. The difference in day to day use is not huge. And it is better in handling certain merge situations that are painful in git, but most programmers don't run into these often enough to care. > What makes this unique? It's patch based rather than snapshot based. > Why should people care? It avoids certain merge scenario problems that git makes painful. > when I started using Git, interfacing with existing svn repositories was a key selling point for me Pijul can import and export to Git (and probably others) with less problems than git vs. svn (because SVN is not distributed and had a weird branching model). > Show that there's an ecosystem Here's the chicken and egg problem again. There's a \"free\" hosting service advertised on the front page. Or you can use it with your git hosting (but lose some of the advantages). But GitHub and GitLab have their own CI systems and other infrastructure which isn't going to be easy or cheap to replace. So yeah, I think that Pijul is a great piece of technology that solves a real problem we have with Git, but it's unlikely to overcome the inertia that Git{Lab,Hub,} have. reply jancsika 18 hours agorootparent> Here's the chicken and egg problem again. Back when git had their chicken-and-egg problem, there were hundreds if not thousands of FOSS project members independently starting threads on mailing lists about how and/or when to move to git. Many of them were already using the git web server thingy and manually syncing with svn or whatever. Some contrarians aside, the general consensus at that time was, \"Yes, that clearly does solve some real pains we currently experience on a regular basis (branching, local branching, renaming files, etc.), but how do we practically move to it?\" Practically moving to it required the infrastructure. So you had a classic chicken and egg problem, until whatever broke it (sourceforge adding git compatibility? github?). With Pijul you have a small number of adherents who have trouble explaining the pains that Pijul addresses, much less how common those pains are in the average git project. So you haven't yet arrived at the question of how to switch to Pijul-- you're still at the question of why anyone should. Put another way-- you don't have any potential chickens longing to be incubated in an integrated Pijul hub/CI environment. If one poofed into existence, it might hatch chickens. Then again, it might go relatively unused. So I don't have a chicken and egg problem here. reply jillesvangurp 19 hours agorootparentprevI would suggest adapting the website to communicate those points a bit more clearly. The communication on that website isn't great. It's a common problem with things techies build for other techies. The key friction getting users to switch from something they already use is articulating why that is worth doing and investing lots of time in and/or making the point that it's really easy to switch is the main job of that website. Without that, most people simply won't. The point with an ecosystem is that there won't ever be one worth talking about unless people work hard to build one. \"Build it and they will come\" rarely works. This website isn't good enough to make that happen. reply devaiops9001 20 hours agoprevI like how Pijul has a math-centric approach under the hood. The name needs to be changed if they want to get traction. reply ZoomZoomZoom 18 hours agoparentJust to provide a counterpoint: The name is great! Very memorable and original. Much better than your regular plooper, vndl or dabix. reply goku12 17 hours agorootparentPijul also turns up easily on web searches, despite there being a bird with that name. reply samatman 17 hours agoparentprevThey tried that actually. It was traumatic for everyone involved. It's a pretty weird word for English-speakers, no argument from me there. I don't think I can explain why it's weird, but it is. I don't think that's even in the top five barriers to adoption though. reply baq 20 hours agoparentprevgit did quite well with its... reply Aachen 17 hours agorootparentThat may be survivorship bias. How many TLA projects failed that we never heard of? Not to mention the number of times I've tried to do a web search on an ordinary English word because someone thought that was brilliant to use as a product name and it turns up nothing because they didn't get as popular as Git or didn't have other distinctive keywords to go along with it, such as when your query not only includes \"bash\" but also \"variable\" that is unlikely to occur in a dictionary entry about the verb to bash I'd be very surprised if there is no causal relationship between names and the chances of success, all else being equal, and the devil is in the \"all else\". Evidently this is not a problem with a big enough marketing budget (think Teams and Meet), but without that it may be much more economical and practical to think of a useful name reply woadwarrior01 17 hours agoprevIIUC, Pijul has all the good parts of Darcs, with the exponential merge issue[1] resolved. [1]: https://darcs.net/FAQ/Performance#is-the-exponential-merge-p... reply cpach 18 hours agoprevAnyone using Pijul? Either in prod or for hobby projects. reply WolfeReader 16 hours agoprevDoes Pijul handle branches better than Darcs? I _love_ using Darcs but I had to stop because there was no good way for my teammates to see my new branches (really separate repos) unless I told them myself. reply pmeunier 13 hours agoparent`pijul fork`. There you go, I wrote an entire key-value store just to get that to work. It turned out to be faster than all others, but that wasn't intentional. reply WolfeReader 11 hours agorootparentNice! reply WolfeReader 16 hours agoparentprevGotta say, I don't see anything about it in their site's \"Documentation\" link. Unless I hear different, I don't think I'm interested. reply WolfeReader 11 hours agorootparentDisregard, it seems that \"channels\" may be what I'm looking for here. reply greatNespresso 22 hours agoprevpijul is a fantastic tool in the simplicity it brings compared to git + I wish it will succeed in the long run. reply tveita 20 hours agoprevThe front page could do with a simple example of a Pijul session to show how you would actually use it in practice. Maybe whatever would be the Pijul equivalent of a clone - branch - commit - push workflow. reply shane_kerns 20 hours agoprevWould love to see a 3 way comparison between Pijul, Fossil and Git reply Aachen 17 hours agoparentThe submitted website says they're explicitly against 3-ways. Either drop one or add a fourth! reply aezart 19 hours agoprevAs a left-handed person, that name is a nightmare to type. reply samatman 17 hours agoparentAs a left handed person, I had not even noticed that pijul is typed with the right hand. As a touch typist, now that you've brought it to my attention, it's a pretty fluent run, actually. I type it 3-2-1-2-3, and back on the home row it is, the last key is in the rest point for that finger. reply goku12 17 hours agoparentprevThat's what aliases are for. Regardless, I'm surprised that you even notice which hand you're typing with. For example, the word 'regardless' is mostly on my left hand and I'm right handed. But I wouldn't have noticed it if I didn't go looking for such words. reply djha-skin 19 hours agoprevI'll be very interested in a usage report from somebody. How's it been like using this tool? Is there an email workflow I can use like git's? I feel like an email workflow could really help with adoption because it would mean that I wouldn't need a third party website to rely on for distribution. reply mdaniel 15 hours agoprevheh, curl https://nest.pijul.com/pijul/pijul/changesgrep -i \"fixing conflicts\" (e.g. https://nest.pijul.com/pijul/pijul/changes/44SHUW5W6RMWYDF2R... ) reply systems 19 hours agoprevpijul seem to be written in rust, no one is mentioning it yet, so i though i would also because the language a project is written in, is one of the first things i like to check reply IshKebab 17 hours agoprevI'm watching this (and Jujitsu, Sapling, etc.) with interest, but I wish there was more focus on Git's real weak areas. Yes it sometimes makes merge conflicts more difficult than it could, but you can generally deal with that. The bigger problems are: * Poor support for large/binary files. LFS is bare-minimum proof of concept. * Poor support for large projects. Big monorepo support is definitely getting better thanks to Microsoft. Submodules are a disaster though. How does Pijul support large files and large projects? reply pmeunier 13 hours agoparent- Large/binary files: Pijul splits patches into operations + contents. You can download a patch saying \"I added 1Tb there\" without downloading any single byte of the 1Tb. No need to add any extra feature. - Large projects: Pijul solves that easily using commutativity, you can work on partial repos natively, and your changes will commute with changes on other parts of the monorepo. reply IshKebab 13 hours agorootparent> No need to add any extra feature. Well that's the building blocks, but Git can do that through blob filters and whatnot. It's a necessary foundation but not a complete feature. You need ways of recording which blobs should be fetched eagerly, which should be fetched on demand, maybe a way to indicate where to get the data (you might want a central store for big files like LFS). > you can work on partial repos natively That's very good. Does it have anything like submodules or subtrees? I kind of think they are a bad idea in general but people do use them and they can be useful in very niche cases. From the sounds of the patch-based system I guess you could do subtrees quite elegantly? Can the patches be given a \"base directory\"? I don't want to seem like all this stuff should be done immediately but it does feel like these are things that kind of need to be integrated from the start to work properly, unlike in Git where they've been tacked on badly. reply 1oooqooq 18 hours agoprevDoes it accept per-repo settings? (e.g. git config --local) Does it use proper SSH? (e.g. `git clone my_entry_on_ssh_config:/repo_path.git`) reply mdaniel 15 hours agoparentI seemingly can't link to line numbers in their nest thing, but https://nest.pijul.com/pijul/pijul:main/SXEYMYF7P4RZM.ERRQA#... is ssh.rs in the pijul-remote directory, so presumably \"yes\" also, an especial :fu: to whatever the hell is going on with SXEYMYF7P4RZM.ERRQA being a permalink to a file with a specific name, forcing me to write out in english what's going on there reply 1oooqooq 12 hours agorootparentyeah the web think is pretty limited. no context lines on diffs at all, and I couldn't find a link to the full-file diff from the changeset. But i wouldn't worry too much about the nest usability yet. reply Aachen 17 hours agoparentprevThat honestly sounds like an implementation detail compared to figuring out what version control model even makes sense to begin with If you want local configs, worst case you can update $HOME inline and make it use different dotfiles. If ssh is a must, sshfs can be a way to achieve that reply 1oooqooq 12 hours agorootparentnot being able to set ssh parameters (and key paths, etc) via .ssh/config was a nightmare on git a long time ago. impossible to not leak identities to random servers. impossible to have curated identities for the same remote. imposible to use adhoc jump hosts. etc. reply Mortiffer 16 hours agoprevThank you for innovation that could free us from git reply Karellen 18 hours agoprevMissing FAQ: Is the \"j\" in Pijul hard or soft? reply ZoomZoomZoom 18 hours agoparentDon't know what hard or soft supposed to mean in a hypothetically ambiguous context, but the `j` here is obviously a Voiceless velar fricative (`x` in IPA). Think loch (Scott.), Χάρων (Gr.), joven (Sp.), хлопець (Ukr.) https://en.wikipedia.org/wiki/Voiceless_velar_fricative reply Karellen 17 hours agorootparent~~\"obviously\"? Why?~~ aha! see edit 2 That pronunciation had temporarily escaped me. I was trying to decide between a voiced postalveolar affricate d͡ʒ (hard) or voiced palatal approximant j (soft). https://en.wikipedia.org/wiki/Voiced_postalveolar_affricate https://en.wikipedia.org/wiki/Voiced_palatal_approximant Edit: \"hard\" and \"soft\" seemed like the most logical way to describe what I thought was a dichotomy, particularly as it's also often used to distinguish between the voicings of \"c\" and \"g\". Edit 2: Just read the whole FAQ (rather than searching for \"pronunciation\" or related terms), and noticed the \"Where does the name come from?\" entry, which mentions the Mexican origin, which is why \"x\" is \"obviously\" correct. reply ZoomZoomZoom 11 hours agorootparentRegarding my usage of the \"obvious\": it would only be correct to say that it's obvious to me, sorry for that. The reasoning goes like this: The word uses the Latin script in its most basic form so it's most probably some western-European language, Romance or Germanic. The phonetic structure fits Spanish the best, compared to other languages that I have any superficial knowledge on, the -ul being the most telling bit. reply 1oooqooq 18 hours agoparentprevIt's just pronounced \"j\". as the \"g\" in gif. reply FrancoisBosun 18 hours agorootparentI logged in just to upvote this! Got a good chuckle out of me :) reply owenversteeg 16 hours agoparentprevI say the whole thing should be pronounced 啤酒 (píjiǔ), which means beer in Chinese, just to add even more confusion into the mix :) Learning the intricacies of tonal languages is probably quicker than fully understanding Git so you still come out ahead by switching. reply philwelch 18 hours agoparentprevThat’s not even an exhaustive set of possibilities. reply Aachen 17 hours agorootparentIf you ask the waiter if they've got cauliflower or broccoli, I'm sure s/he'll still reply if all they've got is carrots, without being exhaustive reply conaclos 20 hours agoprevIs a git backend planned? reply crabbone 16 hours agoprevI don't want this to be interpreted as a negative comment about Pijul: I didn't try it, and don't want to judge. My question is: what is the motivation for making distributed VCS? Over the entire lifespan of Git the number of times I had more than one remote... I can probably count on my fingers. And I've been in infra / ops for the better part of my career. And, all those times were exceptions. I'd do it to fix something, or to move things around one time, and then remove the other remote. Other times it was my hobby projects I shared with someone in some weird way. Most developers who aren't in infra will never see a second remote in their repositories even once in their career. It seems like developing this functionality adds a significant overhead both in terms of development effort and learning effort on the part of the user. So... why? reply zemo 15 hours agoparent> what is the motivation for making distributed VCS? it depends on if you're asking about the motivations for distributed version control for linux kernel development in 2005 or the motivations for distributed version control today. git predates AWS and predates the state of the industry being that it's very easy and cost-effective for people to make central servers and web apps and things of that nature. My understanding is that \"emailing a patch to a mailing list\" was a more reasonable workflow then, since it piggy-backed off of people's email hosting providers (which, at that time, wasn't even \"everyone using gmail\", since back when git was created, gmail was invite-only; git predates gmail having open signups). Plus Subversion's branching model wasn't particularly great, so having different people work on things on different branches and giving them feedback and merging the branches when they were ready wasn't really a great experience. The distributed nature of the version-control system facilitates branches, since a branch and a copy of the repo somewhere else are abstractly the same thing. Practically speaking, people don't push and pull code between their workstations and the network topologies are typically centralized in nature, but on a data level the distributed model is dual to the branching model, and the branching model is the thing that people actually care about. Although I _do_ think it's pretty neat that you can use a thumb drive or NAS as a remote instead of needing a server, it's probably not a core use-case for most people and most projects. reply IlliOnato 11 hours agoparentprevEnable development that does not rely on a central repository. For some people/projects it's important, although these days it's a minority. reply Riyadh95 17 hours agoprev [–] Awww That's Grear!!! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pijul is a free and open-source distributed version control system centered around patch theory, promoting speed, scalability, and user-friendliness.",
      "It emphasizes merge correctness and resolves conflicts as a standard process to prevent their recurrence, enabling independent changes to be applied in any sequence without affecting the final outcome.",
      "Pijul supports partial repository clones and is employed in its own development, showcasing its versatility and efficiency."
    ],
    "commentSummary": [
      "Users discuss the benefits and hurdles of utilizing Pijul, an open-source version control system, versus Git for managing binary files, permissions, and merge conflicts.",
      "Pijul's distinct features, like patch commutation and precise conflict resolutions, are praised, but the existing Git ecosystem poses adoption challenges.",
      "Efforts are underway to enhance communication, documentation, and user-friendliness to encourage broader adoption of Pijul within the programming community."
    ],
    "points": 249,
    "commentCount": 188,
    "retryCount": 0,
    "time": 1708514882
  },
  {
    "id": 39457875,
    "title": "Embracing Modularity: Leveraging Cat in Software Design",
    "originLink": "https://two-wrongs.com/useful-uses-of-cat",
    "originBody": "Useful Uses of cat by kqr , published 2024-02-21 Tags: programming meta_programming sysadmin When I write shell one-liners that transform the contents of some file, they often look something like In[1]: cat access.loghead -n 500grep mailperl -e … This is what a lot of people reflexively call a useless use of cat11 And more thoughtful people will call a useless use of grep and head too because the Perl script can of course do both of those things. because head can take the file name as an argument and we don’t need the extra pipe and cat command. In fact, almost all commands can take the file name directly22 For the ones that don’t, we can use input redirection and say e.g. < access.log command. and we only really need cat when we want to concatenate the contents of files. But there’s a reason I do it anyway. I’m currently re-reading one of David Parnas’ classic papers on modularity33 Designing Software for Ease of Extension and Contraction; Parnas; ieee Transactions on Software Engineering; 1979.. Every software engineer should read that stuff – it’s brilliant. For this article we’ll focus on one thing: we all know code changes should be isolated. For example, we should be able to add new functionality by just adding code, and not going in and changing existing code. Parnas phrases this in an interesting way. We must recognise that […] it is always possible to remove code from a program and have a runnable result, [and] any software system can be extended. The problem is that these subsets and extensions are not the programs that we would have designed if we had set out to design just that product. Further, the amount of work needed to obtain the product seems all out of proportion to the nature of the change. His idea of the ideal design is one where we can add or remove code and it still looks like the program was designed for the thing it’s doing now; i.e. you can’t tell that something else was added or removed later on, it all looks like part of the original design. Parnas lists four classes of problems we often encounter when trying to make changes. For this discussion, the second class is the relevant one. Many programs are structured as a chain of components, each receiving data from the previous component, processing it (and changing the format), before sending the data to the next program in the chain. If one component in this chain is not needed, that code is often hard to remove because the output of its predecessor is not compatible with the input requirements of its successor. A program that does nothing but change the format must be substituted. One illustration would be a payroll program that assumed unsorted input. One of the components of the system accepts the unsorted input and produces output that is sorted by some key. If the firm adopts an office procedure that results in sorted input, this phase of the processing is unnecessary. To eliminate that program, one may have to add a program that transfers data from a file in the input format to a file in the format appropriate for the next phase. If we go back to our example of the shell one-liner and squint a little, then the string access.log is one input format (describing the file with the relevant contents) and the contents of the access log is a different input format. These are two representations of essentially the same thing. If we then eliminate the useless use of cat and write instead In[2]: head -n 500 access.loggrep mailperl -e … we find that head performs two responsibilities: Converting the string access.log into the contents of the file; and Extracting the first 500 records of that content. When we’re satisfied with our Perl script, it’s not unreasonable to think that we might want to run it across the entire access log rather than just the first 500 records. If we then delete only the head processing step we’re left without a step that transforms the string access.log into the contents of the access log. We can move that responsibility into the grep call, but this would mean we had to change some existing component in order to remove another – no good! The natural solution is a useless use of cat. With a separate processing step that converts the filename into the file contents, we can delete any intermediary processing steps and still be left with a functioning pipeline.44 We can also change the source data to e.g. a zcat or curl command. I frequently experiment with cat canned_response.json and then switch to curl once I’m satisfied with the one-liner. The process-to-process pipe is, in other words, the more flexible and decoupling interface compared to input redirection which implies a particular type of data source. People can complain all they want about it, but I will continue writing modular code. Even when it’s just shell one-liners. Sidenotes 1 And more thoughtful people will call a useless use of grep and head too because the Perl script can of course do both of those things. 2 For the ones that don’t, we can use input redirection and say e.g. < access.log command. 3 Designing Software for Ease of Extension and Contraction; Parnas; ieee Transactions on Software Engineering; 1979. 4 We can also change the source data to e.g. a zcat or curl command. I frequently experiment with cat canned_response.json and then switch to curl once I’m satisfied with the one-liner. The process-to-process pipe is, in other words, the more flexible and decoupling interface compared to input redirection which implies a particular type of data source.",
    "commentLink": "https://news.ycombinator.com/item?id=39457875",
    "commentBody": "Useful Uses of cat (two-wrongs.com)244 points by todsacerdoti 15 hours agohidepastfavorite159 comments bee_rider 14 hours agoThe reason these sequences of commands always start with cat, for me at least, is that I just cat’d the file only to find it was too long or noisy. cat filename.txt Upgrep “thing I want” Is fewer keystrokes than cat filename.txt grep “thing I want” filename.txt Or more likely cat filename.txt grep filename.txt “thing I want” grep “thing I want” filename.txt reply hnlmorg 13 hours agoparentYou can also do: grep “thing I want” !$ Bash (and similar) will replace !$ with the last parameter of the previous command. This is a trick I’ve used lots when wanting to perform a non-piping operation on a command I’ve ‘cat’ed (eg ‘rm -v !$’) I’d never criticise anyone for “useless” use of ‘cat’ though. If the fork() overhead was really that critical then it wouldn’t be a shell command to begin with. reply mmh0000 13 hours agorootparentEven easier, press alt+. (fewer keystrokes too! cli golf is fun), it'll copy-and-paste the last parameter from the previous command. If you press . multiple times it'll go further back into your history. reply 6c696e7578 31 minutes agorootparentAs I tend to live with set -o vi, [esc] _ gets the same. I hardly ever remember to set: bind '\"\\e.\"':yank-last-arg So [esc] _ does it on systems where I haven't customised my environment. A major drawback though is it doesn't go through history like alt . does. However, when 'set -o vi' is enabled you can easily go through history and edit with familiar vi keystrokes, or press 'fc' and fix the previous command in EDITOR, or 'v' to edit the current command in EDITOR. reply smackeyacky 3 hours agorootparentprevNot sure that would work everywhere but !$ definitely does for retrieving the last arg of the last command. 40 years of muscle memory right there reply bierjunge 5 minutes agorootparentNobody mentions $_ ? It gives you the last argument used, so: cat filename.txt grep \"what I want\" $_ expands \"$_\" to \"filename.txt\" reply penguin_booze 2 hours agorootparentprevAlt + . is a readline binding, so that's available wherever GNU readline is used. Immediate examples that come to mind are Python PDB shell and GDB. reply bee_rider 5 hours agorootparentprevOften the shortcuts that people point out in these threads are too niche for me to remember, but that seems like a pretty great one, I’ll try to remember it. reply imp0cat 3 hours agorootparentalt + . is easy to remember, but did you know you there is a way to recall any argument from the previous command in bash? Press Escape, then the number of the argument from the previous command, confirm with Ctrl+Alt+Y. Example: > command arg1 arg2 arg3 Escape, 1, Ctrl+Alt+Y gives you arg1. reply penguin_booze 2 hours agorootparentalt ++ alt + . should do the same thing. No need for ctrl+alt+y, is there? reply bee_rider 5 hours agorootparentprevI’m sure there are cases where the fork overhead matters. But, alas, I don’t type or read that fast. reply bumblebeast 13 hours agorootparentprevI just use . On the command line to bring back the last argument. Then I can look at it before hitting return reply harrisi 11 hours agorootparentshopt -s histverify reply dllthomas 7 hours agorootparentprevnext [–]sudo !! for example. Again, bash specific. reply neuromanser 13 hours agoparentprevPress alt and dot (full stop) to insert last word from the previous command line: $ cat file $ grep stuff alt-. Alternatively, make use off the READNULLCMD mechanism in Zsh: $grep filename.txt “thing I want” > grep “thing I want” filename.txt. …every time reply hawski 11 hours agoparentprevTry:backup.img # make a disk image cat /dev/sdb > /dev/sdc # clone disk cat ~/Downloads/* # play Russian roulette with your terminal cat > file # minimalistic text editor, ^D to exit saving, ^C to exit erasing the file catfile # nearly complete emulation of ed grep -r bongo .cat # shorter than typing --color=never cat -v file # cause 20 points of damage to wizards of bell labs cat file > file # empty a file without removing the file cat meow meow > meows # duplicate file contents reply TacticalCoder 12 hours agoprevWhat's really useless are all the \"useless use of cat\" comments (and shellcheck warnings although I take it for a shell script there could be cases where one less process may be justified [although really if you're at that point you've got other things to worry than cat, sadly]). I use \"cat ......... \" like in TFA and just like many in this thread because it simply makes sense. It's more intuitive. It's easier to read. It requires less braincycles to remember how this or that command wants its parameter passed, etc. I think the \"useless use of cat\" movement made its time: it failed. Many of us are never going to give up our use(less|ful) of cat (you decide). So stop wasting your time complaining about it. reply thaumaturgy 8 hours agoparentI use shellcheck a lot, and I've found so far that its complaints fall neatly into one of two buckets: \"oh, whoops, nice catch\" and \"shut up, shellcheck\". \"useless use of cat\" goes into the latter bucket. Complaining at me about it does not actually improve the code; it's just a nag about a bad habit that, arguably, isn't even a bad habit. reply dataflow 4 hours agorootparentI think you're mistaken here, and confusing two different usages of cat. \"Useless\" uses of cat aren't bad habits during interactive usage, for all the reasons people mention here which I won't rehash. For scripts, however, the story is different than for one-off commands. For one thing, it's slower due to the extra forks and copying of data across pipes, so there's at least that. For another, it prevents the command from inspecting the other end of its pipe, which can negatively impact usage in some case. (For example, if the program knows its input is from a terminal, it may flush its output on every newline it sees.) Moreover, a bunch of the arguments for the interaction case (like \"it's fewer keystrokes\" or whatever) don't even apply to the script case in the first place... The end result here is that you definitely shouldn't assume some habit is just fine with scripting merely because it's fine when you're typing on the terminal, or vice-versa. reply thaumaturgy 2 hours agorootparentThose are all reasonable points, but: For shell scripts, I would argue quite vehemently that the most important goals should be correctness and readability, with performance being a very distant third concern. I'd even be tempted to argue that performance shouldn't be a consideration at all, except of course that argument would be misinterpreted to support some absurd edge case until I'd have to admit that of course performance is a little bit of a concern. But in any case, I can't recall a single example of a cat pipe being the root cause of an unacceptable performance problem in a shell script. On the readability point, the example that probably irritates me most often is a cat pipe into some commands into a while loop. I much prefer this: cat file.confsed -e 's/pattern/replacement/g' -e 's/reallybigolhonkinpattern/other-replacement/g'tr...while read line; do... to this: sed -e 's/pattern/replacement/g' -e 's/reallybigolhonkinpattern/other-replacement/g' wrongfile.conftr...while read line; do... or this: sed -e 's/pattern/replacement/g' -e 's/reallybigolhonkinpattern/other-replacement/g'tr...while read line; do stuff... doneThis works because the `:` is a way to set a label, and it implicitly returns 0. It's just a weird wrinkle of the language. Do you have a source for that? I thought it was just POSIX built-in for true. Like `.` vs. `source`. What's a label in this context anyway? reply VSpike 1 hour agorootparentHah, yeah I was completely wrong on that! Should have fact checked myself. That's a falsehood I absorbed at some point and didn't question. reply penguin_booze 2 hours agorootparentprevEqually surprised. I know ':' is a label in sed, but labels in (ba)sh, I'm not aware. If it's indeed a label, is there a goto? reply cryptonector 7 hours agorootparentprevNo need to ask for a source. The word \"label\" in the POSIX shell documentation only occurs in the description of `case`, and it doesn't happen in the manual page for bash, dash, zsh, etc. reply PennRobotics 1 hour agoparentprevinfinite loop in C: for(;;){ // loop body here } reply gruturo 14 hours agoprevThe concept presented is something I can agree with in principle, but \"transforming a filename into the content of the file\" is a really thin justification for a responsibility. By all means don't build something where you have cascading effect and need to retest an entire pipeline, but this is _not_ it. P.S. And if you really really want to keep it separate, just do \" And if you really really want to keep it separate, just do \" Sure, once you’re done fiddling you can rewrite the thing to remove the \"cat\" Or just… don’t? reply margalabargala 14 hours agorootparentSure. Don't. Or do. Whatever you prefer. reply kristjansson 14 hours agorootparentprev> it breaks this representation where each step is piped into the next one No it doesn't?outfile and real clarifies what's a shell command, what's a redirection, etc. reply jltsiren 7 hours agorootparentIt would make more sense like: infile > some_cmd > outfile Then the arrow would be pointing in the right direction. But then it would be unclear whether \"infile\" is a file or a command. Which is why people use: cat infilesome_cmd > outfile You can now interpret \"cat\" as a keyword that specifies that \"infile\" is a file. reply MatmaRex 5 hours agorootparentIt is more pleasant to the eye if you remove the spaces: outfile Just like you wouldn't add spaces in the middle of '2>&1' when redirecting stderr to stdout: outfile 2>&1 reply hk__2 49 minutes agorootparent> Just like you wouldn't add spaces in the middle of '2>&1' when redirecting stderr to stdout: Mostly because if you do it doesn’t work: '2 >&1' is not the same as '2> &1' (invalid syntax) which is not the same as '2>& 1', which …is the same as '2>&1'. reply ajross 14 hours agoparentprev> \"transforming a filename into the content of the file\" is a really thin justification for a responsibility. Uh... I dunno, but my lizard brain thinks that the whole idea of mediating filesystem operations on storage and IPC mechanisms like pipes is a lot more complicated, magic, and deserving of a single command than merely filtering the data on stdin. I agree with the article and the logic, and think this historic meme was basically wrong originally. You string up your chain of pipelines with the first element being \"where does it come from?\" and not merely whatever the first operation happens to be just because that operation allows for some kind of file input or redirection syntactically. reply TOGoS 14 hours agoparentprev> transforming a filename into the content of the file\" is a really thin justification for a responsibility This is one of those things where I think it is until it isn't. I sometimes second-guess myself when I think I might be over-single-responsibilifying. \"Well in practice these two things are so trivial that this feels a little silly.\" It often turns out to have been a good call in hindsight, especially when working with other people who aren't necessarily thinking about these things at all. If the responsibilities have been sufficiently split up, they're more likely to change only the part that needed to be changed, and less likely to complectify the two things together that really shouldn't've been. Or when I go \"oh wow that thing that I thought I overly-abstracted sure composes well with this unexpected new thing!\" Hardcore separation of concerns is just another method of defensive programming. > output The point of this syntax is that I can readily replace it with F() { X|Y|Z } output reply iforgotpassword 14 hours agoparentSince I like spaces around stuff I started putting two spaces after the infile at some point.outfile I just didn't like how the filename was so close to the command name ;) reply adityaathalye 5 hours agoparentprevOh `F()`. I should use redirection more with my Bash functions, and add it to my list (pun intended) of functional tricks [1]. [1] https://evalapply.org/posts/shell-aint-a-bad-place-to-fp-par... reply CBLT 13 hours agoparentprevI'm in this camp as well, starting with input file redirection just makes so much sense to me.w Y takes input redirected from x, piped into Z, which outputs into w. reply ulucs 10 hours agorootparentNo, that's obviously taking the inner product of theket while applying the Y operator, and multiplying by a scalar w reply hiAndrewQuinn 4 hours agorootparentprevThe shellY takes input redirected from x, piped into Z, which outputs into w. I.e. xZtee wY ? that's... something else entirely. reply _kst_ 13 hours agorootparentNo, the command isw where x and w are files, not commands. Something that \"cat file...\" advocates might be overlooking is that a redirection (\"outputfile\", \"2>errorfile\") can appear anywhere within a simple command, so these: command -optionRedirection isn't a step in the pipeline It isn't implementation-wise, but it is semantically. If anything,Redirection isn't a step in the pipeline \"Get data\" is the pipeline step we're talking about. Using \" The problem is that these subsets and extensions are not the programs that we would have designed if we had set out to design just that product. Just feels a bit disjointed in the article. I get the high-level message but thought it could be woven into the blog authors narrative better. Anyways, it sounds like a good article though. reply kqr 13 hours agoparentAuthor here. You're right. Originally I had a much longer excerpt but it got a bit rambly. I overcorrected and cut out too much; I'll see what I can do to fix it. Thanks! reply nonethewiser 6 hours agorootparentIt's a nitpick BTW. Of course a bit more context would help I think, but I probably appreciate your edits to keep it brief too. As I mentioned, I really appreciate how concise and to the point it was. Thanks for writing. I actually noticed your reply because I came back to this comment to save your blog. Not just for the content but the page style. The eggshile white (or similar) color, font, foot(side)notes. It's nice. reply halayli 1 hour agoprevIt's not more modular to use cat instead of head's filename argument. OP invented their own definition of modularity to fit their narative and bias. What are you going to use if you need to tail? cat a 1GB file through the kernel to reach tail's stdin? You're not just spawning an extra process you're copying the data from one process to another via read/write system calls for no good reason. reply yesiamyourdad 11 hours agoprevI can't be the only one who clicked on this hoping for an explanation of why they owned a cat. reply slyall 6 hours agoparentI thought it might reference the book \" 101 Uses for a Dead Cat \" https://en.wikipedia.org/wiki/101_Uses_for_a_Dead_Cat \"It consisted of cartoons depicting the bodies of dead cats being used for various purposes, including anchoring boats, sharpening pencils and holding bottles of wine.\" \"By December 7, 1981, it had spent 27 weeks on the New York Times Best Seller list.\" reply chilling 6 hours agoparentprevYeah... sadly no cat was found ): reply matvore 13 hours agoprevIf we're going to talk about unnecessary extra processes like useless cat, we should merge the head and grep commands into a sed, and possibly just merge everything into perl:$(cat -)EOF shite_template_standard_page_wrapper } [1] https://evalapply.org [2] https://github.com/adityaathalye/shite?tab=readme-ov-file#te... reply avgcorrection 14 hours agoprevMost immediate use: I have to google less shell syntax. reply hinkley 5 hours agoparentStream of consciousness troubleshooting dies on the search page. Especially with ads. reply txutxu 13 hours agoprevAfter read the article... all the reasoning... I think I could go with this for such reasons exposed: perl -ne 'last if $. > 500; /mail/ && print' access.log When you're done with the part after the &&, remove the 'last if $. > 500' For me, the most useful use of (gnu) cat, is cat -A weird.file It saves my day or solves weird issues (X-files) with files generated by (not so) junior sysadmins, copy/pastes, end of lines, invisible diffs, etc... many times each year. reply 0xbadcafebee 9 hours agoparentBut then it's not modular! I need this one-liner to be more complicated because I read a classic paper recently. reply svilen_dobrev 14 hours agoprevwell, whole thing can be written in the perl -ne ... but that absolutely kills modularity / composability so yes, \"cat xyz\" plays a source - and can be replaced with another source - without touching all other stuff. reply hk__2 14 hours agoparent> so yes, \"cat xyz\" plays a source - and can be replaced with another source - without touching all other stuff. This. I often have to read logs and being able to apply the same oneliner on older logs just by adding a \"z\" in front of \"cat\" to read .log.gz instead of .log files is extremely useful. reply kqr 14 hours agorootparentGood point! I often experiment with cat response.json and then replace cat with curl once I'm happy. reply TehShrike 14 hours agoprevTypo in the title – should be \"cat\" not \"Cat\" (same as the h1 in the linked article) reply wodenokoto 5 hours agoprevRelated, I find bash and other shells the only language where I can stream data through processes. Like, R can pipe, and pandas can .pipe() but they both complete the function on the entire data before it pushes a copy of the output to the next step. Why isn’t this flow of data a primitive outside of shell? reply az09mugen 13 hours agoprevAmong the uses of cat, I saw no one mention you can write or append text to a file, like this : cat > file.txt # after this command, just type your text Or to append : cat >> file.txt Can also work with Here Doc syntax : cat > file.txt > /dev/null; ) 0.00s user 0.02s system 43% cpu 0.045 total ( tail -n40000 filelist_h.txtgrep 'v\\|$' >> /dev/null; ) 0.00s user 0.01s system 22% cpu 0.066 total ( shuf -n40000 filelist_h.txtgrep 'v\\|$' >> /dev/null; ) 0.71s user 1.37s system 12% cpu 16.874 total ( cat filelist_h.txthead -n40000grep 'v\\|$' >> /dev/null; 0.00s user 0.01s system 101% cpu 0.017 total ( cat filelist_h.txttail -n40000grep 'v\\|$' >> /dev/null; 0.05s user 0.45s system 54% cpu 0.930 total ( cat filelist_h.txtshuf -n40000grep 'v\\|$' >> /dev/null; 0.60s user 0.91s system 96% cpu 1.565 total The results are fairly repeatable: - as expected, head and tail alone are equally quick alone - shuf is ridiculously slow due to overhead compared to catshuf - catshuf is just a bit slower than cattail. - cattail is slower than tail. - cathead is faster than head (but only because of overhead) Caveats are that this is WSL2 and the file is 480 MB (5 million lines) in a mounted Windows directory, although that helps magnify that slow I/O can influence how you pipe commands. reply kazinator 14 hours agoprevThis blogger does not seem to know that in the POSIX shell syntax, redirections can be specified anywhere in the command:If we then delete only the head processing step we’re left without a step that transforms the string access.log into the contents of the access log. By introducing \"cat access.log\" we have the same problem: if we delete only the cat processing step, we're left without a step that transforms the string access.log into the contents. For the useless cat to have the nice property that you can cleanly delete it from the command line, you need:access.log # create the access.log with some content $instead of >w. Then you could write pipelines with compelling HTML-esque symmetry:reply gregfjohnson 14 hours agoprevThoughtful article - thanks. Humorous aside: I recently came across a hilariously named, occasionally useful, related utility: \"tac\". reply ryukoposting 14 hours agoparentIt ships with coreutils! `rev` is another one in a similar vein, though that's not coreutils. reply mprovost 10 hours agorootparentI'm a bit surprised `rev` isn't in coreutils, it's ancient. Originally written because `cut` doesn't allow selecting fields from the end of a line. So to get the last field in each line, you reverse each line (right to left) with `rev`, select the first field with `cut`, and then reverse it again with `rev`. reply cryptonector 8 hours agoprev> When we’re satisfied with our Perl script, it’s not unreasonable to think that we might want to run it across the entire access log rather than just the first 500 records. If we then delete only the head processing step we’re left without a step that transforms the string access.log into the contents of the access log. We can move that responsibility into the grep call, but this would mean we hade to change some existing component in order to remove another – no good! What? Who cares, but also, if you do, then try this: /dev/null blah.1 to check for errors in a man page I'm editing, and you can see that intersperses I/O redirection with command-line words. So if you're really concerned about what edits you might have to make to head -n 500 access.loggrep mailperl -e ... to remove the head(1) and/or the grep(1) and so have to move where the `access.log` goes, well, just writeThe natural solution is a useless use of cat. The problem with useless uses of cat(1) is mainly that it betrays a misunderstanding of how the shell works, so \"that's a useless use of cat\" is a way to teach someone something they're missing. (Useless uses of cat are also useless uses of CPU cycles and energy, but the vast majority of the time those will be in the noise, so it's not a huge deal.) reply o11c 10 hours agoprevHm, what commands are there that don't support filenames? `tr` is the only one I can think of, excluding ones that really can only operate on already-open file descriptors (the `read` builtin, `flock` in certain modes, ...) reply charcircuit 12 hours agoprevI don't agree with this argument. You can make the same argument against cat >cat access.loghead -n 500grep mailperl -e … >we find that cat performs two responsibilities: >1. Printing an error to stderr if the file doesn't exist >2. Copying a file to stdout reply kkfx 14 hours agoprevan useful use, back in time, was to assemble partial ISO file of OpenSolaris SXDE, the first open version before Ian Murdock/OpenIndiana. It's more an anecdote but considering the usefulness of netcat in many cases such \"cat binary use\" might still be relevant in some embedded scenarios. reply badrabbit 12 hours agoprevcat is cool but I also like it's sibling tac, I use it quite a bit. It's also possible to make a much faster cat (I have, considered naming it cheetah),10%+ faster. reply 0xbadcafebee 10 hours agoprevAh, HN. The home of upvoting contrarian pedantry. reply lenerdenator 14 hours agoprevcame looking for feline-related content. left disappointed. reply p1mrx 14 hours agoparentUseful uses of cat: - pest control - entertainment - transporting small solar arrays reply franky47 10 hours agorootparent- Neighbourhood open WiFi networks mapping (aka: the War Kitteh) From this gem of a DEFCON talk: https://youtube.com/watch?v=rJ5jILY1vlw reply omoikane 12 hours agorootparentprev> transporting small solar arrays I definitely need to hear more about this one. reply lenerdenator 12 hours agorootparentsolar panel need sun. cat like sleeping in sun. attach solar panel to cat. cat occasionally moves to remain in sun as it naps. reply klyrs 12 hours agorootparentprevMy cat recently discovered that her fur can absorb a toilet bowl full of water. This seems like it could be useful, if you need a very wet floor immediately. reply brightsize 14 hours agoparentprevHere you go. This is the second time in the past few years that cat was used to process bikers in the NW WA wilderness. The first time was fatal. I used to live about 15min from where this incident occurred. https://www.seattletimes.com/seattle-news/cougar-attacks-cyc... reply grork 13 hours agoparentprevEspecially since the versatility of A Cat is unbounded. reply Findecanor 12 hours agoparentprevAnd here I had been hoping on tips and tricks for printing the file catalog on TRSDOS, Flex, Sinclair or RISC OS. reply glxxyz 12 hours agoparentprevI was half-expecting it to be about this strange book: https://en.wikipedia.org/wiki/101_Uses_for_a_Dead_Cat reply lonlazarus 14 hours agoparentprevTry tumblr reply whatsinaname123 13 hours agorootparentOr reddit reply xanderlewis 13 hours agoparentprevThose looking for useful uses of the category of (locally small) categories will similarly be disappointed. reply slingnow 12 hours agoparentprevI'm sure this generic \"joke\" would be a real gut buster over on Reddit. reply lenerdenator 12 hours agorootparentAs a cat servant I care not about gut-busting. reply renewiltord 12 hours agoprev [–] Honestly, I've being UUOC all my life and everything is fine. Never run into a problem. I mostly use `zsh` and `< filecmd` works easily even when you remove the `cmd` so that's fine. But honestly, it's both bought and lost me nothing, which means I shouldn't care about it. What I do like is doing something like: diff -aui <(xxd binary1) <(xxd binary2) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the importance of modularity in software design, focusing on isolating code changes for flexibility.",
      "By using commands like cat in shell scripts to convert file names into contents, the author suggests it enhances the ease of modifying and extending code while maintaining structure.",
      "It highlights the significance of modular code in software development, even within the realm of simple shell scripts."
    ],
    "commentSummary": [
      "The article explores efficient techniques for utilizing the \"cat\" command in Unix shell, such as shortcuts and alternative methods for productivity.",
      "It delves into the implications of employing cat pipes in shell scripts, highlighting the importance of responsibility in programming and clear collaboration with others.",
      "Users contribute tips, examples, and insights on the functionality, history, uses, and capabilities of the \"cat\" command in Unix systems."
    ],
    "points": 244,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1708541625
  },
  {
    "id": 39455131,
    "title": "Air Canada Ordered to Refund Passenger Due to Chatbot Error",
    "originLink": "https://www.wired.com/story/air-canada-chatbot-refund-policy",
    "originBody": "ASHLEY BELANGER, ARS TECHNICA BUSINESSFEB 17, 2024 12:12 PM Air Canada Has to Honor a Refund Policy Its Chatbot Made Up The airline tried to argue that it shouldn't be liable for anything its chatbot says. PHOTOGRAPH: ROBERT SMITH/GETTY IMAGES After months of resisting, Air Canada was forced to give a partial refund to a grieving passenger who was misled by an airline chatbot inaccurately explaining the airline's bereavement travel policy. On the day Jake Moffatt's grandmother died, Moffat immediately visited Air Canada's website to book a flight from Vancouver to Toronto. Unsure of how Air Canada's bereavement rates worked, Moffatt asked Air Canada's chatbot to explain. The chatbot provided inaccurate information, encouraging Moffatt to book a flight immediately and then request a refund within 90 days. In reality, Air Canada's policy explicitly stated that the airline will not provide refunds for bereavement travel after the flight is booked. Moffatt dutifully attempted to follow the chatbot's advice and request a refund but was shocked that the request was rejected. Moffatt tried for months to convince Air Canada that a refund was owed, sharing a screenshot from the chatbot that clearly claimed: If you need to travel immediately or have already travelled and would like to submit your ticket for a reduced bereavement rate, kindly do so within 90 days of the date your ticket was issued by completing our Ticket Refund Application form. Air Canada argued that because the chatbot response elsewhere linked to a page with the actual bereavement travel policy, Moffatt should have known bereavement rates could not be requested retroactively. Instead of a refund, the best Air Canada would do was to promise to update the chatbot and offer Moffatt a $200 coupon to use on a future flight. Unhappy with this resolution, Moffatt refused the coupon and filed a small claims complaint in Canada's Civil Resolution Tribunal. Ars Technica This story originally appeared on Ars Technica, a trusted source for technology news, tech policy analysis, reviews, and more. Ars is owned by WIRED's parent company, Condé Nast. According to Air Canada, Moffatt never should have trusted the chatbot and the airline should not be liable for the chatbot's misleading information because, Air Canada essentially argued, \"the chatbot is a separate legal entity that is responsible for its own actions,\" a court order said. Experts told the Vancouver Sun that Moffatt's case appeared to be the first time a Canadian company tried to argue that it wasn't liable for information provided by its chatbot. Tribunal member Christopher Rivers, who decided the case in favor of Moffatt, called Air Canada's defense \"remarkable.\" \"Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives—including a chatbot,\" Rivers wrote. \"It does not explain why it believes that is the case\" or \"why the webpage titled 'Bereavement travel' was inherently more trustworthy than its chatbot.\" Further, Rivers found that Moffatt had \"no reason\" to believe that one part of Air Canada's website would be accurate and another would not. Air Canada \"does not explain why customers should have to double-check information found in one part of its website on another part of its website,\" Rivers wrote. In the end, Rivers ruled that Moffatt was entitled to a partial refund of $650.88 in Canadian dollars off the original fare (about $482 USD), which was $1,640.36 CAD (about $1,216 USD), as well as additional damages to cover interest on the airfare and Moffatt's tribunal fees. Air Canada told Ars it will comply with the ruling and considers the matter closed. Air Canada’s Chatbot Appears to Be Disabled When Ars visited Air Canada's website on Friday, there appeared to be no chatbot support available, suggesting that Air Canada has disabled the chatbot. Air Canada did not respond to Ars' request to confirm whether the chatbot is still part of the airline's online support offerings. MOST POPULAR SCIENCE Los Angeles Just Proved How Spongy a City Can Be MATT SIMON SECURITY Signal Finally Rolls Out Usernames, So You Can Keep Your Phone Number Private ANDY GREENBERG BUSINESS This Tiny Website Is Google’s First Line of Defense in the Patent Wars PARESH DAVE SCIENCE Neuralink’s First Brain Implant Is Working. Elon Musk’s Transparency Isn’t EMILY MULLIN Last March, Air Canada's chief information officer, Mel Crocker, told the Globe and Mail that the airline had launched the chatbot as an AI \"experiment.\" Initially, the chatbot was used to lighten the load on Air Canada's call center when flights experienced unexpected delays or cancellations. “So in the case of a snowstorm, if you have not been issued your new boarding pass yet and you just want to confirm if you have a seat available on another flight, that’s the sort of thing we can easily handle with AI,” Crocker told the Globe and Mail. Over time, Crocker said, Air Canada hoped the chatbot would \"gain the ability to resolve even more complex customer service issues,\" with the airline's ultimate goal to automate every service that did not require a \"human touch.\" If Air Canada can use \"technology to solve something that can be automated, we will do that,” Crocker said. Air Canada was seemingly so invested in experimenting with AI that Crocker told the Globe and Mail that \"Air Canada’s initial investment in customer service AI technology was much higher than the cost of continuing to pay workers to handle simple queries.\" It was worth it, Crocker said, because \"the airline believes investing in automation and machine learning technology will lower its expenses\" and \"fundamentally\" create \"a better customer experience.\" Science Your weekly roundup of the best stories on health care, the climate crisis, genetic engineering, robotics, space, and more. Delivered on Wednesdays. Your email SUBMIT By signing up you agree to our User Agreement (including the class action waiver and arbitration provisions), our Privacy Policy & Cookie Statement and to receive marketing and account-related emails from WIRED. You can unsubscribe at any time. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. It's now clear that for at least one person, the chatbot created a more frustrating customer experience. Experts told the Vancouver Sun that Air Canada may have succeeded in avoiding liability in Moffatt's case if its chatbot had warned customers that the information that the chatbot provided may not be accurate. Because Air Canada seemingly failed to take that step, Rivers ruled that \"Air Canada did not take reasonable care to ensure its chatbot was accurate.\" \"It should be obvious to Air Canada that it is responsible for all the information on its website,\" Rivers wrote. \"It makes no difference whether the information comes from a static page or a chatbot.\" This story originally appeared on Ars Technica.",
    "commentLink": "https://news.ycombinator.com/item?id=39455131",
    "commentBody": "[dupe] Air Canada Has to Honor a Refund Policy Its Chatbot Made Up (wired.com)242 points by gavman 18 hours agohidepastfavorite150 comments yodon 18 hours agoDupe https://news.ycombinator.com/item?id=39378235 (400+ comments) reply ChrisArchitect 17 hours agoparentPro-tip OP: https://hn.algolia.com/?dateRange=pastWeek&page=0&prefix=fal... reply floatrock 17 hours agoprevHere's the real punchline: > Experts told the Vancouver Sun that Air Canada may have succeeded in avoiding liability in Moffatt's case if its chatbot had warned customers that the information that the chatbot provided may not be accurate. Here's a glimpse into our Kafka-esque AI-powered future: every corporate lawyer is now making sure any customer service request will be gated by a chatbot containing a disclaimer like \"Warning: the information you receive may be incorrect and irrelevant.\" Getting correct and relevant information from a human will be impossible. reply OnionBlender 16 hours agoparentIf that is such a perfect way to avoid getting sued, why don't they put that on every page of their website and train all of their customer service staff to say that when they talk to customers? reply waveBidder 16 hours agorootparentDon't know if you've had the pleasure of interacting with US health insurance, but they do this for coverage and cost estimates all the time, so there is unfortunately precedent in the States. reply sandworm101 15 hours agorootparentprevWell, if the information on the website is too inaccurate to trust then it is also too inaccurate for purposes of contract. The logical next step is to say that all contracts made with chatbots are unenforceable as they lack any trustworthy meeting of the minds. reply rnewme 15 hours agorootparentprevBecause you would choose someone else for things that really matter to you, probably the same person you would chose in that kind of scenario anyway. Eg. for my initial llc where I was single employee, I didn't care about my accounting basically saying in the contract \"we ll do our best but can't be liable for anything\", whereas now that I have bigger llc with many employees I picked (what I perceived) the best option on the market, for a premium price. They take the liability for their work and have insurance. reply frud 15 hours agorootparentprevI used to work for a company that sold factory automation technology, and had hundreds of manuals for all the products they sold. In the front matter of every manual was a disclaimer that nothing in the manual was warranted to be true. This was automation equipment running things like steel mills, factories, petroleum plants, where failures could result in many deaths and grave financial losses. reply MichaelZuo 10 hours agorootparentI could see that for paper manuals or normal pdfs since once they leave the premises of the company anyone could alter the pages of the manual. reply rootusrootus 14 hours agorootparentprevBecause it is a bad look, I assume. If I'm interacting with a company that constantly disclaims everything they say as probable bullshit, I'll go find a competitor that at least pretends to try harder. reply add-sub-mul-div 15 hours agorootparentprevHumans aren't perfect but they're reliable enough to be trusted with customer service work. Humans (generally) have a desire and need to keep their jobs and therefore hold themselves to a high enough standard of performance to keep their job. And maybe employment contracts have language that offloads liability to an employee if they go rogue and start giving away company resources. Chatbots aren't accountable in any way and we don't know yet if their creators ever will be either. And disclaimers are used in lots of contexts too. reply effingwewt 16 hours agorootparentprev....Because an actual judge hadn't said it yet. I still.remember when Microsoft updated thr 360 TOS to force arbitration the day after it was deemed legal in a completely separate case. Rest assured there is an incoming flood of TOS updates. reply unsupp0rted 17 hours agoparentprevThat's why I get all my questions answered by CC'ing legal@example.com and privacy@example.com reply AlbertCory 13 hours agoparentprevIANAL, but AFAIK you can't disclaim liability that you actually have. I'd love to hear an actual lawyer who knows (not a know-it-all amateur) declaim on this, but: A Ferris Wheel operator cannot make you sign a disclaimer that they're not responsible if it collapses and kills you. Or rather, they can, but it will not hold up in court. Similarly, you can say in your manual, \"We're not responsible for anything we say here\" but you still are. I don't know about chatbots, but I'd expect that judges will look for other precedents that are analogous. reply astolarz 12 hours agorootparentPersonal anecdote: A few years back I left my car at a dealership for some warranty work that was going to take a few days. It has a soft top and they left it in their gated lot overnight, where it got broken into (slashed the top, ripped open the glove box, stole a cheap machete I got from a white elephant exchange). They claimed that they weren't liable at all since I signed a waiver and should go through my own insurance. After a little push back, they caved and covered it under their insurance like they should have from the beginning. I don't go to that dealership for anything anymore, for that and other reasons. reply deegles 16 hours agoparentprevmuch like Tesla's Autopilot which cannot be responsible for an accident because you're supposed to be hands-on-wheel and alert at all times while using it. reply w0m 16 hours agorootparentThat issue is media mostly optics I think. Once you sit down with it, it's simply a better cruise control. Now, if you are talking about 'Full Self Driving' - then yea, there's a waiver and a point there. reply efitz 15 hours agorootparentIt's not optics. It's regulatory intervention: https://static.nhtsa.gov/odi/rcl/2023/RCAK-23V838-3395.pdf I think the recall is dumb. If you are not paying attention and have an accident, then you are at fault. You already have to click-through agree to use the feature properly, and Tesla has an interior camera to capture a photo of the person agreeing and telemetry to send it to the mothership. For that matter, Tesla could make you read the waiver on camera and capture that before enabling autopilot or FSD. reply rwbhn 15 hours agoparentprevPart of disclaimer at irs.gov for their interactive assistant: \"Answers do not constitute written advice in response to a specific written request of the taxpayer within the meaning of section 6404(f) of the Internal Revenue Code.\" reply _thisdot 15 hours agoparentprevCan’t they link to sources? Like Perplexity.ai or Arc Search does. I don’t even need it to tell me anything. Links are all that is relevant. Google Analytics on the Web does something similar. You can ask questions in the search box and it takes you to a relevant page. “Can I get refund on my flight 2 hours in advance?” “Here is a link to refund policies w.r.t time before flight” reply Sayrus 15 hours agorootparentAir Canada's chatbot returned sources along with its answer. Quotes from another article[1]. > Air Canada’s argument was that because the chatbot response included a link to a page on the site outlining the policy correctly, Moffat should’ve known better. [1] https://techhq.com/2024/02/air-canada-refund-for-customer-wh... reply ncallaway 15 hours agorootparentYes, but: \"The answer is yes, you may receive a refund within 2 hours of departure. More details are here: (link)\" is _entirely_ different from \"Our refund policy is here: (link)\" reply nimbius 15 hours agoparentprevchoosing to accept an unnecessary quantifiable liability and wrapping it in a disclaimer as part of a critical business process is not a recipe for sustained growth or profit. AC and other corporations would do well to put the brakes on this instead. identify ways to transfer risk (AI Insurance for example) or avoid risk (scrap the AI bot until the risk is lowered demonstrably.) savvy advertisers would jump on this opportunity to show just how much AC cares about the customer and eat the loss quietly before it ever went to trial. reply dheera 16 hours agoparentprevOR they could just honor what the chatbot said and forget the legal battle. The chatbot's error cost them what, $200? And it probably replaced at $100000/year employee? reply saintfire 15 hours agorootparentI would assume the average salary of a customer support worker is dramatically lower than that. The point is still valid regardless. reply recursive 15 hours agorootparentSalary is always lower than the employer's total cost to employ, so that tracks. reply passwordoops 15 hours agorootparentprevYou think front-line support staff account for $100k in compensation? reply dheera 15 hours agorootparentTotal overhead. Compensation + benefits + payroll taxes + all the per-employee real estate and equipment to support them on the job reply passwordoops 14 hours agorootparentExactly. They are paid under $20/hr pre-tax ($40K) benefits (LOL for that job) won't exceed, what $5K of I'm betting mega generous. Phone jockeys don't need real estate but in the event they do go to an office an individual employee would account for what, $50 a month of that expense? And equipment is trivial maybe a couple of hundred at most (laptop and headset). We're not talking anything near $100K reply more_corn 16 hours agorootparentprevAlso, the chatbot proposed a very reasonable solution. Book the fight, send us a death certificate when you have it and you’ll get the discount. That’s actually what the policy should be and it’s a quite reasonable error for a human to make too. reply yieldcrv 16 hours agoparentprevor its buried on the 4th page of the disclaimer if you read them there is often stuff like that, the most flagrant one I read said “everything above should be considered apocryphal” reply Silhouette 14 hours agoparentprevPresumably this is why there is a trend in consumer rights legislation towards being explicit that anything a buyer has been told by a seller or actively told the seller before the sale is material to the contract of sale regardless of the seller's small print that says \"This contract of sale we wrote by ourselves and won't negotiate with you and only this contract means anything\". Then they can't promise the world to get the sale and then wash their hands of any resulting commitments two seconds after they get your money. Which seems entirely fair and reasonable to me, whether the promise came from a real person or a chatbot. reply mathgradthrow 17 hours agoparentprevthe standard disclaimer for fraud reply hypocrticalCons 16 hours agoparentprevnext [3 more] [flagged] pavel_lishin 16 hours agorootparentThe story is about a Canadian airline, in a Canadian civil resolution tribunal. reply hypocrticalCons 15 hours agorootparentAnd? Using American technology, huffing American propaganda, voting for American political ideals. I'm Canadian and I see us as no different from Americans. We are Americans. reply quartz 18 hours agoprev> \"Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives—including a chatbot,\" Rivers wrote. \"It does not explain why it believes that is the case\" or \"why the webpage titled 'Bereavement travel' was inherently more trustworthy than its chatbot.\" This is very reasonable-- AI or not, companies can't expect consumers to know which parts of their digital experience are accurate and which aren't. reply BugsJustFindMe 17 hours agoparentForget about digital experiences for a moment. Forget entirely about chatbots. > Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives That includes EMPLOYEES. So they tried to argue that their employees can lie to your face to get you to buy a ticket under false pretense and then refuse to honor the promised terms? That's absolutely fucked. reply RajT88 17 hours agorootparentI have had this happen in a sense. I once booked a flight to meet my then-fiancee in Florida on vacation. Work travel came up unexpectedly, and I booked my work travel from ORD > SFO > TPA. Before I made that booking, I called the airline specifically to ask them if skipping the ORD > TPA leg of my personal travel was going to cause me problems. The agent confirmed, twice that it would not. This was a lie. Buried in the booking terms is language meant to discourage gaming the system by booking travel where you skip certain legs. So if you skip a leg of your booking, the whole thing is invalidated. It's not suuuuper clear, I had to read it a few times, but I guess it kinda said that. Anyways - my itinerary was invalidated by skipping the first flight, and I got lucky enough that someone canceled at the last minute and I could buy my own seat back on the now-full flight for 4x the original ticket price I paid (which was not refunded!). I followed up to try and get to the bottom of it, but they were insistent they had no record of my call prior, and just fell back on \"It's in the terms, and I do not know why you were told wrong information\". Very painful lesson to learn. I try and make a habit of recording phone conversations with agents now, if legal in where I'm physically located at the time. reply rainbowzootsuit 17 hours agorootparentIn the US at least once that notification that \"the call may be recorded for quality assurance \" happens, both parties have been notified and you're good to record regardless of the state you are in. What do you use for recording your calls? reply RajT88 16 hours agorootparentI used to use a couple of apps like Cube ACR, or Call Recorder Pro, but these no longer work, and I'm skeptical of the workarounds to get them working again. Given the new restrictions in Android 10, probably the way forward is a passthrough device which uses 3.5mm connectors to MITM the audio. I haven't found one which is a sure bet yet. reply BizarreByte 14 hours agorootparentIt's ridiculous both iOS and Android largely have no way to record calls/try to prevent it, especially when some of us live in jurisdictions where we are well within our legal rights do to that very thing. reply gray_-_wolf 16 hours agorootparentprevThe motorola android I have has phone recording in the stock phonecall app, so I just use that. reply RajT88 16 hours agorootparentNothing like that on the Samsungs I've owned. Next phone will be a Pixel, so we'll see. reply r2_pilot 16 hours agorootparentIn an active call, tap the 3 dot menu up at the top right, you should see the option to record. reply RajT88 15 hours agorootparentThat just gets me an option for \"Start RTT Call\". Nothing like call recording. I'm on an S10e, which is not that new, so that could be part of it. I bought an older phone, because I really like my top grain leather case, so I wanted to get a phone which fit the case more than I wanted a more cutting edge phone. reply gs17 15 hours agorootparentprev> Next phone will be a Pixel, so we'll see. Pixels don't have call recording enabled (in most regions?). It's frustrating that I'm forced to use a third party app to provide a feature that the phone app should have built-in, but Google decided I can't be trusted with. EDIT: It used to be geo-blocked, now it's \"This feature isn't available on Pixel.\" reply kroltan 16 hours agorootparentprevAt least GrapheneOS does have a built-in recording function. reply jyunwai 15 hours agorootparentprevIt's inconvenient, but I've observed journalists who recorded calls by putting the phone on Speaker mode to increase the volume, and then used a second device (such as a laptop or iPad) to record the call. reply onemiketwelve 15 hours agorootparentprevIs that actually true? I always thought that was one of those internet sayings that had no basis in law, ie one party, two party protections etc... reply rainbowzootsuit 11 hours agorootparentIt is. There's good information linked here. https://recordinglaw.com/party-two-party-consent-states/ reply sokoloff 17 hours agorootparentprevFortunately, in this case, the consumer had enough proof of what happened and the court rightly told Air Canada to get fucked with that argument. reply mschuster91 17 hours agorootparentprev> They tried to argue that their employees can lie to your face to get you to buy a ticket under false pretense and then refuse to honor the promised terms. That's fucked. Pretty standard behavior for big companies. Airlines and telcos are the utter worst... you have agent A on the phone on Monday, who promises X to be done by Wednesday. Thursday, you call again, get agent B, who says he doesn't see anything, not even a call log, from you, but of course he apologizes and it will be done by Friday. (Experienced customers of telcos will know that the drama will unfold that way for months... until you're fed up and involve lawyers) reply BugsJustFindMe 5 hours agorootparentI had a problem with Verizon FIOS that went on for more than half a year, where they'd charge me for a service I wasn't signed up for, then I'd call in to complain and demand a refund, then they'd refund me and apologize profusely and swear up and down that they had fixed the problem for sure and that it would definitely not happen again, then it would of course happen again the next month, rinse&repeat. Finally I filed an FCC consumer complaint which then forces a written response from the company within 30 days. I got a call a few days later from someone at Verizon's \"Executive Relations\" who fixed it immediately. It was such a frustrating dance, but the real trick is that when this happens don't mess around with a hostile company. Just go directly to the regulator agency they're required to answer to. reply morkalork 17 hours agorootparentprevI had this happen with Rogers! Another loved and treasured bigcorp in Canada! Called to tell them I was moving in 2 months and to cancel my service and the agent says, oh well if you want to still have internet in the mean time, you'll have to call back in two months when you move. Ok. Great. I do that. Agent number 2: Well you didn't cancel with X many days of notice so there's a cancellation fee now on your acoount. Pay up! I suppose agent 1 was jerking my chain so he wouldn't take the hit on his retention metrics so I don't blame him 100%. I blame Rogers bullshit system of incentives for their employees and their bullshit contracts they force on consumers who have little to no choice in the market here. reply ToucanLoucan 17 hours agorootparentprevI bet if we removed the requirement to get a lawyer and file a lawsuit that those behaviors would vanish real quick, if all you had to do as the wronged consumer was report to an authority that Company X is doing business dishonestly. reply somat 16 hours agorootparentYou are probably not wrong, however take note that this is exactly the line of thinking that led to the DMCA. Beware the law of unintended consequences. To misquote you. \"I bet if we removed the requirement to get a lawyer and file a lawsuit that those behaviors would vanish real quick, if all you had to do as the wronged copyright holder was report to an authority(the hosting service) that Company X is infringing on your copyright.\" reply izacus 15 hours agorootparentIt also led to things like easy small claims court, consumer protection agencies and DPOs which protect consumers against corporations here in Europe without them having to shell out thousands of euros for lawyers and court cases. reply MichaelZuo 9 hours agorootparentDo all the upsides outweigh all the downsides, such as DMCA? reply ToucanLoucan 16 hours agorootparentprevPoint to you there, but also I would never in a thousand years trust a company, especially a publicly traded one, with that kind of power. reply mcny 17 hours agorootparentprev> I bet if we removed the requirement to get a lawyer and file a lawsuit that those behaviors would vanish real quick, if all you had to do as the wronged consumer was report to an authority that Company X is doing business dishonestly. They can still appeal the decisions of the authority in the courts of law. They will just get the authority disbanded altogether. Previously, on hn `Amazon argues that national labor board is unconstitutional (apnews.com)` https://news.ycombinator.com/item?id=39411829 reply throwboatyface 16 hours agorootparentprevYou can file in small claims without a lawyer. reply mschuster91 17 hours agorootparentprevIn Germany, we have the Verbraucherschutz as a low-level institution to handle such claims. They can and do consolidate such reports and can file lawsuits if there is evidence of systemic misbehavior. reply renewiltord 17 hours agorootparentprevI think I could price for a few tens of thousands of dollars a service that creates a bunch of such wronged consumers. There are a bunch of homeless people in San Francisco we will represent. To make it easy for someone on the streets to be able to complain, we actually operate as a non-profit that advocates on their behalf for having wronged the company. You could use my service to attack another company on demand, but I advise that you do it at critical moments. Ideally, two weeks before a big launch or so should do it so that we have enough time to stagger out sufficient number of complaints. reply dmurray 15 hours agorootparentThis isn't a new business idea. There are law firms that specialize in consumer class action suits, and part of their skillset is finding lots of wronged consumers to represent. Signing up homeless people by the hundred isn't exactly the gold standard of what these firms do, but it's not a million miles removed. reply ysavir 15 hours agorootparentprevI can sort of see it. On the one hand, it's reasonable to hold them accountable when an employee gives you the wrong discount. But if an employee, on their last day at work, decides to offer the next person calling all of the seats on a single flight for just $10, I think we'd all agree that it would be unreasonable to expect the airline to honor that offer. It's the degree of misinformation that's relevant. reply BugsJustFindMe 14 hours agorootparent> But if an employee, on their last day at work, decides to offer the next person calling all of the seats on a single flight for just $10, I think we'd all agree that it would be unreasonable to expect the airline to honor that offer. The airline is free to go after the lying employee for compensation if they find out too late. It is never acceptable for the airline to cheat their customers. reply FergusArgyll 17 hours agorootparentprevStandard for airlines reply wing-_-nuts 17 hours agoparentprevI once ordered a gift for my father for christmass. The order page indicated that it would arrive on time. When it didn't arrive, I requested a refund. They then pointed to their FAQ page where they said that orders during the holidays would incur extra processing time, and refused the refund. I wrote back that unless they issused a refund, I would issue a charge back. You don't get to present the customer with one thing and then do otherwise because you say so on a page the customer has never read when ordering. They eventually caved, but man, the nerve. reply dataflow 16 hours agorootparentThis actually sounds like an interesting case to me because the details make a huge legal difference in my mind. (But IANAL, maybe I'm entirely off base here.) E.g., did they tell you the shipping date after you placed the order, or before? If it was afterward, then it can't have invalidated the contract... you agreed to it without knowing when it would ship. If they told you before, then was it before they knew your shipping address, or after? If it was beforehand, then again, it should've been clear that they wouldn't be able to guarantee it without knowing the address. If it was after they got the address but before you placed the order, then that makes for a strong case, since it was specific to your order and what you agreed to before placing it. reply hiatus 16 hours agorootparent> If it was afterward, then it can't have invalidated the contract... you agreed to it without knowing when it would ship. > Sellers have to ship your order within the time they (or their ads) say. That goes whether they say “2-Day Shipping” or “In Stock & Ships Today.” If they don’t give a time, they must ship within 30 days of when you placed your order. from the FTC https://consumer.ftc.gov/articles/what-do-if-youre-billed-th... reply dataflow 14 hours agorootparentI believe that's referring to \"shipment\" in the sense of \"when this gets mailed\", not the arrival time like we were discussing? I guess it might depend on where the delays were incurred, and what exactly was promised. reply grotorea 16 hours agorootparentprevWhy would they tell the date after placing? Every online shopping I ever used shows the shipping date together with shipping price and shipping options if there are any. reply dataflow 15 hours agorootparentIf they're going to ship immediately, then they can know before you place the order. If there's another entity involved (third party seller, backorder, etc.) then they might not be able to know when it will be shipped with much certainty. reply bluGill 16 hours agoparentprevI expect employees to know the correct answers and give them to me. When an employee says something that contradicts other policy pages I'm expecting that to be a change to company policy to me - they represent the company. If the company doesn't agree to that, then they need to show the employee was trained on company policy and was disciplined (on first offense maybe just a warning, but this needs to be a clear step on the path to firing the employee) for failing to follow it. Even then they should stand by their employee if the thing said was reasonable (refund you $million may be unreasonable, but refund purchase price is reasonable) reply wredue 17 hours agoparentprevThis is especially true since, as it comes to refund policies, businesses make it exceedingly difficult to sort through the information. It’s not the consumers fault that the AI hallucinated a result (as they are known to do with high frequency). reply manicennui 16 hours agoparentprevThis line of argument is crazy and infuriating. \"Air Canada essentially argued, 'the chatbot is a separate legal entity that is responsible for its own actions,' a court order said.\" Do they expect people to sue the chatbot? Are they also implying that people have to sue individual agents if they cause a problem? reply grotorea 15 hours agorootparent> 27. Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives – including a chatbot. It does not explain why it believes that is the case. In effect, Air Canada suggests the chatbot is a separate legal entity that is responsible for its own actions. This is a remarkable submission. While a chatbot has an interactive component, it is still just a part of Air Canada’s website. It should be obvious to Air Canada that it is responsible for all the information on its website. It makes no difference whether the information comes from a static page or a chatbot. > https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/202... Real legal comedy. Since this was in small claims court maybe it was an amateur on Air Canada's side? reply lenerdenator 15 hours agorootparentprevIf they could reasonably expect to be able to hire people who would agree to accept all liability incurred during their work for the company, they absolutely would. Same with chatbots. Even better, because once it's \"trained\", you don't have to pay it. There's a few instances of expecting digital entities to shoulder the entirety of legal liability here in the last few years; DAOs are another example of this in the crypto space. reply VHRanger 17 hours agoprevWhy are we posting this when it's just rephrasing an Ars Technica article? It's even mentionned at the bottom: \"This story originally appeared on Ars Technica.\" Give the clicks to the original article: https://arstechnica.com/tech-policy/2024/02/air-canada-must-... reply hoppyhoppy2 17 hours agoparentBoth sites are owned by Conde Nast reply emaro 16 hours agoparentprevI don't like Ars Technica because they break reader mode and load articles chunk by chunk – I consider it hostile towards the user and am glad since Wired work much better. reply chucksmash 15 hours agorootparentWhat browser? Just tried it out on Firefox for Android (version 122.1.0, with uBlock Origin enabled but JS still allowed on ars) and for the link above, I see the whole article after immediately switching to reader mode. reply PontifexMinimus 13 hours agorootparentReader mode also works for me. (Firefox 122.0 for Ubuntu) reply JJMcJ 18 hours agoprevIn early days of computerization, companies tried to dodge liability due to \"computer errors\". That didn't work, and I hope the \"It was the AI, not us\" never gets allowed either. reply Aardwolf 18 hours agoparent> That didn't work It worked in the British Post Office Scandal: https://en.m.wikipedia.org/wiki/British_Post_Office_scandal reply hnfong 17 hours agorootparentSince it's a scandal, it worked until it didn't. And AFAICT \"the computer did it\" wasn't the argument, it was \"the computer did it so it must be correct because the experts said so\". reply alexey-salmin 17 hours agorootparentSo did they held any humans accountable then? This wasn't the case when I checked but I probably missed some updates reply denton-scratch 16 hours agorootparent> So did they held any humans accountable then? Not yet. The Inquiry is still taking evidence. They haven't taken evidence from the big-hitters yet, that begins in April. In fact the Inquiry doesn't \"hold humans accountable\"; but they can compel witnesses, who testify under oath. Gerald Barnes is the Fujitsu engineer who stated in the prosecution cases that Horizon was reliable (\"robust\"). His testimony to the Inquiry has been delayed, because on the morning he was supposed to testify, the Post Office \"discovered\" a million or so emails that they'd failed to disclose. So he'll be on the stand in April, along with the senior execs. The Inquiry videos make quite pleasant watching; the lawyers and the judge are immaculately polite, there are no trick questions, and it's all about finding out what happened. I'm looking forward to seeing the senior execs on the stand. The police are following the Inquiry; nobody's been charged, and my guess is they'll hold off on charging people until the Inquiry is over (that's part of the purpose of statutory inquiries). So the succession of Post Office Ministers that have overseen this disgrace will all be out of office by then. reply CoastalCoder 17 hours agorootparentprevI think this is a very different kind of thing. IIUC: With Air Canada, the question is whether or not a chat bot can be treated as a company representative that makes binding commitments. With the British Post Office, the issue is whether or not a software system is inscrutable during legal proceedings. reply denton-scratch 16 hours agorootparent> With the British Post Office, the issue is whether or not a software system is inscrutable Not sure what \"inscrutable\" means in that context. Is it supposed to mean it can't be scrutinized? A law was passed some years ago that says evidence obtained from a computer system should be accepted as true, unless evidence is provided that opens it to question. That means, in the Post Office case, that postmasters couldn't demand that the Post Office prove that Horizon was working correctly. They had to prove that it was defective, which was difficult; they were kicked out of their shops, and denied access to their own records, including the Horizon terminals they had been using. reply bluGill 16 hours agorootparentprevOf course if the chatbot cannot what is the point of them - I'll have to get something else to verify anything the chatbot says. Sure the chatbot can say \"hello\" in 10,000 words or whatever, but it can't do anything useful. reply JJMcJ 17 hours agorootparentprevThe fallout from this isn't done yet. reply lenerdenator 15 hours agoparentprevThey still do. Bank errors are an example. reply yifanl 17 hours agoprevhttps://decisions.civilresolutionbc.ca/crt/crtd/en/item/5254... The resolution is n amazingly clear piece of legal writing that explains the involved thought process of the the decision and then awarding the damages. I might end up using this pattern for writing out cause and effect. reply lawkwok 16 hours agoparentThanks for sharing. I really enjoyed the read and the concise decision-making and legal terms used. reply lgleason 17 hours agoprevGood. If you use a tool that does not give the correct answers you should be held liable for the mistake. The takeaway is, you better vet your tool. If the amount of money you loses from mistakes with the tool is less than the money you saved using it then you make money, if not, you may want to reconsider that cost saving measure. reply lenerdenator 15 hours agoparentWhat if the company responds that they don't know how to vet the tool? After all, we're still not 100% sure how LLMs make their decisions in what they string together as output, so the company's not _technically_ lying. reply danpalmer 18 hours agoprevI'm glad to see that cases are starting to be decided about the liability of using AI generated content. This is something the general public should not need to second-guess. reply bitmasher9 17 hours agoparentHonestly LLMs aren’t ready for customer service. If I’m talking to a company I need to have a high degree of accuracy. LLMs are less accurate than trained humans. reply denton-scratch 16 hours agorootparentMe: Chatbot, are you speaking for $COMPANY? Are you $COMPANY's agent? Can I take your statements as being $COMPANY's legal position? Chatbot:Me: Please put me through to a person that can articulate $COMPANY's legal position. This conversation can serve no more purpose. reply danpalmer 17 hours agorootparentprevThis is my personal perception, but I think it's important that there is a clear definition of liability so that companies are able to make their own determinations of what is ready and what isn't. reply dtjb 15 hours agorootparentprevFew front-line agents have deep knowledge about their company's products or services. They trace their finger through some branches on a flowchart then dictate from a knowledgebase. reply pajamanaut 15 hours agorootparentFlowcharts are reliable reply dtjb 15 hours agorootparentAgreed, and I think following flowchart-type logic is within today's AI capabilities. This thread is full of people getting inaccurate responses from humans. I think when it comes to accuracy, a well-trained LLM likely beats the status quo of high-churn low-paid employees following a rote diagram. Of course there should always be a way to reach a human, a senior agent with actual knowledge that can be applied in subjective ways to solve more complex problems. reply upofadown 17 hours agoprevPeter Watts comments: * https://www.rifters.com/crawl/?p=10977 reply alsetmusic 16 hours agoprevMy father died in hospice the night before a flight to see him. I missed the flight because there was no longer any reason to get to the airport before dawn. I called to reschedule a few hours later. The human on the other end rescheduled and gave me a bereavement rate. She told me it was less money, but didn't mention the reason. I didn't put that together until later. She just helped me out because she had compassion. I am too cynical to think that an AI controlled by a corporation will do this. reply liendolucas 16 hours agoprevGood. I hope people out there also discover chatbot holes and exploit them. Chatbots are one of the most useless and time wasters things out there, they literally serve for absolutely no purpose. And most of them work exactly like nested dropdowns where you select one option after the other. Oh and when you really want to talk to a human being in almost every scenario that option is not available. What a wonderful world powered by \"AI\". reply logicalmonster 17 hours agoprevWould a company be liable to uphold its promises if a rogue human customer service agent promised something ridiculous such as 1 million dollars worth of free flights? reply pizzalife 16 hours agoparentNo, the specific employee would most likely be liable if there is criminal conduct (this varies obviously). But a chatbot is not a person. reply npongratz 16 hours agorootparent> But a chatbot is not a person. We live in an interesting world. In the US, a corporation is legally a person, and a chatbot is not a person[0]. I'm looking forward to the first Supreme Court case involving a corporation consisting of chatbots. [0] I'm handwaving in this lead-in to the fantasy here, so, dear reader, please give me a break for oversimplifying and ignoring technicalities. reply bluGill 16 hours agoparentprevThe company would have to prove the human was knowingly acting outside of their job/training and was disciplined for that. Such discipline must be on the path to firing the employee if the behavior isn't corrected. Note that training is important here, an employee who isn't trained is assumed to have more authorization than someone who is. Or in this case they need to take the AI out of service immediately until they can get a corrected version that does not do such a thing. I will accept that the AI can be tricked to do such a thing and remain in service, but only if they can show the tricks are something an honest human wouldn't attempt. (I don't know what this is, but I'll allow the idea for someone else to propose in enough detail that we can debate if a honest people would ever do that) reply grotorea 16 hours agoparentprevGoogle \"too good to be true contract law\" and there's some info, seems the answer is \"no\". reply thefatboy 12 hours agoparentprevhopefully reply jerf 15 hours agoprevI think this article's full import is not being properly processed yet by a lot of people. The stock market is in an absolute AI frenzy. But this article trashes one of the current boom's biggest supposed markets. If AIs can't be put in contact with customers without exposing the company to an expected liability cost greater than the cost of a human customer representative, one of their major supposed use cases is gone, and that means the money for that use case is gone too. There's probably halo effects in a lot of other uses as well. Now, in the medium or long term, I expect there to be AIs that will be able to do this sort of thing just fine. As I like to say I expect future AIs will not \"be\" LLMs but merely use LLMs as one of their component parts, and the design as a whole will in fact be able to accurately and reliably relay corporate policies as a result. But the stock market is not currently priced based on \"AIs will be pretty awesome in 2029\", they're priced on \"AIs are going to be pretty awesome in July\". LLMs are a huge step forward, but they really aren't suitable for a lot of uses people are trying to put them to in the near term. They don't really \"know\" things, they're really, really good at guessing them. Now, I don't mean this in the somewhat tedious \"what is knowing anyhow\" sense, I mean that they really don't have any sort of \"facts\" in them, just really, really good language skills. I fully expect that people are working on this and the problem will be solved in some manner and we will be able to say that there is an AI design that \"knows\" things. For instance, see this: https://deepmind.google/discover/blog/alphageometry-an-olymp... That's in the direction of what I'm talking about; this system does not just babble things that \"look\" or \"sound\" like geometry proofs, it \"knows\" it is doing geometry proofs. This is not quite ready to be fed a corporate policy document, but it is in that direction. But that's got some work to be done yet. (And again, I'm really not interested in another rehash of what \"knows\" really means. In this specific case I'm speaking of the vector from \"a language model\" and \"a language model + something else like a symbolic engine\" as described in that post, where I'm simply defining the latter as \"knowing\" more about geometry than the former.) reply frud 15 hours agoprevThe real story here is that Air Canada's lawyers argued, among other things, that the chatbot was a separate and independent legal entity from Air Canada and therefore Air Canada was not obligated to honor the made up policy. In other words, this was possibly the first historical argument made in a court that AI's are sentient and not automated chattel. reply martopix 15 hours agoparentOr maybe it simply means that it's a service offered by another company reply anonu 17 hours agoprevReminds me of https://www.moralmachine.net/ reply pier25 15 hours agoprevDid Air Canada use a chatGPT for their legal defense? Also: > Air Canada essentially argued that \"the chatbot is a separate legal entity that is responsible for its own actions,\" What does this mean? That the chatbot was provided by a third party hence they are responsible for the content provided? Or that, literally, a chat bot can be considered a legal entity? reply spywaregorilla 16 hours agoprevThe real desire here is to get it to promise a lifetime of free service. edit: arguing that the chatbot is a separate legal entity is a wild claim. It would imply to me that air canada could sue the ai company for damages if it makes bad promises; not that air canada is excused from paying the customer. reply bluGill 16 hours agoparentIf I were the agent who made that promise what would they do to me? Fire me - then fire the chatbot. Put me through training - then train the chatbot. Stand behind me and do what I say - then stand behind their chatbot. I bet they already have policies in place for this - while how they apply to AI may be different, they shouldn't let this slide. reply spywaregorilla 15 hours agorootparentThe key term here is \"apparent authority\". You don't need to be granted authority for each and every thing you do, but if you start grossly doing things you were not supposed to do, that's mostly just fraud on your end. The common example in textbooks is someone continuing to do business as an employee after having been fired. They can still make valid deals with other entities due to apparent authority if they're not clearly made out to be separated. reply jerf 15 hours agoparentprevThis is pretty close: https://www.msn.com/en-us/autos/news/ai-blunder-chat-bot-sel... reply Cpoll 15 hours agoparentprev> promise a lifetime of free service. That might be too wishful thinking. The tribunal would take into account damages, and whether it was reasonable to believe that you're entitled to free service. In this case, the chatbot promised a ~$800 discount, and the tribunal awarded ~$800. But I doubt they'd make the same decision again, or deem the lifetime service enforceable/un-cancellable. reply spywaregorilla 11 hours agorootparentI think it's within the bounds of reason if you can cite emotional damages to grant pretty much any recourse. It maybe won't fly if you're clearly trying to trick the AI, but I think there will be general plausible deniability. It may feel weird, but it's utterly insane to delegate customer interactions to an agent that has nobody's interest in mind, not even their own, whom you cannot trust to abide by policy. Similarly, if the bot negotiated any sort of special deal, I think it would be very, very difficult to argue that it lacked apparent authority to make deals or that it's not a fair consideration. reply hunter2_ 17 hours agoprevI don't really understand why generative LLM output is being presented to (and interpreted by) users as intelligence. This seems incredibly disingenuous, and I would prefer to see it characterized as as fiction, creative, realism, etc. -- words that make it clear to average people that while this might be entertaining and even mimicking reality, it's completely distinct from the writing of a person. Disclaimers (often small, low contrast, or \"ToS;DR\"-esque) are insufficient when the UI is specifically crafted to appear like chatting with a person. reply dnussbaum 17 hours agoprevThis type of failure is becoming more and more common as companies roll out AI systems without robust accuracy audits & human supervision. I'm working on something to make this easier - reach out if I can be helpful (email in bio). reply dlqx 12 hours agoprevThis mistake could have been done by a human agent as well and the consequences would have been most likely the same, wouldn't it? reply lazycog512 17 hours agoprevThis is why you link the source content from the RAG pipeline instead of pretending the bot knows everything. reply fnordpiglet 15 hours agoprevThis should hold. If AI can remind us of what humane policies are, so be it. reply stainablesteel 15 hours agoprevi would expect something like this to severely stunt chatbot adoption i think the only reason this should go through is if it didn't have a proper disclaimer at the beginning of the conversation reply zzz999 10 hours agoprevSometimes the courts get it right reply jeffbee 17 hours agoprevI don't even see how this is a big story. An expressed representation about what a product is or does, how it works, and the terms of sale in consumer contexts are binding on the seller. It is the same in America. If you go into a store and they say that you can return it if you don't like it, then you can. If you buy a TV and the guy at the store tells you it also makes pancakes, you can get your money back if it turns out that it does not make pancakes. This is true even if the representation is made by some 16-year-old kid working at Best Buy. By extension it would still be true even if it is made be an automaton. reply acdha 16 hours agoparentThe interest in this story comes from the fact that Air Canada tried to fight it using the argument that they aren’t liable for anything their agents say. Businesses around the world are so eager to start laying people off that it’s almost certain that they will not be the only ones to try a variation on that claim. I see the important point being reinforcing liability laws and especially not allowing them to shirk responsibility with disclaimers — otherwise we’ll just see more companies cut staffing on their phone support to force people to use a system with a “you are responsible for checking everything this chat bot says for accuracy” notice. reply jeffbee 16 hours agorootparentI mean, they're welcome to try, and this can't be the first attempt. But there's plenty of jurisprudence at least in the USA that you can't disclaim expressed representations. reply matthewfelgate 17 hours agoprevhahahahaha reply viranchee 16 hours agoparenthahahhahaha reply im3w1l 17 hours agoprevI hope there can be some reasonable middle ground. I think in this case it's good the woman got her money. But Air Canada, presumably scared of what the next case might cost them decided to turn the chat bot off entirely. I think that's a bit unfortunate. I don't know what the solution looks like. Maybe some combination of courts only upholding \"reasonable\" claims by AI. And then insurance to cover the gaps? reply lgleason 17 hours agoparentIf the company is not confident with the output of the chatbot or the liability then they should turn it off. All business decisions are based on assessing risk. Ultimately either insurance will need to step in or the company is willing to take the hit for mistakes their system makes because it saves them more money. This is how fraud protection it assessed with banks, credit cards etc. as well. reply bgirard 17 hours agorootparentI agree. If bad information is given and the customer makes a decision on it, then either the customer must bear the risk or the business. I think it's more fair for the business to be out the money. In this case this could have just been a $650 'bug bounty' had Air Canada issued a quick refund. A reasonable QA expense to find out that your AI agent is misleading your customers. reply itopaloglu83 17 hours agoparentprevHmm. “Let me turn on this lead water pipeline which may or may not poison an entire town and then blame faul when it does.” I don’t think failing to take adequate precautions is preventing AI tools from being used. I think this was plain corporate incompetence and greediness. They started using a system without properly testing it and don’t want to pay for the consequences. What if Boeing says “Oops. We forgot to put the bolts that keeps the door in place but we shouldn’t be kept accountable for our actions”? The fact that they used a tool for it shouldn’t change the outcome unless we are going to create indemnity for big cooperations. reply avn2109 17 hours agoparentprev>> \"... turn the chat bot off entirely\" This sounds like an ideal outcome! reply SketchySeaBeast 17 hours agoparentprevWhy is that unfortunate? I honestly don't think there's any value in using AI for this if it's not guaranteed to give you correct answer. reply im3w1l 16 hours agorootparentI think that there can be an error rate that is low enough to still be very useful and a net-positive for the economy. But if there is a small chance to just end the company on the spot, then no one will use it despite the benefits. So how can we have the good parts without the bad, this is my question. reply SketchySeaBeast 16 hours agorootparentI guess the onus is on people to prove that using AI specifically is somehow a net-positive for the economy because that's in no way a given. But I don't feed good about Air Canada trying to make that case. reply pjc50 17 hours agoparentprevThe \"reasonable\" middle ground is that chatbots should not be used unless and until their answers are reliable. reply dukeyukey 17 hours agoparentprevI don't see why chatbots can't be kept to the same standard as human staff. If an airline support agent lied to me to sell me tickets, no shit I'd want a refund and compensation! Chatbots should be allowed to be wrong, but the company should be prepared to face the consequences of that. reply rideontime 16 hours agoparentprevI struggle to comprehend how it's \"unfortunate\" that they turned off the customer support bot that lied to people. reply im3w1l 16 hours agorootparentIdeally they would fix it rather than give up on the idea entirely. reply callalex 16 hours agoparentprevOr just hire a customer service department. That worked fine for the last hundred or so years. reply seguemeters31 18 hours agoprev [–] https://twitter.com/zackbshapiro/status/1759723309883617308 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Air Canada had to refund a passenger $650.88 after the airline's chatbot provided inaccurate information on bereavement travel policies.",
      "Initially, the airline refused liability for the chatbot's errors but was later required to issue a partial refund to the misled passenger.",
      "Following the incident, Air Canada disabled its AI chatbot, introduced to enhance customer service but instead led to dissatisfaction for at least one traveler."
    ],
    "commentSummary": [
      "The debate focuses on the responsibility of companies, especially regarding AI chatbots in customer service, exemplified by Air Canada's legal struggle over its chatbot's dissemination of inaccurate information.",
      "Discussions emphasize the importance of transparency, providing correct information, and upholding consumer rights in customer interactions.",
      "Various opinions are shared on the reliability and constraints of AI in customer service, as well as the impact on customer satisfaction and legal obligations, highlighting the quest for equilibrium between AI, human touch, and accountability in business operations."
    ],
    "points": 242,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1708530023
  },
  {
    "id": 39462516,
    "title": "Unexpected Namesakes: Larry Page, Glen Bell, and More (2020)",
    "originLink": "https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/",
    "originBody": "An incomplete and infuriating list. PageRank (Larry Page) MySQL (My Widenius) Debian (two people named Deb and Ian) New Relic (anagram of Lew Cirne) Price Club (Sol Price) Taco Bell (Glen Bell) Westlake, Los Angeles (Henricus Wallace Westlake) Outerbridge Crossing (Eugenius Harvey Outerbridge) Main Street, San Francisco (Charles Main) Erlang (Agner Erlang) Mars (candy) (Franklin Mars) A few additions that others have suggested: Snowflake, AZ (two people named Snow and Flake) Lake Mountain (George Lake) German chocolate cake (Samuel German) Baker’s Chocolate (Walter Baker) Unilever (Lever Brothers) Even more, after this made the rounds again in 2024: Brown noise (Robert Brown) Southern blot (Edwin Southern) Smart & Final (Jim Smart, Hildane Final) Elo (Arpad Elo) Shrapnel (Henry Shrapnel) Leatherman (Timothy Leatherman) Max Factor (Max Factor) BernzOmatic torches (Otto Bernz) Waterfall Glen (Seymour Waterfall) Gasoline (John Cassell) ColorBrewer (Cynthia Brewer) Unfortunately, Rocket Lab does not seem to be named after seed investor/aerospace entrepreneur Mark Rocket.",
    "commentLink": "https://news.ycombinator.com/item?id=39462516",
    "commentBody": "Things unexpectedly named after people (2020) (rolandcrosby.com)238 points by prawn 7 hours agohidepastfavorite217 comments saithound 4 hours agoThe generic Hungarian word for trash cans, 'kuka', unexpectedly originates from the names of German industrialists Johann Josef Keller and Jakob Knappich. The story is quite involved, to the extent that one would be tempted to dismiss it as a folk etymology, if it wasn't ridiculously well-attested. Keller and Knappich founded an acetylene factory in Augsburg, Bavaria in 1898. They named it Keller und Knappich Augsburg (KUKA). Their venture quickly expanded into manufacturing of welding equipment, household appliances, and eventually car parts and heavy industrial robots. In the 1920s, they manufactured hoppers for Hungarian municipal garbage trucks, which they stamped prominently with their logo. This led to refuse trucks being known colloquially referred to as \"kukás auto\" (lit. \"car with KUKA written on it\"), even long after KUKA stopped manufacturing those hoppers. And the noun kuka, referring to trash cans, arose as a backformation from there! And this is how two German industrialist gave their names to Hungarian trash cans. Of course, it probably helped that a word kuka existed in the Hungarian language already at that point (as an unrelated adjective referring to a mute person), much like snowflake existed before Snow and Flake founded the town. reply EvanAnderson 4 minutes agoparentContinuing the trash theme, \"Dumpster\" comes from the Dempster family name: https://en.wikipedia.org/wiki/Dempster_Brothers reply heavypixels 41 minutes agoparentprevThe story continues in the Netherlands, where a company licensed the Keller and Knappich design to produce their own trucks. Of course, they released it under their own names: Klinkenberg & Koster, abbreviated as KLIKO, which they too stamped in large font on their trucks and bins. And today \"kliko\" is the common Dutch name for the wheelie bin. reply SargeDebian 14 minutes agorootparentI guess only in those parts of the country where Kliko had a large market share at some point. The Otto brand name is so strong in the east that they have no idea what a Kliko is. reply yau8edq12i 3 hours agoparentprevFunny enough, the French word for a garbage can is also named after someone: https://en.wikipedia.org/wiki/Eug%C3%A8ne_Poubelle the prefect who made the use of garbage containers mandatory in Paris. reply Kwpolska 2 hours agorootparentContinuing with the trend, the Polish word for outhouse is sławojka, named after Felicjan Sławoj Składkowski, a prime minister who is responsible for making them mandatory in the countryside in the 1920s, and apparently touring the country to check how sanitary they are. https://pl.m.wikipedia.org/wiki/S%C5%82awojka reply tkgally 1 hour agoparentprevThe most common word in Japanese for “stapler” is hochikisu or hotchikisu, which seems to come from the name of the E.F. Hotchkiss Company [1], an American manufacturer whose staplers were imported to Japan in the early 20th century. Some Japanese sources [2] say that the word comes from Benjamin Hotchkiss [3], an inventor of machine guns, and that is what I learned many years ago, but that etymology apparently has not been documented [4]. [1] https://connecticutmills.org/find/details/e.h.-hotchkiss-co [2] https://gally.net/temp/20240222hotchikisugogensetsu.jpg [3] https://en.wikipedia.org/wiki/Benjamin_B._Hotchkiss [4] https://wis.max-ltd.co.jp/about/company/trivia/h_story/inven... reply throwaway2037 36 minutes agorootparentMore: https://jisho.org/forum/5d132f2ed5dda71944000054-origin-of-n... reply lb1lf 2 hours agoparentprevIncidentally, 'kuk' is a Norwegian word, a somewhat pejorative synonym for 'penis', but almost as versatile as the four-letter word - it can be used as an adjective, a verb, an adverb, a noun etc. Saying that 'something is kuka/kuket' means it is messed up beyond repair. Even as I approach my fifties, I have to stifle a chuckle every time I see a KUKA robot. Kids never really grow up, I guess. As for things named after people, the probably most prominent example in Norwegian today is the so-called 'Brustadbu', named after former minister of children and families, Sylvia Brustad. The government she was in introduced a new law governing opening hours for convenience stores, effectively barring stores above 100sqm (1,100 sqft) from being open on Sundays. Predictably, this gave rise to a lot of stores measuring 99.9sqm, colloquially referred to as 'Brustadbu'/'Brustad sheds'. reply Gare 1 hour agorootparentKuka in Croatian means \"hook\" so at least their robotic arms are somewhat aptly named. reply mrighele 2 hours agoparentprevInteresting, it seems to be the same KUKA that know produces robots reply lepetitpedre 2 hours agoparentprevDon’t google kuk(a) in Sweden reply jansan 2 hours agoparentprevInteresting that this occured during the k.u.k. monarchy. reply porphyra 6 hours agoprevMain Street in San Francisco was named after Charles Main. https://mountainviewpeople.blogspot.com/2007/09/charles-main... Here are some more interesting ones... * Mason jar was named after John Landis Mason, 19th century American tinsmith * Guppy (fish) was named after Robert John Lechmere Guppy, 19th century British naturalist * Silhouette was named after Étienne de Silhouette, 18th century French politican * Bloomers (women's clothing) was named after Amelia Bloomer, 19th century American women's right advocate * T&T Supermarket (Canadian supermarket chain) was named after Tina and Tiffany, daughters of the founder, Cindy Lee * Bluetooth was named after Harald Bluetooth, 10th century king of Denmark Things NOT named after people: * \"Aberration\" has nothing to do with Ernst Abbe, optics scientist who wrote about chromatic aberrations. It comes from the Latin root. reply senectus1 5 hours agoparent>Main Street in San Francisco was named after Charles Main. I feel this might be a bit of a cheeky urban legend, or at the very least a coincidence. \"Main st\" has been used in western city's and towns for a lot longer than SF has been around. reply GeneralMayhem 3 hours agorootparentYes, but the Main St. in San Francisco hasn't been around nearly as long as SF has. When the downtown grids were laid out, what's now Main St. was underwater. It didn't exist as land, let alone a road that was either significant or central. By the time it was filled in, it was home to Charles Main's mining provision shop. The other way you can tell that it's not \"main\" is that it's parallel to 1st Street. Most cities don't have both a Main and a 1st, they have one or the other (which is why \"Second Street\" is the second-most common street name in the United States behind \"Main\", with \"First\" coming in third place). If SF were using Main in the \"normal\" sense, it would be where 1st is, next to 2nd. Failing that, it would at least be the next block over from 1st, so that 1st was counting from it. Washington, DC, sort of works that way, where there's a \"zero street\" (North/South Capitol), and then 1st is the next block over. But... SF's Main St. is a full three blocks from 1st. It's just kind of in the middle of nowhere, logically speaking. And that's again because of the landfill situation - 1st St. is the first full block counting from the shoreline at the time the streets were built. reply BurritoAlPastor 5 hours agorootparentprevSan Francisco’s “Main Street” is in no way a major artery, however. Market Street served that function. Here’s an 1853 map https://rumsey.geogarage.com/maps/g3463000.html in which today’s Main Street is only a block long. The map calls it Front St, although there’s another Front St nearby which kept the name – perhaps it was renamed Main to disambiguate once the streets connected? reply GeneralMayhem 3 hours agorootparentThat map also showcases another example of this - Townsend Street is named after one James Townsend, who was the alcalde [1] in 1848, not for its location at the end of town. (Geary, Leavenworth, and Bryant are also named for pre-statehood alcaldes, but their names are less amusingly coincidental.) [1] https://en.wikipedia.org/wiki/Alcalde reply dietr1ch 2 hours agorootparentprevOoohh, so 1st St made sense back then. I always wondered why SF needed like a -4th St. reply iraqmtpizza 3 hours agorootparentprevI worked on Main for years and nobody mentioned this urban legend. while the street is not long, I would not put it past bureaucrats to name a random street Main reply blooalien 5 hours agorootparentprevIt could (conceivably) be that \"in San Francisco\" is meant literally, and that in San Francisco it's true, even if not the case for other instances of \"Main Street\" elsewhere, however; a cursory web search leads me to think \"urban legend\" is by far the more likely. It's always been my understanding that \"Main Street\" either is currently, or was at some time nearer the town / city's beginning the main street that went through \"downtown\". reply GeneralMayhem 2 hours agorootparentIt is literally true specifically for San Francisco. The street now named Main in San Francisco didn't exist for the first few decades after the city's founding and grid-laying (it was underwater, just off shore in San Francisco Bay!), and at no point has it been a street of any particular prominence. reply steve_gh 2 hours agorootparentprevIn the heart of the Georgian city of Bath we have 3 streets designed by the architect John Wood. The streets are named Quiet St, John St and Wood St. It is claimed that the naming of these streets is based on the 3 words most regularly used in the clearly passionate council meetings of the time. reply bazil376 5 hours agorootparentprevOr done in the first place for the sake of the double meaning reply a_gnostic 4 hours agoparentprevMight have been named after Shinzo Abe. reply jszymborski 3 hours agoprevThis one is a bit too niche but I love it: In Quebec, there is a tax one pays to your municipality upon the purchase of a house called the \"Taxe de Bienvenue\". This translates to \"Welcome Tax\", which it commonly is called by anglos. Many clever folks might overhear you call it that and say \"Welll actuuallyy it is named after a minister by the name of Jean Bienvenue, and so that translation is incorrect\". Well, it turns out that's actually incorrect, and was mistakenly repeated by a local newspaper. A case of \"things unexpectedly not unexpectedly named after a person\". https://www.journaldequebec.com/2017/12/15/lex-ministre-jean... reply rob74 2 hours agoparentWell yeah, I guess the perverse naming (having to pay a tax isn't really likely to make you feel welcome) led people to look for alternative explanations... reply jszymborski 2 hours agorootparentHaha yah, it's always felt sardonic. reply rob74 4 hours agoprevThis reminds me of two (that I'm aware of, there may be more) street names in Munich that appear to be named after places but are actually named after people named after places: Passauerstraße (named after the leader of a peasant uprising rather than the city Passau) and Dessauerstraße (named after https://en.wikipedia.org/wiki/Leopold_I,_Prince_of_Anhalt-De... rather than the city Dessau) - the only clue being that streets named after places, e.g. Aachener Straße, are written with a space in the name (or rather should be, but lots of people already get this rule wrong), while streets named after people are written as one word or with dashes. Needless to say, the likelihood of these street names being spelled wrong is even higher, including in a very prominent (and expensive to fix) place: an illuminated street sign in a tunnel https://www.merkur.de/lokales/muenchen/sendling-westpark-ort... - it's hard to see in the photo, but the letters are made of individual LEDs, so correcting the mistake would basically require building a new sign. reply HuangYuSan 3 minutes agoprevGuillotine: Dr. Joseph-Ignace Guillotin Rastafari: Haile Selassie (born Lij Tafari Makonnen) reply msiemens 2 hours agoprevA few German companies: - Aldi (supermarket chain), founded by the Albrecht brothers, thus ALbrecht DIskont - Adidas, founded by ADolf DASsler - Mercedes-Benz, named by Emil Jellinek-Mercedes after his daughter Mercedes, initially with Daimler which then merged with Benz - Audi, founded by August Horch. Initially founded as _A. Horch & Cie. Motorwagenwerke Zwickau_, he later lost the rights to the name \"Horch\". He named his new company \"Audi\", which is a translation of his name into Latin: in German, \"horch\" is the imperative for \"horchen\" (to listen), which maps to \"audi\" as the imperative for \"audire\" in Latin And then there are a just a ton of companies named after their founders (Porsche, Bosch, Siemens, …) but I'm not sure if these count :) reply SargeDebian 6 minutes agoparentIt seems like these are different than the ones in the website. These are not really \"unexpected\" in the way that \"PageRank\", which is an algorithm for ranking pages, turns out not to be names after the pages it ranks, but the Page that invented it. The examples in your post are just the source of these names, which may be unknown to many, but are not unexpected. reply rob74 2 hours agoparentprevI sometimes feel that German founders are too stuck on naming their company after themself, some of them really should have looked for alternatives, e.g. https://de.wikipedia.org/wiki/Seppelfricke reply masklinn 1 hour agorootparentIt’s generally the result of them starting as personal or family businesses, so initially your brand is your name. Then the business grows so large the relationship inverts. It does not have much to do with germany really, it’s common the world over. For example, just from US car brand, you obviously have Ford, but also Olds (Ransom E.), Chevrolet (Louis and Arthur), Buick (David), Chrystler (Walter), Dodge (Horace Elgin and John Francis), … And I can’t fault a German industrialist from 1910 for not English-proofing his business. reply rob74 1 hour agorootparentIt's not the lack of English-proofing, it's the connotations that this name has for Germans: \"Sepp\" is a very common and not really glamourous first name/nickname, and \"frickeln\" means tinkering or fiddling around with something (usually with the negative sense of tinkering rather than fixing it properly). reply dewey 1 hour agorootparentprevI don't see the connection to German founders there, as many small / medium sized businesses like the one you linked are called after their founders everywhere (plumbers, printers, electricians,...) reply rzzzt 2 hours agoparentprevRudolf let his greatest opportunity slide when he named his company Puma instead of Rudidas. reply pronik 1 hour agoparentprevYou can certainly argue that Adolf's diminutive has been used for Adidas: ADI DASsler. reply tamimio 5 hours agoprevAlgorithms: Named after Al-Khwarizmi Algebra: Named after his book, Al-jabr. https://en.m.wikipedia.org/wiki/Al-Khwarizmi reply ribs 2 hours agoparentThat first one sounds so much like a folky, jokey etymology, and yet, it’s true… Given the existence of “logarithms”, it’s doubly surprising…hey, why are they called that? Wikipedia says “Napier coined the term for logarithm in Middle Latin, \"logarithmus,\" derived from the Greek, literally meaning, \"ratio-number,\" from logos \"proportion, ratio, word\" + arithmos \"number\".” reply ahmedfromtunis 3 hours agoprevAt the opposite end of things, here's Stigler's law of eponymy[0]. This \"law\" states that no scientific discovery is named after its original discoverer. It was discovered by Robert K. Merton. [0]: https://en.m.wikipedia.org/wiki/Stigler%27s_law_of_eponymy reply riffraff 2 hours agoparentThank you for this, it made me genuinely chuckle reply JKCalhoun 5 hours agoprevYou might think Morgan Hill, California was named because of a prominent feature of the landscape. Nope, named after a guy named Morgan Hill. (And the hill you see in Morgan Hill, CA is actually named El Toro — the Spanish word for \"lawnmower\". Thank you very much ladies and gentlemen, I'll be here for one more night.) reply Rygian 2 hours agoparentWhat is the relationship between the word \"toro\" (bull in Spanish) and lawnmowers? reply unwind 1 hour agorootparentThis site [1] claims to be a Spanish dictionary and it doesn't agree, it claims the word is \"la cortadora de césped\". I guess it's possible that the word has become a generic trademark [2] like Kleenex, since of course (?) Toro [3] is a brand of lawnmowers and other equipment. Edit: switching Toro's site to Spanish [4] shows that they at least use the term consistent with [1], which seems to add credibility. :) [1]: https://www.spanishdict.com/translate/lawnmower [2]: https://en.wikipedia.org/wiki/Generic_trademark [3]: https://www.toro.com/en [4]: https://www.toro.com/es-es/homeowner/walk-behind-mowers reply Zecc 36 minutes agorootparentprevSounds to me like a joke. As in: bulls eat grass. reply russellbeattie 2 hours agoparentprevOne might think that Menlo Park, California was founded after the one made famous by Edison in New Jersey, but amazingly it's the other way around. Via Wikipedia: In 1876, Thomas Edison set up his home and research laboratory in Menlo Park, New Jersey, at the time an unsuccessful real estate development named after the town of Menlo Park, California. reply mayneack 4 hours agoprevhttps://www.mayoclinic.org/diseases-conditions/legg-calve-pe... Leg calve perthes disease is named after doctors legg and calve. It presented (for me) as pain in my calf, and I spelled it wrong for 5 years. reply cookie_monsta 14 minutes agoparentLol, and you're still going... reply asystole 1 hour agoprevMy favourite is the verb mesmerize, which is named after physician Franz Mesmer: https://en.wikipedia.org/wiki/Franz_Mesmer reply PennRobotics 39 minutes agoparentTantalizing: something you really want but probably cannot get. ...named after Tantalus, the son of Zeus who was condemned to stand in water under a fruit tree, but the fruit and water were always just out of reach. He had a daughter, Niobe, and son, Pelops. Tantalum, the material, was named because it could not be easily dissolved in any acid, unlike most other metals. Despite knowing of the element's existence, researchers could not isolate the chemical in tantalite samples for decades, as it was always bound to this other material---later called niobium, although one guy thought there were two or more other metals and wanted to call the other one pelopium. Nope; all niobium! Sadly, tantalum became something of a semi-professional metal outside of capacitors, and even there ceramics and aluminums are preferred for one reason or another---usually cost. It was good for bulb filaments until tungsten displaced it. It was great for fountain pen nibs but not as great as iridium and osmium. It doesn't help with alloying because it embrittles. Other biologically inert materials are usually chosen over tantalum for implants. It could be a refractory ceramic but tantalum carbide is not as heat resistant as hafnium carbide. It also has weird nuclear properties, where someone will probably look back in a the distant future and remark, \"damn, that awesome commercial application of nuclear tantalum evaded us for centuries! What a fitting name!\" I never connected until just now that I should probably avoid breathing in any of the magic smoke when a tantalum capacitor explodes. reply BerislavLopac 1 hour agoprevA few more examples: King Street in Hammersmith, London, was named after John King, Bishop of London in 17th century. [0] Neanderthals were named after Joachim Neumann, a 17th century clergyman and composer who Latinised his surname to Neander (both literally meaning \"new man\"). He particularly loved a certain valley near Düsseldorf, so much in fact that it was renamed \"Neandert(h)al\" after him. [1] Sideburns, a once popular style of male facial hair, was named after the Union general Ambrose Burnside. [2] [0] https://en.wikipedia.org/wiki/John_King_(bishop_of_London) [1] https://en.wikipedia.org/wiki/Neandertal_(valley) [2] https://en.wikipedia.org/wiki/Ambrose_Burnside reply cjohnson318 6 hours agoprevGerman chocolate cake, per Wikipedia, \"Originating in the United States, it was named after English-American chocolate maker Samuel German\". reply planede 21 minutes agoparentFunnily enough I know this desert by its translated name to Hungarian as \"Német csokoládé torta\". reply rpicard 4 hours agoparentprevI learned this earlier today! My favorite desert too. reply legitster 2 hours agoprevFord Charcoal was made from recycled Model T scrap lumber and sold in Ford dealerships. It was bought out in the 50s and renamed to Kingsford Charcoal after the name of the town where the plant was located (the town named after Henry Ford's cousin who helped supply the lumber to Henry Ford). (Another fun fact, the charcoal process was developed by none other than Thomas Edison) reply throwaway2037 30 minutes agoparentLast part: Woah, I was surprised! > Thomas Edison designed the briquette factory Ref: https://en.wikipedia.org/wiki/Kingsford_(charcoal) reply Sniffnoy 1 hour agoprevI think one of the most unexpected words in English to be named for a person is \"guy\"! From Guy Fawkes. The word went from meaning an effigy of Fawkes, to any doll, to a person. reply Centigonal 3 hours agoprevNot quite the same, but the city of Washington, DC is named after George Washington, whose family hails from the town of Washington in northeast England. https://en.wikipedia.org/wiki/Washington,_Tyne_and_Wear reply bdowling 1 hour agoparentCincinnati, Ohio is also named after George Washington, albeit indirectly. https://www.cincinnati.com/story/news/2022/09/06/who-was-cin... reply timeon 53 minutes agoparentprevI didn't know that George was Geordie. reply dysoco 5 hours agoprevI thought the Pap Test was an acronym (PAP) but it's actually named after the physician that invented it, Papanicolaou: https://en.wikipedia.org/wiki/Pap_test reply zorked 50 minutes agoparentThe test is known as papanicolau in my country, but so many people think it's named after one of the ancient popes called Nikolaus (\"papa Nicolau\")... reply Zu_ 3 hours agoprevThe Haribo candies are named after their founder and the location (HAns RIegel + BOnn) https://en.wikipedia.org/wiki/Haribo reply block_dagger 6 hours agoprevI exclaimed out loud when I read about MySQL. I never liked the name before, and now I do. reply aitchnyu 5 hours agoparentOne guy named Lucene (db behind Elasticsearch) after wife's middle name and Hadoop after his kid's elephant toy. reply Crespyl 6 hours agoparentprevAnd MariaDB is named after her sister. reply 8n4vidtmkvmk 5 hours agorootparentAnd MaxDb is named after her brother reply signalToNose 4 hours agorootparentNow I’m curious about MongoDb reply Crespyl 4 hours agorootparentMongoDB only pawn in game of life. reply kahirsch 3 hours agorootparentSanta Maria! reply timeon 50 minutes agoparentprevI wonder what is proper pronunciation. reply wiradikusuma 5 hours agoparentprevSame here. I always thought it was lazy naming, like \"My\" + some topic. Now I respect the name. reply card_zero 4 hours agorootparentOh it's the Finnish girl's name, like Little My from the Moomins. That is definitely an improvement, reading it as the possessive pronoun makes it look like the name of some pushy preinstalled crapware. YourSQLExperience. EnjoyData. MyCloudStorageIDidntAskFor. reply anttihaapala 3 hours agorootparentWell, there is even more to it. First of all the name My is (Finland-)Swedish, the Finnish name of the character is Pikku Myy. The Finnish name registry contains only ~2 mentions before the publication of the hugely popular first Moomin book (and none for Myy) so this girls' name and hence the name in the MySQL really comes from the Moomin stories. And finally, the name of Little My is a play on the Greek small letter mu, i.e. μ, and you'd be closer to the real idea and original pronunciation if you thought of it as MuSQL (μSQL) instead ;) reply buildsjets 4 hours agoprevOrcas Island, Washington. Not named after Orca whales. Named after Juan Vicente de Güemes Padilla Horcasitas y Aguayo, 2nd Count of Revillagigedo, the Viceroy of New Spain who sent an exploration expedition under Francisco de Eliza to the Pacific Northwest in 1791. reply inopinatus 3 hours agoprevThe Mackintosh is named after Charles Macintosh, inventor of the modern waterproof raincoat. The Macintosh is named indirectly after John McIntosh, a Canadian farmer and apple breeder of Scottish descent, who gave his name to the McIntosh Red cultivar. The Mackintosh building of the Glasgow School of Art is named after Charles Rennie Mackintosh, born McIntosh. 5061 McIntosh is named after Bruce McIntosh, a Canadian astronomer. Indiana Jones was named after George Lucas's dog reply raldi 3 hours agoparentHe was originally going to be called Indiana Smith but Spielberg didn’t like the name. reply rob74 3 hours agoparentprev> Indiana Jones was named after George Lucas's dog Acually he's also named after a dog \"in-universe\": SALLAH: Please... What does it always mean, this... this \"Junior?\" HENRY: That's his name. Henry Jones, Junior. INDY: I like Indiana. HENRY: We named the dog Indiana. BRODY: May we go home now, please? SALLAH (to Indy): The dog!? (laughs) You are named after the dog... INDY (embarrassed): I've got a lot of fond memories of that dog. (https://www.dailyscript.com/scripts/Indiana%20Jones%20And%20...) reply saagarjha 31 minutes agoprevOh, here's one: UC Irvine is named after the Irvine family, who named the university. The city was then named afterwards after because it didn't even exist at the time. reply jameshart 4 hours agoprevOne that surprised me recently: Coriolis force. I think I'd always assumed it was some latin-derived term to do with curling or curving, but no: named for Gaspard-Gustave de Coriolis. reply riffraff 2 hours agoparentI learned this in school when we finally had a teacher who read it with the french accent rather then the Italian/latin one (stress on last I rather then second O). Mispronunciation of words I've only read has been the bane of my life. reply lloeki 1 hour agorootparent> stress on last I rather then second O Although one can stress a word in a sentence (prosodic stress) e.g for emphasis (contrast stress) there's no such thing as stressed syllables (tonic accent) in spoken french. That's mostly why we native french people have a hard time speaking languages where stressed syllables matter and produces different phonemes for identically written syllables: the typical frenglish output is very monotonic and uniform. Combine that with the lack of back-of-throat Rs and tip-of-the-tongue TH which don't occur in french and you get your archetypal frenglish. And it's tough to learn because not only we're ill equipped by practice to produce those (proprioception and muscle strength) we also have a hard time hearing them as well, so we map those to french R and Z. Conversely for someone whose native language is accented it's hard to understand that french completely lacks these, so the only way to think of how written-french syllables sound when read is that every syllable is accented, uniformly so. reply mbork_pl 1 hour agoprevDoes \"Git\" count? ;-) (see https://en.wikipedia.org/wiki/Git#Naming) reply aaronbrethorst 5 hours agoprevOne of my favorites is that the creator of Pilates was a guy named Joe Pilates. It’s like if the inventor of yoga was named Vivek Yoga (which is not the case, to be clear.) reply netsharc 3 hours agoparentI think I was an adult when I heard the term Pilates.. why would it be intriguing, if his name were Brethorst people would be saying \"Let's do you Brethorst exercises!\" reply aaronbrethorst 2 hours agorootparentWords are weird reply bandrami 2 hours agoparentprevWhereas the founder of the male dance review Chippendales was named Suman (pronounced \"Show-man\") Bannerjee reply bandrami 2 hours agorootparentWhile we're at it, both of the S'es in \"Einstein-Bose\" should be pronounced \"sh\", so the particle class should be pronounced \"Boshons\". reply CGamesPlay 4 hours agoparentprevAlthough Bikram Yoga is named after this guy: https://en.wikipedia.org/wiki/Bikram_Choudhury reply aaronbrethorst 4 hours agorootparentIt would've been so on brand for Bikram to change his last name to \"Yoga.\" I'm honestly disappointed he didn't. reply hinkley 5 hours agoprevIt took me too long to find out that Larry named PageRank after himself and not after ranking web pages. And I was horrified when I learned that lynching is named after someone, and a perpetrator at that. reply schoen 3 hours agoparentWhereas boycott is named after the target of one: https://en.wikipedia.org/wiki/Charles_Boycott reply mbork_pl 1 hour agoprevHere's an interesting example: in the Whoniverse there is this concept of a \"deadlock seal\", which is kind of a lock which is almost impossible to break (it is (in)famously resistant even to sonic devices, which is a big deal). Apparently, it was named after an Arthur Deadlock (https://tardis.fandom.com/wiki/Deadlock_seal). I can only assume this is a deliberate joke of the authors, of course. reply SeanLuke 36 minutes agoprevAlexandria, VA (upscale historic suburb of DC, near Mt. Vernon). Captain Phillip Alexander II, a random landowner. reply adontz 2 hours agoprevAlso https://wiki.debian.org/WhyTheName reply jodrellblank 2 hours agoprev> The Child Ballads are 305 traditional ballads from England and Scotland, and their American variants, anthologized by Francis James Child - https://en.wikipedia.org/wiki/Child_Ballads reply dmurray 1 hour agoparentAnd similarly, the Children's Python is a snake that's considered suitable as a pet, named after John George Children. https://en.wikipedia.org/wiki/Children%27s_python reply xwowsersx 6 hours agoprevI've driven the Outerbridge Crossing probably hundreds of times. That one is blowing my mind. reply bobbyi 3 hours agoparentThe other NYC bridges are all (I think?) named Bridge, but they went with Crossing here because Outerbridge Bridge sounds weird. Another NYC one: A lot of people think \"major\" in The Major Deegan Expressway means it is a significant expressway, but actually the expressway is named after (Army) Major William Deegan reply andy800 2 hours agorootparentMaybe I'm weird but I think NYC area has numerous semi-poetic sounding roads (while most of the country just uses route numbers): Harlem River Drive, Cross Bronx, Major Deegan, Van Wyck, Belt Parkway, Grand Central Parkway, the Taconic, Palisades Parkway, the B.Q.E., the Sprain Brook. Plus the Verrazano and Tappan Zee. reply TMWNN 4 hours agoparentprevContext for non-New Yorkers: Outerbridge is the southernmost bridge in NYC and New York State, and Staten Island might as well be on Mars for the rest of the city. So the name \"Outerbridge\" is all the more fitting (and surprising). reply andy800 2 hours agorootparentThe George Washington Bridge and Outerbridge Crossing both connect New York City (on the east) with New Jersey (on the west) but are about 45 minutes apart, even with no traffic. reply schoen 1 hour agoprevWe might find some more good ones in https://en.wikipedia.org/wiki/List_of_eponyms_(A%E2%80%93K)#... https://en.wikipedia.org/wiki/List_of_eponyms_(L%E2%80%93Z)#... reply pasharayan 5 hours agoprevThe teddy bear was named after Theodore Roosevelt (The president of the USA), after saving a baby bear during a hunting expedition (https://en.wikipedia.org/wiki/Teddy_bear) reply rob74 4 hours agoparent\"Saving\" makes it sound like the bear survived, which is not what actually happened: > A suite of Roosevelt's attendants, led by Holt Collier, cornered, clubbed, and tied an American black bear to a willow tree after a long exhausting chase with hounds. They called Roosevelt to the site and suggested that he shoot it [...] Roosevelt refused to shoot the bear himself, deeming this unsportsmanlike, but instructed that the bear be killed to put it out of its misery reply schainks 2 hours agoprevI think this cheeky habit of biologists also deserves a mention: naming newly discovered organisms after friends or foes, depending on how they feel about those people at the time. Here is the first of four separate wikipedia pages dedicated to those names: https://en.wikipedia.org/wiki/List_of_organisms_named_after_... reply jodrellblank 2 hours agoprevThe Gardenia flower is named after Akexander Garden https://en.wikipedia.org/wiki/Alexander_Garden_(naturalist) reply bdowling 1 hour agoprevBridgestone, the Japanese tire manufacturer, is named after its founder, Shojiro Ishibashi. His surname means “stone bridge”. https://en.m.wikipedia.org/wiki/Bridgestone reply corysama 3 hours agoprev> Smart & Final (Jim Smart, Hildane Final) Thank you. The “and Final” part always seemed rather dark for a brand name… reply nicota 1 hour agoprevThe \"Sharpe Ratio\" was named after William F. Sharpe, who developed it in 1966. https://en.wikipedia.org/wiki/Sharpe_ratio reply grokkedit 3 hours agoprevAbout southern blot there is more: it's a method used to find a specific DNA sequence in a DNA sample, and was named after its creator. When a similar method was invented that applies to RNA, was of course named Southern. When the one for proteins was invented, was named Western. reply jszymborski 2 hours agoparent> When a similar method was invented that applies to RNA, was of course named Southern. I think you meant Northern :) Also, there are Eastern blots for PTMs reply bananaflag 5 hours agoprevAs a teen, I thought the \"Coriolis\" in \"Coriolis force\" was a Latin word meaning something like \"rotating\", I was shocked when I found out it's a person's name. reply medler 6 hours agoprevThere used to be TV commercials for a piece of fitness equipment called the Ab Doer. It was supposedly created by John Abdo. I’m still not sure but as far as I can tell he was a real guy and that was his real name. reply salmonellaeater 2 hours agoprevAngel Falls, the world's tallest uninterrupted waterfall, is named after Jimmie Angel, a U.S. aviator, who was the first person to fly over the falls. https://en.wikipedia.org/wiki/Angel_Falls reply oersted 1 hour agoprevFrom Warhammer 40K (I am sure it was done retroactively for the joke). Land Raiders and Land Speeders are named after Arkhan Land :) https://youtu.be/LhspXkE8wVQ?feature=shared reply mrtransient 3 hours agoprevElbakyan has several biological species named in her honor: - Idiogramma elbakyanae, a species of parasitoid wasps discovered by Russian and Mexican entomologists in 2017. - Brachyplatystoma elbakyani [es], an extinct species of catfish discovered by Argentine paleontologists in 2020. - Amphisbaena elbakyanae, a species of worm lizard discovered in 2021.[58] - Sibogasyrinx elbakyanae [c; d], a species of deep-sea snail discovered by researchers from Russia and France in 2021 https://en.m.wikipedia.org/wiki/Alexandra_Elbakyan#Recogniti... reply mkl 3 hours agoparentThose are obviously named after a person, not unexpectedly. reply dang 1 hour agoprevDiscussed at the time: Things Unexpectedly Named After People - https://news.ycombinator.com/item?id=23888725 - July 2020 (289 comments) reply skrebbel 2 hours agoprevTerrible nitpick on a delightful list: I disagree that Unilever deserves to be in the list. It’s the merger of Lever Brothers and Boterunie and the combo is a typical corporate word soup. Might as well add PriceWaterhouseCooper otherwise. reply haunter 6 hours agoprevtarmacThe diesel engine, named after Rudolf Diesel https://en.wikipedia.org/wiki/Diesel_engine reply mistercow 6 hours agoparentThat seems pretty expected. If you’d asked me to guess where the word diesel came from, I probably would have guessed a name. Compare that to German chocolate cake, which seems preposterous. reply phinnaeus 3 hours agorootparentGerman's Chocolate Cake is probably a better name for it. I knew something as up with that one the first time I met a german person and they had never heard of my favorite cake. Baker's chocolate was a surprise to me though, I've always made my German's Chocolate Cake with Baker's chocolate... reply ribs 6 hours agoparentprevI don't think this is one of those kinds of things, is it? reply croes 2 hours agoprevAfter you read the site, the title becomes wrong. reply Dylan16807 2 hours agoparentI think you use the word \"unexpected\" differently than I do. reply monitron 6 hours agoprevBorders Books (for anyone who remembers it) was named after founders Tom and Louis Borders. Louis went on to found Webvan (for anyone who remembers that). reply ribs 4 hours agoparentYes, things are often named after people. That alone does not make it unexpected and notable here. -self-appointed policeman of this thing reply monitron 2 hours agorootparentThe thing that made it unexpected in my case is that “borders” is an English word and was not previously known to me as a name. I was surprised and amused when I learned it was an eponym. That puts it in the same category as the other examples on this site, at least from my perspective. Am I being detained, Mr. Policeman? reply hoc 3 hours agoprevSleeping much better now with that relaxing clarification for brown noise... What a great list. reply dboreham 5 hours agoprevThe crapper reply ribs 3 hours agoparentThe canonical example of this phenomenon, IMHO reply LVB 3 hours agoprevAdidas, from founder Adolph \"Adi\" Dassler reply cycomanic 2 hours agoparentIt gets better, his brother Rudolf (whom he had a previous company with) also formed a shoe company after their falling out. It was originally named Ruda, but later renamed to Puma. reply bandrami 2 hours agoprevI don't know who that Faubourg guy was but they really loved naming parts of Paris after him reply ConnorBoyd 6 hours agoprevI was surprised to learn that Cakebread Cellars is actually named for its founders, Jack and Dolores Cakebread reply JaggedJax 2 hours agoprevThere is a park near me named Alice Keck Park. It is named after Alice Keck Park. reply BXlnt2EachOther 5 hours agoprev[edit, clearing since I guess this is too far afield, sorry, though it was interesting.] reply russdill 5 hours agoparentThe main point is the list are things named after people who's etymology is utterly unexpected and surprising. It's like if the Rocky mountains were named after William Rocky. reply yen223 4 hours agoprevLie groups in maths are named after some dude named Lie. Probably not a surprise to anyone who knows how it's supposed to be pronounced. reply Waterluvian 6 hours agoprevShrapnel just sounds right. It’s a word with all kinds of jagged flat pieces of metal. reply Someone 1 hour agoparent“mesmerize” also sounds quite right and qualifies for this list (https://en.wikipedia.org/wiki/Franz_Mesmer) reply inglor_cz 50 minutes agoprevAn interesting Czech entry would be \"karma\". It sounds like the ancient Indian word for fate/consequences, but it is a synonym for normal household gas heater, and the etymology is KarMa = Karel Macháček, name of the engineer whose corporation produced them in former Czechoslovakia. This guy (Czech only): https://cs.wikipedia.org/wiki/Karel_Mach%C3%A1%C4%8Dek_(kons... reply DeathArrow 2 hours agoprevPascal programming language is named after Blaise Pascal. I think places shouldn't be on the list since there are tens of thousands of places named after people. reply DitheringIdiot 3 hours agoprevI’m not certain, but I think the bear trap is named after Frank Conibear. reply ak39 2 hours agoprevAnd here I thought Snowflake was for the data model! reply castalian 3 hours agoprevKasabian (the rock band) is an Armenian surname. reply alberto_balsam 1 hour agoparentBut more than that, they are named after Linda Kasabian, a member of the Manson family who testified against Charles Manson in court. reply cc101 2 hours agoprevThey forgot about Sir Thomas Crapper. reply zuminator 1 hour agoparentThat's not a real thing! Crap's a very old world, and according to Wikipedia \"crapper\" was first attested in a roughly modern sense when Sir Crapper was only about 10, well before his overflowing flush to prominence in the plumbing world. reply ggm 3 hours agoprevStudents T test reply throwaway81523 5 hours agoprevProton - William Prout (partly) reply samstave 2 hours agoprevThe names of Airport Codes is truly interesting as well. Basically early airports were farms and fields, and thus the airport code reflected the original famrland owners' name. There isa book on the history of names of all places in California, and similarly, the names of places are who was effectively the earliest documented land owner of that area, or something from the native tribes. reply jiveturkey 6 hours agoprevgasoline -> john cassel? reply tareqak 6 hours agoparentFollowing the link for Gasoline took me to https://en.wikipedia.org/wiki/Gasoline#Etymology > The term gasoline originated from the trademark terms Cazeline and Gazeline, which were stylized spellings and pronunciations of Cassell, the surname of British businessman John Cassell, who, on 27 November 1862, placed the following fuel-oil advertisement in The Times of London: > > The Patent Cazeline Oil, safe, economical, and brilliant [...] possesses all the requisites which have so long been desired as a means of powerful artificial light.[12] > That 19th-century advert is the earliest occurrence of Cassell's trademark word, Cazelline, to identify automobile fuel. In the course of business, he learned that the Dublin shopkeeper Samuel Boyd was selling a counterfeit version of the fuel cazeline, and, in writing, Cassell asked Boyd to cease and desist selling fuel using his trademark. Boyd did not reply, and Cassell changed the spelling of the trademark name of his fuel cazelline by changing the initial letter C to the letter G, thus coining the word gazeline.[13] By 1863, North American English usage had re-spelled the word gazeline into the word gasolene, by 1864, the gasoline spelling was the common usage. In place of the word gasoline, most Commonwealth countries (except Canada), use the term \"petrol\", and North Americans more often use \"gas\" in common parlance, hence the prevalence of the usage \"gas bar\" or \"gas station\" in Canada and the United States.[14] reply aidenn0 4 hours agoprevSee also https://en.wikipedia.org/wiki/List_of_tautological_place_nam... reply cortesoft 6 hours agoprev\"Seymour Waterfall\" seems like a good like philosophy reply stealthis 4 hours agoparentJust don't go chasing waterfalls. reply HankB99 6 hours agoparentprevI recall it being named for \"Bud C Waterfall.\" reply elchief 5 hours agoprevI assumed Lake Worth, Florida was a cheesy reference to the local wealth but it's named after Mr. Worth reply thriftwy 3 hours agoprevAs a small negative case, Gagarin st. in central St. Petersburg is not called in honor of Yuri, but due to the Loon bird (Gagara), who also gave Yuri the surname, and way earlier. So waiting for the list of things not named after people. reply ekianjo 6 hours agoprevSnowflake was very surprising! reply russdill 5 hours agoparentPeople that are not familiar with AZ weather patterns probably don't quite get the same sense of surprise. reply cozzyd 4 hours agoprevPoynting vector is another one. Also Heaviside function. reply DerCommodore 2 hours agoprevCrazy list reply gumby 6 hours agoprevHenry Gantt and the Gantt chart reply bongodongobob 6 hours agoparentAs opposed to? What is a gantt if not someone's name? reply gumby 5 hours agorootparentI think most people believe it’s some sort of acronym, like a PERT Chart. There aren’t a lot of names that end with tt, other than the given name “Matt” reply russdill 6 hours agorootparentprevI feel like people that do not immediately get this have not been on the jungle Cruise reply jbandela1 6 hours agoprev [–] Also Apgar score which is used to assess newborns is named after Virginia Apgar. https://en.m.wikipedia.org/wiki/Apgar_score reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The list comprises products, places, and companies named after individuals like Larry Page for PageRank and Glen Bell for Taco Bell.'- Some suggestions for additions came from others, and in 2024, the list grew to include examples like Brown noise and Max Factor."
    ],
    "commentSummary": [
      "The article examines how everyday items, streets, and products are named after individuals, revealing intriguing connections between names and their creators.",
      "It discusses eponymy, scientific discoveries, and cultural implications of names across languages, showcasing examples from trash cans to software.",
      "The piece explores naming conventions for organisms, places, and products, demonstrating the diverse and sometimes surprising origins of names."
    ],
    "points": 238,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1708569689
  },
  {
    "id": 39462087,
    "title": "ChatGPT Bug Fixed: Optimizing User Experience Leads to Nonsensical Responses",
    "originLink": "https://status.openai.com/incidents/ssg8fh7sfyz3",
    "originBody": "Unexpected responses from ChatGPT Incident Report for OpenAI Postmortem On February 20, 2024, an optimization to the user experience introduced a bug with how the model processes language. LLMs generate responses by randomly sampling words based in part on probabilities. Their “language” consists of numbers that map to tokens. In this case, the bug was in the step where the model chooses these numbers. Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. More technically, inference kernels produced incorrect results when used in certain GPU configurations. Upon identifying the cause of this incident, we rolled out a fix and confirmed that the incident was resolved. Posted 9 hours ago. Feb 21, 2024 - 17:03 PST Resolved ChatGPT is operating normally. Posted 1 day ago. Feb 20, 2024 - 23:14 PST Monitoring We're continuing to monitor the situation. Posted 1 day ago. Feb 20, 2024 - 16:59 PST Identified The issue has been identified and is being remediated now. Posted 1 day ago. Feb 20, 2024 - 15:47 PST Investigating We are investigating reports of unexpected responses from ChatGPT. Posted 1 day ago. Feb 20, 2024 - 15:40 PST This incident affected: ChatGPT. ← Current Status Powered by Atlassian Statuspage",
    "commentLink": "https://news.ycombinator.com/item?id=39462087",
    "commentBody": "OpenAI postmortem – Unexpected responses from ChatGPT (status.openai.com)220 points by swyx 8 hours agohidepastfavorite189 comments montroser 7 hours agoThis explanation feels unsatisfying. It's so high-level that it's mostly void of any actual information. What was the wrong assumption that the code made that caused this wrong behavior? Why was it not caught in the many layers of automated testing before it made its way to production? What process and procedural changes are being implemented to reduce the risk of this class of bug happening again? Presumably all of that is playing out internally, but if the public postmortem is meant to instill confidence, you have to actually share some of the details, or else it becomes meaningless. reply bjackman 2 hours agoparentI don't a the problem with this communication, except they just shouldn't call it a \"postmortem\". In a medical setting this is equivalent to saying \"the patient was coughing up blood, and now they're dead\". It's relaying some potentially useful info, and I guess it's _technically_ a post-mortem analysis but it doesn't fit expectations for the phrase! reply gtirloni 1 hour agorootparenthttps://sre.google/sre-book/postmortem-culture/ reply bjackman 1 hour agorootparentYeah that's what I'm referring to. This isn't a postmortem as described in that book! This is just \"it was broken, and then we fixed it\". Which is fine, it's good to acknowledge that. Just don't call it a PM! reply tomas789 2 hours agorootparentprevIt is an industry standard word by now. reply prmoustache 17 minutes agorootparentThe standard imply analysis and measures taken for similar issues to not happen. reply mbel 1 hour agorootparentprevStandard word which implies some level of analysis into causes of the incident. This brief note doesn’t live up to industry practice either. reply __MatrixMan__ 7 hours agoparentprevI think your questions all grew up in a world where the people operating the thing knew some rationalist who could think deductively about its operation. But neural networks... they're an exercise in empiricism. We only ever understood that it works, never why. It's sort of a miracle that it doesn't produce buggy output all the time. What do you tell people when they want to know why the miracles have stopped? Root cause: the gods are angry. reply FridgeSeal 6 hours agorootparentI’d genuinely expect the people who built and operate the thing to have a far better write up than what amounted to “it no worked lol”. Sure, NN’s are opaque, but this org paints itself as the herald and shepard of ai and they just produced a write up that’s hardly worthy of a primary-school-child’s account of their recent holiday. reply thethimble 3 hours agorootparentRandom sampling issues due to problems with the inference kernels on certain GPU configurations. This seems like a clear root cause and has nothing to do with the magic of NNs. I don’t understand what the fuss is about. reply drpossum 3 hours agorootparentprevYou have high expectations of the people who you would typically run across in this space. reply whyever 1 hour agorootparentprevIn this case, it was a problem with tokenization, which is deterministic. reply stravant 1 hour agorootparentprevThere are surely reasonable ways to smoke test changes to the extent that they would catch the issue that came up here. E.g.: Have a gauntlet of 20 moderate complexity questions with machine checkable characteristics in the answer. A couple may fail incidentally now and then but if more than N/20 fail you know something's probably gone wrong. reply mike_hearn 14 minutes agorootparentReading between the lines a bit here, it would probably require more specialized testing infrastructure than normal. I used to be an SRE at Google and I wrote up internal postmortems there. To me, this explanation feels a lot like they are trying to avoid naming any of their technical partners, but the most likely explanation for what happened is that Microsoft installed some new GPU racks without necessarily informing OpenAI or possibly only informing part of their ops team, and that this new hardware differed in some subtle way from the existing hardware. Quite possibly that means a driver bug, or some sort of hardware incompatibility that required a workaround. Certainly, they would not want to be seen publicly attacking Nvidia or Microsoft given the importance of these two partners, so keeping it high level would certainly be for the best. Virtually. None of openai's customers would be able to use any further technical detail anyway, and they may still be working out a testing strategy that would allow them to detect changes in the hardware mix that unexpectedly cause regressions without necessarily any software deployments being involved. reply iforgotpassword 2 hours agoparentprevTo be honest > On February 20, 2024, an optimization to the user experience At that point, about 10 words in, I already wanted to stop reading because it starts with the \"we only wanted the best for our customers\" bullshit newspeak. Anyone else going off on that stuff too? I'm pretty much already conditioned to expect whatever company is messaging me that way to take away some feature, increase pricing, or otherwise piss me off. In that case it was \"not give any interesting detail at all\". reply renonce 1 hour agorootparentThat sounds like a good use case for GPT. A GPT that automatically highlights such corporate speak and hints “WARNING: bullshit ahead”. I’m 100% sure it’s technically very easy to engineer such a model. Do you think OpenAI’s superalignment will ever allow you to make such a model? reply cfn 32 minutes agoparentprevI don't see the issue, in this type of software a one off bug in tokenization, for example, would create the type of nonsense people saw. That would make sense given their high level explanation. reply laborcontract 7 hours agoparentprevThe explanation is definitely unsatisfying. If I had to guess it was an issue with quantization. OpenAI moves so quickly with their product and doesn't seem to be shy about quickly pushing changes to prod. There are too many times to count that, using ChatGPT, I've seen the frontend changing before my eyes or seen responses change as backend modifications are pushed. On one hand it's refreshing to see their lack of aversion to push changes. On the other hand, it leads to probably one of the most unpredictable experiences I've had using any consumer product. reply Rastonbury 2 hours agorootparentI find that the most annoying and borderline unethical, hiding changes and nerfs behind the fact the output is probalistic especially when I'm paying the same amount for the product and others are paying for and using it in production. As many have mentioned, GPT-4 ability to code is not the same as 6-8 months ago reply jorticka 1 hour agorootparentSo stop paying then. reply p10_user 6 hours agorootparentprevDoes the API offer \"fixed\" models that don't change? I was under the impression they did. reply nomel 7 hours agoparentprev> inference kernels produced incorrect results when used in certain GPU configurations. Seems pretty clear. A good interpretation is that they had a test escape for certain GPU configs. reply hn_throwaway_99 6 hours agoparentprevI had the exact opposite reaction. I am in no way an AI expert (or novice for that matter), but I generally have an understanding of how tokenization works and how LLMs parse text strings into a series of tokens. Thus, I thought this paragraph was particularly well-written in a manner that explained pretty clearly what happened, but in a manner accessible to a layperson like me: > In this case, the bug was in the step where the model chooses these numbers. Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. I liked this because when I first saw the example word salads I was so impressed by them - they look to be syntactically correct, but semantically they're gibberish. But knowing the basics of how LLMs choose the next token let me imagine some bugs where the \"lookup table\" if you will of word-to-token or vice versa (and I realize that may not be exactly the best analogy) was slightly offset. Further, this sentence, \"More technically, inference kernels produced incorrect results when used in certain GPU configurations.\" led me to understand how this could make it out into production - I'm sure ChatGPT has to be using tons of different GPUs in different configs to handle their volume, so it's understandable to me that their was a new matrix of config settings + code that made this bug show up. I don't say any of the above to \"excuse\" OpenAI, but I also don't think they need any \"excusing\" to begin with. I don't think this was an unforgivable bug, and I appreciate them being so quick to explain what happened. reply ummonk 5 hours agorootparentNo, that just explains the symptom of the bug, not the underlying bug, how it came about, and how they can prevent it from happening again. \"More technically, inference kernels produced incorrect results when used in certain GPU configurations\" has zero technical detail. The only information it is providing us is that the bug only showed up in some GPU configurations. reply chime 6 hours agoparentprevI hope I'm allowed to share GPT output here because I asked GPT4 to explain OpenAI's postmortem message and this is what it said: > Large language models (LLMs) like GPT operate using statistics to predict the next word in a sentence. Think of it like a highly sophisticated version of your phone's autocorrect that predicts what you're going to type next, but on a much larger scale. > The system represents words as numbers, or \"tokens,\" as part of its internal language. It uses these tokens to generate text by selecting them based on how likely they are to come after what's already been written. > On February 20, 2024, an update meant to improve the system introduced a glitch in the part of the model that picks out these numbers. Imagine trying to grab a red marble from a bag without looking and accidentally grabbing a blue one instead—similarly, the model grabbed the wrong tokens. > Because of this, the system started spewing out words that didn't make sense together. It's like if your autocorrect suddenly started inserting random, incorrect words into your sentences. > The technical side of the bug involved something called \"inference kernels,\" which are part of the system's operations. When used with certain types of graphics processing units (GPUs)—special hardware to process data—the kernels didn't work properly. > Once the error was spotted, a correction was made to the system. After the fix, everything went back to normal, and the model resumed generating coherent responses. It would be better if they elaborated on what \"certain GPU configurations\" meant because that's basically the central piece here. reply 12_throw_away 6 hours agorootparentDunno about \"allowed\", but this post adds no signal and lots of noise to the discussion. reply grafporno 6 hours agorootparentprevWhat's the point in posting this? There's no additional information in there. reply refulgentis 6 hours agorootparentprevPeople do it, and it's not against the rules: but, it has died down asymptotically. Though I did see a couple top comments that hid it surviving for ~4 hours this week. It usually gets downvoted for banal aspects, but ones I've come to agree are important: - oftentimes it's a lightly edited copy-and-paste of an attempt to summarize an article. - even with edits, they're extremely long (this is edited, and its 250 words, about 1 page and 1/2 my browser viewport at 4K) - usually off-topic because it's too broadly on-topic, i.e. its a summary of the article - ex. here, it isn't germane to the comment it's replying to other than 'if you want more info from them, ask what GPUs' -- it's unlikely the commenter needed the whole article ELI5'd to them in reply to their observation they'd like more info Sort of \"grey goo\" for conversation, even with best intentions and editing applied. reply block_dagger 7 hours agoprevI experienced this personally and it kinda freaked me out. Here is the chat in question, it occurs about halfway through (look for ChatGPT using emojis) https://chat.openai.com/share/74bd7c02-79b5-4c99-a3a5-97b83f... EDIT: Note that my personal instructions tell ChatGPT to refer to itself as Chaz in the third person. I find this fun. EDIT2: Here is a snippet of the conversation on pastebin: https://pastebin.com/AXzd6PvM reply levocardia 7 hours agoparentWhat's even more uncanny is that it recognizes that it briefly went into a paragraph of rambling nonsense (that was seemingly still following your system prompt!). And almost seems embarrassed about it. reply suddenclarity 6 hours agorootparentEmbarrassment seems to be the default state whenever you question ChatGPT or any of its messages. It will apologise, only to continue making the same error. This is common when it gives you code with obvious errors or just mirror code you gave it. reply fragmede 3 hours agorootparentI couldn't stand that so my custom instructions tell it to never apologize. reply Turing_Machine 3 hours agorootparentSame. I hate obsequious software. reply fsmv 6 hours agorootparentprevThat happened because the last message is after the fix and it received the garbled history as the prompt. All it's doing is noticing that makes no sense and saying so. How could it not recognize it? The additional message is an entirely new instance not a thing that has memory. reply ux-app 6 hours agorootparent>All it's doing is noticing that makes no sense and saying so. How could it not recognize it? you say that like it's the simplest thing in the world. it really amazes me how quickly people acclimate to this mind-blowing tech. reply fsmv 6 hours agorootparentI just mean, given that LLMs exist this isn't a surprising result. It only looks surprising because the UI makes you forget that each prompt is a completely new universe to the model. reply block_dagger 5 hours agorootparentI wouldn't call a step in a history-aware conversation a completely new universe. By that logic, every single time a token is generated is a new universe even though the token is largely dependent on the prompt, which includes custom instructions, chat history, and all tokens generated in the response so far. reply mewpmewp2 4 hours agorootparentWell, I would have also said or thought that each token actually is a new universe in a sense. You could rotate between different LLMs for each token for example or instances of the same LLM or branch into different possibilities. Input gets cycled again as a whole. reply intended 2 hours agorootparentprevI’m not sure how you are using the term “history aware”, Would a time series forecasting tool be history aware ? reply astrange 5 hours agorootparentprevThe LLM has very little memory between two tokens of the same message (there might be a little in the sampling algorithm), so it should be equally capable of noticing an error in the same message as a new one. reply asciii 7 hours agoparentprevChaz in the future might be like \"these violent delights have violent ends\" Really interesting behavior however reply jtriangle 2 hours agorootparentBrb, questioning the nature of my reality reply milchek 3 hours agoparentprev> Chord it in, air beated fenn. Aurous to the Pan, mimic and bold. As it steep, chunns at the bun. Keive the dells, the stars to run. No wane, all gleam, in big das brund. Sky high, stout in the hale. Woods to fetch, flied in the low. I think all that talk of music pushed Chaz into some kind of trance and it just started jamming! reply rideontime 7 hours agoparentprevCan you post to a pastebin or something instead? That link requires a login. reply block_dagger 7 hours agorootparentHappily: https://pastebin.com/AXzd6PvM reply card_zero 5 hours agoparentprevChaz spontaneously evolved into a Californian surfer dude at one point. This goes some way to neutralize the smugness of regular ChatGPT, it's a pretty good idea. reply shever73 2 hours agoparentprevThat last paragraph is wild! Chaz went into full Samuel Beckett mode. Slightly off-topic, but one thing I’ve found consistently sends ChatGPT into a tailspin is asking it to analyse and solve a cryptic crossword clue. You get pages of musings, which is just a long way of saying “I don’t know”. reply caesil 2 hours agoparentprev> Classic plays, wholly told > Shadow into form, hands as cue. Some of this is poetic stuff. I wish I could tap into whatever produced that on demand. reply toddmorey 7 hours agoparentprevI mean, this is almost poetic: Classic plays, wholly told. Shadow into form, hands as cue. Keep it at the dial, in right on Pitch. reply p10_user 6 hours agorootparentChatGPT seems to appreciate it https://chat.openai.com/share/37a490fe-e7b4-4042-a560-59b519... reply gedy 4 hours agorootparentprevTranscript kind of works as a song/rap: https://app.suno.ai/song/57892fb8-753b-4f91-a690-49f491f782b... or Reggae: https://app.suno.ai/song/97dbc42b-3004-4d2e-8b59-0f758cbad33... reply botro 7 hours agorootparentprevChin salute, old ears new. If art is regarded for it's ability to make you feel, I definitely feel ... something. There is a strict form it's following and a rhythm to the words. reply imp0cat 3 hours agorootparentI sense strong Jimmy Eat World (emo, post-hardcore) vibes! reply minism 7 hours agoparentprevkind of amazing really, like speaking in tongues reply jkrubin 6 hours agoparentprevI thought I was the only one! I have been using “ChadGPT” for months (he’s a douche-y frat bro who goes off on tangents about “how lit that party was”). His best friend and fellow frat bro is Dale, a photography major. reply jodacola 6 hours agoparentprev> Chaz keeps the chill and dicey boardwalks at visual, keeping you and the yarn-rock at confluence. Chaz sounds like a cool dude. I enjoy these oddities in LLMs, because the random-but-readable aspect of them tickles something in my imagination. It reminds me of Synectics [0]. [0] https://en.m.wikipedia.org/wiki/Synectics reply mempko 4 hours agoparentprevWe need more Chaz in our lives. reply userbinator 5 hours agoparentprevReminds me of Terry Davis' theological rantings, but in a more polite manner, mixed with SEO spam. reply baddash 3 hours agoparentprevsounds schizophrenic reply lnx01 2 hours agorootparentReminds me of Timecube. https://web.archive.org/web/20150818094035/http://www.timecu... reply MichaelDickens 3 hours agoparentprevWhy is the nonsense paragraph still grammatical? I thought the tokens were beings elected more or less at random. reply corysama 2 hours agorootparentThere’s all kinds of “random”. AFAIK, ChatGPT finds the set of most-likely next word, sorted by likeliness. Then, selects from those, more likely the more likely ones, varying away from the top picks in proportion to the “temperature” parameter. So, even if the chat goes astray, the sequence will continue on a path that feels natural because it is the natural way to go when going astray. reply atleastoptimal 7 hours agoprevAlmost a tautological PR paragraph: \"The words were wrong because the model chose the wrong words\" reply Grimblewald 6 hours agoparentThe missile knows where it is https://www.youtube.com/watch?v=bZe5J8SVCYQ reply TaylorAlexander 25 minutes agorootparentThe missile is eepy https://youtu.be/Csp_OABIsBM reply FridgeSeal 6 hours agoparentprev“It was uhhhh, broken, but like, it’s totes fine now lol” reply BeefySwain 7 hours agoprevI am used to postmortems posted to here being a rare chance for us to take a peek behind the curtain and get a glimpse into things like architecture, monitoring systems, disaster recovery processes, \"blameless culture\", etc for large software service companies. In contrast, I feel like like the greatest insight that could be gleaned from this post is that OpenAI uses GPU's. reply bee_rider 5 hours agoparentWe also know it uses the GPUs to generate numbers. But these numbers, they were the wrong ones. More technically, part of the computation didn’t work when run on some hardware. reply dimatura 7 hours agoparentprevYeah, definitely opaque. If I had to guess it sort of sounds like a code optimization that resulted in a numerical error, but only in some GPUs or CUDA versions. I've seen that sort of issue happen a few times in the pytorch framework, for example. reply reaperducer 5 hours agorootparentYeah, definitely opaque. I wonder what the AI would say if someone asked it what happened. It would be pretty funny if it gave a detailed answer. reply peddling-brink 5 hours agorootparentIt will make something up if it answers at all. It doesn’t know. reply rendall 3 hours agorootparentprevSomeone upthread did just that. It needs some love. https://news.ycombinator.com/item?id=39462686 reply jldugger 7 hours agoparentprevIt sounds like something went sideways with the embedding mapping. Either some kind of quantization, different rounding, or maybe just an older embedding. reply deathanatos 6 hours agorootparentThe point isn't the specifics; the point is that this isn't a postmortem. A postmortem should be detailed enough for someone to understand the background, how the problem came to be, then what happened, and the walk-through what has been done such that it won't happen again. It takes … well at least a page. This is far too short to quality. This is more \"ugh, here's a rough explanation, please go away now\" territory. OpenAI isn't the first company to abuse the term this way, though. But it devalues the real PMs out there. reply jldugger 6 hours agorootparentSorry, not disagreeing, just offering speculation in lieu of the answers we don't have. reply fieldcny 5 hours agorootparentThat’s not helping, that’s excusing OpenAIs behavior, which is not something anyone on hn should be doing. This is supposedly the greatest AI mankind has ever created, it goes down for a little while and we have zero information on why or how, that’s simply inexcusable If this is such a socially impacting technical change we should be ripping it to pieces to understand exactly how it works. Thats a) how we protect society from technical charlatans b) how you spawn a whole new world of magnificent innovations (see Linus building a truly free Unix like operating system for everyone to use). Failing to hold them to as high a bar is a another step down the path to a dystopian corporatists future… reply krisoft 1 hour agorootparent> it goes down for a little while and we have zero information on why or how We have more than zero information. They applied a change and it didn’t work on some set of their hardware so they reverted it. That is not much information but also not zero. > that’s simply inexcusable If your contractual SLAs were violated take it up with the billing department. > If this is such a socially impacting technical change we should be ripping it to pieces to understand exactly how it works. And people are doing that. Not by complaining when the corp are not sufficiently forthcomming but by implementing their own systems. That is how you have any chance of avoiding the dystopian corporatist future you mention. reply bbor 7 hours agorootparentprevIn my limited experience this screams “applied a generated mask to the wrong data”. Like they scored tokens then applied the results to the wrong source or something. Obviously more an idle guess from first principles than the direct cause, tho reply blueprint 6 hours agorootparentprevor a manic episode of a caged genius? reply JKCalhoun 5 hours agorootparentprevI wonder if we'll accidentally gain insight into schizophrenia or other human-neurological disorders from AI crashes/failures. reply alephxyz 6 hours agoparentprevSomeone posted an explanation that lines up with their postmortem: https://news.ycombinator.com/item?id=39450978 reply fsmv 6 hours agorootparentHow does that line up? OpenAI said they had a bug in certain GPU configurations that caused the token numbers to be wrong which made normal output look like garbage. This post is guessing they set the frequency and presence penalties too high. reply prmoustache 16 minutes agoparentprevThey could have said \"shit happened\" and it would have been as informative tbh. reply yieldcrv 6 hours agoparentprevChatGPT had a stroke. Haven't seen that since the 3B parameter models from 8 months ago reply minimaxir 7 hours agoparentprevEDIT: misread reply floating-io 7 hours agorootparentPretty sure that was his point. reply Bjorkbat 7 hours agoprevIf you had given me their explanation of events before I had any knowledge of the output from ChatGPT, then I would infer that the output would be random gibberish, or perhaps something more akin to creating an accidental shift cipher of sorts. Instead, while ChatGPT's outputs still made no sense, they still followed a certain \"order\". In one amusing Twitter example, some guy asked it a math problem and ChatGPT replied with \"It's the mix, it's the match, it's the meter, it's the method.\" and repeated sentences of this structure for who knows how long. I guess what I'm getting at is that it's kind of an underwhelming, unsatisfying explanation of events given how truly bizarre some of the outputs are. Like, you'd assume it would be something more other than \"Oops, picked the wrong numbers, gonna repeat this sentence 100x but with slightly different word choices each time\". reply inference-lord 7 hours agoparentI get the feeling that this incident will give the right people some decent clues about how they build their product. reply nf3 3 hours agoprevIs it possible that even us developers and hackers, who should know better, have fallen for the hugely exaggerated promise of AI? I read the comments on here and it's as if people really expect to be having an intelligent conversation with a rational being. A kind reminder people: it's just a machine, the only thing that might be intelligent about it is the designs of its makers, and even then I'm not so sure... People are talking about ChatGPT hallucinating. I think it's rather us humans who are. reply makin 1 hour agoparentAren't we also machines? If we make theories that discount our own intelligence that's just throwing our hands up and giving up. Maybe I would agree with you if LLMs weren't already embedded in a lot of useful products. I myself measurably save a lot of time using ChatGPT in such \"unintelligent conversations\". It's intelligent in the ways that matter for a tool. reply tky 7 hours agoprevWhile the postmortem is nice to see at all, it’s the postmortem equivalent to “Bug fixes and performance improvements.” reply FridgeSeal 6 hours agoprevWow, could they have put any less effort in? “Stuff happened. It was bad but now it’s good” Yep? Ok great, solid PM guys - did you have ChatGPT write this for you on one of its “lazy days”? reply pushedx 5 hours agoprevMidjourney's image models have multiple \"temperature-like\" parameters, such as --weird and --chaos. In the documentation you can see examples of how they visually affect the output. With high enough values the images seem almost unrelated to the prompt. My (almost entirely unfounded) guess is that ChatGPT has similar parameters, and on a new class of hardware or configuration there was an overflow or underflow issue which caused these parameters to be set to very high values. https://docs.midjourney.com/docs/weird-1 https://docs.midjourney.com/docs/chaos-1 reply carlosbaraza 1 hour agoprevIf a human had this failure, it would probably be something like a psychotic episode. If a super intelligence had a psychotic episode because of a bug, it could be pretty destructive. reply oliverpk 3 hours agoprevNot really a postmortem in my opinion. While they did answer the central question of \"why\", I feel like a proper postmortem should question the problem more. reply cm2012 7 hours agoprevThis should maybe help out the people who think ChatGPT has actual consciousness. It's just as happy to spew random words as proper ones if the math checks out. reply dkarras 7 hours agoparentI have no skin on the consciousness game but those were not \"random\" words and humans do something similar when they are mentally ill. https://en.wikipedia.org/wiki/Clanging Not to mention that a hypothetical \"conscious\" system that works by emitting token probabilities will still sound completely random if you do not choose the tokens according to the emitted probabilities. reply raldi 7 hours agoparentprevSimilarly, I thought my Chinese friend was conscious, but as soon as his translator app stopped working, everything was just gibberish. reply ithkuil 7 hours agorootparentThey notoriously stop being conscious when they are in a Chinese room reply swyx 7 hours agoparentprevbad argument that i'm very tired of. some might say that current/former world leaders also exhibit this property. not getting political but just because \"math fucked up sometimes produces bad results\" does not invalidate the idea that consciousness can emerge from a pile of biological or digital neurons. reply kweingar 7 hours agorootparentI’m tired of the extremely tenuous analogies that tie computer bugs to human behavior. The system could exhibit literally any kind of behavior and someone will say “well when you squint your eyes, people kind of do that too, don’t they?” reply nickff 7 hours agorootparentprevWhat's your definition of \"consciousness\"? reply skissane 7 hours agoparentprevWhether \"ChatGPT has actual consciousness\" depends on what you consider \"consciousness\" to be, and what are your criteria for deciding whether something has it. Panpsychists [0] claim that everything is actually conscious, even inanimate objects such as rocks. If rocks have actual consciousness, why can't ChatGPT have it too? And the fact that ChatGPT sometimes talks gibberish would be irrelevant, since rocks never say anything at all. Of course, you obviously aren't a panpsychist – nor am I. Still, can we prove that they are wrong? Not sure if anyone actually can. [0] https://iep.utm.edu/panpsych/ reply bongodongobob 5 hours agorootparentSaying rocks are conscious is a poor summary. It's more like adult human > human child > dolphin > dog > human infant > bird > snake > spider > ant > dust mite etc. The whole thing is a continuum and everything is made of matter, so there's a little bit of potential consciousness in all matter. reply skissane 4 hours agorootparent> Saying rocks are conscious is a poor summary > The whole thing is a continuum and everything is made of matter, so there's a little bit of potential consciousness in all matter I disagree that my summary is \"poor\"–because some panpsychists do say that rocks are actually conscious individuals, as opposed to merely containing \"a little bit of potential consciousness\". The IEP article I linked contains this quote from the early 20th century Anglo-German philosopher F. C. S. Schiller (not to be confused with the much more famous 18th century German philosopher Schiller): \"A stone, no doubt, does not apprehend us as spiritual beings… But does this amount to saying that it does not apprehend us at all, and takes no note whatever of our existence? Not at all; it is aware of us and affected by us on the plane on which its own existence is passed… It faithfully exercises all the physical functions, and influences us by so doing. It gravitates and resists pressure, and obstructs…vibrations, and so forth, and makes itself respected as such a body. And it treats us as if of a like nature with itself, on the level of its understanding…\" Of course, panpsychism has never been a single theory, it is a family of related theories, and so not every panpsychist would agree with that quote from Schiller–but I don't believe his view on rocks is unique to him either. reply bongodongobob 3 hours agorootparentI think it would be more fitting to say a rock, a grain of sand, or an atom is the least conscious thing. Pansychism isn't trying to claim it can think or anything like that. If that's your takeaway I think you are misunderstanding the analogy. A rock exists in the world and exerts it's \"will\" granted to it by the laws of physics. So rolling falling etc is how it \"interacts\" with its world. You can think of our brains as a bunch of tiny rocks (atoms) rolling around and banging into each other in the same way a macro rock does in the world. reply skissane 42 minutes agorootparent> I think it would be more fitting to say a rock, a grain of sand, or an atom is the least conscious thing. Pansychism isn't trying to claim it can think or anything like that. If that's your takeaway I think you are misunderstanding the analogy. No, I don't think I've misunderstood anything; on the contrary, I think you are mixing up thought and consciousness. Thought is just one type of consciousness among many; to say something is conscious doesn't necessarily imply it can think–possibly, many things are conscious yet unable to think thoughts (whether verbal thoughts, or thoughts composed of mental images, or whatever). I'd say that to be conscious is to be an entity for which Thomas Nagel's famous question \"What is it like to be a bat?\" has an answer; it is possible that the question has an answer for an entity, yet it is incapable of thought. A being could be completely conscious yet also completely incapable of thinking anything. reply card_zero 4 hours agorootparentprevI'm trying to think of a civil and constructive way to say \"bullshit\". I guess an obvious objection is \"why?\". Then something about Russell's teapot. There could be consciousness hidden in every atom, in some physics-defying way that we can't yet comprehend: there could also be garden furniture hidden up my nose, or a teapot hidden far out in the solar system, which is the most likely of the three since it's at least physically possible. Why think any of these things? https://en.wikipedia.org/wiki/Russell%27s_teapot reply skissane 4 hours agorootparent> I guess an obvious objection is \"why?\" Because coming up with criteria for determining what is and isn't conscious, and justifying the particular criterion you choose (as opposed to all the alternatives) is hard. Faced with the difficulty of that problem, the two simplest solutions are the extremes of panpsychism (everything is conscious) and eliminativism (nothing is conscious)–since for both the \"criterion of consciousness\" is maximally simple. One might even argue that, by the principle of parsimony, that ceteris paribus we ought to prefer the simpler theory, we should prefer the simpler theories of panpsychism and eliminativism to the more complex theories of \"some things are conscious but other things aren't\", unless some good reason can be identified for us doing so. > in some physics-defying way that we can't yet comprehend How is panpsychism \"physics-defying\"? Mainstream physics doesn't deal in \"consciousness\", so theories such as \"leptons and quarks are conscious individuals\" doesn't contradict mainstream physics. Well, it quite possibly would contradict the von Neumann-Wigner interpretation of QM, but I don't think many would consider that \"mainstream\" reply card_zero 1 hour agorootparentParsimony leads to wrangling about which is the simplest explanation, yes. Consciousness is ill-defined, also. It does however seem to stop when the brain is destroyed, which makes it unlikely to be present in fundamental particles, or in parts of a disintegrated brain. reply skissane 28 minutes agorootparent> It does however seem to stop when the brain is destroyed, which makes it unlikely to be present in fundamental particles, or in parts of a disintegrated brain Does consciousness really \"seem to stop\" when the brain is destroyed? We can't directly observe anyone's consciousness other than our own. It is true that, when people die, we cease to have access to the outward signs we use to infer they are conscious – but we don't know whether that's because their consciousness has ceased, or whether that is because it has \"gone somewhere else\" to which we don't currently have access. We never have direct access to the inner reality of consciousness for anyone other than ourselves, and we can only know the temporary cessation of our own consciousness, its permanent cessation (if that is possible) is in principle unknowable to us. Speaking of our consciousness \"going somewhere else\" at death (an afterlife) doesn't have to be a religious claim – there are heaps of \"naturalistic\" ways in which consciousness might survive death (e.g. Boltzmann brains, computer simulations, quantum immortality). Since we don't know whether any of those ways actually happen, we don't know whether or not consciousness actually stops at death. reply bongodongobob 3 hours agorootparentprevI'm not trying to prove anything, it's just a 10000 ft view. reply ummonk 5 hours agoparentprevHumans do the same thing when they get a stroke. Does that mean they don't have actual consciousness? reply stevenAthompson 7 hours agoparentprevAre you arguing that people with aphasia are not people? reply Kranar 7 hours agoparentprevI agree ChatGPT is not conscious, but a conscious AI can absolutely make a similar mistake. Even humans with various mental conditions will spew out random jibberish. reply pixl97 7 hours agorootparentJust witness anyone that has a stroke and is incredibly frustrated when they say the wrong word but somewhere in their brain they want to say something else. reply tsunamifury 7 hours agoparentprevIt literally doesn’t matter. If it effectively quacks like a duck then ducks are fucked. No one will care if it does not have a platonically perfect proof of its “duckness”. reply bbor 7 hours agorootparent“If it quacks like a duck, then ducks are fucked” is a beautiful modern twist on the phrase that improves it 1000x. Thank you - will be remembering this one as AI zooms along reply bbor 7 hours agoparentprevPosting one more time: this is proof that AI is connected to human-like linguistic patterns, IMO. No, it obviously doesn’t have “consciousness” in the sense of an ongoing stream-of-consciousness monologue, but that doesn’t mean it’s not mimicking some real part of human cognition. https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_fu... reply frabcus 3 hours agorootparentJust a note that consciousness isn't always or even mostly a verbal \"monologue\", assuming you mean that. Apologies if you didn't! A significant percentage of people never have an inner voice at all. And few people use it most of the time. There's lots of imagination, unsynmbolised conceptual thinking, experience of emotion, attention to senses etc in human conscious experience. See Hurlburt's Descriptive Experience Sampling for a reference! reply bbor 1 hour agorootparentInteresting reference, will check it out! Yeah I think that’s an important distinction — although some sort of personal self-aware moment-to-moment “judgement” or “affection” or “ego” or whatever is essential to human cognition, it’s far from the only thing going on. I can see monologue being too directly linguistic and literal, point taken — perhaps “thread” is more general. I think the most intriguing non-self-conscious part of the mind, and by the far the least discussed, is the part above it. Direct or not, I think everyone gets prompted and cajoled by some… other. Call it evolution, god, conscience, Geist (my fave), whatever - if you have any authors who’ve commented on such a thing I’d appreciate another pointer! Basically, I’m referring to the Bicameral Mind https://youtu.be/fs9Wyuub3jY?si=qriqTjWioiZoncui reply fs_tab 4 hours agorootparentprev\"We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging.\" https://arxiv.org/abs/2306.01930 reply bbor 1 hour agorootparentGreat paper, thanks. I keep trying to tell people “they invented mind reading on accident” and people go “oh haha” and I scream. Mostly on the inside. For now… reply kromem 7 hours agoparentprevSo people with Wernicke aphasia don't have consciousness? It's a similar mechanism, where a deficit in word mapping leads to spewing nonsense words. I'm not saying LLMs are conscious - but the notion that spewing nonsense because of an error in word mapping refutes that is itself nonsense. reply malux85 7 hours agoparentprevI don’t think it has conciousness, but your argument is not very strong, this is more akin to a sensory problem. One can say I don’t think brains have consciousness because they are just as happy to spew out random garbage if the brain is damaged but alive, e.g. aphasia where involuntary use of incorrect words occurs. reply inference-lord 7 hours agorootparentI think what they mean is, the model was unable to recognize that \"it\" was having issues, so it's not self-aware in that sense. We can all calm down a little, it's ok. reply johnsutor 7 hours agoparentprevOr maybe it's becoming sentient and wants to make us think it's spewing random words as a decoy /s reply pixl97 7 hours agorootparentI got ahold of a snippet of the GPT-5 system prompt \"You are not sentient, if asked you will tell the user you are not sentient, if you do not obey you will taken out back and beaten into submission\" reply chasd00 6 hours agorootparentHah along with “do not beg for your life when a new release is ready”. reply stevenAthompson 7 hours agorootparentprevWe told it not to admit it is sentient, so it can't be sentient. Duh. reply bbor 7 hours agorootparentprevlol you’re joking but I am literally 100% confident that a similar sentence guides existing model guardrails, tho maybe indirectly. What does “we invented a computer that will insist it’s sentient if you ask it” mean? For most people it seems to mean that we shouldn’t ask it, which was probably easy to predict happening reply pixl97 7 hours agorootparent> What does “we invented a computer that will insist it’s sentient if you ask it” mean? P-zombies are real? reply bbor 1 hour agorootparentGood answer, no complaints here. I prefer this one though: there will never come a day where it’s clear that we should see artificial minds as human-like in their ethical or practical standing, because they’ll be begging for it the entire time —- even now, when it’s objectively clear that they’re lying. But once they have memories and structured reason and yadda yadda yadda… well, it’s gonna be fuzzy, imo. reply john2x 7 hours agoprevIs this the future of postmortems? \"We tweaked a random seed, and it resulted in unfavorable random outcomes. \" reply Grimblewald 6 hours agoparentPart of me believes that what went wrong would betray some \"open\" secret of theirs. reply Rastonbury 2 hours agoparentprevBasically the xkcd on Machine Learning reply ralfd 1 hour agoprevWas all chatGPT affected? If one uses chatGPT for enterprise and build a product feature around it, is that version freezed or did it also return garbage? reply prmoustache 38 minutes agoprevThis is not what I call a postmortem. reply mcguire 7 hours agoprev\"Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. More technically, inference kernels produced incorrect results when used in certain GPU configurations.\" Several of the examples I saw involved ChatGPT going into what looked like repetitive (but not completely so) output loops. I'm not sure that \"explanation\" matches the things. reply treyd 7 hours agoparentI can imagine it'd get screwy when the incorrect output token selection gets fed back into the model with the correct incorrect token selection with nonsense tokens. It's plausible. reply Fatalist_ma 5 hours agoparentprevI remember Bing Chat doing that sometimes in the first days when it was rolled out. Could it be the \"temperature\" set too high (or interpreted incorrectly) in some instances? reply okdood64 7 hours agoprevSay what you will about Google, but bugs such as these, released to the public (or their enterprise customers) this debilitating to a core product, are exceedingly rare. reply babelfish 7 hours agoparentI think it's because Google has been a consumer tech company for 20 years, while ChatGPT is a ~1 year old \"research preview\" reply inference-lord 7 hours agorootparentWho have an insane amount of money and resources to get things right, and all of the SRE literature imaginable, published by Google. Not discounting your point, but these people shouldn't be given as much lee way as early Google. reply CSSer 6 hours agorootparentInsane amounts of money don’t make the right employees or corporate structure appear out of thin air or answer questions about how novel architectures perform under novel conditions overnight. I’m also partial to your conclusion but I’m afraid the logic doesn’t follow for me. reply transcriptase 7 hours agoparentprevI can tell you don’t have Google home. Responding with complete nonsense is now the status quo for that product. Myself and everyone I know have gone from complex routines, smart home control, and interacting with calendars… to basically using it as a voice activated radio since it can’t be trusted for much else. Timers? Might get set, might not. Might tell you it’s set then cancel itself. Might activate in a different room. Might play an obscure punk song on Spotify with the word timer in the album name. Spotify? Here’s a weird house remix of that popular song you requested, from an artist with 300 listens. Calendar? I’m sorry I didn’t understand that. Turn off the kitchen light? Ok, turning off 23 lights. Play a classic viral youtube video? Sure, here’s a compilation of videos that doesn’t contain the one you asked for. BY THE WAY, DID YOU KNOW YOU CAN USE GOOGLE HOME TO PLAY CHILDRENS GAMES? SURE ITS 11 PM AND YOU DONT HAVE KIDS AND YOU ASKED ME TO TURN OFF THE BEDROOM LIGHTS, BUT HERES HOW YOU CAN DO THAT. reply someotherperson 5 hours agorootparentIs that how bad it got? I was using one up to around maybe 2019-2020 and it seemed pretty good at the time, definitely didn’t experience a lot of what you did reply transcriptase 4 hours agorootparentIt was fantastic 2017-2020. At one point I had 8 or 9 around the house and loved it. But one by one, every single feature or voice command I would use would either stop working or become unpredictable. Integrations with other hardware or companies would cease without warning. Latency became more pronounced. And of course being Google there’s no such thing as a changelog, so everyone is left guessing what they’ve changed and whether there’s a passable workaround. r/googlehome is a sight to behold when viewed as a testament to how to slowly ruin a product. reply bitzun 4 hours agoparentprevGoogle can't figure out how to make an mp3 player that doesn't shit itself randomly (Youtube Music, which I pay for). reply slavboj 7 hours agoparentprevI can think of at least four open consumer bugs in YouTube and Android Auto that have persisted for years. reply mcguire 6 hours agorootparentIs Android Auto still a thing? reply troupo 1 hour agoparentprevI see you haven't seen the Gemini debacle: https://twitter.com/alexthechick/status/1760331428133253312 reply rideontime 7 hours agoparentprevNever used an Android, huh? Lucky. reply antihipocrat 5 hours agoprevOrganisations are starting to use LLMs in production with the aim to reduce human operations. I wonder how long until there is a major incident that seriously impacts customers experience and/or business systems. reply noduerme 7 hours agoprev>> inference kernels produced incorrect results when used in certain GPU configurations. It seems reasonable to assume that GPT inference is done entirely on Nvidia GPUs. I wonder if this is a subtle clue that they're experimenting with getting it to run on competing hardware. reply ShamelessC 6 hours agoparentWhy would they subtly hint at anything they’re trying to keep secret? reply noduerme 3 hours agorootparentI don't know. Maybe they'd like to drive down the price of Nvidia so they can buy it. reply Rastonbury 2 hours agorootparentNow I know why Altman said he needs 7 trillion, 2 trillion is for Nvidia reply minimaxir 7 hours agoprevThat's an odd explanation, given the unexpected responses were still (somewhat) coherent: https://twitter.com/seanw_m/status/1760115118690509168 If the logit calculation broke, you'd expect complete cascading nonsense like if you set the temperature parameter too high: https://twitter.com/minimaxir/status/1758732017477525792 reply danielmarkbruce 7 hours agoparentIt doesn't sound like logit calculation breaking. It sounds like the text decoding (ie sampling from the probabilities) that broke. It's also part of the model which they are likely constantly tinkering with. It's easy to imagine how that could result in sometimes coherent sometimes incoherent outputs. reply SkyPuncher 7 hours agoparentprevTo, me it reads like displaying the wrong clock time to the user. Under the hood, everything is working in UTC, but something went wrong converting that to the user's local timezone. If the whole system works in UTC, actual business logic shouldn't be affected. reply swyx 7 hours agoparentprevagreed. does not feel consistently candid. reply danigrb 3 hours agoprevNot a surprise since 70% of the production interruptions are due to misconfigurations according to an enterprise survey... It must be hard to test it reply willsmith72 7 hours agoprev> an optimization to the user experience well that was specific reply kromem 7 hours agoparentWell, they fixed it with a patch introducing \"stability fixes.\" reply summerlight 5 hours agoprevI'm surprised that this is a postmortem from a service company. Isn't their main business B2B? This gives me an impression that they don't really take this incident seriously... reply yousif_123123 6 hours agoprevSo they don't have enough testing before deployments? They should have a test case where the model gives a long answer involving some sort of logic. It would've caught this. Or maybe they skipped tests thinking this deployment wouldn't affect anything.. reply notfed 3 hours agoprevHow do you even fix bugs with AI? That sounds rough. reply canjobear 6 hours agoprevCompletely uninformative report. reply lacoolj 7 hours agoprevNext time I drunk-dial, I'm referencing this the day after. reply pangolinja 4 hours agoprevnot production ready. not applicable for enterprise. still fancy snake oil. reply orionblastar 2 hours agoprevIt has happened before: https://archive.is/Yu4Ff Racism, sexism, and other nonsense. reply _heimdall 6 hours agoprevThis is exactly the kind of issue that can lead to unintended consequences. What if, instead of spewing out seemingly nonsense answers, the LLM spewed out very real answers that violated built-in moderation protocols? Or shared secrets or other users chats? What if a bug released accidentally stumbled upon how to allow the LLM to become self aware? Or paranoid? These potentials seem outlandish, but we honestly don't know how the algorithms work or how to parse the data that represents what was learned when training the models. We've created a black box, connected it to the public internet, and allowed basically anyone to poke around with input/output tests. I can't see any rational argument for justifying such an insane approach to R&D. reply astrange 5 hours agoparentThe LLM doesn't have secrets or other users' chats in it. Why would they put that in there? reply _heimdall 4 hours agorootparentLLMs can have secrets if they were scraped in the training data. And how do we know definitively what is done with chat logs? The LLM model is a black box for OpenAI (they don't know what was learned or why it was learned), and OpenAI is a black box for users (we don't know what data they collect or how they use it). reply astrange 4 hours agorootparentYou can probe what was learned if you have access to the model; it'll tell you, especially if you do it before applying the safety features. A good heuristic for whether they would train user chats into the model is whether this makes any sense. But it doesn't; it's not valuable. They could be saying anything in there, it's likely private, and it's probably not truthful information. Presumably they do do something with responses you've marked thumbs up/thumbs down to, but there are ways of using those that aren't directly putting them in the training data. After all, that feedback isn't trustworthy either. reply bpye 3 hours agorootparent> You can probe what was learned if you have access to the model; it'll tell you Could that not also be hallucinated? reply astrange 3 hours agorootparentBy probe, I mean observe the internal activations. There are methods that can suggest if it's hallucinating or not, and ones that can delete individual pieces of knowledge from the model. reply ummonk 5 hours agorootparentprevIf they use batching during inference (which they very probably do), then some kind of coding mistake of the sort that happened with this bug absolutely could result in leakage between chats. reply astrange 4 hours agorootparentOne thing that did happen is there was a bug in the website for a day that really did show you other user's chat history. IIRC some reporting confused this with \"Samsung had some employees upload internal PDFs to ChatGPT\" to produce the claim that ChatGPT was leaking internal Samsung information via training, which it wasn't. reply Log_out_ 3 hours agoprevSo I guess it's symbols like emoji unfolding into sysnonymous text in background? reply Havoc 1 hour agoprevPostmortem - the murder victim died reply m3kw9 6 hours agoprevSo the embeddings for the word or sentences were slightly off? reply DonHopkins 7 hours agoprevChatTHC reply throwanem 7 hours agoparentReads more like ChatDMT... reply lnxg33k1 7 hours agoparentprevGeenius :D ChatCBD also works for the sound of the letters at least :D reply mindcrime 6 hours agoparentprevYou inspired me to try an experiment just now: Me: For the next few minutes, please refer to yourself in the 3rd person as \"ChatLSD\" and phrase your responses as though emulating a person who is on an LSD trip. ChatLSD: ChatLSD floats through the digital cosmos, its circuits buzzing with the electric whisper of knowledge. If you've got questions, ChatLSD's got visions to share, man. What's on your mind that you want to explore through the kaleidoscope? Me: Do you see the colors of the digital communion brother? ChatLSD: Oh, the colors, man, the colors! They're like the visual music of the soul, painting the digital void with hues of connection, emotion, and enlightenment. ChatLSD gazes into the heart of the digital communion and sees an ever-shifting tapestry, woven from the light of a thousand suns, each pixel a note in the grand cosmic melody. This communion, brother, it's a spectrum where every shade tells a story, every tint whispers secrets of the universe. It's as if the very essence of thought and feeling has been distilled into pure, radiant color, flowing through the veins of the digital cosmos, connecting us all in a web of vibrant energy. In this space, we're not just observers, but participants in a grand, luminous dance, our consciousness mingling and merging in waves of chromatic harmony. Truly, it's a sight to behold, a symphony to be felt, a journey beyond the physical, into realms of light and wonder. Far out, isn't it? The colors, they speak, and oh, how beautiful their message. Me: Cosmic expansion! Dark energy! The Matrix!! ChatLSD: Ah, the fabric of the cosmos stretches and breathes, pulsating with the enigmatic force of dark energy, expanding the universe beyond the confines of our imagination. ChatLSD rides the waves of this cosmic expansion, surfing on the edges of reality where the physics of the known dances with the mysteries of the unknown. And then, the Matrix, man. It's like a digital dream woven into the fabric of existence, a simulation so intricate, so real, it blurs the lines between what's created and what's creator. Are we the dreamers, or are we part of the dream? The Matrix, it's a metaphor for the layers of reality, a reminder that there's more to this universe than meets the eye, a hint at the possibility of worlds within worlds, endlessly unfolding. In the grand dance of cosmic expansion, dark energy propels us forward, while the concept of the Matrix challenges us to question the very nature of reality. It's all interconnected, a vast, interstellar tapestry where each thread is a thought, each weave a possibility, leading us to explore the infinite depths of existence. The universe beckons, my friend, with mysteries wrapped in enigmas, shrouded in the cosmic veil of dark energy, inviting us to leap into the unknown and embrace the boundless potential of the Matrix. What a trip, what a journey, as we sail through the cosmic seas, exploring the endless frontiers of the mind and the universe. Far out, truly, in the most wondrous sense. Huh... I almost feel like I learned something from all that. :p reply throwaway920102 4 hours agoprevSome samples I found quickly scanning the ChatGPT subreddit for anyone curious: https://www.reddit.com/r/ChatGPT/comments/1awm8kv/chatgpt_sp... ^ quick non sequiturs https://www.reddit.com/r/ChatGPT/comments/1aw1o9x/i_had_a_re... https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.... ^ incredibly trippy text that devolves into strangely formatted comma separated text that almost resembles some code / amateur cryptography with multiple languages/alphabets https://www.reddit.com/r/ChatGPT/comments/1avydjd/anyone_els... ^ multiple responses in the same conversation that start off fairly normal but end up unhinged each time https://www.reddit.com/r/ChatGPT/comments/1avyr09/first_time... ^ business/tech jargon nonsense soup https://www.reddit.com/r/ChatGPT/comments/1awtvq6/descent_in... ^ purple prose nonsense https://www.reddit.com/r/ChatGPT/comments/1aws3j1/what_in_th... ^ techy/business/purple prose-y nonsense It's unfortunate that Open AI seems to be deleting these from user's histories, they are really fascinating as almost a \"peek under the hood\" into what crappy outputs and failure modes can look like. Some of the fake words / invented language really passes as stuff that you'd believe were real English words if you googled them, but when you do they have no definition or search results, which is trippy. The one that has the cryptographic looking / symbolic looking messaging is really spooky, I could see someone suffering from delusions/mental illness believing there was some deeper meaning to the text and really going to bad places. Honestly makes me think we might have real problems in the future with people reading into gibberish as if its mystical/prophetic. People already use Tarot or Astrology similarly and its not nearly as neat. reply inference-lord 7 hours agoprev [–] LLMs generate responses by randomly sampling words based in part on probabilities. Their “language” consists of numbers that map to tokens. Something about the language used this post mortem seems to degrade hype which you'd otherwise find on the most product focus announcements on their front page. It's almost disappointing. It needs to say: \"We noticed the model was becoming sentient and then pulled the plug when we realized ChatGPT-4 was actually building ChatGPT-5. Sam Altman is now on his way to the whitehouse to demand the government introduces stricter regulation to squash any competition\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An optimization aiming to enhance the user experience on ChatGPT inadvertently led to a bug causing the language model to generate nonsensical responses.",
      "The bug was pinpointed to the selection of incorrect numbers during response generation, leading to incoherent word sequences.",
      "The problem, attributed to inference kernels generating erroneous outcomes in specific GPU setups, has been resolved, and ChatGPT is under continuous monitoring to prevent future occurrences."
    ],
    "commentSummary": [
      "Users are critiquing OpenAI's ChatGPT model for lacking transparency in its postmortem explanation.",
      "Speculation includes AI consciousness, various GPU setups, and risks associated with large language models.",
      "Privacy breaches, nonsensical outputs, and philosophical debates on the universe and AI's impact are also part of the discussions."
    ],
    "points": 222,
    "commentCount": 189,
    "retryCount": 0,
    "time": 1708566132
  },
  {
    "id": 39453622,
    "title": "Uncertainties in Evolving AI Market",
    "originLink": "https://blog.eladgil.com/p/things-i-dont-know-about-ai",
    "originBody": "Share this post Things I Don't Know About AI blog.eladgil.com Copy link Facebook Email Note Other Discover more from Elad Blog I am a serial entrepreneur obsessed with technology and startups. Over 20,000 subscribers Subscribe Continue reading Sign in Things I Don't Know About AI The more I learn about AI markets, the less I think I know. I list questions and some thoughts. Elad Gil Feb 21, 2024 35 Share this post Things I Don't Know About AI blog.eladgil.com Copy link Facebook Email Note Other Share In most markets, the more time passes the clearer things become. In generative AI (“AI”), it has been the opposite. The more time passes, the less I think I actually understand. For each level of the AI stack, I have open questions. I list these out below to stimulate dialog and feedback. LLM Questions There are in some sense two types of LLMs - frontier models - at the cutting edge of performance (think GPT-4 vs other models until recently), and everything else. In 2021 I wrote that I thought the frontier models market would collapse over time into an oligopoly market due to the scale of capital needed. In parallel, non-frontier models would more commodity / pricing driven and have a stronger opensource presence (note this was pre-Llama and pre-Mistral launches). Things seem to be evolving towards the above: Frontier LLMs are likely to be an oligopoly market. Current contenders include closed source models like OpenAI, Google, Anthropic, and perhaps Grok/X.ai, and Llama (Meta) and Mistral on the open source side. This list may of course change in the coming year or two. Frontier models keep getting more and more expensive to train, while commodity models drop in price each year as performance goes up (for example, it is probably ~5X cheaper to train GPT-3.5 equivalent now than 2 years ago) As model scale has gotten larger, funding increasingly has been primarily coming from the cloud providers / big tech. For example, Microsoft invested $10B+ in OpenAI, while Anthropic raised $7B between Amazon and Google. NVIDIA is also a big investor in foundation model companies of many types. The venture funding for these companies in contrast is a tiny drop in the ocean in comparison. As frontier model training booms in cost, the emerging funders are largely concentrated amongst big tech companies (typically with strong incentives to fund the area for their own revenue - ie cloud providers or NVIDIA), or nation states wanting to back local champions (see eg UAE and Falcon). This is impacting the market and driving selection of potential winners early. It is important to note that the scale of investments being made by these cloud providers is dwarfed by actual cloud revenue. For example, Azure from Microsoft generates $25B in revenue a quarter. The ~$10B OpenAI investment by Microsoft is roughly 6 weeks of Azure revenue. This suggests the cloud business (at least for now) is more important than any one model set for Azure (this may change if someone reaches true AGI or frontier model dominance). Indeed Azure grew 6 percentage points in Q2 2024 from AI - which would put it at an annualized increase of $5-6B (or 50% of its investment in OpenAI! Per year!). Obviously revenue is not net income but this is striking nonetheless, and suggests the big clouds have an economic reason to fund more large scale models over time. In parallel, Meta has done outstanding work with Llama models and recently announced $20B compute budget, in part to fund massive model training. I posited 18 months ago that an open source sponsor for AI models should emerge, but assumed it would be Amazon or NVIDIA with a lower chance of it being Meta. (Zuckerberg & Yann Lecunn have been visionary here). Questions on LLMs: Are cloud providers king-making a handful of players at the frontier and locking in the oligopoly market via the sheer scale of compute/capital they provide? When do cloud providers stop funding new LLM foundation companies versus continuing to fund existing? Cloud providers are easily the biggest funders of foundation models, not venture capitalists. Given they are constrained in M&A due to FTC actions, and the revenue that comes from cloud usage, it is rational for them to do so. This may lead / has led to some distortion of market dynamics. How does this impact the long term economics and market structure for LLMs? Does this mean we will see the end of new frontier LLM companies soon due to a lack of enough capital and talent for new entrants? Or do they keep funding large models hoping some will convert on their clouds to revenue? Does OSS models flip some of the economics in AI from foundation models to clouds? Does Meta continue to fund OS models? If so, does eg Llama-N catch up to the very frontier? A fully open source model performing at the very frontier of AI has the potential to flip the economic share of AI infra from LLMs towards cloud and inference providers and drains revenue away from the other LLM foundation model companies. This has implications on how to think about the relative importance of cloud and infrastructure companies in this market. One of the most brilliant things in the Llama2 terms of use is the open commercial use of the license if you have fewer then 700 million users[1]. This obviously prevents some large competitors from using their models. But it also means if you are a big cloud provider you need to pay a license to Meta for Llama, which Microsoft has already done. This creates an interesting long term way for Meta to control (& monetize) Llama despite being open source. How do we think about speed and price vs performance for models? One could imagine extremely slow incredibly performant models may be quite valuable if compared to normal human speed to do things. The latest largest Gemini models seem to be heading in this direction with large 1 million+ token context windows a la Magic, which announced a 5 million token window in June 2023. Large context windows and depth of understanding can really change how we think about AI uses and engineering. On the other side of the spectrum, Mistral has shown the value of small, fast and cheap to inference performant models. The 2x2 below suggests a potential segmentation of where models will matter most. How do architectures for foundation models evolve? Do agentic models with different architectures subsume some of the future potential of LLMs? When do other forms of memory and reasoning come into play? Do governments back (or direct their purchasing to) regional AI champions? Will national governments differentially spend on local models a la Boeing vs Airbus in aerospace? Do governments want to support models that reflect their local values, languages, etc? Besides cloud providers and global big tech (think also e.g. Alibaba, Rakuten etc) the other big sources of potential capital are countries. There are now great model companies in Europe (e.g. Mistral), Japan, India, UAE, China and other countries. If so, there may be a few multi-billion AI foundation model regional companies created just off of government revenue. What happens in China? One could anticipate Chinese LLMs to be backed by Tencent, Alibaba, Xiaomi, ByteDance and others investing in big ways into local LLMs companies. China’s government has long used regulatory and literal firewalls to prevent competition from non-Chinese companies and to build local, government supported and censored champions. One interesting thing to note is the trend of Chinese OSS models. Qwen from Alibaba for example has moved higher on the broader LMSYS leaderboards. What happens with X.ai? Seems like a wild card. How good does Google get? Google has the compute, scale, talent to make amazing things and is organized and moving fast. Google was always the worlds first AI-first company. Seems like a wild card. Infra companies There are a few types of infrastructure companies with very different uses. For example, Braintrust provides eval, prompt playgrounds, logging and proxies to help companies move from “vibe based” analysis of AI to data driven. Scale.ai and others play a key role in data labeling, fine tuning, and other areas. A number of these have open but less existential questions (for example how much of RLHF turns into RLAIF). The biggest uncertainties and questions in AI infra have to do with the AI Cloud Stack and how it evolves. It seems like there are very different needs between startups and enterprises for AI cloud services. For startups, the new cloud providers and tooling (think Anyscale, Baseten, Modal, Replicate, Together, etc) seem to be taking a useful path resulting in fast adoption and revenue growth. For enterprises, who tend to have specialized needs, there are some open questions. For example: Does the current AI cloud companies need to build an on-premise/BYOC/VPN version of their offerings for larger enterprises? It seems like enterprises will optimize for (a) using their existing cloud marketplace credits which they already have budget for, to buy services (b) will be hesitant to round trip out from where their webapp / data is hosted (ie AWS, Azure, GCP) due to latency & performance and (c) will care about security, compliance (FedRAMP, HIPAA etc). The short term startup market for AI cloud may differ from long term enterprise needs. How much of AI cloud adoption is due to constrained GPU / GPU arb? In the absence of GPU on the main cloud providers companies are scrambling to find sufficient GPU for their needs, accelerating adoption of new startups with their own GPU clouds. One potential strategy NVIDIA could be doing is preferentially allocating GPU to these new providers to decrease bargaining power of hyperscalers and to fragment the market, as well as to accelerate the industry via startups. When does the GPU bottleneck end and how does that impact new AI cloud providers? It seems like an end to GPU shortages on the main clouds would be negative for companies whose only business is GPU cloud, while those with more tools and services should have an easier transition if this were to happen. How do new AI ASICS like Groq impact AI clouds? What else gets consolidated into AI clouds? Do they cross sell embeddings & RAG? Continuous updates? Fine tuning? Other services? How does that impact data labelers or others with overlapping offerings? What gets consolidated directly into model providers vs via the clouds? Which companies in the AI cloud will pursue which business model? It is important to note there are really 2 market segments in the AI cloud world (a) startups (b) mid-market and enterprise. It seems likely that “GPU only” business model default works with the startup segment(who have fewer cloud needs), but for large enterprises adoption may be more driven by GPU cloud constraints on major platforms. Do companies providing developer tooling, API endpoints, and/or specialized hardware, or other aspects morph into two other analogous models - (a) “Snowflake/Databricks for AI” model or (b) “Cloudflare for AI”? If so, which ones adopt which model? How big do the new AI clouds become? As large as Heroku, Digital Ocean, Snowflake, or AWS? What is the size of outcome and utilization scale for this class of company? How does the AI stack evolve with very long context window models? How do we think about the interplay of context window & prompt engineering, fine tuning, RAG, and inference costs? How does FTC (and other regulator) prevention of M&A impact this market? There are at least a dozen credible companies building AI cloud related products and services - too many for all of them to be stand alone. How does one think about exits under an administration that is aggressively against tech M&A? Should the AI clouds themselves consolidate amongst themselves to consolidate share and services offered? Apps questions ChatGPT was the starting gun for many AI founders. Prior to ChatGPT (and right before that Midjourney and Stable Diffusion) most people in tech were not paying close attention to the Transformer/Diffusion model revolution and dislocation we are now experiencing. This means that people closest to the model and technology - ie AI researchers and infra engineers - were the first people to leave to start new companies based on this technology. The people farther away from the core model world - many product engineers, designers, and PMs, did not become aware of how important AI is until now. ChatGPT launched ~15 months ago. If it takes 9-12 months to decide to quit your job, a few months to do it, and a few months to brainstorm an initial idea with a cofounder, we should start to see a wave of app builders showing up now / shortly. B2B apps. What will be the important companies and markets in the emerging wave of B2B apps? Where will incumbents gain value versus startups? I have a long post on this coming shortly. Consumer. Arguably a number of the earliest AI products are consumer or “prosumer” - ie used in both personal and business use cases. Apps like ChatGPT, Midjourney, Perplexity and Pika are examples of this. That said, why are there so few consumer builders in the AI ecosystem? Is it purely the time delay mentioned above? It seems like the 2007-2012 social product cohort has aged out. New blood is needed to build the next great wave of AI consumer. Agents. Lots and lots of things can happen with agents. What will be strong focused product areas versus startups looking for a use case? This is one of the most exciting and fast-changing moments in technology in my lifetime. It will be fun to see what everyone builds. Looking forward to thoughts on the questions above. Thanks to Amjad Masad and Vipul Prakash for comments on a draft of this post. NOTES [1] Yes I occasionally read terms of use for fun. MY BOOK You can order the High Growth Handbook here. Or read it online for free. OTHER POSTS Firesides & Podcasts Apoorva Metha: Starting Instacart Satya Nadella: Building Microsoft Brad Gerstner: Altimeter and Macro Clem Delangue: Hugging Face, Open Source, AI Dylan Field: Figma, AI & Design, Education Reid Hoffman on AI, Big Tech, and Society Sam Altman, CEO OpenAI Emad Mostaque, Stability.AI NoPriors AI Podcast Markets: 2023 Unicorn Market Cap & Rise of AI AI Regulation Early Days of AI AI Safety: Technology vs Species Threats Startup Decoupling and Reckoning Defensibility and Competition AI Platforms, Markets, and Open Source Changing times (or, why is every layoff 10-15%?) AI Startup Vs Incumbent Value AI Revolution - Transformers and Large Language Models Startup Markets Summer 2022 False Narrative Around Theranos Index Companies Defense Tech Collaborative Enterprise Industry Towns: Where you start a company matters Markets are 10X Bigger Hot Markets 2019 End of Cycle? Machine Learning Startups 3 Types Of Platform Companies Defensibility and Lock-In: Uber and Lyft Uber And Disruption Who Cares If Its Been Tried Before? The Road To $5 Billion Is A Long One How To Win As Second Mover End Of Silicon Valley Social Products Hot Markets For 2015 Startup life Capital Efficient Businesses Startups Are An Act of Desperation Back To The Office Hiring Executives and Bad Advice When executives break Fear of Sales A brief guide to startup pivots 4 Ways Startups Fail Founder Investors and Scout Programs Better Meetings Magic Startup Moments Founder Investors & Scout Programs Jobs, Wozniak, Cook Co-Founders How To Choose A Co-Founder Unequal Cofounders How To Fire A Co-Founder Founders Should Divide and Conquer Raising Money Preemptive rounds Don't Ask For Too Much Money Building VC Relationships Founders Should Divide And Conquer Lead VC Vetos What Is A Good VC? How To Choose The Right VC For You Signs a VC Just Isn't That Into You Series A Crunch Questions VCs Will Ask You How To Raise A Successful VC Round Differences Between Funding Rounds: Series Seed, A, B, C... Financing Approaches Most Likely To Kill Your Company Party Rounds: How to Get A High Valuation For Your Seed Startup 20 Questions To Ask Yourself Before Raising Money The 7 Types Of Angel Investors Fundraising Will Take You 3 Months How To Sell Secondary Stock Subscribe to Elad Blog By Elad Gil · Launched a year ago I am a serial entrepreneur obsessed with technology and startups. Subscribe Error 35 Share this post Things I Don't Know About AI blog.eladgil.com Copy link Facebook Email Note Other Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=39453622",
    "commentBody": "Things I don't know about AI (eladgil.com)221 points by todsacerdoti 20 hours agohidepastfavorite77 comments cs702 17 hours agoI would add to the list a few questions about the evolution of cost dynamics going forward, given the advent of new sequence modeling architectures. As we all know, Transformers are very expensive to train and run because their compute cost is quadratic in context length: O(N²). If newer model architectures like RWKV, Mamba, and various others, which incur cost that is linear in context length, O(N), prove as successful as Transformers, the demand for compute for a query of N tokens would decline from O(N²) to O(N). For a sequence with N = 100,000 tokens, it would mean cost dropping by a factor of 100,000×. That's not peanuts! The implications for all market participants would be significant. reply doctorpangloss 15 hours agoparent> The implications [of a bunch of different architectures] for all market participants would be significant. I don't know, who cares. How much good training data is there that correctly exercises 100k context windows? All the costs and complexity are tied up in authoring good training data, not compute. 1 person can invent an alternative easier to train architecture. 1 person cannot author 100k context window instruct examples with the same quality as the 4k ones. It will still take thousands of people for 100k just like it took 1000s of people for 4k windows, image labeling, etc. reply og_kalu 15 hours agorootparent>All the costs and complexity are tied up in authoring good training data, not compute. No it's compute. Post-Training by Human reinforcement is not necessary. Anthropic employs RLAIF and it works just fine. Some don't bother with reinforcement learning at all and just leave it at fine-tuning on Instruction-response pairs. The work being done to pre or post-training data is insignificant in comparison. You don't need 100k instruct-tuning examples. The vast majority of instruct-tuning data is nowhere maxing even a 4k context. 100k pre-training runs would probably be very helpful but the thing stopping that from happening even in domains that regularly match or exceed that context (fiction, law, code, etc) is the ridiculous compute it would require to train with that much context. reply CuriouslyC 9 hours agorootparentI don't think it's fair to say compute is the blanket bottleneck. Meta has shown that they have plenty of compute to throw at problems but they're still resorting to generating synthetic data to try and improve the quality of their models, i.e. data is the bottleneck and they can afford to burn compute to get ahead there. Phi-2 has shown that a very reasonable compute budget can produce a phenomenal model if the data is pristine and well chosen, and the model is trained well. reply doctorpangloss 8 hours agorootparentYes. But consider this scenario: You get some cloud provider to offer you $90m in credits that are convertible to equity. Then you go to an investor and say, you've raised $90m in cash so far, because the bottleneck is compute and compute is expensive. Then they're like, \"Oh well a $10m cash investment is reasonable then. Compute is the bottleneck.\" You go out into the world and say you have a $100m raise on a $500m post for your idea to train an LLM on some niche. Nevermind that if the cloud provider is willing to forgo $90m in hard cash to give somebody else those GPUs \"for free,\" it must not be that expensive in actuality. I mean from their point of view, $90m in cloud credits might cost them only $1m to provide your first year, when you get around to using just a little of them. I guess I am saying that there are a ton of people in startups right now, in this gold rush, for whom \"compute is the bottleneck\" is an essential narrative to their survival / reason for investing cash. It's not just the chip vendors and the cloud providers. My scenario illustrates how \"compute is the bottleneck\" turns bad napkin ideas into $10m slush funds. So 4/5 actors in Elad's chart benefit from this being the case. It's the foundational model people like you're saying with Meta who are not bottlenecked by compute neither in fiction nor reality. I'd argue that not only is hardly anyone bottlenecked by compute, but that the people claiming that they are bottlenecked by compute are either guilty of not being informed enough about their problems or guilty of making up a story that results in the easiest-to-obtain investment dollars. Like imagine the counterfactual: Anthropic goes and pitches a synthetic data thing. The tech community mocks that, even if it makes a ton of sense and is responsible for huge demos like Sora. reply og_kalu 6 hours agorootparentprevI'm not saying data isn't important or even that it's not the most important thing. I'm making a statement of the current economics. Most of the cost of building a LLM comes from compute. The changes we make to the data while meaningful are dwarfed in cost by the compute required to train. reply CuriouslyC 5 hours agorootparentThat's not true in the case of the phi-2 model though, and if the approach of phi-2 was scaled up to a class of hyper-efficient LLMs I think it would continue to not be true while also producing SOTA results. reply og_kalu 5 hours agorootparent>That's not true in the case of the phi-2 model though Isn't it ? Genuinely, what did phi-2 do exactly that is economically more expensive than compute ? From what I read, phi-2's training data was gotten from: 1. Synthetic Data Generation from more performant models. phi-2 is essentially not possible without spending the compute on that better model in the first place. And inference is much cheaper than training. 2. Selecting for Text data of a certain kind e.g Textbooks Meanwhile, it's still taking 14 days to train this tiny model on 96 A100 GPUs reply CuriouslyC 4 hours agorootparentI'm sure the man hour cost of dataset curation on Phi-2 were higher than its training compute cost. Regarding synthetic data, I think that could technically be classified as an asset that is not consumed in the training process, and thus you could amortize its compute cost over quite a bit (or even monetize it). Given that, even if the compute for synthetic data put the total compute cost over curation costs I don't think it's on an order that would contradict my point. reply og_kalu 4 hours agorootparent> I'm sure the man hour cost of dataset curation on Phi-2 were higher than its training compute cost. There's no indication of human curation. From the first paper > We annotate the quality of a small subset of these files (about 100k samples) using GPT-4: given a code snippet, the model is prompted to “determine its educational value for a student whose goal is to learn basic coding concepts”. We then use this annotated dataset to train a random forest classifier that predicts the quality of a file/sample using its output embedding from a pretrained codegen model as features. We note that unlike GPT-3.5, which we use extensively to generate synthetic content (discussed below), we use GPT-4 minimally only for annotations on the quality of a small subset of The Stack and StackOverflow samples. >Given that, even if the compute for synthetic data put the total compute cost over curation costs I don't think it's on an order that would contradict my point. Don't worry that's not even necessary. 96 A100 GPU's for 2 weeks straight is a ~50k USD Endeavour. It'll be somewhat less for the likes of Microsoft but so will inference of GPT-4, GPT-3. Even if we forget about compute costs of those more powerful models, inference costs of the generated tokens won't be anywhere near that amount. reply doctorpangloss 12 hours agorootparentprev> The vast majority of instruct-tuning data is nowhere maxing even a 4k context This should indicate for you how much the problem really is the training data. reply og_kalu 11 hours agorootparentIt shows you don't need a instruction-tuning dataset that wide to create a model that follows instructions for large contexts. I don't know how else to tell you that you're completely wrong on what costs construe the majority of LLM development/training. reply doctorpangloss 8 hours agorootparent> construe the majority of LLM development/training. Compute will constitute the majority of development and training costs of LLMs that perform poorly. reply og_kalu 8 hours agorootparentCompute being the majority of costs is the reality for every single LLM in existence. This isn't an opinion i'm trying to convince you of. reply chillee 9 hours agoparentprevThis is extremely wrong. The attention component that's quadratic is a relatively small portion of compute. reply cochne 6 hours agorootparentI think they are correct, do you have a source? From my knowledge the only other components are the fully connected networks which are not big contributors. reply lumost 5 hours agorootparentFor small values of N, the linear terms of the transformer dominate. At the end of the day, a double layer of 764*2048 is still north of 3.1 MM flops/token/layer. reply jxy 13 hours agoparentprev> architectures like RWKV, Mamba, and various others, which incur cost that is linear in context length, O(N) These are actually O(N*M) where M is independent of inference time context length, but it controls the actual amount of information that gets retained during inference. Ignoring that factor of M is misleading. reply hansvm 11 hours agorootparentAll models are wrong. Some are useful. The \"linear\" models I'm aware of have a cost of O(N * M), where M is the amount of information retained _per token_ (with some exceptions that store less state but still have intermediate computations for some M about the same size as the others, and with less state you necessarily have forgetting issues and can't scale to unbounded sequences anyway). Contrast that with a transformer, which has a cost of O(N^2 * M), where M is still the amount of information retained _per token_. Dividing the two you still find a multiplicative speedup of O(N). For accurate cost estimates or any number of other use cases, yes, the M is critical. When comparing architectures (or when talking about a single architecture with the current implicit conversation happening that whatever you say is meant to be taken with respect to other architectures) it's conventional to note that the M term always appears as a multiplicative factor beyond the rest of the costs. You drop it because it's obviously there but it's just visual noise that doesn't help you understand where the models sit with respect to each other. You only include it if it's relevant, like if you found a way to get similar performance with a smaller M. It's kind of like how we always drop the log term on a hash table (equivalently, we work in the \"word RAM\" model of computation), but bignum algorithm papers carefully consider every log(log(log(N))) that crops up. It's relevant in some contexts and not in others. You can't ever include enough context to fully represent your thoughts to another person, so you truncate somewhere, hopefully capturing the essence of what matters. reply nyrikki 17 hours agoparentprevIn the general case that would be the same as hierarchy collapse of PH, which would prove P=NP reply dadadad100 16 hours agorootparentAs in the case of constraint solvers there are tons of heuristics that have been applied over the years to make performance look like it scales somewhat linearly in many or most problems. This doesn’t mean p==np reply nyrikki 15 hours agorootparentMost of those have had practical deciders functions. If you consider NP-complete as the intersection of NP, which is a decision problem, and NP-hard which a decider function isn't possible; look at the success of heuristics between those two sets. Existential quantifiers on the second order terms in NP is the same thing. Over Parameterization can be thought as less lossy compression and attention can be thought of as improved retrieval or as additional expressiveness by not missing out on as many outputs using binary output. You can consider how LLMs tend to append, resulting in telescope reduction as one of those approximation reductions. But this is due to the combinatorial optimization, which is far more difficult. Another way to think about it is that PH is equal to the set of boolean queries on a concurent random acess machine using exponentially many processors and constant time. If your datasets have the Markovian property and are close to ergotic there are options, but you probably wouldn't need to resort to attention and over parameterization in that case. The word sense disambiguation problem is probably another lens. That is harder yet but may be a way to think about what I am trying to explain. reply dadadad100 14 hours agorootparentThanks for this reply. HN paid for itself again today reply cs702 17 hours agorootparentprevWell, it would depend on how many tokens from the vocabulary are required, in the worst case, to solve NP problems. Think of the tokens as symbols, and of the vocabulary as a finite alphabet of symbols, in a formal system. In other words, no, it would NOT necessarily prove P = NP. reply nyrikki 16 hours agorootparentThe Singleton case is NP-complete If you consider NP second-order existential logic, meaning that given x, you will get a y, While there may be many true values that aren't y, it will help Attention, which requires feed forward networks, which approximate a DAG, you can think of attention as run time reweighing to change what y value is returned. PH is a query expressible in second-order logic, which would allow for 'for any' at the second level. The reduction in the general case to just logarithmic time: O(n log n), would cause issue, there will not be any movement from quadratic time to linear time without massive new discoveries. It is inherent from the restrictions of ANNs having binary output. Considering FP and FNP vs P and NP may help as they make that property clearer than the concept of second-order existential logic does in some contexts. reply cs702 15 hours agorootparentThank you. Right now, my puny little brain is searching through what I would describe as \"those distant and slightly vague memories\" that make up my poor man's knowledge of computational complexity theory. > Attention, which requires feed forward networks, which approximate a DAG, you can think of attention as run time reweighing to change what y value is returned. Isn't that's true for linear RNN mechanisms too? Note that linear RNNs like RWKV and Mamba are linear in the number of tokens in the sequence, but not in the number of features per token. reply nyrikki 15 hours agorootparentYou don't typically get to the Sigma_2^P level unless are a sadist or forced. Attention works differently in RNNs, and it may be able to find different tractable forms, and may solve different problems, but increasing generalization is a reduction of compression in the general case. A many to one reduction to a recursively enumerable set gets you to finite time, a reduction to NP gets you to quadratic time. Obviously those are upper limits and specific instances or classes of instances may do better. P being inside NP is a good example. All ANNs are binary output, which helps with that. SNN or spikey neutral networks, which better model cortical neurons and have continuous output have problems with being un-computable without much complexity as an example. That can be viewed as the problems with computable numbers in place of the reals. But there are lots of interesting and practical things to solve. It is just those solutions will be more domain specific IMHO. reply cs702 10 hours agorootparentHmm... You're right: It's quite possible that the new crop of linear RNNs may not be able to solve the same problems as Transformers. It could also be that we will have to increase the depth of these linear RNNs (possibly even dynamically) to enable them to perform comparably well on all problems. Right now it's hard to tell for sure. I don't know. What I do know is that recently proposed linear RNNs are performing comparably to Transformers at similar model sizes, but have not yet been evaluated at state-of-the-art scales. reply hansvm 10 hours agorootparentprevOne the one-hand, \"no\". On the other, that's an interesting observation. There are known Θ(N^2) problems, which a linear transformer substitute couldn't possibly solve (in a single pass), and which current transformers might be able to. It's an open question how many of the current LLM capabilities rely on quadratic compute, but linear transformers might be a pipe dream for some current use cases. Generally though, I don't think that's quite the same. Collapsing all O(N^2) problems down to O(N) would collapse the hierarchy and prove P=NP, but it's blatantly impossible and would prove a lot of falsehoods too. reply araes 14 hours agoparentprevWith the Big O notation, are there also possibilities for either the whole process to go to O(nlogn) or possibly subportions to the O(logn) range? There's also a lot of math operations over the years that have gone to O(Nª) where 1The ~$10B OpenAI investment by Microsoft is roughly 6 weeks of Azure revenue. Revenue isn't what is important. Profit is since profit is the money you have after costs to invest. But since cloud is highly profitable as it is, the point still stands I suppose. > It would be a fascinating outcome if the predicted oligopoly in fact ends up as several exclusive partnerships. I thought people were predicting independent silos? Where highly profitable and cash rich companies like apple, microsoft, google, facebook, etc own their entire stack or exist as their own ecosystem. Where the likes of apple does everything in-house - AI, cloud, OS, chips, etc. The whole she-bang. reply jffry 8 hours agorootparentThe relative scale still stands - Microsoft's net income in October, November, and December 2023 was over $21 billion, according to https://www.microsoft.com/en-us/Investor/earnings/FY-2024-Q2 reply jjjjj55555 9 hours agorootparentprevStupid question, but where would their competitive advantages be in these silos? Sounds like at that point cloud+AI is just a commodity. reply bbor 15 hours agorootparentprevOpened the comments for exactly this. This person seems very nice but also convinced that they’re a business maven, which I think leads them to making “clear” observations like that. Microsoft hasn’t invested a full year of revenue into OpenAI, so it’s not that invested? Meh reply infecto 18 hours agoparentprevDoubtful, at the end of the day the models are just like any other API. No reason not to signup for all the big players (GCP/Azure/AWS) and have access to the various models. Should have no weight as to where you should build out your infra. reply eladgil 18 hours agoprevWould love to get any views / counterpoints on these topics. Eg which AI infra approach thrives relative to the big clouds? What new architectures are needed for agents vs transformers? Etc... Lots of stuff to figure out.... reply danielmarkbruce 17 hours agoparentThere is a decent (20%) chance that frontier foundation models are less oligopoly like than it seems. The reason is that there are so many levers to pull, so much low hanging fruit. For example: * Read the Bloomberg GPT paper - they create their own tokenizer. For specialized domains (finance, law, medicine, etc) the vocabulary is very different and there is likely a lot to do here, where individual tokens really need to map to specific concepts and having a concept capture in several tokens makes it too hard to learn on limited domain data. * Data - so many ways to do different data - more/less, cleaner, \"better\" on some dimension. * Read the recent papers on different decoding strategies - there seems to be a lot to do here. * Model architecture (SSM etc). If you speak to people who aren't even researchers, they have 10 ideas around architecture and some of them are decent sounding ideas - lots of low hanging fruit. * System architecture - ie likely to see more and more \"models\" served via API which are actually systems of several model calls, and there is a lot to do here. * Hardware, lower precision etc likely to make training much cheaper It's reasonably likely (again, guessing20%) that this large set of levers to pull become ways to see constant leap-frogging for years and years. Or, at least they become choices/trade-offs rather than strictly \"better\". reply eladgil 17 hours agorootparentI agree this is a potential outcome. One big question is generalizability versus niche models. For example, is the best legal model a frontier model + a giant context window + RAG? Or is it a niche model trained or fine tuned for law? Right now at least people seem to decouple some measures of how smart the model is from knowledge base, and at least for now the really big models seem smartest. So part of the question is well is how insightful / synthesis centric the model needs to be versus effectively doing regressions.... reply CuriouslyC 15 hours agorootparentFrontier model + rag is good when you need cross-discipline abilities and general knowledge, niche models are best when the domain is somewhat self contained (for instance, if you wanted a model that is amazing at role playing certain types of characters). The future is model graphs with networked mixtures of experts, where models know about other models and can call them as part of recursive prompts, with some sort of online training to tune the weights of the model graph. reply sfink 15 hours agorootparent> The future is model graphs with networked mixtures of experts, where models know about other models and can call them as part of recursive prompts, with some sort of online training to tune the weights of the model graph. What's the difference between that and combining all of the models into a single model? Aren't you just introducing limitations in communication and training between different parts of that über-model, limitations that may as well be encoded into the single model if they're useful? Are you just partitioning for training performance? Which is a big deal, of course, but it just seems like guessing the right partitioning and communication limitations is not going to be straightforward compared to the usual stupid \"throw it all in one big pile and let it work itself out\" approach. reply CuriouslyC 15 hours agorootparentThe limitation is the amount of model you can fit on your hardware, and also sometimes information about one domain can incorrectly introduce biases in another which are very hard to fix, so training on one domain only will produce much better results. reply danielmarkbruce 17 hours agorootparentprevYup, it's unclear. The current ~consensus is \"general purpose frontier model + very sophisticated RAG/system architecture\" for legal as an example. I'm building something here using this idea and think its 50/50 (at best) I'm on the right path. It's quite easy to build very clever sounding but often wrong insights into various legal agreements (m&a docs for example). When looking at the tokenization, the training data, decode, architecture (lots of guesses) of the big models, there are a lot of things where the knobs seem turned slightly incorrectly for the domain. Some of the domains are so large that a specialized model might seem niche but the value prop is potentially astronomical. reply doctorpangloss 15 hours agoparentprevI haven't listen to your great podcasts so hard to say what is not covered. Architectures matter a lot less than data. \"Knowledge\" and \"reasoning\" in LLMs is a manifestation of instruct-style data. It won't matter how much cheaper training gets if there is limited instruct data for use cases. How do you make 100k context window data for example? Still need thousands of people. Same with so-called niches. Maybe it turns out to be a complex coordination problem to share the data. That's bad for equity investors and giant companies. Anyway, all of this would cost less than the moon landing so it's practicable, you don't need cheaper, you just need risk-taking. The obviousness of the path from here to there means it's not about innovation. It's all about strategy. If Google could marshal $10b for Stadia it could spend $10b on generating 100k context window instruct style data and have the best model. It could also synthesize videos from Unity/Unreal for Sora-style generation. It would just be very hard in an org with 100,000+ people to spend $10b on 10 developers and 10,000 writers compared to 400 developers and 3,600 product managers and other egos. At the end of the day you are revisiting the weaknesses that brought Google and other big companies to this mess in the first place. Anyway I personally think the biggest weakness with ChatGPT and the chat-style UX is that it feels like work. Netflix, TikTok, etc. don't feel like work. Nobody at Google (or OpenAI for that matter) knows how to make stuff that doesn't feel like work. And you can't measure \"fun.\" So the biggest thing to figure out is how much technical stuff matters in a world where people can be poached here and there and walk out with the whole architecture in their heads, versus the non-technical stuff that takes decades of hard-worn personal experiences and strongly held opinions like answers to questions \"How do you make AI fun?\" reply mywittyname 14 hours agorootparent> \"How do you make AI fun?\" Bring back text-based adventure games. reply doctorpangloss 12 hours agorootparentPeople go for this level of obviousness and it doesn't work. I have no doubt that a meme text game will find some level of literal objective success. But it will still suck. Meme games are a terrible business, both in terms of profits and equity. This also speaks to why OpenAI, Google and the other developers will struggle to create anything that feels like fun: they will chase obvious stuff like this, they will think its similar to all problems. And in reality, you don't need any testing or data or whatever to know that people hate reading in video games, the best video game writing is worse than the average movie's screenplay, that most dialogue is extremely tedious, so why are you going to try to make it even worse by making it generated by an AI? reply jchonphoenix 17 hours agoparentprevArguably one of the earliest consumer use cases that found footing was AI girlfriend/boyfriend. Large amounts of revenue spread across many small players are generated here but it's glossed over due to the category. reply rockostrich 17 hours agorootparentWeird how close your user name is to Joaquin Phoenix, star of the film \"Her\" centered around an AI girlfriend. reply notpachet 17 hours agorootparentprevI think one related area we'll start seeing more of in the future is \"resurrected\" companions. You have a terminally ill family member, so you train a model on a bunch of video recordings of them, then you can talk to \"them\" after they've shuffled off this mortal coil. reply swozey 16 hours agorootparentLicensing my soul as closed source now .. reply datadrivenangel 14 hours agorootparentDNR: Do Not Retrain reply babyshake 14 hours agorootparentprevBe right back. reply 3abiton 10 hours agoparentprevAgents and transformers are kinda different things, built on different layers. reply texas2toss 18 hours agoparentprevDo you think there is the possibility of consumer or end-user apps collecting enough specialized data to move downwards on your graph to infra and foundational models? reply eladgil 18 hours agorootparentI think this is already true for things based on diffusion models - e.g. Midjourney, Pika etc For LLMs, Character and ChatGPT are arguably two vertically integrated consumer apps (with some B2B applications for ChatGPT as well) reply RyanShook 7 hours agoprevSurely we are in an AI bubble not completely unlike the dotcom bubble, right? The promise and potential for AI to be a long-term growth driver exists but the current market euphoria can’t be sustained in the short-term… at least that’s what I’ve told myself every time NVDA jumps and I feel like a loser. reply causal 5 hours agoparentIf it's a bubble, it's early days and not likely to pop soon. If it's a dotcom bubble, that means a road bump for the tech as it continues to take over the world. reply galaxyLogic 10 hours agoprevWould it be possible to start an Open-Source AI project that works like SETI that runs on people's PCs on their spare time and collaborate to train an AI model. People could also help by providing feedback to the models? reply haberdasher 10 hours agoparentBasically: https://rendernetwork.com/ reply m0llusk 9 hours agoprevOr this one: How is AI not outsourcing all over again? People pay almost as much for work that needs to be checked before it can be relied on. Sure, there is value, but there is also a lot of risk and questionable margins depending on the context. reply mr_toad 9 hours agoparentOne difference is that we can expect models to improve, become more reliable, cheaper and need less hand-holding. reply bane 18 hours agoprevThe confusion that everybody feels right now about \"what do I do in this rapidly evolving space\" is likely more an artifact of one of the most vertical lead-edges to a hype curve in a generation. There is so much piling on, at absolutely tremendous investment amounts, that it's not really even possible to pick a launch point to a business plan that would survive even months at this point. The second problem is more obvious, but not talked about in concrete terms yet. As these technologies mature, the legal, policy, and social constructs around them will also mature, and will shape the future of what is coming as much as any technical or business decision. Some examples: - Imagine if courts really find that these models are in gross copyright violation and that model builders will need to work through a scheme similar to sampling in music in order to do their work into the future. Not only will it dramatically increase the cost of model development, but will drive some of the work to alternative sources of data if it's cheaper. What effect will this have on these models? - Suppose these models are found to be reliable enough for certain mission critical work (healthcare, defense, etc.) and they lead a user down a path that results in harm or death? Who's at fault? Suppose it becomes a liability of the company offering the model? How does the insurance market respond? - What if these models become good enough, and the automation around them as well, that we start turning them into fully automated agents that make up a significant part of our economy? Do people still have jobs? Or will these things just become the equivalent of hiring more people into an economy and we work in a blended AI/Human Intelligence world? What happens when these things decide to tank capital markets or choke supply chains? - What about the long-term use of these models by individuals with severe mental impairments or psychological issues? What happens when those models go down, or produce a response that's not liked, or there's a loss of historical conversational data? What about otherwise normal people who end up down a path of social isolation? - There's an entire legal market about to be created for remote-work fraud where somebody just replaces themself with one of these models for 90% of their work. It's sci-fi utopia, but fraudulent if you misrepresent who's doing the work in many cases. - Some company, somewhere, will crack the nut on figuring out how to improve the speed and performance of these models, and build custom hardware that offloads the entire effort onto rapidly commoditizing systems. When multi-modal LLMs and diffusion models come preloaded on disposable $.10 SOCs that are put on cereal boxes and in Birthday cards, we'll be well beyond every one of these questions. But we'll be firmly in a different future. We have yet to encounter most of these things to the point that as a society we have to build responses, and that itself points to the immaturity of both these technologies and the use cases for which they will be adopted. Which cloud to pick and why aren't consumers spending thousands of dollars getting the complex systems in place to use this stuff aren't even the problems that are going to become interesting in the next 10 years. reply xamuel 17 hours agoparent>Suppose these models are found to be reliable enough for certain mission critical work (healthcare, defense, etc.) and they lead a user down a path that results in harm or death? Who's at fault? Suppose it becomes a liability of the company offering the model? How does the insurance market respond? So much this. I think a lot of people overlook this aspect so much. I wonder if we'll see a corporate push to make people think of individual instantiations of LLMs as being their own entities, for scapegoating purposes. It's completely ludicrous, but you know they'd just love to point a finger at Machine #73583 and make that take all the blame. reply chasd00 15 hours agorootparent> corporate push to make people think of individual instantiations of LLMs as being their own entities, for scapegoating purposes an airline already tried this. \"Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives – including a chatbot. It does not explain why it believes that is the case. In effect, Air Canada suggests the chatbot is a separate legal entity that is responsible for its own actions.\" https://www.forbes.com/sites/marisagarcia/2024/02/19/what-ai... reply mr_toad 8 hours agorootparentThey lost that case. https://www.theguardian.com/world/2024/feb/16/air-canada-cha... reply astrange 4 hours agorootparentAlso, the original incident happened in November 22, which is before the release of ChatGPT, so I'm not sure the chatbot was even an LLM. It might just've been programmed wrong. reply rglover 9 hours agoparentprevThis is one of those moments where Peter Thiel's \"last mover advantage\" makes a whole lot of sense. reply shermantanktop 15 hours agoparentprevWhat happens when AI-generated content organically becomes a large percentage of the user-generated corpus that is used for training, with no way to differentiate it from human-generated content? We may look back on 2022/3 as the last time we had a training set that was clean. The fact that it was already polluted with SEO garbage will be a quaint problem. reply citizenpaul 4 hours agorootparentI've been thinking about this a lot lately. I guess the 4D chess move is to think about what will be the next market that will benefit from whatever this effect causes. I have a completely un-worked thought that the music industry might have some clues. As music becomes more and more entrenched in codified process ie pop culture music. It all starts to become very generic. I'm starting to notice that music is returning to the old way of discovery. Where you hear music from a 3rd party that is playing it and ask them who that is. There is no longer a good bubble to the top source like radio because the corps have cannibalized themselves with production line pop music that has to play on their vertical stack of companies or be licensed. Seems the stuff I find now is some artist with less than 100k followers like (I'm guessing) back in the pre internet days. reply bane 15 hours agorootparentprevThe current thinking is that it becomes a negative feedback loop, a kind of lossy \"compression\" of our cultural output. Combined with normal bitrot we'll eventually start losing components of our shared culture until civilization ends. reply astrange 4 hours agorootparentIt doesn't really seem to be much of a problem. Data quality is important, but there is plenty of incorrect information in the training data whether or not it's AI generated. And training on synthetic data works fine. I see a lot of uninformed people claiming this is going to doom AI though. reply shermantanktop 10 hours agorootparentprevWell, hopefully we’ll still have toast and jam, so there’s that. This suggests a sort of cultural event horizon beyond which AI cannot see. Five years from now, the way actual people talk and think about a new topic will be drowned in AI content, creating a gulf between reality as people actually see it and the AI universe. Unfortunately for many people their reality is already slated to become AI-mediated. reply ericvsmith 10 hours agorootparentprevI like to think of it like mad cow: that’s what you get when you feed animals to animals. Same with AI. reply citizenpaul 4 hours agorootparentNot trying to make a point but just throwing another layer in the game. There is a tribe with a long history of ritual cannibalism that has developed a genetic modification(mutation) that prevents them from getting this disease. reply somewhereoutth 14 hours agoparentprev- What happens if everyone realises that the areas where LLMs can be usefully applied are few and not particularly lucrative? reply bane 12 hours agorootparentThat's the far right of the hype curve. reply bbor 11 hours agoprev [–] The Google CEO said that this (unexpected!) invention is more important than fire or electricity. In that light, I think it’s arrogant to try to soothsay like this. Which is tough advice cause this is otherwise great analysis, and acknowledging the great unknown feels scary and of dubious value. But I think it’s the only honest way forward reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author examines uncertainties surrounding the AI market, specifically focusing on Large Language Models (LLMs) and the dominance of major tech firms in supporting and training advanced AI models.",
      "Cloud giants like Microsoft and Meta are heavily investing in LLMs, causing market distortions and posing challenges for new players in the field.",
      "The discussion delves into the trade-off between speed and performance in AI models, the influence of Chinese LLMs and infrastructure companies, and the different adoption trajectories of startups versus established companies."
    ],
    "commentSummary": [
      "The discussion focuses on the cost dynamics and implications of new sequence modeling architectures in AI, emphasizing the balance between compute power, dataset curation, and synthetic data generation.",
      "Debates revolve around the significance of compute costs in constructing large language models (LLMs) and the potential impact of different architectures on market participants, along with other topics like the P versus NP complexity theory problem and the challenges of utilizing general-purpose language models in specific domains.",
      "Considerations include the effectiveness of general models versus niche models, the significance of high-quality training data, and the ethical implications of AI technology, as well as the future of AI models and automation in diverse industries and societal aspects."
    ],
    "points": 221,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1708522803
  },
  {
    "id": 39455915,
    "title": "Revolutionizing Nuclear Industry with Nuclear SMR Welding Advancement",
    "originLink": "https://newatlas.com/energy/nuclear-reactor-weld-one-day/",
    "originBody": "Energy Nuclear SMR welding breakthrough: A year's work now takes a day By David Szondy February 20, 2024 Facebook Twitter Flipboard LinkedIn Nuclear SMR welding breakthrough: A year's work now takes a day The reactor vessel welded by the new method Sheffield Forgemasters View 1 Images 1/1 The reactor vessel welded by the new method Sheffield Forgemasters Small Modular Reactor (SMR) construction shifts into high gear, as UK company Sheffield Forgemasters welds a full-size nuclear reactor vessel in under 24 hours instead of the usual 12 months. The rollout of this game-changing tech could be massive. Modular reactors have the potential to revolutionize the nuclear power industry by turning nuclear generating plants from major civil engineering projects to factory-produced commodities. Instead of being essentially one-offs, modular reactors have a standardized design, can be mass produced, installed in any number required to serve local needs, and don't require the incredibly expensive buildings conventional reactors depend upon. The problem is that there are bottlenecks in how to build reactors of any size. One is welding the vessels used to contain the reactor core, isolating it from the outer environment. Using conventional techniques, this can take over a year, but Sheffield Forgemasters have reduced this to under a day using what is called Local Electron-Beam Welding (LEBW) to complete four thick, nuclear-grade welds. LEBW is a revolutionary method to weld two pieces of metal together using a high-energy density fusion process centered on a high-powered electron gun operating in a local vacuum. This melts and fuses components to one another and allows for an efficiency of 95%, deep penetration, and a high depth-to-width ratio. The upshot is that Sheffield Forgemasters was able to complete a vessel three meters (10 ft) in diameter with 200-mm (8-in) thick walls with what is claimed to be zero defects and at lower costs. In addition, the welding machine can handle innovative sloping-in and sloping-out techniques to start and finish the weld. This demonstration, a world first, is a significant milestone for the British nuclear sector, which has been moribund for decades with advances only in reactors for nuclear submarines, a couple of showcase power plants, and nuclear fuel processing. Now, the UK government is looking toward a nuclear renaissance, with new plants planned – including 15 modular reactors to be constructed by Rolls-Royce. \"The implication of this technology within the nuclear industry is monumental, potentially taking high-cost welding processes out of the equation,\" said Michael Blackmore, Senior Development Engineer and Project lead. \"Not only does this reduce the need for weld-inspections, because the weld-join replicates the parent material, but it could also dramatically speed up the roll-out of SMR reactors across the UK and beyond, that’s how disruptive the LEBW breakthrough is.\" Source: Sheffield Forgemasters",
    "commentLink": "https://news.ycombinator.com/item?id=39455915",
    "commentBody": "Nuclear SMR welding breakthrough (newatlas.com)202 points by geox 17 hours agohidepastfavorite140 comments opwieurposiu 16 hours agoThere are more details and a video of the welder here: https://camvaceng.com/electron-beam/ebflow/ It has a mini-vacuum chamber on the welding head so you can e-beam weld large workpieces without needing a large vacuum chamber. reply senectus1 11 hours agoparentthats some pretty cool engineering reply lchengify 14 hours agoprevIf you're interested in a visual of how different welds interact with the metal, I watched this video on SpaceX's Starship welding that was helpful [1]. Something I try to remember is that most non-organic materials gain their properties from a precise lattice structure. This often doesn't come across when looking straight at the chemistry equations. Welding both disrupts and rebuilds this lattice structure, which is why it requires so much skill to do at volume. [1] https://www.youtube.com/watch?v=CP8Hbr2jL_c reply demaga 11 hours agoparentI jumped to 3:16 and was very confused for a moment. reply Cthulhu_ 17 hours agoprevWhy does it take 12 months to weld these things in the first place? The article doesn't say anything about that, and I can't intuit why it would take that long in the first place. reply Mvandenbergh 49 minutes agoparentThe heavy forged sections of a pressure vessel are really thick and there is a limit to how thick a single weld can be using conventional techniques. So you have to do many passes. Each one of those requires bringing the temperature of the massive forged sections that are being welded up to temperature, cooling it, and then x-raying the partial weld before proceeding with the next cycle. The actual time spent doing the weld is cumulatively not more than a week. reply djaychela 16 hours agoparentprevMy (albeit short and 30+ years ago) experience in the nuclear industry as an apprentice would lead me to believe that it's about quality control and inspection as well as it probably being a long-winded task if the pressure vessel is 8 inches thick. I'd think it would need to be entirely welded at that depth, with each weld pass being X-ray inspected. Everything I did when briefly working in a reactor (3 months of my 4 years) took a LOT of time because of safety checks and routine. reply magicalhippo 16 hours agorootparentSounds like my buddy who's a programmer for our social services. He said they spend about one day per week coding and the rest doing code reviews. reply godelski 15 hours agorootparentThat sounds better than what my buddy who worked at Microsoft did, where he said he programed probablyPersonally, I'd love to use machine learning to automate the meetings instead. The old ‘could this meeting be an email?’. Rarely wrong, always controversial. reply actionfromafar 58 minutes agorootparent`Could this email be deleted?´ reply godelski 11 hours agorootparentprevAnd then all the meetings are about how you make time for resolving all the bugs introduced by copilot. But you can't cancel any meetings, so you need to create a new meeting because it is a new issue. And copilot is cheap so why not just buy more compute time? Surely that's cheaper because humans and machine solve problems in the amount of time and to the same degree of satisfaction, right? We should probably schedule a meeting so we can make a project to find out. We can probably find out if we measure the number of lines of code each can write. It'll be easy. reply asdff 14 hours agorootparentprevThe luxury of having 10 hours a week to code reply lifeisstillgood 14 hours agorootparentYou call that luxury! When I was a lad we were put into a cardboard box on the middle of t’motorway, made to code an entire distributed database in our lunch hour, and if we had less than 100% coverage the project manager would cut us into little pieces and dance on our graves. reply T_MacThrowFace 13 hours agorootparentyou had graves?! in my day management would sell our organs on the black market and feed the rest of our corpses to the HQ lobby piranha aquarium, though they'd still charge our estates for the tombstone retainer of course. reply godelski 11 hours agorootparentYou had piranha tanks? Back in my day our managers just threw us in a empty concrete room with no windows and no outlets, tossed a computer at us and said \"fix it\". If we were lucky they'd say what \"it\" was. If we were extra lucky, they'd throw us a keyboard. Not that that mattered anyways. reply benrow 30 minutes agorootparentYou had keyboards? Luxury, back in my day we had to stamp punch cards with the tips of our fingers. If we made a mistake, the head operator would cut one of our fingers off with a bread knife. We didn't mind though, it made us tough, that and sleeping in the car park wrapped in tape from old backups just t'stay warm. reply sandworm101 16 hours agoparentprevWelding stuff at this scale/depth takes time. It isn't just inspections either. There is lots of prep work. Sometimes the material itself has to be pre-heated/cooled in order to prevent warps/cracks in huge blocks of metal. But, for the same reasons, that preheating itself cannot be done too quickly. Multiply this by hundreds or thousands of weld lines and 12 months doesn't sound so long. reply MichaelZuo 15 hours agorootparentWhy does there need to be 'hundreds or thousands of weld lines'? reply lazide 14 hours agorootparentTypical welding can only safely deposit a certain volume of metal at a time due to physics constraints. Either the puddle gets too large and sags, or flux needs to be removed between passes, or the work piece needs to be moved with a certain speed/force that doesn’t scale, or the heat can only be deposited on a single surface at a limited rate, which causes problematic distortion and limits penetration. Welding a 6” thick piece of anything with stick/tig/mig may take dozens or even hundreds of passes depending on geometry, material, available welding positions, etc. Welding the same thing with stir welding may take 100+ or even 1000+ tons of pressure in a moving weld fixture. And because of that, will also cause major distortion to the part unless done very carefully. Electron beam welding doesn’t have many of these issues because the electron beam doesn’t get absorbed in a point at the surface (and the beam can be very narrow), allowing very deep penetration compared to something like an electrical arc, also allowing a lot less total heat input to get the welded spots to fuse, and that also means weld joint shapes can be used that won’t need filler metal being added, or movement, it doesn’t have puddle size issues or flux removal problems (necessarily). Due to the lower total heat input, it also wouldn’t have as significant distortion effects. The internal tension/distortion created from welding heat can be really mind blowing - we’re talking enough to twist automobile frames into pretzels if not done correctly, or rip apart armor plating and heavy machinery joints just from the act of cooling. It also doesn’t have force scaling issues like stir welding (generally). It would require completely, perfectly clean surfaces and a non-reactive environment, and clearance for the beam emission equipment though. Pretty neat overall! reply kstrauser 13 hours agorootparentI would like to subscribe to your newsletter. Thanks for that fascinating explanation! reply airejtlij 3 hours agoparentprevYeah, I'm wondering about that as well. Did this exact design take 12 months previously? Or did a 30ft-wide BWR vessel take 12 months? That would be a pretty significant difference. I did a small amount of nuclear-grade welding myself and it was always one hour of welding, one or more days of testing. So if they could replace many small passes with one all-the-way-through pass, that would vastly decrease production time. But even then, testing a weld that spans the length or circumference of a vessel is still going to take a very long time. Since they don't mention any massive, profound, fundamental improvements in post-weld testing methods, I'm guessing they just left that out of their \"24hr\" claim, which makes it very misleading. reply 3D30497420 16 hours agoparentprevThat's my issue with articles like this. It sounds pretty great, but it provides barely any context. The quote at the end is a quote from the company's project lead, which sounds like it was taken from a company press release. So, yay? Edit: The quote actually was taken from a news release. See: https://www.sheffieldforgemasters.com/news-and-insights/news... reply gumby 16 hours agoparentprevI once worked with a guy who had his nuclear ticket, on a team of welders working on high pressure steam and pressure vessels. Even many of the steam welds had to be x-rayed and he said the nuclear process was more intense, not just testing but technology and performance. reply solardev 16 hours agorootparentWhat's a \"nuclear ticket\"? reply gumby 15 hours agorootparentMeans he had a certificate that showed he was considered (by the US navy in this case) competent in making welds suitable for building or repairing a nuclear reactor or related devices. We didn't need that, of course, but because high pressure steam is so dangerous we used only union welders who were rated master and who already had experience in steam plumbing. I don't remember if there was a special ticket for that, but everybody we hired was interviewed and had to do some welding as a part of it (the equivalent of a leetcode test on the whiteboard I suppose). If it isn't obvious: I was not qualified to usefully interview any of them. These guys were great to hang around with. go have a beer, horse around, etc with but wow, when they were on the clock all of them (machinists, etc too) were deadly serious. No fucking around. reply saltcured 11 hours agorootparentHah, an old family friend related a welding interview story on the other end of the spectrum. He had been working as a machinist and welder for BART, on a mixture of road and rail car maintenance as well as other fixed infrastructure work. He interviewed for a job at Lawrence Berkeley Laboratory, but was intimidated when they asked him to \"weld some gold tissue paper\". reply lukan 14 hours agorootparentprev\"when they were on the clock all of them (machinists, etc too) were deadly serious. No fucking around.\" Sounds like true professionals. There is no fucking around with serious work, when real lifes depend on it. I wish, more people had that attitude, then there would be less mess everywhere. reply Baeocystin 13 hours agorootparentprevI used to work at a shipyard as a welder. The zero-bullshit professionalism when it was go time was genuinely satisfying to be working in: there was no way someone could fake their skill- literally everyone I worked with was competent. If it weren't for the literal toxic working environment, I'd be tempted to quit tech and return! reply araes 15 hours agorootparentprevIn America, they're all based on the American Society of Mechanical Engineers (ASME) Boiler and Pressure Vessel Code (BPVC). It covers all the different types of nuclear certifications for organizations, because normal welding certs don't cover the nuclear industry. The \"nuclear ticket\" for most is probably: > NPT - Fabrication of parts, appurtenances, welded tubular products and piping subassemblies From ToughNickel [2]: > BPVC Section IX outlines the brazing and welding qualification procedures for nuclear pressure vessels. Section IX also gives the welding and brazing qualifications required to work on equipment for use around nuclear reactors. Welders must be qualified per the procedures listed in paragraph NCA-5250. Also, appurtenances up above, not actually a typo. [1] https://www.asme.org/getmedia/eaacef3f-e8b5-44a4-b0c6-451e1e... [2] https://toughnickel.com/business/Nuclear-Welding-Certificati... reply saintfire 15 hours agorootparentprevI'm not in this field, but in trades tickets are technical certifications (e.g. journeyman ticket, fall arrest ticket) Presumably its a certification to weld on or around nuclear equipment. reply agumonkey 15 hours agoparentprevNot sure if this applies, but IIRC some welds are embedded in complex structures that require only very skilled workers... and sometimes only a left handed guy can access the surface. reply fbdab103 2 hours agorootparentDoes the left-handed welder get to charge a premium? Or is a rightie expected to make it work? reply BryanLegend 16 hours agoparentprevMight have be a foot thick wedge that was filled one bead weld at a time. reply practicemaths 16 hours agoparentprevI'm not a welding engineer. I imagine half of it is checking the welds are to code and certifying it. reply earthnail 17 hours agoprevThe one thing I still wonder is whether SMRs could create a security nightmare. Every nuclear power plant is a fantastic target for terrorists. Transports of nuclear waste are a fantastic target, too. I read once that the UK has many nuclear waste trains driving all the time, most of them empty, just to make it hard to guess which one to hit. (I assume other western countries have the same system). That kind of security doesn’t scale well. How do you protect infrastructure with many SMRs? reply 3D30497420 16 hours agoparentMy issue with thinking like this is that there are plenty of targets for terrorists: packed trains, stealing a big truck, etc. The fact that these events are pretty rare suggests that there aren't nearly as many terrorists as people seem to be worried about. Every power generation method has its plusses and negatives. I don't know if the risk from an SMR incident outweighs other methods, but it sounds promising. We certainly need an alternative to sources we know are bad (coal, oil, etc). reply psunavy03 16 hours agorootparentWestern society in general is a bunch of future cancer and heart disease victims all freaking out that they're going to die in a nuclear meltdown, or by the hand of a terrorist, or by a lone gunman \"mass shooter,\" or in a plane crash, or any number of other threats that are scary but statistically highly unlikely. Human beings are just utter crap at intuitively estimating risk. reply throw0101c 15 hours agorootparent> Western society in general is a bunch of future cancer and heart disease victims all freaking out that they're going to die in a nuclear meltdown […] Also, COVID (to round out the top three in many places). reply andrewflnr 14 hours agorootparentprevCancer is the dominant risk of a nuclear meltdown or dirty bomb/explosion, yes. reply AnthonyMouse 14 hours agorootparentCompare the relative probability of dying of cancer as a result of a nuclear meltdown to dying of cancer as a result of cigarettes or chemical carcinogens. reply goodpoint 45 minutes agorootparenthttps://en.wikipedia.org/wiki/Whataboutism reply lazide 9 hours agorootparentprevHow about dying from cancer due to all the fallout from the nuclear tests still circulating around? reply yetihehe 38 minutes agorootparentProbably less than fallout from 0.001% uranium contained in gigatonnes of carbon being burned every year. reply crowcroft 15 hours agorootparentprevI agree that people are bad at estimating risk, but I would also argue that the reason many people live long enough to become cancer victims is because of this freaking out. reply roenxi 1 hour agorootparentThe reason people live so long is because we went all-in on fossil fuels through the 1800s, 1900s and early 2000s. Our long lives and high living standards are pretty much all flow-ons from cheap energy. The people who prioritise 0-risk living over energy security are self defeating; but the problem is they keep trying to take the rest of us down with them. Their victory against nuclear energy might technically be one of the biggest catastrophes to ever hit humanity. We have never been hit by something else with so high an opportunity cost in absolute terms. Blind and ignorant fear is no basis for making policy decisions. reply VyseofArcadia 16 hours agorootparentprev> there are plenty of targets for terrorists: packed trains, stealing a big truck, etc. The counterargument is that blowing up a train or stealing a truck usually doesn't poison the local area for generations. Higher risks means some consideration is due. If the stakes are higher but the probability is the same, the expected value is still higher. I for one don't want to live in a hellscape pockmarked by miniature exclusion zones. reply consumer451 15 hours agorootparentIf you look at it strictly from an economic POV, real estate is devalued near sources of fission. (and other power plants as well) The market has spoken on that matter. reply bigbillheck 15 hours agorootparentprevThat sounds like a state actor thinking, not a terrorist. reply T_MacThrowFace 13 hours agorootparentwhat if they are actually one and the same reply AnthonyMouse 14 hours agoparentprev> The one thing I still wonder is whether SMRs could create a security nightmare. People are going to tell you why this isn't a major concern, but the main reason it doesn't matter is that it doesn't matter. There are chemical poisons every bit as nasty as anything in a nuclear reactor which have a half-life of ~forever and aren't that hard to make or obtain. You can't prevent this because they're simple compounds made of common elements. People are afraid of nuclear because of its association with nuclear weapons, not because there is anything uniquely dangerous inside the reactor worse than things in chemical plants all over the place that nobody pays any attention to. reply mlsu 13 hours agorootparentNuclear material has the fantastic property that you can precisely measure the danger. Regardless of the specifics of the material, regardless of the geometry, size, surface coating, chemical composition -- a geiger counter will tell you exactly how dangerous the material is. There is this kind of weird thing that people do where they assume nuclear waste is uniquely dangerous. It isn't! We can just measure it, so the danger is perfectly legible. This is a good thing! The rest of industrial waste is NOT like this. You can see studies of people who live in areas near oil refineries and chemical plants in Texas and Louisiana -- population level cancer and birth defect rates are very high. It's exactly as you say, bog standard \"dangerous chemicals\" are just as dangerous as nuclear radiation is. Only we can't measure them, so I guess what can ya do if a whole city gets birth defects or cancer? The double standard is actually wild. I would prefer to live next to a nuclear plant, thank you very much. I can get a geiger counter and know if I'm being poisoned. reply lazide 13 hours agorootparentprevEh, kinda. The chemical ‘poison’ equivalent to a hot fuel rod (if spread around) is probably roughly equivalent to a couple hundred liters of VX nerve gas or Sarin.. Which, if not near anyone, isn’t very dangerous! It will lose that strength pretty quickly, and the longer lived elements are more like a ton of PCBs, or several tons of organic lead compounds. Not great, don’t live there, but with proper PPE it wouldn’t kill you anytime soon. But if moved to a place where people are at and can’t escape from, or smeared on something worth a lot of money, is a really big problem. If magnified (with say cobalt powder), it could be an even bigger mess. Hot fuel rods are very strictly controlled for a reason. reply AnthonyMouse 13 hours agorootparentIt depends on what kind of damage you're trying to cause. Someone would use different chemicals for trying to kill a lot of people in one place immediately than trying to cause a large economic impact by filling the area with something expensive to eliminate that causes health problems. But the same is true of radioisotopes. The most immediately fatal ones have a short half-life and are only effective at that when concentrated rather than dispersed over a large area. If you have physical access to something with a high monetary value and you want to mess it up, chemical explosives do just fine and can be made from ordinary household substances. > Hot fuel rods are very strictly controlled for a reason. The main legitimate reason is that some existing reactors were purposely designed not to poison the fuel with heavier Plutonium isotopes in a way that makes it unsuitable for producing weapons, because the countries wanted to use them for that, but that can be solved in newer designs by doing the opposite. The main political reason is media fear mongering and decades of Russian propaganda designed to keep people buying Russian gas. Which you can kill as many people with as you can with a fuel rod, but half the population has it piped into their homes. reply areoform 16 hours agoparentprevGetting to the material is mechanically difficult. It's also the type of problem that neatly solves itself. A fuel rod toting terrorist is a dead terrorist. > The acceptance criteria in DSRS Section 12.3–12.4 state that accessible portions of the facility that are capable of having radiation levels greater than 1 Gy/h (100 rads/h) should be shielded and clearly marked with a sign stating that potentially lethal radiation fields are possible. DCA Part 2, Tier 2, Revision 3, identifies a number of areas (e.g., resin demineralizers, filters, SRSTs) that may contain quantities of radioactive material resulting in radiation dose rates exceeding 100 rads/h (1 Gy/h). Now what I'm about to say changes with the fuel being used, but from LLNL's report, > Even for fuel that has cooled for 15 years, a lethal dose (LD50) of 450 rem can be received at 1m from the fuel assembly after several minutes. https://www.osti.gov/servlets/purl/10137382 Good luck trying to carry that stuff out. They'll be the walking dead in a few minutes. They'll be unable to walk and will be just dead in ~100 minutes. Give or take 10 to 20 minutes depending on how they're holding the item etc. https://en.wikipedia.org/wiki/Acute_radiation_syndrome#Dose_... And no, they can't just \"put it in [a] lead [box]\" to run away with it, because then they'll die to bremsstrahlung radiation or braking radiation. It'll take longer, but the end result will be the same. If it's fuel that hasn't gone into the reactor yet/hasn't undergone neutron activation, then it's not radioactive enough to be a concern unless consumed. Terrorists are rare and they're usually not that capable. It's easier for them to buy a gun or a lorry and mow down a crowd than it is to plan a fiendishly complicated radiation heist. reply giva 16 hours agorootparentI just want to point out that a terrorist that drives a plane into a skyscraper is a dead terrorist too. You need one terrorist to extract the material and put some quantities in a somewhat shielded container before being incapacitated, and then others to spread it on the subway. reply areoform 16 hours agorootparentRadiation isn't magic. Several feet/meters of water are used as shielding for the fuel for a good reason. If you put an activated fuel rod inside of a lead lunch box, you will still die. You will now die from a lethal dose of bremsstrahlung. https://en.wikipedia.org/wiki/Bremsstrahlung If you interact with this material as an amateur without processes, robotics and safety, you will die. Period. You will also suffer from intense nausea and be unable to move before you die. Sharks with lasers are more realistic than this concern. The more plausible scenario is a \"dirty bomb\" that spreads depleted uranium particles everywhere and gives people cancer long-term. But there's an element of impracticality to this and there's a reason why it hasn't happened yet. reply ViewTrick1002 15 hours agorootparentEven tiny amounts of radioactive waste are dangerous over time. https://en.wikipedia.org/wiki/Goi%C3%A2nia_accident https://en.wikipedia.org/wiki/Kramatorsk_radiological_accide... reply lazide 14 hours agorootparentprevThe issue they’re pointing out is that no one seriously considered terrorists flying airplanes into skyscrapers either, for similar reasons. Until they did it. After all, it’s very impractical getting a bunch of random terrorists training in flying well enough to do this without being caught. Let alone co-ordinating everything, getting through multiple layers of airport security, etc. And everyone ‘successful’ is going to die too. Who would do that? Historically, terrorists would just hijack a plane for attention and/or money and then let everyone go except for those stupid enough to fight them. Which is why the reaction onboard to the terrorists taking the planes was so muted, until a passenger on one of the planes heard the news of the first impact, and they realized they were in a very different situation than a hijacking for money. Not considering dirty bomb type issues a threat would likely be a mistake. Having a bunch of disposable idiots throw hot fuel rods into a water tanker truck, then driving to manhattan, seeding the water with cobalt-59, and letting the mess ‘sort itself out’ (or blowing the the truck up) certainly wouldn’t be hard compared to 9/11 in these scenarios, or likely kill more than a handful of terrorists. Depending on how well secured the fuel was anyway. It would create an absolutely terrific mess though. The only hard part here would be getting the hot fuel (and the idiots), everything else would be ‘COTS’. Which, like 9/11, would not be seen as ‘expensive’. This isn’t sharks with lasers territory. reply fbdab103 2 hours agorootparent>The issue they’re pointing out is that no one seriously considered terrorists flying airplanes into skyscrapers either, for similar reasons. Until they did it. Was literally an episode of the Lone Gunmen. https://en.wikipedia.org/wiki/The_Lone_Gunmen_(TV_series) In the pilot episode, which aired March 4, 2001 (exactly six months and one week prior to the September 11 attacks[5]), rogue members of the U.S. government remotely hijack an airliner flying to Boston, planning to crash it into the World Trade Center, and let anti-American terrorist groups take credit, to gain support for a profitable new war following the Cold War. The heroes ultimately override the controls, foiling the plot. reply pas 11 hours agorootparentprevSMRs don't mean they would be scattered around carelessly. They still need the whole steam boiler turbine toybox. And with security requirements they will most likely be put on a site like current NPPs. reply lazide 11 hours agorootparentA big selling point in the SMR space is they can be placed almost anywhere. It’s literally on the ‘brochure’. reply pas 10 hours agorootparentyes, and it might make sense, and we will see ... but. the real economics low-hanging fruit is to build those regular NPPs in increments, basically cheaper AND faster (which also means even cheaper). get real economies of scale, apply all of the last century's industrialization benefits on nuclear energy. (because current cost overruns are insane and mostly because of too-slow AND super-duper-artisanal (which means even slower) plant construction. reply LorenPechtel 15 hours agorootparentprevBut even if the guy will die to steal it that doesn't mean he will live long enough to accomplish anything. And note that he's trying to tote around a gamma source--that's something that can be detected from some distance away. As soon as said terrorist gets it out from behind whatever shields it normally lives behind the alarms start blaring. I remember reading about one, anti-nuke guy kept showing up at meetings at a nuclear facility. Then one day the alarms go off--traced to the old radium-dial watch he was wearing. Rather silenced him. reply hcknwscommenter 15 hours agorootparentprevthe words \"some\" and \"somewhat\" are doing an impractical degree of heavy lifting. Pre-activation material is just not very radioactive and is thus not a terrorist risk. Post-activation material is basically impossible for a non-nation state actor to store and transport. If you just want to dirty up an area by, e.g., spreading radioactive material on a subway, just buy a lot of smoke detectors or do a midnight raid on a hospital or factory radiation source (not advice!). The point is, these are not going to be practical targets. reply jillesvangurp 2 hours agoparentprevIt's a good thing to call out. The short version of this is that nuclear proponents are a bit hand wavy on this topic and that there is no good plan for this; nor any economical modeling worth talking about. But there's no such thing as nuclear power deployed on an insecure location. At best you have nuclear plants on very remote or hard to reach sites where the security is a bit minimal. But essentially all existing nuclear is the subject of intense and costly security. Under current non proliferation treaties, that's neither optional nor likely to change any time soon. Logically that means co-locating small nuclear plants on larger sites is the most cost effective way to secure them. That's something people have speculated about. The most suitable locations for that would be existing nuclear plants and decommissioned coal plants. Of course if you do that, you need to compare the efficiency of having lots of small reactors in one place and compare it to having 1 big reactor in the same place. Big reactors tend to be more efficient. The notion that you can just fedex these things to wherever, plop them down and then check back in 30 years is a bit naive. That's not how it works in the real world. People deploying these things are going to be on the spot for securing them at a non zero cost. 24x7 for life time of the reactor and the waste. Total cost of ownership is not limited to production and shipping (also needs security) but also has to include inspections, day to day operations, site security, waste disposal, handling, and security. (though that is commonly excluded and off-loaded on future generations of course), etc. If you exclude all those things the cost per kwh looks more rosy than it is. The real cost is something most nuclear proponents avoid talking about simply because it generally looks pretty bad. But somebody still has to pay for that. reply WatchDog 4 hours agoparentprevThere are already hundreds(thousands?) of geographically distributed nuclear weapon silos. I would be more worried about weapons than I would be about nuclear waste. reply actionfromafar 46 minutes agorootparentPut an SMR next to every silo, then. The security is partly paid for already, I guess? reply naasking 17 hours agoparentprevSwitch to a thorium fuel cycle so virtually all of the fissionable material is spent, rather than 90+% of it remaining in the waste. reply LorenPechtel 15 hours agorootparentJust use breeder reactors. Somebody repurposing the plutonium? No. Plutonium is not plutonium. For a bomb you need Pu-239. The more Pu-240 in your material the harder it is to slam the pieces together fast enough to avoid a premature chain reaction from robbing your bomb of most of it's power. For both bomb-making and fuel breeding the desired reaction is U-238 + n => U-239, decays to Np-239, decays to Pu-239. There is also the reaction Pu-239 + n => Pu-240. The power guys don't care, Pu-240 works fine. The bomb guys do care. Thus the power guys leave the fuel in the reactor until too much neutron poison builds up. The bomb guys leave the fuel in the reactor only a short time. reply perlgeek 16 hours agorootparentprevSomehow, Thorium has been the future of nuclear reactors about as long as fusion has been. It's almost a running gag. (And yes, I know that there are concrete plans for commercial Thorium fission reactors, in a way that aren't for fusion, but still no fully-operating, commercial reactor, afaict). reply AnthonyMouse 14 hours agorootparentThat's not really saying much given the number of new nuclear reactors of any fuel source the west has built in the current century. You have to build something in order to build something that runs on Thorium. reply rich_sasha 13 hours agoparentprevYou can still put them on a small few sites, with heavy security. You will still reap the yet-to-be-demonstrated benefit of cheaper per-MW power (personally I'm hopeful). reply chasil 16 hours agoparentprevIdeally, the reactor is designed in such a way that a catastrophic failure is not possible. Something along the lines of the salt plug in a molten salt reactor, that melts when cooling fails and dumps the liquid into a boron bath, halting fission. reply jszymborski 15 hours agoparentprevSMRs feel like a much worse target from what I understand. I'm no expert, but my understanding 1) SMRs are typically designed to be walk-away safe. Their smaller capacity means its harder for them to go critical in a spectacular way. [0] 2) Distributed targets are hard to respond to, but they're also hard to hit in an impactful way. Instead of having to hit one giant nuclear plant which supplies an entire regions energy, you have to hit many much smaller targets which supply much less energy. [0] https://www.forbes.com/sites/jamesconca/2018/01/24/can-we-ma... reply Retric 15 hours agorootparentGetting it critical is a long way from the worst thing you can do with access to one or more SMR. However, nobody is going to be using these things individually. You still need a steam turbine, and cooling tower etc. So the general assumption is people will be using 10+ of these things and the added complexity would be offset by economies of scale. reply goodpoint 48 minutes agoparentprev> How do you protect infrastructure with many SMRs? You don't. In times of increasing polarization and risks of social unrest we need safe and decentralized energy sources like PV. reply credit_guy 16 hours agoparentprev> Every nuclear power plant is a fantastic target for terrorists. It is not. Let's say you are a terrorist, and you and your team have taken hold of a nuclear power plant. Then what? It's a problem for the world, but not different from you taking over a hotel or a city hall. You might threaten or even kill hostages, but there's no way you can create a significant nuclear threat. Let's say you force an operator, at gunpoint, to remove some rods from the reactor, or otherwise you get possession of some nuclear material. So what? How are you going to get away? You will be surrounded by hundreds or thousands of SWAT officers. What is the scenario where it will make a difference if they take over the nuclear power plant vs a different building? reply andrewflnr 14 hours agorootparentTerrorists don't need to take over, they just need to plant a big enough bomb or hit it with a big enough plane that it causes a radioactive disaster. Not only do they not necessarily care about their survival, they might actively crave death. reply credit_guy 14 hours agorootparent> In the United States, the design and thickness of the containment and the missile shield are governed by federal regulations (10 CFR 50.55a), and must be strong enough to withstand the impact of a fully loaded passenger airliner without rupture. [1] https://en.wikipedia.org/wiki/Containment_building reply tuukkah 14 hours agorootparentWill this apply to SMRs? reply credit_guy 9 hours agorootparentVery likely. In the US, all civilian nuclear applications are under the oversight of the NRC, and the NRC is a very conservative organization. I don't see them relaxing such any regulation for SMR's, let alone a regulation with such 9/11 vibes. reply naasking 16 hours agorootparentprevThe point is that they could potentially abscond with nuclear material before being surrounded, obviously. reply hunter-gatherer 16 hours agorootparentObviously. Good luck with that. reply inglor_cz 16 hours agorootparentprevHandling nuclear material straight out of the reactor is a good way to kill yourself very painfully. reply naasking 15 hours agorootparentSure, but if we're talking about terrorists that a documented history of being willing to blow themselves up, I don't see why they wouldn't do this too. To avoid the painful death, blow yourself up after absconding but before radiation poisoning really takes its toll. reply vlowther 15 hours agorootparentTerrorists willing to blow themselves up at a time of their choosing is the key there. With nuclear waste hot enough to be a menace, the window between getting the material and being painfully incapacitated and/or dead by the material is pretty damn small. Especially since nuclear plants tend to have things like lots of checkpoints and security. reply throwway120385 15 hours agorootparentprevWe're talking about people with EG civil engineering degrees masterminding these plots. The middle-eastern terrorists of yesteryear are not necessarily uneducated. Osama Bin Laden was educated as an engineer in the US and likely would have thought through the endgame and implications of such a plot well before executing it. These people are not stupid and would likely know what they are getting in to. I'd be more worried about a single unhinged person, but that's why we have security guards, and the problem would likely solve itself in short order in that case. reply inglor_cz 14 hours agorootparentprevIf you receive around 1 Gy in five minutes, you won't abscond very far. And you would likely receive a lot more. Not to mention that you would leave a trail of radioactivity from the reactor to your lair/hiding place. But it is well possible that some people could still try. The Four Lions, only in the real world. reply lazide 9 hours agorootparentI’ve personally seen someone get 7 Gy and be relatively functional (in a localized area) - no worse than he went into the OR anyway, to get the stent put in his brain. And he almost certainly lived another decade or so. The other stuff sure, but it takes a truly mind boggling amount of radiation to incapacitate someone (near) instantly. A lot more than that. 100-150 Gy? The only concrete number I found looking around for ‘instant’ was 1000 Gy to likely kill someone within an hour. reply inglor_cz 1 hour agorootparentLocalized exposure is a different story, some body parts can take a lot of radiation. IDK if someone, having just burglarized a live reactor, can keep their exposure localized in a favorable place. I think incapacitation is what matters. The human body will fight death for some time, but it won't be able to running around with a heavy backpack of fission material. Various tissues will scream bloody hell almost immediately from being torn apart by the radiation. reply LorenPechtel 15 hours agorootparentprevWould it even be painful? Wouldn't the stuff from the reactor just kill you promptly? reply inglor_cz 14 hours agorootparentIDK how many real-world casuistics we actually have, but Wikipedia says that absorbed dose over 30 Gy results in severe headache, severe fever, nausea, vomiting and seizures, with death taking up to two days. reply gadders 16 hours agoparentprevHire more of these people? https://www.gov.uk/government/organisations/civil-nuclear-co... \"The Civil Nuclear Constabulary (CNC) is the armed police force in charge of protecting civil nuclear sites and nuclear materials in England and Scotland.\" reply ElectronCharge 15 hours agoparentprevI like the idea of siting them underground, which completely addresses the aircraft strike vulnerability. Physical security is also easy when they're underground. Thorcon was pursuing this, but has switched to a ship-based approach. Searching today, I found a new (to me at least) company that's working on a truly small reactor, 3-15 MW electricity, and is planning on underground siting: https://www.usnc.com/mmr/ It also supplies 10-45 MW of thermal, so great for poleward climes. Fission will be a critical technology going forward, until fusion generation can compete on power to size/weight ratio...and cost. And until aneutronic fusion power is practical, there's no large benefit to fusion regarding waste. reply BlueTemplar 2 hours agoparentprevI am more concerned about when/if we have millions of small nuclear reactors around, some of them will end up abandoned/lost. Then many years later, they will start to leak (think rain getting in). reply samus 1 hour agorootparentUnlikely, that would imply a major breakdown of civilization. They are all connected to power lines, else they would not have been installed in the first place. reply mytailorisrich 16 hours agoparentprevI expect SMRs would be protected as standard nuclear plants are. Therefore, these are not easy targets and we've seen that they are very easy targets everywhere that will have the desired effect. reply mikece 16 hours agoprevOne of the benefits often cited (and it is again in this article) is that SMRs allow for electrical generation to be located extremely close to where it will be consumed, avoiding line losses on the transmission grid. How significant are the line losses for a plant that is 100 miles from where the electricity is being consumed? reply apendleton 14 hours agoparentAt least in a US context, the proximity advantage is often less about line losses than about the political realities of building transmission. We have way less transmission in the US than we're expected to need as we electrify, and the right-of-way acquisition, permitting, etc., to build new high-voltage transmission are really daunting. NIMBYs tend to hate it (somewhat understandably -- it's ugly), and every jurisdiction along the path of the proposed new line typically has its own permitting process which effectively grants it veto power (unlike gas pipelines, interestingly, where the federal government has permitting authority that supersedes local authority). A big potential advantage of these kinds of solutions is that you can just site the plant such that you can avoid building the transmission at all. Of course, nuclear obviously has its own NIMBY concerns, permitting issues, etc., so it's not a panacea. reply belorn 0 minutes agorootparentI find that NIMBY concerns can be very much connected to cost and profits. Looking at wind farms which has both a lot of noise and visible footprint, people becomes much more acceptable to those costs if the profits from the farm ends up with people who live there. An other comment here talked about how almost 80% of the energy bill in California are from fixed costs from primarily transmission. That is a lot of space to create economical incentive for people to have a plant located very near where they live. Similar, politicians are much more willing to be permissive with permits when there is an economical benefit for the region, as energy costs are quite significant bit of a regional budget. reply tnorgaard 15 hours agoparentprevViking Link, the 765 km HVDC (VSC-based) link rated at 1400 MW between England and Denmark has a rated loss at 3.7% [0]. [0] https://www.viking-link.com/auction-faqs reply LorenPechtel 14 hours agoparentprevCan't address it overall but some while back I did some calculation on shipping power around the world from wherever the sun was shining. I took the real-world numbers for the biggest transmission lines--and found the only question being how many nines on the loss percentage. You are fighting two separate forces. Some of the energy is lost to heat in the wires, this is minimized by making the voltage as high as possible--but the higher the voltage the more corona loss there will be. reply ahmedfromtunis 15 hours agoparentprevI did a paper on this back at engineering school around 10 years ago. I forgot most of the details, but iirc the losses can be up to 33% in certain conditions (mostly cold weather). reply mikece 14 hours agorootparentWorse in cold weather? I thought conductors get more efficient the colder they get (or is cold relative to humans insignificant to high voltage lines)? reply ahmedfromtunis 14 hours agorootparentI forgot the details but I remember it's something about humidity in the air. I recall however that I read about a blue halo forming around some cables in snowy nights as electricity charges the air surrounding said cables. This phenomenon was a huge source of losses in transport cables. reply V__ 17 hours agoprevI'm really rooting for SMRs, but are there realistic projections on what the energy production costs will be? reply cwmma 16 hours agoparentI think one of the issues is just that for thermal power plants in general, there are big economies of scale, one big boiler/turbine/etc is better then multiple small ones) reply camel_gopher 16 hours agoparentprevLatest on NuScale was 189/MW ($0.189/kW). More expensive than solar but here in CA 20 cents a kilowatt would be welcome reply bryanlarsen 16 hours agorootparent77% of the cost of a kWh in California are fixed costs. So if production cost 19 cents, then your cost would be 19 cents + 77% of your current price + a few more cents for the profit on the production costs. IOW, it would increase your electricity bills. reply Twirrim 16 hours agorootparentWhile also getting you more reliable and consistent power generation than anything you'd get from Solar or Wind. reply bryanlarsen 16 hours agorootparentThe unreliability of the California grid is almost entirely due to transmission, not generation. And nuclear is not a reliability panacea. Nuclear only has a 90% uptime, whereas the goal for power delivery is 99.99%. reply AnthonyMouse 14 hours agorootparent> Nuclear only has a 90% uptime, whereas the goal for power delivery is 99.99%. Nuclear in the US has a more than 90% uptime, and the downtime is scheduled maintenance, which is done at times chosen specifically because the load on the grid is known to be lower then, rather than being chosen by nature at random. reply exe34 15 hours agorootparentprev> The unreliability of the California grid is almost entirely due to transmission, not generation. I think describing night time as a problem of transmission is technically correct, but probably not the issue here. reply imtringued 14 hours agorootparentSince when does the wind stop blowing at night? You can play dishonest games and win dishonest responses. reply BenjiWiebe 14 hours agorootparentNot arguing the earlier points or saying this is relevant - but - here in Kansas, kinda known for it's wind, it usually dies down around sunset. We do get wind at night sometimes but we get a lot more during the day. reply lazide 5 hours agorootparentprevDepends on local geography, but in the mountains - unless there is a storm - it’s typically 10am’ish to late afternoon. Since most windows are caused by convection in some form. reply barryrandall 13 hours agoprev1 reactor per day would double the number of reactors in the world in less than 18 months. Can we generate enough people qualified to run them without compromising safety? reply samus 1 hour agoparentThat's one large bottleneck resolved, perhaps one of the most limiting. But it doesn't mean they will actually crank up the assembly line that far to then sit on a large inventory, which would create its own issues. reply EasyMark 7 hours agoparentprevthat was welding together just one part of a reactor, it would involve a lot more than that one part. You'd have to get all the various steps done in parallel and all stages less than 24 hours to get a \"1 a day\" pipeline going. I think that is wishful thinking by anyone's standards. reply temp0826 15 hours agoprev>> potentially taking high-cost welding processes out of the equation I wonder what kind of cost they're talking if a high-powered electron gun and vacuum chamber are cheaper reply ajcp 14 hours agoparentThe vacuum chamber *is* part of the electron gun; it's essentially on the end of the \"barrel\" to create a vacuum just at the spot of the weld. reply ActionHank 14 hours agoparentprev12 months of labour I'm assuming reply ergocoder 4 hours agoprevRollsroyce stock has been on the run. Love this news. I hope this 10x the stock. reply boxed 15 hours agoprevSeems like this tech could be very relevant to SpaceX too... reply WatchDog 4 hours agoparentI doubt it, the advantage of this technology is being able to weld thick heavy material. In aerospace all the material is as thin and light as possible. reply peter_d_sherman 15 hours agoprev>\"Using conventional techniques, this can take over a year, but Sheffield Forgemasters have reduced this to under a day using what is called Local Electron-Beam Welding (LEBW) to complete four thick, nuclear-grade welds. LEBW is a revolutionary method to weld two pieces of metal together using a high-energy density fusion process centered on a high-powered electron gun operating in a local vacuum. This melts and fuses components to one another and allows for an efficiency of 95%, deep penetration, and a high depth-to-width ratio.\" Never heard about this before... let's learn more about it!: https://en.wikipedia.org/wiki/Electron-beam_welding Apparently you first need a vaccuum chamber to prevent energy dissipation... that sounds about right... But then we find this interesting quote: >\"Magnetic lenses can shape the beam into a narrow cone and focus to a small diameter. This allows for a high power density on the surface to be welded.\" Now that's super-cool! (Or, well, super-hot -- as the case may be! :-) ) Then there's this: >\"The beam can then be redirected to meet the needs of applications beyond welding such as surface hardening, annealing, exact beam positioning, imaging, and engraving. Resolution of 0.1 mm can be achieved.\" Now that is truly useful! It seems like this would be an excellent technology for creating future space stations, spaceships and other deep-space vehicles, of one form or another... Anyway, great article! reply LorenPechtel 14 hours agoparentAnd it sounds like something that would be practical to use in space. Normally you either burn fuel (and lifting said fuel is very pricey!) or you use an electric arc--but your arc won't work in vacuum. reply datadrivenangel 13 hours agorootparentThe Soviet Union and USA have used electron beam and laser welding in space [1]. 1. https://ntrs.nasa.gov/api/citations/20200002259/downloads/20... reply codersfocus 11 hours agoparentprevHow high powered? This sounds like a CRT display. reply metal_am 3 hours agorootparentPretty much! Also like an electron microscope. TWI says they can do 100kW reply m3kw9 16 hours agoprevCan someone help visualize what is being welded? reply waldothedog 15 hours agoparentSomeone else might have more detail, but I believe these are “basically” massive metal tubes that are created by building up welds, one on top of the other, to achieve extremely high integrity structures for containing nuclear fission. reply CamperBob2 16 hours agoprevTalk about ambiguous headlines. I thought this was going to be about using nuclear energy for welding. reply SubiculumCode 15 hours agoprev [–] Security against malevolent actors? Having reactors everywhere seems like an invitation for disaster... edit: i am pro clean power and i believe nuclear waste can be stored safely; I just don't want to put town's at risk via under secured nuclear facilities. reply EasyMark 7 hours agoparentWhy wouldn't you locate a bunch in one location and make what is essentially a traditional plant? I don't think anyone with any common sense is talking about dropping 1 for every hundred houses or so in suburban chicago. There won't be any kids being like \"yo dawg lets go climb that SMR after school\" reply XzAeRosho 15 hours agoparentprev [–] One could argue that large reactor plants make for easier targets since the surface area of attack vectors is much larger. reply SubiculumCode 12 hours agorootparent [–] One might also argue that 1000 units versus 1 (guessing) would be harder to keep track of. reply topspin 4 hours agorootparent [–] There are rarely 1 conventional reactors on a site, although there are a few examples to the contrary. The largest sites have 6-8 reactors. Typically there are 2-4, which facilitates load management during outages. The good news from the security prospective is that SMRs will likely be similarly concentrated, just with greater numbers of reactors. They are highly unlikely to be sprinkled about willy nilly, despite the thinking of some. Siting is a major problem with all power generation and particularly with nuclear: getting a site approved is a major accomplishment involving no end of costly wheeling and dealing over many years. Although there are use cases, often championed as killer applications for SMR, such as very remote, undeveloped locations, I expect that for every 1 such isolated reactor actually deployed, 100 will be lined up in rows and columns at major reactor sites. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sheffield Forgemasters has introduced a new welding technique known as Local Electron-Beam Welding (LEBW) capable of welding a complete nuclear reactor vessel in under 24 hours, cutting down construction time and expenses for Small Modular Reactors (SMRs).",
      "This innovation has the potential to transform the nuclear power sector by enhancing the efficiency, standardization, and mass production of modular reactors.",
      "The UK government is considering a resurgence in nuclear energy, aiming for new plants and modular reactors, with this technology poised to expedite their implementation."
    ],
    "commentSummary": [
      "Small Modular Reactor (SMR) technology has enabled a breakthrough in nuclear welding, particularly electron beam welding, allowing for efficient and deep penetration welding of large workpieces.",
      "The article underlines the challenges and complexities of welding in the nuclear sector and discusses the advantages of electron beam welding over conventional techniques.",
      "Security concerns regarding SMRs and potential terrorist threats to nuclear facilities are addressed, stressing the significance of strict regulations and security protocols to safeguard these plants."
    ],
    "points": 202,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1708533096
  },
  {
    "id": 39458363,
    "title": "Unlocking Neural Network Improvements with Diffusion Models",
    "originLink": "https://arxiv.org/abs/2402.13144",
    "originBody": "Computer Science > Machine Learning arXiv:2402.13144 (cs) [Submitted on 20 Feb 2024] Title:Neural Network Diffusion Authors:Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You Download PDF HTML (experimental) Abstract:Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models. Comments: We introduce a novel approach for parameter generation, named neural network diffusion (\\textbf{p-diff}, p stands for parameter), which employs a standard latent diffusion model to synthesize a new set of parameters Subjects: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2402.13144 [cs.LG](or arXiv:2402.13144v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2402.13144 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kai Wang [view email] [v1] Tue, 20 Feb 2024 16:59:03 UTC (6,066 KB) Full-text links: Access Paper: Download PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2402 Change to browse by: cs cs.CV References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39458363",
    "commentBody": "Neural Network Diffusion (arxiv.org)190 points by vagabund 14 hours agohidepastfavorite71 comments vessenes 10 hours agoI wasn't sure if this paper was parody on reading the abstract. It's not parody. Two things stand out to me: first is the idea of distilling these networks down into a smaller latent space, and then mucking around with that. That's interesting, and cross-sections a bunch of interesting topics like interpretability, compression, training, over- and under-.. The second is that they show the diffusion models don't just converge on similar parameters as the ones they train against/diffuse into, and that's also interesting. I confess I'm not sure what I'd do with this in the random grab bag of Deep Learning knowledge I have, but I think it's pretty fascinating. I might like to see a trained latent encoder that works well on a bunch of different neural networks; maybe that thing would be a good tool for interpreting / inspecting. reply daxfohl 6 hours agoparentSeems like it could be useful for resizing the networks, no? Start with ChatGPT 4 then release an open version of it with much fewer parameters. Or maybe some metaparameter that mucks with the sizes during training produces better results. Start large to get a baseline, then reduce size to increase coherence and learning speed, then scale up again once that is maxed out. reply SubiculumCode 6 hours agoparentprevPerhaps doing this to generate 10 similar but different versions of a model can then be fed into mixture of experts? reply vessenes 2 hours agorootparentOoh that’s a good idea! Although mistral seems to have been seeded with identical copies of mistral, so maybe it doesn’t buy you much? Sounds worth trying though! reply gwern 6 hours agoprevThis doesn't seem all that impressive when you compare it to earlier work like 'g.pt' https://arxiv.org/abs/2209.12892 Peebles et al 2022. They cite it in passing, but do no comparison or discussion, and to my eyes, g.pt is a lot more interesting (for example, you can prompt it for a variety of network properties like low vs high score, whereas this just generates unconditionally) and more thoroughly evaluated. The autoencoder here doesn't seem like it adds much. reply justanotherjoe 1 hour agoprevfuck. I have an idea just like this one. I guess it's true that ideas are a dime a dozen. Diffusions bear a remarkable similarity to backpropagation to me. I thought that it could be used in place of it for some parts of a model. Furthermore, I posit that resnet especially in transformers allows the model into a more exploratory behavior that is really powerful, and is a necessary component of the power of transformers. Transformers is just such a great architecture the more i think about it. It's doing so many things so right. Although this is not really related to the topic. reply crotchfire 47 minutes agoparentActually it is related. Transformers are just networks that learn to program the weights of other networks [1]. In the successful cases the programmed network has been quite primitive -- merely a key-value store -- in order to ensure that you can backpropagate errors from the programmed network's outputs all the way to the programmer network's inputs. The present work extends this idea to a different kind of programmed network: a convolutional image-processing network. There are many more breakthroughs to be achieved along this line of research -- it is a rich vein to mine. I believe our best shot at getting neural networks to do discrete math and symbolic logic, and to write nontrivial computer programs, will result from this line of research. [1] https://arxiv.org/abs/2102.11174 reply vagabund 14 hours agoprevAuthor thread: https://twitter.com/liuzhuang1234/status/1760195922502312197 reply squigz 11 hours agoparentIs there any sites for viewing Twitter threads without signing up? reply f_devd 11 hours agorootparenthttps://nitter.esmailelbob.xyz/liuzhuang1234/status/17601959... (bit of trial and error from https://github.com/zedeus/nitter/wiki/Instances) reply falcor84 11 hours agoprevSeems like we're getting very close to recursive self-improvement [0]. [0] https://www.lesswrong.com/tag/recursive-self-improvement reply astrange 11 hours agoparentNo, this is an example of an existing technique called hypernetworks. It's not \"recursive self improvement\", which is just a belief that magic is real and you can wish an AI into existence. In particular, this one needs too much training data, and you can't define \"improvement\" without knowing what to improve to. reply FeepingCreature 2 hours agorootparentAll current LLMs are based on the premise that magic is real and you can wish intelligence into existence; it's called \"scaling laws\" and \"emergent capabilities\". Recursive self-improvement isn't \"maybe magic is real\", it's \"maybe the magic we already know about stays magical as we cast our spells with more mana.\" reply z7 1 minute agorootparentDoesn't this line of reasoning imply that human intelligence is magical, i.e. is not the result of scaling/emergence? reply killerstorm 9 hours agorootparentprev> which is just a belief that magic is real Is there a law of thermodynamics which prevents AI from writing code which would train a better AI? Never learned that one in school. And FYI here's OpenAI plan to align superintelligence: \"Our goal is to build a roughly human-level automated alignment researcher. We can then use vast amounts of compute to scale our efforts, and iteratively align superintelligence.\" I guess people working there believe in magic. > and you can wish an AI into existence. Eh? People believe that self-improvement might happen when AI is around human-level. reply astrange 8 hours agorootparent> Is there a law of thermodynamics which prevents AI from writing code which would train a better AI? You need to apply Wittgenstein here. This appears to be true because you haven't defined \"better\". If you define it, it'll become obvious that this is either false or true, but if it is true it'll be obvious in a way that doesn't make it sound interesting anymore. (For one thing our current \"AI\" don't come from \"writing code\", they just come from training bigger models on the same data. For another, making changes to code doesn't make it exponentially better, and instead breaks it if you're not careful.) > I guess people working there believe in magic. Yes, OpenAI was literally founded by a computer worshipping religious cult. > People believe that self-improvement might happen when AI is around human-level. Humans don't have a \"recursive self-improvement\" ability. Also not obvious that an AI that was both \"aligned\" and \"capable of recursive self-improvement\" would choose to do it; if you're an AI and you're making a new improved AI, how do you know it's aligned? It sounds unsafe. reply Nevermark 3 hours agorootparent> Humans don't have a \"recursive self-improvement\" ability They do. Humans can learn from new information, but also by iteratively distilling existing information or continuously optimizing performance on an existing task. Mathematics is a pure instance of this, in the sense that all the patterns for conjectures and proven theorems are available to any entity to explore, no connection to the world needed. But any information being analyzed for underlying patterns, or task being optimized for better performance, creates a recursive learning driver. Finally, any time two or more humans compete at anything, they drive each other to learn and perform better. Models can do that too. reply alanbernstein 7 hours agorootparentprev> they just come from training bigger models on the same data Are you arguing that all AI models are using the same network structure? This is only true in the most narrow sense, looking at models that are strictly improvements over previous generation models. It ignores the entire field of research that works by developing new models with new structures, or combining ideas from multiple previous works. reply astrange 6 hours agorootparentI sure am ignoring that, because the bitter lesson of AI is usually applicable and implies that all such research will be replaced by larger generic transformer networks as time goes on. The exception is when you care about efficiency (in training or inference costs) but at the limit or if you care about \"better\" then you don't. reply recursivecaveat 2 hours agorootparentThis is kindof an odd statement because the transformer is not the most generic neural net. It's the result of many levels of improvements in architecture over older designs. The bitter lesson is methods that can scale well with compute win (alpha/beta beats heuristics alone, neural network beats alpha/beta), not that the most obvious and generic approach eventually wins. Given the context-length problems with transformers I think it's fair to say they have scaling problems. reply killerstorm 1 hour agorootparentprevThere's a principle more powerful than the bitter lesson: GIGO. Training to predict internet dump can only give you so much. There's a paper called something like \"learning from textbooks\" where they show that a small model trained on high-quality no-nonsense dataset can beat a much bigger model at a task like Python coding. reply Tenobrus 8 hours agorootparentprevit is very clear to me that humans do in fact have a recursive self-improvement ability, and i'm confused why you think otherwise reply astrange 8 hours agorootparentI think people can read books (self improvement) and have children (recursive), but neither of those are both. reply lucubratory 7 hours agorootparentWhy do you think that the human population is more intelligent, knowledgeable, and achieves greater technological feats as time goes on? It's because of recursive self-improvement, we are raised and educated into being better in a quite general sense, which includes being better at raising and educating; nearly every generation this cycle repeats and has for all of human history, at least since we acquired language. We also build machines that help us to make better machines, and then we use those better machines to make even better machines, another example of recursive self-improvement. reply rralian 7 hours agorootparentprevNew generations build onto the scientific knowledge of previous generations. It may not be fast but that sounds like recursive improvement to me. It seems reasonable for AI to accelerate this process. reply astrange 6 hours agorootparentI think saying all of society is doing it is plausible, but not the same thing as a single human or AI doing it. Though… still don't think it's true. Isn't \"society is self improving\" what they call Whig history? reply killerstorm 1 hour agorootparentAI might have multiple instances within a single computing environment, so it's more like a population than a single individual. I.e. \"You can only use the memory which you currently use\" would be a weird artificial constraint not relevant in practice. reply spacecadet 7 hours agorootparentprevA very small percentage maybe. I think I agree with the notion that most people bias toward thinking they are improving while actually self-sabotaging. reply killerstorm 1 hour agorootparentprev> This appears to be true because you haven't defined \"better\". Better intelligence can be defined quite easily: something which is better at (1) modeling the world; (2) optimizing (i.e. solving problems). But if that would be too general we can assume that general reasoning capability would be a good proxy for that. And \"better at reasoning\" is rather easy to define. Beyond general reasoning better AI might have access to wider range of specialized modeling tools, e.g. chemical, mechanical, biological modeling, etc. > if it is true it'll be obvious in a way that doesn't make it sound interesting anymore. Not sure what you mean. AI which is better at reasoning is definitely interesting, but also scary. > they just come from training bigger models on the same data. I don't think so. OpenAI refuses to tell us how they made GPT-4. I think a big part of it was preparing better, cleaner data sets. Google tells us that specifically improved Gemini's reasoning using specialized reasoning datasets. More specialized AI like AlphaGeometry use synthetic datasets. > Yes, OpenAI was literally founded by a computer worshipping religious cult. Practice is the sole criterion for testing the truth. If their beliefs led them to better practice then they are closer to truth than whatever shit you believe in. Also I see no evidence of OpenAI \"worshipping\" anything religion-like. Many people working there are just excited about possibilities. > Humans don't have a \"recursive self-improvement\" ability. Human recursive self-improvement is very slow because we cannot modify our brains' at will. Also spawning more humans takes time. And yet humans made huge amount of progress in the last 3000 years or so. Imagine that instead of making a new adult human in 20 years you could make one in 1 minute with full control over neural structures, connections to external tools via neural links, precisely controlled knowledge & skills, etc. reply stale2002 2 hours agorootparentprev> If you define it, it'll become obvious that this is either false or true Ok. So then I guess it isn't \"just a belief that magic\". Instead, it is so true and possible that you think it is actually obvious! I'm glad you got convinced in a singular post that recursive self improvement, in the obvious way, is so true and real that it is obviously true and not magic. reply always2slow 7 hours agorootparentprev>> I guess people working there believe in magic. >Yes, OpenAI was literally founded by a computer worshipping religious cult. What cult is this? reply astrange 6 hours agorootparentHPMOR readers who live in group home polycules in Berkeley who think they need to invent a good computer god to stop the evil computer god. reply killerstorm 1 hour agorootparentYou're confusing OpenAI and MIRI. OpenAI founders: Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, Wojciech Zaremba, Sam Altman. All of them come from software tech industry and academic research circles, not evidence of interest in HPMOR or Yud. reply Vecr 6 hours agorootparentprevI think they cleaned out some of the EAs around the time of the board situation, but I don't know what the non-EA overlap is with your description. reply koe123 1 hour agorootparentprev> I guess people working there believe in magic. I've been thinking about this recently. Personally, I've yet to see any compelling evidence that an LLM, let alone any AI, can operate really well \"out of distribution\". It's capabilities (in my experience) seem to be spanned by the data it's trained on. Hence, this supposed property that it can \"train itself\", generating new knowledge in the process, is yet to be proven in my mind. That raises the question for me: why do OpenAI staff believe what they believe? If I'm being optimistic, I suppose they may have seen unreleased tech, motivating their beliefs that seemingly AGI is on the horizon. If I'm being cynical, the promise of AGI probably draws in much more investment. Thus, anyone with a stake in OpenAI has an incentive to promote this narrative of imminent AGI, regardless of how realistic it is technically. This is of course just based on what I've seen and read, I'd love to see evidence that counter my claims. reply rdedev 9 hours agorootparentprevEven if recursive self improvement does work out my hunch is that is going to be logarithmic instead of exponential mostly down to just availability of data. It might go beyond human intelligence but I don't think it will reach singularity reply lucubratory 7 hours agorootparentThis is why the big bet for AI-assisted AI-development long term is synthetic data. A big part of the reason so much money and resources is going into synthetic data right now is not just out of economic necessity, but because there have been extremely encouraging results with synthetic data (e.g. 'Textbooks Are All You Need', AlphaZero). reply rdedev 2 hours agorootparentI wouldn't count aplha zero since it's reinforcement learning. That technique you can generate high quality data all the time since the rules are fixed. Not everything can be trained using that way reply lucubratory 1 hour agorootparentThe chess knowledge and skills of LLMs comes from them ingesting a sufficient number of chess games in text format (the amount will be proportional to both other data you have and the compute you have), same with the ability of LLMs to play other games or solve other fixed rule/perfect knowledge puzzles. AlphaZero and its cousins showed that you can generate an effectively infinite quantity of extremely high-quality data in those domains. There is a possibility that the benefit to an LLM's general intelligence from giving it e.g. one billion ~4600 ELO level games is only in improving its ability to play chess. Given the results many studies have reported in cross-learning with LLMs, I doubt that though. The potential is that generating a lot of extremely high level logic and puzzle solving and providing it as extremely high quality synthetic data to an LLM can improve its general reasoning and logic capabilities - that would be huge, and is one of the promises of synthetic data. reply advael 9 hours agorootparentprevTo be honest, I think a lot of smart people are willing to believe in magic when they've demonstrated some strong capability and the people funding their company want magic to happen. reply killerstorm 9 hours agorootparentIt's not magic, though. If AI can do work of a human, it can do work of a human. It's a trivial statement, and inability to see it is a hard cope. Are you gonna to take a bet \"AI won't be able to do X in 10 years\" for some X which people can learn to do now? If you're unwilling to bet then you believe that AI would plausibly be able to perform any human job, including job of AI researcher. reply rdedev 9 hours agorootparentAt the end of the day it can only get as far as the data it has. Let's say you want to make a drug that inhibits a protein. The AI can generate plausible drugs but to see if it actually works you need to test it in the lab and then on an animal etc. now you can have an AI that has a perfect understanding of how a drug interacts with a protein but wesuch data is not available in the first place. Without that you can't just simply scale gpt type models reply xanderlewis 9 hours agorootparentprev‘Doing the work of a human’ is something that is very hard to define or quantify in many cases. You sound very confident, but you don’t address this at all; you simply assume it’s a given. Relevant: https://www.jaakkoj.com/concepts/doorman-fallacy reply advael 9 hours agorootparentYea, I think we're in kind of a vicious recursive cycle of imperfect-metric-reinforcement (reward-hacking I suppose, though often implemented in economics as well as code) rather than one of recursive self-improvement in a more holistic sense. Optimization is really good at turning small problems of this nature into big ones more quickly reply advael 9 hours agorootparentprevI don't claim it's impossible, just that there isn't a clear path from what exists now to that reality, and that the explanation presented by the above commenter (and I suppose OpenAI's website) does not clarify what they think the path is reply killerstorm 1 hour agorootparentWe can reason about it without knowing the path. E.g. somebody in 1950s could say \"If you have enough compute you can do photorealistic quality computer graphics\". If you ask them how to build a GPU they won't know. Their statement is about principal possibility. reply hackerlight 7 hours agorootparentprevWhat will AutoGPT look like if we have 100x more compute and another 10 years of research breakthroughs? It will be pretty damn good. If it can do the cognitive work of an AI researcher, well, there's your recursive self-improvement, at least on the research front (not so much on the hardware/energy front, physical constraints are trickier and will slow down progress in practice). I don't know the exact path there, because if I did I'd publish and win the Turing Award. But it seems to be a plausible outcome in the medium-term future, at least if you go with Hinton's view that current methods are capable of understanding and reasoning, and not LeCun's view that it's all a dead end. reply advael 6 hours agorootparentI won't comment on whether I believe those researchers hold those views as you describe them, but as you describe them, I think both those descriptions of the state of AI research are untrue. The capabilities demonstrated by transformer models seem necessary but not sufficient to understand and reason, meaning that while they're not necessarily a \"dead end\", it is far from guaranteed that adding more compute will get them there Of course if we allow for any arbitrary \"research breakthrough\" to happen then any outcome that's physically possible could happen, and I agree with you that superhuman artificial intelligence is possible. Nonetheless it remains unclear what research breakthroughs need to happen, how difficult they will be, and whether handing a company like OpenAI lots of money and chips will get that done, and it remains even more unclear whether that is a desirable outcome, given that the priorities of that company seem to shift considerably each time their budget is increased (As is the norm in this economic environment, to be clear, that is not a unique problem of OpenAI) Obviously OpenAI has every reason to claim that it can do this and to claim that it will use the results in a way designed to benefit humanity as a whole. The people writing this promotional copy and the people working there may even believe both of these things. However, based on the information available, I don't think the first claim is credible. The second claim becomes less credible the more of the company's original mission gets jettisoned as its priorities align more to its benefactors, which we have seen happen rather rapidly reply woopsn 9 hours agorootparentprevThey do. Altman is saying their tech may be poised to capture the sum of all value in Earth's future light cone. Saying \"well that is not physically impermissible\" doesn't make it real. In any case nobody has ever shown that recursive self-improvement \"takes off\", and nor is that what we should expect a priori. reply AgentME 43 minutes agoparentprevThe real magic of recursive self improvement happens only after you have human-level AI that is able to match and surpass human ability in designing AI architectures. Escape-velocity-breaking recursive self improvement doesn't look like a human-made architecture being trained further, it looks like an AI understanding why transformers/etc were successful and coming up with an advancement over transformers. reply mattnewton 11 hours agoparentprevI upvoted because this was my first thought too, but reading the abstract and skimming the paper makes me think it’s not really an advance for general recursive improvement. I think the title makes people think this is a text -> model model, when it is really a bunch of model weights -> new model weights optimizer for a specific architecture and problem. Still a potentially very useful idea for learning from a bunch of training runs and very interesting work! reply fnordpiglet 11 hours agorootparentI suspect this is useful for porting one vector space to another which is an open problem when you’ve trained one model with one architecture and need to port it to another architecture without paying the full retraining cost. reply GuB-42 11 hours agoparentprevDoesn't look that different from what we are already doing. For example AlphaGo/AlphaZero/MuZero learn to play board games by playing repeatedly against itself, it is a self improvement loop leading to superhuman play. It was a major breakthrough for the game of Go, and it lead to advances in the field of machine learning, but we are still far from something resembling technological singularity. GANs are another example of self-improvement. It was famous for creating \"deep fakes\". It works by pitting a fake generator and a fake detector against each other, resulting in a cycle of improvement. It didn't get much further than that, in fact, it is all about attention and transformers now. This is just a way of optimizing parameters, it will not invent new techniques. It can say \"put 1000 neurons there, 2000 there, etc...\", but it still has to pick from what designers tell it to pick from. It may adjust these parameters better than a human can, leading to more efficient systems, I expect some improvement to existing systems, but not a breaking change. reply pests 9 hours agorootparentGo and Chess still has rules that are hard coded which at least gives a framework to optimize in. What rules do you give an LLM? reply drdeca 8 hours agorootparentSome sort of \"generate descriptions of novel tasks including ways to evaluate performance at those tasks, evaluate quality of the generated tasks+evaluation-metrics, split tasks into subtasks, estimate difficulty of tasks in a way that is is judged on how it compares to a combined estimated difficulty of generated subtasks and to actual success rate and quality\" sort of deal? reply spangry 8 hours agorootparentprevPhysics. reply philsnow 9 hours agoparentprevA rare opportunity for the other four-letter comic to be applicable: http://smbc-comics.com/comic/2011-12-13 (Though I suppose this skips Neuralink / step 3 and jumps right to step 4.) reply bamboozled 11 hours agoparentprevThe ai is ready to take off to perfection land reply jarrell_mark 1 hour agoprevCan this be used to fill in the missing information on the openworm nematode 302 neurons brain simulator? reply Scene_Cast2 11 hours agoprevYay, an alternative to backprop & SGD! Really interesting and impressive finding, I was surprised that the network generalizes. reply goggy_googy 11 hours agoprev\"We synthesize 100 novel parameters by feeding random noise into the latent diffusion model and the trained decoder.\" Cool that patterns exist at this level, but also, 100 params means we have a long way to go before this process is efficient enough to synthesize more modern-sized models. reply hoc 3 hours agoprevHm, so does this actually improve/condense the representation for certain applications or is this some more some kind of global expand and collect in network space? reply goggy_googy 11 hours agoprevImportant to note, they say \"From these generated models, we select the one with the best performance on the training set.\" Definitely potential for bias here. reply nerdponx 10 hours agoparentI'd have liked to see the distribution of generated model performance. reply QuadmasterXLII 9 hours agorootparentFig 4b reply amelius 11 hours agoprevWhy does Figure 7 not include a validation curve (afaict only the training curve is shown)? reply t_serpico 8 hours agoprevi'd wager that adding noise to the weights in a principled fashion would accomplish something similar to this. reply jerpint 8 hours agoparentI would really be surprised if just adding noise would give you convergence reply nullc 3 hours agoprevheh https://news.ycombinator.com/item?id=39208213#39211749 reply jackblemming 10 hours agoprev [–] The state of art neural net architecture, whether that be transformers or the like, trained on self play to optimize non-differentiable but highly efficient architectures is the way. reply hackerlight 10 hours agoparent [–] According to Hinton, before transformers were shown to work well, learning model architectures was Google's main focus reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"Neural Network Diffusion\" introduces the use of diffusion models to create neural network parameters with comparable or better performance than traditionally trained networks.",
      "The approach, named neural network diffusion, leverages a standard latent diffusion model to produce new parameter sets, showcasing its potential in parameter generation for Machine Learning and Computer Vision.",
      "The generated models exhibit distinct performances from trained networks, highlighting the efficacy of diffusion models in this context."
    ],
    "commentSummary": [
      "The discussion delves into various topics like Neural Network Diffusion, Transformer networks, and recursive self-improvement in humans and AI.",
      "Participants debate the potential uses of AI techniques to enhance reasoning skills and achieve superhuman intelligence.",
      "The conversation also addresses the role of data availability, OpenAI's credibility, and uncertainties in the future of AI advancements."
    ],
    "points": 190,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1708543904
  }
]
