[
  {
    "id": 41046773,
    "title": "Open source AI is the path forward",
    "originLink": "https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/",
    "originBody": "In the early days of high-performance computing, the major tech companies of the day each invested heavily in developing their own closed source versions of Unix. It was hard to imagine at the time that any other approach could develop such advanced software. Eventually though, open source Linux gained popularity – initially because it allowed developers to modify its code however they wanted and was more affordable, and over time because it became more advanced, more secure, and had a broader ecosystem supporting more capabilities than any closed Unix. Today, Linux is the industry standard foundation for both cloud computing and the operating systems that run most mobile devices – and we all benefit from superior products because of it. I believe that AI will develop in a similar way. Today, several tech companies are developing leading closed models. But open source is quickly closing the gap. Last year, Llama 2 was only comparable to an older generation of models behind the frontier. This year, Llama 3 is competitive with the most advanced models and leading in some areas. Starting next year, we expect future Llama models to become the most advanced in the industry. But even before that, Llama is already leading on openness, modifiability, and cost efficiency. Today we’re taking the next steps towards open source AI becoming the industry standard. We’re releasing Llama 3.1 405B, the first frontier-level open source AI model, as well as new and improved Llama 3.1 70B and 8B models. In addition to having significantly better cost/performance relative to closed models, the fact that the 405B model is open will make it the best choice for fine-tuning and distilling smaller models. Beyond releasing these models, we’re working with a range of companies to grow the broader ecosystem. Amazon, Databricks, and NVIDIA are launching full suites of services to support developers fine-tuning and distilling their own models. Innovators like Groq have built low-latency, low-cost inference serving for all the new models. The models will be available on all major clouds including AWS, Azure, Google, Oracle, and more. Companies like Scale.AI, Dell, Deloitte, and others are ready to help enterprises adopt Llama and train custom models with their own data. As the community grows and more companies develop new services, we can collectively make Llama the industry standard and bring the benefits of AI to everyone. Meta is committed to open source AI. I’ll outline why I believe open source is the best development stack for you, why open sourcing Llama is good for Meta, and why open source AI is good for the world and therefore a platform that will be around for the long term. Why Open Source AI Is Good for Developers When I talk to developers, CEOs, and government officials across the world, I usually hear several themes: We need to train, fine-tune, and distill our own models. Every organization has different needs that are best met with models of different sizes that are trained or fine-tuned with their specific data. On-device tasks and classification tasks require small models, while more complicated tasks require larger models. Now you’ll be able to take the most advanced Llama models, continue training them with your own data and then distill them down to a model of your optimal size – without us or anyone else seeing your data. We need to control our own destiny and not get locked into a closed vendor. Many organizations don’t want to depend on models they cannot run and control themselves. They don’t want closed model providers to be able to change their model, alter their terms of use, or even stop serving them entirely. They also don’t want to get locked into a single cloud that has exclusive rights to a model. Open source enables a broad ecosystem of companies with compatible toolchains that you can move between easily. We need to protect our data. Many organizations handle sensitive data that they need to secure and can’t send to closed models over cloud APIs. Other organizations simply don’t trust the closed model providers with their data. Open source addresses these issues by enabling you to run the models wherever you want. It is well-accepted that open source software tends to be more secure because it is developed more transparently. We need a model that is efficient and affordable to run. Developers can run inference on Llama 3.1 405B on their own infra at roughly 50% the cost of using closed models like GPT-4o, for both user-facing and offline inference tasks. We want to invest in the ecosystem that’s going to be the standard for the long term. Lots of people see that open source is advancing at a faster rate than closed models, and they want to build their systems on the architecture that will give them the greatest advantage long term. Why Open Source AI Is Good for Meta Meta’s business model is about building the best experiences and services for people. To do this, we must ensure that we always have access to the best technology, and that we’re not locking into a competitor’s closed ecosystem where they can restrict what we build. One of my formative experiences has been building our services constrained by what Apple will let us build on their platforms. Between the way they tax developers, the arbitrary rules they apply, and all the product innovations they block from shipping, it’s clear that Meta and many other companies would be freed up to build much better services for people if we could build the best versions of our products and competitors were not able to constrain what we could build. On a philosophical level, this is a major reason why I believe so strongly in building open ecosystems in AI and AR/VR for the next generation of computing. People often ask if I’m worried about giving up a technical advantage by open sourcing Llama, but I think this misses the big picture for a few reasons: First, to ensure that we have access to the best technology and aren’t locked into a closed ecosystem over the long term, Llama needs to develop into a full ecosystem of tools, efficiency improvements, silicon optimizations, and other integrations. If we were the only company using Llama, this ecosystem wouldn’t develop and we’d fare no better than the closed variants of Unix. Second, I expect AI development will continue to be very competitive, which means that open sourcing any given model isn’t giving away a massive advantage over the next best models at that point in time. The path for Llama to become the industry standard is by being consistently competitive, efficient, and open generation after generation. Third, a key difference between Meta and closed model providers is that selling access to AI models isn’t our business model. That means openly releasing Llama doesn’t undercut our revenue, sustainability, or ability to invest in research like it does for closed providers. (This is one reason several closed providers consistently lobby governments against open source.) Finally, Meta has a long history of open source projects and successes. We’ve saved billions of dollars by releasing our server, network, and data center designs with Open Compute Project and having supply chains standardize on our designs. We benefited from the ecosystem’s innovations by open sourcing leading tools like PyTorch, React, and many more tools. This approach has consistently worked for us when we stick with it over the long term. Why Open Source AI Is Good for the World I believe that open source is necessary for a positive AI future. AI has more potential than any other modern technology to increase human productivity, creativity, and quality of life – and to accelerate economic growth while unlocking progress in medical and scientific research. Open source will ensure that more people around the world have access to the benefits and opportunities of AI, that power isn’t concentrated in the hands of a small number of companies, and that the technology can be deployed more evenly and safely across society. There is an ongoing debate about the safety of open source AI models, and my view is that open source AI will be safer than the alternatives. I think governments will conclude it’s in their interest to support open source because it will make the world more prosperous and safer. My framework for understanding safety is that we need to protect against two categories of harm: unintentional and intentional. Unintentional harm is when an AI system may cause harm even when it was not the intent of those running it to do so. For example, modern AI models may inadvertently give bad health advice. Or, in more futuristic scenarios, some worry that models may unintentionally self-replicate or hyper-optimize goals to the detriment of humanity. Intentional harm is when a bad actor uses an AI model with the goal of causing harm. It’s worth noting that unintentional harm covers the majority of concerns people have around AI – ranging from what influence AI systems will have on the billions of people who will use them to most of the truly catastrophic science fiction scenarios for humanity. On this front, open source should be significantly safer since the systems are more transparent and can be widely scrutinized. Historically, open source software has been more secure for this reason. Similarly, using Llama with its safety systems like Llama Guard will likely be safer and more secure than closed models. For this reason, most conversations around open source AI safety focus on intentional harm. Our safety process includes rigorous testing and red-teaming to assess whether our models are capable of meaningful harm, with the goal of mitigating risks before release. Since the models are open, anyone is capable of testing for themselves as well. We must keep in mind that these models are trained by information that’s already on the internet, so the starting point when considering harm should be whether a model can facilitate more harm than information that can quickly be retrieved from Google or other search results. When reasoning about intentional harm, it’s helpful to distinguish between what individual or small scale actors may be able to do as opposed to what large scale actors like nation states with vast resources may be able to do. At some point in the future, individual bad actors may be able to use the intelligence of AI models to fabricate entirely new harms from the information available on the internet. At this point, the balance of power will be critical to AI safety. I think it will be better to live in a world where AI is widely deployed so that larger actors can check the power of smaller bad actors. This is how we’ve managed security on our social networks – our more robust AI systems identify and stop threats from less sophisticated actors who often use smaller scale AI systems. More broadly, larger institutions deploying AI at scale will promote security and stability across society. As long as everyone has access to similar generations of models – which open source promotes – then governments and institutions with more compute resources will be able to check bad actors with less compute. The next question is how the US and democratic nations should handle the threat of states with massive resources like China. The United States’ advantage is decentralized and open innovation. Some people argue that we must close our models to prevent China from gaining access to them, but my view is that this will not work and will only disadvantage the US and its allies. Our adversaries are great at espionage, stealing models that fit on a thumb drive is relatively easy, and most tech companies are far from operating in a way that would make this more difficult. It seems most likely that a world of only closed models results in a small number of big companies plus our geopolitical adversaries having access to leading models, while startups, universities, and small businesses miss out on opportunities. Plus, constraining American innovation to closed development increases the chance that we don’t lead at all. Instead, I think our best strategy is to build a robust open ecosystem and have our leading companies work closely with our government and allies to ensure they can best take advantage of the latest advances and achieve a sustainable first-mover advantage over the long term. When you consider the opportunities ahead, remember that most of today’s leading tech companies and scientific research are built on open source software. The next generation of companies and research will use open source AI if we collectively invest in it. That includes startups just getting off the ground as well as people in universities and countries that may not have the resources to develop their own state-of-the-art AI from scratch. The bottom line is that open source AI represents the world’s best shot at harnessing this technology to create the greatest economic opportunity and security for everyone. Let’s Build This Together With past Llama models, Meta developed them for ourselves and then released them, but didn’t focus much on building a broader ecosystem. We’re taking a different approach with this release. We’re building teams internally to enable as many developers and partners as possible to use Llama, and we’re actively building partnerships so that more companies in the ecosystem can offer unique functionality to their customers as well. I believe the Llama 3.1 release will be an inflection point in the industry where most developers begin to primarily use open source, and I expect that approach to only grow from here. I hope you’ll join us on this journey to bring the benefits of AI to everyone in the world. You can access the models now at llama.meta.com. , MZ Categories : Meta Technology and Innovation Tags: Artificial Intelligence and Machine Learning Open Source",
    "commentLink": "https://news.ycombinator.com/item?id=41046773",
    "commentBody": "Open source AI is the path forward (fb.com)767 points by atgctg 3 hours agohidepastfavorite337 comments dang 1 hour agoRelated ongoing thread: Llama 3.1 - https://news.ycombinator.com/item?id=41046540 - July 2024 (114 comments) cs702 3 hours agoprev> We’re releasing Llama 3.1 405B, the first frontier-level open source AI model, as well as new and improved Llama 3.1 70B and 8B models. Bravo! While I don't agree with Zuck's views and actions on many fronts, on this occasion I think he and the AI folks at Meta deserve our praise and gratitude. With this release, they have brought the cost of pretraining a frontier 400B+ parameter model to ZERO for pretty much everyone -- well, everyone except Meta's key competitors.[a] THANK YOU ZUCK. Meanwhile, the business-minded people at Meta surely won't mind if the release of these frontier models to the public happens to completely mess up the AI plans of competitors like OpenAI/Microsoft, Google, Anthropic, etc. Come to think of it, the negative impact on such competitors was likely a key motivation for releasing the new models. --- [a] The license is not open to the handful of companies worldwide which have more than 700M users. reply advael 49 minutes agoparentLook, absolutely zero people in the world should trust any tech company when they say they care about or will keep commitments to the open-source ecosystem in any capacity. Nevertheless, it is occasionally strategic for them to do so, and there can be ancillary benefits for said ecosystem in those moments where this is the best play for them to harm their competitors For now, Meta seems to release Llama models in ways that don't significantly lock people into their infrastructure. If that ever stops being the case, you should fork rather than trust their judgment. I say this knowing full well that most of the internet is on AWS or GCP, most brick and mortar businesses use Windows, and carrying a proprietary smartphone is essentially required to participate in many aspects of the modern economy. All of this is a mistake. You can't resist all lock-in. The players involved effectively run the world. You should still try where you can, and we should still be happy when tech companies either slip up or make the momentary strategic decision to make this easier reply ladzoppelin 21 minutes agorootparentIs forking really possible with an LLM or one the size of future Lama versions, have they even released the weights and everything? Maybe I am just negative about it because I feel Meta is the worst company ever invented and feel this will hurt society in the long run just like Facebook. reply ori_b 28 minutes agorootparentprev> If that ever stops being the case, you should fork rather than trust their judgment. Fork what? The secret sauce is in the training data and infrastructure. I don't think either of those is currently open. reply quasse 18 minutes agorootparentI'm just a lowly outsider to the AI space, but calling these open source models seems kind of like calling a compiled binary open source. If you don't have a way to replicate what they did to create the model, it seems more like freeware than open source. reply advael 3 minutes agorootparentAs an ML researcher, I agree. Meta doesn't include adequate information to replicate the models, and from the perspective of fundamental research, the interest that big tech companies have taken in this field has been a significant impediment to independent researchers, despite the fact that they are undeniably producing groundbreaking results in many respects, due to this fundamental lack of openness This should also make everyone very skeptical of any claim they are making, from benchmark results to the legalities involved in their training process to the prospect of future progress on these models. Without being able to vet their results against the same datasets they're using, there is no way to verify what they're saying, and the credulity that otherwise smart people have been exhibiting in this space has been baffling to me As a developer, if you have a working Llama model, including the source code and weights, and it's crucial for something you're building or have already built, it's still fundamentally a good thing that Meta isn't gating it behind an API and if they went away tomorrow, you could still use, self-host, retrain, and study the models reply JKCalhoun 17 minutes agorootparentprevA good point. Forgive me, I am AI naive, is there some way to harness Llama to train ones own actually-open AI? reply logicchains 13 minutes agorootparentprevThey actually did open source the infrastructure library they developed. They don't open source the data but they describe how they gathered/filtered it. reply tambourine_man 1 hour agoparentprevPraising is good. Gratitude is a bit much. They got this big by selling user generated content and private info to the highest bidder. Often through questionable means. Also, the underdog always touts Open Source and standards, so it’s good to remain skeptical when/if tables turn. reply sheepscreek 1 hour agorootparentAll said and done, it is a very expensive and blasy way to undercut competitors. They’ve spent > $5B on hardware alone, much of which will depreciate in value quickly. Pretty sure the only reason Meta’s managed to do this is because of Zuck’s iron grip on the board (majority voting rights). This is great for Open Source and regular people though! reply wrsh07 45 minutes agorootparentZuck made a bet when they provisioned for reels to buy enough GPUs to be able to spin up another reels-sized service. Llama is probably just running on spare capacity (I mean, sure, they've kept increasing capex, but if they're worried about an llm-based fb competitor they sort of have to in order to enact their copycat strategy) reply ricardo81 35 minutes agorootparentprev>selling user generated content and private info to the highest bidder Was always their modus operandi, surely. How else would they have survived. Thanks for returning everyone else;s content and never mind all the content stealing your platform did. reply germinalphrase 1 hour agoparentprev\"Come to think of it, the negative impact on such competitors was likely a key motivation for releasing the new models.\" \"Commoditize Your Complement\" is often cited here: https://gwern.net/complement reply swyx 2 hours agoparentprev> the AI folks at Meta deserve our praise and gratitude We interviewed Thomas who led Llama 2 and 3 post training here in case you want to hear from someone closer to the ground on the models https://www.latent.space/p/llama-3 reply pwdisswordfishd 1 hour agoparentprevMakes me wonder why he's really doing this. Zuckerberg being Zuckerberg, it can't be out of any genuine sense of altruism. Probably just wants to crush all competitors before he monetizes the next generation of Meta AI. reply spiralk 50 minutes agorootparentIts certainly not altruism. Given that Facebook/Meta owns the largest user data collection systems, any advancement in AI ultimately strengthens their business model (which is still mostly collecting private user data, amassing large user datasets, and selling targeting ads). There is a demo video that shows a user wearing a Quest VR headset and asks the AI \"what do you see\" and it interprets everything around it. Then, \"what goes well with these shorts\"... You can see where this is going. Wearing headsets with AIs monitoring everything the users see and collecting even more data is becoming normalized. Imagine the private data harvesting capabilities of the internet but anywhere in the physical world. People need not even choose to wear a Meta headset, simply passing a user with a Meta headset in public will be enough to have private data collected. This will be the inevitable result of vision models improvements integrated into mobile VR/AR headsets. reply bun_at_work 45 minutes agorootparentprevI really think the value of this for Meta is content generation. More open models (especially state of the art) means more content is being generated, and more content is being shared on Meta platforms, so there is more advertising revenue for Meta. reply chasd00 23 minutes agorootparentprevAll the content generated by llms (good or bad) is going to end up back in Facebook/Instagram and other social media sites. This enables Meta to show growth and therefore demand a higher stock price. So it makes sense to get content generation tools out there as widely as possible. reply phyrex 47 minutes agorootparentprevYou can always listen to the investor calls for the capitalist point of view. In short, attracting talent, building the ecosystem, and making it really easy for users to make stuff they want to share on Meta's social networks reply tintor 1 hour agoparentprev> they have brought the cost of pretraining a frontier 400B+ parameter model to ZERO It is still far from zero. reply cs702 41 minutes agorootparentIf the model is already pretrained, there's no need to pretrain it, so the cost of pretraining is zero. reply y04nn 16 minutes agoparentprevDon't be fooled, it is a \"embrace extend extinguish\" strategy. Once they have enough usage and be the default standard they will start to find any possible ways to make you pay. reply tyler-jn 13 minutes agoparentprevSo far, it seems like this release has done ~nothing to the stock price for GOOGL/MSFT, which we all know has been propped up largely on the basis of their AI plans. So it's probably premature to say that this has messed it up for them. reply troupo 1 hour agoparentprevThere's nothing open source about it. It's a proprietary dump of data you can't replicate or verify. What were the sources? What datasets it was trained on? What are the training parameters? And so on and so on reply throwaway_2494 1 hour agoparentprev> We’re releasing Llama 3.1 405B Is it possible to run this with ollama? reply vorticalbox 1 hour agorootparentIf you have the ram for it. Ollama will offload as many layers as it can to the gpu then the rest will run on the cpu/ram. reply jessechin 1 hour agorootparentprevSure, if you have a H100 cluster. If you quant it to int4 you might get away with using only 4 H100 GPUs! reply sheepscreek 1 hour agorootparentAssuming $25k a pop, that’s at least $100k in just the GPUs alone. Throw in their linking technology (NVLink) and cost for the remaining parts, won’t be surprised if you’re looking at $150k for such a cluster. Which is not bad to be honest, for something at this scale. Can anyone share the cost of their pre-built clusters, they’ve recently started selling? (sorry feeling lazy to research atm, I might do that later when I have more time). reply rty32 41 minutes agorootparentYou can rent H100 GPUs. reply sandworm101 37 minutes agoparentprev>> Bravo! While I don't agree with Zuck's views and actions on many fronts, on this occasion I think he and the AI folks at Meta deserve our praise and gratitude. Nope. Not one bit. Supporting F/OSS when it suits you in one area and then being totally dismissive of it in every other area should not be lauded. How about open sourcing some of FB's VR efforts? reply JumpCrisscross 3 hours agoprev“The Heavy Press Program was a Cold War-era program of the United States Air Force to build the largest forging presses and extrusion presses in the world.” This ”program began in 1944 and concluded in 1957 after construction of four forging presses and six extruders, at an overall cost of $279 million. Six of them are still in operation today, manufacturing structural parts for military and commercial aircraft” [1]. $279mm in 1957 dollars is about $3.2bn today [2]. A public cluster of GPUs provided for free to American universities, companies and non-profits might not be a bad idea. [1] https://en.m.wikipedia.org/wiki/Heavy_Press_Program [2] https://data.bls.gov/cgi-bin/cpicalc.pl?cost1=279&year1=1957... reply epaulson 3 hours agoparentThe National Science Foundation has been doing this for decades, starting with the supercomputing centers in the 80s. Long before anyone talked about cloud credits, NSF has had a bunch of different programs to allocate time on supercomputers to researchers at no cost, these days mostly run out of the Office of Advanced Cyberinfrastruture. (The office name is from the early 00s) - https://new.nsf.gov/cise/oac (To connect universities to the different supercomputing centers, the NSF funded the NSFnet network in the 80s, which was basically the backbone of the Internet in the 80s and early 90s. The supercomputing funding has really, really paid off for the USA) reply JumpCrisscross 3 hours agorootparent> NSF has had a bunch of different programs to allocate time on supercomputers to researchers at no cost, these days mostly run out of the Office of Advanced Cyberinfrastruture This would be the logical place to put such a programme. reply alephnerd 1 hour agorootparentThe DoE has also been a fairly active purchaser of GPUs for almost two decades now thanks to the Exascale Computing Project [0] and other predecessor projects. The DoE helped subsidize development of Kepler, Maxwell, Pascal, etc along with the underlying stack like NVLink, NGC, CUDA, etc either via purchases or allowing grants to be commercialized by Nvidia. They also played matchmaker by helping connect private sector research partners with Nvidia. The DoE also did the same thing for AMD and Intel. [0] - https://www.exascaleproject.org/ reply jszymborski 1 hour agorootparentprevAs you've rightly pointed out, we have the mechanism, now let's fund it properly! I'm in Canada, and our science funding has likewise fallen year after year as a proportion of our GDP. I'm still benefiting from A100 clusters funded by tax payer dollars, but think of the advantage we'd have over industry if we didn't have to fight over resources. reply xena 21 minutes agorootparentWhere do you get access to those as a member of the general public? reply cmdrk 49 minutes agorootparentprevYeah, the specific AI/ML-focused program is NAIRR. https://nairrpilot.org/ Terrible name unless they low-key plan to make AI researchers' hair fall out. reply CardenB 3 hours agoparentprevDoubtful that GPUs purchased today would be in use for a similar time scale. Govt investment would also drive the cost of GPUs up a great deal. Not sure why a publicly accessible GPU cluster would be a better solution than the current system of research grants. reply ygjb 3 hours agorootparentOf course they won't. The investment in the Heavy Press Program was the initial build, and just citing one example, the Alcoa 50,000 ton forging press was built in 1955, operated until 2008, and needed ~$100M to get it operational again in 2012. The investment was made to build the press, which created significant jobs and capital investment. The press, and others like it, were subsequently operated by and then sold to a private operator, which in turn enabled the massive expansion of both military manufacturing, and commercial aviation and other manufacturing. The Heavy Press Program was a strategic investment that paid dividends by both advancing the state of the art in manufacturing at the time it was built, and improving manufacturing capacity. A GPU cluster might not be the correct investment, but a strategic investment in increasing, for example, the availability of training data, or interoperability of tools, or ease of use for building, training, and distributing models would probably pay big dividends. reply dmix 2 hours agorootparentI don't think there's a shortage of capital for AI... probably the opposite Of all the things to expand the scope of government spending why would they choose AI, or more specifically GPUs? reply hluska 1 hour agorootparentLook at it from the perspective of an elected official: If it succeeds, you were ahead of the curve. If it fails, you were prudent enough to fund an investigation early. Either way, bleeding edge tech gives you a W. reply devmor 1 hour agorootparentprevThere may however, be a shortage of capital for open source AI, which is the subject under consideration. As for the why... because there's no shortage of capital for AI. It sounds like the government would like to encourage redirecting that capital to something that's good for the economy at large, rather than good for the investors of a handful of Silicon Valley firms interested only in their own short term gains. reply JumpCrisscross 3 hours agorootparentprev> A GPU cluster might not be the correct investment, but a strategic investment in increasing, for example, the availability of training data, or interoperability of tools, or ease of use for building, training, and distributing models would probably pay big dividends Would you mind expanding on these options? Universal training data sounds intriguing. reply ygjb 2 hours agorootparentSure, just on the training front, building and maintaining a broad corpus of properly managed training data with metadata that provides attribution (for example, content that is known to be human generated instead of model generated, what the source of data is for datasets such as weather data, census data, etc), and that also captures any licensing encumbrance so that consumers of the training data can be confident in their ability to use it without risk of legal challenge. Much of this is already available to private sector entities, but having a publicly funded organization responsible for curating and publishing this would enable new entrants to quickly and easily get a foundation without having to scrape the internet again, especially given how rapidly model generated content is being published. reply whimsicalism 2 hours agorootparentprevthere are many things i think are more capital constrained, if the government is trying to subsidize things. reply JumpCrisscross 3 hours agorootparentprev> Doubtful that GPUs purchased today would be in use for a similar time scale Totally agree. That doesn't mean it can't generate massive ROI. > Govt investment would also drive the cost of GPUs up a great deal Difficult to say this ex ante. On its own, yes. But it would displace some demand. And it could help boost chip production in the long run. > Not sure why a publicly accessible GPU cluster would be a better solution than the current system of research grants Those receiving the grants have to pay a private owner of the GPUs. That gatekeeping might be both problematic, if there is a conflict of interests, and inefficient. (Consider why the government runs its own supercomputers versus contracting everything to Oracle and IBM.) reply rvnx 3 hours agorootparentIt would be better that the government removes IP on such technology for public use, like drugs got generics. This way the government pays 2'500 USD per card, not 40'000 USD or whatever absurd. reply kube-system 1 hour agorootparent> It would be better that the government removes IP on such technology for public use, like drugs got generics. 20-25 year old drugs are a lot more useful than 20-25 year old GPUs, and the manufacturing supply chain is not a bottleneck. There's no generics for the latest and greatest drugs, and a fancy gene therapy might run a lot more than $40k. reply JumpCrisscross 3 hours agorootparentprev> better that the government removes IP on such technology for public use, like drugs got generics You want to punish NVIDIA for calling its shots correctly? You don't see the many ways that backfires? reply gpm 3 hours agorootparentNo. But I do want to limit the amount we reward NVIDIA for calling the shots correctly to maximize the benefit to society. For instance by reducing the duration of the government granted monopolies on chip technology that is obsolete well before the default duration of 20 years is over. That said, it strikes me that the actual limiting factor is fab capacity not nvidia's designs and we probably need to lift the monopolies preventing competition there if we want to reduce prices. reply JumpCrisscross 2 hours agorootparent> reducing the duration of the government granted monopolies on chip technology that is obsolete well before the default duration of 20 years is over Why do you think these private entities are willing to invest the massive capital it takes to keep the frontier advancing at that rate? > I do want to limit the amount we reward NVIDIA for calling the shots correctly to maximize the benefit to society Why wouldn't NVIDIA be a solid steward of that capital given their track record? reply gpm 2 hours agorootparent> Why do you think these private entities are willing to invest the massive capital it takes to keep the frontier advancing at that rate? Because whether they make 100x or 200x they make a shitload of money. > Why wouldn't NVIDIA be a solid steward of that capital given their track record? The problem isn't who is the steward of the capital. The problem is that economically efficient thing to do for a single company is (given sufficient fab capacity, and a monopoly) to raise prices to extract a greater share of the pie at the expense of shrinking the size of the pie. I'm not worried about who takes the profit, I'm worried about the size of the pie. reply whimsicalism 2 hours agorootparent> Because whether they make 100x or 200x they make a shitload of money. It's not a certainty that they 'make a shitload of money'. Reducing the right tail payoffs absolutely reduces the capital allocated to solve problems - many of which are risky bets. Your solution absolutely decreases capital investment at the margin, this is indisputable and basic economics. Even worse when the taking is not due to some pre-existing law, so companies have to deal with the additional uncertainty of whether & when future people will decide in retrospect that they got too large a payoff and arbitrarily decide to take it from them. reply gpm 2 hours agorootparentYou can't just look at the costs to an action, you also have to look at the benefits. Of course I agree I'm going to stop marginal investments from occurring into research into patent-able technologies by reducing the expect profit. But I'm going to do so very slightly because I'm not shifting the expected value by very much. Meanwhile I'm going to greatly increase the investment into the existing technology we already have, and allow many more people to try to improve upon it, and I'm going to argue the benefits greatly outweigh the costs. Whether I'm right or wrong about the net benefit, the basic economics here is that there are both costs and benefits to my proposed action. And yes I'm going to marginally reduce future investments because the same might happen in the future and that reduces expected value. In fact if I was in charge the same would happen in the future. And the trade-off I get for this is that society gets the benefit of the same actually happening in the future and us not being hamstrung by unbreachable monopolies. reply whimsicalism 1 hour agorootparent> But I'm going to do so very slightly because I'm not shifting the expected value by very much I think you're shifting it by a lot. If the government can post-hoc decide to invalidate patents because the holder is getting too successful, you are introducing a substantial impact on expectations and uncertainty. Your action is not taken in a vacuum. > Meanwhile I'm going to greatly increase the investment into the existing technology we already have, and allow many more people to try to improve upon it, and I'm going to argue the benefits greatly outweigh the costs. I think this is a much more speculative impact. Why will people even fund the improvements if the government might just decide they've gotten too large a slice of the pie later on down the road? > the trade-off I get for this is that society gets the benefit of the same actually happening in the future and us not being hamstrung by unbreachable monopolies. No the trade-off is that materially less is produced. These incentive effects are not small. Take for instance, drug price controls - a similar post-facto taking because we feel that the profits from R&D are too high. Introducing proposed price controls leads to hundreds of fewer drugs over the next decade [0] - and likely millions of premature deaths downstream of these incentive effects. And that's with a policy with a clear path towards short-term upside (cheaper drug prices). Discounted GPUs by invalidating nvidia's patents has a much more tenuous upside and clear downside. [0]: https://bpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/d/312... reply JumpCrisscross 1 hour agorootparentprev> I'm going to do so very slightly because I'm not shifting the expected value by very much You're massively increasing uncertainty. > the same would happen in the future. And the trade-off I get for this is that society gets the benefit Why would you expect it would ever happen again? What you want is an unrealized capital gains tax. Not to nuke our semiconductor industry. reply hluska 1 hour agorootparentprevYou have proposed state ownership of all successful IP. That is a massive change and yet you have demonstrated zero understanding of the possible costs. Your claim that removing a profit motivation will increase investment is flat out wrong. Everything else crumbles from there. reply whimsicalism 2 hours agorootparentprevthere is no such thing as a lump-sum transfer, this will shift expectations and incentives going forward and make future large capital projects an increasingly uphill battle reply hluska 1 hour agorootparentprevSo, if a private company is successful, you will nationalize its IP under some guise of maximizing the benefit to society? That form of government was tried once. It failed miserably. Under your idea, we’ll try a badly broken economic philosophy again. And while we’re at it, we will completely stifle investment in innovation. reply Teever 2 hours agorootparentprevThere was a post[0] on here recently about how the US went from producing woefully insufficient numbers of aircraft to producing 300k by the end of world war 2. One of the things that the post mentioned was the meager profit margin that the companies made during this time. But the thing is that this set the America auto and aviation industry up to rule the world for decades. A government going to a company and saying 'we need you to produce this product for us at a lower margin thab you'd like to' isn't the end of the world. I don't know if this is one of those scenarios but they exist. [0] https://www.construction-physics.com/p/how-to-build-300000-a... reply rvnx 2 hours agorootparentIn the case of NVIDIA it's even more sneaky. They are an intellectual property company holding the rights on plans to make graphic cards, not even a company actually making graphic cards. The government could launch an initiative \"OpenGPU\" or \"OpenAI Accelerator\", where the government orders GPUs from TSMC directly, without the middleman. It may require some tweaking in the law to allow exception to intellectual property for \"public interest\". reply whimsicalism 2 hours agorootparenty'all really don't understand how these actions would seriously harm capital markets and make it difficult for private capital formation to produce innovations going forward. reply inetknght 28 minutes agorootparent> y'all really don't understand how these actions would seriously harm capital markets and make it difficult for private capital Reflexively, I count that harm as a feature. I don't like private capital markets because I've been screwed by private capital on multiple occasions. But you are right: I don't understand how these actions would harm. So please do expand your concerns. reply freeone3000 1 hour agorootparentprevIf we have public capital formation, we don’t necessarily need private capital. Private innovation in weather modelling isn’t outpacing government work by leaps and bounds, for instance. reply whimsicalism 1 hour agorootparentbecause it is extremely challenging to capture the additional value that is being produced by better weather forecasts and generally the forecasts we have right now are pretty good. private capital is absolutely the driving force for the vast majority of innovations since the beginning of the 20th century. public capital may be involved, but it is dwarfed by private capital markets. reply freeone3000 57 minutes agorootparentIt’s challenging to capture the additional value and the forecasts are pretty good because of continual large-scale government investment into weather forecasting. NOAA is launching satellites! it’s a big deal! Private nuclear research is heavily dependent on governmental contracts to function. Solar was subsidized to heck and back for years. Public investment does work, and does make a didference. I would even say governmental involvement is sometimes even the deciding factor, to determine if research is worth pursuing. Some major capital investors have decided AI models cannot possibly gain enough money to pay for their training costs. So what do we do when we believe something is a net good for society, but isn’t going to be profitable? reply panarky 2 hours agorootparentprevTo the extent these are incremental units that wouldn't have been sold absent the government program, it's difficult to see how NVIDIA is \"harmed\". reply jvanderbot 2 hours agorootparentprevA much better investment would be to (somehow) revolutionize production of chips for AI so that it's all cheaper, more reliable, and faster to stand up new generations of software and hardware codesign. This is probably much closer to the program mentioned in the top level comment: It wasn't to produce one type of thing, but to allow better production of any large thing from lighter alloys. reply ks2048 2 hours agoparentprevHow about using some of that money to develop CUDA alternatives so everyone is not paying the Nvidia tax? reply zitterbewegung 2 hours agorootparentEither you port Tensorflow (Apple)[1] or PyTorch to your platform or you allow CUDA to run on your hardware (AMD) [2]. Companies are incentives to not have NVIDIA having a monopoly but the thing is that CUDA is a huge moat due to compatibility of all frameworks and everyone knows it. Also, all of the cloud or on premises providers use NVIDIA regardless. [1] https://developer.apple.com/metal/tensorflow-plugin/ [2] https://www.xda-developers.com/nvidia-cuda-amd-zluda/ reply lukan 2 hours agorootparentprevIt would be probably cheaper to negate some IP. There are quite some projects and initiatives to make CUDA code run on AMD for example, but as far as I know, they all stopped at some point, probably because of fear of being sued into oblivion. reply whimsicalism 2 hours agorootparentprevIt seems like rocm is already fully ready for transformer inference, so you are just referring to training? reply janalsncm 1 hour agorootparentROCm is buggy and largely undocumented. That’s why we don’t use it. reply erickj 1 hour agorootparentprevThat's the kind of work that can come out of academia and open source communities when societies provide the resources required. reply belter 2 hours agorootparentprevPlease start with the Windows Tax first for Linux users buying hardware...and the Apple Tax for Android users... reply fweimer 3 hours agoparentprevDon't these public clusters exist today, and have been around for decades at this point, with varying architectures? In the sense that you submit a proposal, it gets approved, and then you get access for your research? reply NewJazz 1 hour agorootparentThis is the most recent iteration of a national platform. They have tons of GPUs (and CPUs, and flash storage) hooked up as a Kubernetes cluster, available for teaching and research. https://nationalresearchplatform.org/ reply JumpCrisscross 3 hours agorootparentprevNot--to my knowledge--for the GPUs necessary to train cutting-edge LLMs. reply Maxious 2 hours agorootparentAll of the major cloud providers offer grants for public research https://www.amazon.science/research-awards https://edu.google.com/intl/ALL_us/programs/credits/research https://www.microsoft.com/en-us/azure-academic-research/ NVIDIA offers discounts https://developer.nvidia.com/education-pricing eg. for Australia, the National Computing Infrastructure allows researchers to reserve time on: - 160 nodes each containing four Nvidia V100 GPUs and two 24-core Intel Xeon Scalable 'Cascade Lake' processors. - 2 nodes of the NVIDIA DGX A100 system, with 8 A100 GPUs per node. https://nci.org.au/our-systems/hpc-systems reply varenc 1 hour agoparentprevI just watched this 1950s DoD video on the heavy press program and highly recommend it: https://www.youtube.com/watch?v=iZ50nZU3oG8 reply aiauthoritydev 1 hour agoparentprevOverall government doing anything is a bad idea. There are cases however where government is the only entity that can do certain things. These are things that involve military, law enforcement etc. Outside of this we should rely on private industry and for-profit industry as much as possible. reply pavlov 1 hour agorootparentThe American healthcare industry demonstrates the tremendous benefits of rigidly applying this mindset. Why couldn’t law enforcement be private too? You call 911, several private security squads rush to solve your immediate crime issue, and the ones who manage to shoot the suspect send you a $20k bill. Seems efficient. If you don’t like the size of the bill, you can always get private crime insurance. reply sterlind 29 minutes agorootparentFor a further exploration of this particular utopia, see Snowcrash by Neal Stephenson. reply fragmede 17 minutes agorootparentprev> Overall government doing anything is a bad idea. that is bereft of detail enough to just be wrong. There are things that government is good for and things that government is bad for, but \"anything\" is just too broad, and reveals an anti-government bias which just isn't well thought out. reply chris_wot 1 hour agorootparentprevThat’s not correct. The American health care system is an extreme example of where private organisations fail overall society. reply spullara 1 hour agoparentprevIt makes much more sense to invest in a next generation fab for GPUs than to buy GPUs and more closely matches this kind of project. reply prpl 2 hours agoparentprevGreat idea, too bad the DOE and NSF were there first. reply kjkjadksj 2 hours agoparentprevThe size of the cluster would have to be massive or else your job will be on the queue for a year. And even then what are you going to do downsize the resources requested so you can get in earlier? After a certain point it starts to make more sense to just buy your own xeons and run your own cluster. reply goda90 2 hours agoparentprevI'd like to see big programs to increase the amount of cheap, clean energy we have. AI compute would be one of many beneficiaries of super cheap energy, especially since you wouldn't need to chase newer, more efficient hardware just to keep costs down. reply Melatonic 1 hour agorootparentYeah this would be the real equivalent of the program people are talking about above. That an investing in core networking infrastructure (like cables) instead of just giving huge handouts to certain corporations that then pocket the money..... reply BigParm 1 hour agoparentprevSo we'll have the government bypass markets and force the working class to buy toys for the owning class? If anything, allocate compute to citizens. reply _fat_santa 1 hour agorootparent> If anything, allocate compute to citizens. If something like this were to become a reality, I could see something like \"CitizenCloud\" where once you prove that you are a US Citizen (or green card holder or some other requirement), you can then be allocated a number of credits every month for running workloads on the \"CitizenCloud\". Everyone would get a baseline amount, from there if you can prove you are a researcher or own a business related to AI then you can get more credits. reply light_hue_1 3 hours agoparentprevThe problem is that any public cluster would be outdated in 2 years. At the same time, GPUs are massively overpriced. Nvidia's profit margins on the H100 are crazy. Until we get cheaper cards that stand the test of time, building a public cluster is just a waste of money. There are far better ways to spend $1b in research dollars. reply JumpCrisscross 3 hours agorootparent> any public cluster would be outdated in 2 years The private companies buying hundreds of billions of dollars of GPUs aren't writing them off in 2 years. They won't be cutting edge for long. But that's not the point--they'll still be available. > Nvidia's profit margins on the H100 are crazy I don't see how the current practice of giving a researcher a grant so they can rent time on a Google cluster that runs H100s is more efficient. It's just a question of capex or opex. As a state, the U.S. has a structual advantage in the former. > far better ways to spend $1b in research dollars One assumes the U.S. government wouldn't be paying list price. In any case, the purpose isn't purely research ROI. Like the heavy presses, it's in making a prohibitively-expensive capital asset generally available. reply ninininino 2 hours agorootparentprevWhat about dollar cost averaging your purchases of GPUs? So that you're always buying a bit of the newest stuff every year rather than just a single fixed investment in hardware that will become outdated? Say 100 million a year every year for 20 years instead of 2 billion in a single year? reply blackeyeblitzar 3 hours agoparentprevWhat about distributed training on volunteer hardware? Is that feasible? reply oersted 2 hours agorootparentIt is an exciting concept, there's a huge wealth of gaming hardware deployed that is inactive at most hours of the day. And I'm sure people are willing to pay well above the electricity cost for it. Unfortunately, the dominant LLM architecture makes it relatively infeasible right now. - Gaming hardware has too limited VRAM for training any kind of near-state-of-the-art model. Nvidia is being annoyingly smart about this to sell enterprise GPUs at exorbitant markups. - Right now communication between machines seems to be the bottleneck, and this is way worse with limited VRAM. Even with data-centre-grade interconnect (mostly Infiniband, which is also Nvidia, smart-asses), any failed links tend to cause big delays in training. Nevertheless, it is a good direction to push towards, and the government could indeed help, but it will take time. We need both a more healthy competitive landscape in hardware, and research towards model architectures that are easy to train in a distributed manner (this was also the key to the success of Transformers, but we need to go further). reply codemusings 32 minutes agorootparentprevEver heard of SETI@home? https://setiathome.berkeley.edu reply Aperocky 2 hours agoparentprevImagine if they made a data center with 1957 electronics that cost $279 million. They probably won't be using it now because the phone in your pocket is likely more powerful. Moore law did end but data center stuff are still evolving order of magnitudes faster than forging presses. reply the8thbit 3 hours agoprev\"Eventually though, open source Linux gained popularity – initially because it allowed developers to modify its code however they wanted ...\" I find the language around \"open source AI\" to be confusing. With \"open source\" there's usually \"source\" to open, right? As in, there is human legible code that can be read and modified by the user? If so, then how can current ML models be open source? They're very large matrices that are, for the most part, inscrutable to the user. They seem akin to binaries, which, yes, can be modified by the user, but are extremely obscured to the user, and require enormous effort to understand and effectively modify. \"Open source\" code is not just code that isn't executed remotely over an API, and it seems like maybe its being conflated with that here? reply causal 3 hours agoparent\"Open weights\" is a more appropriate term but I'll point out that these weights are also largely inscrutable to the people with the code that trained it. And for licensing reasons, the datasets may not be possible to share. There is still a lot of modifying you can do with a set of weights, and they make great foundations for new stuff, but yeah we may never see a competitive model that's 100% buildable at home. Edit: mkolodny points out that the model code is shared (under llama license at least), which is really all you need to run training https://github.com/meta-llama/llama3/blob/main/llama/model.p... reply input_sh 2 hours agoparentprevOpen Source Initiative (kind of a de-facto authority on what's open source and what not) is spending a whole lot of time figuring out what it means for an AI system to be open source. In other words, they're basically trying to come up with a new license because the existing ones can't easily apply. I believe this is the current draft: https://opensource.org/deepdive/drafts/the-open-source-ai-de... reply Zambyte 2 hours agoparentprev> If so, then how can current ML models be open source? The source of a language model is the text it was trained on. Llama models are not open source (contrary to their claims), they are open weight. reply mkolodny 3 hours agoparentprevLlama’s code is open source: https://github.com/meta-llama/llama3/blob/main/llama/model.p... reply Flimm 2 hours agorootparentNo, it's not. The Llama 3 Community License Agreement is not an open source license. Open source licenses need to meet the criteria of the only widely accepted definition of \"open source\", and that's the one formulated by the OSI [0]. This license has multiple restrictions on use and distribution which make it not open source. I know Facebook keeps calling this stuff open source, maybe in order to get all the good will that open source branding gets you, but that doesn't make it true. It's like a company calling their candy vegan while listing one its ingredients as pork-based gelatin. No matter how many times the company advertises that their product is vegan, it's not, because it doesn't meet the definition of vegan. [0] - https://opensource.org/osd reply 8note 4 minutes agorootparentIsn't the MIT license the generally accepted \"open source\" license? It's a community owned term, not OSI owned reply CamperBob2 27 minutes agorootparentprevOpen source licenses need to meet the criteria of the only widely accepted definition of \"open source\", and that's the one formulated by the OSI [0] Who died and made OSI God? reply mesebrec 2 hours agorootparentprevThis is like saying any python program is open source because the python runtime is open source. Inference code is the runtime; the code that runs the model. Not the model itself. reply mkolodny 1 hour agorootparentI disagree. The file I linked to, model.py, contains the Llama 3 model itself. You can use that model with open data to train it from scratch yourself. Or you can load Meta’s open weights and have a working LLM. reply causal 1 hour agorootparentYeah a lot of people here seem to not understand that PyTorch really does make model definitions that simple, and that has everything you need to resume back-propagation. Not to mention PyTorch itself being open-sourced by Meta. That said the LLama-license doesn't meet strict definitions of OS, and I bet they have internal tooling for datacenter-scale training that's not represented here. reply apsec112 3 hours agorootparentprevThat's not the training code, just the inference code. The training code, running on thousands of high-end H100 servers, is surely much more complex. They also don't open-source the dataset, or the code they used for data scraping/filtering/etc. reply the8thbit 2 hours agorootparent\"just the inference code\" It's not the \"inference code\", its the code that specifies the architecture of the model and loads the model. The \"inference code\" is mostly the model, and the model is not legible to a human reader. Maybe someday open source models will be possible, but we will need much better interpretability tools so we can generate the source code from the model. In most software projects you write the source as a specification that is then used by the computer to implement the software, but in this case the process is reversed. reply blackeyeblitzar 2 hours agorootparentprevThat is just the inference code. Not training code or evaluation code or whatever pre/post processing they do. reply patrickaljord 2 hours agorootparentIs there an LLM with actual open source training code and dataset? Besides BLOOM https://huggingface.co/bigscience/bloom reply navinsylvester 1 hour agorootparentHere you go - https://github.com/apple/corenet reply osanseviero 1 hour agorootparentprevYes, there are a few dozen full open source models (license, code, data, models) reply bilsbie 3 hours agoparentprevCan’t you do fine tuning on those binaries? That’s a modification. reply the8thbit 3 hours agorootparentYou can fine tune the models, and you can modify binaries. However, there is no human readable \"source\" to open in either case. The act of \"fine tuning\" is essentially brute forcing the system to gradually alter the weights such that loss is reduced against a new training set. This limits what you can actually do with the model vs an actual open source system where you can understand how the system is working and modify specific functionality. Additionally, models can be (and are) fine tuned via APIs, so if that is the threshold required for a system to be \"open source\", then that would also make the GPT4 family and other such API only models which allow finetuning open source. reply whimsicalism 2 hours agorootparentI don't find this argument super convincing. There's a pretty clear difference between the 'finetuning' offered via API by GPT4 and the ability to do whatever sort of finetuning you want and get the weights at the end that you can do with open weights models. \"Brute forcing\" is not the correct language to use for describing fine-tuning. It is not as if you are trying weights randomly and seeing which ones work on your dataset - you are following a gradient. reply the8thbit 2 hours agorootparent\"There's a pretty clear difference between the 'finetuning' offered via API by GPT4 and the ability to do whatever sort of finetuning you want and get the weights at the end that you can do with open weights models.\" Yes, the difference is that one is provided over a remote API, and the provider of the API can restrict how you interact with it, while the other is performed directly by the user. One is a SaaS solution, the other is a compiled solution, and neither are open source. \"\"Brute forcing\" is not the correct language to use for describing fine-tuning. It is not as if you are trying weights randomly and seeing which ones work on your dataset - you are following a gradient.\" Whatever you want to call it, this doesn't sound like modifying functionality in source code. When I modify source code, I might make a change, check what that does, change the same functionality again, check the new change, etc... up to maybe a couple dozen times. What I don't do is have a very simple routine make very small modifications to all of the system's functionality, then check the result of that small change across the broad spectrum of functionality, and repeat millions of times. reply Kubuxu 1 hour agorootparentThe gap between fine-tuning API and weights-available is much more significant than you give it credit for. You can take the weights and train LoRAs (which is close to fine-tuning), but you can also build custom adapters on top (classification heads). You can mix models from different fine-tunes or perform model surgery (adding additional layers, attention heads, MoE). You can perform model decomposition and amplify some of its characteristics. You can also train multi-modal adapters for the model. Prompt tuning requires weights as well. I would even say that having the model is more potent in the hands of individual users than having the dataset. reply emporas 1 hour agorootparentprev> When I modify source code, I might make a change, check what that does, change the same functionality again, check the new change, etc... up to maybe a couple dozen times. You can modify individual neurons if you are so inclined. That's what Anthropic have done with the Claude family of models [1]. You cannot do that using any closed model. So \"Open Weights\" looks very much like \"Open Source\". Techniques for introspection of weights are very primitive, but i do think new techniques will be developed, or even new architectures which will make it much easier. [1] https://www.anthropic.com/news/mapping-mind-language-model reply the8thbit 1 hour agorootparent\"You can modify individual neurons if you are so inclined.\" You can also modify a binary, but that doesn't mean that binaries are open source. \"That's what Anthropic have done with the Claude family of models [1]. ... Techniques for introspection of weights are very primitive, but i do think new techniques will be developed\" Yeah, I don't think what we have now is robust enough interpretability to be capable of generating something comparable to \"source code\", but I would like to see us get there at some point. It might sound crazy, but a few years ago the degree of interpretability we have today (thanks in no small part to Anthropic's work) would have sounded crazy. I think getting to open sourcable models is probably pretty important for producing models that actually do what we want them to do, and as these models become more powerful and integrated into our lives and production processes the inability to make them do what we actually want them to do may become increasingly dangerous. Muddling the meaning of open source today to market your product, then, can have troubling downstream effects as focus in the open source community may be taken away from interpretability and on distributing and tuning public weights. reply bilsbie 2 hours agorootparentprevYou make a good point but those are also just limitations of the technology (or at least our current understanding of it) Maybe an analogy would help. A family spent generations breeding the perfect apple tree and they decided to “open source” it. What would open sourcing look like? reply the8thbit 1 hour agorootparent\"You make a good point but those are also just limitations of the technology (or at least our current understanding of it)\" Yeah, that is my point. Things that don't have source code can't be open source. \"Maybe an analogy would help. A family spent generations breeding the perfect apple tree and they decided to “open source” it. What would open sourcing look like?\" I think we need to be weary of dilemmas without solutions here. For example, let's think about another analogy: I was in a car accident last week. How can I open source my car accident? I don't think all, or even most things, are actually \"open sourcable\". ML models could be open sourced, but it would require a lot of work to interpret the models and generate the source code from them. reply jsheard 3 hours agoparentprevI also think that something like Chromium is a better analogy for corporate open source models than a grassroots project like Linux is. Chromium is technically open source, but Google has absolute control over the direction of it's development and realistically it's far too complex to maintain a fork without Googles resources, just like Meta has complete control over what goes into their open models, and even if they did release all the training data and code (which they don't) us mere plebs could never afford to train a fork from scratch anyway. reply skybrian 3 hours agorootparentI think you’re right from the perspective of an individual developer. You and I are not about to fork Chromium any time soon. If you presume that forking is impractical then sure, the right to fork isn’t worth much. But just because a single developer couldn’t do it doesn’t mean it couldn’t be done. It means nobody has organized a large enough effort yet. For something like a browser, which is critical for security, you need both the organization and the trust. Despite frequent criticism, Mozilla (for example) is still considered pretty trustworthy in a way that an unknown developer can’t be. reply Yizahi 2 hours agorootparentIf Microsoft can't do it, then we can reasonably conclude that it can't be done for any practical purpose. Discussing infinitesimal possibilities is better left to philosophers. reply skybrian 1 hour agorootparentDoesn’t Microsoft maintain its own fork of Chromimum? reply orthoxerox 3 hours agoparentprevOpen training dataset + open steps sufficient to train exactly the same model. reply the8thbit 3 hours agorootparentThis isn't what Meta releases with their models, though I would like to see more public training data. However, I still don't think that would qualify as \"open source\". Something isn't open source just because its reproducible out of composable parts. If one, very critical and system defining part is a binary (or similar) without publicly available source code, then I don't think it can be said to be \"open source\". That would be like saying that Windows 11 is open source because Windows Calculator is open source, and its a component of Windows. reply orthoxerox 2 hours agorootparentThat's what I meant by \"open steps\", I guess I wasn't clear enough. reply the8thbit 2 hours agorootparentIs that what you meant? I don't think releasing the sequence of steps required to produce the model satisfies \"open source\", which is how I interpreted you, because there is still no source code for the model. reply blackeyeblitzar 2 hours agorootparentprevHere’s one list of what is needed to be actually open source: https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e73... reply Yizahi 2 hours agorootparentprevThey can't release training dataset if it was illegally scrapped all over the web without permission :) (taps head) reply stale2002 2 hours agoparentprevOk call it Open Weights then if the dictionary definitions matter so much to you. The actual point that matters is that these models are available for most people to use for a lot of stuff, and this is way way better than what competitors like OpenAI offer. reply the8thbit 2 hours agorootparentThey don't \"[allow] developers to modify its code however they want\", which is a critical component of \"open source\", and one that Meta is clearly trying to leverage in branding around its products. I would like them to start calling these \"public weight models\", because what they're doing now is muddying the waters so much that \"open source\" now just means providing an enormous binary and an open source harness to run it in, rather than serving access to the same binary via an API. reply Voloskaya 1 hour agorootparentFeels a bit like you are splitting hair for the pleasure of semantic arguments to be honest. Yes there are no source in ML, so if we want to be pedantic it shouldn't be called open source. But what really matters in the open source movement is that we are able to take a program built by someone and modify it to do whatever we want with it, without having to ask someone for permission or get scrutinized or have to pay someone. The same applies here, you can take those models and modify them to do whatever you want (provided you know how to train ML models), without having to ask for permission, get scrutinized or pay someone. I personally think using the term open source is fine, as it conveys the intent correctly, even if, yes, weights are not sources you can read with your eyes. reply wrs 50 minutes agorootparentCalling that “open source” renders the word “source” meaningless. By your definition, I can release a binary executable freely and call it “open source” because you can modify it to do whatever you want. Model weights are like a binary that nobody has the source for. We need another term. reply candiddevmike 3 hours agoparentprevNone of Meta's models are \"open source\" in the FOSS sense, even the latest Llama 3.1. The license is restrictive. And no one has bothered to release their training data either. This post is an ad and trying to paint these things as something they aren't. reply JumpCrisscross 3 hours agorootparent> no one has bothered to release their training data If the FOSS community sets this as the benchmark for open source in respect of AI, they're going to lose control of the term. In most jurisdictions it would be illegal for the likes of Meta to release training data. reply mesebrec 2 hours agorootparentRegardless of the training data, the license even heavily restricts how you can use the model. Please read through their \"acceptable use\" policy before you decide whether this is really in line with open source. reply JumpCrisscross 2 hours agorootparent> Please read through their \"acceptable use\" policy before you decide whether this is really in line with open source I'm not taking a specific posiion on this license. I haven't read it closely. My broad point is simply that open source AI, as a term, cannot practically require the training data be made available. reply exe34 3 hours agorootparentprevthe training data is the source. reply JimDabell 2 hours agorootparentI don’t think it’s that simple. The source is “the preferred form of the work for making modifications to it” (to use the GPL’s wording). For an LLM, that’s not the training data. That’s the model itself. You don’t make changes to an LLM by going back to the training data and making changes to it, then re-running the training. You update the model itself with more training data. You can’t even use the training code and original training data to reproduce the existing model. A lot of it is non-deterministic, so you’ll get different results each time anyway. Another complication is that the object code for normal software is a clear derivative work of the source code. It’s a direct translation from one form to another. This isn’t the case with LLMs and their training data. The models learn from it, but they aren’t simply an alternative form of it. I don’t think you can describe an LLM as a derivative work of its training data. It learns from it, it isn’t a copy of it. This is mostly the reason why distributing training data is infeasible – the model’s creator may not have the license to do so. Would it be extremely useful to have the original training data? Definitely. Is distributing it the same as distributing source code for normal software? I don’t think so. I think new terminology is needed for open AI models. We can’t simply re-use what works for human-editable code because it’s a fundamentally different type of thing with different technical and legal constraints. reply sangnoir 1 hour agorootparentprevWe've had a similar debate before, but the last time it about whether Linux device drivers based on non-public datasheets under NDA were actually open source. This debate occurred again over drivers that interact with binary blobs. I disagree with the purists - if you can legally change the source or weights - even without having access to the data used by the upstream authors - it's open enough for me. YMMV. reply wrs 46 minutes agorootparentprevI don’t think even that is true. I conjecture that Facebook couldn’t reproduce the model weights if they started over with the same training data, because I doubt such a huge training run is a reproducible deterministic process. I don’t think anyone has “the” source. reply exe34 16 minutes agorootparentnumpy.random.seed(1234) reply JumpCrisscross 2 hours agorootparentprev> the training data is the source Sure. But that's not going to be released. The term open source AI cannot be expected to cover it because it's not practical. reply tintor 2 hours agorootparentMeta can call it something else other than open source. Synthetic part of the training data could be released. reply plsbenice34 2 hours agorootparentprevOf course it could be practical - provide the data. The fact of that society is a dystopian nightmare controlled by a few megacorporations that don't want free information does not justify outright changing the meaning of the language. reply JumpCrisscross 2 hours agorootparent> provide the data Who? It's not their data. reply diggan 2 hours agorootparentprevSo because it's really hard to do proper Open Source with these LLMs, means we need to change the meaning of Open Source so it fits with these PR releases? reply JumpCrisscross 2 hours agorootparent> because it's really hard to do proper Open Source with these LLMs, means we need to change the meaning of Open Source so it fits with these PR releases? Open training data is hard to the point of impracticality. It requires excluding private and proprietary data. Meanwhile, the term \"open source\" is massively popular. So it will get used. The question is how. Meta et al would love for the choice to be between, on one hand, open weights only, and, on the other hand, open training data, because the latter is impractical. That dichotomy guarantees that when someone says open source AI they'll mean open weights. (The way open source software, today, generally means source available, not FOSS.) reply diggan 2 hours agorootparent> Open training data is hard to the point of impracticality. It requires excluding private and proprietary data. Right, so the onus is on Facebook/Meta to get that right, then they could call something Open Source, until then, find another name that already doesn't have a specific meaning. > (The way open source software, today, generally means source available, not FOSS.) No, but it's going in that way. Open Source, today, still means that the things you need to build a project, is publicly available for you to download and run on your own machine, granted you have the means to do so. What you're thinking of is literally called \"Source Available\" which is very different from \"Open Source\". The intent of Open Source is for people to be able to reproduce the work themselves, with modifications if they want to. Is that something you can do today with the various Llama models? No, because one core part of the projects \"source code\" (what you need to reproduce it from scratch), the training data, is being held back and kept private. reply unethical_ban 2 hours agorootparentprev>Meanwhile, the term \"open source\" is massively popular. So it will get used. The question is how. Here's the source of the disagreement. You're justifying the use of the term \"open source\" by saying it's logical for Meta to want to use it for its popularity and layman (incorrect) understanding. Other person is saying it doesn't matter how convenient it is or how much Meta wants to use it, that the term \"open source\" is misleading for a product where the \"source\" is the training data, and the final product has onerous restrictions on use. This would be like Adobe giving Photoshop away for free, but for personal use only and not for making ads for Adobe's competitors. Sure, Adobe likes it and most users may be fine with it, but it isn't open source. >The way open source software, today, generally means source available, not FOSS. I don't agree with that. When a company says \"open source\" but it's not free, the tech community is quick to call it \"source available\" or \"open core\". reply JumpCrisscross 2 hours agorootparent> You're justifying the use of the term \"open source\" by saying it's logical for Meta to want to use it for its popularity and layman (incorrect) understanding I'm actually not a fan of Meta's definition. I'm arguing specifically against an unrealistic definition, because for practical purposes that cedes the term to Meta. > the term \"open source\" is misleading for a product where the \"source\" is the training data, and the final product has onerous restrictions on use Agree. I think the focus should be on the use restrictions. > When a company says \"open source\" but it's not free, the tech community is quick to call it \"source available\" or \"open core\" This isn't consistently applied. It's why we have the free vs open vs FOSS fracture. reply Palomides 2 hours agorootparentprevsource available is absolutely not the same as open source you are playing very loosely with terms that have specific, widely accepted definitions (e.g. https://opensource.org/osd ) I don't get why you think it would be useful to call LLMs with published weights \"open source\" reply JumpCrisscross 2 hours agorootparent> terms that have specific, widely accepted definitions OSF's definition is far from the only one [1]. Switzerland is currently implementing CH Open's definition, the EU another one, et cetera. > I don't get why you think it would be useful to call LLMs with published weights \"open source\" I don't. I'm saying that if the choice is between open weights or open weights + open training data, open weights will win because the useful definition will outcompete the pristine one in a public context. [1] https://en.wikipedia.org/wiki/Open-source_software#Definitio... reply Palomides 22 minutes agorootparentdiluting open source into a marketing term meaning \"you can download something\" would be a sad result reply diggan 2 hours agorootparentprevFor the EU, I'm guessing you're talking about the EUPL, which is FSF/OSI approved and GPL compatible, generally considered copyleft. For the CH Open, I'm not finding anything specific, even from Swiss websites, could you help me understand what you're referring to here? I'm guessing that all these definitions have at least some points in common, which involves (another guess) at least being able to produce the output artifacts/binaries by yourself, something that you cannot do with Llama, just as an example. reply JumpCrisscross 2 hours agorootparent> For the CH Open, I'm not finding anything specific, even from Swiss websites, could you help me understand what you're referring to here Was on the HN front page earlier [1][2]. The definition comes strikingly close to source on request with no use restrictions. > all these definitions have at least some points in common Agreed. But they're all different. There isn't an accepted defintiion of open source even when it comes to software; there is an accepted set of broad principles. [1] https://news.ycombinator.com/item?id=41047172 [2] https://joinup.ec.europa.eu/collection/open-source-observato... reply diggan 2 hours agorootparent> Agreed. But they're all different. There isn't an accepted defintiion of open source even when it comes to software; there is an accepted set of broad principles. Agreed, but are we splitting hairs here and is it relevant to the claim made earlier? > (The way open source software, today, generally means source available, not FOSS.) Do any of these principles or definitions from these orgs agree/disagree with that? My hypothesis is that they generally would go against that belief and instead argue that open source is different from source available. But I haven't looked specifically to confirm if that's true or not, just a guess. reply JumpCrisscross 1 hour agorootparent> are we splitting hairs here and is it relevant to the claim made earlier? I don't think so. Take the Swiss definition. Source on request, not even available. Yet being branded and accepted as open source. (To be clear, the Swiss example favours FOSS. But it also permits source on request and bundles them together under the same label.) reply SquareWheel 39 minutes agorootparentprev> specific, widely accepted definitions Realistically, nobody outside of Hacker News commenters have ever cared about the OSD. It's just not how the term is used colloquially. reply Palomides 24 minutes agorootparentwho says open source colloquially? ime anyone who doesn't care about software licenses will just say free (per free beer) and (strong personal opinion) any software developer should have a firm grip on the terminology and details for legal reasons reply root_axis 1 hour agorootparentprevNo. It's an asset used in the training process, the source code can process arbitrary training data. reply light_triad 21 minutes agoprevThey are positioning themselves as champions of AI open source mostly because they were blindsided by OpenAI, are not in the infra game, and want to commoditize their complements as much as possible. This is not altruism although it's still great for devs and startups. All FB GPU investments is primarily for new AI products \"friends\", recommendations and selling ads. https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ reply hubraumhugo 2 hours agoprevThe big winners of this: devs and AI startups - No more vendor lock-in - Instead of just wrapping proprietary API endpoints, developers can now integrate AI deeply into their products in a very cost-effective and performant way - Price race to the bottom with near-instant LLM responses at very low prices are on the horizon As a founder, it feels like a very exciting time to build a startup as your product automatically becomes better, cheaper, and more scalable with every major AI advancement. This leads to a powerful flywheel effect: https://www.kadoa.com/blog/ai-flywheel reply boringg 51 minutes agoparent- Price race to the bottom with near-instant LLM responses at very low prices are on the horizon Maybe a big price war while the market majors fight out for positioning but they still need to make money off their investments so someone is going to have to raise prices at some point and youll be locked into their system if you build on it. reply danielmarkbruce 57 minutes agoparentprevIt creates the opposite of a flywheel effect for you. It creates a leapfrog effect. reply boringg 50 minutes agorootparentAI might cannabalize a lot of first gen AI businesses. reply anthomtb 14 minutes agoprev> My framework for understanding safety is that we need to protect against two categories of harm: unintentional and intentional. Unintentional harm is when an AI system may cause harm even when it was not the intent of those running it to do so. For example, modern AI models may inadvertently give bad health advice. Or, in more futuristic scenarios, some worry that models may unintentionally self-replicate or hyper-optimize goals to the detriment of humanity. Intentional harm is when a bad actor uses an AI model with the goal of causing harm. Okay then Mark. Replace \"modern AI models\" with \"social media\" and repeat this statement with a straight face. reply bun_at_work 2 hours agoprevMeta makes their money off advertising, which means they profit from attention. This means they need content that will grab attention, and creating open source models that allow anyone to create any content on their own becomes good for Meta. The users of the models can post it to their Instagram/FB/Threads account. Releasing an open model also releases Meta from the burden of having to police the content the model generates, once the open source community fine-tunes the models. Overall, this move is good business move for Meta - the post doesn't really talk about the true benefit, instead moralizing about open source, but this is a sound business move for Meta. reply natural219 1 hour agoparentAI moderators too would be an enormous boon if they could get that right. reply jklinger410 2 hours agoparentprevThis is a great point. Eventually, META will only allow LLAMA generated visual AI content on its platforms. They'll put a little key in the image that clears it with the platform. Then all other visual AI content will be banned. If that is where legislation is heading. reply zoogeny 23 minutes agoprevTotally tangential thought, probably doomed to be lost in the flood of comments on this very interesting announcement. I was thinking today about Musk, Zuckerberg and Altman. Each claims that the next version of their big LLMs will be the best. For some reason it reminded me of one apocryphal cause of WW1, which was that the kings of Europe were locked in a kind of ego driven contest. It made me think about the Nation State as a technology. In some sense, the kings were employing the new technology which was clearly going to be the basis for the future political order. And they were pitting their own implementation of this new technology against the other kings. I feel we are seeing a similar clash of kings playing out. The claims that this is all just business or some larger claim about the good of humanity seem secondary to the ego stakes of the major players. And when it was about who built the biggest rocket, it felt less dangerous. It breaks my heart just a little bit. I feel sympathy in some sense for the AIs we will create, especially if they do reach the level of AGI. As another tortured analogy, it is like a bunch of competitive parents forcing their children into adversarial relationships to satisfy the parent's ego. reply j_m_b 9 minutes agoprev> We need to protect our data. This is a very important concern in Health Care because of HIPAA compliance. You can't just send your data over the wire to someone's proprietary API. You would at least need to de-identify your data. This can be a tricky task, especially with unstructured text. reply kart23 3 hours agoprev> This is how we’ve managed security on our social networks – our more robust AI systems identify and stop threats from less sophisticated actors who often use smaller scale AI systems. Ok, first of all, has this really worked? AI moderators still can't capture the mass of obvious spam/bots on all their platforms, threads included. Second, AI detection doesn't work, and with how much better the systems are getting, it's probably never going to, unless you keep the best models for yourself, and it's is clear from the rest of the note that its not zuck's intention to do so. > As long as everyone has access to similar generations of models – which open source promotes – then governments and institutions with more compute resources will be able to check bad actors with less compute. This just doesn't make sense. How are you going to prevent AI spam, AI deepfakes from causing harm with more compute? What are you gonna do with more compute about nonconsensual deepfakes? People are already using AI to bypass identity verification on your social media networks, and pump out loads of spam. reply simonw 2 hours agoparent\"AI detection doesn't work, and with how much better the systems are getting, it's probably never going to, unless you keep the best models for yourself\" I don't think that's true. I don't think even the best privately held models will be able to detect AI text reliably enough for that to be worthwhile. reply OpenComment 3 hours agoparentprevInteresting quotes. Less sophisticated actors just means humans who already write in 2020 what the NYT wrote in early 2022 to prepare for Biden's State Of The Union 180° policy reversals (manufacturing consent). FB was notorious for censorship. Anyway, what is with the \"actions/actors\" terminology? This is straightforward totalitarian language. reply openrisk 1 hour agoprevOpen source \"AI\" is a proxy for democratising and making (much) more widely useful the goodies of high performance computing (HPC). The HPC domain (data and compute intensive applications that typically need vector, parallel or other such architectures) have been around for the longest time, but confined to academic / government tasks. LLM's with their famous \"matrix multiply\" at their very core are basically demolishing an ossified frontier where a few commercial entities (Intel, Microsoft, Apple, Google, Samsung etc) have defined for decades what computing looks like for most people. Assuming that the genie is out of the bottle, the question is: what is the shape of end-user devices that are optimally designed to use compute intensive open source algorithms? The \"AI PC\" is already a marketing gimmick, but could it be that Linux desktops and smartphones will suddenly be \"ΑΙ natives\"? For sure its a transformational period and the landscape T+10 yrs could be drastically different... reply btbuildem 1 hour agoprevThe \"open source\" part sounds nice, though we all know there's nothing particularly open about the models (or their weights). The barriers to entry remain the same - huge upfront investments to train your own, and steep ongoing costs for \"inference\". Is the vision here to treat LLM-based AI as a \"public good\", akin to a utility provider in a civilized country (taxpayer funded, govt maintained, non-for-profit)? I think we could arguably call this \"open source\" when all the infra blueprints, scripts and configs are freely available for anyone to try and duplicate the state-of-the-art (resource and grokking requirements nonwithstanding) reply rybosworld 3 hours agoprevHuge companies like facebook will often argue for solutions that on the surface, seem to be in the public interest. But I have strong doubts they (or any other company) actually believe what they are saying. Here is the reality: - Facebook is spending untold billions on GPU hardware. - Facebook is arguing in favor of open sourcing the models, that they spent billions of dollars to generate, for free...? It follows that companies with much smaller resources (money) will not be able to match what Facebook is doing. Seems like an attempt to kill off the competition (specifically, smaller organizations) before they can take root. reply mattnewton 3 hours agoparentI actually think this is one of the rare times where the small guys interests are aligned with Meta. Meta is scared of a world where they are locked out of LLM platforms, one where OpenAI gets to dictate rules around their use of the platform much like Apple and Google dictates rules around advertiser data and monetization on their mobile platforms. Small developers should be scared of a world where the only competitive LLMs are owned by those players too. Through this lense, Meta’s actions make more sense to me. Why invest billions in VR/AR? The answer is simple, don’t get locked out of the next platform, maybe you can own the next one. Why invest in LLMs? Again, don’t get locked out. Google and OpenAi/Microsoft are far larger and ahead of Meta right now and Meta genuinely believes the best way to make sure they have an LLM they control is to make everyone else have an LLM they can control. That way community efforts are unified around their standard. reply mupuff1234 2 hours agorootparentSure, but don't you think the \"not getting locked out\" is just the pre-requisite for their eventual goal of locking everyone else out? reply yesco 1 hour agorootparentDoes it really matter? Attributing goodwill to a company is like attributing goodwill to a spider that happens to clean up the bugs in your basement. Sure if they had the ability to, I'm confident Meta would try something like that, but they obviously don't, and will not for the foreseeable future. I have faith they will continue to do what's in their best interests and if their best interests happen to align with mine, then I will support that. Just like how I don't bother killing the spider in my basement because it helps clean up the other bugs. reply mupuff1234 55 minutes agorootparentBut you also know that the spider has been laying eggs - So you better have an extermination plan ready. reply myaccountonhn 2 hours agorootparentprev> I actually think this is one of the rare times where the small guys interests are aligned with Meta Small guys are the ones being screwed over by AI companies and having their text/art/code stolen without any attribution or adherence to license. I don’t think Meta is on their side at all reply MisterPea 2 hours agorootparentThat's a separate problem which affects small to large players alike (e.g. ScarJo). Small companies interests are aligned with Meta as they are now on an equal footing with large incumbent players. They can now compete with a similarly sized team at a big tech company instead of that team + dozens of AI scientists reply Salgat 3 hours agoparentprevThe reason for Meta making their model open source is rather simple: They receive an unimaginable amount of free labor, and their license only excludes their major competitors to ensure mass adoption without benefiting their competition (Microsoft, Google, Alibaba, etc). Public interest, philanthropy, etc are just nice little marketing bonuses as far as they're concerned (otherwise they wouldn't be including this licensing restriction). reply ketzo 2 hours agoparentprevMeta is, fundamentally, a user-generated-content distribution company. Meta wants to make sure they commoditize their complements: they don’t want a world where OpenAI captures all the value of content generation, they want the cost of producing the best content to be as close to free as possible. reply chasd00 2 hours agorootparenti was thinking along the same. A lot of content generated by LLMs is going to end up on Facebook or Instagram. The easier it is to create AI generated content the more content ends up on those applications. reply Nesco 1 hour agorootparentprevEspecially because genAI is a copyright laundering system. You can train it on copyrighted material and none of the content generated with it are copyright-able, which is perfect for social apps reply xpe 5 minutes agoprevZuck needs to get real. They are Open Weights not Open Source. reply smusamashah 1 hour agoprevMeta's article with more details on the new LLAMA 3.1 https://ai.meta.com/blog/meta-llama-3-1/ reply 6gvONxR4sf7o 1 hour agoprev> Third, a key difference between Meta and closed model providers is that selling access to AI models isn’t our business model. That means openly releasing Llama doesn’t undercut our revenue, sustainability, or ability to invest in research like it does for closed providers. (This is one reason several closed providers consistently lobby governments against open source.) The whole thing is interesting, but this part strikes me as potentially anticompetitive reasoning. I wonder what the lines are that they have to avoid crossing here? reply phkahler 1 hour agoparent>> ...but this part strikes me as potentially anticompetitive reasoning. \"Commoditize your complements\" is an accepted strategy. And while pricing below cost to harm competitors is often illegal, the reality is that the marginal cost of software is zero. reply Palomides 9 minutes agorootparentspending a very quantifiable large amount of money to release something your nominal competitors charge for without having your own direct business case for it seems a little much reply avivo 1 hour agoprevThe FTC also recently put out a statement that is fairly pro-open source: https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/202... I think it's interesting to think about this question of open source, benefits, risk, and even competition, without all of the baggage that Meta brings. I agree with the FTC, that the benefits of open-weight models are significant for competition. The challenge is in distinguishing between good competition and bad competition. Some kind of competition can harm consumers and critical public goods, including democracy itself. For example, competing for people's scarce attention or for their food buying, with increasingly optimized and addictive innovations. Or competition to build the most powerful biological weapons. Other kinds of competition can massively accelerate valuable innovation. The FTC must navigate a tricky balance here — leaning into competition that serves consumers and the broader public, while being careful about what kind of competition it is accelerating that could cause significant risk and harm. It's also obviously not just \"big tech\" that cares about the risks behind open-weight foundation models. Many people have written about these risks even before it became a subject of major tech investment. (In other words, A16Z's framing is often rather misleading.) There are many non-big tech actors who are very concerned about current and potential negative impacts of open-weight foundation models. One approach which can provide the best of both worlds, is for cases where there are significant potential risks, to ensure that there is at least some period of time where weights are not provided openly, in order to learn a bit about the potential implications of new models. Longer-term, there may be a line where models are too risky to share openly, and it may be unclear what that line is. In that case, it's important that we have governance systems for such decisions that are not just profit-driven, and which can help us continue to get the best of all worlds. (Plug: my organization, the AI & Democracy Foundation; https://ai-dem.org/; is working to develop such systems and hiring.) reply whimsicalism 39 minutes agoparentmaking food that people want to buy is good actually i am not down with this concept of the chattering class deciding what are good markets and what are bad, unless it is due to broad-based and obvious moral judgements. reply frabjoused 1 hour agoprevWho knew FB would hold OpenAI's original ideals, and OpenAI now holds early FB ideals/integrity. reply boringg 49 minutes agoparentFB needed to differentiate drastically. FB is at its best creating large data infra. reply typpo 2 hours agoprevThanks to Meta for their work on safety, particularly Llama Guard. Llama Guard 3 adds defamation, elections, and code interpreter abuse as detection categories. Having run many red teams recently as I build out promptfoo's red teaming featureset [0], I've noticed the Llama models punch above their weight in terms of accuracy when it comes to safety. People hate excessive guardrails and Llama seems to thread the needle. Very bullish on open source. [0] https://www.promptfoo.dev/docs/red-team/ reply swyx 2 hours agoparentis there a #2 to llamaguard? Meta seems curiously alone in doing this kind of, lets call it, \"practical safety\" work reply suyash 29 minutes agoprevOpen source is a welcome step but what we really need is complete decentralisation so people can run their own private AI Models that keep all the data private to them. We need this to happen locally on laptops, mobile phones, smart devices etc. Waiting for when that will become ubiquitous. reply indus 2 hours agoprevIs there an argument against Open Source AI? Not the usual nation-state rhetoric, but something that justifies that closed source leads to better user-experience and fewer security and privacy issues. An ecosystem that benefits vendors, customers, and the makers of close source? Are there historical analogies other than Microsoft Windows or Apple iPhone / iOS? reply kjkjadksj 2 hours agoparentLets take the iphone. Secured by the industries best security teams I am sure. Closed source, yet teenagers in eastern europe have cracked into it dozens of times making jailbreaks. Every law enforcement agency can crack into it. Closed source is not a security moat, but a trade protection moat. reply itissid 2 hours agoprevHow are smaller models distilled from large models, I know of LoRA, quantization like technique; but does distilling also mean generating new datasets for conversing with smaller models entirely from the big models for many simpler tasks? reply tintor 2 hours agoparentSmaller models can be trained to match log probs of the larger model. Larger model can be used to generate synthethic data for the smaller model. reply userabchn 3 hours agoprevInterview with Mark Zuckerberg released today: https://www.bloomberg.com/news/videos/2024-07-23/mark-zucker... reply Dwedit 36 minutes agoprevWithout the raw data that trained the model, how is it open source? reply wesleyyue 2 hours agoprevJust added Llama 3.1 405B/70B/8B to https://double.bot (VSCode coding assistant) if anyone would like to try it. --- Some observations: * The model is much better at trajectory correcting and putting out a chain of tangential thoughts than other frontier models like Sonnet or GPT-4o. Usually, these models are limited to outputting \"one thought\", no matter how verbose that thought might be. * I remember in Dec of 2022 telling famous \"tier 1\" VCs that frontier models would eventually be like databases: extremely hard to build, but the best ones will eventually be open and win as it's too important to too many large players. I remember the confidence in their ridicule at the time but it seems increasingly more likely that this will be true. reply mav3ri3k 2 hours agoprevI am not deep into llms so I ask this. From my understanding, their last model was open source but it was in a way that you can use them but the inner working were \"hidden\"/not transparent. With the new model, I am seeing alot of how open source they are and can be build upon. Is it now completely open source or similar to their last models ? reply whimsicalism 2 hours agoparentIt's intrinsic to transformers that the inner workings are largely inscrutable. This is no different, but it does not mean they cannot be built upon. Gradient descent works on these models just like the prior ones. reply carimura 2 hours agoprevLooks like you can already try out Llama-3.1-405b on Groq, although it's timing out. So. Hugged I guess. reply TechDebtDevin 2 hours agoparentAll the big providers should have it up by end of day. They just change their API configs (they're just reselling you AWS Bedrock). reply jamiedg 2 hours agorootparent405B and the other Llama 3.1 models are working and available on Together AI. https://api.together.ai reply jmward01 1 hour agoprevI never thought I would say this but thanks Meta. *I reserve the right to remove this praise if they abuse this open source model position in the future. reply throwaway1194 2 hours agoprevI strongly suspect that what AI will end up doing is push companies and organizations towards open source, they will eventually realize that code is already being shared via AI channels, so why not do it legally with open source? reply talldayo 1 hour agoparent> they will eventually realize that code is already being shared via AI channels Private repos are not being reproduced by any modern AI. Their source code is safe, although AI arguably lowers the bar to compete with them. reply amusingimpala75 3 hours agoprevSure but under what license? Because slapping “open source” on the model doesn’t make it open source if it’s not actually license that way. The 3.1 license still contains their non-commercial clause (over 700m users) and requires derivatives, whether fine tunings or trained on generated data, to use the llama name. reply redleader55 3 hours agoparent\"Use it for whatever you want(conditions apply), but not if you are Google, Amazon, etc. If you become big enough talk to us.\" That's how I read the license, but obviously I might be missing some nuance. reply mesebrec 3 hours agorootparentYou also can't use it for training or improving other models. You also can't use it if you're the government of India. Neither can sex workers use it. (Do you know if your customers are sex workers?) There are also very vague restrictions for things like discrimination, racism etc. reply resters 2 hours agoprevThis is really good news. Zuck sees the inevitability of it and the dystopian regulatory landscape and decided to go all in. This also has the important effect of neutralizing the critique of US Government AI regulation because it will democratize \"frontier\" models and make enforcement nearly impossible. Thank you, Zuck, this is an important and historic move. It also opens up the market to a lot more entry in the area of \"ancillary services to support the effective use of frontier models\" (including safety-oriented concerns), which should really be the larger market segment. reply passion__desire 1 hour agoparentProbably, Yann Lecun is the Lord Varys here. He has Mark's ear and Mark believes in Yann's vision. reply ChrisArchitect 2 hours agoprevRelated: Llama 3.1 Official Launch https://news.ycombinator.com/item?id=41046540 reply starship006 2 hours agoprev> Our adversaries are great at espionage, stealing models that fit on a thumb drive is relatively easy, and most tech companies are far from operating in a way that would make this more difficult. Mostly unrelated to the correctness of the article, but this feels like a bad argument. AFAIK, Anthropic/OpenAI/Google are not having issues with their weights being leaked (are they?). Why is it that Meta's model weights are? reply skybrian 2 hours agoparentI think it’s hard to say. We simply don’t know much from the outside. Microsoft has had some pretty bad security lapses, for example around guarding access to Windows source code. I don’t think we’ve seen a bad security break-in at Google in quite a few years? It would surprise me if Anthropic and OpenAI had good security since they’re pretty new, and fast-growing startups have a lot of organizational challenges. It seems safe to assume that not all the companies doing leading-edge LLM’s have good security and that the industry as a whole isn’t set up to keep secrets for long. Things aren’t locked down to the level of classified research. And it sounds like Zuckerberg doesn’t want to play the game that way. At the state level, China has independent AI research efforts and they’re going to figure it out. It’s largely a matter of timing, which could matter a lot. There’s still an argument to be made against making proliferation too easy. Just because states have powerful weapons doesn’t mean you want them in the hands of people on the street. reply dfadsadsf 44 minutes agoparentprevWe have nationals/citizens of every major US adversary working in those companies with looser security practice than security at local warehouse. Security check before hiring is a joke (mostly checks that resume checks out), laptops can be taken home and internal communication are not segmented on need to know basis. Essentially if China wants weights or source code, it will have hundreds of people to choose from who can provide it. reply whimsicalism 2 hours agoparentprevWe have no way of knowing whether nation-state level actors have access to those weights. reply meowface 2 hours agoparentprev>AFAIK, Anthropic/OpenAI/Google are not having issues with their weights being leaked. Why is it that Meta's model weights are? The main threat actors there would be powerful nation-states, in which case they'd be unlikely to leak what they've taken. It is a bad argument though, because one day possession of AI models (and associated resources) might confer great and dangerous power, and we can't just throw up our hands and say \"welp, no point trying to protect this, might as well let everyone have it\". I don't think that'll happen anytime soon, but I am personally somewhat in the AI doomer camp. reply InDubioProRubio 3 hours agoprevCrowdStrike just added \"Centralized Company Controlled Software Ecosystem\" to every risk data sheet on the planet. Everything futureproof is self-hosted and open source. reply tpurves 1 hour agoprev405 sounds like a lot of B's! What do you need to practically run or host that yourself? reply popcorncowboy 2 hours agoprev> Developers can run inference on Llama 3.1 405B on their own infra at roughly 50% the cost of using closed models like GPT-4o Does anyone have details on exactly what this means or where/how this metric gets derived? reply rohansood15 2 hours agoparentI am guessing these are prices on services like AWS Bedrock (their post is down right now). reply PlattypusRex 1 hour agoparentpreva big chunk of that is probably the fact that you don't need to pay someone who is trying to make a profit by running inference off-premises. reply tpurves 1 hour agoprev405 is a lot of B's. What does it take to run or host that? reply danielmarkbruce 49 minutes agoparentquantize to 0 bit. Run on a potato. Jokes aside ~ 405b x 2 bytes of memory (FP16), so say 810 gigs, maybe 1000 gigs or so required in reality, need maybe 2 aws p5 instances? reply pja 1 hour agoprev“Commoditise your complement” in action! reply Oras 3 hours agoprevThis is obviously good news, but __personally__ I feel the open-source models are just trying to catch up with whoever the market leader is, based on some benchmarks. The actual problem is running these models. Very few companies can afford the hardware to run these models privately. If you run them in the cloud, then I don't see any potential financial gain for any company to fine-tune these huge models just to catch up with OpenAI or Anthropic, when you can probably get a much better deal by fine-tuning the closed-source models. Also this point: > We need to protect our data. Many organizations handle sensitive data that they need to secure and can’t send to closed models over cloud APIs. First, it's ironic that Meta is talking about privacy. Second, most companies will run these models in the cloud anyway. You can run OpenAI via Azure Enterprise and Anthropic on AWS Bedrock. reply simonw 1 hour agoparent\"Very few companies can afford the hardware to run these models privately.\" I can run Llama 3 70B on my (64GB RAM M2) laptop. I haven't tried 3.1 yet but I expect to be able to run that 70B model too. As for the 405B model, the Llama 3.1 announcement says: > To support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node. reply GaggiX 2 hours agoprevLlama 3.1 405B is on par with GPT-4o and Claude 3.5 Sonnet, the 70B model is better than GPT 3.5 turbo, incredible. reply aliljet 3 hours agoprevAnd this is happening RIGHT as a new potential leader is emerging in Llama 3.1. I'm really curious about how this is going to match up on the leaderboards... reply whimsicalism 2 hours agoprevOpenAI needs to release a new model setting a new capabilities highpoint. This is existential for them now. reply fsndz 2 hours agoprevSmall language models is the path forward https://medium.com/thoughts-on-machine-learning/small-langua... reply Invictus0 3 hours agoprevThe irony of this letter being written by Mark Zuckerburg at Meta, while OpenAI continues to be anything but open, is richer than anyone could have imagined. reply mensetmanusman 2 hours agoprevIt’s easy to support open source AI when the code is 1,000 lines and the execution costs $100,000,000 of electricity. Only the big players can afford to push go, and FB would love to see OpenAI’s code so they can point it to their proprietary user data. reply LarsDu88 1 hour agoprevObligatory reminder of why tech companies subsidize open source projects: https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ reply 87 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Open-source Linux became the industry standard for cloud computing and mobile devices due to its modifiability, affordability, and advanced features, similar to the expected trajectory of AI.",
      "Meta has released Llama 3.1 405B, the first frontier-level open-source AI model, along with improved 70B and 8B models, emphasizing better cost/performance and suitability for fine-tuning.",
      "Meta collaborates with companies like Amazon, Databricks, and NVIDIA to support developers, aiming to make open-source AI the industry standard, promoting transparency, security, and economic growth."
    ],
    "commentSummary": [
      "Meta has launched Llama 3.1, an open-source AI model, featuring a 405 billion parameter model and enhanced 70 billion and 8 billion parameter models.",
      "This release is perceived as a strategic move to challenge competitors like OpenAI, Google, and Microsoft by offering advanced AI models for free, with restrictions for companies exceeding 700 million users.",
      "Critics argue that without access to the training data and infrastructure, these models are more akin to freeware than genuinely open-source, sparking debate on Meta's true intentions and the broader implications."
    ],
    "points": 767,
    "commentCount": 337,
    "retryCount": 0,
    "time": 1721747321
  },
  {
    "id": 41038552,
    "title": "Kawaii – A Keychain-Sized Nintendo Wii",
    "originLink": "https://bitbuilt.net/forums/index.php?threads/kawaii.6474/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"bitbuilt.net\",cType: 'managed',cNounce: '88577',cRay: '8a7dec77faf02510',cHash: '658e8715b3adb81',cUPMDTk: \"\\/forums\\/index.php?threads\\/kawaii.6474\\/&__cf_chl_tk=8asGbfu15FWnc0yBN8Y7gjrbbtjEFloyARUmFX63QhY-1721761335-0.0.1.1-3924\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '390000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/forums\\/index.php?threads\\/kawaii.6474\\/&__cf_chl_f_tk=8asGbfu15FWnc0yBN8Y7gjrbbtjEFloyARUmFX63QhY-1721761335-0.0.1.1-3924\",md: \"eGpqx4QpK_4w40pjLMwpAcj4y6Rdc_q.0Q2tl0v7GCY-1721761335-1.1.1.1-.yZUhFIZPv1jiRdLewmbkxs58Ko6mchEBxHiQiXdsuRz6THqrdjC5TQICJexFtN5boIYnGPVkN2uU4hmMXyKEvw8LcM8lTi84vJgdMLaQaslthjQvnDiea9N3uTYSb9f9OHhIPYJw9ekpcbIsWWP_Y7tzs5VC0js5XCthcCWvjl3HR0b5Fn4QzZFzJS9fbQOZPIYja4jA2w2TPpkW.KwSab4lPWjaF2wMTDk9jTU6qZ5TVqOi73f7bcqHtLNiJ3aFDo.4GdbZBFaJKtUTTuSieDsAFMazE3ZED8cy7zvtGybpBoE7UhgMlH6USk.7L6Fu2NQ_f.m8AxG1QoBYsE_QvTOHcyNCNikU77gHijdeUn7Ae4PMDprSBtP7qH5irXeRRlQKSaAv5FMYiMN1zzda_QQ6nx.U9qtv0Vu_sYRhGDLO.KX1lnU73T991zfD8xpG_3UXsC8zbuZkX_RX4vYKUFnh_NGmhG7veTfR1J_EkNuvj0nPwPdnx4ucJ0DUCcuBwe_.5o2_yfZZB.bp1eRIGi7bHml2ToC3o1eXUw0vE0Dch2xSTUSiqdpO1sOxP8qO8U4DuIPKYAlmcIRNFHyNRcJh1pXYmNxzsr4hDD28XvfZTgtaflpUBueLHIuptXtdQIkHC4PXZ1x05a0M1x2QpxGCTZjGdBGjtsX5EVhNBW0RQxZrZ50XSLa9HOBtdWluUdYc7I_mhhj.kxpKaGFBONmvlMN0sutkl9ZRaT4ELE6tqJX.D5KtL8lJqAYNXTQ2ONrzeSSAtWMORe8jouZB4X_z6WktbRpXavlUf_2fStJJwb5uyQIT7eAnGVymNoJ81.uULggG1i1jg5dsHwTG4_qObYvvtgfP1aUDR3wzyYAzfSRNwX4fs3T3oTF_eymQK2tB_T4XygB6KD6EhkMU2BQMTrUqutEfHsyfu5O6yceODSaL1BjBf6qeIfcsfzpJzvBfmw3x4qmhiZEvbUG5H9nk7yCMjk38V4V1pJhKAvF0M1gcXsx4png.Z5cZl0wkfn2rpYHbX.zYZVIROgjzZUI9W6ZdhWhUb3reF4sX0AjLbkISKfBnz1JX93CvDqW5uLjkUKL8KWz32F0U_A5mUb8Fyb705fEZemsKKEfDmyQjVFR8jU5b82GmksBuIAfcVuCMyjs2o0oXMYKJZT6zD2tSjp6KoYcmBu0l0ok6NIxz8.80qVIGZmjWSYrJf5wMnqMQUOp6HThelzqGSWSHbOpW0_ioas.6oSEpi7EZhcFNUudtICnP9lBTQugJHT5ZCgIDU90q0tkV8c_J6io4jGxsKwNA1uT1jrEGYlaZrdOjg2XKqFXlm2pw5Gs1KPOqioQeuri7pCI2BwqImll.jn3MqD_A7qoAIrnY3Tz1O2V40f_h17UhvEjyXzxWHuIh8D7DolvmnkNSq5JCNu8gkXJe6R7ACDJeZHacRhItBc\",mdrd: \".HwSa3RLegHsAoGITRmH2QBNParnehtPvTQxgc1QoQA-1721761335-1.1.1.1-6vO9KfQdcie5tX79SK4pjxPN7TjaAg5EquWsJcIW0.EyiSXg4Q6wVoDWoyTuFNNtyc1yrxP8AADMW.6Vx5dlwA6pehCn24REC3KSsUBiuv9r2aLVDwEuA8LJ2vi7pyFgnpcV2N.sx83ZoM8PAee.ZIf1BPtgCYDepJZPHuqo1135qv1bD.eoXeDjCMuY_731oInOIVajY6WP9f5vVgopGE1CYes_6i5oz8LXl_2vIrAcX7VI1s63tuoUFLm8qI33ns2YBwcW7ByWZUaoGpynZZZWIcLqb.bTgV0ImT8hexs8t5dgZKZdB_cE605dZDP1Qq_C.i7PAKq9CxRZZO0mn2NxPxO_umiQH86itNdBYZ1rd79nYn6P6VjW6BnFbFiVNbaig3rZQagwhSo7zy7HBmNYZH3sb1_D1AmmpCykdOROsPqE.prUnWxn.29yQFVIhGrh8NZQGdEkrGK8Lh7QiR_N0JeyjncerlzkEiyhibuOHNBUj0aEzAIpxtslY6_pIoABf6C7J6C8ESqNo2xNXN85xtyIEQpbfgy.CetAUB0aGWJcKzlSkVeNLQh3iaTYUnXcWKQUfTPpUec04cUVulSH78OB_w8mntMMZRkV8_Wqx7Wb8tpJe7qE9WzbCnOcrAlCwclk5ZLvLOgf1bHrJizWPlKK4p8H5laEpBtbbT3EhqF5YMuCUbnNbc5wSa8rfFoaQrn0encOJwjsfhkuq0ZpR_OUJPjn3_6PtB648p013ZNGvUnqwJ7E.oLm4u.AaSnV.UbCEHJqBDTrIeGLi2CuLYztJG5Ki9SJiLAUiiqZaPkOPlJLDVftDNCOXYlV_._HWgkSkbjwKVwKsnR4nUS_hOPh2phZewLT_4t5ORuX1N7Zf5QKhpIVtSRRZ2hZ_vbWgAMpYkmbxIgv0MiTkYB5fW2Xhbn60fseYqy2gVci_l1KQ2.f2djosvMCqo.lNqFCwyl5hXbuNXep8k8SavskYTxEG7rxQFT08ITYu5UwzCysTmV6x3QTc9OXEVRqvF2T931JpOgIJQrrnVFPLY2EYwAGQwaR5oJAsbZA6zuupJuejuXacQVHJ9JdG5Q.QPg7KNJygl_wrwrqVka1LcOS2P8B2qB8tQC.RBPhM99J8tzSjYiQ3NXECU4et.WuNOah_9cccH0MQZnkvK88VrLE0qros15XNFdYv67OZmT6L3sfdPBRU9aMHNL3Zzg6ZYjiyUwLbPzs6ujZV9IEXPv0P7Ac_ot535SeUy2ugYR3u9TfGS95XnHBsOldH6HAq840L2GeIB1kQs5cOhw4Oaw3zSlNeKQCodH2Ws9V_KGQi8dYuTJPZec03CRPuRt9LjvgsvF1lJBRILR99M5Cw2UFim.u.R7QopQBfymwub5Sk70Ow7nqFq3hRlghm1i2ZeB3LMpVKYB8H6HwCE9iMasvl4QEwf53LN7ht2nIWX4hFtG3fKP0uLrgK3VveAOFzwnlvYWK07M8KWF9haXBuHhwTNhD2VcO0Bm2i7rzt5EQyKJUxRhM2fBzYPAiXD_xoKGW95oqV2KhA1zMVGdZ9qCAgKHaLG3_Q.qN0SfnC5KWwjCKY02nfZbaPTF.wXkh1SpJ.1FTc.r1rpUUz_yEaYQQDgRZEym3wa4ROIAZ20Wr13krpWbhIJ9Ch07I_MjkOXov4pLxCPO1cqW8ihP3LJYoVes2kKHDQs77FdU_KfCWoGzarOEpk3WWcigCD8wobX8jt9jY7IwcSQ4hAZDieG2LDn.qp66VX12fnJNmq5.GZY2ADJbZEJR9bcO4cChKlXFUMytHoVAk9aouSgtVgI6rIBp35k0ojsshM6mnwi25KkBbHqvWeKzKtJDl1GiGMKlMyxITS4vnkM3.KYyzt31mZCkXgh8y35jXJpjtqlBfHMCOeayQHWvBeggDjaRTbQUBsYNHsyJIukH0p1TiUXeSDmldhQwmgbuG9MtT4lXhN0Ac4.5nGLb4GZsbzdehIORB8afBTJbR5YKuYP6Y1TA.WHY.jUgwNISXD1YnTjZmr2Csq_cQIXSRYxjq_.jEUkIjC1qroRe4F2YsGor62Um2lC.NoXOw1KtMk1ZVbDcHR55KHaZTOtoEGIDA0zfVy2oDpH7QrYx.YcwSmiHlQUE2p9w0rD64GxundgrlS3RjlAtnyLL_QsIsmq0xKUAXNgrQltj_XUQoIjxztndc_eGEgrK1VIMPBjeh0BRsDJbMdFcaHwnLXdVbUbEugXhYQ472KRFozQmgYQbqto6zXoHOYvnS2vkTgnezR7a6r3kl5okInLTEFwjKFuEKzcIqx1ZCJCwTvgXH7S7SJMv.KRVJr1YWUOqVqjpKN6zlMf4\",cRq: {ru: 'aHR0cHM6Ly9iaXRidWlsdC5uZXQvZm9ydW1zL2luZGV4LnBocD90aHJlYWRzL2thd2FpaS42NDc0Lw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'qhzdjuq5hdf+QZxbdEkHyVu5SvzuorLtSqVad8B4uj+5I8zqwl9J6QzuU2qzE2YVvI4vvGeJOj0YpHJp4cVooXjdLvCPUaJ5PMWUyIMYfvw1puhI8wQrO6Dx3ZN1pO15znL1NXrQgCq4AugCJqZ5p1T4CIked57XzrgFoJJgL3caUaYlFIJUpTIDxt59yA7wYS6NeZDT9sDr06+7OiTkOa3JaexVTwhok3e+sk2rexPnEk0CaeQdiaB1kStcoTfXznlBemNGm5j19nyt8b8+HijOV3rtAI8rFbxgtfrCnfXDHiNnge5T5WR6ybz/Tw8PL5MkANU+iFxStT5LHGGp3DlVmJM/oF0BnWRi1d/7TUheXyB5VLRLbwAafXsaVq00FcAFHKMRUSMb2RjoSC0puS4FVQvd6HSpHvQ9cGBHTV45M3GfGtKUrHOz4G2XsvUPSXmlGSUFGZpaKGi8bO7RPA==',t: 'MTcyMTc2MTMzNS4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'DBfGuJg6YRZslyQ9ICrgdTwwnWMshISkaEWzCp35Vek=',i1: 'fVvOg6nfXdMXnlQ9SDW7oA==',i2: '0r2CyJ3d0CeTC/fY06Zv5Q==',zh: 'Zvma6PVoOSiAK1p5siYRdHjsbXGOgSXxuSx/QGj6pwk=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'HdX1d5Rqjox5KT/exL0BLvloG9zcc0fRww8SHQDrs60=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=8a7dec77faf02510';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/forums\\/index.php?threads\\/kawaii.6474\\/&__cf_chl_rt_tk=8asGbfu15FWnc0yBN8Y7gjrbbtjEFloyARUmFX63QhY-1721761335-0.0.1.1-3924\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=41038552",
    "commentBody": "Kawaii – A Keychain-Sized Nintendo Wii (bitbuilt.net)693 points by realslimjd 23 hours agohidepastfavorite209 comments hatsunearu 22 hours agoThe \"Thundervolt\" reference in that post is a project where they cut up a Wii PCB to leave just the DRAM and the processors on the PCB, and then they slap an external DCDC board on top of that cut up PCB to provide power to it, while also undervolting it since you reduce the IR losses. https://bitbuilt.net/forums/index.php?threads/thundervolt.62... That is pretty insane. reply monocasa 6 hours agoparentAt this point I'm a bit surprised that nobody has created a netlist of the board and simply reinstalled the relevant chips on it. There has to be more density that can be eked out for easier that way than carefully taking a Dremel to an existing board. reply maronato 4 hours agorootparentThere are a few reasons for it: - the cut board is compact enough for most/all hobby projects - you can get Wiis for very cheap nowadays, perhaps cheaper than the parts themselves - the original board makes heavy use of serpentine tracks. If they are not just to equalize track length, it’d be very hard to account for all delays in a redesign. ofc I’m not a part of the community so their reasons might be complete different reply Nition 20 hours agoparentprevHere's some more info on the motherboard and what can be trimmed off and/or replaced: https://bitbuilt.net/forums/index.php?threads/wii-motherboar... reply Eduard 12 hours agoparentprevis there a goal in undervolting? Is it about minimizing the energy consumption of a Wii system? If so, how much did they save? reply Cloudef 12 hours agorootparentThe kawaii forum post says the undervolting allows them to passively cool the wii reply 01HNNWZ0MV43FF 20 hours agoparentprevIR losses? Never heard that one reply 0l 20 hours agorootparentI believe he means I²R losses in resistive elements reply NavinF 14 hours agorootparentNo I think he literally means IR losses. ie voltage droop V=IR Modern VRMs also reduce output voltage when the CPU draws more current. That way when the CPU later draws less current, the voltage doesn't inductively spike up and damage the CPU. Overclockers call this LLC (load line calibration), but don't google that because electrical engineers don't use that term and most articles and reddit threads explain this ass-backwards. Google \"Active Voltage Positioning\" instead to find correct documentation. If your VRM is close to the chip, voltage droop will be ~0 and LLC can be ~0. This allows you to undervolt more and save power without instability. This is probably why most server CPUs have voltage conversion inside the chip (FIVR, Fully integrated voltage regulators) reply mikepurvis 16 hours agorootparentprevWhich would reduce heat and therefore make it easier to cool in a small form factor. reply enragedcacti 22 hours agoprevIn case the scale renderings weren't illustrative, this is just how small the GC Nano is https://www.reddit.com/r/Gamecube/comments/13u8km5/worlds_sm... reply maxglute 11 hours agoparentSeems... large? I was expecting something substantially smaller than a flip foldable phone. reply Nursie 8 hours agorootparentThat's not the Kawaii though, a little under halfway down the linked article, there's a size comparison between the GC Nano and the Kawaii - https://bitbuilt.net/forums/index.php?threads/kawaii.6474/ reply Izkata 5 hours agorootparentThe base is the same size, still quite large for being called \"keychain-sized\". reply skeaker 1 hour agorootparentIt's about the size of just the screen of a GBA. It's shorter than your smartphone. You could absolutely hang that from a keychain at that size, even if it wouldn't really make sense to. reply remram 4 hours agorootparentprevWallet-sized, I guess. 60x60x16mm, so it is 20% smaller than a credit card (in surface area) reply redundantly 14 hours agoparentprevThank you. I was confused by the renders on the page. reply bonney_io 23 hours agoprevIt's crazy that we could now build a Wii that's self-contained within the sensor bar... reply cushpush 22 hours agoparentInstead of a sensor bar you can use two burning candles. reply tomtheelder 22 hours agorootparentYou can what now? reply ladberg 22 hours agorootparentThe sensor bar isn't actually a sensor, just two IR blasters that the cameras on the wiimotes use for positioning. You can use any two sources of infrared light instead! reply chabons 22 hours agorootparentBefore I knew this I had someone pull out their lighter and point the remote at it when our sensor bar died. Took me a little bit to figure it out. reply eloisant 5 hours agorootparentI blame Nintendo for calling their \"2 lights bar\" that doesn't have any sensor a \"sensor bar\". reply burnte 10 minutes agorootparentIt's a bar that the sensors need to find. It's not optimal but it's a fine use. reply hbn 1 hour agorootparentprevIf I were to place you on a team in my company, I'd more likely place you in engineering than marketing :) reply mrguyorama 22 hours agorootparentprevThe sensor bar is ACTUALLY not two IR blasters, but two sets of 5 commodity IR LEDs! https://forums.dolphin-emu.org/Thread-making-a-diy-wiibar reply Klonoar 1 hour agorootparentprevFor fun, go look up Johnny Lee Wiimote candles. It showcases it fairly well. Also man do I feel old - coming up on 15-20 years since that and I actually remember HN discussion about it from the earlier days. reply przemub 22 hours agorootparentprevThe sensor bar is passive - it's just two infrared diodes so the Wiimote can get an idea of its own position. So you can replace it with candles as they emite infrared light as well! reply mattnewton 22 hours agorootparentprevthe \"sensor\" is actually in the remote. The bar is just two infrared leds seperated by a known distance, that the infrared camera in the remote uses to figure out it's position. reply 0x1ch 21 hours agorootparentI play a bit of flightsim and our head tracking works the same way. Camera receives IR LED position for head movement axis, program does the interpretation of movement. reply extraduder_ire 14 hours agorootparentThere's a homebrew head tracking demo [0] for the wii that has you put a sensor bar on your head, and a wiimote on top of your TV. I messed around with it over a decade ago and found it very convincing. 0: https://www.wiibrew.org/wiki/Headtracking reply 20after4 13 hours agorootparentVery convincing indeed. IMO it's almost as good as (if not better than) head mounted VR goggles. At least it doesn't cause motion sickness. The person who came up with that idea, Johnny Lee¹, went on to work on the xbox and I believe was also involved in development of the Kinect. 1. https://www.youtube.com/@jcl5m reply Nursie 17 hours agorootparentprevYeah I was amazed when I first bought a wireless sensor bar, and there was nothing to plug into the console! Turns out all the smarts are in the controllers, the bar is just there to show a couple of fixed points for positioning. reply bena 21 hours agoparentprevThe Wii isn't that huge to start with. You also have to figure the Wii unit houses full optical drive as well. https://www.ifixit.com/Teardown/Nintendo+Wii+Teardown/812 https://guide-images.cdn.ifixit.com/igi/ewv3yZPOujCRpKEj.hug... That's it. And they didn't include the controller ports and other bits. For instance, I don't think it has Bluetooth or WiFi antennas, so it can't connect to Wiimotes or a network. So if you wanted all of that back, it would be a little bigger. But not by much. Probably the size of the Game Boy Advance in the picture. If that. But if all you wanted was Smash Bros on a keychain, here you go. reply Sparkyte 23 hours agoparentprevDon't give Nintendo any more ideas. :P reply mcphage 22 hours agorootparentWhy not? That's a fantastic idea, and I'd love to see Nintendo do that. reply Sparkyte 22 hours agorootparentIt's a running joke the internet has about Nintendo. They will run with ideas and sue you later. reply clemiclemen 3 hours agoprevThis is very impressive but I think Short-Stack [1] is a more impressive project because it is a fully fonctionnal Wii (as in, it works on its own as you would expect from a regular Wii) compared to this one where it needs other accessories to be able to play. [1]: https://github.com/loopj/short-stack previously discussed 3 months ago here: https://news.ycombinator.com/item?id=40071826 reply jonathanyc 3 hours agoparent> fully fonctionnal Wii (as in, it works on its own as you would expect from a regular Wii) compared to this one where it needs other accessories to be able to play. I don’t believe the Wii you linked includes an IR bar, which is what your statement led me to expect. reply LZ_Khan 22 minutes agoprevKind of mad it's not called Kawa-wii reply windowshopping 46 minutes agoprevHow can they make a Nintendo-labeled product like this without being sued? reply VyseofArcadia 22 hours agoprevDoes it count if you need to plug it into an external dock to play? reply Neywiny 21 hours agoparentI'm thinking similarly. But you don't need GameCube controllers to use a wii. I think that's all the dock adds. reply ivanbakel 20 hours agorootparentThe discussion in the forum points out that the Kawaii doesn't come with any wireless capabilities (they're all trimmed off the board), so unless the console is docked, you seemingly can't control it at all. Perhaps you could come up with a separate controller connector that mates with the plugs on the console without the rest of the dock. reply Neywiny 20 hours agorootparentThat's fair. So then yeah I guess the dock is needed. Considering it didn't look that big, if I bought one of these I'd probably want it integrated instead. reply Mogzol 17 hours agorootparentThe point of this project is more about squeezing the Wii into the smallest possible footprint rather than making it convenient to use. If you wanted something similar with ports integrated though, check out the GC Nano (which despite looking like a Gamecube has Wii internals), or Short Stack. GC Nano: https://bitbuilt.net/forums/index.php?threads/gc-nano-the-wo... Short Stack: https://bitbuilt.net/forums/index.php?threads/short-stack-th... reply Andrex 4 hours agorootparentBecause it works on its own, I consider the Short Stack the more impressive project. Both are super exciting though. Maybe a few years down the line, we get some 2\" LCDs integrated or something for truly portable play. reply starkparker 2 hours agorootparentThe \"dock\" could just be a 7\" display, controllers, and a power input. Not fully wireless but certainly handheld. Could even give it a catchy name, like \"Wii You\". reply Nursie 16 hours agorootparentprevI think if you could give the unit power, get a video signal out, and control it somehow, it would feel more complete. It's an amazing little device but if it really can't be used at all without the dock... does it count as the smallest? As someone who is absolutely not part of that scene, I obviously don't get a say in that. reply HanayamaTriplet 20 hours agorootparentprevYou can't use the base unit by itself - according to the specs from the link, the dock has the actual power input and A/V output connectors. reply prmoustache 10 hours agorootparentBasically they cheated. reply djmips 18 hours agorootparentprevDock with USB-C power input, x4 GCC controller ports, composite/component video output, & stereo audio output reply Nursie 17 hours agoparentprevThis is absolutely lovely work, and the whole trim concept is mindblowing. Buuuut yeah I thought similarly - there's no video output, power input or any way to connect controllers without that dock. Compare it to one of the other tiny builds - https://github.com/loopj/short-stack - which seems to support wireless remotes, has HDMI and takes USB-C for power. reply pryelluw 23 hours agoprevThis is just fantastic. I wonder how small older consoles can be these days while still maintaining full hardware compatibility. reply spondylosaurus 23 hours agoparentThe PS2 Ultra Slim is a fun one: https://bitbuilt.net/forums/index.php?threads/ps2-ultra-slim... And it still has the original controller/memory card ports! reply userbinator 15 hours agoparentprevA NES SoC would fit easily within the area of a microSD card containing all ROMs ever published for it, and the embedded controller in the latter would still have a few orders of magnitude more transistors and be faster than it. reply haunter 23 hours agoparentprevSee the R36S clones from China https://www.aliexpress.com/item/1005006152991376.html reply jsheard 22 hours agorootparentIf we're counting emulation they can get even smaller than that, practicality be damned. https://www.funkey-project.com reply pryelluw 21 hours agorootparentprevI already own a miyoo with the emus though I meant something that replicates the original hardware and can run the actual game cartridges/ISOs reply 0cf8612b2e1e 23 hours agoparentprevYou would likely get into “full compatibility” lawyering very quickly. Many of the consoles have weirdo hardware components in some module or another that is still poorly understood. reply whalesalad 23 hours agoparentprevwith FPGA's you can have 100 consoles in one. https://misteraddons.com/ reply pryelluw 23 hours agorootparentThough I might say that’s cheating, it is a welcome solution reply yieldcrv 23 hours agoparentprevvery, could make an adapter dongle for anything requiring pins reply bscphil 23 hours agoprevSo is this project (a) taking the real Wii parts and putting them on a smaller PCB, (b) a different design with a more efficient same-architecture CPU, or (c) an entirely new design that is emulating the Wii hardware? Can the device run the real Wii OS or is it running a replacement OS capable of launching Wii games? reply sspiff 23 hours agoparentIt is based on the Wii Omega trim, which is a cut down original Wii motherboard removing all the non essentials. Some components in this build are reconnected to the board using a flexible PCB connector, but the core is just a cut down OEM Wii board. reply yincrash 22 hours agoparentprevCheck out the short stack GitHub for an overview of how a previous mod was done. Literally chopping up the motherboard to the bare minimum then adding back things with daughterboards https://github.com/loopj/short-stack reply ThrowawayTestr 22 hours agoparentprevThere's a long history of people taking an original Wii motherboard and physically trimming the PCB with rotary tools (or a hacksaw) to put them in smaller enclosures, usually to make them portable. reply grishka 9 hours agoprevFeels like the logical next step would be to ditch the stock motherboard altogether and make a custom one that you transfer the chips onto. reply nsteel 6 hours agoparentThat's https://bitbuilt.net/forums/index.php?threads/nintendo-vegas... I don't think it's worth the huge amount of effort extra compared to a simple trim. reply GrantMoyer 21 hours agoprevFor reference, 60mm is less than the width of even a compact smartphone, and 16mm is 1.5 to 2 times as thick. This thing is tiny. Hell, it has about the same footprint as a gamecube disc. reply bpye 14 hours agoparentIt’s a hair smaller than a stack of 4 UMDs, the media the PSP used. reply zamadatix 7 hours agoparentprevOr for some more common comparables: a bit less than the area of a credit card with a depth slightly less than the width of a dime. reply Reason077 20 hours agoprevThere should be an ongoing contest to see who can produce the smallest functional miniaturisations of Nintendo Wii and other consoles. For science! reply lhnz 23 hours agoprevIs this something you'd need to download and install ROMs to use? reply skeaker 1 hour agoparentNo need to download if you've got physical copies. A hacked Wii (which is simple to set up nowadays) can easily dump your games to a usable legal ROM. reply thenewnewguy 23 hours agoparentprevYou could rip Wii games that you own the physical disk for. reply sam_perez 17 hours agoprevThe name is so close to being truly perfect, but I guess being just a little bit off is a perk here? reply chefandy 17 hours agoparentIt's cute, but personally I'd have gone for WiiChain or Nintendo Wee. reply hencoappel 11 hours agorootparentNot WeeWii? reply chefandy 1 hour agorootparentSolid option! reply IAmPym 18 hours agoprevIt's not called the Kawii? reply Bartkusa 8 hours agoparentI would’ve chosen “Key-wii”. reply efilife 15 hours agoparentprevWhy Kawii? Kawaii is japanese for cute reply bigstrat2003 14 hours agorootparentBecause then it would be a better pun on \"Wii\". I also thought that \"kawaii\" was kind of a missed opportunity, personally. I would've gone for \"kawaii\" or something. reply glandium 12 hours agorootparentI would have gone with Kawawii, which would sound like Fujimori Shinjo's かわうぃーねー reply efilife 7 hours agorootparentprevWhy am I downvoted for asking a question ffs, reddit mentality reply ThrowawayTestr 22 hours agoprevThe Wii has got be the most hacked (literally!) console ever. reply anthk 20 hours agoparentNot even close. That would be the Play Station or the Play Station 2. reply jyrkesh 11 hours agorootparentFor pirating games: PS1, PS2, Dreamcast, for sure For straight up modding: definitely the Xbox. The 007 and Mechwarrior bugs blew everything wide open, and the fact that it was just a PC with real (upgradeable!) storage spawned projects like XBMC, now known as Kodi: https://en.wikipedia.org/wiki/Kodi_(software) And also piracy was rampant, but not the Swapmagic or Modchip kind. You could just upgrade the drive, _backup_ your games on there, and play 'em all of the drive. The Wii and 3DS are also suuuuper open and hackable though. The homebrew scenes on both are incredibly impressive, not to mention the whole ecosystem of full blown launchers and shells and stuff. (Which, now that I think about it, was also a big deal on Xbox.) reply anthk 8 hours agorootparentOn the XBOX, I wish PostmarketOS supported it. I know, x86, not x86_64, but it's still a nice platform to have. With the 128MB addon, Alpine/Linux/PmOS can do tons of things with the forked dillo (light HTTP/SGopherGemini client, a musis/video player with MPV, light office with Abiword/Gnumeric, a rescue system in case of something bad happens on the main PC, retrogaming with emulators, ScummVM (it will work with tinyGL)... reply ThrowawayTestr 16 hours agorootparentprevAre there a lot of mods that literally hack up the mobo? I haven't seen many portable ps2s. reply pessimizer 19 hours agorootparentprevDon't forget the Dreamcast. It got hacked on more than it got played. reply klik99 17 hours agorootparentIt was def the dreamcast - the first model didn't require any hardware, just a burned CD-ROM. It's demise and Segas departure from consoles is blamed on the amount of piracy. A real shame, because it had some great games reply jyrkesh 11 hours agorootparentOne thing I was found interesting about Dreamcast piracy was that everyone was burning them onto 700 MB CD-Rs. But the retail games were actually pressed onto 1GB GD-ROMs: https://en.wikipedia.org/wiki/GD-ROM For a lot of games, it totally didn't matter (shoutout Ikaruga, 38 MBs! https://www.thedreamcastjunkyard.co.uk/2023/03/the-worlds-sm...) But for games that took advantage of the extra 300 MBs, pirates had to use all these tricks to get the game down to a CD-R size. They'd compress assets, compress or sometimes rip out the FMVs...I think they might have even split some games across multiple CDs. That's why DRM cracks me up, the pirates will always figure a way around it one way or the other. (Especially in today's day and age where the live service model is so effective. I'd weep for the AAA single-player game, but I can't remember the last one I played and enjoyed. They've been dead for a long time. Long live the indie single-player game.) reply bpye 14 hours agorootparentprevThe Dreamcast GPU was also really neat. It was tile based and could do order independent transparency! reply th4tg41 17 hours agoprevI’m a complete sucker for retrogaming stuff and I. Want. That. reply jhatemyjob 16 hours agoprevIs there anyone out there that will make a GC Nano for a fee? Don't have time / skillz to do this myself but I want one reply prmoustache 10 hours agoparentI would say that it is just a bad idea for a start unless you plan to use wireless controllers. With cable connected one you are just looking for a console that would be dragged left and right every time you pull a bit with the controller. reply jhatemyjob 3 hours agorootparentThis is an insane response. This does not concern me at all and has nothing to do with what I said. reply latexr 23 hours agoprevUsing Nintendo’s branding in the box seems ill-advised. That’s giving Nintendo more fodder for the eventual lawsuit. reply kyleyeats 20 hours agoparentIt might not work without the Nintendo logo. reply lawlessone 20 hours agorootparentvery funny :) reply wengo314 7 hours agorootparentin case you don't know, some Gameboy games required to have Nintendo logo in the game data as part of copy protection. allegedly that was legal protection against bootlegs. https://www.copetti.org/writings/consoles/game-boy/#anti-pir... Playstation2 used something similar. ( https://github.com/mlafeldt/ps2logo ) I suppose it gave companies in question additional legal leverage - they could not distribute copies of games without violating the trademark laws. reply talldayo 21 hours agoparentprevLawsuit to what? Their CAD files, the build instructions? The board shipped with the Nintendo Wii? reply wyldfire 20 hours agorootparentUsing the word \"nintendo\" on something intended to play any kind of games is trademark infringement. The Kawaii devs likely don't intend to confuse people, but if a consumer saw this product for sale they'd rightly assume it's a Nintendo product. Using a brand name like this just makes things easier when Nintendo attorneys barely have to roll out of bed when sending a cease and desist order. Just call it Kawaii and stay slightly under the radar. Sadly, Nintendo will probably come for you anyways. reply weberer 11 hours agorootparentWell it is a modded Wii. Its not like they're taking some other SoC and putting an emulator on it. reply jrockway 15 hours agorootparentprevMany years ago I bought an Intel processor. It came with a sticker inside a book that was several pages of terms and conditions on what the sticker can be stuck on. Mentioned as something not to do was applying the sticker to a computing device that did not contain an Intel processor, so I immediately stuck it on my Switch. Still not in prison. reply RajT88 17 hours agorootparentprevSome of these console mods only really get sold as kits or products on places like Aliexpress. Needless to say, they are pretty safe from Nintendo. If these guys aren't selling the schematics, and posting them for free, Nintendo has a lot less of a leg to stand on. reply latexr 8 hours agorootparentNintendo is notoriously litigious. It is naive to think you’re “pretty safe” from them. If they want to sue you, they will, and could bankrupt you with the legal fees alone. And they will use the logo as a way in. https://retrocomputing.stackexchange.com/questions/11736/why... reply shakna 21 hours agorootparentprevReusing branding always opens you up to liability. There are a lot of angles that you wouldn't expect, that trademark can be used to attack you with. And Nintendo are very hostile to any and all uses. reply peanutz454 19 hours agorootparentWhile Nintendo might not lose the trademark entirely if they don't sue, they could risk weakening its strength, therefore they have to sue in this case. Consistent inaction against infringers can lead to the public perceiving the trademark as less distinctive. This can make it harder to protect the trademark in the future, and can encouraging further infringement. reply ssl-3 17 hours agorootparentprevIt appears that they're gathering orders for to do a group-buy of a custom-machined aluminum shell for the keychain widget, and that this newly-minted custom-machined hunk of aluminum includes the a replication of the Nintendo logo. That's commerce. Now, obviously: Their target market knows exactly what they're buying, and they aren't going to be confused by any of this at all. But trademark law (and the surrounding case law) may not see it that way. It's easier (and a lot less fear-inducing) to cease-and-desist before Nintendo's IP lawyers send a nastygram than it is to do so afterward. (And in order to keep their trademark intact, they pretty much have to send that nastygram. Trademarks are very much a defend-it-or-lose-it thing.) --- \"Sorry guys, the first order had to be scrapped along with all of the money we collected and spent on it. If anyone is still interested, the price is still $55 for a shell without the logo if we can get another 30 orders in again.\" reply root_axis 21 hours agorootparentprevUse of Nintendo's trademarked branding. reply pininja 20 hours agoparentprevI wonder if there’s a reusable Nintendo logo they could extract from the Wii enclosure? It’s incredible how upcyclable the Wii is. reply latexr 8 hours agorootparentThat makes zero difference. You aren’t suddenly allowed to use someone else’s branding just because you’re reusing a piece of branding from a product. reply notum 22 hours agoparentprevIsn't this using Nintendo hardware as well? I thought that was the point of these minification projects. reply numpad0 21 hours agorootparentDoesn't matter. Reselling a modified brand product can count as counterfeiting. Legal conditional checks don't always coincide with human instinctive one, law is code too after all. reply rustcleaner 21 hours agorootparentThis is why juries must be instructed on nullification. It's The People's protection against money's use of criminal lawfare. reply PhasmaFelis 18 hours agorootparentHistorically, it's also been very useful when you want to murder a black person and get away with it in front of an all-white Southern jury. Nullification is no more inherently righteous than a butcher knife. reply autoexec 3 hours agorootparentUltimately it just means that we have a way to make sure that We the People aren't being punished by laws that we don't consent to being held to. It puts power directly into the hands of the typical American citizen, which is why our legal system is terrified of it. You don't have to be rich or well-connected to sit on a jury. It also effectively limits what can be done using that power to what a \"random\" (and presumably representative) selection of the community agrees to. That's what a \"jury of your peers\" was supposed to be all about. I'd say that nullification makes it possible for people to truly govern themselves and that makes it an inherently righteous system. It's the righteousness of the people who make up a community that is questionable, but even imperfect people deserve democracy and the right to self-govern. reply theultdev 16 hours agorootparentprevWhat you are referencing is an edge case (and an old one at that). A more recent one is the OJ trial. But those are perfect examples of bad jurors. It's up to you and your peers to be good jurors. What system do you suggest? reply PhasmaFelis 14 hours agorootparentIt's all an edge case. Nullification isn't an intended right, it's an unavoidable loophole. It's the necessary consequence of a system where no one is allowed to tell a juror how to vote or demand that they justify their decision: there's no way to maintain those requirements and also punish jurors for ignoring the law completely, so we just ask them to pretty please not do that. And that's fine. It's certainly better than letting anyone legally pressure jurors. Democracy and freedom are all about compromise. I'm just saying, it's not corruption for judges to prefer jurors who don't ignore the law. reply pbj1968 18 hours agorootparentprevAh yes, the delicious false equivalence. reply resters 23 hours agoprevMaking video games fun does not require anywhere near as much hardware as we typically use in modern systems. I look forward to an eventual return to fun video games. reply conradev 23 hours agoparent\"Yokoi said 'The Nintendo way of adapting technology is not to look for the state of the art but to utilize mature technology that can be mass-produced cheaply.' He articulated his philosophy of 'Lateral Thinking of Withered Technology' (枯れた技術の水平思考, Kareta Gijutsu no Suihei Shikō) (also translated as 'Lateral Thinking with Seasoned Technology'), in the book Yokoi Gunpei Game House.\" https://en.wikipedia.org/wiki/Gunpei_Yokoi#Design_philosophy reply pbj1968 19 hours agorootparentAnd then he got drunk and walked in front of a car. reply sanj 18 hours agorootparentThis is in poor taste. reply astrange 18 hours agorootparentprevThat's the road design's fault, not his. Japan had a very very high rate of pedestrian accidents back then, they fixed it, and they didn't do it by drinking any less or losing pedestrian right of way. reply thrdbndndn 17 hours agorootparentNot that matters, but according to Wikipedia he was killed by a passing car when inspecting an previous incident, presumably on road. Saying it was \"road design's fault\" or even implying he was a \"pedestrian\" in this context is kinda weird without any further explanation. reply whalesalad 23 hours agoparentprevNintendo has been doing this ... forever? The switch is ancient tech, and was outdated the moment it was released. reply segasaturn 22 hours agorootparentActually Nintendo consoles used to be powerhouses until recently. The NES, SNES, N64 and GameCube were all considered state-of-the-art in terms of performance. It wasn't until the Wii when they began cutting down on performance in favor of fun features like they had been doing in the handheld space. reply Laremere 22 hours agorootparentThat doesn't match my recollection. The Gameboy is a early counter example: it was black and white during a time where the game gear had color, yet the Gameboy was far more popular. Also I believe the Xbox was more powerful than the GameCube. reply einr 21 hours agorootparentThe Game Gear didn't come out until one and a half year later. It's easy to see how it wasn't even remotely practical to release a color handheld system in 1989, and it's easy to argue that it wasn't practical in 1990 either, but Sega did it anyway. So when the Game Boy came out it was easily the most powerful handheld system on the market (admittedly by virtue of being essentially the only one worth mentioning) reply lapetitejort 20 hours agorootparentThe Atari Lynx came out a few months after the Game Boy with a backlit color screen reply nilamo 19 hours agorootparentAnd we're all still talking about that one regularly... reply barbecue_sauce 18 hours agorootparentI remember thinking the commercials made it look cool when I was 4 or 5 (I vividly remember some sort of surfing game), but then I never encountered a single person who owned one. Same with the TurboGrafx-16. reply JNRowe 17 hours agorootparent\"Some sort of surfing game\" immediately screams California Games¹ to me. There was surfing² plus a few other sports, and it is still good fun if you find yourself at a museum/nerd house that has one. ¹ https://en.wikipedia.org/wiki/California_Games ² https://www.youtube.com/watch?v=ql2S-wXa-H8 reply imp0cat 14 hours agorootparentprevAlso, the color screen on the Game Gear wasn't that great and the battery life was terrible. I think Sega had realized this, because later on they were selling an external battery pack as an official accessory. reply mejutoco 22 hours agorootparentprevI remember kids with the game gear. Hardly ever saw them playing because of the batteries. For a portable console I think it was a choice on battery life. reply tadbit 21 hours agorootparentThe game gear was extremely lousy to use. Too small of a screen, ate through batteries incredibly quickly, the original, external battery pack (not included) was poorly made and didn't help that much either. And the game selection early on was pretty lousy too. Sonic was only fun for a while. People are doing amazing things with game gear hardware as of late, though. All of that addressed spectacularly. reply barbecue_sauce 18 hours agorootparentAlso huge in size compared to the screen dimensions. Could barely get my hands around it as a little kid. Then the Nomad was even bigger! reply stavros 21 hours agorootparentprevAnd it only took thirty-five years! reply imp0cat 14 hours agorootparentprevA \"wall wart\" power source was a necessity. reply callalex 20 hours agorootparentprevThe game boy got almost 30hours out of 4xAA whereas the game gear got about an hour or two of life out of 6xAA. I hated that about the game gear and it meant I hardly ever got to play it. reply hansoolo 20 hours agorootparentprevI found my Gameboy recently, but I did not find my games... Sad times... reply Keyframe 22 hours agorootparentprevEspecially N64 - SGI indy in a small box. They did change the narrative after they couldn't or wouldn't compete on those numbers (rightfully so it turned out), however, they were always experimenting with controls and were highly influential in doing so. appropriate username, btw, but that console is for another topic! reply to11mtm 21 hours agorootparentprevNES? Yes. SNES... Somewhat? I think there were tradeoffs here between that and the genesis; You got more colors and could get better sound out of the SNES... On the flip side people did -amazing- things with the YM2612 and for all the SNES RPG Soundtracks I love, they don't slap like the Streets of Rage series or Sanic. N64 had pretty good perf but the Cartridge format made it -very- expensive to do anything very fancy; this is one of the reasons that lots of folks feel PS1 had better looking games despite N64's superior specs. GameCube... Sits in a very weird spot IMO, but that whole generation was a bit Zany due to how everyone was experimenting with different 'paths to faster/better 3d'. Dreamcast had lots of 'special' stuff, GC was unique in it's own right, PS2's biggest stumble IIRC was too little ram for the GS... To me, the bigger 'paradigm shift' that Nintendo made with the Wii was preferring more COTS-y stuff versus more special custom things... NES had the Special Ricoh 6502 variant. SNES had the SPC. N64... TBH was mostly SGI based so possibly the exception. Gamecube had a custom GPU (Flipper)... Wii is for the most part an 'incremental' upgrade from GC Hardware, and the Switch uses a not-that-special Tegra AFAIK. reply blkhp19 21 hours agorootparentprev\"recently\" as in nearly 25 years ago reply Andrex 16 hours agorootparentYes, in terms of their video game history, Nintendo has been blue ocean (2004-2024+) longer than they were red (1983-2003). reply ssl-3 17 hours agorootparentprevRecently? My dude, the GameCube was released nearly 23 years ago. There is a wider time delta betwixt the GameCube's release and today than there is between the NES and the GameCube. reply andrepd 19 hours agorootparentprevNintendo's first example of this is probably the most famous: the Gameboy was very underpowered compared to its competitor and absolutely trounced them on its way to become a household name and one of the most popular consoles of all time. reply ekianjo 20 hours agorootparentprevthe Gamecube certainly not. it was on par with other consoles of the time but released later so nothing that you could call SOTA reply Frenchgeek 22 hours agorootparentprevPretty sure the NES was designed to a price point first and foremost. Especially after the video game crash. Hence the dirt-cheap 6502 derivative in it. reply einr 21 hours agorootparentThe NES -- as far as its basic hardware architecture -- was not designed for a market where the video game crash had even occurred. It was designed for release in Japan in 1983 as the Famicom, undoubtedly the most powerful console in the market at the time -- a time where by the way I'm not sure what else you would even put in a console other than a 6502 or Z80. If you wanted cheap above all, you could have gone for a plain 6502 or a cut-down variant (like the 6507 in the Atari VCS), but they also didn't do that -- the Ricoh 2A03 is a custom part that includes custom sound hardware. reply monocasa 10 hours agorootparent> If you wanted cheap above all, you could have gone for a plain 6502 or a cut-down variant (like the 6507 in the Atari VCS), but they also didn't do that -- the Ricoh 2A03 is a custom part that includes custom sound hardware. The higher integration on a single chip for the 2A03 was absolutely a cost saving move. reply 0cf8612b2e1e 23 hours agorootparentprevSwitch could definitely have used more oomph. Many frame rate drops in the Zelda games. Many emulators claim to have the superior experience with those games. reply ThatMedicIsASpy 23 hours agorootparentWhich is correct. Plus a Wii game with 4k texture packs will look better than any HD remake reply klodolph 22 hours agorootparentI am deeply unimpressed with most of the 4K texture packs out there. I see a lot of this: https://twitter.com/letofski/status/982947652072488962 reply andrepd 19 hours agorootparentSame. Just rendering at 1080p / 4k is good enough to give most titles a nicer shine though :) reply alliao 19 hours agorootparentprevswitch was ancient tech, but still predates usb-c enough that they're rolling their own power protocols.. hence deluge of broken switch on ebay with fried usb-c ports... reply Andrex 16 hours agorootparentSwitch in no way predated USB-C, even talking widespread support... Nintendo rolled their own protocol because they could and USB allows for it. reply latexr 23 hours agoparentprevFun video games never went away. Look for games by indie developers instead of AAA titles. reply BiteCode_dev 21 hours agorootparentIt's actually a golden age for fun video games, because we are swimming in new beautiful, engaging, original titles every year. Some things really take you by surprise as well. I never saw Inscryption, Disco Eliseum or Hades coming, and I think nobody did. And even oldish games still have great value. I still play LoL or Isaac, and they are as good as they were on day 1. Plus, you get the Switch then the Deck refreshed portable gaming experience. The latter made emulation so nice as well. With terrific communities, insane speed runners, devs coming up with crazy new concepts and hardware that never stop to get better, it's hard to complain except that with a busy life, you will see only 1% of those masterpieces. reply latexr 17 hours agorootparent> I never saw (…) Hades coming, and I think nobody did. I don’t think Hades came as a surprise to anyone who was already a fan of the devs from Bastion and Transistor. reply BiteCode_dev 4 hours agorootparentCome on, Bastion is nice but nowhere as sophisticated as Hades. Neither the gameplay nor the replayability would have let you think the team had the ability at the time. As for transistor, the story is basically \"futuristic world is being destroyed by virus-type-invaders and your sword/companion is the key to beating it\", with a predictable end and almost zero character dev. Being able to make ok games doesn't translate to the skill to make a masterpiece. It was a quantum leap. It would be like saying you can deduce Divinity Original sin 2 would be amazing because you played the first one. reply tines 19 hours agorootparentprevInscryption is a must play. reply BiteCode_dev 8 hours agorootparentI will remember it forever, it's a unique experience. But it's such a weird combination of aesthetic, story telling and gameplay I have to assume it prevents a huge part of the gaming population from enjoying it. If anybody read those comments, DO NOT LOOK THE GAME UP if you plan to play it. Go blind. reply tines 3 hours agorootparentI binged it during the time I was trapped in my room with COVID. You're right, it is a very weird game(s?) in the best way, it's literally sent me off on a card game design jaunt that's still ongoing haha. And I found myself loving the characters of the, what was it, ocelot and the wizard apprentice who is glad to have any kind of STIM-U-LA-TION? reply anal_reactor 17 hours agorootparentprevI disagree. Sure, there are fun games, but they're so hard to find among all the crap reply BiteCode_dev 9 hours agorootparentThe noise/signal ratio is worse for everything today: movies, music, tv shows. But \"finding good games requires a tiny bit of effort to me\" is a first-world problem. reply haunter 23 hours agorootparentprevThere are many fun AAA titles, more than one can play reply latexr 22 hours agorootparentThe conversation’s context is fun games without needing the latest hardware. reply randac 17 hours agorootparentLook at anything from publisher New Blood Interactive on Steam for a starting point. Mostly retro style FPS from differing eras, but there are a few other game types. Plus you'll struggle to find any that don't have thousands of user ratings in either very positive or overwhelmingly positive brackets. Gloomwood (first person stealth) and Fallen Aces are a couple of gems still in early access. reply hiccuphippo 20 hours agorootparentprevA lot of fun old AAA games run on potatoes. And there's so many of them that you won't have issues finding something new to you. reply Eji1700 22 hours agorootparentprevThe majority of my indie titles run on a potato. reply n_plus_1_acc 21 hours agorootparentMany indie games use Unity and have terrible performance. Source: I have a potato (by which i mean i use the integrated graphics of an i7-56xx) It can run many games well, so it depends how much developers value performance. reply barbecue_sauce 18 hours agorootparentAs a player, I do not really value performance unless we're talking sub 25fps. reply account42 9 hours agorootparentPerformance is not just a simple number. 25 FPS with good frame pacing is much more enjoyable than something that averages 60 FPS but with individual frame times all over the place. That said, for first-person action games especially on a non-tiny monitor, anything below ~40 FPS will be noticeably non-smooth. Other game types have more tolerance, e.g. a top down strategy game could still be playable at ~15 FPS. reply n_plus_1_acc 9 hours agorootparentprevSame. But some titles have like 3fps (Train Valley World for example) reply squeaky-clean 19 hours agorootparentprev> I look forward to an eventual return to fun video games They weren't saying they wanted games that run on old hardware. It's just the trope of \"back then hardware was bad and games were good. Now hardware is good and games are bad.\" reply rjh29 20 hours agoparentprevRay tracing might be eye candy, but fast streaming of assets from SSD enables experiences not possible before (large scale open world, super fast movement a la Spiderman, instant teleportation). GPU-powered dynamic lighting and LOD is also pretty crazy. reply solardev 23 hours agoparentprevIt wasn't clear from your post, but have you kept up with the PC indie scene of the last decade or so? There's a lot of great small gems on Steam these days that can run on old hardware (or the Deck). But apparently the golden age is ending, as big publishers this year and last canceled a lot of projects and closed a bunch of studios. Sad, but there's still a huge backlog of great titles to go through. reply roxil 3 hours agoparentprev\"I want shorter games with worse graphics made by people who are paid more to work less and I'm not kidding,\" It's been a meme for a while and I unironically agree. reply agumonkey 23 hours agoparentprevWell said. Some ingredients that were in old games has vanished due to the post 2000 culture, but we can go back. reply mcphage 22 hours agorootparent> Some ingredients that were in old games has vanished due to the post 2000 culture Hmm, like what? reply agumonkey 21 hours agorootparentOne factor (surprisingly I've seen this mentioned by a video game guy on youtube few years ago) is the disbelief made by non game visual art. Game boxes, booklets, they bootstraped the imagination. Handmade art was 100x more detailed than 8bit games yet we didn't care having a low res 8bit characters because we were already mentally in the world displayed on paper. I do sincerly miss the limited rendering aspect of old titles. The limitations gave ways to a distinct style, and kept the game a game, in a strange world. It also provided you with some surprises.. how did they manage to pull off some effect on a tiny 8 or 16bit machine. Hardware of today removes that wonder. There's less contrast. reply kchr 20 hours agorootparentThe limitations of old game platforms didn't vanish, they are still used (and being re-discovered by new generations). One of my favorite games on this side of the new millennium is Celeste, for example. Some indie studios are even producing new games for GBA, GB, NES and other platforms from the 90s, sometimes including booklet and packaging! reply roywiggins 4 hours agorootparentThere's been a bit of a \"PS1 aesthetic\" enthusiasm recently too. reply agumonkey 20 hours agorootparentprevAh fine, I lost track of the indie space. I shall resume. reply smolder 7 hours agorootparentprev> the disbelief made I assume you mean the suspension of disbelief? I.e. immersion. Suspension is a key word there, as in, your disbelief is halted, allowing you to be immersed. reply opan 8 hours agorootparentprevYou may find TIC-80 interesting. reply raytopia 19 hours agoparentprevFor handhelds I'd say the Playdate [0] does this pretty well. Lots of fun and very experimental indie games. For home consoles I hope a single board computer flls this role one day. In fact I've been experimenting with the raspberry pi to try and turn it into a console for new games but just haven't spent enough time on the project yet. [0] https://play.date/ reply haunter 23 hours agoparentprevThe problem is when even Nintendo’s own first party titles are struggling with the hardware. That wasn’t that common with the Wii, 3DS, or previous consoles but very very very noticeable on Switch reply mrguyorama 22 hours agorootparentSuper Mario 64 had abysmal performance for a title on the N64 that wasn't even that complicated compared to things that would later release on the console. But in the 90s, when you got home with your very first device capable of rendering \"real time\" 3D graphics for $200, you didn't really care that \"real time\" meant 12fps at times. We used to have pretty low standards for framerate. reply scns 21 hours agorootparentOne guy optimized Mario 64 to run at 60FPS: https://www.youtube.com/watch?v=t_rzYnXEQlE reply anthk 20 hours agorootparentMario 64 on the N64 was build without -O2 flags. Maybe with -O0 or even -g. After a simple compiler switch, the speed skyrocketed. reply syndeo 20 hours agorootparentAnd from what I understand, it’s not due to incompetence; rather, it’s due to not yet having confidence that those optimizers wouldn’t introduce bugs. The SDK and toolchain were very new; SM64’s development itself parallels that of the dev toolchain. So, better safe than sorry, especially with a pack-in launch title. reply ThatPlayer 16 hours agorootparentprevKaze, who did that 60FPS optimization, has commented on other videos about how the CPU isn't fully utilized anyways, so -O2 doesn't make too much a difference in most scenarios. https://www.youtube.com/watch?v=9_gdOKSTaxM&lc=UgyhTG4Ol46Rr... His comment, not the video. reply jmdots 17 hours agorootparentprevI forgive them for abysmal for one of the first games released for a wildly new platform for them. reply lawlessone 20 hours agoparentprevGood point. Most of the games I have played in recent years have been indie titles. Sometimes they are CPU intensive but rarely GPU intensive. It feels like graphics in games have reached a sort of plateau now where the most visually realistic games are only marginally more realistic looking than something from nearly 10 years ago. reply ralusek 23 hours agoparentprevInscryption Subnautica Satisfactory Factorio Hollow Knight RE7 Baba is You Baldur's Gate 3 Elden Ring Dead Cells Hades Ori and the Will of the Wisp Disco Elysium Dishonored 1 & 2 Orcs Must Die Planet Coaster Portal 1 & 2 Read Dead Redemption 2 Valheim I dont' know what you mean by \"modern,\" but these were all games I enjoyed recently-ish, and I'm sure I forgot some. reply jacoblambda 22 hours agorootparentOff the top of my head I'd say to throw in Outer Wilds (wilds, not worlds), Tunic, The Riven remake, and The Talos Principle 1 & 2 as well. reply vmladenov 6 hours agorootparentThe Talos Principle is phenomenal, fantastic effort by Croteam reply jacoblambda 8 minutes agorootparentYeah. And tbh while I enjoyed the first game quite a bit I was absolutely blown away by the second game. Visuals, OST, all the vibrant characters were absolutely breathtaking. reply BolexNOLA 20 hours agoparentprevA story does not require a bunch of words either but there are a lot of great, long books. There are also great short stories. Same thing goes for games that demand high performance rigs. It’s all about what you want in the end, and there’s no single answer for what makes a game fun. Some people really like beautiful, realistic looking games with high resolutions and frame rates. To them that is fun. reply parl_match 22 hours agoprevI'm looking for recommendations for a 30~50 run anodized aluminum case, in a similar size as the Kawaii. Does anyone have any recommendations? The quotes I'm getting are closer to $95/pc and that seems quite high. reply ryukoposting 21 hours agoparentThe price for a small-batch run is going to depend heavily upon how difficult it is to manufacture at small scale. If you got that $95 quote from a local shop, you can try asking them what you can do to make it cheaper. There might be some tricky features in your design that are jacking up the labor costs. The cheapest way to make a small-batch aluminum enclosure is probably to base it off an off-the-shelf extrusion stock. I'd go on McMaster and find some C-channel stock that fits my needs, then I'd design a base plate that nests inside the C-channel. If you're trying to go for an upscale, professional look, you can have the machine shop run a wire wheel over the C-channel before anodizing it. reply frickinLasers 14 hours agoparentprevHave you tried https://www.xometry.com/ ? They have a network of vetted shops who bid on jobs when otherwise unoccupied. reply Razengan 11 hours agoprev [–] +99 points for the name reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The \"Thundervolt\" project involves modifying a Nintendo Wii by trimming down its PCB (Printed Circuit Board) to retain only essential components like DRAM (Dynamic Random-Access Memory) and processors, and adding an external DCDC (Direct Current to Direct Current) board for power.",
      "The project aims to create a keychain-sized Wii, named \"Kawaii,\" which is smaller than other miniaturized versions like the GC Nano, but still requires a dock for full functionality, including power input and controller connections.",
      "This project highlights the ongoing interest and innovation in retro gaming and console miniaturization, showcasing the community's dedication to preserving and enhancing classic gaming hardware."
    ],
    "points": 693,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1721675542
  },
  {
    "id": 41039967,
    "title": "Timeshift: System Restore Tool for Linux",
    "originLink": "https://github.com/linuxmint/timeshift",
    "originBody": "Timeshift Timeshift for Linux is an application that provides functionality similar to the System Restore feature in Windows and the Time Machine tool in Mac OS. Timeshift protects your system by taking incremental snapshots of the file system at regular intervals. These snapshots can be restored at a later date to undo all changes to the system. In RSYNC mode, snapshots are taken using rsync and hard-links. Common files are shared between snapshots which saves disk space. Each snapshot is a full system backup that can be browsed with a file manager. In BTRFS mode, snapshots are taken using the in-built features of the BTRFS filesystem. BTRFS snapshots are supported only on BTRFS systems having an Ubuntu-type subvolume layout (with @ and @home subvolumes). Timeshift is similar to applications like rsnapshot, BackInTime and TimeVault but with different goals. It is designed to protect only system files and settings. User files such as documents, pictures and music are excluded. This ensures that your files remain unchanged when you restore your system to an earlier date. If you need a tool to back up your documents and files please take a look at the excellent BackInTime application which is more configurable and provides options for saving user files. History Timeshift was originally developed and maintained by Tony George. His original repository is still available on Github. Nowadays Timeshift is part of the Xapp project which is a collection of cross-DE and cross-distributions applications which are maintained by Linux Mint. Features Minimal Setup Timeshift requires very little setup. Just install it, run it for the first time and take the first snapshot. Cron job can be enabled for taking automatic snapshots of the system at regular intervals. The backup levels can be selected from the Settings window. Snapshots are saved by default on the system (root) partition in path /timeshift. Other linux partitions can also be selected. For best results the snapshots should be saved to an external (non-system) partition. Multiple Snapshot Levels Multiple levels of snapshots can be enabled - Hourly, Daily, Weekly, Monthly and Boot Number of snapshots to retain can be specified for each level Boot snapshots provide an additional level of backup and are created every time the system starts. Boot snapshots are created with a delay of 10 mins so that system startup is not affected. Snapshots are tagged to indicate their time interval: H: Hourly D: Daily W: Weekly M: Monthly B: Boot O: On-demand (Manually created) Rsync & BTRFS Snapshots Supports rsync snapshots on all systems Supports BTRFS snapshots on BTRFS systems It is strongly recommended to use BTRFS snapshots on systems that are installed on BTRFS partition. BTRFS snapshots are perfect byte-for-byte copies of the system. Nothing is excluded. BTRFS snapshots can be created and restored in seconds, and have very low overhead in terms of disk space. User Data is Excluded by Default Timeshift is designed to protect system files and settings. It is NOT a backup tool and is not meant to protect user data. Entire contents of users' home directories are excluded by default. This has two advantages: You don't need to worry about your documents getting overwritten when you restore a previous snapshot to recover the system. Your music and video collection in your home directory will not waste space on the backup device. You can selectively include items for backup from the Settings window. Selecting the option \"Include hidden items\" from the Users tab will back up and restore the .hidden files and directories in your home folder. These folders contain user-specific config files and can be included in snapshots if required. Note: It is not recommended to include user data in backups as it will be overwritten when you restore the snapshot. Unlike similar tools that are scheduled to take backups at a fixed time of the day, Timeshift is designed to run once every hour and take snapshots only when a snapshot is due. This is more suitable for desktop users who keep their laptops and desktops switched on for few hours daily. Scheduling snapshots at a fixed time on such users will result in missed backups since the system may not be running when the snapshot is scheduled to run. By running once every hour and creating snapshots when due, Timeshift ensures that backups are not missed. Applications like rsnapshot rotate a snapshot to the next level by creating a hard-linked copy. Creating a hard-linked copy may seem like a good idea but it is still a waste of disk space, since only files can be hard-linked and not directories. The duplicated directory structure can take up as much as 100 MB of space. Timeshift avoids this wastage by using tags for maintaining backup levels. Each snapshot will have only one copy on disk and is tagged as \"daily\", \"monthly\", etc. The snapshot location will have a set of folders for each backup level (\"Monthly\", \"Daily\", etc) with symbolic links pointing to the actual snapshots tagged with the level. System Restore Snapshots can be restored by selecting a snapshot from the main window and clicking Restore button on the toolbar. Snapshots can be restored either from the running system (online restore) or from another system that has Timeshift installed on it (offline restore). If the main system is not bootable, then it is possible to boot from an Ubuntu Live CD, install Timeshift on the live system, and restore a snapshot on the main system. Restoring backups from the running system requires a reboot to complete the restore process. Cross-Distribution Restore You can also Timeshift across distributions. Let's say you are currently using Xubuntu and decide to try out Linux Mint. You install Linux Mint on your system and try it out for a week before deciding to go back to Xubuntu. Using Timeshift you can simply restore the last week's snapshot to get your Xubuntu system back. Timeshift will take care of things like reinstalling the bootloader and other details. Since installing a new linux distribution also formats your root partition you need to save your snapshots on a separate linux partition for this to work. It is recommended to include hidden items in home directory by selecting the option \"Include Hidden Items\" from Settings > Users. Post Restore Hooks Scripts can be run at the end of a restore job for anything that may need to be done prior to rebooting. The location for these scripts is /etc/timeshift/restore-hooks.d. Note: the script(s) will be run from the restored filesystem. Supported System Configurations Normal - OS installed on non-encrypted partitions LUKS Encrypted - OS installed on LUKS-encrypted partitions LVM2 - OS installed on LVM2 volumes (with or without LUKS) BTRFS - OS installed on BTRFS volumes (with or without LUKS) Only Ubuntu-type layouts with @ and @home subvolumes are supported @ and @home subvolumes may be on same or different BTRFS volumes @ may be on BTRFS volume and /home may be mounted on non-BTRFS partition If swap files are used they should not be located in @ or @home and could instead be stored in their own subvolume, eg @swap Other layouts are not supported Make sure, that you have selected subvolume @ or /@ for root. You can check that executing script below, and if output is OK, then everything is alright. grep -E '^[^#].+/\\s+btrfs' /etc/fstab\\ grep -oE 'subvol=[^,]+'\\ cut -d= -f2\\ grep -qE '^/?@$' && \\ echo 'OK' || \\ echo 'Not OK' Default BTRFS subvolume must be /. You can make it using script below. MP=\"$(mktemp -d)\" mountawk '/on \\/ type btrfs/{print $1}'sudo xargs -I{} mount {} \"$MP\" && \\ sudo btrfs subvolume set-default 5 \"$MP\"; \\ sudo umount \"$MP\" GRUB2 - Bootloader must be GRUB2. GRUB legacy and other bootloaders are not supported. EFI - EFI systems are supported. Make sure that /boot/efi partition is selected for mounting before restoring snapshots (application will do it automatically). Encrypted Home - For users with encrypted home, files in /home/.ecryptfs/$USER will be backed-up and restored. The decrypted contents in $HOME will be excluded. This avoids the security risk of decrypted contents becoming available outside the user's home directory. Encrypted Private Directory - For users with encrypted Private directory, the encrypted files in $HOME/.Private, as well as the decrypted files in $HOME/Private, will be excluded (as it contains user data). Filters added by user to include files from $HOME/.Private or $HOME/Private will be ignored. Docker & Containers - Docker and containerized systems are not supported. Running Timeshift on such systems will have unpredictable results. Installation Building and Installing from Source Code You can find the exact instructions in the development docs. Debian-based Distributions Debian, Ubuntu, Linux Mint, Elementary OS, etc. Install Timeshift from the repositories: sudo apt-get update sudo apt-get install timeshift Fedora Fedora is not fully supported. BTRFS snapshots only support Ubuntu-specific layouts. sudo dnf update sudo dnf install timeshift Arch sudo pacman -S timeshift Removal Run the following command in a terminal window: sudo apt-get remove timeshift or sudo dnf remove timeshift or sudo pacman -R timeshift depending on your package management system. Remember to delete all snapshots before un-installing. Otherwise the snapshots continue to occupy space on your system. To delete all snapshots, run the application, select all snapshots from the list (CTRL+A) and click the Delete button on the toolbar. This will delete all snapshots and remove the /timeshift folder in the root directory. Known Issues & Limitations BTRFS volumes BTRFS volumes must have an Ubuntu-type layout with @ and @home subvolumes. Other layouts are not supported. Systems having the @ subvolume and having /home on a non-BTRFS partition are also supported. Text file busy / btrfs returned an error: 256 / Failed to create snapshot can occur if you have a Linux swapfile mounted within the @ or @home subvolumes which prevents snapshot from succeeding. Relocate the swapfile out of @ or *@home, for example into it's own subvolume like @swap. Disk Space Timeshift requires a lot of disk space to keep snapshot data. The device selected as snapshot device must have sufficient free space to store the snapshots that will be created. If the backup device is running out of space, try the following steps: Reduce the number of backup levels - Uncheck the backup levels and keep only one selected Reduce the number of snapshots that are kept - In the Schedule tab set the number of snapshots to 5 or less. You can also disable scheduled snapshots completely and create snapshots manually when required Bootloader & EFI Only those systems are supported which use GRUB2 bootloader. Trying to create and restore snapshots on a system using older versions of GRUB will result in a non-bootable system. EFI systems are fully supported. Ensure that the /boot/efi partition is mapped while restoring a snapshot. It will be mapped automatically if detected. If you are restoring from Live CD/USB, and your installed system uses EFI mode, then you must boot from Live CD/USB in EFI mode. Contribute You can contribute to this project in various ways: Submitting ideas, and reporting issues in the tracker Translating this application to other languages in Launchpad Contributing code changes by fixing issues and submitting a pull request (do not modify translations, this is done in Launchpad) To get started with coding, see the development docs",
    "commentLink": "https://news.ycombinator.com/item?id=41039967",
    "commentBody": "Timeshift: System Restore Tool for Linux (github.com/linuxmint)318 points by gballan 21 hours agohidepastfavorite153 comments pixelmonkey 19 hours agoI've probably spent way too much time thinking about Linux backup over the years. But thankfully, I found a setup that works really well for me in 2018 or so, used it for the last few years, and I wrote up a detailed blog post about it just a month ago: https://amontalenti.com/2024/06/19/backups-restic-rclone The tools I use on Linux for backup are restic + rclone, storing my restic repo on a speedy USB3 SSD. For offsite, I use rclone to incrementally upload the entire restic repository to Backblaze B2. The net effect: I have something akin to Time Machine (macOS) or Arq (macOS + Windows), but on my Linux laptop, without needing to use ZFS or btrfs everywhere. Using restic + some shell scripting, I get full support for de-duplicated, encrypted, snapshot-based backups across all my \"simpler\" source filesystems. Namely: across ext4, exFAT, and (occasionally) FAT32, which is where my data is usually stored. And pushing the whole restic repo offsite to cloud storage via rclone + Backblaze completes the \"3-2-1\" setup straightforwardly. reply ratorx 17 hours agoparentOne problem with file based backups is that they are not atomic across the filesystem. If you ever back up a database (or really any application that expects atomicity while it’s running), then you might corrupt the database and lose data. This might not seem like a big problem, but can affect e.g. SQLite, which is quite popular as a file format. Then again, the likelihood that the backup will be inconsistent is fairly low for a desktop, so it’s probably fine. I think the optimal solution is: 1) file system level atomic snapshot (ZFS, BTRFS etc) 2) Backup the snapshot at a file level (restic, borg etc) This way you get atomicity as well as a file-based backup which is redundant against filesystem-level corruption. reply _flux 14 hours agorootparentYou can also use lvm2 and then you get atomic snapshots with any file system (I think it needs to support fsfreeze, I guess all of them do). reply Am4TIfIsER0ppos 56 minutes agorootparentlvm requires unallocated space in the volume which makes it kind of garbage to use for snapshots reply pixelmonkey 7 hours agorootparentprevI never knew this. Thanks for sharing! reply pixelmonkey 17 hours agorootparentprevI agree with you, of course. On macOS, Arq uses APFS snapshots, and on Windows, it uses VSS. It'd be nice to use something similar on Linux with restic. In my linked post above, I wrote about this: \"You might think btrfs and zfs snapshots would let you create a snapshot of your filesystem and then backup that rather than your current live filesystem state. That’s a good idea, but it’s still an open issue on restic for something like this to be built-in (link). There’s a proposal about how you could script it with ZFS in this nice article (link) on the snapshotting problem for backups.\" The post contains the links with further information. My imperfect personal workaround is to run the restic backup script from a virtual console (TTY) occasionally with my display server / login manager service stopped. reply vladvasiliu 13 hours agorootparentI run this from a ZFS snapshot. What I want backed up from my home dir lives on the same volume, so I don't have to launch restic multiple times. I have dedicated volumes for what I specifically want excluded from backups and ZFS snapshots (~/tmp, ~/Downloads, ~/.cache, etc). I've been thinking of somehow triggering restic by zrepl whenever it takes a snapshot, but I haven't figured a way of securely grabbing credentials for it to unlock the repository and to upload to s3 without requiring user intervention. reply magicalhippo 16 hours agorootparentprevWindows' Volume Shadow Copy Service[1] allows applications like databases to be informed[2] when a snapshot is about to be taken, so they can ensure their files are in a safe state. They also participate in the restore. While Linux is great at many things, backups is one area I find lacking compared to what I'm used to from Windows. There I take frequent incremental whole-disk backups. The backup program uses the Volume Shadow Copy Service to provide a consistent state (as much as possible). Being incremental they don't take much space. If my disk crashes I can be back up and running like (almost) nothing happened in less than an hour. Just swap out the disk and restore. I know, as I've had to do that twice. [1]: https://learn.microsoft.com/en-us/windows/win32/vss/the-vss-... [2]: https://learn.microsoft.com/en-us/windows/win32/vss/overview... reply lmz 13 hours agorootparentLVM snapshots are copy on write and can be used the same way. reply magicalhippo 11 hours agorootparentAny backup software that utilizes LVM in this way? Ie automatically creates a snapshot and sends the incremental changes since previous snapshot to a backup destination like a NAS or S3 blob storage. reply abbbi 6 hours agorootparentwyng backup does this. It uses the device mappers thin_dump tools to allow for incremental backups between snapshots, too: https://github.com/tasket/wyng-backup edit: requires lvm thin provisioned volumes There is also thin-send-recv which basically does the same as zfs send/recv just with lvm: https://github.com/LINBIT/thin-send-recv it uses the same functions of the device mapper to allow incremental sync of lvm thin volumes. reply magicalhippo 3 hours agorootparentThanks for the pointers, looks very relevant. It's just such a low-effort peace of mind. Just a few clicks and I know that regardless what happens to my disk or my system, I can be up and running in very little time with very little effort. On Linux it's always a bit more work, but backups and restore is one of those things I prefer is not too complicated, as stress level is usually high enough when you need to do restore to worry about forgetting some incantation steps. reply abbbi 3 hours agorootparentit depends. Doing a complete disaster recovery of a windows system IMHO can be a real struggle. Especially if you have to restore a system to different hardware, which the system state backup that microsoft offers does not support afaik. Backing up a linux system in combination with REAR: https://github.com/rear/rear and a backup utility of your choice for the regular backup has never failed me so far. I used it to restore linux systems to complete different hardware without any troubles. reply magicalhippo 30 minutes agorootparentFor my cases it's been quite easy, but then I've mostly had quite plain hardware so didn't need vendor drivers to recover. While I've had to recover in anger twice, I've used the same procedure to migrate to new hardware many times. Just restore to the new disk in the new machine, and let Windows reboot a few times and off I went. REAR looks useful, hadn't seen that before. reply _flux 8 hours agorootparentprevI think block-level snapshots would be very difficult to use this way. I just make a full dedupped backups from LVM snapshots with kopia, but I've set that up only on one system, on others I just use kopia as-is. It takes some time, but that's fine for me. Previous backup of 25 GB an hour ago took 20 minutes. I suppose if it only walked files it knew were changed it would be a lot faster. reply magicalhippo 3 hours agorootparentThanks, sounds interesting. So you create a snapshot, then let kopia process that snapshot rather than the live filesystem, and then remove the snapshot? > I suppose if it only walked files it knew were changed it would be a lot faster. Right, for me I'd want to set it up to do the full disk, so could be millions of files and hundreds of GB. But this trick should work with other backups software, so perhaps it's a viable option. reply _flux 3 hours agorootparentExactly so. Here's the script, should it be of benefit to someone, even if it of course needs to be modified: #!/bin/sh success=false teardown() { umount /mnt/backup/var/lib/docker || true umount /mnt/backup/root/.cache || true umount /mnt/backup/ || true for lv in root docker-data; do lvremove --yes /dev/hass-vg/$lv-snapshot || true done if [ \"$1\" != \"no-exit\" ]; then $success exit $? fi } set -x set -e teardown no-exit trap teardown EXIT for lv in root docker-data; do lvcreate --snapshot -L 1G -n $lv-snapshot /dev/hass-vg/$lv done mount /dev/hass-vg/root-snapshot /mnt/backup mount /dev/hass-vg/docker-data-snapshot /mnt/backup/var/lib/docker mount /root/.cache /mnt/backup/root/.cache -o bind chroot /mnt/backup kopia --config-file=\"/root/.config/kopia/repository.config\" --log-dir=\"/root/.cache/kopia\" snap create / /var/lib/docker kopia --config-file=\"/root/.config/kopia/repository.config\" --log-dir=\"/root/.cache/kopia\" snap create /boot /boot/efi success=true reply magicalhippo 34 minutes agorootparentAwesome, thanks! reply lmz 10 hours agorootparentprevI don't think the diffs are usable that way. They're actually more like an \"undo log\" in that the snapshot space is taken by \"old blocks\" when the actual volume is taking writes. It's useful for the same reasons as volume shadow copy: a consistent snapshot of the block device. (Also this can be very bad for write performance as any writes are doubled - to snapshot and to to the real device) reply magicalhippo 3 hours agorootparentYeah ok, that makes sense. Write performance is a concern, but usually the backups run when there's little activity. reply hashworks 13 hours agorootparentprevWhile I do that, is that really the case? I can imagine database snapshots are consistent most of the time, but it can't be guaranteed, right? In the end it's like a server crash, the database suddenly stops. reply jlokier 1 hour agorootparentThat works if the backup uses a snapshot of the filesystem or a point in time. Then the backup state is equivalent to what you'd get if the server suddenly lost power, which all good ACID databases handle. The GP is talking about when the backup software reads database files gradually from the live filesystem at the same time as the database is writing the same files. This can result in an inconsistent \"sliced\" state in the backup, which is different from anything you get if the database crashes or the system crashes or loses power. The effect is a bit like when \"fsync\" and write barriers are not used before a server crash, and an inconsistent mix of things end up in the file. Even databases that claim to be append-only and resistant to this form of corruption usually have time windows where they cannot maintain that guarantee, e.g. when recycling old log space if the backup process is too slow. reply lmz 13 hours agorootparentprevYour DB is supposed to guarantee consistency even in server crashes. (The Consistency, Durability part of ACID). reply mdavidn 12 hours agorootparentThat consistency is built on assumptions about the filesystem that may not hold true of a copy made concurrently by a backup tool. e.g. The database might append to write-ahead logs in a different order than the order in which the backup tool reads them. reply grumbelbart2 11 hours agorootparentThat's why you do a filesystem snapshot before the backup, something supported by all systems. The snapshot is constant to the backup tool, and read order or subsequent writes don't matter. The main difference is that Windows and MacOS have a mechanism that communicates with applications that a snapshot is about to be taken, allowing the applications (such as databases) to build a more \"consistent\" version of their files. In theory, of course, database files should always be in a logically consistent state (what if power goes out?). reply Sakos 10 hours agorootparent> something supported by all systems Well, supported by Windows and MacOS. Linux only if you happen to use zfs or btrfs, and also only if the backup tool you use happens to rely on those snapshots. reply c45y 7 hours agorootparentI believe basically any filesystem will work if you have it on LVM. Bonus of lv snaps being thin snapshots too reply bongobingo1 13 hours agoparentprevDo you have much of an opinion on why you went with Restic over Borg? The single Go binary is an obvious one, perhaps that alone is enough. I remember some people having un-bound memory usage with Restic but that might have been a very old version. reply pixelmonkey 7 hours agorootparentFor me, these traits made restic initially attractive: - encrypted, chunk-deduped, snapshotted backups - single Go binary, so I could even backup the binary used to create my backups - reasonable versioning and release scheme - I could read, and understand, its design document: https://github.com/restic/restic/blob/master/doc/design.rst I then just tried using it for a year and never hit any issues with it, so kept going, and now it's 6+ years later. reply marcus0x62 7 hours agorootparentprevI use both to try to mitigate the risk of losing data due to a backup format/program bug[1]. If I wasn't worried about that, I'd probably go with Borg but only because my offsite backup provider can be made to enforce append-only backups with Borg, but not Restic, at least not that I could find.[2] Otherwise, I have not found one to be substantially better than the other in practice. 1 - some of my first experiences with backup failures were due to media problems -- this was back in the days when \"backup\" pretty much meant \"pipe tar to tape\" and while the backup format was simple, tape quality was pretty bad. These days, media -- tape or disk -- is much more reliable, but backup formats are much more complex, with encryption, data de-dup, etc. Therefore, I consider the backup format to be at least as much of a risk to me now as the media. So, anyway, I do two backups: the local one uses restic, the cloud backup uses borg. 2 - I use rsync.net, which I generally like a lot. I wrote up my experiences with append-only backups, including what I did to make them work with rsync.net here: https://marcusb.org/posts/ransomware-resistant-backups/ reply hashworks 13 hours agorootparentprevI use both, and I never had problems with any of them. Restic has the advantage that it supports a lot more endpoints than ssh/borg, f.e. S3 (or anything that rclone supports). Also borg might be a little bit more complicated to get started with than restic. reply dsissitka 11 hours agorootparentprevThe big one for me was https://borgbackup.readthedocs.io/en/stable/faq.html#can-i-b.... reply _flux 8 hours agorootparentThis was basically one big reason why I went with https://kopia.io . The other might have been its native S3 support. reply e12e 7 hours agoparentprevI've been mulling over setting up restic/kopia backups - and recently discovering httm[1] support restic directly in addition to zfs (and) more - I think I finally will. [1] https://github.com/kimono-koans/httm reply pixelmonkey 6 hours agorootparentI only discovered httm thanks to this thread, and I'll definitely be trying it out for the first time today. Maybe I'll add an addendum to my blog post about it. reply dikei 9 hours agoparentprevI used to use restic with scripting, then I discovered resticprofile, and swiftly replace all my scripts with it. https://github.com/creativeprojects/resticprofile I also use Kopia as an alternative to Restic, in case some critical bugs happen to either one of them. https://kopia.io/ reply AdaX 4 hours agorootparentPersonally, I've had some issues with Kopia. I found their explanation here: https://github.com/kopia/kopia/issues/1764 https://github.com/kopia/kopia/issues/544 Still not solved after many years :( Now I use Borg + Restic and I am happy + GUI for Restic https://github.com/garethgeorge/backrest + GUI for Borg https://github.com/borgbase/vorta reply kmarc 11 hours agoparentprevFor home backup, I have a similar setup with dedup, local+remote backups. Borgbackup + rclone (or aws) [1] It works so well, I even use this same script on my work laptop(s). rclone enables me to use whatever quirky file sharing solution the current workplace has. [1]: https://github.com/kmARC/dotfiles/blob/master/bin/backup.sh reply tlavoie 18 hours agoparentprevOne question, why use rclone for the Backblaze B2 part? I use restic as well, configured with autorestic. One command backs up to the local SSD, local NAS, and B2. reply pixelmonkey 18 hours agorootparentI explain in the post. Here's a copypasta of the relevant paragraph: \"My reasoning for splitting these two processes — restic backup and rclone sync — is that I run the local restic backup procedure more frequently than my offsite rclone sync cloud upload. So I’m OK with them being separate processes, and, what’s more, rclone offers a different set of handy options for either optimizing (or intentionally throttling) the cloud-based uploads to Backblaze B2.\" reply tlavoie 15 hours agorootparentSo you did! Sorry, hadn't read the post beforehand. Oh, and I too mourned the loss of CrashPlan. Being in Canada, I didn't have the option offered to have a restore drive sent if needed, but thought it was a brilliant idea. On the other hand, I think Backblaze might! reply bobek 13 hours agoparentprevI have ended up with something very similar. Restic/rclone is awesome combo. https://bobek.cz/restic-rclone/ reply carderne 6 hours agoparentprevEnjoyed the post, thanks. One question: why don’t you use restic+rclone on macOS? They both support it and I’d assume you could simplify your system a bit… reply pixelmonkey 5 hours agorootparentI only have one macOS system (a Mac Mini) and Arq works well for me. Also I prefer to use Time Machine for the local backups (to a USB3 SSD) on macOS since Apple gives Time Machine all sorts of special treatment in the OS, especially when it comes time to do a hardware upgrade. reply setopt 4 hours agorootparentI’ve also found Arq to be brilliant on MacOS. It’s especially nice on laptops, where you can e.g. set it to pause on battery and during working hours. Also, APFS snapshots is a nice thing given how many Mac apps use SQLite databases under the hood (Photos, Notes, Mail, etc.). On Linux, the system I liked best was rsnapshot: I love its brutal simplicity (cron + rsync + hardlinks), and how easy it is to browse previous snapshots (each snapshot is a real folder with real files, so you can e.g. ripgrep through a date range). But when my backups grew larger I eventually moved to Borg to get better deduplication + encryption. reply pixelmonkey 3 hours agorootparentrsnapshot was definitely my favorite Linux option before restic. I find that restic gives me the benefits of chunk-based deduplication and encryption, but via `restic find` and `restic mount` I can also get many of the benefits of rsnapshot's simplicity. If you use `restic mount` against a local repo on a USB3 SSD, the FUSE filesystem is actually pretty fast. reply setopt 3 hours agorootparentThanks for the info, I’ll have a closer look at Restic then. Borg also has a FUSE interface, but last time I tried it I found it abysmally slow – much slower than just restoring a folder to disk and then grepping through it. I used a Raspberry Pi as my backup server though, so the FUSE was perhaps CPU bound on my system. reply pixelmonkey 2 hours agorootparentYea, I don't want to oversell it. The restic FUSE mount isn't anywhere near \"native\" performance. But, it's fast enough that if you can narrow your search to a directory, and if you're using a local restic repo, using grep and similar tools is do-able. To me, using `restic mount` over a USB3 SSD repo makes the mount folder feel sorta like a USB2 filesystem rather than a USB3 one. reply bulletmarker 6 hours agoparentprevI have used pretty much the same setup for the last 6 years. I run borg to a small server then rclone the encrypted backup nightly to B2 storage. reply PhilippGille 12 hours agoparentprevDo you only back up your home directory, or also others? I didn't find info about that in your post. reply pixelmonkey 7 hours agorootparentI backup everything except for scratch/tmp/device style directories. Bytes are cheap to store, my system is a rounding error vs my /home, and deduping goes a long way. reply PhilippGille 52 minutes agorootparentI'm less worried about the size and more about something breaking when doing a recovery. Let's say you're running Fedora with Gnome and you want to switch to KDE without doing a fresh install. You make a backup, then go through the dozens of commands to switch, with new packages installed, some removed, display managers changed etc. Now something doesn't work. Would recovering from the restic backup reliably bring the system back in order? The tool from the original post seems to be geared towards that, while most Restic and rclone examples seem to be geared towards /home backup, so I wonder how much this is actually an alternative. reply pixelmonkey 30 minutes agorootparentOh, I see what you're saying. I personally wouldn't use it to do a 100% filesystem restore. For the sake of simplicity, I'd just use dd/ddrescue to make a .img file and then load that .img file directly into a partition to boot from a new piece of hardware. Likewise if I were doing a big system change like GNOME to KDE or vice versa, I'd just make an .img file before and restore from it if it went wrong. I think of restic system backups covering something like losing a customized /etc file in an apt upgrade and wanting to get it back. reply LorenDB 19 hours agoprevI prefer using openSUSE, which is tightly integrated with snapper[0], making it simple to recover from a botched update. I've only ever had to use it when an update broke my graphics drivers, but when you need it, it's invaluable. Snapper on openSUSE is integrated with both zypper (package manager) and YaST (system configuration tool) [1], so you get automatic snapshots before and after destructive actions. Also, openSUSE defaults to btrfs, so the snapshots are filesystem-native. [0]: http://snapper.io/ [1]: https://en.opensuse.org/Portal:Snapper reply Arnavion 19 hours agoparentAnd it's also integrated into the bootloader (if you use one of the supported ones). The bootloader shows you one boot entry per snapshot so you can boot an old snapshot directly. reply jwrallie 19 hours agorootparentVery nice, sometimes people claim that the only difference between distros is the repository and package management tools. It is when the defaults make the parts integrate nicely like this that the “greater is more than the sum of its parts” come into place. reply Spunkie 18 hours agorootparentprevThis is a feature I've really been missing since switching from grub to systemd-boot. Has anyone figured out an easy way to get this back with systemd-boot? reply Arnavion 17 hours agorootparentSome time ago they did add systemd-boot as a supported option and apparently it also generates one entry per snapshot. https://news.opensuse.org/2024/03/05/systemd-boot-integratio... https://en.opensuse.org/Systemd-boot#Installation_with_full_... https://github.com/openSUSE/sdbootutil I haven't tried it though so I don't know for sure. (I have my own custom systemd-boot setup that predates theirs, and since my setup uses signed UKIs and theirs doesn't, I don't care to switch to theirs. I can still switch snapshots manually with `btrfs subvol` anyway; it just might require a live CD in case the default snapshot doesn't boot.) reply Vogtinator 2 minutes agorootparentI'm using Tumbleweed with btrfs snapshots, systemd-boot and transparent disk encryption (using TPM + measured boot), works fine. Currently this needs to be set up semi-manually (select some options in the installer, then run some commands after install), but it'll be automatic soon. boomboomsubban 17 hours agorootparentprevsystemd-boot has relatively recently added support for loading filesystems, https://github.com/systemd/systemd/blob/71e5a35a5be99a1f244d... meaning you should be able to set up something similar. I wouldn't describe it as \"easy\" yet. reply whiztech 10 hours agoparentprevI use btrfs-assistant with Kubuntu because I can't get Timeshift to work properly. It's basically some kind of front-end for snapper and btrfsmaintenance. [0]: https://gitlab.com/btrfs-assistant/btrfs-assistant reply abbbi 7 hours agoparentprevfor RHEL based distributions you can do the same with an LVM and using boom boot manager. https://github.com/snapshotmanager/boom-boot reply Barrin92 13 hours agoparentprevopenSUSE honestly is so criminally underrated. I've been using Tumbleweed for a few years for my dev/work systems and YaST is just great. Also that they ship fully tested images for their rolling release is just so much saner. OBS is another fantastic tool that I see so few people talking about, despite software distribution still being such a sore point in the linux ecosystem. reply Rinzler89 9 hours agorootparent>openSUSE honestly is so criminally underrated Because it's not very popular in the US which has mostly cemented around fedora/ubuntu/arch so you don't hear much about any other distros, and most other countries around the world tend to just adopt what they learn from the US, due to the massively influential gravitational field the US has on the tech field. But in the german speaking world many know about it. It's a shame that despite the internet being relatively borderless it's still quite insular and divided. I'm not a native german speaker but it helps to know it since there's a lot of good linux content out there that's written in german. reply OldMatey 19 hours agoprevI adore Timeshift. It has made my time on Linux so much more trouble free. I have used Linux for 10+ years but over the I have spent hours, days and weeks trying to undo or fix little issues I introduce by tinkering around with things. Often I seem to break things at the worst times, right as I am starting to work on some new project or something that is time sensitive. Now, I can just roll back to an earlier stable version if I don't want to spend the time right then on troubleshooting. I've enabled this on all my family members machines and teach them to just roll back when Linux goes funky. reply gooseyman 16 hours agoparentI enabled this four months ago and I have had the same experience. It’s not that I couldn’t retype the config file I accidentally wrote over while tinkering, but I like the safety that comes with Timeshift to try and fail a few times. Hard lessons come hard. This softens those lessons a little while maintaining the learning. reply pmarreck 17 hours agoparentprevWhile it's not quite average-user-friendly (YET), one of the reasons I switched to NixOS is because it provides this out-of-the-box. I was frustrated with every other Linux for the reasons you cite, but NixOS I can deal with, since 1) screwing up the integrity of a system install is hard to begin with, 2) if you DO manage to do it, you can reboot into any of N previous system updates (where you set N). Linux is simultaneously the most configurable and the most brittle OS IMHO. NixOS takes away all the brittleness and leaves all the configurability, with the caveat that you have to declaratively configure it using the Nix DSL. reply rrix2 14 hours agorootparentNixOS also has out of the box support for zfs auto snapshots, where you can tell it to keep 3 months, four weeks, 24 hourly, and frequent snapshots evert fifteen minutes so you can time shift your home directory, too reply pmarreck 5 hours agorootparentI'm zfs on root and haven't set that up yet! I should reply tombert 21 hours agoprevThis reminds me of the default behavior of NixOS. Whenever you make a change in the configuration for NixOS and rebuild it, it takes a snapshot of the system configurations and lets you restore after a reboot if you screw something up. Similarly, it doesn't do anything in regards to user files. reply choward 20 hours agoparentI can't tell you the number of times I see a project and think to myself \"NixOS already solves that problem but better.\" reply fallingsquirrel 20 hours agorootparentIn fairness, this app supports snapshotting your home directory as well, and that's not solvable with Nix alone. In fact, I'm running NixOS and I've been meaning to set up Timeshift or Snapper for my homedir, but alas, I haven't found the time. reply __MatrixMan__ 20 hours agorootparentIs there something about your home directory that you'd want to back up that is not covered by invoking home manager as a nix module as part if nixos-rebuild? https://nix-community.github.io/home-manager/index.xhtml#sec... To me, it's better than a filesystem-backup because the things that make it into home manager tend to be exactly the things that I want to back up. The rest of it (e.g. screenshots, downloads) aren't something I'd want in a backup scheme anyhow. reply fallingsquirrel 20 hours agorootparentI want to keep snapshots of my work. I run nightly backups which have come in handy numerous times, but accessing the cloud storage is always slow, and sometimes I've even paid a few cents in bandwidth to download my own files. It would be a lot smoother if everything was local and I could grep through /.snapshots//. reply SAI_Peregrinus 6 hours agorootparentprevData (documents, pictures, source code, etc.) is not handled by home-manager. Backing up home.nix saves your config, but the data is just as if not more important. reply __MatrixMan__ 1 hour agorootparentHmm, different strokes I guess. Maybe it's just that too much kubernetes has gone to my head, but I see files as ephemeral. Code and docs are in source control. My phone syncs images to PCloud when I take them. Anything I download is backed up... wherever I downloaded it from. reply SAI_Peregrinus 8 minutes agorootparentCloud sync != backup. Cloud sync won't help if you accidentally delete the file, backups will. Cloud sync won't help if you make an undesired edit, backups will. alfalfasprout 19 hours agorootparentprevThe problem, unfortunately, is that Nix often finds itself in a chicken and egg scenario where nixpkgs fails to provide a lot of important packages or has versions that are old(er). But for there to be more investment in adding more packages, etc. you need more people using the ecosystem. reply arianvanp 11 hours agorootparentNixpkgs is the largest and most up to date package repository according to https://repology.org/ I'm honestly curious what packages you have a problem with reply SAI_Peregrinus 6 hours agorootparentProprietary package vendors often provide a. deb that assumes Ubuntu. Maybe also a. rpm for RedHat if you're lucky. reply tombert 4 hours agorootparentThat's definitely true, but maybe I've just been lucky, pretty much every proprietary program I've wanted to install in NixOS has been in Nixpkgs. Skype, Steam, and Lightworks are all directly available in the repos and seem to work fine as far as I can tell. I'm sure there are proprietary packages that don't work or aren't in the repo, but I haven't really encountered them. reply SAI_Peregrinus 13 minutes agorootparentI've unfortunately encountered a few. TotalPhase's Data Center software for their USB protocol analyzers is my current annoyance, someday I'll figure out how to get it to work but thus far it's been easier to just dedicate a second laptop to it. reply atlintots 18 hours agorootparentprevLuckily Nix is also an excellent build system, and does provide escape hatches here and there when you really need them (e.g nix-ld). reply NoThisIsMe 16 hours agorootparentprevWhat are you talking about? Nixpkgs is one of the largest and most up-to-date distro package repos out there. reply autoexecbat 20 hours agorootparentprevI've seen the configuration.nix file, it doesn't look like it captures specific versions. How does it handle snapshotting? reply somnic 19 hours agorootparentFor managing your configuration.nix file itself you can just use whichever VCS you want, it's a text file that describes one system configuration and managing multiple versions and snapshots within that configuration file is out of scope. For the system itself, each time you run \"nixos-rebuild switch\" it builds a system out of your configuration.nix, including an activation script which sets environment variables and symlinks and stops and starts services and so on, adds this new system to the grub menu, and runs the activation script. It specifically doesn't delete any of your old stuff from the nix store or grub menu, including all your older versions of packages, and your old activation scripts. So if your new system is borked you can just boot into a previous one. reply pmarreck 16 hours agorootparentprevImagine installing an entirely new window manager without issue, and then undoing it without issue. NixOS does that. And I'm pretty sure that no other flavor of Linux does. First time I realized I could just blithely \"shop around window managers\" simply by changing a couple of configuration lines, I was absolutely floored. NixOS is the first Linux distro that made me actually feel like I was free to enjoy and tinker with ALL of Linux at virtually no risk. There is nothing else like it. (Except Guix. But I digress.) reply tombert 3 hours agorootparentCompletely agree; being able to transparently know what the system is going to do by just looking at a few lines of text is sort of game-changing. It's trivial to add and remove services, and you can be assured that you actually added and removed them, instead of just being \"pretty sure\" about it. Obviously this is just opinion (no need for someone to supply nuance) but from my perspective the NixOS model is so obviously the \"correct\" way of doing an OS that it really annoys me that it's not the standard for every operating system. Nix itself is an annoying configuration language, and there are some more arcane parts of config that could be smoothed over, but the model is so obviously great that I'm willing to put up with it. If nothing else, being able to trivially \"temporarily\" install a program with nix-shell is a game-changer to me; it changes the entire way of how I think about how to use a computer and I love it. Flakes mostly solve my biggest complaint with NixOS, which was that it was kind of hard to add programs that weren't merged directly into the core nixpkgs repo. reply pmarreck 4 minutes agorootparent> but from my perspective the NixOS model is so obviously the \"correct\" way of doing an OS that it really annoys me that it's not the standard for every operating system - Literally every person who's read the Nix paper and drank the kool-aid thinks this lol. I STILL don't completely understand every element of my nix config but it's still quite usable. Adding software requires adding it to the large-ish config file, largely because I created overlay namespaces of \"master.programname\", \"unstable.programname\" and \"stable.programname\" (with the default being \"unstable\" in my case) but those would all ideally be moved out into 2 text files, 1 for system level (maybe called system_packages.txt) and one for a named user (perhaps called _packages.txt) and if those could be imported somehow into the configuration.nix, I think that would make things a bit easier for end-users, at least initially. The commandline UI (even the newer `nix` one) could still use an overhaul IMHO. The original CL utils were CLEARLY aimed directly at Nix developers, and not so much at end-users... I've been working on my own wrapper to encapsulate the most common use-cases I need the underlying TUI for https://github.com/pmarreck/ixnay /etc. reply michaelmior 5 hours agorootparentprevOn most systems, that is not the case. Typically a user's home directory is `/home/USERNAME` so `~/etc` would be `/home/USERNAME/etc`. reply ijustlovemath 5 hours agorootparentprevTry it for yourself: [ /home/etc = ~/etc ] || echo theyre different reply sieve 13 hours agoprevZFS Snapshots + Sanoid and Syncoid to manage and trigger them is what people should be doing. Unfortunately, booting from ZFS volumes seems to be some form of black art unless things have changed over the last couple of years. The license conflict and OpenZFS always having to chase kernel releases often resulting in delayed releases for new kernels means I cannot confidently use them with rolling release distros on the boot drive. If I muck something up, the data drives will be offline for a few minutes till I fix the problem. Doing the same with the boot drive is pain I can live without. reply rabf 6 hours agoparentBest option to date: https://github.com/zbm-dev/zfsbootmenu A shame most distro's installers don't support it natively, but an encrypted rootfs on ZFS is great once you get it setup. reply sieve 5 hours agorootparentYeah. I am somewhat wary of trying this, mucking something up and wasting a lot of time wrestling with it. Will probably play around with it in a vm and use it during the next ssd upgrade. Would have been so much better if the distros showed more interest in ZFS reply aeadio 3 hours agorootparentIn principle there's no reason you can't install this next to GRUB in case you're wary. If you're not using ZFS native encryption, and make sure not to enable some newer zpool features, GRUB booting should work for ZFS-on-root. That said, I've been using the tool for a while now and it's been really rock solid. And once you have it installed and working, you don't really have to touch it again, until some hypothetical time when a new backward-incompatible zpool feature gets added that you want to use, and you need a newer ZFSBootMenu build to support it. Because it's just an upstream Linux kernel with the OpenZFS kmod, and a small dracut module to import the pool and display a TUI menu, it's mechanically very simple, and relying on core ZFS support in the Linux kernel module and userspace that's already pretty battle tested. After seeing people in IRC try to diagnose recent GRUB issues with very vanilla setups (like ext4 on LVM), I'm becoming more and more convinced that the general approach used by ZFSBootMenu is the way to go for modern EFI booting. Why maintain a completely separate implementation of all the filesystems, volume managers, disk encryption technologies, when a high quality reference implementation already exists in the kernel? The kernel knows how to boot itself, unlock and mount pretty much any combination of filesystem and volume manager, and then kexec the kernel/initrd inside. The upsides to ZFSBootMenu, OTOH, * Supports all ZFS features from the most recent OpenZFS versions, since it uses the OpenZFS kmod * Select boot environment (and change the default boot environment) right from the boot loader menu * Select specific kernels within each boot environment (and change the default kernel) * Edit kernel command line temporarily * Roll back boot environments to a previous snapshot * Rewind to a pool checkpoint * Create, destroy, promote and orphan boot environments * Diff boot environments to some previous snapshot to see all file changes * View pool health / status * Jump into a chroot of a boot environment * Get a recovery shell with a full suite of tools available including zfs and zpool, in addition to many helper scripts for managing your pool/datasets and getting things back into a working state before either relaunching the boot menu, or just directly booting into the selected dataset/kernel/initrd pair. * Even supports user mode SecureBoot signing -- you just need to pass the embedded dracut config the right parameters to produce a unified image, and sign it with your key of choice. No need to mess around with shim and separate kernel signing. reply e12e 19 hours agoprevHmm, this doesn't appear to be what I hoped it was: > Timeshift is similar to applications like rsnapshot, BackInTime and TimeVault but with different goals. It is designed to protect only system files and settings. User files such as documents, pictures and music are excluded. On the other hand, a quick search looking for \"that zfs based time machine thing\" did reveal a new (to me) project that looks very interesting: https://github.com/kimono-koans/httm reply tamimio 19 hours agoparentYou can include the user files too in the home directory. I have some snapshots that include them and some that do not, so you are covered both ways. reply 8organicbits 5 hours agoprevI've found Debian Stable to be extremely stable, especially in recent years, I honestly don't think about system restore as much as I worry about a drive crashing or a laptop getting stolen. I assumed Linux Mint LTS was similarly stable. Folks who have run into issues, what was the root cause? reply tracker1 2 hours agoprevI've just got a simple script that uses rclone for most of my home directory to my NAS. For nearly everything else, I don't mind if I have to start mostly from scratch. reply gchamonlive 18 hours agoprevI use a series of scripts to make daily Borg backups to a local repository: https://github.com/gchamon/borg-automated-backups Currently the local folder is a samba mount so it's off-site. The only tip I'd have for people using Borg is to verify your backups frequently. It can get corrupted without much warning. Also if you want quick and somewhat easy monitoring of backups being created you can use webmin to watch for the modifications in the backup folder and send an email if there isn't a backup being sent in a while. Similarly, you can regularly scan the Borg repo and send email in case of failures for manual investigation. This is low tech, at least lower tech than elastic stack or promstack, but it gets the job done. reply khimaros 10 hours agoparenti've had positive experience with borgmatic which is available in debian repos. reply gchamonlive 9 hours agorootparentNeat! I'll take a look, thanks! reply ThinkBeat 20 hours agoprevA bit of a side note and a bit of old man reveal, it would be nifty to have the backup system write the snapshots to cd/dvd/bluray disk. I remember working in a company that had a robot WORM system. It would grab a disc, it would be processed, take it out, place it among the archives. If a restore as needed the robot would find the backup, and read off the data. I never worked directly on the system, and I seem to remember there was a window that the system could keep track of (naturally) but older disks were stored off site somewhere for however long that window was. (Everything was replicated to a fully 100% duplicate system geographically highly separated from the production system. reply gballan 20 hours agoparentAFAIK timeshift can use any mount. I tried a USB stick, but it was too slow. Now I'm experimenting with a partition on a second drive. reply croniev 13 hours agoprevTimeshift does not work for me because I encrypted my ssd, decrypt on boot, but linux sees every file twice, once encrypted and once decrypted, thinking that my storage is full, and thus timeshift refuses to make backups due to no storage. At least thats as far as I'm understanding it atm reply sulandor 12 hours agoparent> linux sees every file twice, once encrypted and once decrypted fixing this should prove profitable reply Lord_Zero 16 hours agoprevI just switched from windows to Mint and the first thing it asked me was to configure backups and snapshots and stuff. Pretty cool! reply Groxx 14 hours agoparentMint's first-launch welcome-list is excellent. It's a relatively small thing but it helps a lot. reply ivanjermakov 7 hours agoprevMagical thing about timeshift is that you can use it straight from your live CD. It will find root, backups, and restore it together with a boot partition. reply nubinetwork 4 hours agoprevIsn't timeshift what apple calls their snapshot/backup thingy? reply aaronmdjones 1 hour agoparentNo, that's Time Machine. reply jenscow 18 hours agoprevI use BackInTime, which works in a similar way but is much more configurable. I have hourly backups of all my code for the past day, then a single daily for the past week, etc. Saved my ass a few times. reply Springtime 15 hours agoparentSounds like rsnapshot (rsync with hardlinks and scheduling) but the BackInTime repo doesn't mention any comparison of how it's different, though Timeshift says they're similar. Anyone have experience with BiT vs rsnapshot? reply bayindirh 11 hours agorootparentBackInTime works similar to Apple TimeMachine. It uses hardlinks + new files. Plus, it keeps settings for that backup inside the repository itself, so you can install the tool, show the folder, and start restoring. On top of that BiT supports network backups and multiple profiles. I'm using it on my desktop systems with multiple profiles for years and it's very reliable. However it's a GUI first application, so for server applications Borg is a much better choice. reply raudette 4 hours agoparentprevI've used BackInTime since 2010. I loved that, even without using the tool, you could just poke through the file structure, and get an old version of any backed up file. reply stevefan1999 15 hours agoprevCan someone recommend a solution that works well with immutable distros such as Project Bluefin or Fedora Kinoite/Silverblue? We just need to backup maybe the etc and dotfiles. Also great if it can backup NixOS too. reply phoe-krk 21 hours agoprevI'd like some sort of a comparison with Duplicity/Déjà Dup that seems to be the default on Gnome/Cinnamon. reply fallingsquirrel 20 hours agoparentDifferent categories of app. Duplicity is geared toward backing up files to a separate machine, and this tool snapshots your filesystem on the same machine. reply phoe-krk 20 hours agorootparentOK, thanks. I was confused because Time Machine is capable of backing up to a remote device. reply mkesper 8 hours agoparentprevIs that usable nowadays? Last time I checked it was hellishly slow compared to borg. reply phoe-krk 8 hours agorootparentUsable enough for me. I don't mind since it's running in the background anyway. reply crabbone 1 hour agoprevMy first \"real\" experience with Linux was with Wubi (Ubuntu packaged as a Windows program). I think it was based on Ubuntu version 6 or 8. I also tried to update it, when the graphical shell displayed a message saying that update is available. Of course, it bricked the system. I've switched from Ubuntu to Mint to Debian to Fedora to Arch to Manjaro for personal use and had to support a much wider variety of distributions professionally. My experience so far has been that upgrades inevitably damage the system. Most don't survive even a single upgrade. Arch-like systems survive several major package upgrades, but also start falling apart with time. Every few years enough problems accumulate that merit either a complete overhaul or just starting from scratch. With this lesson learned, I don't try to work with backups for my own systems. When the inevitable happens, I try to push forward to the next iteration, and if some things to be lost, then so be it. To complement this, I try to make the personal data as small and as simple to replicate and to modify moving forward as possible. I.e. I would rule against using filesystem snapshots in favor of storing the file contents. I wouldn't use symbolic links (in that kind of data) because they can either break or not be supported in the archive tool. I wouldn't rely on file ownership or permissions (god forbid ACLs!) Try to remove as much of a \"formatting\" information as possible... so I end up with either text files or images. This is not to discourage someone from building automated systems that can preserve much richer assembly of data. And for some data my approach would simply be impossible due to requirements. But, on a personal level... I think it's less of a software problem and more of a strategy about how not to accumulate data that's easy to lose. reply metadat 21 hours agoprevCan timeshift work with ext4 filesystems? I know it won't have the atomicity of a CoW fs, but I'd be fine with that, as the important files on my systems aren't often modified, especially during a backup - I'd configure it to disable the systemd timers while the backup process is running. reply tamimio 19 hours agoparentYep, been using it for a while, incl ext4, you can have scheduled snapshots too, saved my arse few times, especially when you install something that cannot be easily uninstalled like hyperland or similar. reply mbreese 20 hours agoparentprevCan’t you also snapshot LVM volumes directly? So if you have an LVM volume, it shouldn’t matter what the filesystem is, provided it is sync’d… in theory. (I’ve only done this on VMs that could be paused before the snapshot, so YMMV.) reply nijave 19 hours agorootparentYeah, you can take live snapshots with LVM. You can use wyng-backup to incrementally take and back them up somewhere outside LVM. This has been working pretty well for me to backup libvirt domains backed by LVs reply gballan 21 hours agoparentprevJust getting started with it--but I think so, using rsync. reply umvi 20 hours agoprev> Creates filesystem snapshots using rsync+hardlinks Sounds like it works similarly to git fork on GitHub? That is, if no files have changed, the snapshot doesn't take up any extra room? reply Izkata 16 hours agoparentDirectories and hardlinks take up space, just very little. It would make sense to hardlink a directory if everything in that tree was unchanged, but no filesystem will allow hardlinking a directory due to the risk of creating a loop (hardlinking to a parent directory), so directories are always created new and all files in the tree get their own hardlink. Apple's Time Machine was given an exception in their filesystem to allow it, since they have control over it and can ensure no such loops are created. So it doesn't have that penalty creating hardlinks for every single individual file every time. reply dmitrygr 20 hours agoprev> similar to the System Restore feature in Windows and the Time Machine tool in Mac OS This makes no sense! System Restore is a useless wart that just wastes time making \"restore points\" at every app/driver install and can rarely (if ever) produce a working system when used to \"restore\" anything. It does not back up user data at all. Time Machine is a whole-system backup solution that seems to work quite well and does back up user data. To me the quoted statement might as well read \"a tool similar to knitting needles (in hobby shops) and dremels (in machine shops)\" Reading their description further, it seems like they are implementing something similar to TimeMachine (within the confines of what linux makes possible), and not at all like \"System Restore\". This seems sane as this implements something that is actually useful. They, sadly, seem to gloss over what the consequences are of using non-btrfs FS with this tool, only mentioning that btrfs is needed for byte-exact snapshots. They do not mention what sort of byte-inexactness ext4 users should expect... reply magicalhippo 18 hours agoparentThey're talking about the Volume Shadow Copy Service[1], which effectively provides snapshots[2] of the filesystem. Which files are part of a shadow copy is determined by the one creating a shadow copy, so it could include user data. You can view and access the files in a shadow copy using ShadowExplorer[3] if you don't have the pro versions. [1]: https://learn.microsoft.com/en-us/windows-server/storage/fil... [2]: https://learn.microsoft.com/en-us/windows/win32/vss/the-vss-... [3]: https://www.shadowexplorer.com/ reply nijave 20 hours agoparentprevI believe System Restore takes a registry backup and can recover from a bad driver install but it's been years since I used it last. I think just about anything System Restore does can be replicated by \"just fixing it\" in Safe Mode but I think System Restore is geared for less technical folks. Newer versions of Windows have File History to backup user data (I don't think they have an integrated system/file solution quite like Time Machine though). However it makes some sense to keep system/user data separate. You don't want to lose your doc edits because you happened to have a bad driver upgrade at the same time. Likewise, you don't want to roll your entire system back to get an old version of a doc. Time Machine is trivial to implement (without the UI) with disk snapshots (that's what it does--store disk snapshots to an external disk) reply twodave 20 hours agoparentprevMy main use of system restore was to return to a “clean” install + just the bare minimum installs I needed back when windows was more likely to atrophy over time. I agree it is mostly useless today. reply trinsic2 14 hours agoprevDon't forget Atpik. great for migrating a system to a new distro. reply yuumei 21 hours agoprevHas the btrfs sub volume quota bug been fixed yet? I always had issues when using it reply sschueller 19 hours agoparentI don't know but synology uses BTRFS now as well and if something crucial like that was broken I don't think they would support it on a NAS. reply marcus0x62 19 hours agorootparentSynology uses custom extensions to BTRFS for much of their functionality. reply nurettin 19 hours agoprevTimeshift saved my system so many times over the past 6-7 years. Botched upgrades, experimenting with desktop environments, destroying configuration defaults, it works and does what it says on the tin. reply prmoustache 10 hours agoparentHow can you \"botch\" upgrades so many times? I may have had only one update that went wrong in 30 years of using Linux and that was just a bug introduced by a gfx driver in a new minor kernel version. I downgraded it and waited for the bug to be fixed upstream and that was it. reply nurettin 4 hours agorootparentbravo, I guess? reply tamimio 19 hours agoparentprevCan’t agree more with this, it does what it says! reply kkfx 10 hours agoprevNice UI :-) Random notes/suggestions - rsync is not a snapthot tool, so while in most of the cases we can rsync a live volume issueless on a desktop it's not a good idea doing so - zfs support in 2024 is a must, btrfs honestly is the proof of how NOT to manage storage, like stratis - it seems not much a backup tool, witch is perfectly fine but since the target seems to be end users not too much IT literate it should be stated clear... reply exe34 21 hours agoprevoh this brings back memories, i found a script that did this about 15 years ago. it kept three versions of backups using rsync and hard-links to avoid duplication. reply nijave 20 hours agoparenthttps://rsnapshot.org/ ? reply exe34 12 hours agorootparent> rsnapshot was originally based on an article called Easy Automated Snapshot-Style Backups with Linux and Rsync, by Mike Rubel. must have been this one :-D thanks for finding it! reply pmarreck 17 hours agoprevYet another solution that is wholly unnecessary in NixOS. Nice idea, though, since you can too easily screw up every other Linux. reply commercialnix 11 hours agoprev [2 more] [flagged] Argonaut998 8 hours agoparent [–] It’s not really a ‘system restore tool’ it’s a fully fledged backup program like Veeam reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Timeshift for Linux is a system backup tool similar to Windows System Restore and Mac OS Time Machine, focusing on system files and settings.",
      "It supports two modes: RSYNC (using rsync and hard-links) and BTRFS (using BTRFS filesystem features), with the latter requiring a specific subvolume layout.",
      "Developed by Tony George and now part of the Xapp project by Linux Mint, Timeshift offers features like multiple snapshot levels, cross-distribution restores, and post-restore hooks, with installation instructions available for various Linux distributions."
    ],
    "commentSummary": [
      "Timeshift is a system restore tool for Linux, similar to macOS's Time Machine and Windows' System Restore, allowing users to create filesystem snapshots using rsync and hardlinks.",
      "Users discuss various backup solutions and configurations, including restic, rclone, ZFS, BTRFS, and LVM snapshots, highlighting the importance of atomic snapshots for database consistency and reliable backups.",
      "The conversation emphasizes the need for robust backup strategies, comparing tools like restic, Borg, and kopia, and discussing the pros and cons of different filesystems and snapshot methods."
    ],
    "points": 319,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1721683382
  },
  {
    "id": 41039708,
    "title": "July 2024 Update on Instability Reports on Intel Core 13th/14th Gen Desktop CPUs",
    "originLink": "https://community.intel.com/t5/Processors/July-2024-Update-on-Instability-Reports-on-Intel-Core-13th-and/m-p/1617113#M74792",
    "originBody": "Browse Support Community About Developer Software Forums Developer Software Forums Software Development Tools Toolkits & SDKs Software Development Topics Software Development Technologies Intel® DevCloud oneAPI Registration, Download, Licensing and Installation GPU Compute Software Intel® Developer Cloud Edge Developer Toolbox Software Archive Product Support Forums Product Support Forums FPGA Memory & Storage Visual Computing Embedded Products Graphics Processors Wireless Ethernet Products Server Products Intel vPro® Platform Intel® Enpirion® Power Solutions Intel® Unison™ App Intel® QuickAssist Technology (Intel® QAT) Intel® Trusted Execution Technology (Intel® TXT) Thunderbolt™ Share Intel® Gaudi® AI Accelerator Gaming Forums Gaming Forums Intel® ARC™ Graphics Gaming on Intel® Processors with Intel® Graphics Developing Games on Intel Graphics Blogs Blogs @Intel Products and Solutions Tech Innovation Thought Leadership Intel Foundry Private Forums Private Forums Intel oneAPI Toolkits Private Forums Intel AI Software - Private Forums Intel® Connectivity Research Program (Private) Intel-Habana Gaudi Technology Forum HARP (Private Forum) Neural Object Cloning Beta Processors Intel® Processors, Tools, and Utilities Intel Community Product Support Forums Processors July 2024 Update on Instability Reports on Intel Core 13th and 14th Gen Desktop Processors 14988 Discussions July 2024 Update on Instability Reports on Intel Core 13th and 14th Gen Desktop Processors Subscribe More actions Subscribe to RSS Feed Mark Topic as New Mark Topic as Read Float this Topic for Current User Bookmark Subscribe Mute Printer Friendly Page Thomas_Hannaford Employee 07-22-2024 12:04 PM 133,502 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Based on extensive analysis of Intel Core 13th/14th Gen desktop processors returned to us due to instability issues, we have determined that elevated operating voltage is causing instability issues in some 13th/14th Gen desktop processors. Our analysis of returned processors confirms that the elevated operating voltage is stemming from a microcode algorithm resulting in incorrect voltage requests to the processor. Intel is delivering a microcode patch which addresses the root cause of exposure to elevated voltages. We are continuing validation to ensure that scenarios of instability reported to Intel regarding its Core 13th/14th Gen desktop processors are addressed. Intel is currently targeting mid-August for patch release to partners following full validation. Intel is committed to making this right with our customers, and we continue asking any customers currently experiencing instability issues on their Intel Core 13th/14th Gen desktop processors reach out to Intel Customer Support for further assistance. Labels (1) Labels: Labels: Intel® Core™ Processors 3 Kudos Reply All forum topics Previous topic Next topic Link Copied × « Previous 1 2 Next » 27 Replies DaPoets New User 07-22-2024 12:24 PM 131,938 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Looking forward to testing this update with various Motherboard, CPU, & RAM configurations. 1 Kudo Copy link Reply zzetta New Contributor II 07-22-2024 12:43 PM 131,474 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content LOL so it was after all a microcode issue haha. Good thing you managed to fix it!! 1 Kudo Copy link Reply lucasholt New User 07-22-2024 02:28 PM 128,579 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content So what is the guidance for folks who own one of these processors until these updates are available? Should we do anything beyond the guidance from June? 1 Kudo Copy link Reply MrAgapiGCarlos New User 07-22-2024 02:38 PM 128,148 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Good news. Btw i need a favor intel... please... Send a msg to aorus gigabyte, none of the 600 series has bios since Dec2023 and has cero reply or comments on it. I do not have those cpus for now, still use my old 12th. But i will keep the eye and push info to the users for this asap on the discord that i help and my comunity. I will leave the pro take over maybe i get one when i can. 1 Kudo Copy link Reply pressed_for_time New Contributor II 07-22-2024 04:12 PM 108,660 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to MrAgapiGCarlos It's unfortunate about Gigabyte and the lack of BIOS updates. I can confirm that ASUS is updating its 600 boards on a similar schedule to the 700 boards. For example, ASUS 600 boards received the most recent BIOS update on July 15, this is the one that has the latest microcode 0x125. In response to MrAgapiGCarlos 1 Kudo Copy link Reply MrAgapiGCarlos New User 07-22-2024 05:34 PM 98,314 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to pressed_for_time sadly that was the board that was available on time that day 1 plus years ago. but i order replacement but i need to wait until work finish in 10 days to swap those out and put it on a bin. let see if intel see this and pressure them since intel will have to cover this and has to be asap. In response to pressed_for_time 0 Kudos Copy link Reply Mornnb New User 07-22-2024 03:54 PM 110,675 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content This leaves many questions open - did this microcode overvolting cause degradation or merely instability? Was it for vcore, uncore or SA? What can I manually adjust to limit it while waiting for the update? 1 Kudo Copy link Reply Sid9911199 New User 07-22-2024 04:27 PM 106,545 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content How does a bad microcode affect only(a certain number of same chips to fail).. seems like a deeper mistake in the architecture or manufacturing quality control here which is being covered up..what about those 13,14 gen chips that have fixed factory set default voltages and powers that are also failing.. 1 Kudo Copy link Reply pressed_for_time New Contributor II 07-22-2024 04:46 PM 104,161 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to Sid9911199 The only connection I have with Intel is that I am a user of 14th gen hardware. As far as I can see Intel have been completely open about the issues with instability affecting some 13th and 14th gen K/KF/KS processors. As a result of the investigations carried out into this a fault with eTVB (Enhanced Thermal Velocity Boost) was discovered and corrected with the latest microcode update 0x125. As noted above, a further microcode update should be available in August to deal with the remaining issue. Speculating that it is anything else is just that speculation. I am aware that one of the YouTubers has published a video claiming that there could be a manufacturing defect related to \"Oxidation\" of copper through silicon vias. A member of Intel staff has commented on this on Reddit. What they said was \"...We can confirm that the via Oxidation manufacturing issue affected some early Intel Core 13th Gen desktop processors. However, the issue was root caused and addressed with manufacturing improvements and screens in 2023. We have also looked at it from the instability reports on Intel Core 13th Gen desktop processors and the analysis to-date has determined that only a small number of instability reports can be connected to the manufacturing issue\". In response to Sid9911199 1 Kudo Copy link Reply Vlad11 New User 07-22-2024 05:14 PM 100,650 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to pressed_for_time No, they have not been completely open at all. That's why people are still speculating. They didn't explain exactly what is going on. They just said incorrect elevated voltages requested by the CPU microcode are the root cause. They didn't describe exactly which voltages they are talking about, and how exactly the elevated voltages caused the instability issue. In particular, they didn't say if the elevated voltages caused irreversible degradation of the CPU, which in turn caused the stability issues. They didn't say if fixing the microcode will make the affected CPUs stable again, or if the fix just prevents further degradation of the CPU. They didn't say if the microcode update will reduce performance to restore stability. It's very likely that there is permanent damage to the CPUs exposed to the excessive voltage. Even CPUs that are still stable now may have had their life significantly reduced already. My CPU is unstable, it became unstable after just a couple of months of use, and I had to reduce its maximum frequency repeatedly, every 3-4 months, to keep it stable. It's very likely that it has been permanently damaged by this issue, and it might have been affected by the oxidation issue too, and it will probably have to be replaced. Even the oxidation question has only been answered on reddit after somebody asked, instead of adding this information here from the beginning. I mean, I appreciate we got some information, but considering the scale of the issues its less than I expected. For the oxidation issue for example, we don't have the exact time range for the products affected, or other ways to identify if we are affected or not. I bought my CPU in December 2022, so since the oxidation issue was fixed in 2023, I'm almost certainly affected. In response to pressed_for_time 0 Kudos Copy link Reply JeremieF New User 07-22-2024 04:56 PM 102,931 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Thank you for finally acknowledging the manufacturing defect. While the microcode patch is a positive step towards resolving instability issues, I have some concerns that I would like to raise. I am not an engineer, but I believe it is important to consider the potential long-term effects of this voltage issue. Elevated operating voltages can cause significant stress on the CPU, potentially leading to electromigration and irreversible damage that may not be obvious under use. Even if the microcode patch effectively corrects the voltage regulation, the CPUs that have been exposed to these higher voltages may have already suffered from some level of degradation. My primary concern is that the affected CPUs may now have a shorter lifespan than initially expected. While the patch might prolong their life beyond the warranty period, these CPUs could still die prematurely or at least sooner than users might anticipate based on their experience with previous generations. I would appreciate it if Intel could provide more detailed information on the potential long-term impacts of this issue and any steps being taken to address them. Additionally, for those of us who have received replacement CPUs, it would be helpful to understand what measures are being implemented to ensure these units have not been compromised by the elevated voltages. Thank you for your attention to this matter. I look forward to any additional insights or assurances Intel can provide regarding the long-term reliability of the affected processors. 0 Kudos Copy link Reply BFeely Beginner 07-22-2024 07:40 PM 83,597 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content I RMAed my last 13900K way back early last year because one of the cores was completely broken. When I sent back the broken processor I had at that point recommended Intel analyze what had gone wrong, especially since it was passing the official diagnostic tool and only failing on third party stress tests. They just said they'd destroy it. Unfortunately, if they had followed through that meant they lost out on some very early warning signs. My current 13900K has shown some signs of degradation, at first requiring limiting of power to prevent instability but when the motherboard showed signs of failure and had to be replaced, I was able to restore stability with the replacement motherboard's Intel Default Settings firmware. 0 Kudos Copy link Reply botmanprocessor Beginner 07-22-2024 07:51 PM 82,468 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to BFeely I too had to RMA a 14900KS because of a bad core. I had problems as soon as I installed it, so it wasn't a degradation thing. My replacement has been solid since I got it a week ago. Hope it doesn't degrade over time in any way. After keeping an eye on the news about the 13/14th gen stuff, its nice to see some sort of statement from Intel and that a fix is on its way. But, as another poster pointed out, it leaves a few questions to be answered. In response to BFeely 0 Kudos Copy link Reply BFeely Beginner 07-22-2024 07:56 PM 81,980 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to botmanprocessor Definitely stay up to date on BIOS updates, and if you have a Z690 motherboard that stopped getting updates consider replacing it with a Z790 to get access to the \"Intel Default Settings\" mode. If you stay on the Z690, research the parameters and manually set them in the BIOS to help stability. In response to botmanprocessor 0 Kudos Copy link Reply RP369 Employee 07-22-2024 08:49 PM 76,472 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Does this issue cane be contained with 800 series motherboards ? 1 Kudo Copy link Reply BFeely Beginner 07-22-2024 10:24 PM 66,319 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content In response to RP369 If you're thinking like Z890 isn't that for Arrow Lake? In response to RP369 0 Kudos Copy link Reply ysk_PLUZ Beginner 07-22-2024 11:47 PM 57,011 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content I have already RMA'd 14000KS twice. In both cases, symptoms appeared within about two weeks. The damage should accumulate by mid-August. Even if microcode solves the problem, it's still a ticking time bomb. 0 Kudos Copy link Reply Xav1erSue New User 07-23-2024 12:28 AM 53,278 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Does the voltage increase caused by the microcode bug refer to a situation where the actual operating voltage is higher than the CPU's working voltage (including overclocking voltage)? Will fixing the microcode bug result in a decrease in the original CPU core frequency? The fundamental issue is whether this fix can truly solve the problem or if it is simply a limitation on the core frequency. The latter is clearly unreasonable. 0 Kudos Copy link Reply Amosf New User 07-23-2024 12:57 AM 51,173 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content I noticed that the instability of the cpu does not come from the voltage. Myself i own the new 14600kf and yes the voltages are a bit high, i undervolted it from 1.295 to 1.150 volts in boost but still had the instability issues. I noticed that the cpu became unstable because of the wattage settings in the mother boards. Those are sett on unlimited. I manually put it on the recommended wattage that is 181 watt on max and the issues are gone. Stable and cool! 0 Kudos Copy link Reply PurSpyk New User 07-23-2024 02:10 AM 45,999 Views Mark as New Bookmark Subscribe Mute Subscribe to RSS Feed Permalink Print Report Inappropriate Content Ok, so how do I get my faulty CPU replaced. Intel support has verified that its indeed faulty? I am based in New Zealand and support told me I have to courier my CPU, it will then get tested and then possibly replaced. Issue is they said this could take a few weeks, this is really inacceptable as I am self-employed and if I am unable to work is a loss of income due to your faulty product. 0 Kudos Copy link Reply Load more replies Post Reply Reply Topic Options Subscribe to RSS Feed Mark Topic as New Mark Topic as Read Float this Topic for Current User Bookmark Subscribe Printer Friendly Page All forum topics Previous topic Next topic « Previous 1 2 Next » li.common.scroll-to.top Community support is provided Monday to Friday. Other contact methods are available here. Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade. For more complete information about compiler optimizations, see our Optimization Notice.",
    "commentLink": "https://news.ycombinator.com/item?id=41039708",
    "commentBody": "July 2024 Update on Instability Reports on Intel Core 13th/14th Gen Desktop CPUs (intel.com)300 points by acrispino 22 hours agohidepastfavorite181 comments phire 15 hours agoI find it hard to believe that it actually is a microcode issue. Mostly because Intel has way too much motivation to pass it off as a microcode issue, as they can fix a microcode issue for free, by pushing out a patch. If it's an actual hardware issue, then Intel will be forced to actually recall all the faulty CPUs, which could cost them billions. The other reason, is that it took them way too long to give details. If it's as simple as a buggy microcode requesting an out-of-spec voltage from the motherboard, they should have been able to diagnose the problem extremely quickly and fix it in just a few weeks. They would have detected the issue as soon as they put voltage logging on the motherboard's VRM. And according to some sources, Intel have apparently been shipping non-faulty CPUs for months now (since April, from memory), and those don't have an updated microcode. This long delay and silence feels like they spent months of R&D trying to create a workaround, create a new voltage spec to provide the lowest voltage possible. Low enough to work around a hardware fault on as many units as possible, without too large of a performance regression, or creating new errors on other CPUs because of undervolting. I suspect that this microcode update will only \"fix\" the crashes for some CPUs. My prediction is that in another month Intel will claim there are actually two completely independent issues, and reluctantly issue a recall for anything not fixed by the microcode. reply gwbas1c 4 hours agoparentIt's most likely both a hardware issue and a microcode issue. Making CPUs is kind-of like sorting eggs. When they're made, they all have slightly different characteristics and get placed into bins (IE, \"binned\") based on how they meet the specs. To oversimplify, the cough \"better\" chips are sold at higher prices because they can run at higher clock speeds and/or handle higher voltages. If there's a spec of dust on the die, a feature gets turned off and the chip is sold for a lower price. In this case, this is most likely an edge case that would not be a defect if shipping microcode already handled it. (Although it is appropriate to ask if it would result in effected chips going into a lower-price bin if they are effected.) reply nequo 1 hour agorootparent> If there's a spec of dust on the die, a feature gets turned off and the chip is sold for a lower price. Do you mean that if a 13900KS CPU has a manufacturing defect, it gets downgraded and sold as 13900F or something else according to the nature of the defect? reply gwbas1c 1 hour agorootparentIt's been almost 20 years since I worked in the industry, so I don't want to make assumptions about specific products. When I was in the industry, it would be things like disabling caches, disabling cores, ect. I don't remember specific products, though. Likewise, some die can handle higher voltages, clock speeds, ect. reply RedShift1 12 hours agoparentprevAs I understand it, there are multiple voltages inside the CPU, so just monitoring the motherboard VRM won't cut it. That said I too am very skeptical. I just issued a moratorium on the purchase of anything Intel 13th/14th gen in our company and waiting for some actual proof that the issue is fully resolved. reply phire 11 hours agorootparentIt's complicated. On Raptor lake, there are a few integrated voltage regulators to which provide new voltages for specialised uses (like the E core's L2 cache, parts of DDR memory IO, PCI-E IO), but the current draw on those regulators is pretty low. The bulk of the power comes directly from motherboard VRMs on one of several rails with no internal regulation. Most of the power draw is grouped onto just two rails, VccGT for the GPU, and VccCore (also known as VccIA in other generations) which powers all the P-cores, all the E-cores and, the ring bus and the last-level cache. Which means all cores share the same voltage, and it's trivial to monitor externally. I guess it's possible the bug could be with only of the integrated voltage regulators, but those seem to only power various IO devices, and I struggle to see how they could trigger this type of instability. reply BeeOnRope 37 minutes agorootparentWhat's special about the E core's L2 cache such that it gets on-chip regulated voltage? reply jfindley 8 hours agoparentprevThe months of R&D to create a workaround could simply be because the subset of motherboards which trigger this issue are doing something borderline/unexpected with their voltage management, and finding a workaround for that behaviour in CPU microcode is non-trivial. Not all motherboard models appear to trigger the fault, which suggests that motherboard behaviour is at least a contributing factor to the problem. reply ploxiln 56 minutes agorootparentI think this issue was sort of cracked-open and popularized recently by this particular video from Level1Techs: https://www.youtube.com/watch?v=QzHcrbT5D_Y Towards the middle of the video it brings up some very interesting evidence, from online game server farms that use 13900 and 14900 variants for their high single-core performance for the cost, but with server-grade motherboards and chipsets that do not do any overclocking, and would be considered \"conservative\". But these environments show a very high statistical failure rate for these particular CPU models. This suggests that some high percentage of CPUs produced are affected, and it's long run-time over which the problem can develop, not just enthusiast/gamer motherboards pushing high power levels. reply starspangled 7 hours agoparentprevAll modern CPUs come out of the factory with many many bugs. The errata you see published are only the ones that they find after shipping (if you're lucky, they might not even publish all errata). Many bugs are fixed in testing and qualification before shipping. That's how CPU design goes. The way that is done is by pushing as much to firmware as possible, adding chicken switches and fallback paths, and all sorts of ways to intercept regular operation and replace it with some trap to microcode or flush or degraded operation. Applying fixes and workaround might cost quite a bit of performance (think spectre disabling of some kinds of branch predictors for an obvious very big one). And in some cases you even see in published errata they leave some theoretical correctness bugs unfixed entirely. Where is the line before accepting returns? Very blurry and unclear. Almost certainly, huge parts of their voltage regulation (which goes along with frequency, thermal, and logic throttling) will be highly configurable. Quite likely it's run by entirely programmable microcontrollers on chip. Things that are baked into silicon might be voltage/droop sensors, temperature sensors, etc., and those could behave unexpectedly, although even then there might be redundancy or ways to compensate for small errors. I don't see they \"passed it off\" as a microcode issue, just said that a microcode patch could fix it. As you see it's very hard from the outside to know if something can be reasonably fixed by microcode or to call it a \"microcode issue\". Most things can be fixed with firmware/microcode patches, by design. And many things are. For example if some voltage sensor circuit on the chip behaved a bit differently than expected in the design but they could correct it by adding some offsets to a table, then the \"issue\" is that silicon deviates from the model / design and that can not be changed, but firmware update would be a perfectly good fix, to the point they might never bother to redo the sensor even if they were doing a new spin of the masks. On the voltage issue, they did not say it was requesting an out of spec voltage, they said it was incorrect. This is not necessarily detectable out of context. Dynamic voltage and frequency scaling and all the analog issues that go with it are fiendishly complicated, voltage requested from a regulator is not what gets seen at any given component of the chip, loads, switching, capacitance, frequency, temperature, etc., can all conspire to change these things. And modern CPUs run as close to absolute minimum voltage/timing guard bands as possible to improve efficiency, and they boost up to as high voltages as they can to increase performance. A small bug or error in some characterization data in this very complicated algorithm of many variables and large multi dimensional tables could easily cause voltage/timing to go out of spec and cause instability. And it does not necessarily leave some nice log you can debug because you can't measure voltage from all billion components in the chip on a continuous basis. And some bugs just take a while to find and fix. I'm not a tester per se but I found a logic bug in a CPU (not Intel but commercial CPU) that was quickly reproducible and resulted in a very hard lockup of a unit in the core, but it still took weeks to find it. Imagine some ephemeral analog bug lurking in a dusty corner of their operating envelope. Then you actually have to develop the fix, then you have to run that fix through quite a rigorous testing process and get reasonable confidence that it solves the problem, before you would even make this announcement to say you've solved it. Add N more weeks for that. So, not to say a dishonest or bad motivation from Intel is out of the question. But it seems impossible to make such speculations from the information we have. This announcement would be quite believable to me. reply ChoGGi 5 hours agorootparentI agree with most of what you said, so cherry picking one thingy to reply to isn't my intention, but \"And some bugs just take a while to find and fix.\" I think it's less that it took awhile to find the bug/etc, more so that they've been pretty much radio silent for six months. AMD had the issue with burning 7 series CPUs, they were quick to at least put out a statement that they'll make customers whole again. reply sqeaky 1 hour agorootparentprev> As you see it's very hard from the outside to know if something can be reasonably fixed by microcode or to call it a \"microcode issue They claimed: > a microcode algorithm resulting in incorrect voltage requests to the processor. reply worthless-trash 14 hours agoparentprevI believe that the waters may be muddied enough that they wont have to do a full recall and only if you 'provide evidence' the system is still crashing. reply burnte 4 hours agoparentprev> I find it hard to believe that it actually is a microcode issue. They learned a lot from the Pentium disaster, even if it's a hardware issue, they can address it with microcode at least, which is just as good. reply yencabulator 3 hours agorootparentExcept normally the result of a microcode workaround is that the chip no longer performs at its claimed/previously-measured level. Not \"as good\" by any standard. For example, Intel CPU + Spectre mitigation is not \"as good\" as a CPU that didn't have the vulnerability in the first place. reply burnte 5 minutes agorootparentMicrocode changes don't have to affect performance negatively. Do you have any evidence this one will? If it's a voltage algorithm failure, then I would expect that they could run it as advertised with corrected microcode. Unstable power is a massive issue for electronics like this and I have no problem believing their explanation. Bad power causes all sorts of weird issues. reply sqeaky 1 hour agorootparentprevAt least with spectre applying the mitigation was a choice. You could turn it off and game at full speed, while turning it on for servers and web browsing for safety. This is busted or working. reply HeliumHydride 18 hours agoprevhttps://scholar.harvard.edu/files/mickens/files/theslowwinte... \"Unfortunately for John, the branches made a pact with Satan and quantum mechanics [...] In exchange for their last remaining bits of entropy, the branches cast evil spells on future genera- tions of processors. Those evil spells had names like “scaling- induced voltage leaks” and “increasing levels of waste heat” [...] the branches, those vanquished foes from long ago, would have the last laugh.\" \"John was terrified by the collapse of the parallelism bubble, and he quickly discarded his plans for a 743-core processor that was dubbed The Hydra of Destiny and whose abstract Platonic ideal was briefly the third-best chess player in Gary, Indiana. Clutching a bottle of whiskey in one hand and a shot- gun in the other, John scoured the research literature for ideas that might save his dreams of infinite scaling. He discovered several papers that described software-assisted hardware recovery. The basic idea was simple: if hardware suffers more transient failures as it gets smaller, why not allow software to detect erroneous computations and re-execute them? This idea seemed promising until John realized THAT IT WAS THE WORST IDEA EVER. Modern software barely works when the hardware is correct, so relying on software to correct hardware errors is like asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. THIS DOES NOT LEAD TO RISING PROP- ERTY VALUES IN TOKYO. It’s better to stop scaling your transistors and avoid playing with monsters in the first place, instead of devising an elaborate series of monster checks- and-balances and then hoping that the monsters don’t do what monsters are always going to do because if they didn’t do those things, they’d be called dandelions or puppy hugs.\" reply mattnewton 18 hours agoparentI haven't read this piece before but I just knew it was going to be written by Mickens about halfway through your comment. reply throwup238 1 hour agorootparentThe \"mickens\" in the URL on the first line was a dead giveaway :-) reply yieldcrv 17 hours agoparentprev> According to my dad, flying in airplanes used to be fun... Everybody was attractive .... this is how I feel about electric car supercharging stations at the moment. There is a definitely a privilege aspect, which some attractive people are beneficiaries of in a predictable way, as well as other expensive maintenance for their health and attraction. so I could see myself saying the same thing to my children reply officeplant 5 hours agorootparentI'm ruining that trend by charging my E-Transit in nice places and dressing poorly. reply tibbydudeza 12 hours agoparentprevThanks - it is rather funny. reply tux3 21 hours agoprevRemains to be seen how the microcode patch affects performance, and how these CPUs that have been affected by over-voltage to the point of instability will have aged in 6 months, or a few years from now. More voltage generally improves stability, because there is more slack to close timing. Instability with high voltage suggests dangerous levels. A software patch can lower the voltage from this point on, but it can't take back any accumulated fatigue. reply oasisbob 1 hour agoparentMaybe a stretch - but this reminds me of blood sugar regulation for people with type 1 diabetes. Too low is dangerous because you lose rational thought, and the ability to maintain consciousness or self-recover. However, despite not having the immediate dangers of being low, having high blood sugar over time is the condition which causes long-term organ damage. reply giantg2 18 hours agoparentprevI was recently looking at building and buying a couple systems. I've always liked Intel. I went AMD this time. It seemed like the base frequencies vs boost frequencies were much farther apart on Intel than with most of the AMDs. This was especially true on the laptops were cooling is a larger concern. So I suspect they were pushing limits. Also, the performance core vs efficiency core stuff seemed kind of gimmicky with so few performance cores and so many efficiency cores. Like look at this 20 core processor! Oh wait, it's really an 8 core when it comes to performance. Hard to compare that to a 12 core 3D cached Ryzen with even higher clock... I will say, it seems intel might still have some advantages. It seems AMD had an issue supporting ECC with the current chipsets. I almost went Intel because of it. I ended up deciding that DDR5 built in error correction was enough for me. The performance graphs also seem to indicate a smoother throughput suggesting more efficient or elegant execution (less blocking?). But on the average the AMDs seem to be putting out similar end results even if the graph is a bit more \"spikey\". reply nullindividual 18 hours agorootparent> It seems AMD had an issue supporting ECC with the current chipsets. AMD has the advantage with regards to ECC. Intel doesn't support ECC at all on consumer chips, you need to go Xeon. AMD supports it on all chips, but it is up to the motherboard vendor to (correctly) implement. You can get consumer-class AM4/5 boards that have ECC support. reply ploek 6 hours agorootparent> AMD supports it on all chips Unfortunately not. I can't say for current gen, but the 5000 series APUs like the 5600G do not support ECC. I know, I tried... But yes, most Ryzen CPUs do have ECC functionality, and have had it since the 1000 series, even if not officially supported. Official support for ECC is only on Ryzen PRO parts. reply mananaysiempre 16 hours agorootparentprev> AMD supports [ECC RAM] on all chips There was a strange happening with AMD laptop CPUs (“APUs”): the non-soldered DDR5 variants of the 7x40’s were advertised to support ECC RAM on AMD’s website up until a couple months before any actual laptops were sold, then that was silently changed and ECC is only on the PRO models now. I still don’t know if this is a straightforward manufacturing or chipset issue of some kind or a sign of market segmentation to come. (I’m quite salty I couldn’t get my Framework 13 with ECC RAM because of this.) reply ThatMedicIsASpy 17 hours agorootparentprevYou need W680 boards (starting at around 500 bucks) for ECC on desktop intel chips. reply giantg2 17 hours agorootparentI was seeing them around $400 (still expensive). reply giantg2 17 hours agorootparentprevMy understanding is that it's screwed up for multiple vendors and chipsets. The boards might say they support it, but there are some updates saying it's not. It seemed extremely hard to find any that actually supported it. It was actually easier to find new Intel boards supporting ECC. reply paulmd 16 hours agorootparentyeah wendell put out a video a few weeks ago exploring a bunch of problems with asrock rack-branded server-market B650 motherboards and basically the ECC situation was exactly what everyone warns about: the various BIOS versions wandered between \"works, but doesn't forward the errors\", \"doesn't work, and doesn't forward the errors\", and (excitingly) \"doesn't work and doesn't even post\". We are a year and a half after zen4 launched and there barely are any server-branded boards to begin with, and even those boards don't work right. https://youtu.be/RdYToqy05pI?t=503 I don't know how many times it has to be said but \"doesn't explicitly disable\" is not the same thing as \"support\". There are lots of other enablement steps that are required to get ECC to work properly, and they really need to be explicitly tested with each release (which if it is \"not explicitly disabled\", it's not getting tested). Support means you can complain to someone when it doesn't work right. AMD churns AGESA really, really hard and it breaks all the time. Partners have to try and chase the upstream and sometimes it works and sometimes it doesn't. Elmor (Asus's Bios Guy) talked about this on Overclock.net back around 2017-2018 when AMD was launching X399 and talked about some of the troubles there and with AM4. That said, the current situation has seemingly lit a fire under the board partners, with Intel out of commission and all these customers desperate for an alternative to their W680/raptor lake systems (which do support ecc officially, btw) in these performance-sensitive niches or power-limited datacenter layouts, they are finally cleaning up the mess like, within the last 3 weeks or so. They've very quickly gone from not caring about these boards to seeing a big market opportunity. https://www.youtube.com/watch?v=n1tXJ8HZcj4 can't believe how many times I've explained in the last month that yes, people do actually run 13700Ks in the datacenter... with ECC... and actually it's probably some pretty big names in fact. A previous video dropped the tidbit that one of the major affected customers is Citadel Capital - and yeah, those are the guys who used to get special EVEREST and BLACK OPS skus from intel for the same thing. Client platform is better at that, the very best sapphire rapids or epyc -F or -X3D sku is going to be like 75% of the performance at best. It's also the fastest thing available for serving NVMe flash storage (and Intel specifically targeted this, the Xeon E-2400 series with the C266 chipset can talk NVMe SAS natively on its chipset with up to 4 slimsas ports...) it's somewhere in this one I think: https://www.youtube.com/watch?v=5KHCLBqRrnY reply justinclift 7 hours agorootparentThe new EPYC processors for AM5 though look like they'll be ok for ECC ram though, at least in the coming months onwards. reply jwond 17 hours agorootparentprevActually some of the 13th and 14th gen Intel Core processors support ECC. reply mackal 5 hours agorootparentIntel has always had randomly supported ECC on desktop CPUs. Sometimes it was just a few low end SKUs, sometimes higher end SKUs. 14th gen it appears i9s and i7s do, didn't check i5s, but i3s did not. reply smolder 5 hours agorootparentprevECC support wasn't good initially on AM5, but there are now Epyc branded chips for the AM5 socket which officially support ECC DDR5. They come in the same flavors as the Ryzen 7xx0 chips, but are branded as Epyc. reply fomine3 17 hours agorootparentprevMore E-core is reasonable for multi threaded application performance. It's efficient for power and die area as the name indicates, so they can implement more E-cores than P-cores for the same power/area budget. It's not suitable for who need many single threaded performance cores like VM server, but I don't know is there any major consumer usage requires such performance. reply Sohcahtoa82 12 hours agorootparent> but I don't know is there any major consumer usage requires such performance. Gaming. There are some games that will benefit from greater single-core performance. reply giantg2 17 hours agorootparentprevI can sort of see that. The way I saw it explained as them being much lower clock and having a pretty small shared cache. I could see E cores as being great for running background processes and stuff. All the benchmarks seem to show the AMDs with 2/3rd the cores being around the same performance and with similar power draw. I'm not putting them down. I'm just saying it seems gimmicky to say \"look at our 20 core!\" with the implicit idea that people will compare that with an AMD 12 core seeing 20>12, but not seeing the other factors like cost and benchmarks. reply philistine 16 hours agorootparentIt's the megahertz wars all over again! reply cyanydeez 8 hours agorootparentComputers have taught is the rubric of truth: Numbers go Up. reply akira2501 17 hours agorootparentprev> so few performance cores and so many efficiency cores I was baffled by this too but what they don't make clear is the performance cores have hyperthreading the efficiency cores do not. So what they call 2P+4E actually becomes an 8 core system as far as something like /proc/cpuinfo is concerned. They're also the same architecture so code compiled for a particular architecture will run on either core set and can be moved from one to the other as the scheduler dictates. reply Dylan16807 15 hours agorootparent> They're also the same architecture so code compiled for a particular architecture will run on either core set and can be moved from one to the other as the scheduler dictates. I don't know if that has done more good than harm, since they ripped AVX-512 out for multiple generations to ensure parity. reply jiggawatts 11 hours agorootparentprevA major differentiator is that Intel CPUs with E cores don’t allow the use of AVX-512, but all current AMD CPUs do. The new Zen 5 chips will run circles around Intel for any such workload. Video encoding, 3D rendering, and AI come to mind. For developers: many database engines can use AVX-512 automatically. reply Dylan16807 15 hours agorootparentprev> Like look at this 20 core processor! Oh wait, it's really an 8 core when it comes to performance. The E cores are about half as fast as the P cores depending on use case, at about 30% of the size. If you have a program that can use more than 8 cores, then that 8P+12E CPU should approach a 14P CPU in speed. (And if it can't use more than 8 cores then P versus E doesn't matter.) (Or if you meant 4P+16E then I don't think those exist.) > Hard to compare that to a 12 core 3D cached Ryzen with even higher clock... Only half of those cores properly get the advantage of the 3D cache. And I doubt those cores have a higher clock. AMD's doing quite well but I think you're exaggerating a good bit. reply ComputerGuru 15 hours agorootparent> If you have a program that can use more than 8 cores, then that 8P+12E CPU should approach a 14P CPU in speed Only if you use work stealing queues or (this is ridiculously unlikely) run multithreaded algorithms that are aware of the different performance and split the work unevenly to compensate. reply Dylan16807 15 hours agorootparentOr if you use a single queue... which I would expect to be the default. Blindly dividing work units across cores sounds like a terrible strategy for a general program that's sharing those cores with who-knows-what. reply ComputerGuru 5 hours agorootparentIt’s a common strategy for small tasks where the overhead of dispatching the task greatly exceeds the computation of it. It’s also a better way to maximize L1/L2 cache hit rates by improving memory locality. Eg you have 100M rows and you want to cluster them by a distance function (naively), running dist(arr[i], arr[j]) is crazy fast, the problem is just that you have so many of them. It is faster to run it on one core than dispatch it from one queue to multiple cores, but best to assign the work ahead of time to n cores and have them crunch the numbers. reply Dylan16807 50 minutes agorootparentIt has always been a bad idea to dispatch so naively and dispatch to the same number of threads as you have cores. What if a couple cores are busy, and you spend almost twice as much time as you need waiting for the calculation to finish? I don't know how much software does that, and most of it can be easily fixed to dispatch half a million rows at a time and get better performance on all computers. Also on current CPUs it'll be affected by hyperthreading and launch 28 threads, which would probably work out pretty well overall. reply dwattttt 7 hours agorootparentprevThe P cores being presented as two logical cores and E cores presented as a single logical core results in this kind of split already. reply Sohcahtoa82 12 hours agorootparentprev> run multithreaded algorithms that are aware of the different performance and split the work unevenly to compensate. This is what the Intel Thread Director [0] solves. For high-intensity workloads, it will prioritize assigning them to P-cores. [0] https://www.intel.com/content/www/us/en/support/articles/000... reply ComputerGuru 5 hours agorootparentThen you no longer have 14 cores in this example, but only len(P) cores. Also most code written in the wild isn’t going to use an architecture-specific library for this. reply giantg2 13 hours agorootparentprevYeah, the 20 core Intels are benchmarking about the same as the 12 core AMD X3Ds. But many people just see 20>12. Either one is more than fine for most people. \"Oh wait, it's really an 8 core when it comes to performance [cores]\". So yes, should not be an 8 core all together, but like you said about 14 cores, or 12 with the 3D cache. \"And I doubt those cores have a higher clock.\" I'm not sure what we're comparing them to. They should be capable of higher clock than the E cores. I thought all the AMD cores had the ability to hit the max frequency (but not necessarily at the same time). And some of the cores might not be able to take advantage of the 3D cache, but that doesn't limit their frequency, from my understanding. reply hellotomyrars 1 hour agorootparentIt’s kind of funny and reminiscent of the AMD bulldozer days where they had a ton of cores compared to the contemporary Intel chips, especially at low/mid price points but the AMD chips were laughably underwhelming for single core performance which was even more important then. I can’t speak to the Intel chips because I’ve been out of the Intel game for a long time but my 5700X3D does seem to happily run all cores at max clock speed. reply Dylan16807 11 hours agorootparentprev> I'm not sure what we're comparing them to. They should be capable of higher clock than the E cores. Oh, just higher clocked than the E cores. Yeah that's true, but if you're using that many cores at once you probably only care about total speed. You said 12 core with higher clock versus 8, so I thought you were comparing to the performance cores. > I thought all the AMD cores had the ability to hit the max frequency (but not necessarily at the same time). The cores under the 3D cache have a notable clock penalty on existing CPUs. > And some of the cores might not be able to take advantage of the 3D cache, but that doesn't limit their frequency, from my understanding. Right, but my point is it's misleading to call out higher core count and the advantages of 3D stacking. The 3D stacking mostly benefits the cores it's on top of, which is 6-8 of them on existing CPUs. reply giantg2 8 hours agorootparent\"The cores under the 3D cache have a notable clock penalty on existing CPUs.\" Interesting. I can't find any info on that. It seems that makes sense though since the 7900X is 50 TDP higher than the 7900X3D. \"Right, but my point is it's misleading to call out higher core count and the advantages of 3D stacking\" Yeah, that makes sense. I didn't realize there was a clock penalty on some of the cores with the 3D cache and that only some cores could use it. reply LtdJorge 6 hours agorootparentIt's due to the stacked cache being harder to cool and not supporting as high of a voltage. So the 3D CCD clocks lower, but for some workloads it's still faster (mainly ones dealing with large buffers, like games, most compute heavy benchmarks fit in normal caches and the non 3D V-Cache variants take the win). reply paulmd 16 hours agoparentprev> Remains to be seen how the microcode patch affects performance intel is claiming 4% performance hit on the final patch https://youtu.be/wkrOYfmXhIc?t=308 reply tpurves 17 hours agoprevI think it's telling that they are delaying the microcode patch until after all the reviewers publish their Zen5 reviews and the comparisons of those chips against current Raptorlake performance. reply zenonu 17 hours agoparentWhy even publish a comparison? Raptor Lake processors aren't a functioning product to benchmark against. reply AnthonyMouse 12 hours agorootparentBecause the benchmarks will still exist on the sites after the microcode is released and a lot of the sites won't bother to go back and update them with the accurate performance level. reply tankenmate 12 hours agorootparentprevBecause if publishers don't publish then they don't make money. reply userbinator 16 hours agoprevReminds me of Sudden Northwood Death Syndrome, 2002. Looks like history may be repeating itself, or at least rhyming somewhat. Back then, CPUs ran on fixed voltages and frequencies and only overclockers discovered the limits. Even then, it was rare to find reports of CPUs killed via overvolting, unless it was to an extreme extent --- thermal throttling, instability, and shutdown (THERMTRIP) seemed to occur before actual damage, preventing the latter from happening. Now, with CPU manufacturers attempting to squeeze all the performance they can, they are essentially doing this overclocking/overvolting automatically and dynamically in firmware (microcode), and it's not surprising that some bug or (deliberate?) ignorance that overlooked reliability may have pushed things too far. Intel may have been more conservative with the absolute maximum voltages until recently, and of course small process sizes with higher potential for electromigration are a source of increased fragility. Also anecdotal, but I have an 8th-gen mobile CPU that has been running hard against the thermal limits (100C) 24/7 for over 5 years (stock voltage, but with power limits all unlocked), and it is still 100% stable. This and other stories of CPUs in use for many years with clogged or even detached heatsinks seem to contribute to the evidence that high voltage is what kills CPUs, and neither heat nor frequency. Edit: I just looked up the VCore maximum for the 13th/14th processors - the datasheet says 1.72V! That is far more than I expected for a 10nm process. For comparison, a 1st-gen i7 (45nm) was specified at 1.55V absolute maximum, and in the 32nm version they reduced that to 1.4V; then for the 22nm version it went up slightly to 1.52V. reply unregistereddev 2 hours agoparent> Back then, CPUs ran on fixed voltages and frequencies and only overclockers discovered the limits. Even then, it was rare to find reports of CPUs killed via overvolting, unless it was to an extreme extent --- thermal throttling, instability, and shutdown (THERMTRIP) seemed to occur before actual damage, preventing the latter from happening. Oh the memories. I had a Thunderbird-core Athlon with a stock frequency of (IIRC) 1050Mhz. It was stable at 1600Mhz, and I ran it that way for years. I was able to get it to 1700Mhz, but then my CPU's stability depended on ambient temperatures. When the room got hot in the summer my workstation would randomly kernel panic. reply slaymaker1907 14 hours agoparentprevInteresting, I hadn’t heard about the Pentium overlocking issues. My theory on the current issue that running chips for long periods of time at 100C is not good for chip longevity, but voltages could also be an issue. I came up with this theory last summer when I built my rig with a 13900k, though I was doing it with the intention of trying to set things up so the CPU could last 10 years. Anecdotally, my CPU has been a champ and I haven’t noticed any stability issues despite doing both a lot of gaming and a lot of compiling on it. I lost a bit of performance but not much setting a power limit of 150W. reply cyanydeez 8 hours agoparentprevI believe the first round of Intel excuses here blamed the motherboard manufacturers for trying to \"auto\" overclock these CPUs. reply magicalhippo 20 hours agoprevThere was recently[1] some talk about how the 13th/14th gen mobile chips also had similar issues, though Intel insisted it's something else. Will be interesting to see how that pans out. [1]: https://news.ycombinator.com/item?id=41026123 reply tedunangst 18 hours agoparentThe mobile issue seems more anecdote than data? Almost as if people on Reddit heard the 13/14 CPUs were bad, then their laptop crashed, and they decided \"it happened to me too\". reply magicalhippo 18 hours agorootparentWell it's not just[1] redditors from what I can gather: Now Alderon Games reports that Raptor Lake crashes impact Intel's 13th and 14th-Gen processors in laptops as well. \"Yes we have several laptops that have failed with the same crashes. It's just slightly more rare then the desktop CPU faults,\" the dev posted. These are the guys who publicly claimed[2] Intel sold defective chips based on the desktop chips crashing. [1]: https://www.tomshardware.com/pc-components/cpus/dev-reports-... [2]: https://www.tomshardware.com/pc-components/cpus/game-publish... reply sirn 12 hours agorootparentThe problem may exist, but Alderon Games' report on the mobile chip is more of an anecdote here because there's not enough data points (unlike their desktop claims), and the only SKU they give (13900HX) is actually a desktop chip in a mobile package (BGA instead of LGA, so we're back into the original issue). So in the end, even with Alderon's claims, there's really not enough data points to come to a conclusion on the mobile side of things. reply Sakos 10 hours agorootparentWhy are you downplaying it too? > \"The laptops crash in the exact same way as the desktop parts including workloads under Unreal Engine, decompression, ycruncher or similar. Laptop chips we have seen failing include but not limited to 13900HX etc.,\" Cassells said. > \"Intel seems to be down playing the issues here most likely due to the expensive costs related to BGA rework and possible harm to OEMs and Partners,\" he continued. \"We have seen these crashes on Razer, MSI, Asus Laptops and similar used by developers in our studio to work on the game. The crash reporting data for my game shows a huge amount of laptops that could be having issues.\" https://old.reddit.com/r/hardware/comments/1e13ipy/intel_is_... reply sirn 10 hours agorootparentI'm not denying that the problem exists, but I don't think Alderon provided enough data to come to a conclusion, unlike on the desktop, where it's supported by other parties in addition to Alderon's data (where you can largely point to 14900KS/K/non-K/T, 13900KS/K/non-K/T, 14700K, and 13700K being the one affected) Right now, the only example given is HX (which is a repackaged desktop chip[^], as mentioned), so I'm not denying that the problem is happening on HX based on their claims (and it makes a lot of sense that HX is affected! See below), but what about H CPUs? What about P CPUs? What about U CPUs? The difference in impact between \"only HX is impacted\" and \"HX/H/P/U parts are all affected\" is a few orders of magnitude (a very top-end 13th Gen mobile SKUs versus every 13th Gen mobile SKUs). Currently, we don't have enough data how widespread the issue is, and that makes it difficult to assess who is impacted by this issue from this data alone. [^]: HX is the only mobile CPU with B0 stepping, which is the same as desktop 13th/14th Gen, while the mobile H/P/U family are J0 and Q0, which are essentially a higher clocked 12th Gen (i.e., using Golden Cove rather than Raptor Cove) reply paulmd 4 hours agorootparentprevAlderon are the people claiming 100% of units fail which doesn’t seem supported by anyone else either. Wendell and GN seem to have scoped the issue to around 10-25% across multiple different sources. Like they are the most extreme claimants at this point. Are they really credible? reply tardy_one 20 hours agoparentprevFor server CPUs there's not a similar problem or they realize server purchasers may be less willing to tolerate it? I'm not all that thrilled with the prospect of buying Intels especially when wondering about waiting to 5 year out replacement compared to a few generations ago, but AMD server choices can be a bit limited and I'm not really sure how to evaluate if there may be increasing surprises more across the board. reply sirn 11 hours agorootparentAre you talking about Xeon Scalable? Although they share the same core design as the desktop counterpart (Xeon Scalable 4th Gen shares the same Golden Cove as 12th Gen, Xeon Scalable 5th Gen shares the same Raptor Cove as 13th/14th Gen), they're very different from the desktop counterpart (monolithic vs tile/EMIB-based, ring bus vs mesh, power gate vs FIVR), and often running in a more conservative configuration (lower max clock, more conservative V/F curves, etc.). There has been a rumor about Xeon Scalable 5th Gen having the same issue, but it's more of a gossip rather than a data point. The issue does happen with desktop chips that are being used in a server context when pairing with workstation chipset such as W680. However, there haven't been any reports of Xeon E-2400/E-3400 (which is essentially a desktop chip repurposed as a server) with C266 having these issues, though it may be because there hasn't been a large deployment of these chips on the server just yet (or even if there are, it's still too early to tell). Do note that even without this particular issue, Xeon Scalable 4th Gen (Sapphire Rapids) is not a good chip (speaking from experience, I'm running w-3495x). It has plenty of issues such as slow clock ramp, high latency, high idle power draw, and the list goes on. While Xeon Scalable 5th Gen (Emerald Rapids) seems to have fixed most of these issues, Zen 4 EPYC is still a much better choice. reply uticus 3 hours agoprevDumb question: let’s say I am in charge of procurement for a significant amount of machines, do I not have the option of ordering machines from three generations back? Are older (proven reliable) processors just not available because they’re no longer made, like my 1989 Camry? reply wmf 1 hour agoparentYeah, 12th gen is probably still available. reply TazeTSchnitzel 20 hours agoprevAfter watching https://youtube.com/watch?v=gTeubeCIwRw and some related content, I personally don't believe it's an issue fixable with microcode. I guess we'll see. reply jpk 16 hours agoparentBecause HN doesn't provide link previews, I'd recommend adding some information about the content to your comment. Otherwise we have to click through to YouTube for the comment to make any sense. That said, the video is the GamersNexus one where they talk about an unverified claim that this is a fabrication process issue caused by oxidation between atomic deposition layers. If that's the case, then yeah, microcode can only do so much. But like Steve says in the video, the oxidation theory has yet to be proven and they're just reporting what they have so far ahead of the Zen 5 reviews coming soon. reply mjevans 12 hours agorootparentHopefully Intel ships them, and allows them to, test and publish benchmarks with the current pre-release microcode revision for review comparison. reply wnevets 21 hours agoprevAre the CPUs that received elevated operating voltage permanently damaged? reply Pet_Ant 20 hours agoparentThis is the most pressing question. If it was just a microcode issue a cooloff and power cycle ought to at least reset things but according to Wendel from Level 1 Tech, that doesn't seem to always be the case. reply kevingadd 18 hours agorootparentThe problem is that running at too high of a voltage for sustained periods can cause physical degradation of the chip in some cases. Hopefully not here! reply chmod775 10 hours agorootparent> can cause physical degradation of the chip in some cases. Not in some cases. Chips always physically degrade regardless of voltage. Higher voltages will make it happen faster. reply Pet_Ant 6 hours agorootparentWhy do chis degrade? Is this due to the whiskers I’ve heard about? reply cesarb 6 hours agorootparent> Why do chis degrade? Is this due to the whiskers I’ve heard about? No, tin whiskers are a separate issue, which happens mostly outside the chips. The keyword you're looking for is electromigration (https://en.wikipedia.org/wiki/Electromigration). reply layer8 18 hours agoparentprevNot instantly it seems, but there have been reports of degradation over time. It will be a case-by-case thing. reply userbinator 16 hours agoparentprevPossible electromigration damage, yes. reply NBJack 21 hours agoprevI was concerned this would happen to them, given how much power was being pushed through their chips to keep them competitive. I get the impression their innovation has either truly slowed down, or AMD thought enough 'moves' ahead with their tech/marketing/patents to paint them into a corner. I don't think Intel is done though, at least not yet. reply brynet 20 hours agoprevCurious why Intel announced this on their community forums, rather than somewhere more official. reply guywithahat 18 hours agoparentThat’s probably where people are mostly likely to understand it. A lot of companies do this, especially while they’re still learning things. reply wmf 16 hours agorootparentThese days people are more likely to see the announcement on YouTube, TikTok, or Twitter. reply slaymaker1907 14 hours agorootparentThe first two require a lot more effort in video editing than creating a forum post. Plus, it’s just going to be digested and regurgitated for the masses by people much better at communicating technical information. reply cyanydeez 8 hours agorootparentprevSobweird hearing high noise channels as the prefwrred distributiom reply paulmd 4 hours agorootparentprevthey did that too https://youtu.be/wkrOYfmXhIc reply samtheprogram 20 hours agoparentprevOptics / stock price reply beart 16 hours agoparentprevBased on what I know about corporations, it's entirely plausible that the folks posting the information don't actually have access to the communication channels you are referring to. I don't even know how I would issue an official communication at my own company if the need ever came up... so you go with what you have. reply langsoul-com 12 hours agoparentprevNote how they mentioned its still going to be tested with various partners before released. Ie we think this might solve it, but if it doesn't we can roll back with the least amount of PR attention. reply ChoGGi 6 hours agoprevHmm, mid August is after the new Ryzens are out, I wonder how bad of a performance hit this microcode update will bring? And will it actually fix the issue? https://www.youtube.com/watch?v=QzHcrbT5D_Y reply Havoc 11 hours agoprev> Intel is delivering a microcode patch which addresses the root cause of exposure to elevated voltages. That’s great news for intel. If that’s correct. If not that’ll be a PR bloodbath reply nubinetwork 19 hours agoprevThey already tried bios updates when they pushed out the \"intel defaults\" a couple months ago... reply tedunangst 18 hours agoparentExcept they didn't. https://www.pcworld.com/article/2326812/intel-is-not-recomme... reply wmf 19 hours agoparentprevFirmware and microcode aren't the same thing. reply nicman23 10 hours agorootparentfirmware can include microcode though reply jeffbee 19 hours agorootparentprevVery true and that's why it is odd that microcode has been mentioned here. Surely they mean PCU software (Pcode), or code for whatever they are calling the PCU these days. reply wmf 19 hours agorootparentI assume Intel's \"microcode\" updates include the PCU code, maybe some ME code, and whatever other little cores are hiding in the chip. reply jeffbee 18 hours agorootparentWell, do they? The operating system can provide microcode updates to a running CPU. Can the operating system patch the PCU, too? When I look at a \"BIOS update\" it usually seems to include UEFI, peripheral option ROMs, ME updates, and microcode. So if the PCU is getting patched I would think of it as a BIOS update. I think the ergonomics will be indistinguishable for end users. reply salamo 13 hours agoprevIs there any info on how to diagnose this problem? Having just put together a computer with the 14900KF, I really don't want to swap it out if not necessary. reply Fabricio20 2 hours agoparentThere is no reliable way to diagnose this issue with the 14th gen, the chip slowly degrades over time and you start getting more and more (usually gpu driver under windows) crashes. I believe the easy way might be to run decompression stress tests if I remember correctly from Wendell's (Level1Techs) video. I highly recommend going into your motherboard right now and manually setting your configurations to the current intel recommendation to prevent it from degrading to the point where you'd need to RMA it. I have a 14900K and it took about 2.5 months before it started going south and it was getting worse by the DAY for me. Intel has closed my RMA ticket since changing the bios settings to very-low-compared-to-what-the-original-is has made the system stable again, so I guess I have a 14900K that isn't a high end chip anymore. Below are the configs intel provided to me on my RMA ticket that have made my clearly degraded chip stable again: CEP (Current Excursion Protection)> Enable. eTVB (Enhanced Thermal Velocity boost)> Enable. TVB (Thermal Velocity boost)>Enable. TVB Voltage Optimization> Enable. ICCMAX Unilimited bit>Disable. TjMAX Offset> 0. C-States (Including C1E) >Enable. ICCMAX> 249A. ICCMAX_APP>200A. Power limit 1 (PL1)>125W. Power limit 2 (PL2)>188W reply J_Shelby_J 12 hours agoparentprevOCCP burn in test with AVX and XMP disabled. Tbh, XMP is probably the cause of most modern crashes on gaming rigs. It does not guarantee stability. After finding a stable cpu frequency, enable xmp and roll back the memory frequency until you have no errors in occp. The whole thing can be done in 20 minutes and your machine will have 24/7/365 uptime. reply LtdJorge 6 hours agorootparentThis is good advice for overclocking, but how does it help with the 13th/14th Gen issue? The issue is not due to clocks, or at least doesn't appear to be. reply sudosysgen 13 hours agoparentprevRunning a full memtest overnight and a day of Prime95 with validation is the traditional way of sussing out instability. reply paulmd 4 hours agorootparentit’s also a terrible stability test these days for the same reasons Wendell talks about with cinebench in his video with Ian (and Ian agrees too). Doesn’t work like 90% of the chip - it’s purely a cache/avx benchmark. You can have a completely unstable frontend and it’ll just work fine because prime95 fits in icache and doesn’t need the decoder, and it’s just vector op, vector op, vector op forever. You can have a system that’s 24/7 prime95 stable that crashes as soon as you exit out, because it tests so very little of it. That’s actually not uncommon due to the changes in frequency state that happen once the chip idles down… and it’s been this way for more than a decade, speedstep used to be one of the things overclockers would turn off because it posed so many problems vs just a stable constant frequency load. reply Covzire 20 hours agoprevJust want to say, I'm incredibly happy with my 7800X3D. It runs ~70C max like Intel chips used to and with a $35 air cooler and it's on average the fastest chip for gaming workloads right now. reply amiga-workbench 17 hours agoparentI'm also very happy with my 5800X3D, it was wonderful value back when AM5 had just released and DDR5/Motherboards still cost an arm and a leg. The energy efficiency is much appreciated in the UK with our absurd price of electricity. reply SushiHippie 10 hours agorootparentSame, in my BIOS I can activate a \"ECO Mode\", which lets me decide if I want to run my 7950x on full 170W TDP, 105W TDP or 60W TDP. I benchmarked it, the difference between 170 and 105 is basically zero, and the difference to 60W is just a few percent of a performance hit, but way worth it, as it's ~0.3€/kWh over here. reply aruametello 40 minutes agorootparent(if you are running windows) you might want to check a tool called PBO2Tunner (https://www.cybermania.ws/apps/pbo2-tuner/), you can tweak values like EDC,TDC and PPT (power limit) from the GUI, and it also accepts command line commands so you can automate those tasks. I made scripts that \"cap\" the power consumption of the cpu based on what applications are running. (i.e. only going all in on certain games, dynamically swaping between 65-90-120-180w handmade profiles) i made with power saving in mind given the idle power consumption is rather high on modern ryzens. edit: actually made a mistake given that PBO2Tunner is for Zen3 cpus, and you mentioned Zen4. reply PedroBatista 19 hours agoprevGood for Intel to finally \"figure it out\" but I'm not 100% sure microcode is 100% of the problem. As in everything complex enough, the \"problem\" can actually be many compounded problems, MB vendors \"special\" tune comes to mind. But this is already a mess very hard to clean since I feel many of these CPUs will die in an year or 2 because of these problems today but by then nobody will remember this and an RMA will be \"difficult\" to say the least. reply johnklos 15 hours agoparentYou're right - at least partly. If the issue is that Intel was too aggressive with voltages, they can use microcode updates as 1) an excuse to rejigger the power levels and voltages the BIOS uses as part of the update, and 2) they can have the processor itself be more conservative with the voltages and clocking it calculates itself. Anything Intel announces, in my experience, is half true, so I'm interested to see what's actually true and what Intel will just forget to mention or will outright hide. reply eigenform 10 hours agoprevby \"microcode\" i assume they meant \"pcode\" for the PCU? (but they decided not to make that distinction here for whatever reason?) reply whalesalad 15 hours agoprevIf I didn’t just recently invest in 128gb of DDR4 I’d jump ship to AMD/AM5. My 13900k has been (knock on wood) solid though - with 24/7 uptime since July 2023. reply thangngoc89 15 hours agoparentI guess you’re lucky. I own 2 machines for small scale CNN training, one 13900k and one 14900k. I have to throttle the CPU performances to 90% for stable running. This cost me about 1 hour / 100 hours of training. reply whalesalad 15 hours agorootparentAre you using any motherboard overclocking stuff? A lot of mobo’s are pushing these chips pretty hard right out of the box. I have mine at a factory setting that Intel would suggest, not the asus multi core enhancement crap. noctua dh15 cooler. It’s really been a stable setup. reply thangngoc89 14 hours agorootparentI didn’t setup anything in BIOS. But my motherboard are from asus. I will look into this. Thanks for your suggestion. reply Dunati 12 hours agorootparentMy 13900k has definitely degraded over time. I was running bus defaults for everything and the pc was fine for several months. When I started getting crashes it took me a long time to diagnose it as a CPU problem. Changing the mobo vdroop setting made the problem go away for a while, but it came back. I then got it stable again by dropping the core multipliers down to 54x, but then a couple months later I had to drop to 53x. I just got an rma replacement and it had made it 12 hours without issue. reply J_Shelby_J 12 hours agoparentprevI evaluated ddr4 vs ddr5 a year ago, and it wasn’t worth it. Chasing FPS and the cost to hit the same speed in ddr5 was just too high, and I’m glad I did. I’m on a 13700k and I’m also very stable. However, with the stock XMP profile for my ram I was very much not stable and getting errors and bsods within minutes on an occp burn in test. All I had to do was roll back the memory clock speed a few hundred mhz. reply christkv 20 hours agoprevThe amount of current their chips pull on full boost is pretty crazy. It would definitively not surprise me if some could get damaged by extensive boosting. reply firebaze 21 hours agoprevNice that Intel acknowledges there are problems with that CPU generation. If I read this right, the CPUs have been supplied with a too-high voltage across the board, with some tolerating the higher voltages for longer, others not so much. Curious to see how this develops in terms of fixing defective silicon. reply Night_Thastus 17 hours agoprev\"Elevated operating voltage\" my foot. We've already seen examples of this happening on non-OC'd server-style motherboards that perfectly adhere to the intel spec. This isn't like ASUS going 'hur dur 20% more voltage' and frying chips. If that's all it was it would be obvious. Lowering voltage may help mitigate the problem, but it sure as shit isn't the cause. reply sirn 11 hours agoparentIt's worth noting that W680 boards are not a server board, they're a workstation board, and often times they're overclockable (or even overclocked by default). Wendell actually showed the other day that the ASUS W680 board was feeding 253W into a 35W (106W boost) 13700T CPU by default[1]. Supermicro and ASRock Rack do sell W680 as a server (because it took Intel a really long time to release C266), but while they're strictly to the spec, some boards are really not meant for K CPUs. For example, the Supermicro MBI-311A-1T2N is only certified for a non-TVB E/T CPUs, and trying to run the K CPU on these can result in the board plumbing 1.55V into the CPU during the single core load (where 1.4V would already be on the higher side)[2]. In this particular case, the \"non-OC'd server-style motherboard\" doesn't really mean anything (even more so in the context of this announcement). [1]: https://x.com/tekwendell/status/1814329015773086069 [2]: https://x.com/Buildzoid1/status/1814520745810100666 reply dwattttt 17 hours agoparentprevThey also admit a microcode algorithm produces incorrect requests for voltages, it doesn't sound like they're trying to shift the blame; ASUS doesn't write that microcode reply paulmd 4 hours agoparentprevSpecifically I think the concerns are around idle voltage and overshoot at this point, which is indeed something configured by OEMs. edit: BZ just put out a video talking about running Minecraft servers destroying CPUs reliably, topping out at 83C, normally in the 50s, running 3600 speeds. Which is a clear issue with low-thread loads. https://m.youtube.com/watch?v=yYfBxmBfq7k reply acrispino 20 hours agoprevAn Intel employee is posting on reddit: https://www.reddit.com/r/intel/comments/1e9mf04/intel_core_1... A recent YouTube video by GamersNexus speculated the cause of instability might be a manufacturing issue. The employee's response follows. Questions about manufacturing or Via Oxidation as reported by Tech outlets: Short answer: We can confirm there was a via Oxidation manufacturing issue (addressed back in 2023) but it is not related to the instability issue. Long answer: We can confirm that the via Oxidation manufacturing issue affected some early Intel Core 13th Gen desktop processors. However, the issue was root caused and addressed with manufacturing improvements and screens in 2023. We have also looked at it from the instability reports on Intel Core 13th Gen desktop processors and the analysis to-date has determined that only a small number of instability reports can be connected to the manufacturing issue. For the Instability issue, we are delivering a microcode patch which addresses exposure to elevated voltages which is a key element of the Instability issue. We are currently validating the microcode patch to ensure the instability issues for 13th/14th Gen are addressed reply hsbauauvhabzb 19 hours agoparentSo they were producing defective CPUs, identified & addressed the issue but didn’t issue a recall, defect notice or public statement relating to the issue? Good to know. reply Dylan16807 19 hours agorootparentIt sounds like their analysis is that the oxidation issue is comfortably below the level of \"defective\". No product will ever be perfect. You don't need to do a recall for a sufficiently rare problem. And in case anyone skims, I will be extra clear, this is based on the claim that the oxidation is separate from the real problem here. reply abracadaniel 19 hours agorootparentThey could recall the defective batch. All of the cpus with that defect will fail from it. The seem to have been content to hope no one noticed. reply Dylan16807 16 hours agorootparentWhat makes you think there was a \"defective batch\"? What makes you think all the CPUs affected by that production issue will fail from it? That description sounds to me like it affected the entire production line for months. It's only worth a recall if a sufficient percent of those CPUs will fail. (I don't want to argue about what particular percent that should be.) reply hsbauauvhabzb 14 hours agorootparentMy CPU was unstable for months, I spent tens of hours and hundreds on equipment to troubleshoot (I _never_ thought my CPU would be the cause). Had I of known this, I would have scrutinised the cpu a lot faster than what I did. Intel not making a public statement about potentially defective products could have been done with good PR spin ‘we detected an issue, believe the defect rate will beWell they didn't notice it for a good while, so it's really hard to say how much impact it had. That negates any arguments you had related to failure rates. > The complaint you're making depends on very particular parts of their statement being true but other very particular parts being not true Er, I’m not even sure how to respond to this. GamersNexus has indicated they know about the oxidisation issue, intel *subsequently* confirm it was known internally but no public statement was made until now. I’m not unreasonably cherry picking parts of their statement and then drawing unreasonable conclusions. Intel have very clearly demonstrated they would have preferred to not disclose an issue in fabrication processes which very probably caused defective CPUs, they have demonstrated untrustworthy behaviour related to this entire thing (L1techs and GN are breaking the defective cpu story following leaks from major intel clients who have indicated that intel is basically refusing to cooperate). Intel has known about these issues for some time and said nothing. They have cost organisations and individuals time and money. Nothing they say now can be trusted unless it involves them admitting fault. reply Dylan16807 12 hours agorootparent> That negates any arguments you had related to failure rates. I mean it's hard for us to say, without sufficient data. But Intel might have that much data. Also what argument about failure rates? The one where I said \"if\" about failure rates? > Er, I’m not even sure how to respond to this. GamersNexus has indicated they know about the oxidisation issue, intel subsequently confirm it was known internally but no public statement was made until now. GamersNexus thinks the oxidation might be the cause of the instability everyone is having. Intel claims otherwise. Intel has no reason to lie about this detail. It doesn't matter if the issue is oxidation versus something else. Also the issue Intel admits to can't be the problem with 14th gen, because it only happened to 13th gen chips. > Intel has known about these issues for some time and said nothing. Nothing they say now can be trusted unless it involves them admitting fault. If you don't trust what Intel said today at all, then you can't make good claims about what they knew or didn't know. You're picking and choosing what you believe to an extent I can't support. reply wslh 19 hours agorootparentprevIt is the Pentium FDIV drama all over again! [1]. It is even in chapter 4 of the Andrew Grove's book! [1] https://en.wikipedia.org/wiki/Pentium_FDIV_bug reply thelastparadise 19 hours agorootparentprevDude's gonna be canned so hard. reply loufe 21 hours agoprevIntel cannot afford to be anything but outstanding in terms of customer experience right now. They are getting assaulted on all fronts and need to do a lot to improve their image to stay competitive. reply scrlk 19 hours agoparentIntel should take a page out of HP's book when it came to dealing with a bug in the HP-35 (first pocket scientific calculator): > The HP-35 had numerical algorithms that exceeded the precision of most mainframe computers at the time. During development, Dave Cochran, who was in charge of the algorithms, tried to use a Burroughs B5500 to validate the results of the HP-35 but instead found too little precision in the former to continue. IBM mainframes also didn't measure up. This forced time-consuming manual comparisons of results to mathematical tables. A few bugs got through this process. For example: 2.02 ln ex resulted in 2 rather than 2.02. When the bug was discovered, HP had already sold 25,000 units which was a huge volume for the company. In a meeting, Dave Packard asked what they were going to do about the units already in the field and someone in the crowd said \"Don't tell?\" At this Packard's pencil snapped and he said: \"Who said that? We're going to tell everyone and offer them, a replacement. It would be better to never make a dime of profit than to have a product out there with a problem\". It turns out that less than a quarter of the units were returned. Most people preferred to keep their buggy calculator and the notice from HP offering the replacement. https://www.hpmuseum.org/hp35.htm reply basementcat 19 hours agorootparentI wonder if Mr. Packard's answer would have been different if a recall would have bankrupted the company or necessitated layoff of a substantial percentage of staff. reply scrlk 18 hours agorootparentI can't speak for Dave Packard (or Bill Hewlett) - but I will try to step in to their shoes: 1) HP started off in test and measurement equipment (voltmeters, oscilloscopes etc.) and built a good reputation up. This was their primary business at the time. 2) The customer base of the HP-35 and test and measurement equipment would have a pretty good overlap. Suppose the bug had been covered up, found, and then the news about the cover up came to light? Would anyone trust HP test and measurement equipment after that? It would probably destroy the company. reply rasz 7 hours agorootparentprevOr potential of killing couple hundred passengers, or few astronauts. Oh, wait... reply Joel_Mckay 19 hours agoparentprevTheir acquisition of Altera seemed to harm both companies irreparably. Any company can reach a state where the Process people take over, and the Product people end up at other firms. Intel could have grown a pair, and spun the 32 core RISC-V DSP SoC + gpu for mobile... but there is little business incentive to do so. Like any rotting whale, they will be stinking up the place for a long time yet. =) reply beacon294 19 hours agorootparentCould you elaborate on the process people versus product people? reply basementcat 19 hours agorootparentI would argue the fabrication process people at Intel are core to their business. Without the ability to reliably manufacture chips, they're dead in the water. reply Joel_Mckay 19 hours agorootparentYou mean manufacturing \"working chips\" is supposed to be their business. It is just performance art with proofing wafers unless the designs work =3 reply bgmeister 19 hours agorootparentprevI assume they're referring to Steve Jobs' comments in this (Robert Cringely IIRC) interview: https://www.youtube.com/watch?v=l4dCJJFuMsE (not a great copy, but should be good enough) reply beacon294 19 hours agorootparentOh yeah, this got rehashed as builders versus talkers too. Yeah, there's a lot of this creative vibe type dividing. It's pretty complicated, I don't even think individual people operate the same when placed in a different context. Usually their output is a result of their incentives, so typically management failure or technical architect failure. reply Joel_Mckay 19 hours agorootparentprevPartly true, Steve Jobs had a charismatic tone when describing these problems in public. Have a great day, =3 reply Joel_Mckay 19 hours agorootparentprevIt is an old theory that accurately points out Marketing/Sales division people inevitably out-compete product innovation people in a successful firm. https://en.wikipedia.org/wiki/Competitive_exclusion_principl... And yes, the Steve Jobs interview does document how this almost destroyed Apples core business. =) reply issafram 21 hours agoprevnext [4 more] [flagged] kjeldsendk 21 hours agoparentWithout those weirdos do you think Intel would be doing anything about this in public? And tell us how customers that bought the most expensive part from their lineup should feel about knowing that their cpu has been over voltaged from day one of operation.. reply sangeeth96 21 hours agoparentprevYeah sure, calling out Intel for lack of any good updates over the crashing laptop/desktop CPUs and demanding a recall after giving them such a long time to come up with a reasonable solution is definitely \"weirdo\" territory. FWIW, I have connections who splurged on these only to deal with BSODs all the friggin' time. Some of them even work at Intel. reply x3n0ph3n3 20 hours agoparentprevWe have yet to see - How much lifespan of these CPUs has already been lost and cannot be recovered by the microcode patch. - How much of a performance hit these CPUs will get after applying the patch. reply xyst 14 hours agoprevWonder what Linus has to say on this. Dude knows how to rip into crappy Intel products reply weberer 10 hours agoparentTorvalds or the Youtube guy? reply happosai 4 hours agorootparentYes reply aruametello 33 minutes agorootparentI can imagine both will bash intel a bit. \"Linus Tech Tips\" for the gaming crowd situation (loss of \"paid for\" premium performance) and Torvalds for the hardware vendor lack of transparency with the community. reply fefe23 20 hours agoprevSo on one hand they are saying it's voltage (i.e. something external, not their fault, bad mainboard manufacturers!). On the other hand they are saying they will fix it in microcode. How is that even possible? Are they saying that their CPUs are signaling the mainboards to give them too much voltage? Can someone make sense of this? It reminds me of Steve Jobs' You Are Holding It Wrong moment. reply aseipp 19 hours agoparentSaying \"elevated voltage causes damage\" is not attributing blame to anyone. In the very next sentence, they then attribute the reason for that elevated voltage to their own microcode, and so it is responsible for the damage. I literally do not know how they could be any clearer on that. reply pitaj 19 hours agoparentprev> Are they saying that their CPUs are signaling the mainboards to give them too much voltage? Yes that's exactly what they said. reply dboreham 19 hours agorootparentSo it's a 737 MAX problem: the software is running a control loop that doesn't have deflection limits. So it tells the stabilizer (or voltage reg in this case) to go hard nose down. reply nahnahno 6 hours agorootparentlol what a stretch of an analogy reply wtallis 19 hours agoparentprevThe voltage supplied by the motherboard isn't supposed to be constant. The CPU is continuously varying the voltage it's requesting, based primarily on the highest frequency any of the CPU cores are trying to run at. The motherboard is supposed to know what the resistive losses are from the VRMs to the CPU socket, so that it can deliver the requested voltage at the CPU socket itself. There's room for either party to screw up: the CPU could ask for too much voltage in some scenarios, or the motherboard's voltage regulation could be poorly calibrated (or deliberately skewed by overclocking presets). On top of all this mess: these products were part of Intel's repeated attempts to move the primary voltage rail (the one feeding the CPU cores) to use on-die voltage regulators (DLVR). They're present in silicon but unused. So it's not entirely surprising if the fallback plan of relying solely on external voltage regulation wasn't validated thoroughly enough. reply basementcat 19 hours agoparentprevMy guess is something like the following: Modern CPU's are incredibly complex machines with a ridiculously large amount of possible configuration states (too large to exhaustively test after manufacture or sim during design), e.g. a vector multiply in flight with an AES encode in flight with x87 sincos, etc. Each operation is going to draw a certain amount of current. It is impractical to guarantee each functional unit with the required current but the supply rails are sized for a \"reasonable worst case\". Perhaps an underestimate was mistakenly made somewhere and not caught until recently. Therefore the fix might be to modify the instruction dispatcher (via microcode) to guarantee that certain instruction configurations cannot happen (e.g. let the x87 sincos stall until the vector multiply is done) to reduce pressure on the voltage regulator. reply hedgehog 18 hours agorootparentIt's worse than that, thermal management is part of the puzzle. Think of that as heat generation happening across three dimensions (X + Y + time) along with diffusion in 3D through the package. reply CoastalCoder 17 hours agorootparentIt's an interesting idea, but there's a caveat: time flows in just one direction. reply ls612 19 hours agoparentprevThe claim seems to be that the microcode on the CPU is in certain circumstances requesting the wrong (presumably too high) voltage from the motherboard. If that is the case fixing the microcode will solve the issue going forward but won’t help people whose chips have already been damaged by excessive voltage. reply cqqxo4zV46cp 19 hours agoparentprevThe “you’re holding it wrong!”angle is all your take. They don’t make that claim. reply k12sosse 18 hours agorootparent\"OK, great, let’s give everybody a case\" lives on reply ChrisArchitect 18 hours agoprev [–] (updated from other post about mobile crashes) Related: Complaints about crashing 13th,14th Gen Intel CPUs now have data to back them up https://news.ycombinator.com/item?id=40962736 Intel is selling defective 13-14th Gen CPUs https://news.ycombinator.com/item?id=40946644 Intel's woes with Core i9 CPUs crashing look worse than we thought https://news.ycombinator.com/item?id=40954500 Warframe devs report 80% of game crashes happen on Intel's Core i9 chips https://news.ycombinator.com/item?id=40961637 reply silisili 18 hours agoparentThat one is mobile, this one is desktop, which they claim are different causes. reply tedunangst 18 hours agoparentprev [–] Not a dupe. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel has identified instability in its 13th and 14th Gen desktop processors caused by elevated operating voltage from a microcode algorithm.",
      "A microcode patch to address this issue is expected by mid-August, and Intel advises affected customers to contact support for assistance.",
      "Users have expressed concerns about potential long-term CPU damage, the need for BIOS updates, and have reported RMA (Return Merchandise Authorization) issues while seeking guidance on managing the instability until the patch is released."
    ],
    "commentSummary": [
      "Intel's July 2024 update indicates a microcode issue in 13th/14th Gen desktop CPUs causing incorrect voltage requests, though some users suspect a hardware fault.",
      "Concerns arise due to Intel's delay in addressing the problem and reports of non-faulty CPUs shipping without updated microcode, potentially leading to long-term CPU degradation.",
      "Intel plans to release a microcode patch, but its effectiveness and impact on performance are uncertain, with users reporting mixed experiences regarding CPU stability."
    ],
    "points": 300,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1721681702
  },
  {
    "id": 41043371,
    "title": "Database Design for Google Calendar: A Tutorial",
    "originLink": "https://kb.databasedesignbook.com/posts/google-calendar/",
    "originBody": "Posts Database Design for Google Calendar: a tutorial May 20, 2024 Author: Alexey Makhotkin squadette@gmail.com. Introduction In this database design tutorial (~9000 words) I’m going to show how to design the database tables for a real-world project of substantial complexity. We’ll design a clone of Google Calendar. We will model as much as possible of the functionality that is directly related to the calendar. This series illustrates an approach explained in the book called “Database Design using Minimal Modeling”, scheduled to be released in Summer 2024. Here is the website of the book: https://databasedesignbook.com/. You can leave your email address to receive occasional updates on the book and related materials. We will first build a complete logical model that describes the calendar data to be stored. This should take most of the effort (~80% of text by word count). After the logical model is finished, we’ll build table design directly based on the logical model. Intended audience The goal of the book is to help you get from a vague idea of what you need to implement (e.g.: “I need to build a website to manage schedule and instructor appointments for our gym”), to the full and complete definition of database tables. The first three quarters of the text require only a general understanding of what databases are and how information is stored there. Parts 1-6 only talk about logical models, that do not depend on the specific database that you use (MySQL, Postgres, other classic relational servers, NoSQL solutions, cloud databases, etc.). So the majority of the text describes how the business requirements are modeled. The last quarter of the text shows how to get from the logical model to physical tables structure. This part is absolutely not comprehensive, it only shows one of many possible approaches to the database table design. This approach, however, is perfectly valid for not very demanding systems. Also, many existing systems are designed in part using this strategy. This part of the text requires more familiarity with common databases: how the tables are created, what physical data types exist, what is primary key and index, how the tables would be queried, and how to insert and update data. Table of Contents Introduction Approach of this book Problem description Part 1: Basic all-day events Anchors Attributes of User Attributes of DayEvent Links A peek into the physical model Part 2: Time-based events Time zones Anchors Attributes of Timezone Attributes of TimeEvent Links Similarities between DateEvent and TimeEvent Part 3. Repeated all-day events Attribute #1, cadence Attribute #2, tangled attributes Attribute #3 Days of the week: micro-anchors Are we done? Repeat limit: more tangled attributes Part 4. Rendering the calendar page A note on tempo General idea Day slots Exercise: TimeSlots How far ahead do you need to think? Part 5. Rendering the calendar page: time-based events. Part 6. Complete logical model so far Part 7. Creating SQL tables Anchors: choose names for tables Attributes: choose the column name and physical type 1:N Links M:N links Finally: the tables Conclusion What’s next? Approach of this book Often people start with designing the tables right away, but we take a different approach. This tutorial is aimed at people who are new to database design. The goal of the process is to answer several important questions: Where to begin? How to make sure that we did not miss anything? how to ask for feedback on our database design; how to fix design mistakes; We begin with a logical model, written in a simple tabular format. We use short formalized sentences to define data attributes and relationships between entities. This helps us to make sure that our logical model aligns with the actual business requirements. Logical model is independent from a specific database implementation. As the second step, when the logical model is decided, we design the physical tables. This process is very straightforward. For each element of the logical model there would be a corresponding table or column. Physical models can be as dependent on a specific database implementation as you need. Problem description We’re going to implement a big part of Google Calendar functionality. Some parts we’ll skip, but we’ll try and implement every feature of calendaring. Some areas we’ll implement just enough to be able to discuss the more interesting parts. In the end you will be able to add missing functionality to the schema, going through the same process. Google Calendar is a multi-user system. For example, users can share the events with other people. We’re going to implement only a bare minimum of user-related data. Events are the central part of Google Calendar, and we’re going to design them as closely as possible to the real thing. Events have title and description, as well as some other minor attributes such as location. The most complex part of calendar events is times and dates: “All day” events vs time-based; Both can be repeated and non-repeated; All day events: Can spread over multiple days; Time-based events: Can have associated time zone; Have begin and end time; Begin and end time can happen on different days; Begin and end time can be in different timezones; Both all-day and time based events: Can be repeated daily, or every N days; Can be repeated weekly, on certain days of the week; again, it can be every two or more weeks; Can be repeated monthly, on a certain day or day of the week; Can be repeated annually; Repeating events can go on forever, until a certain date, or for a certain number of repetitions; For the repeating events, the specific instances can be moved to a different date/time. You can also delete some instances of the repeating events, for example, skipping a certain weekly meeting. You can change the schedule of the repeated event, even if some of the events already happeneds. For example, you can switch from two project meetings every week on Tuesday and Thursday to one meeting every two weeks, on Fridays. Here is a screenshot of a day event editing form: Part 1: Basic all-day events Anchors First thing we need is to find some so-called anchors. Anchors are also known as entities. Anchors are usually nouns, such as User and Event. Anchors are something that can be counted. “No users”, “one user”, “two users”, etc. Also, one defining characteristic of an anchor is that it can be added: “adds a user record to the database”. Anchors are extremely obvious in simple cases, but may get tricky in non-obvious cases. We’re going to write down even the most obvious anchors, to get some experience with handling them. The first two anchors that come to mind are: Anchor Physical table UserDayEventBasically the only thing that anchors handle is IDs and counting. All the data is handled by attributes, discussed in the next section. So, for example, in our database tables there would be a User with, say, ID=23, and DayEvent with ID=100, etc. We won’t be dealing with the last column (“Physical table”) for now, we’ll discuss the physical model in the “Creating SQL Tables” section below. To validate that we have an anchor, we can write a sentence using the name of this anchor. If this sentence makes sense, then this is an anchor. Example sentences: “There are 200 Users in our database”; “When this form is submitted, a new User is added to the database”; Same for DayEvent: “There are 3000 DayEvents in our database”; “When this button is clicked, a new DayEvent is created”; Such sentences will be useful later in more complicated cases. If the sentence does not make sense, then it may be an attribute: “There are 400 Prices in our database” (???); “When this form is submitted, a new Price is added to the database” (???). Attributes of User Attributes store the actual information about anchors. Which data about users should we model? Users are ubiquitous, and different systems may want to store a lot of information about users. For this post, we’re going to model just the bare minimum of user data: emails. Anchor Question Logical type Example value Physical column Physical type User What is the email of this User? string “cjdate@example.org” What can we see here? This attribute belongs to the User anchor that was defined in the previous section; We use questions to describe all sorts of attributes. Later on we’ll discuss why we prefer this style over “User’s email” and such; Logical type is quite simple. If you expected to see “VARCHAR(128)” here or something like that: no, we’ll discuss this much later. We show an example value that helps us to confirm our thinking. Again, in simple cases this is very obvious, but it would help reviewers to confirm that everyone is on the same page. We won’t be dealing with two last columns for now, we’ll discuss the physical model later in this post. We’re going to see more examples of logical types later. We extensively discuss the logical types in the book. Let’s skip ahead a little bit, and show how the table data about the users looks like. We’ll be using a simple strategy to design physical tables, so the result is going to be completely unsurprising: Table users id email … 2 “cjdate@example.org” … 3 “someone@else.com” … … … … This is just to show the part of the final result so that you know where we’re heading to. Here, a user with ID=2 has email “cjdate@example.org”, and a user with ID=3 has email “someone@else.com”. Except for this, we won’t be talking much about the users here. Attributes of DayEvent Suppose that we want to schedule a two-day company retreat that begins on January 14th, 2024. In terms of anchors, this is going to be a DayEvent. Looking at the paragraph above, we can see that we need to store the following data about the DayEvents: Name of the event; Begin date and end date of the event. Let’s write that down in our table: Anchor Question Logical type Example value Physical column Physical type DayEvent What is the name of this DayEvent? string “Company retreat” DayEvent When does the DayEvent begin? date 2024-01-14 DayEvent When does the DayEvent end? date 2024-01-15 What can we see here? We defined our first three attributes; We don’t have any short names for the attributes, and this may bother us a little. We’d expect to have something like “DayEvent_name” or some other identification to refer to the attribute later in the text. We’ll return to this topic later. We have a new logical type: date. We won’t need to deal with timezones in this section. For most events in actual calendars the begin date and end date would probably be the same (most events are single-day). We’ll just store the same date in both attributes. This allows us to treat the special case (single-day events) as the general case (multi-day events). This is a general design strategy, but we’re going to investigate later if this line of thinking is always applicable. Links Where do we store the information that this particular user has created this particular DayEvent? At the first glance, this may look like an attribute of DayEvent, right? (Actually, no). Anchor Question Logical type Example value Physical column Physical type DayEvent Which User has created this DayEvent??? number??? 2??? Attributes cannot contain IDs. Instead, when two anchors are involved, we need to use links. ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column User ”). It is the same as “1:N”, but additionally it allows you to specify which anchor is “only one” and which is “several”. We use two formalized sentences that involve two anchors, a verb, and information about cardinality. Those sentences allow us to validate and document our design. We write down the cardinality again, in a more familiar way in a separate column. Pinning down cardinality is essential, so we make a lot of fuss about it. (We’ll discuss the links much more, of course). A peek into the physical model Again skipping ahead: if we would have stopped right now, and tried to write down the physical design for the schema that we have so far, here is what we’d see. This is just to confirm that we’re heading in a familiar direction. Table: day_events id name begin_date end_date user_id 20 “Company retreat” 2024-01-14 2024-01-15 3 … … … … … We’ll discuss the physical model later in the “Creating SQL Tables” section. Part 2: Time-based events In the previous section, we discussed basic non-repeating date-based events. Let’s see how our modeling approach handles time-based events. We modeled date-based events as the DayEvent anchor with the following attributes: What is the name of this DayEvent?; When does the DayEvent begin? When does the DayEvent end? Also, we defined the link between User and DayEvent: “User creates several DayEvents”. Let’s write down a quick draft of time-based events and see how it compares to date-based events. Quoting the “problem description” section: “Time-based events: Can have associated time zone; Have begin and end time; Begin and end time can happen on different days; Begin and end time can be in different timezones;” Time zones Every country and territory uses one or more time zones. Time zone definitions occasionally change. Each country, being a sovereign state, can decide to change their time zone definition. Time zones may use Daylight Savings Time, or can be uniform. New time zones may be introduced, or retired. In this text, we won’t go into complications of handling time zone definitions. If you were really implementing a serious global calendaring solution, you’d probably have a separate team dealing with such issues. However, in this tutorial we will implement full timezone-aware events that are usable in practice. We have one motivating example: plane tickets. Planes often cross time zone boundaries, and the take-off and landing times in your ticket would be in different time zones. Say, there is a flight from Amsterdam to London that departs on December 24, at 16:50 (Amsterdam time) and lands at 17:05 (London time). So, the flight duration is 1 hour 15 minutes. Time zones inspire a lot of programming folklore. There are many blog posts, horror stories, “things every programmer should know” and other texts related to time zones, particularly in database context. Also, many systems keep breaking in various ways around the daylight switch time. We will discuss only as much as needed for our purposes, and briefly mention some important things to consider. Anchors Having said that, it seems that we need to add two anchors: Anchor Physical table TimezoneTimeEventThere are dozens of time zones in the world. We can confirm the validity of the Timezone anchor by writing down example sentences: “There are 120 Timezones in our database”; “When this import script finishes, a new Timezone is added to our database”. (Timezone data structure is discussed below.) The sentences for TimeEvent are also straightforward: “There are 2500 TimeEvents in our database”; “When this button is clicked, a new TimeEvent is created”; Attributes of Timezone For the purposes of this text, we’ll do only a very minimal model of Timezone. Basically, the only attribute we’d introduce is: Anchor Question Logical type Example value Physical column Physical type Timezone What is the human-readable name of this Timezone? string “Europe/Kyiv” We won’t go into details of how the timezone is actually defined. We assume that there is a complementary logical model that describes the structure of timezones. Also, we assume that there is some function that takes a local time in the specified timezone and returns UTC time (and vice-versa). This will be discussed in more detail in the next section, when we will talk about repeating events. For clarity, here is what else would be included into a time zone definition: What is the UTC offset of this time zone? Does this time zone have Daylight Savings Time? When does DST begin? When does DST end? What is the UTC offset when DST is on? We’d also need to model previous definitions of a time zone. For example, the government may decide to change the day when DST goes into force, or get rid of DST, etc. Is this time zone active or retired? This is an incomplete list. Modeling all of this data using our approach is possible, but is a separate, and quite technical exercise. Let’s get back to events. Attributes of TimeEvent Anchor Question Logical type Example value Physical column Physical type TimeEvent What is the name of this TimeEvent? string “Catch-up meeting” TimeEvent When does the TimeEvent begin? date/time (local) 2024-01-14 12:30 TimeEvent When does the TimeEvent end? date/time (local) 2024-01-14 13:15 Note that we’re using local time here. You may have read that time should be stored in UTC time (without any time zones), and then formatted for human readability using a preferred time zone. Here we have a different situation. Time zones can change. Suppose that we scheduled a billiard game on September 6, 2058, from 09:30 to 11:00, Cologne time. At the moment we don’t know what UTC offset is going to be in that time zone at that time. So we must store the data exactly as entered by the user, and then adjust it as the local legislation changes. Links We have two very similar links here. ⚓Anchor1* ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column TimezoneBETWEEN ‘2024-02-26’ AND ‘2024-03-26’; Maybe this would even be several SQL queries, or even some code in a programming language. The data structure that we’ve considered so far is quite complicated. To find the events that must be shown on a certain week, you need to take a lot into account. This may quickly become impractical. A note on tempo I’ve been writing this chapter in the course of a few months. I’ve been thinking about this problem a lot. I have a quite clear understanding of the end state that I have in mind, I just need to write it down. But if I were to present to you the complete table design right now, it wouldn’t be useful for our goal: to learn database design. You won’t understand why I made certain decisions. At the same time, I don’t want to present very small incremental changes, so that text is not too long. So we need to find some middle ground. We started this section with a question of how to render a weekly page. Let’s remember another requirement that our calendar application definitely has: changing and canceling some events from the series. Say, you have ten weekly project meetings, but you want to cancel one of them because the weather is very good. The existing book-keeping part of our database model won’t change. But we need to add some anchors, attributes and links for the second half: rendering and modification. General idea We want to introduce a new anchor that would store the information about each event in the series. So, if we have 10 weekly project status meetings, we’re going to have ten rows in some table. Each record would correspond to a specific date (e.g., 2024-02-12, 2024-02-19, etc.). First, this would make our rendering very simple. You have a very easy way to find all the events that fall on a specific day. Second, this would allow us to reschedule and cancel some events in the series. If we have a project meeting at 12:00 normally, but on a certain week we want to move it to 14:00 (or even to a different day), we can do that. Data in the original book-keeping anchor, TimeEvent, that we defined previously, won’t change. Also, if we just want to skip one project status meeting, we can mark this particular day as skipped. Day slots First, we have to find a name for those things. In some cases this may be a challenge. Just five minutes ago, having another cup of tea, I realized that a good word for this thing is “slot”. Also, like before, we’re going to treat per-day and time-based events differently. So, we’re going to have DaySlot and TimeSlot anchors. Let’s discuss per-day slots first. Anchor Physical table DaySlotDaySlot needs a surprisingly small number of attributes: Anchor Question Logical type Example value Physical column Physical type DaySlot On which day does this DaySlot happen? date 2024-02-12 DaySlot Is this DaySlot skipped? yes/no yes Note that the user can change the date for a specific slot! So, we see that this requirement is handled cleanly. Also, we need to establish a link between DaySlot and the corresponding DayEvent. ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column DayEvent < DaySlot DayEvent may generate several DaySlots DaySlot corresponds to only one DayEvent 1:NSome things to note: first, we always create a DaySlot for every DayEvent, even for non-repeating ones. This is needed to simplify the rendering code. Second, we have an interesting problem with “infinite” events. Suppose that we added our friend’s birthday to the calendar, repeating annually. How many corresponding DaySlots do we need to create? One possible solution would be to choose some arbitrary limit such as 100 years in the future, and create all the slots for that. Other solutions are possible, such as on-demand creation when user requests to show a calendar page in some distant future. Third, if you think about this, it’s possible that more information could also be different for each slot. For example, it’s possible that some meetings will take place in different locations. Also, the guest list may change: you can invite extra people to a certain meeting. Also, the attendance would certainly be different. We won’t cover those aspects here, because modeling this it’s pretty straightforward: just add more links that connect DaySlot with other anchors. Also, one thing to consider is that date arithmetics needs a bit of care. How do we deal with birthdays of people who were born on February 29? We will have to decide something. Maybe we’ll prohibit the user from creating such events? Maybe we’ll ask them where to move the slot: day earlier or day later? Similar problem also exists for monthly events that happen on the 31st day of the month. Exercise: TimeSlots Here is an exercise for the determined reader. Think about how you would model the TimeSlot anchor, its attributes and links. Fill in the tables with anchors, links and attributes, using the format explained above. Think about what role would time zones play. How far ahead do you need to think? Sometimes you can create a better design if you consider a slightly broader set of requirements. We did it in this chapter: we started thinking about rendering the page, but then we also considered the need to modify some events in the series. Sometimes you can create a better design if you consider requirements independently. In the previous chapters, we looked at time-based and per-day events, and decided to handle them separately for now. So far we did not even mention any hypothetical future requirements. We’ve only been designing stuff that we know is needed. Can we think up something that would be nice to have in the future, and design ahead? If you only include known requirements, there is a chance of overfitting design for known data points. At the same time, people sometimes introduce considerations that never actually materialize. In this case design may introduce extra complexity that is not needed for the actual requirements, adding a bit of friction. There is always a chance of leaning towards one of those traps, or even falling into them. Logical design based on Minimal Modeling insists on modeling only the parts that we actually know are needed. We can afford that because Minimal Modeling treats changing requirements as a given. Also, Minimal Modeling discourages you from adding more abstract concepts, and this is intentional. We’ll discuss this aspect of physical design later in the book. We’ll introduce the concept of Game of Tables, and the idea of Date’s Demon. Part 5. Rendering the calendar page: time-based events. For repeated time-based events, we choose the same approach as with all-day events. We’re going to introduce an anchor called “TimeSlot”. TimeSlot corresponds to a specific event on a specific date and time. A repeated event corresponds to several TimeSlots. Time slots can be rescheduled or canceled manually, just like all-day slots. Here is an anchor: Anchor Physical table TimeSlotAnd the attributes: Anchor Question Logical type Example value Physical column Physical type TimeSlot When does the TimeSlot begin? date/time (local) 2024-01-14 12:30 TimeSlot When does the TimeSlot end? date/time (local) 2024-01-14 13:15 TimeSlot Is this TimeSlot skipped? yes/no yes A specific time slot can generally be moved even to a different day, so we have to keep this information also. Which time zone shall we use for the begin/end time? As you may remember from Part 2, in Google Calendar you can have different time zones for begin and end time. If you think about that, it makes sense to keep it for the time slots also. ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column Timezone < TimeSlot Timezone is used for the start time of many TimeSlots TimeSlot uses only one Timezone for the start time 1:NTimezone < TimeSlot Timezone is used for the end time of many TimeEvents TimeSlot uses only one Timezone for the end time 1:NAlso, we need to connect TimeSlots with TimeEvents, same as we did with DaySlots/DayEvents: ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column TimeEvent < TimeSlot TimeEvent may generate several TimeSlots TimeSlot corresponds to only one TimeEvent 1:NSame as with DaySlot, we would create a TimeSlot even for non-repeated TimeEvents. Part 6. Complete logical model so far Let’s go back and collect everything that we’ve designed so far. First, the complete list of anchors (7 in total): Anchor Physical table ID example User Timezone DayEvent TimeEvent DayOfTheWeek“Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun” DaySlot TimeSlot Second, list of attributes (ordered by anchor): Anchor Question Logical type Example value Physical column Physical type User What is the email of this User? string “cjdate@example.org” Timezone What is the human-readable name of this Timezone? string “Europe/Kyiv” DayEvent What is the name of this DayEvent? string “Company retreat” DayEvent When does the DayEvent begin? date 2024-01-14 DayEvent When does the DayEvent end? date 2024-01-15 DayEvent How often is that DayEvent repeated? either/or/or daily weekly monthly annually DayEvent For repeated events: what is the repetition step? integer 2 (every two days/weeks/etc) DayEvent For monthly repeated events: which day of the month does it fall on? either/or/or same_day same_weekday DayEvent For repeated events: for how long does the DayEvent repeat? either/or/or forever until_date N_repetitions DayEvent For events repeated until a certain date: what is the date? date 2024-01-17 DayEvent For events repeated for a certain number of reps: how many reps? integer 10 TimeEvent What is the name of this TimeEvent? string “Catch-up meeting” TimeEvent When does the TimeEvent begin? date/time (local) 2024-01-14 12:30 TimeEvent When does the TimeEvent end? date/time (local) 2024-01-14 13:15 DaySlot On which day does this DaySlot happen? date 2024-02-12 DaySlot Is this DaySlot skipped? yes/no yes TimeSlot When does the TimeSlot begin? date/time (local) 2024-01-14 12:30 TimeSlot When does the TimeSlot end? date/time (local) 2024-01-14 13:15 TimeSlot Is this TimeSlot skipped? yes/no no Third, list of links (in no particular order): ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column User < DayEvent User creates many DayEvents DayEvent is created by only one User 1:NUser < TimeEvent User creates many TimeEvents TimeEvent is created by only one User 1:NTimezone < TimeEvent Timezone is used for the start time of many TimeEvents TimeEvent uses only one Timezone for the start time 1:NTimezone < TimeEvent Timezone is used for the end time of many TimeEvents TimeEvent uses only one Timezone for the end time 1:NDayEvent = DayOfTheWeek For weekly repeated DayEvents: DayEvent may happen on several DaysOfTheWeek DayOfTheWeek can contain several DayEvents M:NTimeEvent = DayOfTheWeek For weekly repeated TimeEvents: TimeEvent may happen on several DaysOfTheWeek DayOfTheWeek can contain several TimeEvents M:NDayEvent < DaySlot DayEvent may generate several DaySlots DaySlot corresponds to only one DayEvent 1:NTimeEvent < TimeSlot TimeEvent may generate several TimeSlots TimeSlot corresponds to only one TimeEvent 1:NTimezone < TimeSlot Timezone is used for the start time of many TimeSlots TimeSlot uses only one Timezone for the start time 1:NTimezone < TimeSlot Timezone is used for the end time of many TimeEvents TimeSlot uses only one Timezone for the end time 1:NFinally, here is a diagram that shows all anchors and links (but not attributes): Part 7. Creating SQL tables In the previous chapters we defined the complete logical model, so most of the work is actually already done. The rest is pretty straightforward. For teaching purposes we’re going to use one specific table design strategy: “one table per anchor”. It is one of the most common approaches to physical table design. There are several more possible strategies, we’re going to discuss them in a book. We have 7 anchors, 21 attributes and 10 links so far. Given that we use “one table per anchor” strategy, we’re going to have 7 + 2 = 9 tables (number of anchors + number of M:N links), and 21 + 8 = 29 columns in total (number of attributes + number of 1:N links). If our logical design correctly describes the business requirements then the tables will be automatically correct. We’ll talk about evolving requirements in the book. Also, we’ll discuss design mistakes and how to fix them. We’re going to revisit the tables from the previous section, and fill in our choices: For anchors, fill in “Physical table” columns; For each attribute, fill in “Physical column”, and choose the “Physical type”; For each M:N link, choose the name of the physical table; For each 1:N link, fill in the column name in the table that corresponds to the N-side anchor; Anchors: choose names for tables Here we just choose a straightforward plural name for each table. Anchor Physical table ID example User usersTimezone timezonesDayEvent day_eventsTimeEvent time_eventsDayOfTheWeek days_of_the_week NB: this table may be virtual, see below “Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun” DaySlot day_slotsTimeSlot time_slotsSome companies or applications enforce different naming conventions (singular, camel case, etc.). In that case, you would just use the names that comply with the convention. Attributes: choose the column name and physical type For the physical column name, we choose some sensible name. For example: day_events.end_date would be the column name for the “When does the DayEvent begin?” attribute; time_slots.is_skipped would be the column name for the “Is this TimeSlot skipped?” attribute And so on. Due to the way relational databases work, you have to choose a very short name. In many cases this name by itself is not enough to fully explain the meaning of the data. That’s one of the reasons why we begin with the logical schema, and use longer human-readable questions to define the semantics of the attributes. For the physical type, we choose a sensible type without much discussion. This topic is covered extensively in the book. There is also a list of recommended data types for each logical type in the book, and we just use that directly. If you’re working with an existing system, you may be required to choose an alternative physical data type for the column. For example, your database server may support a better suited data type; or there could be some engineering guidelines that make you choose a different data type. We’re going to discuss this variance in the book. However, full discussion of all physical design concerns is well outside of the scope of any book. This is the stuff that you spend your career on learning, and it changes as new technologies emerge. Anchor Question Logical type Example value Physical column Physical type User What is the email of this User? string “cjdate@example.org” users.email VARCHAR(64) NOT NULL Timezone What is the human-readable name of this Timezone? string “Europe/Kyiv” timezones.name VARCHAR(64) NOT NULL DayEvent What is the name of this DayEvent? string “Company retreat” day_events.name VARCHAR(128) NOT NULL DayEvent When does the DayEvent begin? date 2024-01-14 day_events.begin_date DATE NOT NULL DayEvent When does the DayEvent end? date 2024-01-15 day_events.end_date DATE NOT NULL DayEvent How often is that DayEvent repeated? either/or/or daily weekly monthly annually day_events.repeated VARCHAR(24) NULL DayEvent For repeated events: what is the repetition step? integer 2 (every two days/weeks/etc) day_events.repetition_step INTEGER NULL DayEvent For monthly repeated events: which day of the month does it fall on? either/or/or same_day same_weekday day_events.repeated_monthly_on VARCHAR(24) NULL DayEvent For repeated events: for how long does the DayEvent repeat? either/or/or forever until_date N_repetitions day_events.repeated_until VARCHAR(24) NULL DayEvent For events repeated until a certain date: what is the date? date 2024-01-17 day_events.repeated_until_date DATE NULL DayEvent For events repeated for a certain number of reps: how many reps? integer 10 day_events.repeated_reps INTEGER NULL TimeEvent What is the name of this TimeEvent? string “Catch-up meeting” time_events.name VARCHAR(128) NOT NULL TimeEvent When does the TimeEvent begin? date/time (local) 2024-01-14 12:30 time_events.begin_local_time DATETIME NOT NULL TimeEvent When does the TimeEvent end? date/time (local) 2024-01-14 13:15 time_events.end_local_time DATETIME NOT NULL DaySlot On which day does this DaySlot happen? date 2024-02-12 day_slots.the_date DATE NOT NULL DaySlot Is this DaySlot skipped? yes/no yes day_slots.is_skipped TINYINT UNSIGNED NOT NULL DEFAULT 0 TimeSlot When does the TimeSlot begin, in local time? date/time (local) 2024-01-14 12:30 time_slots.begin_local_time DATETIME NOT NULL TimeSlot When does the TimeSlot end, in local time? date/time (local) 2024-01-14 13:15 time_slots.end_local_time DATETIME NOT NULL TimeSlot Is this TimeSlot skipped? yes/no yes time_slots.is_skipped TINYINT UNSIGNED NOT NULL DEFAULT 0 For this problem, we’ve used around half of logical attribute types: string: 4 attributes; date: 4 attributes; either/or/or: 3 attributes; integer: 2 attributes; date/time (local): 4 attributes; yes/no: 2 attributes. You can see that the physical definitions of attributes of the same logical type are almost the same. The only differences are: a) maximum length of strings; and b) NULL vs NOT NULL. We choose “NOT NULL” for attributes where the value always needs to be there due to business requirements. For example, the name of the event, or the start date of the all-day event. For tangled attributes, we choose nullable physical types (“NULL”). We discuss nullability in the book, but note that “NULL” only exists in the physical schema. Just as NULLs, so-called “sentinel values” also do not exist in logical modeling. 1:N Links For 1:N links, we add a column to the N-side anchor table. For example: ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column User < DayEvent User creates many DayEvents DayEvent is created by only one User 1:N day_events.user_id DayEvent < DaySlot DayEvent may generate several DaySlots DaySlot corresponds to only one DayEvent 1:N day_slots.day_event_id Choosing the column name is usually quite easy. The only complication could be when there are two and more different links between the same two anchors. We have that situation with the timezones, and we’ll use two different columns. M:N links For M:N links we must use a separate table for each link. Every such table will have almost identical structure, only the column names would be different. We only need to find a good name for such a table. There is no naming method that works in all cases, you will have to try some combinations, looking for readability. For links this is especially difficult because it’s not clear which of the two anchors is more important and should come first. Same as with attributes, due to the way relational databases work, the name of the table needs to be quite short. In many cases the name by itself is not enough to fully explain the meaning of the data. That’s one of the reasons why we prepare logical schema, and use human-readable sentences to define the semantics of the links. Anyway, here is the full table of links with the names chosen for the tables and columns (see the last column). ⚓Anchor1 * ⚓Anchor2 Sentences (subject, verb, object, cardinality) Cardinality (1:N, M:N, 1:1) Physical table or column User < DayEvent User creates many DayEvents DayEvent is created by only one User 1:N day_events.user_id User < TimeEvent User creates many TimeEvents TimeEvent is created by only one User 1:N time_events.user_id Timezone < TimeEvent Timezone is used for the start time of many TimeEvents TimeEvent uses only one Timezone for the start time 1:N time_events.start_timezone_id Timezone < TimeEvent Timezone is used for the end time of many TimeEvents TimeEvent uses only one Timezone for the end time 1:N time_events.end_timezone_id DayEvent = DayOfTheWeek For weekly repeated DayEvents: DayEvent may happen on several DaysOfTheWeek DayOfTheWeek can contain several DayEvents M:N day_event_dows TimeEvent = DayOfTheWeek For weekly repeated TimeEvents: TimeEvent may happen on several DaysOfTheWeek DayOfTheWeek can contain several TimeEvents M:N time_event_dows DayEvent < DaySlot DayEvent may generate several DaySlots DaySlot corresponds to only one DayEvent 1:N day_slots.day_event_id TimeEvent < TimeSlot TimeEvent may generate several TimeSlots TimeSlot corresponds to only one TimeEvent 1:N time_slots.time_event_id Timezone < TimeSlot Timezone is used for the start time of many TimeSlots TimeSlot uses only one Timezone for the start time 1:N time_slots.start_timezone_id Timezone < TimeSlot Timezone is used for the end time of many TimeEvents TimeSlot uses only one Timezone for the end time 1:N time_slots.end_timezone_id Finally: the tables As we mentioned in the previous section, we’re going to have 8 (eight) SQL tables: 6 for anchors and 2 for M:N links. One anchor (DayOfTheWeek) is special, so we don’t create a physical table for that. We use a very common approach to designing physical tables. Other approaches are also possible, but this discussion is outside the scope of this post. So, let’s just write down all the tables, and add all the attributes that we have. This is a very straightforward and even boring process at this point. CREATE TABLE users ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, email VARCHAR(64) NOT NULL ); CREATE TABLE timezones ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, name VARCHAR(64) NOT NULL ); CREATE TABLE day_events ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, user_id INTEGER NOT NULL, name VARCHAR(128) NOT NULL, begin_date DATE NOT NULL, end_date DATE NOT NULL, repeated VARCHAR(24) NULL, repetition_step INTEGER NULL, repeated_monthly_on VARCHAR(24) NULL, repeated_until VARCHAR(24) NULL, repeated_until_date VARCHAR(24) NULL, repeated_reps INTEGER NULL ); CREATE TABLE time_events ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, user_id INTEGER NOT NULL, start_timezone_id INTEGER NOT NULL, end_timezone_id INTEGER NOT NULL, name VARCHAR(128) NOT NULL, begin_local_time DATETIME NOT NULL, end_local_time DATETIME NOT NULL ); CREATE TABLE day_slots ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, day_event_id INTEGER NOT NULL, the_date DATE NOT NULL, is_skipped TINYINT UNSIGNED NOT NULL DEFAULT 0 ); CREATE TABLE time_slots ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT, time_event_id INTEGER NOT NULL, begin_local_time DATETIME NOT NULL, end_local_time DATETIME NOT NULL, start_timezone_id INTEGER NOT NULL, end_timezone_id INTEGER NOT NULL, is_skipped TINYINT UNSIGNED NOT NULL DEFAULT 0 ); CREATE TABLE day_event_dows ( day_event_id INTEGER NOT NULL, day_of_week VARCHAR(3) NOT NULL, PRIMARY KEY (day_event_id, day_of_week), KEY (day_of_week) ); CREATE TABLE time_event_dows ( time_event_id INTEGER NOT NULL, day_of_week VARCHAR(3) NOT NULL, PRIMARY KEY (time_event_id, day_of_week), KEY (day_of_week) ); Is that really it? Mostly, yes. Though we need to talk a bit about indexes and about the attributes that we’ve skipped for brevity. Most experienced database developers would look at the schema above and immediately notice that some “obvious” indexes are missing. For example, day_events.user_id must certainly be indexed. Unfortunately, there is no hard and fast rule on what columns (and combinations of columns) need to be indexed. That depends on how the tables are going to be queried by the application. The best book about database indexes is called “Use The Index, Luke” (https://use-the-index-luke.com/). Go read it. We will also discuss indexes in a bit more detail in the book. When we were talking about logical schema (especially in the beginning), we skipped some of the attributes, because they were very much similar to other attributes. For example, we would probably want to add the name of the user, and the column that stores the user’s password hash. Some of the data elements just don’t add anything new to this text, for example the event location, or the list of invited guests. As an exercise, you could go ahead and add the elements that we did not discuss, the ones that you’re interested in. Add a few rows to the catalog tables, fill in the contents of each cell, and then edit the schema definition above to include the missing pieces of data. Conclusion Here is a short summary of the process: start with a free-form text that describes the business problem you’re working on; write down the list of anchors, as explained above; use any collaborative tool, such as Google Docs; write down the list of attributes, as explained above, pay particular attention to the questions; write down the list of links, as explained above, pay particular attention to the sentences, because they help you make sure that you get cardinality right; create a graphical schema based on the logical model, if visual representation helps you think; fill in the physical model: table names, column names, physical data types; write down the SQL schema as a series of CREATE TABLE operators, using information from the previous step; submit the schema to your database server, fix typos, re-submit; share the logical model with your team; What’s next? This tutorial is basically a chapter from the upcoming book “Database Design using Minimal Modeling” (https://databasedesignbook.com/). You can leave your email and I’ll send you updates on the book progress and other materials on database design. Particularly I’m looking for motivated beta readers for the current book draft. Contact me if you’re interested (squadette@gmail.com). If this tutorial helped you in understanding some aspect of database design, I’d love to hear your feedback.",
    "commentLink": "https://news.ycombinator.com/item?id=41043371",
    "commentBody": "Database Design for Google Calendar: A Tutorial (databasedesignbook.com)269 points by fagnerbrack 11 hours agohidepastfavorite76 comments bigbones 10 hours agoA random event from my calendar serializes to 740 bytes of iCalendar. An extremely busy calendar containing one event every 15 minutes 9am-5pm 7 days per week only contains 11680 entries for a full year, which works out to around 8MB. Would suggest instead of designing a schema at all, a calendar is a good example of a problem that might be far better implemented as a scan. Optimizing an iCalendar parser to traverse a range of dumped events at GB/sec-like throughputs would mean the above worst-case calendar could be scanned in single-digit milliseconds. Since optimizing a parser is a much simpler problem to solve once than changing or adding to a bad data model after it has many users, and that the very first task involving your new data model is probably to write an iCalendar importer/exporter anyway, I think this would be a really great trade off. reply mosselman 9 hours agoparentWouldn't you have to re-invent things like ranges searches which are 'BETWEEN ... AND ...' queries in SQL? The same for finding events that are for users 1, 2 and 3, etc. In a real application you'd probably have some user accounts of some sort that are stored in a relational database already and then you'd suddenly have to scan for events in a directory that you then have to connect to those records in the database. So there might be some specific set of applications where you are right, but there are specific things that a database is really good at, which would make it a really good choice. With the proper indices you'd probably get the same or even better throughputs, unless you come up with some clever directory structure for your events, which would in fact be the same as an index and only on one dimension whereas in a database you'd be able to create indices for many dimensions and combinations of dimensions. So you are right, trade offs. reply bigbones 9 hours agorootparentI didn't mean to imply avoiding use of a database entirely, almost any DB system tasked with copying a few long strings around in a simple query won't perform much worse than a literal raw disk file. Even just something like: CREATE TABLE calendar(id, user_id, blob) reply mosselman 7 hours agorootparentYes sure. I can imagine though that normally you'd also want to be able to query on details of an event. In which case having most things in columns would make sense because you can combine it with JOIN queries, etc. Also, in the context of web applications, you probably already have a database and probably don't have persisted disks on your application servers, which then adds complexity to the file based scenario. In which case using blobs is a perfectly fine solution indeed. Still you are right that in many cases, let's say a desktop application, you are probably better off reading directly from tens of files on disk rather than having to deal with the complexity of a database. The same applies to vector databases. I read an article a few months ago that spoke about just storing vectors in files and looping through them instead of setting up a vector database and the performance was pretty much the same for the author's use case. reply dotancohen 7 hours agorootparentAnd then you quickly get to the not-author's-use cases where you have to start reinventing wheels, poorly. reply nolok 8 hours agorootparentprevIf I understood him correctly, I think this is where some language collections libraries ought to shine. PHP/Laravel collection or C# Link for exemple. Tell it how to load a record into the collection, add one liners for each criterias you want to define, and in a few dozen lines you're free to go. reply canucker2016 9 hours agoparentpreviCalendar is designed as an interchange format not as a storage format for fast access of calendar data. The format shows its age - you can tell it was designed before XML/JSON were \"hot\". see https://en.wikipedia.org/wiki/ICalendar reply bigbones 9 hours agorootparentIt is also a data model that amounts to a de facto standard across all real calendaring apps. The text serialization sure is ugly and who would want to touch it unless they had to, but that's just a parsing problem. The wiki page shows 14 component types, who is ever going to think of that many when designing a replacement schema? Why not just use the data model you already have 'for free' reply canucker2016 9 hours agorootparentIt's a data model for an early-mid 1990s calendaring app (Lotus Organizer / Microsoft Schedule+). If that works for you, go for it. reply kreetx 8 hours agorootparentBut what is the exact criticism here, e.g the serialized form doesn't look \"nice\" when you open the file? reply asmor 9 hours agorootparentprevShowed up to a doctors appointment during their lunch break once because iCal supports the time format \"TZ is whatever is local, trust me\" and the device I used to add an appointment was set to UTC, not UTC+1. My doctors calendar app vendor was pretty happy I found the root cause of their very occasional mystery appointment drift too. reply wodenokoto 6 hours agorootparent> “TZ is whatever is local, trust me”. New years is midnight, local time, wherever you are. Trust me. reply rockwotj 9 hours agorootparentprevIt’s like the email format - it’s the wild west trying to parse these things, it doesn’t help that date/time is already a tricky subject. reply sleepyhead 7 hours agorootparentprev> you can tell it was designed before XML/JSON were \"hot or you can tell it was designed by people who care more about performance. reply canucker2016 23 minutes agorootparentthe iCalendar rfc shows nov 1998 - https://www.rfc-editor.org/rfc/rfc2445 iCalendar is based on vCalendar, http://www.imc.org/pdi/vcal-10.txt - need to go to archive.org to see it, which shows Sept 1996. from wikipedia, XML is listed as first published in Feb 1998, JSON is early 2000s. edit: XML'd iCalendar, 2011 - https://datatracker.ietf.org/doc/html/rfc6321 JSON'd iCalendar, 2014 - https://datatracker.ietf.org/doc/html/rfc7265 reply dspillett 2 hours agorootparentprev> > you can tell it was designed before XML/JSON were \"hot > or you can tell it was designed by people who care more about performance. I doubt parsing iCal is significantly more performant than JSON for most use cases. In fact I can image it being less so in more cases than it is more so, and as close to the same as makes no odds in the vast majority of cases. reply HelloNurse 6 hours agoparentprevMaybe you can do without a DBMS, but you still need a good schema for your data, and it isn't a bunch of iCalendar records: for example, instead of a stable user ID there are only mutable and repeated email addresses. reply datr 9 hours agoparentprevSuch an approach might also provide a neater solution to the infinite time-slots problem mentioned in the article. reply chandureddyvari 10 hours agoprevThis article about dealing with recurrence in applications was an eye-opener for me. Definitely recommend giving it a read. https://github.com/bmoeskau/Extensible/blob/master/recurrenc... reply Sander_Marechal 7 hours agoparentFunny how the original article's advice and your article oppose each other. - OP says to always store a timezone with each date, yours says to convert everything to UTC (I agree with OP) - OP says to generate database rows for each event, yours says to not do that (I agree with yours) reply UglyToad 6 hours agorootparentHaving built recurring stuff in the past (date based with no time component, luckily for me) I think you gain a lot of usability gains for generating a row for each occurrence of the event. Inevitably the user will come back and say \"oh, I want it monthly except this specific instance\" or if it's a time based event \"this specific one should be half an hour later\". You could just store the exceptions to the rule as their own data-structure but then you need to correlate the exception to the scheduler 'tick' and if they can edit the schedule, well, you're S.O.O.L either way but I think having concrete occurrences is potentially easier to recover from. reply fendy3002 7 hours agorootparentprev> OP says to always store a timezone with each date Jon Skeet talked about this once. https://codeblog.jonskeet.uk/2019/03/27/storing-utc-is-not-a... What I take is convert everything to UTC is fine if it's historical data, even unix timestamp is fine. However for the future datetime, it's more complicated than that. reply sensanaty 9 hours agoprevThe worst I have ever messed up during an interview was building a simple booking system that had to do recurring appointments. I never felt that lost and confused trying to accomplish something in code since my early university days lol To this day whenever I have to work with anything datetime related I dread it, it just does not click in my head for some reason reply tonnydourado 7 hours agoparentAsking a design question like \"Design a booking system, but with recurring appointments\" is like asking \"Write a function to order a list of string, but it has to work with arbitrary UTF-8 strings, in any locale, respecting alphabetical order conventions\". It's a deceptively simple question that gets impossibly hairy if you try to make it work in a real-world, general, way. reply dmd 7 hours agorootparent(As someone who hires) the correct answer to any question involving datetimes is always \"use a [good] library - do not attempt to reimplement all of human civilization from scratch\". reply mattgreenrocks 6 hours agoparentprevDon’t feel bad about it. I was tasked with improving a calendar in a CMS to support a bunch more functions, and basically eaten alive by it. I had 8yrs of experience or so by then. It is deceptively hard and requires excellent data modeling skills. reply munchler 8 hours agoprevModeling a system is an under-appreciated skill. In a new domain, however, this should really start with an analysis of the entire problem so as to capture both the static structure of the system (i.e. class model) and its dynamic behavior (i.e. use cases). Jumping directly into a static database model tends to leave out the dynamic behavior. That might be OK in a simple CRUD app like this one, but could be a big mistake in more complex systems. reply apwheele 8 hours agoparentData scientist, and I have had a few examples of seemingly simple \"how would you build a schema\" job interview questions that I had a difficult time with on the spot. So last one I remember was how would you build a product table with coupons. Ok, so two tables right, no big deal. Well, we are going to need to keep a history right? So now I need to update and have datetimes for different products and coupons. And now I should think about how to do indexes on the tables, and gosh my join to get the discounted price is that a good way to do that? Most coupons only allow a person to use them once, how the hell am I going to implement that? They probably just wanted the simple product + coupon table, but let me spin on it for quite a while like a madman. reply blowski 6 hours agorootparentI'd say this is exactly what the interviewers wanted. They're interested in how you break down the problem, the types of solutions you consider, your understanding of the trade-offs involved. For example, I interviewed somebody who was adamant they could prevent double-booking by polling an end-point and storing the state in Redux. Fantastic JavaScript skills, terrible knowledge of databases. reply nerdponx 5 hours agorootparentprevI don't like these questions. Data warehouse schema design is out of scope for data science, it's data engineering. Yes we have to do a lot of ad-hoc data engineering along the way, but it's such a strange thing to interview for in lieu of the many possible data/math/stats skills and more directly relevant programming skills. It signals a lack of respect for division of labor and specialization, and that lack of respect will be visible in the form of a stretched inefficient team. To be clear, I think the ability to be your own data engineer is a great attribute as a data scientist. I just don't think it's reasonable to expect it: it's not part of the core job description. reply datadrivenangel 4 hours agorootparentI expect a skilled data scientist to be able to articulate the complexities of the real world in relational data. Otherwise, how the hell will they be able to infer the real world from relational data? reply LeonB 11 hours agoprevI found this to be a good introduction and a well-chosen domain to demonstrate modelling. The term “anchor” feels kind of weird to me, but the explanation is so concrete/grounded (like an actual anchor) that I guess it works well enough. The concept of defining the attributes via a question is solid, great way to get clarity quickly. Too often we jump to a minimal column/property name without defining what question we’re trying to answer, and thus not shaking loose any ambiguity in the mind of the customer(s). reply canucker2016 10 hours agoprevTimezones will bend your mind, especially around the transition times. Assuming your timezone jumps forward one hour for daylight savings time and falls back one hour for transition to standard time... When your time skips forward one hour, your 1 hour event may now be displayed as spanning two hours - the second hour will not be reachable/does not exist. When your time falls back one hour, your 1 hour event may now show as spanning 2 hours or 0 hours. Timezones are a man-made construct so don't hardcode values cause things will change... reply zild3d 9 hours agoparentIt gets even more interesting if you're tracking time while considering movement. First time I ran into this was timetracking aboard a ship that is at sea for multiple days. You can cross timezones repeatedly in both directions, the date line, can have a local start datetime that is after the local end datetime, etc. reply dotancohen 5 hours agorootparentIs this logging or planning? If it's logging I would love to know the arguments against storing the records as timestamps. reply 0xEF 9 hours agoparentprevAre you suggesting to just make the calendar in UTC with zero timezones? I agree that they are something we created, but we created them for better local organization. I work with customers globally and everyone uses them. This is a pretty ridiculous ask, in my opinion. Rather just focus on eliminating the Daylight Savings concept from the few localization that still use it, as they tend to cause the most confusion across timezones, especially when planning past an upcoming shift. reply dotancohen 5 hours agorootparent> Rather just focus on eliminating the Daylight Savings concept from the few localization that still use it Oh, certainly, change the laws and cultures of foreign states and peoples in order to simplify your code. Can you get them to just write everything in ASCII while you're at it? reply Lutger 9 hours agorootparentprevOne approach is to store everything in UTC, and display in the timezone of the user. Dealing with timezones - including DST - properly is a must-have, no way around it. I live in a country that uses DST, a lot of Europe does that. If my calendar would be off by 1 hour half of the year, I'd consider it broken and seriously doubt the competence of its authors. This is the core domain of a calendar app! It would be like an email app that just silently drops every other email. I'd love for us to ditch DST by the way, hate it every time. Its bad for the economy, its bad for our health, its bad for software. reply dqv 7 hours agorootparentI don't trust UTC for future dates, only for things that have already happened. Future dates are tentative in that their UTC representation can change. For example, if I converted a Kazakh user's event slated for April 1, 2024 to UTC before February 1, 2024, the event time would be off by one hour. reply canucker2016 9 hours agorootparentprevI did not say that. Timezones are not carved in stone. Prepare for that. A location could go from a +1/-1 timezone as in most of USA/Canada to a fixed timezone with no transitions. There are various ways to adapt, but the user-friendly way involves a lot more work in the app, especially if the app thought that timezone data wouldn't change. reply Lutger 9 hours agorootparentIndeed. There are multiple timezone changes each year, causing gaps in time. Also, not every timezone is a neat 1 hour increment, some are 15 minutes or worse. There are even people living in the same geographical location but in different timezones! Dealing with timezones will drive you mad, quickly. reply canucker2016 9 hours agorootparentDefinitely. The timezone inventors specified the timezone transitions would occur when most people weren't awake or affected - early in the day in the middle of a Saturday/Sunday weekend for USA/Canada. But remote teamwork threw a wrench in that. 2AM Sunday meetings sounded unlikely unless your team needs to communicate with a team several hours ahead or behind and Sunday is a regular workday for one of the teams. reply pantulis 8 hours agorootparentprev> Dealing with timezones will drive you mad, quickly. I guess you mean \"implementing timezone logic yourself\". other than that, the suggested approach (store everything in UTC) and translating to the relevant user timezone with a TZ database in the frontend is the way to go. reply plibblr 6 hours agorootparentNot sure, that's good advice for times in the past, or for times in the next few months, but if we're arranging to meet at midday local time in two years time, I don't think a change in timezone rules in 2025 should cause our meeting to take place at a different time. I suppose you could store the meeting in UTC and use the creation time of the meeting to decide to use the 2024 timezone rules for conversion not the 2026 rules, but that seems pretty confusing too! reply pg_bot 8 hours agoprevYou don't want to store two dates for an event. It's easier to store the start time of the event and a duration for the event. This will make the logic for updating your events simple. You can always calculate the end time of the event based off the start and duration. reply dgrin91 5 hours agoparentIsn't the reverse also true? If you have start & end you can just calc duration. reply sleepyhead 7 hours agoparentprevThat would be problematic for database performance. You would have to calculate the duration on demand when querying. reply pg_bot 7 hours agorootparentIn practice performance problems are a nonissue. Your API should consist of start time and duration because that is how people think about appointments and meetings. (An hour long meeting starting at noon) It's a pain to write a UI that updates two pieces of information when one piece of information changes. If you are truly worried about query performance you can denormalize the data before saving by storing start/end times as a datetime range field, but it still makes no sense to expose that in your API. reply sleepyhead 5 hours agorootparentShowing duration is helpful but so is the exact end time. Visually as a user I would like to see the exact time when the appointment ends instead of calculating it in my head. While it is not that hard to process when an appointment with a duration of 4:15 ends after starting at 2:30 but still. As for API it makes a lot of sense to expose end time. If you for example are creating a calendar widget then it has start and end datetime for all events. With only duration available in the API output you know how to calculate the end time. More lines of codes for you. Fetching from the API you would in most cases limit it to certain dates, for example next week. So now you suddenly do have to deal with start and end time. Not having it otherwise makes no sense. Never had any developers ask for outputting duration in our scheduling API. It would be useful to include it but since no one have asked about it then I think having end time is more critical. https://developer.makeplans.com/#attributes-1 reply canucker2016 2 hours agorootparentprevHow would your code generate all the appointments for a given day? Requirement: Must be able to handle appointments that span a day, i.e. show all Sunday appointments when there's a party appointment that starts at 8PM Saturday and ends at 2AM Sunday, or in your data model, Saturday 20:00 for six hours. reply sgarland 7 hours agorootparentprevTbf at Google / Apple scale this may not hold true, but an addition should not cause any noticeable stress to a DB as a generated column. Or do it in the app, of course. reply skeeter2020 7 hours agorootparentprevone of you is optimizing for transactional operations and the other reporting :) reply lichtenberger 10 hours agoprevI once implemented the backend of a calendar and resource control for a low code platform. The control is highly customizable, with a lot of views to chose from, daily, monthly, yearly... but also resource views (you can book resources with custom groupings, by plugin, by the resource-ID, whatever...), define \"plugins\" on the data sources, what's the from- and to- columns, the title column, what's the resource (may be from a foreign key / 1:1 relationship or 1:N if it's from a \"child\" data source or from the same data source/table). Furthermore I've implemented different appointment series, to chose from (monthly, weekly (which weekdays), daily...), which column values should be copied. Also appointment conflicts (or only conflicts if they book the same resource). You could also configure buffers before and after appointments where no other appointment can be. That was a lot of fun and also challenge sometimes regarding time zones and summer/winter time in Europe and so on :-) reply trwhite 7 hours agoprevYears ago I worked on a calendar application with recurrence. After lots of research I settled on using RRules to represent this, which I was very pleased with. That initial work was when I was at an agency. Later I joined the company full time and discovered to my amazement that a contractor from a different company had removed the RRules in favour of creating and destroying instances of events on the fly. It had no/little fault tolerance so sometimes the script (which did other things that would sometimes fail) would fail to create new events. You'd have monthly recurring events with missing months. I found it so frustrating that (after going through a lot of thought and research) that someone hadn't put anywhere near as much effort into removing mine. It took just a few weeks at that company to realise that the CEO expected the Engineering team to pump out features (that nobody used) at his will and, in the uncertainty of the job market, sadly I stayed there for 2 years. Unrelated footnote: After Googling them, it's really sad to see what are blatantly fake reviews by the CEO on Glassdoor all written in the same style with nothing bad to say. I (and a bunch of other people I know who worked there) hated him, but the silver lining is that I wrote some of my best essays there. The CTO was hopeless too. reply roland35 8 hours agoprevAll I want in Google calendar is a log of changes to the calendar itself. Please add this to the database! reply evnix 10 hours agoprevNicely done. This is one of those aspects which is not really touched on in most courses. Will try to get my company to get a few copies of your book for each of our team member. reply throwaway211 11 hours agoprevInteresting. How about edits, changes of time and location, who's signed up and to which revision. reply michaelmior 11 hours agoparentI think edits and changes of time were pretty well discussed (although certainly some details missing). reply markus_zhang 10 hours agoprev [–] Interesting. I wonder what does a NoSQL one look like? reply evnix 10 hours agoparentI call it the \"Build now, cry later\" approach. reply shreddit 9 hours agorootparentSecond that, i’ve built multiple services with MongoDB just to port them now (and probably for some time) to postgres. reply irisgrunn 9 hours agorootparentThat only makes sense if you wanted to store relational data in a NoSQL database (and that's not what Mongo is meant to do) reply threeseed 9 hours agorootparentAnd if your business is okay with all of your data living in a single instance. Because PostgreSQL is unacceptably poor at HA/replication compared to MongoDB. reply spacebanana7 8 hours agorootparentIs that really true these days? Setting up Postgres read replicas with automatic fail over across multiple machines is pretty trivial in the cloud with services like RDS, spanner etc. And although doing it in your own datacenter is still a big job it's far from impossible. reply sgarland 7 hours agorootparentprevHuh? Replicas are easy, and hot standby nodes aren’t that hard either. There are also various active-active solutions if you need that. reply markus_zhang 9 hours agorootparentprevJust curious why? I have never worked with NoSQL but I always envy the people who does that because they are a bit further away from business than us data warehouse modellers. They are usually our upstream, paid more and less hassle from business. reply throwaway290 6 hours agorootparentprevPostgres and NoSQL are not exclusive, Postgres works pretty well as NoSQL (some would say better than Mongo). reply threeseed 9 hours agorootparentprevAnd yet almost all of the major websites you use today rely on NoSQL. Also if you have non-traditional data structures e.g. document, star, graph, time series then storing them in a SQL database will cause you nothing but problems. There are no black/white answers in tech. Always right tool for the right job. reply fredoliveira 8 hours agorootparent> And yet almost all of the major websites you use today rely on NoSQL. They may use NoSQL in specific use cases, but certainly not exclusively. Using the right tool for the job is crucial; otherwise, you’re doing yourself and your product a disservice. In this case, NoSQL database architecture and internals provide little to no advantage over relational databases. I can’t imagine building a calendar implementation with NoSQL. Some flexible parts of the event model might be stored as NoSQL, but in general? No way. Edit: looks like I wrote my comment as you were editing yours. We agree :-) reply threeseed 8 hours agorootparentIt's a weird argument because the article is wrong. Google Calendar is not implemented on top of a traditional SQL database but rather on top of Spanner which is more akin to a NoSQL database with a SQL front end. reply captn3m0 9 hours agoparentprevI've been building something using a JSON-blob inside SQLite using the Schema.org/Event schema. It's turning out okay. reply rcaught 10 hours agoparentprev [–] It looks like one you shouldn't implement. reply markus_zhang 9 hours agorootparent [–] May I ask why? Actually I never used a NoSQL one so curious. reply Lutger 9 hours agorootparentThe for most people somewhat counter-intuitive answer is that NoSql is very rigid. It is counter intuitive, because having no required schema up front appears to be more flexible, not less. However, having your database not handle schema means your application must do it, there is no way around it. If you ask for an DayEvent and you get back something totally different, what do you do? The rigidness in most NoSql (assuming some form of document store like MongoDB) comes from its inability to combine data in new ways in a performant manner (joins). This is what SQL excels at. That implies you need to design your data in exactly the way it is going to be consumed, because you can't easily recombine the pieces in different ways as you iterate your application. Generally you must know your data access patterns in advance to create a well behaved NoSql database. Changes are hard. This is rigid. Thus, it actually makes more sense to go from sql to a nosql, as you gain experience and discover the data access patterns. The advantage of nosql is not flexibility, that is actually its disadvantage! The advantage is rather its horizontal scalability. However, a decent sql server with competently designed schema will go a very long way. reply irisgrunn 9 hours agorootparentprev [–] Because events are related to users and they both are related to timezones and events can be related to each other. MongoDB is really good for storing big blobs of data you want to retrieve quickly, with some basic search and index, but it's awful at relations between data. reply markus_zhang 9 hours agorootparent [–] Ah I see what you mean. That makes sense! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The tutorial by Alexey Makhotkin provides a comprehensive guide on designing database tables for a Google Calendar clone, following the approach from the upcoming book “Database Design using Minimal Modeling.”",
      "It covers the logical model extensively, detailing how to handle basic all-day events, time-based events, and repeated events, and then transitions to creating physical SQL tables.",
      "The tutorial is aimed at readers with a general understanding of databases, helping them move from conceptual ideas to complete database table definitions, and includes practical steps for implementing the design."
    ],
    "commentSummary": [
      "A discussion on Google Calendar's database design suggests optimizing an iCalendar parser for fast event scanning instead of creating a complex schema.",
      "Concerns were raised about the need for SQL-like range searches and user-specific queries, which traditional databases handle well.",
      "The debate included challenges like timezones, daylight savings, and recurring events, with a consensus that SQL's ability to manage relationships and queries makes it suitable for calendar applications."
    ],
    "points": 269,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1721718345
  },
  {
    "id": 41040543,
    "title": "The Elegance of the ASCII Table",
    "originLink": "https://danq.me/2024/07/21/ascii/",
    "originBody": "The Elegance of the ASCII Table duration 15:19 Podcast Version This post is also available as a podcast. Listen here, download for later, or subscribe wherever you consume podcasts. Download this episode All episodes RSS Spotify Pocket Casts Apple Podcasts YouTube If you’ve been a programmer or programming-adjacent nerd1 for a while, you’ll have doubtless come across an ASCII table. An ASCII table is useful. But did you know it’s also beautiful and elegant. Even non-programmer-adjacent nerds may have a cultural awareness of ASCII thanks to books and films like The Martian2. ASCII‘s still very-much around; even if you’re transmitting modern Unicode3 the most-popular encoding format UTF-8 is specifically-designed to be backwards-compatible with ASCII! If you decoded this page as ASCII you’d get the gist of it… so long as you ignored the garbage characters at the end of this sentence! 😁 History ASCII was initially standardised in X3.4-1963 (which just rolls off the tongue, doesn’t it?) which assigned meanings to 100 of the potential 128 codepoints presented by a 7-bit4 binary representation: that is, binary values 0000000 through 1111111: Notably absent characters in this first implementation include… the entire lowercase alphabet! There’s also a few quirks that modern ASCII fans might spot, like the curious “up” and “left” arrows at the bottom of column 101____ and the ACK and ESC control codes in column 111____. If you’ve already guessed where I’m going with this, you might be interested to look at the X3.4-1963 table and see that yes, many of the same elegant design choices I’ll be talking about later already existed back in 1963. That’s really cool! Table In case you’re not yet intimately familiar with it, let’s take a look at an ASCII table. I’ve colour-coded some of the bits I think are most-beautiful: That table only shows decimal and hexadecimal values for each character, but we’re going to need some binary too, to really appreciate some of the things that make ASCII sublime and clever. Control codes The first 32 “characters” (and, arguably, the final one) aren’t things that you can see, but commands sent between machines to provide additional instructions. You might be familiar with carriage return (0D) and line feed (0A) which mean “go back to the beginning of this line” and “advance to the next line”, respectively5. Many of the others don’t see widespread use any more – they were designed for very different kinds of computer systems than we routinely use today – but they’re all still there. 32 is a power of two, which means that you’d rightly expect these control codes to mathematically share a particular “pattern” in their binary representation with one another, distinct from the rest of the table. And they do! All of the control codes follow the pattern 00_____: that is, they begin with two zeroes. So when you’re reading 7-bit ASCII6, if it starts with 00, it’s a non-printing character. Otherwise it’s a printing character. Not only does this pattern make it easy for humans to read (and, with it, makes the code less-arbitrary and more-beautiful); it also helps if you’re an ancient slow computer system comparing one bit of information at a time. In this case, you can use a decision tree to make shortcuts. That there’s one exception in the control codes: DEL is the last character in the table, represented by the binary number 1111111. This is a historical throwback to paper tape, where the keyboard would punch some permutation of seven holes to represent the ones and zeros of each character. You can’t delete holes once they’ve been punched, so the only way to mark a character as invalid was to rewind the tape and punch out all the holes in that position: i.e. all 1s. Space The first printing character is space; it’s an invisible character, but it’s still one that has meaning to humans, so it’s not a control character (this sounds obvious today, but it was actually the source of some semantic argument when the ASCII standard was first being discussed). Putting it numerically before any other printing character was a very carefully-considered and deliberate choice. The reason: sorting. For a computer to sort a list (of files, strings, or whatever) it’s easiest if it can do so numerically, using the same character conversion table as it uses for all other purposes7. The space character must naturally come before other characters, or else John Smith won’t appear before Johnny Five in a computer-sorted list as you’d expect him to. Being the first printing character, space also enjoys a beautiful and memorable binary representation that a human can easily recognise: 0100000. Numbers The position of the Arabic numbers 0-9 is no coincidence, either. Their position means that they start with zero at the nice round binary value 0110000 (and similarly round hex value 30) and continue sequentially, giving: Binary Hex Decimal digit (character) 011 0000 30 0 011 0001 31 1 011 0010 32 2 011 0011 33 3 011 0100 34 4 011 0101 35 5 011 0110 36 6 011 0111 37 7 011 1000 38 8 011 1001 39 9 The last four digits of the binary are a representation of the value of the decimal digit depicted. And the last digit of the hexadecimal representation is the decimal digit. That’s just brilliant! If you’re using this post as a way to teach yourself to “read” binary-formatted ASCII in your head, the rule to take away here is: if it begins 011, treat the remainder as a binary representation of an actual number. You’ll probably be right: if the number you get is above 9, it’s probably some kind of punctuation instead. Shifted Numbers Subtract 0010000 from each of the numbers and you get the shifted numbers. The first one’s occupied by the space character already, which is a shame, but for the rest of them, the characters are what you get if you press the shift key and that number key at the same time. “No it’s not!” I hear you cry. Okay, you’re probably right. I’m using a 105-key ISO/UK QWERTY keyboard and… only four of the nine digits 1-9 have their shifted variants properly represented in ASCII. That, I’m afraid, is because ASCII was based not on modern computer keyboards but on the shifted positions of a Remington No. 2 mechanical typewriter – whose shifted layout was the closest compromise we could find as a standard at the time, I imagine. But hey, you got to learn something about typewriters today, if that’s any consolation. Bonus fun fact: early mechanical typewriters omitted a number 1: it was expected that you’d use the letter I. That’s fine for printed work, but not much help for computer-readable data. Letters Like the numbers, the letters get a pattern. After the @-symbol at 1000000, the uppercase letters all begin 10, followed by the binary representation of their position in the alphabet. 1 = A = 1000001, 2 = B = 1000010, and so on up to 26 = Z = 1011010. If you can learn the numbers of the positions of the letters in the alphabet, and you can count in binary, you now know enough to be able to read any ASCII uppercase letter that’s been encoded as binary8. And once you know the uppercase letters, the lowercase ones are easy too. Their position in the table means that they’re all exactly 0100000 higher than the uppercase variants; i.e. all the lowercase letters begin 11! 1 = a = 1100001, 2 = b = 1100010, and 26 = z = 1111010. If you’re wondering why the uppercase letters come first, the answer again is sorting: also the fact that the first implementation of ASCII, which we saw above, was put together before it was certain that computer systems would need separate character codes for upper and lowercase letters (you could conceive of an alternative implementation that instead sent control codes to instruct the recipient to switch case, for example). Given the ways in which the technology is now used, I’m glad they eventually made the decision they did. Beauty There’s a strange and subtle charm to ASCII. Given that we all use it (or things derived from it) literally all the time in our modern lives and our everyday devices, it’s easy to think of it as just some arbitrary encoding. But the choices made in deciding what streams of ones and zeroes would represent which characters expose a refined logic. It’s aesthetically pleasing, and littered with historical artefacts that teach us a hidden history of computing. And it’s built atop patterns that are sufficiently sophisticated to facilitate powerful processing while being coherent enough for a human to memorise, learn, and understand. Footnotes 1 Programming-adjacent? Yeah. For example, geocachers who’ve ever had to decode a puzzle-geocache where the coordinates were presented in binary (by which I mean: a binary representation of ASCII) are “programming-adjacent nerds” for the purposes of this discussion. 2 In both the book and the film, Mark Watney divides a circle around the recovered Pathfinder lander into segments corresponding to hexadecimal digits 0 through F to allow the rotation of its camera (by operators on Earth) to transmit pairs of 4-bit words. Two 4-bit words makes an 8-bit byte that he can decode as ASCII, thereby effecting a means to re-establish communication with Earth. 3 Y’know, so that you can type all those emoji you love so much. 4 ASCII is often thought of as an 8-bit code, but it’s not: it’s 7-bit. That’s why virtually every ASCII message you see starts every octet with a zero. 8-bits is a convenient number for transmission purposes (thanks mostly to being a power of two), but early 8-bit systems would be far more-likely to use the 8th bit as a parity check, to help detect transmission errors. Of course, there’s also nothing to say you can’t just transmit a stream of 7-bit characters back to back! 5 Back when data was sent to teletype printers these two characters had a distinct different meaning, and sometimes they were so slow at returning their heads to the left-hand-side of the paper that you’d also need to send a few null bytes e.g. 0D 0A 00 00 00 00 to make sure that the print head had gotten settled into the right place before you sent more data: printers didn’t have memory buffers at this point! For compatibility with teletypes, early minicomputers followed the same carriage return plus line feed convention, even when outputting text to screens. Then to maintain backwards compatibility with those systems, the next generation of computers would also use both a carriage return and a line feed character to mean “next line”. And so, in the modern day, many computer systems (including Windows most of the time, and many Internet protocols) still continue to use the combination of a carriage return and a line feed character every time they want to say “next line”; a redundancy build for a chain of backwards-compatibility that ceased to be relevant decades ago but which remains with us forever as part of our digital heritage. 6 Got 8 binary digits in front of you? The first digit is probably zero. Drop it. Now you’ve got 7-bit ASCII. Sorted. 7 I’m hugely grateful to section 13.8 of Coded Character Sets, History and Development by Charles E. Mackenzie (1980), the entire text of which is available freely online, for helping me to understand the importance of the position of the space character within the ASCII character set. While most of what I’ve written in this blog post were things I already knew, I’d never fully grasped its significance of the space character’s location until today! 8 I’m sure you know this already, but in case you’re one of today’s lucky 10,000 to discover that the reason we call the majuscule and minuscule letters “uppercase” and “lowercase”, respectively, dates to 19th century printing, when moveable type would be stored in a box (a “type case”) corresponding to its character type. The “upper” case was where the capital letters would typically be stored. 21 July 2024 6 tags 8 syndications 2 reposts 3 mentions",
    "commentLink": "https://news.ycombinator.com/item?id=41040543",
    "commentBody": "The Elegance of the ASCII Table (danq.me)248 points by thewub 20 hours agohidepastfavorite141 comments augusto-moura 12 hours agoUseful tip, on linux (not sure about other *nixes) you can view the ascii table by opening its manpage: man ascii It's been useful to me more than once every year, mostly to know about shell escape codes and when doing weird character ranges in regex and C. It can be a bit confusing, but the gist is that you have 2 chars being show in each line, I would prefer a view where you see the same char with shift and/or ctrl flags, but you can only ask so much reply INTPenis 10 hours agoparentThe reason I know this is because in 2004 I was squatting in an apartment with no TV and no internet. So each day after work I would go home and just read manpages for fun. Ended up learning ipfw through the firewall manpage on FreeBSD, and using my skills to setup and manage an IPFW at work. It's amazing how much you get done with no TV and no internet. Also played a lot of nethack. reply w0m 1 hour agorootparentI learned vim proper by reading :help on an eeepc while flying back and forth over the Atlantic alone one year. reply bodyfour 3 hours agoparentprev> not sure about other *nixes Should be available on any UNIX, it was added to V7 UNIX back in the 1970s: https://github.com/dspinellis/unix-history-repo/blob/Researc... Even before that, it existed as a standalone text file https://github.com/dspinellis/unix-history-repo/blob/8cf2a84... This still exists on many systems -- for instance as /usr/share/misc/ascii on MacOS reply dailykoder 11 hours agoparentprevDamn, thanks! Why the hell did I never try this? Maybe because typing ascii table into my favorite search engine and clicking one of the first links was fast enough reply omnicognate 8 hours agorootparentI used to do that until the experience became degraded enough, reflecting the general state of the web, that I took the time to look for a better way and found `man ascii`. reply layer8 8 hours agoparentprevOr even simpler use the ascii command, when installed: https://packages.debian.org/bookworm/ascii reply fitsumbelay 1 hour agoparentprevstrange: on MacOS 14.5 I get output for `man ascii` but `ascii` goes \"command not found\" reply AnimalMuppet 1 hour agorootparentOn my Linux VM, it's the same, and it's because 'man ascii' comes from man(7), not man(1). It's not a man page for a program. It's just a man page. reply bell-cot 10 hours agoparentprevSimilar in FreeBSD. It has octal, hex, decimal, and binary ASCII tables, along with the full names of the control characters. reply userbinator 14 hours agoprevYou might be familiar with carriage return (0D) and line feed (10) You mean 0D and 0A, or 13 and 10, but that mix of base really stood out to me in an otherwise good article. I'm one of numerous others who have memorised most of the base ASCII table, and quite a few of the symbols as well as extended ASCII (CP437), mainly because it comes in handy for reading programs without needing a disassembler. Those who do a lot of web development may find the sequence 3A 2F 2F familiar too, as well as 3Ds and 3Fs. I can see the rationale forbeing in that order, but [\\] and {|} are less obvious, as well as why their position is 1 column to the left of . reply yardshop 7 hours agoparent> You mean 0D and 0A, or 13 and 10 He fixed it. reply dwheeler 19 hours agoprevThe encodings we use today have a surprisingly deep and complex history. For more, see: \"The Evolution of Character Codes, 1874-1968\" https://ia800606.us.archive.org/17/items/enf-ascii/ascii.pdf reply rrwo 7 hours agoparentThanks for posting that. People tend to overlook that the technologies we use today have a much older history. reply pixelbeat__ 22 minutes agoprevI wrote about ASCII and UTF-8 elegance at: https://www.pixelbeat.org/docs/utf8_programming.html reply EvanAnderson 17 hours agoprevI would be remiss not to post a link to the late Bob Bemer's[0] website. https://web.archive.org/web/20150801005415/http://bobbemer.c... He was considered the \"father of ASCII\". Hr wrote very well and gives clear explanations for the motivations behind the design of ASCII. [0] https://en.m.wikipedia.org/wiki/Bob_Bemer reply lucasoshiro 20 hours agoprevOnce I saw a case-insensitive switch in C using that pattern of letters: switch (my_char0x20) { case 'a': ... break; case 'b': ... break; } reply Sharlin 11 hours agoparentYes, that’s very intentional and just masking (or setting) the bit is the intended way to do case-insensitive comparison of the letter range in ASCII (eg. stricmp in C), or to transform text to lower or upper case (tolower, toupper). But what’s more, ever wondered whence the control (Ctrl) key presses like Ctrl-H to backspace, or Ctrl-M for carriage return? Well, inspecting the ASCII chart it becomes evident: the Ctrl key simply masks bit 6 (0x40), turning a letter into its respective control character! reply flohofwoe 2 hours agorootparent...it's a bit of a shame that the same upper/lowercase trick doesn't apply to all UNICODE codepoints (at least those that have upper/lower variants). It seems to work for codepoints up to U+00FF, for instance: - Å (U+00C5) vs å (U+00E5) ...but above 0xFF lowercase follows uppercase: - Ă (U+0102) vs ă (U+0103) Typical for UNICODE though, nothing makes sense ;) reply Findecanor 0 minutes agorootparentU+00A0–U+00FF is the \"Latin-1 Supplement\", sharing code-points with the earlier \"ISO Latin-1\" (ISO 8859-1), itself based on DEC's \"Multinational Character Set\". The upper/lowercase trick does not apply to ß/ÿ in ISO Latin-1, but Ÿ/ÿ exist in MCS at a different pair of code points. ISO Latin-1 was the character set on many Unix systems, Amiga OS, MS-Windows (as \"Windows-1252\" with extra chars), and was for many years the default character set on the web. lucasoshiro 4 hours agorootparentprevNice! I'm an emacs user, and when I use a readline-based REPL I use ctrl-M a lot. I thought it was inherited from the emacs keybindings, like many other shortcuts from GNU readline reply jerf 4 hours agorootparentThen an additional useful command: In the out-of-the-box emacs bindings, C-q is the \"quoted insert\" command. It will take the next character and directly insert it into the buffer. This is useful for things like tab or control characters where emacs would normally use the keystroke to do something else. I've been working in an email-related space lately so I've been doing a good amount of C-q C-m for inserting literal CRs, and C-q TAB for a few places where I want a literal tab in the source, in a buffer that interprets a normal TAB as a command to indentify the current row. I mention this because you can use the ASCII table to work out how to insert a particular control character with your keyboard literally, if you need to insert one of the handful of other characters you may be interested in every so often, like C-l for \"form feed\" (now used for \"page feed\" in some older printer-related contexts) or C-@ for NUL if you're doing something weird with binary files in a \"text\" buffer. reply mananaysiempre 18 hours agoparentprevThis can be made to work for ASCII and EBCDIC simultaneously for extra esoterica points: switch (my_char'A' ^ 'a') { case 'A''a': /* ... */ break; /* ... */ } I don’t know if this is too fancy to have ever made it into real code, but I believe I’ve seen places in the ICU source that still say ('A'\"a + ogonek accent\" and another (properly) sent \"a with ogonek\" (these print the same but are semantically different!) How can these possibly be semantically different? Isn’t the point of combining characters to create semantic characters that are the combination of those parts? reply p_l 1 hour agorootparentThere's a semantic difference between \"accented letter\" and \"different letter that happens to visually look like another language's accented letter\". \"Ą\" in polish is not \"A\" with some accent. And the idea behind unicode was to preserve human written text, including keeping track of things like \"this is letter A1 with an accent, but this is letter A2 that looks visually similar to A1 with accent but is different semantically\". Of course then worries about code page size resulted in the stupidity of Han unification, so Unicode is a bit broken. reply mnau 10 hours agoparentprevAs someone that whose native language isn't representable purely by ASCII, I celebrate it. Plus the first 128 codepoints are same as ASCII in UT-8. Is Unicode kind of messy? Sure, but that's just natural consequences of writing systems being messy. Every point you made was for a sensible reason that is in a scope of Unicode mission (representing all text in all writing systems). reply Retr0id 12 hours agoparentprevLanguage itself is a pile of ugly graffiti and ramshackle addons. It would be weird if Unicode didn't reflect this. reply shepherdjerred 48 minutes agoparentprevWhat would be the alternative? I think Unicode is pretty great. You can pretty easily imagine a world where we had a bunch of different encodings with none being dominant. reply Aardwolf 10 hours agoparentprevFor me it's how they inconsistently, backwards-incompatibly, make some existing characters outside of the emoji-plane (and especially when in technical/mathematical blocks) render colored by default, rather than keep everything colored related in the emoji plane (making copies if needed rather than affecting old character, the semantics are very different anyway), e.g. https://imgur.com/a/Ugi7K1i and https://imgur.com/a/UMppZHG reply jart 13 hours agoparentprevHey at least we got the astral planes. https://justine.lol/dox/unicode.txt reply saagarjha 5 hours agoparentprevUnicode is quite elegant in its encoding too. If you're going to criticize it for its content, maybe start with talking about how ASCII also has invisible characters and those that people rarely use. reply yencabulator 2 hours agoparentprevI can't wait for when the majority of Unicode codepoints/glyphs are emojis that are no longer fashionable! That'll be a really weird relic of history, later. reply RiverCrochet 7 hours agoparentprev> covered with ugly graffiti and ramshackle addons Unfortunately there is plently of precendent for this ramshacklism. Like ACK/NAK - those are protocol signals, not characters! ENQ? What even is Shift In/Shift Out (SI/SO)? Then the database characters toward the end there FS, RS, GS, US. > backwards running text (hey, why not add spiral running text?) You jest, but you do have cursor positioning ANSI sequences which are designed to let text draw anywhere on your screen. And make it blink! You also don't find it weird to have a destructive \"clear-screen\" sequence? > glyphs defined by multiple code points I wonder when they started putting the slash across the 0 to differentiate from the O. > you vote for my made-up emoticon, and I'll vote for yours! I mean you do have the private Unicode range where you can actually do that. But before that, SIXEL graphics. reply kps 5 hours agorootparent> Like ACK/NAK - those are protocol signals, not characters! American Standard Code for Information Interchange reply NoMoreNicksLeft 2 hours agoparentprevThey're all made-up languages, some were just made-up a little bit more transparently. reply chthonicdaemon 12 hours agoparentprevAll languages are made up. For that matter, all glyphs are made up, too. reply bandie91 10 hours agorootparentthere is not only a quantitative difference between a conlang designed by a small group (or 1 person) and a \"human\" language developed organically in the span of centuries by millions of speakers, but also qualitative. reply bloak 5 hours agoprevVaguely related: Apart from £ and €, a typical GB keyboard has a couple of non-ASCII characters printed on it: ¬ and ¦. The key labelled ¦ is usually mapped to |, but the key labelled ¬ often gives you an actual ¬, though I can't remember many occasions on which I've wanted one of them. Apparently the characters ¬ and ¦ are in EBCDIC. reply thristian 16 hours agoprev> That, I’m afraid, is because ASCII was based not on modern computer keyboards but on the shifted positions of a Remington No. 2 mechanical typewriter – whose shifted layout was the closest compromise we could find as a standard at the time, I imagine. According to Wikipedia¹, American typewriters were pretty consistent with keyboard layout until the IBM Selectric electric typewriter. Apparently \"small\" characters (like apostrophe, double-quote, underscore, and hyphen) should be typed with less pressure to avoid damaging the platen, and IBM decided the Selectric could be simpler if those symbols were grouped on dedicated keys instead of sharing keys with \"high pressure\" symbols, so they shuffled the symbols around a bit, resulting in a layout that would look very familiar to a modern PC user. Because IBM electric typewriters were so widely used (at least in English speaking countries), any computer company that wanted to sell to businesses wanted a Selectric-style layout, including the IBM PC. Meanwhile, in other countries where typewriters in general weren't so popular or useful, the earliest computers had ASCII-style punctuation layout for simplicity, and later computers didn't have any pressing need to change, so they stuck with it. Japanese keyboards, for example, are still ASCII-style to this day. ¹: https://en.wikipedia.org/wiki/IBM_Selectric#Keyboard_layout reply sixothree 13 hours agoparentI never realized my first computer used ascii directly for the shifted number keys. https://en.wikipedia.org/wiki/TRS-80_Color_Computer#/media/F... reply kps 5 hours agorootparenthttps://en.wikipedia.org/wiki/Bit-paired_keyboard reply Mountain_Skies 7 hours agorootparentprevSo many things on the CoCo turned out to be the way they were for cost saving reasons. Tandy was good at saving pennies everywhere it could. When I took 'Typing' in high school, my muscle memory was in a constant fight between the IBM Selectric layout of the typewriters at school and the CoCo at home. reply jolmg 18 hours agoprev> So when you’re reading 7-bit ASCII, if it starts with 00, it’s a non-printing character. Otherwise it’s a printing character. > The first printing character is space; it’s an invisible character, but it’s still one that has meaning to humans, so it’s not a control character (this sounds obvious today, but it was actually the source of some semantic argument when the ASCII standard was first being discussed). Hmm.. Interesting that space is considered a printing character while horizontal tab and newline are control characters. They're all invisible and move the cursor, but I guess it makes sense. Space is uniquely very specific in how the cursor is moved one character space, so it's like an invisible character. Newline can either imply movement straight down, or down and to the left, depending on a configuration or platform (e.g. DOS vs UNIX line endings). Horizontal tab can also move you a configurable amount rightwards, and perhaps it might've been thought a bit differently, given there's also a vertical tab, which I've got no idea on how it was used. Maybe it's the newline-equivalent for tables, e.g. \"id\\tcolor\\v1\\tred\\v2\\tblue\\v\" or something like that. Interesting also that BS is a control char while DEL is a printing(?) char. I guess that's because BS implies just movement leftwards over the text, while DEL is all ones like running a black sharpie through text. Guess that's what makes it printing. Wonder if there were DEL keys on typewriters that just stamped a black square, and on keypunchers that just punched 7 holes, so people would press \"backspace\" to go back then \"delete\" to overwrite. I've used ASCII a lot, but even after so many years, I'm getting moments where it's like \"oh this piece isn't just here, it needs to be here for a deep reason\". It's like a jigsaw puzzle. reply kragen 16 hours agoparentdel is not a printing character. it's a control character. if you run a paper tape full of del characters through a teletype it does not print anything. it has to have that bit pattern, even though it greatly complicates the mechanics of the teletype (which has to do all the digital logic with cams and levers) because that way it can be punched over any character on the paper tape to delete it a figure caption in this page says 'This is a historical throwback to paper tape, where the keyboard would punch some permutation of seven holes to represent the ones and zeros of each character. You can’t delete holes once they’ve been punched, so the only way to mark a character as invalid was to rewind the tape and punch out all the holes in that position: i.e. all 1s.' which is mostly correct, except that it wasn't a historical throwback; paper tape was perhaps the most important medium for ascii not just in 01963 and 01967 but probably in 01973, maybe even in 01977. teletype owners today are still using paper tape that was manufactured during the vietnam war, where it was used in unprecedented volume for routing teletype messages by hand the dominant early pc operating system, cp/m (if it's not overly grandiose to call it an 'operating system') had system calls for reading and writing the console, the disk, and the paper tape punch and reader. when i hooked up a modem to my cp/m system to call bbses, i hooked it up as the punch and reader reply 91bananas 1 hour agorootparentjust... this is why this forum exists. thank you reply kragen 36 minutes agorootparentyou're welcome. i'll try to remember your comment the next time someone replies to me with something like https://news.ycombinator.com/item?id=40993821 or https://news.ycombinator.com/item?id=40993328 or https://news.ycombinator.com/item?id=40992456 reply jart 13 hours agorootparentprev> so the only way to mark a character as invalid was to rewind the tape and punch out all the holes in that position So that's why \\177 (DEL) is the loneliest control character. Wow. Thank you! reply kragen 12 hours agorootparenthappy to help reply pwg 17 hours agoparentprevYou also have to keep in mind the \"interface\" for 1962-1968. The printer teletype machine. The \"control codes\" were to \"control\" the printhead. So \"carriage return\" meant move the \"print carriage\" back to the left margin. \"New line\" meant move the paper platen one line height of rotation to move the paper to the next line. In that context, \"back space\" was \"move print head one space left\" (rather more like a \"reverse space\"). The article does mention that there was some debate about whether space should be considered \"printable\", but if you consider a mechanical printer, as the head is moving to the right and banging out characters onto the paper, the spaces between words do, sort of, look like \"printables\" (of a sort, a \"print nothing\" character as it were). Tab's being control characters then make a bit more sense, in that they cause the printhead to jump some fixed distance to the right. The article stated why DEL is where it is (all ones) -- so that for punched paper tape, one could get a punch-out of every position, which was then interpreted as \"nothing here\" by the tape reading machine. As for typewriters, no, none had a \"black box\" blot out key. Correction (for typewriters without built in correction tape) was one of: retype the page, apply an eraser (and hopefully not damage the paper surface too much) then retype character and continue, or apply correction fluid (white-out) and retype character and continue. For those typewriters with built in correction tape options (at least some IBM Selectric models, possibly more) the typewriter would retype the character using the \"white-out\" ribbon, then retype the replacement character using the normal \"typewriting\" ribbon. reply tivert 11 hours agorootparent> Tab's being control characters then make a bit more sense, in that they cause the printhead to jump some fixed distance to the right. Isn't that incorrect? Tab doesn't jump a \"fixed distance to the right,\" it jumps a variable distance to the next tab-stop to the right. reply bandie91 10 hours agorootparentyea he must meant that it jumps to a fixed position reply EvanAnderson 17 hours agorootparentprev> The article stated why DEL is where it is (all ones) -- so that for punched paper tape, one could get a punch-out of every position... I saw an analogous use of backspace on some OS I ran into 30 years ago cruising around either Tymnet or TELENET. (I wish I could remember the OS...) The password prompt assumed local echo. After entering a password the host would send a series of backspaces and various patterns of characters (####, **, etc) to overprint the locally-echoed (and printed) characters. reply kmoser 13 hours agorootparentOn the login to the first timesharing system I used, it would prompt for your password, then type eight M's, W's, and X's on top of each other (on paper, of course, since this was using a Teletype terminal), so when you actually typed your password the characters would be printed on top of those already obscured lines. reply rob74 14 hours agorootparentprev> For those typewriters with built in correction tape options (at least some IBM Selectric models, possibly more) the typewriter would retype the character using the \"white-out\" ribbon there was also a solution for cheaper typewriters: small sheets of \"white-out\" paper (known under the genericized brand name \"Tipp-Ex\" here in Germany) that you could hold between the ink ribbon and the paper to \"overwrite\" a typo. reply layer8 8 hours agoparentprevSpace is what is represented in the output, i.e. in one cell of the terminal grid, whereas control characters like Tab and CR/LF don’t map onto such an output representation. If you want to represent the printed contents of each “grid cell” of a printout or a textmode screen buffer, you don’t need the control characters, only the printable characters. The printable characters are what you’d need in a screen font. reply california-og 11 hours agoparentprevWhile DEL didn't stamp a black square on typewriters, it sometimes did so (or something similar, like diagonal stripes) in various digital character sets. ISO 2047[0] established the graphical representations for the control characters of the 7-bit coded character set in 1975, maily for debugging reasons. This graphical representation for DEL was used by Apple IIGS, TRS-80 and even Amiga! [0]: https://en.m.wikipedia.org/wiki/ISO_2047 reply nikau 10 hours agoparentprevLogically space maps to a character people use with pen and paper unlike tab reply kazinator 14 hours agoparentprevSpace doesn't just move the cursor on a display; it will obliterate a character cell with a space glyph. When a display terminal has nondestructive backspace (backspace character doesn't erase), it can be software emulated with BS-SPACE-BS. At your Linux terminal, you can do \"stty echoe\" (echo erase) to turn this on (affecting the echoing of backspace characters that are input, not all backspace characters). Dial-up BBSes had this as a configurable setting also. reply BobbyTables2 18 hours agoprevI always lament that since at least 1980s or so, it seems the vast majority of the control characters were never used for their intended purpose. Instead, we crudely use commas and tabs as delimiters instead of something like RS (#30). reply EvanAnderson 17 hours agoparentI did some ETL work that used the ASCII delimiter characters. It was very enjoyable. I didn't have to worry about escaping or parsing escaped strings. The control codes were guaranteed to be illegal in input. It was refreshing. reply theamk 16 hours agorootparentCould you do the same with TSV? A lot of datasets can either prohibit tabs in data, or convert it to spaces in early ingestion. reply red_admiral 9 hours agorootparentYes, and as long as you remember to turn off the \"TAB produces 4 spaces\" thing in your editor (grumble makefiles grumble) it's really nice to work with. reply EvanAnderson 14 hours agorootparentprevTSV is a joy compared to CSV, for sure. CLI tools that output TSV are what immediately spring to mind. reply red_admiral 9 hours agoparentprevAs long as your data is not binary, so does not contain record separators itself, this would be a thousand times better than CSV (because text data _does_ often contain commas and double quotes). The only thing you'd need is editors to support some way of entering and displaying the RS, and CTRL+^ is a bit of a kludge as it ends up CTRL+SHIFT+6. Of course, if a record itself can contain RS for subrecords, things become more complicated. I guess you could use `\\^`. reply tracker1 3 hours agoparentprevThat's my thought as well... I remember using them pre-xhr web in order to send data from the server to JS, which I could then split up pretty easily on the client side. I still don't know why we are so tethered to CSV. reply fukawi2 9 hours agoparentprevI recall working on a PICK D3 system, which was a \"multivalue\" database. Each field could have multiple values, those values could have sub values, and a third level beyond that. Values were separated with char(254), subvalues were separated with char(253), and the third level were char(252) separated. It was... unique, but worked. And to be fair, PICK originated in the 60's, so this method probably evolved in parallel to the ASCII table! reply yencabulator 2 hours agoparentprevAh, Deborah␞ Records. Little Debbie Records, we call her. reply thaumasiotes 17 hours agoparentprevThat's because the intended purpose is either useless (for machine control characters) or useless and logically impossible (for delimiters). What do you do if you have a record that includes a record separator character? Given that you have this problem anyway, why do you want a character dedicated to achieving the same thing that a comma achieves? reply penteract 17 hours agorootparentThe record separator isn't on people's keyboards, so it's less likely to show up where it's not expected. Also it's less likely to legitimately occur in something like a name, so there are many users of CSVs who can say they will never need to consider data containing a record separator, and they will be right more often than those who never consider data containing a comma. Of course, the fact that record separators aren't on keyboards is probably why CSVs use commas. reply yardshop 17 hours agorootparentIn the DOS days, you could \"type\" control characters by pressing Ctrl and the corresponding letter key, Ctrl+M is Carriage Return, Ctrl+H is Backspace, Ctrl+Z is End Of File, etc. It was probably possible to type an RS with Ctrl+Shift+. and the others with similar combos. reply penteract 16 hours agorootparentIn a desktop linux terminal, Ctrl-^ or Ctrl-~ work for me. In a tty, I need to press Ctrl-V before them. reply jart 13 hours agorootparentYeah Linux still works exactly this way. The modern WIN32 API even works that way too. When you ReadConsoleInput() it gives you teletypewriter style keyboard codes. When I wrote a termios driver for Cosmopolitan to have a Linux-style shell in CMD it really didn't take much to translate them into the Linux style. We're all still using glorified teletypes at the end of the day. It will always be the substrate of our world. One system built upon another older system. reply _flux 11 hours agorootparentprevI think it's worth mentioning that Ctrl-A is ascii 1, Ctrl-B ascii 2, etc, as it is in Unix today. reply jki275 16 hours agorootparentprevyou can still type them -- alt + 030(for instance) on the keypad will insert that RS character. In Windows at least -- not sure about the other OS. reply Symbiote 12 hours agorootparentOn Linux terminals entering control characters is done with the control key, Ctrl-G for example, but they will often be intercepted by the program that is running. Bash will insert the control character (rather than interpret it) if you prefix it with Ctrl-V. reply keybored 11 hours agorootparentprevI can’t think of a case where someone would write a control character like that into something intended for text on purpose. So you might as well disallow it. reply jerf 3 hours agorootparentThe situation that comes up the most often that you need to consider is when someone embeds the same sort of file into itself, or chunks of the same sort of file into itself. If using the ASCII characters to delimit fields was common, you'd need to consider that over the course of some moderately interesting system's life time the odds of someone copying and pasting something from an encoded file into the spreadsheet application and picking up the ASCII control characters with it is basically 100%. And while we may be able to say with some confidence that nobody is going to embed a CSV file into a CSV file (and I say only some confidence, the world is weird and I'm sure someone will read this who has actually seen someone do this), there's other situations like HTML-in-HTML (for example, every HTML tutorial ever) that are guaranteed by their nature. It is still valid to disallow the ASCII control characters, one just has to make sure that it is done comprehensively, in all places users may input them. But that's not created by using ASCII control characters, that's a consequence of the \"ban the control characters entirely\" approach regardless of what the control characters are. It's neat when you can get away with it, but I generally prefer to define a robust encoding scheme instead. A minimal one like \"replace backslash with double-backslash, replace control characters with backslashed characters\" and \"replace backslash sequences with their control characters, including backslash-backslash as a single backslash\" can be inserted almost anywhere in just a few lines of string replace (or stream processing if you need the speed). The only tricky bit is you need to make sure you get the order correct or you corrupt data, and while I've done this enough to have it almost memorized now I do recall feeling like the correct order is backwards from what I naturally wanted the first few times. But it is simple and robust if you get it right. reply keybored 3 hours agorootparentSomeday I will create both formats: a control-characters are banned format (and never accepted) and one where they are escaped. That ought to be good enough for all needs! (A trivial evening project for some; not for all of us) reply thaumasiotes 17 hours agorootparentprev> Also it's less likely to legitimately occur in something like a name, so there are many users of CSVs who can say they will never need to consider data containing a record separator, and they will be right more often than those who never consider data containing a comma. No, they'll be right exactly as often, 0% of the time. But their mistake will show up less frequently, causing more problems when it does. As soon as it's possible for some of your data to come from someone else's dataset, you're guaranteed to have to accommodate record separators within your data as well as within the metadata. You're better off using a system that plans for this inevitability than one that pretends it can't happen at all. reply penteract 16 hours agorootparent> No, they'll be right exactly as often, 0% of the time. > But their mistake will show up less frequently, causing more problems when it does. Enough people use CSVs (and have limited, small-scale use-cases) that I'd be willing to bet \"less frequently\" means never for at least 1% of people who use CSVs. I don't know whether the chance of no problems is worth the increased difficulty of problems that do occur - considering that balance feels a bit silly because if you're aware there could be a problem in a context where you could choose between commas and unit separators, you could just add validation or escaping. reply thaumasiotes 16 hours agorootparent> considering that balance feels a bit silly because if you're aware there could be a problem in a context where you could choose between commas and record separators, you could just add validation or escaping. As soon as you have validation or escaping, having a record separator character loses its entire purpose. The existence of the character is predicated on the idea that you don't have to do that, and that idea is false. That's why the character is never used. It's a conceptual mistake that was accidentally enshrined in a series of encoding standards that had enough free space to accommodate it. reply penteract 16 hours agorootparent> As soon as you have validation or escaping, having a record separator character loses its entire purpose. The existence of the character is predicated on the idea that you don't have to do that, and that idea is false. I disagree with this - the data needs to be stored somehow, and while other characters (like comma) can be used, having a dedicated character can help - for example if the data might legitimately contain commas or newlines but not unit separators or record separators, then escaping isn't needed if you use unit/record separators (although validation is still necessary). reply Symbiote 12 hours agorootparentI agree. TSV is widely used, but lacks a way to escape the tab and new line characterss. RS-V is the same, but allows including tabs and new lines in records. reply keybored 12 hours agorootparentprev> What do you do if you have a record that includes a record separator character? This comes up every time. Options: 1. You disallow it. And you might as well disallow all the control codes except the carriage return, line feed, and other “spacing” characters. Because what are they doing in the data proper? They are in-band signals. 2. You use the Escape character to escape them 3. Weirdest option: if you really want to nest in a limited way you can still use the group and file separator characters reply AdamH12113 15 hours agorootparentprev> What do you do if you have a record that includes a record separator character? You use the ASCII escape character (0x1B), which is designed for exactly that purpose. reply NoMoreNicksLeft 2 hours agorootparentprevWell, that's what an escape is for. Are we really having a serious discussion in 2024, where someone is suggesting that it's not the responsibility of the software engineer to sanitize inputs before chucking the data into some sort of database? reply georgehotelling 6 hours agoprevDark grey #303030 text on slightly darker grey #1B1C21 background is really hard to read. Maybe I'm just getting old, but I also assume the audience for a blog post about the ASCII table was born in a year that starts with 19. reply Retr0id 5 hours agoparentThe background is white on my machine, are you using some kind of extension to force \"dark mode\"? reply georgehotelling 5 hours agorootparentI'm using pi-hole, uBlock Origin, and Privacy Badger on Firefox. I checked my network tab before complaining and didn't see any resources that failed to load. reply PaulHoule 5 hours agoprevBeats EBCDIC https://en.wikipedia.org/wiki/EBCDIC On the 4th floor of my building the computer systems lab has a glass front that has what looks like a punch card etched in frosted glass but if you look closer it was made by sticking stickers on the glass. I made a \"punchcard decoder\" on a 4x6 card to help people decode the message on the wall https://mastodon.social/@UP8/112836035703067309 The EBCDIC code was designed to be compatible with this encoding which has all sorts of weird features, for instance the \"/\" right between \"R\" and \"Z\"; letters don't form a consecutive block so testing to see if a char is a letter is more complex than in ASCII. I am thinking of redoing that card to put the alphabet in order. A column in a punched card has between 0 to 3 punches, 0 is a space, 1 is a letter or a symbol in the first column, if one of the rows at the top is punched you combine that with the number of the other punched row on the left 3x9 grid. If three holes are punched one of them is an 8 (unless you've got one of the extended charsets) and you have one of the symbols in the right 3x6. Note the ¬ and ¢ which are not in ASCII but are in latin-1. reply Dwedit 19 hours agoprevMany old NES/SNES games had a simpler character encoding system, with 0-9 and A-Z at the beginning of the table. No conversion require to display hex. reply senkora 3 hours agoprevFun fact: sorting ASCII numerically puts all the uppercase letters first, followed by all the lowercase letters (ABC... abc...). A more typical dictionary ordering would be more like AaBbCc... (or to even consider A and a at the same sort level and only use them to break ties if the words are otherwise identical). The order used by ASCII is sometimes called \"ASCIIbetical\", which I think is wonderful. https://en.wiktionary.org/wiki/ASCIIbetical reply NoMoreNicksLeft 2 hours agoparentI thought the point of that was that a single bitflip makes an uppercase lower, or vice versa... reply aronhegedus 1 hour agoprevWas a really fun article to read/podcast to listen to. Favorite fact is that 127 is the DEL because for hole punching it removes all the info. I love those little nuggets of history reply ggm 18 hours agoprevman ascii is never far from my fingers. combined with od -c and od -x it gets the job done. I don't think as fluently in Octal as I used to. Hex has become ubiquitous. reply fsckboy 14 hours agoparentyou mean ? ascii reply ggm 13 hours agorootparentNo I don't -I live in a different universe to you: % (uname; cd /usr/ports; ls -d */ascii) FreeBSD zsh: no matches found: */ascii % which ascii ascii not found % It's the same on OSX and debian by default doesn't install that command. If you live inside a POSIX/IEEE 1003 system and want to know the ascii table reliably then the command I run is the one which works. If your distribution doesn't ship manuals by default you have bigger problems. reply amszmidt 11 hours agorootparent”man ascii” has as much guarantee to work on a POSIX system as a command called ”ascii” seeing neither (specifically a man page called ”ascii”) are part of the standard. So you will either get command not found, or man page not found. reply bandie91 2 hours agorootparentprevman 7 ascii reply gumby 16 hours agoprevI wish the author had included the full ascii chart in 4 bits across / 4 bits down. You can mask a single bit to change case and that is super obvious that way. The charts that simply show you the assignments in hex and octal obscure the elegance of the design. reply kalleboo 15 hours agoparentIt was at some point looking at a chart like that where it also dawned on me where the control codes like ^D, ^H, ^[ etc came from reply kr2 11 hours agorootparentI was going to ask you to please explain as I didn't understand, but I am guessing you are talking about the same thing as this comment[1] right? That's super cool https://news.ycombinator.com/item?id=41042570 reply gumby 7 hours agorootparentYes, though ^m, ^[ et al aren’t so much elegant as coincidental; but look at A and a, for example. I found the chart I was looking for: https://en.wikipedia.org/wiki/ASCII#/media/File:USASCII_code... reply AdamH12113 15 hours agoparentprevThe third and fourth columns of the table are only a single bit apart from each other. If you mentally swap the first two columns, you get a Gray code ordering of the most significant bits, which is pretty close to what you're looking for. reply gumby 7 hours agorootparentFound it: https://en.wikipedia.org/wiki/ASCII#/media/File:USASCII_code... reply red_admiral 9 hours agoprevThe \"16 rows x 8 columns\" version, with the lowercase letters added, seems the most elegant one to me because it makes the internal structure of the thing visible. For example, to lowercase a letter, you set bit 6; a decimal digit is the prefix 011 followed by the binary encoding of the digit etc. It also makes clear why ESC can be entered as `^[` or ENTER (technically CR) as `^M` on some terminals (still works in my xterm), because the effect of the control key is to unset bits 6 and 7 in the original set-up. Of course you can color in the fields too, if you want. reply zokier 12 hours agoprevI think that adopting ASCII as the general purpose text encoding was one of the great mistakes of early computing. It originated as control interface for teletypes and such, and that's arguably where it should have remained. For storing and processing (plain) text ASCII doesn't really fit that well, control characters are a hindrance and the code space would have been useful for additional characters. The ASCII set of printables was definitely a compromise formed by the limited code space. reply ddingus 11 hours agoparentNo way! No amount of extra characters was going to address what Unicode did. ASCII was not a mistake at all. Adopting it unified what was surely going to be a real mess. At the time it made sense, and the control functions were needed. Still are. reply zokier 7 hours agorootparent> At the time it made sense, and the control functions were needed. Still are. Control characters were needed for terminals. They never made sense for text. Mixing the two matters is the problem. reply ddingus 2 hours agorootparentIt isn't a problem. The text is the UX. What else would you have proposed, or would propose? reply kstenerud 11 hours agoparentprevIt's one of the greatest triumphs of early computing. Not only did it harmonize text representation and transmission in a backwards compatible manner; the fact that they deliberately kept it 7 bit for so long also helped for developing a sane set of other language character sets (ISO-8859), and paved the way for a smooth transition to Unicode (UTF-8) - which is now the dominant encoding worldwide. reply ddingus 11 hours agorootparentYes, seconded easily reply hackit2 11 hours agoparentprevYeah you were not around when a kb of memory took up half your room. Looking back it doesn't make sense but at the time a byte was what-ever you wanted it to be. Considering number of characters in English language is 26, it is reasonable for a byte to be 5 bits, giving you a total of 32 possible states. Which leaves you with 6 values which could be used as control characters. how-ever lets not forget there are 7,164 other languages of the world, and they all have their own unique way of doing things. Oh yeah, lets not forget that at the time you had other nationalistic countries/territories/people with their own superior technology all vying for the top position, all well trying to out do each other. Then you also had manipulative monopolies/trade embargo's and wars. It isn't perfect but people aren't perfect. reply KingOfCoders 14 hours agoprevFor everyone who doesn't need ä,ü,ö. Or software that needs to take ä,ü,ö. For everyone else, UTF is a blessing. reply bigstrat2003 13 hours agoparentWhich, given the people who designed this and the time they were designing for, was most of them (and most of their audience). Don't confuse \"this old standard doesn't adequately cover all cases today\" with \"this old standard sucked at the time\". reply lmm 13 hours agoparentprev> For everyone else, UTF is a blessing. Except people who want to use Japanese and not have it render weirdly, something that was easy in any internationalised software that used the traditional codepage system, but is practically impossible in Unicode-based software. reply Retr0id 12 hours agorootparentWhere can I learn more about this issue? reply p_l 12 hours agorootparentProbably referring to so-called \"Han unification\" which tried to use same codepoints for different glyphs to reduce code space for ideograms derived from Chinese ones. But that only causes confusion because you need to provide external information which way to interpret them, just like a code page reply zokier 12 hours agorootparentprevhttps://en.wikipedia.org/wiki/Han_unification reply blahedo 14 hours agoprevAnother piece of elegance: by putting the uppercase letters in the block beginning at 0x40 (well, 0x41) it means that all the control codes at the start of the table line up with a letter (or one of a small set of other punctuation: @[\\]^_), giving both a natural shorthand visual representation and a way to enter them with an early keyboard, by joining the pressing of the letter with... the Control key. Control-M (often written ^M) is carriage return because carriage return is 0x0D and M is 0x4D. reply snvzz 11 hours agoprevThe ASCII table is defective; it is missing a dedicated code for newline. CR and LF aren't dedicated, and have precise cursor movement meanings, rather than being a logical line ender. There was a proposal in the 80s to reassigning the -otherwise useless- VT (vertical tab) character for the purpose. Unfortunately unfruitful. reply Gormo 1 hour agoparent> Unfortunately unfruitful. Fortunately unfruitful, since if it had gained adoption, there'd be a mix of three different line endings (and combinations thereof) in widespread use, instead of two. reply bregma 8 hours agoparentprevA separate control character was not needed to indicate where your Hollerith string ended: it ended at the end of the Hollerith string. If you wanted to render a Hollerith string onto print media, you'd often want to feed the line and then return the carriage before printing the next Hollerith string. Of course, that wasn't strictly necessary if you were using a line printer, which would just print the line and advance. The filesystems I used had 5 kinds of file: random-access, sequential, ISAM, Fortran-carriage-control and carriage-return-carriage-control. The only people who used the latter were the eggheads that used that new-fangled C programming language brought over from Bell Lab's experimental Unix system. You're probably just looking for the record separator (036). If you are storing multiple text records and a block of memory, that would be the ideal ASCII code to separate them. reply 1vuio0pswjnm7 10 hours agoprevMentioned in footnote 7: https://ia601808.us.archive.org/2/items/mackenzie-coded-char... reply kragen 16 hours agoprevunfortunately this page is based on mackenzie's book. mackenzie is the ibm guy who spent decades trying to kill ascii, promoting its brain-damaged ebcdic as a superior replacement (because it was more compatible, at least if you were already an ibm customer). he spends most of his fucking book trumpeting the virtues of ebcdic actually bob bemer more or less invented ascii. he was also an ibm guy before mackenzie's crowd pushed him out of ibm for promoting it. he wrote a much better book about the history of ascii which is also freely available online, really more a pamphlet than a book, called \"a story of ascii\": https://archive.org/details/ascii-bemer/page/n1/mode/2up tom jennings, who invented fido, also wrote a history of ascii, called 'an annotated history of some character codes or ascii: american standard code for information infiltration'; it's no longer online at his own site, but for the time being the archive has preserved it: https://web.archive.org/web/20100414012008/http://wps.com/pr... jennings's history is animated by a palpable rage at mackenzie's self-serving account of the history of ascii, partly because bemer hadn't really told his own story publicly. so jennings goes so far as to write punchcard codes (and mackenzie) out of ascii's history entirely, deriving it purely from teletypewriter codes—from which it does undeniably draw many features, but after all, bemer was a punchcard guy, and ascii's many excellent virtues for collation show it as dwheeler points out, the accomplished informatics archivist eric fischer has also written an excellent history of the evolution of ascii. though, unlike bemer, fischer wasn't actually at the standardization meetings that created ascii, he is more careful and digs deeper than either bemer or jennings, so it might be better to read him first: https://archive.org/details/enf-ascii/ it would be a mistake to credit ascii entirely to bemer; aside from the relatively minor changes in 01967 (including making lowercase official), the draft was extensively revised by the standards committees in the years leading up to 01963, including dramatic improvements in the control-character set for the historical relationship between ascii character codes and keyboard layouts, see https://en.wikipedia.org/wiki/Bit-paired_keyboard reply yawl 15 hours agoprevI also wrote a chat novel about ASCII: https://www.lostlanguageofthemachines.com/chapter2/chat reply johanneskanybal 7 hours agoprevKind of hard to read something where the author considers every non-english languages equally worthy to emoji’s.. It was good in the 50’s but was important like 4-5 decades too long. reply DonHopkins 2 hours agoprevThe Apple ][ and TTYs and other old computers had \"bit pairing keyboards\", where the punctuation marks above the digits were aligned with the ASCII values of the corresponding digits, different by one bit. Typewriter: !@#$%^&*() Apple: !\"#$%&'() Digits: 1234567890 https://en.wikipedia.org/wiki/Bit-paired_keyboard >A bit-paired keyboard is a keyboard where the layout of shifted keys corresponds to columns in the ASCII (1963) table, archetypally the Teletype Model 33 (1963) keyboard. This was later contrasted with a typewriter-paired keyboard, where the layout of shifted keys corresponds to electric typewriter layouts, notably the IBM Selectric (1961). The difference is most visible in the digits row (top row): compared with mechanical typewriters, bit-paired keyboards remove the _ character from 6 and shift the remaining &() from 7890 to 6789, while typewriter-paired keyboards replace 3 characters: ⇧ Shift+2 from \" to @ ⇧ Shift+6 from _ to ^ and ⇧ Shift+8 from ' to . An important subtlety is that ASCII was based on mechanical typewriters, but electric typewriters became popular during the same period that ASCII was adopted, and made their own changes to layout.[1] Thus differences between bit-paired and (electric) typewriter-paired keyboards are due to the differences of both of these from earlier mechanical typewriters. >[...] Bit-paired keyboard layouts survive today only in the standard Japanese keyboard layout, which has all shifted values of digits in the bit-paired layout. >[...] For this reason, among others (such as ease of collation), the ASCII standard strove to organize the code points so that shifting could be implemented by simply toggling a bit. This is most conspicuous in uppercase and lowercase characters: uppercase characters are in columns 4 (100) and 5 (101), while the corresponding lowercase characters are in columns 6 (110) and 7 (111), requiring only toggling the 6th bit (2nd high bit) to switch case; as there are only 26 letters, the remaining 6 points in each column were occupied by symbols or, in one case, a control character (DEL, in 127). >[...] In the US, bit-paired keyboards continued to be used into the 1970s, including on electronic keyboards like the HP 2640 terminal (1975) and the first model Apple II computer (1977). reply transfire 18 hours agoprevOne downside of ASCII is the lack of two extra “letters” (whatever they might be, e.g. perhaps German ß), as it makes it impossible to represent base 64 alphanumerically. So we ended up with many alternatives picking two arbitrary punctuation marks. reply th0ma5 18 hours agoprevI heard someone describe the ASCII table as a state machine. Guess I could understand that as a state machine needed to parse it? This is surprisingly hard to search for but I was wondering if anyone knows what they were talking about. reply kevin_thibedeau 16 hours agoparentBespoke hardware for text handling isn't a thing these days but would have been in the 60's and 70's. A table layout that can be easily decoded in hardware simplifies the necessary circuitry for responding to control characters or converting binary numbers to/from decimal when the microprocessor hadn't been invented yet. reply gumby 7 hours agoparentprevIt was originally implemented in actual hardware (rods and bars). Just look inside a teletype, like a KSR-23 (pre ascii, but similar) reply EvanAnderson 17 hours agoparentprevThey might be talking about using escape sequences to map additional codepoints into ASCII. It was designed to be extensible. See: https://web.archive.org/web/20150810075144/http://bobbemer.c... reply jiveturkey 19 hours agoprevebcdic is also quite elegant https://news.ycombinator.com/item?id=13543715 reply theamk 16 hours agoparentIt's really not. In base-2 machines, the letters are mixed with punctuation, which is pretty horrible design which makes simple things complex, and does not actually bring anything new to the table. In BCD machines it is slightly better, except letters aren't contiguous either - row 0 is bad, but it's the extra space between R and S which is really ugly. And it's unusable with BCD operations anyway, as high nibble values are used extensively. Naive sorting simply does not work... lowercase before uppercase, punctuation in the middle of the alphabet, numbers after letters. I see no elegance there, it's like the worst example of legacy code. reply jiveturkey 1 hour agorootparentIt was designed for a specific purpose ... elegance in context reply theamk 12 minutes agorootparentWhich would that \"specific purpose\" be? Even punch cards have alphabet interrupted between R and S. And the whole punch card -> 8-bit is pretty illogical, just like the cards themselves. How come no punches in zone don't correspond to 0 high bits? (and don't get me started on punch card.. it started with \"let's do 1 hole per column for digits\" - OK, makes sense; then \"let's do 2 hole/column for uppercase\" - I guess OK but why did you put extra char in the middle... but then it's 4 holes/column for superscripts? 3-6 holes/col for punctuation? If someone were to design punch cards today but using same requirements, they could easily come up with a much more logical schema) reply gerdesj 18 hours agoparentprevIts shit if you don't routinely speak or write English. On those grounds, I'll decry it as not only shit but purposely shit. OK a bit over the top ... the designers of EBCSDIC had a rather tight set of constraints to deal with, none of which included: \"be inclusive\". Again, if I really had to be charitable (I looked after a System/36, back in the day), the hardware was rather shit too, sorry ... constrained. Yes constrained. Why should six inch fans fire up reliably after a few years of use and not need a poke after an IPL? No real dust snags and I carefully sprayed some WD40 on the one that I could get at. I have modern Dells and HPs in horrid environments that do better with shitty plastic fans. EBCDIC is not elegant at all unless excluding non English characters in an encoding system is your idea of elegant. According to this: https://en.wikipedia.org/wiki/EBCDIC it expended loads of effort with dealing with control eg: \"SM/SW\" instead of language. ASCII and EBCDIC and that basically say: fuck you foreigners! We now have hardware that is apparently capable of messianic feats. Let's do the entirety of humanity some justice and really do something elegant. It won't involve EBCDIC. reply netcraft 15 hours agoprev [–] I've searched off and on for a great stylistic representation of the ASCII table, id love a poster to hang on my wall, or possibly even something I could get as a tattoo. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The ASCII table, standardized in 1963, assigns meanings to 100 of 128 possible 7-bit binary codepoints and remains relevant today, especially with UTF-8 encoding being backward-compatible.",
      "The table includes control codes, printable characters, and follows specific binary patterns, with the space character being the first printable character for sorting purposes.",
      "ASCII’s design is logical and aesthetically pleasing, making it coherent for humans to learn and understand, reflecting its historical significance and refined logic."
    ],
    "commentSummary": [
      "The post discusses the elegance and utility of the ASCII table, a character encoding standard used in computing.",
      "It highlights how users can access the ASCII table on Linux systems via the command `man ascii`, which is useful for shell escape codes and regular expressions (regex).",
      "The conversation includes historical context and personal anecdotes about learning various technologies through manpages, emphasizing the ASCII table's longstanding relevance in computing."
    ],
    "points": 248,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1721687477
  },
  {
    "id": 41042123,
    "title": "Is it possible to make FAANG salaries without working there?",
    "originLink": "https://news.ycombinator.com/item?id=41042123",
    "originBody": "The gap between FAANG and non-FAANG salaries is quite substantial. For example, a Staff Engineer at an avg startup might get $250k base salary, in a HCOL area and maybe a 10-20% bonus.But a FAANG Staff Engineer will get a similar base, and then $1mil in stock for 4 years.-- Are there people outside of FAANG and Big Banks &#x2F; HFT firms making those kinds of monies?-- if So who are they? and what do they do?",
    "commentLink": "https://news.ycombinator.com/item?id=41042123",
    "commentBody": "Is it possible to make FAANG salaries without working there?241 points by zer0sand0nes 15 hours agohidepastfavorite442 comments The gap between FAANG and non-FAANG salaries is quite substantial. For example, a Staff Engineer at an avg startup might get $250k base salary, in a HCOL area and maybe a 10-20% bonus. But a FAANG Staff Engineer will get a similar base, and then $1mil in stock for 4 years. -- Are there people outside of FAANG and Big Banks / HFT firms making those kinds of monies? -- if So who are they? and what do they do? keiferski 3 hours agoIt's interesting how so many people have an aversion to being a business owner / entrepreneur, and will instead put in so much effort to optimize being a well-paid employee, instead of learning how to start and run a business. These salaries listed are great salaries, no doubt, but they are not dramatically more than say, a guy running a successful local plumbing business. And the ceiling is limited as an employee, whereas it's virtually unlimited as a business owner. If you aren't a shoo-in at FAANG and you want to make that kind of money, your best bet is to learn how to start and run a business. I don't mean try to launch a billion dollar social app that needs venture funding. I mean a SAAS that solves a boring problem for other businesses with money to spend. reply Rinzler89 3 hours agoparent> instead of learning how to start and run a business Because the stress and risks of running a small business as a solo inexperienced first time business owner are insane compared to a regular 9-5. Especially if you start hiring other people, your liability then increases 10 fold as now anything can happen with them (sick leave, absenteism, low performance, sabotage, etc) but you're still bound to the same deadlines you agreed with your customers or you'll get sued for damages by them. As an employee you have some basic rights and protections from the state on the limits your employer can squeeze from you, more or less, depending on where you live. As a contractor or company however, you don't, and can be fully liable in court for your failures to deliver regardless of your personal circumstances. Unless you know what you're getting into and have the know-how, experience, or mentorship, it's not worth it in most cases. reply keiferski 3 hours agorootparentI get that, but if you’re trying to get a job at one of the best paying companies in the world, is the stress level actually that different? And there are plenty of examples (search on HN for them) of people running one person internet businesses and making a healthy living. reply brigadier132 3 hours agorootparentIm doing both and yes its very different. The stress at the big companies is nothing. reply keiferski 3 hours agorootparentSure, I get that. But are the people trying to maximize income as an employee really looking for the low stress route in the first place? I suppose I should have qualified my comment by saying it's if your primary aim is to maximize income. reply The_Colonel 2 hours agorootparentStress level has very unclear correlation with compensation. My very personal experience was that higher the compensation, less stressful the job was. reply dontlikeyoueith 2 hours agorootparentAt my compensation my only stress is what happens if I get laid off and can't find new work in this market. reply filoleg 2 hours agorootparentprevMy primary goal is to maximize my income with the least amount of stress possible, and FAANG hits that spot just right. I guess you can say I could maximize further by switching to finance or starting my own business, but the amount of extra stress is just not worth it at the moment, personally. reply Rinzler89 3 hours agorootparentprev> is the stress level actually that different? Your liability is. Worst case scenario is you don't get hired, or you get hired, and then get fired. That the worst that can happen. FAANG isn't gonna come after you for damages if you accidentally brick something in production or are just lazy and incompetent. As a business, your customers might. Let's watch how the Crowdstrike scenario unfolds in the coming months, when they'll probably get lawsuits up the ass for the damages they caused. Get your popcorn ready. >And there are plenty of examples (search on HN for them) of people running one person internet businesses and making a healthy living. Sure, but not everyone can get there, the same how not everyone can become a successful football player despite so many successful examples. There's too many variables. Usually, the right connections are your most valuable asset as one man company, and not everyone has them. reply shrubble 3 hours agorootparentCrowdStrike is a publicly traded company; no one is coming after the founders or the CEO for money they have cashed out via stock sales, prior to the event. So it's not a great example. A better example is simply having your business evaporate overnight while you have a building/offices to pay for, employees to pay, etc. Which happened to someone I know back in the early days of Windows NT: his servers and desktops that he built, worked under NT but then a service pack for NT 3.51 broke everything; overnight his contract with a large local hospital went away; that a few months later the next service pack fixed the issues, meant nothing to his former customers. He went from making the equivalent of $250K today to approximately zero. reply keiferski 3 hours agorootparentprevCrowdstrike is essentially on the level of a FAANG, so I don't think this is remotely comparable to the type of business I mentioned. The likelihood of a one-person SAAS getting sued into oblivion is pretty low, especially if you aren't providing something as fundamental as security services to banks and airlines. reply arrosenberg 3 hours agorootparentprevIf you are a tiny business, you just need an indemnity clause. If you have employees, you pay for E&O insurance. Im not saying any of this is obvious, but if you are applying to work at a FAANG, I will assume you are both smart enough and ambitious enough to do a bit of research. reply ghaff 3 hours agorootparentIt's totally understandable if you don't want to sell/market your business or you just don't want the unpredictability which is probably greater than with a larger company. But, oh no, I might be sued should almost certainly not be a major consideration relative to other factors. reply FooBarWidget 2 hours agorootparentprev\"Just\" an indemnity clause. First, you don't really know whether that'll hold up in court until it happens. Second, by the time you're done with the court case, you'll have spent a tremendous amount of time and energy. Not only is that stressful by itself, it's also a big distraction from running the business, and every day you are busy with the court case you'll wish you were spending time working your business. The smaller you are, the more impactful distractions are. reply jt2190 2 hours agorootparentThis ignores the fact that nothing is stopping someone from suing you personally today, small business owner or not. There are two types of people who sue: - rational actors who have acted in good faith, exhausted all avenues of recourse, and feel like the courts are their only hope for some sort of resolution. - irrational actors who sue for weird personal reasons, can’t be reasoned with, refuse all attempts to come to a reasonable settlement, etc. You definitely want the first type of person as a customer. Again, nothing is stopping anyone from suing you personally right now. reply sillysaurusx 3 hours agorootparentprev> FAANG isn't gonna come after you for damages if you accidentally brick something in production or are just lazy and incompetent. As a business, your customers might. So? Worst case, the business folds and you start it again. This is like being worried about being fired, and people telling horror stories about so-and-so who got fired. They’re fine. You’ll be fine. The whole point of an LLC is to prevent people from going after the corporation’s owner. They have to go after the corporation itself. It’s in the name: limited liability corporation. And sure, if you end up harming someone physically then they might have a case against you, but most of us don’t run into that problem. reply gmadsen 3 hours agorootparentprevisnt this the purpose of LLCs? reply robotcookies 50 minutes agorootparentLLCs limit your financial liability, but you can still work thousands of hours on a business and end up making nothing. If you work the same thousands of hours as an employee, you will make money. reply stonethrowaway 3 hours agorootparentprev> Because the stress and risks of running a small business as a solo inexperienced first time business owner are insane compared to a regular 9-5. Believe it or not you get over this. It’s kind of like the fear of going up on stage to give a presentation. Some people get an addictive rush, these are usually serial entrepreneurs who just love to be in charge because they have no problems with risk/stress tolerance (mostly). You get better at handling the stress as you get more comfortable with where your business lies and what value you offer, et cetera. Hiring/firing is hard for people but you have no choice and certainly you’re not beholden to give people jobs. People are there to work, do their best at the moment, your job is to ensure them they’re looked after adequately and your door is always open to them if they have to talk to you. Everyone should attempt to start a small business or a side consulting gig. Failing that, you should strive to make it to VP-suite of your company where in most cases you’ll be tasked with running an arm of the business - the risks are there but it’s more of a simulated environment as well. The CEO will shit test you in almost everything you do and say and this is the grilling you will need, because they expect you to be capable of running things in their stead. reply timcobb 3 hours agorootparent> You get better at handling the stress as you get more comfortable with Some people, but not many people... > Everyone should attempt to start a small business or a side consulting gig Why? > Failing that, you should strive to make it to VP-suite of your company Why? Why take time out of your day to tiger parent strangers on HN when you can just have your own children and damage them instead? reply cushpush 1 hour agorootparent>just have your own children and damage them instead reply Thorrez 1 hour agorootparentprevI agree with your first 3 sentences. But let's try to keep the guidelines in mind: >Be kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes. https://news.ycombinator.com/newsguidelines.html reply HWR_14 2 hours agorootparentprev> these are usually serial entrepreneurs who just love to be in charge because they have no problems with risk/stress tolerance (mostly). More accurately, they have made enough money from a successful venture that they can afford to fail and their risk/stress is lowered. I know serial entrepreneurs who failed several times in a row. Without some form of safety net along the way (an exit, taking money off the table in a later round, simply nice cashflows they were able to bank for a few years), they all seem to burn out. reply poulsbohemian 52 minutes agorootparentprev>Believe it or not you get over this. If your business succeeds. But if your business doesn't, you either enter zombie mode or you find yourself in the bread line. reply wepple 3 hours agorootparentprev> Everyone should attempt to start a small business or a side consulting gig. Failing that, you should strive to make it to VP-suite Utter horse shit. I want to earn enough to live without too many worries, and spend time with friends and family. Being able to leave at 5 is irreplaceable reply tracerbulletx 1 hour agorootparentNot to mention the existence of society literally depends on massive large coordinated efforts. It would be completely impractical to operate a society where everyone was an independent small business owner. Imagine trying to operate the global logistics system, or plan, build and operate off-shore oil wells. Being part of a team is a valid and honorable thing. reply IG_Semmelweiss 1 hour agorootparentprevAs an entrepreneur, you can leave anytime you want, but of course, you can't leave the responsibility aside. With an employer not offering remote, you can set your own rules around it. You can work from Bali, if you want and your customers are OK with it. But i agree , the small business route is certainly not for everyone. reply riehwvfbk 2 hours agorootparentprevI don't think there are many places in tech where an employee can leave at 5 and expect to keep their job. reply wepple 2 hours agorootparentI politely disagree. If you think “hours spent at desk” correlates with performance, I don’t really know what to tell you. I do work late, and over my career have absolutely pulled all nighters and weekends and been on the redeye, but that’s when I stretch or flex, and it’s simply not sustainable. reply The_Colonel 2 hours agorootparentprevOf course there are. In my 20 years of experience I've never worked in a place which would expect people to work overtime. (this wasn't in my selection criteria BTW) reply SoftTalker 2 hours agorootparentprevThere absolutely are, but they aren't startups. reply ghaff 2 hours agorootparentprevI've always been somewhat flexible, especially when traveling, but I've pretty much never worked super-long days as a routine. reply zeendo 45 minutes agorootparentprevYou've been brainwashed. reply aprdm 1 hour agorootparentprevNot true at all. reply copperroof 2 hours agorootparentprevNo. Speak for yourself. reply hluska 45 minutes agorootparentprevI don’t see much evidence of actual experience in this comment. Maybe you have a lot of experience and aren’t very good at conveying it but honestly, it sounds like you have thoroughly drank the startup koolaid. > Believe it or not you get over this. This screams either a lack of experience or a lack of meaningful social relationships. Many people never get over this - it is incredibly risky and stressful being a solo founder. > You get better at handling the stress as you get more comfortable with where your business lies and what value you offer, et cetera. Again, not true. Some people can do this and others can’t. Fundamentally, it depends on actually providing value, being aware of the value you provide and being able to quantify that. Those are three difficult skills and many people cannot acquire them. > Everyone should attempt to start a small business or a side consulting gig. Absolutely no. Many people should not attempt this. In fact, I’ve been doing this for around thirty years and I think the majority of people should avoid it. People with young families, credit card debt and spending problems have no business going out on their own. > Failing that, you should strive to make it to VP-suite of your company where in most cases you’ll be tasked with running an arm of the business - the risks are there but it’s more of a simulated environment as well. No, people should strive to make a living, pay their bills and find something approaching happiness. reply racional 1 hour agorootparentprevBelieve it or not you get over this. You can get over it if you dig the day-to-day of running a business. But not everyone digs it. Everyone should attempt to start a small business or a side consulting gig. No they shouldn't because as an endeavor it just doesn't appeal to lots of people. The \"everyone should try and become good at the stuff I think is fun\" tone of this response is just weird, and grossly out of touch with observations of human nature. reply brotchie 2 hours agoparentprevStartup guy who’s now at FAANG. I’m technical technical technical (IC track). I love building products. Prototyping to find PMF and then spinning that up into a staffed project, then onto the next new thing. Get to work with excellent SWEs, PMs, PgMs, etc. In startup land I was doing so much, pitches, recruiting, coding, landing page design, dev ops, pretty much the full stack. In FAANG land I can just focus on existence proofs and then there’s a whole support structure helping that happen and bringing the product to market. You could argue that setting up this pipeline is the goal of a startup, but it’s a LOONG road. The opportunity cost is immense. It’s immensely difficult to recruit. You have to effectively pause your life. Scary to have children with startup level uncertainty. At a certain $$ where you can live a comfortable life, enjoy 95% of what you do day to day, get to be in an environment with top notch colleagues, it’s a tough sell to start something from scratch with all that risk. reply nicce 50 minutes agorootparent> I love building products. Prototyping to find PMF and then spinning that up into a staffed project, then onto the next new thing. How many products you have actually finished and worked on and polished over years if they produced something valuable in the end? That defines the actual realised value of the business. Jumping from one product to another does not mean that all of them were constantly producing revenue or were success. It might look good in the CV but what was end result? reply brotchie 22 minutes agorootparentHit and miss. It’s pretty equivalent to % of finding product market fit in startup land. Mostly worked on internal products. One HUGE hit, a few moderate hits, and quite a few failures. The calculus for projects at a large bigco are different from a startup though. It’s not necessarily clear bottom line revenue, especially for innovation teams. Crypto, for example, bigco needs at least some folks working on it to build an understanding of the technology, market, pain points. Some of these projects are started with literally no intention of profitability, but as an explicit build-to-learn. There’s a wide spectrum of SWE interest and capabilities. Some folks love the creativity of greenfield new product exploration. Others love the focus of crafting a perfectly scalable, maintainable system. These are really just points along a product lifecycle. I understand myself enough and am unapologetic about getting “bored” easily, and need to be constantly working on creative things. Others at the other end of the spectrum have a visceral reaction to that. They can’t do what I do, and I can’t do what they do. reply anomaloustho 3 hours agoparentprevStarted a business doing 1M annual recurring. I have plenty of reasons why it’s more advantageous. • When times get tough, cofounders playing chicken to see who can take the lowest salary • Giving yourself the lowest pay to prevent income loss of your employees • Always having to hunt for sales to maintain your survival (which is typically nebulous and subject to when they feel like signing) • HR and always trying to figure out how to not get sued • Losing big customers or big customers going out of business For any one of those bullet points there’s a coulda woulda shoulda, but again, just taking your 400k from a well established company that isn’t fighting for its survival every day is not inherently inferior to the heroics and cortisol required to operate a business. reply havefunbesafe 2 hours agorootparentJust went through a lot of these, and can confidently say: way way more dangerous to own a company than be a part of one. Significantly less fun, however. I believe it comes down to personality type. reply epicureanideal 3 hours agoparentprev> I mean a SAAS that solves a boring problem for other businesses with money to spend. Profitable software-based ideas are increasingly difficult to find. Thousands of people are exploring every niche and trying to start startups in them, which is great, but competition is fierce and unexplored niches are shrinking. And of course, it's still not straightforward even if you find a relatively unexplored niche. Most people I see who have attempted startups had extensive family support or personal wealth from exits as an employee of a company that had an IPO. reply keiferski 3 hours agorootparentPeople say this but I don’t buy it at all. If anything, it feels like the vast majority of the business world is still operating on some combination of Excel and a software program that was outdated a decade ago. The main issue is that their problems are too boring and obscure for anyone without direct personal experience to care about them. reply bnhlth 1 minute agorootparentI’ll add to that: in many industries/domains the incumbents are benefiting from the status quo and have no interest in upending it, while others have accepted some of these as “hard facts”[0]. I’m working on one such problem in the healthcare space, as an outsider. In early stages, so we’ll see if it goes anywhere. [0] https://www.sequoiacap.com/article/pmf-framework/ reply epicureanideal 3 hours agorootparentprev> The main issue is that their problems are too boring and obscure for anyone without direct personal experience to care about them. I think there are probably several hundred HN readers who would be happy to tackle those problems. Boring is probably not the issue, but obscure is, assuming these problems exist. If they're difficult to discover that the problem exists, that's a challenge. Further, if the problems exist but are highly unique in each case, there may be a reason it's solved in a one-off way by Excel or scripts rather than a scalable product. The use of Excel isn't inherently ripe for disruption. There are plenty of problems to be solved that just can't be solved profitably. It would be no better than solving a problem that someone has but is unwilling to pay for. reply dannyphantom 3 hours agorootparentYour comment reminded me of a cool little project I found a few years ago re: niche problems and smaller solutions - the target demo being soldiers who (very generally) have some disposable income to spend on a small app that can improve their professional QoL was kind of genius imo. There was (still is, though the website is now dead) a really cool website/app that a fmr Air Force soldier made called AFI SWiM[0] that extracted 'Shall, Will and Must' references from Air Force publications which blew up in popularity among the target demo. The dev recently(ish) wrote a blog post[1] how it surprised him to see the app[2] had gotten around ~20k downloads, though I am unsure if the app was always priced at $1.99 (which would be just under $40k). I like the idea of developing a problem solver for a niche market, though finding and monetizing it definitely is pretty tough. [0] https://web.archive.org/web/20220601160702/https://swim.afie... [1] https://willswire.com/blog/chat-afi/ [2] https://apps.apple.com/us/app/afi-explorer/id1564964107 the github repo: https://github.com/willswire/afi-swim reply cvwright 2 hours agorootparentYour story supports the position that it's safer to take the FAANG job. This guy found an underserved market opportunity, built something, got some success marketing it, and made a whopping total of $40k. Meanwhile the FAANG guys are making that every two months, without all the risk. reply wonger_ 1 minute agorootparentImagine if this app was B2B instead of B2C, or if it pivots to B2B in the future, or if it's acquired. Imagine also future passive income with little maintenance. And his project seemed like a side project written over a few months. Still risky, but the expected value is comparable to a salaried job IMO, once you find a decent market opportunity. Not to mention the qualitative value of self-employment, and that FAANG employment seems a bit risky these days with layoffs. ak_111 2 hours agorootparentprev50% of those places are using excel and outdated tech because they have 0 budget to spend on tech no matter how good the replacement (in fact even if it is cheaper, approving the alternative is too painful a process), \"their problem\" is not that nobody had solved their actual problem but trying to convince someone upstairs to approve spend on an alternative solution. reply hn_throwaway_99 3 hours agorootparentprevJust check out the number of posts you see on HN where you have people that started \"successful\" small (their problems are too boring and obscure for anyone without direct personal experience to care about them. And the problems were so important that somebody, anybody, stepped up to the plate with a digital solution as soon as it was barely possible. Sometimes you have to admire that effort to a greater degree than a more technically advanced alternative. I'm sure some of these are ripe for transition to a truly more effective approach in every way but often there is no fooling them trying to provide anything less. And disruption itself can be the enemy in some things like this. reply Merad 3 hours agorootparentprevI used to work in internal IT at a Fortune 500 working with different departments in the company to turn Excel spreadsheets and Access97 tools into web apps with real databases. The problem is that the companies/departments where this is the case all have their own workflows and business processes, and 99% of the time are completely unwilling to consider change. It'll be really challenging to develop software that can be sold to multiple companies even when they ostensibly do the same thing. reply AnarchismIsCool 2 hours agorootparentprevOne word. Hardware. There is limitless opportunity outside of the webtech/adtech bullshit bubble. Go create a hardware startup, it's never been easier. It's still harder than adtech bullshit was 10 years ago, but the turnkey manufacturing industry is faaaar better than all the \"hardware is hard\" weekly blogspam would lead you to believe. reply wild_egg 1 hour agorootparentAny hints on how to even begin looking into this? There's a plethora of content on starting SaaS projects but can't say I've ever come across any for hardware. reply borroka 53 minutes agoparentprevA comment, the one I am responding to, that would fit much better in the \"hopeful\" bucket than in the \"realistic\" one. I, like many others, hold a white-collar job in a technology company, not FAANG, with a salary between 300 and 400K per year, with lunch breaks, paid vacation days, good health insurance, and the opportunity to work from home at least 75% of the time. At the end of my workday, which I have not found to be stressful in at least 3 years, I am fresh, hydrated, and ready to spend late afternoons, evenings, and nights pursuing my interests, taking care of the kids, and maybe learning French at last. I see guys running fairly successful local businesses, both physical and digital. Assuming you have a choice between the two paths, a brief look at the physical condition and a conversation with me and the business owner would quickly lead you to prefer one of the two paths. I might be privileged, lucky, in an unusual situation. But the same is true for the successful local entrepreneur running a plumbing business with profits equivalent to those of experienced people working in tech. reply mattmaroon 3 hours agoparentprevAs someone who has been self-employed for over 20 years, done the startup thing (including going through YC in the very early days), I totally understand why people would not want to. There are real downsides to owning your own business. It’s probably been 15 years since I had a day where I didn’t check my email. Most people are Indians, not chiefs, and I think that’s actually an evolutionary adaptation that’s built into us. Someone may want to be the best programmer they can be rather than the best at managing employees, setting business strategy, dealing with taxes and other paperwork, etc. Totally get it. reply ghaff 3 hours agorootparentIn general, one-person shops have real downsides for both owners and customers. As an employee, I've often been in situations where I simply haven't been reachable for extended periods of time which obviously isn't great for a customer either absent a fallback. Fallbacks of various sorts are available but probably not at \"the SaaS is down\" level. reply scosman 2 hours agoparentprev> your best bet is to learn how to start and run a business Startups are so hit and miss. It's really really hard to make $500k a year at a startup. There's many years of zero, near zero or sub-intern salary. After that there's either failure or a big payout that dwarfs the FAANG salary. Classic businesses can get a substantial salary sooner than a startup, but do have a ceiling. If you're selling a well known need (plumbing, consulting, etc), there's competition and a ceiling at hours worked. Having done both, a FAANG job is infinitely less effort than running a company. Also lower stress, lower risk, and immediate rewards/pay. I still have more fun starting companies though! reply fuzzfactor 1 hour agorootparentThat's a really informed comment from both sides of the exact fence. >there's either failure or a big payout that dwarfs the FAANG salary. Tech or not I think it usually doesn't dwarf anything but if it comes close, a little bit low or high, that's a huge milestone above failure. In that case it might allow you to maintain readiness for an incredible bonanza which might be within reach only from beyond that point, and that can be what dwarfs the \"big payout\". IOW depending on the situation, occasional big payouts that really just even things out may not be very life-changing even if they are big, but it can give you more rolls of the dice at things that can't be approached any other way. reply John23832 42 minutes agoparentprevIt's not as simple as \"start a business\". You have to find a business that makes sense for you (you know it, you want to put up with it, etc). You then have to execute. All the while hoping that you don't get struck by lightning (something kills your business), while also hoping to get struct by lightning (you become wildly successful). God forbid you need job security (family, health care, etc.). Contrast this to going into a job, where what you do is laid out for you, you just have to execute. With insurance. With job security. Even as someone who has run a startup and still has the itch, it's a no brainer. reply kevinsync 3 hours agoparentprevYes. This is a bit reductive, but literally everything is out there for the taking -- you just have to go out and get it! Anecdotally, I've noticed that a disproportionate amount of formally-educated people I've encountered in life are more allergic to this idea than the opposite.. I don't have any solid theories as to why, exactly, but it's curious nonetheless. reply xeromal 3 hours agorootparentBecause a lot of people don't want everything resting on them. Takes a special kind of person. I know I ain't one. My dad owned his own business with a 100 employees and it was miserable growing up reply randomdata 1 hour agorootparentprev> I don't have any solid theories as to why What sees you discount the obvious? That is, formally educated people end up in formal education because they have a proclivity towards not wanting to have to go out and get it. They want someone else to hand it to them. Which is, after all, what the \"formal\" part of formal education describes. reply sakopov 1 hour agorootparentprevThe problem is identifying what to go out and get. Because most people will go out and get the wrong thing and end up burning themselves out of future endeavors. reply mr90210 3 hours agorootparentprevHave look at Nassim Taleb’s Skin In The Game. He explores this topic at length. reply randomdata 1 hour agoparentprev> will instead put in so much effort to optimize being a well-paid employee, instead of learning how to start and run a business. Being an employee is still running a business. The separation you are trying to draw isn't meaningful. That particular line of business is one of the few businesses out there with a near-guaranteed customer base, though. That's a highly attractive trait for someone operating a business. With small risk comes small reward, of course, but it is no surprise that most businesses venture in that direction. reply rco8786 2 hours agoparentprev> so much effort to optimize being a well-paid employee It seems like you are vastly underestimating the effort (and risk) it takes to start a successful SaaS that brings its founder $500k+ annually. You're right about everything if you can actually start and grow that business, but you're glossing over how difficult that actually is to do, and the fact that you have to start from $0 income. reply sakopov 1 hour agoparentprevNothing interesting. No matter what people say, starting a business is incredibly difficult and takes a certain amount of luck. I've done 3. Failed at all 3. Made $0. Some people aren't cut out and are better off being employees. reply seanmcdirmid 1 hour agoparentprev> It's interesting how so many people have an aversion to being a business owner / entrepreneur I like coding. I don't want to run a business, and these are very separate skills. This is like asking why an IC doesn't want to become a people manager. reply y-c-o-m-b 1 hour agoparentprevI don't think this works out well for most people anymore. I mean that was true a few years ago when things were booming even, but now you're almost guaranteed to have a very hard time. I know someone that is over-employed and has over half a dozen jobs (yes you read that right, and it's been verified); most of them are at entrepreneur owned companies and they are all struggling right now, hanging by a thread with a skeleton crew. reply dontlikeyoueith 2 hours agoparentprev> And the ceiling is limited as an employee, whereas it's virtually unlimited as a business owner. It's also unlimited as a gambler. That doesn't mean everyone should start buying lotto tickets. reply entropyneur 3 hours agoparentprevYou make it sound so easy. I've ran small businesses for most of my career and I've never made more than a quarter of what I currently make working as a contractor (ironically, for a business I once started, now owned by someone else). And most of the time I earned 5-10% of that. reply fuzzfactor 1 hour agorootparentGood to get your message, I have some of the same experience. I do better as a consultant/contractor for a client who is a previous customer from when I had my own facility. Fundamentally I used to provide the paperwork (the deliverable/invoiceable product) and supported everything needed to generate it, including any free advice which goes along to smooth the flow. Now I'm paid to help their paperwork achieve and maintain value with none of mine in the mix at all, but I'm basically doing a lot of the same stuff when I'm on site. As a contractor aren't you now a small business operator of a different type, perhaps much smaller? I would estimate in more of the exact same business in so many ways very few ever come close to that. Maybe even doing some of the same things under different degrees of ownership and responsibility, and this could be a good example of way different compensation when the only difference is the underlying arrangement. Looks like ironc can be good :) reply keiferski 2 hours agorootparentprevI'm not sure how I made it sound so easy, because presumably getting a 250k job at a FAANG isn't easy either. Which is what my point was - if you're going to deal with the time and stress to get one of the most desirable jobs, why not use that same time and effort to build a business? reply hn_throwaway_99 2 hours agorootparent> getting a 250k job at a FAANG isn't easy either Except, it kinda is for people that have the right background and are well suited to it. That is, the reason I think you're getting a lot of pushback is that many folks are just inherently risk averse. There are people who are smart, know how to work hard, and know that if they have a path laid out in front of them, they can succeed. For entrepreneurs, there is simply much more variability. You can be smart, work hard, do everything right, and still fail in unexpected ways due to things outside of your control. I don't think entrepreneurship is bad at all, but it's not surprising at all to me that people who want to go the FAANG route aren't well suited to starting their own business. reply entropyneur 2 hours agorootparentprevBecause it's a lot more predictable. Put in the necessary effort and you are almost guaranteed to get a FAANG job. And the best part, it's completely clear where that effort needs to be directed. Yes, you need to have the smarts, but for a business that pays as much you need to have a lot more than that, plus luck. I'm all for entrepreneurship and personally couldn't work for one of those companies, but with FAANG salaries being this high I don't think those people are making an irrational choice at all. reply johann8384 2 hours agoparentprevBecause I am happier as a great employee. I have a large family and have chosen base salary over stock when it comes up for example. I make a great salary and we can afford to have a lot of fun. We took four months and traveled in an RV. I have 7 lids and they all have a sport so we do lots of Soccer, Field Hockey, Softball, Baseball, etc. That is expensive and time consuming but I am at 99% of their games. I can't do that as well while starting and running a company. reply harimau777 3 hours agoparentprevHow do you get the startup capital to do this? At least in the area where I have know how it would cost at least a million to get the space, hardware, raw materials, and employees to start turning out products. Then you would need money for marketing and logistics. reply whynotmaybe 2 hours agorootparentDepending on where you are, you can often find gov support to start your business. (Here in Canada, this support can be grants, loans or event free experts time.) When it's gov monetary support, you usually have to bring 50% of the total amount. Otherwise, you can contact incubators like YC. If they approve your idea and grant you money, you can then easily apply for loans at banks because you've been vetted by a well known company. After all that energy spent, you will understand the quote : > Everyone has an idea. But it’s really about executing the idea and attracting other people to help you work on the idea. reply dayvid 2 hours agoparentprevDifferent skillsets involved. You will eventually have to give up programming and focus on developer sales/marketing/recruiting skills. Very rare to see businesses where you're sole developer and salesperson unless you are the main SME on a niche field/technology you probably developed. Also A LOT of FAANG is immigrants who can't start a business in the US or children of immigrants whose parents put strong pressure on them having an upper middle class job. reply desdenova 2 hours agoparentprevAnd how, exactly, does one \"learn\" that? To me it's always been a matter of personality. As an introvert, I can't picture myself doing the kinds of social interactions required to sell a business on my own. reply tiffanyh 3 hours agoparentprevStatistically, you'll make more money working for someone else than working for yourself (and have a better work/life balance as well). And you might say, \"well - you'll never become a billionaire working for someone else\". That's not true. There's plenty of CEOs (non-founders) who are billionaires. And the chances of you running your own company and becoming a billionaire, is about the same probability as you becoming a CEO at a Fortune 500. reply randomdata 37 minutes agorootparent> There's plenty of CEOs (non-founders) who are billionaires. Being a business owner doesn't necessarily mean being the founder. No CEO is becoming a billionaire by collecting a paycheque. They might become a billionaire by accepting the right stock offer in lieu of wages, but that makes them the business owner again. reply mciancia 25 minutes agorootparentWith that logic, most (every?) SWE working at faang is a business owner since they all get stock options/rsu reply randomdata 20 minutes agorootparentAbsolutely. FAANG wages are certainly competitive, but not out of line from what you can make in any old Poducksville software company. It is the becoming an owner of FAANG that sets them apart financially. reply anticorporate 1 hour agorootparentprevWhile true, I'd suggest if someone's career choices are primarily motivated by a desire to become a billionaire, they are not making rational career choices anyway and are likely to be unswayed by, you know, evidence. reply brigadier132 3 hours agoparentprevBuilding a business is really hard. reply eloisant 2 hours agorootparentEspecially a business that will get you the same revenue as a FAANG job. reply neilyio 2 hours agoparentprevWould you be open to mentoring someone curious about doing so? reply Bjorkbat 2 hours agoparentprevI mean, that's what I'm trying to do (the business, not the monetary milestone), but something to be mindful of is that on Indie Hackers only 12 people out of 17,000 in the community got to $10k MRR. https://www.indiehackers.com/post/holy-heck-this-is-hard-8eb... Granted, this is Indie Hackers, so take this with a grain of salt, but I think the general idea still applies. Getting to $10k a month, let alone making $250k a year (so ~$20k/month) is really hard, probably about as hard as making $250k a year as an employee, maybe even harder. reply alex_suzuki 3 hours agoparentprevWhile running a small business can be incredibly fulfilling and teach you a ton of adjacent skills that you wouldn’t learn as a SwEng at a FAANG (sales, marketing, …), I find it infinitely more stressful than regular employment. There’s just so much more responsibility resting on your shoulders. Holidays have never felt the same since I left employment in 2011. No regrets here, just saying this path is not for everyone and not as accessible as you might think. reply hanniabu 3 hours agoparentprevBird in the hand reply greenthrow 2 hours agoparentprevRunning a business is so much more work and most businesses fail. This is terrible advice. reply joshuamorton 3 hours agoparentprevThey're probably 2-3x the guy running a local plumbing business doing remarkably fewer hours. And of course it's more accessible. The average guy running a plumbing business is probably 30 something, while people can hit 300k at faang a year or two out of college with like one good promotion. reply StopTheWorld 1 hour agorootparentI worked with a guy who went to a public university - not a top 25 public university but in the ballpark. He was hard-working, smart and \"got it\", but he wasn't at the level of some super-geniuses I met coming out of school. He worked with us (Fortune 100, non-FAANG, non-tech, non-big metro) as a junior SWE for a year and did not get promoted as he desired. He jumped ship for a FAANG-adjacent company after a year working for us, and his TC jumped fromTypical middle class houses cost $300k - $500k here, with luxury houses costing roughly $1m and small starter homes costing $200k (just using real estate as a rough proxy for cost of living) That used to be the case where I live, just a few years ago. Now what used to be a starter home is over $400k. reply orochimaaru 4 hours agoparentprevnext [52 more] [flagged] reaperman 4 hours agorootparent[Edited to remove snark/barbs as per HN guidelines]. I am genuinely curious though - you seem like you've had a successful career and life. I typically assume people in your position are rather familiar with the basic math behind personal taxes, 401k's, HSA's, child tax credits, standard deductions vs. large charitable deductions, etc. I think I may have been grossly mischaracterizing a potentially large group of people and would love to hear about how you've handled your taxes since entering the workforce...and what you personally would attribute your misunderstanding of taxes to. I think that would help me better understand and empathize with people who aren't like me. reply medvezhenok 2 hours agorootparentIt's a common misconception because there are plenty of situations where making more money actually decreases your take-home pay. It's just not via the bracket system, it's via other incentives in the tax code (and welfare benefits in certain cases). There are a lot of things (IETC for example, or benefits, or health insurance) which have $ cutoffs. Or tax breaks (i.e. electric vehicle credit) that only apply below a certain income. Exceeding those fixed points can certainly make you make less money. One thing I ran into recently was fiscal incentives for homeownership (though not directly related to taxes) - there is a cutoff at a certain income level where if you stay below it you get a $10K credit towards closing costs. The tax code is full of things like that (or getting your student loans forgiven if you make less than $X for Y years). If you want to see how bad the welfare cliffs are, there are cases where a single mother is better off making $28,000 per year (and receiving all federal benefits), than making ~$70,000 per year (and qualifying for none of them). Image of the welfare cliff effect, example in Pennsylvania: https://qph.cf2.quoracdn.net/main-qimg-7de186ce44f6a86805609... reply freetinker 16 minutes agorootparentprevIt’s an arbitrary human rule, not fundamental knowledge. Quite understandable how someone may not have the interest or knowledge to know this nuance (perhaps to their own detriment). It’s somewhere along the spectrum of the knowledge of arcane laws, for example. The federal tax code perhaps isn’t ’arcane’ by popular consensus in educated circles, but it’s on the spectrum of arbitrary human knowledge that keeps changing and evolving. Would you be shocked if a CPA knows more about this than a theoretical physicist? reply buildbot 3 hours agorootparentprevUnfortunately, it also could be an intentionally false statement in order to push a specific political viewpoint. For example, flat tax rates without income brackets. I don’t think it’s wrong to correct falsehoods strongly in general. reply EVa5I7bHFq9mnYK 1 hour agorootparentprevWell, there is a grain of truth to his words. Over certain income, you are no longer qualified for food stamps, medicaid and section 8, so you are better off earning less. reply yieldcrv 1 hour agorootparentthis line of discussion is amusingly charitable for a person replying to a conversation about earning $150k - $300k/yr they're not eligible for any welfare and are above the best phaseouts reply brodouevencode 3 hours agorootparentprev~Calm down. It's a common misconception.~ reply QuercusMax 3 hours agorootparentIt's a common misconception among people who haven't done even the slightest bit of research. Or done their taxes themselves. Confidently stating stuff that is obviously false SHOULD be ridiculed. reply xeromal 3 hours agorootparentSeems strange that ridicule should be your instinct rather than simple education reply Maxatar 2 hours agorootparentI hate to agree with the ridicule point, but honestly ridicule also deters other people from making a similar mistake. I agree ridicule should be used cautiously, but in a case like this where someone speaks so confidently and boldly gives advice on a topic they clearly do not understand, I think ridicule is a good way not only to inform them, but also indicate to others that it's not okay to give people this kind of advice unless you have a basic understanding of the topic. reply xeromal 2 hours agorootparentI have nothing to back this up but the reason I'm sensitive to ridicule as an initial reaction is that I believe it causes people to shy away and there's potential that they surround themselves with other ridiculed people and now you have a group that is intentionally dense and now powerful. Education should always be the first step. I grew up in rural appalachia and the amount of grief than the uneducated get there for being uneducated causes them to become even more hostile and I really think that the Mr Rogers approach of education would eventually neuter any hostility they had towards the educated. * edit: changed education to educated reply Maxatar 2 hours agorootparentThat is an excellent and profound point and one I can admire. Thanks for sharing it. reply berdario 3 hours agorootparentprevAn educative reply is easier to ignore than a (or multiple) replies in which you're ridiculed. Moreover, being thought about how you're wrong about X might not necessarily translate in the subject being more careful before confidently expressing themselves on a Y topic on which they don't know anything about. Hopefully, ridiculing might make the subject reconsider that part of their character. That said, the HN policies recommend to \"be kind\"... I have no idea how to \"ridicule someone's post in a kind way\" ¯\\_(ツ)_/¯ reply nequo 2 hours agorootparentThe pile of downvotes drives the point home. No need to ridicule, education is better. If you can't educate the poster, at least you educate other readers who also didn’t know, without the unnecessary noise of ridicule. reply reaperman 2 hours agorootparentI don’t think I see the same pile of downvotes that you’re seeing? My comment of ridicule got +10 votes in 15 minutes before I edited it, and now is down to +9 an hour after editing the ridicule out. I see that “Calm down. It's a common misconception” did get downvoted. reply nequo 2 hours agorootparentI mean the original comment that misunderstood marginal tax rates. That one looks very gray to me. (And I think your comment is better without the ridicule! The sources you found for your follow-up comment are interesting and I learned from it.) reply brodouevencode 3 hours agorootparentprevhttps://news.ycombinator.com/newsguidelines.html No, they shouldn't. reply reaperman 3 hours agorootparentprevApparently this belief is far more common than I thought. I expected it to be closer to the percentage of Americans who think the Earth is flat (10%)[0]. Evidently 50% of Americans[1] believe you pay your highest marginal rate on all of your income. I'm blown away, and genuinely humbled. I don't understand how someone can get a Fannie Mae/Freddie Mac home loan, raise children, have a long and successful career and not understand tax brackets -- I clearly am incredibly out of touch with how some people live their lives, and I have a lot to learn about other people. 0: https://carsey.unh.edu/publication/conspiracy-vs-science-sur... 1: https://www.aei.org/economics/survey-confirms-that-many-amer... reply swiftcoder 3 hours agorootparentIt seems to be advantageous to a significant portion of the political class that people hold this misconception - this type of misconception explains much of the widespread public support for tax breaks for the ultra-rich... reply Scarblac 3 hours agorootparentprevThere is a class of rich people in the US who really like that people believe this. It allows them to spread a lot of fear about higher tax rates for high incomes. reply dawnbreez 3 hours agorootparentNot to mention the influence of companies like Intuit, which has been pushing against convenience in tax filing for decades because its entire business model relies on people being unwilling or unable to file by themselves: https://www.propublica.org/article/inside-turbotax-20-year-f... reply brodouevencode 3 hours agorootparentprevIt was preached to me that this was how it worked since I started working in the mid-90s. I didn't learn better until an accountant friend clarified that when I got him to do my taxes when I started making real money in the mid-00s. I dare say that it was probably the most common understanding until fairly recently. The internet has probably helped a lot with that. That is to say, if you're of a certain generation you probably think this way because that's what was generally understood. reply reaperman 3 hours agorootparentI guess it's a fair assumption you never once attempted to calculate your own taxes in the first decade of filing taxes? Did you just drop some papers off to an accountant the first year that you reached adulthood? I've used an accountant occasionally, but only when I had genuinely confusing tax issues involving international work or work across many states where I wasn't sure whether there existed a \"nexus\" that required me to pay taxes in State A or State B. Even then I did my best to double-check their work, and often caught errors/omissions. Most of my friends also do their own taxes (and have since 2006), regardless if they were line cooks or painters or engineers or MBB consultants. So I'm just not familiar with the lifestyles that lead to this. I'd actually understand it more for people who've entered the workforce since 2005, because TurboTax/etc became much more popular. I did taxes with my dad in the mid-90's before most homes had internet, and back then it seemed far more likely that people would understand how taxes worked, because there wasn't super-easy software to do it for you. reply brodouevencode 3 hours agorootparentIf you're in the lowest tax bracket making just above the poverty line, as I was then (as well as most people around me), it was what we all understood to be. reply reaperman 3 hours agorootparentI was in that tax bracket for half my adult life (often did not have enough money to eat chicken and rice at home and just went hungry). I had to do my own taxes because I couldn’t afford an accountant and TurboTax starting taking the piss on their dark pattern pricing schemes. All of my friends at the time were also very poor, it was a giant recession and we worked in restaurants that weren’t getting customers. We got paid $2.65/hr so if we actually had a “good week” our paychecks were $0 (actually negative, honestly, but paychecks bottom out at $0) due to taxes on tips. Now I’m even more confused. Accountants were genuinely expensive and none of my peers in my economic class could afford one. At the risk of repeating myself: how did you file taxes for the first decade of adulthood? If you were too poor pay an accountant like I was…how did you misunderstand how to calculate taxes owed for ten years, but still arrive at the correct amount to pay? Did the IRS often return money to you saying that you over-paid? Did anyone ever try to correct your misconception? I’m trying to understand this in more detail than just “memorize the fact that 50% of Americans were lied to by (someone?) about taxes, and also just blindly accept that there is some magic unknown to me which allowed them to not be affected by that misunderstanding through most of their adult life”. You were in poverty when you had this misunderstanding. This other commenter bought a house while they had the same misunderstanding. (So, “being poor” isn’t the experience you both had in common while holding this misconception and can’t explain it for both of you). I’m trying to understand how that misunderstanding never affected either of you. I’m trying to understand how neither of you ever had to read about how to calculate taxes owed. reply medvezhenok 2 hours agorootparentWhen you're in that tax bracket, there are plenty of tax break/benefit cliffs that are more impactful than tax rates. And they can certainly make your take-home pay go down if you accept a raise. See: https://news.ycombinator.com/item?id=41048205 reply reaperman 1 hour agorootparentBut that still doesn't explain the common denominator between a person living comfortably and a person in poverty both not understanding how tax brackets work or ever having to consider the most basic parts of how their own taxes are calculated, for so, so many years of their lives. reply QuercusMax 2 hours agorootparentprevWho was preaching it to you? reply jgwil2 3 hours agorootparentprevWell around 40% of households don't pay federal taxes at all, and presumably a large chunk of those who do are in the lowest tax bracket so they might not ever need to know how it actually works. reply humansareok1 49 minutes agorootparentprevWait til you learn how many people can't do simple percentages like 10% of 150 or how many people can't do basic arithmetic like 19+15. reply KK7NIL 4 hours agorootparentprevThat's just not how taxes work: https://www.irs.gov/filing/federal-income-tax-rates-and-brac... reply sys_64738 3 hours agorootparentprevYour marginal dollars are taxed at the rate they fall into. So even if you get paid another $200k on top of what you already earn, that additional $200K is taxed on top of your unchanging lower tax rates. reply dvdbloc 4 hours agorootparentprevIn the USA this is not how tax brackets work. You are taxed on your income in each bracket at that rate, not overall reply 698969 3 hours agorootparentOut of curiosity, since you said \"In the USA\" specifically, is there any tax regime in these times where tax brackets don't work like this? reply disgruntledphd2 8 minutes agorootparentI have never heard of one. I believe that in the UK, because of caps on childcare benefits, a wage increase can actually decrease your take-home but generally marginal tax rates operate on marginal income. reply reaperman 3 hours agorootparentprevThey likely don’t know every country’s tax code and instead of confidently spreading misinformation like the above commenter, this commenter is being careful to only state facts they actually do know. reply coev 4 hours agorootparentprevTax rates are marginal, this is wrong. reply gregors 2 hours agorootparentprevYou actually get a tax break after $168,600. After that you no longer get taxed for Social Security. reply Sohcahtoa82 2 hours agorootparentThat tax limit should be removed IMO. reply rconti 1 hour agorootparentprevThis is a completely incorrect understanding of tax brackets. If you make $1 more than your 'old' bracket, you only get taxed 32% on that $1. reply pasc1878 4 hours agorootparentprevRubbish. Suppose your salary is X and the level that 32% starts is M You get taxed on your first $X earnings at the lower rates. Then only the amount above the 32% mark ie X-M is taken at 32%. Thus everyone in the 32% band gets more money than those only in in the 24%. reply redblacktree 4 hours agorootparentprevThat's not how tax brackets work, but it is a common misconception. If you salary goes up, your take-home goes up, full stop. If you are making $191,950/year (the very top end of the 24% bracket for 2024) you will pay $39,110.50 in federal income tax, (before any deductions) for a net of $152,839.50. (You may notice that this is less than 24%) If you make an additional $1,000/year: $192,950, only the additional $1,000 is taxed at the 32% rate. Which makes your total federal income tax bill $320 more, and improving your net take-home to $153,519.50 Edit to add: I'm disappointed that sibling comments to mine are so degrading. We all had to learn this at some point. reply jtbayly 2 hours agorootparentThere are cases where if your salary goes up, your take-home goes down, especially for lower income levels. It’s just not on the basis of tax brackets. There are means-tested benefits that you can lose if you cross certain income levels where an increase in income of $5 can cost you thousands of dollars. reply medvezhenok 2 hours agorootparentYup. Most are for lower income levels but some affect middle class & up as well (electric vehicle credits, financial aid for university, first time homeowner credits, or affordable homeownership breaks... etc) reply freestyle24147 3 hours agorootparentprevI'm not disappointed. This is a classic case of someone who knows nothing about a topic making confident statements that will undoubtedly spread. People who do this should absolutely be ridiculed to discourage spreading total nonsense. And to be clear we're not talking about some esoteric subject -- this is the most basic part of how your salary is taxed by the US government. reply Bluescreenbuddy 1 hour agorootparentprevThat's not how that works. At all. It's marginal. jfc how are we still parroting this falsehood reply TimPC 4 hours agorootparentprevYou don't understand how tax brackets work. You never make less money by moving up a bracket. The bracket rate only applies to income within the bracket. reply smsm42 56 minutes agorootparentTaking into account only the federal income tax brackets, this is true. But tax cliffs exist, e.g. see: https://smartasset.com/financial-advisor/tax-cliff so accounting for all taxes and other factors, there could be a situation where you earn more pre-tax, but end up with less after all is accounted for. For most high-earners it's less relevant as they are past of the most cliffs anyway, but sometimes things like losing access to IRA deductions may be a factor too. reply medvezhenok 2 hours agorootparentprevThe more correct statement is \"you never pay more in taxes by moving up a bracket\" - though even that is not correct in all cases. There are a lot of things (IETC for example, or benefits, or health insurance) which have $ cutoffs. Or tax breaks (i.e. electric vehicle credit) that only apply below a certain income. Exceeding those fixed points can certainly make you make less money. One thing I ran into recently was fiscal incentives for homeownership (though not directly related to taxes) - there is a cutoff at a certain income level where if you stay below it you get a $10K credit towards closing costs. The tax code is full of things like that. reply reaperman 58 minutes agorootparent*EITC (nit) reply Der_Einzige 3 hours agorootparentprevThis guys idiotic belief about how taxes work is likely shared by at least 30% of all Americans and at least 50% of all Republicans. Please laugh at this peon for continuing to spread FUD and fake news about the US tax system! Edit: Flaggers are mad that I call out the truth about how FUD spreads. reply chollida1 4 hours agoprevSure, if you work at many hedge fund you'll make similar money, and likely more if you are good at the job and the fund has success. One other nice thing is that instead of restricted stock you get actual money. And generally the money will be locked up for less time than the 4 year vesting that these companies often follow. And on top of that you can usually invest your money fee free in your own fund to help juice your returns. The downside is that there is almost certainly going to be more stress on you and probably no where to hide if you want to just rest and vest. Your team and company will likely be pretty small and everyone will be very aware of what everyone's contribution to the company's success. reply packetlost 4 hours agoparentIt's pretty hard to get into these IME. I'm considered a top-performer at the deep tech company I currently work at, and interviewed at a hedge fund awhile back. The process was way less intense than what I would expect a big tech co. interview to be. I made it to the top three candidates, but didn't get it. In hindsight, the role wouldn't have been a good fit, it was \"higher up in the stack\" than I prefer to be, which is also what they said. I think you need to be a pretty unique individual to succeed in environments like that. Most people I know break down under intense pressure when shit hits the fan, even if they're wildly competent. I can only assume it's that much more intense when tons of money is at stake. reply diggan 4 hours agorootparentI'm not sure it's enough to experience something once and then assume it looks like that across the industry, it's very context-dependent. While I've done zero attempts to work at any hedge fund, I know plenty of people who both had success when wanting to join one (even though some of them were under-qualified) and people who failed (some who were over-qualified). reply packetlost 3 hours agorootparentOh most certainly. But that industry has a reputation for a reason, no? I would expect every place to have different culture, interview processes, etc. reply ldjkfkdsjnv 3 hours agorootparentprevAlot of getting hired into these roles depends on your upbringing/perceived social class by the people at the fund, the school you went to, how you present yourself, whether they want to work with you, how trustworthy you are, etc. The technical pieces are pretty far behind. reply chollida1 1 hour agorootparent> Alot of getting hired into these roles depends on your upbringing/perceived social class by the people at the fund, the school you went to, how you present yourself, whether they want to work with you, how trustworthy you are, etc. The technical pieces are pretty far behind. I'd disagree about your perceived upbringing mattering at all. I don't know of any fund that hires technical staff like that. I mean we're here to make money, if someone can do that, no one cares what their social upbringing is, I mean like at all. Now the school you went to matters, though it matters about as much as it would at a FAANG company. And as to \"how you present yourself, whether they want to work with you, how trustworthy you are, \" I would hope that all companies hire based on how someone presents themselves and would we want to work with them. Everyone should base hiring on how the candidate acts and presents themselves, that seems like an incredibly positive step, no? > The technical pieces are pretty far behind. Which specific fund are you referring to here. This seems very uninformed from my experience, but i'm willing to look at your evidence that hedge funds don't really value technical ability. reply ldjkfkdsjnv 42 minutes agorootparentThese are for very under the radar hedge funds that chip their programmers 500-2 Million for just writing basic software. I'm not talking about sweat shops like citadel. I'm talking about places that hire very rarely. The fund manager makes 25-150 million. Rest of the employees are in the low millions. Their employees are always a reflection of the firm, and to their outside investors. Its about more than technical ability. Jobs are never posted online, minialist linkedin profiles, no public footprint reply yellowstuff 1 hour agorootparentprevI've done a fair amount of technical hiring at hedge funds, my experience doesn't support this comment. There is some amount of credentialism/elitism in hedge fund hiring, but I see that more on the fundamental investment side, and much less for technical roles. There are a lot more hedge funds than FAANG companies, and different funds can have very different cultures. reply ldjkfkdsjnv 44 minutes agorootparentI'm talking about traditional small long short funds, private equity, and under the radar very elite hedge funds. Its different for quant shops like citadel or two sigma. Those treat you like a commidity programmer that they pay a 20% premium on over FAANG reply packetlost 2 hours agorootparentprevYup. There's a lot of credentialism and in-group stuff. I work with a ton of PhD physicists currently and it's one of those things that you can never quite forget in an environment like this as someone with \"only\" a bachelor's degree from a no-name college. reply triceratops 1 hour agoparentprev> And generally the money will be locked up for less time than the 4 year vesting that these companies often follow. Err...4-year vesting doesn't mean the money is literally locked away until you hit 4 years' tenure. A $1m stock grant vesting over 4 years might pay $250-350k every year, depending on the vesting schedule and stock performance. If a hedge fund pays $1m every 2 years then they're just paying more period. It's not the vesting schedule which causes the difference in comp. reply chollida1 1 hour agorootparent> Err...4-year vesting doesn't mean the money is literally locked away until you hit 4 years' tenure No one said it did:) My point was that a typical bonus is about 2/3's employees at the moment its granted and 1/3 held back for a year sometimes two, so you get money faster than a typical 4 year vesting schedule. Does that clear things up? reply triceratops 1 hour agorootparentSee my second sentence. That means the hedge fund flat out pays much more than FAANG. It's got nothing to do with vesting schedules. (BTW FAANG pays bonuses too. They're 100% the employee's when they're paid) reply boxfire 3 hours agoparentprevand probably no where to hide if you want to just rest and vest. \"Rest and vest\". What a luxury. Some of us are trying to tread water with an anvil chained to the waist. That's what I get for choosing to work for something I'm passionate about. What a world reply pxx 2 hours agorootparentif you refuse a higher paying job because you would like to work for something you're passionate about, you are explicitly valuing your passion at at least the delta between the job offers. reply humansareok1 46 minutes agoparentprev>One other nice thing is that instead of restricted stock you get actual money. Stock at FAANG is basically equivalent to cash. You can sell it as soon as you get it. reply dazh 4 hours agoparentprevYMMV but the one hedge fund offer I got required putting 50% of the bonus into the fund itself, which then vests out over 4 years. I was told this is industry standard, so it's analogous to the 4 year RSU vesting you'd get at FAANG. reply wocram 2 hours agorootparentUsually the deferred bonus is paid out to you if you leave, so it's a little bit different than an RSU grant. reply makestuff 4 hours agoparentprevThe other downside is that it is harder to break into because there are less roles. reply ska 5 hours agoprevTwo tried and tested methods, neither guaranteed of course. 1) Be better than the overwhelming majority of your peers, and be noticed for this. You are quite likely to find interesting opportunities come your way. I do mean overwhelming though - small fraction of a percent. 2) Specialize in something most people can’t or won’t do. This includes difficult, obscure, and unpleasant work. In both cases some of the biggest $ comes from independent consulting, once you’ve built up a network and a reputation for fixing a particular pain point corporations have (bill for value, of course). reply mitthrowaway2 3 hours agoparentEven if you're uniquely good at what you do, a lot of companies won't reward it. You do still have to look hard for those opportunities. reply asveikau 2 hours agorootparentA lot of people who are very good at what they do have a hard time selling it. A lot of people who are good at selling it are not actually accomplishing much. So there is often a gap between perception of high performers and actual high performers. reply vagab0nd 55 minutes agoparentprev3) Be first at something new. See the AI girlfriend project a while ago, which was pulling in 1m/mo. reply slashdave 2 hours agoparentprevI appreciate the sentiment, but I'll add that it is possible for difficult and obscure work to be quite pleasant, if you have the right skill set. reply angarg12 3 hours agoprevI just wrapped up a long job search process, where I got offers from a handful of companies. I currently work at FAANG, so I was looking for a pay bump. The only companies that could beat my current comp are other big tech and finance, but there were a couple of surprises along the way. For example, a Tier 3 (i.e. you never heard about them) company offered me 450k TC for a Staff level position. This compares to the 500k TC for a senior level position at FAANG. I also interviewed for a Startup that offered 650k TC for a Principal level, although the stocks were paper money. To your point, \"random\" companies can come close to FAANG salaries for high enough levels, but it's far from a given. In fact I'd say most random companies probably top in the 200k range. If you are interested just start every conversation with a recruiter asking for their comp range. I found these days almost all recruiters answer, and then you can decide if it is worth your time to go ahead. reply y-c-o-m-b 2 hours agoparentWhat in the world... I'm a FAANG employee - I think I'm around 18 years of experience now (I stopped counting) - with a great resume (or so I'm told), and I can't even get 100K jobs to respond to me let alone FAANG jobs. I'm talking about the initial step too; I've had zero interviews or screenings. reply angarg12 1 hour agorootparentThat sounds odd. With that much experience you should at least be getting offers in the 200k range easily. Are you cold applying or using referrals? I found these days CVs get screened by software that's very sensitive to keywords, and it's very easy to get auto-rejected. I documented my process of getting job offers here, you can check it out in case it helps https://www.teamblind.com/post/My-experience-getting-offers-... reply sbrother 1 hour agoparentprevAnother data point from a couple months ago -- I have about 15 years of experience including FAANG, went on the job market (fully remote only; I'm not in SF) and ended up with two offers. Senior SWE at a small but public tech company everyone has heard of, and Staff at a late-stage startup on track to IPO ~next year. Both offers were about $500k (the startup a little more but paper money); I took the former one. I've also spent large parts of my career as an independent consultant -- similar compensation most years. reply FireBeyond 1 hour agoparentprevSimilar. Tier 2, maybe, you've heard of them but definitely not in the FAANG range. They are public and about $8B revenue. Staff PM, around $250k base, 20% bonus, and $150k RSUs per year so pretty much on the money with yours, $450k TC. reply GlenTheMachine 4 hours agoprevCivil servants living in the DC or SF metro areas max out at about $195k not including benefits. The benefits include an actual pension, really good health insurance, and 1-1 salary matching into a 401(k)-like retirement fund (the TSP). Also, you're actually expected to work 40 hours a week and generally not more, and you're expected to take all your vacation. It takes a while to get to that level, or you need to get hired directly into a GS-15 equivalent position. But it's doable, especially at an agency that does technical work (eg NASA, NOAA, DOE, parts of the DOD, etc.) reply mettamage 2 hours agoparentThis is being a software engineer? reply jotux 1 hour agoparentprevJust going to jot down some thoughts about this, as it's something I'm very familiar with. GS-15s aren't particularly common (fedscope data says 3.1% of federal employees are GS-15) and overwhelmingly they are supervisory positions (or more commonly supervisor-of-supervisors). Even if you do find a non-supervisor GS-15, the federal government mostly contracts development, so it's exceptionally rare to find a federal engineering position that does something other than contract oversight (generically called acquisition). I'm not saying those positions don't exist, they are just rare, and typically reserved for promotions from within the agency or federal government. Also, since GS-15 is so senior in the federal government, even if the position is technical it will start to be political. Pay is pretty low for new grads and experienced engineers, but decent when you're in the ~3-8 year range of experience. Most agencies are hiring new grads at GS-7 or GS-9 (in SF that's $61k to $75k to start) which is a joke. So even if you can get on at a federal agency as a GS-14 or GS-15 level, you'll have an exceptionally hard time hiring new engineers. Pay at the high end, especially in HCOL areas, very quickly hits the congressional limit and makes it pointless to seek promotion (Look at the SF are GS-14 table: https://www.opm.gov/policy-data-oversight/pay-leave/salaries... ). You can theorhetically bonus up to 20-25%, and I've heard that some intelligence agencies just do that across the board, but most agencies are probably giving a 2-3% bonuses each year. This is a sad and interesting factoid: Long ago congress passed a law saying they need to track federal vs private sector pay and adjust the pay to federal employees to match that. Of course, there is an exception that allows the president to disband this requirement in case of national emergency. So now every year the president literally writes a letter declaring a national emergency and manually decides what the pay increase will be, which is always significantly lower than what is recommended. Here are the national emergency letters from 2023 and 2022, but they have been issued by every president for decades now: 2023: https://www.whitehouse.gov/briefing-room/presidential-action... 2022: https://www.whitehouse.gov/briefing-room/statements-releases... Benefits are good, but not amazing. Yes, there is a pension and a wide variety of health insurance options, but they're mediocre compared to what a lot of high-paying tech jobs offer. TSP is basically vanguard with lower fees and a 4% match. You can look at the healthcare options here: https://www.opm.gov/healthcare-insurance/healthcare/plan-inf... The 40-hour work week it completely true. If you're trying to work overtime, you'll actively be discouraged and told to stop it and go home. There is generally support for telework, but it's very political, and who knows if it will get killed in the future. Lots of agencies, especially the agencies that need a lot of technical work, will require you get a security clearance and that comes with it's own considerations. Drug use and foreign contacts are a big problem and makes recruiting even more difficult. It's possible to find a job at some agencies working on stuff that feels very rewarding and very important, but you will always constantly feel like the bureaucracy is constantly fighting you. reply zxexz 5 hours agoprevI’m aware of a couple people at health systems/hospitals that work maybe 400 hours/year making $500k+/year. Experts in legacy languages, specifically COBOL and MUMPS. For legacy billing and EMR/EHR and related systems code, respectfully. One guy gets flown out to the east coast from his ”chateau” in Utah every month or two; the only downside is he’s on-call pretty much all year round. I’ve heard banks have people like this on-site too but I’ve never met any. reply rqtwteye 4 hours agoparentI have heard/read of these people but people I know who work at hospitals doing MUMPS get paid very little. Same for COBOL programmers. There are some mythical people who make bank but most COBOL guys make way less than the typical JS guy. reply chaostheory 4 hours agorootparentAre they the original employees who started decades ago? If so, that’s no surprise. My aunt was in the same position years ago, despite my attempts to update her on modern salaries. Those awesome salaries tend to be paid to mercenaries and not the lifers. reply rqtwteye 1 hour agorootparentAnd only to a very small subset of the mercenaries. Most institutions that hire COBOL or MUMPS people can't fathom paying a lowly \"IT\" guy that much money. They prefer to hire some cheap offshore guys. reply epgui 5 hours agoparentprev> respectfully I think you mean \"respectively\" reply zxexz 5 hours agorootparentYou are right! Though I do have immense respect for those languages - not love, but respect. The way one respects a grouchy aging tiger. Not extinct, its ilk will be around forever, and I don’t want to be the one caring for it. reply seabass-labrax 5 hours agorootparentYour comment reminded me of a certain other tiger that one might want to be careful around..! https://www.gocomics.com/calvinandhobbes/1987/05/26 reply kayge 2 hours agoparentprevAs a 30-something \"full stack web developer\" who has also spent the last 3-4 years learning about IBM i (aka AS/400) servers and the CL and RPG programming languages... I have a glimmer of hope that I might move into a gig like that in my twilight years of employment :) reply ecshafer 1 hour agorootparentI highly doubt that those gigs will exist in the future, or will become even rare than they are. The big banks and financial companies are making a decent effort to get rid of those legacy systems (but are on decade + time scales). They also are weary to even hire new people in those stacks, and are increasingly relying on contractors through places like TCS that will put people through bootcamps. I know several places that have the policy of no new cobol code, only maintenance. These people with the ludicrous salaries, I assume are consultants, or have even more niche specific and rare skills beyond just COBOL and JCL and RPG or whatever, probably with specific, ancient implementations on ancient machines. reply barryrandall 5 hours agoparentprevMultiple airlines do this as well. reply dzonga 7 hours agoprevit'a catch 22 with some of those companies. 1. they don't hire often & usually have small teams 2. they make so much money, that they don't make noise about it. & you won't hear about them, unless one day you randomly run into one of their employees saying they're hiring on reddit / hn 3. they work in unsexy industries or high risk stuff e.g gambling 4. they're usually located in places you wouldn't expect reply ozim 6 hours agoparentHigh risk stuff pays well but I take my decent salary making crud web apps over it, along with work life balance and no stress. reply pavel_lishin 4 hours agorootparentSame. I'm 40, and I have a wife and child. I will absolutely give up those rock star salaries if it means having the time to spend with them, and not taking ulcer medication. reply amonith 5 hours agoparentprevWhy would gambling be high risk for employees? Unless you compare it to some cushy govt job it isn't really more \"layoff-prone\" than others. reply some_random 5 hours agorootparentThe risk is that if you ever want to leave the gambling industry you'll be the candidate who made their living swindling vulnerable people. reply amonith 3 hours agorootparentReally interesting how this is viewed in different countries. How far does this go? Does this include Gacha game devs? How about people working on loot boxes in gaming (like finance/marketing people who come up with new boxes)? Or NFTs or crypto in general? I wouldn't even thought to think about people coming from gambling companies differently. reply phpnode 3 hours agorootparentI live close to a city with a bunch of gambling companies and yeah, devs at those companies are definitely judged for their choices, similar to people who work for crypto / NFT companies. It has a reputational risk similar to working for porn companies (but less severe). reply malux85 4 hours agorootparentprevCapitalist societies where the alternative to employment is destitution can force people into working at huge online gambling companies due to the need for employment and their responsibility (kids, aging parents, etc) While I agree in an ideal world, individuals should all stand up against unethical employment options, I am also mature enough to know not to penalise someone who might be otherwise meritorious based on what could be situational. We should take a stand against predatory companies that attack the vulnerable, but this should be done at a government level, not an individual employee level, because it’s a much more efficient utilisation of resources for a large organisation with teeth to be subdued by another large organisation with teeth. I would hire someone who used to be in gambling (if they were a good fit culturally and technically of course) because that would take a skilled developer OUT of the gambling industry, implying that you shouldn’t hire someone who used to work in gambling is a sure way to make sure that predatory industry keeps enough developers to keep shirking the poor reply some_random 3 hours agorootparentPeople can absolutely be forced into working immoral jobs by muh capitalism, but some of you are way too enthusiastic about it. Someone doing blue team work for MGM after getting laid off in 2022 is one thing, a Staff Addiction Engineer looking to try a new industry in 2019 is another completely. Secondly, I agree that we should be hiring good people out of these industries, but there are a lot of people who just aren't going to be comfortable with that. reply malux85 2 hours agorootparent> but there are a lot of people who just aren't going to be comfortable with that. Actual LOL - who? I’ve worked at enormous orgs, Google, to medium sized orgs, to startups over my 20 year career, as a senior dev, engineering lead, CTO and now founder, I have hired and mentored hundreds, maybe close to 1000 engineers, hiring managers, project managers, and this has never ever been an issue, in one case we hired 20 engineers from Betsson, a huge (~400M/y) gambling company in Malta into a much larger organisation (non-gambling), and nobody from senior management, middle management or engineers ever even raised this point. I think you are uncomfortable with it, and are projecting that starry-eyed wishful and naive thinking onto others by saying “a lot of people aren’t going to be comfortable with that” I have 20 years experience working in 9 different countries, all with very different cultures, that says otherwise, nobody has ever mentioned it. reply epc 5 hours agorootparentprevLayoffs are not the primary risk of being an employee at a gambling company. reply PaulHoule 5 hours agorootparentI would be worried about organized crime. All sorts of sketchy things go on. The only sports event at my Uni where I wasn't able to photograph was college tennis because they have a problem with people past-posting on college tennis: https://en.wikipedia.org/wiki/Late_betting On one level I can understand the thrill of having some money riding on a game but I am baffled by the prop bets. Fixing a whole game is not so easy and rather risky, but influencing the silly little events some people bet on looks easy and hard to stop. It's particularly crazy that people bet on decisions by individuals and small groups as opposed to the outcome of a contest. reply amonith 5 hours agorootparentGambling itself is sketchy but legally hired employees especially developer-level have to be somehow protected from the decisions of the business, no? Unless you're worried about the crime itself (as in somebody might threaten you personally and send you to a hospital kind of thing) and not prosecution. reply totallymike 5 hours agorootparentprevYou’ve piqued my curiosity. What would you say is the primary risk of being an employee at a gambling company? reply epc 53 minutes agorootparentCrime, criminals. Mostly organized crime, but the occasional whack jobs who “just need an edge”. Doesn’t matter where you are in the organizational hierarchy, I’d argue the most at risk are the lowest level employees. reply camdenreslink 4 hours agorootparentprevPossibly reputation risk. Future employers may pass over applicants that worked at gambling companies if they consider it unsavory. reply jefc1111 5 hours agorootparentprevI have no expertise here but I'd guess changes in legislation torpedo'ing the business model is probably a big risk. reply amonith 5 hours agorootparentAll startups are probably way more risky. Law works against gambling. Entire market works against startups. reply amonith 5 hours agorootparentprevThen what is? I know the higher ups / owners have to deal with some straight up mafia shit or money laundering but a gambling company doesn't really look bad on a CV the way porn industry or even a government job (at least here in Poland) does. reply meiraleal 5 hours agorootparentprevYou might not easily find another job after reply amonith 5 hours agorootparentMaybe it's a country thing but here in Poland no one would care. Various forms of gambling are legal under supervision of authorities so legal companies exist around that. Unlike porn it wouldn't really be an embarrassing topic on an interview. Lots of interesting technical stuff to deal with in such a job. reply gamepsys 4 hours agorootparentI would be equally embarrassed if either of those were on my resume. I'd have similar levels of embarrassment with alcohol, firearms, cannabis or politics. You don't want your resume to suggest you are controversial in anyway. Maybe 80% of the people you run across will be cool with it, but it only takes one person in a company to veto your resume and there are usually 3+ people that look at a resume with veto power. It'll close doors for sure. reply amonith 3 hours agorootparentInteresting topic because I wouldn't want to work with those 20% who reject people based on n-th level relationship to anything controversial. I get not wanting to deal with people who run these things but employees? Those \"anti-political\" places typically mean a specific side (depending on your country that may anti-right in western countries and anti-left in eastern). When I worked in the office before 2019 we had all kinds of people there. It was kind of beautiful in a way. Straight up communists discussing stuff with conservatives without any heat. Isolation breeds extremism. It's true that it would close doors but man until the market takes that choice away from me I don't want to open those, and I say that as a guy who never worked with any of this even indirectly. And to be fair - not having experience with those things also closes doors, just different ones. reply meiraleal 3 hours agorootparentYou asked why that would be a problem, not that we agree. I like your point of view but I think in general society is less diverse than our personal experience. At least in my country, which is a mix of very liberal and very conservative (Brazil), most recruiters would discriminate against gambling and porn, not by the other tech people but from the other areas of the company. Less likely to happen in a full tech company. reply gamepsys 2 hours agorootparentThere's also the issue of money. If you are turned down for a job that would have paid 20% more because of working in a controversial industry that can have a big impact on your life. I'd prefer to be able to have civil discussions about politics with coworkers, but I don't care enough to sacrifice even 1% of my salary. This entire Ask HN is about income maximizing after all. I think getting into vice or anything controversial is a bad long term move, even if it includes a sizable temporary bump in compensation. reply janalsncm 1 hour agorootparent> If you are turned down for a job If you don’t take a job that would pay more you are also hurting your income opportunities. Turning a job down because of some hypothetical problem at some future date isn’t rational imo. If you genuinely believe certain industries would hurt your future job prospects, you need to quantify the risk. And I have a hard time believing the particular industry would matter at all to an employer. If you can build CRUD apps for a gambling company you can build CRUD apps for a pediatric cancer charity. reply Zambyte 5 hours agoparentprevWhat are some examples of unsexy industries that are high paying? reply amonith 5 hours agorootparentIronically probably porn is the most unsexy on a CV. reply PaulHoule 5 hours agorootparentFunny I worked for a place that, before I was there, was a major vendor of search engines for the porn industry despite the owner being a conservative mutual fund manager. (e.g. I had a poster for an ETN I liked on the wall of my office and it got taken down before he visited) I know things went bad and I had something in my contract that I'd be immediately dismissed if I was found to have pornography on my laptop or any other computer belonging to the company. reply Der_Einzige 3 hours agorootparentprevRight now, anyone who works in text-to-image or interacts with the website Civit.AI is effectively “working in porn” as an AI engineer. reply hintymad 36 minutes agoprev> For example, a Staff Engineer at an avg startup might get $250k base salary, in a HCOL area and maybe a 10-20% bonus. I think you are taking it in the wrong way. A startup has lower mean income but can have a much higher return in the future. Most startups are also cash strained, so you won't get as much cash for your income. I'd suggest you look into the expected return of your startup instead, and the take into at least the following situations: - Your peers. Do you get to build lasting relationship with truly amazing peers? With such peers, you won't regret it even if your startup fails as you will have great experience, let alone your career will likely take off in the near future. - The problem space. Does the problem you solve inspire you? - The potential of your startup. You get to to evaluate the potential continuously, as the situation of a startup changes constantly. The good news is that a startup is usually quite transparent. You get to know how your company performs easily and how the founders lead too. - Know yourself. How much risk can you take? How optimistic are you? And etc - Your expected income. Take your equity into consideration, but multiply the value with a probability of success in a given time frame. For example, I've never been a big risk taker, but I've made peace with it. Instead, I picked the top (or I thought so) late-stage startups in a category that I like, and I picked the startups that built the products that I used daily. Also, I focus on the startups that have potential scales to build my technical and product chops, as I'm interested in build general abstractions and systems, and am not patient or excited to deal with nitty-gritty details of a customer-facing product. I didn't become filthy rich of course, but my life has been exceedingly comfortable so far with me almost always focusing on building the things I like. reply padraicmahoney 14 hours agoprevYes. Some of the opportunities exist in odd corners of the software universe. To offer just one example, I'm aware of a software company that does nine figures in ARR making high performance computing software for very large financial institutions. There's basically no chance you've ever heard of them. They're very low key. The company hasn't grown headcount much in the last decade, and their employees don't really leave. But some of their people writing really low-level code are certainly making multiples of representative peers at FAANG. reply fleabagmange 14 hours agoparentI used to work at such a place. Contracts with places large as BAML and other places you e never heard of… Unless you’re the low level wizard you’re just going to make an industry standard salary, maybe a bump but nothing special. reply uh_uh 12 hours agorootparentWhat does a low level wizard need to know more specifically? reply sigmoid10 6 hours agorootparentAssembly would be a good start. But to really make use of that you need to learn the internal intricacies of processor architectures and operating systems. If you want to be employable, there's just a gigantic amount of seemingly boring details to learn before you can even start to build something interesting. With high level languages you can jump into the job market almost immediately, so the cost of entry vs. potential reward is better for most people. If you don't have fun when spending years of your life banging your head while digging down incredibly complex rabbit holes, it's probably not worth it. reply eschneider 5 hours agorootparentprevThere's nothing too terribly special about low level wizardry, other than you really _do_ need to understand how the hardware works, and how your language works. Oddball skills you really need to know (and can easily pick up if you have a mind to) include: a) Reading processor/part data books. These are your API docs. Most of your answers are in there if you care to look. b) Reading schematics. You don't need to be able to write/design hardware, but you need to be able to figure out what each CPU pin is wired to and how you're expected to access peripherals. Again, not hard to figure out if you put your mind to it. Very handy to know: How to use a multimeter/oscilloscope/JTAG programmer/logic analyzer. There are your debuggers. You don't need them often, but when you need them, you NEED them. reply gjadi 50 minutes agorootparentThis sounds like a typical embedded software engineer and I frequently see people around here complaining about the low salaries for these roles. What am I missing? reply esprehn 6 hours agorootparentprevMagic missile, garbage collection enchantment, server resurrection. reply zxexz 5 hours agorootparentMulticlassing is hard, but some get by as a making a divine pact and taking Eldritch Blast and Prestidigitation as cantrips. When all you have is sledgehammer, even smaller hammers are nails. reply araes 4 hours agorootparentprevServer rez on a low level wiz build? That's way high level thoughts and prayers to the Top 500 (also a cleric build) or lim. \"request\" to the Blackrock genie over in Budapest. Besides, 7th? Pff, call \"company of choice\" to fight for you, save or die server vileness!, and of course \"peon:cleanse everything? peon:what do mean everything? cleric:EVERYTHING.\" Frankly you probably want a PRC, and at those admin levels much, much save or die. Probably got so many server pokes per minute you're gonna be furiously holy wording your file system all day long. Team members? Rez? Please. You solo that. Low level wiz? \"Welcome to name brand, and lack of female desirability, here's your special hat...\" \"Why aren't the lights on?\" [1] \"Help, I lit my own hands on fire!\" [2] \"Ah, every threat one hits me!\" [3] \"Man they target me bad at conferences, how'd I draw DARPA as agro...\" [4] \"Holy s*t there's a lot of documentation to write!\" [5] High level wiz? \"Computation hates all magical ideas and their very implication!\" \"Curse your sudden but inevitable betrayal of my trust, our own company is secretly controlled by Orcus and seeks the death and slavery of all life on Earth\" [6] \"Join me in the bag within bag abyss together and we shall quest to Jubilex for glory and honor across the eternal battlefields of tomorrow!\" [7] ITGETSOEPIC::THEYWONTLETULEAVE::ALREADYATWARP::AUTOLADDEREJECTED::HOWDIDRAWULTRONASAGRO::MIDASTOUCHISHORRIFYING::AHHHHHHH [8] [1] https://www.d20srd.org/srd/spells/light.htm [2] https://www.d20srd.org/srd/spells/burningHands.htm [3] https://tvtropes.org/pmwiki/pmwiki.php/Main/SquishyWizard [4] https://tvtropes.org/pmwiki/pmwiki.php/Main/ShootTheMageFirs... [5] https://www.d20srd.org/srd/feats.htm#scribeScroll [6] https://tvtropes.org/pmwiki/pmwiki.php/Main/FaceHeelTurn [7] https://learn.microsoft.com/en-us/windows/uwp/cpp-and-winrt-... [8] https://www.qwantz.com/index.php?comic=1355 reply b20000 13 hours agorootparentprevBAML? reply 12thhandyman 12 hours agorootparentPossibly BofA Securities formerly Bank of America Merrill Lynch (BAML) https://en.m.wikipedia.org/wiki/BofA_Securities reply david_allison 11 hours agorootparentprevBank of America Merrill Lynch reply paullth 12 hours agorootparentprevNow BofA reply sa-code 13 hours agoparentprevDo you have a name for said company? reply tetris11 13 hours agorootparentNote that the two accounts responding in this chain were made only a few days ago, and that this entire thread might be some kind of phishing/advert reply henryteeare 13 hours agorootparentIt might be, though it also sounds like where I work, which is a company that has been mentioned a couple of times on HN (to a mixed reception). reply alfiedotwtf 13 hours agorootparentYep can confirm - if you know who this is, then it’s obvious who they are talking about. reply postexitus 6 hours agorootparentIs that the company which shall not be named? reply b20000 7 hours agorootparentprevwho is it? reply kachapopopow 11 hours agoparentprevAren't you just indirectly working at FAANG then? In most cases you end up being hired by a 'talent' agency that finds work for you at these major companies. reply AnimalMuppet 6 hours agorootparentIf I understood the top-level post correctly, no. They are very much not working for a FAANG, nor for a talent agency tha",
    "originSummary": [
      "The salary gap between FAANG (Facebook, Amazon, Apple, Netflix, Google) and non-FAANG companies is substantial, with FAANG engineers receiving significantly higher compensation packages.",
      "A Staff Engineer at a typical startup might earn a $250k base salary plus a 10-20% bonus, whereas a FAANG Staff Engineer could receive a similar base salary plus $1 million in stock over four years.",
      "The post questions if individuals outside of FAANG and big banks/high-frequency trading (HFT) firms earn comparable amounts, and seeks to identify who they are and their roles."
    ],
    "commentSummary": [
      "Earning FAANG-level salaries outside of FAANG companies is possible but rare, often involving niche or high-stress roles.",
      "High-paying alternatives include hedge funds, high-frequency trading firms, specialized consulting, niche software companies, legacy systems expertise, and entrepreneurship.",
      "These roles typically come with high stress, intense competition, or significant risk and effort, making them less common and straightforward compared to FAANG jobs."
    ],
    "points": 241,
    "commentCount": 442,
    "retryCount": 0,
    "time": 1721703798
  },
  {
    "id": 41046540,
    "title": "Llama 3.1",
    "originLink": "https://llama.meta.com/",
    "originBody": "Meet Llama 3.1 The open source AI model you can fine-tune, distill and deploy anywhere. Our latest instruction-tuned model is available in 8B, 70B and 405B versions. Start building Download models Try 405B on Meta AI Llama 3.1 models Documentation Hub 405B Flagship foundation model driving widest variety of use cases. Download 70B Highly performant, cost effective model that enables diverse use cases. Download 8B Light-weight, ultra-fast model you can run anywhere. Download Key capabilities Start building more advanced use cases, leveraging our resources. Get started on Github Tool use Upload a dataset and analyze it. Prompt model to plot graphs and fetch market data. Get Started in GitHub Multi-lingual agents Prompt: Translate the story of Hansel and Gretel into Spanish. Try 405B on Meta AI Complex reasoning Prompt: I have 3 shirts and 5 shorts, and 1 sun dress. I'm traveling for 10 days do I have enough for my vacation? Try 405B on Meta AI Coding assistants Prompt: Create a program that generates a perfect maze, using a recursive backtracking algorithm or a depth-first search algorithm, with customizable size and complexity. Try 405B on Meta AI Make Llama your own Using our open ecosystem, build faster with a selection of differentiated product offerings to support your use cases. See all services Inference Choose from real-time inference or batch inference services. Download model weights to further optimize cost per token. Fine-tune, Distill & Deploy Adapt for your application, improve with synthetic data and deploy on-prem or in the cloud. RAG & Tool Use Use Llama system components and extend the model using zero shot tool use and RAG to build agentic behaviors. Synthetic Data Generation Leverage 405B high quality data to improve specialized models for specific use cases. Quick start with partners Partner starter guides Features for 405B models Real-time inference Batch inference Fine-tuning Model evaluation RAG Continual pre-training Safety guardrails Synthetic data generation Distillation recipe Model evaluations As measured on over 150 benchmark datasets that span a wide range of languages and extensive human evaluations. Model card Research paper Llama 3.1 Performance Benchmarks Category Benchmark General MMLU (CoT) MMLU PRO (5-shot, CoT) IFEval Code HumanEval (0-shot) MBPP EvalPlus (base) (0-shot) Math GSM8K (8-shot, CoT) MATH (0-shot, CoT) Reasoning ARC Challenge (0-shot) GPQA (0-shot, CoT) Tool use API-Bank (0-shot) BFCL Gorilla Benchmark API Bench Nexus (0-shot) Multilingual Multilingual MGSM Llama 3.1 8B 73.0 48.3 80.4 72.6 72.8 84.5 51.9 83.4 32.8 82.6 76.1 8.2 38.5 68.9 Llama 3 8B - April 65.3 45.5 76.8 60.4 70.6 80.6 29.1 82.4 34.6 48.3 60.3 1.7 18.1 - Llama 3.1 70B 86.0 66.4 87.5 80.5 86.0 95.1 68.0 94.8 46.7 90.0 84.8 29.7 56.7 86.9 Llama 3 70B - April 80.9 63.4 82.9 81.7 82.5 93.0 51.0 94.4 39.5 85.1 83.0 14.7 47.8 - Llama 3.1 405B 88.6 73.3 88.6 89.0 88.6 96.8 73.8 96.9 51.1 92.3 88.5 35.3 58.7 91.6 Model Pricing Hosted Llama 3.1 API public pricing as of 12pm PST on 7/23/24. This table will be updated as more pricing becomes available. Model AWS Azure Databricks Fireworks.ai IBM Octo.ML Snowflake Together.AI 8B 70B 405B Input $0.30 $0.30 - $0.20 $0.60 $0.15 - $0.18 Output $0.60 $0.61 - $0.20 $0.60 $0.15 - $0.18 Input $2.65 $2.68 $1.00 $0.90 $1.80 $0.90 - $0.88 Output $3.50 $3.54 $3.00 $0.90 $1.80 $0.90 - $0.88 Input - $5.33 $10.00 $3.00 $35 $3.00 $15.00 $5.00 Output - $16.00 $30.00 $3.00 $35.00 $9.00 $15 $15 Latest Llama updates Open Source AI is the path forward Learn more Introducing Llama 3.1: Our most capable models to date Learn more The Llama 3 Herd of Models Learn more Stay up-to-date Our latest updates delivered to your inbox Subscribe to our newsletter to keep up with the latest Llama updates, releases and more. Sign up",
    "commentLink": "https://news.ycombinator.com/item?id=41046540",
    "commentBody": "Llama 3.1 (meta.com)230 points by luiscosio 4 hours agohidepastfavorite128 comments dang 1 hour agoRelated ongoing thread: Open source AI is the path forward - https://news.ycombinator.com/item?id=41046773 - July 2024 (278 comments) lelag 3 hours agoprevThe 405b model is actually competitive against closed source frontier models. Quick comparison with GPT-4o: +----------------+-------+-------+MetricGPT-4o| Llama|| 3.1|| 405B+----------------+-------+-------+MMLU88.788.6| GPQA53.651.1| MATH76.673.8| HumanEval90.289.0| MGSM90.591.6+----------------+-------+-------+ reply bamboozled 19 minutes agoparentThis nodel is not “open source”, free to use maybe. reply cchance 3 hours agoparentprevSuper cool, though sadly 405b will be outside most personal usage without cloud providers which sorta defeats the purpose of opensource to some extent atleast sadly, because .. nvidia's rampup of consumer VRAM is glacial reply aabhay 3 hours agorootparentZoom out a bit. There’s a massive feeder ecosystem around llama. You’ll see many startups take this on and help drive down inference costs for everyone and create competitive pressure that will improve the state of the art. reply gkk 3 hours agorootparentprevIf you think of open source as a protocol through which the ecosystem of companies loosely collaborate, then it's a big deal. E.g. Groq can work on inference without a complicated negotiations with Meta. Ditto for Huggingface, and smaller startups. I agree with you on open source in the original, home tinkerer sense. reply loudmax 3 hours agorootparentprevI agree that 405B isn't practical for home users, but I disagree that it defeats the purpose of open source. If you're building a business on inference it can be valuable to run an open model on hardware that you control, without the need to worry that OpenAI or Anthropic or whoever will make drastic changes to the model performance or pricing. Also, it allows the possibility of fine-tuning the model to your requirements. Meta believes it's in their interest to promote these businesses. I'd think of the 405B model as the equivalent to a big rig tractor trailer. It's not for home use. But also check out the benchmark improvements for the 70B and 8B models. reply duchenne 3 hours agorootparentprevMost SMBs would be able to run it. This is already a huge win for decentralized AI. reply stuckinhell 2 hours agorootparentprev100% reddit is full of people trying to solder more vram reply gaogao 1 hour agorootparentI've been wondering if you could just attach a chunk of vram over NVLink, since that's very roughly what FSDP is doing here anyways. reply bick_nyers 1 minute agorootparentThe best NVLINK you can reasonably purchase is for the 3090, which is capped somewhere around 100 Gbit/s. This is too slow. The 3090 has about 1 Tbit/s memory bandwidth, and the 4090 is even faster, and the 5090 will be even faster. PCIE 5.0 x16 is 500 Gbit/s if I'm not mistaken, so using RAM is more viable an alternative in this case. kingsleyopara 3 hours agorootparentprevYou might be able to get away with running a heavily quantizied 405b model using CPU inference at a blistering fast token every 5 seconds on a 7950x. reply wuschel 3 hours agorootparentOK, I am curious now: What kind of hardware would I need to run such a model for a couple of users with decent performance? Where could I get a mapping of token / time vs hardware? reply danieldk 2 minutes agorootparentYou can run the 4-bit GPTQ/AWQ quantized Llama 405B somewhat reasonably on 4x H100 or A100. You will be somewhat limited in how many tokens you can have in flight between requests and you cannot create CUDA graphs for larger batch sizes. You can run 405B well on 8x H100 and A100, either with the mixed BFloat16/FP8 checkpoint that Meta provided or GPTQ/AWQ-quantized models. Note though that the A100 does not have native support for FP8, but FP8 quantized weights can be used through the GPTQ-Marlin FP8 kernel. Here are some TGI 405B benchmarks that I did with the different quantized models: https://x.com/danieldekok/status/1815814357298577718 angoragoats 2 hours agorootparentprevUnsure if anyone has specific hardware benchmarks for the 405b model yet, since it's so new, but elsewhere in this thread I outlined a build that'd probably be capable of running a quantized version of Llama 3.1 405b for roughly $10k. The $10k figure is likely roughly the minimum amount of money/hardware that you'd need to run the model at acceptable speeds, as anything less requires you to compromise heavily on GPU cores (e.g. Tesla P40s also have 24GB of VRAM, for half the price or less, but are much slower than 3090s), or run on the CPU entirely, which I don't think will be viable for this model even with gobs of RAM and CPU cores, just due to its sheer size. reply bick_nyers 8 minutes agorootparentEnergy costs are an important factor here too. While Quadro cards are much more expensive upfront (higher $/VRAM), they are cheaper over time (lower Watts/Token). Offsetting the energy expense of a 3090/4090/5090 build via solar complicates this calculation but generally speaking can be a \"reasonable\" way of justifying this much hardware running in a homelab. I would be curious to see relative failure rates over time of consumer vs Quadro cards as well. reply monkeydust 2 hours agorootparentprevGreat for Groq whos already hosting it but at what cost I guess. reply ajhai 47 minutes agoprevYou can already run these models locally with Ollama (ollama run llama3.1:latest) along with at places like huggingface, groq etc. If you want a playground to test this model locally or want to quickly build some applications with it, you can try LLMStack (https://github.com/trypromptly/LLMStack). I wrote last week about how to configure and use Ollama with LLMStack at https://docs.trypromptly.com/guides/using-llama3-with-ollama. Disclaimer: I'm the maintainer of LLMStack reply foundval 3 hours agoprevYou can chat with these new models at ultra-low latency at groq.com. 8B and 70B API access is available at console.groq.com. 405B API access for select customers only – GA and 3rd party speed benchmarks soon. If you want to learn more, there is a writeup at https://wow.groq.com/now-available-on-groq-the-largest-and-m.... (disclaimer, I am a Groq employee) reply geepytee 2 hours agoparentWe also added Llama 3.1 405B to our VSCode copilot extension for anyone to try coding with it. Free trial gets you 50 messages, no credit card required - https://double.bot (disclaimer, I am the co-founder) reply sagz 3 hours agoparentprev405B is already being served on WhatsApp! https://ibb.co/kQ2tKX5 reply Workaccount2 2 hours agorootparentHow do you get that option? reply quotemstr 2 hours agoparentprevGroq's TSP architecture is one of the weirder and more wonderful ISAs I've seen lately. The choice of SRAM in fascinating. Are you guys planning on publishing anything about how you bridged the gap between your order-hundreds-megabytes SRAM TSP main memory and multi-TB model sizes? reply foundval 2 hours agorootparentThere is a lot out here. I gave a seminar about the overall approach recently, abstract: https://shorturl.at/E7TcA, recording: https://shorturl.at/zBcoL. This two-part AMA has a lot more detail if you're already familiar with what we do: https://www.youtube.com/watch?v=UztfweS-7MU https://www.youtube.com/watch?v=GOGuSJe2C6U reply quotemstr 2 hours agorootparentThanks! reply meetpateltech 3 hours agoprevOpen Source AI Is the Path Forward - Mark Zuckerberg https://about.fb.com/news/2024/07/open-source-ai-is-the-path... reply ninjin 3 hours agoparentSo are they actually making the models open now or are they staying the course with \"kind of open\" as they have done for LLaMA 1, 2, and 3 [1]? [1]: https://opensource.org/blog/metas-llama-2-license-is-not-ope... As I have stated time and again, it is perfectly fine for them to slap on whatever license they see fit as it is their work. But it would be nice if they used appropriate terms so as not to disrupt the discourse further than they have already done. I have written several walls of text why I as a researcher find Facebook's behaviour problematic so I will fall back on an old link [2] this time rather than writing it all over again. [2]: https://news.ycombinator.com/item?id=38427832 reply Zambyte 3 hours agorootparent> it is perfectly fine for them to slap on whatever license they see fit as it is their work. Is it? Has there been a ruling on the enforceability of the license they attach to their models yet? Just because you say what you release can only be used for certain things doesn't actually mean what you say means anything. reply gkfasdfasdf 3 hours agoparentprevMeta the new \"Open\" AI? reply netsec_burn 3 hours agoprevToday appears to be the day you can run an LLM that is competitive with GPT-4o at home with the right hardware. Incredible for progress and advancement of the technology. Statement from Mark: https://about.fb.com/news/2024/07/open-source-ai-is-the-path... reply lolinder 3 hours agoparent> at home with the right hardware Where the right hardware is 10x4090s even at 4 bits quantization. I'm hoping we'll see these models get smaller, but the GPT-4-competitive one isn't really accessible for home use yet. Still amazing that it's available at all, of course! reply petercooper 3 hours agorootparentIt's hardly cheap starting at about $10k of hardware, but another potential option appears to be using Exo to spread the model across a few MBPs or Mac Studios: https://x.com/exolabs_/status/1814913116704288870 reply dunefox 3 hours agoparentprevIt's not really competitive though, is it? I tested it and 4o is just better. reply primaprashant 3 hours agoprevI have found Claude 3.5 Sonnet really good for coding tasks along with the artifacts feature and seems like it's still the king on the coding benchmarks reply cubefox 37 minutes agoparentI have found it to be better than GPT-4o at math too, despite the latter being better at several math benchmarks. reply Workaccount2 2 hours agoprev@dang why was this removed/filtered from the front page? reply dado3212 2 hours agoprev> We use synthetic data generation to produce the vast majority of our SFT examples, iterating multiple times to produce higher and higher quality synthetic data across all capabilities. Additionally, we invest in multiple data processing techniques to filter this synthetic data to the highest quality. This enables us to scale the amount of fine-tuning data across capabilities. [0] Have other major models explicitly communicated that they're trained on synthetic data? [0]. https://ai.meta.com/blog/meta-llama-3-1/ reply tommy_axle 1 hour agoparentIt's in theWhy are (some) Europeans surprised when they are not included in tech product débuts? We had a brief, abnormal, and special moment in time after the crypto wars ended in the mid-2000s where software products were truly global, and the internet was more or less unregulated and completely open (at least in most of the world). Sadly it seems that this era has come to a close, and people have not yet updated their understanding of the world to account for that fact. People are also not great at thinking through the second order effects of the policies they advocate for (e.g. the GDPR), and are often surprised by the results. reply cubefox 42 minutes agorootparentprev> Why are (some) Europeans surprised when they are not included in tech product débuts? Why do you think he is surprised? I think very few are surprised. reply Daunk 2 hours agorootparentprevMost things do dèbute in the EU, unless the product or company behind it doesn't value your privacy. Meta does not value your privacy. reply lolinder 2 hours agorootparentPrivacy was the first thing that the EU did that started this trend of companies slowing their EU releases because of GDPR. Now there's the Digital Markets Act and the AI Act that both have caused companies to slow their releases to the EU. Each new large regulation adds another category of company to the list of those who choose not to participate. Sure, you can always label them as companies who don't value principle X, but at some point it stops being the fault of the companies and you have to start looking at whether there are too many enormous regulations slowing down tech releases. reply sva_ 3 hours agoparentprevYou can load the page using a VPN and then turn off the VPN and the page will still work. reply sunaookami 3 hours agorootparentYou can't sign in though, that worked before. Seems like they also check from which country your Facbook/Instagram account is. You can't create images without an account sadly. reply WinstonSmith84 1 hour agorootparentI changed my Facebook country (to Canada), using also a VPN to Canada, but that didn't help. That used to work before somehow. reply lawlessone 1 hour agorootparentprevSomeone will torrent it soon enough i'm sure. reply lolinder 3 hours agoparentprevCompetition is a funny thing—it doesn't just apply to companies competing for customers, it also applies to governments competing for companies to make products available to their citizens. Turns out that if you make compliance with your laws onerous enough they can actually just choose to opt out of your country altogether, or at a minimum delay release in your country until they can check all your boxes. The only solution is a worldwide government that can impose laws in all countries at once, but that's unlikely to happen any time soon. reply Teknomancer 3 hours agorootparentBe careful what you wish for. A Gibsonesque global Turing Police is a sure sign of Dystopia. reply primaprashant 4 hours agoprevThe resources for link to model card[1], research paper, and Prompt Guard Tutorial[2] on the page doesn't exist yet [1]: https://github.com/meta-llama/llama-models/blob/main/models/... [2]: https://github.com/meta-llama/llama-recipes/blob/main/recipe... reply kingsleyopara 3 hours agoprevThe biggest win here has to be the context length increase to 128k from 8k tokens. Till now my understanding is there hasn't been any open models anywhere close to that. reply HanClinto 1 hour agoparentIt is notable, but it's not alone. Mistral NeMo just released last week with a 128k context window: https://news.ycombinator.com/item?id=40996058 reply kingsleyopara 12 minutes agorootparentThanks! Not sure how I missed that :) reply HanClinto 9 minutes agorootparentIt's easy to miss things. Trying to keep up with the latest in AI news is like drinking from the firehose -- it's never-ending. reply chown 2 hours agoprevWow! The benchmarks are truly impressive, showing significant improvements across almost all categories. It's fascinating to see how rapidly this field is evolving. If someone had told me last year that Meta would be leading the charge in open-source models, I probably wouldn't have believed them. Yet here we are, witnessing Meta's substantial contributions to AI research and democratization. On a related note, for those interested in experimenting with large language models locally, I've been working on an app called Msty [1]. It allows you to run models like this with just one click and features a clean, functional interface. Just added support for both 8B and 70B. Still in development, but I'd appreciate any feedback. [1]: https://msty.app reply downvotetruth 55 minutes agoparentTried using msty today and it refused to open and demanded an upgrade from 0.9 - remotely breaking a local app that had been working is unacceptable behavior. Good luck retaining users. reply unraveller 2 hours agoprevWhat are the substantial changes from 3.0 to 3.1 (70B) in terms of training approach? They don't seem to say how the training data differed just that both were 15T. I gather 3.0 was just a preview run and 3.1 was distilled down from the 405B somehow. reply thntk 1 hour agoparentCorrect me if I'm wrong, my impression is that 3.1 is a better fine-tuned variant of base 3.0 with extensive use of synthetic data. reply denz88 3 hours agoprevI'm glad to see the nice incremental gains on the benchmarks for the 8B and 70B models as well. reply loudmax 3 hours agoparentSome of those benchmarks show quite significant gains. Going from Llama-3 to Llama-3.1, MMLU scores for 8B are up from 65.3 to 73.0, and 70B are up from 80.9 to 86.0. These scores should always be taken with a grain of salt, but this is encouraging. 405B is hopelessly out of reach for running in a homelab without spending thousands of dollars. For most people wanting to try out the 405B model, the best option is to rent compute from a datacenter. Looking forward to seeing what it can accomplish. reply sfblah 3 hours agoprevIs there an actual open-source community around this in the spirit of other ones where people outside meta can somehow \"contribute\" to it? If I wanted to \"work on\" this somehow, what would I do? reply sangnoir 3 hours agoparentThere are a bunch of downstream fine-tuned and/or quantized models where people collaborate and share their recipes. In terms of contributing to Llama itself - I suspect Meta wants (or needs) code contributions at this time. reply sfblah 2 minutes agorootparentCan you give me a tip of where to look? I'm interested in participating. reply ChrisArchitect 2 hours agoprevRelated: Open Source AI Is the Path Forward https://about.fb.com/news/2024/07/open-source-ai-is-the-path... (https://news.ycombinator.com/item?id=41046773) reply Atreiden 3 hours agoprevIs there a way to run this in AWS? Seems like the biggest GPU node they have is the p5.48xlarge @ 640GB (8xH100s). Routing between multiple nodes would be too slow unless there's an InfiniBand fabric you can leverage. Interested to know if anyone else is exploring this. reply woodson 3 hours agoparentYou can run multi-node with tensor parallel plus pipeline parallel inference, e.g. with vLLM (https://docs.vllm.ai/en/latest/serving/distributed_serving.h...). reply Tiberium 3 hours agoparentprevAWS has a separate service for running LLMs called Amazon Bedrock, it shouldn't take long for them to add 3.1 since they have 3 and 2 already. reply tpm 3 hours agoparentprevfp8 quantization should work if that's acceptable? reply TheAceOfHearts 3 hours agoprevDoes anyone know why they haven't released any 30B-ish param models? I was expecting that to happen with this release and have been disappointed once more. They also skipped doing a 30B-ish param model for llama2 despite claiming to have trained one. reply michaelt 3 hours agoparentI suspect 30B models are in a weird spot, too big for widespread home use, too small for cutting edge performance. For home users 7B models (which can fit on an 8GB GPU) and 13B models (which can fit on a 16GB GPU) are in far more demand. If you're a researcher, you want a 70B model to get the best performance, and so your benchmarks are comparable to everyone else. reply drdaeman 1 hour agorootparentI thought home use is whatever fits in 24GB (a single 3090 GPU, which is pretty affordable), not 8 or 16. 30B models fit. reply prvc 3 hours agoparentprevWhy should they? reply TheAceOfHearts 19 minutes agorootparentUnless I'm misremembering, they announced it at one point. It's just giving people more options. reply nickpsecurity 3 hours agoparentprevMaybe they think more people will just use quantized versions of 70B. reply TechDebtDevin 4 hours agoprevNice, someone donate me a few 4090s :( reply lawlessone 4 hours agoparentmaybe someone will figure out some ways to prune/ quantize it a huge amount ;-; edit: If the AI bubble pops we will be swimming in GPUs... but no new models. reply foxhop 4 hours agoparentprevYour going to need a lot more than a few, 800G VRAM needed reply AaronFriel 4 hours agorootparentIf previous quantization results hold up, fp8 will have nearly identical performance while using 405GiB for weights, but the KV cache size will still be significant. Too bad, too, I don't think my PC will fit 20 4090s (480GiB). reply knicholes 3 hours agorootparentI've got a motherboard that will support 8! reply Zambyte 3 hours agorootparent40,320 4090s?? What witchcraft is this?! :D reply lolinder 4 hours agorootparentprevQuantized to 4 bits you'll only need ~200GB! 5 4090s should cover it. reply downvotetruth 1 hour agorootparentIf an implementation had Heterogeneous memory management implemented, then 192 GB RAM DDR5 + GPU VRAM would seem to be close. reply woodson 3 hours agorootparentprevI wonder if AutoAWQ works out of the box, given no architectural changes (?). That would be most straightforward together with vLLM for serving. reply angoragoats 4 hours agorootparentprevYou'll probably need 9 or more. 4090s have 24GB each. reply lolinder 3 hours agorootparentOops, I read 48 somewhere but that's wrong. Thanks. reply htrp 3 hours agorootparenta6ks however reply pat2man 3 hours agorootparentprevTwo 128gb Mac studios networked via thunderbolt 4? reply Teknomancer 3 hours agorootparentThis is actually a promising endeavor. Id love to see someone try that. reply angoragoats 2 hours agorootparentThere's already at least one project that attempts this: https://github.com/exo-explore/exo reply whalesalad 3 hours agorootparentprevfollow the trail of tears to my credit card reply TechDebtDevin 4 hours agorootparentprevOof. reply beeboobaa3 3 hours agorootparentprevhow is this even useful? no one can run it. reply jermaustin1 3 hours agorootparentYou don't use the 405B parameter model at home. I have a lot of luck with 8B and 13B models on a single 3090. You can quantize them down (is that the term) which lowers precision and memory use, but still very usable... most of the time. If you are running a commercial service that uses AI, you buy a few dozen A100s, spend a half million, and you are good for a while. If you are running a commercial inferencing service, you spend tens of millions or get a cloud sponsor. reply beeboobaa3 3 hours agorootparentI can't expect all my users to have 3090s and if we're talking about spending millions there are better things to invest in than a stack of GPUs that will be obsolete in a year or three. reply jermaustin1 2 hours agorootparentNo, but if you are thinking about edge compute for LLMs, you quantize. Models are getting more efficient, and there are plenty of SLMs and smaller LLMs (like phi-2 or phi-3) that are plenty capable even on a tiny arm device like the current range of RPi \"clones\". I have done experiments with 7B Llama3 Q8 models on a M3 MBP. They run faster than I can read, and only occasionally fall off the rails. 3B Phi-3 mini is almost instantaneous in simple responses on my MBP. When I want longer context windows, I use a hosted service somewhere else, but if I only need 8000 tokens (99% of the time that is MORE than I need), any of my computers from the last 3 years are working just fine for it. reply loudmax 3 hours agorootparentprevIf you want to run the 405B model without spending thousands of dollars on dedicated hardware, you rent compute from a datacenter. Meta lists AWS, Google and Microsoft among others as cloud partners. But also check out the 8B and 70B Llama-3.1 models which show improved benchmarks over the Llama-3 models released in April. reply glitchc 4 hours agorootparentprevChrist!! reply albert_e 3 hours agoprevthis \"Model Card\" github link on [https://llama.meta.com/docs/overview/] seems broken? https://github.com/meta-llama/llama-models/blob/main/models/... reply daft_pink 3 hours agoprevWhat kind of machine do I need to run 405B local? reply 93po 3 hours agoparentaccording to another comment, ~10x 4090 video cards. reply Teknomancer 3 hours agorootparentThat was the punchline of a joke. reply daft_pink 3 hours agorootparentprevthanks. hoping the Nvidia 50 series offers some more VRAM. reply monkmartinez 3 hours agoparentprevYou can't. Sorry. Unless... You have a couple hundred $k sitting around collecting dust... then all you need is a DGX or HGX level of vRAM, the power to run it, the power to keep it cool, and place for it to sit. reply causal 2 hours agorootparentYou could run a 4bit quant for about $10k I'm guessing. 10x3090s would do. reply angoragoats 2 hours agorootparentprevYou can build a machine that will run the 405b model for much, much less, if you're willing to accept the following caveats: * You'll be running a Q5(ish) quantized model, not the full model * You're OK with buying used hardware * You have two separate 120v circuits available to plug it into (I assume you're in the US), or alternatively a single 240v dryer/oven/RV-style plug. The build would look something like (approximate secondary market prices in parentheses): * Asrock ROMED8-2T motherboard ($700) * A used Epyc Rome CPU ($300-$1000 depending on how many cores you want) * 256GB of DDR4, 8x 32GB modules ($550) * nvme boot drive ($100) * Ten RTX 3090 cards ($700 each, $7000 total) * Two 1500 watt power supplies. One will power the mobo and four GPUs, and the other will power the remaining six GPUs ($500 total) * An open frame case, the kind made for crypto miners ($100?) * PCIe splitters, cables, screws, fans, other misc parts ($500) Total is about $10k, give or take. You'll be limiting the GPUs (using `nvidia-smi` or similar) to run at 200-225W each, which drastically reduces their top-end power draw for a minimal drop in performance. Plug each power supply into a different AC circuit, or use a dual 120V adapter with a 240V outlet to effectively accomplish the same thing. When actively running inference you'll likely be pulling ~2500-2800W from the wall, but at idle, the whole system should use about a tenth of that. It will heat up the room it's in, especially if you use it frequently, but since it's in an open frame case there are lots of options for cooling. I realize that this setup is still out of the reach of the \"average Joe\" but for a dedicated (high-end) hobbyist or someone who wants to build a business, this is a surprisingly reasonable cost. Edit: the other cool thing is that if you use fast DDR4 and populate all 8 RAM slots as I recommend above, the memory bandwidth of this system is competitive with that of Apple silicon -- 204.8GB/sec, with DDR4-3200. Combined with a 32+ core Epyc, you could experiment with running many models completely on the CPU, though Lllama 405b will probably still be excruciatingly slow. reply yinser 3 hours agoprevThe race to the bottom for pricing continues. reply Vagantem 3 hours agoprevAs someone who just started generating AI landing pages for Dropory, this is music to my ears reply casper14 4 hours agoprevDamn 405b params reply ThrowawayTestr 4 hours agoprevAre there any other models with free unlimited use like chatgpt? reply phyrex 3 hours agoparentmeta.ai reply Zambyte 3 hours agoparentprevmistral.ai reply diimdeep 3 hours agoprevThis 405B seriously need quantization solution like 1.625 bpw ternary packing for BitNet b1.58 https://github.com/ggerganov/llama.cpp/pull/8151 reply hubraumhugo 4 hours agoprev [–] I wrote about this when llama-3 came out, and this launch confirms it: Meta's goal from the start was to target OpenAI and the other proprietary model players with a \"scorched earth\" approach by releasing powerful open models to disrupt the competitive landscape. Meta can likely outspend any other AI lab on compute and talent: - OpenAI makes an estimated revenue of $2B and is likely unprofitable. Meta generated a revenue of $134B and profits of $39B in 2023. - Meta's compute resources likely outrank OpenAI by now. - Open source likely attracts better talent and researchers. - One possible outcome could be the acquisition of OpenAI by Microsoft to catch up with Meta. The big winners of this: devs and AI product startups reply adam_arthur 3 hours agoparentIt's pretty clear the base model is a race to the bottom on pricing. There is no defensible moat unless a player truly develops some secret sauce on training. As of now seems that the most meaningful techniques are already widely known and understood. The money will be made on compute and on applications of the base model (that are sufficiently novel/differentiated). Investors will lose big on OpenAI and competitors (outside of greater fool approach) reply lolinder 3 hours agorootparent> There is no defensible moat unless a player truly develops some secret sauce on training. This is why Altman has gone all out pushing for regulation and playing up safety concerns while simultaneously pushing out the people in his company that actually deeply worry about safety. Altman doesn't care about safety, he just wants governments to build him a moat that doesn't naturally exist. reply changoplatanero 3 hours agoparentprev> Open source likely attracts better talent and researchers I work at OpenAI and used to work at meta. Almost every person from meta that I know has asked me for a referral to OpenAI. I don’t know anyone who left OpenAI to go to meta. reply tintor 2 hours agorootparentWhat % of them were from FAIR vs non-FAIR? reply kkielhofner 1 hour agorootparentSample size of one but I know someone who went from FAIR to OpenAI. reply beeboobaa3 3 hours agorootparentprevSo they just pay better? reply lossolo 2 hours agorootparentprevWhen was that? reply foolswisdom 3 hours agoparentprevIt could definitely be seen as part of that strategy, but do you mind elaborating why you think \"this launch confirms it\"? reply jeffchao 3 hours agoparentprevThis is very impressive, though an adjacent question — does anyone know roughly how much time and compute cost it takes to train something like the 405B? I would imagine with all the compute Meta has that the moat is incredibly large in terms of being able to train multiple 405B-level morels and compete. reply sva_ 3 hours agorootparent30.84M H100 compute-hours, according to the model card https://github.com/meta-llama/llama-models/blob/main/models/... reply Escapado 3 hours agorootparentInterestingly that’s less energy than the mass energy equivalent of one gram of matter or roughly 5 seconds worth of the worlds average energy consumption (according to wolfram alpha). Still an absolute insane amount of energy, as in about 5 million dollars at household electricity rates. Absolutely wild how much compute goes into this. reply moffkalast 3 hours agoparentprev [–] https://gwern.net/complement Classic strategy. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Llama 3.1 is an open-source AI model available in three versions: 8B, 70B, and 405B, catering to different performance and cost needs.",
      "It supports advanced use cases, including coding assistants, multi-lingual agents, and complex reasoning, with capabilities for real-time and batch inference, fine-tuning, and synthetic data generation.",
      "The model has been evaluated on over 150 datasets, showing high performance in general, code, math, reasoning, tool use, and multilingual benchmarks."
    ],
    "commentSummary": [
      "Llama 3.1, an open-source AI model by Meta, is showing competitive performance against closed-source models like GPT-4.",
      "The 405B model demonstrates significant benchmark improvements but is impractical for home use without cloud support, highlighting challenges in running large models locally.",
      "Meta's release of powerful open models aims to disrupt the competitive landscape, sparking interest in hardware requirements, quantization solutions, and the broader implications for AI development and accessibility."
    ],
    "points": 231,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1721746072
  },
  {
    "id": 41042294,
    "title": "Button Stealer",
    "originLink": "https://anatolyzenkov.com/stolen-buttons/button-stealer",
    "originBody": "Button Stealer A Chrome extension that “steals” a button from every website you open. Button Stealer works automatically. Do your usual everyday online stuff and watch the collection of your stolen buttons grow. It's fun, useless, and free! Install Button Stealer Button Stealer works locally and doesn't send data anywhere, so your data remains private. Install Button Stealer",
    "commentLink": "https://news.ycombinator.com/item?id=41042294",
    "commentBody": "Button Stealer (anatolyzenkov.com)213 points by kickofline 15 hours agohidepastfavorite69 comments purple-leafy 11 hours agoIssue with this “benign” extension is that it will be using “host_permissions”: “” In its manifest means it can basically do anything on any webpage you visit, scrape data etc. As an extension developer, no thanks. “Fun” pointless extensions like this that have no real utility, but funnily enough require broad permissions, are dangerous reply elaus 11 hours agoparent> [...] but funnily enough require broad permissions I don't think there is a way to implement this without said permission. You can always check out the code from GitHub and install the extension locally to avoid any malicious changes in the future. reply skybrian 10 hours agorootparentAnd that’s why it shouldn’t be in the Chrome app store at all. As a hobbyist developer, having that kind of access in other people’s browsers is not something I want, and I’m suspicious of developers who do seem to want it. It’s like “hey, I wrote a fun game that requires root access.” At least limit it to people who know what Github is. reply firtoz 8 hours agorootparent> hey, I wrote a fun game that requires root access So... Just like AAA game studios, eh? reply MOARDONGZPLZ 7 hours agorootparentNo. They are large and presumably have some sort of trust, and can lose the trust of people if they do particularly shady things. This may not bear itself out in practice of course. But a game studio has something to lose, whereas hobbyist developer 73683 asking for root permissions for no real gain to you has nothing to lose from any number of things like scraping sites you visit or using your browser as a tor exit node or any number of things. reply sandworm101 5 hours agorootparent>> No. They are large and presumably have some sort of trust, and can lose the trust of people if they do particularly shady things. Sony? Microsoft? EA? Apple? Exactly which giant megacorporation is beyond shady things? https://en.wikipedia.org/wiki/Sony_BMG_copy_protection_rootk... reply gryn 7 hours agorootparentprevyeah, I'm sure Genshin impact's creators went out of business when their Kernel access anti-cheat was hacked by ransomware or more recently the hacks mid live broadcasted tournaments (don't remember which game, I think it was apex). I mean that's what kids, teenagers, and young adults and non technical people in general are known for: their prudence and good technical decision making. lets not talk about the other risk vector that Tencent, a chinese company is the one buying most of these game studios that have Kernel access (not exclusively). reply dspillett 5 hours agorootparentIt doesn't even need to be a hack, or a malicious new owner taking over a game or other software package that has such access. The original company could be malicious/stupid/both. See https://en.wikipedia.org/wiki/Sony_BMG_copy_protection_rootk... for the most famous example of “both”. reply MOARDONGZPLZ 7 hours agorootparentprevI don’t know what that is. reply dspillett 5 hours agorootparentWhich is the problem with games having kernel access for anti-cheat and . You don't know what they are doing in there. You don't really know who they are. Even if you do, corporate machinations might mean who has access to the facility toon your PC could change at any moment without your knowledge. Most end-users are blissfully unaware of the potential consequences of these level of access (Games having kernel access, and browser extensions having all-sites/all-contexts access). reply techjamie 4 hours agorootparentCan you imagine if one of the big anti-cheats got hit with a supply chain attack? That would be devastating. reply oneeyedpigeon 4 hours agorootparentprevRight, but Google is surely the one at fault here. There should be absolutely no reason that this extension can \"change all my data on all websites\", whatever the hell that actually means. reply tetromino_ 2 hours agorootparentNot \"change all my data on all websites\" but \"read the content of all websites I visit\". Because an extension that finds all button elements on all websites you visit, must necessarily start by reading the content of all websites you visit. reply oneeyedpigeon 1 hour agorootparentYes, I think that one's acceptable in this case. It's the \"change all my data\" that is problematic. reply skybrian 1 hour agorootparentprevI think both the developer and Google have some control over what appears in the Chrome store. reply Retr0id 2 hours agorootparentprevI was under the vague impression that Manifest V3 was supposed to prevent this sort of thing. But looking at the extension, it is using MV3. Maybe it really was just about weakening ad blockers. reply beeboobaa3 3 hours agorootparentprevWhat are you even talking about? Every piece of desktop software you have ever ran has more permissions than a browser extension. Is your stance that hobbyist developers should not be allowed to develop desktop software or CLI tools? The entire software development ecosystem would collapse in an instant. Or are you just not familiar with Windows & Macs (lack of a) permission system? reply skybrian 1 hour agorootparentThe lack of sandboxing in desktop applications is bad, but you aren’t going to be writing code to read every web page a user visits by accident, and that’s what some browser extensions do on purpose. They’re inherently working with more sensitive data. So that’s worse in certain ways. (And they are more sandboxed in other ways.) reply beeboobaa3 47 minutes agorootparent> but you aren’t going to be writing code to read every web page a user visits by accident No, instead you're just reading all files on the filesystem, including the browser's cookie store or whatever. The data you are, or can be, handling is just as, if not more, sensitive since it's literally a superset of what the browser has access to. > The lack of sandboxing in desktop applications is bad Some sandboxing would be nice, but the Google/Apple approach of needing to beg the vendor for every little permission isn't the way to go, either. I'd rather have software that can actually do things as opposed to only having useless sandboxed \"apps\". reply skybrian 17 minutes agorootparentMy Mac sometimes prompts me to see if a Mac application should have access to certain directories, such as “Downloads,” so I’m not sure that’s entirely true anymore? But in any case I think this is missing a distinction between what software developers can install in “developer mode” versus stuff that’s in the store for non-technical people to use. Apps in app stores see widespread use by people who barely know what a computer is, so I think there should be hoops you need to jump through to get distribution to the masses, at least for certain types of apps. And those apps aren’t useless, they do important but security-sensitive things like banking, things us developers need to do too sometimes. It’s a different world than hacking around on your Raspberry Pi or an old phone, and I think it should be different. Treating these situations the same muddies the issues. reply purple-leafy 9 hours agorootparentprevThere is a way: 1) Extension could use the “activeTab” permission (would require user to click the extension once when inside the current tab to activate the extension, then the extension will run for any url they visit reply lofaszvanitt 7 hours agorootparentAnd that is so annoying noone would do it. reply jimvdv 9 hours agorootparentprevIf chrome permissions made sense a user could choose to activate the extension when they visit a site. Also the extension could have no network access and have read-only access to the DOM to name a few improvements. reply m3kw9 5 hours agorootparentprevhard to know if github code is the code it is being installed unless you build it from github. 1/10000 people check+install like this reply resonious 9 hours agoparentprevIt seems crazy that extensions don't have a permission for making network requests. Getting permission to access the DOM on all pages I visit is fine if there's no way to exfiltrate! reply robryk 8 hours agorootparentYou can always exfiltrate by inserting stuff into the page's DOM that will do the exfil from the page's context. reply aembleton 8 hours agorootparentShould have a seperate permission to modify the DOM. This extension only needs to read the DOM. reply teruakohatu 3 hours agorootparentYes, a network access and DOM write permission should be one and the same. I think the reason it isn't done is because there are so many ways to leak data over a network. If the extension can trigger a DNS lookup somehow, it can exfiltrate data. Android used to have a network permission but Google removed it. reply beeboobaa3 42 minutes agorootparent> Android used to have a network permission but Google removed it. That's because google is in the ads business and wants apps to always be able to exfiltrate data to google (google analytics, google ads, etc) & display ads without needing additional permissions. Having a network permission means there is an incentive for apps to not have the network permission which means they can't load ads. And Google wants you to look at their ads. reply pastage 36 minutes agorootparentprevI block all external resources on my pages, but sure it works well in most places! It think the default policy should be block on most pages. reply gtsteve 5 hours agorootparentprevI would hope that high value target sites such as banks would implement CSPs to prevent that or make it more difficult though. reply pigeonhole123 5 hours agorootparentYou can save the data and exfiltrate through a site without CSP reply emadda 6 hours agorootparentprevOr also a permission to disable automatic updates to reduce the issue of “popular extension sold to malware corp”. reply ivanjermakov 7 hours agoparentprev1. Wait for an extension to become popular 2. Sell it to a company with malicious intentions 3. Get ad/spy/malware in your browser reply geek_at 7 hours agorootparentthat happened to me. I installed a plugin that would parse all pages for email addresses and store them for later reference. A few months later i started to see strange ads on pages that shouldn't have ads. reply rc_mob 4 hours agoparentprevHow is OP supposed to build the extension without doing this? reply d--b 9 hours agoparentprevYes they sell for quite a bit, and the buyer may not have the same idea of “fun” than the original guy. reply Refusing23 11 hours agoparentprevjust like 'Grammarly' which is basically just a keylogger reply MrSS 10 hours agorootparentGrammarly has to be able to connect back to their online service while the button addon could be implemented in a way that it can read every website but not send antyhing anywere (in theory, the addon could of course simulate a form and send data out through that or somehow). But yeah i tested grammarly for 5 minutes and found it crazy. there has to be a better way getting both worlds :| reply dspillett 5 hours agorootparentIn DayJob we've had to block (actually block, because people didn't listen to being asked not to use it and similar tools) Grammarly because it sending text that could potentially include client data off to their servers for checking would have given us a nasty fail should a client request or conduct an audit. As an alternative there is LanguageTool which you can install locally. We have it running on a small VM that people can configure their installs to talk to, and block the public service end-point (as sending to that would be a big no-no for us for the same reason as Grammarly). It doesn't have all the features of Grammarly so isn't a complete drop-in replacement, but the self-hosted version works as well as the free features of Grammarly. reply Suppafly 3 hours agorootparent>As an alternative there is LanguageTool which you can install locally. We have it running on a small VM that people can configure their installs to talk to, and block the public service end-point I'm surprised that Grammarly hasn't come up with a local service like that, I bet they have a ton of enterprise users that would appreciate it. reply bargainbin 10 hours agorootparentprevLocal software of course! But good luck getting funding for a product that doesn’t phone home every 5 seconds and present an opportunity to plague the user with ads “that they want to see” reply vstollen 9 hours agorootparentI haven’t used it myself, but the LanguageTool browser extension might allow users to use a self-hosted or locally running instance. reply vstollen 9 hours agoparentprevAre (updates to) extensions from the Chrome and Firefox store usually vetted before publication? I‘ve heard that Firefox will only run signed extensions. Would you trust this process? reply zinekeller 8 hours agorootparent> Are (updates to) extensions from the Chrome and Firefox store usually vetted before publication? Mozilla does not manually review most extensions (only extensions which Mozilla recommended are manually reviewed: https://support.mozilla.org/en-US/kb/add-on-badges). Chrome's policy is extensions are \"reviewed periodically for compliance\", but is unclear on how frequent is this periodic review (https://developer.chrome.com/docs/webstore/review-process). reply 6510 9 hours agoparentprevThe permissions need to be more specific some how. I think the correct approach is to have the option to have a function isolated from the rest of the code. Then pay a trusted party to review the functionality of the function. In this case said function may only 1) access the html on the website, 2) find the button and 3) return only that what makes the button. Then the permission prompt, written by the trusted party, can be something accurate like: This extension wants to copy buttons from websites. I'm calling it DEWISOTT computing: does exactly what it says on the tin You can go wild update your extension 1000 times per day without touching the function. reply dotancohen 4 hours agorootparent> written by the trusted party This is the weak, and expensive, link. reply 6510 2 hours agorootparentThe programming notary should be expensive per line. If a function gets certified and a decent description it can be published for other developers to further scrutinize and use the same. With user.script or greasemonkey scripts it is kinda expected to read the script before use. Short scripts are easier to check. Funny example https://userscripts-mirror.org/scripts/show/179526 If the potentially dubious part can be isolated the notary, the publisher, the other developer and the user can easily review it. It seems much better than the current installing black boxes? reply skybrian 12 hours agoprevThis is the app version of a phishing email. Give us access to everything on every website you visit, just for some eye candy. reply mavamaarten 11 hours agoparentBonzi buddy vibes reply koito17 13 hours agoprevIs there a particular reason this uses Chrome-specific APIs instead of the standard WebExtensions API? I have considered experimenting with web extensions, but wondering what the practical limitations of the standard API are compared to the browser-specific APIs. reply sn0wleppard 10 hours agoparentThere's some difference but a lot of overlap in the basic functionality - Firefox is compatible with all the chrome.* API calls I use in my own extension reply purple-leafy 11 hours agoparentprevchrome doesn’t support web extension API reply creesch 10 hours agorootparentTechnically correct, but it is a bit more complex. The original web extension API is based on the chrome extension API. So most (there are some annoying exceptions at times) of the chrome extension API calls also work with very little adjustment on firefox. It becomes even easier when you use mozilla's polyfill library https://github.com/mozilla/webextension-polyfill Then you can just target the promise based webextension syntax and as long as you still stick to the calls also available in chrome your extension works with very little effort in both browsers. Safari is a different story which basically amounts to Apple being Apple and sort of supporting webextensions but in such a roundabout way that it is barely worth it for the majority of extension devs. reply kmoser 1 hour agoprevDoes it store the HTML/CSS for creating the buttons so you can easily repurpose them (which would be quite useful), or are they stored as images (which would be fun but less useful)? If the latter, how difficult are they to extract from the page that shows them all? reply graypegg 4 hours agoprevI love the idea but theaccess is a bit scary. This could be recreated in a bookmarklet ideally, though it would require saving the button html snippets into a file that you'd have to make downloadable with some Blob weirdness. reply kickofline 15 hours agoprevgithub: https://github.com/anatolyzenkov/button-stealer reply neontomo 3 hours agoparentnow add a leaderboard for most collected... btw i had a look at the code and it seems benign. no clue if there's a way to verify the same code is in the chrome extension store. reply stuffoverflow 29 minutes agorootparentOn windows the location of chrome's extensions is \"AppData\\Local\\Google\\Chrome\\User Data\\Default\\Extensions\". You can read the source code of all of your installed extensions there. This requires you to install the extension first. It is also possible to download the crx file of any extension from the chrome web store and just unzip it to inspect the source, though i'm not sure how to do it with the official chrome. Ungoogled chromium downloads the crx file if you press \"add to chrome\" and then cancel. reply elitepleb 13 hours agoprevreminds me of https://adnauseam.io/ 's clicked ad view https://adnauseam.io/img/adnauseam_vault.png reply erremerre 11 hours agoparentI love watching mine, and love watching the cost to advertisers. Modern problems require modern solutions! reply Hamuko 12 hours agoprevI'd be worried about installing these sorts of extensions in case someone decides to offer the developer a lucrative amount of money to buy it and then uses it for less-than-fun purposes. Not sure if they'd need additional permissions for it, but at least the current content script is ran against \"https://*/\\*\" already. reply odo1242 3 hours agoprevIs there a Firefox version? reply sweca 13 hours agoprevThis sounds like a great way to find inspiration for UI UX designs reply rgbrgb 3 hours agoprevcool! i want this for safari please. is that an easy port? reply jer0me 13 hours agoprev“It's fun, useless, and free!” reply peanut_worm 7 hours agoprevcute idea but im not installing this malware lol reply impure 13 hours agoprevICH WILL MEINE 5€! reply ape4 5 hours agoprev [–] In addition to all the security concerns mentioned, you don't really need it. You can google or ask a chatBot to make you custom button. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Button Stealer is a Chrome extension that automatically removes a button from every website you visit.",
      "The extension is designed to be fun and is free to use, with no practical utility.",
      "It ensures user data privacy by operating locally on the user's device."
    ],
    "commentSummary": [
      "Button Stealer, a Chrome extension, raises security concerns due to its broad permissions, which could allow it to scrape data from any webpage.",
      "Users suggest reviewing the code on GitHub and installing it locally to avoid potential malicious updates, while others argue such risky extensions should not be in the Chrome store.",
      "The discussion emphasizes the need for more specific permissions and improved security practices for browser extensions."
    ],
    "points": 213,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1721705873
  },
  {
    "id": 41041537,
    "title": "Re: Do people IRL know you have a blog?",
    "originLink": "https://lars-christian.com/re-do-people-irl-know-you-have-a-blog/",
    "originBody": "Re: Do people IRL know you have a blog? This post is a response to Do people IRL know you have a blog? A short while before I came across bacardi55’s call to conversation, I asked my wife if she wanted to see something cool. She said yeah. I showed her the Reading section, and explained that I was constructing the functionality to track my reading on my own website. Her response was a blank stare. Nonplussed, she asked me why anyone would care about which books I was reading. I told her that it didn’t matter, because I do. And to do it on my own website — as opposed to Goodreads or some other such walled garden of a corporate behemoth — is a small act of defiance. It is to stand up to the tech overlords and cry “No!” It is one small step in the direction of preserving the independent web. By the time I finished my speech, she had moved on with her life. I think she was in the next room, folding clothes. That’s the usual response whenever I try to talk about my website — this website — in real life (IRL). In the nearly twenty years that’s passed since I first published this website, the times I’ve brought it up in conversation I’m usually confronted with why. People around me didn’t get why I wanted a personal website in 2005. With the rise of social media platforms, they certainly didn’t get it a decade later. And my feeling is that, as yet another decade has passed — with popularity of big tech platforms on the vane — they still don’t get it. I’ve always thought of this as a kind of freedom. A licence to be myself. To explore my interests unapologetically and without having to explain myself. If nobody cares what I’m doing here, it means I’m free to do whatever I want. To say what I’m thinking about whatever I’m thinking about. At the very top of my to-do list for my personal website, I have highlighted the following quote: No one else has anything invested in this. No one cares if you do it, or don’t do it. To the world this pursuit is just your cute little hobby. My personal website has never been popular. No post I’ve written has ever gone viral. At most, an online acquaintance or two has responded in some way to anything I’ve published. Bar the odd exception, nobody I’ve met in real life has ever commented on something I wrote online. It is a form of anonymity through obscurity and disinterest, which I’ve grown to appreciate. This has freed me from my website becoming a worry stone, as highlighted by Robin Rendle in his post I am a poem I am not software. He writes: There’s a constant tug of war between wanting to be professional and wanting to be cool online. Sometimes those things overlap and sometimes they don’t. And sometimes the folks who have the opportunities to make a weirdo website are doing so because they’re not financially dependent on their website selling a service or landing a new gig. Their economic livelihood isn’t at risk if someone is turned off by the strange fonts or experimental navigation on their website. My personal website has never been a source of income. It never will be. To the extent that it is boring, corporate-looking, it is just because I am a boring, perhaps slightly corporate guy. But through browsing the small and independent web, I pick up new inspiration all the time. Perhaps that will eventually see me become a more daring, more edgy person. I wouldn’t hold my breath. But if it does, you can be sure that my website will reflect it. Either way, I doubt the people around me IRL will take much notice. 22 July 2024 In Miscellaneous #Blogging, #IndieWeb, #PersonalWebsites, #smallweb Responses Respond to this post with a webmention or a comment using the form below. Want to discuss in private? Email me. 7 responses to “Re: Do people IRL know you have a blog?” Ross Hartshorn 23 July 2024 My blog is primarily a way for me to clarify my thoughts on a topic by writing about it. Once that is done, the amount of effort required to post those thoughts is small, so why not do it? http://www.rosshartshorn.net, by the way. Reply Lars-Christian 23 July 2024 I wholeheartedly agree. And your setup rocks! But I wish you’d set up an RSS feed for https://www.rosshartshorn.net/stuffrossthinksabout/. Reply Copperfield 23 July 2024 Are you concerned at all that though irl people don’t know or care that you have a website there are systems that certainly do and are training eg. ai models on it? I love the idea of having my own online space, but the thought of bots exploiting that in a way that’s totally opaque to me is unsettling and puts me off the idea. How do you think about this? Reply Lars-Christian 23 July 2024 I can’t wrap my head around LLMs and what they are doing to society. It’s constantly on my mind, and I’ve tried to write and clarify my thoughts on the matter at least ten times this year. Without success. One thing’s for sure, though, I wish there was a foolproof way for anyone who creates content of any kind to opt out of feeding the LLM machines. On a personal level, though, it’s not something I worry about. My “contributions” feel so small and insignificant that, if any LLM were to use treat them as a signal (as opposed to the noise I think it to be, in the grand scheme of things), all the less power to that model. It’s not exactly a fully formed opinion, though, I’ll gladly admit. So I’m perfectly open to the fact that I’ll end up regretting those words at some point. Reply Tim 23 July 2024 I don’t know anyone else IRL who has the same interests as me so there would be no point. It’s not related to my job or any of my main 3 hobbies. The whole reason to use the internet is to reach other people. Reply Jeffrey Woods 23 July 2024 I haven’t posted jack shit on my website, and I’m in the privileged position of having a Raspberry Pi hosting it within my house. Loved this article; you’re really pressuring me to post something now Reply Dani Pecos 23 July 2024 Couldn’t agree more with your post! I too started my own website long ago (~20 years) and I’ve pivoted between different options, from plain HTML, WordPress and now, for a while already, back to static HTML generated by Hugo. Something I did a few years ago that I completely recommend to anyone having a website as hobby is to drop any kind of analytics you may have and just write, don’t worry about how many people read it, just do it for yourself. Happy blogging! Reply Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment.",
    "commentLink": "https://news.ycombinator.com/item?id=41041537",
    "commentBody": "Re: Do people IRL know you have a blog? (lars-christian.com)175 points by 8organicbits 17 hours agohidepastfavorite121 comments prmoustache 7 hours agoI love personal websites but I don't really like blogs. I prefer when people can rework/refine some of their pages instead of publishing new blog posts related to previous ones when revisiting a topic. And as a writer it allows me to write whenever I feel like to. With a blog you kind of feel guilty if you don't publish on a regular basis, and end up abandoning it altogether too easily if you cannot sustain a rhythm. A web page doesn't force you into a rhythm. A blog might be useful for historians in the future, when chronology might be useful but as a publisher and casual reader I find it lazy and unwelcoming for the reader. I do like however personnal pages that have a small log mentionning the updates to which I can subscribe to. reply alpinisme 7 hours agoparentAgreed completely. From a reader perspective, blogs are also often not friendly to new visitors. How do I find the best entry point? Which entries are fluff and which are deep dives? The best you can usually hope for is some tagging mechanism, but blogs generally expend little to no effort thinking about architecture, discovery, browsability, or the reader’s progression through the site. Don’t get me wrong - that’s a lot to ask of a casual effort. But I do wish we had a genre/format for sharing one’s thoughts online that did encourage reflection and iteration on that level. reply Suppafly 2 hours agorootparent>From a reader perspective, blogs are also often not friendly to new visitors. How do I find the best entry point? Which entries are fluff and which are deep dives? A lot of that is a flaw in the blogging software and the failure of the author to realize they need to customize the design to make it accessible to readers. reply wannabebarista 7 hours agorootparentprevOne thing I've tried to combat the chaos of blog structures is to include links to other posts in series as a header (when it makes sense). The biggest hurdle of moving away from a blog to a static format is that blog posts are timestamped and there's no real expectation that they're maintained. With static pages, however, I try to keep them up-to-date. reply maroonblazer 6 hours agoparentprevCan you share some examples of what you're describing. I enjoy blogs, have one of my own that I haven't added to in quite a while. I don't feel under any pressure, for many of the reasons stated in TFA, to update it regularly. But I'm now interested in exploring this alternative you describe. reply prmoustache 5 hours agorootparentAny personal homepage of the pre-blog era is a good example. See here the personal homepage of the late Sheldown Brown, famous for his technical articles on bicycle maintenance, that is still maintained by his spouse Harriett Fell[1] who still add content regularly. I still visit once in a while: https://www.sheldonbrown.com/org/personal-pages.html It may look like a big mess and it is ugly by modern standards[2] but it is a real pleasure to visit with tons of articles classed by topics. I find it more interesting to visit than a blog. Here some humor pages Harriett Fell added in recent year to make fun of Zwift or OpenAI: https://www.sheldonbrown.com/time-travel.html https://www.sheldonbrown.com/openai.html [1] which also happen to have her own personal page: https://www.khoury.northeastern.edu/home/fell/ [2] it doesn't have to be, this one simply was built in the late 90's reply jackstraw14 4 hours agorootparentprevI think https://gwern.net/ is a pretty good example of a \"personal site\" that in another universe could be a regular blog. https://gwern.net/changelog is the only log part of it. reply rrishi 7 hours agoparentprevyou might be interested in the concept of digital gardens: https://maggieappleton.com/garden reply rchaud 6 hours agoparentprev100%. Reading blogs feels 'messy' exactly for this reason. It's not 2008 anymore when blog posts had a vibrant comment section, so changing the content of the post felt dishonest. Maybe it's modern social media that makes people treat their blogs like a tweet - uneditable, frozen in time. reply Chris2048 2 hours agoparentprevlike a wiki-blog? But then, when is a refinement to something a new work (like a part 2), a small refinement to the original, or a noteworthy modification e.g. such that subscribers would be notified of the change? You don't need dated entries, just a topic name, a few subheadings for different parts of related text, and a history page; but presenting update notifications can still be a problem? maybe different levels of notification, or subscribing to specific articles(subheading)/topics? reply langsoul-com 15 hours agoprevThe thing about irl blogs is how, if it's tech related, then it kinda follows you. So there's a pressure to keep things corporate to not scare away job opportunities. There's a reason why LinkedIn reads like garbage, and even if it's obvious, people neither point it out or stop. reply prmoustache 7 hours agoparentI am using a pseudonym on my blog and I tend to use a different pseudonym on all websites/services/medias I use. I have often pondered wether I should just publish under my own name or not. The thing is once you do that, there is no going back and I have never felt ready for that. I know it puts off some people who mistake that for anonymity and think this is ruining the web, that people can't behave if they aren't talking under their own name, I respect that. I am not looking for anonymity. I don't pretend to hide from authorities. However pseudonimity allows me to express myself while not engaging my current or future employers directly. reply _heimdall 6 hours agorootparent> I know it puts off some people who mistake that for anonymity and think this is ruining the web Its interesting that we got to a point where anyone would consider anonymity to be ruining the web. When I was growing up in the 90s and early 00s anonymity online was the whole point. Everything online was behind usernames, and usernames weren't expected to be connected to your real identity at all. Thank you Facebook, you did a wonderdul job of convincing an entire generation that they should put their real name, profile photo, hometown, etc online and tied to everything they say online. reply reaperducer 6 hours agorootparentWhen I was growing up in the 90s and early 00s anonymity online was the whole point. Everything online was behind usernames, and usernames weren't expected to be connected to your real identity at all. You consider that the norm because that's what you grew up with. Before then, people would sign everything on the internet (and its predecessor networks) with their real names, work/home addresses, phone numbers, and more. That's why when you create a new user on a *nix box, it asks for that information. reply ebiester 1 hour agorootparentInside the US, we all were given historical precedents for anonymous speech. While the Declaration of Independence was signed, most of the leaflets arguing for independence were anonymous. Or, consider Publius as the author of the Federalist papers, a pseudonym for Hamilton, Madison, and John Jay. reply _heimdall 5 hours agorootparentprevSure, that is the timeframe I grew up with so there is bias there. It also happens to be when the internet was starting be used more broadly. Correct me if I'm wrong here because it is before my time, but prior to WWW wasn't the common use for the internet government and academic research, and technical communications of those actually building the internet protocols? Unless I'm wrong there, I just wouldn't really lump that in when comparing how the average person uses the internet. reply defrost 5 hours agorootparentMainly academic and military and spouses and family, yes. FWiW shitposting on UseNet started very early on - binary porn coming from German airbases was a thing and while many official things were sent from @MyRealName there were plenty of handles used and an0n postings. reply beeboobaa3 6 hours agorootparentprevIt's what the internet grew up with, actually. > That's why when you create a new user on a *nix box, it asks for that information. That's your local account, not information that is visible to anyone outside of your network. reply Suppafly 2 hours agorootparent>That's your local account, not information that is visible to anyone outside of your network. There are a lot of unix tools to ask networks to query that information and they'll happily do it for other unix systems they connect to. When I was in college in the last 90s, I'd often lookup phone numbers and such of students at other universities using command line unix utilities. reply beeboobaa3 39 minutes agorootparentLike what? I'm not familiar with any daemons that expose this info on a network interface. Maybe identd? AFAIK that can expose the username, none of the other fields, but is typically set to an alternate identifier. Or do you mean shared mainframes with many users on the same system? That'd still be local to the system, you just happened to have access to that system. Are you sure you're not just thinking of ldap/directory lookups? reply defrost 6 hours agorootparentprev.. and when Usenet appeared people commented in tech newsgroups with their realnames and a good number of them also posted in alt newsgroups under nom de guerres. That was the early 1980s as I recall them. reply Suppafly 2 hours agorootparentprevPseudonyms are nice too because they are, for all practical purposes, identities, just not real life identities. You can build a brand around a pseudonym, even if you can't necessarily convert that to real life cachet. That is somewhat different from pure anonymity, because if you act like a dick eventually no one will take you seriously and you'll lose something of value, even if that value is just the goodwill of your readers. reply Zak 4 hours agorootparentprev> I know it puts off some people who mistake that for anonymity and think this is ruining the web, that people can't behave if they aren't talking under their own name I haven't run across this sentiment with regard to personal websites or blogs. Who is saying that? I do see it for forums, comment sections, and social media. Those often bring together people who did not consciously seek to interact with each other, and do tend to reward antisocial behavior with attention. reply manuelmoreale 13 hours agoparentprevIt’s funny how different lives can be. I’ve never been part of the corporate world, been self employed basically my whole working life and my blog is not even a thought when it comes to work. I’d honestly be super happy if someone I interact with for work asked me about something I wrote because I write about things I care about and I’m always happy to chat with people. reply aadhavans 2 hours agorootparent> I’d honestly be super happy if someone I interact with for work asked me about something I wrote As a student (who also has no corporate experience), I share this feeling. The few times I've been asked by other students about my blog, I've always been excited (perhaps a little too much) to share my thoughts and opinions in a conservation. reply manuelmoreale 2 hours agorootparentWell ping me via email and send me your blog and I’ll be more than happy to ask you about it because I love reading blogs. My address is on my profile here on HN reply pflenker 14 hours agoparentprevAgreed. The minute I learned one of my directs was reading my small blog posting to it became much, much harder. reply fullspectrumdev 8 hours agoparentprevI’ve had this fear a few times - and when my last workplace started getting a bit weird (layoffs) I largely stopped publishing much of anything. I experimented with publishing under an alternate identity a bit, but it never stuck. Now I’m working through my backlog of drafts, dumping a load of stuff ASAP, and then trying to commit myself to some form of publishing schedule to force myself to “catch up” on the project backlog. reply bdw5204 14 hours agoparentprevI'm personally less averse to calling that stuff out these days because I found that I wasn't exactly getting job opportunities in the current market by playing it safe. Since being quiet and uncontroversial doesn't even work, what's the point? I think people are making a mistake by not saying what they actually think. The only reason I don't have a blog yet is inertia/laziness. It'll happen eventually. reply manuelmoreale 13 hours agorootparentDo it! You’ll have a bunch of people happy to read it: https://manuelmoreale.com/i-ll-read-it reply tpoacher 8 hours agoparentprevalso why gemlogs have taken off massively, even if gemini itself hasn't reply Brajeshwar 14 hours agoprevDuring the early days of Blogging (I meant the early 2000s), when we met up in person during conferences, we talked a lot about blog posts, etc. I once let one of my cousins tag along as I spoke at a conference organized by Macromedia. I did the usual thing with others there, talking about blogs. When we returned home, my cousin said, “You are kind of a deal. People know you by your blog - brajeshwar.com.” I’ve been lucky to have been recognized by a few people in the wild (IRL) who walked up to me and asked, “You are brajeshwar.com?” These days, I just write for myself. Fast forward 15+ years, my daughter somehow decided to search the Internet for me, and she said, “You are like a ChatGPT-powered Discord bot, answering questions on an antique Reddit forum.” https://x.com/brajeshwar/status/1630852614303924224 reply Carrok 10 hours agoparent> Macromedia Reading this term hit my brain with such a sudden wave of nostalgia. It really was a different time. reply sudohackthenews 13 hours agoparentprevI don’t know whether to be impressed or scared that the first thing that came to her mind was a llm powered bot haha reply blitzar 12 hours agoparentprev> when we met up in person during conferences, we talked a lot about blog posts Now people talk about high engagement twitter threads, linked-in posts or just film each other filming each other for their \"content\". reply knorker 11 hours agoparentprevActually producing value is so last millennium. Ok boomer, you add value. We don't do that here. Unless you make TikToks endlessly aspiring to be a societal net loss kardashian, why are you even breathing? Writing text? Can't the computer just do that, somehow? reply FartinMowler 10 hours agorootparent(having a flashback to the bile blog) reply brynet 14 hours agoprevI showed my Mom my personal website a few years ago, and her only comment was it needed more pictures, so I added a picture of a window plant to exactly one page. I haven't showed it to anyone else IRL. https://brynet.ca/article-x395.html reply blitzar 12 hours agoparentThe subject in the picture is the bottle of wine, not the plant. reply brynet 11 hours agorootparentNon-alcoholic. I don't drink. :-) reply blitzar 9 hours agorootparentBottle of grape juice reply dewey 11 hours agoprevI felt very seen by this blog post. I sent it to my partner and she replied with “Sounds like you. What is IRL” which perfectly sums up the disconnect on some topics that the author also mentioned. I just spend the past month rebuilding my blog, even though there’s nobody reading it and it really only is my “online home” to play around with and be creative. My main source of traffic is random Google visits for some “I’ll write this down for myself in case I run into it again” type posts. reply markwrobel 12 hours agoprevLars-Christian, your site got a little attention today :-) No post I’ve written has ever gone viral. I also have a personal website. If anyone notice what I've written it's a very nice added bonus. For me it's also about personal ownership of my content, and perhaps also a reminder to myself of the old internet - which I miss. reply ggm 13 hours agoprevThe point of the article is one I align with: You're writing it for yourself, 99% of the time. The other 1% is the future you. I blog for work. I don't discuss it with family. I think I'd find it very stressful answering the \"why did you say that\" questions. The corollary of this, is that I write notes by hand in almost every meeting I attend, and never ever read them again -But for things like IETF I do a mixture of .org and meetecho (markdown) because there is at least some possibility others may get value from the shared log in meetecho, and I know I will use the .org to .. write the blog. reply vasco 12 hours agoparentI take many handwritten notes throughout the day like you, but found that reviewing them for 2 minutes at the start of the next day has high return on investment. reply matrix87 13 hours agoparentprev> You're writing it for yourself, 99% of the time. The other 1% is the future you. Or alternatively it could be a place for educational stuff like the more detailed answers on stackoverflow. Except the middleman gets removed reply fullspectrumdev 8 hours agorootparentIn the past I’ve used a blog literally to make small notes on very simple things that I always ended up having to look up, or for notes on the “right way” to do something. For example: for years I was using Pythons requests library incorrectly. The way I was doing it worked - but it wasn’t correct. Once I published a short blog on the correct way - the correct way seemed to stick better in my mind! reply rietta 4 hours agoprevYes, and I have been surprised by at least one who told me that I needed to blog more often. We have very little interaction since graduate school. I have been running the same website, which has undergone several redesigns, for 25 years and counting. It is also interesting about having a personal domain name. People pause when you give the email. No, I really did mean first@lastname.com. Yes, I've had this for decades. My sister tells a different variant, she at least once had someone comment on it and then stand up straight - which she interpreted as being a little impressed as if my sister was a celebrity or something. Very interesting in a world where most people do not have a domain with their family name. The challenge for me has been that over time it morphed from a personal site, to a professional site, to a corporate site. Now that I have employees and this work supports my family I have less freedom to do just anything I want with it. It has to be on topic. That constraint does also bring freedom in its own way. I do not have a good place for personal interests that are not related to my cybersecurity work any longer though. reply runamuck 2 hours agoprevSince 2016 I publish a new blog post once a month. A long lost friend (20 years no contact) found it, read a handful of my deep dives and then offered me what I consider a dream job based on my content and perceived writing ability. I got to jump from an IC track to an executive role. My then company of 15 years did not offer that opportunity. I recommend a blog just to show the world your chops. You don't know who might read it. reply ehPReth 16 hours agoprevI have exactly one IRL friend that cares about my technology stuff, but he lives far away and we don't see each other in person too often. My ex tolerates me talking about it a little bit before he tells me to shut up, but that's it. It kind of sucks. I've just re-internalized to that nothing I do really matters, and therefore neither do I. reply CoastalCoder 16 hours agoparentSorry, existential dread sucks. Hope you find some more things that bring joy. (I know platitudes are grating, but I thought you might want to know someone is rooting for you.) reply criddell 8 hours agoparentprev> nothing I do really matters That’s not necessarily negative. I’ll sometimes say that to myself when I’m feeling down as a way of freeing myself from whatever thoughts are feeling too heavy. I’m not really into stoicism, but I think it’s what memento mori is getting at. reply malfist 15 hours agoparentprevI assume nobody reads my blog, at most a couple people check out the pretty pictures but that's about it. My opinion on blogging is do it for yourself, not to achieve something. That way it doesn't feel like wasted effort when nobody reads it and it can still be fun or cathartic to write. reply tempfile 8 hours agoparentprevoh man, this is sad to read. I'm sorry. reply craigkerstiens 16 hours agoprevI love when friends do this. It's hard to keep up with people and what they're up to. Publishing and letting people subscribe to me is a great way to share things. A few examples of some friends who are doing this: Justin Searls (fairly known in Ruby and Rails community) mostly quit a lot of various social channels though publishes on some of them one direction. He started a podcast that wasn't meant to be guests of some specific topic, it's just him updating you on things. What he's working on, what he's learning, random stories, etc. - https://justin.searls.co/casts/ Brandur who I've worked with at a couple of places (Heroku previously, and now Crunchy Data) who writes great technical pieces that often end up here also has more of a personal newsletter. While there are technical pieces in there at times he'll also talk about personal experiences my favorite one is some of the unique experiences hiking the Pacific Trail (https://brandur.org/nanoglyphs/039-trails). reply throwaway290 15 hours agoparentFor many it was supplanted social media. IG, TG, even TikTok (shudder) channels. It monetizes the same motivation reply wannabebarista 7 hours agoprevI have a blog that's connected to my academic website. While my homepage gets some traffic from people googling papers, my blog gets much less. I post a few times each year, mostly about stuff I've been reading. I did have one post go viral-ish a few years ago (https://bcmullins.github.io/foreign-affairs-100/). I was surprised to find that some workplace acquaintances and even students read my blog. A colleague out-of-the-blue messaged me about some python function I'd written (https://bcmullins.github.io/parsing-json-python/). A student asked about reading recommendations and how I choose books. So people you know IRL may be reading your stuff (or some of it) but just not mentioning it. As another post mentioned, I feel much more pressure about my writing after learning that IRL people read it. reply tcgv 7 hours agoparent> A colleague out-of-the-blue messaged me about some python function I'd written Just the other day a colleague at work was searching the web and eventually ended up in a blog post of mine. He was enthusiastic that he had \"discovered\" my blog. It was fun. reply Eiriksmal 15 hours agoprevAh, Lars-Christian! No one wants to talk to you in real life about your blog because you're in Norway. If you lived here in the United States, you'd have a perpetual string of very interesting conversations about your personal website because Americans are all about technology and hearing how any of us peons are fighting the power, man. At least, I think that's what my wife's reaction is before she leaves the room to find a book to read. And my friends who think a blog is just part of my weird, personal brand, like using a phone with a keyboard. reply rurban 8 hours agoparentYou apparently have no idea about Norway. Norway was the very first internet country, because of its vast coastline and enormous distances. Schoolkids had internet remote classrooms since the 90ies. Everybody there is online, and blogs are widely read. Much more than in the US. reply globalnode 14 hours agoparentprevHN voting system is stupid. reply karaterobot 3 hours agoprevI've had a blog at the same URL since 2000. Search engines are disallowed, comments are closed, I'm not on social media to announce it. I don't talk about it. Twice a week I publish something, but it's meant as a record for me rather than other people. The only reason it's on the web is so I can access it easily from different computers. Once I checked in Feedly, and it said that there were two people subscribed to it. To the best of my knowledge that's the readership. reply ericyd 6 hours agoprevI've recently had the morbid thought that perhaps my loved ones could appreciate my blog if I died prematurely. I don't maintain my blog much and I certainly don't think anyone reads it. It is much more for intrinsic benefits of working through thoughts in a structured way. reply HermanMartinus 15 hours agoprevI haven't been on traditional social media for about 6 years and so the way people keep up with me is via my blog. This way they get an update maybe once a month about what it is I've been doing/thinking about, then they reach out to me via email. Even the IRL people know I have a blog, but I guess it kinda comes up since I also run a blogging platform. reply xivzgrev 15 hours agoprevEven many personal blogs these days are suspect, they’re building a brand. I like this guy is doing his own thing. reply worthless-trash 15 hours agoparentCan you tell me the difference between building a brand and this guy ? I struggle to understand what this means, as I dont know what \"building a brand\" means when a person does it. reply Suppafly 1 hour agorootparent>Can you tell me the difference between building a brand and this guy ? I think it's about intent. Everyone builds a brand to a degree just by being themselves online, but some people do it with an intent to capitalize on it and it often come through in their work in a way that can be distasteful. reply vasco 12 hours agorootparentprevWanting attention and for other people to remember you with the purpose of getting ahead later on, by getting favors from the audience, selling things to the audience, etc. Commonly done back in the day to later send traffic to whatever SaaS you'd try to start. reply 6510 9 hours agorootparentprevYou will get an audience (if any) that fits the topics. If you want to have more readers you will have to increasingly include things they would like and exclude things they don't like. In reality an interesting topic is one where the answers are unknown. You might be biased or super objective, you might carefully compare theories and evidence on both sides or perspectives of an argument or theory. At some point you will have the tendency to include what YOU think about it. It doesn't matter what that is, you will lose readers. Perhaps you welcome Jesus in your life or reject him. One could switch between those two every other week and if that is what is on ones mind one should just talk about it. Talk about it often enough and you will build an audience of similar doubt while some Christians and atheist won't stick around. For those with more patience or sufficiently impressed with your other writings you need only repeat the \"offense\" often enough. Surely a thoughtful person should have an opinion about every empire, corporation or ideology murdering, maiming or torturing people? If you are completely indifferent about it, that would be your opinion. Disclose your opinion and those who don't agree wont be amused. You have to actively avoid the topic which isn't easy as everything in the universe is connected. You can be Steve Jobs, that doesn't mean you can just talk about alternative medicine. Facts have nothing to do with it. I can write cookie cutter stuff that everyone can read (perhaps even enjoy!) but my private thoughts gravitate straight towards the controversy and I ponder those things deeply, sometimes for decades. If I mistakenly write those thoughts down Best I can hope for is an audience of anger. If you talk about something it means you must believe in it. Someone on twitter just asked what the rope hanging from the American flag is. I got a 12 hour ban for saying \"I dunno, it seems enough rope to hang yourself?\" That is your free speech absolutism sandwich all bagged up for ya. ha-ha reply hug 14 hours agorootparentprevA good enough personal brand means you have a name (generally) disconnected from your individual job and you are known as an entity in your own right, often not entirely pinned to a single discipline. The most cynical example here is probably Kylie Kardashian, ostensibly a billionaire entrepreneur, but you could just as easily sub in a lot of modern content creators like Mr. Beast. These can come about naturally through some amount of fame, naturally through having a natural talent for something that generates interest in you and your opinions, or you can grind for it in a kind of numbers game. If you are working towards getting your name \"out there\" and having some of that sense of recognition, if you're hustling for subscribers or follows, if you are following trends to post about in order to try to get more readers, you're behaving in brand-building exercises. If you're just writing for the sake of writing, or to keep a journal for your future self, or just so your close friends can keep up with you, you're not. reply worthless-trash 12 hours agorootparentThank you for taking the time to expand that, you've said it more clearly than what I have been able to find elsewhere. reply elric 4 hours agoprevI have multiple blogs (the link to one of which is in my HN profile). The tech stuff is pretty safe, and so some tech people in my life know about it. The other blogs are reserved for people who are involved in other aspects of my life. In general, I find I compartmentalize a lot, and there are very few people in my life who are part of multiple compartments. reply janalsncm 10 hours agoprevI think you can either have a “corporate” blog attached to your name (whose purpose is to bolster your professional reputation, among other things) and an opinion-based blog with a pseudonym. The two can never touch. My philosophy on this is that anything worth saying, that isn’t some tepid opinion about pizza or your pet, will probably irritate some people. And while the internet isn’t forever, it’s got a fairly long memory. reply RajT88 12 hours agoprevThe people in your real life don't give a shit about your blog. But random strangers do! At least, if you're contributing something to the world which is wonderful and useful. In my experience, the posts which get the most traffic are simply not going to be the ones you think they will be. I am thinking of both my public and internal blog posts at my company - the most read articles are dumb basic shit that nobody thought to write down (but should have), and my greatest masterpieces languor in obscurity. reply froh 12 hours agoparentTIL to languor https://en.wiktionary.org/wiki/languor#Verb reply Suppafly 1 hour agorootparentme too, I've heard it before but never seen it written, which is the opposite of how I learn most words. reply hellweaver666 10 hours agoprevI remember back in the early 00's someone at work found out about my blog and news got around the office (it was a personal blog, more like a diary and I was in my late teens so it was kinda \"edgy\"). Anyways... within a few days I started getting a bunch of troll comments and it took all the fun out of it knowing everything I wrote was being read by my colleagues. reply whorleater 16 hours agoprevHuh, kinda funny, I feel the exact same way. Few people IRL know about my website, 2-3 people occasionally write in a year about something, but weirdly it feels like the idea of a personal, non commoditized internet space has become so rare it's seen as odd. reply kaiwenwang 13 hours agoprevA website is a Presentation of Self extended to artwork and 2D media. What is a Presentation of Self? It comes the book The Presentation of Self in Everyday Life by Erving Goffman. The two primary senses of humans are sight and vision. If you take the time to write down and enumerate the ways by which people understand the internal state of others, you realize that a computer simply communicates impressions of people over long distances, some of which have limited correspondence with who a person really is. Photos could be fake. Videos could be scripted and a certain impression. You don't really see someone for who they are except in real life, talking to them for a duration. Most people talk to or avoid certain people internally based on how they act and what they say. But once you realize the structure of communication, you feel like a robot talking to others. What was implicit becomes explicit. So yes, a website matters, if the increasing trends for humans and human ability are greater knowledge consumption, production, network communication over locality, and so on. It reveals your internal perception, taste, intelligence, and processing of information, by which people use to judge you and ask how relation to you improves the group fitness. Having a website selects for intellectualism. Social media is also a presentation. The question could be rephrased: Do people know you have a TikTok or Instagram? People such as his wife and digital people have increasingly different lifestyles and diverge. Had the author made his way to a metropole or more major place, it's likely having a personal brand would've mattered more. reply outofpaper 13 hours agoparentWhat do you meany by ouw two primary senses are, \"sight and vision\" reply kaiwenwang 13 hours agorootparentsight and hearing* reply manuelmoreale 13 hours agoprevThe thing I find most interesting, every time something like this gets posted here on HN is that in the comments, depending on the day, dozen or hundreds or even thousand of people post some permutation of \"hey, I feel the same\" And that makes me smile because on the one hand people keep repeating that blogs are dead, but on the other you're all proof that it's clearly not the case. reply brontosaurusrex 12 hours agoprevSome do, mostly when the problem they are trying to solve is somewhere in my blog past. I've learned over the years not to advertise the blog/page, due to weird questions I get, for example: Q: How costly is this? (A: it's free, minus my time), Q: Why is it so ugly? (A: When you write your perfect jekyll skin, I will use it, you know it's about my future self searching for the info, not about beauty), Q: Why? (A: I always find something interesting to read in my blog, unlike your corporate page). p.s. The long-tail effect has been predicted at least 20 years ago, so I understand that views will be low or none: https://en.wikipedia.org/wiki/Long_tail My blog: https://brontosaurusrex.github.io/ reply wannabebarista 6 hours agoparentI find the \"Why?\" question amusing. It's usually coupled with a disapproving tone. reply voidUpdate 10 hours agoprevI've attempted to show my website to my family a couple of times, but I'm a techy in a generally non-techy family, so they're not particularly interested. I mostly use it to talk about things I've done and I have referred back to it on a couple of occasions when I know I've solved a problem before and want to see how I did it at the time. I attach it to my CV so I keep it professional enough for someone else to look at that I would want to impress, but still pretty casual because I try to write like I'm explaining it to a friend, someone who knows basic programming stuff but is a novice to the actual topic of the article reply Pyrodogg 11 hours agoprevI had personal site on shared hosting with blog, microblog, \"lifestream\", photo gallery in the late 00s. It's been a 'placeholder' status for redevelopment since I think about 2011-12. I got a job and started (over)working. Bugs/exploits in the PHP framework I was using took the site down a few times. Maintenance lagged, and I eventually zipped everything and shut it down. I've found a better work life balance over the years, but just haven't connected the dots to ever doing something new with it. I think the last attempt foundered on picking a static site generator. Most of my IRL friends and family barely use Facebook so I'm pretty sure few were ever very aware of what was on there. reply thomashabets2 11 hours agoparentThis is why I use a static site generator. No security fix is ever urgent. No risk of needing to make a bunch of changes in case I need to move host, and they have a slightly different version of PHP. The thing never \"fails to start\" after a reboot. No database that can cause problems. It's just files. Anything can host just files. My first site actually started as what we'd now call a static site generator back in the 1990s. Then PHP a few years later, then Python built on webpy.org (including an admin interface), and now back to a static site generator. Most of my friends are computer people, and know and occasionally read my blog. My non computer friends could not care less. reply guappa 9 hours agorootparentIn the end it turns out that the way of doing websites by creating .html files directly was the more future proof :D reply nicbou 11 hours agoprevMy blog has some of my recipes, and some of my friends appreciate that. Otherwise my blog is a beacon to other nerds on the internet, extra material for people who are curious about me. People do check it and reach out, so it works. reply laurentlb 10 hours agoprevYes. I've (re)started my blog a few months ago. Some articles went viral. Some people mentioned my blog IRL (and I didn't know they were following it). Some of my blog posts led to nice discussions online or offline. Some people have asked me to write about specific things. So right now, I'm quite happy with how it's going and I have a list of articles to write. But writing a post takes multiple hours, so I don't know what the frequency will be on the long run. reply tempfile 8 hours agoprevI have a blog that I consider permanently unfinished. Being (still) too embarrassed to share it is a pretty good motivator for adding things, and it's nice to have something you can be a perfectionist about without feeling guilty. Formally, it's an exercise in learning the W3C Accessibility standards, but the content is very much \"write for yourself\". reply sien 14 hours agoprevI have a blog that has all my book reviews that I've written over the past ~17 years. It's got over 1000 reviews on it. I post them all to Goodreads as well. Hardly anyone read them on my own blog. It's convenient for me to have them all in one spot so I can export them and whatnot. I've had one review that a successful blogger (now substacker) linked to and that probably resulted in more than half the hits the blog ever got coming in about two weeks. Posting to Goodreads people seem to appreciate more. My reviews get some reaction a few times a week there. A few people IRL know about my blog and reviews on Goodreads. I don't generally tell people about it but if people really read and it comes up I tell people. I keep it largely separate from my Twitter account and Linked In profile. reply WA 12 hours agoparent> Posting to Goodreads people seem to appreciate more. Yeah because reviews there are put into context of other reviews. I never google for book reviews, because I expect mostly spam. I always go directly to goodreads or amazon. reply firefoxd 15 hours agoprevEarlier in my career, every single person in my team had a website. At lunch we would talk about how we built it, servers, fail2ban, zipbomb, etc. Many years later, one coworker has a website under construction, and others say they have nothing to say online. When i bring up a blog post I wrote years ago, relevant to a conversation, I feel like a charlatan trying to sell a product. But hey, I'm happy to write in the dark. Especially after some of my posts have literally landed me on TV. I felt like everything I Wrote after was scrutinized. But the world has forgotten about me so I'm free again. reply lifestyleguru 10 hours agoparent> When i bring up a blog post I wrote years ago, relevant to a conversation, I feel like a charlatan trying to sell a product. Influencing killed it. Nowadays people bring up their online presence when they want views, impressions, and subscriptions, when they are trying to bump follower count. reply KenHV 13 hours agoprevMy blog is mostly just me sharing things to my friends; some of them IRL. Short tutorials, highlights from books, etc. https://kenhv.com/blog reply _thisdot 4 hours agoparentLove the design! What did you use to build this? reply FLpxpyJ 11 hours agoprevI think, a little ironically ofc, opening yourself up like that ends up capping how personal you can (are willing to) get. Not always a bad thing, but I believe the modern internet is missing a lot of soul that comes from being vulnerable. Lord knows social media personalities and corporate brands will never do it. https://foreverliketh.is/ reply winternewt 11 hours agoprevIn the vein of the discussion here, I think it's relevant to highlight Kagi's Small web [1] initiative, which is intended to give more discoverability to sites like this. I find it to be admirable and it gives me some small hope for the future of the web. [1] https://blog.kagi.com/small-web reply KingOfCoders 13 hours agoprevWhen I moved to Berlin, I had a blog mostly for my mother :-) reply calini 8 hours agoprevI have a 3D printed shopping trolley coin with my blog written on it, and even offered a couple of them to friends. That being said the blog's been inactive for a couple of years now :D. reply cafard 8 hours agoprevInteresting question. A sometime co-worker does, and a sometime neighbor. On the whole, I figure that my family gets enough of my opinions in conversation. reply underdeserver 10 hours agoprevPeople who read blogs, especially in the software industry, are still a tiny minority. There are maybe tens of millions of software engineers, designers, product managers, entrepreneurs and other tech-adjacent workers in the world. HN is one of the most popular tech sites among hardcore techies, at least in my circles. And yet the top posts of all time get thousands of upvotes. Add 10x lurkers and you get tens of thousands of users. That's still 0.1% of the total above. reply vstollen 12 hours agoprevWhile I don‘t write much, my website is directly associated with my real name. As a result, friends regularly stumble upon it. Most of them don’t care for the topics I write about. However, they usually get a little excited, as if they found a real-life Easter egg. reply croniev 12 hours agoprevPiranesi by Susanna Clarke... I've been wanting to read that too! Jonathan Strange and Mr Norell has been one of my favorite and most immersive books, absolutely brilliant! Too bad Susanna Clarke got CFS, a very ill researched illness :/ reply yantzr3j 10 hours agoprevMy late brother once sent me a link to an article in his blog. After he passed, I was far more grateful that he'd sent it to me. reply v3ss0n 10 hours agoparentScheduled email? reply porphyra 13 hours agoprevI kinda get it, but the opening paragraphs betray the author's problematic, arrogant attitude --- passive-aggressively complaining about his wife for not appreciating the grand genius of his blog. My wife, my parents, my friends etc are all aware of my website. My wife enthusiastically shows my website to our friends. I have a blog post about food recommendations that is very popular with my friends. reply DHPersonal 3 hours agoparentAn alternative interpretation (and the way in which I read it) was that the author is incredibly passionate about something mundane and his wife is very obviously not interested anywhere near as much or at all, just like everyone else who likely stumbles across his blog. I don't think it was attempting to show disdain but acceptance, admitting that he builds a blog because he cares about it alone; even his wife doesn't read it. reply impure 13 hours agoprevIt occasionally comes up in conversations. But I don't think people are that interested. Although I will say my programming hot takes occasionally do get popular. reply INTPenis 10 hours agoprevThey're pretty much the only people who know it. Because I get no visits, it's mostly cathartic. reply copywrong2 3 hours agoprevAll the important ones reply _the_inflator 14 hours agoprevDude is right, no one except for the audience cares. I was a successful blogger in the 00th years and with a side business around Made for Adsense because once you understood SEO it was inevitable, if this rings a bell. Reading a blog seems magical. Just imagine “Do ppl IRL my print magazine?” to use another metaphor. Nope and yes. Same goes for artists, say singer and songwriters: “Do you listen to my stuff?” Why should they? Understanding your readers and fans is not easy. Statistically speaking, if all your close friends read your blog, you either are on to something or you get lied to. So if all around read your blog you might be some truly impressive author with a huge fan base - and 99,9999% won’t fit in here. As a side note, I still know quite a fraction of successful YouTubers. They are publishers, content creators. It is work for them, maybe evolved from something they did for fun. These dudes always prioritize money now - because they know their niche a bit better now and want to appeal to it. reply Brajeshwar 14 hours agoparentI had removed Analytics from my website/blog quite a while back and I don't know who read, or what happens to my website anymore. It is a pleasure not knowing. Cloudflare does do the basic Analytics that comes built-in and I rarely see them. reply vstollen 12 hours agorootparentAlong the lines of Julia Evans’ “Optimize for conversions” [1], can you gauge the popularity of your post by the replies you get? [1]: https://jvns.ca/blog/2018/02/20/measuring-blog-success/ reply jtwoodhouse 7 hours agoprevThe key is to write stuff worth talking about IRL. reply blitzar 12 hours agoprevYes, There are 2 posts from 2007. reply lelanthran 4 hours agoprevPlease. People who know me online don't remember about my blog, nevermind IRL people. reply incomingpain 7 hours agoprevNope. Nobody wants to read what I have to say. I'd say I'm banned, shadow banned/flagged, or otherwise censured on social media all the time. reply komali2 12 hours agoprevYes, because my blog is basically a collection of things people have asked me more than once. * How do I rent a motorcycle in Taiwan? * What's a coding bootcamp like? * What's your emacs config? * Got any book recommendations? * You got into Raw? How was it? * Didn't you parents come to Taiwan? Mine are coming next month, what did you do with them? etc. I'm constantly dropping links to people at networking events or when they come into my restaurant. I also just forget things constantly and so my blog is basically my external brain. reply jowdones 9 hours agoprev [–] Yeah, well, in today's shallow and blood thirsty culture, one mistake and you're fucked. And doesn't have to be today, it will follow you all your life, when you're more comfortable, bam! Someone finds something you said 15 years ago that no longer fits the increasingly edgy sensitivities of today and you're canceled. Doxxing you, trashing you, death threats, you name it. Unless you walk on eggs more carefully than in Stalin's Russia and only talk of weather and puppies, you are liable for being torn apart by an angry mob of mediocres who finds fulfilment in destroying a defenseless guy's life. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discusses the personal significance of maintaining a blog for nearly twenty years, despite indifference from people in real life (IRL).",
      "The blog serves as a form of self-expression and freedom, contrasting with the pressures of social media and big tech.",
      "Responses from readers highlight various perspectives, including the benefits of blogging for personal clarity, concerns about AI exploitation, and encouragement to write without analytics."
    ],
    "commentSummary": [
      "Personal websites offer flexibility and refinement without the pressure of regular updates, unlike blogs which can feel lazy and unwelcoming.",
      "Blogs may be useful for historians and sharing thoughts, but personal pages are preferred for their organization and creative potential.",
      "Some individuals use pseudonyms on personal websites to avoid professional repercussions, highlighting the personal nature of these platforms."
    ],
    "points": 175,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1721696929
  },
  {
    "id": 41042034,
    "title": "Wiz walks away from $23B deal with Google",
    "originLink": "https://www.cnbc.com/2024/07/23/google-wiz-deal-dead.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN CNBC DISRUPTOR 50 METHODOLOGY 2024 LIST 2023 LIST 2022 LIST 2021 LIST NEWSLETTER CNBC DISRUPTOR 50 Wiz walks away from $23 billion deal with Google, will pursue IPO PUBLISHED MON, JUL 22 202410:30 PM EDTUPDATED MOMENTS AGO Rohan Goswami @IN/ROHANGOSWAMICNBC/ @ROGOSWAMI Jennifer Elias @JENN_ELIAS Jordan Novet @JORDANNOVET KEY POINTS Wiz has walked away from a deal with Google that would have valued the company at $23 billion. CEO Assaf Rappaport told employees the company would pursue an IPO as originally planned. In this article GOOGL Follow your favorite stocks CREATE FREE ACCOUNT Sundar Pichai, CEO of Alphabet Inc., during Stanford's 2024 Business, Government, and Society forum in Stanford, California, April 3, 2024. Justin SullivanGetty Images Wiz has walked away from a $23 billion deal to be bought by Google , in what would have been the search giant's largest-ever acquisition, telling employees it would pursue an IPO as previously planned. \"Saying no to such humbling offers is tough,\" Wiz co-founder Assaf Rappaport wrote in a memo obtained by CNBC to the company's employees. A person familiar with Wiz's thinking, who requested anonymity to discuss private matters freely, said the company weighed antitrust and investor concerns as reasons for abandoning the potential deal. Rappaport wrote that the company would focus on its next milestones: an initial public offering and $1 billion in annual recurring revenue. Wiz had been eyeing both targets well before talks with Google had been reported. The deal would have nearly doubled the $12 billion valuation of the startup from its most recent round of funding. Wiz was founded in 2020 and has grown rapidly under Rappaport, who had been targeting an IPO as recently as May. The company hit $100 million in annual recurring revenue after 18 months, and reached $350 million last year. Wiz's cloud security products include prevention, active detection and response, a portfolio that's appealed to large firms and would have helped Google compete with Microsoft , which also sells security software. Alphabet's cloud segment has faced increased competition from front-runners Microsoft and Amazon . The cloud unit reached profitability in 2023 after years of hefty investment. While Google Cloud has seen consistent growth in recent years, the unit, led by CEO Thomas Kurian, is under pressure to continue growing in efforts to capture business during the artificial intelligence boom. Google didn't immediately respond to requests for comment. Exits in technology have been rare this year, between startups waiting for more receptive markets before going public and a challenging regulatory environment for acquisitions. The collapse of the transaction will be seen as a disappointment by Index Ventures, Insight Partners, Lightspeed Venture Partners, Sequoia and other venture backers that have raised multibillion-dollar funds in recent years. Funds that run into the billions require exits of more than $10 billion in order to generate sizable returns for their limited partners, and those events have been rare, said Brendan Burke, a senior analyst at PitchBook. Wiz's founders previously built security startup Adallom, raised money from Sequoia and Index and sold the business to Microsoft for $320 million in 2015. Former Sequoia leader Doug Leone has said investing in Wiz in its earliest days was \"a no-brainer.\" Soon after Wiz's launch came the Covid pandemic. Companies rushed to adopt cloud-based software and infrastructure to help employees work remotely. The shift benefited Wiz, which can flag security issues for applications and data on the Amazon, Google, Microsoft and Oracle public clouds. Less than a year after its founding, Wiz announced a $100 million funding round. \"I think what was unique with Wiz in the early days was the amount of money raised from the get-go,\" Sid Trivedi, an investor at Foundation Capital, told CNBC in an interview. Google acquired cybersecurity company Mandiant for $5.4 billion in 2022. Google's largest deal remains the acquisition of hardware maker Motorola in 2012 for $12.5 billion. The company ended up selling assets from that purchase to Lenovo for $2.9 billion in 2014. Google recently ended conversations to acquire sales software maker HubSpot. In an interview with CNBC's Sara Eisen and Carl Quintanilla at the New York Stock Exchange last year, Eisen asked Rappaport if he wants to take Wiz public. \"Yeah, definitely,\" Rappaport said. He laughed. \"That's why we're here.\" Don’t miss these insights from CNBC PRO Berkshire has eliminated 10% of outstanding shares as Buffett values the enduring power of buybacks Bank of America strategist says it's time to get bearish Morgan Stanley is pounding the table for these stocks, including Apple, ahead of earnings ‘Trump trade’ could stall if Biden drops out of race, analyst says WATCH NOW VIDEO04:50 Wiz ranks as No. 5 on Disruptor 50 list with a start as a $10 billion startup MORE IN CNBC DISRUPTOR 50 These are the 2024 CNBC Disruptor 50 companies: See the full list of startups riding the AI wave CNBC.com staff Why OpenAI is the first company to be No. 1 on the CNBC Disruptor 50 list two years in a row Julia Boorstin The 2024 CNBC Disruptor 50: How we chose the companies David Spiegel READ MORE Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=41042034",
    "commentBody": "Wiz walks away from $23B deal with Google (cnbc.com)161 points by uger 16 hours agohidepastfavorite114 comments ds 15 hours agoFar more likely is Google was not willing to complete the deal and was pulling the plug after looking at internal data. Wiz, fearing the bad press of Google backing out rushes to tell journalists that THEY are walking away because they are worth more. Wiz's valuation is insane. Most people havent even heard of them. I think it was a > 60x ARR multiple on this deal. Id actually be kinda pissed if I was a google shareholder and they went through with it. Something very strange is going on with Wiz. My gut tells me if they ever IPO to go big on puts. reply dboreham 14 hours agoparentAdd me to the haven't heard of them list. Mind you I almost hadn't heard of Crowdstrike and they managed to brick the world. reply rob74 12 hours agorootparentI hadn't heard of either Wiz or Crowdstrike before... while reading the article I was thinking \"$23B? Probably AI! And called Wiz? Yeah, must be AI...\". Turns out I was wrong after all... reply heraldgeezer 8 hours agorootparentprevCrowdstrike is enterprise only. Do you know of Active Directory? Most have no idea, even though it is a Windows Server feature from 2000. Some will live a life and even work not knowing. reply dingaling 4 hours agorootparentAD is fairly well known due to its relationship with LDAP and Kerberos. Samba can act as an AD DC. reply RandomThoughts3 3 hours agorootparentAD is incredibly more popular than Kerberos despite part of it using the protocole. Microsoft is everywhere in the corporate world and most people know of AD but have never heard of neither LDAP nor Kerberos. And to be honest, it's fairly understandable. AD manages to be somewhat turnkey while doing the same thing on Linux systems is a major pain. reply ilrwbwrkhv 13 hours agorootparentprevYup the world is big and even though we think we have heard stuff, there are more things beyond that. For example, I know a dev who makes mobile apps and clears 500_000 / month in profit and yet their app isn't really \"popular\". It is crazy. reply exikyut 13 hours agorootparentHuh, cool. What sector is the app in, what are some other interesting (non-identifying) aspects of the app that stand in contrast to revenue? Is that in ads, or does the app have in-app purchases et al? reply xenospn 2 hours agorootparentThe only way to make that much of money is with dating apps, IMHO. There’s a million out there and some of them make really good money in certain niches. reply gustavorg 10 hours agorootparentprevYou are going to say what the app is, right? right? reply supportengineer 3 hours agorootparentprevThrough sales, subscriptions, or ads? reply kuzmanov 14 hours agoparentprevThis is exactly how I felt as a shareholder. There is no real reason to pay this much and it seems like Google is the one that walked away from the deal. reply pjc50 10 hours agorootparentShareholder in Wiz, or Crowdstrike? reply nar001 10 hours agorootparentGoogle wasn't trying to buy Crowdstrike, so Wiz reply xyst 13 hours agoparentprevA company built during the pandemic, likely peaked following the Solar Winds aftermath. yup, overvalued reply qaq 13 hours agoparentprevWell realistically if they have a chance to take on Crowdstrike they might not be wrong to walk away. reply solatic 12 hours agorootparentOn the one hand, even with the post-crash dip, CRWD has a $60.9 billion market cap, there's certainly marketshare to be taken from them. On the other hand, Wiz doesn't have an endpoint protection product (which is what failed for CRWD). They'd have to build one from scratch, which requires dedicated talent (engineers with kernel experience) that they might not have. If anyone is going after CRWD it'll be one of their other competitors. reply thriftwy 12 hours agorootparentThese numbers sound like a complete out of world fantasy to me. CRWD has a product that the user is not going to notice, best case. Now you said Wiz doesn't even have that one (what does it have then?) And their valuation is on par with the whole annually Western support of Ukraine. A country at war and with 30M people in it. That for some completely invisible product. It is also 17 millions of these most expensive brand new 155m artillery shells. reply pjc50 10 hours agorootparentI think this is just a representation of where the money is in the world. Two things: - stocks are called stocks for a reason, they're not flows. $60bn is effectively an estimate of all future profits of the company over its lifetime - Crowdstrike generates a return by charging enterprises huge amounts of money to feel secure and tick security boxes (Actual security is questionable). Big enterprises have a lot of money to waste, but they feel they're getting a return on it - hardly anyone outside Ukraine gets a specific return from backing Ukraine. The same goes for all sorts of other worthy projects of the \"end world hunger\" kind - there's huge benefits, but not to the people actually spending the money. reply falcor84 9 hours agorootparent>stocks are called stocks for a reason, they're not flows Indeed, and of course we have Kalecki's famous quip that economics is \"the science of confusing stocks with flows\" reply qaq 6 hours agorootparentprevPretending that being geopolitical superpower has no direct economic benefits is just silly. If USD lost the status of world's reserve currency it would have pretty catastrophic consequences for US economy. reply pjc50 5 hours agorootparentHow do I, as an individual investor, capture the return of sending a shell to Ukraine? > If USD lost the status of world's reserve currency it would have pretty catastrophic consequences for US economy .. but for everyone at once. Collective action problem. You've argued why it's in the interest of the US government to tax people and send shells to Ukraine, but this is not an argument for Blackrock to divert VC funding to individual armored brigades. reply qaq 3 hours agorootparentVery true reply qaq 6 hours agorootparentprevIt's hard to make a leap from war to company valuation. Also Ukraine support is highly inflated number. If say Ukraine gets supplied with an old design MLRS rockets from US that was slated to be replaced in a few years and had very limited shelf life remaining the number counted is not the market cost of that old rocket (which would be a few 100K) but the 3 mil new top of the line replacement thing that US is producing for itself and Ukraine will never see. reply sofixa 10 hours agorootparentprevCrowdstrike does endpoint security (user's PCs and servers too for checkbox ticking reasons). Wiz does cloud security. The same thing as Crowdstrike, but runs in your cloud environment (AWS/GCP/Azure) to detect issues there. Different customers, different profiles, different costs and prices. reply thriftwy 10 hours agorootparentI just don't see why that should have $23B market cap as opposed to $230M. A small team can challenge them with similar product. reply borski 12 hours agorootparentprevWiz does not do endpoint security. Different products entirely. reply qaq 6 hours agorootparentThey certainly have resources to expand into that if needed reply borski 3 hours agorootparentIt is an entirely different problem with almost nothing in common with their existing product, and there are a ton of incumbents, some of whom are even quite good (Carbon Black, SentinelOne, etc) reply qaq 53 minutes agorootparentThere were quite a few of those when CrowdStrike entered there is always room there. reply toomuchtodo 6 hours agorootparentprevThey’d be competing with Crowdstrike, SentinelOne, Microsoft Defender, and Trend Micro not to mention existing CNAPP/CSPM offerings that have an agent for cloud runtime security as well as other cloud runtime security focused startups. Adding a runtime security and EDR offering is not going to get them to a $23B valuation. reply qaq 6 hours agorootparentSure and many others but outside of CrowdStrike most are not very competitive and being a fresh entry has it's benefits. reply deycallmeajay 11 hours agorootparentprevNot yet… reply helsinkiandrew 10 hours agoparentprev> Far more likely is Google was not willing to complete the deal and was pulling the plug after looking at internal data Wouldn't it be more likely that they would have lowered their offer after seeing the internal data - perhaps so much that Wiz would certainly walk away. reply chatmasta 14 hours agoparentprevThey’ve had some really nice writeups but I always thought they were your generic security firm doing some bug hunting. Recently I happened upon their domain submissions to HN and saw they raised $1b+ and was like wtf? What do they actually do? I mean what are their products that people pay for? Maybe there are obvious answers to these questions, but if a company is worth $23bn I’d expect that as somebody in the industry, I could answer them without doing in depth research. This is exactly the kind of gut feeling of “something’s off” that I’ve learned to pay attention to. reply walterbell 14 hours agorootparenthttps://old.reddit.com/r/cybersecurity/comments/1c1s9r2/wiz_... > Wiz combines a graph search for asset management with agentless vuln and malware scanning that clones EBS volumes and scans them on their infrastructure. That's a great combo for vuln management, but has some downsides like delays between scans and cloud costs. They have a sensor with solid detection rules, and are okay at a bunch of other stuff like cloud log threat detection and sensitive data detection. They've basically pushed what you can do without an agent to the limit. reply chatmasta 14 hours agorootparent> clones EBS volumes and scans them on their infrastructure Crowdstrike: “you just install a kernel module with ring zero access and we’ll make sure you’re protected” Wiz: “hold my Red Bull…” reply golemiprague 10 hours agorootparentFrom the explanation here it sounds completely opposite concept, they download the server and check it rather than doing the checks on production environment reply chatmasta 4 hours agorootparentYeah, I was thinking more about the risk of data leaks. reply pwdisswordfishd 12 hours agorootparentprevThis sounds uselessly crippled, as it's not going to catch malware that doesn't drop anything to disk, or that adequately cleans up after itself if it does. reply nshelly 12 hours agorootparentI would assume they could also dump memory, i.e. `/dev/mem`. Agreed they would need to also do frequent memory snapshots, but lots of malware will also run in the background waiting indefinitely, and often as the same name as common Linux processes but different hashes. reply qeternity 11 hours agorootparentYou would need an agent to do this. Cloning EBS won’t dump memory. reply stefan_ 10 hours agorootparentprevThe people who have /dev/mem and run this garbage must form a complete overlapping circle. reply danpalmer 15 hours agoparentprevWhile I don't have any comment on this instance, in general I think it's easier to hype the public markets who have limited information than it is to type a bunch of people doing due diligence on an acquisition, even if ultimately the latter is still a case of public market valuation through the acquiring public company. This is particularly true in the current age of extremely hype driven retail investing. reply generic92034 12 hours agorootparent> a bunch of people doing due diligence on an acquisition I bet those people rarely get promoted for preventing an acquisition, though. Probably that is why we see so many crazy acquisitions, in general. reply danpalmer 12 hours agorootparentAt the SVP level, sure, but at the IC level, I doubt any accountant gets promoted for saying \"looks fine\", whereas highlighting details that superiors can use to make a decision like this might be something that gets you promoted. This is a misunderstanding I think many non-googlers have, thinking people only get promoted for launches (or in this case acquisitions). It's more nuanced than that: people get promoted for impact and while launches are one obvious form, you can sell pretty much anything useful as impact if you can show how it's useful. In the case of M&A, avoiding a bad acquisition, if you can justify it, would be impact. reply raldi 13 hours agoparentprevWouldn't they be giving up a huge breakup fee if that were the case? reply sulam 13 hours agorootparentNo, breakup fees are post term sheet. reply vlovich123 13 hours agorootparentprevMaybe not if the breakup fee is forfeited if due diligence reveals fraud? Not sure. reply moandcompany 13 hours agoparentprevThe Groupon of 2024? reply Centigonal 15 hours agoprevFounded in 2020, with a $23B valuation, aiming for $1B ARR? CloudFlare is worth $26B and had ~$1.3B revenue last year. Something's fishy here. reply oceanplexian 14 hours agoparentLook at their financials, CloudFlare might be popular among the HN crowd but the stock is massively overvalued. At the end of the day they are a commodity CDN that somehow can’t run a profitable business. reply dilyevsky 13 hours agorootparentI did look at their financials - 77% gross margin with 30% revenue growth. Much higher than “commodity cdn” can command. If they cut marketing and r&d they’d be bagging half a b a year in profit but they are investing in growth as they should because it’s working reply J_Shelby_J 12 hours agorootparentCloud flare will still be the internet in ten years, while Amazon will just be Walmart, reply oblio 10 hours agorootparentAWS is making $100+bn per year and growing at about 20% per year. Good job, \"Walmart\"! reply walthamstow 11 hours agorootparentprevAmazon is the internet too. The us-east-1 outage in 2023 took down a lot of stuff. reply vinay_ys 10 hours agorootparentprevAnd, that's a dis, why? reply foolswisdom 13 hours agorootparentprevI think GP's point is that wiz's valuation makes even less sense. reply jppope 13 hours agorootparentprevYeah nah... they have great products and their primary business is allowing them to enter markets where other companies can't compete. The valuation is based on potential not the business as it stands today. And to your point they are popular with the HN crowd which is usually a strong indicator. reply zild3d 9 hours agorootparentprevcloudflare is a commodity CDN, amazon is an ecommerce site reply ilrwbwrkhv 13 hours agorootparentprevAnd now with their bad hiring practices and shady business deals they might not be popular among HN crowd for long too. reply walterbell 14 hours agoprevhttps://www.calcalistech.com/ctechnews/article/b1a1jn00hc > [Cyberstarts] has a portfolio of only 22 companies whose combined value is $35 billion. Five of these companies are unicorns, first and foremost Wiz, which seems to be breaking all the rules of growth and success and setting new standards in the industry.. In all three of his funds, Ra'anan shows an internal rate of return of more than 100%, an unusual figure even for the best funds in the world.. The first sales come from the loyal CISOs who work with the fund.. The whole CISO advisory committee issue has gotten out of hand for corporate America. https://www.calcalistech.com/ctechnews/article/hjpwti2dr > Wiz announced two months ago that it had raised $1 billion at a $12 billion valuation, bringing the company’s total funding to $1.9 billion. reply leereeves 14 hours agoparentThe first link alleges that Cyberstarts pays kickbacks to CISOs who buy their products. Is that legal? reply borski 12 hours agorootparentIt's not a kickback because Cyberstarts is providing 'points' which eventually equate to carry for them as advisors to the startups in the fund, to which they donate their time, expertise, and so on. The implication, you could argue, is that that also includes purchases from the startups, but that isn't at all a requirement of the program, at least as far as anything legal is concerned, to my knowledge. That said, it makes sense that if you're advising companies that are building products with your advice in mind, they're going to be solving problems you need solved, so you're more likely to buy from them. The fact that you have a good working relationship with them already is a bonus, of course. That's the optimistic view. The cynical view is that it's no different from drug companies paying doctors to shill their pills. reply windexh8er 13 hours agorootparentprevIt's definitely not uncommon. Illegal? Should be, the question is how they got paid and for what. There are so many C-level shills these days, it's sickening. reply lmeyerov 13 hours agorootparentEverything is securities fraud, so a payola consortium pumping Wiz through funding rounds sounds bad. Wiz is great, so the question is if they are > $23B great, and how they got there. And more so, why would a gov prosecutor bet on this case over others, or a private investor who has shares & reputation at risk. OTOH, maybe The Honest Services Statute where CISOs violate their fiduciary duty by prioritizing security/risk budget & focus to a VC paying them. I don't see most impacted companies making this public vs a warning or voluntary resignation. It clearly happens a lot, but the only successful prosecution I remember was at Netflix: https://www.justice.gov/usao-ndca/pr/former-netflix-executiv... . A funny thing is the VC is doing all this semi-officially: many but not all of the illegal bits go away - the gov has less to prosecute on when everyone files their taxes accurately ! reply jwally 9 hours agoprev> saying no to such humbling offers is tough Minor pet peeve: misuse of the word \"humbling\". A $23B offer is not humbling (on my planet at least). Humbling would be turning this offer down and then failing to get enough interest to generate an ipo. reply n4r9 9 hours agoparentYou might be right. On the other hand, Rappaport might be humbled thinking about how fortunate circumstances and others' hard work has led to this offer. He could be thinking \"gosh, I do not feel I deserve that\". reply glandium 9 hours agoprevI was like \"how the f* is a website builder possibly worth $23B\" and then I realized Wiz is not Wix. And then I realized Wix has a market capitalization of $9B, and I'm still WTF. reply nikanj 9 hours agoparentWix has over $1.5B annual recurring revenue, so a market cap of $9B makes sense. You and I might not use them, but it is a real business, not just a hype bubble. reply rurban 1 hour agorootparentMy non-IT wife is using wix for her homepage. She loves it, I never heard of it before. 9B sounds okish. But wiz? There are reading like Mossad/Unit 8200. And who wants to have them in their backend? Worse than Cloudstrike, which sounds like CIA to me. reply gnrlst 5 hours agoparentprevThought the same until I saw your comment. For those in the same boat: Wiz, Inc. is a cloud security startup. reply resist_futility 13 hours agoprev>Wiz’s founders previously built security startup Adallom, raised money from Sequoia and Index and sold the startup to Microsoft for $320 million in 2015. Copy, Paste and ask for 70 times more? reply ssl-3 14 hours agoprevI feel like I am uninformed. Can someone provide some background? Who in the fuck is Wiz, and why in the fuck might anyone think that they're worth 11 figures? reply teractiveodular 13 hours agoparentInstead of old-school installing agents to run on your VMs Crowdstrike style, with all the maintenance, performance and crash-the-world risks that entails, with Wiz you can clone your EBS volumes and examine them for vulnerabilities at your leisure. It's a neat party trick, but I'm also not convinced it's worth 11 figures. reply SJC_Hacker 13 hours agoparentprevI've been waiting for the Silly Valley insanity to stop for about the past 15 years, but its like its been permanently stuck in 1999 mode. With a few brief hiccups like Juicera and Theranos. I guess VCs just don't have anything better to do with their money, than throw it at a wall of shit and see what sticks reply boguscoder 13 hours agorootparentIm afraid its been 25 not 15 years since 1999 ;( reply ncruces 10 hours agorootparentWhoosh… that's the entire point you missed right there. reply blobbers 54 minutes agorootparent… what was the point? I missed it too!? VCs returns looking like power law investment in the back end / mud on wall in the front end of a fund? Or that the author has been waiting for VC investment to stop? I don’t understand what he’s trying to suggest. reply piltdownman 8 hours agorootparentprevIn a ZIRP era, they'd be non-optimal to do anything else. 80 failed projects is still a +ev outcome for a VC if they hit their 100x proposition on the 81st. reply thegrizzlyking 12 hours agoprevMost likely reason is FTC. Wiz integrates with big 4 cloud providers. No way FTC is allowing Google to take control. JD Vance's nomination and support for current FTC chair(Lina Khan) doesn't help. Current FTC is good(personal opinion) from anti trust point of view but maybe bad for startup exits[0]. [0] https://x.com/ID_AA_Carmack/status/1812978264484552987 reply titanomachy 3 hours agoparentIf they’re actually worth anything near $23B they can just go public. I’m sure they’ll be fine haha reply olalonde 12 hours agoprev> The company hit $100 million in annual recurring revenue after 18 months That's fast... Is that 18 months after launch or literally 18 months after starting development? reply mylastattempt 11 hours agoparentYour quote from the article, is a link to another article, which provides the answer: > The cybersecurity software vendor said in August that it reached $100 million in annual recurring revenue after selling its product for just a year and a half. So that's 18 months after formal launch. Since we don't have their financial statements, you can only guess wether this is the value of signed deals or a review of actual recognized revenue for those 18 months. I suspect the first. reply DrScientist 9 hours agoprevI feel like there is an opportunity for a startup that protects you from the risks associated with your security vendors :-) reply tzury 8 hours agoprevA screenshot of the internal mail https://x.com/mikeeisenberg/status/1815644472945819788?s=46 reply Kostchei 11 hours agoprevAs a user of Wiz (and I like what I get from them) ,I am relieved . 1. huge valuations are bad for customers- that investor cash has to come back from somewhere 2. Google has a habit of subsuming products into their stack and either sunsetting them or holding them so close that nobody uses them (beyondcorp etc) *spelling reply drumhead 11 hours agoprevThis is positive for Google. Spending $23bn on a company of that size was insane. Shareholders are probably relieved. reply jsiepkes 8 hours agoparent> Shareholders are probably relieved. Don't think so. Google has a relatively good track record for acquisitions. So I don't think they were really worried. reply physicsguy 9 hours agoprevWonder if it got downvalued during due-dilligence... reply shrubble 14 hours agoprevCould it be cold feet or fallout from the Crowdstrike debacle? reply kats 12 hours agoprevGood for Google. reply KoftaBob 12 hours agoprevFun fact: > Wiz was founded in January 2020 by Assaf Rappaport, Yinon Costica, Roy Reznik, and Ami Luttwak, all of whom previously founded Adallom > Adallom was founded in 2012 by Assaf Rappaport, Ami Luttwak and Roy Reznik, who are former members of the Israeli Intelligence Corps’ Unit 8200 https://en.wikipedia.org/wiki/Wiz_(company) reply xenospn 2 hours agoparent8200 veterans are highly sought after, and you can probably get a blank check from any VC if you have that on your resume. reply johnpython 11 hours agoparentprevnext [2 more] [flagged] yoavm 9 hours agorootparentwhat is American about them? reply shmatt 12 hours agoprevI have to take the other side than most comments here. Most of the coverage about the Wiz offer called out that this was an odd way for them to end up in - as the founders openly talked about waiting for that $1b ARR since almost day one To me it feels like Google was trying to put pressure on employees and any other non board option holders. There were dozens of articles and analysis of exactly bow many new millionaires / billionaires will be minted after the sale in Israel reply sofixa 10 hours agoparent> that this was an odd way for them to end up in - as the founders openly talked about waiting for that $1b ARR since almost day That doesn't mean much. I've worked at a company that kept taking about \"our goal is to reach magic number X\" and etc. until one they announce a sale. Situations change, business plans evolve, and money talks. reply mupuff1234 14 hours agoprevFrom the start this seemed more like a PR move from Wiz than a real offer. reply jmsflknr 16 hours agoprevOriginally covered here: https://news.ycombinator.com/item?id=41042004 reply warbaker 14 hours agoprevand nobody beats them! reply globular-toast 12 hours agoprevWhy would Google even consider this? They're an advertising company and haven't really made any money from anything but advertising. What would they do with this? reply sebastiennight 8 hours agoparentAdding a cloud security offering to your existing \"cloud infrastructure\" suite makes sense. Buying a strategic complement where you can multiply the company's ARR overnight by integrating with your own suite makes sense. Or commoditizing this complement by offering it for free as a differentiator to competitors (Azure, AWS) would also make sense. Plenty of options to choose from. reply animuchan 12 hours agoparentprevI suppose they'd add this to the GCP family of products, same as Apigee. reply krisgenre 12 hours agoparentprevMaybe this was about advertising after all ;). I now know there is a company named Wiz. reply KoftaBob 11 hours agoparentprevThe proportion of their revenue that comes from segments other than advertising is growing. I would imagine like most companies, Google/Alphabet wants to diversify their source of revenue/profit over time. https://www.statista.com/statistics/1093781/distribution-of-... reply anon5278525283 11 hours agoprevToday's (relevant) SMBC: https://www.smbc-comics.com/comic/investment-2 reply worthless-trash 15 hours agoprevThis is great news ! I use wiz for my home lighting and automation, I'm so glad that google did not buy it due to its habit of killing things that I find useful. I want my hardware to last longer than the current decision makers employment. Edit: hah, the site becomes available AFTER i submit so now I can read it. reply rando_person_1 15 hours agoparentHow are you using a cyber security platform for home lighting/automation? reply CaveTech 15 hours agoparentprevThis is not the same Wiz. reply samspenc 15 hours agorootparentDidn't realize there was a WiZ that specializes in home lighting https://www.wizconnected.com/en-us Per their About Us page, they are an \"IoT platform for smart lighting solutions and smart services\" and \"offer people connected lighting\". reply Marsymars 14 hours agorootparentThat's actually the only \"wiz\" I was familiar with in the tech space. They're Philip Hue's budget line for the past 5 years and they've got various partner brands (most the notably Walmart house brand) that use their smart platform. reply HaZeust 14 hours agoparentprevThis is wiz.io BUT I appreciate the enthusiasm reply jimt1234 14 hours agoprev [–] Why settle for a measly $23B, when a successful IPO will get them 10x that? reply 0x500x79 3 hours agoparent [–] There are a lot more factors that they will be subject to in an IPO that I think they could have avoided through this deal. IPOs/Stock market has not been kind to companies that do not have great margins and the current rumor mill has Wiz spending quite a lot in infrastructure costs to power their platform (a lot of it is built on Neptune and snapshot data transfer/processing is costly). At the end of the day there were a lot of employees up and down the organizational chart that would have been very happy with this deal. So I wish that we could see the inner workings of what went wrong. The constant rumor mill around Wiz keeps turning, and one starts to ask if there are nefarious actions at play. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Wiz has decided to abandon a $23 billion acquisition deal with Google and will pursue an Initial Public Offering (IPO) instead, as announced by CEO Assaf Rappaport.",
      "The decision was influenced by antitrust and investor concerns, with Wiz aiming for $1 billion in annual recurring revenue.",
      "Founded in 2020, Wiz has quickly grown, achieving $350 million in annual recurring revenue last year, and offers cloud security products."
    ],
    "commentSummary": [
      "Wiz has withdrawn from a $23 billion deal with Google, possibly due to Google re-evaluating the deal after an internal data review.",
      "Wiz claims they walked away because they believe their valuation is higher, though skepticism exists about their high valuation, reportedly over 60 times their Annual Recurring Revenue (ARR).",
      "The deal's collapse raises questions about Wiz's actual value and future, despite their rapid growth to $100 million ARR since their founding in 2020."
    ],
    "points": 161,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1721702579
  },
  {
    "id": 41046956,
    "title": "Intent to End OCSP Service",
    "originLink": "https://letsencrypt.org/2024/07/23/replacing-ocsp-with-crls.html",
    "originBody": "Today we are announcing our intent to end Online Certificate Status Protocol (OCSP) support in favor of Certificate Revocation Lists (CRLs) as soon as possible. OCSP and CRLs are both mechanisms by which CAs can communicate certificate revocation information, but CRLs have significant advantages over OCSP. Let’s Encrypt has been providing an OCSP responder since our launch nearly ten years ago. We added support for CRLs in 2022. Websites and people who visit them will not be affected by this change, but some non-browser software might be. We plan to end support for OCSP primarily because it represents a considerable risk to privacy on the Internet. When someone visits a website using a browser or other software that checks for certificate revocation via OCSP, the Certificate Authority (CA) operating the OCSP responder immediately becomes aware of which website is being visited from that visitor’s particular IP address. Even when a CA intentionally does not retain this information, as is the case with Let’s Encrypt, CAs could be legally compelled to collect it. CRLs do not have this issue. We are also taking this step because keeping our CA infrastructure as simple as possible is critical for the continuity of compliance, reliability, and efficiency at Let’s Encrypt. For every year that we have existed, operating OCSP services has taken up considerable resources that can soon be better spent on other aspects of our operations. Now that we support CRLs, our OCSP service has become unnecessary. In August of 2023 the CA/Browser Forum passed a ballot to make providing OCSP services optional for publicly trusted CAs like Let’s Encrypt. With one exception, Microsoft, the root programs themselves no longer require OCSP. As soon as the Microsoft Root Program also makes OCSP optional, which we are optimistic will happen within the next six to twelve months, Let’s Encrypt intends to announce a specific and rapid timeline for shutting down our OCSP services. We hope to serve our last OCSP response between three and six months after that announcement. The best way to stay apprised of updates on these plans is to subscribe to our API Announcements category on Discourse. We recommend that anyone relying on OCSP services today start the process of ending that reliance as soon as possible. If you use Let’s Encrypt certificates to secure non-browser communications such as a VPN, you should ensure that your software operates correctly if certificates contain no OCSP URL. Fortunately, most OCSP implementations “fail open” which means that an inability to fetch an OCSP response will not break the system. Internet Security Research Group (ISRG) is the parent organization of Let’s Encrypt, Prossimo, and Divvi Up. ISRG is a 501(c)(3) nonprofit. If you’d like to support our work, please consider getting involved, donating, or encouraging your company to become a sponsor.",
    "commentLink": "https://news.ycombinator.com/item?id=41046956",
    "commentBody": "Intent to End OCSP Service (letsencrypt.org)157 points by soheilpro 3 hours agohidepastfavorite67 comments notepad0x90 0 minutes agoCRLs don't scale, that's their problem and they take too long to update. Why isn't there a standard binary format for CRLs that is a cuckoo filter or a similar data structure? That way you don't have to worry about a CRL ballooning to gigabytes and you can expect clients to fetch the latest binary blob frequently. reply agwa 2 hours agoprevAfter I created OCSP Watch[1], I regularly detected CAs returning an OCSP response of unknown or unauthorized for certificates found in CT logs, indicating that they had basically forgotten that they had issued a certificate. I find that rather troubling. Indeed, OCSP Watch is currently reporting several forgotten NETLOCK certificates. (The certificates from other CAs are recently issued and will probably have OCSP responses provisioned in the near future.) CRLs can't be used to detect this, because they only list revoked certificates rather than providing a definitive status for every issued certificate. I do wish the root programs had merely removed the requirement to include an OCSP URL in certificates, but required OCSP URLs for every issuer to be disclosed in the CCADB. That would have solved the privacy problems and made OCSP responders much cheaper to operate, while continuing to provide transparency into the status of certificates. [1] https://sslmate.com/labs/ocsp_watch/ reply jaas 1 hour agoparentOCSP systems at scale are complex. At the core there is an on-demand or pre-signed OCSP response generation system, then there is usually an internal caching layer (redis or similar), then there is an external CDN layer. Just because the outer layer appears not to know about a certificate (a bug, to be sure), doesn't necessarily mean that other CA systems don't know about it. Requiring CAs to run OCSP makes running a CA more complex and expensive, which has considerable downsides like it does for any system, particularly as it is a zero sum game. Every dollar or hour of engineering time that Let's Encrypt spends on OCSP (and it is a considerable amount) is a dollar or hour not being spent on other parts of the CA. From the outside that may not be very visible because it's not obvious what isn't getting done instead but there is a real and considerable cost that is not worth it IMO. reply lokar 22 minutes agorootparentThey are so complex, and in practice unreliable, that my employer runs a caching proxy for our (non-browser) users (they mostly don’t want fail open). IMO it is unfixable and should go reply agwa 51 minutes agorootparentprevDo you disagree that OCSP would be significantly less costly and complex if the responder URL were not included in certificates, freeing the responders from having to serve hundreds of millions of clients? reply jaas 40 minutes agorootparentYes, I disagree. Best case scenario I think it would just allow us to get rid of the CDN layer. We'd still have to build and manage the rest with utmost care. Even that really depends on what the \"SLA\" expectation is. How many QPS would we be required to support before we're allowed to start dropping queries? If there's any possibility of a burst beyond what we can handle locally we'd keep the CDN, so maybe the CDN bill would be significantly smaller on account of lower volume but all the complexity and care of operation would still be there. reply nine_k 1 hour agoparentprevBTW what is the need for a CA to remember issued certificates? Certificates should work as long as they are not expired, not on a CRL, and while the same applies to their.CA's certificate used to sign it. The only good use for remembering issued certificates that I see is that a CA could detect a compromise if it's handed a certificate it does not remember issuing. It looks far-fetched to me, but I know nothing about CA operation. reply agwa 1 hour agorootparentWhen a CA has an incident, such as learning that one of their domain validation methods is flawed, they need to be able revoke all certificates impacted by the incident. Without a reliable database of certificates, there's no guarantee they'll find all of the impacted certificates. OCSP could fail closed in this situation. CRLs always fail open. Aside: it was in fact envisioned that OCSP could be used to detect CA compromises, and the BRs used to say \"The CA SHOULD monitor the OCSP responder for requests for 'unused' serial numbers as part of its security response procedures.\" I'm not sure how many CAs actually implemented that, and in any case I don't think it ever detected any compromises. reply nashashmi 36 minutes agorootparentThis central authority for certificate validation seems like extra infrastructure without which the internet fails. Once upon a time, internet communication was between two computers. Now there is a third computer to verify that the communication between two computers is legitimate. Is there another communication design that works without the need for a third computer? Edit: I don't think so. Identity validation of a public computer should be done by another well-trusted computer. reply devman0 12 minutes agorootparentCould dust off DANE here, the connection probably already included a DNS lookup, why not get the public key info from the same. reply jesboat 26 minutes agorootparentprevrequire sufficient info to identify the validation method to be included in an extension in the precert? reply phasmantistes 1 hour agorootparentprevCAs need to know every certificate they've issued so that a) if they receive a request to revoke that certificate, they can actually do so; and b) if they need to revoke all of their certs (e.g. due to discovering a validation process failure) they don't miss any. reply masklinn 1 hour agorootparentIsn't it more \"a lot\" of certs? Surely if they need to revoke all certs they can just revoke the intermediate. reply patrakov 48 minutes agorootparentThere is a middle ground between \"all certs\" and \"a few certs\". An example would be https://community.letsencrypt.org/t/revoking-certain-certifi... (2.4% of all certificates). This needs a full list of affected certificates but is too small to revoke the intermediate. reply masklinn 25 minutes agorootparent> There is a middle ground between \"all certs\" and \"a few certs\". And the GP used the word \"all\" which is what I was replying to. reply fragmede 38 minutes agorootparentprevit's exactly that. the thing is that if you're a CA, that's not farfetched at all. reply hlieberman 2 hours agoprevWhy not simply add the Must Staple restriction unconditionally to all certificates (aka \"status_request\")? That will eliminate privacy concerns -- no compliant TLS implementation should fetch a OCSP ticket given a stapled response -- while still allowing for broad non-browser support. reply blahgeek 2 hours agoparentI don't get the OCSP Stampling protocol: if the server needs / is able to to contact the CA frequently anyway, why not simply use very short-lived certificates? reply eqvinox 2 hours agorootparentIssuing a certificate has significantly more overhead (e.g. Certificate Transparency logs) than signing a statement that an existing certificate is still valid. reply briansmith 12 minutes agorootparentprevWhich CA's will issue short-lived certificates without negotiating a custom ($$$) contract with them? reply agwa 6 minutes agorootparentGoogle Trust Services lets you go down to 1 day using ACME. No payment required. reply agwa 2 hours agoparentprevMany web servers have terrible to nonexistent OCSP stapling implementations. reply eqvinox 2 hours agorootparentThis competes against terrible to nonexistent CRL checking on clients, but there are generally more clients than servers. reply citrin_ru 1 hour agoparentprevIn addition to listed below performance is another concern - OCSP stapling make TLS handshake large and slower. While it removes need for the client to query OCSP server itself in practice it such queries happen infrequently, asynchronously and in some may not happen at all. In theory a clients could include Certificate Status Request into the Client Hello only when a local cache expires but IIRC a few years ago Firefox did request OCSP on every TLS handshake making every handshake large if stapling is enabled on the server. reply fiddlerwoaroof 1 hour agoparentprevOCSP sounds to me like a really bad idea: certificate expiration is already a significant cause of issues and now we add a whole new way for certificates to break your site? reply mcpherrinm 2 hours agoparentprevWhile it would eliminate privacy concerns, it would also likely break an enormous number of websites which don't implement OCSP stapling, or don't implement it correctly. reply doctorpangloss 44 minutes agorootparentYou talk about it like there’s a vibrant community of websites out there. There’s streaming sites and social media, and then a bunch of apps that are delivered through the web. All of them are sophisticated web operations. reply fragmede 36 minutes agorootparentin practice I think you're right, but oof. reply eqvinox 2 hours agorootparentprevI'd suggest making browsers soft-enforce it with e.g. a yellow URL bar, giving server operators a window of time to fix things. reply mcpherrinm 1 hour agorootparentThe browsers, generally, are not onboard with improving OCSP and supporting OCSP stapling. Chrome already doesn't check OCSP at all. Apple and Firefox are all working on out-of-band revocation information based on CRL data. Presumably Chrome and its family are going to go down that path too, but I'm not sure I've heard commitments from them yet. Firefox has the best documentation on what's happening: https://blog.mozilla.org/security/2020/01/09/crlite-part-1-a... reply nick238 1 hour agorootparentprevI think with the whole menagerie of green locks, green bars & EV sites replacing/cluttering the address bar (meh [0]), only showing PSL+1 [1] (which actually does help security, but vocal power-users hate [2])...UI/UX designers are probably loathe to mess with it again. [0]: https://www.troyhunt.com/extended-validation-certificates-ar... [1]: https://blog.chromium.org/2020/08/helping-people-spot-spoofs... [2]: https://www.reddit.com/r/chrome/comments/h11gde/how_do_i_sto... [3]: https://www.wired.com/story/google-chrome-kill-url-first-ste... reply datadrivenangel 2 hours agoprev\"As soon as the Microsoft Root Program also makes OCSP optional, which we are optimistic will happen within the next six to twelve months, Let’s Encrypt intends to announce a specific and rapid timeline for shutting down our OCSP services. We hope to serve our last OCSP response between three and six months after that announcement. The best way to stay apprised of updates on these plans is to subscribe to our API Announcements category on Discourse.\" Interesting to see Microsoft dragging here. reply nick238 1 hour agoparentI don't see anything about OCSP policy changing in the Mozilla CA Program Policy release notes [1] or Chrome's [2], so I wonder if Microsoft was more proactive (for better or worse) in requiring it vs. other popular root programs. [1]: https://wiki.mozilla.org/CA/Root_Store_Policy_Archive [2]: https://www.chromium.org/Home/chromium-security/root-ca-poli... reply agwa 1 hour agorootparentMozilla and Chrome were requiring it via the incorporation of the Baseline Requirements into their policies. The removal of the requirement from the BRs won't appear in the release notes of their policies. reply jrochkind1 1 hour agoprevletsencrypt is the nonprofit community-focused infrastruture that we imagined the internet would consist of 20 years ago. I love you letsencrypt. reply klysm 1 hour agoprevCertificate management is an interesting problem along the intersection of human behavior and computer science that feels similar to BGP. In theory, it's simple, but when met with reality things get messy really really fast. reply c0balt 3 hours agoprevInteresting, how does the support for CRLs in web servers look? Afaik, NGINX and Apache only have OCSP stapling support. reply c0balt 2 hours agoparentNote: for nginx there is https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_... which needs to be pointed to a pem-encoded file with the CRL list (list of revoked certs). So no API-based configuration, like OCSP stapling, that just works. I can probably try to configure this with a cronjob/systemd timer but this is significantly less ergonomic. reply Asmod4n 2 hours agoparentprevWhy would CRL require web server support? reply c0balt 2 hours agorootparentTo verify client and server certificates. For certificate-based auth this is quite important, but it's also nice to, e.g., know that your server cert is not revoked. reply Asmod4n 2 hours agorootparentYour client does the checking of the server cert, no need for any configuration server side. If you use certs for client auth it’s unlikely it’s used on the web and only in a company setting where you control the PCs, where you could just use something more suited for that case. reply eqvinox 2 hours agorootparentprevIt's the client that is supposed to fetch the CRL. The server doesn't care unless it does client certificate authentication (which is incredibly rare due to poor UX.) reply hunter2_ 1 hour agorootparentA server (nginx / httpd) might very well make outbound HTTPS calls and want to verify the origin's certificate to the fullest extent possible. This is known as a reverse proxy. However, many -- certainly not all -- reverse proxy configurations use origin servers that reside on the same exact network, and therefore don't need TLS connections (or use them with `SSLProxyVerify none` for e.g. self-signed certs to simplify things [0]), but I digress. [0] https://httpd.apache.org/docs/current/mod/mod_ssl.html#sslpr... reply iso8859-1 2 hours agorootparentprevSo that admins find out that the certificate they have configured is no longer valid. reply mcpherrinm 2 hours agorootparentFor webservers, our recommendation is that your ACME client queries the ACME Renewal Information endpoint to find out if the certificate needs to be renewed: https://letsencrypt.org/2023/03/23/improving-resliiency-and-... reply eqvinox 2 hours agorootparentprevIf your certificate has been revoked and you don't know that (e.g. from a very alarming e-mail), something has seriously gone wrong. In the general case, it'd be yourself requesting the revocation to begin with. But even if it isn't, I'm pretty sure the CA is supposed to tell you expeditiously. reply phasmantistes 2 hours agorootparentNot all subscribers provide email addresses through which they could be informed of a revocation. reply Asmod4n 2 hours agorootparentprevYou’d need a monitoring solution for that, your web server won’t just send you a mail when it’s cert got revoked. The client does that for you, by checking back at the CA. No need for any configuration server side. reply remram 1 hour agoprevSo if I'm not using Chrome or Firefox, how do I check for revoked certificates? This just makes the web less open again. reply codegeek 2 hours agoprevCan someone ELI5 what does this mean for people using LetsEncrypt today with servers like Nginx or Caddy ? Do we need to make any changes to adjust ? reply mcpherrinm 2 hours agoparentFor regular webserver users, accessed by web browsers, no changes are needed. Note this is still a long ways out. At this time, only people who are writing code against OCSP need to be aware of the future roadmap. reply candiddevmike 2 hours agoparentprevThis mostly only impacts verification of Lets Encrypt certificates. No changes for ACME users. reply trillic 2 hours agoprevAre there any free or very inexpensive certificate providers which support ACME, DNS-01 challenges, and OCSP other than ZeroSSL? reply psz 2 hours agoparentacme.sh has a list in their Wiki: https://github.com/acmesh-official/acme.sh/wiki/CA reply agwa 2 hours agorootparentLet's Encrypt will surely not be the only CA to discontinue OCSP after Microsoft removes their requirement. reply Sohcahtoa82 2 hours agoprevI'm curious how much of a factor this is. How often do certificates get revoked before expiration? I would think the only reason to revoke a certificate is if it was compromised. reply agwa 2 hours agoparentCurrently only 20,830,034 revoked certificates per my daily CRL download, versus ~1 billion active certificates. But the number of revoked certs could balloon if there's another Heartbleed-like event, or a mass misissuance. reply mintplant 2 hours agorootparentSo are embedded devices just expected to not check for certificate revocation? reply woodruffw 2 hours agoparentprevCompromised, or mis-issued. In practice, currently, mis-issuance is significantly more common (in terms of # of certificates revoked) than compromise is. reply arsome 3 hours agoprevDisappointing to hear considering the limitations of CRLs - is there any intention to go forward with OCSP stapling or is that completely abandoned at this point? reply tialaramex 2 hours agoparentMy understanding is that stapling is the victim of the usual incompetence and laziness that infects a lot of systems where if one in a billion fail closed that would be considered a disaster but one in ten fail open is considered fine. You can't achieve meaningful security this way. The browser vendors have learned that you have to do it yourself or it won't be done well enough to be useful. So you pull every CRL, do a bunch of compression or other tricks, then give your users that data and now they have working revocation. When Bob's CA and Kebab Shop breaks their revocation stack, instead of dozens of poor individual users or web site owners confused and calling Bob's outsourced call centre in Pakistan with no sign of a fix, now a Google account exec asks Bob's CTO whether they forgot to say they were getting out of the CA business... I agree this isn't a desirable outcome, but it might be all we have. reply eqvinox 2 hours agorootparent> The browser vendors have learned that you have to do it yourself Cool. We already got the internet ossified on TCP + UDP, other L4 protocols just get stuck in firewalls and whatnot. Now we're progressing in ossification of HTTP.To be clear: this OCSP decision seems to be driven directly and only by web/HTTP consumers. Anything else is just not considered. reply eqvinox 2 hours agoprevI… er… what? First of all, privacy was one of the points of OCSP stapling. Second, this breaks all non-http applications in the sense that they could previously work through OCSP stapling which would be communicated in-line. CRLs need to be fetched by the client, generally over HTTP. Third, most non-browser TLS clients simply do not fetch CRLs, the implementation hurdle was too high. … I'm left seriously befuddled by this decision by Let's Encrypt [edit: or rather the CA/B forum] :( reply andrewaylett 2 hours agoparentOCSP pretty much has to fail open, and stapling doesn't fix this. If you're in a position to MITM a client using an exfiltrated certificate that no longer passes OSCP, you're probably also in a position to block the request to the OSCP server. And you're not going to staple while you MITM. As a client, you can't really tell the difference between \"this certificate is valid but not stapled and the OSCP server is down\" and \"this certificate isn't stapled because I'm being MITM'd, and the OSCP server is blocked\". For those who could successfully staple, really short-lived certificates might be a suitable answer -- that's effectively what OSCP gave you, only without actually ensuring that the certificate would cleanly expire. reply eduction 2 hours agoprev [–] It would be nice if one didn't need to be a TLS expert to understand the post -- particularly since the whole point of Let's Encrypt was to democratize TLS access. I have no idea if this means my setup will break even after consulting the docs of my ACME client. Can I still use ACME Tiny[1] with nginx? Any reason to think that will break? How can I tell if I'm using OCSP or CRL? Totally incomprehensible blog post. [1] https://github.com/diafygi/acme-tiny reply mcpherrinm 2 hours agoparentFor regular webserver users, accessed by web browsers, no changes are needed. Note this is probably at least a year, if not more, away. I'm sorry this post wasn't accessible enough, and we'll have more communications in the coming years as this gets closer. (I work at Let's Encrypt and proofread this post before publishing) reply phasmantistes 2 hours agoparentprev [–] Nothing will change for you, and nothing will break. The point of this post is to give a maximum-lead-time heads-up to the folks who _do_ need to care (the folks writing revocation-checking code in clients) so that later, more specific announcements don't come as a surprise. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Let’s Encrypt is ending support for Online Certificate Status Protocol (OCSP) in favor of Certificate Revocation Lists (CRLs) due to privacy risks and resource efficiency.",
      "This change will not affect websites or visitors but may impact some non-browser software; users are advised to ensure their software functions without an OCSP URL.",
      "The CA/Browser Forum's decision makes OCSP optional for publicly trusted Certificate Authorities (CAs), with Microsoft being the only exception; a timeline for shutting down OCSP services will be announced once Microsoft also makes it optional."
    ],
    "commentSummary": [
      "Let's Encrypt plans to discontinue its OCSP (Online Certificate Status Protocol) service due to issues with Certificate Revocation Lists (CRLs) and the complexity of OCSP systems.",
      "Proposals include using a binary format for CRLs to improve efficiency and mandatory OCSP stapling to address privacy concerns, though this could disrupt many websites.",
      "The shift away from OCSP may affect non-browser applications and embedded devices, but users of standard web servers like Nginx or Caddy should not need immediate changes."
    ],
    "points": 157,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1721748314
  },
  {
    "id": 41042753,
    "title": "The Linux audio stack demystified (and more)",
    "originLink": "https://blog.rtrace.io/posts/the-linux-audio-stack-demystified/",
    "originBody": "The Linux audio stack demystified 2024-07-20 Ruffy Linux 🎶 Audio PipeWire JACK PulseAudio ALSA Sound Sampling Quantization Frequency Basics 1. Introduction 2. Exclaimer 3. Understanding Sound and Sound Waves 4. How humans process sound waves 5. How Digital Audio Works 5.1. Audio-Input and Sampling 5.1.1. Nyquist-Shannon 5.1.2. Quantization 5.1.3. Storing Digital Audio signals? 5.1.4. Mono and Stereo 5.2. Audio-Output 6. The Role of a Sound Card / Audio Interface 6.1. What makes a good Sound Card / Audio Interface? 6.2. The Stairstep Fallacy 7. The Linux Audio Stack 7.1. ALSA (Advanced Linux Sound Architecture) 7.2. JACK 7.3. PulseAudio 7.4. PipeWire 7.5. So what is it a sound server does? 7.5.1. Mixing multiple input streams 7.5.2. Multiple Output Streams/Mixes 7.5.3. Volume Control per input stream 7.5.4. Virtual Outputs 7.5.5. Sound effects for streams 7.5.5.1. Example: Using a Compressor 7.5.5.2. Example: Adding Reverb 7.5.5.3. Example: Applying an Equalizer 7.5.5.4. Example: Denoising 7.5.5.5. Combining Multiple Effects 7.5.6. Providing an API for application developers 7.6. Which sound server is better for me? Introduction Digital audio processing is a complex yet fascinating subject. Getting a deeper understanding requires knowledge in multiple technical disciplines, including physics, electrical/electronical engineering, and some software engineering when it comes to parts of the Linux ecosystem. This article aims to demystify the Linux audio stack by explaining the basics of sound, how humans perceive sound, and the workings of digital audio. We will then dive into the components that make up the Linux audio stack and explore how these interact. Buckle up; it’s going to be a long and informative read. Exclaimer In this article, I will simplify concepts and ideas to provide you with the basic knowledge needed to follow along. Each chapter could easily be expanded into a dedicated, lengthy blog post. However, for simplicity’s sake, I have refrained from doing so. If you would like further details about a specific chapter, please feel free to contact me or leave a comment in the comment section below. I’ll be happy to go into more detail if needed/wanted. Understanding Sound and Sound Waves Before deep-diving the technicalities of audio processing on Linux, it’s essential to understand what sound is. In a nutshell, sound is a vibration that propagates as an acoustic wave through a medium such as air. These waves are created by vibrating objects (such as speakers, headphones, piezo buzzers, etc.), causing fluctuations in air pressure that our ears can pick up and perceive as sound. Audio waves can have many shapes, the most common waveform is the sine wave. It is the fundamental building block for all other waves existing, and thus it’s important to learn about the sine wave first. All waveforms have 2 fundamental key properties: Period (duration): The time it takes the wave to perform a single oscillation. Amplitude: This represents the wave’s strength or intensity, which we perceive as volume. Higher amplitudes generally correspond to louder sounds. We can derive the Frequency from the period duration of a wave form. The frequency is measured in Hertz (Hz), and it represents the number of oscillations (waves) per second. Higher frequencies correspond to higher-pitched sounds. In other words, the shorter the period duration, the higher the pitch. The higher the frequency the higher the pitch. Thus, the frequency is the inverse of the period duration, or in mathematical terms: f = 1 / (period duration[s]) = (period duration[s])⁻¹. Amplitudes can be measured as the pressure variation in the air - the so called Sound Pressure Level (abbreviated as SPL). The standard unit for pressure is Pascal (Pa). In the context of audio, we often deal with very small pressures, so micropascals (µPa) are typically used. However, the most common way of describing amplitude is Decibels (dB). Decibels are a logarithmic unit used to describe the ratio of a particular sound level to a given reference level. Decibels are the preferred unit, because the human ear perceives sound pressure levels logarithmically. dB SPL: Sound pressure level in decibels. In air, the reference pressure is typically 20 µPa (typically referred to as P0), the threshold of human hearing. It is calculated using the formula SPL = 20 * log₁₀(P/P0), where P is the measured pressure and P0 is the reference pressure (20 µPa). dBV and dBu: When sound is converted into an electrical signal by a microphone, the amplitude can be measured in volts (V). This is common in audio equipment and recording. dBV and dBU are units for voltage measurements in audio systems, referencing 1 volt for dBV or 0.775 volts for dBu. Similarily to db SPL, dbV and dBu can be calculated using the same formula, just exchange the reference pressure with a reference voltage. dBV = 20 * log₁₀(V/V0), where Vis the measured voltage andV0` is the reference voltage. You might wonder how a sine wave actually sounds like? Here’s an example of a sine wave at a frequency of 440 Hz. Please check your volume before you play! How humans process sound waves Sound waves are captured by the pinna, the visible part of the outer ear. The pinna helps direct sound waves into the ear canal. The sound waves travel through the ear canal and reach the eardrum (tympanic membrane). The sound waves cause the eardrum to vibrate (very similar to sound waves bring microphone membranes to vibrate). These vibrations correspond to the frequency and amplitude of the incoming sound waves. The vibrations from the eardrum are transmitted to the three tiny bones in the middle ear, known as the ossicles (malleus, incus, and stapes). The ossicles amplify the vibrations and transmit them to the oval window, a membrane-covered opening to the inner ear. The vibrations pass through the oval window and enter the cochlea, a spiral-shaped, fluid-filled structure in the inner ear. Inside the cochlea, the vibrations create waves in the cochlear fluid, causing the basilar membrane to move. The movement of the basilar membrane stimulates tiny hair cells located on it. These hair cells convert mechanical vibrations into electrical signals through a process called transduction. Different frequencies of sound stimulate different parts of the basilar membrane, allowing the ear to distinguish between various pitches. The hair cells release neurotransmitters in response to their movement, which generates electrical impulses in the auditory nerve. These electrical signals travel along the auditory nerve to the brain. The auditory signals reach the auditory cortex in the brain, where they are processed and interpreted as recognizable sounds, such as speech, music, or noise. Some “Hard Facts” to better understand the dimensions of sound waves: The typical frequency range of human hearing is from ~20 Hz to ~20.000 Hz (20 kHz). This range can vary with age and exposure to loud sounds. The frequency of humans speaking typically falls within the range of ~300 Hz to ~3.400 Hz (3.4 kHz). Human ears are most sensitive to frequencies between 2,000 Hz and 5,000 Hz. The quietest sound that the average human ear can hear is around 0 dB SPL (sound pressure level), equivalent to 20 micropascals. The loudest sound that the average human ear can tolerate without pain is around 120-130 dB SPL. Sounds above this level can cause immediate hearing damage. The range between the threshold of hearing and the threshold of pain is known as the dynamic range of human hearing, which is approximately 120 dB. How Digital Audio Works Now, that we got a rudimentary understanding of sound waves in the analog world work, let’s continue with sound waves in a world of zeroes and ones. When we talk about audio in a digital context, we’re referring to sound that’s been captured, processed, and played back using computers. Audio-Input and Sampling Sampling to computers is what hearing is to humans. Sampling is the process of converting analog sound waves into digital data that a computer can process. This is done by taking regular measurements of the amplitude of the sound wave at fixed intervals. The rate at which these samples are taken is called the sampling rate and is measured in samples per second (Hz). Common sampling rates include 44.1 kHz (CD quality) and 48 kHz (professional audio). Each lollipop on the sine wave represents a single discrete sample/measurmeent. The first oscillation has a higher sample rate, the second oscillation has a lower sample rate for demonstration purposes only. Typically, the sample rate is constant, and more than double of the highest frequency humans can hear. Nyquist-Shannon The Nyquist-Shannon Sampling Theorem provides a criterion for determining the minimum sampling rate that allows a continuous signal to be perfectly reconstructed from its samples. A continuous-time signal that has been bandlimited to a maximum frequency fmax (meaning it contains no frequency components higher than fmax) can be completely and perfectly reconstructed from its samples if the sampling rate fs is greater than twice the maximum frequency of the signal. Mathematically, this can be expressed as: fs > 2 * fmax This nicely explains why common sampling rate are typically a little bit more than double of the human hearing range (e.g. 44.1 kHz for CD quality). Quantization In the analog world, signals have infinite precision. For example, if you zoom in on a sine wave, you can always find more detail — the waveform continues to be smooth and precise no matter how close you zoom in. However, in the digital world, precision is limited. To understand this limitation, consider the question: “How many numbers can you represent between 0 and 1?” If you use 0.1 as the smallest possible increment value, you can only represent 10 discrete values between 0 and 1. If you refine the smallest increment to 0.01, you get 100 values between 0 and 1. In the analog realm, there’s no smallest possible increment; values can be infinitely precise. But in digital systems, precision is constrained by the number of bits used to represent the signal. Digital audio uses a fixed number of bits to encode each sample of sound. For instance, with 16-bit audio, there are 65,536 discrete levels available to represent the amplitude of each sample. This means that the smallest difference between two levels, or the smallest possible increment, is approximately 0.000030518 for 16-bit audio. Professional audio will typicall utilize 24-bit. Everything higher than that is usually bogus. Bogus where only audiophiles will hear a difference. The process of approximating each sample’s amplitude to the nearest value within these discrete levels is known as quantization. The number of discrete levels depends on the bit depth of the audio. For example: 16-bit audio provides 65.536 possible amplitude values. 24-bit audio offers 16.777.216 possible amplitude values. Quantization is essential because it allows us to represent analog signals with a finite number of discrete values, making digital storage and processing feasible, even though we sacrifice some level of precision compared to the analog world. Once again, for all the audiophiles reading this: 24-bit resolution is enough! No, you don’t hear the difference between analog signals and 24-bit digital signals. Storing Digital Audio signals? With an understanding of sampling and quantization, we can continue to explore how digital audio can be stored. For illustrative purposes, let’s consider designing a basic audio format to store digital audio, using the simplicity of the CSV (Comma-Separated Values) format as our foundation. Although this proposed format would be inefficient compared to established audio formats like MP3, WAV, or FLAC, it serves as a useful exercise in understanding the principles behind audio file storage. CSV files are a well-known format (for software developers as well as spreadsheet power users), used to store tabular data in an easily readable manner. CSV files consist of rows separated by newlines and columns separated by commas, making them an ideal starting point for conceptualizing audio storage. The key here is to recognize that a sampled audio signal can be represented like table with data for a line-chart in a spreadsheet, where each row corresponds to a discrete sample in time. Let’s define a CSV-based audio format with the following structure: Sample Index: Each row represents a discrete sample point. For example, in the case of a sine wave, each row corresponds to a specific point along the waveform. Typically the sample index is just a self-incrementing number. Quantized Value: The second column contains the quantized amplitude value for each sample. for simplicity sake, we assume that the sample rate is 44.1 kHz (this information is needed for later playback) and the bit depth is statically set to 16 bit. a simple CSV file could like this: Sample Index, Quantized Value 1, 0.32767 2, 0.32780 3, 0.32809 4, 0.32853 ... csv Copy While CSV is of course not a practical choice for real-world audio storage due to its inefficiency and lack of support for metadata, compression, multiple channels, and other critical features, this format helps to illustrate how digital audio data can be organized and stored. Real-world formats like MP3, WAV, and FLAC utilize more sophisticated techniques to handle audio data more efficiently, like applying Compression to reduce the file size (in the best case: while maintaining audio quality) Metadata, for storing additional information about the audio file, such as title, artist, and duration, sample rate, bit-depth Error Correction for ensuring data integrity during storage and playback. as well as support for multiple channels (e.g. for stereo signals or 5.1/7.1 channels for cinematic sound systems) Mono and Stereo Mono (short for monaural) refers to audio that is recorded and played back through a single channel. In a mono audio setup, all sounds are combined into a single channel, and this single channel is then played through one or more speakers or headphones. The primary advantage of mono audio is its simplicity and the uniformity it provides — sound is uniformly distributed regardless of how many speakers are used. This makes mono suitable for applications where spatial sound is not crucial, such as telephone conversations. Even audio on bigger venues is typical mono. On the other hand, stereo (short for stereophonic) involves two audio channels: left and right. Stereo recording captures sound from two distinct channels, creating a sense of spatial separation between sounds. This allows for a more immersive listening experience because different sounds can be directed to different speakers or headphones. For instance, in a stereo mix, you might hear a guitar predominantly through the left speaker and a vocal through the right speaker. This spatial separation mimics the way humans (with typically only 2 ears) naturally hear sounds in real life, making stereo effective for music, movies, and other applications where a realistic sound field enhances the experience. Audio-Output We’ve learned a lot about sound waves, and how sound waves can be captured and stored. Now it’s time to move forward and think about how digitally stored audio is played back. As discussed in the previous section, digital audio data is typically stored as a sequence of discrete samples that represent the amplitude of a sound wave at various points. This data is then streamed to your computers sound card. A DAC (abbreviation for Digital-To-Analog) converter converts these digital values into a continuous analog signal. This conversion process typically involves 2 steps. First, the digital samples are mapped to discrete voltage levels. In case your sound card outputs line-level, the highest possible digital representable value would map to 0.447V (447 mV). To smooth out the discrete steps and reconstruct a continuous waveform, the DAC uses a low-pass filter. This filter removes high-frequency artifacts (often referred to as “sampling noise”) that result from the discrete nature of the digital data. The Low Pass Filter is typically filtering out every frequency above ~22 kHz. Finally, the filtered analog signal is sent to the output stage, which then drives the speakers or headphones. The Role of a Sound Card / Audio Interface A sound card handles the input and output of audio signals, converting between analog and digital forms. Sound cards typically include ADCs for recording, DACs for playback, and additional circuitry for audio processing (such as amplification, filtering, noise-removal, etc. …). In a nutshell sound cards act as the ears and the mouth of your computer. It handles all the inputs, like microphones, line-inputs and makes it consumable from software running on your computer. And then it also handles output of digital audio to analog systems such as your speakers and your headphones. The most basic sound card consists out of the following components: ADCs (Analog-to-Digital Converters): Converts analog signals from microphones or other input devices into digital data. DACs (Digital-to-Analog Converters): Converts digital audio data into analog signals for playback through speakers or headphones. Amplifiers: Boosts the audio signal to a level that can drive speakers or headphones. What makes a good Sound Card / Audio Interface? Often times I get asked what audio interface is a good audio interface. I usually don’t want to recommend a particular brand, but explain people what they should look for when comparing audio interfaces to find one that suits their needs. Here’s a short and opinionated set of aspects you should look out for. Signal-to-Noise Ratio (SNR): A high SNR indicates that the sound card produces clear audio with minimal background noise. Look for a sound card with a high SNR, typically 100 dB or higher. Total Harmonic Distortion (THD): Low THD means that the sound card introduces minimal distortion to the audio signal. Good sound cards often have THD ratings below 0.01%. Dynamic Range: A wide dynamic range allows the sound card to accurately reproduce both very quiet and very loud sounds without distortion or noise. Sampling Rate: A higher sampling rate allows for more accurate representation of audio. Common high-quality sound cards support rates up to 192 kHz or higher. Bit Depth: A higher bit depth enables better resolution of audio details. Good sound cards support 24-bit depth, providing a broader range of amplitudes and finer detail. Low Latency: Good sound cards have low latency, which is essential for real-time applications like gaming, live recording, and professional audio production. Latency is the delay between input and output, and lower latency ensures more immediate audio responses. The Stairstep Fallacy Source: Invidious on yt.artemislena.eu provided by artemislena The Linux Audio Stack Now that we’ve covered the basics, let’s have a look at how Linux manages audio. The Linux audio system utilizes a modular and layered architecture to handle audio processing, which provides both abstraction and flexibility. Layering in Linux audio allows for a structured approach to audio management. By separating concerns into different layers, the system abstracts the complexities of lower-level operations, such as direct hardware interactions. This separation means that user-space applications do not need to deal with the specifics of hardware configurations, streamlining development and improving compatibility. Modularity complements the idea of layering by allowing individual components to be swapped or reconfigured without disrupting other layers in the system. This means that different layers or components can be replaced or updated as needed. For example, if you plug in a headset, the system can dynamically switch from speaker output to headset output thanks to its modular design. Similarly, if you change your sound card or audio driver, the modular system can adapt without requiring much reconfiguration. ALSA (Advanced Linux Sound Architecture) Let’s start with the foundation of all Linux systems when it comes to audio. ALSA is the core layer of the Linux audio stack. It provides low-level audio hardware control, including drivers for sound cards and basic audio functionality. ALSA provides a standardized interface for audio hardware, allowing applications to interact with sound cards and other audio devices without needing to know the specifics of the hardware. It includes a set of drivers for different types of sound cards and audio interfaces. These drivers translate the high-level audio commands from applications into low-level instructions understood by the hardware. ALSA operates in the kernel space, meaning it interacts directly with the Linux kernel. This position allows it to manage hardware resources and perform low-level audio operations efficiently. To allow the user space from profiting from ALSA too, ALSA provides libraries and tools in user space, such as libasound (the ALSA library), which applications can use to interface with the ALSA parts in the kernel space. This library offers functions for audio playback, recording, and control. ALSA supports multi-channel audio, enabling more advanced configurations like 5.1 or 7.1 surround sound. Additionally, it is designed to offer low-latency audio, which is essential for real-time applications like music production. Further, ALSA includes a mixer interface that allows users to control audio levels and settings, such as adjusting volume or muting channels. JACK The Jack Audio Connection Kit - better known as JACK - is a professional-grade audio server designed for UNIX-oid operating systems, including Linux. It is tailored for real-time, low-latency audio processing and is widely used in professional audio production. JACK is designed to provide low-latency, real-time audio. This is essential for applications where timing and synchronization are critical, such as live music performance, recording, and audio production. It allows multiple audio applications to connect and communicate with each other. This means that the output of one application can be flexibly routed to the input of another, enabling complex audio processing chains. JACK excels in delivering minimal latency. This ensures that audio signals are processed and transmitted with minimal delay, maintaining synchronization and responsiveness across multiple tracks/applications outputting audio at the same time. It offers sample-accurate synchronization between audio streams, ensuring precise timing and phase alignment across different audio applications and devices. JACK uses a client-server model, where the JACK server manages audio data and clients (like your desktop applications) connect to it for audio processing. This modular approach enables flexibility and scalability in audio setups. JACK includes transport control features that allow users to synchronize playback and recording across multiple applications, making it easier to manage more complex audio projects. JACK builds on top of ALSA and requires ALSA to be installed and configured correctly. PulseAudio PulseAudio is a sound server for Linux and other UNIX-oid operating systems that sits on top of the lower-level ALSA. It provides a higher-level interface for audio management, offering additional features beyond those available through ALSA. PulseAudio acts as an intermediary between applications and the underlying sound hardware. It handles audio streams, mixing, and routing, allowing multiple applications to produce sound simultaneously and manage their audio independently. One of PulseAudio’s key features is its ability to stream audio over a network. This allows users to play audio from one machine on another machine or device connected to the same network. PulseAudio enables the mixing of multiple audio streams into a single output. This means you can listen to music while receiving notifications from other applications, all mixed seamlessly into your speakers or headphones. Additionally, it allows for independent volume control of each application. This means you can adjust the volume of your music player separately from your web browser or other applications. PulseAudio supports various audio effects and processing features, such as equalization, sound enhancements, and echo cancellation. Further, it manages audio devices and allows easy switching between them. For instance, you can switch audio output from speakers to headphones or to an external Bluetooth device without disrupting playback. PulseAudio provides a more intuitive and user-friendly way to manage audio compared to working directly with ALSA. Its ability to handle various audio sources and outputs simultaneously, along with network capabilities, makes it versatile for different audio needs and setups. By providing advanced features such as audio mixing, device switching, and network streaming, PulseAudio contributes to a richer and more flexible audio experience on Linux systems - particularly the Linux Desktop. While it’s a little bit of a meme, PulseAudio made “the year of the Linux desktop” a lot more likely. PipeWire PipeWire is a modern multimedia framework designed for Linux systems that aims to unify and enhance the handling of both audio and video streams. Developed as a replacement for both PulseAudio and JACK, PipeWire provides a flexible and efficient infrastructure for managing complex multimedia tasks. PipeWire combines the capabilities of audio servers like PulseAudio and low-latency audio servers like JACK into a single framework. It also supports video processing and capture, making it a comprehensive solution for managing both audio and video streams. Like JACK, PipeWire offers low-latency audio processing, which is crucial for professional audio applications such as live music production and sound design. PipeWire is designed to be highly flexible, accommodating various use cases ranging from consumer audio and video to professional audio production and due to its video capabilities also video conferencing. It provides a unified API that can handle both audio and video streams, simplifying development and integration for applications that need to process multimedia content. Further, PipeWire is engineered from the ground up to deliver low-latency audio and real-time performance, making it suitable for high-performance audio tasks and complex multimedia workflows. Finally, PipeWire includes features for improved security and sandboxing, allowing applications to access only the resources they need and protecting against potential security vulnerabilities. By combining audio and video management into a single framework, PipeWire provides a more integrated and cohesive approach to handling multimedia content. Its design incorporates modern requirements and use cases, including enhanced security, better real-time performance, and support for a wide range of multimedia applications. PipeWire also offers compatibility with existing audio and video servers, easing the transition from older systems and allowing a more gradual adoption. To give an example, even if one of your applications is built to use PulseAudio, PipeWire supports a PulseAudio compatible input API, so that an application does not have to care if it’s PulseAudio or PipeWire. A similar input API exists also for JACK - PipeWire will emulate the Sound Server API of JACK. Unlike PulseAudio and JACK, PipeWire does not require ALSA user-space libraries at all. Often times user space ALSA applications are re-routed to PipeWire for sound output and input So what is it a sound server does? Mixing multiple input streams Briefly summarized, a sound server acts as an intermediary that takes multiple audio input streams with varying sample rates and bit depths, mixes them into a single output signal, and sends it to the sound card. For example, imagine one application generating a sine wave and another producing a rectangular wave. In today’s world, it would be unimaginable not to be able to watch a video while simultaneously receiving notification sounds from your messenger (or more generally to receive audio from multiple input sources). This is where the sound server implementations such as PipeWire, JACK and PulseAudio come in — a sound server combines the two (or more) input signals into one output signal and outputs it to the sound card. The sound server achieves this by summing each sample of the input signals, effectively merging multiple inputs into a single output. The operation performed is sample-wise addition. In case a faster-sampled signal needs to mixed with a lower-sampled signal, the audio server will also have to interpolate the values for the signal with the lower sample rate. All this is happening in real time with little to no latency introduced (depending on the buffer size of the mixer buffer). Multiple Output Streams/Mixes If you own a laptop, there’s high chance you have 2 (or more) audio output devices. One Headphone output if you connect your headphones to the audio output jack. And one for the built-in speakers in your laptop. A sound server can also output audio to multiple outputs at the same time. Let’s say you prefer listening to music through your headphones, but friend is with youoand you’d like to show them a cool song you enjoy. So you decide to activate output on both devices at the same time. Volume Control per input stream Typically each input stream comes from different applications on your Linux Desktop. And sometimes you’d like to individually control volume for each of your applications. Let’s say you’re watching a movie and you’d like to mute all messengers in the meanwhile. You can configure the sound server to reduce/mute the volume of the corresponding input streams of your messengers. Virtual Outputs Not every signal needs to be routed to an actual output. Think about your microphone. Most of the time the microphone is not outputted on your speakers/headphones, yet the input stream is provided to applications such as your favorite VOIP/conferencing tool. The concept of virtual outputs allows you to create an arbitrary number of output streams that applications can consume. Sound effects for streams A sound server enables you to apply various effects to individual input streams or output streams using effect plugins. These plugins, available in formats such as LADSPA, CLAP, VST2, VST3, AU, and RTAS, allow you to integrate software effects into your audio routing chain. Example: Using a Compressor One practical example of an effect plugin is a compressor. If you often find yourself adjusting the volume while watching movies—turning it down during loud action scenes and back up for quieter dialogue—a compressor can help. By adding a compressor to the input stream of your media player or browser, you can reduce the volume of the loud parts without affecting the quieter sections. Most compressors include a make-up gain feature to increase the overall volume after compression, ensuring a balanced listening experience. Example: Adding Reverb Another use case is enhancing your microphone input with a bright reverb effect, making it sound as though you’re speaking in a large room. This can add a sense of space and presence to your voice. Example: Applying an Equalizer You might also want to use an equalizer to adjust your output stream. For instance, if your speakers produce a muddy bass around 200Hz, you can use an equalizer to reduce the gain in that frequency range, resulting in clearer sound. Example: Denoising If your microphone has a lot of background noise, you can apply a denoising effect to clean up the signal before it reaches your VOIP tool. This ensures that your voice comes through clearly during calls. Combining Multiple Effects For more advanced setups, you can combine multiple effects like reverbs, delays, compressors, and equalizers into your audio chain. Whether you need simple adjustments or complex processing, a robust sound server provides the flexibility and control you need. The modular and layered design of the Linux audio stack, combined with the wide array of available plugins, allows you to customize your audio experience to suit your specific needs. If you’re looking to enhance your media playback, improve your microphone input, or create a unique sound environment, the Linux audio stack has the tools to make it happen. Providing an API for application developers Another important aspect of a sound server is to provide a convenient API for application developers to consume. The following Python script demonstrates how the ALSA API can be used to playback a *.wav file. import audioop import alsaaudio # Initialize ALSA-Audio-Output audio_output = alsaaudio.PCM(alsaaudio.PCM_PLAYBACK) audio_output.setchannels(1) # mono audio_output.setrate(44100) # 44.1 kHz audio_output.setformat(alsaaudio.PCM_FORMAT_S32_LE) with open('example-audio-file.wav', 'rb') as wav_file: try: audio_output.setperiodsize(160) data = wav_file.read(320) while data: # Write read bits of audio file to sound card audio_output.write(data) data = wav_file.read(320) except Exception as exc: print(exc) python Copy Which sound server is better for me? To help you understand which sound server suits your needs the best, I tried to collect some use-cases and rated the corresponding implementations capability to fulfill these use-cases. Note, that the rating might be a very opinionated rating, and you might disagree. Let’s discuss in the comment section below! Use Case JACK PulseAudio PipeWire Professional Audio Production ★★★★★ ★★★ ★★★★☆ Real-Time Audio Processing ★★★★★ ★★ ★★★★☆ Low-Latency Audio ★★★★★ ★★ ★★★★☆ Live Music Performance ★★★★★ ★★ ★★★★☆ General Desktop Audio ★★ ★★★★★ ★★★★★ Network Audio Streaming ★★★★ ★★★ ★★★★☆ Bluetooth Audio Devices ★★ ★★★★ ★★★★★ Video Conferencing - - ★★★★★ Screen Recording - - ★★★★★ Audio Routing Between Apps ★★★★★ ★★★ ★★★★★ Multi-User Audio Management ★★★ ★★★★ ★★★★★ Security and Sandboxing ★★★ ★★★ ★★★★★ Ease of Configuration ★★ ★★★★★ ★★★★☆ Compatibility with Consumer Apps ★ ★★★★★ ★★★★★ Handling MIDI ★★★★★ ★★★ (requires FluidSynth) ★★★★☆ High-Performance Gaming Audio ★★ ★★★★ ★★★★☆ Podcasting and Broadcasting ★★★★☆ ★★★★ ★★★★★ Remote Audio Processing ★★★★☆ ★★★ ★★★★☆ Developing Audio Applications ★★★★★ ★★★ ★★★★★ Effect Support (LADSPA, etc. …) ★★★★☆ ★★★ ★★★★★ PipeWire is emerging as a versatile and powerful option that integrates the best features of both JACK and PulseAudio, making it suitable for a wide range of use cases from general desktop audio to professional audio and video production. JACK remains the best choice for specialized professional audio tasks requiring the lowest latency and real-time performance. PulseAudio continues to excel in general desktop and consumer audio applications, offering ease of use and broad compatibility. The Linux distribution you’re using likely employs either PipeWire or PulseAudio. RHEL-based are on PipeWire already (e.g. Fedora), most Debian-based are still on PulseAudio, but most of the time PipeWire is installable through your package manager. If you’re satisfied with your current audio stack, there’s no need to change it. However, if you seek lower-latency capabilities, consider upgrading to PipeWire. Generally speaking I’d not recommend transitioning to JACK unless you’re already familiar with its configuration and setup. PipeWire will eventually completely replace PulseAudio and JACK, as from an architectural and technical view it is a lot more capable. Linux 🎶 Audio PipeWire JACK PulseAudio ALSA Sound Sampling Quantization Frequency Basics Thank you for reading ❤ I hope you enjoyed reading this article. Maybe it was helpful to you, maybe it was not? Maybe you learned something new? You disliked the article? I'd be glad to hear your opinions. Your feedback is much appreciated and very welcome. Either (anonymously) write a comment in the section below, or send a mail to blog@rtrace.io Comments 💬 Comment Editor Name Log inPost",
    "commentLink": "https://news.ycombinator.com/item?id=41042753",
    "commentBody": "The Linux audio stack demystified (and more) (rtrace.io)144 points by ruffyx64 13 hours agohidepastfavorite121 comments kmarc 11 hours agoI hear many complaining (even here) about \"the mess\" of linux audio. First, in the article, [1] shows in one single diagram where the complexity is coming from; the audio system has to handle a good deal of different hardware on many different systems and also provide extra functionality for multiplexing, network features, wireless headsets and their codecs, etc. All this: open source. Second: linux is the only platform where everything works right now flawlessly for me: My Bose joins without any problems, switches to headset mode during zoom calls, switches back to high definition audio otherwise. I can select a different sink, even networked, whenever I want the output to appear on a different networked device. MacOS sometimes needs a reboot so that the bluetooth subsystem works, what the hell. And all this worked with PulseAudio, and now works with Pipewire, which is an even higher quality iteration of PA. I don't complain. I wish MacOS/Windows had such a versatile, configurable, but sanely-working-out-of-the-box audio system as an off-the-shelf Fedora, or even freaking Arch linux has. HTH [1]: https://blog.rtrace.io/images/linux-audio-stack-demystified/... reply woolion 11 hours agoparentI wholeheartedly agree, but the frustrations are quite understandable. If you want to do something somewhat advanced and you have to care about Jack, Alsa, and then the Pulse or Pipewire layers, it's quite overwhelming (why would you have to know the Linux audio history to work with it). PulseAudio was quite buggy when it was considered ready for general public by Ubuntu. I kept using some scripts to do what it could theoretically do for 2/3 years, and then it didn't have any quirks anymore. Now that it is replaced by the pipewire stack, audio bugs are back. For instance, on a laptop if system audio is set to 'output' instead of duplex, switching between speakers and headphones does not work, and volume change would sometimes get stuck. And sometimes it duplicates the streams on another system, which can be solved by killing the daemon. Even with this less stable state, I agree that this is nothing compared to how bad the situation is in Mac or Windows world. In the professional environments I've been in, basically all people have given up on the system properly switching their device parameters correctly. So calls often need a few minutes to reconfigure audio. Same with external screen switching. reply kristopolous 10 hours agoparentprevFor me it's been nothing but problems. The primary reason is I'm probably doing things that (almost) nobody else is doing but I assume both countless people are and it should be working fine. Once I hook up some midi devices, want things to be recorded, run a synthesizer stack through pw-jack, expect the midi clock to go down the USB bus, etc, all kinds of interesting behavior starts happening. It even completely locked up my machine a few times. Like hard freeze. I had Audacity just nope right out. It somehow corrupted the local configuration and I had to blow it away to get it to start up again. And then there's the whole new suite of programs you have to learn where their implementation is constantly in flux so the documentation isn't exactly accurate. pw-record for instance. That \"--list-targets\" option it tells you to use is long gone (that princess is now in the \"wpctl status \" castle). You gotta check the date of everything written online because the month it was written matters. It's still far from great. I used an Amiga about 30 years ago to do similar things. Now that was something that genuinely just worked. People are still using it. That's how functional it was. But like all these things, I should find the motivation to shutup the complaining and get to cracking on the code to make it suck less. When it comes to linux and things are broken, your assumption on how many people have seen it and who is working on the section of code it's caused by is invariably an order of magnitude or two too high. That's why you can't find any fixes on the web. You're one the first to see it. Exciting, isn't it? reply opyate 9 hours agorootparentIt would be helpful if folks on here who say \"works for me\"/\"doesn't work for me\" would indicate which flavour/version of Linux they use. Some distros don't have the latest pipewire stack, and you're left to fend for yourself, having to follow some incomplete or poorly written blog post to side-step what your distro does and put pipewire on top. Me: Ubuntu 24.04 LTS, and happily using carla, ardour, lmms and a bunch of midi devices with pw-jack. (I'm aware pw-jack is not required anymore, but that was my old workflow, and it speaks to the backwards-compatibility that the stack offers.) reply kristopolous 8 hours agorootparentUnderstood but I thought it was clear at least from my post that this isn't a distribution problem. I've certainly tried the distribution version, building from source, and various other things. I have a counter-request: It's really frustrating when people are like \"It just works! I had no problem! It's so easy!\" in response to someone who has clearly struggled to get things working. It'd be like someone telling you they just had a car crash from a mechanical failure and you responding \"Well I didn't! I drove home just fine!\" Instead, if you're going to respond at all, something like \"I'm sorry, don't give up. I hope you figure it out\" would be nice. reply woolion 8 hours agorootparentHave you tried Linux audio forums/community? I think the problem is that, by experience, regular musicians will just say \"you need to use windows (or mac) to do anything serious\" so it's complicated to find enough people that are competent in both skills. reply kristopolous 6 hours agorootparent> Have you tried Linux audio forums/community Actually I've gotten everything to work adequately but claiming the alsa/pulse/jack to pipewire transition wasn't just a new nightmare would be wrong, well for me. > \"you need to use windows (or mac) to do anything serious\" Correct and maybe! Serious as in commercial or production? Yes! Serious as in exploration in HCI and digital instrument creation and what kind of new sounds can come from that? Now we're back into Linux! I try to (poorly in my opinion) explore the uncharted, I'm not really looking to make a single penny. Take for instance, the classic chaotic pendulum (https://m.youtube.com/watch?v=yQeQwwXXa7A), you can hook that up to an Arduino sensor pack and convert the values to midi notes that get piped through a synthesizer or they can be the filter control of the synthesizer. How can the arrangement of the chaotic magnet surface affect the aesthetics of the sound? For instance, if you hook the values up to a sequencer playing arpeggiators and limit the chord choice wisely, it kinda sounds like bach. Especially if you do time dilation and don't try for things to be real-time. Recording a composition is a sequence of 2D diagrams with time signatures. Here's another one, this time with synesthesia. You take a number of sticky notes in various colors and aim a camera at a wall and then assign different roles and rules to the colors and their adjacencies and do a similar pipeline but this time you're playing a concert by sticking post-its to a wall on top of each other. And yet a 3rd. You take a couple hour capture of rush hour from a freeway traffic camera and assign instruments to the lanes, scale signatures to their densities and then you can hear an orchestration of Friday traffic. In all these you're still \"playing\" music because you're taking an active role in a bunch of aesthetic decisions and constraints, it's just a new relationship. reply woolion 4 hours agorootparentDo you have some repos and/or write-ups on these projects? That sound pretty cool. reply lloeki 6 hours agorootparentprev> which flavour/version of Linux they use. > this isn't a distribution problem Exactly. This all shouldn't be distro dependent when the only things involved are ALSA (kernel) and pipewire/pulseaudio. Both unfortunately have distros do \"things\" to them all. Some default to dmix, some to direct hardware with PW/PA dynamically spawned and thus getting exclusive access for a single Unix user while the others are SOL, and other painful conundrums because they thought \"we'll just do this and then it just works\" for a single basic use case. reply opyate 8 hours agorootparentprevI hope you figure it out :) I think the stars just aligned for me, and it worked. Not an expert at all on these things. reply aa-jv 6 hours agorootparentprevWould you use a distribution that doesn't prioritize the use-case for which you intend? Use a Linux distribution that is intended for professional audio use. >Once I hook up some midi devices, want things to be recorded, run a synthesizer stack through pw-jack, expect the midi clock to go down the USB bus, etc, all kinds of interesting behavior starts happening. On Ubuntu Studio [0]/[1] - this Just Plain Works™, you know. I've been doing exactly this for years on my Ubuntu Studio machine, and I just don't have any of the issues you've encountered. [0] - note that ubuntustudio is a metapackage you can install on most Ubuntu instances, which will set up audio for professional use. [1] - see also, Zynthian: https://zynthian.org/ reply Pinus 10 hours agoparentprevI’d say a lot of the complexity comes from the fact that there are at least four things — Jack, PulseAudio, Pipewire and the ALSA userland stuff, which the article fails to mention — that try to solve more or less the same problems (probably less, in the case of the ALSA userland). Add to this the fact that everything can be, and often is, run as a compatibility layer on top of (or below) everything else, and the naïve user who just wants to get some music though the speakers can be excused for feeling a bit dizzy. reply bux93 11 hours agoparentprevI'm sure the windows audio stack is many times more complex, and it's pretty opaque. OEMs also love to include all kinds of audio related bloatware, which makes getting audio hardware to work reliably(!) quite challenging. The HP laptop I'm typing this on has an \"Intel microphone array\" (just 2 mics) which has it's own intel drivers, but there's also some HP control panel, realtek and 'sound research' branded stuff, Fortemedia SAMsoft effects(?), Intel smart sound... If I'm recording seriously, I usually just go to the device manager (devmgmt.msc) and disable as much as I can and enable devices in a trial-and-error way to see what the minimum is to get audio to work again. Otherwise, all kinds of 'enhancements' end up in the audio path. reply p_l 10 hours agorootparentIntel Smart Audio is the worst, it takes over normal USB Audio devices and does its own weird processing on top, resulting sometimes with \"hilarious\" crashes. And on corporate laptop you might not be able to disable its driver :/ Fortunately, it's too dumb to deal with devices that are behind more hubs than the root one... reply hparadiz 11 hours agorootparentprevIf Windows could do what Linux can out of the box VoiceMeter would not exist. reply simoncion 10 hours agorootparentI assume you meant VB-Audio's \"Voicemeeter\"? If so, yeah, that's solid software, and it's NUTS that Windows hasn't made it unnecessary yet. (For those who may be reading this comment and wondering what the shit this software is, go give it a look. (I use it regularly and don't do Pro Audio stuff... I just want to be able to independently adjust relative volumes of groups of software (like Voice Chat and Video Games).) If the software's feature set looks intriguing, do give it a try and use Voicemeeter Potato. IIRC, all \"flavors\" of Voicemeeter have the same trial period, and I can't think of any reason to not use the the \"flavor\" with the most knobs and interconnects.) reply JodieBenitez 10 hours agoparentprevThat's the very first time I read \"sanely-working-out-of-the-box\" about Linux audio. I have a very different experience of course, such as \"why do my BT headphones suddenly play everything at 8000hz sampling rate while I never asked for this and why the UI won't let me switch it back to 24khz ?\". reply jcelerier 7 hours agorootparentThis happens exactly the same for instance on Mac or Windows with BT headphones, it's simply a fact of Bluetooth. If at any point you open an app that accesses the microphone of the earbuds / headphones, the format will downgrade from high-quality, playback only BT profile to a low-quality duplex profile. There's no high quality duplex audio profile in the Bluetooth protocol yet afaik, and certainly not implemented by any vendor. Just do t use Bluetooth if you care about sound quality is the answer. reply JodieBenitez 7 hours agorootparentNever had such issue on MacOs with the same headphones and the same apps running. reply fulafel 8 hours agorootparentprevSounds like HFP. For me on Ubuntu you can choose between audio playback profile and HFP in the sound settings ui, but maybe you have an app running that is changing that setting. (Apologies for injecting advice into a grumbling thread) reply JodieBenitez 7 hours agorootparentNo need to apologize, advice is good. Even if for now I'll keep Linux at what it's best: servers. reply shrimp_emoji 6 hours agorootparentprevBT headphones? BT is false dharma. Mobile shit. RF headphones work great on Linux because RF is true desktop dharma. reply Joker_vD 10 hours agoparentprevWell, I am glad it works for you. My Ubuntu setup at work, on the other hand, has recently picked up a habit of randomly switching to \"Family 17h (Models 00h-0fh) HD Audio controller\" which has nothing plugged into it instead of my actual headphones. reply f1shy 10 hours agoparentprevI’m trying right now to connect a BT speaker to a RPi, while using mpg123 as a player. At least from the command line, it is all but easy. And I do not think what I’m doing is not the most basic scenario. Right now after coupling I have to restart mpg123. reply bigstrat2003 4 hours agorootparentI mean... it should work of course, and if it's not working for you then improvement is needed. But anything involving BT is most definitely not the most basic scenario. The most basic scenario is \"plug speaker into sound card\". reply hawski 8 hours agoparentprevFor me it all sometimes fall apart after suspend/resume and I am on a desktop, other than that I enjoy watching a movie with my wife with two pairs of Bluetooth headphones while having a USB attached microphone input overlaid so we can hear if our daughter went out of her room. reply boffinAudio 10 hours agoparentprev100% agree with you. I have had a Linux-based DAW running in my studio, alongside the requisite MacOS and Windows machines, for decades now. It runs Ubuntu Studio, has superlative audio performance (72 channels of digital audio), and is a rock solid workhorse for doing large edits on tracks. The key to it is in using Ubuntu Studio, which is a well-tuned distribution focused on superlative Audio performance, and to choose your hardware wisely. In my case, its all Presonus - because they have been Linux-friendly for a long time - and it easily delivers latency numbers that outperform even the Mac in the room. reply knorker 8 hours agoparentprevI think it's fair to say that your experience is extremely atypical. PulseAudio constantly stands out as the most common frustration in all of Linux. systemd is pretty frustrating too, but it's only frustrating the 1% of the time when it breaks (because it means your whole system is broken), whereas the 70% of the time PulseAudio doesn't work, it's more mildly infuriating. For anything except a single audio in and single audio out, I would not bet on a beginner-to-intermediate ever getting it working, and PulseAudio is probably the main reason I in 2024 cannot recommend Linux on the desktop for non-engineers. reply prmoustache 5 hours agorootparentI have been using pipewire for a few years already. Which distro are you using that is still using pulseaudio? I can't complain really, in last decade audio has been working really well on my computer globally bar the occasional bluetooth pairing/connection issue. But I've seen people struggling with BT on all OS/platforms anyway. Pulseaudio was a PITA in the early years but matured a lot and Pipewire has been flawless on my distro since it took over. And I can do a lot of stuff out of the box that requires third party tools on other system to do the same. reply bigstrat2003 4 hours agorootparentprev> whereas the 70% of the time PulseAudio doesn't work, it's more mildly infuriating. I have installed Linux on I don't know how many systems at this point. My desktop, my work laptop, other people's work laptops, etc. Audio just worked flawlessly on each and every one. I cannot believe that PulseAudio has issues 70% of the time. There's simply no way I've been that lucky to have never seen an issue. reply probablybetter 8 hours agoprevThe \"mess\" of Linux audio is due to ONE reason: single-client ALSA driver model. every other layer is a coping mechanism and the plurality and divergence of the FOSS community responds in various ways: - Jack - PulseAudio - PipeWire I am unclear why Jaroslav Kyocera chose to make ALSA single-client, but Apples CoreAudio multi-client driver model is the right way to do digital audio on general-purpose computing devices running multi-tasking OS'es on application processors, in my opinion. Current issues this article does not address that actually constitute large parts of the \"mess\" of Linux Audio: - channel mapping that is not transparent nor clearly assigned anywhere in userspace. (aka, why does my computer insist that my multi-input pro-audio interface is a surround-sound interface? I don't WANT high-pass-filters on the primary L/R pair of channels. I am not USING a subwoofer. WTF) - the lack of a STANDARD for channel-mapping, vs the Alsa config standards, /etc/asound.conf etc. - the lack of friendly nomenclature on hardware inputs/outputs for DAW software, whether on the ALSA layer, or some sound-server layer. (not to mention that ALSA calls an 8-channel audio-interface \"4 stereo devices\") - probably more, but I can't remember. My current audio production systems have the DAW software directly opening an ALSA device. I cannot listen to audio elsewhere until I quit my DAW. This works and I can set my latency as low as the hardware will allow it. this is the thing: more than about 10ms latency is unacceptable for audio recording in the multitrack fashion, as one does. reply tleb_ 6 hours agoparentI disagree. Applications want to receive/provide a stream (X sample-rate, Y sample format, Z channels) and have it routed to the right destination, that probably is not configured with the same parameters. Having all applications responsible for handling this conversion is not doable. Having the kernel handle this conversion is not a good idea. The routing decision-making needs to be implemented somewhere as well. Let's not ignore the complexity involved in format negotiation as well. The scenario of a DAW (pro-audio usage) is too specific to generalise from that. That is the only kind of software that really cares about codec configuration, latencies and picking its own routing (or rather to let the user pick routing from the DAW GUI). reply miki123211 8 hours agoparentprev> The \"mess\" of Linux audio is due to ONE reason: single-client ALSA driver model. This is one of the major reasons why Linux accessibility sucks IMO. Audio is one thing that you need to \"just work™\" if you want to get accessibility right, as there's no way for a screen reader user to fix it without having working audio in the first place[1]. On Linux, it does not \"just work\", and different screen readers have different ideas on how they want audio to be handled. In particular, the terminal Speakup screen reader (with a softsynth) wants exclusive control of your device through ALSA IIRC, while the Orca screen reader for the GUI goes through Pulse. That makes it impossible to use both of them at the same time. [1] Well, you can sort of fix it by having a second machine and SSHing into the broken one, but that's not what I mean. reply codedokode 5 hours agorootparent> the terminal Speakup screen reader (with a softsynth) wants exclusive control of your device through ALSA If you have Pulseaudio or Pipewire, they add a plugin to ALSA library that reroutes audio to audio daemon, so ALSA applications should work correctly. reply bigstrat2003 4 hours agorootparentprevI mean... I've never seen a single audio issue on Linux. It does \"just work\" in my experience. I realize the people citing issues in this thread aren't just making shit up for the fun of it, but I think there's a lot of going too far and saying it sucks for everyone when it seems to work just fine for most. reply lynx23 7 hours agorootparentprevI would be surprised if Orca did use Pulse directly, it uses speech-dispatcher (IIRC) which then uses PA if configured that way. Also, Accessibility != Audio. I, for instance, use Braille only. No need for speech synthesis. So equating Accessibility issues wth the crazy audio stack is a little bit too simple. reply codedokode 5 hours agoparentprevSingle-client model is not bad because it doesn't require kernel to do the mixing, sample rate conversion and they can be moved to userspace (which Windows does these days as well [1]). The less code in kernel, the better. [1] https://learn.microsoft.com/en-us/windows/win32/coreaudio/us... reply jcelerier 8 hours agoparentprev> current audio production systems have the DAW software directly opening an ALSA device. I mean, I remember this being the case for a very long time on windows with ASIO too, which is the only reasonable way to run a DAW with acceptable latency there. MacOS has multi-client but I was never able to get latency as low as fine-tuned windows and Linux systems, and in the end that's what matters - you just use your motherboard's chip for OS audio and your pro soundcard for the actual workload. Pipewire is very close to giving a good experience but there'll always be some overhead - I'm making some art installations running various chains of audio effects on a raspberry pi zero and the difference between going through pipewire even if my app (https://ossia.io) is the only process doing any sound, and going straight to ALSA, is night and day in terms of \"how many reverbs I can stupidly chain before I hear a crack\". reply frabert 7 hours agorootparentMy presonus interface allows multiple applications to access it over ASIO simultaneously, while letting regular Windows audio through, at 16 samples of latency. ASIO does not mandate exclusive access, bad drivers do. reply vetinari 6 hours agoparentprev> I am unclear why Jaroslav Kyocera chose to make ALSA single-client, but Apples CoreAudio multi-client driver model is the right way to do digital audio on general-purpose computing devices running multi-tasking OS'es on application processors, in my opinion. Because ALSA is a different layer in the audio stack than CoreAudio. ALSA corresponds to MacOS drivers and I/O Kit. CoreAudio (Audio Toolbox / Audio Unit) corresponds to Pipewire / Pulseaudio. But on the Mac side everyone is OK with using CoreAudio (with the accompanying set of daemons), while on Linux, for some reason, everyone wants to go as low-level as possible, \"just open the device file\" and is wondering, why something is missing. Because you skipped that, that's why. reply dimsuz 7 hours agoparentprevIs this planned to be addressed/fixed? (single-client model) Maybe there were previous attempts? reply vetinari 5 hours agorootparentNo, because there's nothing to fix (at the system side). Apps should use the right API from the right layer; when they skip something, no wonder they will miss whatever the skipped layer provides. When they do not need exclusive access to the device and want to play nice with the other apps, they should use pipewire/pulseaudio. For 99% of apps, using ALSA directly is the wrong approach. You don't use IOKit directly in Mac apps either. reply codedokode 5 hours agorootparentPipewire/Pulseaudio install a plugin for ALSA library so that ALSA applications audio is rerouted to audio daemon. So apps using ALSA can work at both systems with and without an audio daemon. reply vetinari 2 hours agorootparentAt this day and age there should not be a system without an audio daemon. At least not one, that is not broken. Apps should not certainly accommodate for broken systems, and forcing workarounds for the correct ones. reply laserbeam 9 hours agoprevHere's my background 1. I have to modify my audio settings every time I start a call in Teams on Linux because it keeps losing my audio device. 2. In my audio settings UI, half the time I switch my devices the speaker test doesn't work. 3. In my audio settings UI, whenever I switch my mic I hear myself. The mic feedback only disappears 30 seconds after I close the settings UI. 4. My work headsets have a robotic sound (likely caused by an incorrect bitrate or buffer size). I can only use work bluetooth headsets via their dedicated dongle. This was my default experience on a popular debian based distro. And it mirrors the general experience I see online. Things are unstable and a mess. I started reading this article and it's embelished with phrases like: \"is a professional-grade audio server\", \"widely used in professional audio production environments\", and general language that sounds like a sales pitch. This does not fit with anything I'm familiar with. I would have preferred a neutral and semi technical approach, with 10% of the buzzwords. As written, I trust nothing. reply juujian 8 hours agoparentThat would be your headset being in headset mode. I'm on Debian Testing, and I'm finally able to exit headset mode and use a high quality audio codec instead. I hope that's the direction Linux is heading. reply laserbeam 8 hours agorootparentOh no, it's not. I've swapped modes more than imaginable. And even if it were in headset mode the quality would be inacceptable compared to what I was getting on windows. I even manually tried to change pulseaudio settinga for that device with no luck. And I don't feel like turning this thread into a debugging session. But, like, correctly figuring out reasonable bitrates should work by default. reply n_plus_1_acc 9 hours agoparentprev1. Is a known Problem with Teams reply laserbeam 9 hours agorootparentYeah, teams definitely shares in the blame there. That one hurts more and I blame both microsoft and linux. reply pedrocr 8 hours agorootparentThe Teams web app works very well in Chrome. I only miss being able to set a custom background to my video and popping out presentations into their own window. It seems much lighter on resources too, maybe because Chrome does more of the video encoding or decoding using hardware. reply Cloudef 11 hours agoprevPipewire has pretty much unified the userland linux audio stack (+ supports video as well as bonus). Kernel side it has always been alsa. There's TinyAlsa so you don't have to use libasound to interface with the kernel alsa. (userland alsa is quite PITA) reply alexey-salmin 10 hours agoparent> Kernel side it has always been alsa. Well you probably didn't mean it in a literal sense, but it was OSS up to kernel 2.4 and 2.6 had both OSS and ALSA reply gen2brain 9 hours agoprevWhenever someone mentions Linux and Audio I always remember this image (made by Adobe I think) https://harmful.cat-v.org/software/operating-systems/linux/a.... It is missing Pipewire but it should be easy to add a dozen of new lines. This is the reason why I simply use plain ALSA without any sound daemons. reply pedrocr 9 hours agoparentThat's about maximum mess historically but thankfully most stuff on that diagram is no longer present in a modern install. And that's before pipewire which has further unified the stack. reply creshal 9 hours agorootparentPipewire hasn't really unified anything, it's just pw -> alsa -> hardware instead of pa -> alsa -> hardware for the common case. And not much has fallen out of use. OpenAL, libao, jack, portaudio, libcanberra, Gstreamer and phonon at least are still used widely, and a bunch of others keep cropping up occasionally in cross-platform software. reply pedrocr 9 hours agorootparentMy understanding is that pipewire finally unifies JACK and Pulseaudio. You no longer have to decide if you want a general audio setup or a low-latency one, there's a single audio server now that does everything well. So from that list the unification is now: - JACK and Pulseaudio are both replaced by pipewire - OSS is long gone, ALSA is now the only low-level interface - ESD, NAS, ClanLib, xine, portaudio, allegro and Phonon are not present in the Ubuntu install I just checked Basically we're down to a unified stack that has BlueZ and ALSA to access actual hardware and pipewire as the single audio (and video) daemon. Everything else is either shims so apps don't need to change interface or cross platform APIs like SDL and OpenAL. We are much better than what this diagram shows. reply gen2brain 8 hours agorootparentThere is still an ALSA OSS emulation in the kernel, probably distros do not enable it but I have had it enabled for years. All it needs is some ioctl system calls, which does everything for me internally. ALSA with just system calls and without libalsa can work for some cards but it would be hit and miss. I like that I can use OSS in Go without C/CGo, i.e. https://github.com/gen2brain/oss. reply creshal 8 hours agorootparentprevOn an infinite featureless plain with spherical cows, yes, in theory we've \"unified\" everything, as soon as closed source software stops existing (together with BSDs) and everyone rewrites their audio stacks everywhere. (Just like we had unified almost everything before pipewire was invented.) In practice, OSS and Jack still stick around, as do portaudio, libao and others. reply hawski 9 hours agorootparentprevAren't most of the libs you mentioned cross-platform or with a very specific use in mind which have direct analogues in Windows world? reply creshal 9 hours agorootparentCorrect. You can draw just as messy a diagram for Windows. DirectSound versus OpenAL versus gstreamer(win32) versus jack(win32) versus XAudio2 versus DirectShow versus DirectMusic versus WDM Audio versus WASAPI versus UAA versus … If there's any difference it's that under Windows, the only debugging and error handling you have is \"lol reinstall all drivers and codecs and pray\". reply gen2brain 9 hours agorootparentprevAnd then there is Pipewire with PulseAudio plugin, and not to mention you can compile Pipewire with support for GStreamer, Jack, etc. I guess we can remove only the Arts and ESD daemons from that list, but the mess just adds up. reply Galicarnax 11 hours agoprevThe text has a strong GPT-ish flavor. reply ssahoo 10 hours agoparentGpt is just emulating best writing quality. This article is very well written. If it was gpt generated, I'd be happy to read more like it. reply pickledoyster 6 hours agorootparentI'd argue gpt emulates the lowest (i.e. cheapest to mass produce) passable quality writing optimized for longest page time/views reply bowsamic 8 hours agorootparentprevI genuinely think humans are going to degrade their writing style in order to sound less like an LLM reply sihox 11 hours agoprevIt's pretty nice article but for me - just for introductory purposes. It shows how sound and digital audio works and what basic libraries and tools we have in linux to deal with sound. But I'm still stupid when it comes to details and user interface tools. The article(s) I really love to see is, on one hand, more technically detail-specific and, on the other hand, broadly defining options I can have as an end user. I mean - from basic tools (CLI, GUI) that are available for simple purposes like volume control, stream selection, etc. to pro-audio, complex scenarios. For me it's too many tools and options I can use in linux for audio and this is the reason for being lost sometime. Of course for daily use I have pipewire with pulse, alsa and jack \"plugins\" which gives me seamless cooperation with lots of apps and controls but maybe I can get rid of some module or app... reply codedokode 5 hours agoprevI don't really understand how was JACK supposed to be used. On Windows or Mac you typically run a DAW and load plugins into it. But on Linux the user is supposed to run every plugin as a separate application and connect them using JACK? Doesn't this mean there would be lot of context switches? Also, in a DAW you can save your configuration, but how do you do this with JACK and a bunch of independent applications? Also, given that Pulseadio and Pipewire both support ALSA clients, does it mean that the preferred API for applications should be ALSA? This way they can play sound on any system, even where there is no audio daemon. reply cod1r 9 hours agoprevAn issue I've experienced very often is that sometimes when my laptops goes to sleep and I wake it up, the speakers occasionally aren't switched to unless I restart pipewire. Same thing for headphones, sometimes when I plug them in, they aren't switched to unless I replug it in a couple of times. Might be hardware related but situations like this make me feel like I should just use linux for servers instead of for a personal computer. reply trelane 2 hours agoparentWhat version of Linux did your laptop ship with? reply probablybetter 8 hours agoprevAm I the only one that sees unfriendly input/output channel names with Pipewire in client software? (Bitwig, Ardour, Reaper, more? I would like to see \"Input 1\" or \"Channel 1\" and not some strange ciphers when trying to assign things in a little dropdown selector in a DAW) reply trelane 50 minutes agoparentThis sounds (ha) like broken hardware. reply krs_ 6 hours agoparentprevIt seems to try to name the outputs based on the hardware name, but with multiple audio devices it can become quite confusing indeed. I've resorted to creating a wireplumber[1] config to rename things more reasonably and also disable a bunch of the inputs/outputs that I never use so they don't clutter up the lists. [1] https://pipewire.pages.freedesktop.org/wireplumber/index.htm... reply moogly 3 hours agoprevI want to move my DAW to Linux, but giving up TotalMix is quite a blow... Not sure how well the NI Native Access crap is working on WINE either. Does anyone know? reply lynx23 8 hours agoprevWell, we had a lot of layers in Linux Audio the last 30 years. But when PulseAudio was forced into the world by a c-section, with everyone in the LA community already knowing its a still-birth, I kind of lost trust in coordinated project creation. PA makes me so unhappy that I totally uninstall it whereever I see it. Good for me that I am just a console user, because the damn beast is all over the GUI space. Fact is, RT audio is hard, and the peoplebehind JACK have cared for the underlying problems for a long time already. Maybe PipeWire, but to be honest, it reminds me too much of PA. I guess I will stay with plain JACK and SuperCollider as my toolbelt, and not care about PA or PW. Like the grumpy old hacker I am. reply g15jv2dp 12 hours agoprevI now have flashbacks to when I tried to get sound working on my Gentoo install 15 years ago. It seems that now everything has changed (wtf is pipewire) and still requires arcane knowledge to get everything working smoothly... reply bigstrat2003 12 hours agoparentTo be honest, I wouldn't say it requires arcane knowledge these days. When setting up both Arch and Gentoo, I did a bit of research, determined that Pipewire was probably the best for me (because it seems to be largely a superset of the other options), and installed it following the wiki instructions. That's it. I haven't had to configure things in any sort of detail, it just worked smoothly the first time without any config. reply erremerre 11 hours agorootparentLet me guess, you don't have a Realtek controller for audio. reply freedomben 6 hours agorootparentRealtek audio has gotten to a pretty good place on Linux. I've had several over the past few years, and they just work for the most part. Fedora, especially has been a wonderful out of the box experience for audio. Bluetooth on the other hand... reply erremerre 6 hours agorootparentI must be the only unlucky one because my linux approved motherboard Intel DQ77MK with a Realtek ALC892 8-channel for audio kept switching between front and back panel (although nothing was plugged in on front) on Fedora 39 :( . It does work fine on windows. reply freedomben 4 hours agorootparentActually now that you mention it, I did have a similar problem on one particular motherboard. I don't attribute it to linux though so hadn't thought of it. It turned out to be a hardware issue (with the motherboard) though. The card kept resetting due to too low of input power. I had hoped a BIOS update would fix it, but it didn't. I have had a couple of built-in audio modules die and stop being seen by the kernel on old laptops. Honestly I would just buy a USB interface. I got a Focusrite Scarlett 2i2 and have been very happy with it. reply trelane 4 hours agorootparentprevDid you try OSX on the same hardware? reply vladvasiliu 11 hours agorootparentprevI do have one, whatever HP figured was good enough for their \"high-end\" elitebooks. Worked perfectly on Arch without any fiddling. Even the mute leds on the keyboard work as expected. reply bigstrat2003 10 hours agorootparentprevI believe I do, though I'm not certain. I have whatever is built into my Asus motherboard, which seems like it is ultimately a Realtek chip. Not sure though. reply oleg_antonyan 11 hours agoparentprevPipeWire is drop-in replacement for pshshaudio with additional features (while preserving pshsh part sometimes). Only difference is 15 years ago half of the problems with sound were fixed by uninstalling pulseaudio as it was in early stages full of bugs but still pushed to many distros similar to systemd. Today's pipewire is as easy as uninstalling pulseaudio, installing pipewire, and most of the times it will continue working reply Cloudef 11 hours agorootparentTo be fair. Pulseaudio in early days exposed tons of audio driver bugs that were since then fixed. Bluetooth audio was barely working back then. Now bluetooth works so well that it makes other operating systems look bad. reply thriftwy 6 hours agorootparentprevPulseaudio had a great GUI mixer/settings app. That does not seem to come with pipewire and tries to install PA when explicitly installed. reply xoac 12 hours agoparentprevI just install fedora and everything works out of the box. reply shmerl 12 hours agoparentprevPipewire is a major development that's especially important for Wayland desktop. It was hard to miss unless you haven't used Linux in a long time. reply nlpparty 12 hours agorootparentI don't know what Pipewire is as well. I've used Linux daily for 10+ years, but I've never bothered to use Wayland. reply p_l 10 hours agorootparentPipewire ends up being used to paper over Wayland deficiencies in some areas, thus why it's important for Wayland desktop ;) By itself it's essentially grand unification of audio servers that actually works better and is way less... opinionated about the only true way some things works, which was a problem with pulseaudio at times. reply shmerl 2 hours agorootparentSeparation of responsibilities is good, instead of having a mix of everything but not well enough situation with X11. reply p_l 1 hour agorootparentSeparation of responsibilities is something Wayland fails hard, by effectively hardcoding coexistence of significant part of display driver, windows management, simple things like windows decorations (which, thanks to Gnome's insistence, are by default only client-side, so your concerns invade internals of client apps!) etc. reply shmerl 1 hour agorootparentJust don't use Gnome. KDE is fine with server side decorations and it's false that Wayland mandates client side ones. Wayland ≠ Gnome and Gnome itself indeed made a bunch of pretty questionable decisions. But if anything it's X11 that tries to lump a ton of stuff together. Wayland is very minimalistic in comparison. That's why stuff like libinput and Pipewire are part of making a functional desktop. reply shmerl 12 hours agorootparentprevWell, time to move with the progress. I've been using KDE Wayland session for several years already. reply exe34 11 hours agorootparentnah I'm okay thanks. X11 still works flawlessly for me. reply shmerl 2 hours agorootparentThe main issue is that X11 is an increasingly unmaintained case and all new development happens with Wayland anyway. So as long as you can deal with lack of support - I guess no need to move, but otherwise Wayland with KDE has been well usable for a while already. reply exe34 13 minutes agorootparentYeah I'll move when I have hardware that really needs it. I use a 12 year old laptop. reply g15jv2dp 12 hours agorootparentprevI've been a WSL user for years now. Way less effort. reply daghamm 11 hours agorootparentUntil vmmem starts using 100% of your CPU and you need to kill the WSL service to get your computer back (without the unsaved work of course). But yeah WSL is just too easy. Specially with native vscode support it has become a favourite for many developers reply g15jv2dp 6 hours agorootparentIf I had been using using linux as main OS in that situation, I would have had to reboot. How is that different? reply daghamm 2 hours agorootparentIt's different because this is windows hyper-v (or something related) crashing and bringing down your WSL session with it. A native Linux is far more stable. reply shmerl 12 hours agoprev> ALSA is the core layer of the Linux audio stack. It provides low-level audio hardware control, including drivers for sound cards and basic audio functionality. > ... > Ulike PuleAudio and JACK, PipeWire does not require ALSA on a system, in fact if ALSA is installed the output of ALSA is very likely pushed through PipeWire I don't get this part. If ALSA represents the kernel level hardware drivers for audio, how does Pipewire bypass it? Does it implement an alternative set of kernel drivers? I assumed Pipewire still relies on ALSA base. reply evil-olive 11 hours agoparentALSA had both kernel-space drivers and a user-space API layer. I think what they're getting at is that PipeWire speaks the ALSA API, so an app or game that linked against ALSA will connect to PipeWire and should Just Work, without needing to be rewritten to target PipeWire's API. PipeWire does the same trick with the PulseAudio API as well. on my PipeWire-using NixOS box, for example, I can connect through the `pavucontrol` GUI, my `pactl`-based keybindings work the same, etc. it's a clever design that allows them to avoid what would otherwise be a nasty pile of backwards-compatibility issues and poor desktop user experience. reply runiq 11 hours agoparentprevThat part is simply wrong. Pipewire uses Alsa to drive the soundcard. Alsa is a hard requirement. reply codedokode 5 hours agoparentprevThis is probably wrong. ALSA is a low-level API that allows direct access to audio hardware, but only for one application at a time. ALSA has a kernel component and an userspace library that can be configured. Pipewire uses ALSA to interface with audio hardware. But Pipewire also adds a plugin to an ALSA userspace library so that audio from apps that use ALSA API is rerouted to Pipewire. Pulseaudio did the same trick. reply bornfreddy 12 hours agoparentprevThis. Also, how does PulseAudio not support videoconferencing? reply ruffyx64 11 hours agorootparentWell. PulseAudio is a plain sound server. It does not handle video at all. reply shmerl 12 hours agorootparentprevMay be it's about a higher layer of ALSA API, not about actual hardware drivers. reply exe34 11 hours agorootparentyeah I think there are two ALSAs, one in the kernel as drivers and one as a userspace library. reply hawski 9 hours agoparentprevI think it could only make sense for sending sound over the network. reply bigstrat2003 12 hours agoparentprevI was also confused by this bit. If ALSA is part of the kernel, how can it not be installed? reply SSLy 12 hours agorootparentThe article doesn't note kernel-ALSA and the old userspace-ALSA libraries that apps use to push audio. reply Novosell 10 hours agorootparentprevWell, you could compile your kernel without ALSA. reply hcfman 10 hours agoprevBeautiful work reply thriftwy 12 hours agoprevI remember there was oss and alsa. Then on top of that you had esd or artsd, with incompatible APIs and userland delays. Then at some point they were replaced with pulseaudio. I've just noticed now that pulseaudio is also replaced with pipewire in latest Ubuntu. That does not instill confidence in that they have any idea what they are doing. I remember that as far as on 2012 some Linux game ports from Steam (before Proton was a thing) failed to play sound. Other than that, it took them a decade to figure out that sound should switch to HDMI when it is plugged in. It may still require arcane config changes and may break down. I've opened the post to read about pipewire, and it seems that clicking an anchor does nothing. So it's not only sound they can't get right. reply Cloudef 11 hours agoparentPipewire unifies all the apis, and is fully backward compatible. It has own API too, but you don't have to use it. You can write app using any of the apis, alsa, pulse, jack, and pipewire can work with it. You only need pipewire api if you need more control or pipe through video as well. JACK is very flexible approach to audio (think of unix pipes but audio) and this is what pipewire is inspired by, but targets the general desktop use and not just audio production as JACK does. Generally pipewire stays out of the way and works as pulseaudio replacement, but with the pipelines you can do a lot as a power user. Personally I think pipewire is one of the best things that has happened to linux, and since it shims all the previous audio apis instead of trying to tape the existing solutions into the package, it actually removes bloat from your system and you don't have to write weird libasound config files anymore. reply shmerl 12 hours agoparentprevDon't worry, Pipewire is a very good development. reply poakjsn 9 hours agoprev [–] Linux distros would be best off lifting as much as possible from the Android Open Source Project, i.e. a professional and streamlined Linux-based system that actually works and isn't just a mishmash of incompatible, poorly designed hobbyist trash. The year of the Linux desktop never arrived but most of the world has Linux sitting in our palms. Let's build on that instead of the dead ends of Debian, Slackware, etc. reply curt15 8 hours agoparentHasn't Android audio always struggled with input latency? reply Ylpertnodi 3 hours agorootparentI use several audio apps on Android, on a mid-low phone. The most intensive use comes from Cubase - Audio and synths, mixer, inputs and outputs, midi/ phone input program. After 30 years on Atari then windows I've had my ups and downs with the program, but I can easily say that my android version is a1 solid, connected to/ from my hardware synths reply probablybetter 8 hours agoparentprev [–] wow. no. debian is the opposite of a dead end. now, ubuntu and lubuntu and xubuntu = i agree. android, however, is garbage. - it most certainly is not consistent across devices and editions - having to do phone-like-stuff on a desktop/laptop is painful - i hate Android. It's settings bother me the most. there is ZERO organization here! There are DARK patterns galore, it's built to dissuade you from doing anything serious. I can only take your comment as either a joke, or you literally only use a phone/tablet in life. news flash: Android audio is ALSA at it's core. Additionally: native Android audio is ultra-high-latency... completely unacceptable for pro-audio reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article provides a comprehensive overview of the Linux audio stack, explaining sound basics, human sound perception, and digital audio processing.",
      "Key components of the Linux audio stack include ALSA for low-level hardware control, JACK for low-latency real-time audio, and PulseAudio for higher-level audio management, with PipeWire emerging as a unifying solution.",
      "PipeWire is highlighted as a versatile option that integrates features of both JACK and PulseAudio, potentially replacing them in future Linux distributions."
    ],
    "commentSummary": [
      "The Linux audio stack, though complex, supports diverse hardware and functionalities, including multiplexing and network features.",
      "Pipewire aims to unify the Linux audio stack, supporting various APIs and improving on PulseAudio, offering enhanced versatility and stability.",
      "While some users face challenges with advanced setups, many appreciate the improvements and flexibility Pipewire brings to Linux audio."
    ],
    "points": 144,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1721711898
  },
  {
    "id": 41040417,
    "title": "United States Discloses Nuclear Warhead Numbers; Restores Nuclear Transparency",
    "originLink": "https://fas.org/publication/united-states-discloses-nuclear-warhead-numbers-restores-nuclear-transparency/",
    "originBody": "SEE ALL PUBLICATIONS SHARE GLOBAL RISK United States Discloses Nuclear Warhead Numbers; Restores Nuclear Transparency 07.20.246 MIN READTEXT BY HANS KRISTENSEN Note: The initial NNSA release showed an incorrect graph that did not accurately depict the size of the stockpile for the period 2012-2023. The corrected graph is shown above. [UPDATED VERSION] The Federation of American Scientists applauds the United States for declassifying the number of nuclear warheads in its military stockpile and the number of retired and dismantled warheads. The decision is consistent with America’s stated commitment to nuclear transparency, and FAS calls on all other nuclear states to follow this important precedent. The information published on the National Nuclear Security Administration (NNSA) web site today shows that the U.S. nuclear weapons stockpile as of September 2023 included 3,748 nuclear warheads, only 40 warheads off FAS’ estimate of 3,708 warheads. The information also shows that the United States last year dismantled only 69 retired nuclear warheads, the lowest number since 1994. FAS has previously requested that the United States release the size of the US nuclear arsenal for FY2021, FY2022, and FY2023, but those requests were denied. FAS believes the information was wrongly withheld and that today’s declassification decision vindicates our belief that stockpile disclosures do not negatively affect U.S. security but should be provided to the public. With today’s announcement, the Biden Administration has restored the nuclear stockpile transparency that was created by the Obama administration, halted by the Trump administration, revived by Biden administration in its first year, but then halted again for the past three years. While applauding the U.S. disclosure, FAS also urged other nuclear-armed states to disclose their stockpile numbers and warheads dismantled. Excessive nuclear secrecy creates mistrust, fuels worst-case planning, and enables hardliners and misinformers to exaggerate nuclear threats. What The Nuclear Numbers Show The declassified stockpile numbers show that the United States maintained a total of 3,748 warheads in its military stockpile as of September 2023. The military stockpile includes both active and inactive warheads in the custody of the Department of Defense. The information also discloses weapons numbers for the previous two years, numbers that the U.S. government had previously declined to release. Although there have minor fluctuations, the numbers show that the U.S. nuclear weapons stockpile has remained relative stable for the past seven years. The fluctuations during that period do not reflect decision to increase or decrease the stockpile but are the result of warheads movements in and out of the stockpile as part of the warhead life-extension and maintenance work. Although the warhead numbers today are much lower than during the Cold War and there have been reductions since 2000, the reduction since 2007 has been relatively modest. Although the New Start treaty has had some indirect effect on the stockpile size due to reduced requirement, the biggest reductions since 2007 have be caused by changes in presidential guidance, strategy, and modernization programs. The initial chart released by NNSA did not accurately show the 1,133-warhead drop during the period 2012-2023. NNSA later corrected the chart (see top of article). The chart below shows the number of warheads in the stockpile compared with the number of warheads deployed on strategic launchers over the years. This graph shows the size of the U.S. nuclear stockpile over the years plus the portion of those warheads deployed on strategic launchers. The stockpile number for 2024 and the strategic launcher warheads are FAS estimates. The information also shows that the United States last year dismantled only 69 retired nuclear warheads. That is the lowest number of warheads dismantled in a year since 1994. The total number of retired nuclear warheads dismantled 1994-2023 is 12,088. Retired warheads awaiting dismantlement are not in the DOD stockpile but in the DOE stockpile. The information disclosed also reveals that there are currently another approximately 2,000 retired warheads in storage awaiting dismantlement. This number is higher than our most recent estimate (1,336) because of the surprisingly low number of warheads dismantled in recent years. Because dismantlement appears to be a lower priority, the number of retired weapons awaiting dismantlement today (~2,000) is only 500 weapons lower than the inventory was in 2015 (~2,500). FAS’ Work For Nuclear Transparency The Federation of American Scientists has worked since its creation to increase responsible transparency on nuclear weapons issues; in fact, the nuclear scientists that built the bomb created the “Federation of Atomic Scientists” to enable some degree of transparency to discuss the implications of nuclear weapons (see FAS newsletter from 1949). There are of course legitimate nuclear secrets that must remain so, but nuclear stockpile numbers are not among them. This FAS newsletter from 1949 describes the debate and FAS effort in support of transparency of the US weapons stockpile. One part of FAS’ efforts, spearheaded by Steve Aftergood who for many years directed the FAS Project on Government Secrecy, has been to report on the government’s discussions about what needs to be classified and repeatedly request declassification of unnecessary secrets such as the stockpile numbers. This work yielded stockpile declassifications in some years (2012-2018, 2021) while in other years (2019-2020 and 2022-2024) FAS’ declassified requests were initially denied. Most recently, in February 2024, an FAS declassification request for the stockpile numbers as of 2023 was denied, although the letter added: “If a different decision is made in the future, we will notify you of this decision” (see rejection letters). Given these denials, FAS in March 2024 sent a letter to President Biden outlining the important reasons for declassifying the numbers. The new disclosure of the stockpile numbers suggests that denial of earlier FAS declassification requests in 2023 and 2024 may not have been justified and that future years’ numbers should not be classified. The other part of FAS’ efforts has been the Nuclear Information Project, which works to analyze, estimate, and publish information about the U.S. nuclear arsenal. In 2011, when the Obama administration first declassified the history of the stockpile, the FAS estimate was only 13 warheads off from the official number of 5,112 warheads. The Project also works to increase transparency of the other nuclear-armed states by providing the public with estimates of their nuclear arsenals. The project described the structures that enabled Matt Korda on our team and others to discover the large missile silo fields China was building, and NATO officials say our data is “the best for open source information that doesn’t come from any one nation.” Why Nuclear Transparency Is Important FAS has since its founding years worked for maximum responsible disclosure of nuclear weapons information in the interest of international security and democratic values. In a letter to President Biden in March 2024 we outlined those interests. After denials in 2023 and February 2024 of FAS declassification requests, FAS in March sent President Biden a letter outlining why the denials were wrong. Click here to download full version of letter. First, responsible transparency of the nuclear arsenal serves U.S. security interests by supporting deterrence and reassurance missions with factual information about U.S. capabilities. Equally important, transparency of stockpile and dismantlement numbers demonstrate that the United States is not secretly building up its arsenal but is dismantling retired warheads instead of keeping them in reserve. This can help limit mistrust and worst-case scenario planning that can fuel arms races. Second, the United States has for years advocated and promoted nuclear transparency internationally. Part of its criticism of Russia and China is their lack of disclosure of basic information about their nuclear arsenals, such as stockpile numbers. U.S. diplomats have correctly advocated for years about the importance of nuclear transparency, but their efforts are undermined if stockpile and dismantlement numbers are kept secret because it enables other nuclear-armed states to dismiss the United States as hypocritical. Third, nuclear transparency is important for the debate in the United States (and other Allied democracies) about the status and future of the nuclear arsenal and strategy and how the government is performing. Opponents of declassifying nuclear stockpile numbers tend to misunderstand the issue by claiming that disclosure gives adversaries a military advantage or that the United States should not disclose stockpile numbers unless the adversaries do so as well. But nuclear transparency is not a zero-sum issue but central to the democratic process by enabling informed citizens to monitor, debate, and influence government policies. Although the U.S. disclosure is not dependent on other nuclear-armed states releasing their stockpile numbers, Allied countries such as France and the United Kingdom should follow the U.S. example, as should Russia and China and the other nuclear-armed states. Acknowledgement: Mackenzie Knight, Jon Wolfsthal, and Matt Korda provided invaluable edits. More information on the FAS Nuclear Information Project page. This research was carried out with generous contributions from the Carnegie Corporation of New York, the New-Land Foundation, the Ploughshares Fund, the Prospect Hill Foundation, Longview Philanthropy, and individual donors. publications See all publications Global Risk Blog United States Discloses Nuclear Warhead Numbers; Restores Nuclear Transparency The Federation of American Scientists applauds the United States for declassifying the number of nuclear warheads in its military stockpile and the number of retired and dismantled warheads. 07.20.246 min read read more Global Risk Press release North Korean Nuclear Weapons, 2024: Federation of American Scientists Release Latest North Korea Nuclear Weapons Estimate North Korea may have produced enough fissile material to build up to 90 nuclear warheads. 07.15.243 min read read more Global Risk Press release Nuclear Experts from the Federation of American Scientists Call for More Transparency from the Defense Department with Its Decision to Certify the Sentinel ICBM Program Secretary Austin’s likely certification of the Sentinel program should be open to public interrogation, and Congress must thoroughly examine whether every requirement is met before allowing the program to continue. 07.09.243 min read read more Global Risk Blog Photo Depicts Potential Nuclear Mission for Pakistan’s JF-17 Aircraft Researchers have many questions about the modernization of Pakistan’s nuclear-capable aircraft and associated air-launched cruise missiles. 07.01.247 min read read more",
    "commentLink": "https://news.ycombinator.com/item?id=41040417",
    "commentBody": "United States Discloses Nuclear Warhead Numbers; Restores Nuclear Transparency (fas.org)132 points by philipkglass 20 hours agohidepastfavorite185 comments transcriptase 19 hours agoEvery time I’ve seen nuclear stockpiles and the reduction thereof discussed, I’ve wondered: Assuming for some reason the United States needed to ramp back up to an absurd number of warheads (ignore the MAD/political practicalities), how quickly could they do so? What’s the lead time or rate limiting factors in production? Because if they could start churning out a dozen or a hundred a week within a short period of time, why does the standing arsenal really matter? Does it really make a difference in global safety or geopolitics? I don’t know the first thing about the topic so this is all genuine curiosity, and I feel like the googling required to get an answer would put me on lists I don’t really feel like being on. reply jjk166 1 hour agoparentIt works the same as industrial capacity for things like planes or artillery shells. Once you ramp up production you can produce lots very quickly, the US produced 70,000 between 1945 and 1990, averaging 30 per week with a peak rate around 54 per week; but building out the factories to ramp up takes a long time - for the first 10 years the US averaged 6 per week. Most of the US's nuclear production capacity was dismantled. With WW2 levels of hustle and disregard for safety, we could probably build new facilities in around a year or two. These efforts would be pulling resources from attempts to ramp up production of other wartime necessities. Also you don't just need to build the nukes, you also need to build adequate delivery systems, which are all advanced aerospace manufacturing. If you eliminate your arsenal and then decide later you want it back, you're giving adversaries a lot of time to beat you to the punch, all the while advertising that you are pursuing nuclear as opposed to conventional weapons to fight your war. reply GemesAS 18 hours agoparentprevPit production is likely the rate limiting factor. We disassembled a bunch of AFAPs so have a lot of weapons grade plutonium around. But Pu is nasty to work with & Rocky Flats--the previous pit production facility--closed down years ago. Pit production moved to Los Alamos but it is at a much reduced capability. Also, Pantex--where nuclear weapons are assembled--isn't exactly the model for speed & efficiency. reply holowoodman 18 hours agorootparentI guess mass produced nukes would rather be enriched uranium based, due to the far easier construction. No fiddly implosive lens assembly. No weird multi-phase cristallization that goes critical if you blink. Metal that is merely as dangerous and nasty as lead, magnesium or arsenic, not plutonium. If you really want to go carpet-bombing with nukes, miniturization isn't as important as having a lot, quickly and reliably. reply sadhorse 17 hours agorootparentEnriching uranium is more expensive than making plutonium by a long shot. Modern nukes are two point implosion, not really fiddly. And when was the last criticality incident related to phase transition? Can't remember one. reply credit_guy 16 hours agorootparent> Enriching uranium is more expensive than making plutonium by a long shot I'm not sure. This used to be the case in WW2, but today enriching uranium is quite inexpensive. Here's an enrichment calculator [1]. The cost of enriching to 80% (weapons grade uranium) is $80000/kg, so you can enough uranium for a Hiroshima-style bomb for about $5 million. $5 million for a nuclear bomb is basically nothing. [1] https://www.uxc.com/p/tools/FuelCalculator.aspx reply harshreality 12 hours agorootparentprevDoes the U.S. make weapons-grade U235 anymore except for research? I thought gun-type fission weapons were phased out for safety and efficiency reasons. I also thought essentially all \"fission\" weapons today are fusion-boosted, and I thought the implosion type was the only production-ready design of fusion-boosted weapons. reply GemesAS 1 hour agorootparentThe US isn't producing HEU anymore but still has a decent sized stockpile of it. A number of the pits in the active stockpile are actually composite Pu/HEU pits & you can actually use HEU in implosion weapons as well. reply bbatha 4 hours agorootparentprevPlutonium is very corrosive and sensitive to phase changes so it needs to be refurbished and replaced regularly. The weapons grade plutonium lying around is probably not bomb ready. reply skellington 19 hours agoparentprevThere is no form of full scale nuclear war where the production apparatus for anything becomes a factor. reply slg 19 hours agorootparentThat is assuming nuclear war breaks out with zero warning. There is usually a build up to wars that could involve ramping up production of a nuclear arsenal before any nuclear weapons are actually used. reply roenxi 17 hours agorootparentWhat type of warning do you expect to see? We currently have a war in Ukraine involving between 2-4 of the major nuclear powers depending on how you want to count them (Russia, US, UK, China). Russia is bleeding heavily and it is hard to tell how close they are to some sort of internal crisis or collapse into groupthink by the military leaders. There have probably been Able Archer style near misses and we could have a repeat of the Cuban missile crisis without much changing. China is building up its nuclear arsenal and the political positioning in APAC suggests that a US-China war is on the cards. If we escalated in to full-scale nuclear war this July that'd be unexpected but we're way past 0 warning. There are lots of warnings. In terms of raw risk the last few years might be the biggest risk of a nuclear war breaking out that the species has ever faced. reply slg 15 hours agorootparentMaybe we aren't at 0 warning at the moment, but if there is a spectrum from 0 warning to imminent, we are close enough to 0 that the distinction doesn't really matter. The US, UK, and China are not actively fighting in Ukraine and even if they were, this wouldn't be the first time these countries have directly fought each other in a proxy war in the nuclear age. So unless you think the Russian military personnel that would actually carry out a full scale attack on the West would prefer destroying civilization to losing in Ukraine, I would expect some type of escalation beyond the position we have been in for the better part of the last 80 years. reply roenxi 13 hours agorootparent> I would expect some type of escalation beyond the position we have been in for the better part of the last 80 years. We have escalated beyond the point we have been in for the last 80 years. Russia have lost more troops than in any war since WWII. That is a lot of dead Slavs. Their strategic nuclear defences have already been attacked [0] and NATO currently appears to be organising direct strikes on Russian territory. They've made it quite clear that they want the war to continue until something in Russia breaks. When more warnings are you expecting to see? There are a lot of warnings out there. We could easily discover that someone tried to launch the nukes already in this conflict. It would be precedented; the situation is more tense than it ever has been before and we've had fortuitous near misses in similar situations. We're already in territory where we are rolling the dice for a catastrophe with low odds. [0] https://www.nytimes.com/2024/05/30/world/europe/ukraine-dron... reply slg 2 hours agorootparentOne or more nuclear powers has been at war for basically the entire nuclear era. They have all had wars in which they have \"lost more troops than in any war since WWII\". Even if this is the end of the Putin regime, this wouldn't even be the first time that the Soviet Union/Russia collapsed. I don't know what the path you think you see from where we are now to \"full scale nuclear war\", but it seems incredibly silly to suggest \"the situation is more tense than it ever has been before\", especially after you have already name checked Able Archer and the Cuban Missile Crisis. reply yencabulator 2 hours agorootparentprev> They've made it quite clear that they want the war to continue until something in Russia breaks. Huh? I'd expect most non-Russian-aligned parties would be happy to see Russia retreat from Ukraine, pay reparations, and call that a peace. Russia only needs to break if Russia persists in occupying other countries. reply TiredOfLife 5 hours agorootparentprevIs that NATO currently in the room with you? reply oceanplexian 12 hours agorootparentprev“Civilization destruction” isn’t a realistic scenario and I think people need to get over that. It’s not the 1980s. It’s almost certain what would actually happen is one or two pop off in a conflict zone like Ukraine and then nukes start getting used tactically like conventional weapons. The larger issue is once the “nuclear taboo” is broken nation states will start using them. Nukes aren’t magic, they’re just really big bombs. Most likely the smaller ones are more practical to deliver and will be used on military targets (Bunker busting, destroying fortifications, etc). It wouldn’t play out like Mad Max but basically WWII but with small nukes and regional missile defense systems playing a huge role. reply 542354234235 7 hours agorootparent>but basically WWII but with small nukes and regional missile defense systems playing a huge role So a total war scenario, but with multi megaton nuclear weapons? That sounds civilization ending to me. “There was a strong wind that night and as I came out of the shelter, all I could see around us was fire…burning clothing, 'tatami' mats, and debris were blowing down the road and it looked like a flowing river of fire… I remember seeing other families, like us, holding hands and running through the fires…I saw a baby on fire on a mother's back. I saw children on fire, but they were still running. I saw people catch fire when they fell onto the road because it was so hot.” [1] This isn’t an account of the atomic bombs. This is the firebombing of Tokyo, which killed more people and destroyed more homes than either atomic bombs. The US was firebombing Japanese cities week after week, leveling over 60 Japanese cities and killing between 330,000 and 900,000 people (though we will never know for sure because the very records needed were obliterated in the conflagrations). WWII destruction was limited completely by the technology of the time. Total war means total war. [1] https://www.dw.com/en/tokyo-firebombing-survivors-recall-mos... reply sandspar 12 hours agorootparentprevFrom ChatGPT: Normalcy bias is when people underestimate the possibility and impact of a disaster, believing things will always stay the same. It leads to inaction and unpreparedness during emergencies. reply temporarely 8 hours agorootparentprev> (Russia, US, UK, China) +France reply justin66 16 hours agorootparentprevDuring times of escalating tensions with a resourceful geopolitical adversary, you would try to cool things off with diplomacy but simultaneously... start building lots of new nuclear weapons? Smart! reply JumpCrisscross 19 hours agorootparentprev> no form of full scale nuclear war where the production apparatus for anything becomes a factor Where full nuclear war means a full exchange of strategic fire, yes. For tactical nukes or bombardment of a non-retaliating state, less so. reply llamaimperative 18 hours agorootparentIt's not clear there's any such thing as \"tactical nukes\" given that they're strategically useless, and it's actually not even clear there's such a thing as nuclear exchange that isn't full scale war. At least as told by Ellsberg in the Doomsday Machine, there was literally no mechanism for the US to launch a partial nuclear attack. reply cfraenkel 18 hours agorootparentAll these other comments should just go read the book, it's worth it and a good, if horrifying read. What 'no mechanism' above means is that for many decades the SIOP consisted of 'launch everything'. The only way it was a 'plan' was to time the arrival times to avoid fratricide. This btw meant that even if there was a 'tactical' shooting event in Western Europe, all the targets in China would have been hit, even if they weren't involved. Needless to say, Japan was never informed of this.... reply hyeonwho4 16 hours agorootparentFrom a MAD game theoretic perspective that makes a lot of sense. To avoid non-essential use of nukes, only give policymakers the option of launching everything. Then they will only launch in extreme circumstances. Hopefully only circumstances where there are already missiles inbound. This avoids the possibility of gradual nuclear escalation, which can be more easily miscalibrated. reply somenameforme 15 hours agorootparentprevThe main difference between tactical and strategic comes down to intended use. Tactical nukes are intended for battlefield use, strategic nukes are intended to end other civilizations. They also come in different delivery methods. For instance there are tactical nuclear landmines, artillery, and so on, whereas most strategic weapons are just going to be missiles and ICBMs in particular. But I do agree that the labeling is largely pointless because there are nominally \"tactical\" weapons with payloads exceeding 100kt. For contrast, Hiroshima (which was enough to destroy a mid-sized city and kill hundreds of thousands with a single bomb) was 16kt. So \"tactical weapons\" can easily destroy cities. Even if strategic weapons can be hundreds of times higher yield, at some point you're just beating a dead horse, or city as it may be. reply sillywalk 12 hours agorootparent> there are tactical nuclear landmines, artillery, and so on, Not sure about the landmines, but the US and USSR retired their nuclear artillery decades ago. I'm not sure how much effort it would be to put existing warheads inside shells, or about other countries. reply somenameforme 12 hours agorootparentI take the disarmament claims with some degree of skepticism. Alot of these weapons provide substantial flexibility and destructive capability, which superpowers are generally not fond of relinquishing. A lot of the nuclear disarmament stuff hit its peak in the years following the collapse of the USSR, at which point US and Russian relations looked very positive and optimistic moving forward. We're now back to lows not seen since the Cold War. In any case, for the specifics - Wiki gives 2004 [1] as the date the US reportedly dismantled its nuclear artillery, and in 2000 Russia reported that \"nearly all\" of its nuclear artillery had been dismantled. Nuclear landmines [2] fall under 'atomic demolition munitions' which are basically any sort of small/mobile nuke, so you get everything from landmines to the suitcase nuke weirdness. [1] - https://en.wikipedia.org/wiki/Nuclear_artillery [2] - https://en.wikipedia.org/wiki/Atomic_demolition_munition reply tyingq 18 hours agorootparentprevI take tactical to mean something like \" no mechanism for the US to launch a partial nuclear attack Yep, the trajectory to North Korea (from US mainland) has to pass over Russia and the Russians have to trust that it's not coming for them. Not that Russia would be okay with us striking NK in the first place, but you get the point. reply sillywalk 13 hours agorootparentAgainst North Korea why would the US even use an ICBM? Why not a B-2 flown from Guam or from the continental US? reply fshbbdssbbgdd 18 hours agorootparentprevAre you saying that just because the great circle from US to NK goes over Russia? Can we not fire on a less optimal trajectory? Or from a submarine? reply crmd 18 hours agorootparentprevYou can fire an SLBM from the Pacific or the Sea of Japan without traversing Russia or China. reply hypeatei 18 hours agorootparentAssuming you're striking first, yes. Nuclear subs take ~15 minutes to deploy, though, and that isn't the first option when counter striking. The U.S. president has six minutes to decide/launch a counter attack from the missile silos. Annie Jacobsen has a book \"Nuclear War: A Scenario\" on all this where she interviews high ranking officials and pries into government documents related to nuclear war. reply gpderetta 9 hours agorootparentOn the other hand, NK is not launching a first strike that can take out all US land-bases ICBM sites anytime soon. reply maxglute 14 hours agorootparentprevNK's geographic position is interesting. Unless US boat is launching east from PRC's Yellow / Bohai sea / PLAN bastion, there isn't a trajectory to NK that doesn't look like it's heading towards PRC mainland. And even then, unless timed during summer months, prevailing winds is going to push fallout / radiation towards BJ. During winter downwind will drift to SKR / JP / east coast PRC. I don't know what proportional counter retaliation is, maybe a few nukes off CONUS west coast urban centres, but PRC isn't going to sit there and eat incidental radiation over major population centres even if target is NK. reply data-ottawa 16 hours agorootparentprevWouldn’t it be more reasonable to Russia that the US is attacking NK and not just nuking Kamchatka? Can the trajectory of an ICBM be inferred by the height of it’s arc? reply hypeatei 16 hours agorootparent> Wouldn’t it be more reasonable to Russia that the US is attacking NK and not just nuking Kamchatka? I guess that depends on current relations between the two countries and assumes there wouldn't be a breakdown of communications when launches are detected. > Can the trajectory of an ICBM be inferred by the height of it’s arc? From the book I mentioned in another comment, Russia has very flawed satellite systems for tracking nuclear launches. There is a lot of focus on the fact that you don't have much time in the event of an imminent nuclear strike so I don't think there is much calculations being done if the missile is (generally) coming towards your homeland. reply data-ottawa 2 hours agorootparentThanks, that’s helpful, I’ll checkout that book reply nradov 15 hours agorootparentprevThere are a limited set of scenarios where a major nuclear state might use a tactical weapon against a lower-tier state. For example, if the USA got into a conflict with Iran and we had actionable intelligence that they were assembling a nuclear weapon in an underground bunker then we might take it out with a small number of tactical nuclear ground strikes. I'm not recommending this but you can game out scenarios where this seems like the least bad course of action. B-2 bomber crews regularly train for this exact mission. reply umbra07 14 hours agorootparentbut why use a nuke? we have all sorts of non-nuclear weaponry. we have bunker busters that can penetrate hundreds of feet. even if iran can't retaliate with nukes, the geopolitical cost would be insane. reply 542354234235 3 hours agorootparent>\"Iran’s underground nuclear facility could be between 80 meters (260 feet) and 100 meters (328 feet) below the surface... That could be a problem for the GBU-57 since the US Air Force stated that the bomb could rip through 60 meters (200 feet) of cement and ground before detonating. US officials have talked about detonating two of these bombs consecutively to guarantee the destruction of a location. However, the new depth of the Natanz tunnels still poses a significant obstacle.\" [1] [1] https://www.eurasiantimes.com/us-flaunts-massive-ordnance-pe... reply jmpman 14 hours agorootparentprevIran has some ultra tough concrete. I question if even our best bunker busters can penetrate them. reply skellington 18 hours agorootparentprevI dunno...too hypothetical a question to answer, since we already have enough nukes to destroy everything and nobody is going to reduce their arsenal to one. reply pythonguython 19 hours agorootparentprevAgreed. It’s over in half a day. Ramping up production is a rung on the escalation ladder. It’s generally good to have more rungs. reply toast0 15 hours agoparentprev> Because if they could start churning out a dozen or a hundred a week within a short period of time, why does the standing arsenal really matter? There's probably a declared number where this matters, but the current number of warheads is high enough that's there's no need to make more. 3,000 is plenty to retaliate against an opponent with 30,000. More doesn't provide a benefit. Nuclear disarmament, as practiced by the US and Russia is a negotiation to reduce the number of warheads in a coordinated fashion so that it's possible to convince warmongers on both sides that it's reasonable. The benefits are primary a reduction in cost to maintain and secure the warheads and a significant reduction in the risk of accidents related to the warheads. Mutual destruction is still assured --- you'd need a lot fewer warheads for that and involvement of other nuclear states; but then your question of production capacity would be more interesting. reply yencabulator 2 hours agorootparent> 3,000 is plenty to retaliate against an opponent with 30,000. More doesn't provide a benefit. I think a big part of this is that the long-distance missiles, when all of this was invented, were not very accurate. Sending 10 to do the job of 1 might have been necessary just to hit the intended targets. Modern missiles are quite capable of precision strikes. reply ianburrell 18 hours agoparentprevThe US has plenty of any conceivable war. There is probably long time to restart production since haven’t done it in a while. The big factor is that the deployment platforms are limited. There are 400 Minuteman III missiles sitting in silos. They could put more warheads on them, but those are sitting in storage. The same is true of Trident missiles on submarines. They could make nuclear gravity bombs but those aren’t really useful. We also have lots of those in storage. reply fragmede 15 hours agoparentprev> googling required to get an answer would put me on lists I don’t really feel like being on. But asking in the clear under the pseudonym \"transcriptase\" here isn't going to get you put on the exact same lists? How do you think this list making process works? reply transcriptase 14 hours agorootparentI would assume that asking a question in the comments section of a relevant article, and making clear why I’m doing so is slightly less flag worthy than randomly googling questions about logistics and production. reply TiredOfLife 13 hours agorootparentIf lists like that are being made, then Googling would put you on a huge automatic list that would be queried only as part of a targeted inquiry. But asking in comment section of relevant article would put you on a short \"immediate action\" list. reply GJim 10 hours agorootparentprevThis is the 'chilling effect' in action. reply justin66 16 hours agoparentprev> Because if they could start churning out a dozen or a hundred a week within a short period of time, why does the standing arsenal really matter? Possessing an overwhelming amount of retaliatory force and the combined ability and willingness to deliver it immediately in the face of an enemy's first strike serves a useful purpose for deterrence. \"Mutually assured destruction\" means that both sides are prevented from attacking, because the other side can respond in kind. It's irrational for either side to attack, since everybody would just die. (and yes, MAD comes with its own problems) The ability to build a bunch of bombs in the future is entirely unrelated. I mean, who cares? reply HaZeust 16 hours agorootparentI'd also like to add to this; that the ability to consecutively create additional warheads is not of any particular inherent value, especially when our reserve count is more than enough to wipe out any and all civilization - regardless of target diversity. It's not like missiles or ammo, where the more we produce in times of conflict, the more of an upper-hand we have. We've already reached the ceiling for the finite amount of nuclear warheads required to do the most conceivable damage. Beyond is irrelevant. reply hollerith 15 hours agorootparentWhat do you imagine happens when thousands of nukes explode? The Earth splits into pieces? reply HaZeust 14 hours agorootparentNuclear winter??? Strange question. reply hollerith 14 hours agorootparentIt's not a strange question: many falsehoods get repeated over and over on the internet and here on HN. The conversation around nuclear winter focused on burning petroleum storage tanks because (in contrast to burning houses and burning trees) those kinds of fires produce the darkest smoke with a particle size small enough to get high in the atmosphere and to stay in the atmosphere for a long time. \"100 oil refinery fires would be sufficient to bring about a small scale, but still globally deleterious nuclear winter,\" said one prominent paper. Then Saddam lit 700 oil wells on fire (and deployed land mines to slow down firefighters with the result that it took 7 months to put the fires out), and although there was some slight cooling effect, you really had to go looking for it with precision instruments to detect it at all: https://en.wikipedia.org/wiki/Nuclear_winter#Kuwait_wells_in... reply HaZeust 14 hours agorootparentWho said anything about petroleum storage tanks? The conversation around nuclear winter in relation to nukes is because it's an understood consequence of 100-some Hiroshima-sized warheads being detonated between two major city centers: https://en.wikipedia.org/wiki/Nuclear_winter#Climatic_effect... reply hollerith 14 hours agorootparent>Who said anything about petroleum storage tanks? Repeating myself: before the 700 Kuwaiti oil fires, the most influential scientists warning about nuclear winter, like Carl Sagan, relied heavily on petroleum fires to make their argument. reply defrost 14 hours agorootparentAs a completely serious question, what about the non-influencer scientists, the ones doing actual detailed physical modelling, what were they saying? As I recall from the time there were three camps on this: * pro MAD cold war political scientists who stressed that world ending Mutualy Assurred Destruction scenarios were essential to peace keeping, * antinuclear horrified scientists, Carl Sagan, Betrand Russell, et al who wanted disarmament and peace through understanding and stressed the world ending horror of nuclear weapons and nuclear winter and wrote a lot of papers light on detail. * actual working geophysicists modelling the world who seemed largely undecided about the actual threat of nuclear winter .. very much in the maybemaybe not camp. ADDED: I just read through the wikipedia Nuclear Winter article and seems (by my recollection) to have been culled in the decade since I last read it when (by my recollection) it referenced a great many more papers that fell on the probably not catastrophic side. It now appears to emphasis only papers that agree with the nuclear winter hypothesis. reply justin66 13 hours agorootparentprevI don't recall petroleum fires being a huge part of the dialog, back when nuclear winter was a big public topic, such that I'd bring it up first thing and to the exclusion of other concerns. It was part of it, for sure. There was at least one apocalyptic science fiction story about the Soviets testing a bomb underground and accidentally setting a massive oil field on fire. (it was called Anvil? Or written by Christopher Anvil? jeez, it's been a while...) But it's strange to see it commented on as the main concern. reply Yeul 9 hours agorootparentI saw 9/11 cloud on TV. That was one building. If Earth's megacities get nuked I refuse to believe that it would not have consequences for the climate. reply _visgean 19 hours agoparentprevIsnt the enriching process super slow? https://education.cfr.org/learn/reading/how-do-countries-cre... this article puts somewhere in months, but you are still limited by the number of centrifuges and probably some other factors. reply ianburrell 18 hours agorootparentThe US has tons and tons plutonium from old nuclear weapons. Enough to build tens of thousands of nukes. Also, centrifuge aren’t used by advanced nations to make nukes. They use plutonium from spent reactor fuel. The US has lots of spent fuel that could be reprocessed for the plutonium. reply somenameforme 15 hours agoparentprevLet's talk about just how destructive nukes are, because I think most people grossly underestimate this. One bomb in Hiroshima killed hundreds of thousands. And that was a tiny little bomb relative to modern standards - 16kt of yield in a mid-sized city of ~350k people. Modern tactical weapons (weapons intended for battlefield use) can have yields exceeding 100kt. Strategic weapons (weapons intended to end other civilizations) go into the thousands of kt. The strongest weapon ever tested being \"Tsar Bomba\" which had a yield of 55,000kt, so a few thousand times greater yield than the Hiroshima nuke - which was by itself enough to instantly destroy a mid-sized city and kill more than 40% of its population. I think it's easy to lose scale/context when looking at things like nuclear test footage, so let's go the other direction. This [1] is the \"Mother of All Bombs / MOAB / GBU-43\" that was detonated in Afghanistan. It's the second largest conventional weapon ever fielded, weighing more than 20,000lbs and and 30+ft long (so that little blip on the screen is 30 ft for scale). It had a yield of 0.01kt. So now imagine something with literally hundreds of thousands of times greater yield - that's a modern nuke. Or, if it helps for visualization purposes, imagine hundreds of thousands of those raining down - same net effect. So if nuclear war ever breaks out it's not going to be countries using their nukes to target isolated (and nuclear fortified) launch silos and bunkers in the middle of nowhere - they're going to try to destroy the other country (targeting things like population, economic, health, agriculture), so that they can completely eliminate the threat. And suffice to say - it won't take many nukes. The only reason you'd have thousands is to overwhelm any sort of future-tech missile defense systems as well as to eliminate any possibility for an effective first strike attack attack against you. Although even the nukes themselves are also designed to deal with missile defenses, with one missile often breaking up into multiple independent warheads on approach. This also maximizes the damage for reasons outside the scope of the post. --- So the point of this is that thousands of nukes is already enough to basically destroy every single major city in the world and thus destroy basically every single country in the world. There's no scenario where suddenly you need to scale up to tens of thousands of nukes or whatever. In fact nations like North Korea already clearly have an effective deterrent with a stockpile that's in the tens of missiles. [1] - https://www.youtube.com/watch?v=Q6rSxJnpGNg reply justin66 15 hours agorootparent> So if nuclear war ever breaks out it's not going to be countries using their nukes to target isolated (and nuclear fortified) launch silos and bunkers in the middle of nowhere - they're going to try to destroy the other country (targeting things like population, economic, health, agriculture), so that they can completely eliminate the threat. There are at least two falsehoods here. Of course missile silos will be targeted if it's possible to do so. If you're the Russians you might need to make an honest assessment of whether your weapons are accurate enough to destroy a hardened silo, but the US believes they can target silos (and has since at least the eighties). And prioritizing destruction of enemy population over destruction of the enemy's nuclear weapons and other military assets would just be dumb. reply somenameforme 15 hours agorootparentIt's not just about accuracy. Targeting silos comes with multiple problems. The first is that they are deep underground and fortified to withstand nuclear blasts. The second is that even if you believe you can disable a silo, there's a very good chance that by the time your nuke gets there - what was in the silo has already been launched. There are also other practical issues - you don't know where every silo is, there are likely dummy silos meaning you end up completely wasting a high yield weapon, and so on. US Cold War targets have been declassified. [1] That was from an era with less effective detection, and also where launching would generally involve planes, so airfields were targeted, but again you can see the extreme focus on agriculture, industry, medical, economic, and many targets simply labeled \"population.\" The USSR's target list would have looked, more or less, identical. Modern target lists likely aren't even bothering with silos and just going for complete destruction of the enemy civilization. Nuclear war, has as a prerequisite, the end of any sort of norms. It's not about destroying the opponent's military, but about literally destroying the opponent's country. Military can be rebuilt and redeployed - by targeting population, industry, economic, medical, population, and so on you completely eliminate the enemy's ability to ever be a threat again. [1] - https://www.nytimes.com/2015/12/23/us/politics/1950s-us-nucl... [1] - https://nsarchive2.gwu.edu/nukevault/ebb538-Cold-War-Nuclear... (a much more informative, but less approachable article/datacache) reply justin66 14 hours agorootparent> It's not about accuracy. In the case of US silos, it sure is. Nobody believes the doors on those silos would survive a direct hit (edit: meaning, a hit with a US warhead's sort of CEP accuracy), but if the warhead lands a mile away... > The first is that they are deep underground and fortified to withstand nuclear blasts. If you have any references indicating that Russian ICBM silos have been deemed by the US to be indestructible, I would like to read about that. It is possible to build a bunker deep underground that is difficult to destroy with a single warhead, yes, but what we're talking about is actual silos where ICBMs are deployed. > but again you can see the extreme focus on agriculture, industry, medical, economic, and many targets simply labeled \"population.\" I see that being referenced as one potential target (category number 275, out of how many I'm not sure) of many. Not the subject of \"extreme focus\" as you've said here, nor a target that would be prioritized over the enemy's military assets, as you suggest in a parent comment. (the real war crime is the design of that website) > Modern target lists likely aren't even bothering with silos and just going for complete destruction of the enemy civilization. I guess this is the gist of my disagreement with your comments. I have no idea why you would believe this. I'm not suggesting the people who do this kind of planning are humanitarians, nor am I suggesting I expect many people to survive a big nuclear exchange. My disagreement is: the idea silos would not be targeted by a party launching a first strike, in favor of hitting soft targets, is silly. edit: there's enough wrong here that I could go a little crazy with responses. here's just a little more. > There are also other practical issues - you don't know where every silo is, If you're the US government, you view it as your job to know where all the silos, and to the fullest extent possible all the warheads, are. (and if you're the adversary, you're interested in using your silos as a tool for deterrence and negotiation, which wouldn't work if they all existed in secret) > there are likely dummy silos meaning you end up completely wasting a high yield weapon, and so on. Russia's strategy has included road-mobile ICBMs that are deliberately difficult to track, but if they've ever built fake silos, I've never heard about it. During the cold war that would have been problematic - the treaties involved inspecting silos. Post-cold war... I don't know, what's the point? In any case, do you have any evidence that this is something they've done? I guess a cynical person could wonder about how well maintained those Russian ICBMs are today, and whether they're all really \"fake silos.\" ahem You read pretty negative things, but I've never seen anything that seemed better than rank speculation. reply somenameforme 13 hours agorootparentReread the source. We were aiming for quote, the \"systematic destruction\" of urban industrial targets. To be clear, that quote is coming from the released documents, not the site covering it. We were explicitly targeting population in each and every city, alongside other non-military targets. A \"dummy\" silo does not mean a fake silo, though those may also be used, but simply a silo without a live weapon. Silos are cheap and be constructed extremely rapidly. Beyond dummies, there's also the issues of them having already launched their payload, hardened against attacks (which does not mean immune), and so on. Then there's also the nuclear triad in that weapons will also be coming from the sea and possibly from the air as well. The goal of this obfuscation and deception is not to avoid masking how many weapons you have, but rather to prevent the enemy from being able to meaningfully disrupt your nuclear retaliation capability; in other words - to protect yourself against a nuclear first strike. In modern times it's unlikely either side believes they can significantly disrupt the opponent's nuclear retaliation capability (unlike in the past when strikes would generally have come from the air and had far lesser range overall), and so it simply makes much more logical sense to optimize the damage caused by your own strikes in pursuit of your opponent's \"systematic destruction.\" reply knappe 15 hours agorootparentprevTo add to this, the Tsar Bomba was so large, it created a shock wave that circled the globe _3_ times. reply 542354234235 3 hours agorootparentAnother fun Tsar Bomba fact. If you build a thermonuclear weapon’s bomb casing out of U238, the fast neutrons released by the fission/fusion reactions cause the U238 to fission, increasing the yield by about 50%, while also increasing the fallout. The Tsar Bomba variant tested was utilizing a lead casing, because the Soviets were worried that the 100MT version would kill the crew dropping it, and irradiate a significant amount of territory. It is weird to think about how the one tested was the half strength version. reply ein0p 14 hours agorootparentprevYou don’t have to imagine, here’s a web site that lets you model the impact: https://nuclearsecrecy.com/nukemap/. I positioned the “moderate” 1MT single warhead strikes at all nearby army and naval bases and concluded that I won’t be incinerated right away, but will die of radiation sickness and starvation instead. reply lumost 18 hours agoparentprevThe final cost of an h-bomb at industrial scale was estimated at 10k in the 1960s (for low yield). However for practical purposes. One simply needs enough missiles and warheads of sufficient yield to overwhelm opposing interceptors, avoid risks of pre-emptive strike/miss-fires+500. After that there are rapidly diminishing returns for more warheads. reply _aavaa_ 7 hours agoparentprevGiven the surprise with FOGBANK, I don’t have high hopes for it going fast. https://en.wikipedia.org/wiki/Fogbank reply _rm 11 hours agoparentprevNukes tend to blow up all your stuff quite a bit faster than you can build more of it. reply ranger_danger 17 hours agoparentprevI suppose the easier solution (not that I'm advocating it) would be to lie about disposing of them in the first place. reply corry 18 hours agoprevWhat an odd situation, where we can applaud \"the transparency\" (and I do, honestly) while the US is also simultaneously cheerfully delivering a public reminder that \"we have more than enough to absolutely annihilate anyone and everyone... so don't fuck with us.\" reply IIAOPSW 18 hours agoparentAs they said at the end of Dr. Strangelove, these things have absolutely no value as a deterrent if you don't tell anyone you have them! reply nimbius 15 hours agoparentprevMost of the stockpile remains or grows solely due to senators from colorado, Wyoming and montana who host the ground based strategic nuclear arsenal. Force reduction would mean lost jobs and revenue. At least the transparency can start a discussion hopefully. reply TiredOfLife 15 hours agorootparentAnd nothing to do with russia invading its neighbors and threatening to nuke anyone helping them. reply 542354234235 3 hours agorootparentProbably not. The US nuclear triad is very well known and doesn’t rely on exact numbers. It isn’t like Russia would think “maybe they decommissioned all their SSBN nuclear subs and we are in the clear” until they saw the yearly numbers. The point is that we already have plenty enough to obliterate any nation on earth in either a first or second strike. The arms reduction treaties and transparency have always been to reassure war hawks that we still have plenty to kill everyone, so we don’t need to spend money making more and risk an accident. If you want to saber rattle, you test new systems like hypersonic missiles that are capable of carrying nuclear warheads. Or you perform “military exercises” to show how capable and/or stealthy your nuclear subs and bombers are. reply golergka 16 hours agoparentprevWhat's odd about this? Both a good things worth applauding to. reply refurb 14 hours agoparentprev> so don't fuck with us. That's the entire point of nuclear weapons, isnit? reply pythonguython 19 hours agoprevHans has always been the guy keeping count. Just about anytime you hear someone cite the US stockpile count, it’s his number. Truly impressive how accurate he is able to be with no special accesses. Only 40 off. reply wahern 17 hours agoparentThe data had already been declassified up to 2020, so they only needed to estimate changes over the past 3-4 years: > Between 2010 and 2018, the US government publicly disclosed the size of the nuclear weapons stockpile; however, in 2019 and 2020, the Trump administration rejected requests from the Federation of American Scientists to declassify the latest stockpile numbers (Aftergood 2019; Kristensen 2019a, 2020b). In 2021, the Biden administration restored the United States’ previous transparency levels by declassifying both numbers for the entire history of the US nuclear arsenal until September 2020—including the missing years of the Trump administration. Source: https://www.tandfonline.com/doi/full/10.1080/00963402.2024.2... reply pythonguython 16 hours agorootparentI was unaware they published in the 2010s. Worth noting Hans was counting even before then reply PoignardAzur 19 hours agoprev3000 nukes is better than Cold War highs, but it's still massive overkill. Even with a 90% interception rate, 300 nukes would be enough to kill tens of millions of citizens of any country from the blast alone. If an enemy leader isn't deterred by that, 2700 extra nukes aren't going to change their mind. reply JumpCrisscross 19 hours agoparent> it's still massive overkill The bombs are there because “the only way to avoid being the victim of a nuclear first strike (that having the enemy hit you with their nukes) was being able to credibly deliver a second strike.” “Thus the absurd-sounding conclusion to fairly solid chain of logic: to avoid the use of nuclear weapons, you have to build so many nuclear weapons that it is impossible for a nuclear-armed opponent to destroy them all in a first strike, ensuring your second-strike lands. You build extra missiles for the purpose of not having to fire them.” https://acoup.blog/2022/03/11/collections-nuclear-deterrence... reply llamaimperative 18 hours agorootparentYou don't need a huge number of nuclear weapons to achieve this, you need nuclear weapons in secret and ideally changing locations (i.e. submarines) reply justin66 16 hours agorootparentThe fixed missile silos out in the middle of nowhere (sorry Montana!) serve a lot of useful purposes. If you do not intend to be the one who pushes the button and destroys the entire world, it is nice having weapons like that precisely because the adversary can see whether or not you've fired them. If the enemy can see that your ICBMs haven't launched and your bombers aren't in range to fire their cruise missiles, they know you haven't gone all the way. Additionally, silos like that are something the adversary must target if they have any hope to survive, which is one less warhead they can drop somewhere else (an air base, fleet, city, or whatever). A submarine is inherently less predictable, yes, and in terms of ratcheting down tensions that is not always great. reply 542354234235 3 hours agorootparentOne additional reason, that the enemy would have to destroy a large amount of American territory to realistically neutralize the threat. They can’t just target some military bases and subs in an attempt to cripple us while attempting to keep it limited. No half measures. reply jncfhnb 18 hours agorootparentprevWhich is a lot more fallible than just having a huge number reply JumpCrisscross 13 hours agorootparentprev> You don't need a huge number of nuclear weapons to achieve this, you need nuclear weapons in secret and ideally changing locations (i.e. submarines) No. Most fundamentally because secret deterrents are, by definition, unverifiable. reply SkyPuncher 16 hours agorootparentprevThis doesn't work since you can't really prove that your opponent doesn't know the secret or changing locations - either out of sabotage or technical advancement. reply nradov 15 hours agorootparentI can't imagine any sort of sabotage that would make SSBNs easier to locate without also being obvious to the crew. As for technical advancements, what are you thinking of there within the current laws of physics? No other country has the economic resources to blanket the ocean with detection platforms. Some researchers have proposed using satellites to detect submarine wakes but that would take a huge constellation and could only even potentially work if the sub was moving fast at a shallow depth. reply ls612 14 hours agorootparentSSBN stealth could be at risk by mid-century if current technologies continue to advance in surveillance and information processing. That would be enormously destabilizing from a grand strategic perspective but is still very much a future worry not a contemporary problem yet. reply maxglute 12 hours agorootparentprevPLAN copying / rotating fleet of DARPA ACTUV (ASW Continuous Trail Unmanned Vessel). Park them outside sub home ports (outside of territorial waters / contiguous zone but within active sonar range) and blast sonar to keep continuous track once they're underway. Likely then hand off to faster/more powerful surface ASW platforms. Combine with rumored mini nuke boats from PLAN, especially if autonomous improves to drastically reduce manning (less mouths to feed to match endurance of big nuke boats), basically tailgate nuke boats the second they leave parking lot. Trumpet following girl meme. Economically, hard to say, but only fraction of very expensive to operate 14 boomers/SSNs on deterrent patrol at any given time. That said, IMO hardly matters, sea/air leg of triad can be replaced by redundant / distributed land, not silos but road mobile TELs in hardened shelters probably for MUCH cheaper per warhead - PLA/PLARF model. Can have 1000s of cheap TEL trucks / fake warheads to play shell game and be just as impossible to decapitation strike as a few SSNs with 20 missiles. Mobile land leg just politically not sensible because it means unambigiously painting target on homeland, not just empty silo fields in bumfuck nowhere. IMO half the reason of sea/air leg now is they're out of sight/out of mind doing distant patrols. No one wants to see a nuclear TEL driving down the highway and process implications. If anti missile defense improves, cost/benefit of limited magazine SSNs gets even worse - a tube on a 2.5B+ nuke boat cost 100m to acquire + likely very expensive operation costs per shot to field. You can buy 500+ HEMTT 8x8 to hull around solid fuel ICBMs for that price. PRC can probably buy 1,000. Economics of nuke boats as delivery platforms outside of psychology does not seem to make sense, but civilian psychology when it comes to nuclear planning is very important. reply nradov 12 hours agorootparentThe level of resources it would take to even try something like that is way beyond what even China can afford. Just completely unrealistic. reply maxglute 11 hours agorootparentWhich part specifically? Darpa toying with ACTUV expliclity because it was _dirt_ cheap vs relying on large surface combatants + air for ASW - small hulls with 10-20k per day operating cost vs 500k-800k per day ASW destroyer (US costs). It would be cheap to build out a fleet of ~50 small autonomous / minimally manned surface/subsurface fleet to follow the handful of SSNs on deployment at any given time. 30-80 ACTUV for surface ASW (US costs) is already 5-10 shadowing each deterrent patrol SSN. Economy of scale enters picture even if nuclear powered - danger is mostly political, parking reactors in adversaries EEZs. PLA relative cost vs USN acquiring/maintaining SSNs likely much cheaper / lopsided. Look at PRC's 80 type 22s that existed basically on the hope that it can deter the 1-2 carriers US normally throws in theatre. As for mobile TEL / mobile land triad, like 30k+ HEMTT+LVSR has been built, and that's just Oshkosh Defense building tactical trucks. # of USD 300-500k heavy/tactical trucks globally in 100,000s. Add in land infra/tunnel/harden costs and you won't get close to 100m per shot. Can probably get Oshkosh to hammer out budget decoy TELs with autonomous driving without any of the the expensive TEL/launch hardware. Say what you will about US industry, US auto still pretty capable of building things that roll on wheels vs US ship building. SSNX is projected to be 5-7B per unit (200m+ per launch tube), that buys you a lot of road mobile launchers. E: USN plants to acquire like 30 of them. I would be very surprised PLAN needs to spend 150-200B on a ASW UAV fleet to counter that. That's ~400-600 054s frigates. An ACTUV drone would cost fraction of manned frigates. I think you're dramatically underestimating just how expensive US SSN force is / will projected to be and how economics will incentivize scope of counters. reply golergka 16 hours agorootparentprevSecurity through obscurity is fragile. Strength in numbers — less so. reply buildsjets 16 hours agorootparentprevThus, the need for the Air Force to appropriate a multitrillion dollar black-box budget to develop a Sub-Supersonic Invisible and Noiseless Defensive Second-Strike Offensive Attack Bomber that flies faster than light so that you can bomb someone yesterday. reply PoignardAzur 7 hours agorootparentprevI could argue the claim on its merits, eg that the world has changed since The Delicate Balance of Terror was written in (checks) 1958, that omnipresent satellite surveillance means that a first strike could never wipe out the enemy nuclear arsenal, etc. But I think that's giving Brett too much credit here. His argument rests purely in the realm of game theory and logical-sounding ideas. In actual practice, the US military has never in its existence ran an analysis of how many nuclear weapons would be necessary to achieve strategic objectives in any specific scenarios. Brett later points out that: > This buildup, driven by concerns beyond even deterrence did lead to absurdities: when the SIOP (‘Single Integrated Operational Plan’) for a nuclear war was assessed by General George Lee Butler in 1991, he declared it, “the single most absurd and irresponsible document I had ever reviewed in my life,” Having more warheads than targets had lead to the assignment of absurd amounts of nuclear firepower on increasingly trivial targets. Brett notes this, but it doesn't seem to give him pause or to cause his to reevaluate the validity of the doctrines he cites, even though those doctrines were largely written to justify what he rightfully describes as absurdities. The US military has always, from the moment the nuclear bomb was invented, operated with the mindset of \"more nukes is better\". There is no conceivable number of nukes that would make the military go \"okay, that's enough, we have enough to achieve our strategic objectives in any plausible scenario\". As the quote above points out, giving them more nukes just makes them assign more per potential target. The only administration that chose to conduct a survey of the SAC's war plan for deploying nukes, the fucking Bush administration under Dick Cheney, found that the plan was ridiculously overkill (hence the quote above) which directly lead to the US signing the Strategic Arms Reduction Treaty.[1] HN commenters in this thread are giving a bunch of rationalizations why the US's nuclear policy is perfectly reasonable game theory, but any times military analysts with clearance actually looked at the US's nuke arsenal and the plans to deploy it, their conclusion was the same: \"We have way more than we need\". [1] https://asteriskmag.com/issues/01/the-illogic-of-nuclear-esc... reply ranger_danger 16 hours agorootparentprevThey also have \"dial-a-yield\" now so the stockpile of larger weapons can be used on smaller targets without ridiculous fallout. reply trynumber9 19 hours agoparentprevThey aren't targeting cities anymore but Russian and Chinese weapon installations. Consider the number of these. And that the ballistic missile defense is concentrated at these locations. It is surely more than 300. reply PoignardAzur 7 hours agorootparentI already gave another in-depth reply, so I'll keep this one short: the idea that the current US nuclear arsenal would be fully needed to cripple Russia or China's military and industrial capacity is ridiculous, and has been thoroughly rejected any time military analysts were actually asked by the government to make a survey of the US's nuclear plans (which is, not that often). You don't need to bomb every station of a train line to cripple the line. If you want to stop car production, you don't need to blow up the car factory, the bolt factory, the windshield factory, and every single rare earth mine in the country. Yet those are the kind of assumptions the US doctrine relies on. Quoting from [1]: again: > Another jaw-dropping example: One part of the nuclear war plan called for destroying the Soviet tank army. As a result, JSTPS aimed a lot of weapons at not only the tanks themselves, but also the factory that produced the tanks, the steel mill that supplied the factory, the ore-processing facility that supplied the steel mill, and the mine that furnished the ore. [1] https://asteriskmag.com/issues/01/the-illogic-of-nuclear-esc... reply llamaimperative 18 hours agorootparentprev> They aren't targeting cities anymore Citation needed. reply trynumber9 17 hours agorootparentFrom when the US had 3x as many warheads https://www.gao.gov/assets/nsiad-91-319fs.pdf reply llamaimperative 5 hours agorootparent> We did not have access to the policy documents used by DOD’S war planners or the details of the U.S. strategic nuclear weapons targeting plans. This limited our ability to verify that the process of transforming policy into targeting options functions as described by DOD officials. This document is a description of the targeting process, which existed even when the actual output of that process was \"every single city in Russia and China with a population greater than 50,000\" reply jupp0r 17 hours agoparentprevThere is no system in existence that would provide a 90% interception rate. Existing anti ballistic missile systems are designed to intercept smaller attacks with low single digit warheads, no MIRVs and no decoys not a full scale attack. reply dyauspitr 18 hours agoparentprevGood. It needs to be a threat with plenty of teeth. reply FredPret 20 hours agoprevMax warheads = 31k Current warheads = 3.7k I wonder how long a nuke in storage lasts - ie, how much work does it take to maintain a stockpile of x nukes, and if you can turn those swords into ploughshares relatively easily. reply openasocket 19 hours agoparentI think it’s shorter than you would imagine. I recall an episode of the podcast Arms Control Wonk talking about the nukes in possession of Ukraine during the collapse of the Soviet Union. Professor Lewis stated that those warheads likely had a service life of 5-10 years. But that may be specific to those Soviet warheads, and I think that different components need to be replaced at different intervals. reply hollerith 17 hours agoparentprev>I wonder how long a nuke in storage lasts Russia's nuclear stockpile -- at least the strategic warheads -- have all been built anew since the end of the Cold War. The US is also modernizing its stockpile in the same way, but it has not finished yet. \"built anew\": made with all new components except that the fissile material is recycled from an old Cold-War-era warhead. (They probably re-cast and re-machine the fissile material.) The reader might be asking, How can Russia, a poor country, afford that? Well, nukes aren't that expensive once you have the fissile material and the design and manufacturing expertise and infrastructure. The pay for the soldiers to guard the nukes and constantly be on the ready to launch them is more expensive, according to one report I saw recently (and Russia has low personnel costs). reply philipkglass 19 hours agoparentprevIt takes quite a bit of work to maintain nuclear warheads. All active US weapons contain plutonium 239, which has a half life of 24,100 years. It's radioactive by alpha decay, which leads to changes in the material properties due to energetic collisions and the buildup of microscopic helium bubbles (alpha particles are merely ionized helium nuclei, so stopped alpha particles become helium). Since the US stopped testing actual nuclear warheads in the early 1990s, it takes a great deal of indirect theoretical and experimental evidence to make sure that nuclear warheads are reliable without live fire tests. That's part of \"stockpile stewardship.\" [1] If the plutonium has deviated too far from its original mechanical behavior, it would need to be removed from warheads, purified, and remanufactured into replacements that match the original specs. And again, the rebuilt components need to be reliable but they can't actually be tested via explosion. US weapons also rely on tritium gas \"boosting\" to operate reliably and efficiently [2], and tritium decays with only a 12.3 year half life. The gas reservoirs of weapons need their tritium replaced at significantly shorter intervals. Even manufacturing enough tritium to maintain the stockpile has become a challenge because the US has retired its Cold War era weapons-material reactors that used to operate at Hanford and Savannah River. Currently the US uses a power reactor owned by the Tennessee Valley Authority to make tritium for weapons [3]. It's possible to make nuclear weapons (even thermonuclear weapons) with only uranium 235 for fissile material and no stored tritium. Such weapons could last a much longer time without active maintenance, since U-235 decays thousands of times slower than Pu-239. However, they would be larger and heavier for the same explosive yield, which complicates delivery. They would also lose certain safety features. Finally, without being able to perform full scale tests, it is doubtful that the US would have the confidence to replace its current high-maintenance weapons stockpile with a new generation of low-maintenance weapons. [1] https://en.wikipedia.org/wiki/Stockpile_stewardship [2] https://nuclearweaponarchive.org/Nwfaq/Nfaq4-3.html#Nfaq4.3.... [3] https://www.wvlt.tv/2022/05/24/watts-bar-lone-source-nuclear... \"Watts Bar lone source of a nuclear weapon material; TVA increasing production\" reply bamboozled 19 hours agorootparentI really wonder what the state of Russia's nuclear arsenal is like then? Better or worse? Maybe that still have a lot of the old Nuclear power stations running to better supply the materials to maintain their warheads ? reply danielodievich 18 hours agorootparentThis is a frequent topic of discussion in various forums and I am sure by Very Serious People in Charge [TM]. Extrapolating from the general sad state of the weapon systems in use by Russians where they are at this point unpacking tanks made in 1950, the quality of maintenance on their vehicles, and is difficult to plausibly claim that all the ancient rusty USSR stuff across the strategic rocket barrier is in any sort of usable shape. Now, if an order to launch is given, some rockets may launch, some of those may actually fly, some of those flying actually get somewhere, and perhaps some of THOSE may actually detonate should they reach the target, and maybe if you're lucky at the designed yield. The percentages in that funnel that aren't known. And nobody [perhaps except some crazies] wants to find them out because even one in the middle of big city is enough. reply hollerith 17 hours agorootparent>difficult to plausibly claim that all the ancient rusty USSR stuff across the strategic rocket barrier is in any sort of usable shape. There is no more ancient rusty USSR stuff -- at least in the Kremlin's strategic nuclear arsenal: https://news.ycombinator.com/item?id=41041532 reply bamboozled 15 hours agorootparentWithout evidence, that comment is an opinion piece. Hopefully we never have to find out. reply hollerith 15 hours agorootparentCould it be that the source of your skepticism is the fact that you have over the last 2 years seen many comments here on HN asserting that Russia's military is probably incapable of maintaining an effective strategic nuclear capability whereas my comment is the first one you've seen that takes the opposite position? According to one comment I saw here a few weeks ago, Russia's nukes are probably made of wood. I suggest searching the web for \"Russian nuclear weapons modernization\", restricting yourself to credible news outlets. reply bamboozled 7 hours agorootparentStill feels like an opinion, as the other commenter said, look at the state of their equipment when going against Ukraine, there is little indication their nukes or their silos, bombers are much better except your opinion. Opinions are fine, but the evidence against your opinion is currently stronger. reply hollerith 4 hours agorootparentWhen credible news organizations repeat Russian claims that they've completed a modernization of their strategic nukes then years and years go by with no expert or credible source (that I have noticed) contradicting the Russian claims, that is evidence. Who besides the Russian military might know about the state of Russia's stockpile? Spies for one. And until 2022 US nuke experts regularly inspected all Russian strategic nuclear sites per the START 3 treaty. They probably weren't able to disassemble any warheads as part of their inspections, but there's a lot you can learn merely by, e.g., measuring the gamma rays produced by the warhead. Also, you do realize that Russia is currently winning in Ukraine -- in part because they have an equipment advantage over Ukraine? E.g., the glide bombs they recently developed have proven effective. reply racional 3 hours agorootparentAlso, you do realize that Russia is currently winning in Ukraine They are not \"currently winning\". By any objective measure, the war on the ground is currently a stalement. Only problem is that in the long term -- stalemates never work out for the occupiers. In Russia's case: If the situation continues as it does, and moving at the glacial pace that it does, and draining 10 percent of its GDP every year -- they will ultimately have to give up on their optional neocolonial adventure, pick up their toys and go home. reply GJim 10 hours agorootparentprev> I really wonder what the state of Russia's nuclear arsenal is like then? Does it matter? The thing about a nuclear *deterrent* is that it doesn't have to work. There just has to be a realistic possibility that (at least some of it) it might. reply Loughla 19 hours agorootparentprevHow do you know all this? Is that a hobby or a job? reply philipkglass 19 hours agorootparentIt's a hobby. I read a lot and I have enough formal education to digest primary sources (mostly; my highest qualification is auditing a neutronics course while in grad school). If you too would like to know way more about nuclear weapons than is useful in civilian life, I'd recommend reading: Richard Rhodes, The Making of the Atomic Bomb The nuclear weapons FAQ, authored by Carey Sublette, a hobbyist researcher who is extraordinarily dedicated to understanding nuclear weapons from declassified documents and physical principles: https://nuclearweaponarchive.org/Nwfaq/Nfaq0.html Anne C. Fitzpatrick's dissertation Igniting the Light Elements: The Los Alamos Thermonuclear Weapon Project, 1942-1952: https://www.osti.gov/biblio/10596 The Arms Control Wonk blog/podcast: https://www.armscontrolwonk.com/ Nuclear historian Alex Wellerstein's blog Restricted Data: https://blog.nuclearsecrecy.com/ The nuclear weapons subreddit, particularly posts on it authored by Alex Wellerstein, Carey Sublette, and a few others whose names currently escape me: https://old.reddit.com/r/nuclearweapons/ Chuck Hansen's book \"U.S Nuclear Weapons: The Secret History\" (out of print, sadly; will have to pay $$$ or find a scanned pirate copy) and his massive book/PDF \"The Swords of Armageddon\" available for purchase here: http://www.uscoldwar.com/ reply MengerSponge 18 hours agorootparentLet me add \"Inventing Accuracy\" to your list of recommended reading. It's fascinating, and it's a powerful microscope to reveal the relationship between strategic need and technological development https://mitpress.mit.edu/9780262631471/inventing-accuracy/ reply nulltxt 19 hours agoparentprevhttps://www.youtube.com/watch?v=N2OUzBrLEFk is a interesting video about the transport of these weapons reply mikewarot 19 hours agoparentprevPlutonium pits slowly transmute, and the emitted alpha particles are trapped and form helium bubbles. They have to be replaced periodically. reply wdh505 19 hours agorootparentI think the tritium \"preignition\" has to be replaced every 10 years. reply jmyeet 19 hours agoparentprevSo this varies depending on what kind of nuclear weapon is and the delivery system. The major deterrant is the LGM-30G Minuteman III [1]. Most of our rockets use liquid propellants. Since the alert window is under 10 minutes, you can't keep a liquid-fuelled rocket permanently fueled so the Minuteman was developed as a solid rocket fuel booster. There's a whole team responsible for maintaining the boosters and warheads of this first line of defense [2]. But there are a variety of other systems. Some dropped by strategic bombers, others on mobile launchers, shorter range missiles deployed in Europe (eg MRBMs in Turkey), nuclear weapons deployed on submarines and so on. Also you have a mix of types. AFAIK the US was moved away from highly-enriched uranium weapons in favor of plutonium. Or at least, HEU reactors have shut down. Maybe there's a sufficient stockpile? Also, a lot of these weapons will be thernonuclear so you have to worry about the production and storage of tritium. IIRC a lot of tritium is a byproduct of plutonium production. Maintaining a significant nuclear arsenal is actually really complex and expensive. [1]: https://en.wikipedia.org/wiki/LGM-30_Minuteman [2]: https://minutemanmissile.com/missilemaintenance.html reply sitharus 18 hours agorootparent> you can't keep a liquid-fuelled rocket permanently fueled so the Minuteman was developed as a solid rocket fuel booster. You absolutely can! The Soviet doctrine was to use storable liquid propellants in their ICBMs - typically unsymmetrical dimethylhydrazine (UDMH) as the fuel and nitrogen tetroxide as the oxidiser. I don’t know if they need the fuel/oxidiser replaced periodically but that combination is storable for over a decade. The US went with solid rockets as they are more reliable - no turbines or valves etc - at the expense of performance, but the US perfected making large solid rockets before the USSR. The USSR however perfected oxidiser-rich staged combustion which extracted a lot more performance. Storable liquid propellants are still used on satellites and deep space missions that need to perform large course corrections during their missions. reply sillywalk 13 hours agorootparentprevForgive me if you meant all of your comment historically. > Most of our rockets use liquid propellants. Which ones? As far as I know the US only has solid fuel nuclear armed missiles. The Minuteman and the Trident. > others on mobile launchers, shorter range missiles deployed in Europe (eg MRBMs in Turkey), The US only has aircraft dropped bombs in Europe. The US retired their nuclear capable rockets and cruise missiles under the INF treaty in 1988. They retired their nuclear artillery etc at the end of the Cold War. The (liquid fuelled) Jupiter missiles were removed from Turkey in 1962 after the Cuban missile crisis, in exchange for the USSR removing their nukes from Cuba, though there are still US nukes in Turkey. > Also, a lot of these weapons will be thernonuclear All of them are. reply lmm 19 hours agorootparentprev> Since the alert window is under 10 minutes, you can't keep a liquid-fuelled rocket permanently fueled so the Minuteman was developed as a solid rocket fuel booster. Huh? The Titan II was developed to do precisely that and worked that way for decades, they were liquid-fuelled and kept fuelled in their silos. reply openasocket 18 hours agorootparentLiquid propellants are generally less stable than solid fuel. I had read in multiple places (from reputable sources) that you generally couldn’t keep liquid missiles permanently fueled like that. The propellants are extremely corrosive and dangerous. But you are right that the Titan II is liquid fueled and was kept permanently fueled in the silo. I’m not entirely sure how to resolve those two facts. The Wikipedia page about the Titan II does mention multiple accidents and fatalities related to propellant leaks, so I’m guessing that they were just more risky to operate? reply mr_toad 18 hours agorootparentThe main problem with traditional liquid fuelled rockets is keeping the propellant cold. It’s not feasible to keep a rocket with liquid oxygen fuelled for long periods. The Titan missiles use hypergolic fuel which is more storable, but also extremely toxic and volatile. More than 50 people have been killed in accidents involving this rocket. reply silverquiet 18 hours agorootparentprevThere was an incident where a technician dropped a large socket down a silo that impacted the side of a Titan missile and set off a chain of events that ended in an explosion that nearly detonated a nuclear bomb on US soil. reply sillywalk 14 hours agorootparentThe Damascus Incident[0]. There's a book called Command and Control: Nuclear Weapons, the Damascus Accident, and the Illusion of Safety by Eric Schlosser that details the incident, and a PBS based on the book about it. [0] https://en.wikipedia.org/wiki/1980_Damascus_Titan_missile_ex... reply knappe 13 hours agorootparentI was hoping this book would be a recommended. It really, really focused, reinforced is really not the right word to use here, my views on nuclear weapons. This should be required reading. reply justin66 15 hours agorootparentprevIt helps if you think of liquid-fueled rockets relying on cryogenics as something entirely different than those using hypergolics. Cryogenics can't sit there for a long time but hypergolics can. reply jiggawatts 19 hours agoparentprev> how long a nuke in storage lasts Decades, certainly. All but the first few generations of bombs were designed for long periods of storage. Notably, many of the TOP500 supercomputers were built with the singular goal of simulating the ageing of nuclear weapons in storage. If a supercomputer is owned by the DoE or SANDIA, then that's what it is for. > turn those swords into ploughshares relatively easily. Yes! Cold-war era warheads from both the Soviet Union and the US have been used as nuclear fuel. A notable one was the Megatons to Megawatts program: https://en.wikipedia.org/wiki/Megatons_to_Megawatts_Program The plutonium in bombs is essentially \"super high grade\" reactor fuel. Even degraded after decades in storage it is still far, far better than what is typically used. It just needs to be converted into the MOX (metal oxide) fuel pellets and then used in a reactor, pretty much as-is. reply dekhn 19 hours agorootparentWhile many supercomputers were funded by stockpile stewardship, the goal was to produce high performance computers capable of a wide range of simulation needs. One good example would be NERSC at LBL- it's unclassified research only, and their series of supercomputers were never intended to simulate ageing nuclear weapons. Hard to say exactly what goes on in the classified supercomputers, but they certainly weren't spending much of their time simulating aging nuclear weapons- that was the ostensible reason. reply johnohara 19 hours agoprevIs it possible that over the years the number of nuclear devices per warhead has increased? 1, 4, 8, etc. per warhead, thereby increasing your capability while claiming a reduction? reply BMc2020 18 hours agoparentWhat you are thinking of is (or used to be, called a MIRV, a mulitiple independently-targeted re-entry vehicle). There's still only one atom bomb per warhead, but 6 warheads per mirv. Think of the MIRV like a cylinder on a revolver with 6 warheads per MIRV like the bullets in it. The warhead has the minimum amount of hardware necessary to make it blow up, while the MIRV has the computers and rocket engines on it. So the MIRV zips forward, backward, up down, then releases a warhead at the right moment for the warhead to fall to the ground. (which has no movement capability at all, not even fins). Then the MIRV zips around and releases another warhead and so on until all 6 are gone. That being said, there are LOTS of ways to deliver warheads. The one that scares me the most is that the Russians have hidden ones pre-positioned in our 40 biggest cities or so. Fun Fact: The russians don't even have to fire their missiles to wipe us all out. They could set them all off in their silos and create a nuclear winter that would accomplish the same thing. edit: Sorry to pile on, you went from 0 to 3 replies in the time it took me to write this. reply jcranmer 13 hours agorootparent> Fun Fact: The russians don't even have to fire their missiles to wipe us all out. They could set them all off in their silos and create a nuclear winter that would accomplish the same thing. There's a lot about nuclear winter that is controversial, but having nuclear weapons go off in their silos is one thing that almost everybody can agree can't cause a nuclear winter. The basic premise of nuclear winter is a) nuclear explosions on cities cause massive uncontrollable firestorms b) that pump soot into the stratosphere c) which causes massive global cooling. If any one of those links in the chain fails to hold, then nuclear winter just can't occur. A nuclear weapon going off in its silo will be more of a massive earthworks project than a firestorm, especially if the silo isn't located in the heart of a city with lots of juicy combustible material to cook off all at once. reply johnohara 18 hours agorootparentprevWhat I was thinking was that whenever the U.S. enters into any form of weapons reduction agreement there has to be an enormous amount of internal reluctance to actually make those reductions. Reagan's so-called \"trust, but verify\" policy. I find it difficult to accept that any party to those agreements would actually reduce anything without having equivalent plans B, C, and D. I am not a weapons expert so thank you for your insight. reply holowoodman 18 hours agorootparentAll those arms-reduction treaties are accompanied by some regime of mutual inspections and checks. Each party visits the other's facilities and counts stuff, assesses production and storage capabilities, paperwork, etc. And compares those results to their otherwise-obtained (read: by spies) data about the other party's weapons counts. Of course it isn't foolproof, and each party can and will maybe try to sneak a few more warheads somewhere. But those inspections at least provide some rough limit on the sneakiness, because if they had more than X% more, we would have noticed or so... reply hollerith 17 hours agorootparentprevTill 2022 Washington and Moscow were regularly inspecting each other's nuclear installations to verify compliance with the treaty. reply liamwire 13 hours agorootparentprev> The one that scares me the most is that the Russians have hidden ones pre-positioned in our 40 biggest cities or so. I find it hard to believe you seriously consider this, even in the absurd world of MAD-driven decisions. Simply put, for it to be even remotely likely would require that none of these devices had as of yet been discovered, nor the intelligence nor logistics surrounding them been compromised or otherwise intercepted. Consider that if such a situation were to be true, and uncovered, that the only possible responses would be either immediate action to have them removed, immediate retaliation, or allowing them to exist. In the first two scenarios, the weapons are no longer relevant, whether because they’re removed or war has started. In the final scenario, we’re functionally at the same place as we are with the traditional nuclear triad, albeit far closer to the precipice due to reasons made clear in the Cuban Missile Crisis. All of this to say, the challenges, costs, and risks of enacting such a situation, as utterly ridicule-worthy in their totality as they are, can perhaps be hand-waved away by pointing to other Cold War era events. However, to argue this has actually been done, despite the entire lack of any strategic benefit, and the immeasurable net loss of position and risk to the Russians that results? Come on. reply literallycancer 14 hours agorootparentprevThe nuclear winter thing is based on the paper where they assume that there's zero days of stockpiled grain isn't it? When in reality we could go for years with just the grain farmers keep to balance out market volatility. reply alexb23 18 hours agoparentprevNo, a MIRV [1] missile has multiple warheads, each warhead has a single device and is counted as such. [1] https://en.m.wikipedia.org/wiki/Multiple_independently_targe... reply Scaevolus 18 hours agoparentprevThe US recently improved trigger accuracy, detonating warheads at precise distances to maximize kills on hardened targets, which had the effect of increasing the number of effective warheads. reply sillywalk 14 hours agorootparentI remember reading about this a few years ago... the MC4700 \"super-fuze\" It looks like it was deployed back in 2009 on the warheads on Trident SLBMs. From [0]: \"Before the invention of this new fuzing mechanism, even the most accurate ballistic missile warheads might not detonate close enough to targets hardened against nuclear attack to destroy them. But the new super-fuze is designed to destroy fixed targets by detonating above and around a target in a much more effective way. Warheads that would otherwise overfly a target and land too far away will now, because of the new fuzing system, detonate above the target. The result of this fuzing scheme is a significant increase in the probability that a warhead will explode close enough to destroy the target even though the accuracy of the missile-warhead system has itself not improved. As a consequence, the US submarine force today is much more capable than it was previously against hardened targets such as Russian ICBM silos. A decade ago, only about 20 percent of US submarine warheads had hard-target kill capability; today they all do.\" [0] https://thebulletin.org/2017/03/how-us-nuclear-force-moderni... reply ianburrell 18 hours agoparentprevNuclear warhead is a single nuclear device. Are you thinking MIRV where multiple nuclear warheads are mounted to missile? The US has down rated lots of SLBMs and ICBMs recently. Many of the warheads in storage are from missiles. They could put them back, but then they would show in the active count. reply johnohara 18 hours agorootparentThat's insightful because it means it may be strictly an accounting issue versus an actual reduction in the number of warheads i.e. the number of warheads counted == the number mounted, not the total number possessed. reply holowoodman 18 hours agorootparentThe number of mounted warheads is a very important issue, because a nuclear war will take at most half a day. So all the mounted warheads at that point are the ones that can be used, nothing more. Of course if the war doesn't come as a surprise but with a lot of buildup, then you can remount everything over a few weeks. reply ianburrell 17 hours agorootparentprevThe number mounted matters cause it determines how missiles can be used. With multiple warheads, ICBMs could be used for first strike. First strike won’t work, but opponent still needs to worry about it. With one warhead, ICBMs are only useful for retaliation strike. Also, the US is still following New START treaty after Russsia pulled out. reply jauntywundrkind 14 hours agoparentprev> LGM-118 Peacekeeper > MIRV ICMB produced and deployed by the United States from 1985 to 2005. The missile could carry up to twelve Mark 21 reentry vehicles (although treaties limited its actual payload to 10), each armed with a 300-kiloton W87 warhead. Initial plans called for building and deploying 100 MX ICBMs, but budgetary concerns limited the final procurement; only 50 entered service. Disarmament treaties signed after the Peacekeeper's development led to its withdrawal from service in 2005. https://en.wikipedia.org/wiki/LGM-118_Peacekeeper So, that's what we had been doing for a couple decades. The most crazy ass nuclear cluster bomb. Now we're still trying to replace the LGM-30 Minutemen ICBMs we have had since 1962: Northrop Grumman's LGM-35 Sentinel. And it's taking forever & costing an unbelievable sum ($200B, $210M/missile including ground systems, although they're back to the drawing board to try to get costs down). https://www.defensenews.com/air/2024/07/08/pentagon-keeps-co... This is after Minuteman was ~$7m/missile, a super-cheap cast-solid-fuel design, with McNamera shutting down efforts on more expensive & fancy Atlas and Titan missiles. Weighing 1/9th the weight of the monstrous Soviet R7. One persistent dude (Hall) convinced everyone we didn't need fancy we needed a survivable competent second strike capable missile swarm. Minuteman is wild. There are some great submissions on it; the communication network submission from three days ago was fabulous & shows very much a Paul Baram of RAND/Arpanet style network resiliency idea. https://en.wikipedia.org/wiki/LGM-30_Minuteman https://news.ycombinator.com/item?id=41019604 Also of note, Minuteman's original D-17B computer is also quite the thing. There are some great submissions on it. It uses an early hard-disk like thing as working memory. It uses diode-resistor logic (DRL) since diode-transistor-logic (DTL) wasn't reliable enough yet. Incredibly stunningly built guidance computer that was core to a reliable totally cutting edge inertial guidance system. Anyhow yeah, we built them decommissioned utterly crazy multi-warhead missiles. And are trying and having trouble going back and building a new single warhead ICBM. reply sillywalk 13 hours agorootparentNote that the Minuteman 3, the currently deployed one, also had (has?) a MIRV capability (3 warheads). I believe it's just a single warhead now because of treaties (which may have expired), and I don't know how difficult it would be to re-MIRV it. reply switch007 16 hours agoprevWhat was going through their minds in the 60s when they amassed 5,000...10,000 but went on to amass over 30,000? Was there any point after a few thousand? reply jjk166 1 hour agoparentBack in the day, most of these weapons were to be dropped from massed strategic bombers or launched on short range missiles. You needed lots both because a lot of these were never going to make it to the target and those that did would be highly inaccurate. This is combined with most of the nukes being intended to either take out the adversary's large stockpile, or survive the use of the adversary's stockpile. reply murderfs 13 hours agoparentprevKeep in mind that Sputnik only happened in 1957, so the chances of a nuke actually hitting its target were much smaller during the build up than after the rollout of ICBMs. A lot of these were tactical nuclear weapons deployed in Europe to be launched at invading Soviet forces (including by infantry! Look up the Davy Crockett if you want to see Fallout's mini-nuke's real life counterpart) reply smiley1437 15 hours agoparentprevI'd say it was a dick measuring contest against the USSR: https://en.m.wikipedia.org/wiki/File:US_and_USSR_nuclear_sto... reply sillywalk 13 hours agoparentprevWatch Dr. Strangelove. reply voxadam 9 hours agorootparent\"Of course, the whole point of a Doomsday Machine is lost, if you keep it a secret! Why didn't you tell the world, EH?\" reply devwastaken 18 hours agoprevNobody asks how many anti nuke missiles. :) reply BMc2020 18 hours agoparentFunny enough, I know a little bit about this one. We have them, but they are all come-from-behind missiles, so the launch site has to be as close to Russia (or wherever) as possible, which angers the Russians. The head-on hit-a-bullet-with-another-bullet-is missile still impossible, although many would like us to believe they are already here. Nuh uh. Plus those are still useless against nukes delivered inside ton of cocaine. reply jjk166 1 hour agorootparenthttps://en.wikipedia.org/wiki/Ground-Based_Midcourse_Defense reply amenhotep 18 hours agorootparentprevYour contention is that GMD, THAAD, PAC-3 and Arrow are essentially expensive fireworks? Interesting. Russia and Iran will be delighted to hear that in fact their ballistic missiles were not intercepted. reply holowoodman 17 hours agorootparentWhile an ICBM is a ballistic missile, the trajectory is far far higher than the usual short- to medium-range ballistic missiles. Velocity at the receiving end of the trajectory will be far higher. Visibility into the trajectory will be worse, because of the huge range and the earth being \"in the way\". reply BMc2020 17 hours agorootparentprevBMc2020: \"I would have to be a complete nut to say Hitler was a great man.\" amenhotep: \"You said Hitler was a great man.\" reply yodon 16 hours agorootparentI suspect in other contexts you're aware of this, but personal attacks generally don't land well here. Consider continuing to focus on making important points well, as that tends to work better, as does assuming the best when someone disagrees with you, since sometimes that's a sign your comment can be legitimately interpreted or read in ways you didn't expect or intend. reply BMc2020 5 hours agorootparentHe was trying to put words in my mouth and got caught. reply dingaling 7 hours agorootparentprevThat's incorrect, boost-phase or inflight head-on interception is the norm. In fact it's easier and more efficient than trying a pursuit interception. https://world-defense.com/threads/thaad-terminal-high-altitu... reply starik36 19 hours agoprevWhy did US disclose this number? Is there a treaty compelling them to? Did other nuclear powers disclose them? reply holowoodman 18 hours agoparentEveryone knows the rough number anyways I'd suspect. Publishing numbers is a part of all arms-control treaties, so unilaterally publishing it may be an attempt to show willingness and shame the others towards a new arms-control treaty. And of course, as the sister comment said, an announcement of dick-size. reply hugh-avherald 17 hours agoparentprevWhile generally it's an advantage to keep information about your true military capability secret, for nuclear weapons it's the opposite. reply maxglute 18 hours agoparentprevIMO context is: recent US/PRC nuclear talk broke down. US/RU nuclear monitoring also broke down post UKR war. There is no strategic monitoring system anymore between large nuclear powers. Only mind games now. Throw number out there for deterrence/mind game as everyone is either building out (PRC) or modernizing nuke force (US/RU). Maybe anchoring technique to limit proliferation range. That said, there's no reason US adversaries should believe # is credible. And even if it is, each country has different # of launchers/warheads for deterence posture due to different geopolitical/technological constraints. I.e. ballistic missile defense changes penetration ratios. reply _visgean 18 hours agoparentprevit has no reason not to, its a detterent, russia and china needs to believe that the nukes are working and that there is enough for them to ensure the mutual assured destruction. reply justin66 15 hours agoparentprevHow would you write treaties governing the number of weapons of various types each side deploys and where they deploy them, if you kept this information secret? reply no_exit 17 hours agoparentprevpositive press side of updating arms posture in Europe, like the B-52 flight Sunday: https://www.newsweek.com/american-b52-bombers-historic-nato-... reply thaumasiotes 17 hours agoparentprevThey're scared and they want to intimidate. Talking about them is the first step toward using them. reply credit_guy 10 hours agoprev [–] I think the point of this sudden urge for transparency is to send a message to Russia and to the US allies about the tactical nukes. There is currently a perception of a disparity of capabilities in tactical nukes. There is parity in strategic nukes because of the New Start treaty [1]. Here's a report to the US Senate about the tactical nukes [2] produced in April 2024, and here's a quote by Putin lifted from that report: > On June 16, 2023, President Putin declared, “We have more such [tactical] nuclear weapons than NATO countries. They know about it and never stop trying to persuade us to start nuclear reduction talks. Like hell we will …. It is our competitive advantage.” Of course the US allies are worried that maybe the US nuclear umbrella is not that strong after all. Then how many tactical nukes do Russia and the US have? The perception was that Russia has between 1000 and 2000 and the US just a few hundred, but the numbers were uncertain. I think this report sends the message that the US has thousands of such tactical weapons, not just a few hundred. The message is not exactly spelled out but here's my reading. The only current tactical nuke in the US arsenal is the B61 [3]. More than 3000 were built, but it's not clear how many are still available today. The latest versions are B61-12 and B61-13, of which 400 were supposed to be made (in total, not each). The current number of B61-12 and B61-13 is not available, and I saw an estimate of 100 [4]. With this report, we can infer the total number of B61. How? The number of strategic warheads is capped at 1550. The latest US report [5] is that the number as of January this year was 1419, but this includes heavy bombers (B52, B1 and B2), of which there are 60, so actual deployed strategic warheads are 1359. The total number in the nuclear stockpile according to this new transparency report is 3748. The report explains what this number represents: > all types of nuclear weapons, including deployed and non-deployed, and strategic and non-strategic. Since we know the number of deployed strategic (1359) and the total number (3748), it follows that the rest (2389) is the total of the non-deployed strategic warheads and all the B61s (deployed or not). I can't find number of non-deployed strategic warheads, but I think it should be very small, otherwise the arms treaty is a total joke. The difference between deployed and non-deployed is quite minimal, for example the non-deployed weapons have their tritium bottles removed. So my guess is that the majority of the 2389 number above is B61 tactical nukes. Not all of them are deployed or active, but they can become so in a very short timespan. I think this is the message that the US is trying to send. [1] https://en.wikipedia.org/wiki/New_START [2] https://www.state.gov/report-on-the-status-of-tactical-nonst... [3] https://en.wikipedia.org/wiki/B61_nuclear_bomb [4] https://www.armscontrol.org/factsheets/nuclear-weapons-who-h... [5] https://www.state.gov/2023-report-to-congress-on-implementat... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Federation of American Scientists (FAS) praises the U.S. for declassifying its nuclear warhead numbers, enhancing nuclear transparency.",
      "As of September 2023, the U.S. has 3,748 nuclear warheads, with only 69 dismantled last year, the lowest since 1994.",
      "The Biden administration's disclosure reinstates transparency paused by the Trump administration, and FAS encourages other nuclear states to adopt similar transparency to prevent mistrust and misinformation."
    ],
    "commentSummary": [
      "The United States has revealed its nuclear warhead numbers, reinstating nuclear transparency and sparking discussions on production capacity and strategic importance.",
      "Key topics include the speed of potential production ramp-up, the impact of stockpile size on global safety and geopolitics, and the maintenance challenges like plutonium aging and tritium replacement.",
      "The disclosure aims to reassure allies and deter adversaries by showcasing the U.S.'s significant nuclear capabilities."
    ],
    "points": 132,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1721686564
  },
  {
    "id": 41041575,
    "title": "Lisp in C#",
    "originLink": "https://github.com/codr7/sharpl",
    "originBody": "sharpl $ dotnet run sharpl v11 1 (say 'hello) 2 hello introduction Sharpl is a custom Lisp interpreter implemented in C#. It's trivial to embed and comes with a simple REPL. The code base currently hovers around 4kloc and has no external dependencies. All features described in this document are part of the test suite and expected to work as described. novelties Pairs, arrays, maps and method compositions have dedicated syntax. Varargs use *, similar to Python. Splatting is supported using *, simlar to Python. Unified, deeply integrated iterator protocol. Default decimal type is fixpoint. Nil is called _. Both true (T) and false (F) are defined. Zeros and empty strings, arrays and maps are considered false. = is generic and compares values deeply, is may be used to compare identity. Lambdas actually look like anonymous methods. eval evaluates its arguments at emit time. Explicit tail calls using return. There is no List, except for nested pairs, which is not as bad as it sounds thanks to the syntax. Parens are used for calls only. Most things are callable, simlar to Clojure. Maps are ordered, not hashed. Errors include line numbers, even inside the REPL. bindings Bindings come in two flavors, with lexical or dynamic scope. lexical scope New lexically scoped bindings may be created using let. (let [x 1 y (+ x 2)] y) 3 dynamic scope New dynamically scoped bindings may be created using define. (define foo 35) (^bar [] foo) (let [foo (+ foo 7)] (bar)) 42 branching if may be used to conditionally evaluate a block of code. (if T (say 'is-true)) is-true if-else may be used to specify an else-clause. (if-else F 1 2) 2 methods New methods may be defined using ^. (^foo [x] x) (foo 42) 42 lambdas Leaving out the name creates a lambda. (let [f (^[x] x)] (f 42)) 42 closures External bindings are captured at method definition time. (^make-countdown [max] (let [n max] (^[] (dec n)))) (define foo (make-countdown 42)) (foo) 41 (foo) 40 tail calls return may be used to convert any call into a tail call. This example will keep going forever without consuming more space: (^foo [] (return (foo))) (foo) Without return it quickly runs out of space: (^foo [] (foo)) (foo) System.IndexOutOfRangeException: Index was outside the bounds of the array. varargs methods may be defined as taking a variable number of arguments by suffixing the last parameter with *. The name is bound to an array containing all trailing arguments when the method is called. (^foo [bar*] (say bar*)) (foo 1 2 3) 123 composition Methods may be composed using &. (^foo [x] (+ x 1)) (^bar [x] (* x 2)) (let [f foo & bar] (say f) (f 20)) (foo & bar [values*]) 42 quoting Expressions may be quoted by prefixing with '. value vs. identity = may be used to compare values deeply, while is compares identities. For some types they return the same result; integers, strings, pairs, methods etc. For others; like arrays and maps; two values may well be equal despite having different identities. (= [1 2 3] [1 2 3]) T (is [1 2 3] [1 2 3]) F atomic types symbols Identifiers turn into symbols when quoted. (= (Symbol 'foo 42) 'foo42) T strings Strings use double quotes. (string/up \"Foo\") \"FOO\" integers Integers support the regular arithmetic operations. (- (+ 1 4 5) 3 2) 5 Negative integers lack syntax, and must be created by way of subtraction. (- 42) -42 fixpoints Decimal expressions are read as fixpoint values with specified number of decimals. Like integers, fixpoints support the regular arithmetic operations. 1.234 1.234 Leading zero is optional. (= 0.123 .123) T Also like integers; negative fixpoints lack syntax, and must be created by way of subtraction. (- 1.234) -1.234 composite types pairs Pairs may be formed by putting a colon between two values. 1:2:3 1:2:3 Or by calling the constructor explicitly. (Pair [1 2 3]*) 1:2:3 arrays Arrays are fixed size sequences of values. New arrays may be created by enclosing a sequence of values in brackets. [1 2 3] Or by calling the constructor explicitly. (Array 1:2:3*) [1 2 3] maps Maps are ordered mappings from keys to values. New maps may be created by enclosing a sequence of pairs in curly braces. '{foo:1 bar:2 baz:3} Or by calling the constructor explicitly. (Map '[foo:1 bar:2 baz:3]*) {'bar:2 'baz:3 'foo:1} iterators reduce may be used to transform any iterable into a single value. (reduce + [1 2 3] 0) 6 Which could also be expressed in a more condensed form thanks to the fact that integers are iterable. (reduce + 4 0) libraries lib may be used to define/extend libraries. (lib foo (define bar 42)) foo/bar 42 When called with one argument, it specifies the current library for the entire file. (lib foo) (define bar 42) And when called without arguments, it returns the current library. (lib) (Lib user) errors fail may be used to signal an error. Since there are no facilities for handling errors yet, this means evaluation will stop unconditionally. (fail 'bummer) Sharpl.EvalError: repl@1:2 bummer evaluation eval may be used to evaluate a block of code at emit time. (eval (say 'emitting) 42) emitting 42 tests check fails with an error if the result of evaluating its body isn't equal to the specified value. Take a look at the test suite for examples. (check 5 (+ 2 2)) Sharpl.EvalError: repl@1:2 Check failed: expected 5, actual 4! When called with a single argument, it simply checks if it's true. (check 0) Sharpl.EvalError: repl@1:2 Check failed: expected T, actual 0! benchmarks benchmarkmay be used to measure the number of milliseconds it takes to repeat a block of code N times with warmup. dotnet run -c Release benchmarks.sl debugging emit may be used to display the VM operations emitted for an expression. (emit (+ 1 2)) 1 Push 1 2 Push 2 3 CallMethod (+ []) 2 False",
    "commentLink": "https://news.ycombinator.com/item?id=41041575",
    "commentBody": "Lisp in C# (github.com/codr7)130 points by codr7 17 hours agohidepastfavorite63 comments coder94 2 hours agoThere's also Clojure CLR (lisp) https://github.com/clojure/clojure-clr reply codr7 39 minutes agoparentYeah, I admire Rich Hickey a lot, it took a lot of guts; and I learned a lot from Clojure and his talks; but I unfortunately find the language a bit too opinionated and dogmatic. reply codr7 15 hours agoprevAuthor here. I'm afraid I've been out of the C# loop too long to know what's fast and what isn't these days. Now that maybe I have the attention of some serious C# nerds, any assistance in making this thing run faster would be much appreciated. It's not terrible atm, given a managed host language, but I'm sure there are plenty of knobs left to turn. See the benchmarks section in the README for more info, and the same benchmarks ported to Python in python/fib.py. Oh, and there's some undocumented yet potentially useful stuff in /Libs; strings, terminal control and IO mainly. reply codr7 13 hours agoparentBy all means, keep them coming people :) We just roughly doubled the speed by changing one line of code, that means there's plenty more fun before it gets really tricky. My issue isn't really profiling, its not knowing enough about the platform to do anything constructive with the hot spots. reply mst 8 hours agorootparentI don't know the platform and would -guess- that this won't actually help, but it's sufficiently fascinating I thought I'd share anyway: https://trout.me.uk/lisp/vlist.pdf (roughly \"stitching together efficient cons lists out of arrays\") (and /lisp/ has an index if you want to see the other random lisp-related stuff I've collected copies of so I can find it again later) reply codr7 7 hours agorootparentInteresting, thanks. reply neonsunset 14 hours agoparentprevhttps://github.com/codr7/sharpl/pull/1 Was: 686 98 1195 Now: 226 79 293 (with net9.0 preview: 201 70 269, another release another free >10%) The reason for such a significant difference is that `ArrayStack` only implements `IEnumerable`, which prevented the Enumerable.Last(stack) call from seeing that the type has an indexer which can be used to quickly access the last element instead of traversing it in its entirety. Now, it still requires JIT (or, in this case, compiler back-end and ILC) to reason about the actual type of ArrayStack to optimize away type tests, inline Last() call and devirtualize indexer access, but the better option is simply replacing it with just [^1] which does the same on any indexable type. Generally speaking, it's recommended to use out of box collections whenever appropriate like Stack, List, etc. which already implement all the necessary interfaces which the standard library takes advantage of unless there's a specific need to do otherwise. Also, it is always nice to have .net's aot emit \"canonical\" native binaries, so it took me about 45s to find the bottleneck by bumping up the numbers in benchmarks.sl and clicking \"Sample\" in macOS's Activity Monitor. All in all, the code in the project is terse and thanks for showcasing it! reply codr7 14 hours agorootparentNice work, thanks! I'll have a look later, the only part I didn't get was [^1], could you elaborate? About the ArrayStack, I wanted a stack backed by a fixed array, because my suspicion was using a fixed array for fundamental collections would be faster. Hence the VM.Config object to set the max sizes. There is also a DynamicArrayStack that supports reallocation, which is currently used by OrderedMap. The plan is to eventually benchmark the alternatives, but start with the simplest thing that could possibly work. reply neonsunset 14 hours agorootparentHere, calling `.Last()` on `ArrayStack` traverses it from the start. `.Last()` is a static extension method defined on `System.Linq.Enumerable` class and has a signature `static T Last(this IEnumerable source)`. Internally, `.Last()` will try to optimize for the common cases where a type implements `IList` and uses an indexer to simply get the last element. However, because ArrayStack does not implement IList, .Last() does not know that this is possible, therefore costs O(n) as noted above. Instead, we can simply use an index operator `[^1]` which gets the first element from end, which is short-hand for `[stack.Count - 1]`. Other than that, it’s a good idea to lean towards out-of-box tools to avoid investing effort into reinventing another language within C# and use spans for slicing data types - you almost never need to call methods like Array.ConstrainedCopy - this is something quite ancient. The idiomatic way of copying a portion of array today is `source.AsSpan(start, length).CopyTo(dest)`, slicing destination as well if you need so. The prime slice types in .NET are Span and ReadOnlySpan, and can wrap memory of any origin. reply codr7 14 hours agorootparentJust the kind of information/expertise I was looking for, awesome! reply zebracanevra 14 hours agorootparentAnd you can find that specilisation here: https://source.dot.net/#System.Linq/System/Linq/Last.cs,80 reply neonsunset 13 hours agorootparentprevThank you too! reply diggan 4 hours agorootparentprevInteresting .gitignore addition. As someone who basically never writes C# (besides for some mods for games sometimes), is that the default ephemeral files that Visual Studio Code/some other tool writes when dealing with C# projects? Seems like an awful amount of trash if that's the case. reply neonsunset 4 hours agorootparentAs description in PR indicates, it’s just a default catch-all gitignore that you can add with ‘dotnet new gitignore’ that covers all kinds of tools and build artifacts, I see no reason to customize it. For large amounts of trash in project you would need to look for other languages, like JS ecosystem or certain Java-related projects :) reply diggan 3 hours agorootparent> As description in PR indicates, it’s just a default catch-all gitignore that you can add with ‘dotnet new gitignore’ that covers all kinds of tools and build artifacts, I see no reason to customize it. Ah, someone should just come up with a huge file so we can ignore every possible combination at this point :) > like JS ecosystem Heh, JS is probably the language that generates the least trash by default as it's just a index.html file + your single JavaScript file, nothing else required or generated at all, and now you have a interactive website :) reply WorldMaker 3 hours agorootparentGitHub doesn't have it all in one huge file, but has a somewhat comprehensive template repo of many ecosystems: https://github.com/github/gitignore (This is what is used to populate the templates if you ask GitHub to include a gitignore when creating a new repo, or if you add a new file to a Repo and name it .gitignore and get the template selector to show up.) I believe `dotnet new gitignore` basically shares the same template, even, as this file: https://github.com/github/gitignore/blob/main/VisualStudio.g... reply diggan 2 hours agorootparentJust seem a bit backwards to start with a gitignore that ignores every possible tool one could use with a particular language, rather than going directory/file/pattern by directory/file/pattern. Hence my proposal for these people to take the contents of the github/gitignore repository, fold it all into one big file they can reuse in all their repos, and call it a day. Like why is there a `.DS_Store` (which is specific to macOS) in the `core.gitignore` file? That belongs in the global `.gitignore` macOS users should have in their home directory, rather than including it in project specific .gitignore. But I digress, not exactly a huge problem and I won't lose any sleep over it :) reply WorldMaker 2 hours agorootparentOne of the problems in having a single gitignore to ignore all the possible things that there's no strict superset without overlaps. Visual Studio uses files called .user that contain user-specific metadata that should never be committed to source control and other systems might use .user for components that should be committed to source control. Breaking it down by language ecosystem seems an adequate compromise as few repos use multiple languages and when they do there's often a folder hierarchy to respect with nested gitignores. reply adzm 11 hours agorootparentprevAlternatively ArrayStack could be modified to implement IList right? reply codr7 11 hours agorootparentRight, already tried that; Last() is still slightly slower (I guess because of more layers of indirection). reply lloydatkinson 9 hours agoparentprevI haven't had chance to look over the code but I know that using Span for parsing might be a nice thing to try out. Have you considered making it compile to IL? Or if not that, using the Roslyn API to compile it to C# which is then automatically faster as a result of being compiled. Then getting AOT (ahead-of-time compilation) at build time gives you improved perf basically for free. I see neon has contributed a perf change, I know that he's an expert at using https://github.com/dotnet/BenchmarkDotNet so hopefully you'll be able to do some scientific tests soon. I would love to see another language here for the *Common* Language Runtime (CLR, the core of .NET). reply WorldMaker 3 hours agorootparentA fun middle ground to compiling to IL is using System.Linq.Expression as a compiler. This was the middle ground that the \"DLR\" (Dynamic Language Runtime) used to great effect. System.Dynamic is still in the BCL and still useful, even though the dreams of IronPython and IronRuby and interop between them are sort of dead/dormant. System.Dynamic.DynamicMetaObject is still a fun thing to play with in how much power it gives you do some surprisingly optimized dynamic language things, all using System.Linq.Expression as the higher level intermediate language than raw IL. reply codr7 3 hours agorootparentInteresting, Linq.Expression actually looks doable within my complexity budget. reply buybackoff 2 hours agorootparentprevFor small short-lived scripts the compilation (in any native form) time may be much bigger than bytecode execution time. And if a platform does not support dynamic code generation the end result is non-functional code or interpreter inside interpreter for LINQ expressions (as they can still run as interpreted, at least they try). See a comment in Jint readme about that. I tried to compare some script languages implemented in C# with Roslyn script compilation. Same code to sum up 1M doubles. Roslyn takes almost 100ms to compile the code. This may become especially painful when source code changes on every execution and reusing compilation result is not possible for some reason, e.g. user input or parametrized string template. reply pjc50 8 hours agorootparentprevBit of a tradeoff there; so long as it's using its own bytecode, the sharpl executable itself can easily be AOT. Once you start trying to create your own assembly at runtime and run that, it's a LOT of work to get the host to still AOT (because you have to include the dotnet runtime anyway to run the inner assembly!) And AOT isn't a lot of free perf; it's mostly equivalent to JIT, the big advantage is faster and smaller startup. (I have used the Roslyn compile API; it's pretty cool but you do have to do more setup. e.g. https://joshvarty.com/2016/01/16/learn-roslyn-now-part-16-th... ) reply codr7 7 hours agorootparentprevSure thing, Span is one of the features I haven't had time to dig into yet. Parsing is a one time thing though. Yes, different levels of compiling to C# are definitely on the table as options; for performance, but also to hopefully be able to generate self contained executables. Even just generating my own bytecode for faster startup. reply default-kramer 3 hours agoprevCool, thanks for showing. Are you planning for any sort of FFI/interop in either direction? I don't see it in your TODO, but I also don't understand why you would write it in C# unless you had this in mind. reply codr7 3 hours agoparentGiven how easy it is to expose functionality from the host language using existing facilities [0], and how complicated a reflection based FFI could get; I would rather not, at least not right now. It's also one of those decisions that will drastically limit my choices for future evolution of the implementation. The general idea is to use both languages together, as complements; not calling one from the other. https://github.com/codr7/sharpl/blob/main/src/Sharpl/Libs/Co... reply default-kramer 3 hours agorootparentAh, this is actually what I had in mind by \"interop\". So from C# I can evaluate some Lisp code and then test that the result is `PairType` (for example)? Maybe this is so obvious it goes without saying, but I didn't see any examples of that. reply codr7 2 hours agorootparentI understand. Sure thing, VM.Eval(\"your code\") returns a value if one is produced. if (vm.Eval(\"1:2\") is Value v and v.Type == Libs.Core.Pair) { Console.WriteLine(\"Pair: \" + v.Cast(Libs.Core.Pair)); } Have a look in the main Program.cs to see how to get a VM up and running. reply codr7 12 hours agoprevIf only we could get a tiny Lisp loving push from the HN crew, wink, nudge. I don't much like the odds of Lisp vs. Nintendo hardware kickstarters. reply davidelettieri 12 hours agoprevIt's worth mentioning https://github.com/IronScheme/IronScheme reply codr7 12 hours agoparentRight, I recognize the name. Curious though, I'm pretty sure it emits MSIL rather than its own bytecode like sharpl? So that would be one difference, in most cases an advantage because of performance. The other obvious difference is I'm not aiming for any standards, quite the contrary; this is about being so fed up with the alternatives (including Scheme) that spending the rest of my life getting it just right looks like a reasonable deal. reply keithnz 14 hours agoprevyears ago someone posted http://norvig.com/lispy.html here on HN I wrote a lisp in C# based on that, it was only a 100+ ish lines of code. It was a great way to get into Lisp. reply dualogy 10 hours agoparentThere's also Make-A-Lisp and, unlike most write-you-a-lisp/scheme-s out there, that one also covers TCO interpretation, quasiquotation/unquote for macros and their expansion, and goes up to self-hosting: https://github.com/kanaka/mal/ I just went through it over 2-3 days, great practice IMHO to do once in your life from start to finish: https://github.com/metaleap/go-lisp reply KineticLensman 5 hours agorootparentConcur, although it took me two to three months, not days (was working full time is my excuse). Biggest grief point was getting macro expansion to work correctly; TCO worked almost first time, IIRC. reply codr7 5 hours agorootparentUnquoting took me several implementations to wrap my head around and consistently get good enough. As did register allocation, which is more of a VM issue. reply KineticLensman 4 hours agorootparentInteresting. I'm about to start doing a MAL-like again, but this time (I used C# before) using Rust and building a VM as in Crafting Interpreters rather than following the MAL guide. Macros are one of the things that I anticipate being a challenge. reply dualogy 3 hours agorootparentAFAICT, their expansion still happens during an interpretation cycle — but when you \"interpret\" AST into your byte-code. reply codr7 3 hours agorootparentOr compile, since it's usually a transformation. I like to call that the emit phase, as in emitting byte code. The interesting thing is that it only happens once, before the code is evaluated. reply codr7 11 hours agoparentprevYes, I am aware. I started out designing Forth interpreters, actually calculators and template engines, but Forth was a natural progression. My design differs a lot from idiomatic Lisp implementations (likewise Forth), and I do sometimes wonder what it would look like if you started in that end and worked your way towards supporting all the features of sharpl. reply codr7 2 hours agorootparentHaving thought a bit about it, one motivation for choosing the route I did; rather than the more idiomatic one; is that it relies on pipeline of source transformations to get to optimal code. The initial expansion is often pretty sloppy. I don't do many source transformations, everything is designed to emit pretty optimal byte code on the first pass. reply mst 7 hours agoprevSeparate from anything else, I'm ... concerned ... at the idea of ints being iterable, because it seems like something I'd be much more likely to invoke accidentally than intentionally and then wonder wtf my program was doing. I'd prefer to have to write something like (reduce + (range 1 3) 0) and if you find yourself wanting the natural number iteration regularly maybe (^upto (n) (range 1 (- n 1))) as sugar. This concern brought to you by e.g. the great pain induced by the difference between for x in \"foo\": and for x in \"foo\",: in python, for example. It may turn out in practice that a lispy language and/or programmers who make different mistakes to me will make it not an issue ... but were it -my- project I'd probably comment out the iterator implementation for int and see if its absence annoyed me enough to decide it was worth bringing back. (when perpetrating language myself I often find that some of my favourite bits of clever don't pass the 'annoyed me enough' test and end up as documentation examples or similar in the end instead ... hopefully you have better luck ;) reply kazinator 3 hours agoparentIntegers are also iterable in TXR Lisp. But in a different way; you get an infinite sequence starting from the value. 1> [mapcar list '(a b c) 10] ((a 10) (b 11) (c 12)) This crops up on a regular basis in my coding, removing verbosity; I don't regret the decision. It's one of the \"take alongs\": something to repeat in a future language. reply mst 1 hour agorootparentI think I'd rather have e.g. [mapcar list '(a b c) [count-from 10]] or so. They look great in examples, but once you're doing [mapcar list '(a b c) x] then I still suspect that you'll confuse yourself every so often by passing the wrong x. OTOH if you've got the entire codebase in your head that isn't really an issue; it's coming back to tweak something a few months later where I usually find such features give me an unexpectedly ventilated foot :) reply codr7 2 hours agorootparentprevThe duality of meaning (is it from or to the specified value) actually speaks against the feature for me, you just ruined it :) Or fixed it, depending on which way you lean. reply kazinator 1 hour agorootparentWe can't get rid of plurality of meaning because syntax isn't semantics. The same syntax can be assigned completely different semantics. In Lisps, we program syntax with different semantics regularly, so the plurality is part of the daily existence. (defclass a (b c)) and (list a (b c)) have the same shape, but are completely different things. 10 is a piece of syntax, but how do we assign its semantics when it is an argument to a parameter that expects a sequence? That is up in the air the same way. Is it an error, like in most Lisp-like languages? does it count up to 10 from 1? Below 10 from zero? From 10 ad infinitum?) The choice becomes a paradigm in the language that everyone has to understand. What is undesirable is inconsistencies. If certain functions counted up to the number, but others from the number, arbitrarily, that would be objectively worse than consistently doing one or the other. reply kazinator 2 hours agorootparentprevIt would not be often useful to have it count up from 1 or zero up to or below the value; I would not have designed it that way. In many situations you don't know the upper limit of what is enumerated; it comes implicitly from the lengths of other sequences or in other ways. It's also less inefficient, because the value has to be converted to an iterator object that knows about the range, and keeps track of the state. In TXR Lisp, certain objects are self-iterable, like characters, numbers and good old conses. 1> (iter-begin \"abc\") # 2> (iter-begin 3) 3 3> (iter-begin '(a b c)) (a b c) To iterate a string, we need to obtain a seq-iter object, but for 3 and (a b c), we do not. With these objects, we have all the state we need in order to iterate. 4> (iter-more 3) t 5> (iter-item 3) 3 6> (iter-step 3) 4 7> (iter-more '(a b c)) t 8> (iter-item '(a b c)) a 9> (iter-step '(a b c)) (b c) 10> (iter-more *1) t 11> (iter-item *1) #\\a 12> (iter-step *1) # 13> (iter-item *1) #\\b iter-step may or may not destructively update its argument, so you always have to capture the return value and forget about the original. You can see how for a list, iter-more is equivalent to (not (null ...)), iter-item is just car, and iter-step is just cdr. There is also lazy processing in TXR Lisp. E.g. lazy mapcar which is mapcar*. The following will only read the first few lines of the syslog, returning instantly: 14> (take 3 [mapcar* cons 1 (file-get-lines \"/var/log/syslog\")]) ((1 . \"Jul 23 00:09:54 sun-go systemd[1]: openvpn@client.service: Service hold-off time over, scheduling restart.\") (2 . \"Jul 23 00:09:54 sun-go systemd[1]: openvpn@client.service: Scheduled restart job, restart counter is at 1044619.\") (3 . \"Jul 23 00:09:54 sun-go systemd[1]: Stopped OpenVPN connection to client.\")) reply codr7 2 hours agorootparentCounting from 0 up to a value is the standard for loop, or numbering things (possibly with using the value with an offset). But like I said, I'm not so sure there is a right way to interpret it any more. reply codr7 7 hours agoparentprevNoted, I feel like I need more use cases and experience to say for sure. Some way of forming ranges is planned anyways. reply mst 5 hours agorootparentYeah, 'int is iterable' triggered \"oh that's cute!\" immediately followed by \"... and the exact sort of cute I often find irresistably tempting myself and then regret later.\" My current thing in progress has basically all of its temptation points already spent because I decided I was going to make it fexpr based, but I'm very definitely having fun with that so far - \"no special forms required\" makes for some interesting possibilities. (kernel-lisp and kernel-thesis in /lisp/ are worth looking at if fexprs sound interesting, though I'm on the pragmatics side of things and will not pretend I came anywhere close to understanding the $vau calculus part) reply codr7 5 hours agorootparentA certain level of helpfulness is nice though, Ruby and to a somewhat lesser extent Perl get that part right. I guess Perl could be seen as a warning example of what happens if you go all in. Yep, I've been on a ride down the fexpr implementation hole previously; which is another reason I've been delaying user macros; still processing the experience. reply mst 2 hours agorootparentThere's a bunch of stuff in perl that's best restricted to one-liner or short script usage - i.e. the sort of cases where you're using it for the early inspiration super-awk purposes. Writing perl as a programming language is, I find, a very different dialect, and perhaps amusingly one in which I rely on function composition, closures and block scoping heavily due to also loving lisp - javascript's 'let' behaves very similarly to perl's 'my' and I find it ... difficult ... to deal with python or ruby for any length of time since their scoping is \"stuff randomly pops into existence at function scope\" and, just, aaaaa. Perl of course can't really have macros, but it does have the ability to define custom (also block scoped :) keywords which can allow one to achieve similar results - see e.g. https://p3rl.org/Keyword::Declare for a demonstration built on top of that functionality (there's also a code rewriter called Babble but I got distracted so while it works, it's woefully underdocumented, keep forgetting to get back to that). I've always had a fondness for the \"building the language up towards the problem\" style of programming (I wrote the first ever proof of concept for custom perl keywords, though that code has happily long since been obsoleted by people who knew what they were doing) which has led me to 'fexprs for making DSLs' since those are usually building up a config structure or similar which makes fexprs' being tricky to optimise less of an obstacle. (also with fexprs I don't have to limit myself to 'block in front' ala perl or 'block at the end' ala ruby/elixir (though it's amazing how much mileage elixir gets out of its macros, Kernel.ex is well worth a read if you're so inclined)) reply Zambyte 17 hours agoprevCool! I didn't get a chance to run it but I dug around the code a little bit. I noticed there is a Macro class, but no mention of macros in the README. Are macros working? reply codr7 17 hours agoparentWithin the host language for now, they more or less just need to be plugged in. So far there has been no shortage of more important features at the scale of programs it's currently viable for. reply healeycodes 9 hours agoprevYou can annotate the code blocks in the README to get generic lisp syntax highlighting. ```lisp (+ 1 2) ``` reply codr7 7 hours agoparentThanks. It's unfortunately enough of a bad fit for me to prefer no highlighting; my own fault, the price you pay for adding syntax. reply zczc 9 hours agoprev [–] So, it is a shortcut around the tenth rule: \"Any sufficiently complicated C or Fortran program contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp\" [1] [1] https://en.wikipedia.org/wiki/Greenspun's_tenth_rule reply codr7 7 hours agoparent [–] Observant :) Polyglot projects have been sort of a thing lately, because micro services made them doable I guess; but I feel the combination of a capable and portable host language and a scripting language implemented in that language captures the best of both worlds while adding nice super powers on top. reply pjmlp 4 hours agorootparent [–] You mean rebranding distributed computing and OS IPC for newer generations? reply codr7 2 hours agorootparent [–] It's the same old ideas, over and over again. But it's not a circle, it's a spiral; we learn something new every time. reply pjmlp 2 hours agorootparent [–] Sometimes, and suddenly modular monoliths become a thing, as the microservices generation learns why we weren't doing SUN RPC, TOOL, DCOM and CORBA for every little piece of application functionality. reply codr7 2 hours agorootparent [–] We could definitely do a better job of learning from those who came before, it would save a lot of time and effort. Evolution is messy, every combination has to be tried, every minor detail thoroughly investigated from all angles. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sharpl is a custom Lisp interpreter written in C# with around 4,000 lines of code and no external dependencies, making it easy to embed.",
      "Key features include pairs, arrays, maps, method compositions, varargs, a unified iterator protocol, and a fixpoint decimal type, among others.",
      "It supports lexical and dynamic bindings, tail call optimization to prevent stack overflow, and provides detailed error reporting with line numbers."
    ],
    "commentSummary": [
      "A developer named codr7 is working on a Lisp implementation in C# and seeking optimization help from the community.",
      "Significant performance improvements have been achieved by community suggestions, such as changing how ArrayStack is accessed and using Span and ReadOnlySpan for data slicing.",
      "The project is not aiming for standard compliance but rather for practical performance and usability, with ongoing discussions about integrating features like macros and Fexprs (function expressions)."
    ],
    "points": 130,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1721697283
  }
]
