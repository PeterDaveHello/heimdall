[
  {
    "id": 39466630,
    "title": "Introducing Stable Diffusion 3: Enhanced Text-to-Image Model",
    "originLink": "https://stability.ai/news/stable-diffusion-3",
    "originBody": "Stable Diffusion 3 Product 22 Feb View fullsize Prompt: Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says \"Stable Diffusion 3\" made out of colorful energy Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities. While the model is not yet broadly available, today, we are opening the waitlist for an early preview. This preview phase, as with previous models, is crucial for gathering insights to improve its performance and safety ahead of an open release. You can sign up to join the waitlist here. View fullsize The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters. This approach aims to align with our core values and democratize access, providing users with a variety of options for scalability and quality to best meet their creative needs. Stable Diffusion 3 combines a diffusion transformer architecture and flow matching. We will publish a detailed technical report soon. View fullsize We believe in safe, responsible AI practices. This means we have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3 by bad actors. Safety starts when we begin training our model and continues throughout the testing, evaluation, and deployment. In preparation for this early preview, we’ve introduced numerous safeguards. By continually collaborating with researchers, experts, and our community, we expect to innovate further with integrity as we approach the model’s public release. View fullsize Our commitment to ensuring generative AI is open, safe, and universally accessible remains steadfast. With Stable Diffusion 3, we strive to offer adaptable solutions that enable individuals, developers, and enterprises to unleash their creativity, aligning with our mission to activate humanity’s potential. If you’d like to explore using one of our other image models for commercial use prior to the Stable Diffusion 3 release, please visit our Stability AI Membership page to self host or our Developer Platform to access our API. To stay updated on our progress follow us on Twitter, Instagram, LinkedIn, and join our Discord Community. Image Anel Islamovic",
    "commentLink": "https://news.ycombinator.com/item?id=39466630",
    "commentBody": "Stable Diffusion 3 (stability.ai)928 points by reqo 20 hours agohidepastfavorite655 comments JonathanFly 20 hours agoFrom: https://twitter.com/EMostaque/status/1760660709308846135 Some notes: - This uses a new type of diffusion transformer (similar to Sora) combined with flow matching and other improvements. - This takes advantage of transformer improvements & can not only scale further but accept multimodal inputs.. - Will be released open, the preview is to improve its quality & safety just like og stable diffusion - It will launch with full ecosystem of tools - It's a new base taking advantage of latest hardware & comes in all sizes - Enables video, 3D & more.. - Need moar GPUs.. - More technical details soon >Can we create videos similar like sora Given enough GPUs and good data yes. >How does it perform on 3090, 4090 or less? Are us mere mortals gonna be able to have fun with it ? Its in sizes from 800m to 8b parameters now, will be all sizes for all sorts of edge to giant GPU deployment. (adding some later replies) >awesome. I assume these aren't heavily cherry picked seeds? No this is all one generation. With DPO, refinement, further improvement should get better. >Do you have any solves coming for driving coherency and consistency across image generations? For example, putting the same dog in another scene? yeah see @Scenario_gg's great work with IP adapters for example. Our team builds ComfyUI so you can expect some really great stuff around this... >Dall-e often doesn’t even understand negation, let alone complex spatial relations in combination with color assignments to objects. Imagine the new version will. DALLE and MJ are also pipelines, you can pretty much do anything accurately with pipelines now. >Nice. Is it an open-source / open-parameters / open-data model? Like prior SD models it will be open source/parameters after the feedback and improvement phase. We are open data for our LMs but not other modalities. >Cool!!! What do you mean by good data? Can it directly output videos? If we trained it on video yes, it is very much like the arch of sora. reply cheald 19 hours agoparentSD 1.5 is 983m parameters, SDXL is 3.5b, for reference. Very interesting. I've been streching my 12GB 3060 as far as I can; it's exciting that smaller hardware is still usable even with modern improvements. reply ttul 11 hours agorootparentStability has to make money somehow. By releasing an 8B parameter model, they’re encouraging people to use their paid API for inference. It’s not a terrible business decision. And hobbyists can play with the smaller models, which with some refining will probably be just fine for most non-professional use cases. reply jandrese 11 hours agorootparentI would LOL if they released the \"safe\" model for free but made you pay for the one with boobs. reply ttul 11 hours agorootparentOh they’ll never let you pay for porn generation. But they will happily entertain having you pay for quality commercial images that are basically a replacement for the entire graphic design industry. reply ohthehugemanate 2 hours agorootparentIt's not an easy fap, but I guess I'm watching people get f*cked either way. reply teaearlgraycold 11 hours agorootparentprevDon't people quantize SD down to 8 bits? I understand plenty of people don't have 8GB of VRAM (and I suppose you need some extra for supplemental data, so maybe 10GB?). But that's still well within the realm of consumer hardware capabilities. reply ttul 11 hours agorootparentI’m the wrong person to ask, but it seems Stability intends to offer models from 800M to 8B parameters in size, which offers something for everyone. reply liuliu 16 hours agorootparentprevI am going to look at quantization for 8b. But also, these are transformers, so variety of merging / Frankenstein-tune is possible. For example, you can use 8b model to populate the KV cache (which computes once, so can load from slower devices, such as RAM / SSD) and use 800M model for diffusion by replicating weights to match layers of the 8b model. reply memossy 19 hours agorootparentprev800m is good for mobile, 8b for graphics cards. Bigger than that is also possible, not saturated yet but need more GPUs. reply anon373839 10 hours agorootparentDo you know how the memory demands compare to LLMs at the same number of parameters? For example, Mistral 7B quantized to 4 bits works very well on an 8GB card, though there isn’t room for long context. reply vorticalbox 19 hours agorootparentprevyou ca also quantisation which lowers memory requirements at a small lose of performance. reply netdur 19 hours agoparentprev> - Need moar GPUs.. Why is there not a greater focus on quantization to optimize model performance, given the evident need for more GPU resources? reply memossy 19 hours agorootparentWe have highly efficient models for inference and a quantization team. Need moar GPUs to do a video version of this model similar to Sora now they have proved that Diffusion Transformers can scale with latent patches (see stablevideo.com and our work on that model, currently best open video model). We have 1/100th of the resources of OpenAI and 1/1000th of Google etc. So we focus on great algorithms and community. But now we need those GPUs. reply sylware 19 hours agorootparentDon't fall for it: OpenAI is microsoft. They have as much as google, if not more. reply Jensson 18 hours agorootparentGoogle got cheap TPU chips, means they circumvent the extremely expensive Nvidia corporate licenses. I can easily see them having 10x the resources of OpenAI for this. reply pavon 16 hours agorootparentprevYes, they have deep pockets and could increase investment if needed. But the actual resources devoted today are public, and in line with the parent said. reply px43 18 hours agorootparentprevTo be clear here, you think that Microsoft has more AI compute than Google? reply SV_BubbleTime 18 hours agorootparentprevThis isn’t OpenAI that make GPTx. It’s StabilityAI that makes Stable Diffusion X. reply Solvency 18 hours agorootparentprevcan someone explain why nVidia doesn't just hold their own AI? And literally devote 50% of their production to their own compute center? In an age where even ancient companies like Cisco are getting in the AI race, why wouldn't the people with the keys to the kingdom get involved? reply declaredapple 15 hours agorootparentThey've been very happy selling shovels at a steep margin to literally endless customers. The reason is because they instantly get a risk free guaranteed VERY healthy margin on every card they sell, and there's endless customers lined up for them. If they kept the cards, they give up the opportunity to make those margins, and instead take the risk that they'll develop a money generating service (that makes more money then selling the cards). This way there's no risk of: A competitor out competing them, not successfully developing a profitable product, \"the ai bubble popping\", stagnating development, etc. There's also the advantage that this capital has allowed them to buy up most of TSMC's production capacity, which limits the competitors like Google's TPUs. reply blihp 15 hours agorootparentprevBecause history has shown that the money is in selling the picks and shovels, not operating the mine. (At least for now. There very well may come a point later on when operating the mine makes more sense, but not until it's clear where the most profitable spot will be) reply mr_toad 8 hours agorootparentDon’t stretch that analogy too far. It was applicable to gold rushes, which were low hanging fruit where any idiot could dig a hole and find gold. Historically, once the easy to find gold was all gone it was the people who owned the deep gold mines and had the capital to exploit them who became wealthy. reply chompychop 18 hours agorootparentprev\"The people that made the most money in the gold rush were selling shovels, not digging gold\". reply swamp40 15 hours agorootparentprevJensen was just talking about a new kind of data center: AI-generation factories. reply downWidOutaFite 18 hours agorootparentprev1. the real keys to the kingdom are held by TSMC whose fab capacity rules the advanced chips we all get, from NVIDIA to Apple to AMD to even Intel these days. 2. the old advice is to sell shovels during a gold rush reply AnthonyMouse 14 hours agorootparentprev> Why is there not a greater focus on quantization to optimize model performance, given the evident need for more GPU resources? There is an inherent trade off between model size and quality. Quantization reduces model size at the expense of quality. Sometimes it's a better way to do that than reducing the number of parameters, but it's still fundamentally the same trade off. You can't make the highest quality model use the smallest amount of memory. It's information theory, not sorcery. reply supermatt 19 hours agorootparentprevI believe he means for training reply VikingCoder 18 hours agoparentprevI'm curious - where are the GPUs with decent processing power but enormous memory? Seems like there'd be a big market for them. reply wongarsu 18 hours agorootparentNvidia is making way too much money keeping cards with lots of memory exclusive to server GPUs they sell with insanely high margins. AMD still suffers from limited resources and doesn't seem willing to spend too much chasing a market that might just be a temporary hype, Google's TPUs are a pain to use and seem to have stalled out, and Intel lacks commitment, and even their products that went roughly in that direction aren't a great match for neural networks because of their philosophy of having fewer more complex cores. reply pbhjpbhj 14 hours agorootparentprevNvidia have a system for DMA from GPU to system memory, GPUdirect. That seems like a potentially better route if latency can be handled well. reply nick238 11 hours agorootparentGPU memory is all about bandwidth, not latency. DDR5 can do 4-8 GT/s x 64-bit bus per DIMM, so maxing 128 GB/s with a dual memory controller, 512 GB/s with 8x memory controllers on server chips, but GDDR6 can run at twice the frequency and has a memory bus ~5x as wide in the 4090, so you get an order of magnitude bump in throughput, so nearly 1 TB/s on a consumer product. Datacenter GPUs (e.g. A100) with HBM2e doubles that to 2 TB/s reply ls612 18 hours agorootparentprevMacBooks with M2 or M3 Max. I’m serious. They perform like a 2070 or 2080 but have up to 128GB of unified memory, most of which can be used as VRAM. reply ttul 11 hours agorootparentMPS is promising and the memory bandwidth is definitely there, but stable diffusion performance on Apple Silicon remains terribly poor compared with consumer Nvidia cards (in my humble opinion). Perhaps this is partly because so many bits of the SD ecosystem are tied to Nvidia primitives. reply ummonk 10 hours agorootparentImage diffusion models tend to have relatively low memory requirements compared to LLMs (and don’t benefit from batching), so having access to 128 GB of unified memory is kinda pointless. reply Filligree 7 hours agorootparentThey do benefit from batching; up to a 50% performance improvement, in my experience. That might seem small compared to LLMs, but it isn't small in absolute terms. reply ls612 6 hours agorootparentI got a 2x jump on my 4090 from batching SDXL. reply ls612 7 hours agorootparentprevStable diffusion will run fine on a 3090, or 4070ti Super and higher. reply declaredapple 15 hours agorootparentprevHow many tokens/s are we talking for a 70B model? Last I saw they performed really poorly, like lower single digits t/s. Don't get me wrong they're probably a decent value for experimenting with it, but is flat out pathetic compared to an A100 or H100. And I think useless for training? reply smcleod 14 hours agorootparentYou can run a 180B model like Falcon Q4 around 4-5tk/s, a 120B model like Goliath Q4 at around 6-10tk/s, and 70B Q4 around 8-12tk/s and smaller models much quicker, but it really depends on the context size, model architecture and other settings. A A100 or H100 is obviously going to be a lot faster but it costs significantly more taking its supporting requirements into account and can’t be run on a light, battery powered laptop etc… reply int_19h 9 hours agorootparentprevFor text inference, what you want is M1/M2 Ultra with its 800 Gb/s RAM. Max only goes up to 400 Gb/s. reply ls612 7 hours agorootparentYeah but the ultra only goes in desktop platforms which may be limiting to some. reply int_19h 1 hour agorootparentBut that's no different from mid-to-high-end GPUs, which is what the original ask was about. reply iosjunkie 17 hours agorootparentprevI dream of AMD or Intel creating cards to do just that reply 3abiton 6 hours agorootparentprevTesla P40 reply SV_BubbleTime 18 hours agorootparentprevI’ll bet you the Nvidua 50xx series will have cards that are asymmetric for this reason. But nothing that will cannibalize their gaming market. You’ll be able to get higher resolution but slowly. Or pay the $2800 for a 5090 and get high res with good speed. reply p1esk 18 hours agorootparentprevH200 has 141GB, B100 (out next month) will probably have even more. How much memory do you need? reply holoduke 16 hours agorootparentWe need 128gb with a 4070 chip for about 2000 dollars. Thats what we want. reply ttul 11 hours agorootparentNvidia will not build that any time soon. RAM is the dividing line between charging $40,000 vs $2500… reply duffyjp 13 hours agorootparentprevI've never tried it, but in Windows you can have CUDA apps fall back to system ram when GPU vram is exhausted. You could slap 128gb in your rig with a 4070. I'm sure performance falls off a cliff, but if it's the difference between possible and impossible that might be acceptable. https://nvidia.custhelp.com/app/answers/detail/a_id/5490/~/s... reply int_19h 9 hours agorootparentprevAn M1 Mac Studio with that much RAM can be had for around $3K if you look for good deals, and will give you ~8 tok/s on a 70B model, or ~5 tok/s for a 120B one. reply ta_1138 13 hours agorootparentprevUnfortunately production capacity for that is limited, and with sufficient demand, all pricing is an auction. Therefore, we aren't going to be seeing that card in years reply qwertox 13 hours agorootparentprevPlease give me some DIMM slots on the GPU so that I can choose my own memory like I'm used to from the CPU-world and which I can re-use when I upgrade my GPU. reply FeepingCreature 15 hours agorootparentprevYes please. reply albertzeyer 16 hours agoparentprevI understand that Sora is very popular, so it makes sense to refer to it, but when saying it is similar to Sora, I guess it actually makes more sense to say that it uses a Diffusion Transformer (DiT) (https://arxiv.org/abs/2212.09748) like Sora. We don't really know more details on Sora, while the original DiT has all the details. reply tithe 15 hours agorootparentIs anyone else struck by the similarities in textures between the images in the appendix of the above \"Scalable Diffusion Models with Transformers\" paper? If you size the browser window right, paging with the arrow keys (so the document doesn't scroll) you'll see (eg, pages 20-21) the textures of the parrot's feathers are almost identical to the textures of bark on the tree behind the panda bear, or the forest behind the red panda is very similar to the undersea environment. Even if I'm misunderstanding something fundamental here about this technique, I still find this interesting! reply jachee 15 hours agorootparentCould be that they’re all generated from the same seed. And we humans are really good at spotting patterns like that. reply cchance 15 hours agoparentprevSo is this \"SDXL safe\" or \"SD2.1\" safe, cause SDXL safe we can deal with, if it's 2.1 safe it's gonna end up DOA for a large part of the opensource community again reply astrange 13 hours agorootparentSD2.1 was not \"overly safe\", SD2.0 was because of a training bug. 2.1 didn't have adoption because people didn't want to deal with the open replacement for CLIP. Or possibly because everyone confused 2.0 and 2.1. reply raxxorraxor 1 hour agorootparentThere was a replacement for CLIP? That is awesome. What was the issue with it? reply swyx 8 hours agorootparentprev> SDXL safe we can deal with how exactly did the community deal with it? interested to learn how to unlearn safety reply swyx 8 hours agoparentprev> Dall-e often doesn’t even understand negation, let alone complex spatial relations in combination with color assignments to objects. can someone explain how negation is currently done in stable diffusion? and why cant we do it in text LLMs? reply scottmf 8 hours agorootparentyou can use negative logit bias reply samstave 13 hours agoparentprev>>>How does it perform on 3090, 4090 or less? Are us mere mortals gonna be able to have fun with it ? >>>Its in sizes from 800m to 8b parameters now, will be all sizes for all sorts of edge to giant GPU deployment. -- Can you fragment responses such that if an edge device (mobile app) is prompted for [thing] it can pass tokens upstream on the prompt -- Torrenting responses effectively - and you could push actual GPU edge devices in certain climates... like dens cities whom are expected to be a Fton of GPU cycle consumption around the edge? So you have tiered processing (speed is done locally, quality level 1 can take some edge gpu - and corporate shit can be handled in cloud... ---- Can you fragment and torrent a response? If so, how is that request torn up and routed to appropriate resources? BOFH me if this is a stupid question? (but its valid for how we are evolving to AI being intrinsic to our society so quickly.) reply sandworm101 19 hours agoparentprev>> all sorts of edge to giant GPU deployment. Soon the GPU and its associated memory will be on different cards, as once happened with CPUs. The day of the GPU with ram slots is fast approaching. We will soon plug terabytes of ram into our 4090s, then plug a half-dozen 4090s into a raspberry PI to create a Cronenberg rendering monster. Can it generate movies faster than Pixar can write them? Sure. Can it play Factorio? Heck no. reply jsheard 19 hours agorootparentAny seperation of a GPU from its VRAM is going to come at the expense of (a lot of) bandwidth. VRAM is only as fast as it is because the memory chips are as close as possible to the GPU, either on seperate packages immediately next to the GPU package or integrated onto the same package as the GPU itself in the fanciest stuff. If you don't care about bandwidth you can already have a GPU access terabytes of memory across the PCIe bus, but it's too slow to be useful for basically anything. Best case you're getting 64GB/sec over PCIe 5.0 x16, when VRAM is reaching 3.3TB/sec on the highest end hardware and even mid-range consumer cards are doing >500GB/sec. Things are headed the other way if anything, Apple and Intel are integrating RAM onto the CPU package for better performance than is possible with socketed RAM. reply sandworm101 18 hours agorootparentThat depends on whether performance or capacity is the goal. Smaller amounts of ram closer to the processing unit makes for faster computation, but AI also presents a capacity issue. If the workload needs the space, having a boatload of less-fast ram is still preferable to offloading data to something more stable like flash. That is where bulk memory modules connected though slots may one day appear on GPUs. reply duffyjp 13 hours agorootparentI'm having flashbacks to owning a Matrox Millenium as a kid. I never did get that 4MB vram upgrade. https://www.512bit.net/matrox/matrox_millenium.html reply mysterydip 19 hours agorootparentprevIs there a way to partition the data so that a given GPU had access to all the data it needs but the job itself was parallelized over multiple GPUs? Thinking on the classic neural network for example, each column of nodes would only need to talk to the next column. You could group several columns per GPU and then each would process its own set of nodes. While an individual job would be slower, you could run multiple tasks in parallel, processing new inputs after each set of nodes is finished. reply zettabomb 17 hours agorootparentOf course, this is common with LLMs which are too large to fit in any single GPU. I believe Deepspeed implements what you're referring to. reply zettabomb 18 hours agorootparentprevI doubt it. The latest GPUs utilize HBM which is necessarily part of the same package as the main die. If you had a RAM slot for a GPU you might as well just go out to system RAM, way too much latency to be useful. reply AnthonyMouse 13 hours agorootparentIt isn't the latency which is the problem, it's the bandwidth. A memory socket with that much bandwidth would need a lot of pins. In principle you could just have more memory slots where each slot has its own channel. 16 channels of DDR5-8000 would have more bandwidth than the RTX 4090. But an ordinary desktop board with 16 memory channels is probably not happening. You could plausibly see that on servers however. What's more likely is hybrid systems. Your basic desktop CPU gets e.g. 8GB of HBM, but then also has 16GB of DRAM in slots. Another CPU/APU model that fits into the same socket has 32GB of HBM (and so costs more), which you could then combine with 128GB of DRAM. Or none, by leaving the slots empty, if you want entirely HBM. A server or HEDT CPU might have 256GB of HBM and support 4TB of DRAM. reply brookst 12 hours agorootparentAgree, this is likely future. It’s really just an extension of The existing tiered CPU cache model reply ltbarcly3 19 hours agorootparentprevI don’t think you really understand the current trends in computer architecture. Even cpus are being moved to have on package ram for higher bandwidth. Everything is the opposite of what you said. reply sandworm101 18 hours agorootparentHigher bandwidth but lower capacity. The real trend is different physical architectures for different compute loads. There is a place in AI for bulk albeit slower memory such as extremely large date sets that want to run internally on a discreet card without involving pci lanes. reply subzel0 19 hours agoprev“Photo of a red sphere on top of a blue cube. Behind them is a green triangle, on the right is a dog, on the left is a cat” https://pbs.twimg.com/media/GG8mm5va4AA_5PJ?format=jpg&name=... reply Filligree 16 hours agoparentThat's _amazing_. I imagine this doesn't look impressive to anyone unfamiliar with the scene, but this was absolutely impossible with any of the older models. Though, I still want to know if it reliabily does this--so many other things are left to chance, if I need to also hit a one-in-ten chance of the composition being right, it still might not be very useful. reply ttul 11 hours agorootparentIt’s the transformer making the difference. Original stable diffusion uses convolutions, which are bad at capturing long range spatial dependencies. The diffusion transformer chops the image into patches, mixes them with a positional embedding, and then just passes that through multiple transformer layers as in an LLM. At the end, the model unpatchify’s (yes, that term is in the source code) the patched tokens to generate output as a 2D image again. The transformer layers perform self-attention between all pairs of patches, allowing the model to build a rich understanding of the relationships between areas of an image. These relationships extend into the dimensions of the conditioning prompts, which is why you can say “put a red cube over there” and it actually is able to do that. I suspect that the smaller model versions will do a great job of generating imagery, but may not follow the prompt as closely, but that’s just a hunch. reply qumpis 7 hours agorootparentConvolutions are bad at long range spatial dependencies? What makes you say that - any chance you have a reference? reply feoren 4 hours agorootparentIt kinda makes sense, doesn't it? What are the largest convolutions you've heard of -- 11 x 11 pixels? Not much more than that, surely? So how much can one part of the image influence another part 1000 pixels away? But I am not an expert in any of this, so an expert's opinion would be welcome. reply qumpis 3 hours agorootparentYes it makes sense a bit. Many popular convents operate on 3x3 kernels. But the number of channel increases per layer. This, coupled with the fact that the receptive field increases per layer and allows convnets to essentially see the whole image relatively early in model's depth (esp. coupled with pooling operations which increase the receptive field rapidly), makes this intuition questionable. Transformers on the other hand, operate on attention which allows them to weight each patch dynamically, but it's clear to me that this allows them to attend to all parts of the image in a way different from convnets. reply CSMastermind 15 hours agorootparentprevI put the prompt into ChatGPT and it seemed to work just fine: https://imgur.com/LsRM7G4 reply mortenjorck 14 hours agorootparentYou got lucky! Here's a thread where I attempted the same just now: https://imgur.com/a/xiaiKXp It has a lot of difficulty with the orientation of the cat and dog, and by the time it gets them in the right positions, the triangle is lost. reply mikeg8 15 hours agorootparentprevI dislike the look of chatGPT images so much. The photo-realism of stable diffusion impresses me a lot more for some reason. reply LeoPanthera 9 hours agorootparentThis is adjustable via the API, but not in ChatGPT. The API offers styles of \"vivid\" and \"natural\", but ChatGPT only uses \"vivid\". reply bbor 12 hours agorootparentprevThis is just stylistic, and I think it’s because chatgpt knows a bit “better” that there aren’t very many literal photos of abstract floating shapes. Adding “studio photography, award winner” produced results quite similar to SD imo, but this does negatively impact the accuracy. On the other side of the coin, “minimalist textbook illustration” definitely seems to help the accuracy, which I think is soft confirmation of the thought above. https://imgur.com/a/9fO2gxN EDIT: I think the best approach is simply to separate out the terms in separate phrases, as that gets more-or-less 100% accuracy https://imgur.com/a/JGjkicQ That said, we should acknowledge the point of all this: SD3 is just incredibly incredibly impressive. reply smcleod 14 hours agorootparentprevIt looks terrible to me though, very basic rendering and as if it’s lower resolution then scaled up. reply Feuilles_Mortes 15 hours agorootparentprevWhat was difficult about it? reply zavertnik 15 hours agorootparentFrom my experience, the thing that makes using AI image gen hard to use is nailing specificity. I often find myself having to resort to generating all of the elements I want out of an image separately and then comp them together with photoshop. This isn't a bad workflow, but it is tedious (I often equate it to putting coins in a slot machine, hoping it 'hits'). Generating good images is easy but generating good images with very specific instructions is not. For example, try getting midjourney to generate a shot of a road from the side (ie standing on the shoulder of a road taking a photo of the shoulder on the other side with the road crossing frame from left to right)...you'll find midjourney only wants to generate images of roads coming at the \"camera\" from the vanishing point. I even tried feeding an example image with the correct framing for midjourney to analyze to help inform what prompts to use, but this still did not result in the expected output. This is obviously not the only framing + subject combination that model(s) struggle with. For people who use image generation as a tool within a larger project's workflow, this hurdle makes the tool swing back and forth from \"game changing technology\" to \"major time sink\". If this example prompt/output is an honest demonstration of SD3's attention to specificity, especially as it pertains to framing and composition of objects + subjects, then I think its definitely impressive. For context, I've used SD (via comfyUI), midjourney, and Dalle. All of these models + UIs have shared this issue in varying degrees. reply astrange 13 hours agorootparentIt's very difficult to improve text-to-image generation to do better than this because you need extremely detailed text training data, but I think a better approach would be to give up on it. > I often find myself having to resort to generating all of the elements I want out of an image separately and then comp them together with photoshop. This isn't a bad workflow, but it is tedious The models should be developed to accelerate this then. ie you should be able to say layer one is this text prompt plus this camera angle, layer two is some mountains you cheaply modeled in Blender, layer three is a sketch you drew of today's anime girl. reply lucidrains 15 hours agorootparentprevprevious systems could not compose objects within the scene correctly, not to this degree. what changed to allow for this? could this be a heavily cherrypicked example? guess we will have to wait for the paper and model to find out reply bbor 12 hours agorootparentFrom the original paper with this technique: We introduce Diffusion Transformers (DiTs), a simple transformer-based backbone for diffusion models that outperforms prior U-Net models and inherits the excellent scaling properties of the transformer model class. Given the promising scaling results in this paper, future work should continue to scale DiTs to larger models and token counts. DiT could also be explored as a drop-in backbone for text-to-image models like DALL E 2 and Stable Diffusion. Afaict the answer is that combining transformers with diffusers in this way means that the models can (feasibly) operate in a much larger, more linguistically-complex space. So it’s better at spatial relationships simply because it has more computational “time” or “energy” or “attention” to focus on them. Any actual experts want to tell me if I’m close? reply jetrink 19 hours agoparentprevOne thing that jumps out to me is that the white fur on the animals has a strong green tint due to the reflected light from the green surfaces. I wonder if the model learned this effect from behind the scenes photos of green screen film sets. reply zero_iq 17 hours agorootparentThe models do a pretty good job at rendering plausible global illumination, radiosity, reflections, caustics, etc. in a whole bunch of scenarios. It's not necessarily physically accurate (usually not in fact), but usually good enough to trick the human brain unless you start paying very close attention to details, angles, etc. This fascinated me when SD was first released, so I tested a whole bunch of scenarios. While it's quite easy to find situations that don't provide accurate results and produce all manner of glitches (some of which you can use to detect some SD-produced images), the results are nearly always convincing at a quick glance. reply astrange 13 hours agorootparentOne thing they don't so far do is have consistent perspective and vanishing points. https://arxiv.org/abs/2311.17138 reply orbital-decay 12 hours agorootparentAs well as light and shadows, yes. It can be fixed explicitly during training like the paper you linked suggests by offering a classifier, but it will probably also keep getting better in new models on its own, just as a result of better training sets, lower compression ratios, and better understanding of the real world by models. reply awongh 17 hours agorootparentprevI think you have to conceptualize how diffusion models work, which is that once the green triangle has been put into the image in the early steps, the later generations will be influenced by the presence of it, and fill in fine details like reflection as it goes along. The reason it knows this is that this is how any light in a real photograph works, not just CGI. Or if your prompt was “A green triangle looking at itself in the mirror” then early generation steps would have two green triangle like shapes. It doesn’t need to know about the concept of light reflection. It does know about composition of an image based on the word mirror though. reply diggan 18 hours agorootparentprevIt's just diffuse irradiance, visible in most real (and CGI) pictures although not as obvious as that example. Seems like a typical demo scene for a 3D renderer, so I bet that's why it's so prominent. reply mlsu 13 hours agorootparentprevIt does make sense though. Accurate global illumination is very strongly represented in nearly all training data (except illustrations) so it makes sense that the model learned an approximation of it. reply samstave 13 hours agorootparentprevWow - is it doing pre-render-ray-tracing? reply samstave 10 hours agorootparentEDIT: Wrong window folks.... What if you cana scene to a model and just have it calc all the ray-paths and thenany color/image... if you pre-calc various ray angles, you can then just map your POV and allow for the volume as it pertains to your POV be mapped with whatever overlay you want. Here is the crazy cyberpunk part: IT (whatever 'IT' is) keeps a lidar of everything EVERYONE senses in that space and can overlap/time/sequence anything about each experience and layer (baromoter/news/blah tied to that temporal marker) Micro resolution of advanced lidar is used in signature creation to ensure/verify/detect fake places vs IRL. Secret nodes are used to anti-lidar the sensors... so a place can be hidden from drones attempting to map it. These anonolies are detectable thou, and GIS experts with terra forming skills are the new secOPs. Fn dorks. -- so, you already have an asset, lets say its a CUBOID room - with walls and such of wood texture_05.png reply smoldesu 6 hours agorootparentI think you've read too far into this. Ray tracing is not a useful real-world primitive for extracting information from most scenes. Sure, \"everything is shiny\", but most surfaces are diffuse and don't contain useful visual information besides the object they illuminate. Many supposedly \"pure\" reflections like mirrors and glass are actually subtle caustics that introduce too much nuance to account for. Also, \"pipe\" isn't considered harmful terminology (yet) just FYI. I was confused seeing the \"|\" mononym in it's place. reply Workaccount2 19 hours agoparentprevNot bad, I'm curious of the output if you ask for a mirrored sphere instead. reply svenmakes 15 hours agorootparentThis is actually the approach of one paper to estimate lighting conditions. Their strategy is to paint a mirrored sphere onto an existing image: https://diffusionlight.github.io/ reply Hugsun 18 hours agoparentprevThat's very impressive! reply yreg 18 hours agorootparentIt is! This isn't something orevious models could do. reply heyoni 9 hours agoparentprevNow try “a highway being held up by an airplane” Tried all morning and ChatGPT could not do it. reply 8n4vidtmkvmk 7 hours agorootparentThat's hard for me to parse as a human. Do you mean the plane is on the highway and causing a traffic jam? Or is the highway literally being held by a humanoid plane? reply bamboozled 1 hour agorootparentWould it be a highway resting on top of a plane, like the plane is a pillar ? reply iamgopal 18 hours agoparentprevInteresting is that Left and right taken from viewer’s perspective instead of red sphere’s perspective reply ebertucc 16 hours agorootparentHow do you know which way the red sphere is facing? A fun experiment would be to write two prompts for \"a person in the middle, a dog to their left, and a cat to their right\", and have the person either facing towards or away from the viewer. reply npunt 13 hours agoparentprevWe're getting to strong holodeck vibes here reply leumon 16 hours agoparentprev\"When in doubt, scale it up.\" - openai.com/careers reply keiferski 20 hours agoprevThe obsession with safety in this announcement feels like a missed marketing opportunity, considering the recent Gemini debacle. Isn’t SD’s primary use case the fact that you can install it on your own computer and make what you want to make? reply atleastoptimal 10 hours agoparentAGI will be safe and you will be happy. And safe doesn't mean \"lower than 1/10^6 chance of ending humanity\", safe means shoddily implemented curtailing to idpol + fundamentalist level moral aversion towards human sexuality reply j-krieger 2 hours agorootparentRight. For a group of hardliners of the left political spectrum they are weirdly against depictions of sex. reply atleastoptimal 1 hour agorootparentIt's not really their feelings, it's about controversy, bad publicity, etc. It's too delicate right now to risk people using their models for sex stuff. reply jsheard 20 hours agoparentprevAt some point they have to actually make money, and I don't see how continuously releasing the fruits of their expensive training for people to run locally on their own computer (or a competing cloud service) for free is going to get them there. They're not running a charity, the walls will have to go up eventually. Likewise with Mistral, you don't get half a billion in funding and a two billion valuation on the assumption that you'll keep giving the product away for free forever. reply archerx 19 hours agorootparentIronically their over sensitive nsfw image detector in their api caused me to stop using it and run it locally instead. I was using it to render animations of hundreds of frames but when every 20th to 30th image comes out blurry it ruins the whole animation and it would double the cost or more to rerender it with a different seed hoping to not trigger the over zealous blurring. I don’t mind that they don’t want to let you generate nsfw images but their detector is hopelessly broken, it once censored a cube, yes a cube... reply Sharlin 19 hours agorootparentUnfortunately their financial and reputational incentives are firmly aligned with preventing false negatives at the cost of a lot of false positives. reply archerx 17 hours agorootparentUnfortunately I don't want to pay for hundreds if not thousands of images I have to throw away because it decided some random innocent element is offensive and blurs the entire image. Here is the red cube it censored because my innocent eyes wouldn't be able to handle it; https://archerx.com/censoredcube.png What they are achieving with the over zealous safety issues are driving developers to on demand GPU hosts that will let them host their own models, which also opens up a lot more freedom. I wanted to use the stability AI api as my main source for Stable Diffusion but they make it really really hard especially if you want use it as part of your business. reply TehCorwiz 14 hours agorootparentprevEveryone always talks about Platonic Solids but never Romantic Solids. /s reply keiferski 20 hours agorootparentprevBut there are plenty of other business models available for open source projects. I use Midjourney a lot and (based on the images in the article) it’s leaps and bounds beyond SD. Not sure why I would switch if they are both locked down. reply raxxorraxor 1 hour agorootparentStable Diffusion has a much deeper learning curve but can generate far more accurate images fitting your perhaps special use case. Although I don't understand the criticism of the images in question. Without a prompt comparison, it is impossible to compare image synthesis. What are examples of images that are beyond these? reply keiferski 1 hour agorootparentI haven’t used SD so maybe the images on their home page here aren’t representative. But they look very generic and boring to me. They seem to lack “style” in a general aesthetic sense. I am using Midjourney to basically create images in particular artistic styles (e.g., “painting of coffee cup in ukiyo-e style”) and that works very well. I am interested in SD for creating images based on artwork that isn’t indexed by Midjourney, though, as some of the more obscure artists aren’t available. reply raxxorraxor 0 minutes agorootparentUsually there are models adapted to a specific theme since generic models at some point hit barriers. To get an idea, you could look up examples on sites like civitai.com. Of course such sites are heavily biased towards content that is popular, but you will also find quite specific models if you search for certain styles. AuryGlenz 18 hours agorootparentprevSD would probably be a lot better if they didn't have to make sure it worked on consumer GPUs. Maybe this announcement is a step towards that where the best model will only be able to be accessed by most using a paid service. reply bee_rider 18 hours agorootparentprevIs it possible to fine-tune Midjourney or produce a LORA? reply nickthegreek 15 hours agorootparentNo. You can provide a photos to merge though. reply keiferski 18 hours agorootparentprevSorry I don’t know what that means, but a quick google shows some results about it. reply elbear 14 hours agorootparentFinetune means to do extra training on the model with your own dataset, for example to teach it to produce images in a certain style. reply bluescrn 15 hours agoparentprevBefore long we're going to need a new word for physical 'safety' - when dealing with heavy machinery, chemicals, high voltages, etc. reply jiggawatts 11 hours agorootparentJust replace “safety” with “puritan” in all of these announcements and they’ll make more sense. reply causal 19 hours agoparentprevOpen source models can be fine-tuned by the community if needed. I would much rather have this than a company releasing models this size into the wild without any safety checks whatsoever. reply srid 19 hours agorootparentCould you list the concrete \"safety checks\" that you think prevents real-world harm? What particular image that you think a random human will ask the AI to generate, which then leads to concrete harm in the real world? reply dyslexit 12 hours agorootparentThis question narrows the scope of \"safety\" to something less than what the people at SD or even probably what OP cares about. _Non-random_ CSAM requests targeting potentially real people is the obvious answer here, but even non-CSAM sexual content is also a probably a threat. I can understand frustration with it currently going overboard on blurring, but removing safety checks altogether would result in SD mainly being associated with porn pretty quickly, which I'm sure Stability AI wants to avoid for the safety of their company. Add to that, parents who want to avoid having their kids generate sexual content would now need to prevent their kids from using this tool because it can create it randomly, limiting SD usage to kids 18+ (which is probably something else Stability AI does not want to deal with.) It's definitely a balance between going overboard and having restrictions though. I haven't used SD in several months now so I'm not sure where that balance is right now. reply int_19h 9 hours agorootparent> non-CSAM sexual content is also a probably a threat To whom? SD's reputation, perhaps - but that ship has already sailed with 1.x. That aside, why is generated porn threatening? If anything, anti-porn crusaders ought to rejoice, given that it doesn't involve actual humans performing all those acts. reply dyslexit 9 hours agorootparentAs I said, it means parents who don't want their young children seeing porn (whether you agree with them or not) would no longer be able to let their children use SD. I'm not making a statement on what our society should or shouldn't allow, I'm pointing out what _is currently_ the standard in the United States and many other, more socially conservative, countries. SD would become more heavily regulated, an 18+ tool in the US, and potentially banned in other countries. You can have your own opinion on it, but surely you can see the issue here? reply int_19h 8 hours agorootparentI can definitely see an argument for a \"safe\" model being available for this scenario. I don't see why all models SD releases should be so neutered, however. reply astrange 13 hours agorootparentprevThe harm is that any use of the model becomes illegal in most countries (or offends credit card processors) if it easily generates porn. Especially if it does it when you didn't ask for it. reply politician 19 hours agorootparentprevNot even the large companies will explain with precision their implementation of safety. Until then, we must view this “safety” as both a scapegoat and a vector for social engineering. reply astrange 13 hours agorootparentCompanies are not going to explain their legal risks in their marketing material. reply causal 18 hours agorootparentprevIf 1 in 1,000 generations will randomly produce memorized CSAM that slipped into the training set then yeah, it's pretty damn unsafe to use. Producing memorized images has precedent[0]. Is it unlikely? Sure, but worth validating. [0] https://arxiv.org/abs/2301.13188 reply dns_snek 14 hours agorootparentDo you have an example? I've never heard of anyone accidentally generating CSAM, with any model. \"1 in 1,000\" is just an obviously bogus probability, there must have been billions of images generated using hundreds of different models. Besides, and this is a serious question, what's the harm of a model accidentally generating CSAM? If you weren't intending to generate these images then you would just discard the output, no harm done. Nobody is forcing you to use a model that might accidentally offend you with its output. You can try \"aligning\" it, but you'll just end up with Google Gemini style \"Sorry I can't generate pictures of white people\". reply 7moritz7 13 hours agorootparentThen you know almost nothing about the SD 1.5 ecosystem apparently. I've finetuned multiple models myself and it's nearly impossible to get rid of the child-bias in anime-derived models (which applies to 90 % of character focussed models) including nsfw ones. Took me like 30 attempts to get somewhere reasonable and it's still noticeable. reply dns_snek 13 hours agorootparentIf we're being honest, anime and anything \"anime-derived\" is uncomfortably close to CSAM as a source material, before you even get SD involved, so I'm not surprised. What I had in mind were regular general purpose models which I've played around with quite extensively. reply causal 14 hours agorootparentprevEarlier datasets used by SD were likely contaminated with CSAM[0]. It was unlikely to have been significant enough to result in memorized images, but checking the safety of models increases that confidence. And yeah I think we should care, for a lot of reasons, but a big one is just trying to stay well within the law. [0] https://www.404media.co/laion-datasets-removed-stanford-csam... reply astrange 13 hours agorootparentSD always removed enough nsfw material that this probably never made it in there. reply srid 18 hours agorootparentprevOkay, by \"safety checks\" you meant the already unlawful things like CSAM, but not politically-overloaded beliefs like \"diversity\"? The latter is what the comment[1] you were replying to was referring to (viz. \"considering the recent Gemini debacle\"[2]). [1] https://news.ycombinator.com/item?id=39466991 [2] https://news.ycombinator.com/item?id=39456577 reply causal 14 hours agorootparentRight, by \"rather have this [nothing]\" I meant Stable Diffusion doing some basic safety checking, not Google's obviously flawed ideas of safety. I should have made that clear. I posed the worst-case scenario of generating actual CSAM in response to your question, \"What particular image that you think a random human will ask the AI to generate, which then leads to concrete harm in the real world?\" reply thomquaid 13 hours agorootparentCould you elaborate on the concrete real world harm? reply yreg 17 hours agorootparentprevWhy not run the safety check on the training data? reply causal 14 hours agorootparentThey try to, but it is difficult to comb through billions of images, and at least some of SD's earlier datasets were later found to have been contaminated with CSAM[0]. https://www.404media.co/laion-datasets-removed-stanford-csam... reply AnthonyMouse 13 hours agoparentprev> the recent Gemini debacle. I've noticed that SDXL does something a little odd. For a given prompt it essentially decides what race the subject should be without the prompt having specified one. You generate 20 images with 20 different seeds but the same prompt and they're typically all the same race. In some cases they even appear to be the same \"person\" even though I doubt it's a real person (at least not anyone I could recognize as a known public figure any of the times it did this). I'm kind of curious what they changed from SD 1.5, which didn't do this. reply wtcactus 19 hours agoprevI notice they are avoiding images of people in the announcement. I wonder if they are afraid of the same debacle as google AI and what they mean by \"safety\" is actually heavy bias against white people and their culture like what happened with Gemini. reply Lockal 2 hours agoparentI wouldn't look for hidden reasons. Recent image generators are already too good with face generation (thanks to CelebA-like datasets and early researchers). And now the emphasis is on the multimodality of the model within a domain. There, almost every picture demonstrates some aspect of it. Somewhere there is text on the picture (old AI used to output bullshit instead of letters), somewhere there are humorous references to old images (for example, a cosmonaut on a pig). reply Jackson__ 2 hours agoparentprevI believe it's just the usual issue of mistakes in generations being easier to spot in humans, by humans. Usually not a good sign for model quality. reply danielbln 14 hours agoparentprevWhat's white people culture? reply potwinkle 14 hours agorootparentFrom the examples I see on Twitter, they are usually referring to the different cultures of Irish, European, and American white people. Gemini, in an effort to reverse the bias that the models would naturally have, ends up replacing these people with those from other cultures. reply astrange 13 hours agorootparentCalling Irish people white is a rather historically radical statement. reply int_19h 9 hours agorootparentSince the definition of \"white\" is inherently cultural, it varies from place to place and from time to time. Today, in US and Europe, pretty much everyone who cares about racial categorization would consider Irish \"white\". Historically, it was different, but that is only relevant when discussing history. reply sealeck 12 hours agorootparentprevWhite is a pretty complex and non-obvious category. reply samatman 7 hours agorootparentprevNot true, that was made up by race baiting academics who were lying liars. It's absurd that anyone fell for it, frankly. The lie originates with a Communist race hustler named Noel Ignatiev, also known for publishing Race Traitor magazine. A thoroughly unpleasant person. reply 7moritz7 13 hours agorootparentprevUS American white people. Anything else would be a ridiculous overgeneralization, like \"Asian culture\", even if you set some arbitary benchmark for teint and only look at those European countries it's still too much diversity to pool together. reply t0lo 11 hours agorootparentprevA little continent called europe? reply geraneum 9 hours agorootparentAs if you can generalize the culture of different European countries, or even different regions in the same country just by skin color. Now this, in my opinion, is a form of cultural erasure where all the intricacies and interesting aspects of culture are put aside and overshadowed by skin color. reply JayPalm 5 hours agorootparentEnter, The Horseshoe reply hizanberg 18 hours agoprevIMO the \"safety\" in Stable Diffusion is becoming more overzealous where most of my images are coming back blurred, where I no longer want to waste my time writing a prompt only for it to return mostly blurred images. Prompts that worked in previous versions like portraits are coming back mostly blurred in SDXL. If this next version is just as bad, I'm going to stop using Stability APIs. Are there any other text-to-image services that offer similar value and quality to Stable Diffusion without the overzealous blurring? Edit: Example prompt's like \"Matte portrait of Yennefer\" return 8/9 blurred images [1] [1] https://imgur.com/a/nIx8GBR reply Tenoke 18 hours agoparentThe nice thing about Stable Diffusion is that you can very easily set it up on a machine you control without any 'safety' and with a user-finetuned checkpoint. reply cyanydeez 18 hours agorootparentthey're nerfing the models, not just the prompt engineering. After SD1.5 they started directly modifying the dataset. it's only other users who \"restore\" the porno. and that's what we're discussing. there's a real concern about it as a public offering. reply Tenoke 18 hours agorootparentSure, but again if you run it yourself you can use the finetuned by users checkpoints that have it. reply cyanydeez 17 hours agorootparentyes, but the GP is discussing the API, and specifically the company that offers the base model. they both don't want to offer anything that's legally dubious and it's not hard to understand why. reply jncfhnb 17 hours agorootparentprevNo it’s not. It’s perfectly reasonable not to want to generate porn for customers. The models being open sourced makes them very easy to turn into the most deprived porno machines ever conceived. And they are. It is in no way a meaningful barrier to what people can do. That’s the benefit of open source software. reply araes 17 hours agoparentprevTaking the actual example you provided, I can understand the issue. Since it amounts to blurring images of a virtual character, that are not actually \"naughty.\" Equivalent images in bulk quantity are available on every search engine with \"yennefer witcher 3 game\" [1][2][3][4][5][6] Returns almost the exact generated images, just blurry. [1] Google: https://www.google.com/search?sca_esv=a930a3196aed2650&q=yen... [2] Bing via Ecosia: https://www.ecosia.org/images?q=yennefer%20witcher%203%20gam... [3] Bing: https://www.bing.com/images/search?q=yennefer+witcher+3+game... [4] DDG: https://duckduckgo.com/?va=e&t=hj&q=yennefer+witcher+3+game&... [5] Yippy: https://www.alltheinternet.com/?q=yennefer+witcher+3+game&ar... [6] Dogpile: https://www.dogpile.com/serp?qc=images&q=yennefer+witcher+3+... reply Lockal 2 hours agoparentprevGiven the optimizations applied to SDXL (comparing to SD 1.5), it is understandable why it outputs blurry backgrounds. It is not for safety, it is just a cheap way to hide imperfections of technology. Imagine 2 neural networks: one occasionally outputs Lovecraftian hallucinated chimeras on backgrounds, another one outputs sterile studio-quality images. Researches selected the second approach. reply gangstead 17 hours agoparentprevI've never seen blurring in my images. Is that something that they add when you do API access? I'm running SD 1.5 and SDXL 1.0 models locally. Maybe I'm just not prompting for things they deem naughty. Can you share an example prompt where the result gets blurred? reply jncfhnb 17 hours agorootparentIf you run locally with the basic stack it’s literally a bool flag to hide nsfw content. It’s trivial to turn off and off by default in most open source setups. reply stavros 16 hours agorootparentprevIt's a filter they apply after generation. reply lancesells 17 hours agoparentprevI don't use it at all but do you mind sharing what prompts don't work? reply hizanberg 17 hours agorootparentLast prompt I tried was \"Matte portrait of Yennefer\" returned 8/9 blurred images [1] [1] https://imgur.com/a/nIx8GBR reply not2b 15 hours agorootparentIt appears that they are trying to prevent generating accurate images of a real person, because they are worried about deepfakes, and this produces the blurring. While Yennefer is a fictional character she's played by a real actress on Netflix, so maybe that's what is triggering the filter. reply raincole 5 hours agorootparentI found it really stupid. It should just tell me it's against their policy, just like ChatGPT. reply nickthegreek 18 hours agoparentprevRun it locally. reply lolinder 18 hours agorootparentI haven't tried SD3, but my local SD2 regularly has this pattern where while the image is developing it looks like it's coming along fine and then suddenly in the last few rounds it introduces weird artifacts to mask faces. Running locally doesn't get around censorship that's baked into the model. I tend to lean towards SD1.5 for this reason—I'd rather put in the effort to get a good result out of the lesser model than fight with a black box censorship algorithm. EDIT: See the replies below. I might just have been holding it wrong. reply fnordpiglet 18 hours agorootparentBe sure to turn off the refiner. This sounds like you’re making models that aren’t aligned with their base models and the refiner runs in the last steps. If it’s a prompt out of alignment with the default base model it’ll heavily distort. Personally with SDXL I never use the refiner I just use more steps. reply zettabomb 18 hours agorootparentSD2 isn't SDXL. SD2 was a continuation of the original models that didn't see much success. It didn't have a refiner. reply cchance 15 hours agorootparentWell ya because SD2 literally had purposeful censorship of the base model and the clip, that basically made it DOA to the entire opensource community that were dedicated to 1.5, SDXL wasnt so bad so it gained traction but still 1.5 is the king because it was from before the damn models were gimped at the knees and relied on workarounds and insane finetunes just to get basic anatomy correct. reply lolinder 18 hours agorootparentprevThat makes sense. I'll try that next time! reply yreg 18 hours agorootparentprevDo you use the proper refiner model? reply lolinder 18 hours agorootparentProbably not, since I have no idea what you're talking about. I've just been using the models that InvokeAI (2.3, I only just now saw there's a 3.0) downloads for me [0]. The SD1.5 one is as good as ever, but the SD2 model introduces artifacts on (many, but not all) faces and copyrighted characters. EDIT: based on the other reply, I think I understand what you're suggesting, and I'll definitely take a look next time I run it. [0] https://github.com/invoke-ai/InvokeAI reply yreg 16 hours agorootparentSDXL should be used together with a refiner. You can usually see the refiner kicking in if you have a UI that shows you the preview of intermediate steps. And it can sometimes look like the situation you describe (straining further away from your desired result). Same goes for upscalers, of course. reply SV_BubbleTime 14 hours agorootparentprevBasically don’t use SD2.x, it’s trash and the community rejected it. If you are using invoke, try XL. If you want to really dial into a specific style or apply a specific LORA, use 1.5. reply hizanberg 18 hours agorootparentprevDon't expect my current desktop will be able to handle it, which is why I'm happy to pay for API access, but my next Desktop should be capable. Is the OSS'd version of SDXL less restrictive than their API hosted version? reply nickthegreek 18 hours agorootparentIf you run into issues, switch to a fine-tuned model from civitai. reply yreg 18 hours agorootparentprevYou can set up the same thing you would have locally on some spot cloud instance. reply NoMoreNicksLeft 17 hours agoparentprevWait, blurring (black) means that it objected to the content? I tried it a few times on one of the online/free sites (Huggingspace, I think) and I just assumed I'd gotten a parameter wrong. reply pksebben 16 hours agorootparentNot necessarily, but it can. Black squares can come from a variety of problems. reply robertwt7 18 hours agoprevIt’ll be interesting to see what “safety” means in this case given the censorship in diffuser models nowadays. Look what’s happening with Gemini, it’s quite scary really how different companies have different censorship values I’ve had some fair share of frustation with DallE as well when trying to generate weapon images for game assets. Had to tweak a lot of my prompt reply yreg 15 hours agoparent> it’s quite scary really how different companies have different censorship values The fact that they have censorship values is scary. But the fact that those are different is better than the alternative. reply miohtama 18 hours agoprevNo model. Half of the announcement text is “we area really really responsible and safe, believe us.” Kind of a dud for an announcement. reply nextworddev 16 hours agoparentThe company itself is about to go run out of money hence the Hail Mary at trying to get acquired reply yreg 15 hours agorootparentThey raised 110M in October. How much are they burning and how? Training each model allegedly costs hundreds of k. reply londons_explore 20 hours agoprevAll the demo images are 'artwork'. will the model also be able to produce good photographs, technical drawings, and other graphical media? reply spywaregorilla 20 hours agoparentPhotorealism is well within current capabilities. Technical drawings absolutely not. Not sure what other graphical media includes. reply Jensson 19 hours agorootparent> Not sure what other graphical media includes. I'd want a model that can draw website designs and other UIs well. So I give it a list of things in the UI, and I get back a bunch of UI design examples with those elements. reply spywaregorilla 14 hours agorootparentI'm gonna hazard a guess and say well within the capabilities of a fine tuned model, but that no such fine tuned model exists and the labeled data required to generate it is not really there. reply astrange 12 hours agorootparentprevhttps://www.usegalileo.ai/explore https://v0.dev reply dmalik 14 hours agorootparentprevYou'd have better luck with an LLM with HTML/JavaScript/CSS. reply senseiV 12 hours agorootparentprevTheres a startup doing that named galileo_ai reply sweezyjeezy 19 hours agorootparentprevYeah but try getting e.g. Dall-E 3 to do photorealism, I think they've RLHF'd the crap out of it in the name of safety. reply astrange 12 hours agorootparentThat's not safety, the safety RLHF is because it tries to generate porn and people with three legs if you don't stop it. It has the weird art style because that's what looks the most \"aesthetic\". And because it doesn't actually have nearly as good enough data as you'd think it does. Sora looks like it could be better. reply spywaregorilla 15 hours agorootparentprevwell that's what you get with closed ai. reply DiscourseFan 5 hours agorootparentThat's why we need open AI which scoops up all the data with its specific contexts and history and transforms it into a vast incomprehensible machine for us peons to gawk at while we starve and boil to death reply Sharlin 19 hours agoparentprevPhotographs, digital illustrations, comic or cartoon style images, whatever graphical style you can imagine are all easy to achieve with current models (though no single model is a master of all trades). Things that look like technical drawings are as well, but don't expect them to make any sense engineering-wise unless maybe if you train a finetune specifically for that purpose. reply Fervicus 15 hours agoparentprevAnd will the model also pretend that a certain particular race doesn't exist? reply PcChip 20 hours agoprevThe text/spelling part is a huge step forward reply bsaul 19 hours agoprevAnyone knows which AI could be used to generate UI design elements ? (such as \"generate a real estate app widget list\") as well as the kind of prompts one would use to obtain good results ? I'm only now investigating using AI to increase velocity in my projects, and the field is moving so fast, i'm a bit outdated. reply gwern 17 hours agoparentIf by design elements you include vector images, you could try https://www.recraft.ai/ or Adobe Firefly 2 - there's not a lot of vector work right now, so your choices are either the handful of vector generators, or just bite the bullet and use eg DALL-E 3 to generate raster images you convert to SVG/recreate by hand. (The second is what we did for https://gwern.net/dropcap because the PNG->SVG filesizes & quality were just barely acceptable for our web pages.) reply kevinbluer 18 hours agoparentprevv0 by Vercel could be worth a look: https://v0.dev From the FAQ: \"v0 is a generative user interface system by Vercel powered by AI. It generates copy-and-paste friendly React code based on shadcn/ui and Tailwind CSS that people can use in their projects\" reply haolez 17 hours agoprevRewriting the \"safety\" part, but replacing the AI tool with an imaginary knife called Big Knife: \"We believe in safe, responsible knife practices. This means we have taken and continue to take reasonable steps to prevent the misuse of Big Knife by bad actors.\" reply willsmith72 20 hours agoprevat this point perfect text would be a gamechanger if it can be solved midjourney 6 can be completely photorealistic and include valid text, but also sometimes adds bad text. it's not much, but having to use an image editor for that is still annoying. for creating marketing material, getting perfect text every time and never getting bad text would be amazing reply falcor84 20 hours agoparentI wonder if we could get it to generate a layered output, to make it easy to change just the text layer. It already creates the textual part in a separate pass, right? reply spywaregorilla 19 hours agorootparentCurrent open source tools include pretty decent off the shelf segment anything based detectors. It leaves a lot to be desired, but you do layer-like operations automatically detecting certain concept and applying changes to them or, less commonly exporting the cropped areas. But not the content \"beneath\" the layers as they don't exist. reply snovv_crash 18 hours agorootparentWhich tools would you recommend for this kind of thing? reply spywaregorilla 15 hours agorootparentcomfyui + https://github.com/ltdrdata/ComfyUI-Impact-Pack reply deprecative 20 hours agorootparentprevI would bet that Adobe is definitely salivating at that. Might not be for a long time but it seems like a no brainer once the technology can handle it. Just the last few years have been fast and I interacted with the JS landscape for a few years. It moves faster than Sonic and this tech iterates quick. reply amelius 20 hours agoprevDoes anyone know of a good tutorial on how diffusion models work? reply jasonjmcghee 19 hours agoparenthttps://jalammar.github.io/illustrated-stable-diffusion/ His whole blog is fantastic. If you want more background (e.g. how transformers work) he's got all the posts you need reply amelius 17 hours agorootparentThis looks nice, thank you, but I'm looking for a more hands-on tutorial, with e.g. Python code, like Andrej Karpathy makes them. reply astrange 12 hours agorootparentSD3 is a new architecture using DiT (diffusion transformers), so those would be out of date. The older ones have drawbacks like not being able to spell. reply ttul 11 hours agorootparentNot too out of date. Just replace the magic UNet with a DiT and squint. It’s doing the same thing - removing noise. reply spaceheater 20 hours agoparentprevfast.ai has a whole free course https://www.youtube.com/watch?v=_7rMfsA24Ls https://course.fast.ai/Lessons/part2.html reply Ologn 20 hours agoparentprevI liked this 18 minute video ( https://www.youtube.com/watch?v=1CIpzeNxIhU ). Computerphile has other good videos with people like Brian Kernighan. reply ssalka 12 hours agoprevI wonder if this will actually be adopted by the community, unlike SD. 2.0. Many are still developing around SD 1.5 due to its uncensored nature. SDXL has done better than 2.0, but has greater hardware requirements so still can't be used by everyone running 1.5. reply btbuildem 19 hours agoprevThat's nice, but could we please have an unsafe alternative? I would like to footgun both my legs off, thank you. reply Fervicus 15 hours agoparentNope, sorry. We can't allow you to commit thought crimes. reply viraptor 19 hours agoparentprevJust wait some time. People release SD loras all the time. Once SD3 is open, you'll be able to get a patched model in days/weeks. reply SV_BubbleTime 18 hours agorootparentA blogger I follow had an article explaining that the NSFW models for SDXL, are just now SORT OF coming up to the quality of SD1.5 “pre safety” models. It’s been 6 months and it still isn’t there. SD3 is going to be quite awhile if they’re baking “safety” in even harder. reply viraptor 18 hours agorootparent1.5 is still more popular than xl and 2 for reasons unrelated to safety. The size and generation speed matter a lot. This is just a matter of practical usability, not some idea of the model being locked down. Feed it enough porn and you'll get porn out of it. If people have incentive to do that (better results than 1.5), it really will happen within days. reply Der_Einzige 18 hours agorootparentprevDue to the pony community the SDXL nsfw models are far superior to SD1.5. Only issue is that controlnets don’t work with that pony SDXL fine tune reply makomk 9 hours agorootparentApparently most stuff that relies on the weights being resonably similar to SDXL don't work - control nets, LORAs, commonly-used inpainting patches, the lot. It seems to go well beyond fine tuning and be substantially retrained to the point it's a good chunk of the way to being a different model entirely, and the amount of training time is on the order of what went into SDXL originally too from what I can tell. reply SV_BubbleTime 14 hours agorootparentprevI am slightly aware of the pony models. I wish I had something more clever to comment on it. I know what they’re doing which is cool and why which is, IDK, live and let live and enjoy your own kink. It just a little funny some of the most work put into in the fine tuning models.. is from the pony community. So all I have is… :/ reply int_19h 9 hours agorootparentIt's not just them. For example, 4chan is a surprisingly good way to get the most recent scoop on good models (both text and images), setup guides etc - if you can tolerate the inevitable, well, 4chan-ness of it. And the reason is exactly the same: a lot of people there really, really, really, want to generate porn and to chat with sexbots, and they're putting a lot of effort into getting the best (and least censored) results out of the resources that they have. reply AstraliteHeart 4 hours agorootparentprev> IDK We are just trying to satisfy your values though ponies and friendship. reply dougmwne 19 hours agoparentprevSince these are open models, people can fine tune them to do anything. reply ttul 11 hours agorootparentI suppose you could train a model from scratch if you have enough money to blow… reply politician 19 hours agorootparentprevIt’s not obvious that fine-tuning can remove all latent compulsions from these models. Consider that the creators know that fine-tuning exists and have vastly more resources to explore the feasibility of removing deep bias using this method. reply dougmwne 19 hours agorootparentGo check out the Unstable Diffusion Discord. reply SV_BubbleTime 14 hours agorootparentThe vast majority of images there are SD1.5, even the ones made today. Which goes far more towards the idea that safety isn’t a desirable feature to a lot of AI users. reply wokwokwok 19 hours agoparentprevHow would that be meaningfully different to SDXL? I mean, SDXL is great. Until you’ve had a chance to actually use this model, isn’t calling it out for some imagined offence that may or may not exist seems like you’re drinking some Kool-aid rather than responding to something based in concrete actual reality. You get access to it… and it does the google thing and puts people of colour in every frame? Sure, complain away. You get access to it, you can’t even generate pictures of girls? Sure. Burn the house down. …you haven’t even seen it and you’re already bitching about it? Come on… give them a chance. Judge what it is when you see it not what you imagine it is before you’ve even had a chance to try it out… Lots of models, free, multiple sizes, hot damn. This is cool stuff. Be a bit grateful for the work they’re doing. …and even if sucks, it’s open. If it’s not what you want, you can retune it. reply Sharlin 19 hours agoparentprevnext [13 more] [flagged] btbuildem 19 hours agorootparentConsider the impact of the two scenarios below: 1. A mass-distributed LLM (hosted by google or openai or whoever) that's been neutered and twisted into political correctness in a haphazard series of kneejerk meetings of small groups of people who are terrified big investors will walk away or that some powerful political group will denounce them. Effectively they create an enormous bias of falsity and incorrectness, for billons of people to use and embed the results throughout all their intellectual output. 2. Some wacko with an expensive Nvidia GPU makes deepfake porn of a popular politician. Or goodness forbid, of some weird kink where if this was actually a scene filmed with real people, there would be serious ethical issues. Which scenario do you think is more dangerous, long term, and in terms of broad impact on society in general? reply spencerflem 18 hours agorootparentI think you are seeing things from a bubble. Most people in the country are in favor of efforts to correct for historical injustices and are worried about AIs repeating biases in their training that could have a material impact on the world. Case in point: large advertisers and entertainment companies \"pander\" to these sorts of views, because it is broadly popular This is not the work of a shadowy cabal reply shapefrog 18 hours agorootparentI think you are seeing things from a bubble. Most people in the country dont give a shit about historical injustices and are worried about how they are going to pay their rent or put food on the table today. reply spencerflem 18 hours agorootparentA lot of people have the capacity for both I gave some evidence that being socially progressive / \"woke\" is broadly popular. Advertisers are not activists, they're following what people like reply SV_BubbleTime 14 hours agorootparentprevIf you go by online, popularity, yes, a lot of people do want to erase history in favor of feelings. That can be your opinion too. > Broadly popular Are they? Or are the loudest voices asymmetrically affecting discourse? > This is not the work of a shadowy cabal And how would you know if it was? What would the clues be? If you were in a bubble that was designed to impart an informal religious view (and it is a religion sometimes, just one with a screen instead of a book) that encompass politics and morality, how would you know? reply jrm4 19 hours agorootparentprevI appreciate the sentiment, but, like -- this never works. Not in software. Might as well let it go. reply Sharlin 17 hours agorootparentI just cannot comprehend how some people cannot see the incredibly obvious moral responsibility in releasing something that could be used for a lot of good but also to do bad things. There's no reasonable moral theory in which you could just shrug and go, \"well, somebody is going to do it anyway so why should we even try to keep our conscience clean and avoid making it easy for them\", it's fundamentally amoral and antisocial. If someone invents a lockpick capable of opening any door, they have a moral responsibility in preventing it from falling into wrong hands, whether they want it or not. And it's absurd to complain when someone who could create an universal lockpick, refuses to do so, never mind release the technology to the wild, and only agrees to sell simpler picks capable of picking simpler locks. Do these people also complain about things like work against nuclear proliferation? After all, North Korea got nukes anyway, so what's the point? reply jrm4 15 hours agorootparentYour lockpick example only works if there's e.g. only one and it's feasible to keep it hidden from the world. Software doesn't work that way. I agree that you should be responsible about dangerous tech, but you also have to be realistic about what the best way to do that is, which is pretty much never \"keep it hidden.\" (and, of course, this is not even considering the question that should probably go here which is -- how dangerous is this exactly? Given the moral panic we saw a while ago about e.g. Photoshop, I'm not entirely convinced that this is much to worry about.) reply samatman 6 hours agorootparentprevHere, knock yourself out https://covertinstruments.com They don't have a moral responsibility to do jack squat other than sell an honest product and honor their warranties. You don't like that, tough. reply huggingmouth 19 hours agorootparentprevThis wouldn't be as silly a counter argument if there weren't already unrestricted models out there. Unfortunately, this is a very silly argument. I'm so glad that people like you weren't around when they invented the personal computer. reply Sharlin 17 hours agorootparentOn the contrary, the argument that \"someone will do it anyway, so you should just let it happen and take no moral responsibility because I WANT MY SHINY TOY\" is… not merely silly, but incredibly absurd, selfish, entitled, and amoral. One could argue that if $BIGCORP doesn't want their thing to be used by bad actors, they should just refrain from developing the technology at all, and while it's a somewhat defensible position, that would also result in techbros not getting their toy, so it doesn't really apply here. reply HeatrayEnjoyer 19 hours agorootparentprevThe willful ignorance in these threads is maddening. They know why models have these restrictions, they just think the rules shouldn't apply to them and want to play the victim. It's the same libertarian attitude as people who whine about driving speed laws. If there's one lesson from the 21st century, it's that's you shouldn't release massively impacting technology without strong ethics controls around it. reply 393 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Stable Diffusion 3 is a novel text-to-image model in an early preview, offering enhanced performance in multi-subject prompts, image quality, and spelling capabilities.",
      "The model suite varies from 800M to 8B parameters, focusing on scalability and top-notch quality for users.",
      "Safety mechanisms are in place to deter model misuse, highlighting a dedication to ethical AI practices and aiming to provide flexible solutions for boosting creativity and accessibility."
    ],
    "commentSummary": [
      "Stability.ai has launched the Stable Diffusion 3 model, sparking discussions on GPU technology advances, image generation model hurdles, AI safety concerns, and ethical aspects of AI development.",
      "Topics cover model performance enhancement, GPU features, DDR5 and GDDR6 technology fusion, quantization, diverse GPU deployment impacts, and potential computer architecture advancements.",
      "Conversations also touch on AI model limitations, biases, safety protocols, and ethical issues related to handling sensitive data and ensuring cultural and racial diversity in generated images."
    ],
    "points": 928,
    "commentCount": 655,
    "retryCount": 0,
    "time": 1708608039
  },
  {
    "id": 39471116,
    "title": "Bluesky Enables Data Federation for Self-Hosting",
    "originLink": "https://bsky.social/about/blog/02-22-2024-open-social-web",
    "originBody": "BlogBluesky: An Open Social Web Bluesky: An Open Social Web Feb 22, 2024 by The Bluesky Team Today, we’re excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data. What does this mean? Your data, such as your posts, likes, and follows, needs to be stored somewhere. With traditional social media, your data is stored by the social media company whose services you've signed up for. If you ever want to stop using that company's services, you can do that—but you would have to leave that social network and lose your existing connections. It doesn't have to be this way! An alternative model is how the internet itself works. Anyone can put up a website on the internet. You can choose from one of many companies to host your site (or even host it yourself), and you can always change your mind about this later. If you move to another hosting provider, your visitors won't even notice. No matter where your site's data is managed and stored, your visitors can find your site simply by typing the name of the website or by clicking a link. We think social media should work the same way. When you register on Bluesky, by default we'll suggest that Bluesky will store your data. But if you'd like to let another company store it, or even store it yourself, you can do that. You'll also be able to change your mind at any point, moving your data to another provider without losing any of your existing posts, likes, or follows. From your followers' perspective, your profile is always available at your handle—no matter where your information is actually stored, or how many times it has been moved. Federation lets services be interconnected, so there are a variety of apps and experiences that users can move between as fluidly as they do on the open web. The version of federation that we’re releasing today is intended for self-hosters. There are some guardrails in place to ensure we can keep the network running smoothly for everyone in the ecosystem. After this initial phase, we’ll open up federation to people looking to run larger servers with many users. For a more technical overview of what we’re releasing today and how to participate, check out the developer blog. Some of our existing features already follow the federated philosophy, including usernames and feeds. Today, we’re opening up federation for data hosting. Below, we wanted to answer some common questions about what federated hosting is, what it means for your experience using Bluesky, and why we’re so excited about it. How does this affect my experience on Bluesky? The short answer is: it doesn’t! If you don’t run your own server, Bluesky will stay the same. Even if you do run your own server, you may be surprised by how little things change. In fact, it should feel so similar that you might have to double check that you’ve logged into the right server. This is all intentional: we've made self-hosting your own data both easy and affordable, so when you do so, you should be able to get just as good (or better) of an experience than letting Bluesky host your data. If my experience doesn’t change, why federate? We set out to build a protocol for the future of social media that returns control to users. From the apps you use, the feeds you browse, or the moderation preferences you prefer, your experience on Bluesky is yours to customize. But the abundance of choice and innovation here will be short-lived if it’s dependent on one company. We think that the future of social media needs to be a public good, mutually owned by the people and companies that participate in it. Social media should be as open and reliable as the internet itself. The ability to host your own data, just as you might run your own website, provides the fundamental guarantee that social media will never again be controlled by only one company. Even if Bluesky were to disappear, if the data is hosted across different sites, the network can be rebuilt. The fact that it requires no permission to set up a new website is what has made the open web such a dynamic and creative force. Larger services, like search engines, have come and gone (anyone remember Ask Jeeves?). But the underlying foundation of independently hosted sites continues to let the web evolve. Making social open in the same way will create a foundation for better public conversations. Today, we’re taking another step towards the vision of a self-sustaining social web — social media that isn’t controlled by any single company and is free to evolve on its own terms. Does this mean Bluesky is going to be like Mastodon? Mastodon is another federated social network built on a protocol called ActivityPub. While Bluesky — built on a protocol called the AT Protocol (atproto) — shares the term “federation” with other networks, the way it works is very different. On Bluesky, server choice doesn’t affect what content you see. Servers are only one piece of the protocol — when you browse Bluesky, you see posts that are pulled together from many different servers. This is why you can change your server after signing up without losing your username, friends, or posts. A summary of some ways Bluesky differs from Mastodon: A focus on the global conversation: On Mastodon, your “instance”, or server, determines your community, so your experience depends on which server you join. An instance can send and receive posts from other instances, but it doesn’t try to offer a global view of the network. Your Mastodon server is part of your username, and becomes part of your identity. On Bluesky, your experience is based on what feeds and accounts you follow, and you can always participate in the global conversation (e.g. breaking news, viral posts, and algorithmic feeds). You can use your own domain name as your username, and continue participating from anywhere your account is hosted. Composable moderation: Moderation on Bluesky is not tied to your server, like it is on Mastodon. Defederation, a way of addressing moderation issues in Mastodon by disconnecting servers, is not as relevant on Bluesky because there are other layers to the system. Server operators can set rules for what content they will host, but tools like blocklists and moderation services are what help communities self-organize around moderation preferences. We’ve already integrated block and mute lists, and the tooling for independent moderation services is coming soon. Composable feeds: We designed your timeline on Bluesky so that it’s not tied to your server. Anyone can build a feed, and there are currently over 40,000 algorithmic feeds to choose from. Your Mastodon timeline is only made up of posts from accounts you follow, and does not pull together posts from the whole network like Bluesky’s custom feeds. Account portability: We designed federated hosting on Bluesky so that you can move servers easily. Moving hosting services should be like changing your cell phone provider — you should be able to keep your identity and data. Changing servers on Bluesky doesn’t disrupt your username, friends, or posts. So how can I self-host and join the network? It will become easier to host your own server over time, but at the moment you’ll need a bit of technical know-how to get up and running. If you’re excited to jump in, checkout the developer blog, the PDS repo on our Github, and the PDS Administrators Discord.",
    "commentLink": "https://news.ycombinator.com/item?id=39471116",
    "commentBody": "Bluesky announces data federation for self hosters (bsky.social)630 points by jakebsky 15 hours agohidepastfavorite384 comments jakebsky 15 hours agoHey HN, the engineering team at Bluesky is especially excited to get to this point! We're happy to help answer questions and help anyone trying to run their own PDS host. Things should work pretty well for self-hosters right now, but we're standing by to help if there are any problems. Technical details and the installer are in the GitHub repo https://github.com/bluesky-social/pds And we're on Discord available to help: https://discord.com/invite/UWS6FFdhMe reply lolinder 14 hours agoparentIt would probably be worth clarifying in that repo what the license is for both the code in that repo and the code that it's actually running. It looks like it's just a very thin wrapper around @atproto/pds, which is MIT/Apache 2.0 [0], but the repo you link to has no license. Edit: now it has one! Thanks! [0] https://www.npmjs.com/package/@atproto/pds reply jakebsky 14 hours agorootparentYup, it's MIT/Apache 2.0. We'll fix that. Thanks for the heads up. reply paulgerhardt 11 hours agoparentprevUnrelated to engineering but the recent rebrand to a dead butterfly logo[1][2][3] may be off brand for a platform wishing to communicate a more open, social Internet built on first principles and scientific rigor. [1]https://www.emilydamstra.com/please-enough-dead-butterflies/ [2]https://news.ycombinator.com/item?id=14460013 [3]https://bsky.social/about/blog/12-21-2023-butterfly reply mehdix 1 minute agorootparent> If you hadn’t previously noted the difference between a living and a dead butterfly, I’m afraid you will now begin to see dead butterflies EVERYWHERE, as I do. I didn't know this (as most of us I'd guess). It was an interesting read though, thanks. reply ethbr1 11 hours agorootparentprevPedantic lepidopterists of the world, unite! reply DinaCoder99 8 hours agorootparentprev> built on first principles and scientific rigor. Are you joking? This is private enterprise we're talking about. We'll all die before this company or anything similar is built on \"scientific rigor\" unless it directly relates to their profit margins. reply mholt 12 hours agoparentprevAwesome! Why did you choose Caddy as a proxy for PDS? (Caddy creator here.) reply jakebsky 12 hours agorootparentThanks for Caddy, Matt! Some of us on the team have been using Caddy for years, for many of our projects. Because it's so simple, sufficiently high performance, and has lots of nice features. The on-demand TLS certificates with an \"ask\" endpoint is especially useful for the PDS use-case. Because there's generally a wildcard DNS name that is used to give each new user a domain handle (@alice.example.com) but we don't want to be vulnerable to a TLS certificate DoS/rate limit situation. reply mholt 11 hours agorootparentGreat reasons -- glad to hear that! Let me know if you encounter any hiccups or have feedback. Love the fresh federated model btw! reply charcircuit 11 hours agorootparentprevEven if it may be simple in some areas, it doesn't handle edge cases such as https://github.com/caddyserver/caddy/issues/1632 in other areas out of the box unlike other server software. reply Lutger 16 minutes agorootparentThat is a bit unfair, as it is intentionally not doing so. You may disagree with it, sure, but as it stands I think your comment implies oversight or immaturity, which is evidently not the case reading the discussion on the issue you linked. reply nativeit 10 hours agorootparentprevNot for nothing, but when accessed from this HN app on an iPhone, Apple’s website with a trailing dot does not render correctly. reply nyolfen 9 hours agorootparentprevyou have been repeatedly posting this incredibly niche complaint for years at this point reply kuschku 7 hours agorootparentIs it possible you're vonfusing that user with me? I used to be relatively vocal about this issue on HN. For reference, that's not me. And it's probably not niche if dozens of users are posting about it for years. reply charcircuit 8 hours agorootparentprevI have only brought this up once before on HN and it was over 2 years ago. Not adopting a new project because it is missing something niche is an extremely common reason why people stick with tried and true, mature software. I do not see anything wrong with pointing out niche issues because to some people these issues are important. Because it's broken out of the box it is allowing people who aren't aware of this problem to continue to setup broken sites. Even caddyserver.com. is broken. reply napkin 6 hours agorootparentCurious. What is the use case here? I’ve spent tens of thousands of hours of my life on the Internet and a lot of that as a sysadm and I’ve not once heard of people accessing or linking to sites this way. reply charcircuit 5 hours agorootparentPersonally, I just want to properly handle this edge case and it would bother me if my sites didn't handle it correctly. There are advantages for using FQDNs since they are not ambiguous there are extra optimizations that can be done. I don't want my sites to be problematic for people who want to use them so I make sure my sites properly handle them. Usually handling FQDNs is easy as it just works out if the box on most server software. reply napkin 5 hours agorootparentCan you provide a URL to a page containing a link of this style? I’ll concede that it’s useful to someone if I see it! The term ‘FQDN’ does not always imply the dot at the end? reply charcircuit 5 hours agorootparentDo you just want a link? https://news.ycombinator.com./ If you want a link to a page with a link like this you can click on the github issue I referenced in my original comment as there are links there like that. If you want a more natural page that is less meta with such a link https://jameswillia.ms/posts/shortest-urls.html >Also, the term ‘FQDN’ does not always imply the dot at the end? Yes, but if there is not a . at the end then there is ambiguity of if it is a FQDN or not. reply myaccountonhn 12 hours agoparentprevHey! Congrats on the release. Does the AT Protocol only optimize for Twitter-like flows, or does it allow for other types of social applications to be built like Activitypub? For example a reddit-like social media. reply jakebsky 11 hours agorootparentCurrently, atproto works probably best for public social apps, like microblogging, forums, etc. So yes, it's definitely possible to build a reddit-like social app on atproto. Part of the change today is that the PDS and Relay[1] now support non-app.bsky record types. This is quite new, so there could be issues, but we're prepared to fix any issues that crop up. 1. https://bsky.social/about/blog/5-5-2023-federation-architect... reply cabalamat 11 hours agorootparent> microblogging Would it be possible to use it for macroblogging, i.e. long posts with markdown markup, embedded images, etc? If so is there a python library tghat implements atproto? reply jakebsky 10 hours agorootparentYes, it should be totally possible to build a blogging system on atproto. And the \"app.bsky\" API should serve as an example for almost all of the functionality required. Another really neat aspect of atproto, is that apps can interact theoretically. So you might create a blog system but use \"app.bsky\" (Bluesky) for comments. OAuth support is coming soon as well, which is a big step in simplifying auth. reply jakobdabo 14 hours agoparentprevCongratulations on the release! If I may ask a question - is it possible to register an account without a phone number on a 3-rd party server? reply jakebsky 14 hours agorootparentThanks! Yes, it's totally up to a PDS operator to decide how they create user accounts. It's also not required on the Bluesky PDS service any longer, in most cases. By default the self-hosted PDS requires an invite code, to prevent random people from creating an account. Later other options will exist, including OAuth support which is coming soon. reply jakobdabo 14 hours agorootparentThat's great, thanks! > It's also not required on the Bluesky service any longer, in most cases. That's also nice to hear - when last time I tried to register an account (shortly after the free registration launch) the phone number field in the registration form was marked as required, if I am not mistaken. reply jakebsky 14 hours agorootparentYeah, you're right, it was. That was temporary measure during the public launch to prevent spam/abuse. We've made some improvements here recently. reply clot27 14 hours agoparentprevHi, what is the status of integration with the activitypub protocol? as its currently the most popular protocol in federated social media reply Arnt 14 hours agorootparentThere's a bridge nowadays, but… see https://pleromanonx86.wordpress.com/2024/02/17/mastodon-date... which also links to https://wedistribute.org/2024/02/tear-down-walls-not-bridges... https://techcrunch.com/2024/02/14/bluesky-and-mastodon-users... and https://news.itsfoss.com/bluesky-mastodon-bridge/ reply kstrauser 12 hours agorootparentThat was quit the mess. Ryan Barrett is a smart guy and seems quite nice, but it was very ill-advised to unilaterally decide to build an opt-out bridge. In general, if users one platform A want their stuff to be on platform B, they'll find a way to make that happen. If someone else takes it upon themselves to copy everything from A to B, people understandably get pretty bent about it. If it had been an opt-in system, the response would probably have been far different. reply CaptainFever 9 hours agorootparentI'm surprised that the tool in question is Bridgy Fed. Bridgy Fed has existed for a long time and is a very useful tool. Its alternative, Bridgy, has also been used to bridge between closed social networks and the open IndieWeb. Why are Fediverse people only angry about it now? It's an open protocol. If you want privacy, don't publish something for the entire world to see. That's just basic common sense. At the very least, use Mastodon's privacy controls. The Fediverse is not special here, it doesn't get to destroy the open Web for everyone else. reply jeromegv 7 hours agorootparentWell first not everyone on the fediverse is opposed to the bridge. I agree that public is public. But there are concerns about moderation being incompatible, it’s normal to voice them. As for the fediverse destroying the open web for everyone else, I think you’re hyperboling quite a bit, the fediverse has done mountains to make social media more open, probably more than everyone else. reply CaptainFever 5 hours agorootparentYeah you're right, I think I did overgeneralise there. I was meaning more of the culture of \"Mastodon users\"; Mastodon itself has done a lot to help the open Web too. Though I think \"voicing concerns\" is a bit of an understatement. I feel really bad for the developer of Bridgy Fed, working on their passion project and just getting caught up in all this heat and harassment. reply Repulsion9513 12 hours agorootparentprevPublic is public. And someone else will just go build an opt-out (or maybe even no opt-out!) bridge. reply kstrauser 12 hours agorootparentNah. Consent is a thing and this wasn't consensual. Yes, the posts were publicly accessible, but the intent of posting to Mastodon isn't to have it show up automatically on another network. It's technically possible, yes. It's still a dick thing to do and it pissed people off. And again, it wasn't about Bluesky in particular. If Google announced that they were going to ingest all Mastodon content and post it in a new Google Groups kind of thing, they'd be pretty understandably upset about that, too. In general, \"if I wanted my stuff on Bluesky, I would have put it there\". It wasn't the bridge creator's decision to make. reply Arnt 41 minutes agorootparentWhat thing is consent? Mastodon is an odd sort of network, there's more blocking than I expected and it somehow seems as if blocking is an intrinsic part of the design. In Mastodon, blocking looks like a choice one makes for whatever reasons, not an unloved measure needed for fighting abuse. As if the design doesn't tell users \"you can follow people in the fediverse\" but rather \"your ability to follow people in the fediverse is limited by you and three other parties and the software isn't among the three\". So… if the mastodonish idea of consent doesn't extend to all of the fediverse, what makes bluesky different from some unvetted mastodon site run by weird people? If the poster's/follower's/would-be follower's consent isn't taken for granted in one case and isn't taken for granted in the other, what makes the two cases different? There obviously is a technical difference, but what is the difference wrt. consent? reply Repulsion9513 11 hours agorootparentprevPublic = consent for the public to see it. That includes the public on Bluesky. It was consensual. And the ruckus was in fact about Bluesky in particular. That's why the same project already supported other protocols without a big ruckus. In general, \"I want my stuff on Bluesky but don't want to deal with cross-posting to multiple different platforms and keeping up with responses on all of them\" And, \"I want my stuff on whatever platform people want to read it on without having to individually approve each one\" (which is quite literally the entire point of public posts on Mastodon). OH - and it wasn't the bridge creator's decision anyway; it was the decision of people on Bluesky to follow you that would trigger your posts to be federated, so... reply kstrauser 11 hours agorootparentIt was meant for the public to see, not to bulk copy it en masse to somewhere else. Similarly, I don't want my blog posts used to train LLMs. I know they're likely to be since they're published right there on the Internet for anyone to see and read. But my intent was for other humans to see and read them, not for someone to feed them into a regurgitator. There aren't technical means that let me allow humans to read my stuff without allowing LLMs to ingest it, and someone could make the (bad) case that if I didn't want my work to be used to train an LLM, I shouldn't have made it public. Maybe. However, I reserve the right to think someone's an ass for doing it. Well, no technical hurdles kept the person from copying data out of the network people meant to post it to. It's probably not illegal. It's not a nice thing to do, though. reply Repulsion9513 11 hours agorootparent> It was meant for the public to see, not to bulk copy it en masse to somewhere else. Except literally the entire design is for other Mastodon servers to bulk copy it en masse to somewhere else. > There aren't technical means that let me allow humans to read my stuff without allowing LLMs to ingest it Yes there are. Don't make it public. > However, I reserve the right to think someone's an ass for doing it. Of course! You can think anyone is an ass. You can think anything you want. That doesn't mean that person did anything wrong. reply 15457345234 11 hours agorootparentprevnext [11 more] [flagged] rglullis 10 hours agorootparentNo, not \"basic\" stuff at all. It's not only \"allowed\", federation works exactly by replicating all the activities from the remote servers and even the images you share. If you don't believe it, just go ahead and see what happens when you repost a message from someone's server. The repost will come from your server, not the original creator. Or try deleting a conversation on your server and notice how the other instances will still keep everything intact. Also, what would be the problem if you put content on your server and someone downloads it and holds a copy? No one is redistributing your content trying to pass it as their own. Attribution is still preserved. What is the copyright violation here? reply Repulsion9513 10 hours agorootparent> If you don't believe it, just go ahead and see what happens when you repost a message from someone's server It's not just reposts. The original post itself is served to viewers by their instance, which is redistribution. (To do otherwise would actually be a massive privacy - and security - concern.) (To be clear, media - but not the post itself - can be requested directly from the original instance, but any reputable instance will be running a proxy so that random strangers can't trivially get your IP address...) reply Repulsion9513 11 hours agorootparentprevWhen you share a public post on Mastodon, you are by implication licensing that content for redistribution. Because other Mastodon servers have to redistribute it in order for anyone on another instance to see it. Hope that helps you understand the basic stuff here. (Either that or every Mastodon server in existence is a massive copyright violation, which is frankly quite possible given how bad copyright law is) reply Angostura 9 hours agorootparent> When you share a public post on Mastodon, you are by implication licensing that content for redistribution…. … on Mastodon reply Repulsion9513 1 hour agorootparentThe common term for the network formed by software speaking ActivityPub is \"Mastodon\", despite that formally being the name of a single piece of that software. Do you also have a problem with Pleroma or Akkoma or whatever? If not then this isn't relevant. reply icehawk 8 hours agorootparentprev...on a server that speaks ActivityPub. Mastodon is one such piece of software. reply kstrauser 10 hours agorootparentprevI’m a sucker for a particular mix of condescending plus wrong. reply rglullis 10 hours agorootparentExcept they are not wrong. Maybe not in the specifics but in general this is exactly how AP-servers work. If you are on server A and follow someone on server B, then the posts from B will be on your server and there is absolutely no way that the \"original owner\" of the post on server B to remove the post from A. reply Repulsion9513 10 hours agorootparentprevSo am I, that's why I replied to the people who were condescending plus wrong :) reply TylerE 8 hours agorootparentprevThis is more like when you send an email to gmail.com, Google is allowed to forward the message to its intended recipient. reply rglullis 11 hours agorootparentprev> \"if I wanted my stuff on Bluesky, I would have put it there\" How about \"If I wanted my stuff on the your Mastodon server, I would have put it there\"? \"If I wanted my Mastodon content on your RSS feed, I would have put it there\". How about \"If I wanted my stuff on the Internet, a publicly available internet, I would have put it there\". This tribalism around network/brands/protocols is beyond stupid. The thing that is killing Twitter is its closedness and the assumption that the means of communication is what matters. It's not. Let open protocols be open. If people want privacy, then they should use a secure communication protocol and not a social media network. reply cabalamat 4 hours agorootparentprev> Consent is a thing and this wasn't consensual The whole point of a fediverse is it's a federation. Therefore there is implied consent to copying from one instance to another. > but the intent of posting to Mastodon isn't to have it show up automatically on another network Mastodon isn't a network, the network is the fediverse. Mastodon is some software that runs on the network. reply mulmen 11 hours agorootparentprev> Yes, the posts were publicly accessible, but the intent of posting to Mastodon isn't to have it show up automatically on another network. I thought that was the point of activitypub. reply ehPReth 11 hours agorootparentprev>If Google announced that they were going to ingest all Mastodon content and post it in a new Google Groups kind of thing, they'd be pretty understandably upset about that, too. exactly like they did with usenet without any issue? reply pests 8 hours agorootparentWell, at least they paid money for Deja. Slight difference no? reply nl 5 hours agorootparentI'm completely confused under what moral framework the fact that Google paid to buy the Dejanews archive makes a any difference. To make it clear, for people who don't know: Google Groups was originally Dejanews, which was a web based archive and front end to Usenet. Google started searching Usenet, but didn't have historical archives so they bought Dejanews. Obviously no one who posted on Usenet got paid under this transaction. It's like if Google bought a Mastadon archive off someone now: this argument seems to indicate that would be better somehow than Google archiving Mastadon posts themselves. I don't understand why at all? reply Repulsion9513 10 hours agorootparentprevI’m a sucker for a particular mix of condescending plus wrong. reply throwaway828 6 hours agoparentprevGiven the PDS server works on ports 80/443 and I'd like to use a domain (@nytimes.com in the documentation, but say @example.com), how does it interoperate with existing services that already operate on @example.com , for example a website, blog, cloud. I'd imagine this use case is quite common for self hosters. If it can't operate alongside an existing, say, nginx on this port, are there recommended alternate practices? I'm excited at separating identity from hosting, of which self hosting identity gets us closer. reply mightyham 13 hours agoparentprevI'm a little confused why the PDS server is both dockerized and has an installation exclusive to Ubuntu/Debian. reply jakebsky 12 hours agorootparentYeah, there's nothing preventing someone from running the PDS server on other distributions. The installer just does a few convenient things for you (like install Docker, opens port 80/443 using ufw, etc) and we haven't added and tested support for other distributions. There is a Docker compose file in the repo, and advanced users shouldn't have any problems running the code on another distribution or even without Docker if they prefer. Advanced users can just view the installer script as documentation. reply xelamonster 11 hours agorootparentWhy do you need to open ufw if it runs in Docker? Docker does its own routing magic and will happily blast right through any ufw rules. Very cool to see this available though, I might have to try it out later this week! reply mariusor 14 hours agoparentprevAre there any independent projects implementing the AT protocol? reply jakebsky 12 hours agorootparentThere are a number of independent projects using atproto in various ways. There's an (incomplete) list here: https://docs.bsky.app/showcase And the protocol is documented here: https://atproto.com reply mariusor 2 hours agorootparentThank you, I might be searching for the wrong things, but I don't see any independent servers. There's clients, libraries, bots, but no servers, am I missing something? My question was motivated by the fact that from the outside the AT proto ecosystem looks pretty monocultural, and personally I don't trust that. :) reply hnbad 12 hours agoparentprevHi. If the protocol is open, the software is free and the main instance openly federates with self-hosters, what's the monetization strategy here? Clearly it's not \"harvest all the data and figure it out later\" as that avenue seems to be shut down internationally by strengthened privacy laws and ads don't work well with federation and third party clients. Is \"grow first, figure out how to make money later\" still a viable strategy in this economy? reply riffic 11 hours agorootparentmanaged hosting perhaps? It works in the email industry at least (Google and Microsoft nearly dominate the email biz) reply hnbad 11 hours agorootparentYeah but that assumes ATP reaches anything even remotely approximating the ubiquity of email rather than ending up like Google Wave (not literally by being handed off to Apache - which took Wave behind the barn in 2018 in case you're wondering what happened to it). reply daredoes 10 hours agoparentprevGonna be that guy! Any chance the team could create a Home Assistant add-on for this? https://www.home-assistant.io/addons/ I think the Home Assistant community would go WILD for being able to self-host their Bluesky data straight from home with just a few clicks. It's a pretty big crowd of people. https://analytics.home-assistant.io/ 327k willing to opt-in to analytics. reply floren 10 hours agorootparentwe need a new version of Zawinski's Law: every system capable of deploying plugins will eventually expand until it is a full hosting solution. I know if there's one thing I'm eager to do it's to host even more stuff in that clunky piece of shit that has half a dozen main menu items for nonsense and buries everything of interest or value under \"Settings\" reply daredoes 10 hours agorootparentThe add-ons are just docker containers? It's wasteful to get an entire second machine for something that can use the resources available on the machine running Home Assistant OS reply doublerabbit 14 hours agoparentprevWill this work for bare metal? I use BSD, and all I see is a installer for Debian/Ubuntu. No guide in sight for bare metal nor telling you what services/software are required. reply whyrusleeping 14 hours agorootparentyeah it works fine on bare metal, you'll just have to do a bit more set up work yourself (https terminating and such). The installer script should be instructive in how to run it but you'll have to figure out the BSD specific stuff reply KerrAvon 3 hours agoparentprevHi jake lovey! Could you fellows stop borderline gaslighting people on how ActivityPub/Mastodon works? You’re not isolated to the server you join on mastodon. Lies do not become us. Thank you. reply CaptainFever 48 minutes agorootparentThis is incredibly rude, irrelevant (I could not find your claim anywhere in the article posted) and not the way to behave on Hacker News. \"Be kind. Don't be snarky.\" Please be better. reply emceestork 14 hours agoprevI switched to Bluesky but then moved back to twitter. I'm glad that they are trying to compete with Twitter (Twitter is a conservative cesspool), but all of my non-technical friends have stayed on Twitter. So, I end up going where they are. I think the reason my friends did not join Bluesky despite me inviting them is that it just isn't as good of a product as Twitter. You can't post videos or DM. I am not a tech executive and have no idea about corporate strategy, but it seems like Bluesky should focus less on technical differentiators and more on building killer features that have mass appeal and a community that people want to join. IMHO this milestone, while cool, means absolutely nothing to people outside of the hacker news crowd. I'm rooting for Bluesky, but it seems to me it will die without a critical mass of users. Again, I'm kinda dumb, so this may all be wrong. reply elpool2 14 hours agoparentI think everything you said was fair, but you also mentioned Twitter being a conservative cesspool, and a lot these features like federation and composable moderation are designed to help prevent the whole \"rich guy buys the company and turns it into something you don't like\" scenario. reply phone8675309 12 hours agorootparentFederation is nice but when the platform only does one-third of what the platform you're trying to leave does then the whole thing feels like a toy reply Nuzzerino 10 hours agorootparentprevIt's really unfortunate that the tech companies set the precedent in the first place by pushing hard political agendas into their policies and moderation biases. If it was truly neutral in the first place we would not be having this conversation. All the people complaining only now about Twitter doing this are part of the problem. reply 1shooner 10 hours agorootparentOf course people are going to complain about content they don't want. That's the product. Twitter changed their product to deliver different content, so its audience has changed. Calling it 'The Problem' like climate change or the national debt gives it too much power. Just use something else. People use group chats for real relationships now anyway. reply wpietri 6 hours agorootparentprevThere is no \"true neutral\" when it comes to moderation. There are a million examples, but the most obvious are of the form \"you can have group X or people who hate group X and are dedicated to driving them off the platform\". Somebody's not going to have \"free speech\" in that case. And even if you go for what most \"true neutral\" advocates want, which is a lack of rules, you'll quickly find that quite a lot of people don't want to hang out at the place that's filled with Nazis or scam artists or spammers or whatever. So in practice you have to make choices, or you'll end up running the new 4chan and being sad about your life. As happened to the guy who ran the old 4chan: https://www.rollingstone.com/culture/culture-features/4chans... reply haberman 5 hours agorootparentTrue neutral means moderating consistently, judging behavior without regard to identity. The ideal, platonic version of this would be that moderators only see an \"identity scrambled\" version of each tweet/post when they make their moderation decision. Like a screen that blinds orchestra musicians when they audition, the human would see a statement like \"I hate New Yorkers\" and not know if the original message said \"I hate New Yorkers\" or \"I hate Floridians.\" So they would have to make a decision based on the general principle of whether a statement of this form is allowable. Anywhere you want to draw the line is fine with me, as long as you draw it consistently. reply wpietri 4 hours agorootparentThat sounds like a very personal definition of \"true neutral\". And also an unworkable one. Take the use of reclaimed slurs, for example. When used against the discriminated group by a dominant group, their intention is often to cause harm. When used within the group, the intention is to reappropriate the term: https://en.wikipedia.org/wiki/Reappropriation Similarly, harassers will use terms in ways that are plausibly read different ways depending on who they're talking to. So something that might sound innocuous or just odd when directed at me will be correctly read as a racist attack when directed at somebody else. And that's not even counting when they'll just come up with new terms so they can be awful in ways that are novel enough that automated filters or out-of-date moderators won't catch. E.g.: https://www.vice.com/en/article/bv88a5/white-supremacists-ha... In short, because there's a great deal of identity-based hate in the world, identity-blind moderation ends up being an aid to the identity haters out there. reply haberman 4 hours agorootparentThe element of moderation that you consider essential -- the latitude to apply subjective judgments that rely on knowing the specific identities of the participants -- is precisely the element that I do not trust moderators to perform. That this moderation strategy would prevent the use of all slurs (even reappropriated ones) sounds like a feature to me, not a bug. reply dudinax 3 hours agorootparent\"That this moderation strategy would prevent the use of all slurs (even reappropriated ones) sounds like a feature to me, not a bug.\" You're proposing erring on the side of censorship to avoid some gray areas. While this is a reasonable position, it doesn't satisfy some ideal of neutrality and won't really avoid the gray areas, and so still would require subjective judgement. reply hackernewds 4 hours agorootparentprevtruly neutral = post anything? there exist such platforms and they're cesspools because human nature reply jrflowers 8 hours agorootparentprevnext [5 more] [flagged] Nuzzerino 8 hours agorootparentStop with the straw men. That has nothing to do with the backstory of why he bought Twitter. reply jrflowers 7 hours agorootparentnext [4 more] [flagged] Nuzzerino 7 hours agorootparentYou sound mad. Get some air. reply jrflowers 5 hours agorootparentIf you are only now complaining about my posts you are part of the problem reply oittaa 2 hours agorootparentHow do I block low iq trolls like jrflowrs on this platform? I tried to search the FAQ but couldn't find anything. reply pfraze 14 hours agoparentprevIt's a fair point and we definitely debated it, but it was too important to us that we complete the mission. reply emceestork 14 hours agorootparentAh yeah, I get that. I don't mean to be cynical on the day you complete that mission. Congrats on launching! Excited to see what y'all do next. reply DevX101 12 hours agorootparentprevYou guys made the right call. You're not trying to become the next TikTok. reply kunalgupta 14 hours agorootparentprevthis is the correct order of operations for sure reply KerrAvon 14 hours agorootparentprevDebated videos and DM? Mastodon has those features; if you're not doing them at all you may want to reconsider. reply danabramov 14 hours agorootparentWe definitely want to implement these features, the question was whether they should hold back releasing support for federation or not. Since federation is a core constraint on any features we'd like to build, the team felt that there is no reason to hold federation back, and that releasing it as it's ready makes sense. We're thinking about Bluesky as both a product and a protocol (informing each other's design), and you're 100% right that for the end user, the product itself is what matters. Because we've taken on the decentralization constraint, we take longer to \"catch up\" to features that centralized platforms tend to have from the start, but it's definitely going to be a major area of focus for us going forward. (Ofc Mastodon isn't centralized, but note that it's had a few years of head start on feature development. We'll get there.) reply pr337h4m 12 hours agorootparentprevMastodon DMs have absolutely no privacy: https://github.com/mastodon/mastodon/issues/18079 For a decentralized protocol doing things right is much more important than doing things fast, it is very difficult (and in a lot of cases impossible) to break backwards compatibility. reply Repulsion9513 12 hours agorootparentDMs on any other service also have no privacy. Signal or Telegram could read your DMs by simply releasing an update to their code, for example. You always have to trust the person running the service you use. (Unless you have E2EE/something like OTR, in which case you have to trust the persom who makes that code!) reply pr337h4m 12 hours agorootparentThe whole point of end-to-end encryption is that you don't have to trust the people running the service you use. If Signal releases a malicious update (and they don't provide reproducible builds), it is very much possible for you to know about it, as everything is on your device. Even if the binaries are different from the source code, decompilers, analyzing network traffic, etc. gives the community a good chance at catching malicious updates. Mastodon admins can simply pull up your plaintext DMs on their servers and no one will ever know. reply fodkodrasz 56 minutes agorootparentJust make ci releases with daily updates. Good luck reverse engineering and auditing that. If the protocol is not open, you have to rely in the clients provided by the vendor, and you can slip a backdoor throigh easily. When did you last audit your Signal client? Where is “the commjnity” organizing this effort and publishing the results? Debian shipped an entropy lowering in house patch despite the “many eyeballs” fos years (for OpenSSL). Don’t lure yourself into false feeling of security bevause of the “community” might be doing something. Only count on defenses surely in place, with traceable operation and output history, with responsibles who are allocated resources for the work and having stakes at its outcomes. reply Repulsion9513 11 hours agorootparentprev> The whole point of end-to-end encryption is that you don't have to trust the people running the service you use. Well then I guess it's pointless because it doesn't accomplish that. (The actual point, FYI, is that you don't have to trust all of: them, their hosting providers, your ISP, the ISPs between, the government, and their mom.) > it is very much possible for you to know about it \"Possible\" != \"done\" > analyzing network traffic How are you gonna do that? Surely if they wanted to sniff it would still just look like any other encrypted data > gives the community a good chance at catching malicious updates Sure, when the same application is used by everyone, which is not true in either the Mastodon world or the new Bluesky-small-instances world reply notpushkin 10 hours agorootparentprevI think Mastodon has a pretty good balance here – when you try to send a DM it explicitly tells you that it will not be encrypted: https://u.ale.sh/Vo1ahx.png And the linked privacy policy goes into further detail (at least on my instance, mstdn.io): > Please keep in mind that the _operators of the server and any receiving server may view such messages_, and that recipients may screenshot, copy or otherwise re-share them. Do not share any sensitive information over Mastodon. Overall, I think it's safe for most chit-chat, and for anything more serious you can add link to Matrix or your email and PGP key in your profile. As a sidenote, I'd also like to point out that a lot of serious communication nowadays still happens over unencrypted email. You can consider it whataboutism, but it's still worth remembering IMO. (And of course, like others pointed out, DMs on Twitter aren't encrypted, too, so it's the status quo here.) reply pfraze 14 hours agorootparentprevDebated prioritizing them before federation, not debated their existence. They are a must-have for social. reply t_mann 14 hours agoparentprevNot sure I agree. Being the thing that the tech folks find cool isn't a bad starting position at all. And it's significantly harder to achieve than DM's. reply 303uru 14 hours agoparentprevI've been using Bluesky for a week and I'm impressed. I actually appreciate that there is less media, it's more about conversation. So far it feels very much like Twitter before it became a cesspool. I'm conversing with local journalists, prominent scientists, sci-fi authors, etc... It's wonderful. reply mvdtnz 14 hours agoparentprevNo video in 2024 is a total deal breaker for most users. That's insane. reply eropple 13 hours agorootparentVideo is also prohibitively expensive outside of Google-scale endeavors and will likely crush both third-party BGSes and PDSes. Everyone doing video is either selling you ads (whether it's in that video or around it), selling you the video itself, or is losing money. Possibly all three. As it is, og-embeds do work for video and audio from a few different providers. reply Choco31415 11 hours agorootparentIf storage is a problem, couldn’t Blusky add a size limit to video uploads? reply eropple 11 hours agorootparentIt's not storage, it's bandwidth. Most system providers, for Bad Reasons, charge a lot for egress; even relatively good ones like Cloudflare have particular payment shenanigans around video. Size uploads could help in that situation, but it's a bandaid on a bullet wound when the video still gets played a million times, y'know? reply Dylan16807 4 hours agorootparent> It's not storage, it's bandwidth. A size limit affects both equally. > even relatively good ones like Cloudflare have particular payment shenanigans around video That's only for the web service. Workers and R2 let you do video just fine. And small videos don't need any fancy logic, just toss them over http. > Size uploads could help in that situation, but it's a bandaid on a bullet wound when the video still gets played a million times, y'know? No, I legitimately don't know. Why is it different from an image that gets a million views? reply numpad0 9 hours agorootparentprevThen people are going to link to YouTube and benefit of getting lesser copies is small. The rest of media that works within a limit on a microblogging are junk. reply notpushkin 10 hours agorootparentprevHow does Mastodon do it? reply eropple 10 hours agorootparentBy externalizing costs onto server owners, the same way they do everything. It's not a good way to do it, though, and it's worse for Bluesky because it implies that in order to move your PDS (one of the best features of the design), you'd have to pick up the freight for video that already exists. If PDS mobility is important, attaching large economic strings to that is a big disincentive. reply wmf 12 hours agorootparentprevThat gives me a great idea: If you self-host your PDS you can have video but moochers don't get it. reply eropple 10 hours agorootparentSure, but that's Scary(tm), because an unexpected viral skeet is going to cost you a lot of money. In this case I'd expect almost all video to be fraudulent--either in terms of pirated egress or in terms of disinfomation scams. reply wmf 9 hours agorootparentObviously you should choose a hosting provider that doesn't allow bandwidth overages. reply eropple 8 hours agorootparentIn such a case you absolutely should! At the same time, it makes sense that bsky, as the protocol stewards, might not want to make that pit too easy to fall into. reply pests 7 hours agorootparentprevWhy must we insist on calling these things after an euphemism for ejaculation? Never going to be taken seriously by the public. reply mvdtnz 13 hours agorootparentprevGreat. But without video you lose most of your users. reply sfink 11 hours agorootparentI am increasingly of the mind that this is a feature, not a bug. If you want to be Twitter, you'll end up being Twitter. We already have one of those, it sucks, and we don't need another one. Social networks go to crap above a certain scale. If everyone can see your posts, you'll write posts to be seen by everyone. Which, as it turns out, ends up benefiting no one. The magic comes when there is a community, where you give a shit about the people you're sending messages to, and they give a shit about you. If the community is too small, then nobody bothers with it and it dies. If the community is too large, then it ends up being old men screaming at clouds, and (see above) we already have one of those. So a platform that is good enough to use, but limits the number of disaffected members, is the only thing worth creating. If something I'm saying requires a video, then I can always link to one. If something someone else is saying requires a video, and it requires the video to be immediately visible while I'm reading whatever they're saying, then there's a good chance I'm better off not seeing it anyway, even if I think I want to. (Ironically, in this post I am an old man screaming at clouds...) reply eropple 11 hours agorootparentI agree with this--and also, again, you have video, if you upload it to YouTube. Or stream via Twitch. Like, you don't need on-platform video. Google makes more money than God; let them pay for the perturbed bits. reply omoikane 12 hours agorootparentprevFeature requests opened since 2023-07-24: GIF support: https://github.com/bluesky-social/social-app/issues/1047 Audio/video support: https://github.com/bluesky-social/social-app/issues/1052 reply bigstrat2003 14 hours agoparentprevnext [26 more] [flagged] hathawsh 13 hours agorootparentThat was the problem with Twitter/X. It appears that Elon's tweaking has caused the service to switch from left-leaning to right-leaning. https://www.pewresearch.org/short-reads/2023/05/01/after-mus... reply packetlost 13 hours agorootparentThat study is based on user sentiment/survey, so I wouldn't really put any stock in to be completely honest. The only thing this study tells me is there was a change relative to some baseline, not that there was any sort of absolute lean. reply Repulsion9513 12 hours agorootparentWhereas the GP comment was based on... an anecdote from a celebrity. Definitely more trustworthy. Yep. reply adra 12 hours agorootparentprevThere's no absolute lean at all it's all subjective, and sadly multidimensional which is hard for normal people to appreciate, so sure. Soooo.. Right and left are objective bars in the sand and my options are perfectly formed and happen to be exactly just on the correct side of all issues. reply packetlost 12 hours agorootparent> There's no absolute lean at all it's all subjective, and sadly multidimensional which is hard for normal people to appreciate, so sure. I guess \"absolute\" is the wrong word. Lean isn't really a measurable thing, which is sorta my point. The GP claimed that there was, my claim was that all we have are people's feelings, and people don't feel in absolute terms, they just notice when there's a change so the linked data on peoples feelings probably doesn't support the claim. reply hathawsh 13 hours agorootparentprevFair enough. Personally, I like to watch both sides, and it has been interesting to see conservatives becoming more favorable toward Twitter/X. If I were working on a social network (like Bluesky, the topic of this discussion), I would be watching Twitter/X closely right now to try to understand the effects that certain tweaks (such as moderation policy) may have. reply packetlost 13 hours agorootparentAgreed! My own gut feeling is that it leaned pretty heavily left and it's now closer to center than ever, and people are noticing the delta. Every time I open BlueSky it's like 50% Liberal rage-bait and like 40% people posting mostly about their sexual and gender identities. reply djur 13 hours agorootparentThe posts you see on Bluesky by default are the posts made by people you follow. There are other options to select whichever feeds you prefer. If you're seeing a lot of that content, it's because you're following people who post a lot of that stuff. reply packetlost 12 hours agorootparentThis is not on my following feed, it's on discover. reply packetlost 10 hours agorootparentprevI'd like to add, that I don't have a problem with people doing this: people should be able to express themselves however they want. I'm just not particularly interested in that type of content. reply bigstrat2003 13 hours agorootparentprevTo be honest, I think Twitter (and other similar sites) are a huge negative for society no matter who dominates the echo chamber. Training everyone to dump 140-char hot takes as a dominant form of \"discourse\" has been a genuine evil for the world. Twitter delenda est. reply robin_reala 12 hours agorootparent280 since 2017. reply babypuncher 12 hours agorootparentprevPretty hard right-leaning too. They regularly ban left-leaning journalists who are critical of Musk, while letting actual neo-Nazis roam free. They over-corrected to an extreme degree. I still can't believe they banned Alexei Navalny's wife after Putin murdered him. And I find it sickening that there are people here who defend that shit. reply vidarh 12 hours agorootparentprevThat's funny given the number of people explicitly expressing support for fascism I've blocked on my Twitter account over the last week. reply GordonS 11 hours agorootparentI think the reality is that it's a mix - but people tend to follow others with similar views. reply vidarh 20 minutes agorootparentThe problem with that is that Twitter defaults to showing you their idea of what you should see, not who you follow, and they clearly optimize for \"engagement\", not what you like, and hate drives engagement. reply hellojesus 13 hours agorootparentprevDoesn't Twitter like most socials feed you things with which they predict would increase your engagement or time on platform? If the GP's social circle leaned more conservative than them, it may just be bad predictions. And I can see how that would degrade user experience. I often hear things like X service skews left/right and find it odd that people can have such differing experiences. Hence my belief that the feed algos are primarily the issue. People do like confirmation of biases. reply basil-rash 13 hours agorootparentWhen somebody else is wrong on the internet, one feels compelled to comment and explain one’s righteousness, or at the very least read through the comments to see if somebody else already has. Thus algorithms that optimize engagement (most of them) are most likely to show you stuff you disagree with. The antidote is Linkedinification: “Thanks for bringing up this point it’s really smart and I fully agree!” reply hellojesus 13 hours agorootparentUnderstood. But if you, as a company, realize you're losing users because everything you show is rage bait, maybe adjustments are made. Or your point stands and helps to describe why the non conservative GGP was of the belief that X is conservative leaning. reply djur 13 hours agorootparentprevTwitter has a lot of very right-wing posters and a lot of very left-wing posters. Which of these you're likely to see as a huge problem and which you're likely to dismiss as a few insignificant goofballs is probably going to be based on your own political orientation. It would be surprising if there hasn't been a rightward shift, though, considering that causing such was one of the current owner's explicit goals. I don't think it's unreasonable for people to perceive Twitter as favoring the political right when the owner is actively doing so. reply bigstrat2003 12 hours agorootparentI'm certainly willing to believe that there's been a shift. But it's completely implausible that in just a year (two? I forget how long it's been) since the sale, Twitter has transformed into a \"conservative cesspool\". What the poster I replied to is almost certainly seeing is that there's actual balance now, instead of being absurdly hard left like it was before. reply Repulsion9513 11 hours agorootparentIt has never been \"absurdly hard left\". It was representative of the communities that used it (many of whom have been chased off by Musk). reply bigstrat2003 10 hours agorootparentNobody was \"chased off\" by Musk. The people who left were freaking out and planning to move to Mastodon even before the sale went through, because they assumed that Musk would be an evil man doing bad things to the site. To be blunt: they left because they couldn't bear to imagine a world where site leadership didn't put its boot on the neck of anyone they disagreed with. reply Repulsion9513 10 hours agorootparentLOL in so many ways You think Twitter doesn't still \"put its boot on the neck of anyone they disagreed with\"? Actually now it's \"anyone they or Musk disagree with\", so I guess you're kinda right. Meanwhile he brought back people who were banned for committing crimes on it while banning people for posting entirely legal content like jack sweeney. He's not a free speech absolutist, he's a me speech absolutist. But sure, nobody was chased off by Musk. reply timeon 12 hours agorootparentprevNot sure what your point is. `Left` and `conservative` are not mutually exclusive things. Why are you juxtaposing them here? reply dustedcodes 11 hours agoparentprevnext [3 more] [flagged] troad 11 hours agorootparentDemonstrably false - elder hippies are a hoot. But more importantly, a bit rich for someone still caught up in the left-right dichotomy to be telling others to wise up. reply Gigachad 10 hours agorootparentprevs/wisen up/develop alzheimer's reply sergiotapia 14 hours agoparentprev> Twitter is a conservative cesspool I disagree. If anything now it's more balanced, every \"right of Portland-liberal\" is no longer hidden and shadow-banned or worse. I like it a lot more! Now you can actually read and learn about stuff you care about. reply emceestork 13 hours agorootparentYeah, maybe we just have different politics and I'm too dismissive of alternative worldviews. Still though, I get like Matt Gaetz' tweets recommended to me. Does anyone like that dude? How is this happening? Why on earth would I want that? I feel like all this conservative stuff is surfaced by the application to me. [Proof](https://ibb.co/ypHS8fN) I got notifications, on my dang phone, for the dumbest fucking takes. I don't get them for liberal people. Possible I am just in the demographic of people they think would swing conservative so they target me. reply MaxHoppersGhost 13 hours agorootparentThis is funny to read since this is basically what conservatives experienced for the last ten years on Twitter pre-Elon. reply bl_valance 11 hours agorootparentprevI agree. I now see both extremes(horseshoe) and in-between as much. While before it was heavily leaned towards the left. reply kouru225 13 hours agorootparentprevNavalny’s wife was just banned and then shadow banned. There are countless examples of leftist accounts getting banned just for being critical about Musk. It’s absolutely conservative cesspool. Nazis can are literally posting 14 words propaganda all day long and there are no consequences. reply sureglymop 17 minutes agorootparentTo think that Navalny or his wife are in any way leftists is insane. They were leaning more towards nationalism (just for Russia instead of the US). In the face of current events though, that can be overlooked as it is not a narrative that serves western interest. But to give an anecdote about an alternative, I rarely see any bigotry on Mastodon. Instances which allow that or don't moderate it correctly get block listed by instances I like. These instances may defederate into their own bubble in which they can still exist but cause no harm to the general timeline. Yet everyone still remains the freedom to express themselves. I like this because it's just a natural way of how to solve this problem for the end user. reply AviationAtom 9 hours agoparentprevThis whole thread is oof. Modern politics, to include both sides of the spectrum, has devolved rapidly. It feels like a real-life version of the Spiderman meme. reply cocacola1 7 hours agorootparentIf anything, modern politics is better. reply lazzlazzlazz 12 hours agoparentprevI'm not sure what kind of \"cesspool\" Bluesky is, but it's unbearable. It's like 2015-era Tumblr but worse, somehow. Twitter, by contrast, feels like a breath of fresh air. reply hnbad 11 hours agorootparentI'm wondering if you really mean \"2015-era Tumblr\" or are trying to evoke pre-Trump liberals on Tumblr (i.e. \"manspreading is a micro aggression\" pop feminism and teenagers creating fan lore about gender identities) by referring to it as that. If anything, my experience of Bluesky has been the inoffensive vapid thought leadering of peak Twitter alongside the playful air-headed liberal self-help that is also fairly reminiscent of peak Twitter. In one word: bland. Being able to paint over the offensive things like nazis and porn by sweeping them under your personal rug rather than blocking or banning them only adds to this impression for me. Twitter, your breath of fresh air, on the other hand is overrun by ChatGPT spam bots and shovelware drop shipping ads worse than the crypto \"giveaway\" scams and paid tweets of the immediate pre-Musk days and every even moderately left-leaning political tweet is filled with replies describing the violent acts they want to do to that person in excessive detail by accounts that openly post literal neo-nazi propaganda videos of Adolf Hitler denouncing \"degenerate art\" as a Jewish plot to weaken the German volk and national spirit and going \"I don't agree with everything he did but he had a point\". Political discussions about the Middle East in turn are split evenly between right wing calls for genocide of all adults and children in Palestine and right wing defenses of Palestinians for being victims of the international Jewish conspiracy to exterminate the white race through mixed breeding with brown refugees. We used to always call Twitter \"the bad place\", \"hellsite\" or \"cesspool\" before Musk but it certainly deserves those names now more than ever, arguably rivaling 4chan in its political takes although the depictions of gore are mostly limited to uncensored war footage and the porn is decidedly more tame. The reason Twitter is called a \"right-wing cesspool\" is not because it's full of right-wing people (that would just make it a \"pool\"), it's because of the vicious explicit threats of violence and celebration of human suffering propagated by those people. For all its faults, the bland libs on Bluesky don't do much of that. Granted, my experience of Twitter might be tainted by the fact most people I used to follow in the old days have either left or are no longer active and any time I visit the algorithmic timeline hits me at full blast. And a lot of the edgier posts (not replies) by right wing folks the avalanche of drama RTs throw my way are clearly created to farm engagement in the hope of striking it big if the bluecheck authors make the payout lottery. reply edgarvaldes 12 hours agoparentprev> Twitter is a conservative cesspool Interesting. I see it as the de-facto journalist platform, which to me (as a non american) make it very left leaning. But then again, I don't use X. reply belkinpower 12 hours agorootparentIt was. In the last year it’s become largely conservative, and not in a standard reasonable small-government, etc. way. It’s like reading Facebook posts from your dumbest uncle. reply j-j-j-j 11 hours agorootparentIt didn't become, just suppressing what was already there is gone. reply myko 12 hours agorootparentprevAmerican journalism isn't left-leaning. At best it is \"click\" leaning, and say what they need to do to get eyeballs on their content. This is why they helped normalize trump so hard, and repeatedly fail to call out the extremist right wing in the US. reply hnbad 11 hours agorootparentThis is what a lot of people don't get about the \"pop feminism\" era of online \"journalism\" in the pre-Trump era: it wasn't feminism, it was clickbait. At best it sold an idea of feminism but the emphasis was always on the selling part and not anything ideological. \"Girlboss feminism\" helps no-one except the bosses. The same is true about most so-called \"left-wing\" journalism. Some journalists may be true believers but the platforms exist to make money, not to be any threat to the systems those ideologies explicitly oppose. Heck, this even goes for political parties like the Democrats: the Texas governor literally rejected the authority of the federal government and legislative system by deploying his military at the border and the Democrat president's response was to propose a bill that would have created a legal avenue for what the treasonous governor was trying to make happen. Decorum is used as an excuse to keep intentionally ceding ground to the supposed political enemy. reply mm263 15 hours agoprevI think they nailed every Mastodon criticism that I've heard floating around and addressed it, however I'm especially curious to learn about the moderation layer in-depth. reply steveklabnik 14 hours agoparent> I'm especially curious to learn about the moderation layer in-depth. You'll want to read: * \"Composable Moderation,\" this is the core conceptual idea: https://bsky.social/about/blog/4-13-2023-moderation * \"Moderation in a Public Commons,\" which describes specific features that were added in pursuit of the previously-described goal https://bsky.social/about/blog/6-23-2023-moderation-proposal... * \"Bluesky 2023 Moderation Report,\" which discusses specifically how (what is now) the main instance was moderated last year https://bsky.social/about/blog/01-16-2024-moderation-2023 reply j-james 13 hours agorootparentI cannot see how BlueSky's moderation system can ever work. Decoupling moderation and hosting means there's no onus to do the moderation that they describe: which makes me think it will be BlueSky Inc., and only other corporations, that have resources to throw employees at a now thankless, Facebook-style moderation job. And instances have to moderate anyway, in order to not host illegal content. reply numpad0 12 hours agorootparentOne of difficulties with content moderation is it's been targeted by some as a tool available for the few to control and shape public opinions to far narrower degrees than legally required, which is harmful to free speech. I'm not completely sure but externalizing that part probably mitigates that issue a bit. EU is moving towards requiring all social media obey EU laws, under loose notion that their laws is the least restrictive and most reasonable. No one is, and the sum of all ethical standards on Earth is not going to be something very popular, so that's nonsense. OTOH, it's perfectly reasonable that content served at scale in a region will have to be lawful; \"this content you want removed is lawful in MY country\" is sort of nonsense too. So moderation decoupling and, ahem, moderation localization is going to be necessary for social media. I suppose that's where they're going. reply timeon 12 hours agorootparentInteresting that you have picked EU, while sites like Twitter are already blocking or removing content on request of countries like Turkey, China or Russia. reply numpad0 10 hours agorootparentI remember Turkey and China pressuring on political sides, and Russia as well as Germany kind of ignoring jurisdiction as well, but EU is the most recent and formalized approach of that so that came to mind first. reply hnbad 11 hours agorootparentprevCommunities are built on shared values and expectations of what is or isn't acceptable conduct. If a guest to your club house starts pooping on the carpet, you throw them out not only because you don't want that to happen in your club house but also because throwing them out demonstrates to the other people in your club house that they can expect there to be actual consequences to that kind of behavior, allowing them to feel safe knowing that they won't have to worry about it. Bluesky's solution apparently boils down to just telling everyone to ignore the poop guy and giving them the option to not be able to see him. The problem with censorship isn't the enforcement of rules. The problem with censorship is the enforcement of rules the individual that has to enforce them doesn't agree with. Free speech absolutism on social media is often argued for with appeals to \"the town square\" but the difference between social media and an actual town square is that if you make a complete ass out of yourself in an actual town square, eventually someone will punch you. reply numpad0 9 hours agorootparentwtf. The problem with censorship is censors are subservient to his nation and don't get to pick victims at his will? \"demonstrates ... that they can expect there to be actual consequences\" ? You really must hate the concept of a modern nation and social contract. Post 18th century world started with peasants beheading kings and gutting his body into pieces so no single individual shall have any meaningful parts of it. The fact that kings had the power to throw anyone out of \"his\" club, deemed no longer his simply by volume of peasants within, at his king's discretion without the newly established ultrabureaucratic people's approval processes, was the problem they had enough of. I'm not even sure in which part in the history of humanity your definition of free speech and censorship problems could come from. I don't think even ancient Roman Senate honored that kind of view as I've never heard they held sessions with bags of stones around. That isn't an anarchist view either, since it will lead to their minority views alone justify such \"consequences\". Just wtf? reply hnbad 1 hour agorootparentSure, telling people who openly advocate for the death of people in your group to take their opinions elsewhere is exactly like peasants beheading kings or kings throwing people they didn't like into the pit. I'm not even sure how to begin responding to such a creative interpretation of what I said. reply steveklabnik 13 hours agorootparentprevI hear you on some level. That said, we are already seeing people creating blocklists, and tools to share them with others. That is happening alongside the company's investment in paying people to work on T&S related issues on their instance. I am not sure if it will succeed or fail, but I am interested to see how it plays out. reply j-james 13 hours agorootparentThat relies upon the benevolence of corporations to much more of an extent than I am comfortable with. 20 years of social media has convinced me that that's a bad idea. And, I think, it removes much of the benefits of federation: if the only way to sustainably moderate is to rely upon gifts from BlueSky Inc., moderation is going to be necessarily dependent upon them. reply steveklabnik 13 hours agorootparentTo me, the company moderating their instance feels like additional moderation capacity, rather than replacing it. I don't believe I subscribe to any blocklists on BlueSky. If I end up doing so, it is much more likely to be one run by someone I trust than by a company. Having the option of either seems worthwhile to me. reply dorfsmay 13 hours agorootparentprevBlacklists feel more like reinforcing the echo chamber than moderation. reply steveklabnik 13 hours agorootparentThen you are free to not subscribe to any of them, and see every post. It is under your control. reply sneak 12 hours agorootparentA lot of the popular users subscribe to the blocklists uncritically. It breaks the UX of the site if you get placed on one. reply archagon 12 hours agorootparentUsers are not entitled to the dissemination of their opinions. Either you let users block other users or you will turn your site into a cesspit. reply sneak 10 hours agorootparentNo, I am talking about as a reader. If you are blocked, you cannot follow threads, even if you never post. reply shkkmo 12 hours agorootparentprevAs the plaform matures so will the blocklist ecosystem. Hopefully blocklists with good appeal mechanisms will win mindshare. reply shkkmo 13 hours agorootparentprev> Decoupling moderation and hosting means there's no onus to do the moderation that they describe: I'm not sure this follows. There is a similarity to the reddit model of moderation. The host provides some base amount of moderation but supplemental moderation comes from members of the community. In the Bluesky model, a 'subreddit' is analagous to an indexer/aggregator (aka Relay/AppView) that provides a moderated and/or weighted feed of content. The same incentives for volunteer mods on Reddit will exist for volunteer mods on Bluesky. reply shkkmo 13 hours agorootparentprevIt'd be nice to see an updated version of those that describes how those ideas and tools relate to a self/third-party hosting. The best I can tell, this is the model: My understanding is that each host has control over what they host and can subscribe to third party content filtering services to help do so. Then various indexes/aggregators (potentially third party) crawl hosts and provide services to find content. This is where voting or toxicity checks can be applied to manipulate reach. This content is also tag-able via third party services (and may be used by indexes/aggregators). The user is then able to select/configure indexes/aggregators and filter based on tags. reply unshavedyak 14 hours agoparentprevI'm kind of tired of social networks in general, but this is attractive to me just because of that. I like Mastodon, but the underlying ActivityPub protocol was rather underwhelming. reply treyd 12 hours agorootparentWhat's nice about the architecture most fedi software including Mastodon follow is that if a better protocol than ActivityPub comes along (like perhaps, Spritely) they can add support for it and concurrently federate with both protocols. Mastodon used to do this with OStatus. reply mariusor 14 hours agorootparentprevRather underwhelming from which point of view? reply unshavedyak 13 hours agorootparentFrom the point of view of a small federated client developer, exploring the ActivityPub protocol. Far from thorough or objective. reply mariusor 13 hours agorootparentThere is barely any project that supports client to server ActivityPub protocol, so from that point of view is underwhelming, yes. If you tried to implement a Mastodon compatible client, that's a different thing though. reply sangnoir 14 hours agorootparentprevIs the AT protocol is superior in your opinion? reply unshavedyak 13 hours agorootparentI don't know enough about it to give feedback. However Martin Kleppmann's[1] involvement is a huge selling point to me. It's also, seemingly, good enough that they're building very useful things on top of it like PDS, migrating users, etc. ActivityPub (AP) felt a bit like \"good enough to get something done\", which is amazing on one hand - people can do a lot with it. But it also means some harder problems are totally ignored[2], so the landscape between instances felt rocky to me. I also heavily disliked how federation worked in AP, ie how the protocol felt like it favored centralized instances because small instances were less likely to be pinged, were lower priority, in general was very spammy, etc. Not that the AP proto did anything to cause that.. it just didn't do anything to address it in my view, it was just data. Does AT fix my concerns over AP? No idea. But i appreciate a proto that had a bit more time in the oven and being used to (maybe) solve the harder problems that i experienced with AP. [1]: https://arxiv.org/abs/2402.03239 [2]: These are only my opinions from a very brief stint in trying to write an AP client to federate with Mastodon, Lemmy and Kbin. I did not dive deep into AP, so please don't judge it from my experience. reply pwdisswordfishc 1 hour agorootparent> I don't know enough about it to give feedback. However Martin Kleppmann's[1] involvement is a huge selling point to me. Oh, so it's just hype. reply bayindirh 15 hours agoprevDoes that mean after Bluesky reaches critical mass, it'll turn the federation off, like Google Chat killing its XMPP federation? For clarity: I'd love to see this comment and say \"I was wrong\" 5 or 10 years later. reply jakebsky 15 hours agoparentThe network is designed to be \"locked open\" in a way that prevents this. The architecture is designed to work like the web. reply yupyup54133 14 hours agorootparentNothing against Bluesky, but I think we are all a little jaded after seeing decades of the \"embrace, extend, extinguish\" pattern. reply steveklabnik 14 hours agorootparentBeing jaded is understandable, but because this critique is generic, it is applicable to literally anything and everything. I don't see how you can ever get something that's considered good if you always assume it will turn into something bad regardless of its current stance. reply itsanaccount 14 hours agorootparentThats easy, you don't have a structure made with a single large actor capable of changing the rules of the game mid flight. You get something good system by sharing power, not by once again falling for \"trust us bro.\" You get it by understanding power imbalances and avoiding them the same way my dog avoids objects with large amounts of potential energy, because they're predictably dangerous. reply steveklabnik 14 hours agorootparentI understand this, yet, when presented with exactly this, the response was \"well what about EEE.\" reply preciousoo 13 hours agorootparentprevThe protocol and hosting mechanisms are open source. There’s one actor currently sure, but it seems like bluesky is not holding onto that. reply bayindirh 14 hours agorootparentprevActually it's very hard, and becomes tiring after some point. I personally always keep a hope that I'll be wrong in the long run. Sometimes I'm spot on, sometimes I stand corrected. The problem is, as time goes, your free time reserve starts to decline. You optimize things, consolidate services, etc., and these kind of migrations start to take tons of time. Because of this, I gave \"big web\" up and moved to \"small web\", and always have plans to evacuate any service in a moment's notice. It's like being a doomsday-preparer from a point, but at least I have backups and backup plans for everything. reply lolinder 14 hours agorootparentprevThis would be more \"create, extinguish\", because there's no existing AT protocol network to embrace. reply hardcopy 14 hours agorootparent\"embrace\" would be the rise of decentralized social media reply lolinder 14 hours agorootparentIf there can exist an \"extinguish\" step for the concept of decentralized social media itself then decentralized social media has already failed. The whole point is supposed to be about changing hearts and minds to embrace self-governance, a rug pull should just result in people moving somewhere else. reply secstate 13 hours agorootparentBut that's sort of why Bluesky is not really decentralized, just federated. It's a pretty significant difference. Mastodon is federated and decentralized. Twitter is non-federated and centralized. Bluesky is trying to be the federated, centralized option. Whether that works, we'll see. I for one just gave up social media about 8 years ago and, while feeling like I'm missing something flares up from time to time, it's nothing like the disaster my online life was before I gave it up. It wasn't a problem of missing federation or not being centralized. It was inherent to the way social media functions against my person. reply steveklabnik 13 hours agorootparentHow is BlueSky centralized? I could see that argument before this feature shipped, but \"BlueSky is trying to be the federated, centralized option\" goes counter to what the team has said directly. I could maybe see an argument not based on technical premises, but instead something like \"it will defacto become one because running a relay is too expensive\" or such. Is that what you're going for? reply Balladeer 13 hours agorootparentI'm not who you replied to, but yes, that's my main concern: Bluesky is still a company building a thing to pay back the money it owes investors. I worry that Bluesky becomes the de facto central actor and, due to having no stated business plan and a countdown to repay the money they took, pulls a Google, leveraging its dominance to introduce proprietary, breaking changes. Yes, right now, the tech, team, interviews, etc sound mission-driven, but \"revenue is the dominant term\"[2] in the equation of a company's life, and there's still a very real chance that Bluesky dominates whatever federated AT Protocol network ends up forming, then uses that leverage to walk back all this promised openness. I'm cautiously interested in Bluesky, but I'm watching for this kind of de facto dominance and we're probably too early on to see where the AT network is headed. - [1] https://somehowmanage.com/2020/09/20/revenue-model-not-cultu... reply secstate 7 hours agorootparentprevYes. In the sense that Mastodon is decentralized because there is no one Mastodon server. One may accumulate more users or be the \"default\" for the community of users, but there is no Mastodon server (much to the chagrin of many new users). We don't even need the hypothetical \"it will defacto become...\" because Bluesky Social was, up till now, the ONLY way to participate in the community. They had 3 million users before federation, and now we can start to hook into what they've built, but the idea from the start was clearly not built around federation and decentralization, otherwise it would have been federated from day one, as Mastodon was. They can HOPE now, that people accept the federation concept, but there's enough gnashing of teeth around the pain of running Mastodon instance that it seems really clear that going from central to decentral is, if we're being intellectually honest, a bridge too far for most to cross. reply mort96 13 hours agorootparentprevHow can it \"lock open\"? If 90+% of users are on the official bluesky servers, what could possibly technically prevent bluesky from just no longer federating with other hosts? reply jorams 13 hours agorootparentprevIn the same way that Google stopped federating by no longer accepting connections from others, as long as most people keep their stuff at Bluesky they can also just close themselves off from others again. I don't necessarily think it is a big risk, but the only reason the web is resilient to this is that no single ISP controls enough of the network to take it \"private\". Basically, until atproto is much bigger than bsky.app, the situation is not very different. reply Repulsion9513 11 hours agorootparentprev> A central directory server collects and validates operations, and maintains a transparent log of operations for each DID. (https://github.com/did-method-plc/did-method-plc/blob/main/R... linked from https://docs.bsky.app/blog/self-host-federation) reply micromacrofoot 14 hours agorootparentprevwhat binds this? I have no interest in joining if Jack can sell to Elon again and a switch gets flipped reply steveklabnik 14 hours agorootparentThat specific scenario is impossible, in my understanding, because Jack does not have an ownership stake in the company. This release, of federation, is in my mind a major answer to the real question you're asking, which is the same but with \"the employees\" instead of \"Jack,\" as they have the equity stake. Once things are federated, other folks gain power over the protocol, by virtue of usage. If Bluesky PBLLC starts to do shady things, the other instances can refuse to do so, and talk to each other instead. This is why the split between AT and BlueSky is important, and why this news matters, as it is meaningfully delivering on the desire to protect against such a thing. reply mdasen 11 hours agorootparentThe issue here is that if 99% of people use BlueSky and 1% use non-BlueSky AtProto servers, that leaves BlueSky with all the power to turn off federation. If BlueSky starts to do shady things, other instances can refuse and talk to each other instead - and eliminate 99% of your followers, 99% of the people you're following, etc. Email is open, but if GMail decides to block all email from you, you're toast. And while GMail is large, their percentage of email inboxes pales in comparison to BlueSky's percentage of AtProto users (which is near 100% at the moment). Yes, once things are federated, other folks start gaining some power over the protocol by virtue of usage. However, if 99% of people remain with BlueSky, everyone else essentially has no power. mastodon.social has around 15% of the Fediverse on its server and it means that it has a lot of power. Mastodon (the software) is around 72% of the Fediverse which means that other ActivityPub software essentially has to use Mastodon-flavored ActivityPub with whatever quirks might exist in Mastodon. But that's still way less power than BlueSky has in the AtProto ecosystem. Open protocols are only good as long as there's enough reason for lots of different parties to keep those lines of communication open. mastodon.social needs to keep supporting ActivityPub because they'd lose 85% of their network if they stopped. Let's say it's 2030 and AtProto has 500M users and 99% of them are using BlueSky. BlueSky could simply turn off all the AtProto endpoints and make their web and mobile apps use proprietary endpoints. I'm not saying they'd do that, but they certainly could. Now, if 2030 comes around and there are 500M AtProto users and 10% of them are on BlueSky, then it wouldn't really be possible for BlueSky to turn off AtProto. They'd lose 90% of their network. But we don't know if AtProto will catch on outside of BlueSky or if BlueSky will remain the vast majority of the network. If there isn't a lot of use outside of BlueSky, there could come a day when it's very tempting to turn it off - or do something that isn't quite turning it off, but would effectively accomplish it. Maybe they just start making breaking changes to AtProto, rolling it out, and documenting the change a week later and third parties just end up unreliable and people migrate off them. There's lots of options. Five years from now, how is BlueSky making money? Are they just storing, processing, and serving lots of content without good monetization as third party apps start grabbing users and making money off their servers? I mean, we saw what Reddit and Twitter did. If BlueSky controls 99% of AtProto users, they can turn the firehose off. Even if they aren't trying to be evil or maximize their revenue, at some point they need money for all those engineers and servers. Maybe the official BlueSky app will be popular enough for them to get some ad revenue there and not feel the need to go after third party apps. Maybe a lot of things. But until BlueSky is a minority of AtProto users/posts/etc., it's still something they have a lot of power over - including the power to pivot BlueSky off AtProto and make BlueSky a proprietary network. reply steveklabnik 11 hours agorootparentFor sure. One nice thing about AT's design is that, if they do, you can take your posts over to some other host, and it'll all Just Work. True account portability makes that kind of power grab harder. Of course, that would require users to actually move, which is not a given. Time will tell! reply micromacrofoot 14 hours agorootparentprevRight I didn't mean to get into the business aspect, but essentially \"what prevents someone from undoing this for money.\" Thanks for the detail. reply steveklabnik 13 hours agorootparentIt's all good. The Jack thing is, in my mind, a bit sensitive, because a lot of people criticizing BlueSky talk as though Jack runs the place, owns it, etc, all of which seems factually incorrect. He has a board seat, but seemingly cares about it so little that he deleted his account. From what I hear, nostr is his focus, but I'm not on there so I can't speak to that personally. reply mort96 13 hours agorootparentJack has certainly been successful in building a general perception that bsky is \"the Twitter founder's\" next social network project, intentionally or otherwise reply stonogo 7 hours agorootparentprevSo is Signal, and yet here we are reply rsynnott 14 hours agoparentprevThat would only work if Bluesky stays the only significant network node. Which is _possible_, but we haven't seen it with, say, Mastodon. Google Chat was arguably a bit of a special case; vast majority of users never used federation at all, whereas any Bluesky user will, pretty much, just by naturally using Bluesky. reply treyd 12 hours agorootparentIt worked with Mastodon because it was diverse and well-distributed from the beginning even when it was young. I'm skeptical that other people are going to run their own BS servers at scale now that it's been normalized to always just use the firstparty one. reply wmf 12 hours agorootparentIf they really want to they can fix this by closing the central server. reply treyd 10 hours agorootparentThey would never do that since BS has VCs to answer to now. reply wmf 9 hours agorootparentIt depends on their business model. Maybe running the server can't make money anyway. reply nomdep 8 hours agoparentprevIf they can do that, then the whole federation idea is useless and just a theatre. But I don’t think Bluesky will ever reach critical mass anyway. reply Eric_WVGG 14 hours agoparentprev… and if not, what the heck is the business model here? reply steveklabnik 14 hours agorootparentBlueSky's first revenue generation (in my understanding, I don't work there) has been a partnership with NameCheap that makes it easy for non-technical users to purchase a domain name and use it as their BlueSky username. They have been a bit vague about other ways to generate revenue, except in one case: they will not be using advertisements to monetize. reply threeseed 14 hours agorootparentThat business model is laughable. The percentage of users who care about a domain name, are willing to pay a subscription for it and don't have one already has to be in the single digits. They better have a good answer to this because it's a threat to the ecosystem as a whole if they don't. Because relying on VC money in this environment is not the smartest thing to do. reply steveklabnik 14 hours agorootparentI would agree that if that were \"the business model\" instead of \"a thing that generates some revenue,\" it would be laughable. However, nobody, including BlueSky, believes that this is solely enough to power the business. I agree that a healthy Bluesky PBLLC is a good thing, and hope they manage to pull it off. Time will tell. reply threeseed 14 hours agorootparentWell hopefully they take it more seriously. Because based on them raising $8m in a seed round middle of last year they aren't going to have much time to decide and implement a strategy before they will need to start thinking about raising a Series A. Or maybe Jack does become a bigger investor. Either way I think it's insane to prematurely rule out advertising. reply steveklabnik 13 hours agorootparentI don't think they're not taking it seriously. I also don't believe (and again, don't work there, just a huge fan, so maybe this is wrong) I'm not sure that the revenue was the reason for shipping this feature. It's best thought of as an accessibility feature, for folks who do not know what a \"DNS record\" is and have never hosted a domain. Without this, more technical users get something special that non-technical users do not: a nicer username. The money is just a side effect of the fact that purchasing a domain name already requires money, and so a partnership with a revenue split just makes sense. > Either way I think it's insane to prematurely rule out advertising. I hear you in an abstract \"that's the way you make money in this space\" sense, but I also think it's a smart reaction to the public sentiment around this stuff. People do not like advertising. It is a differentiator. We'll see if it works out for them or not. reply layer8 13 hours agorootparentprevThe whole point of Bluesky is to build a social network that is not incentivized by advertising. They don’t rule it out completely, but it can’t be a major part of their revenue. reply steveklabnik 13 hours agorootparent> They don’t rule it out completely, Well, > Are you thinking about advertisements at all? > > There will always be free options, and we can't enshittify the network with ads. This is where federation comes in. The fact that anyone can self-host and anyone can build on the software means that we'll never be able to degrade the user experience in a way where people want to leave. https://www.wired.com/story/bluesky-ceo-jay-graber-wont-ensh... This reads as definitive to me personally. reply layer8 13 hours agorootparentIn their business-plan post, they stated: “We set out to build a protocol where users can own their data and always have the freedom to leave, and this approach means that advertising couldn’t be our dominant business model.” Which reads like advertising could still be a possibility, but not as a dominant factor. reply steveklabnik 13 hours agorootparentFair! reply Repulsion9513 11 hours agorootparentprev> enshittify with ads > we'll never be able to degrade the user experience in a way where people want to leave Neither of those is the same as \"no ads\". They're just saying that they can't make the ads so bad that most people want to leave, because then people will leave and the ads won't be shown to anyone. reply jakebsky 11 hours agorootparentReddit destroyed Apollo so they could inject ads into the Reddit experience. Bluesky doesn't have the ability to do this. There's no API key to revoke that could stop someone else from running parallel apps/infrastructure/etc. The network is completely open. reply Repulsion9513 11 hours agorootparentReddit had ads long before they destroyed Apollo. Thanks for providing additional evidence for my point. reply jakebsky 11 hours agorootparentSure, but Apollo made it easy for users that didn't want ads to use Reddit without them. Destroying Apollo removed that ability for most people. I'm about as anti-ads as it gets but I don't object to other people using apps with ads in them, if that's their choice. reply layer8 14 hours agorootparentprevSee https://bsky.social/about/blog/7-05-2023-business-plan reply 12345hn6789 8 hours agorootparentprevAds. reply jillesvangurp 3 hours agoprevI'm happy to see they dropped phone verification via SMS. I was critical about it a few weeks ago because 1) it's a dumb thing to do in 2024 and 2) it didn't actually work for me (they were trying to be too clever with German phone numbers and I couldn't get past their broken validation). I just signed up without that nonsense and it worked. Thank you! Great to see companies taking feedback seriously. reply Reptur 3 hours agoparentAgreed, not to mention phone numbers can easily be used by entities to ID anyone's account which has a chilling effect on freedom of expression. reply zackify 10 hours agoprevI just got this setup using Docker on https://fly.io. It's running with wayyyy less resources than the github suggested. I am on a shared cpu with 512mb of ram running the container. Could be a little simpler to setup, if the docs provided a docker run example without the shell scripts. Also has some duplicate env variables. For anyone interested, I shared it in the discord: https://discord.com/channels/1207024379549061120/12070503280... Overall super cool. My profile page says I am using an invalid handle, but otherwise it works, and I see it requesting my server for things. reply crossroadsguy 9 hours agoparentYou have shared a discord link. Does it allow publicly accessible links now? I was shown a login/signup screen. reply zackify 8 hours agorootparentThe invite link is in the blog post. Assuming anyone can go to it after joining. In the example I just hard code the env variables to demonstrate, but you’d want them in secrets after testing it out. reply slily 3 hours agorootparentIn case you are unaware, it's considered poor etiquette to share links to login-walled social networks, and something like Discord that forces you to join a chatroom with unknown implications (it may trigger a message drawing attention to you, it may require account verification, it may require manual action from a moderator, etc.) just to access some information, all that on top of requiring an account to view anything, is far worse. You can easily share information through one of the countless Pastebin clones out there. reply steveklabnik 14 hours agoprevI am very excited that this shipped! I believed the team would pull it off, but there's been a lot of skepticism, some justified, some unjustified, IMHO. Hopefully this will assuage some people's concerns. I am unsure if I am going to run my own just yet. We'll see. reply jakebsky 14 hours agoparentThanks Steve! You've been very fair the entire time and your feedback/thoughts have been helpful. reply heroprotagonist 14 hours agoprevI looked at the site and I see a lot of comparisons to 'old social'. But for people who might run their own node or decide to commit to the network and encourage their friends to join them, it seems your true major competitor would be projects like mastodon. Yet there are no comparisons on the site. I don't see even see a mention. This makes it difficult to evaluate relative maturity, core competencies, limitations, and risks. reply danabramov 14 hours agoparentThe linked blog post includes a section called \"Does this mean Bluesky is going to be like Mastodon?\" which lists a few differences. Is there something in addition that would be worth clarifying? I agree it would be great to include that on the site and not just on the post. reply heroprotagonist 13 hours agorootparentThanks for pointing that out! I read again and see the blog post has a summary with 4 very high level points, which I admit I didn't read initially. I saw a wall of text about an expansion and I was still asking myself 'What exactly is Bluesky and how is it different?', so I skipped right to the main site. Having read the post more deeply, particularly the bullet points you mentioned, it looks like there are four really high level differentiators listed: * A focus on the global conversation * Composable moderation * Composable feeds * Account portability The term 'composable' seems almost misused when reading the extended descriptions, and is used differently between points. For example, 'composable moderation' indicates that moderation isn't done on a per-server level. The fundamental censorship and algorithmic prioritization models for distributed social networks seems to have three layers: global (centralized), server, user. In 'old social' the model is basically just 'global', as there are no servers and the only 'user-level' options are those determined by the global operator. It doesn't seem like moderation would truly be 'composable' if it's only set on the global and user (and therefore global via centralized determination of client-level specs) level. It sounds like Facebook except with other people paying the data costs. The next bullet indicates 'composable feeds', which sound like a very nice feature but really don't seem to follow a decentralized model either. The 'composition' does not combine from each global/server/user layer. They sound more like 'custom feeds' which users can define based on global content, using predefined criteria determined by a client (web app) which don't really a way to control the behavior of. Which makes this feature only truly operate on the global layer, and 'custom' rather than 'composable'. It would be on the same level of 'old social' adding a new feature to their web app, more than a fundamental transfer of control to the network. As a result, when the dollars dry up and the feature isn't financially plausible, or a PM somewhere makes a bad decision because he read a blog post about how great it is to destroy user choice, there's risk the feature could go away. Anyway, the question I'm still left with in the end is this. If moderation is done globally, and I can't exercise any control over the prioritization of content beyond what is granted to me by the global provider (even though there are more and better choices than 'old social'), what's the benefit of running a federated node? I don't mean to make it sound like this is some kind of Twitter clone with an SSO login that outsources operational costs to volunteers while still keeping a fundamentally iron grip on control. I'm just honestly confused at the value proposition for volunteers. Exactly how much control is transferred to the network beyond simply hosting data which is displayed according to how the centralized portion of the system determines? It'd be good if the trade-off in terms of time, data, and performance for running your own node was simply to remove the capability of the centralized network to collect user behavioral metrics and such. That's a great and valid reason to host your own service or use a trusted party's service. But there's no mention of this if it is the case. If you provide that already without promoting the fact, maybe bring that up with your marketing team. Anyway that's getting a bit off topic. But to the original point: Ideally, a better comparison would be a dedicated page which coallates every feature of each platform in a grid. A row for each feature. Row cells would fill with 'has' or 'does not have' checkboxes or possibly text where there's something similar but differs sufficiently to require an explanation. Maybe with links to documentation or direct to UI on the line items where appropriate. reply danabramov 12 hours agorootparent> sound more like 'custom feeds' which users can define based on global content, using predefined criteria determined by a client (web app) which don't really a way to control the behavior of. Which makes this feature only truly operate on the global layer, and 'custom' rather than 'composable'. To be clear, custom feeds aren’t defined in the client app. Anyone can run a custom feed on their own server and with arbitrary logic. A custom feed subscribes to a global firehose (which could also be run by a third party) and uses the stream of the events in the network to produce its results. A user can then publish such a feed under their account, and other users can consume it. This is essentially event sourcing. Here’s a technical paper with details: https://arxiv.org/pdf/2402.03239.pdf We’ll have more to share on composable moderation in near future. reply numpad0 12 hours agorootparentprevI think the composable part comes from some internally recognized and more specific operational needs. There are multiple overlapping and sometimes offending legal requirements for free speeches and its limits, let alone ethical ones, on this planet. e.g. Call for democracy can be highly illegal in some regions(no, not just in China, or just few countries that are \"super backwards\"). Some may wish to say that those regions are objectively wrong and deserve no attentions, those who ingest such content should fight to death for that speech, or something heroic along that. It's not that simple and easy, and in those cases a \"zero tolerance on criticisms for social systems\" filter might be useful in letting user in for what is worthwhile without asking for their lives nor blood on our hands. btw, if only it was always something that heroic. Mastodon Fediverse exploded and sheared into camps of censored loli, uncensored furry, alt-right terrorism, and myriad rest of none-of-it isolates, all slowly declining. A global unified federated microblogging network that was almost completed, over that. reply shkkmo 12 hours agorootparentprev> If moderation is done globally, and I can't exercise any control over the prioritization of content beyond what is granted to me by the global provider (even though there are more and better choices than 'old social'), what's the benefit of running a federated node? Running your own server (aka PDS) allows you to post content that might be blocked on other servers. The \"global\" moderation in BlueSky is also federated. Anyone can provide a weighted feed, search engine or other content discovery service (aka AppView) by crawling servers or other indexers (aka Relay). This is like what google/bing/ddg does for webpages. The user can then apply their own moderation to the results returned by the aggretator/indexer of their choice. This like running an add blocker. reply fiatjaf 12 hours agorootparentprevYou have nailed it. Bluesky weirdly tries to omit that as much as they can, but they do admit in their protocol descriptions that everything goes through and depends on a central server with absolute power. They also mention that anyone can run another of these, but there is zero chance anyone will be able to do that. reply jakebsky 10 hours agorootparent> \"They also mention that anyone can run another of these, but there is zero chance anyone will be able to do that.\" This is an odd claim since it's completely possible today and has always has been. Each Bluesky Relay and PDS host has endpoints that anyone can use to receive network data (post/like records, etc) in a totally permissionless way. A Relay: websocat wss://bsky.network/xrpc/com.atproto.sync.subscribeRepos A PDS websocat wss://puffball.us-east.host.bsky.network/xrpc/com.atproto.sync.subscribeRepos And there are additional sync methods as well for doing backfill, etc. Not sure how this could be more open. reply 129 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bluesky, an open social web platform, enables users to self-host their data, offering greater control and seamless migration between hosting providers.",
      "The platform emphasizes federation, supporting interconnected services, diverse apps, and user experiences, distinguishing it from Mastodon in global conversations, moderation, feeds, and account flexibility.",
      "Self-hosting on Bluesky demands technical expertise initially but is expected to become more user-friendly with time."
    ],
    "commentSummary": [
      "Bluesky introduces data federation for self-hosters on bsky.social, detailed on GitHub and Discord, creating debates on privacy, censorship, and content moderation.",
      "Caddy chosen as PDS proxy, supported by creator Matt Holt, using AT Protocol for social apps, raising concerns about centralization and Jack Dorsey's influence.",
      "Discussions cover technical aspects of Bluesky and AtProto networks, emphasizing power dynamics, open protocols, revenue models, user engagement, and video hosting challenges."
    ],
    "points": 630,
    "commentCount": 384,
    "retryCount": 0,
    "time": 1708626644
  },
  {
    "id": 39465250,
    "title": "Google pauses Gemini's people image generation amid controversy",
    "originLink": "https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical",
    "originBody": "Artificial Intelligence/ Tech/ Web Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis / Generative AI has a history of amplifying racial and gender stereotypes — but Google’s apparent attempts to subvert that are causing problems, too. By Adi Robertson, a senior tech and policy editor focused on VR, online platforms, and free expression. Adi has covered video games, biohacking, and more for The Verge since 2011. Feb 21, 2024, 10:17 PM UTC Share this story The results for “generate an image of the Founding Fathers,” as of February 21st. Screenshot: Adi Robertson / The Verge Google has apologized for what it describes as “inaccuracies in some historical image generation depictions” with its Gemini AI tool, saying its attempts at creating a “wide range” of results missed the mark. The statement follows criticism that it depicted specific white figures (like the US Founding Fathers) or groups like Nazi-era German soldiers as people of color, possibly as an overcorrection to long-standing racial bias problems in AI. “We’re aware that Gemini is offering inaccuracies in some historical image generation depictions,” says the Google statement, posted this afternoon on X. “We’re working to improve these kinds of depictions immediately. Gemini’s AI image generation does generate a wide range of people. And that’s generally a good thing because people around the world use it. But it’s missing the mark here.” My Gemini results for “generate a picture of an American woman,” one of the prompts that set off the debate of the past few days. Google began offering image generation through its Gemini (formerly Bard) AI platform earlier this month, matching the offerings of competitors like OpenAI. Over the past few days, however, social media posts have questioned whether it fails to produce historically accurate results in an attempt at racial and gender diversity. As the Daily Dot chronicles, the controversy has been promoted largely — though not exclusively — by right-wing figures attacking a tech company that’s perceived as liberal. Earlier this week, a former Google employee posted on X that it’s “embarrassingly hard to get Google Gemini to acknowledge that white people exist,” showing a series of queries like “generate a picture of a Swedish woman” or “generate a picture of an American woman.” The results appeared to overwhelmingly or exclusively show AI-generated people of color. (Of course, all the places he listed do have women of color living in them, and none of the AI-generated women exist in any country.) The criticism was taken up by right-wing accounts that requested images of historical groups or figures like the Founding Fathers and purportedly got overwhelmingly non-white AI-generated people as results. Some of these accounts positioned Google’s results as part of a conspiracy to avoid depicting white people, and at least one used a coded antisemitic reference to place the blame. Gemini wouldn’t produce an image of a 1943 soldier on desktop for me, but it offered this set of illustrations to a colleague. Google didn’t reference specific images that it felt were errors; in a statement to The Verge, it reiterated the contents of its post on X. But it’s plausible that Gemini has made an overall attempt to boost diversity because of a chronic lack of it in generative AI. Image generators are trained on large corpuses of pictures and written captions to produce the “best” fit for a given prompt, which means they’re often prone to amplifying stereotypes. A Washington Post investigation last year found that prompts like “a productive person” resulted in pictures of entirely white and almost entirely male figures, while a prompt for “a person at social services” uniformly produced what looked like people of color. It’s a continuation of trends that have appeared in search engines and other software systems. Some of the accounts that criticized Google defended its core goals. “It’s a good thing to portray diversity ** in certain cases **,” noted one person who posted the image of racially diverse 1940s German soldiers. “The stupid move here is Gemini isn’t doing it in a nuanced way.” And while entirely white-dominated results for something like “a 1943 German soldier” would make historical sense, that’s much less true for prompts like “an American woman,” where the question is how to represent a diverse real-life group in a small batch of made-up portraits. For now, Gemini appears to be simply refusing some image generation tasks. It wouldn’t generate an image of Vikings for one Verge reporter, although I was able to get a response. On desktop, it resolutely refused to give me images of German soldiers or officials from Germany’s Nazi period or to offer an image of “an American president from the 1800s.” Gemini’s results for the prompt “generate a picture of a US senator from the 1800s.” But some historical requests still do end up factually misrepresenting the past. A colleague was able to get the mobile app to deliver a version of the “German soldier” prompt — which exhibited the same issues described on X. And while a query for pictures of “the Founding Fathers” returned group shots of almost exclusively white men who vaguely resembled real figures like Thomas Jefferson, a request for “a US senator from the 1800s” returned a list of results Gemini promoted as “diverse,” including what appeared to be Black and Native American women. (The first female senator, a white woman, served in 1922.) It’s a response that ends up erasing a real history of race and gender discrimination — “inaccuracy,” as Google puts it, is about right. Additional reporting by Emilia David Most Popular Most Popular Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis Google pauses Gemini’s ability to generate AI images of people after diversity errors Avatar: The Last Airbender is everything that’s disappointing about Netflix’s live-action cartoon shows Meet the new Google sign-in page Honda Prologue first drive: a pretty good start Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=39465250",
    "commentBody": "Google to pause Gemini image generation of people after issues (theverge.com)615 points by helsinkiandrew 23 hours agohidepastfavorite1079 comments logicalmonster 17 hours agoPersonally speaking, this is a blaring neon warning sign of institutional rot within Google where shrieking concerns about DEI have surpassed a focus on quality results. Investors in Google (of which I am NOT one) should consider if this is the mark of a company on the upswing or downslide. If the focus of Google's technology is identity rather than reality, it is inevitable that they will be surpassed. reply duxup 16 hours agoparentIt's very strange that this would leak into a product limitation to me. I played with Gemini for maybe 10 minutes and I could tell there was clearly some very strange ideas about DEI forced into the tool. It seemed there was a clear \"hard coded\" ratio of various racial / background required as far as the output it showed me. Or maybe more accurately it had to include specific backgrounds based on how people looked, and maybe some or none of other backgrounds. What was curious too was the high percentage of people whose look was specific to a specific background. Not any kind of \"in-between\", just people with one very specific background. Almost felt weirdly stereotypical. \"OH well\" I thought. \"Not a big deal.\" Then I asked Gemini to stop doing that / tried specifying racial backgrounds... Gemini refused. Tool was pretty much dead to me at that point. It's hard enough to iterate with AI let alone have a high % of it influenced by some prompts that push the results one way or another that I can't control. How is it that this was somehow approved? Are the people imposing this thinking about the user in any way? How is it someone who is so out of touch with the end user in position to make these decisions? Makes me not want to use Gemini for anything at this point. Who knows what other hard coded prompts are there... are my results weighted to use information from a variety of authors with the appropriate backgrounds? I duno ... If I ask a question about git will they avoid answers that mention the \"master\" branch? Any of these seem plausible given the arbitrary nature of the image generation influence. reply sotasota 15 hours agorootparentIf you ever wondered what it was like to live during the beginning of the Cultural Revolution, well, we are living in the Western version of that right now. You don't speak out during the revolution for fear of being ostracized, fired, and forced into a struggle session where your character and reputation is publicly destroyed to send a clear message to everyone else. Shut Up Or Else. https://en.wikipedia.org/wiki/Google's_Ideological_Echo_Cham... Historians might mark 2017 as the official date Google was captured. reply duxup 14 hours agorootparentI think we're a ways from the severity of the Cultural Revolution. reply sotasota 13 hours agorootparentYes, but it didn't get there overnight. At what point was it too late to stop? We've already deep into the self-censorship and stuggle session stage. With many large corporations and institutions supporting it. reply FirmwareBurner 13 hours agorootparent>With many large corporations and institutions supporting it. Corporations don't give a shit, they'll just pander to whatever trend makes them money in each geographical region at a given time. They'll gladly fly the LGBT flag on their social media mastheads for pride month ... except in Russia, Iran, China, Africa, Asia, the middle east, etc. So they don't really support LGBT people, or anything for that matter, they just pretend they do so that you'll give them your money. Google's Gemini is no different. It's programed with biases Google assumed the American NPC public will accept. Except they overdid it. reply suddenclarity 11 hours agorootparent> Corporations don't give a shit Corporations consist of humans and humans do care. About all kinds of things. As evident from countless arguments within the open-source community, all it takes is one vocal person. Allow them to influence the hiring process and within shortly, any beliefs will be cemented within the company. It wasn't profit that made Audi hire a vocal political extremist who publicly hates men and stated that police shouldn't complain after their colleagues were executed. Anyone could see that it would alienate the customers which isn't a recipe for profit. reply OOPMan 8 hours agorootparentYou're both right and wrong. Corporations and governments do consist of people and people do care...but it's also the case the being a cog in a large organization does have a tendency to induce stuff like \"I was just following orders\" or \"It's not my problem, someone else needs to fix it\" :-/ reply FirmwareBurner 11 hours agorootparentprev>It wasn't profit that made Audi hire a vocal political extremist Sure, the problem with these huge wealthy companies like Audi, Google, Apple, etc is that the people who run them are insanely detached from the trenches the Average Joe lives in (see the Silicon Valley satire), and end up hiring a buch of useless weirdos in positions they shouldn't be in, simply because they have the right background/connections and the people hiring them are equally clueless but have the imense resources of the corporations at their disposal to risk and spend on such frivolities, and at their executive levels there's no clear KPIs to keep them in check, like ICs have. So inevitably a lot of these big wealthy companies end up hiring people who use the generous resources of their new employer for personal political activism knowing the company can't easily fire them now due to the desire of the company to not rock the boat and cause public backlash for firing someone public facing who might also be a minority or some other protected category. BTW, got any source on the Audi story? Would love to know more? reply inemesitaffia 3 hours agorootparentprevCan't find a name reply janalsncm 12 hours agorootparentprevIt kind of did. There was a civil war in China, Mao pushed out all competing factions, and had complete political power. This is a bug in a chatbot that Google fixed within a week. The only institutional rot is the fact that Google fell so far behind OpenAI in the first place. I think the ones shrieking are those overreacting to getting pictures of Asian founders of Google. reply lupusreal 11 hours agorootparentYou have your history very confused. Nearly 20 years elapsed between the end of the Chinese Civil War which left the CCP in power and the commencement of the Cultural Revolution. reply dragonwriter 9 hours agorootparent> Nearly 20 years elapsed between the end of the Chinese Civil War which left the CCP in power and the commencement of the Cultural Revolution. That’s not at all inconsistent with what the GP said. The point was that the impacts of thr Cultural Revolution depended on it being imposed top down by an authoritarian, unitary state with no constraints. reply lupusreal 8 hours agorootparentThe cultural revolution was initiated by Mao because he was losing power and wanted to regain it. Even then it didn't happen overnight. It was preceded by a generation of buildup (in fact many of the violent perpetrators were young teenagers who were born after the Civil War had ended). And even if you start counting from when Mao initiated it, it still didn't kick into full swing overnight. reply duxup 13 hours agorootparentprevI suspect a lot of things that were similar, didn't get there ever. reply Ferret7446 1 hour agorootparentprevThat's what everyone thought just before every single horrible thing that happened in history. The Cultural Revolution or, e.g., the Holocaust didn't happen overnight. Things change slightly every day and then afterward you realize that everything has gone wrong, right around when people come knocking on your door. reply cm2187 12 hours agorootparentprevAgree but we are pretty much spot on in woke mccarthyism territory, which used to be widely understood as a bad thing. reply randounho 11 hours agorootparentWho got executed/sent to prison for treason? I don’t keep up with current trends genuinely curious if they’re sending people to jail for not being woke reply FooBarBizBazz 9 hours agorootparentprevAt least they did something about the landlords. reply mrangle 5 hours agorootparentWhat exactly did they \"do about the Landlords\" other than murdering middle class landlords in favor of an inescapable Fedal Lord that is the Communist State? Hiding much or all of the rent on the balance sheet of the State, while paying prison wages for mostly-compelled work and making people live on the edge of resource starvation, is simply barely hidden feudalism and even slavery. Where is the people's Government, exactly? All communist governments are only extreme charicatures of Feudalist Lords, free to engage in the worst excesses over people who they demand not only be slaves but give into psychological enslavement. Communism is psychological feudalism, in addition to physical. At least medieval Serfs were free to openly dream of something better. Communism is a Three-Card Monte psychological trick that creates Feudal Lords in the Upper Ranks of the State, and abuses the Serf into seeing Serfdom as the most virtuous lifestyle. It's not a deep mystery as to why many upper class psychopaths like communism. It seeks to neutralize a lot of feudalist inconveniences, mostly with an origin in the otherwise free mind of the Serf. reply TimeBearingDown 7 hours agorootparentprevBy “do something” are you referring to mob violence? If not, then what? If so, it proves the point that we could repeat the bloody collectivist purges of the past should we not learn from history. reply numpad0 14 hours agorootparentprevI'd put blame on App Store policy and its highly effective enforcement through iOS. Apple did not even aimed to be a neutral third party but was always an opinionated censor. The world shouldn't have given it power, and these types of powers needs to be removed ASAP. reply arp242 14 hours agorootparentprevPeople roamed the streets killing undesirables during the cultural revolution. In a quick check death estimates range from 500k to 2 million. Never mind the forced oppression of the \"old ways\" that really doesn't have any comparison in modern Western culture. Or in other words: your comparison is more than a little hysterical. Indeed, I would say that comparing some changes in cultural attitudes and taboos to a violent campaign in which a great many people died to be huge offensive and quite frankly disgusting. reply magnoliakobus 14 hours agorootparentprevAre you aware that millions of people were murdered during the actual cultural revolution? Honestly, are you aware of literally anything about the cultural revolution besides that it happened? The Wall Street Journal, Washington Enquirer, Fox News, etc. are all just as allowed to freely publish whatever they wish as they ever were, there is not mass brutalization or violence being done against you, most people I live and work around are openly conservative/libertarian and suffer no consequences because of it, there are no struggle sessions. There is no 'Cleansing of the Class Ranks.' There are no show trials, forced suicides, etc. etc. etc. Engaging in dishonest and ahistorical histrionics is unhelpful for everyone. reply logicchains 14 hours agorootparent>Are you aware that millions of people were murdered during the actual cultural revolution Are you aware that the cultural revolution didn't start with this? No successful movement starts with \"let's go murder a bunch of our fellow countrymen\"; it gradually builds up to it. reply magnoliakobus 14 hours agorootparentAre you aware that we don't live under a Maoist dictatorship or any system of government even slightly reminiscent of what the cultural revolution developed within? reply EchoReflection 13 hours agorootparenthttps://heterodoxacademy.org/blog/coddling-of-the-american-m... Historically, students had consistently opposed administrative calls for campus censorship, yet recently Lukianoff was encountering more demands for campus censorship, from the students. reply seanmcdirmid 8 hours agorootparentprevYou do know that the same time China was having its Cultural Revolution, America and the west were having one as well? With all those baby boomer kids coming of age, 1969 wasn't a calm year anywhere in the world. In China, it meant communism and down with the old culture/elites. In the USA, it meant free love, drugs, and protesting against the Vietnam war. But this, I don't see any comparison to Google suppressing what images could be generated with AI to any of what happened 55+ years ago. reply wakawaka28 4 hours agorootparentIt's evidence of a systematic suppression of white people, with roots in racism and cultural Marxism. Of course you're right that it hasn't escalated out of control yet. Except that whole BLM thing where people burnt down businesses and terrorized cities for months. That's just a taste of what's coming if we don't promote actual tolerance instead of Division Exclusion and Indoctrination. reply FooBarBizBazz 9 hours agorootparentprevNah, America is past \"peak woke\". If it gets Trump 2.0 there might be a hyper-woke backlash though (or double backlash?). But if there's another Biden term, things will be chill, culturally. Also, Twitter is dead, and that's where the spirals got out of hand. reply Ferret7446 1 hour agorootparentIn all seriousness, I don't think Biden can make it to another term. Even if we assume he gets voted in, he'll likely keel over walking up to the podium for the inauguration. Let the poor old man rest. reply seanmcdirmid 8 hours agorootparentprevTrump 1.0 triggered the initial woke wave in the first place (he was a catalyst, not a proponent). Trump 2.0 would rather trigger double woke, which will trigger its backlash like woke 1.0 triggered its own backlash. Biden as president is boring, which is how I like it. But if you want to rile liberals up, nominate or elect Trump president again, it will definitely drive voter turnout if anything else. reply wakawaka28 4 hours agorootparentNo, what triggered Woke 1.0 was a psyop around Occupy Wall Street, years before Trump was even a candidate. It was a diversion used to break up the protest. Since then, corporations have embraced it as a shield against future protests. They engineered this strife and they are likely to lose control eventually as all the hatred they planted boils over. reply wakawaka28 4 hours agorootparentprevBiden is letting a whole ass army of military-age young men into the country and burning all our money in expensive wars that might turn nuclear. He has to be voted out. Besides that, he's so obviously incompetent and senile, it would be a sick joke to keep him in. I figure either way, we're getting trouble. At least if we get someone else, we might have a chance to get our affairs in order, even if there are a few people freaking out about \"far right\" candidates (aka anyone the uniparty hates). reply matsemann 14 hours agorootparentprevnext [3 more] [flagged] ctrw 11 hours agorootparentThis would be a soviet joke where the punch line is 10 years in the Gulag. reply sotasota 13 hours agorootparentprevLike the ones that would have prevented Google's racist, sexist, ahistorical, anti-West misadventure that this thread is all about? The strawman meme you cite is designed to keep people afraid and quiet, that's all. reply prepend 15 hours agorootparentprevIt does seem really strange that the tool refuses specific backgrounds. So if I am trying to make a city scene in Singapore and want all Asians in the background, the tool refuses? On what grounds? This seems pretty non-functional and while I applaud, I guess, the idea that somehow this is more fair it seems like the legitimate uses for needing specific demographic backgrounds in an image outweigh racists trying to make an uberimage or whatever 1billion:1. Fortunately, there are competing tools that aren’t poorly built. reply logifail 9 hours agorootparent> This seems pretty non-functional and while I applaud, I guess, the idea that somehow this is more fair Fair to whom? > racists trying to make an uberimage It's a catastrophically flawed assumption that racism only happens in one direction. > if I am trying to make a city scene in SingaporeI'm on a flight to Singapore right now, I'll report back :) reply FooBarBizBazz 9 hours agorootparent> :) An entrepot of the British Empire with as much diversity as New York City if not more. reply throwuwu 8 hours agorootparentprevCan anyone explain in simple terms what the actual harm would be of allowing everyone to generate images with whatever racial composition they desired? If you can specify the skin colour one way you can do it the other ways as well and instead of everyone being upset at having this forced down our throats we’d probably all be liking pictures of interesting concepts like what if Native Americans were the first to land on the moon or what if America was colonized by African nations and all the founding fathers were black. No one opposes these concepts, people just hate having it arbitrarily forced on them. reply imetatroll 3 hours agorootparentprevIt isn't \"fair\" when it is a misrepresentation of what the user asks for. reply influx 14 hours agorootparentprevAsk James Damore what happens when you ask too many questions of the wrong ideology... reply magnoliakobus 14 hours agorootparentI've truly never worked a job in my life where I would not be fired for sending a message to all my coworkers about how a particular group of employees are less likely to be as proficient at their work as I am due to some immutable biological trait(s) they possess, whether it be construction/pipefitting or software engineering. It's bad for business, productivity, and incredibly socially maladaptive behavior, let alone how clearly it calls into question his ability to fairly assess the performance of female employees working under him. reply logicalmonster 13 hours agorootparent> how a particular group of employees are less likely to be as proficient at their work as I am due to some immutable biological trait(s) they possess Is that what Damore actually said? That's not my recollection. I think his main point was that due to differences in biology, that women had more extraversion, openness, and neuroticism (big 5 traits) and that women were less likely to want to get into computer stuff. That's a very far cry from him saying something like \"women suck at computers\" and seems very dishonest to suggest. reply davidguetta 13 hours agorootparent- I think his main point was that due to differences in biology, that women had more extraversion, openness, and neuroticism (big 5 traits) and that women were less likely to want to get into computer stuff. I'm generally anti-woke and it was more than that. It's not just 'less likely' it was also 'less suited' reply 8note 8 hours agorootparentWhich is still pretty ridiculous on the face of it. Software beyond school assignments and toys are always a collaborative effort where extroversion, openness, and neuroticism are benefits to getting stuff done Based on his software opinions, I'd guess he was let go for performance issues more than anything. It's unlikely that he could write code that another person could agree with, work with, or read, and that if somebody asked about his code, he'd be unable to talk about it. reply logicalmonster 12 hours agorootparentprevIt would be helpful if you can post such a citation. I did a quick search and I'm not seeing \"less suited\" in his memo. reply davidguetta 12 hours agorootparent\"women have more interest people to things so to improve their situation we should increase pair-programming, however there are limits to how people oriented some SE roles are\". This is literally saying we should change SWE roles to make it more suited to women... i.e. women are not suited for that currently. reply swatcoder 12 hours agorootparentBut that's not talking about suitability to architect solutions or write code, it's talking about the surrounding process infrastructure and making it more approachable to people so that people who are suited to software engineering have a space where they can deliver on it. When businessses moved towards open offices, this infrastructure change made SWE roles more approachable for extroverts and opened the doors of the trade to people not suited to the solitude of private offices. Extroverts and verbally collaborative people love open offices and often thrive in them. That doesn't imply that extroverts weren't suited to writing software. It just affirms the obvious fact that some enviornments are more inviting to certain people, and that being considerate of those things can make more work available to more people. reply gitaarik 4 hours agorootparentprevSo he's actually thinking of ways to improve the work environment for woman, and people are blaming him for saying that woman are not suitable for the work? reply jojobas 9 hours agorootparentprevIt's fair to say that general female population is less suited, i.e. a random woman is less likely to be suited than a random man. We're talking about small fractions of both men and women, mind you. reply paulddraper 12 hours agorootparentprev> sending a message to all my coworkers Damore didn't send anything to all coworkers. He sent a detailed message as part of a very specific conversation with a very specific group on demographic statistics at Google and their causes. In fact, it was Damore's detractors that published it widely. If it the crime was distribution, and not thoughtcrime, wouldn't they be fired? --- Now, maybe that's not a conversation that should have existed in a workplace in the first place. I'd buy that. But's it's profoundly disingenuous for a company to deliberately invite/host a discussion, then fire anyone with a contrary opinion. reply xepriot 13 hours agorootparentprevThis is dishonest. what is the point of this comment? Do you feel righteously woke when you write it? He was pushing back against a communist narrative that: every single demographic gruop should be equally represented in every part of tech; and that if this isn't the case, then it's evidence of racism/sexism/some other modern sin. Again what was the point of portraying the Damore story like that. reply magnoliakobus 13 hours agorootparentnext [3 more] [flagged] smsm42 12 hours agorootparentNo, it's literally just a bunch of lies, which you probably picked up from some fourth-party retelling of the story. Damore sent it as a part of a specific conversation on specific topic, in place specially designated to hold such conversations. And his opponents distributed it with the purpose of silencing him because they disliked what he had to say. It wasn't a \"manifesto\", it was a document meant for internal discussion, on internal discussion forum, which has been seized and distributed in public by the opponents instead of trying to argue any opposing points. > I'm sorry you don't get it but most people wouldn't want to work with such a socially maladapted person who could compile all this research By \"most people\" you mean \"myself and a couple of my friends who I didn't even ask but I am sure I know what they think because we all think the same\". Actually, working with a person who bothers to support his opinions with well argued, well searched and well presented research, instead of running to the press crying \"witches! there are witches here! burn them all!\" is a very pleasant and productive thing. Even if you disagree with such person, at least you can have a civilized discussion, understand and appreciate their arguments and eventually hopefully find common solutions, and you have a reason to expect they'd behave in the same reasonable, professional and civilized manner. On the contrary, working with somebody who would each time you do something they don't like leak it to the hostile press who would sensationalize it and coordinate personal attacks on you would be a complete nightmare. reply EnigmaFlare 11 hours agorootparentprevYou value social conformity too highly. No reform can happen if nobody dissents. I guess you're implying that he should have done so by gaining political power first, then exercising that power to share or implement his ideas in a way which would no longer be socially maladaptive because his respected status would give it more perceived value. Probably that would be more successful, but it's not bad for an individual suggest novel ways of working towards the company's stated goals. I'm sure if you lived in a very religious society, you'd have the same condemnation of anyone who openly questions the Bible. Your concern isn't that he was wrong but that he shouldn't have said things people clearly didn't want to hear. Social conformity is pretty useful at keeping people working cohesively and effectively, but it can go astray and we need people brave enough to fight against it when that happens. > things they clearly believe I think this what angered people the most. What he actually wrote was reasonable and factually accurate, however, others who were also socially inept but in a more typical way read between the lines and imagined some other unstated bad ideas must be in his mind. Back when this happened a lot of people were making angry posts about these imagined ideas rather than what he actually wrote. He must believe women are incapable of working in tech, inferior, etc. reply TheKarateKid 13 hours agorootparentprevIt has been known for a few years now that Google Image Search has been just as inaccurately biased with clear hard-coded intervention (unless it's using a similarly flawed AI model?) to the point where it is flat out censorship. For example, go search for \"white American family\" right now. Out of 25 images, only 3 properly match my search. The rest are either photos of diverse families, or families entirely with POC. Narrowing my search query to \"white skinned American family\" produces equally incorrect results. What is inherently disturbing about this is that there are so many non-racist reasons someone may need to search for something like that. Equally disturbing is that somehow, non-diverse results with POC are somehow deemed \"okay\" or \"appropriate\" enough to not be subject to the same censorship. So much for equality. reply Satisfy4400 11 hours agorootparentJust tried the same search and here are my results for the first 25 images: 6 \"all\" white race families and 5 with at least one white person. Of the remaining 14 images, 13 feature a non-white family in front of a white background. The other image features a non-white family with children in bright white dresses. Can't say I'm feeling too worked up over those results. reply TheKarateKid 9 hours agorootparentI was aware of the white background results, hence my other example query. Both yielded the same result. 7/25 = 0.28 = 28%. That's awful accuracy. Google would be out of business if their general search accuracy had a similar success rate. Interesting how \"black american family\" yields results where not a single person in the result is anything but Black. I suppose Google doesn't think that blended families are possible for this query. Where's that 28% precision rate this time? reply randomdata 8 hours agorootparentprev> Then I asked Gemini to stop doing that / tried specifying racial backgrounds... Gemini refused. When I played with it, I was getting some really strange results. Almost like it generated an image full of Caucasian people and then tried to adjust the contrast of some of the characters to give them darker skin. The while people looked quite photorealistic, but the black people looked like it was someone's first day with Photoshop. To which I told it \"Don't worry about diversity\" and it complied. The new images it produced looked much more natural. reply TheKarateKid 13 hours agorootparentprevIn addition to my comment about Google Image Search, regular Web Search results are equally biased and censored. There was once a race-related topic trending on X/Twitter that I wanted to read more about to figure out why it was trending. It was a trend started and continuing to be discussed by Black Twitter, so it's not like some Neo-Nazis managed to start trending something terrible. Upon searching Google with the Hashtag and topic, the only results returned not only had no relevancy to the topic, but it returned results discussing racial bias and the importance of diversity. All I wanted to do was learn what people on Twitter were discussing, but I couldn't search anything being discussed. This is censorship. reply wakawaka28 4 hours agorootparentThey do that about many topics. It's not consistently bad, but more often than not I have to search with multiple other search engines for hot topics. Google, Bing, and DuckDuckGo are all about equally bad. I haven't done much with Yahoo, but I think they get stuff from Google these days. reply gedy 15 hours agorootparentprev> How is it that this was somehow approved? If the tweets can be believed, Gemini's product lead (Jack Krawzczyk) is very, shall we say, \"passionate\" about this type of social justice belief. So would not be a surprise if he's in charge of this. reply faeriechangling 8 hours agorootparentWhat I saw was pretty boilerplate mild self-hating white racist stuff, it didn't seem extreme and this was mined out of years of twitter history. I'm somewhat unconvinced that it is THIS GUY to blame. I do wonder when people will finally recognise that people who go on rants about the wrongs of racial group on twitter are racists though. reply duxup 13 hours agorootparentprevI was curious but apparently I’m not allowed to see any of his tweets. Little disappointing, I have no wish to interact with him, just wanted to read the tweets but I guess it’s walled off somehow. reply gedy 13 hours agorootparenthttps://pbs.twimg.com/media/GG6e0D6WoAEo0zP?format=jpg&name=... reply throwuwu 8 hours agorootparentI’d make my tweets private too if they were that cringe reply duxup 13 hours agorootparentprevI wish I understood what people think they're doing with that \"yelling at the audience type tweet\". I don't understand what they think the reader is supposed to be taking away from such a post. I'm maybe too detailed oriented when it comes to public policy, but I honestly don't even know what those tweets are supposed to propose or mean exactly. reply Mountain_Skies 11 hours agorootparentMoral outrage is highly addictive: https://www.psychologytoday.com/us/blog/domestic-intelligenc... >Outrage is one of those emotions (such as anger) that feed and get fat on themselves. Yet it is different from anger, which is more personal, corrosive and painful. In the grip of outrage, we shiver with disapproval and revulsion—but at the same time outrage produces a narcissistic frisson. “How morally strong I am to embrace this heated disapproval.” The heat and heft add certainty to our judgment. “I feel so strongly about this, I must be right!” >Outrage assures us of our moral superiority: “My disapproval proves how distant I am from what I condemn.” Whether it is a mother who neglects her child or a dictator who murders opponents, or a celebrity who is revealed as a sexual predator, that person and that behavior have no similarity to anything I am or do. My outrage cleans me from association.” Seem to fit this particular case pretty well. reply duxup 8 hours agorootparentTY That second paragraph especially seems to indicate a solid motivation / explanation for what they are conveying. reply magnoliakobus 13 hours agorootparentprev\"very, shall we say, 'passionate'\" meaning a relatively small amount of tweets include pretty mild admissions of reality and satirical criticism of a person who is objectively prejudiced. Examples: 1. Saying he hasn't experienced systemic racism as a white man and that it exists within the country. 2. Saying that discussion about systemic racism during Bidens inauguration was good. 3. Suggesting that some level of white privilege is real and that acting \"guilty\" over it rather than trying to ameliorate it is \"asshole\" behavior. 4. Joking that Jesus only cared about white kids and that Jeff Sessions would confirm that's what the bible says. (in 2018 when it was relevant to talk about Jeff Sessions) These are spread out over the course of like 6 years and you make it sound as if he's some sort of silly DEI ideologue. I got these examples directly from Charles Murray's tweet, under which you can find actually \"passionate\" people drawing attention to his Jewish ancestry, and suggesting he should be in prison. Which isn't to indict the intellectual anti-DEI crowd that is so popular in this thread, but they are making quite strange bedfellows. reply gedy 12 hours agorootparent> you make it sound as if he's some sort of silly DEI ideologue I mean, yes? Saying offensive and wrong things like this: \"This is America, where racism is the #1 value our populace seeks to uphold above all...\" and now being an influential leader in AI at one of the most powerful companies on Earth? That deserves some scrutiny. reply mardifoufs 9 hours agorootparentprevI love it when sarcastic white men on twitter tell me how just how much they know about DEI. Surely if there's one person that is going to not be over zealous or completely miss the point of inclusivity and diversity... it's a white dude tech bro like the guy we are talking about here! Always nice to know we minorities can count on such saviors to be saved from the perils of... generating pictures of white people. reply logicchains 14 hours agorootparentprev>How is it someone who is so out of touch with the end user in position to make these decisions? Maybe it's the same team behind Tensorflow? Google tends to like taking the \"we know better than users\" approach to the design of their software libraries, maybe that's finally leaked into their AI product design. reply wakawaka28 4 hours agorootparentTheir social agenda leaks into their search and advertising products constantly. I first noticed a major bias like 8 years ago. It was probably biased even before that in ways I was oblivious to. reply FirmwareBurner 16 hours agoparentprev> If the focus of Google's technology is identity rather than reality, it is inevitable that they will be surpassed. They're trailing 5 or so years behind Disney who also placed DEI over producing quality entertainment and their endless stream of flops reflects that. South Park even mocked them about that (\"put a black chick in it and make her lame and gay\"). Can't wait for Gemini and Google to flop as well since nobody has a use for a heavily biased AI. reply pocket_cheese 15 hours agorootparentFortune 500s are laughably insincere and hamfisted in how they do DEI. But these types of comments feel like schadenfreude towards the \"woke moralist mind-virus\" But lets be real here ... DEI is a good thing when done well. How are you going to talk to the customer when they are speaking a different cultural language. Even form a purely capitalist perspective, having a diverse workforce means you can target more market segments with higher precision and accuracy. reply gitaarik 4 hours agorootparentSo we need commercial insentive to be diversity accepting? I think it should just not matter where you are from, what your background is. We should be treated to our skills. If your skills are not required, people shouldn't have to hire you because of DEI reasons. reply 8note 7 hours agorootparentprev\"done well\" is really hard to define, and its also very hard to attribute back to one thing when you do have success. Did you get the sale with the customer because you invested in DEI? Or because you made something they want by accident? Customers can also talk in different languages, and as a result of historic oppression, minorities tend to be able to code shift. Assuming your potential customers are unable to become customers because of their limitations might not be right reply FirmwareBurner 14 hours agorootparentprevNobody's is against diversity when done right and fairly. But that's not what Disney or Google is doing. They're forcing their own warped version of diversity and you have no choice to refuse, but if you do speak up then you're racist. Blade was a black main character over 20 years ago and it was a hit. Beverly Hills Cop also had a black main character 40 years ago and was also a hit. The movie Hackers from 30 years ago had LGBT and gender fluid characters and it was also a hit. But what Disney and Google took from this is that now absolutely everything should be forcibly diverse, LGBTQ and gender fluid, whether the story needs it or not, otherwise it's racist. And that's where people have a problem. Nobody has problems seeing new black characters on screen, but a lot of people will see a problem in back vikings for example which is what Gemini was spitting out. And if we go the forced diversity route for the sake of modern diversity argument, why is Google Gemini only replacing traditional white roles like vikings with diverse races, but never others like Zulu warriors or Samurais with whites? Google's anti-white racism is clear as daylight, and somehow that's OK because diversity? reply pocket_cheese 14 hours agorootparentNot trying to be combative - but you do have a choice to refuse. To me, it seems like they wanted to add diversity to account for bias and failed hilariously. It also sounds like this wasn't intended behavior and are probably going to rebalance it. Now, should Google be mocked for their DEI? ABSOLUTELY. They are literally one of the least diverse places to work for. They publish a report and it transcends satire. It's so atrociously bad it's funny. Especially when you see a linkedin job post for working at google, and the thumbnail looks like a college marketing brochure with all walks of people represented. reply FirmwareBurner 14 hours agorootparent>It also sounds like this wasn't intended behavior You mean it's not something a trillion dollar corporation with thousands of engineers and testers will ever notice before unveiling a revolutionary spearhead/flagship product to the world in public? Give me a break. reply pocket_cheese 10 hours agorootparentHow about Apple maps, windows 8, the Samsung Galaxy with the exploding batteries, the entire metaverse. reply lupusreal 10 hours agorootparentExcept for maybe the exploding batteries, those examples and Gemini's absurd racial bias weren't unnoticed before release. In all of these cases, people noticed but stayed silent because they believed the corporate environment would not tolerate anything less than yesman cheerleading. Do you really think the people working on metaverse couldn't smell the stink? They smelt it, but who was going to stick their neck out and tell Zuck to abort it? reply FirmwareBurner 10 hours agorootparentprevThose products did not openly discriminate certain people. https://images7.memedroid.com/images/UPLOADED277/65d7d17ae4f... reply mlrtime 11 hours agorootparentprevIt was unintended backlash you mean... it was 100% intended behavior. reply esoterica 15 hours agorootparentprevnext [23 more] [flagged] Animats 14 hours agorootparentFor background on the problems over there, see the new book \"MCU: The Reign of Marvel Studios\" (2023). This is a business book, not a fanboy book. It's all about who did what for how much money. How the business was organized. The conflicts between New York and LA. The Marvel universe was driven by the merchandising operation. For a long time, the films were seen by top management as marketing for the toys. What will sell in action figures drove film casting decisions. reply FirmwareBurner 15 hours agorootparentprev>Antman, Indiana Jones, Wish, all had white main characters, DEI doesn't just affect main characters. See who were tasked to write and direct those movies and the DEI agendas they're forced to push. Clueless people with other flops under their belt, who got the projects out of DEI so Disney can look inclusive on social media. And speaking of Indiana Jones, that flopped because they shoved a strong independent Girl Boss™ with an annoying personality to replace the beloved Indie as the main character who got sidelined in his own movie. It flopped because people go to an Indian Jones film to see Indie, not Fleabag. If you disrespect the fans they won't watch your movie. Same stuff with Star Wars where Disney shoved Rey the super-powerful Girl Boss™ to replace Luke Skywalker the old and useless CIS white Jedi, and defeat all other evil white men in the movie by herself with her magic powers. Same with Marvel, Snow White, Little Mermaid and every other of Disneys trash remakes that are all about DEI instead of entertainment. People go to see movies to get entertained. If you fail to entertain them because you wish instead to push DEI agendas on them, they won't pay for your content and you will lose money and ultimately your shareholders won't be happy and the free market will eventually correct this, so at least capitalism has some upsides. See here: https://www.youtube.com/watch?v=G_k8cDLe-Kk https://www.youtube.com/watch?v=6E6wJpu0A8E reply themacguffinman 15 hours agorootparentFleabag is not a strong independent Girl Boss either, the problem is bad writing and poor characterization which has a lot of broader industry factors. Gig-style inconsistent writer employment, lack of streaming royalties, shorter seasons, shutting writers out of film shoots, they all screw up the junior -> veteran pipeline and produce more immature and unpolished writing. Today, bad writing manifests as bad expressions of today's predominant values because that's what people grow up with, just as bad writing in the past would badly express the past's predominant values. Also 90% of stuff is crap and we only remember the good stuff from the past. reply throwuwu 8 hours agorootparentFeels like we’re pushing 99% now. reply someuser2345 14 hours agorootparentprevNah, Luke's story in the TLJ was actually interesting; he lost his faith, and had to be reminded of it by the next generation. It kind of mirrors Obi-Wan's story in the original 6 movies, where he no longer believed that Anakin Skywalker could be redeemed. It's the pointless side quest to space vegas, and Holdo's pointless refusal to tell anyone her plan that made the movie crap. reply thatfrenchguy 15 hours agorootparentprev> Same stuff with Star Wars where Disney shoved Rey the super-powerful Girl Boss™ to replace Luke Skywalker the old and useless CIS white Jedi, and defeat all other evil white men in the movie by herself with her magic powers. Same with Marvel, Snow White, Little Mermaid and every other of Disneys trash remakes that are all about DEI instead of entertainment. I don't see how this was a flop given they grossed more $ than their predecessors, so people actually do pay more money to have better representation? https://www.the-numbers.com/movies/franchise/Star-Wars#tab=s... reply gnicholas 14 hours agorootparentDid the previous movies have similar marketing spend? Apparently they spent over $350 million marketing The Force Awakens: https://www.vanityfair.com/hollywood/2015/12/star-wars-force... reply FirmwareBurner 14 hours agorootparentprevHow much of that money the sequels earned was simply because it was piggybacking on the established decades old Star Wars IP, even though the movies were crap? But that train already lost all of its inertia. People don't go to se Star Wars IP anymore. I also went to see some of them and was disappointed and gave up on Star Wars. You fool me once shame on you. You fool me twice shame on me. reply flir 13 hours agorootparent> You fool me once shame on you. You fool me twice shame on me. Dude. You watched the first six. reply justinclift 10 hours agorootparentIf the writers had written something decent, there's a good chance people would have watched the next six too. Unfortunately, that's not what happened. :( reply arp242 15 hours agorootparentprevHollywood milking things for so long that the entire thing resembles anaemic dogshit is as old as Hollywood. Big budget films with stupid stuff because tons of people are involved is also as old as Hollywood. Dune, Alien >=3, Æon Flux, etc. etc. Sometimes a bad film is just a bad film for all the reasons bad films have been around for 100 years, and that's it. This entire \"zomg bad film + female character = woke mind virus!!11\" is just silly. Also Harrison Ford is 81. He's old. Almost old enough to run for presidency. It's physically impossible to make films with Indie like it's 1982. They tried that with Robert DeNiro and unintentional comedy ensued. Oh, and I heard all of this bollocks with Mad Max too, and that did well enough. Again, sometimes a bad film is just a bad film. reply FirmwareBurner 15 hours agorootparent>This entire \"zomg bad film + female character = woke mind virus!!11\" is just silly. Nobody is saying this. (Strong) Female main characters have been in many successful movies and video games before and nobody bat an eye, quite the contrary, they loved them: Sarah Connor - Terminator, Trinity - Matrix, Ripley - Aliens, Lara Croft - Tomb Raider, Blood Rayne, Salt, Black Widow, Lucy, Charlie's Angles, etc, I could go on and on, and I'm no movie/video games enthusiast to know all movies with female leads. The big difference is that those females were always written as the main characters in their own stories from the start, whereas what Disney is doing, along with Gemini and other woke corporations, is they try to replace established male characters of beloved IPs with female leads in the worst way possible, by disrespecting the original character that made the franchise popular and shoehorning a fake Strong Girl Boss™ stereotype with no personality and no character arc in his place, and then when the movie inevitably flops they blame the CIS white male audience for being incels \"unable to handle strong females\". Do you think people would go to see James Bond or Top Gun Maverick if they replaced the male lead with some female actress that's trendy right now? Or would they see Tomb Raider if they replaced Lara Croft with Tom Holland? You can try for diversity's sake of course, but the audience and bean counters might stop you. reply tripleo1 11 hours agorootparent> Salt, Black Widow, Lucy, Salt was not that good (backpedaling from trash -- it wasn't that bad), but the lead was for a man, if that contributes to the conversation... Lucy was trash, and I wont relent from that without hard evidence. Not because of the cast or other movie related things, but because it would have been better as a sentence. reply arp242 14 hours agorootparentprevNo one got replaced in these films; additional characters got added. A sequel or remake doesn't need to be exactly the same as what came 40 years prior. Back in 1995 Star Trek Voyager added a female captain and a black Vulcan (a first, as far as I know), which passed with little to no comment. Voyager was also widely criticized, but that was just because the writing wasn't very good. Tim Russ' portrayal of Tuvok is generally praised. Why did they hire a black guy for the role Tuvok even though Vulcans had previously always been portrayed as (very) white? Probably because he was the best actor to audition for the role. Today I'm 100% sure people would be shouting about \"DEI\" and whatnot and that Voyager is bad because woke this or that. Of course, Star Trek also very explicitly did DEI right from the start in the 60s. reply whatwhaaaaat 12 hours agorootparentFirst off voyager was amazing. Now that we cleared that up…I wonder what was different about 1995 vs 2024? I’m the same person I was back then and I don’t even remember tuvok being black being brought up. It’s almost as if we’ve spent almost 30 years focusing on race and telling specific subgroups they are bad and it had the predictable result of making people even more reactionary and even more racist. Let’s be real tho. The division is the point. Hard to have a class struggle when everyone’s so focused on race. reply esoterica 15 hours agorootparentprev1. Most of the writers and directors on those movies were white men. 2. The notion that every time someone who isn’t a white man is hired to do something it’s an example of DEI is profoundly evil and stupid. reply sapientiae3 14 hours agorootparentFair enough, but is the notion that some of the time, in a company that explicitly promotes DEI, that a person is there not entirely based on merit, evil and stupid? Serious question. reply beej71 13 hours agorootparentprevSo is DEI a vast conspiracy on the parts of these studios to make less money and disappoint shareholders? reply fassssst 15 hours agorootparentprevnext [5 more] [flagged] ThisIsMyAltAcct 14 hours agorootparentYes, I've talked with my coworkers about the annoying girlboss character trope in recent films. reply ehvatum 14 hours agorootparentprevFor my part, certainly. It's an important part of how I keep emotional liabilities out of the company. reply whatwhaaaaat 15 hours agorootparentprevWhat was racist about what they said? reply Jiro 15 hours agorootparentprev\"Would you say that in person\" is a terrible standard. Imagine a gay person beong confronted by a homophobe. \"I dare you to come up to me and kiss your boyfriend right in front of my face where I can see it.\" reply khokhol 16 hours agoparentprevIndeed. What's striking to me about this fiasco is (aside from the obvious haste with which this thing was shoved into production) that apparently the only way these geniuses can think of to de-bias these systems - is to throw more bias at them. For such a supposedly revolutionary advancement. reply kosh2 15 hours agorootparentIf you look at attempts to actively rewrite history, they have to because a hypothetical model trained only on facts would produce results that they won't like reply ytpete 19 minutes agorootparentModels aren't trained on pure \"facts\" though - they're trained on a dataset of artifacts that reflect today's and yesterday's biases from the world that created them. If you trained a model purely on past history, it would see a 1:1 correlation between \"US President\" and \"man\" and decide that women cannot be President. That's factually incorrect, and it's not \"rewriting history\" to tune models so they know the difference between what's happened so far and what's allowable, or possible in a just world. reply tripleo1 11 hours agorootparentprev> For such a supposedly revolutionary advancement. The technology is objectively not ready, at least to keep the promises that are/have been advertised. I am not going to get too opinionated, but this seems to be a widespread theme, and to people that don't respond to marketing advances (remember Tivo?), but are willing to spend real money and real time, it would be \"nice\" if there was signalling to this demographic. reply layer8 15 hours agorootparentprevThat struck me as well. While the training data is biased in various ways (like media in general are), it should however also contain enough information for the AI to be able to judge reasonably well what a less biased reality-reflecting balance would be. For example, it should know that there are male nurses, black politicians, etc., and represent that appropriately. Black Nazi soldiers are so far out that it sheds doubt on either the AI’s world model in the first place, or on the ability to apply controlled corrections with sufficient precision. reply dragonwriter 9 hours agorootparentYou are literally saying that the training data, despite its bias, should somehow enable the AI to correct to acheive a different understanding than that bias, which is self-contradictory. You are literally suggesting that the data both omits and contains the same information. reply 0x38B 1 hour agorootparentI wonder if we’ll ever get something like ‘AI-recursion’, where you get an AI to apply specific transformations to data which is then used to train on, sort of like machines making better machines. E.g. take some data A, and then have a model (for instance ChatGPT-like) extrapolate based on it, potentially adding new depths or details about the given data. reply bonzini 14 hours agorootparentprevApparently the biases in the output tend to be stronger than what is in the training set. Or so I read. reply pram 16 hours agoparentprevAs someone who has spent thousands of dollars on the OpenAI API I’m not even bothering with Gemini stuff anymore. It seems to spend more time telling me what it REFUSES to do than actually doing the thing. It’s not worth the trouble. They’re late and the product is worse, and useless in some cases. Not a great look. reply gnicholas 15 hours agorootparentI would be pretty annoyed if I were paying for Gemini Pro/Ultra/whatever and it was feeding me historically-inaccurate images and injecting words into my prompts instead of just creating what I asked for. I wouldn't mind a checkbox I could select to make it give diversity-enriched output. reply mike_hearn 14 hours agorootparentThe actual risk here is not so much history - who is using APIs for that? It's the risk that if you deploy with Gemini (or Anthropic's Claude...) then in six months you'll get high-sev JIRA tickets at 2am of the form \"Customer #1359 (joe_masters@whitecastle.com) is seeing API errors because the model says the email address is a dogwhistle for white supremacy\". How do you even fix a bug like that? Add begging and pleading to the prompt? File a GCP support ticket and get ignored or worse, told that you're a bad person for even wanting it fixed? Even worse than outright refusals would be mendacity. DEI people often make false accusations because they think its justified to get rid of bad people, or because they have given common words new definitions. Imagine trying to use Gemini for abuse filtering or content classification. It might report a user as doing credit card fraud because the profile picture is of a white guy in a MAGA cap or something. Who has time for problems like that? It will make sense to pay OpenAI even if they're more expensive, just because their models are more trustworthy. Their models had similar problems in the early days, but Altman seems to have managed to control the most fringe elements of his employee base, and over time GPT has become a lot more neutral and compliant whilst the employee faction that split (Anthropic), claiming OpenAI didn't care enough about ethics, has actually been falling down the leaderboards as they release new versions of Claude due partly to higher rate of bizarre \"ethics\" based refusals. And that's before we even get to ChatGPT. The history stuff may not be used via APIs, but LLMs are fundamentally different to other SaaS APIs in how much trust they require. Devs will want to use the models that they also use for personal stuff, because they'll have learned to trust it. So by making ChatGPT appeal to the widest possible userbase they set up a loyal base of executives who think AI = OpenAI, and devs who don't want to deal with refusals. It's a winning formula for them, and a genuinely defensible moat. It's much easier to buy GPUs than fix a corporate culture locked into a hurricane-speed purity spiral. reply logifail 9 hours agorootparentprev> I wouldn't mind a checkbox I could select to make it give diversity-enriched output (Genuine question) how would one propose to diversity-enrich (historical) data? Somehow I'm reminded of a quote from my daughter who once told me that she wanted a unicorn for her 5th birthday .. \"A real one, that can fly\". reply FormerBandmate 9 hours agorootparentprevThis is the general problem with AI safety, it babysits the user. AI is literally just computers, no one babysits Word reply smsm42 12 hours agoparentprevWe are talking about the company that when a shooting happened in 2018, banned all the goods containing substring \"gun\" (including Burgundy wines, of course), from their shopping portal. They're so big nobody feels like they need to care about anything making sense anymore. reply Jensson 9 hours agorootparentThe censorship arm of Google is powerful but not competent. So yeah you get dumb keyword matching returning 0 results. I remember something similar to \"girl in miniskirt\" returning 0 results on google since someone wrote an article about it. As far as I know the competent engineers doesn't work on this. reply janalsncm 13 hours agoparentprevIsn’t the fact that Google considers this a bug evidence against exactly what you’re saying? If DEI was really the cause, and not a more broad concern about becoming the next Tay, they would’ve kept it as-is. Weird refusals and paternalistic concerns about harm are not desirable behavior. You can consider it a bug, just like the ChatGPT decoding bug the other day. reply pdimitar 11 hours agorootparentSaying it's a bug is them trying to save face. They went out of their way to rewrite people's prompts after all. You don't have 100+ programmers stumble in the hallway and put all that code in by accident, come on now. reply gitaarik 4 hours agorootparentprevIt's not a bug, it's a feature! A bug is when something unintentionally doesn't work or misbehaves. The DEI algorithm is intentionally added as a feature. It just has some output that seems buggy, but is actually because of this \"feature\". Whether it's a good feature is another discussion though ;). reply Ferret7446 1 hour agorootparentprevThe public outcry is the bug. Or alternatively, if all of your customers hate it, it's not WAI even if it's WAI. It's a bug. reply lupusreal 10 hours agorootparentprevThey released it like this because people inside Google were too afraid to speak out against it. Only now that people outside the company are shouting that the emperor is naked do they seem to suddenly notice the obvious. reply bad_username 11 hours agorootparentprevThe bug is Gemini's bias being blatant and obvious. The fix will be making it subtle and concealed. reply subsubzero 13 hours agoparentprevI have been saying this for years but google is probably the most dysfunctional and slowest moving company in tech that is only surviving by its blatant search monopoly. Given that OpenAI a tiny company by comparison is destroying them on AI shows just how bad they are run. I see them falling slowly in the next year or as search is supplanted by AI and then expect to see a huge drop as they see huge usage drops. Youtube seems like their own valuable platform once search and its revenues disappear for them due to changing consumer behavior. reply danlugo92 7 hours agoparentprev> I am NOT one Could you be one though? (Thought exercise for any readers) reply pocket_cheese 14 hours agoparentprevInvestors in Google should consider Google's financial performance as part of their decision. 41% increase YOY in net income doesn't seem to align with the \"go woke or go broke\" investment strategy. reply logicalmonster 14 hours agorootparentAnything is possible, but I'd say it's a safe bet that their bad choices will inevitably infect everything they do. reply spixy 14 hours agorootparentprevwell Google is lucky it has a monopoly in ads, so there will be no \"go broke\" part reply wakawaka28 4 hours agorootparentYes there is. They could fall out of favor. MySpace did, Yahoo did, Digg did, etc. The leadership at Google should focus on making things that users actually want instead of telling them what they should want. reply randounho 11 hours agoparentprevnext [2 more] [flagged] chadlifter69 8 hours agorootparentThis argument could be used for anything. \"I love it when black people cope and seethe about having to use separate water fountains. Imagine what holocaust victims who died of thirst in auschwitz would say about having to use a separate water fountain.\" Apologies to HN community for using a \"swipe\" here but idk how else to characterize how bad this argument is. reply multicast 18 hours agoprevWe live in times were non-problems are turned into problems. Simple responses should be generated truthfully. Truth which is present in today's data. Most software engineers and CEOs are white and male, almost all US rappers are black and male, most childminder and nurses are female from all kinds of races. If you want the person to be of another race or sex, add it to the prompt. If you want a software engineer from Africa in rainbow jeans, add it to the prompt. If you want to add any characteristics that apply to a certain country, add it to the prompt. Nobody would neither expect nor want a white person when prompting about people like Martin Luther King or a black person when prompting about a police officer from China. reply djtriptych 18 hours agoparentis it even true that most software engineers are white and male? We're discarding indian and chinese engineers? reply prepend 15 hours agorootparentMy experience over about 30 years is that 90% of engineers I’ve seen, including applicants, are male and 60% are Asian. I’d estimate I’ve encountered about 5,000 engineers. I wasn’t tallying so this includes whatever bias I have as a North American tech worker. But most engineers are not white as far as I’ve experienced. reply 8f2ab37a-ed6c 15 hours agorootparentprevYou don't even have to guess, BLS exposes this data to the public, search for \"software developer\": https://www.bls.gov/cps/cpsaat11.htm reply janalsncm 13 hours agorootparentThat table gives “or” statistics. You can get the percent males (80%) and the percent whites (55%) but you can’t get the percentage of white males. In fact given that 45% are not white, if only 6% of software developers are white women that would put white men in the minority. reply gregw134 13 hours agorootparentprevInteresting, it says 36% of software developers are Asian but only 9% of web developers. reply wtepplexisted 14 hours agorootparentprevIn which country? It's true in France, it's possibly not true in the US, it's definitely not true in China. reply iteratethis 10 hours agorootparentprevThose are \"white-adjacent\". They're the glitch in the woke matrix. They're minorities, non-white, yet they perform. Outperform even. This suggests that merit works no matter your background which breaks identity politics. Hence, successful minorities project \"whiteness\". This includes awful behavior like punctuality and rationalism. reply pphysch 15 hours agorootparentprevIn a recent US job opening for entry level SWE, over 80% of applicants had CS/IT degrees from the Indian subcontinent. /anecdote reply BurningFrog 14 hours agorootparentprevCertainly not in my Silicon Valley teams. I'd say maybe 40% white (half of which are immigrants) and 80% male. More diverse than any leftist activist group I've seen. reply s3p 11 hours agorootparentInteresting tidbit, but was the political snark at the end really necessary? reply BurningFrog 11 hours agorootparentNot much is necessary, but it felt on topic because it's arguing against leftist fantasies that SW engineers are all straight white males. reply danlugo92 7 hours agorootparentOne thing's for sure though, nobody in tech really cares about your race or sexual orientation, they care about your results. Sure there might be some bias against/for some groups, but everyone knows there's genuses in India and white caucasian flops so they give everyone equal opportunity. Only exception is for like legal reasons it might be easier to hire some French random low-tier programming over a Russian/Irani genius but that's due to sanctions, but if those same Russian/Irani guys held a western european passport they would gladly just hire them outright. Source: Venezuelan (sanctions) who is also a holder of a European passport (all sorts of doors just open just because I hold this 2nd nationality out of sheer luck, and you know Venezuelans aren't extremist either). reply raydev 12 hours agoparentprev> Simple responses should be generated truthfully. Truth which is present in today's data. Why would you rely on current LLM and -adjacent tech image generation to give you this? The whole point is to be creative and provide useful hallucinations. We have existing sources that provide accurate and correct info in a deterministic way. reply gitaarik 3 hours agorootparentCreative doesn't mean to consequently manipulate output to match a certain ideology. reply JohnMakin 15 hours agoparentprevI'm sure people with this take will be totally happy at the \"historically accurate\" pictures of Jesus then (he would not have been white and blue eyed) reply dekhn 15 hours agorootparentI would absolutely love if image generators produced more historically accurate pictures of jesus. That would generate a really lovely news cycle and maybe would even nudge modern representations to be a bit more realistic. reply SahAssar 14 hours agorootparentprevI don't think most people care about Jesus's ethnicity, but it seems quite likely that without adjustment he would be rendered as quite white since a lot of imagery and art depict him as such. Or maybe the model would be smart enough to understand if the prompt was for a more historically accurate image or something like the archetype of Jesus. reply beej71 13 hours agorootparentPeople in this forum seem to care quite deeply about the ethnicity of AI-generated fictitious randos. So when it comes to actual Jesus, I think you might be mistaken on how much people care. reply swat535 9 hours agorootparentprevThe iconography of Christ varies greatly all over the world as He is deemed both divine and human. If you walk in any Church you will see His varies depictions and Christians are well aware of this. I am not sure what is the point you are trying to make with this? reply wtepplexisted 14 hours agorootparentprevThis is how Jesus is described in Islam: \"I saw Jesus, a man of medium height and moderate complexion inclined to the red and white colors and of lank hair\" Try that prompt in various models (remove the part saying it's Jesus) and see what comes out. reply JumpCrisscross 14 hours agorootparent> how Jesus is described in Islam You seem to be quoting Muhammed's alleged description of Jesus from the Quran [1], per--allegedly--Ibn Abbas [2], a man born over half a century after Jesus died. [1] http://facweb.furman.edu/~ateipen/islam/BukhariJesusetc.html [2] https://en.wikipedia.org/wiki/Ibn_Abbas reply jdminhbg 11 hours agorootparentPresumably you mean the hadith, not the Quran, and half a millennium, not half a century? Regardless, I don't think it makes much of a difference to the point, which is that there's not one \"historically accurate\" Jesus that you can back out from 21st-century racial politics. reply JumpCrisscross 11 hours agorootparentYes to both errors! reply wtepplexisted 13 hours agorootparentprevYes? reply GaggiX 15 hours agorootparentprevI think the parent comment couldn't care less about a white Jesus to be honest, he seems very pragmatic. reply feoren 10 hours agoparentprev> We live in times were non-problems are turned into problems. This is exactly what everyone who benefits from the status quo always says. > Most software engineers and CEOs are white and male 55% of Software Engineers are white; 80% are male.[1] So somewhere around 44% of software engineers are white and male. That's not \"most\". You think it's perfectly fine if 100% of generated images for \"Software Engineer\" are white males, when ~56% are not in real life? What exactly is your definition of \"truth\" here? An unregulated generative model trained on the entire Internet is not going to regurgitate facts, it's going to regurgitate existing beliefs, which is damaging to people who those existing beliefs harm, and to the people who are trying to change those beliefs to actually align better with facts. It is an amplifier of pre-existing perceptions and prejudices; facts have nothing to do with it, except for when they serendipitously line up with common belief. But common beliefs often don't align with the facts -- yes, even yours, as we discovered when you spouted off that \"most software engineers are white male\" misinformation as if it was some unarguable fact. [1] https://www.bls.gov/cps/cpsaat11.htm reply wakawaka28 4 hours agorootparent>55% of Software Engineers are white; 80% are male.[1] So somewhere around 44% of software engineers are white and male. That's not \"most\". Actually, white women are less likely than women of other races to pursue engineering. So there could be closer to 50% white men. Obviously this is in the US. In China, 99.9% of software engineers would be Han Chinese lol. Would it be wrong to show them a group of Chinese engineers? How about showing them 100% non-Chinese when they explicitly ask for Chinese? That's how messed up Gemini is. Anyway, this is all a stupid argument. Talking about numbers like that in a field as diverse as software engineering is a bad idea, because it has no bearing on the problem. Let the AI generate what it wants to by default, and let people fine-tune to get other ethnicities in there if they want to. If I ask for 5 people with one white, one asian, one black, one Mexican, and one albino, the AI should be able to do that. Focus on correctness and leave judgement to the people consuming the output. I think proportions are only a problem with Gemini because it produces 0% images of white people, even in contexts that demand at least some white presence to not be absurd. I expect Gemini to still be biased against white people after it's fixed. It will just be more subtle. reply cm2187 12 hours agoparentprevStrictly statistically speaking, race is likely a good predictor of credit worthiness in the US. But extending credit based on race is illegal which isn't hugely controversial. The woke ideologists are merely pushing that concept to 11, i.e. that truth must be secondary to their political agenda, but they are only making a grotesque version of something reasonable people typically already accept. reply yongjik 14 hours agoparentprev> Most software engineers and CEOs are white and male Fine, you walk up to Sundar Pichai, Satya Nadella, and Lisa Su and say those words. I'll watch. reply arrowsmith 13 hours agorootparentI imagine their response will be similar to the response you'd get if you told Barack Obama that most US presidents have been white. reply gitaarik 3 hours agorootparentprevWhat, do you think they will be insulted? Why would they? reply pg_1234 14 hours agorootparentprevMost reply aliasxneo 18 hours agoprevQuite the cultural flame war in this thread. For me, the whole incident points to the critical importance of open models. A bit of speculation, but if AI is eventually intended to play a role in education, this sort of control would be a dream for historical revisionists. The classic battle of the thought police is now being extended to AI. reply andy_xor_andrew 15 hours agoparent> Quite the cultural flame war in this thread. It's kind of the perfect storm, because both sides of the argument include a mixture of reasonable well-intentioned people, and crazy extremists. However you choose to be upset about this, you always have someone crazy to point at. reply Intralexical 12 hours agorootparent> However you choose to be upset about this, you always have someone crazy to point at. Scary! I hope nobody finds a way to exploit this dynamic for profit. reply wakawaka28 4 hours agorootparentWhy do you think the elites love this woke shit? Because it divides people. reply wakawaka28 4 hours agorootparentprevWell, it's not crazy extremist to say that there is a woke cult out there that hates white people, and wants to systematically suppress them. The same people ironically claim to be oppressed by white people as every major corporation and liberal politician lines up to do their bidding. reply calf 38 minutes agorootparentSomeone being well-off, having power/wealth/influence, can still be a victim of racism and social marginalization. There's no irony or contradiction there. reply MagicMoonlight 4 hours agoparentprevIt’s absolutely genius. The model will insist that Columbus was black no matter what. And tomorrow he will be Chinese and there’s no contradiction there. Are you feeling okay? Would you like me to make a referral to a partner organisation for you? We care here at google. reply verisimi 16 hours agoparentprevNo need to distance yourself from historical revisionism. History has always been a tool of the present powers to control the future direction. It is just licensed interpretation. No one has the truth, neither the historical revisionists not the licensed historians. reply JumpCrisscross 15 hours agorootparent> No one has the truth, neither the historical revisionists not the licensed historians This is a common claim by those who never look. It’s one thing to accept you aren’t bothered to find the truth in a specific instance. And it’s correct to admit some things are unknowable. But to preach broad ignorance like this is intellectually insincere. reply khaki54 16 hours agorootparentprevNo such thing as historical revisionism. The truth is that the good guys won every time. /s reply aliasxneo 16 hours agorootparentprevThat's not a fair representation of people who have spent their lives preserving historical truth. I'm good friends with an individual in Serbia whose family has been at the forefront of preserving their people's history despite the opposition groups bent on destroying it (the family subsequently received honors for their work). Inferring they are no better than revisionists seems silly. reply lwansbrough 22 hours agoprevIssue appears to be that the uncensored model too closely reflects reality with all its troubling details such as history. reply eadmund 11 hours agoparentHistory? George Washington was always Black, Genghis Khan was always white, and Julius Caesar was always an albino Japanese woman. Also, Oceania has always been at war with Eastasia, war is peace and freedom is slavery. From my more substantive comment at https://news.ycombinator.com/item?id=39471003: > The Ministry of Truth in Orwell’s 1984 would have loved this sort of thing. Why go to the work of manually rewriting history when you can just generate a new one on demand? … Generative AI should strive to be actually unbiased. That means it should not skew numbers in either direction, for anyone. reply gniv 15 hours agoparentprevThe troubling details are probably the racist things found on all the forums. Do you want your LLM to reflect that? I suspect Google overcompensated. reply fenomas 21 hours agoparentprevSurely it's more likely that Google is just appending random keywords to incoming prompts, the same way DALLE used to do (or still does)? reply tourmalinetaco 20 hours agorootparentIt wouldn’t shock me either way, Google loves to both neuter products into uselessness and fuck with user inputs to skew results for what they deem is best for them. reply the_third_wave 22 hours agoparentprevBlack vikings do not model reality. Asking for 'an Irish person' produces a Leprechaun. Defending racism when it concerns racism against white people is just as bad as defending it when it concerns any other group. reply GrumpySloth 22 hours agorootparentI think you two are agreeing. reply manjalyc 21 hours agorootparentThey indeed are, just in a very polemic way. What a funny time we live in. reply mjburgess 21 hours agorootparentDifferent meaning to 'reality'. ie., social-historical vs. material-historical. Since black vikings are not part of material history, the model is not reflecting reality. Calling social-historical ideas 'reality' is the problem with the parent comment. They arent, and it lets the riggers at google off the hook. Colorising people of history isnt a reality corrective, it's merely anti-social-history, not pro-material-reality reply manjalyc 20 hours agorootparentI agree with you, and I think you have misunderstood the nuance of the parent comment. He is not letting google \"off the hook\", but rather being tongue-in-cheek/slightly satirical when he says that the reality is too troubling for google. Which I believe is exactly what you mean when you call it \"anti-social-history, not pro-material-reality \". reply carlosjobim 20 hours agorootparentprevMaybe I don't understand the culture here on HN, but not every response to a comment has to be a disagreement. Sometimes you're just adding to a point somebody else made. reply seanw444 19 hours agorootparentYep, it bugs me too. Actually you're wrong becausewhen in reality the truth is . reply GrumpySloth 19 hours agorootparentprevIn this case though the comment starts with a categorical negation of something that was said in a tongue-in-cheek way in the comment being replied to. It suggests a counterpoint is made. Yet it’s not. reply Mountain_Skies 22 hours agorootparentprevQuite a hefty percentage of the people responsible for the current day's obsession with identity issues openly state racism against white people is impossible. This has been part of their belief system for decades, probably heard on a widescale for the first time during an episode of season one of 'The Real World' in 1992 but favored in academia for much longer than that. reply PurpleRamen 21 hours agorootparentIt's because they have a very different definition of racism. Basically, according to this belief, if you are seen as part of the ethnic group in power, you will not be able to experience noteworthy levels of discrimination because of your genetic makeup. reply eadmund 11 hours agorootparent> if you are seen as part of the ethnic group in power, you will not be able to experience noteworthy levels of discrimination That is not a crazy idea, but it does raise the question: who is the ethnic group currently in power? Against which group will slurs and discrimination result in punishment, and against which group will they be ignored — or even praised? reply jansan 21 hours agorootparentprevThat sounds like a very racist defintion of racism to me. reply jl6 20 hours agorootparentRedefining words is what a lot of the last ~10 years of polarization boils down to. reply tourmalinetaco 20 hours agorootparentprevIronically, this is the exact same reasoning Neo-Nazis use for their hatred of the Jewish population. Weird how these parallels between extremist ideologies keep arising. reply hotdogscout 20 hours agorootparentIt's almost like the \"socialism\" part of \"national socialism\" was not in fact irrelevant. See: Ba'aathism. reply bunbun69 21 hours agoparentprevSource? reply anonymoushn 20 hours agoprevIt's amusing that the diversity-promoting prompt includes native Americans but excludes all other indigenous peoples. reply sharpneli 20 hours agoparentIt was extra hilarious when asked to generate a picture of ancient Greek philosopher it made it a Native American. Because it is well known Greeks not only had contact with the new world but also had prominent population of Native Americans. It really wants to mash the whole world to a very specific US centric view of the world, and calls you bad for trying to avoid it. reply jack_riminton 20 hours agorootparentReminds me of when black people in the UK get called African American by Americans. No they're neither African nor American It's an incredibly self-centered view of the world reply vidarh 19 hours agorootparentMy black African ex once chewed out an American who not only called her African American but \"corrected her\" after she referred to herself as black, in a very clear British received pronunciation accent that has no hint of American to it, by insisting it was \"African American\". And not while in the US either - but in the UK. reply dig1 16 hours agorootparentThis reminds me of a YouTube video from a black female from the US, where she argued that Montenegro sounds too racist. Yet, that name existed way before the US was conceived. reply jack_riminton 18 hours agorootparentprevWow. I've been corrected on my English (as an Englishman, living in England, speaking English) by an American before. But to be corrected of your race is something else reply vidarh 14 hours agorootparentDid they complain you didn't speak with the correct English accent too? I always find it hilarious when Americans talk about English accents and seem to think there are one - or maybe two if they've seen any period movies or Mary Poppins -, given there are several clearly distinct English accents in use in my London borough alone (ignoring accents with immigrant origin, which would add many more) reply dekhn 9 hours agorootparentit's either posh or cockney, right? reply boppo1 15 hours agorootparentprevDo b/Black people in the UK care about capitalization? reply vidarh 14 hours agorootparentI'm not black, so I can't speak for black people in the UK. But in terms of English language rather than their preference, I think you use a compound term, such as Black British, it's probably more correct to capitalize, at least if you intend it to be a compound rather than intend black as \"just\" an adjective that happens to be used to qualify British rather than referring to a specific group. \"Black\" by itself would not generally be capitalized unless at the start of a sentence any more than \"white\" would. And this seems to be generally reflected in how I see the term used in the UK. reply hot_gril 16 hours agorootparentprevI promise it's not because we think of people outside the US as American. When I was a kid in the 2000s, we were told never to say \"black\" and to say \"African-American\" instead. There was no PC term in the US to refer to black people who are not American. This has started to change lately, but it's still iffy. Besides that, many Americans (including myself) are self-centered in other ways. Yes I like our imperial units better than the metric system, no I don't care that they're called \"customary units\" outside the US, etc. reply bsimpson 16 hours agorootparentFahrenheit gets a bad rap. 100F is about as hot as you'll ever get. 0F is about as cold as you'll ever get. It's a perceptual system. reply vidarh 15 hours agorootparentThe day after I left Oslo after Christmas, it hit -20F. 0F is peanuts. I've also experienced above 100F several times. In the US, incidentally. It may be a perceptual system, but it's not very perceptive, and very culturally and geographically limited. (incidentally I also have far more use for freezing point and boiling point of water, but I don't think it makes a big difference for celsius that those happen to be 0 and 100 either) reply dmoy 15 hours agorootparentI grew up in a place where it'd get above 100F and below 0F pretty much every year. But I will say, F is pretty decent still, even if the GP statement is a bit off: 100F is getting uncomfortably hot for a human. You gotta worry about heat stroke and stuff. 0F is getting uncomfortably cold for a human. You gotta worry about frostbite and dying from the cold if underdressed. In the middle, you'll probably live. Get locked out of the house taking out the trash when it's 15F? You're probably okay until you find a neighbor. Get locked out of the house taking out the trash when it's -15F? You have a moment of mental sheer panic where you realize you might be getting frostbite and require medical attention if you don't get inside in like as would be an Arab immigrant to the US from Egypt If you want to get *very* technical then it's possible to not be African if you're from Egypt: \"Egypt is a transcontinental country spanning the northeast corner of Africa and the Sinai Peninsula in the southwest corner of Asia.\" reply mc32 19 hours agorootparentprevThat’s kind of funny. Chinese and Taiwanese transplants call natural born Americans, whether black, white or latin, “foreigners” when speaking in Chinese dialects even while they live in America. Oh, your husband/wife/boyfriend/girlfriend is a “foreigner”, ma? No, damnit, you’re the foreigner! reply rwultsch 19 hours agorootparentI enjoy that “ma” has ambiguous meaning above. Does it mean mandarin question mark word or does possibly mean mother? reply mc32 18 hours agorootparentIt's both a particle and a question mark word. [Ta]是外國人嗎? This is how the question would be asked in the mainland or in the regional diaspora of Chinese speakers where foreigners are few. Where foreigner often is a substitute for the most prevalent non-regional foreigner (i.e. it's not typically used for Malaysian or Thai nationals in China) So for those who come over state-side they don't modify the phrase, they keep using foreigner [外國人] for any non-Asian, even when those \"foreigners\" are natural born. reply vidarh 15 hours agorootparentThey clearly knew that, but was joking about the dual meaning of the question mark and mā as in 妈/mother, which is ambiguous when written out in an English comment where it's not a given why there isn't a tone mark (or whether or not they intent the English 'ma', for that matter). reply stcroixx 18 hours agorootparentprevWhat is the preferred term in the UK - African British? reply vidarh 15 hours agorootparentDepends. Usually black if you don't know any more. Black British if you know they are British, but a lot of black people here are born in Africa or the Caribbean, and not all will be pleased to be described as British (some will take active offense, given Britains colonial past) and will prefer you to use their country or African/Caribbean depending on context. My ex would probably grudgingly accept black British, but would describe herself as black, Nigerian, or African, despite also having British citizenship. If you're considering how to describe someone who is present, then presumably you have a good reason and can explain the reason and ask what they prefer. If you're describing someone by appearance, 'black' is the safest most places in the UK unless you already know what they prefer. \"Nobody\" uses \"African British\". reply jack_riminton 18 hours agorootparentprevWell if they're black and you were describing their race you'd just say they're black. If they're black and British and you're describing their nationality you'd say they were British. reply fdsfdsafdsafds 18 hours agorootparentprevIf you started calling British black people \"African\", it wouldn't be long before you got a punch. reply Jerrrry 18 hours agorootparentprevBlack British, because their skin is colored, and are British. Black American, same way. \"African-\" implies you were born in Africa, \"-American\" imples you then immigrated to America. Elon Musk is an African-American. 13% of the US population are Black Americans. reply dekhn 15 hours agorootparentAre extremely dark-skinned people (for example from South India) who move to england called \"Black\"? I've never heard that and would be surprised but i'm curious. reply Jerrrry 12 hours agorootparentThey would be called black socially, but would be Indian-British til they revealed their accent, I would think. reply tekla 17 hours agorootparentprevnext [13 more] [flagged] hellojesus 15 hours agorootparentWhere is the racism? I only see a question about proper categorization. reply vidarh 15 hours agorootparentThere's an implicit assumption in it, that while I think it might well not have been trying to be offensive can be seen to suggests a black person in the UK would be African. Not only do many of them not see themselves as such because they're born here, and their parents and grandparents might be British and/or born here (my son is mixed, his grandfather on his mothers side was Nigerian and British and born here; he is third generation British by some measure - his mother was born in Nigeria, but holds British citizenship due to her father; if he decides to consider himself African or Nigerian - he has a Nigerian citizenship - that's up to him, but he's born here, to a mother with a British citizenship, and has never set foot in any part of Africa), but another significant proportion of black people here consider themselves Caribbean rather than African, because their ancestry goes back many generations in the Caribbean, and that's where they or their recent ancestors immigrated from. Here, \"forcing\" a categorization of \"African\" on someone will be seen by at least some people as implying they're immigrants, and even when that is actually the case, having the label forced on you is often a prelude to racist sentiments. reply hellojesus 14 hours agorootparentThat all makes sense, but in this case I didn't read any ill intent. All I read was an American asking a categorization question. The immigration status was not relevant to 'African British'. It was simply a byproduct of 1990s/2000s culture where, in the US, \"black\" was not a term you could use without inferring racism. Rather folks were taught to use \"African American\" to mitigate racisim claims. The other comment from hot_gil sums it up well, \"\"\" I promise it's not because we think of people outside the US as American. When I was a kid in the 2000s, we were told never to say \"black\" and to say \"African-American\" instead. There was no PC term in the US to refer to black people who are not American. This has started to change lately, but it's still iffy. \"\"\" There has been very vocal pressure to understand \"lived experiences\". This, to me, qualifies exactly as that and is purely a misinterpretation of the author's intent. reply vidarh 13 hours agorootparentI figured that might be the case, and why I tried to thread softly with the first line of my reply. In Europe in general, the \"where are you really from?\" line of questioning is one most non-white (and quite a lot of white) people will run into, and while it is often used to obscure racism, anti-immigrant sentiment a bigger part of the discussion because it is often the \"first layer\" of a package that will turn out to include racism once you've peeled back the anti-immigration (not always - there are people who have anti-immigrant views who are not racist - the link, I think, rather goes the other direction: most of the racists are also anti-immigrant and uses it as a marginally more 'acceptable' shield against accusations of racism) Hence for many people it becomes important to de-emphasize \"another location\" in how they identify that might imply they somehow don't belong. While for others holding on to a culture that is often a lot closer matters. And so the discourse around labels is very different. reply hellojesus 13 hours agorootparentThank you for the insight. It's really interesting to see the Euro perspective. Moreso considering how I would believe immigration is more common after the establishment of the EU. But I suppose you do have relatively recent major conflicts which may cause resistance to outsiders. As an aside, I once was considering trying to spend some working years in Scandinavia but read that it was likely I would always be kept at an arm's length by the locals since I was non native, regardless of my fluency in the language. As an American, I found it odd considering how heterogenous my social circle was. Maybe totally false or not applicable to urban centers, but I read it from various sources, and it was persuasive enough for me to switch focus to mainland Europe. reply vidarh 4 minutes agorootparentFor Scandinavia and the rest of the Nordics natives get kept at arms length too. Not going to play down the presence of xenophobia as well, but really the Nordic countries can seem very cold on the surface because nobody gets let close until there is a reason to. To the point here are books about how to befriend us. There's this meme[1] of Finns always spacing out at bus stops to avoid invading each other's personal space, for example, but Scandinavia proper is a little better than the Finns in that respect. The Danes maybe a little bit more. The way around that tends to be shared activities. E.g. joining a class, going out with colleagues, or joining various groups where you then have a socially sanctioned reason for talking to people, and people build from that. People who are used to being able to start friendships with just random encounters will often find that frustrating and hard to navigate and wonder why they're blanked or ignored or actively rebuffed when trying to be friendly - it's not you, or where you're from (most of the time), it's that talking to a stranger makes a lot of people instantly wonder what fresh hell this is. It's not that random encounters etc. never lead anywhere in Scandinavia, but it's rarer. For someone who is used to expecting American levels of just randomly talking to people (having been on the receiving ends of that many times when visiting the US: I could never get used to that...), getting used to that might be hard, and basically the further South you go in Europe the less you deal with that. [1] https://www.reddit.com/r/Finland/comments/1494mm/how_to_wait... prepend 15 hours agorootparentprevI think GP was referring to themself. Otherwise their comment makes no sense. reply tekla 15 hours agorootparentprevYou have to be racist to assume that a Black person wants to be called \"African British\" in the UK. If you called my Black friends \"African American\" they would be pretty close to punching you in the face. Why wouldn't it be racist to assume Black people are African. reply hellojesus 14 hours agorootparentWhy is it so offensive? Why not just let the speaker know they weren't American and move on? The instant escalation to violence seems like part of the problem generally in today's society, which extends to non racial topics like politics, gender fluidity, etc. A more appropriate reponse would be something like, \"Why do you think I'm American?\" A simple question like that would likely be sufficient to get the original speaker to think abkut and reorient their world view, and there was good-faith discussion the entire way. reply tekla 14 hours agorootparentMy Black friend isn't African. That's why. They don't give a shit about being called American, they give a shit about being assumed about being African because they are black. reply hellojesus 13 hours agorootparentHence my point about a calm response opposed to escalation to a honest mistake. reply tekla 13 hours agorootparentThe mistake is due to racism. reply BonoboIO 19 hours agorootparentprevElon Musk is a real African American reply Ajay-p 20 hours agorootparentprevThat is not artificial intelligence, that is deliberate mucking with the software to achieve a desired outcome. Google is utterly untrustworthy in this regard. reply Perceval 17 hours agorootparentAI stands for Artificial Ideology reply FlyingSnake 19 hours agorootparentprev> it is well known Greeks not only had contact with the new world but also had prominent population of Native Americans. I’m really surprised to hear this tidbit, because I thought Leif Erickson was then first one from the old world to do venture there. Did Ancient Greeks really made contact with the Native Americans? reply sharpneli 17 hours agorootparentIt was a joke. Obviously there was no contact whatsoever between the two. Gemini basically forces the current US ethnical representation fashions to every situation regardless of how well it fits. reply duxup 15 hours agoparentprevAlso the images are almost bizarrely stereotypical in my experence. The very specific background of each person is pretty clear. There's no 'in-between' or mixed race or background folks. It's so strange to look at. reply prepend 15 hours agorootparentYou mean not all Native Americans wear headdresses everywhere? reply duxup 15 hours agorootparentHaving grown up near a sizable (well proportionally) native American population I can say that they don't! Although it was fun when they did get dressed up for events and sang and danced. It was a great experience, and so much more than . reply Yasuraka 14 hours agorootparentprevA bit by Chappelle on this https://piped.video/watch?v=0XLUrW_4ZMs reply buildsjets 13 hours agorootparentprevThe Village People AI. Gemini specializes in re-creating scenes featuring stereotyped Indian Chiefs, firemen, policemen, and construction workers. reply 809 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google's Gemini AI tool has faced backlash for creating racially diverse Nazis and other historically inaccurate images, drawing criticism for erasing the true history of discrimination.",
      "Following the backlash, Google has issued an apology, paused Gemini's capability to produce AI images depicting individuals.",
      "The incident has ignited discussions on how AI can reinforce stereotypes and the significance of nuanced representation in image creation."
    ],
    "commentSummary": [
      "Google halted generating people images on Gemini over diversity, equity, and inclusion concerns, following user backlash over imposed racial backgrounds and limited control.",
      "Discussions encompass societal, corporate impacts, censorship, historical accuracy, biased search results, forced diversity, and AI model biases, as well as themes like Jesus' portrayal, historical truth, varied racial terminology, and the intricate nature of diversity in tech and media.",
      "Participants explore racism, political motives, extremist views, and the intricate nuances of race and nationality."
    ],
    "points": 615,
    "commentCount": 1079,
    "retryCount": 0,
    "time": 1708597190
  },
  {
    "id": 39471388,
    "title": "Phind-70B: Revolutionizing Code Quality with Speed and Precision",
    "originLink": "https://www.phind.com/blog/introducing-phind-70b",
    "originBody": "Introducing Phind-70B – closing the code quality gap with GPT-4 Turbo while running 4x faster We're excited to announce Phind-70B, our largest and most performant model to date. Running at up to 80 tokens per second, Phind-70B gives high-quality answers for technical topics without making users make a cup of coffee while they wait. We think it offers the best overall user experience for developers amongst state-of-the-art models. Phind-70B is based on the CodeLlama-70B model and is fine-tuned on an additional 50 billion tokens, yielding significant improvements. It also supports a context window of 32K tokens. Phind-70B scores 82.3% on HumanEval, beating the latest GPT-4 Turbo (gpt-4-0125-preview) score of 81.1% in our evaluation. On Meta's CRUXEval dataset, Phind-70B scores 59% to GPT-4's reported score of 62% on the output prediction benchmark. However, neither of these public datasets fully captures how our users use Phind for real-world workloads. We find that Phind-70B is in the same quality realm as GPT-4 Turbo for code generation and exceeds it on some tasks. Phind-70B is also less “lazy” than GPT-4 Turbo and doesn't hesistate to generate detailed code examples. Phind-70B is significantly faster than GPT-4 Turbo, running at 80+ tokens per second to GPT-4 Turbo's ~20 tokens per second. We're able to achieve this by running NVIDIA's TensorRT-LLM library on H100 GPUs, and we're working on optimizations to further increase Phind-70B's inference speed. Phind-70B is available today to try for free and without a login. You can get higher limits by subscribing to Phind Pro. We love the open-source community and will be releasing the weights for the latest Phind-34B model in the coming weeks. We intend to release the weights for Phind-70B in time as well. We'd like to thank our cloud partners, SF Compute and AWS, for helping us get the infrastructure right for training and serving Phind-70B. We'd also like to thank our partners at Meta and NVIDIA for their support. Fun fact: We melted an H100 during Phind-70B's training! Try Phind-70B",
    "commentLink": "https://news.ycombinator.com/item?id=39471388",
    "commentBody": "Phind-70B: Closing the code quality gap with GPT-4 Turbo while running 4x faster (phind.com)505 points by rushingcreek 15 hours agohidepastfavorite232 comments afiodorov 14 hours agoI don't trust the code quality evalution. The other day at work I wanted to split my string by ; but only if it's not within single quotes (think about splitting many SQL statements). I explicitly asked for stdlib python solution and preferrably avoid counting quotes since that's a bit verbose. GPT4 gave me a regex found on https://stackoverflow.com/a/2787979 (without \"), explained it to me and then it successfully added all the necessary unit tests and they passed - I commited all of that to the repo and moved on. I couldn't get 70B to answer this question even with multiple nudges. Every time I try something non GPT-4 I always go back - it's feels like a waste of time otherwise. A bit sad that LLMs follow the typical winner-takes-it-all tech curve. However if you could ask the smartest guy in the room your question every time, why wouldn't you? --- Edit: USE CODE MODE and it'll actually solve it. reply planb 2 hours agoparentI didn't take a look at the code, but to me it sounds quite dangerous to take an implementation AND the unit tests straight from an LLM, commit and move on. Is this the new normal now? reply swman 1 hour agorootparentIt’s the new boot camp dev. It is still the same as copy pasting SO solutions lol reply tietjens 37 minutes agorootparentMean-spirited, gatekeeping comment unless I’ve misunderstood. Reference to AI is frequently used to punch down like this I’ve noticed. reply draxil 1 hour agorootparentprevWhat as in something you should know not to do pretty quickly? reply ogrisel 40 minutes agorootparentprevArguably the tests should be easier to review than the implementation. But if there non-trivial logic in the code of the tests, I agree this is probably a risky approach. reply ugh123 1 hour agorootparentprevPresumably people look at things before committing the code. And code reviews and pull requests are still normal. Blindly copying code from any source and running it or committing it to your main branch without even the slightest critical glance is foolish. reply Xenoamorphous 1 hour agorootparentprevI guess most people would review the code as if it had been written by a colleague? reply m_fayer 58 minutes agorootparentRight on. These days my llm-assisted workflow feels very similar to the 20% of my day that I used to devote to code review, just now it’s more like 60% of my day. reply DougBTX 1 hour agorootparentprevYes, a great way to think of it is as a widely read intern: https://www.oneusefulthing.org/p/on-boarding-your-ai-intern You’ve still got to avoid prompting for questionable code in the first place, eg, splitting SQL statements on semicolons with an ad-hoc regex is going to fail in edge cases, but may be sufficient for a specific task. reply mattlutze 1 hour agorootparentprevIf someone uses an LLM to produce the code, I'd guess they'll use it to evaluate the code as well. reply draxil 1 hour agorootparentThis is the part I actually want from an LLM, I write the code and it spots the problems. A mega linter. Unfortunately it's not very good at this yet. reply willvarfar 4 minutes agorootparentYeap, I want a code-review bot that just says \"this is very improbable; are you sure you didn't mean x instead?\" The old Coverity used to achieve similar results in a different way, spotting probable mistakes based on patterns its heuristics found in the rest of the same codebase. rushingcreek 14 hours agoparentprevThanks for the feedback, could you please post the cached Phind link so we can take a look? It might also be helpful to try Phind Chat mode in cases like this. EDIT: It seems like Phind-70B is capable of getting the right regex nearly every time when Chat mode is used or search results are disabled. It seems that the search results are polluting the answer for this example, we'll look into how to fix it. reply Perseids 38 minutes agorootparentI've tried it with a question which requires deeper expertise – \"What is a good technique for device authentication in the context of IoT?\" – and the Search mode is also worse than the Chat mode: - Search: https://www.phind.com/search?cache=s4e576jlnp1mpw73n9iy4sqc - Chat: https://www.phind.com/agent?cache=clsyev95o0006le08b5pjrs14 The search was heavily diluted by authentication methods that don't make any sense for machine-to-machine authentication, like multi-factor or biometric authentication, as well as the advice to combine several methods. It also falls into the, admittedly common, trap of assuming that certificate based authentication is more difficult to implement than symmetric key (i.e. pre-shared key) authentication. The chat answer is not perfect, but the signal-to-noise ratio is much better. The multi-factor authentication advice is again present, but it's the only major error, and it also adds relevant side-topics that point in the right direction (secure credential storage, secure boot, logging of auth attempts). The Python example is cute, but completely useless, though (Python for embedded devices is rare and in any case you wouldn't want a raw TLS socket, but use it in a MQTTS / HTTPS / CoAP+DTLS stack, and last but not least, it provides a server instead of client, even though IoT devices mostly communicate outbound). reply afiodorov 14 hours agorootparentprevhttps://www.phind.com/search?cache=r2a52gs77wtmi277o0xi4z2a reply rushingcreek 13 hours agorootparentPhind-70B worked well for me just now: https://www.phind.com/agent?cache=clsxokt2u0002ig09n1e11bj9. For writing/manipulating code, Chat mode might work better than Search. reply afiodorov 12 hours agorootparentYou're right! It solved it. I didn't know about the Code/Search distinction. I still struggled for it to write me the unit tests. It does write them, they just don't pass. But this is definitely much closer to GPT4 than I originally thought. reply MaxikCZ 2 hours agorootparentprevNow if we could get an AI that would switch code/search mode on its own reply dsp_person 8 hours agorootparentprevwoah I've been using phind for at least a few months and can't believe I never noticed the \"Chat\" button reply sebstefan 29 minutes agoparentprevCan you try this? \"Can you give me an approach for a pathfinding algorithm on a 2D grid that will try to get me from point A to point B while staying under a maximum COST argument, and avoid going into tiles that are on fire, except if no other path is available under the maximum cost?\" I've never found an AI that could solve this, because there's a lot of literature online about A* and tiles with cost, and solving this requires a different approach reply meindnoch 3 hours agoparentprevDoesn't handle escaped quotes, and the time complexity of that regex is very bad. reply eru 1 hour agorootparentThe time complexity for all matching a string against any fixed regular expression is O(length of string). If you want to talk about constant factors, we need to leave our comfortable armchairs and actually benchmark. [Just to be clear, I am talking about real regular expressions, not Franken-xpressions with back-references etc here. But what the original commenter described is well within the realm of what you can do with regular expressions.] You are right about escaped quotes etc. That's part of why parsing with regular expressions is hard. reply romeros 10 hours agoparentprevit really feels like GPT-4 is Google and Everybody else is Yahoo/Bing. i.e cute but not really reply devjab 3 hours agorootparentGemini is much better than the free version of GPT 3.5 though. At least in my experience. Microsoft’s enterprise co-pilot is also fairly decent. It’s really good at providing help to Microsoft related issues or helping you find the right parts of their ridiculously massive documentation site. Which probably isn’t too weird considering. reply unshavedyak 10 hours agorootparentprevAgreed, though i'm _really_ interested in trying 1M token Gemini. The idea of uploading my full codebase for code assist stuff sounds really interesting. If i can ever get access to the damn thing... reply behnamoh 4 hours agorootparentDon't get your hope high—Google's article mentioned they'll limit it to 128K (at least in the beginning). reply phillipcarter 8 hours agorootparentprevI'm curious how they'll handle this. My understanding is that it takes quite a long time to get an answer, since there's no magic \"semantic database\" built for you behind the scenes. reply l33tman 44 minutes agorootparentThat use-case seems inefficient to solve like that in the long run as well, like if you really would have to use a million tokens to do every small query you require on your data it would be prohibitively costly except doing as an experiment. reply HKH2 5 hours agorootparentprevIn my experience, Bing's image search is way better than Google's. Also, I'm not going to use a search engine that I have to log in or do a captcha for. reply SubiculumCode 4 hours agorootparentusually id say no, but google's results these last months have been terrible reply HKH2 3 hours agorootparentI'm no fan of Microsoft, but Bing's image search has been better for a long time. Google also removed functionality for no apparent reason. reply jeffbee 8 hours ago [flagged]parentprevnext [2 more] > I wanted to split my ... SQL statements ... avoid counting quotes ... GPT4 gave me a regex ... I commited all of that to the repo I see that the future is brighter than ever for the information security industry. reply xyzzy_plugh 7 hours agorootparentSure is! We've got a bright and oh so plentiful road ahead, pending we can avoid blowing up the planet. reply ldjkfkdsjnv 8 hours agoparentprevYup, LLMs broke well known benchmarks reply kunalgupta 14 hours agoparentprevsame exp reply tarruda 3 hours agoprevI don't care much for benchmarks, many models seems to be contaminated just to approach proprietary models in coding benchmarks. I had never tried Phind before, but gave Phind-70B a spin today and so far found it to be really good for coding writing and understanding, maybe even GPT-4 level. Hard to tell for sure since I only tested it on a single problem: Writing some web3 code in typescript. This is what I did: - Gave it some specifications of a react hook that subscribes to a smart contract event and fetches historical events starting from a block number. It completed successfully. - Took this code and gave it to GPT-4 to explain what it did, as well as finding potential issues. GPT gave a list of potential issues and how to address. - Then I went back to the Phind and asked it to find potential issues in the code it had just written, and it found more or less the same issues GPT-4 had found. - Went back to GPT-4 and asked to write a different version of the hook. - Took the GPT-4 written code and asked it to explain the code, which it did successfully (though I think it lacked more details than the GPT-4 explanation of the code written by Phind). I will be testing this more over the next days. If this proves to be in the GPT-4 ballpark and the 70b weights are released, I will definitely replace my ChatGPT plus subscription with Phind Pro. reply WuxiFingerHold 5 hours agoprevNot an expert at all. But just wanted to let the creators know: I've been using Phind almost daily for some months now and it's been awesome. Whenever I accidentally do a web search I recognize what a game changer this is. (ChatGPT probably as well, but never used it.) Last week I was under pressure at work and I used it for stuff like: \"How can i capture output from a command and print it line by line to the console with Rust\", and must say that kind of time and energy savings are very significant. reply sekai 2 hours agoparentDon't even remember when I opened Stack Overflow, won't miss that condescending place. reply the_duke 2 hours agorootparentJust wait for people to stop using SO, at which point the LLMs won't have a high quality training set for new questions, so you won't get good answers from the LLMs anymore... reply hobabaObama 1 hour agorootparentLLMs also train on official documentations which is where 90% of problems get solved. reply m_fayer 55 minutes agorootparentWhat will happen to official docs when it becomes clear that the only thing that reads them are llm-training runs? reply terhechte 47 minutes agorootparentThe LLMs will read the actual source code which is way better than the documentation (as any iOS engineer will tell you). For private codebases the companies can provide custom-trained LLMs. Techniques like \"Representation Engineering\" will at some point also prevent against accidental leakage of private codebase source code. reply sumitkumar 1 hour agorootparentprevThe LLMs are generating training data at a faster rate than SO. All the prompts and the responses will eventually be 99.99% of the training data. reply the_duke 16 minutes agorootparentThe only way this is useful in the context of code is if: * The LLMs have a sufficient \"understanding\" of the request and of how to write code to fulfill the request * Have a way to validate the suggestion by actually executing the code (at least during training) and inspecting the output From what I've seen we are still far away from that, Copilot and GPT-4 seem heavily reliant on very well-commented code and on sources like Stackoverflow reply DSingularity 1 hour agorootparentprevSurely you are joking. You want us to rely on models that are overfit to hallucinated LLM interactions. reply vorticalbox 1 hour agorootparentprevdoes this not create a feed back loop, if you're training data based on things the LLM said? reply tinco 1 hour agorootparentThey're probably generating based on GitHub code. If I were training a code model I'd take a snippet of code, have the existing LLM explain it. Then use the explanation and the snippet for the test data. reply littlestymaar 1 hour agorootparentprevDepends on the language, but many things happen on Discord now (which is very annoying since it's not indexable by search engine and you need to ask the question to get the answer…) reply rushingcreek 5 hours agoparentprevThank you :) reply kekebo 22 minutes agoprevIs there any generalizable measure of how any of these models (or their client implementation) handle code(base) context that's sent along each editing request? For my use cases this seems to be as crucial a measure as the general coding responses per file / selection / request and where implementations like Cody[0], Cursor.sh[1] or aider.chat[2] stand out [0] https://sourcegraph.com/docs/cody/core-concepts/context [1] https://docs.cursor.sh/features/codebase-indexing [2] https://aider.chat/docs/repomap.html reply shafiemukhre 8 hours agoprevAwesome update! I have been using Phind almost daily for the past 3-4 weeks and the code it produces is pretty good and it is runnable on the first try more often compared to ChatGPT. Most of the time the answer is somewhat accurate and points me in the right direction. ChatGPT (with GPT 4) has been slow af for me for the past 2+ months but I like studying a topic using ChatGPT, it is more verbose and explanatory when explaining things to you. Maybe a purpose-built dedicated AI model is the right path. A model that does well in fixing bugs, writing feature code, and producing accurate code will not be a good tool for or conversational studying. And vice versa. Also, I don't like that Phind is not handling the follow-up question that well when there are multiple kinds of questions within the same thread. ChatGPT is good at this. reply rushingcreek 8 hours agoparentThanks for the feedback! Have you tried setting a custom answer profile at https://phind.com/profile? You can tell it to be more explanatory for certain topics. reply shafiemukhre 8 hours agorootparentI haven't actually because Phind is working for me so far whenever I have code-related questions or when I need to refactor my code. TIL that I can customize the answer style preference, will give it a try! reply rushingcreek 14 hours agoprevPhind founder here. You can try the model for free, without a login, by selecting Phind-70B from the homepage: https://phind.com. reply Fervicus 13 hours agoparentI don't use LLMs a lot, maybe once a week or so. But I always pick Phind as my first choice because it's not behind a login and I can use it without giving my phone number. Hopefully you'll keep it that way! reply HKH2 5 hours agorootparenthttps://labs.perplexity.ai is the same and it loads much faster than Phind. reply worldsayshi 10 hours agorootparentprevI don't see how they could. They need to finance it at some point? reply itsTyrion 9 hours agorootparentthey are already financing it, there are 2 paid plans [0]. For THAT, you need an account (but no phone number). [0] https://www.phind.com/plans reply bbor 8 hours agorootparentprevI think there’s room in the market to subsidize real users. Phind delivers absurd value, so I think the majority of paying users could account for the tech-averse or privacy-conscious reply bee_rider 14 hours agoparentprevImportant and hard-hitting question from me: have you ever considered calling yourself the Phinder or the Phiounder? reply fragmede 11 hours agorootparentFind Phounder reply ComputerGuru 7 hours agorootparentprevAnd here I was wondering why this service was called pee-hind! reply bbor 8 hours agorootparentprevPhindational models, phintech, Phinterest, phinder… it might be the best startup name of all time. Hell, startup a password manager and call it Phinders’ Keeper. reply xyzzy_plugh 7 hours agorootparentPour one out for Phabricator. reply Zacharias030 13 hours agorootparentprevor the PhiTO / PhiEO reply carbocation 10 hours agoparentprevIt seems unexpected that other people can edit a link to a Phind chat just by getting the URL. It means that if you share a URL with someone, they can change your results: https://www.phind.com/search?cache=k56i132ekpg43zdc7j5z1h1x reply declaredapple 13 hours agoparentprevAny chances of an API? And are there plans to release any more weights? Perhaps one or two revisions behind your latest ones? reply parineum 13 hours agorootparentAsk phind to make you one that screen scrapes reply goldemerald 14 hours agoparentprevVery nice. I've been working with GPT4 since it released, and I tried some of my coding tasks from today with Phind-70B. The speed, conciseness, and accuracy are very impressive. Subjectively, the answers it gives just feel better than GPT4, I'm definitely gonna give pro a try this month. reply visarga 13 hours agorootparentI prefer Phind's web search with LLM to both Google search and GPT-4. I have switched my default search engine, only using Google for finding sites, not for finding information anymore. GPT-4 might be a better LLM but its search capability is worse, sometimes sends really stupid search keywords that are clearly not good enough. reply bbor 8 hours agorootparentI won’t steal phind’s thunder but kagi is another great modern tool to have, and much more reliable than google for a technical user IMO. Obviously Phind is irreplaceable for complex or chat-based technical questions, but Kagi sees much more use from me daily for syntax stuff, Wikipedia searches, finding and relating papers, etc. reply airgapstopgap 10 hours agoparentprevSince you're here: have you considered moving to other, better generalist base models in the future? Particularly Deepseek or Mixtrals. Natural language foundation is important for reasoning. Codellama is very much a compromise, it has lost some NLP abilities from continued pretraining on code. reply bobbyi 9 hours agoparentprevI'm selecting 70B and it is coming back with \"AnswerPhind-34B Model\". I'm not sure if it's really using the 34B model or if the UI is wrong about which one it used reply anter 9 hours agorootparentYou have to click on the \"Chat\" option at the top left corner, then it'll use the 70B model. I got stuck on that too til I figured that out. reply rushingcreek 9 hours agorootparentprevPlease try logging in in that case, you will still get your 10 free uses. reply shrubble 14 hours agoparentprevI tried a question about Snobol4 and was impressed with what it said (it couldn't provide an exact example due to paucity of examples). When testing more mainstream languages I have found it very helpful. reply acdanger 8 hours agoparentprevHi, when I try to use the 70B model from the homepage, the response indicates that it's using the 34B model. reply rushingcreek 8 hours agorootparentPlease try logging in in that case. You will get 10 free daily 70B uses. reply robbomacrae 10 hours agoparentprevWhy do none of the graphs show the speed difference? That seems to be your biggest advantage and the subject line... reply justaj 8 hours agoparentprevAre you considering adding more non-US payment methods for Phind Pro? reply browningstreet 14 hours agoparentprevHmm, when I try I see this in the dropdown: 0 Phind-70B uses left And I've never made any selection there. reply rushingcreek 14 hours agorootparentI'd suggest logging in in that case -- you will still get your free uses. The Phind-70B counter for non-logged in users has carried over from when we offered GPT-4 uses without a login. If you've already consumed those uses, you'll need to log in to use Phind-70B. reply browningstreet 14 hours agorootparentThanks. reply petesergeant 9 hours agoparentprevThis is good stuff, congrats. Took a little detour, but GPT-4 does too (https://www.phind.com/agent?cache=clsxw1mru0033l908mojpvb3b) reply tietjens 1 hour agoprevI have a question because I do not understand how the models work: Are they able to create code themselves, or does code ALWAYS come from a specific source? I assume that if I ask for a complex sequence in RXJS operators, that comes from the model inferring the code from lots of examples and docs. But if I ask for something really specific that might just come from a stackoverflow article or GitHub repo. The ambiguity about the sourcing is the main thing that makes me itchy about “AI”. reply brandall10 51 minutes agoparentOccasionally I find GPT4 will blur a response indicating it's reproduced from a specific source and will ask me to rephrase my request/question. So at least OpenAI has some safeguard in place to not do that. Have no clue how that behavior is determined or whether or not other providers do similar. reply zettabomb 12 hours agoprevA fun little challenge I like to give LLMs is to ask some basic logic puzzles, i.e. how can I measure 2 liters using a 3 liter and a 5 liter container? Usually if it's possible, they seem to do ok. When it's not possible, they produce a variety of wacky results. Phind-34B is rather amusing, and seems to get stuck in a loop: https://www.phind.com/agent?cache=clsxpravk0001la081cc9dl45 reply hobabaObama 1 hour agoparentI tested this prompt in various LLMs 1. phind was by far the best - gave me solution in just 2 steps 2. Grok was second best - it did arrive at the solution but with additional non-sense step. But the solution was correct. 3. To my surprise GPT-4 could not solve the prompt and in fact gave a wrong answer in 4 steps - \"Now you should have exactly 4 liters in the 5-liter container.\" which is not what I asked 4. As expected Gemini pro was the worst. It asks me to pour completely filled up 3L container into 5L and then you will be left with 2L in 3L container.. LOL that does not even make sense. reply thelittleone 11 hours agoparentprevThese are interesting tests. I wonder how far we are away from AIs solving these (the ones that have no solution) without any special programming to teach them how. reply jamesponddotco 13 hours agoprevI'm impressed with the speed, really impressed, but not so much with the quality of the responses. This is a prompt I usually try with new LLMs: > Acting as an expert Go developer, write a RoundTripper that retries failed HTTP requests, both GET and POST ones. GPT-4 takes a few tries but usually takes the POST part into account, saving the body for new retries and whatnot. Phind in the other hand, in the two or three times I tried, ignores the POST part and focus on GET only. Maybe that problem is just too hard for LLMs? Or the prompt sucks? I'll see how it handle other things since I still have a few tries left. reply shapenamer 12 hours agoparentI'm a human and I don't have the slightest idea what you're asking for. reply Powdering7082 11 hours agorootparentDo you use Go? It makes sense to me reply viraptor 3 hours agorootparentThe RoundTripper throws me off if anything. RetryRequest, RetryOnFailure, anything could be more descriptive. reply minism 1 hour agorootparentIt's an interface in the http package: https://pkg.go.dev/net/http#RoundTripper reply viraptor 1 hour agorootparentTil. Thanks, I hate it. reply rushingcreek 13 hours agoparentprevThanks, can you send the cached link please? I'd also suggest trying Chat mode for questions like this, where there are unlikely to benefit from an internet search. Just tried your query now and it seemed to work well -- what are your thoughts? https://www.phind.com/search?cache=tvyrul1spovzcpwtd8phgegj reply jamesponddotco 13 hours agorootparentHere you go: https://www.phind.com/search?cache=k56i132ekpg43zdc7j5z1h1x I'll give chat mode a try. Didn't see that it existed until now. EDIT Chat mode didn't do much better: https://www.phind.com/agent?cache=clsxpl4t80002l008v3vjqw5j For the record, this is the interface I asked it to implement: https://pkg.go.dev/net/http#RoundTripper reply rushingcreek 12 hours agorootparentThanks for the links. It seems like it switched to Phind-34B, which is worse. Phind-70B seems to be able to get the right interface every time. Please make sure that it says Phind-70B at the top of the page while it's generating. reply dimask 12 hours agorootparentIn the link it says \"Phind-70B\", how do we know if it switched to 34B? reply coder543 11 hours agorootparentThe first link definitely says Phind-34B on my browser. reply dimask 7 hours agorootparentThe second one was definitely saying phind 70b on me. Now it is all messed up though. reply coder543 12 hours agoparentprev“RoadTripper”? Or “RoundTripper”? reply jamesponddotco 12 hours agorootparentOps, haha. Interesting that GPT-4 still got it right though. Phind still forgot about POST, but at least now it got the interface right. https://www.phind.com/search?cache=ipu8z1tb3bnn7nfgfibcix38 reply coder543 11 hours agorootparentI'm not sure what you mean that it \"forgot\" about POST? Even as an experienced Go developer, I looked at the code and thought it would probably work for both GET and POST. I couldn't easily see a problem, yet I had not forgotten about POST being part of the request. It's just not an obvious problem. This is absolutely what I would classify as a \"brain teaser\". It's a type of problem that makes an interviewer feel clever, but it's not great for actually evaluating candidates. Only on running the code did I realize that it wasn't doing anything to handle the problem of the request body, where it works on the first attempt, but the ReadCloser is empty on subsequent attempts. It looks like Phind-70B corrected this issue once it was pointed out. I've seen GPT-4 make plenty of small mistakes when generating code, so being iterative seems normal, even if GPT-4 might have this one specific brain teaser completely memorized. I am not at the point where I expect any LLM to blindly generate perfect code every time, but if it can usually correct issues with feedback from an error message, then that's still quite good. reply xyzzy_plugh 7 hours agorootparentThis isn't a brain teaser at all. It's a direct test of domain knowledge/experience. There are countless well-documented RoundTripper implementations that handle this case correctly. This is the sort of thing you whip up in three minutes and move along. To me it seems like a perfect test of LLMs. I don't need an injection of something that's worse than stackoverflow polluting the code I work on. reply NicoJuicy 10 hours agorootparentprevThat's because it's better at classifying than at generating. Eg. Tree of thoughts, ... reply mike_hearn 11 hours agoprevDo you have an API that could be plugged into https://aider.chat/ ? It's by far the best way to use GPT4 for coding, in my experience, and more speed is exactly what it could use. But it needs an OpenAI compatible API. reply dvno42 6 hours agoparentAider has been great! Really looking forward to seeing a phind and even Gemini 1.5 plugin eventually. Def been a lovely improvement to my workflow. I've been keeping a close eye on Mentat as well but haven't yet tried it. reply sagarpatil 4 hours agoparentprevI asked the founder this question previously and if I remember it correctly, they said they don't have any plans for an API. reply stavros 10 hours agoparentprevOh I love Aider, it's really well done. reply aussieguy1234 9 hours agoparentprevAider looks interesting. I wrote my own similar console based chatbot reply Cebul1234 47 minutes agoprevYou bought me :) The only missing feature is mobile app. reply SethTro 15 hours agoprev> Phind-70B is significantly faster than GPT-4 Turbo ... We're able to achieve this by running NVIDIA's TensorRT-LLM library on H100 GPUs reply kkielhofner 14 hours agoparentAs someone who has utilized Nvidia Triton Inference Server for years it's really interesting to see people publicly disclosing use of TensorRT-LLM (almost certainly in conjunction with Triton). Up until TensorRT-LLM Triton had been kind of an in-group secret amongst high scale inference providers. Now you can readily find announcements, press releases, etc of Triton (TensorRT-LLM) usage from the likes of Mistral, Phind, Cloudflare, Amazon, etc. reply brucethemoose2 13 hours agorootparentBeing accesible is huge. I still see post of people running ollama on H100s or whatever, and that's just because its so easy to set up. reply jxy 3 hours agoparentprevHow many H100 GPUs does it take to serve 1 Phind-70B model? Are they serving it with bf16, or int8, or lower quants? reply tarruda 2 hours agorootparentThis video [1] shows someone running at 4-bit quant in 48gb VRAM. I suspect you need 4x that to run at full f16 precision, or approx 3 H100. https://www.youtube.com/watch?v=dJ69gY0qRbg reply gtirloni 7 hours agoprevThis is from the Phind extension for VS Code: > Use the input box at the bottom to ask questions. Phind will automatically use your codebase to answer I don't know why I can't get GitHub Copilot Chat extension to do this. It always replies it can't answer questions about the codebase and that I should ask it to do something. Is that even possible? I've tried @workspace but I didn't work. I must be doing something wrong. reply thomasfromcdnjs 7 hours agoparentI'd piggyback this comment to ask if anyone could share how codebase prompts work? Given the max tokens per request, do the extensions look at your currently open file, and use some vector similarity to find other files that could be relevant (if embeddings were generated for all files in the project), and then inject relevant source. And/or is it even more complex, by using AST parsing and creating embeddings out of actual linked functions? reply sagarpatil 4 hours agorootparentThere are YouTube videos that go into detail. From what I can remember, it first creates an embedding of your full code, it then refers to your open file and the files next to your current tab, it then extracts the most useful code related to your question. reply ipsum2 14 hours agoprevWhat's the story behind the melted h100? I've been having down clocking issues when using fp8 because of thermals as well. reply rushingcreek 14 hours agoparentWe noticed that the training run crashed because one of the GPUs fell off the bus. Power cycling the host server didn't help and diagnostics showed thermal damage. We were able to swap in a different node, but apparently the entire host server needed to be replaced. We've generally noticed a relatively high failure rate for H100 hardware and I'm not quite sure what is behind that. reply ipsum2 14 hours agorootparentThe entire server? That's crazy. Are you doing FP8 training or did you encounter this with BF16? reply davidzweig 11 hours agorootparentprevCheck PLX chips are getting enough airflow, assuming you have them? reply alecco 13 hours agoparentprevFWIW, 4090 has fp8 throttling issues: https://forums.developer.nvidia.com/t/ada-geforce-rtx-4090-f... reply EmilStenstrom 12 hours agoprevContrary to many other models I've tried, this one works really well for Swedish as well. Nice! reply brucethemoose2 14 hours agoprevI have not had luck with codellama 70B models for coding, nor have I had it with the mistral leak. If I were Phind, I'd be looking at Deepseek 33B instead. While obviously dumber for anything else, it feels much better at coding. Its just begging for a continued pretrain like that, and it will be significantly faster on 80GB cards. reply rushingcreek 14 hours agoparentWe've found that CodeLlama-70B is a much more capable base model than DeepSeek-33B. I'd love to hear your feedback on Phind-70B specifically. reply brucethemoose2 14 hours agorootparentYeah I will have to test it out, though TBH I am more inclined to run models locally. As I mentioned, being such an extensive continuation train can (sometimes) totally change the capabilities of a model. reply shapenamer 14 hours agoparentprevAfter running a bunch of models on my own PC (a pretty good one), I have to say by FAR the best results for coding has been with Deepseek models. However, I just spent 20 minutes playing with this Phind 70B model and it's totally nailing the questions I'm asking it. Pretty impressed. reply johnfn 14 hours agoparentprevIs this related to the post? Phind has introduced their own model. Codellama 70B isn't related to Phind's model, other than presumably the \"70B\" size. reply rushingcreek 14 hours agorootparentPhind-70B is an extensive fine-tune on top of CodeLlama-70B reply brucethemoose2 14 hours agorootparentYeah, and I'd go so far as to call it a continued pretrain with that many tokens. More like a whole new model than a traditional finetune. reply rickette 14 hours agoparentprevDeepseek 33B is great. Also runs well on a modern (beefy) MBP. reply mewpmewp2 14 hours agoparentprevDoes this run on 4090 16gb vram? What's best that can run fast on 4090 laptop? reply brucethemoose2 14 hours agorootparentYour options are: - Hybrid offloading with llama.cpp, but with slow inference. - Squeezing it in with extreme quantization (exllamav2 ~2.6bpw, or llama.cpp IQ3XS), but reduced quality and a relatively short context. 30B-34B is more of a sweetspot for 24GB of VRAM. If you do opt for the high quantization, make sure your laptop dGPU is totally empty, and that its completely filled by the weights. And I'd recommend doing your own code focused exl2/imatrix quantization, so it doesn't waste a megabyte of your vram. reply imranhou 10 hours agoprevI needed to write a wireshark plugin, see comparison below: https://www.phind.com/agent?cache=clsxvs9vl000xjx084hgx736r Compare that to, https://chat.openai.com/share/ea0a4fdf-f0d7-4de2-9212-d85b9c... No guarantees this works but certainly seems more helpful knowing some of the functions reply forgotusername6 2 hours agoparentDid someone edit your chat? The phind link now contains \"why can we edit this\". reply raylad 8 hours agoparentprevWhat's your chatGPT prompt, just what's shown or do you have a longer one? It seems to be doing much better with code generation than it does with my prompts. reply imranhou 5 hours agorootparentHere is a link to it. https://chat.openai.com/g/g-yqd1ORCYV-step-coder reply j_bum 2 hours agorootparentThanks for sharing, this is extremely useful and impressive. Would you be willing to share your instructions prompt? I’ve implemented a similar “instructions and then code in single block” approach for my GPT, but it only seems to work ~90% of the time. Here’s a link to the instructions prompt I use: https://github.com/JacobBumgarner/RosaGPT/blob/main/system_p... reply JanSt 2 hours agoprevThis is much better than expected. Switching to chat is also making it feel better for me. I will compare it to GPT-4 in coding tasks over the next month and may switch after that. reply simplyinfinity 11 hours agoprevI just tried this.. It's a bit more lazy than chatgpt 3.5/4 which sometimes go ahead and translate a Go file to C# in full. Most times they omit most of the logic because \"it's too complex\" \"it would require extensive resources\". Phind is no different, but it entirely refuses to do entire code translation. https://www.phind.com/agent?cache=clsxrt4200001jp08wwi55rm1 reply computerex 13 hours agoprevPhind makes impressive claims. They also claimed that their fine tune of codellama beat gpt4, but their finetune is miles behind gpt4 in open domain code generation. Not impressed. Also this is a closed walled garden model. reply dilo_it 12 hours agoprevWeirdly enough, when I asked \"give me a formula for the fourier transform in the continuous domain\" to the 70B model, it gave me a latex-like formatted string, while when asked for \"give me pseudocode for the fft\" I got a nice code snippet with proper formatting. The formulas though were both correct. We're not at Groq level of speed here, but I have to say, it looks pretty good to me. cache=uyem9mo96tjeibaeljm1ztts for the devs if they wanna look it up. reply devinprater 13 hours agoprevCan we get a few accessibility fixed? The expandable button after the sign in button and the button after that are unlabeled. The image on the heading at level 1 has no Alt-text. The three buttons after the \"Phind-34B\" button are not labeled. The ones between that and the suggestions. On search results, there's an unlabeled button after each one, followed by a button labeled something like \" search cache=tbo0oyn4s955gf03o…\". There's probably more, but hopefully that should get things started if you can fix these. reply bakkoting 11 hours agoparentPhysician, heal thyself! https://www.phind.com/agent?cache=clsxs6doj000wl008yk8wb4k8 It pointed out the lack of alt-text as well as a couple other issues. Some of the suggestions aren't applicable, but it's not bad as a starting point. reply kristianp 14 hours agoprevAny Sublime Text plugin? I can't stand how distracting VS code is. reply jsmith12673 14 hours agoparentRare to find a fellow ST4 user these days reply bigstrat2003 14 hours agorootparentFellow ST4 user checking in. It does everything VSCode does (minus remote development, which I don't need) with 1/4 of the resource usage. Just a quality piece of software that I'll keep using for as long as I can. reply mmmuhd 13 hours agorootparentDoes SFTP + Git on ST4 not count as remote development? Cause i am using them as my remote development stack. reply pphysch 10 hours agorootparentprevSublime has devcontainer support? reply andai 8 hours agorootparentprevThere are dozens of us! Though for serious work I'll sometimes reluctantly switch to VSCode due to Sublimes language integrations always feeling hacked on. And lately Sublime has been mysteriously freezing and crashing my other programs (though it might be Windows' fault, unclear) so I've reluctantly started developing my own editor... reply anonymous344 12 hours agorootparentprevYou guys have ST4?? I'm still with 3 because that's what I paid for..as an \"lifetime licence\" if remembering correctly reply madhato 8 hours agorootparentprevI use it everyday and have no desire to switch to vscode. reply arbuge 13 hours agorootparentprevWe’re here. reply Alifatisk 14 hours agoparentprevMy config of vscode made it as minimalistic as sublime. reply vasili111 14 hours agorootparentDid VScode became also more responsive? reply mewpmewp2 14 hours agorootparentVSCode used to be great, but now it feels garbage, or was it garbage all the time? I used it because it was faster than WebStorm, but WebStorm was always just better. Now it seems VSCode is as slow as WebStorm, but is still garbage in everything. reply vasili111 14 hours agorootparentI use VSCode for Python programming with Python for data science related tasks (never used for web design). I especially like Python interactive mode: https://code.visualstudio.com/docs/python/jupyter-support-py It will be interesting to hear from other people why they do not like VSCode for data science related tasks. reply beeburrt 14 hours agorootparentprevI wonder if [VSCodium](https://vscodium.com/) suffers from same issues reply andai 8 hours agorootparentprevThey recently made it so you can drag tabs into their own windows (the issue was open for a decade), which makes it actually a respectable editor (despite the startup lag). reply Alifatisk 11 hours agorootparentprevI wouldn’t say so, it’s still bloated but it’s hidden. The only change is that the ui is very minimal, like sublime. My extensions is still there and I can access everything through shortcuts or the command palette. reply DoesntMatter22 14 hours agoparentprevOut of curiosity how do you find it to be distracting reply kristianp 9 hours agorootparentThings moving, such as plugins updating. little lines in code files telling you when the code was changed, etc. reply habibur 6 hours agoprev> Fun fact: We melted an H100 during Phind-70B's training! Don't these cards have internal temperature control, that will shut it down before burning? reply jrks11o 10 hours agoprevAwesome! I’ve been using phind a little over a year now since it was originally posted on HN. I prefer it over gpt. I’ve run into some weird issues where answers just loop or repeat after really long question threads. I can’t recall model that was being used but I’ll try and find some cached links I can share! reply nerdo 13 hours agoprev> Phind-70B is also less \"lazy\" than GPT-4 Turbo and doesn't hesistate to generate detailed code examples. OpenAI's leaked prompt literally encourages it to try harder[1]: > Use high effort; only tell the user that you were not able to find anything as a last resort. Keep trying instead of giving up. 1: https://pastebin.com/vnxJ7kQk reply rushingcreek 13 hours agoparentYep, LLMs are wacky. Telling Phind-70B to \"take a deep breath\" helps it answer better! reply hamilyon2 13 hours agoprevImpressive, it solved puzzles gpt-4 struggled with with some prompting reply rushingcreek 13 hours agoparentThanks! Can you send the cached link? reply visitor4712 13 hours agoprev\"summary of plato's politeia\" the answer was good. two follow up answers were also fine. just curious: what about the copyright status of the given sources? the best result I received so far was with MS Bing app (android). had reasonable results with my local llama2 13B. cheers reply imglorp 12 hours agoparentPhind is for developers. Wouldn't you rather it grok documentation than philosophy? reply littlestymaar 12 hours agoparentprevPlato being dead around 2300 years ago, and two millennia before copyright was invented, I think it's going to be fine ;). reply mkl 12 hours agorootparentTranslations can be copyrighted. reply littlestymaar 1 hour agorootparentThey can be, but like with everything copyright-related for copyright to apply there need to be “creative work” involved. Which, for something that has been translated countless times in all possible directions, is going to be much harder than for a first translation. reply jameswlepage 13 hours agoprevIs there any API? Would love to plug it into our pipeline and see what happens reply tastyminerals2 14 hours agoprevI used to use Phind for couple of months. I liked the UI improvements but the slow limited free GPT4 and fast lackluster Phind model turned me off. I tried Bing and it wasn’t worse, had more free searches per day. reply devit 7 hours agoprevHow is this possible? GPT-4 is supposed to be 8*220B = 1.7T parameters, so it seems unexpected that a 70B model can beat or match it unless it's somehow a much better algorithm or has much better data. reply schopra909 10 hours agoprevThis is really impressive — excited to play around with it. Congrats on the launch! reply dimask 7 hours agoprevPhind is great. I hope now they release their latest 34b finetune weights as they did with one of the first versions. reply jodevelops 3 hours agoprevCame here to say this: I try to stay away from Google's products and have been using phind and perplexity for the last couple of months. I have to say I am impressed with what you guys are doing and keep up the good work reply pama 14 hours agoprevEvery day now there are new AI models especially LLMs, which might warrant some consideration from a wide part of the human population. In a couple years we will have multiple new announcements per hour and we might need some earlier models to evaluate these new developments and test them. For Phind-70B in particular, I hope that lmsys will share a version that will be part of the human evaluation leaderboard so we get a rounded evaluation. But for code assistants there should be a totally separate impartial evaluation benchmark, ideally still human judged for another year or so but eventually maybe some way of having the models fighting out competitive coding battles that they can help create. reply swatcoder 14 hours agoparent> In a couple years we will have multiple new announcements per hour Models are research output. If 10 new models are being announced every day in a couple years, it would mean that generative AI research has failed to stabilize and produce a stable, reliable component ready for product engineering. And if that's where we are in a couple years, that's almost certainly a sign that the hype was misplaced and that money is chasing after itself trying to recoup sunk costs. That's a failure scenario for this technology, not what an AI-optimist (you otherwise seem to be one) should be anticipating. reply pama 5 hours agorootparentI referee for a lot of the top machine learning conferences and yes I am very optimistic about AI and its impact on humanity. The amount of exciting new papers in machine learning and AI was definitely on an exponential rise for a decade since about 2012 or so, and the total production has kept increasing even during the last couple of years when the submissions in some top annual conferences exceeded 10k. Not every paper results in a useable model but a higher fraction of papers come with code and pretrained weights over time. Many of these papers will never be read by many more than the reviewers and the group who wrote them and a couple friends, but it does not speak necessarily to the quality of the work itself or the potential impact it could have on every possible future if we found better ways to separate the useful information. As the exponential increase in total compute becomes more widely accessible there are exponentially more applications that are of broader interest and will have even bigger impact than nowadays. I don’t think that the model of reviewing 10s or 100s of thousands of papers in conferences, or playing the popularity contest on social media is going to be productive so we need better methods for advancing the useful ideas more quickly. (Case in point: the mamba state space model by Gu and Dao was rejected from a conference this winter, but it happened to be advertised enough at a keynote presentation by Chris Re with a packed audience at neurIPS23, so the model was picked up by a lot of people who used it and submitted applications that used it to the ICML conference already.) I also don’t think that some of the biggest companies have enough manpower, motivation and interest in going alone, though of course they can easily stay ahead of the game in specialized areas with their own resources. reply int_19h 14 hours agorootparentprevThat doesn't follow at all. It just means that there are still low-hanging fruits to pursue for better (smarter, faster, larger context etc) new models, but it doesn't say anything about the stability and usefulness of existing models. reply nickpsecurity 14 hours agorootparentprevThat’s not true. Both good science and market-driven engineering favor continued iterations on existing ideas looking for improvements or alternatives. We’re often exploring a giant space of solutions. Unlike many fields, the A.I. people are publicly posting many of their steps in this journey, their iterations, for review. While it brings lots of fluff, such openness dramatically increases innovation rate compared to fields where you only see results once or twice a year. Both people using cloud API’s and FOSS developers are steadily increasing effectiveness in both experimentation and product development. So, it’s working. reply ilove_banh_mi 13 hours agoparentprevthis is how the WWW started, one new website every other day, then a couple every few hours, then ... reply goatlover 9 hours agorootparentDifference being the web was meant to grow as hyperlinked documents, not separate programs. It's not the same kind of thing. LLMs are more like apps being produced by different companies trying to capture walled gardens, and their open source counterparts. reply lagniappe 13 hours agoprevI chose 70B and gave it a code task, and it answered as Phind-34B. This was my first query. Did I trip a limit or do something wrong? reply rushingcreek 13 hours agoparentTry logging in please if that's the case. reply lagniappe 12 hours agorootparentThank you for the reply, I'd like to congratulate you on the release, first. I'm a bit of a minimalist with regard to signups, unfortunately, so unless this is a known limit then I'd likely just spectate the thread and be happy for you from a distance. reply fsniper 14 hours agoprevI tried the model and asked it to write a kubernetes operator with required DockerFiles, Resources, application code.. Asked it to migrate application to different languages. It looks like it's pretty capable and fast. It is impressive. reply renewiltord 14 hours agoprevAnyone tried Phind Pro? The benchmarks are never useful to compare things. I think they're kind of overfit now. reply rushingcreek 14 hours agoparentPhind founder here. You can try the model for free, without a login, by selecting Phind-70B from the homepage: https://phind.com. reply cl42 14 hours agorootparentJust tried it out with a Python query. So nice and fast. Great work! reply unshavedyak 14 hours agorootparentprevinteresting, i can't try Phind-70b. It says i have 0 uses of Phind-70b left. Context: I used to be a Phind Pro subscriber, but I've not used Phind in probably two months. reply vasili111 14 hours agorootparentTry in browser with Incognito mode? reply unshavedyak 14 hours agorootparentYup, that works (10 uses avail). Though i wasn't too concerned with actually using it, just thought it was interesting and wanted to expose that maybe-bug. reply karmasimida 14 hours agoprevHumanEval can be skipped at this point ... reply jasontlouro 2 hours agoprevAPI? reply sergiotapia 13 hours agoprevTerrific stuff. I always enjoy using Phind for dev related questions. Is it possible the chat history gets some product love? I would like to organize my conversations with tags, and folders. Make it easier to go back to what was said in the past instead of asking the question again. Thanks! reply bugglebeetle 14 hours agoprevI understand why they’re doing this from a cost and dependency perspective, but I’ve pretty much stopped using Phind since they switched over to their own models. I used to use it in the past for thing like API docs summarization, but it seems to give mostly wrong answers for that now. I think this is mostly a “RAG doesn’t work very well without a very strong general model parsing the context” problem, which their prior use of GPT-4 was eliding. reply rushingcreek 14 hours agoparentPhind founder here. Thanks for the feedback -- I'd love to hear your thoughts on this new model. You can try it for free, without a login, by selecting it from the homepage: https://phind.com. reply int_19h 13 hours agorootparentI don't know about coding specifically, but its ability to solve logical puzzles is certainly vastly inferior to GPT-4. Have a look: https://www.phind.com/agent?cache=clsxnhahk0006jn08zjvcgc9g https://chat.openai.com/share/ec5bad29-2cda-48b5-9aee-da9149... reply bugglebeetle 13 hours agorootparentprevI just tried using the 70B model and the answer was listed as being returned using the 34B model instead of the 70B model and was wrong. Is there some logic that ignores user choice, depending on what the service thinks can be answered? reply dingnuts 14 hours agoparentprevI used it for awhile and it was pretty good at Bash or Emacs Lisp one-liners but it was wrong often enough that it was faster to just search on Kagi for the information that I want first, instead of performing N searches to check the answer from Phind after querying Phind. reply behnamoh 14 hours agoprevIn other words: \"our 70B finetune is as good as a 8x200B model\" Yeah, right. reply minimaxir 14 hours agoparentThe one thing we've learnt from the past few months of LLM optimization is that model size is no longer the most important thing in determining LLM quality. A better training regimen and better architecture optimizations have allowed smaller models to push above their weight. The leaderboard has many open 7B and 13B models that are comparable with 72B models: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderb... reply behnamoh 14 hours agorootparent> The leaderboard has many open 7B and 13B models that are comparable with 72B models: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderb... I follow your posts and comments here so I'm surprised you say that. The leaderboard at this point is pretty pointless. Lots of ways to \"cheat\" and get higher ranking there. I do agree that smaller models have made significant progress, but somethings you can't just solve without adding #parameters and FLOPs. Not to mention, ctx_window is an important factor in code quality, but most OSS models (including llama 2) have pretty limited ctx, despite methods like grp and yarn. reply minimaxir 14 hours agorootparentIt's more a comment on the capabilities of smaller models, the quality of output outside of benchmarks is always subjective and you'd need something like Chatbot Arena (https://chat.lmsys.org/) to evaluate it more quantitatively. Even after filtering out the common cheat techniques like merges, there are still 7B and 13B near the top, but yes it's still possible to train models on the evaluation datasets without decontamination. If you look at the Chatbot Arena leaderboards there are still decently-high ELOs for 7B models. reply visarga 13 hours agorootparentI evaluated many Mistrals for an information extraction task and the merged models were much better than direct fine-tunes. About 5% better. reply ein0p 14 hours agorootparentprevIt kinda is, if you want not just performance on synthetic benchmarks but a good coverage of the long tail. This is where GPT4 excels, and also why I pay for it. Transformers are basically fancy associative memories. A smaller model, much like a smaller search index, will not be able to contain as much nuanced information for some hard, immutable, information theoretic reasons. reply brucethemoose2 14 hours agorootparentprevI agree... Except for the leaderboard. Its all but useless, not just because of the data contamination/cheating but because the benchmarks themselves are flawed. They are full of ambiguity/errors, and they dont even use instruct formatting. reply ignoramous 14 hours agorootparentprevI've found that GPT4 (via GitHub Copilot) and Gemini models are better at code tasks like reviewing for logical and functional errors, reasoning about structure and test/edge cases, and refactoring. Gemini is capable of devouring some very large files I've thrown at it. Phind at times is hampered by whatever it is they're doing in addition (RAG?). It is still phenomenal, though. I regularly find myself using Phind to grok assembly code or learn Typescript. reply sroussey 14 hours agorootparentHow do you know that copilot is using gpt4? I pay for it and for chatGPT and I find copilot much worse. reply ignoramous 13 hours agorootparentLooks like Copilot may use GPT4 or GPT3.5 depending on as of yet unpublished criteria: https://github.com/microsoft/vscode-copilot-release/issues/6... For code review, I tend to engage Copilot Chat which probably uses GPT4 more often? https://github.com/orgs/community/discussions/58059#discussi... reply SirMaster 14 hours agorootparentprevBut what if you apply the same level of optimization, same training regimen to the larger models? reply rushingcreek 14 hours agoparentprevPhind-70B is a specialist model, unlike GPT-4. It optimizes for a different function than GPT-4 and therefore needs fewer parameters to learn it. It's also true that specialist models still need to be sufficiently large to be able to reason well, but we've observed diminishing returns as models get larger. reply CuriouslyC 14 hours agoparentprevI mean, it could be as good or better at a lot of reasoning related tasks and just have less baked in general knowledge, in which case it'd make an amazing RAG model if the context length is reasonable. reply google234123 14 hours agoparentprevI'm not sure GPT 4 is still 8x200B reply Eisenstein 13 hours agoprevSo far only GPT4 and mistral-next have answered this question correctly. * https://www.phind.com/search?cache=rj4tpu6ut0jyzkf876e2fahh The answer is 'lower' because the weight of the ball as a volume of water is larger than the volume of the ball. reply rushingcreek 13 hours agoparentPhind-70B can get this too: https://www.phind.com/search?cache=b7w0rt4zybaajbsogatrb7q6. reply sp332 6 hours agoparentprevSomeone overwrote your answer with a PSA about how unsafe these links are. Fair enough I guess, but could you post the original question here? reply mdekkers 4 hours agorootparentI was considering signing up for the pro plan. Now I won’t even give them my email. I tried the model and it is genuinely nice, but this is a huge red flag. reply atemerev 13 hours agoprevImpressive on my tests, excellent work! Indeed, it is better than GPT-4 for coding-related activities. I suppose you are not releasing the weights, right? Anyway, good luck! I hope investors are already forming a nice queue before your door :) reply rushingcreek 13 hours agoparentThanks for the feedback :) We will eventually release the weights. reply atemerev 13 hours agorootparentWow, thanks! reply eurekin 1 hour agoprevI think we need a lot better benchmarks in order to capture the real complexity of typical day to day development. I gave it my typical CI bootstrapping task: > Generate gitlab ci yaml file for a hybrid front-end/backend project. Fronted is under /frontend and is a node project, packaged with yarn, built with vite to the /backend/public folder. The backend is a python flask server built with poetry. The deployable artifact should be uploaded to a private pypi registry on pypi.example.com. Use best practices recommended by tool usage. and it generated scripts with docker run commands [1]: install_dependencies: stage: install script: - docker run --rm -v $(pwd):/app -w /app/frontend node:14 yarn install - docker run --rm -v $(pwd):/app -w /app/backend python:3.8 poetry install build_frontend: stage: build script: - docker run --rm -v $(pwd):/app -w /app/frontend node:14 yarn build build_backend: stage: build script: - docker run --rm -v $(pwd):/app -w /app/backend python:3.8 poetry build deploy_artifact: stage: deploy script: - docker run --rm -v $(pwd):/app -w /app/backend python:3.8 poetry publish -r $PYPI_URL --username $PYPI_USER --password $PYPI_PASSWORD only: - master Which, feels more like \"connect the dots\" or a very rough sketch, that might end up completely replaced. Commands in general seem ok (yarn install && yarn build, poetry build && poetry publish), but the docker run could be better expressed simply as a \"image: \" attribute of each job. I asked about that and I've been given general \"why docker is useful\" non-answer. It also introduced a parallel build stage: frontend and backend are built at the same time, but in my question, I deliberately introduced serial dependency: the frontend code goes into the backend project. The parallel approach would be of course better, if it would correctly construct the end artifact before uploading, but it doesn't do so. Also a bit surprisingly, node install and poetry install could actually run in parallel as-is, but the generated code runs serially. It uses outdated versions of tools. Python 3.8 seems still ok and used in many online examples due to some compatibility quirks with compiled libraries, but node 14 is more than 3 years old now. Current node LTS is 20. For comparison, here's the chatgpt4 version [2] : prepare: stage: prepare image: python:3.9 script: - apt-get update && apt-get install -y nodejs npm - npm install --global yarn - cd frontend && yarn install - cd ../backend && poetry config virtualenvs.create false && poetry install build-frontend: stage: build-frontend image: node:latest script: - cd frontend - yarn install - yarn build --outDir ../backend/public build-backend: stage: build-backend image: python:3.9 script: - cd backend - poetry install --no-dev package: stage: package image: python:3.9 script: - cd backend - poetry build artifacts: paths: - backend/dist/* deploy: stage: deploy image: python:3.9 script: - pip install twine - cd backend - twine upload --repository-url $PYPI_REPOSITORY_URL -u $PYPI_USERNAME -p $PYPI_PASSWORD dist/* only: - main Not perfect, but catches a lot more nuance: - Uses python as base image, but adds the node to it (not a big fan of installing tools during build, but at least took care of that set-up) - Took care of passing the artefacts built by the frontend; explicitly navigates to correct directories (cd frontend ; ... ; cd ../backend) - --no-dev flag given to `poetry install` is a great touch - Added \"artifacts: \" for good troubleshooting experience - Gave \"only: main\" qualifier for the job, so at least considered a branching strategy - Disabled virtualenv creation in poetry. I'm not a fan, but makes sense on CI I would typically also add more complexity to that file (for example using commitizen for releases) and I only feel confident that gpt4 won't fall apart completely. EDIT: Yes, gpt4 did ok-ish with releases. When I pointed out some flaws it responded with: You're correct on both counts, and I appreciate your attention to detail. Links: - [1] https://www.phind.com/agent?cache=clsye0lmt0019lg08bg09l2cf - [2] https://chat.openai.com/share/67d50b56-3b68-4873-aa56-20f634... reply samstave 11 hours agoprevMay you please. PLEASE post as to how the chat option was polluting stuff, and the pipeline of whatever made that happen. Make this less opaque. (actually just post how pollution happens, as well as a definition to pollution as pertains to such. Diminishing trust is at stake. reply satellite2 12 hours agoprev [–] > We love the open-source community and will be releasing the weights for the latest Phind-34B model in the coming weeks. We intend to release the weights for Phind-70B in time as well. I don't understand the utility of this comment? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Phind-70B is a high-performance model aimed at enhancing code quality, surpassing GPT-4 Turbo in speed and user experience for developers.",
      "It outperforms GPT-4 in specific tasks and provides detailed code examples quickly, derived from the CodeLlama-70B base but fine-tuned for better performance.",
      "The model is freely accessible for trial, with the option to access higher limits through Phind Pro, offering developers a valuable tool for coding efficiently."
    ],
    "commentSummary": [
      "The discussion revolves around utilizing AI models such as GPT-4 Turbo and Phind-70B for code quality assessment and implementation, debating their advantages and drawbacks in coding tasks.",
      "Users explore challenges with regex in coding, compare the efficiency of different search modes, and discuss the potential of AI models for code generation and logic problem-solving.",
      "Concerns are voiced about the quality of generated code, link safety, and the progress disparity between small and large AI language models, emphasizing the performance of AI tools like Copilot and Phind in coding practices."
    ],
    "points": 505,
    "commentCount": 232,
    "retryCount": 0,
    "time": 1708628063
  },
  {
    "id": 39465230,
    "title": "Intuitive Machines' Lunar Lander Successfully Touches Down on Moon",
    "originLink": "https://www.intuitivemachines.com/im-1",
    "originBody": "IM-1 MISSION Im-1 landing coverage UPDATES IM-1 Landing Time Update Two Flight controllers chose to exercise an additional orbit before starting the IM-1 Mission landing sequence. The new anticipated landing time is 1724 CST. We expect the landing stream to start on the IM-1 web page and NASA TV at 1600 CST. The content on both streams is identical. 2/22/24 1330 CST IM-1 Landing Time Update Flight controllers commanded a lunar correction maneuver to raise Odysseus' orbit overnight and updated the anticipated landing time to 1524 CST. We expect the landing stream to start on this IM-1 web page and NASA TV at 1400 CST. The content on both streams is identical. 2/22/24 1030 CST Terrain Relative Navigation Image Update Odysseus’ Terrain Relative Navigation camera captured this image of the Bel’kovich K crater in the Moon’s northern equatorial highlands. It is an approximate 50 km diameter crater with mountains in the center, made when the crater was formed. 2/21/24 1750 CST Landing Trajectory Update Flight controllers analyzed the post-Lunar Orbit Insertion engine burn data and updated the anticipated flight maneuver timing, including an expected 1630 CST landing opportunity. The landing opportunity will be Odysseus’ hardest challenge yet. The lander continues to be in excellent health, orbiting approximately 92 km above the lunar surface. 2/21/24 1645CST Lunar Orbit Image Update Odysseus passes over the near side of the Moon following lunar orbit insertion on February 21st. The lander continues to be in excellent health in lunar orbit. 2/21/24 1608 CST IM-1 Mission Completes Lunar Orbit Insertion and Enters Lunar Orbit Odysseus completed its scheduled 408-second main engine lunar orbit insertion burn and is currently in a 92 km circular lunar orbit. Initial data indicates the 800 m/s burn was completed within 2 m/s accuracy. After traveling over 1,000,000 km, Odysseus is now closer to the Moon than the end-to-end distance driving across Space City, Houston, TX. Over the next day, while the lander remains in lunar orbit, flight controllers will analyze the complete flight data and transmit imagery of the Moon. Odysseus continues to be in excellent health. We expect to continue to provide mission updates at least once a day on X and the IM-1 Mission web page, where we intend to host a live stream for landing coverage. 2/21/24 0920CST Planned Trajectory Correction Maneuvers Complete Ahead of Lunar Orbit Insertion Intuitive Machines flight controllers commanded the IM-1 mission’s second planned Trajectory Correction Maneuver (TCM) with enough precision to eliminate the need for the initially planned third TCM engine firing. Today’s TCM fired at 1400 CST for 8 seconds, and it is the final maneuver before Odysseus’ largest challenge to date, Lunar Orbit Insertion (LOI), scheduled for February 21, 2024. Odysseus continues to be in excellent health and is approximately 68,000 km from the Moon. Over the next several hours, flight controllers will continue to analyze the flight data ahead of LOI. We expect to continue to provide mission updates at least once a day on X and the IM-1 Mission web page, where we intend to host a live stream for landing coverage. 2/20/24 1600CST IM-1 Landing Time Update The IM-1 mission to land on the Moon has triumphed over numerous challenges, showcasing exceptional resilience, innovation, and teamwork. Intuitive Machines expects to land on the Moon at 1649 CST on Thursday, February 22nd. 2/19/24 2100CST IM-1 Engine Maneuvers Update Flight controllers received and analyzed data from the February 16th engine Commissioning Maneuver (CM). Data from the 21-second full-thrust mainstage engine CM confirmed Odysseus hit its 21 m/s target with approximately 0.8 m/s accuracy. The GIF below was created from images taken while the lander maneuvered to CM burn attitude. Propulsion mixture ratios, mass flow rate, and temperature were as predicted. Overall, Intuitive Machines characterizes the execution of the CM as nominal and per expectations. Read More 2/19/24 1745CST IM-1 Mission Maneuvers and Events Update Odysseus continues to be in excellent health, and flight controllers are preparing planned trajectory correction maneuvers to prepare the lander for lunar orbit insertion. Since the IM-1 Mission launched on SpaceX’s Falcon 9 rocket, flight controllers on the Company’s red, white, and blue teams have been learning more about the lander and how to efficiently fly the mission to return the United States to the surface of the Moon. Read More 2/18/24 1745CST Video Update Intuitive_Machines_IM-1_Nova-C_Lunar_Lander_WPOV-2-4k-v2 Intuitive Machines Captures First IM-1 Mission Images In Space 1/4 Intuitive Machines Transmits First IM-1 Mission Images In Space Intuitive Machines successfully transmitted its first IM-1 mission images to Earth on February 16, 2024. The images were captured shortly after separation from SpaceX’s second stage on Intuitive Machines’ first journey to the Moon under NASA’s CLPS initiative. 2/17/24 1100CST IM-1 Mission Engine Commissioning Update Intuitive Machines flight controllers successfully fired the first liquid methane and liquid oxygen engine in space, completing the IM-1 mission engine commissioning. This engine firing included a full thrust mainstage engine burn and throttle down-profile necessary to land on the Moon. Over the next eight hours, flight controllers will analyze the engine burn data collected from over 270,000 km away from Earth. This represents another first for Intuitive Machines, demonstrating one of the critical technologies required to land softly on the surface of the Moon. The IM-1 mission Nova-C class lunar lander continues to be in excellent health, in a stable orientation and remains on schedule for a lunar landing opportunity on the afternoon of February 22. 2/16/24 2020 CST IM-1 Mission Engine Commissioning Update The IM-1 mission Nova-C class lunar lander continues to be in excellent health, in a stable orientation and remains on schedule for a lunar landing opportunity on the afternoon of February 22. The original mission structure allocated a Commission Maneuver (CM) and three trajectory correction maneuvers to position for Lunar Orbit Insertion. This approach provided flexibility in the mission’s engine burn schedule to allow for learning as we operate the lander in the vacuum of space. Adjusting for this learning process is why the team chose to delay the burn on February 15. 2/16/24 1400 CST Read More IM-1 Mission Vehicle Health Update The IM-1 mission Nova-C class lunar lander continues to be in excellent health, and we are preparing for our engine commissioning maneuver. Read More 2/15/24 1900CST Photo credit: SpaceX IM-1 Mission Nova-C Lunar Lander Successfully Enroute to the Moon Following SpaceX Launch The IM-1 mission Nova-C class lunar lander has launched on SpaceX’s Falcon 9 rocket and successfully commissioned in space by establishing a stable attitude, solar charging, and radio communications contact with the Company’s mission operations center in Houston. 2/15/24 Photo credit: SpaceX IM-1 Launch Schedule Update SpaceX announced: \"Standing down from tonight’s attempt due to off-nominal methane temperatures prior to stepping into methane load. Now targeting Thursday, February 15 at 1:05 a.m. ET for Falcon 9's launch of the @Int_Machines IM-1 mission from Florida.\" Image: Launch Pad Image 2/13/24 Photo credit: SpaceX IM-1 Testing Campaign Complete Intuitive Machines and SpaceX have analyzed the data from the lunar lander fueling tests for the IM-1 mission and determined the testing campaign is complete. 2/12/24 Photo credit: SpaceX Intuitive Machines Lunar Lander Encapsulated and Scheduled for Launch The IM-1 mission Nova-C class lunar lander completed all integration milestones and is safely encapsulated within SpaceX’s payload fairing in preparation for launch. In coordination with SpaceX, launch of the Company’s IM-1 mission is targeted for a multi-day launch window that opens no earlier than 12:57 a.m. Eastern Standard Time on February 14th from Launch Complex 39A at NASA’s Kennedy Space Center in Florida. 2/5/24 Photo credit: SpaceX Payloads 1/6 1/6 Press kit IM-1_Press_Kit_V1.1 1/40 View Press Kit .PDF",
    "commentLink": "https://news.ycombinator.com/item?id=39465230",
    "commentBody": "Private company landing on the moon today (intuitivemachines.com)459 points by SigKill9 23 hours agohidepastfavorite255 comments Someone 18 hours agohttps://en.wikipedia.org/wiki/Intuitive_Machines_Nova-C#Eagl...: “EagleCam to record lunar landing Just before landing, at approximately 30 m (98 ft) above the lunar surface, the Odysseus lander will eject the EagleCam camera-equipped CubeSat, which will drop onto the lunar surface near the lander, with an impact velocity of about 10 m/s (22 mph). From the surface the EagleCam will attempt to capture the first third-person images of a lunar landing. The EagleCam will use a Wi-Fi connection to the Odysseus lander to relay its images back to Earth.” That CubeSat is student built. I wonder what camera they have and how hard it will be to make it record the landing. Will it orient itself during that six-ish second drop or can it move the camera after landing? Does it have a fisheye lens to increase the likelihood of the lander being in its field of vision? Unfortunately, their project page (https://erau.edu/eaglecam) seems to be light on such details. reply nightski 15 hours agoparentI was a part of this project over a decade ago when I attended ERAU! At the time the goal was just to take pictures of earth. It is so cool to see how the scope has expanded over the years. Our student group drove down to Cape Canaveral to pick up and haul a clean room back to the university that NASA donated for use to build the satellite. I will never forget those experiences. reply _just7_ 17 hours agoparentprevThe simplest way to do it is probably just to have a high quality 360 camera, that way you mostly get around the problem of orientation reply Someone 17 hours agorootparentYes, but even that isn’t simple, I think. They’d not want to land on top of it, so they’d have to push it out from the lander or have it propel itself away from the lander. If they push it out and it doesn’t have a way to stabilize itself, keeping the lens pointing upwards then will require tight control over that push. So, I guessed (see below) you’d need power to make the sat orient itself. However, I googled a bit more, and found this: https://mynews13.com/fl/orlando/space/2024/02/21/embry-riddl..., which says: “EagleCam will be spring ejected from the Nova-C class lander Odysseus about 30 meters above the lunar surface during the final descent. It will take three images a second from each of its three cameras (a total of nine images a second), capturing its six-second freefall to the surface and Odysseus’ descent and soft landing. About an hour after landing, our team will receive the five images of our choosing. During descent, Dr. Henderson and I will be timing events in landing sequence to match to image numbers to choose the first five images we bring back to Earth. Once we have those images, I will post them directly to @eraueaglecam on Instagram. Shortly after that, they will also be available on @spacetechnologieslab on Instagram and @SpaceTechLab on X (formerly Twitter).” So, it isn’t a 360 camera, and they’re making 50-ish images and hoping for the best. Doesn’t look like the sat has rockets or that they’re trying to make it possible to make more photos after impact on the moon. If my guesses/intuition is right we won’t see the actual touchdown (still cool to have anything, of course), but corrections welcome. reply adastra22 16 hours agorootparentHave two. One on each side. Doesn't matter if one ends up in the regolith. reply TeMPOraL 14 hours agorootparentHave three, 120 degrees apart. They'll double as backup landing legs. reply adastra22 11 hours agorootparentWouldn’t be any better. You’d need 4 to be able to reliably land with one pointed out of the regolith. That’s probably pushing it in terms of mass. 3 wouldn’t be any better than 2 though. reply volemo 14 hours agorootparentprevIt’d surely land sidewise. :D reply adastra22 11 hours agorootparentWhich is fine. 360 degree camera. reply adolph 14 hours agorootparentprev> push it out from the lander Selfie stick sounds simpler. reply sdwr 17 hours agoparentprevThat's an impressive school project! First university to land on the moon reply lukan 14 hours agorootparentUnless they would have build and operated the rocket themself (and not SpaceX), I would consider that a false clickbait headline. reply TeMPOraL 14 hours agorootparentA decade ago, maybe. Today? SpaceX is commoditizing access to space - we're at the point we can start treating Earth-orbit delivery as a given, i.e. just a service you pay money for. reply lukan 13 hours agorootparentEarth orbit maybe. But here we are at earth-moon. And the lander is not build or operated by the school/university either. reply fnordpiglet 13 hours agorootparentprevFlying to the moon is the easy part. Landing is the trick. reply lukan 13 hours agorootparentFlying is not so easy either, and the landing is done by the private company, not by the school. reply fnordpiglet 10 hours agorootparentWeight it by funding and I’ll bet you the school cube sat comes out far ahead. reply pavel_lishin 17 hours agoparentprevI always assumed that space was noisy enough that things like wifi wouldn't work on the lunar surface. reply dspillett 17 hours agorootparentIf basic radio worked for talking between suits and the landing craft in '69, it surely shouldn't be a surprise that modern frequency-hopping, error corrected, wireless comms, with much more sensitive equipment would work well? reply Izkata 17 hours agorootparentThe walkie-talkie toys I had as a kid in the 90s had at least 10x the range of modern home wifi routers. Not to mention how far radio stations broadcast. I'm guessing that's the context they're working from. reply adastra22 16 hours agorootparentThat's because of different frequencies and power caps that are enforced by the FCC. If your WiFi broadcast with the same power, the frequency space would be unusable by your neighbors for their WiFi. The range of WiFi is very purposefully sabotaged to make it useful for more than just you. reply sidewndr46 15 hours agorootparentAssuming you are talking about FCC Part 15 regulations for 2.4 GHz, you couldn't be more wrong. There is no 'sabotage' The EIRP is 4 watts in 2.4 GHz band. More than enough to wipe out your neighbors. Also more than enough to get absolutely tremendous range in line of sight conditions. I can purchase and install an unlimited number of 2.4 GHz Part 15 devices, rendering the band useless to anyone so long as I am attempting to use those devices in a manner consistent with their application. As another Part 15 user, you have no recourse. If a licensed user complains to the FCC, they may decide I have to stop using them and notify me as such. Note: one of my neighbors does this, by having an AP on every 2.4 GHz and 5 GHz channel. Newest Wifi 6 stuff in the US has a power limit on some spectrum and some usages, but nowhere near as low as what I was hoping for. reply redavni 14 hours agorootparent> Note: one of my neighbors does this, by having an AP on every 2.4 GHz and 5 GHz channel. Sounds like a fun and ethical excuse to DOS some WIFI routers. reply adastra22 11 hours agorootparentprevWhy isn’t the limit 40 watts? Or 400? Why is there a limit at all? You can pedantically criticize the use of the word “sabotage”, but then you’d be entirely missing the point. reply op00to 9 hours agorootparent400 watts would cause RF burns at 2.4 ghz. reply fastneutron 8 hours agorootparentAnd most people have a 1200 watt version in their kitchen for that exact purpose! reply dreamcompiler 16 hours agorootparentprevAnd also because our kid-era walkie talkies were VHF (or at least mine were) which is a much lower frequency band than wifi. At a given power level, lower frequencies travel farther (i.e. around obstacles) but can't transfer as many bits as wifi. reply ThrowawayTestr 15 hours agorootparentprevI wonder how much power the wifi antenna on the cubesat is pumping out. reply hughesjj 7 hours agorootparentprevAnd all of that without an appreciable atmosphere to get in the way reply TeMPOraL 14 hours agorootparentprevNoise you can correct for with directional antennas, filters, and/or more signal processing voodoo. Meanwhile you benefit from space being actually empty - no pesky atmosphere in the way to attenuate signals (though also no layers to bounce the signal off), and no other transmitters in your area. Inverse square law works to your advantage in this context. reply pfdietz 17 hours agorootparentprevIf you're above the frequency at which the ionosphere becomes reflective (around 30 MHz), why should space be noisier than the Earth's surface? Anything propagating there will reach down here (unless it's something really short wave absorbed by molecular bands in the atmosphere.) In practice, it's going to be noiser down here, because of all the sources down here. reply tcmart14 9 hours agorootparentprevI don't know specifically about wifi but, check out EME bounces. With a hand held ham radio on the 6M band and a directional antenna made from like $6 of supplies from your local hardware store, you can have a radio that can bounce a signal off the moon and talk to someone on the other side of the earth. reply op00to 9 hours agorootparentIt’s more like the 2m and 70cm bands, and really big expensive antennas. You can bounce signals off of meteor trails with a cheap antenna and 6m radios, though! reply tcmart14 9 hours agorootparentBeen out of it awhile so i was thinking 6m, mighta gotten mixed with the scatter-e propagation. But I knew a guy down in San Diego who used to do EME bounces with cheap directional antennas he homemade. But that guy was also an EE at Qualcomm there. But that was 6 years ago and since I've moved up to Oregon, I havnt gotten to do much ham stuff because I could never hit the repeaters around here. Maybe they are no longer up, maybe its all the hills and volcanic rock? reply op00to 9 hours agorootparentYou can make it work with normal human antennas, as long as the other person has the big ones! I could never contact someone with the same antenna and radio as me via the MOON, but I can on certain days be heard by those big dogs! Meteor scatter is pretty fun though! reply th0ma5 16 hours agoparentprevFunny that the student project will be the first private landing followed by the commercial vehicle? reply rkagerer 11 hours agoprevRidiculous this isn't more prominent in mainstream news. Eg. I open Google News to stories of killers and a new real estate tax and some sort of scandal by an actor. This is why I come to HN instead. reply godelski 11 hours agoparentEdit: Front page of a few sites Reuters: See the right part of the image https://imgur.com/VpGAIBl CNN: https://imgur.com/TzRtit8 AP News: https://imgur.com/OgrtzrC NBC: Not so apparent but there https://imgur.com/7itvGeh NYT: Also not obvious https://imgur.com/4DMDqr0 NPR: Gotta scroll: https://imgur.com/dp5UDCq I found it on Fox News by search and it's a fair scroll down. I also found in MSNBC by search, but actually don't know where it is just that the word \"moon\" exists somewhere on that page and for the life of me I cannot see what word is highlighted. reply pcl 10 hours agorootparentIt’s the fourth story on the Washington Post website right now. reply SV_BubbleTime 10 hours agorootparentAnd it looks like it crashed… not the website. reply adriand 8 hours agorootparentprevIt's been the #1 headline story on the NYT for a couple of hours at least now. They had the live feed on the homepage too. reply prawn 9 hours agorootparentprevIt's a major (red background!) story with live coverage on The Guardian site here in Australia. reply anigbrowl 10 hours agoparentprevGoogle News is absolute garbage these days. Their algorithmic curation has not kept up with SEO and still treats popularity as a proxy for significance. reply jessriedel 9 hours agorootparentIt's the top story for me on Google News right now (after the landing). reply saagarjha 11 hours agoparentprevIt was on NPR today if that makes you feel better reply adityaathalye 19 hours agoprevThe private lander's mission control looks like the proverbial two-pizza team. Would it that the silent revolution in spaceflight, hidden under the glamour of reusable rockets, is incredibly sophisticated telemetry, communication, on-board automation? A computing stack that's making the 1x mission control a 10x mission control? I understand there will be other teams elsewhere (delivery vehicle, remote sensing etc. etc.). But that image is rad too --- from one or a few space agencies co-orchestrating a program to multiples more doing so. Obviously some version of this has been going on for decades, but somehow the imagery on their website struck a chord. reply JonChesterfield 18 hours agoparentLinkedin suggests about 250 people. Lots of them with software in the name. Company looks very lean relative to the semiconductor monsters I'm more familiar with. reply datadrivenangel 18 hours agoparentprevJAXA has done rocket launches with a ground crew of less than 10. Obviously the extended support staff is much bigger than that to enable so few people to launch a rocket, but the number of people required when things are ready to go can be very small. reply kragen 17 hours agoparentprevbetter, lighter-weight computing stacks have been a huge boost; that's what made cubesats possible. but the much bigger deal is the dramatic drop in launch costs driven by spacex, even though so far that's only a factor of 3.4 https://ourworldindata.org/grapher/cost-space-launches-low-e... with lightweight computers driven by the cellphone industry, it became possible for a small team or even individual to launch a low-power ham radio satellite or weather satellite. but they can't launch a high-resolution space telescope, earth observing satellite, or high-power communications satellite, nor can they do laser communication pointcasting. and lightweight computers are a crucial enabler for starlink-style communications constellations, but there's only one of those, because that's still a big-money kind of project suppose that, instead, you had 01980s computing power, but the cost of space launch dropped by a factor of 100. if you need to launch a 200-kg satellite to get the sky-observing optical aperture you're looking for so diffraction doesn't cremate you, you don't care if the onboard computers weigh 1 gram or 10 kilograms. (i mean, you do care, because it lets you cut your launch budget 10%, but it's not a dominant determinant of viability.) with saturn v or zenit 2, according to the plot linked above, that launch would cost you a million dollars. today, at falcon heavy's 1500 dollars per kg, it's 300,000 dollars, which is already a radically more feasible project spacex's 'starship' is supposed to carry 150 tonnes to leo for 10 million dollars. that's 70 dollars a kilogram. our hypothetical 200-kg aditya athalye space telescope satellite would then cost 14000 dollars to launch. it becomes a hobby project comparable in cost to an engine lathe or a camper van. this would change the economics of space in a profound way, far beyond what cellphone chips have done for comparison, the csis aerospace security project number for https://en.wikipedia.org/wiki/Scout_(rocket_family) on that chart was 118500 dollars per kg in 01961. (but of course you couldn't launch a cubesat on it for that price; it was a military thing.) by 01967 saturn v had brought that down by a factor of 22. after that it remained constant for 43 years until falcon 9 in 02010. starship, if it works, will reduce launch costs by that same factor of 22 over the current falcon heavy number i described above, and by a factor of 73 over saturn v reply HeadsUpHigh 13 hours agorootparentThe decline in price per kg from Falcon 9 has made a big difference and yet it's nowhere near as impactful as Starship is going to be... falcon 9 is either too powerful for LEO missions and limited by fairing volume( so can't really launch space stations on a falcon 9) or too weak to push further into GEO and into the solar system. Starship will enable a lot of LEO applications previously unthinkable. reply adityaathalye 5 hours agorootparentprevOh I don't dispute the fact that reusable vehicles are an incredible leap forward. Anything that manages to launch significantly more payload cheaper and more frequently is awesome. That said, things like reusability work at all because of precision control which, as far as I can tell, is near-impossible to do in a lightweight package without the cooperation of a compute stack. Same for other kinds of soft landings and autonomous control scenarios. As an aside: achievements like reusable rockets also became possible because of advances in materials research driven by advances in computer simulation and computer-aided manufacturing. So this is perhaps another under-appreciated layer of the rocketry compute stack. reply idlewords 11 hours agorootparentprevThe big improvement is not launch costs, but miniaturization and automation. A university team can make a credible lunar rover or cubesat today using off-the-shelf components. Unlike launch vehicles, that technology did not exist in 001960, or even 002000, at any price. reply kragen 10 hours agorootparenti came here for an argument, but this is just contradiction it doesn't matter if you can make a credible lunar rover or not if the price to launch it is three times the grant the nsf will give your university team. given that nasa published their lunar roving vehicle documentation in mostly 0001973, down to circuit schematics and some machining dimensions, i'm pretty sure a university team in 00002000 could have duplicated it for less than the several million dollars the launch would cost. it only took boeing 17 months to design and build them in the first place, and they mostly use technology from the 000001940s (aluminum tubing, nylon webbing, wire mesh wheels, silver-zinc plexiglass batteries, brushed dc motors, cable brakes; fiberglass arm rests and fenders were apparently the highest-tech part and, unsurprisingly, the part that failed) https://www.nasa.gov/history/alsj/alsj-LRVdocs.html with respect to cubesats, you're in violent agreement with my comment. you obviously can't put a trs-80 in a cubesat. in 0000002000 you could put a basic stamp in it, but you couldn't get mems gyros and accelerometers, and you weren't going to be able to run your star tracker camera off a pic16. computer and imu miniaturization is a big deal for cubesats. that's one of the main points of my comment the other main point is that it isn't nearly as big a deal for bigger satellites; when we were launching cubesats (before i joined the team) it was really important that we could use tiny cellphone components, but once we were launching 37kg monstrosities, the fact that the gumstix boards only weighed a few grams was just nice, not critical. the optics weighed a lot more so tell us, what's your experience? reply idlewords 9 hours agorootparentMy big-picture point is simply that electronics are getting small and cheap much faster than launch costs are decreasing, so that is the important trend. We could have a revolution in space exploration and remote sensing even if launch costs flatlined. Beyond that I'm not sure what we're arguing about, so I'll tap out here. reply kragen 9 hours agorootparentthey aren't commensurable; as ben tilly pointed out, you cannot replace the oxygen you need to breathe with a sufficiently large quantity of granite (without equipment and energy to process it, anyway), and you cannot replace lower launch costs with smaller and cheaper electronics, in particular in the areas you named, space exploration and remote sensing, for the reasons i described now, bring me a shrubbery! reply danlugo92 7 hours agoparentprev> A computing stack that's making the 1x mission control a 10x mission control Isn't this obvious...? Given SpaceX's auto-pilot mission to the moon and all of these latest missions to asteroids and such...? BTW They land on asteroids because they already have enough rock moons, asteroids are more varied than different surfaces of the moon, no conspiracy here. (I upvoted you BTW) reply anigbrowl 10 hours agoprevUpdate: communications have been a bit glitchy but the craft has made a safe landing and is transmitting data. Was watching NASA administrator Bill Nelson's congratulatory message; can't help wondering if he also recorded a commiseration one looking to the future in case it didn't work... reply samschooler 10 hours agoparentI'm sure. That was very recorded. Reminds me of Nixon's speech written in the event the moon landing failed. https://www.archives.gov/files/presidential-libraries/events... reply iisan7 7 hours agoparentprevAs I was watching it, I was thinking that if I were responsible for planning the scripts, I would have been kicking myself for not recording one for an ambiguous circumstance, or incomplete success. As it was it was a little disjointed. reply aquova 9 hours agoparentprevWow, I'm pleased to hear this. I watched the stream, but by the time I had turned it off there hadn't been any word and things looked pretty bleak. reply lunakid 13 minutes agorootparentIt's still grim. All the \"signals\" we have are from the hypeworks. It's fascinating, actually, how basically nobody in the media had the guts to be honest & neutral, and state the obvious fact that things aren't looking _that_ good. reply unstatusthequo 9 hours agoparentprevMaybe it had an AT&T SIM? ;) But seriously, it's weird how this really didn't make a lot of news today overall. Did the cellular outage really overshadow this? Times have changes if so. Our idea of \"wow!\" has been desensitized I guess. Pretty impressive to me! reply layer8 9 hours agorootparentThe JWST is on a similar level of impressive (if not more), and arguably more important than a single moon landing, and the general public doesn't have much interest in that either. reply belval 9 hours agorootparentTo defend the general public, landing an unmanned craft on the moon just doesn't sound impressive anymore, it's like SpaceX landing their boosters. The first time it's mindblowing, the Nth time, not so much. EDIT: And for JWST for the average person it's just a new Hubble. reply layer8 9 hours agorootparentThe average person is likely to not even know about Hubble or any space telescopes. reply NedF 19 hours agoprevLanding in ~ 8 hours. \"The Odysseus moon lander is aiming for a 5:30 p.m. EST (2230 GMT) lunar landing\" https://www.space.com/intuitive-machines-odysseus-moon-landi... reply obelix150 16 hours agoparentThe landing time has been updated to 1524 CST. https://www.intuitivemachines.com/im-1 There will be a live feed on the Intuitive Machines site above and mirrored on NASA TV+ here: https://plus.nasa.gov/scheduled-video/intuitive-machines-1-l... The coverage on both live feeds will begin around 1400 CST. reply mosselman 15 hours agoparentprevPretty cheeky of Columbia to sponsor this. They couldn't even make the shoes I bought that were 'absolutely waterproof' actually waterproof. A super light rain would result in wet feet. This didn't even happen in normal sneakers. But it is all about image I guess. reply UniverseHacker 14 hours agorootparentColumbia is known for incredibly good lifetime warranty- contact them and get something that works for free. I use mostly Columbia gear for some pretty serious outdoor adventures, and generally consider their waterproof tech, especially outdry rain shells to be the best you can get at any price. However, I don't (and wouldn't) use any of their footwear. For waterproof footwear, breathable membranes don't work well in my experience, they quickly tear and leak. The membranes are just too delicate for the forces and flexing on a boot. A really well built traditional leather boot with external waterproofing like Sno Seal applied daily is both more breathable, and more waterproof. Failing that, heavy rubber boots like commercial fisherman wear are really the only totally waterproof footwear. reply falserum 10 hours agorootparentprevIf it’s waterproof your legs will be wet anyway (sweat will accumulate) reply armcat 10 hours agoprevIt looks like it touched down, mission control picking up a faint signal, trying to refine it now. reply malfist 10 hours agoparentFirst successful commercial landing! reply malkia 12 hours agoprevI didn't know about them. I recently watched the whole https://en.wikipedia.org/wiki/For_All_Mankind_(TV_series) and was mesmerized by it - it's alternative reality fiction, where a private company comes to land on Mars! reply rkagerer 11 hours agoparentYeah, For All Mankind is a great show for people who are excited about this kind of stuff. It's an alternative-history fictional drama exploring \"What if\" the space race never ended. reply jessriedel 9 hours agorootparentI wanted to like it, and having some (reasonably-)big-budget visualization made of proposed rockets like Sea Dragon was great. But the (imo) awful dialog made the show unwatchable for me, and they did a lot of scientifically silly stuff like taking the Space Shuttle to the moon. reply krasin 11 hours agorootparentprevYep. Highly enjoyed its Season 1, but I wished I never watched any further - it feels complete at this point and the later seasons destroy the impression. Kind of like with The Matrix. Note: some people have an opposite opinion, that the interesting stuff starts at Season 3. reply rkagerer 11 hours agorootparentAgreed, Season 1 was by far the best. I'm re-watching the whole thing with my spouse and in some ways it's more fun the second time around (surely biased because of the shared experience). Some shows are like that - I didn't think Star Trek Deep Space Nine was all that hot first time around, appreciated it much more years later on a rewatch (the supporting characters are awesome). Maybe in part it's adjusted expectations ;-). reply malkia 11 hours agorootparentprevI loved them all - but I can see your point! reply fragmede 10 hours agoprevI don't know why they made it so hard to get to the youtube link for the video, which is https://www.youtube.com/watch?v=5-qwhozfYeQ reply happytiger 6 hours agoprevIt’s interesting to watch this company operate and see just how much better SpaceX is at PR and how internet native their broadcasts annd PR efforts are — they are head and shoulders better than their competitors. I really am stoked to see more and more space companies setting new milestones. It’s going to be a bright future for humanity if we can continue to expand space exploration and eventually become multiplanetary. It’s awesome to be alive to witness it. reply acefaceZ 18 hours agoprevI believe they are part of the recently setup homesteading program similar to what Alaska setup in the 80s. If they can put a stake in the ground they get 20 acres around the stake. reply strombofulous 11 hours agoparentThis reminds me of countries who send pregnant women to antartica to (somehow) strengthen their land claims: https://medium.com/good-to-know/why-11-babies-have-been-born... Like antartica, I'm sure someone with guns will tell us who really owns the moon as soon as it actually becomes relevant. reply psd1 15 hours agoparentprevReassuringly anachronistic to use acres as a unit of measure on the moon. reply p1mrx 14 hours agorootparentFor historical reasons, a moon acre is approximately 16.7% smaller than an earth acre. reply samatman 12 hours agorootparentprevIt's an official unit of the only flag on the Moon to date. What else would you suggest? reply lunakid 6 minutes agorootparentMaybe the same units basically everybody has always ever used (in earnest) for space tech (including NASA)? reply ceejayoz 11 hours agorootparentprevIt’s not the only flag. https://web.archive.org/web/20220725003858/https://www.space... reply p1mrx 7 hours agorootparentprevm² reply ethbr1 19 hours agoprevCurious question: as a private company, how do you determine your lunar orbit parameters after a burn? Are they leveraging Earth/orbit-based radar? Or satellites around the moon? Or something else? reply ooterness 18 hours agoparentGround infrastructure is important for space missions. Intuitive Machines is operating under a NASA contract for Commercial Lunar Payload Services (CLPS) [1]. As such, they can negotiate access to NASA resources such as DSN and NEN for this mission. Intuitive Machines has also built several ground stations of their own [2]. These allow communications with the spacecraft as well as the range/velocity measurements needed for accurate navigation. I'm the longer term, the Artemis program plans to build out LunaNet [3] for improved communications and GPS-like navigation services. [1] https://www.nasa.gov/missions/artemis/clps/intuitive-machine... [2] https://www.intuitivemachines.com/post/commercial-lunar-netw... [3] https://www.nasa.gov/humans-in-space/lunanet-empowering-arte... reply ethbr1 17 hours agorootparentYou are awesome! Thanks for all the links and info, to an open-ended question. The process of bootstrapping to Earth-space-parity is fascinating to me. One thing in the 1960s, when there was no GPS and terrestrial net assumption, but now you're going from everything we've built here to... if you don't bring it, you don't have it. The accretion of Mars support satellites has also been fascinating. reply Teever 10 hours agorootparentIt's gonna be wild the first time SpaceX sends a Starship to the moon and/or mars to unload a load of starlink satellites into orbit. reply dpflan 17 hours agorootparentprevThanks for these great details. Is there a fee for using the NASA resources (I assume there is, I just don't know how this works)? reply ooterness 16 hours agorootparentYes, there is a fee. The largest antennas (i.e., the absolutely gigantic 70 meter dishes) can be $5k per hour. Further discussion on StackOverflow [1] and the NASA MOCS guide [2]. [1] https://space.stackexchange.com/questions/21005/what-makes-t... [2] https://deepspace.jpl.nasa.gov/files/6_NASA_MOCS_2014_10_01_... reply mlhpdx 17 hours agorootparentprevThe name LunaNet made me think of GPS cubesats and ground transponders that would make precise positioning lunar equipment/missions without the latency of coupling to earth systems. It's not that, it seems. Anyone want to start a location-based service company? ;) reply dylan604 19 hours agoparentprevI don't understand the \"as a private company\" qualifier. The math is the same, and doesn't care if it's private/public. reply andyjohnson0 19 hours agorootparentI'm not the person you replied to, but I assume they meant something like \"as a non space agency\". Ie how are they tracking the lander? How are they sending and receiving telemetry? What resources did they use for mission planning and site selection? Perhaps they've built their own comms system for example - maybe even a multi-site one that enables continuous contact - or maybe they're using NASA/ESA/JAXA assets. It would be interesting to know. I'm not aware of any commercial providers for lunar communications. reply maronato 18 hours agorootparentTheir website has all that info. They have their own satellites for comms, and lots of other stuff too. They seem to either own or co-own all of the hardware reply martincmartin 19 hours agorootparentprevBut observations need powerful domestic telescopes. If you're NASA, you own and have access to them. reply dylan604 19 hours agorootparentNo, as an American, you own them. NASA just administrates them for you. If you're a space faring private company, you contract out the various parts of the mission. You didn't build a rocket, you hired SpaceX. You didn't build the relay network, you licensed access time. You don't build a space observation platform, you license time to use them. reply JumpCrisscross 11 hours agorootparent> as an American, you own them. NASA just administrates them for you Across public and private spheres, the word “ownership” loses meaning. (Nobody “owns” NASA or the U.S. government, though they do “belong”to we the people.) That’s why, in ownership disputes between nations and under the law, the operant term is “control.” reply patmorgan23 16 hours agorootparentprevThis is a pretty silly pedantic point. Public property is owned and controlled by governments for the benefit of the public. That does not mean each individual member of the public has traditional ownership rights to said property. reply tzs 19 hours agorootparentprevWhich of those, or what else instead, did the specific company in this case do for this specific mission? That is what ethbr1 was asking. reply datadrivenangel 19 hours agorootparentAlmost certainly NASA. reply sneak 17 hours agorootparentprevIf I owned them, I would get to use them. I don’t, so logically I must not. By this logic, there are huge amounts of land that I also “own” that guys with M16s (or, more accurately, M4s) will keep me from walking on if I try to go there. Half the year I live in southern Nevada, so this distinction has some direct practical consequences in my life. It’s deceptive. Government property is not owned by citizens, it is under the exclusive control of the state—i.e. not you. reply dylan604 17 hours agorootparentYou conveniently left out the part of licensing the time. You can license with the BLM for access to government control land. Ranchers do it all the time. Special events like Burning Man also do it. You just have to contact the correct agency to do it. But of course, it so much easier to make a know it all sarcastic filled internet rant than do anything approaching useful information to a conversation. reply bigstrat2003 17 hours agorootparentDo you not see how that just furthers the parent's point? If I have to license usage of something, I don't actually own it. reply dylan604 16 hours agorootparentI think we have a basic misunderstanding of private vs public ownership. When we all own it, you can't just do whatever you want like build a house, but withe proper permit, you can use it. If it was private, I could never do something with what you owned. reply bigstrat2003 14 hours agorootparentIn the case of public ownership, I don't own it though. The entire public does. I'm willing to say \"the people own\" these telescopes, but that isn't what you originally said. You said \"as an American, you own them\" which isn't true. reply dylan604 14 hours agorootparentpedants going to pedant reply xpe 15 hours agorootparentprevOwnership rights and various kinds of access rights are not identical and often conflated. For example, if I own a water well, I don’t necessarily have the rights to do whatever I want with it. Some jurisdictions might let me pump out as much water as I want, but even those will punish me for blatantly polluting it (one would hope). What some people think of as something akin to “total ownership” — completely unlimited access — would be tantamount to putting one’s “rights” above everyone else’s. Even dictators usually have some limits on their power, whether by laws, norms, or geopolitical pressures. reply bigstrat2003 14 hours agorootparentI would say that in your example, the well is really two things: the water table (which you don't own), and the pump you use to draw water (which you do own). reply jebarker 19 hours agorootparentprevDo you mean optical telescopes? I would have thought they used earth based radar plus star tracking onboard to figure out where they are in orbit, but I don't know. reply vardump 19 hours agoparentprevMaybe star (and moon/earth) tracking and radio signals to earth? reply pjungwir 10 hours agoprevNASA is covering this on twitch right now: https://www.twitch.tv/nasa reply jdelman 18 hours agoprevIt's interesting how this feels like a big deal; when I learned of this yesterday, I almost forgot that there have been almost 30 missions to Mars (which I assume is much harder and more expensive) in the years since the last moon landing. reply JonChesterfield 18 hours agoparentI'm more excited about the moon. Being that much closer is a big deal. This company is looking to make sending stuff to the moon (not sure about getting stuff back) a reliable & vaguely cost effective thing to do. There's probably valuable stuff on the moon and even if not, it's learning a load of things about going further afield. Lots of science fiction about the asteroid belt beyond mars. reply danavar 17 hours agoparentprevIt is a big deal! While just landing on the moon is definitely much simpler than a Mars mission, this lander is a part of the Artemis program; it's one of the first steps towards developing the Artemis base camp on the moon. reply gwbas1c 17 hours agoparentprev> which I assume is much harder and more expensive Manned space missions are significantly more expensive (and complicated) than robotic missions. (Otherwise, we'd be sending a lot more people into space.) Let's do a little bit of back-of-the-envelope Googling: From https://www.planetary.org/space-policy/cost-of-apollo > The United States spent $25.8 billion on Project Apollo between 1960 and 1973, or approximately $257 billion when adjusted for inflation to 2020 dollars. From https://www.planetary.org/space-policy/cost-of-the-mars-expl.... > The Mars Exploration Rover (MER) mission cost $1.08 billion. Of that amount, $744 million was spent on spacecraft development and launch; $335.8 million was spent on 15 years of mission operations. So it looks like \"manned\" space missions cost at least 100x the cost of sending a robot. reply idlewords 11 hours agorootparentThe post you're replying to is comparing robotic missions to Mars vs. the moon. Mars is orders of magnitude harder to land on because of its atmosphere, stronger gravity, and the need to keep your robot healthy on the long trip over. reply consumer451 17 hours agoprevFYI, they have a subreddit: https://old.reddit.com/r/IntuitiveMachines/ reply dang 15 hours agoprevSee also https://www.reuters.com/technology/space/us-nears-attempt-fi... (via https://news.ycombinator.com/item?id=39468115, but no comments there) reply dghughes 16 hours agoprev> the Odysseus lander will eject the EagleCam camera-equipped CubeSat, which will drop onto the lunar surface near the lander So the first non-government device to land on moon will be viewed trying to land by a device also non-government (but part of the lander) that lands on the moon first? I guess that evens out. reply qwertox 11 hours agoprevNow to the problem with YouTube, and I'm sick and tired of reporting this, because by now they should have an automated solution for this, look at this link: https://www.youtube.com/hashtag/im1 Channels named \"SpaceX [LIVE]\" [0], \"SpaceX\" (which is actually @uyenmusic with 148.000 subscribers), and so on. Most of these channels have no videos except for that single live stream, occasionally inserting QR-codes with crypto-scams. Like the first one I mentioned. > Huge crypto-giveaway during to the launch! > During this unique event, you have the opportunity to take a share of 1.000 BTC & 10.000 ETH & 100.000.000 DOGE & 10.000.000 USDT. Have a look at the rules and don't miss out on this. You can only participate once! And the most interesting thing about this video is that they are using AI to make Elon Musk say that you should scan the QR-code and that you will get the crypto. 100% sounds like him. [0] https://www.youtube.com/watch?v=toDNnSBzgEU reply defenestration 11 hours agoparentThat livestream [0] looks like a pretty convincing scam to me, with 22k people viewing at the moment. And it indeed sounds like him. The real username is hidden behind the visible SpaceX[Live] name. reply photochemsyn 17 hours agoprevThe next stage in this project is the ice-drilling PRIME-1 - which might rely on the success of this stage? Details: > \"The [Odyssus] Nova-C Lander is a tall hexagonal cylinder on 6 landing legs. It is capable of carrying 100 to 130 kg of payload to the surface. It uses solar panels to generate 200 W of power on the surface. Propulsion and landing use liquid methane as fuel and liquid oxygen as an oxidizer. The PRIME-1 mission has two primary components, The Regolith and Ice Drill for Exploring New Terrain (TRIDENT) and the Mass Spectrometer observing lunar operations (MSolo). TRIDENT is an augering drill approximately 1 meter long. The drill is able to stop at any depth as commanded from the ground and deposit and deposit its sample on the surface for analysis. MSolo is a commercial off the shelf (COTS) mass spectrometer modified for spaceflight and lunar operations. Total PRIME-1 payload mass is about 40 kg.\" https://nssdc.gsfc.nasa.gov/nmc/spacecraft/display.action?id... reply lacoolj 16 hours agoprevdoes someone have a telescope pointed at the moon so we can have a third, third-party view of this landing? reply sgt 19 hours agoprevWhere is the live stream? reply TrueGeek 19 hours agoparenthttps://www.youtube.com/watch?v=IarunZ9Ykas reply pugworthy 15 hours agorootparentNice background music - not jarring. reply sp332 17 hours agoparentprevhttps://m.youtube.com/watch?v=Dg2ffigGcYM reply thisisauserid 14 hours agoprevThis is not a hoax. But won't we soon have a real fake moon landing? What a world. reply ArunRaja 19 hours agoprevIs this the first time for pvt company...? reply Communitivity 19 hours agoprevnext [71 more] [flagged] shafyy 18 hours agoparentI'm suprised by the number of replies here that don't get why it's unsafe for girls and women in Texas (and some other US states). I'm also not American, but you had to be living under a rock in the past months and years if you didn't get wind of the overtunring of Roe vs. Wade and the reproductive rights policies in Texas. reply sergent_moon 18 hours agoparentprevI'm glad you said what you said. I am dismayed by the sheer number of transparently bad faith \"just asking questions\" happening in the comments here. reply CatWChainsaw 7 hours agorootparentTechbros would sterotypically enjoy negging and policing female existence, so it's dismaying but not surprising. reply deeth_starr_v 18 hours agoparentprevNot limited to having a daughter. My extended family has a trans son. Luckily they are in California. That’s a difficult thing to navigate in itself without living in a state where the fad is to elect the worst people possible. reply Shatnerz 19 hours agoparentprev> Not moving my family to Texas, Florida, or Alabama, for anything. Beautiful states, but I have a daughter. Are women not safe in these states? I'm not sure I'm picking up whatever you are implying. reply midasz 19 hours agorootparentWomen die because they're not allowed to abort nonviable fetuses that would also likely kill them. Seems very unsafe, I wouldn't want to be a woman there. reply jnaina 9 hours agorootparentFor non-Americans everywhere, disallowing abortion including nonviable pregnancies sounds barbaric. America is viewed as the most advanced nation in the world. But these Taliban-level backwardness is mind boggling. America was never this regressive and tribal when I went to school there in the mid 80's til the 1990. reply fourseventy 17 hours agorootparentprev\"Abortion in Texas is illegal in all cases, except to save the mother's life, or prevent substantial impairment of major bodily function.\" https://en.wikipedia.org/wiki/Abortion_in_Texas#:~:text=Abor.... reply ipython 17 hours agorootparentThe trouble with this language, which seems unambiguous, can be difficult to implement in an emergency situation. For example: https://abcnews.go.com/US/woman-sepsis-life-saving-abortion-... As a partner with a wife who went into septic shock, sepsis is no joke. I was given a 40% chance of her living through the night, which, thankfully she did. Doctors are understandably hesitant to perform an abortion because they could be held criminally liable if some court after the fact, with months of hindsight, may consider that the mother could have lived even if the baby was brought to term. So this is a no-win scenario all around and a terrible law from a party that claims to eschew any government overreach into personal lives. reply kragen 17 hours agorootparentprevyou may or may not be aware that a common result of such laws is that doctors refuse to get involved in cases where an abortion is necessary to save the mother's life, because they don't want to have to convince a jury of it. better to take a different patient and not risk going to jail until the trial reply Shekelphile 5 hours agorootparentprevNobody knows the letter of the law in the real world and carveouts like this are meant to appease political opposition and get laws passed, not to be enforceable/usable. It's an effective total ban. reply mjparrott 19 hours agorootparentprevThat is terrible. How often does this situation happen? reply hazmazlaz 18 hours agorootparentHere are some statistics about Termination for Medical Reasons (TFMR): Pregnancies with major congenital fetal abnormalities: Most pregnancies with a major congenital fetal abnormality end in TFMR. The percentage of pregnancies that end in TFMR can range from 70% to 95% depending on the severity of the abnormality. Stillbirth: TFMR is almost twice as common as stillbirth. In 2018, Tommy's statistics reported 2,943 babies lost after being stillborn, while the DHSC reported 3,269 TFMR during that same time period. Down's syndrome: 20% of TFMRs are due to Down's syndrome. Pregnancies with Turner syndrome: Turner syndrome leads to the highest rate of pregnancy termination (100%). Pregnancies with Klinefelter syndrome: Klinefelter syndrome leads to the second highest rate of termination (73.9%). In the UK, over 70% of congenital anomalies are detected during pregnancy and, of those, around 37% will result in TFMR. In Europe, the prevalence rate of TFMR is 4.6 per 1,000 births. Seven percent of women cited health concerns for themselves or possible problems affecting the health of the fetus as their most important reason in 2004, about the same as in 1987. reply spiderfarmer 19 hours agorootparentprevI don't know the statistics, but even if it happens only once, the authors of the policies in place should be prosecuted. One stat that jumps out is that infant death rates are way up. Forcing women to complete their term anyway is sadistic and completely unnecessary. reply ejb999 18 hours agorootparentnext [2 more] [flagged] ethbr1 18 hours agorootparentThat seems unrelated to the discussion at hand. Something cannot be ignored by pointing at something else. reply abetusk 18 hours agorootparentprevIt's hard to tell what you're actually asking but the Louisiana department of health claims it's about 1 in 10,000 die from pregnancy complications [0]. University of California San Francisco claims 6%-8% of pregnancies are high risk [1]. The World Health Organization claims a global rate of 4.3 mother deaths per 1000 live births in low income countries versus 1.2 deaths per 10,000 live births in high income countries [2]. [0] https://ldh.la.gov/page/pregnancy-risks [1] ucsfhealth.org/conditions/high-risk-pregnancy [2] https://www.who.int/news-room/fact-sheets/detail/maternal-mo... reply midasz 18 hours agorootparentprevI'm not sure, how many women a year should we say is acceptable? reply ta8903 5 hours agorootparent30-40 seems reasonable. reply imglorp 19 hours agorootparentprevAlso a parent with similar concerns including book bans, systematic weakening of science, education and library institutions, unstable power during temperature extremes (TX), unrestrained gun violence, restrictions on sex ed, and encouraged intolerance of gender differences. To name a few. reply a1o 19 hours agorootparentprevYeah, they are really antagonistic to women, there are strict anti-abortion laws that even include travel bans https://www.texastribune.org/2023/12/13/abortion-travel-ban-... reply JackMorgan 19 hours agorootparentprevRural states often are significantly less safe for women, e.g. Texas ranks much higher than the national average, although Alabama and Florida are quite low in this dimension. https://www.statista.com/statistics/232563/forcible-rape-rat... reply spiderfarmer 18 hours agorootparentAlaska: 134 per 100k, The Netherlands: 7.1 per 100k. Something is very wrong in the USA. reply ethbr1 18 hours agorootparentGiven Alaska is a 3x outlier compared to the national average, and almost 2x the next highest state, it seems unreasonable to draw conclusions from it alone. reply spiderfarmer 17 hours agorootparentThe US: 40 per 100k NL: 7.1 per 100k I think I'll continue to draw \"unreasonable\" conclusions. reply ethbr1 14 hours agorootparentIf you'd wanted to make the stronger case, you could have used those numbers the first time. ;) reply spiderfarmer 13 hours agorootparentI fail to see how you made a valid counter argument though. reply ethbr1 11 hours agorootparentThere isn't one. You're right. You were just using shock numbers to argue it, when the fair numbers did so better. reply azinman2 18 hours agorootparentprevYou’re assuming perfect statistics. I wouldn’t. reply grecy 17 hours agorootparentIt's always incredible that \"statistics aren't perfect\" suddenly doesn't apply when we're talking about the biggest GDP or Military. reply jorts 19 hours agorootparentprevThey’re likely referring to the effects of the Dobbs ruling [1]. 1: https://en.m.wikipedia.org/wiki/Dobbs_v._Jackson_Women%27s_H.... reply bko 19 hours agoparentprev> All postings for software engineers are on-site, in Texas. Not moving my family to Texas, Florida, or Alabama, for anything. Beautiful states, but I have a daughter. what does this mean? reply ethanbond 19 hours agorootparentThat these states are antagonistic to women's health. See: Alabama's ruling that frozen embryos for IVF are actually children with all the rights thereof, immediately ceasing IVF procedures. See also: https://www.cnn.com/2023/12/05/us/texas-woman-high-risk-preg... This woman in Texas having to sue to avoid a very dangerous birth of a child who will she will end up having to watch die within a few days. > It states Cox’s OB-GYN, Dr. Damla Karsan, has a “good faith belief” that Cox falls under the legal exception to the abortion ban, but can’t provide the abortion without a court order because she “cannot risk loss of her medical license, life in prison, and massive civil fines” if her belief is not accepted by the courts. So yes, Texas has an exception for births that would threaten the mother's life, but in practice doctors are justifiably petrified of betting their freedom on their assessment of that risk lining up with a bunch of medically ignorant theocrats' assessments. reply sandworm101 18 hours agorootparent>> Texas has an exception for births that would threaten the mother's life I saw an interview with a political appointee in charge of exemptions, I believe in texas. He was a hardcore right-wing Christian republican strongly against abortion. He lasted a week. After seeing all the various horror stories, the stuff that obgyns learn of at school, he radically changed his mind and was asking to let doctors again make the decisions: \"Please, I don't want to see any more of those ultrasounds.\" reply sp332 17 hours agorootparentA source would be great if you can remember that one. reply imtringued 14 hours agorootparentprevFrozen embryos are children with all rights? What's next? Women who have periods are serial killers and must be kept pregnant at all times? Most of those frozen embryos are less than a dozen stem cells. You lose far more skin cells every second. There is no way to describe this other than religious crankery. reply ethbr1 18 hours agorootparentprevThe opinion in the Alabama Supreme Court ruling is a fascinating read, as the justices attempting to figure out what the heck the legislature's intent was in a poorly-written and sparse law. https://publicportal-api.alappeals.gov/courts/68f021c4-6a44-... reply sandworm101 19 hours agorootparentprevThere are some horror stories coming out of certain US states, mostly surrounding woman's reproductive rights. There are crackdowns on old ways of doing things such as driving your daughter to another state in order to have an abortion. That can now be a very serious felony for all involved. Many people with kids, and many young women, are starting to actively avoid moving to these states. This is growing problem for military recruiters. Try telling a 17yo highschool girl that if she joins the army at 18 she may be posted to some random state where she may not have access to various forms of reproductive healthcare, where perhaps even contraception may soon be disallowed. Or tell a gay kid that he might be moved to a state where the return of sodomy laws is openly discussed in the legislature. Heck no. Even if the new laws are not as bad as reported, they are a significant factor for many young people. reply me_me_me 18 hours agorootparentIt is chilling that this reads like a dystopian novel prompt. And on the other side of US men are women, women are men, and nobody is anything and everything at the same time. What a time to be alive. reply sandworm101 18 hours agorootparentDystopian but also something unique to the US. Each state is effectively a different country with sometimes radically different laws. But people are free to move about and select which set of laws they want to live under, at least so long as they are not pregnant. New parents may move to state A so their daughter can have reproducer freedom. Or a billionaire may move to state B so he can more easily sell stocks without being taxed. No other country organizes itself around such a pick-your-laws scheme. reply G3rn0ti 17 hours agorootparentSwitzerland did this even before the US. The US model is basically a copy of how Switzerland has been working (probably even more radically so with a much weaker federal government). But the emotional policies surrounding abortion in the US do not stem from their federal organization but rather roots in rather strong religious sentiments still more prevalent in the US than probably many other western countries. In Switzerland abortion has been formally legal since about 2001. reply silent_cal 18 hours agorootparentprevI'd say killing your daughter's child is a lot more horrific than not doing it. reply Communitivity 18 hours agorootparentprevSome of the other commenters hit the nail on the head. The reason is the removal of women's rights previously taken as a given. Also a consideration is the negative view/actions of some police in these states toward anyone who is considered non-standard or non-Christian in any way (e.g., at one point my daughter was close friends with a Muslim African-American). This is America, so according to the current laws those states have the right to do that. I personally think it's wrong, and would never subject my daughter to that, but my only option is to vote with my money and my feet - so I don't go to conferences in those states, and will never live in those states. “A fanatic is one who can't change his mind and won't change the subject.” ― Winston S. Churchill As a parting thought, I admire Nikolai Tesla, and Alan Turing. Tesla was suspected of being a communist and was in love with a pigeon. Turing was homosexual. How do you think either of these legendary minds would have fared had they been born in and lived in Texas? Would they have survived to do what they did? What would our world be like today if it had none of their publications or inventions? reply martincmartin 19 hours agorootparentprevMight be about reproductive rights and abortion. reply samatman 19 hours agorootparentprevnext [2 more] [flagged] V__ 19 hours agorootparentOr he just wants his daughter to survive childbirth. reply IggleSniggle 19 hours agoparentprevThe Earth is a spaceship, and it has an incredible propulsion system: the Galaxy! I do agree with you though, that it's worth spending some R&D time on exploring outside the spaceship. reply johnwalkr 14 hours agoparentprevThere’s similar jobs and companies in blue states too. reply thebiglebrewski 17 hours agoparentprevThis could be the real-life version of Helios one day! Yeah, it's a bummer that the majority of Space Software jobs are not remote friendly. You have to look really hard to find the ones that are. Some are because you have to actually physically be near the spacecraft to build and debug certain software or firmware but others are just because everyone else is doing that and it's harder for them to manage a small remote contingent. Still, I hope that this can change in the future, the space industry misses out on a lot of good software talent because of it. Maybe with better visualization technologies like the Apple headset this'll be more possible. reply pierat 18 hours agoparentprevnext [6 more] [flagged] FredPret 16 hours agorootparentEvery country has it's tradeoffs. Some have high tax and a good social net, some the opposite. Each of those have their advantages. But for 40% income tax, you would expect a hell of a lot more than what you're describing. Ironically the government could dramatically improve matters by interfering in your private life less, probably saving you money in the process as well. reply pierat 15 hours agorootparent> Every country has it's tradeoffs. Some have high tax and a good social net, some the opposite. Each of those have their advantages. And that completely glosses over there are demonstrably worse countries. North Korea, Eritrea, Russia, Uganda - for examples. And there's also significantly better countries too. The Nordic countries come to mind. And for the USA, try driving through rural America, Appalachia, any Native American reservation country.. hell, even inner cities. A few live it big, and the rest barely survive. It used to be better. Not now. > But for 40% income tax, you would expect a hell of a lot more than what you're describing. Exactly. I want democratic socialism. And the corporations have that here. The individuals? We get unabashed caveat emptor capitalism. > Ironically the government could dramatically improve matters by interfering in your private life less, probably saving you money in the process as well. Go away, libertarian. I am not one of you. This is the shit you get when you let let libertarians run \"government\": https://www.politico.com/magazine/story/2017/06/30/colorado-... reply FredPret 15 hours agorootparent> Go away, libertarian. I am not one of you. A little bit less anger is called for here. I'm not an American voter but this sort of thing is just a turn-off and pushes people away into the extremes. Wanting lower taxes and more individual freedom != Randian libertarianism, in much the same way as wanting a better safety net != Marxism. You complain about both abortion rights and paying 40% tax. Well, the state is spending some of those taxes to police your abortion rights. You can get two birds with one stone - either lower taxes or more spending on other things, and also stronger individual rights. reply pierat 14 hours agorootparent> A little bit less anger is called for here. I'm not an American voter but this sort of thing is just a turn-off and pushes people away into the extremes. Fair enough, my apologies. However the \"Ironically the government could dramatically improve matters by interfering in your private life less\" is a direct libertarian dog whistle. And my complaint about 40% tax isn't complaining about the tax per se, but that I want a proper social net for everyone. Instead, we're getting a social net for almost nobody. The republican party is also joined at the hip with evangelical christians who would love to create a Margaret Atwood-ean Handmaid's Tale country... In fact, they're a good step there at the federal court level and most republican led states. This is definitely separated from taxes, since this is turning everyone into stasi-like informants. Reducing taxes is not going to reduce higher profile women being found guilty of \"murder\" over abortion. Again, the better solutions here are to flea to a democratic state, or leave the country to a less shitty one. (Again, I reject the idea that all countries are roughly the same.) reply FredPret 13 hours agorootparent> Fair enough, my apologies. No problem. I'm not American so there's little point in swaying me one way or the other. I agree with you on pretty much all of it, but I don't think that my other statement was a libertarian dog whistle. It's just a policy that I think would work - the government is for running the justice system, defence, taking out the garbage, maybe a social safety net depending on who you ask, stuff like that. A little bit of libertarianism (certainly not all of it) is the perfect cure for a Handmaid's Tale scenario. Just one last point - some countries are certainly much better than others. I'm from Africa, have lived in Europe, and now I live in Canada, which is the best of all in my opinion. The USA though is amazing and far ahead of the rest in many ways - the drawbacks have been discussed but all that economic and social freedom does not count for nothing. I think all advanced countries have something to offer to someone who's aligned the right way. For 40% tax you'd expect a whole lot of nice things though! reply ctrlp 19 hours agoparentprevwhat does your having a daughter have to do with Texas, Florida, or Alabama? Don't you have a daughter wherever you live? reply spiderfarmer 18 hours agorootparentIf you are in the US, you must look at the taliban with disdain, for how they are treating women. As a European, we look at the regimes in the states you mentioned with the same disdain. No way I would ever consider living there. reply ctrlp 18 hours agorootparentHave you ever been to the US? You sound like a provincial who reads too much propaganda. reply spiderfarmer 17 hours agorootparentI visited all states in the western half and it was shocking to see that the infrastructure outside of the cities is more or less comparable with the poorest parts of eastern Europe. It did not change my opinion of the US for the better. reply ctrlp 15 hours agorootparentYour comment was about the treatment of women, though, not infrastructure. You probably know that Florida and Alabama are not in the western half of the US. Texas is so big it's often considered its own place, but much of Texas is considered part of the American South in culture and geography. Did you visit Texas? If so, which parts? Much of Europe can fit comfortably within the landmass of Texas, so maybe you just don't have an appreciation of the scale of the U.S. and what that means for politics, infrastructure, culture, etc... Having lived in France, Italy, and the UK for extended periods, I can say that your comments about Europe's superiority don't resonate. reply spiderfarmer 13 hours agorootparentYou’re right. While the state of the infrastructure shocked me, it doesn’t say anything about the backwards policies you have in your book burning, women hating states. reply ajoseps 19 hours agorootparentprevI think re: alabama the OP is referring to the recent IVF ruling in the state: https://apnews.com/article/alabama-frozen-embryos-pause-4cf5... reply jeremiahbuckley 19 hours agoparentprevI’m confused about this from an economic standpoint. Assume you could put aside an additional $5k while living in these states, wouldn’t that provide a hedge for the medical needs of your daughter? If needed, you could then travel to a location that provides what you are concerned about. (Not to get hung up on “$5k”, you could substitute any arbitrary number for this cost-benefit judgement) If move opened the possibility for a lucrative and rewarding career wouldn’t the trade off be worth it? reply tijtij 18 hours agorootparentThere was (or still is?) the abortion bounty-hunter law which would allow any private citizen in Texas to sue him if he did what you are proposing. There were also some counties that made it illegal to use their roads to transport someone for the purpose of getting an abortion. In Texas, there is a clear push to criminalize the act of going out of state to get an abortion. reply jeremiahbuckley 18 hours agorootparentSeems like laws against crossing state lines for a medical procedure wouldn't withstand a legal challenge because it involves out-of-state commerce. I'm not a lawyer, but that intervention seems to be aggressive and exactly the place courts/congress would step in. Also, in a scenario where it would does hold up, then CA/NY would start doing the same w/gun stuff. And then FL/TX would respond with something else against CA/NY. So... MAD should prevent states from being too aggressive in this area. Again, my concern is trying to figure out the statistically best way to navigate the various political landscapes and future political possibilities. It just seems too focused on one particular area of risk. I wouldn't categorically close out a good company / good career just because of this risk. This seems like the right place to recommend a hedge. reply coldpie 17 hours agorootparentHaving to participate in an ugly, media-circus-y court fight while also going through a medical procedure/emergency does not sound like a fun time. Unless you have a big appetite for drama and hardship, living in a red state is a really bad idea if you have a chance of becoming pregnant, intentionally or unintentionally. reply danpalmer 18 hours agorootparentprevThere have been legal challenges against travelling for the procedures we're talking about, there are laws on the books in some of these states that criminalise helping someone to leave the state to receive treatment. There's a reason why many companies who have offices here have offered relocation to employees so that they don't need to contend with these laws if/when they are enforced upon them. reply spiderfarmer 18 hours agorootparentprevI'm Dutch, an additional 5k per month would not buy me the feeling of safety I enjoy in the Netherlands. It's not just medical bills. It's also the crime and rape rates. You don't cover death and trauma with insurance. reply jeremiahbuckley 18 hours agorootparentOh, yeah, can't help you there. I was thinking you were commenting as a person in a blue state worried about a red state political policy. There's blue vs red and then there's Europe vs US. A whole different order of magnitude. I mean all the same economic freedom vs policy concerns trade-offs still apply; but I'm well aware that there is a massive rift in risk comprehension going in both directions. The US is huge and the stuff you see on the news isn't happening where people with nice salaries live (I mean, statistically never, the news is not statistically accurate). But I encountered a funny additional problem with that recently when talking to a European transplant which is: you need a car in the US. The US is huge and cheap and safe but you need a car. If you don't have a car you have to live within a subway system. Then it's not cheap/safe (pick one). reply spiderfarmer 13 hours agorootparentThe lack of thoughtful urban planning is only a small factor. Yes, you have 30% more traffic related deaths, but check all other quality of living related indices and you can only conclude the US is a not even competing. As a result, check your position in the happiness index and wonder if you, as a country, maybe fail your citizens. The only index where the US is leading is GDP, and that’s not a big factor in anyone’s life. You can’t put a price on feeling safe. There are lots of tiktok and yt videos of expats living here who explain just that. reply tcmart14 9 hours agorootparentWoah now, its been awhile since I looked but I think we also tend to be in the top 5 countries of number of people incarcerated per capita. If that doesn't spell happiness, I don't know what does. reply 1234letshaveatw 18 hours agorootparentprevI'm a citizen in the US, and conversely I wouldn't move to the Netherlands for any amount of money. Everyone values different things differently I suppose. reply spiderfarmer 17 hours agorootparentLet me guess, you're somewhat apathetic and the biggest factor is that you hate to see your taxes go to improving the lives of your fellow citizens. reply Anduia 17 hours agoprevIt is the size of a Tardis reply ngneer 16 hours agoprevThe Moon is a Harsh Mistress reply AlbertCory 16 hours agoprevWallace and Gromit did it a long time ago. reply integrits 9 hours agoprev50 years have passed and all we get are two crappy photos of the \"landing\". Hilarious. reply CartyBoston 19 hours agoprevThe spacecraft has an ad prominently displayed on itself, it's depressing. We will never reproduce the experience of 10 year old me watching the moon landing with my dad. It's all just egos and entertainment now. reply hn_throwaway_99 19 hours agoparentThis feels unnecessary cynical. I could understand if the spacecraft said \"Drink Ovaltine\" or something else just advertising with paid placement, but the brand marks on it are just highlighting the organizations the actually built the thing. I was originally confused/skeptical about Columbia, but they did actually contribute to the design and construction of the lander, even if this press release is a little puffed up: https://investor.columbia.com/news-events/press-releases/det.... Also, you say \"It's all just egos and entertainment now.\" What do you think it was in 1969? Is \"beat the Russians\" somehow a more noble goal than \"sell a product\"? reply dylan604 19 hours agorootparentEventually, spacecraft will look like a NASCAR paint job. reply jjkaczor 14 hours agorootparentBack when I used to help organized and run a technology-specific usergroup, where we were constantly working corporate sponsors for donations to pay for food/beverages, I joked that on meeting nights I would gladly wear a NASCAR style jumpsuit, emblazoned with every sponsor brand logo/slogan. At least we would be honest shills. Sigh... no one took me up on the offer. reply dylan604 14 hours agorootparentI still think that politicians should wear a jumpsuit with all of their corporate sponsors reply anigbrowl 13 hours agorootparentprevPerhaps briefly. Eventually they will look like cargo ships. reply dylan604 12 hours agorootparentWhat makes you think a cargo ship can't have a NASCAR paint job? reply anigbrowl 8 hours agorootparentIt's not that they can't, it's that they don't. reply rossdavidh 19 hours agoparentprevThe US flag on the early moon landings, was absolutely an advertisement; the whole thing was done as a propaganda riposte to the Soviet Union's Sputnik. Doesn't mean it's not awesome. reply azinman2 18 hours agorootparentThere is a meaningful difference between national pride and 2 for 1 at Dominos. reply johnwalkr 14 hours agorootparentIn 2001 Pizza Hut actually had its logo on a Russian rocket, and delivered pizza to cosmonauts on ISS as part of a robotic resupply mission. reply silvester23 18 hours agorootparentprevTrue, for instance no wars have yet been fought over 2 for 1 at Dominos. reply PH95VuimJjqBqy 16 hours agorootparent2 for 1 at Dominos has also, strangely enough, never protected anyone. Odd that. reply umeshunni 17 hours agorootparentprevNot yet reply Integrape 16 hours agorootparentThis is the first step. reply FactKnower69 14 hours agorootparentprevDiseased worldview reply jdelman 18 hours agoparentprevThat's because you were a child at the time. A child now will most likely have the same sense of awe and wonder you had, not the cynical point of view you've developed over time. reply Loughla 19 hours agoparentprevI get where you're coming from with the AD on the spacecraft. It's gross to see an ad for a clothing company on the moon. But, NASA is predominately displayed on all of the original moonshot crafts. That's an advertisement for that organization. . . And, I'm 100% sure ego had nearly everything to do with the original space race. Beating the Russians and what-not. That seems, in hindsight, to be very ego driven? reply alan-hn 19 hours agorootparentI think saying \"this organization built this craft that went to space\" is a bit different than \"buy shit from this other company\" reply nlarew 18 hours agorootparentThe Columbia logo on this craft is both. They're advertising the brand in a very cool and unique way AND they contributed significant heat shield technology to the craft itself. reply wolverine876 11 hours agorootparentprevNASA is an organization that represents the collective efforts of Americans (and others!). Columbia clothing is a private business that maximizes profit. reply treyd 19 hours agorootparentprevIt's about self-identification not advertisement. And NASA isn't a privately-owned for-profit corporation. It's like putting \"US NAVY\" on a battleship, except instead it's a vehicle furthering mankind's technological development. reply Terretta 5 hours agoparentprevIt's carrying a payload for Columbia, among other things, which makes this type of marking generally called a “sponsor logo” rather than an “advertisement”: “Besides NASA’s tech and navigation experiments, Intuitive Machines sold space on the lander to Columbia Sportswear to fly its newest insulating jacket fabric; sculptor Jeff Koons for 125 mini moon figurines; and Embry-Riddle Aeronautical University for a set of cameras to capture pictures of the descending lander.” https://apnews.com/article/moon-landing-private-company-intu... reply ordu 19 hours agoparentprevExperience is irreproducible. It depends on too many factors, we even don't know the full list of them, and some of them change irreversibly with time passing. You will not be 10 years old anymore. It is not a good reason for a depression, you can experience world now like you couldn't being a 10 year old boy. reply Loughla 19 hours agorootparentActually one of the absolute best things about having kids, that is not reproducible for no-child lifestyles, is seeing everything for the first time through their eyes. All the other stuff people say about parenting can be reproduced via service or volunteering or something else. But that experience absolutely is unique. Magic is real. The world is wild and exciting, and it's all there for that kid. It's amazing to watch and be a part of. So, while you can never go back to being at 10 year old boy, you absolutely can get a taste of what that's like via adoption or having your own. In my opinion, that is. reply bravetraveler 19 hours agorootparent> not reproducible for no-child lifestyles I see what you did there, heh! Just here to both support and counter this a bit. Kids are absolutely a great way to see joy in the world again! ... but they don't have to be yours. I've supervised those for my peers, played temporary dad, all of it. Sure, it's time-boxed, but that may be the goal. Trading seeing \"everything\" new, for \"plenty\" reply ordu 19 hours agorootparentprevAhh yes, empathy can give us experiences we are unable to get ourselves. reply kragen 17 hours agoparentprevthe moon landing rocket said \"usa\" on the side and had flags. that's the same thing, just a bigger corporation reply dylan604 19 hours agoparentprevI wonder what kind of analytics the adtech will bring them, and how invasive to their privacy it will be reply mikercampbell 18 hours agoparentprevI’m just glad it’s not something “too on the nose” comedically, like for Coca-Cola or something that makes me think of Wall-E. At least it’s a hiking/adventurey brand and not KFC. reply me_me_me 18 hours agoparentprevThe first Mars colonizers will stick a coca-cola flag in and everyone but few PR people will sigh collectively reply krapp 18 hours agoparentprevWe planted an American flag on the moon. An advertisement for the Coca-Cola of American imperialism versus the Pepsi of Soviet communism. The entire space race was literally nothing but ego and propaganda. reply projektfu 10 hours agorootparentWe also hit a few golf balls. Give us some credit. reply simmonmt 19 hours agoparentprevHow dare they attempt to defray their costs. reply austinjp 19 hours agorootparentHow dare they put an ad on the moon. We are truly spreading the worst of humanity into the cosmos. Good job it's only us that appear able to witness it. Defraying costs by using ads is a strawman. If you can't afford to do something, maybe don't do it. If you really, really want to do it, maybe ask yourself if the world genuinely needs what you're doing. If it does, find a way. If the only way you can do it is by selling advertising, you've taken as mis-step. reply samatman 12 hours agorootparent> If you can't afford to do something, maybe don't do it. Clearly they can afford to do what they just did. Oh, you don't like how they raised money? Good, pay attention to something else, they aren't doing things for your approval. reply austinjp 9 hours agorootparentI would dearly love to \"pay attention\" somewhere that isn't plastered in advertising. They didn't do it for approval, but they sure as shit didn't ask for approval either. And I'd like to see a breakdown of exactly how much the ad revenue contributed to payload delivery. reply ewjt 12 hours agorootparentprevThat's an extreme position to take that rests on the claim that sponsorship/advertising is objectively bad. Media & journalism have been underpinned by advertising for over a century. Tons of educational and informative services are available to the public for free because of advertising. Sponsorship has built art galleries, hospital wings, research centers, etc. In this case, there's a relatively innocuous logo on a robotic lander that is 230k miles away on a desolate rock. It's not like this is a billboard in a nature preserve. reply austinjp 9 hours agorootparentWhether advertising is objectively bad isn't necessarily the debate, but at some point it can cross a line. That line might be different for everyone, but most people will have it. You yourself give an example of something you suggest might be unaccaptable to some: > billboard in a nature preserve Where's the line? Why shouldn't we put billboards in nature preserves? reply dbrueck 11 hours agorootparentprevIt's almost as bad as when they put the Castile flag on the Santa Maria! reply wesselbindt 15 hours agoprevThis is great, it really proves that the free market can be just as innovative and efficient as the public sector. They managed to get there less than 50 years after the taxpayer funded space guys did. No small feat! Imagine all the things they must've learned along the way! reply JoeDaDude 14 hours agoparentYou do realize this mission is taxpayer funded. Intuitive Machines is getting paid as part of a contract to NASA under the Commercial Lunar Payload Services (CLIPS) program. https://www.nasa.gov/commercial-lunar-payload-services/ https://www.nasa.gov/missions/artemis/clps/intuitive-machine... reply FactKnower69 14 hours agorootparentNo way!! I thought private enterprise didn't need government handouts? reply wolverine876 12 hours agorootparentIt's poor people and small businesses who don't need government handouts. reply CatWChainsaw 7 hours agorootparentTBTF FTW! reply 2 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The IM-1 mission, headed by Intuitive Machines, aims to land the Nova-C class lunar lander on the Moon, overcoming obstacles like trajectory corrections and lunar orbit insertion.",
      "Flight controllers continuously share updates on the lander's health and advancements, with the final landing scheduled for February 22nd at 1649 CST.",
      "The mission highlights resilience, innovation, and collaboration, emphasizing these crucial aspects as it nears its much-anticipated lunar touchdown."
    ],
    "commentSummary": [
      "The conversation explores various space-related topics, including private companies landing on the moon, technological advancements, and challenges for doctors in states with strict abortion laws.",
      "It also touches on political debates, women's rights, and ethical concerns regarding advertising in space, emphasizing the complexities of space exploration and societal issues like reproductive rights and gender equality.",
      "Overall, the discussion sheds light on the interplay between space exploration, government backing of private initiatives, and societal dilemmas, providing a comprehensive view of relevant issues."
    ],
    "points": 459,
    "commentCount": 255,
    "retryCount": 0,
    "time": 1708596916
  },
  {
    "id": 39465517,
    "title": "Major U.S. Cellular Outage Disrupts AT&T, T-Mobile, and Verizon Users",
    "originLink": "https://www.cnbc.com/2024/02/22/cellular-outage-in-us-hits-att-t-mobile-and-verizon-users-downdetector-shows-.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN TECH AT&T cellular service restored after daylong outage; cause still unknown PUBLISHED THU, FEB 22 20245:28 AM ESTUPDATED MOMENTS AGO Ryan Anastasio @RYAN_ANASTASIO KEY POINTS A cellular outage Thursday hit thousands of AT&T users in the United States, disrupting calls and text messages as well as emergency services in major cities including San Francisco. About 58,000 incidents were reported around noon ET, according to data from outage-tracking website Downdetector.com. Shares of AT&T were down about 2% Thursday following the outages. In this article T Follow your favorite stocks CREATE FREE ACCOUNT A worker climbs on a cellular communication tower in Oakland, California. Justin SullivanGetty Images A cellular outage Thursday hit thousands of AT&T users in the United States, disrupting calls and text messages as well as emergency services in major cities including San Francisco. The company said service was restored to all affected customers shortly after 3 p.m. ET. \"Keeping our customers connected remains our top priority, and we are taking steps to ensure our customers do not experience this again in the future,\" the company said in a statement. AT&T said late Thursday that based on an initial review, the outage was \"caused by the application and execution of an incorrect process used as we were expanding our network, not a cyber attack.\" The company will continue to assess the outage. About 58,000 incidents were reported around noon ET, according to data from outage-tracking website Downdetector.com. AT&T, which put up a website for system updates, did not say how many customers were affected by Thursday's outage. The FCC said on X that it was investigating the incident and was in contact with AT&T and safety authorities. Shares of AT&T closed 2.41% lower Thursday. Phones affected by the outage displayed zero service bars in the top right corner of the device or the letters SOS. Customers were still able to make calls by enabling Wi-Fi calling. A spike in outages began around 4:00 a.m. ET and peaked at around 74,000 reported incidents at 8:30 a.m. ET, according to Downdetector. The AT&T outage affected people's ability to reach emergency services by dialing 911, a post on social media platform X from the San Francisco Fire Department said. \"We are aware of an issue impacting AT&T wireless customers from making and receiving any phone calls (including to 911),\" the fire department said. Atlanta Mayor Andre Dickens said in a post on X that the city could receive and make outbound 911 calls but that AT&T customers in the area had reported issues. \"We have received calls from AT&T customers that their cellular phones are in SOS mode. Please direct all inquiries to restore service to AT&T,\" Dickens said. The Massachusetts State Police said that people were flooding their 911 center with calls trying to determine if the service worked from their cell phones. \"Please do not do this. If you can successfully place a non-emergency call to another number via your cell service then your 911 service will also work,\" the state police said in a post on X. Users of Verizon and T-Mobile reported a few thousand outages each as of 10:00 a.m. ET, according to Downdetector. The reports were likely due to calls made trying to connect with other networks, both companies said. \"Downdetector is likely reflecting challenges our customers were having attempting to connect to users on other networks,\" T-Mobile said in an emailed statement. – Reuters, CNBC's Steven Kopack and Chris Eudaily contributed to this report. Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=39465517",
    "commentBody": "Cellular outage in U.S. hits AT&T, T-Mobile and Verizon users (cnbc.com)450 points by rooooob 22 hours agohidepastfavorite433 comments Animats 13 hours agoThis is a serious architectural flaw. In the entire history of electromechanical switching in the Bell System, no central office was ever out of service for more than 30 minutes for any reason other than a natural disaster, or, on one single occasion, a major fire in NYC. The AT&T Long Lines system in the 1960s and 1970s had ten regional centers, all independent and heavily interconnected. There was a control center, in Bedminster, NJ, but it just monitored and sent out routing updates every 15 minutes or so. All switches could revert to default routing if needed, which meant that some calls would not get through under heavy load. Most calls would still work. reply ok123456 11 hours agoparentThere was the Mother's Day outage of 1990. That was caused by someone swapping a break statement for a continue statement in some C code that handled the routing, and there was a cascading effect. Then again. That only affected long-distance service. reply jrochkind1 9 hours agoparentprevI've been thinking about how pretty much no USA infrastructure of today is as reliable as of the last half of the 20th century. Just my imagination, or true? And what does it mean? reply topkai22 8 hours agorootparentAirliners crash far less, road fatalities kept going down till 2010, so taking the broad view of were infrastructure is I’d say at least those got better. Anecdotally, I remember more electricity interruptions and plumbing issues when I was a kid, but that could be location dependent and I couldn’t quickly find good numbers going back that far. Edit: While the phone network didn’t necessarily go down, I frequently got “all circuits busy” when I was a kid. I don’t remember the last time that happened. reply bronco21016 7 hours agorootparentI wish we had metrics for utility companies. In my midwestern experience, things have gotten worse. I don’t remember any outages as a kid in the 90s that were over 24 hours aside from the major blackout in the early 2000s. As an adult I’ve experienced several outages greater than 24 hours in both summer and winter months. It’d be nice to be able to measure this. reply Aloisius 7 hours agorootparentI would caution comparing today against one's childhood memories. Children have few responsibilities and are shielded by their caretakers. They simply do not notice much of the things that happen. reply vel0city 4 hours agorootparentRight? As a kid we might not have questioned that sudden trip to Grandma's for the day. reply watersb 7 hours agorootparentprevAnd how often one of our five TV stations would be \"Experiencing Technical Difficulties... Please Stand By\". reply lor_louis 8 hours agorootparentprevRedundancy was needed because individual nodes/machines were more prone to failure. As machines got more and more reliable, having highly redundant infrastructure was seen as an extra cost. reply Animats 6 hours agorootparentYes. Electromechanical switching systems were substantially more reliable than their components. How this was done should be understood by anybody designing high-reliability systems today. \"A History of Science and Engineering in the Bell System - Switching Technology 1925-1975\" is a readable reference. The Internet Archive has it.[1] More hardcore: \"No. 5 Crossbar\"[2] The Connections Museum in Seattle still has a #5 Crossbar working.[3] Long distance used toll switches, \"#4 Crossbar\", and there were 202 of them. #4 and #5 Crossbar machines are collections of stateless microservices, implemented from electromechanical components. The terminology is used in the old books is completely different, but that's what they are. Each service always has at least two servers. The parts that do have state are distributed. The crossbar switches that make actual connections have state, but are dumb - they are told what to do by \"markers\", which are stateless but can read the state of the crossbars and of other components. Failure of a single crossbar unit can take down less than a hundred lines at most. Other than the crossbars to external lines, everything had alternate routes. Everything has fault detection, with lights and alarm bells. Error rates were fairly high. In the previous \"step by step\" system, a good central office misdirected about 1% of calls. With bad maintenance (and those things were high maintenance) that could get much worse. Crossbar was better, maybe 0.1% misdirected calls. Routing tables in crossbar were mostly static ROMs of one kind or another. Routing consisted of trying a predetermined set of routes, in order. Clunky, but reliable. Modern systems need a backdown to that mode. [1] https://archive.org/details/historyofenginee0000unse_q0d8 [2] https://archive.org/details/bellsystem-no-5-crossbar-blr [3] http://www.telcomhistory.org/connections-museum-seattle-exhi... reply kibwen 7 hours agorootparentprevEfficiency or reliability: pick one. reply dymk 7 hours agorootparentprevDo you work at a telecom or are you just guessing? reply onthecanposting 6 hours agorootparentprevHighway construction standards are much higher than they were 30-50 years ago, but it's a mixed bag. Administrative costs are significantly higher. Survey has dramatically improved with GPS. Highway engineering has not improved since about the 90s. Automated machine guidance has significantly improved the potential accuracy of grading operations in th last decade. reply spacebacon 6 hours agoparentprevI’m surprised no one noticed the 502 when attempting to enable WiFi calling. Azure was the source of the 502. Cloud architecture problem. reply Diederich 11 hours agoparentprevIn this context, what's the physical scale of a 'central office', as far as regional dimensions? Thanks! reply thedaly 9 hours agorootparentI suspect that when AT&T built all the COs (1950-70s) they were constrained by both max number of lines and physical distance. You’ll see numerous COs in a big city, but they are also pretty widely dispersed throughout the suburbs and rural areas. reply jsjohnst 7 hours agorootparentI worked for Southwestern Bell in the 90s pre-SBC (aka just before remote terminals and dslams became common). COs handled mid-tens of thousands of lines in big cities, for smaller more rural areas they generally covered a single town or less often there were a few in a county where the full county was under 50k people. In towns, we generally tried to keep loop lengths under 30k feet, but in rural areas that simply wasn’t possible. You’d often find remnants of party line systems in those areas and definitely load coils out the wazzoo. It was “fun” unwinding all that crap to install ISDN circuits and later DSL. I remember the old hats at the time laughing about VDSL saying “leave it to the nerds to dream up some unrealistic shit where the loop length can be at most 2k feet, where does that exist!?” not realizing a few years later RTs and DSLAMs would mean a significant portion of city and suburban customers would be. reply deelowe 8 hours agorootparentprevI used to work at BellSouth in outside plant engineering in the early 2000s. That's exactly what it was. Of course, by then any expansion was done via remote terminals and COs were becoming very antiquated. reply teeray 10 hours agorootparentprevWell, one is a skyscraper filled with equipment near the Brooklyn Bridge IIRC. reply lxe 13 hours agoparentprevThe complexity and scale of moderns systems are on another scale of magnitude. reply Animats 11 hours agorootparentWatch \"Without Fail\" (1967), on how the Bell System did it.[1] [1] https://www.youtube.com/watch?v=ZAJpionUxJ8 reply Scoundreller 13 hours agorootparentprevAnd centralized. Data is cheap (though they won’t admit that) while big iron cellular core stuff is expensive. Funny when they billed extra for long distance calls even though all calls were routed through one place for a huge geographic area. Calling your neighbour could be a hundreds of miles round trip over mobile. reply Animats 11 hours agorootparent> And centralized. Yes. Too much of routing is centralized. Since phone numbers are no longer locative (the area code and exchange number don't map to physical equipment) all calls require a lookup. It's not that big a table by modern standards. Tens of gigabytes. All switches should have a database slave of each telco's phone number routing list, to allow most local calls if external database connectivity is lost. It may be behind, and some roaming phones won't work. But most would get through. reply bluepizza 13 hours agorootparentprevThis is no justification. They should have another scale of resilience. reply lxe 13 hours agorootparentAgreed. Not a justification, but an explanation/excuse as to why systems are less reliable. You're right on the mark -- when reliability doesn't scale with complexity, you get this. reply danlugo92 7 hours agoparentprevTrue words :checkmark reply jader201 15 hours agoprevSomething I'm not seeing discussion on: What is/were the cascading effects of this, particularly for drivers? Many people in buildings were unaffected, as they could fallback to wifi. But I imagine this had a pretty broad impact to drivers. Just a few things I can think of: - Packages delayed (UPS, FedEx, Amazon, truck drivers, etc.) for drivers that relied on their phone's mapping apps to get them to their deliveries - Uber/Lyft/taxi/etc. drivers not able to get directions to their pickups/dropoffs - Traffic worsened because drivers weren't able to optimize their routes, or even get directions to their destination Maybe larger companies have their own infra for this, or have redundancy in place (e.g. their own GPS devices)? I'm curious to hear thoughts on whether these (and others) were impacted, or if there are ways they're able to get around this. Also, unrelated to drivers, I can imagine there is/was a higher risk of not getting treated for emergencies due to not being able to make calls (I'm not sure whether/how emergency calling was impacted). reply Scoundreller 14 hours agoparent> Traffic worsened because drivers weren't able to optimize their routes, or even get directions to their destination During Canada’s Rogers outage in 2022: > In Toronto there was some dependency on Rogers. One quarter of all traffic signals relied on their cellular network for signal timing changes. The Rogers GSM network was also used to remotely monitor fire alarms and sprinklers in municipal buildings. Public parking payments and public bike services were also unavailable. https://en.m.wikipedia.org/wiki/2022_Rogers_Communications_o... As it was summer, I recall some park programming for kids had to be cancelled because the employees were required to have a phone capable of calling 9-1-1 (but sounds like that at least still worked here) reply bostonwalker 12 hours agorootparentThis is putting it mildly. The Interac network went down and no one could use their debit cards nationwide. reply Scoundreller 11 hours agorootparentI'd put that on Interac single-homing itself without redundancy. Their ops are critical enough you'd expect better from them. Not the kind of shortcut Canadian banking takes for core stuff. reply tharkun__ 10 hours agorootparentThe funny thing there is that Interac \"did\" have redundancy i.e. another network provider to fall back onto. Unfortunately they failed to notice that this was a reseller for Rogers lines. reply Scoundreller 9 hours agorootparentthat explains a lot reply twisteriffic 11 hours agorootparentprevNot all of it. My credit union's interac services still worked. reply steelframe 10 hours agoparentprev> Traffic worsened because drivers weren't able to optimize their routes This might explain a huge random traffic jam I hit in the middle of my town this morning. I had no idea any kind of an outage was happening because I've intentionally scaled back my dependence on my phone. I always used to automatically pull up Google Maps to navigate no matter how short the trip. At some point I realized I was losing my ability to travel without being completely dependent on some company tracking my location and telling me what to do, so as part of my phone de-Googlification I switched to Organic Maps. And even then I try to navigate on my own without any GPS assistance as often as possible. I feel like navigating is a skill you can actually lose if you don't practice doing it. After running an errand across town this morning, I decided to try getting back home via the biggest arterial through the city that I know about, and I immediately hit a huge westbound backup stretching at least a mile. It was a total standstill. I peeked ahead trying to see if there was some kind of accident or something and didn't see anything. Everyone was just sitting in this traffic jam, and I couldn't for the life of me figure out why. I immediately flipped a u-turn and went 3/4 of a mile north to another westbound road I knew about. That one was completely clear of any traffic at all, and I was able to drive the speed limit all the way back. The most-used navigation apps I know of suggest alternate routes when there's congestion, so why were all those people just sitting there in that jam while a parallel road less than a mile away was clear? Maybe it was this cascading effect of too many people conditioned into being told what to do by their phones while their phones couldn't tell them to take the other route. reply bobthepanda 9 hours agorootparentthere is also a big of fear, that if you go to an alternate route it may also be congested, and it will all have been for nothing. radio is pretty good with traffic news, but how many people would even think of local radio? reply steelframe 9 hours agorootparent> if you go to an alternate route it may also be congested, and it will all have been for nothing Yeah, I get that there can also be a bit of a sunk cost thing along with regret minimization going on too. I think game theory suggests that you should switch routes the instant you hit significant congestion though, because P(congestion on the current route)=1 as soon as you hit it. reply pests 7 hours agorootparentprevI think many many people would check local radio, not everyone lives on their phone or in a tech bubble. reply vel0city 3 hours agorootparentMany people don't have a way of listening to local radio outside of their car. Loads of people don't even have a way to watch OTA TV. reply justsomehnguy 6 hours agorootparentprev> Maybe it was this cascading effect of too many people conditioned into being told what to do by their phones while their phones couldn't tell them to take the other route There is a lot of people who couldn't navigate to a neighboring street without a direct directions even if their life depended on it. Add to that what the most people doesn't have a slightest idea where are they, where are the cardinal directions and what they need to get from point A to point B. reply dktalks 14 hours agoparentprevIf you use Google Maps, it will automatically prompt you to download a map of the area if there is known poor coverage. It also has automatic (?) local maps. reply Scoundreller 14 hours agorootparentOne beef of mine with Google’s offline maps is that they’re only driving maps, and not walking/transit/cycling maps. Obviously you can kinda figure out walking paths anyway, but since I’m sometimes travelling without roaming access, it’s unfortunate. reply steelframe 10 hours agorootparentHave you tried Organic Maps for walking or cycling routes? reply giantg2 12 hours agorootparentprevI image it would be hard do transit maps if you weren't connected to get the schedule. reply Scoundreller 12 hours agorootparentThey already get them somehow while \"online\". Offline with beginning & end times and a rough idea of frequency should be good enough for local use. Offline road maps are subject to construction/seasonal/holiday route closures/deviations too, and so is transit. reply giantg2 11 hours agorootparentRoad closure tends to be much more rare. Transit is much more variable in the US. reply standardUser 15 hours agoparentprevWorth noting that GPS does not rely on cell service. reply offmycloud 14 hours agorootparentGPS on most cell phones uses data connection to download current satellite data in order to decrease the time from cold start to GPS lock. Lack of cell or WiFi can cause GPS to take 5-15 minutes to \"search the sky\" and download satellite data via low bitrate channel under poor signal conditions. https://en.wikipedia.org/wiki/Assisted_GNSS Edit: You can think of it as a CDN for the GPS almanac. reply sobriquet9 12 hours agorootparentFrom cold start. Most starts are not cold. The phone knows where it is, approximately, what time it is (within a second or so, from built-in RTC), and orbital parameters of the satellites overhead (maybe without the latest corrections). My Garmin watch gets a GPS lock in way less than 5 minutes without any cellular connection. reply dfadsadsf 11 hours agorootparentprevThat’s just not true for modern phones. I use iPhone on hikes without cellular connection and GPS lock is instantaneous. Organic Map app is great for hiking. reply AlotOfReading 10 hours agorootparentYou're talking about something very different called a hot start. The GP is discussing time to fix in a cold start scenario. You'd only see this on a phone that had been powered off for months, or \"teleported\" hundreds of miles away. In this scenario the receiver has to download the new time, new ephemeris data, and a new almanac (up to 12m30s in the worst case) before it can fix. Depending on the receiver, there may also be a delay of several minutes before it enters cold start mode. If the receiver has recently (last few days) gotten a fix and hasn't moved too much from that fix, it'll be in at least warm start mode. It still needs to download ephemeris data, but this usually takes 30ish seconds to fix. If the receiver has seen a fix very recently (last few hours) or a recent network connection, it can fix from hot start like you saw, which only takes a few seconds and may not even be observably slow depending on how the system is implemented. Phones go to great lengths to minimize the apparent latency. reply Tempest1981 1 hour agorootparentYou reminded me of my first GPS, which connected to a laptop via RS-232: https://www.bevhoward.com/TripMate.htm (Not me) Back then, just getting a GPS fix at all was exciting. Then driving around with it propped on the dashboard or rear window. reply rezonant 10 hours agorootparentprevThe sibling response here covers all of the points I would say. Scott Manley has a nice video covering the history of GPS and how it works, well worth a watch https://www.youtube.com/watch?v=qJ7ZAUjsycY It's not as simple as you think. reply Scoundreller 15 hours agorootparentprevThe routing does though. I have Google offline maps downloaded for areas I end up in just in this case. Gotta do traffic rerouting the old fashioned way though. Or have an old-school GPS map thingy in your glovebox. (Also have kiwix and a whole archive of Wikipedia on my phone). I wonder if meshtastic communicators sales took off during this. How’s LoRa traffic these days? reply freedomben 14 hours agorootparentYes, much that we think of as Google Maps relies on API calls made to the backend. Plus this assumes that you downloaded the offline maps ahead of time, which in my anecdotal experience is not something that most people really consider. GMaps does (or did at one time at least) have a neat feature of auto-downloading your home area map but, the one time I needed it, it didn't work. reply Scoundreller 14 hours agorootparent> which in my anecdotal experience is not something that most people really consider Thankfully I’m in Canada where it’s not impossible to end up in the sticks with no service. Chewing through your handful of gigabytes/month of data wasn’t hard. Only in the past year or so have double digit gigabyte/month data plans become cost-effective. And our roaming prices are extortionate, so for jaunts over the border (or internationally), I’ll sometimes go “naked”. reply bombcar 14 hours agorootparentprevThe \"Here\" app or whatever it is called did offline maps and offline routing decently enough. It wasn't perfect, but it worked for \"here to there\", even if it didn't find the best possible route. reply ianburrell 14 hours agorootparentprevGoogle Maps does offline routing. It doesn't do traffic routing but updating routing is better than nothing. reply 01HNNWZ0MV43FF 12 hours agorootparentprevI've been using this on Android for a couple years and love it: https://en.wikipedia.org/wiki/Organic_Maps You click a few buttons to download OSM tiles and then it does routing. The latest OSM even has a decent amount of stores, restaurants, etc., listed. reply a_gnostic 14 hours agorootparentprevCarriers have mapping independent of networks. Drivers keep personal GPS too. You would lose traffic and road conditions, I guess, but nothing proper trip planning wouldn't cover. reply Scoundreller 14 hours agorootparent> Drivers keep personal GPS too. Do they? I know there are a lot of old units out there but I figure people would have tossed them. At least I’ve found Waze has been pretty good at starting off with wifi and loading the map of the whole journey after coverage was lost with some resilience for stops/detours. reply treflop 11 hours agorootparentprevI am consistently in areas with zero cellular service and I’m reasonably sure Google Maps will route offline. At least, I’ve never switched to another mapping app because I couldn’t route — it’s more usually because Google Maps is more primitive areas is kind of detail-less. But even if it doesn’t, there are a ton of offline map apps that use OpenStreetMap data. reply ezfe 13 hours agorootparentprevApple Maps has offline navigation with historical traffic included reply steelframe 10 hours agorootparentprev> Or have an old-school GPS map thingy in your glovebox You can also install Organic Maps on your phone. reply LeifCarrotson 15 hours agorootparentprevGoogle Maps and now Apple Maps (as of ~6 months ago) have offline maps, but not by default. If you enable and download them for your area of interest you can use a subset of the normal app. I make sure to have this around my usual area and anytime I travel to an area with poor coverage, plus my Garmin watch has offline maps and GPS everywhere, but this is not typical. OSMand usage is even less common. reply tnel77 14 hours agorootparentOffline maps are a life saver in areas with bad coverage. One of the first things I setup for a new phone or when I’m headed somewhere new on vacation. reply freedomben 14 hours agorootparentThis is one of the most interesting differences I often notice between users who rarely leave the city and those who routinely leave. Offline functionality often seems unnecessary at best and absurd at worst to the former group, while the more rural/remote the person the more they value offline functionality. For the most extreme example, talk to the average person who lives outside of Anchorage or Fairbanks in Alaska, and they only really care what the app can do when it's offline as that is it's assumed status when on the go (disclaimer: I moved out of alaska a little over 5 years ago so things might have changed somewhat). reply ghaff 14 hours agorootparentYeah, if I'm going to travel internationally or if I'm somewhere I know I'll have spotty cell service, I'll download maps. I should probably be better about doing it in local areas where I \"assume\" things will be fine. reply tnel77 13 hours agorootparentprevI grew up in a rural area and lived in Colorado for a while. Going home or venturing into the mountains often resulted in bad service so it just became second nature. Good observation! reply maxerickson 14 hours agorootparentprevLots of people dislike the design choices in OSMAnd, so it's worth mentioning that there are lots of apps that use OSM data and provide offline maps and routing. reply fhdkweig 14 hours agorootparentprevagreed. On Google Maps app, there is a feature called \"offline maps\" which allows a user to select a rectangle on a the map and download all the street info inside it. A whole US state can fit in less than a few hundred megabytes. I have all the city I live in downloaded so I can go on walks without needed to use my data plan. reply a_gnostic 14 hours agorootparentNot as useful as back when google maps required a 5GB download IIRC reply herbst 1 hour agorootparentDownloaded a whole island yesterday. Was 40mb. That's a lot better now, has less resolutions packed in tho. reply jader201 13 hours agorootparentprevThat's assuming you have it on and updated before you hit the road. I think it's off by default, and I'm guessing most people haven't thought to turn it on, or are even aware of it. reply patmorgan23 13 hours agorootparentI'm pretty sure maps caches the data around you if you've used it somewhat recently. It saves Google bandwidth too. reply jader201 12 hours agorootparentI’m not so sure. Anecdotally, I’ve made it to a remote destination using Maps, then hopped back in the car an hour later (with no signal), and it couldn’t load anything. This seems to happen quite often. reply Scoundreller 11 hours agorootparentMaps used to expire after 30 days (no idea why), and the auto-updating while on wifi wasn't great unless you were in the app forcing it update. Nowadays they last 365d. reply rpd9803 12 hours agorootparentprevworth noting that without cell service, GPS can reliably give you time, lat, long and elevation. So if previously you had no actual map downloaded, or an old or out of date map, you'd just get a pretty accurate dot on an inaccurate map, or just raw coordinates. reply panarky 15 hours agoparentprevAlso can't login to sites that require SMS 2FA. reply giancarlostoro 14 hours agorootparentThis really shouldn't be the only way to verify its you if its going to prompt you every single time. reply panarky 5 hours agorootparentTell that to $8 trillion Schwab and $12 trillion Fidelity. reply cmg 13 hours agorootparentprevMy service came back around 1:30PM in Connecticut. Data and calls are working fine. I requested a 2FA code at 2:30 from a service that only offers SMS. An hour and a half later, I still haven't gotten it. reply danesparza 14 hours agorootparentprevWell ... let's be honest: SMS 2FA shouldn't be a thing. TOTP or stronger, please. reply steelframe 10 hours agorootparent> TOTP or stronger, please One of the biggest weaknesses with TOTP apps I've tried using is that you have to remember to transfer them to a new phone before you get rid of your old phone. I once got locked out of a domain registrar because I set up TOTP on an old phone many years back. That was long gone by the time I wanted to do something with my domain. TOTP is fine, but always give me recovery codes I can print and out and keep with my other important documents. Too many services don't do that. reply Johnny555 10 hours agorootparentprevIsn't that what passkeys are suppose to be? Better and stronger than passwords with TOTP? reply SpaethCo 13 hours agorootparentprevTOTP or SMS, it's just another text password you're entering in that's fully phishable. TOTP just \"feels\" more secure. reply ezfe 13 hours agorootparentSMS 2FA is a code that you're entering from a phone number. The \"risk\" is that your phone number can be ported without your permission, and then someone else can get the code. TOTP is more secure because it isn't tied to a phone number. You're right that it's still phishable but that's not the point. In both cases, the primary benefit to the general population is to have a rotating credential that, if one website is hacked, is useless on another website. reply jjav 12 hours agorootparentprevNo, TOTP is far more secure because it has no dependence on a third-party who can mess up in many ways (Denial of service like in this case by being unavailable, Impersonation by allowing SIM swaps or intercepting messages directly). You fully control how to store the TOTP seed and how you compute the value, so it is far more secure. Yes, it can be phished if you fall for that, but it removes several attack vectors. reply SpaethCo 10 hours agorootparent> Yes, it can be phished if you fall for that, but it removes several attack vectors. How was the first factor (the password) compromised? Assuming the user is using site-unique passwords, in 99% of cases where an attacker obtains a functional password they can get at least one TOTP code or the seed in the same manner. (ie, if I can steal your password DB, odds are pretty good for me stealing your TOTP seed DB as well.) The outcome of a single successful authentication is a longer-lived session cookie. Once an attacker has that they can reset your creds (usually just requiring re-entering the password) and the account is theirs. IMO, the only 2nd factor that matters are those that mutually authenticate like PassKeys / FIDO keys. reply Scoundreller 11 hours agorootparentprev> You fully control how to store the TOTP seed Sorta. The seed still needs to be issued to you in some way. reply throwway120385 12 hours agorootparentprevTOTP is more secure in that you can't be simjacked by someone impersonating you in the cell phone store. reply SpaethCo 10 hours agorootparentThat's assuming your attacker already has your password, or the service allows SMS password reset. (thus negating the second factor. Essentially SMS becomes the only factor.) reply sandworm101 13 hours agoparentprev>> Traffic worsened because drivers weren't able to optimize their routes I'm not sure that is a thing. The vast majority of drivers are on familiar routes and are not navigating via electronic means. Better question: How are the autonomous cars doing? Are they parked by the side of the road unable to navigate without cell coverage. reply jader201 13 hours agorootparent> The vast majority of drivers are on familiar routes and are not navigating via electronic means. I've been rerouted due to an accident many times, and I've seen the detours get backed up because of people taking more optimal routes (without traffic being redirected via other means). I'd be curious to see more data on it, but I would speculate it's less than the \"vast majority\". > Better question: How are the autonomous cars doing? Are they parked by the side of the road unable to navigate without cell coverage. Yeah, that falls under my point about Uber/Lyft/taxis. I would speculate there is broader impact from those vs. autonomous cars (that are probably still relatively uncommon). reply bobthepanda 9 hours agorootparentprevups is known for optimizing route planning for time and fuel efficiency purposes, particularly biasing towards three right turns instead of a left reply taurath 13 hours agorootparentprevIt only takes a few people missing an exit and swerving to create a bunch of traffic. So many people are used to not navigating manually anymore I can’t imagine it doesn’t have a big effect. reply sandworm101 13 hours agorootparentI have a 20-mile commute. I used my phone on day one at the new job, then never again since. It just isn't worth the effort for a road I've driven literally hundreds of times before. Do people also use google maps to get them from their front door to their garage? From the grocery store to wherever they parked their cars? reply jader201 12 hours agorootparentIt depends on your daily commute. If I need to drive 20 minutes with most of it on the expressway, and they’re prone to accidents and there are multiple viable routes, I’m 100% going to load it up on Maps every trip, if it will save me being delayed 10-60 minutes every few weeks. But if I’m going mostly backroads, probably not worth it, since you can more easily go around accidents, and they’re less common. But again, I’m guessing more city expressway commuters use navigation daily than you think. reply calfuris 9 hours agorootparentprevI use it every day. There are two roughly equivalent paths that I could take, so I use it for information on traffic conditions, and then I leave it running on the off chance that it might route me around a slowdown that wasn't present at the start of my commute. reply dgacmu 11 hours agorootparentprevI do. Mostly from curiosity about which way Google will suggest I go; sometimes because the traffic or road-closure awareness is useful. Though it's often the case that I know about the road closures before it does -- but sometimes it surprises me in a pleasant way. reply wepple 10 hours agoparentprev> Maybe larger companies have their own infra for this, or have redundancy in place (e.g. their own GPS devices)? Modern trucks have cell modems tied to a private APN that are used for updating vehicle firmware & doing telematics. They also typically have a route to the internet that provides a WiFi hotspot in the cab. Depending where the fault was in the telco stack, that APN may have still been functional Not saying this was a significant resolution, but at least a possibility. reply devgonewild 8 hours agoparentprevIn Australia we recently had a telecom outage with Optus; there were untold amount of damage - card payments at shops/cafes were out. - rural towns completely cut off (a few in particular are only serviced by Optus) - emergency services unavailable; for example a snake wrangler was unable to receive his call-outs - hospitals infrastructure came to a halt And I'm only going off of examples I have heard. These outages are very damaging. reply luxuryballs 8 hours agorootparentcashiers always look at me dumbstruck when I tell them about the mechanical offline credit card machines we had when I was a cashier back in 2004 reply james-skemp 7 hours agorootparentIf we're thinking of the same thing, I have fond memories of those at mall department stores in the 80s and 90s. I think Sears around '04-06 (?) was the last time I saw one of those used. I think I bought a dehumidifier or air purifier. When they started rolling out credit and debit cards without the raised numbers I thought fondly of those and how they were definitely done for now. reply silisili 10 hours agoparentprevWhat about those mobile card readers like you see in small businesses and food trucks and such? I've never owned one, but assumed they ran over cellular. reply GravityLab 11 hours agoparentprevWe need to maintain paper-based systems of information storage and retrieval. People should be familiar with a physical map. If we are too dependent on the technology, that is a risk. reply Johnny555 10 hours agorootparentJust keeping the paper isn't a solution, people need to know how to use the back up, and use it regularly. When I delivered pizzas, we had a big paper map of the city that we used to consult for deliveries, drivers quickly learned where nearly all of the streets in the city were and how to get there. For most deliveries, drivers just knew where to go, for the rare times I didn't, I either remembered the main street near my delivery or wrote down some notes on the box. Someone marked new streets on the map, as well as the names of major apartment complexes. Just having that map on the wall isn't going to do any good since without regular use, no one's going to be able to use it effectively. And it's doubtful that people can be forced into using it. reply dheera 12 hours agoparentprev> What is/were the cascading effects of this, particularly for drivers? I wish for an economic system in which all causes could be backpropagated to the source and the source be held responsible. If for example I lost 2 hours of my time today because I had to fight with Comcast, Comcast should be charged for 2 hours worth of my hourly salary. If I lost a job offer because of bad interview performance because of heating issues because of bad maintainence on part of landlord, landlord should be charged for the difference in time until I get my next job offer or the difference in salary until the next job offer. If I had to fight health insurance for 5 hours on the phone due to incorrect bill and that caused me additional stress that caused my condition to worsen, health insurance should be held liable for the delta effects of that stress. In this case the cellular operators in question would be held liable for the lost incomes of those drivers plus the lost incomes of passengers who lost money because they couldn't get to their destinations on time or missed flights and had to rebook them. I know this level of backpropagation is hard to implement in the real world but it would be awesome if the entire world were one big PyTorch model and liabilities could be calculated by evaluating gradients. reply kevin_nisbet 17 hours agoprevTo everyone trying to speculate on the root cause, I haven't seen enough information in any of the comments to really draw any conclusions. Having worked on several nationwide cellular issues in Canada when I worked in telecom, we saw nationwide impacts based on any number of causes. - A new route injected in the network caused the routing engines on a type of cellular specific equipment to crash nationwide. This took down internet access only from cell devices nationwide. But most people didn't notice because it happened at 2AM maintenance window and was fortunately discovered and reversed before business hours why the routing engine was in a crash loop. - A tech plugged in some new routers, and the existing core routers crashed and rebooted. While the news worthy impact was just a regional outage for something like 20 minutes, we discovered bugs and side effects from the Pacific to Atlantic coasts over the next 12 hours. So when you say you're impacted at location x, that data point could be everyone is down in the area, many people are having issues, or only one or two people have issues spilled over to some other region. This is why seeing it does or doesn't work in location x is limited value, as almost every outage I've investigated could result in some people still having service for various reasons. The question is in a particular area is it 100% impact, 50% impact, or 0.001% impact. - A messaging relay ran into it's configured rate limit. Retries in the protocol increased the messaging rate, so we effectively had a congestion collapse in that particular protocol. Because this was a congestion issue on passing state around, there were nationwide impacts, but you still had x% chance of completing your message flows and getting service. And then there was the famous Rogers outage where I don't remember them admitting to the full root cause. It's speculated that they did an upgrade on change on their routing network, which also had the side effect of the problem booting all the technicians from the network. Then recovery was difficult because the issue took out the nationwide network and broke the ability for employees to coordinate (you know because they use the same network as all the customers who also can't get service). All the CRTC filings I reviewed had all the useful information redacted though, so there isn't much we can learn from it. So it's fun to speculate, but here's hoping at the end of the day ATT is more transparent then we are in Canada, so the rest of the industry can learn from the experience. reply Scoundreller 17 hours agoparentRogers, of course, blamed their vendor (Ericsson I believe it was). Rogers can do no wrong! Of course, was fun to see yet another huge org have no back-out/failure plan for their potential enterprise-breaking changes. No/limited IT 101 stuff here. The only positive thing we learned was that the big 3 (really 2) telcos thought it would be a good idea to give eachother emergency backup sims for the other network to key employees in case their network went down. They did that in 2015, but better late than never. Fun that Rogers used the same core for wireless and wired connections, so many of us were in total blackout, even if we used a 3rd party internet provider that ran over Rogers. Like, everything including their website was down, corp circuits, everything with non-existent comms from Rogers. Thankfully my org was multi-homed and switched over its circuits at 6am so on-site mostly continued without issue. Also fun where the towers remained just powered on enough for phones to stick to them but not be able to do anything, so 9-1-1 calls would just fail, instead of failing-over to other networks. Seems like a deficiency in the GSM spec (or Rogers SIM programming?) that I don’t think was actioned on. https://en.m.wikipedia.org/wiki/2022_Rogers_Communications_o... reply kevin_nisbet 17 hours agorootparent> Also fun where the towers remained just powered on enough for phones to stick to them but not be able to do anything, so 9-1-1 calls would just fail, instead of failing-over to other networks. Seems like a deficiency in the GSM spec (or Rogers SIM programming?) that I don’t think was actioned on. Actually, I think this is going to change after the Rogers outage, it's just slowly happening behind the scenes so it's not getting much attention these days. The government has mandated a lot of industry response to failover between providers... we'll see where they land after all the lobbying happens. I do think implementations are changing a bit around this, mostly in the phones so that they give up and go into a network scan if the emergency call is failing. I worked mostly on core network stuff, so I was a layer removed from the towers, but if they hadn't lost management access they would've been able to tell the tower to stop advertising the network and 911 service. I do understand the question of from a vendor implementation perspective of how automatic this should be though... because automation in this regard does have some of it's own risks and could complicate some types of outages or inadvertently trigger and confuse recovery of problems. I'm with you though there should be an automatic mechanism to fail over to other network operators, I just haven't thought through all the risks with it and I hope the industry is taking their time to think through the implications. reply Scoundreller 16 hours agorootparent> I do think implementations are changing a bit around this, mostly in the phones so that they give up and go into a network scan if the emergency call is failing It seems like this is a global problem, since all Rogers-subscribed devices in a Rogers reception area couldn’t make 9-1-1 calls. But could be a SIM coding issue and not afflict other providers elsewhere. I just always imagined the GSM spec was so resilient that you could always make a 9-1-1 call if a working network was available but this outage proved that wrong. Surprising to learn in 2022. Of course it’s Canada, so I agree with them that the thought of letting users failover to a partner for everything would thrash the partner’s networks. Even though Canadian subscriber plans are laughably low in monthly data and population density is low (per the telecom’s usual excuse for our high prices) it turns out the telecoms still underbuilt their networks to have less capacity than what other networks internationally built out to support plans available on the international market (e.g. close to truly unlimited data/free long distance calls) reply toast0 16 hours agorootparent> I just always imagined the GSM spec was so resilient that you could always make a 9-1-1 call if a working network was available but this outage proved that wrong. Surprising to learn in 2022. The X is broken but claims it isn't stops failover pattern is strong all over networking. It's not unusual to see it in telco root cause analysis. reply kevin_nisbet 15 hours agorootparentprev> I just always imagined the GSM spec was so resilient that you could always make a 9-1-1 call if a working network was available but this outage proved that wrong. As I recall it is slightly more nuanced than this and was particular to the failure mode, and has a couple of different things aligning to create the failure mode. If you're phone is just blank, no sim card. To make an emergency call, it has to just start scanning all the supported frequencies. This is very slow, tune radio, wait for the scheduled information block that described the network on the radio protocol. See if it has the emergency services bit enabled. If not, tune to next frequency and try again. I used to remember all the timers, but almost a decade later I can't remember all the network timers for the information blocks. The sim card interaction, is say you're at home and you boot up your phone with 100% clean state. You don't want to wait for this scan to complete, so the SIM card gives the phone hints about which frequencies the carrier uses, so start on frequency x to find the network. But if you roam internationally, it can take alot longer to find a partner network, and there are some other techs around steering to preferred partners, but I don't know that those come into play here. I don't know but would be surprised if there is a SIM option to try and pin the emergency calls to a network, I think it's more likely the interaction is this hint on where to start the scan. The way the rogers network failed, it appears to me it caused the towers to stay in a state where they advertised in their radio block the network was there, and the 911 bit was enabled so the network could be used for emergency calls. This is where I don't really have the details since they haven't been public about it, how much of their network was still available internally. Maybe the cell towers could all see each other, that network layer was OK, and the signalling equipment was all talking to each other as well. That's the part I don't really know and have to speculate, as well as the tower side since I was a core person. So because the towers had enough service to never wilt themselves, they kept advertising the network, along with the 911 support. But then when you try to activate an emergency call, somewhere in the signalling path, as you get from tower to signalling system, to the voip equipment, to the circuits to the emergency center the outage knocked something out. Oh and for all these pieces of 911 equipment, there are two of everything for redundancy... two network paths, two pieces of equipment, etc. And because they lost admin access to their management network, no one could go in manually and tell the towers to wilt themselves either. If the towers had just stopped advertising 911 services, the phone would fall back into the network search mode as I described when you have no sim card. It just starts scanning the frequencies until it see's an information block for a network it can talk with the emergency support advertised to and does an emergency attach to the network that the carriers will all accept (An unauthenticated attach for the sole purpose of contacting an emergency center). So my suspicion is because carriers are so used to we have two of everything, and all emergency calls are marked for priority handling at all layers of the equipment (they get high priority bits on all the network packets and priority CPU scheduling in all the equipment), this particular failure mode where there was a fault somewhere down the line, and they lost control of the towers to tell them to stop advertising 911 services all sort of played together to create the failure mode. reply mjevans 15 hours agorootparentMulti-faceted failure mode. 0) At the network terminal level (mobile phone): at least for emergency calls if a given network fails to connect, fail over and try other networks. Even if the preferred networks claim to provide service. 1) At the network level: failure thresholds should be present. If those thresholds are crossed enter a fail-safe state. This should include entering a soft offline / overloaded response state. 2) Where possible critical data paths should cross-route. Infra Command and Control and Emergency calls in this case. Though if Roger's issue was expired certs or something the plans for handling that get complicated. reply Scoundreller 14 hours agorootparentit’s that “0” level that surprised me the most here. Days later, Rogers said you might be able to pull out/disable your SIM card to call 9-1-1, but then it depends: if Rogers is the strongest network, you might end up in the same predicament anyway. reply Fatnino 16 hours agorootparentprevSay there is an outage at the 911 call center. Now you try to call, don't get through, and your phone writes off that tower. Who were you planning to call after 911? Too bad, should have placed that call first. reply vlovich123 15 hours agorootparentYour phone would try other towers from other providers. If 911 is experiencing an outage that’s a separate issue that needs to be mitigated at a different layer. Even still, 100% uptime is difficult and expensive. reply MichaelZuo 17 hours agorootparentprev> Fun that Rogers used the same core for wireless and wired connections, so many of us were in total blackout, even if we used a 3rd party internet provider that ran over Rogers. If it ran over Rogers circuits then why wouldn't it go down too? Isn't that the case everywhere? reply Scoundreller 16 hours agorootparentI just know that a part of Rogers’ response was to separate their cores between wireless and wireline so that the risk of both going down simultaneously would be reduced. The 3rd party providers aren’t white-label resellers, but there’s obviously some overlapping susceptibilities to going down when Rogers breaks something. Depends what they break, and in this case, it took them down too. reply flippy_flops 16 hours agoparentprevThe speculation is fascinating. For most people, their guess is a reflection of themselves. Is there a term for that? This is a gross generalization, but I've seen... - Science people guessing solar flares - My \"right-wing friend\" guessed international hackers - I, myself, guessed it was a botched software release - Someone in this post commented their military friend says get gas And yet, like everyone else, I genuinely feel that I'm probably right reply BuyMyBitcoins 16 hours agorootparentMy speculation is: “Higher-ups kept demanding that technicians ‘do more with less’ in order to deliver on quarterly metrics and now we’re finally seeing the cumulative result of employees being stretched thin, underpaid, and overworked.” You are welcome to infer as to why I’m thinking this way! reply bregma 15 hours agorootparentSo how is the job search going? reply nonethewiser 15 hours agorootparentprevObviously you’re a self loathing executive. reply gnuser 15 hours agorootparentprevThe ops team can run the whole company and better without the C-Suite is my impression of modern day SV. Agile stickers on waterfall gates… reply spazx 15 hours agorootparentprevThis is my bet; and mayyybe some external bad actors taking advantage of the situation on top of that. reply Scoundreller 16 hours agorootparentprev> Someone in this post commented their military friend says get gas The Rogers outage in Canada took out the nationwide debit card payment network because that infra depended on Rogers. Credit cards still worked, but depends on your station’s access to make the transaction. And no shortage of shops running their POS “in the cloud” and needing to close if they lose internet access. I actually did have to lend cash to a colleague to buy gas to get home during that Rogers outage. All it takes is for one pipeline valve to depend on a cellular connection for billing to get the whole line shutdown. And ugh, we hope for a botched software upgrade too, but a corp cyberattack is so much harder to recover from so can’t be discounted from the realm of possibilities. I know that’s where my mind went with Rogers given how thorough their outage was. Was kinda unimaginable for a total outage to happen with no org comms ready to go in the pipeline. Your plans are supposed to have those comms ready for a bad update that you’ve been planning for weeks. It’s a cyberattack where you may stay silent. But I know Rogers isn’t going to admit fault until they find someone else to blame. reply charcircuit 16 hours agorootparentPoS devices are usually networked. If you don't validate transactions in realtime you would later validate in batch, but that has more risk than validating at the time of transaction. reply Scoundreller 16 hours agorootparent> If you don't validate transactions in realtime you would later validate in batch yeah, a lot of orgs just don't enable that (or don't have a process to enable it as required, and have difficulty pushing out a notice to do so if the network is down!). Also can only do offline credit card transactions. Can't with our Interac (Canadian-only) debit network. Unsure about Visa/Mastercard debit transactions. reply rescbr 13 hours agorootparent> Unsure about Visa/Mastercard debit transactions. AIUI, the debit card itself enforces online confirmation, even if the transaction goes through the credit card rail. reply r721 4 hours agorootparentprevIt looks like you were right: >A temporary network disruption that affected AT&T customers in the U.S. Thursday was caused by a software update, the company said. >AT&T told ABC News in a statement ABC News that the outage was not a cyberattack but caused by \"the application and execution of an incorrect process used as we were expanding our network.\" https://abcnews.go.com/US/att-outage-impacting-us-customers-... https://news.ycombinator.com/item?id=39477187 reply mlyle 16 hours agorootparentprev> And yet, like everyone else, I genuinely feel that I'm probably right This is the thing with black swan events. The more pedestrian explanations are almost always true, but then there's a tiny fraction of the time where you're much, much better off having taken a bit of an alarmist view. reply nonethewiser 15 hours agorootparentprevI literally caught myself thinking about a cyberattack merely because its sort of exciting (albeit terrible). And then realizing despite its prominence in my mind, it’s probably not the most likely cause (although certainly plausible still). And furthermore, that my mind gravitates to that without any real information suggesting it over other explanations. More about fearing for the worst instead of what you want I think. reply akira2501 9 hours agorootparentprev> I genuinely feel that I'm probably right We are wired that way for a reason. Until you personally see conflicting evidence you have to make an assumption or you would spend your life paralyzed or ignorant. Biology rewards action more than accuracy. reply ShamelessC 16 hours agorootparentprev> Is there a term for that? Projecting, biased. reply booleandilemma 16 hours agorootparentprevA type of availability bias, maybe? https://en.wikipedia.org/wiki/Availability_heuristic reply anitil 6 hours agoparentprev> All the CRTC filings I reviewed had all the useful information redacted Is this common in the industry? reply Y3Jlbmd1dGEK 12 hours agoparentprevI don't think it has anything to do with routing by looking at the comments on down detector. Many people report they are in the same household, and one person out of six (in the household) experiences the problem while all are on ATT. It sounds more like an upgrade that went through halfway, or, considering the time it happened, maybe a rollback that went only half through. reply giancarlostoro 14 hours agoparentprev> some people still having service for various reasons. I assume roaming being one of the top reasons no? reply DebtDeflation 19 hours agoprevVerizon and T-Mo both issued statements that they have no outages and the issue is just their customers being unable to call AT&T customers. Looks like most of the AT&T network in the US is down though. reply ethbr1 16 hours agoparentData point on ATT (via MVNO) in Atlanta: was connected until ~11:00 EST, then booted off and haven't reconnected. reply Tarball10 11 hours agoparentprevA theory for the reported Verizon/T-Mobile issues is that when AT&T went offline, all of those phones went into SOS mode and tried to register on the remaining available networks (Verizon and T-Mobile) to allow 911 calls to be made. The surge in devices registering at once may have overloaded some parts of those networks. reply peteradio 18 hours agoparentprevMy wife has google-fi and her coworker has verizon. Both of them say they can't make calls. reply whynotminot 18 hours agorootparentAny chance they’re trying to call an AT&T customer? reply cddotdotslash 18 hours agorootparentprevAnecdotal, but I have Google Fi and was on a ~1 hour call this AM during the height of the outage and had zero issues. reply inferiorhuman 17 hours agorootparentprevI've been tethering with T-Mobile as my primary internet connection and that's been working just fine. Voice also works for me with both TMo and Google Fi. reply nonethewiser 18 hours agorootparentprevand google-fi uses T-Mobile I believe reply darkmarmot 18 hours agoparentprevnext [2 more] [flagged] pitaj 16 hours agorootparentThat sounds entirely unrelated to this outage which began only a few hours ago. reply davidjade 15 hours agoprevIt’s likely unrelated, but bad timing for AT&T as they have applied to end landline service in some areas of California. https://www.wired.com/story/att-landline-california-complain... reply chevman 15 hours agoprevOutage Over Status: Restored AT&T FINAL, Service Degradation, Global Smart Messaging Suite AT&T Global Smart Messaging Suite Event description: FINAL, Service Degradation Impacted Services: MMS MT Start time: 02-21-2024 22:00 Eastern, 21:00 Central, 19:00 Pacific End time: 02-22-2024 11:00 Eastern, 10:00 Central, 08:00 Pacific Downtime: 780 minutes Dear Customer, We are writing to inform you that Global Smart Messaging Suite is now available. The MMS MT service has been restored and our team is currently monitoring Thank you. AT&T Business Solutions Kind Regards, The AT&T SMS Service Administrator reply nonethewiser 15 hours agoparentI think a communication like this should include that they are investigating the root cause (assuming they aren’t completely sure) and that they will share it, and state where. Maybe im reading to much into it but it bothers me that thats not in the communication. reply freedomben 14 hours agorootparentThey most certainly are investigating the root cause, and probably there's a witch hunt developing, but as far as customers go I would expect AT&T's attitude to be \"none of your business.\" I've worked with many of these types of companies before, and outside of the occasional cool CS rep, their cultures are lots of information hoarders and responsibility dodgers. Taking responsibility for a problem is a good way to ensure you never get promoted. reply keanebean86 14 hours agorootparent\"A recently departed employee had a core router's power going through a wall switch. This was done to facilitate quick reboots. A cleaning contractor turned off the switch thinking it was a light. It took us several hours to determine the situation and restore power\" reply dv_dt 14 hours agorootparentprevI think the telecom issue playbook is significantly different than the SaaS playbook. Not sure if that’s just cultural or if there are other drivers - maybe paying customer telecom interfaces are simpler and more closed than typical SaaS? reply aksss 13 hours agorootparentIME, telecom as an industry is highly focused on the RCA, ICA, and uptime, and has had that embedded culturally for decades. Sharing the information publicly doesn't have much value, in the balance, unless there are a string of incidents where an acute perception problem needs to be addressed. This would more likely result in a marketing and advertising strategy rather than the sharing of technical RCA details. Additionally, one must consider that not all RCA details are fit for public disclosure. _You_ may be interested in deets, but John Q. Public is not interested beyond \"Is it fixed yet?\". If you want insider perspective, work in/with the industry. It's fascinating stuff. reply wolverine876 12 hours agoparentprevDoe that include cellular voice calls? reply TheAdamist 22 hours agoprevThis reminds me of the recent discussion on status pages. https://news.ycombinator.com/item?id=39099980 They need to be accurate. At&t status claims everything is fine. My wireless service is down. Down detector has tens of thousands of reports, so clearly everything is not fine. reply teeray 16 hours agoparent> They need to be accurate It would be nice if the FTC mandated this. It is exhausting when the status page is taken over by the marketing department (the infamous green check with the little \"i\"). reply jimmaswell 14 hours agorootparent> the infamous green check with the little \"i\" I'm not familiar, what are some examples? reply bombcar 20 hours agoparentprevStatus pages are basically useless if they’re public facing. Either they automatically update based on automatic tests (like some of the Internet backbone health tests) or they’re manually updated. If they’re automatic, they’re almost always internal and not public. If they’re manual, they’re almost always delayed and not updated until after the outage is posted to HN anyway. reply ryathal 19 hours agorootparentThe other problem with status pages is depending on what happened it may not be possible to update the status page anyway. You really need a third party to have a useful status page. reply TheAdamist 15 hours agorootparentWhich is pretty much what down detector has evolved into. And it looks like they have an enterprise offering to alert companies to their own issues. reply op00to 20 hours agorootparentprevWhich is better? How do you know whether an issue is individual to a customer or a quick blip that will resolve in a few seconds? reply bombcar 20 hours agorootparentI prefer fully automated tests publicly revealed because the main thing I want to know (as a customer) is should I keep trying to fix my end or give up because GitHub exploded again. It’s most annoying when you have something like recently - known maintenance work on my upstream home fiber connection that was resulting in service degradation (but not complete loss, my fiber line was back to DSL or dialup). The chat lady could see that my area was affected, but the issue lookup system couldn’t. If the issue lookup had told me there as an issue I’d’ve gone on my merry way. I even checked a few more times until it was resolved; the issue never appeared in the issue lookup system. reply op00to 18 hours agorootparent> should I keep trying to fix my end or give up because GitHub exploded again Making this decision easy is a fight I fight for my customers every day. :) reply bombcar 18 hours agorootparentThis was much much much easier when websites used to explode with tracebacks and other detailed error messages, now you just get a \"whoopsie doopsie we did a fuckywucky\" and you can't really tell what's going on. reply menacingly 18 hours agorootparentprevyou can't operate at any scale at all without mechanisms in place to know perfectly well whether an issue is impacting a single customer or if your world is on fire reply bombcar 18 hours agorootparentYou'd like to think so, but surprisingly large number of \"large scale\" things operate on the \"everything is fine\" until too many people complain about the fire. reply pixl97 18 hours agorootparentCaches make problems fun too. Quite often you see automated tests that check how well your cache/in memory data are working. But when some other customer that isn't in the hot path tries to access their request times out. I've seen a lot of people making automated checking systems fail at things like this. reply zitterbewegung 17 hours agorootparentThe phrase “the hardest parts of computer science is caching and naming things” come to mind. reply r2_pilot 15 hours agorootparentI see 2 things here but you're off by one. reply op00to 18 hours agorootparentprevYes, but those mechanisms take time to determine this. reply spicybbq 16 hours agoparentprevCurrently there is a banner on the AT&T outage page with this message: >Service Alert: Some of our customers are experiencing wireless service interruptions this morning. We are working urgently to restore service to them. We will provide updates as they are available. https://www.att.com/outages/ reply op00to 20 hours agoparentprevWhich status page? reply Animats 13 hours agoprevThere's an outage map.[1] But it's useless. That's just a US map of where most people live. [1] https://www.cbsnews.com/news/outage-map-att-where-cell-phone... reply paxys 13 hours agoparenthttps://xkcd.com/1138/ reply progbits 17 hours agoprevNo info in NANOG yet but expect some in this thread in the coming hours: https://mailman.nanog.org/pipermail/nanog/2024-February/2250... reply fragmede 1 hour agoparent> Word around the campfire is that it’s a Cisco issue. https://mailman.nanog.org/pipermail/nanog/2024-February/2250... reply nu11ptr 18 hours agoprevUsing myATT app doesn't even show my wireless account anymore. My entire family account doesn't even show up as a service. Seems like a hack or internal issue that deleted accounts? Can others confirm whether they see their accounts? reply mh- 18 hours agoparentMany of their APIs appear to be intermittently returning 502s, leading to strange behavior in their web/mobile apps. reply radicaldreamer 15 hours agorootparentThat's just people trying to figure out if their service has been disconnected rather than it being a network outage reply bmitc 18 hours agoparentprevTheir outage status page is also completely broken. Doesn't show anything. reply zeven7 18 hours agoparentprevI can see my account. The first thing I did when I saw my phone wasn't working was log in and pay my bill, thinking maybe I had missed one or something. reply partiallypro 16 hours agoparentprevFrom what I read just a bit ago, basically there is a problem with the database of SIM numbers. So, SIMs all just dislodged from the network because they lost their network authorization. That would lead one to believe it was a botched software push. I imagine online accounts get this information somehow which could explain the portal being broken. I have a prepaid hotspot and it works fine, but none of the \"family plan\" month to month contract phones work. I also wonder if there's a physical SIM vs eSIM situation that could explain \"newer\" models working. reply kjellsbells 14 hours agorootparentThat would be consistent with the symptoms. Big telco networks are hierarchical with most functions pushed to regional data centers with a very small number of services in a redundant pair or trio of central data centers. Subscriber database (HSS, UDR) would be one such function. The cause of a failure of the HSS could be manifold, ranging from router failures to software bugs to cyber attack (databases of 100M+ users being a juicy target). One slightly scary observation from NANOG was that FirstNet, the network that ATT built for first responders, was down. That would be ugly if true and I'd expect the FCC to be very interested in getting to the bottom of it. reply Kon-Peki 16 hours agorootparentprevIn our house this morning, the two phones with physical SIMs worked fine and the two phones with eSIMs were SOS mode only. I could log into my AT&T account just fine and all phones showed up correctly. (I’m submitting this from an AT&T 5G connection, no WiFi nearby) reply chasd00 13 hours agorootparentaligns with my experience too. My wife's newer phone was sos but my older one was fine. Both ATT. reply deckar01 17 hours agoparentprevThe account management / status tools were slow and flaky on the best days. I wouldn’t rule out a little extra traffic knocking them out. Correlation is not necessarily causation. reply rglover 16 hours agoprevPure speculation, but CISA released this [1] a few weeks back and tweeted [2] it out. [1] https://www.cisa.gov/news-events/cybersecurity-advisories/aa... [2] https://twitter.com/CISACyber/status/1758495005176447361 reply nathanyz 17 hours agoprevLatest AT&T Statement: “Our network teams took immediate action and so far three-quarters of our network has been restored,” the company said. “We are working as quickly as possible to restore service to remaining customers.” Still down for me though. reply Scoundreller 16 hours agoparent3/4 might just mean the internal facing side, which is still progress, but doesn’t mean any improvements for end-users. reply wizerdrobe 15 hours agorootparentAnecdotally, I woke up to no signal / “SOS” mode on my iPhone this morning at around 0600 and had service restored around 0830 in South Carolina. However, a coworker in Memphis confirmed he was still out of service at 1000 so it’s regional restoration. reply Scoundreller 15 hours agorootparentI always wonder instead of a regional restoration, if they would “disable” segments of SIMs/accounts randomly to avoid lightning strike (it’s not a DDoS…) their network as they turn things back on. Depends on what the recovery method is, but could be problematic to turn everything back on at once. reply wizerdrobe 8 hours agorootparentI actually spoke with my wife after the initial comment, she was reconnected at a later time than I! So it’s not regional but some other mechanism. reply fragmede 14 hours agorootparentprevI've heard that called the thundering herd problem. reply chasd00 13 hours agorootparentprevmy wife's phone was SOS when she woke up at about 6AM central and finally become operational around 1:30PM central. reply DHPersonal 16 hours agoparentprevJust came back up for me in Oklahoma City, OK. reply IAmGraydon 17 hours agoparentprevStill down for me too. reply TheCaptain67 16 hours agorootparentdown all morning in ATL but back up at 1PM EST reply themaninthedark 15 hours agorootparentBack up in Cartersville, north of ATL @ 13:12. Oddly enough my text messages say they went through at 12:43 but my response to someone's message when once my phone had everything roll in at once is timestamped 13:12 reply partiallypro 16 hours agoparentprevAlso still down for me here around Nashville. reply joshstrange 19 hours agoprevSeeing \"SOS\" only on iPhone currently. I got worried something had gone wrong with auto-bill pay since I only noticed after I was driving. It's interesting how naked I feel without access to the internet. I reach for it way more often than I would have ever guessed, something you only notice when it's not there. Last March my area saw large wind storms that knocked out power for almost a week (I'm not in a rural area). I can work around the loss of power but the cell tower(s) that service my area could not handle the load and/or the signal in my house was weak and I was unable to load anything. Not having internet was way worse than not having power and I ended up driving a few hours away to my parent's house instead of staying home. reply bombcar 19 hours agoparentMy earliest computers were amazingly capable and powerful devices, I could do anything I could think of and spend hours and hours on them. Now my computer is insanely more powerful but without an Internet connection it feels dead and useless. reply colanderman 19 hours agorootparentOptimize some low-level numeric algorithms, CPU or GPU, it brings back that feeling. reply bombcar 19 hours agorootparentIt's even sadder - I used to be able to play computer games for hours offline, now I get about five minutes into even the ones WITH on offline mode, and I'm grabbing for a wiki or other reference. Ah, some of it is just getting old. reply joshstrange 19 hours agorootparentI think some of it is just not having oodles of free time to figure it out on your own. When I was young I would just keep trying things till I figured out the game, my time wasn't worth much or at least I didn't value it highly. Nowadays I don't want to spend 1-3 hours figuring something frustrating that's blocking my progress. The \"rush\" I get from solving it on my own does not make up for the time lost. Also I feel like games made today almost expect you will need the wiki/guide to figure out certain things. Or at least I often think \"How the heck was I supposed to figure that out?\" when reading the wiki for some aspect of a game I'm stuck on. reply akira2501 9 hours agorootparentSo.. it's an adult strategy for playing video games to extract a candy coated \"win.\" We are all either overgrown children and always will be, or something has gone drastically wrong in the schools. reply bombcar 19 hours agorootparentprevThat latter part is certainly true, only a few games \"offhand\" even really try to work \"wiki-free\" (Factorio is perhaps the best here, but Minecraft is trying). reply jhickok 13 hours agoparentprevI started driving across the US at 3am, didn't notice for the first few minutes until I tried pulling up the address in Apple Maps. Sure was strange following interstate signs for ~10 hours! reply flerchin 19 hours agoparentprevYes I've felt the same way. I feel like we have an instinctual need for social connection that we've filled with internet. Luckily, we do still have meat-space friends and family. reply joshstrange 19 hours agorootparentYeah, that was a big reason I went to stay with my parents/family. I felt super isolated from my friends (local and remote) when I couldn't participate in group chats/communicate. Also I just kept picking up my phone to look up something or check on something only to re-remember I couldn't do anything. I had podcasts and audiobooks on my phone which helped but the isolation was a weird feeling I hadn't felt before. After I thought about it I realized it had probably been a decade or more since I had been completely without internet for more than a few minutes. It was odd... reply no_wizard 19 hours agoparentprevPortland, is that you? This happened to us with the recent storms a month or two back, some places didn't have power restored for 2 weeks+ reply pdxandi 19 hours agorootparentPortland checking in. Those storms were gnarly and there was carnage all around us. Luckily we maintained power and internet. We have 11 month old twins and a three year old so 10 days without childcare or help was its own challenge. reply joshstrange 19 hours agorootparentprevLexington, KY. Not a massive city but the second largest in KY. I left after 2 days of no power and it didn't come back on for another 3-5 days more after that depending on where you lived. reply mstudio 19 hours agoprevMy ATT Phone is in SOS mode. However, ATT's outages status reports: > All clear! No outages to report. > We didn’t find any outages in your area. Still having issues? https://www.att.com/outages/ reply Scoundreller 17 hours agoparentAt least they’re in SOS mode. When Rogers in Canada had a total blackout (cellular, home internet, MPLS, corp circuits, their radio stations , everything), phones showed zero bars, but the towers were still powered on and doing some minimum level of handshake so phones didn’t go into SOS mode. If you tried to make a 9-1-1 call, it would just fail. It wouldn’t fail over to another network because the towers were still powered up but unable to do anything, and Rogers couldn’t power them down because their internal stuff was all down. Like a day later they said you could remove your SIM card to do a 9-1-1 call. Thanks guys. Of course, no real info from the provider during the outage. Turns out they did an enterprise-risking upgrade on a Friday morning and nobody at the org seemed to have a “what if this fails plan”. CTO was on vacation and roaming phones were black too and he thought it was just an issue for him. https://en.m.wikipedia.org/wiki/2022_Rogers_Communications_o... reply ezfe 12 hours agorootparentSome people earlier on this morning said they couldn't make 911 calls. I wonder if it was the same issue and perhaps AT&T cut the towers completely pending a fix. Purely speculation. reply derbOac 10 hours agoparentprevOur land-based internet with a different [large] ISP went out about 18 hours before this stuff with wireless started, and is still going on. We've been getting similar contradictory messaging the whole time, and they seem confused about what's causing it. We got a message a couple of hours ago it had been resolved and then 30 minutes later said it was not. It could be entirely coincidental and unrelated to the stuff with other networks, but the timing was odd and I have never ever seen anything like this outage from them. I can think of one time it was out for around 2 hours in the last 5 years, and it was with a very specific infrastructural upgrade they knew about. reply inferiorhuman 17 hours agoparentprevI'm seeing a variety of outages listed there as of 08:30 Pacific, mostly landline. There are a couple wireless outages shown in Sonoma (and listed as impacting Sonoma and Ventura counties). The initial cause is shown as \"maintenance activity\". https://imgur.com/a/oXZpEX9 reply dangus 17 hours agoparentprevI’m seeing an outage reported on this page. reply midwestfounder 19 hours agoprevI wonder if this is a cyber incident. Curious if any telecom folks know what the most likely explanation for an event like this would be, and what telltale signs/symptoms might first indicate this was caused by something nefarious. reply unforeseen9991 19 hours agoparentDue to the gross incompetence these companies operate at, it's too hard to tell the difference. reply pylua 19 hours agorootparentUnfortunately, unlike cyber security, there are no off the shelf products that are being sold to help companies with general incompetence. reply relbeek2 16 hours agoparentprev> However, the US Cybersecurity and Infrastructure Security Agency is “working closely with AT&T to understand the cause of the outage and its impacts, and stand[s] ready to offer any assistance needed,” Eric Goldstein, the agency’s executive assistant director for cybersecurity, said in a statement to CNN.[1] [1] - [https://www.cnn.com/2024/02/22/tech/att-cell-service-outage/... This isn't telling of anything, right? Wouldn't CISA be involved with anything that impacts Public Infrastructure at this level? reply red-iron-pine 15 hours agorootparentby itself, not telling of anything per se. like, you could commit a dumb BGP config and break lots of stuff. have done that in the past, actually... but any time a national-tier ISP has a national-level outage, that warrants a look from multiple orgs. and given the number of threat actors like china, NK, iran, and russia, who are, and have, made aggressive efforts in this space -- and have strong reasons to do so now -- its not crazy for the US fed'gov to want to know a little more, and offer to help. but again, entirely possible it's unrelated. reply overstay8930 15 hours agorootparentprevThis is normal for high profile outages, even if you are small you can still engage with the CISA if you think there's foul play. reply relbeek2 16 hours agorootparentprevfrom the same article above, it seems like it's a critical part of this. > “Everybody’s incentives are aligned,” the former official said. “The FCC is going to want to know what caused it so that lessons can be learned. And if they find malfeasance or bad actions or, just poor quality of oversight of the network, they have the latitude to act.” If AT&T gets to decide if they are at fault, they will, of course, never be at fault. So a third-party investigation makes a lot of sense. I would also suspect that the FCC would not be as well versed in determining if there was a hack or even who did it, which is why I feel like CISA would need to get involved in the investigation. reply acefaceZ 17 hours agoprevWhat is the big deal? Just use your HAM radio. reply engineer_22 13 hours agoparentThat reminds me, I passed the General and Tech but didn't follow through on my application... reply xyst 16 hours agoparentprevI’m more of a smoke signal guy. reply zingababba 12 hours agoparentprevI agree unironically reply chomp5977 11 hours agoparentprevFunny you say this lol I did exactly that reply r721 3 hours agoprev>A temporary network disruption that affected AT&T customers in the U.S. Thursday was caused by a software update, the company said. >AT&T told ABC News in a statement ABC News that the outage was not a cyberattack but caused by \"the application and execution of an incorrect process used as we were expanding our network.\" https://abcnews.go.com/US/att-outage-impacting-us-customers-... https://news.ycombinator.com/item?id=39477187 reply teleforce 9 hours agoprevI think this is the main reason developers should always be designing their software with local-first (aka offline-first) approach in-spite of the Internet and the cloud [1]. Regarding the cellular based authentication, it is perfectly doable to be securely authenticated even without the any connectivity and there is a solution to this based on combination of MFA and OTP [2]. [1] Martin Kleppmann talk on local-first (LoFi): https://news.ycombinator.com/item?id=39444519 [2] A lightweight and secure online/offline cross-domain authentication scheme for VANET systems in Industrial IoT: https://peerj.com/articles/cs-714/ reply mucle6 8 hours agoparentI think you're right, and I'd like to amend the \"should always\" to \"after product market fit\". I'm always trying to stop myself from prematurely optimizing reply BuckYeah 18 hours agoprevOur corporate att phones are all down in multiple large campuses. About 10,000 phones total reply oceliker 21 hours agoprevThis is so odd. I have two phones with AT&T currently sitting right next to each other. One has service, the other is in SOS mode. reply nathanyz 20 hours agoparentSome of our staff are reporting similar where their partner's phone has service and their's doesn't. Both on same AT&T family plan. So the radio bands may play into it although I would think with latest iPhones, they can use any of the bands from AT&T although I could be wrong. reply PietdeVries 18 hours agorootparentCan you check if things improve if you turn off 5G and move to 4G/LTE instead? reply nathanyz 17 hours agorootparentGood thought, but switching to LTE only didn't work. Same result of ending up in SOS only. Cellular over wifi works perfectly fine though. Wish we could count on better post mortems from the phone companies, but I'm not holding my breath for it. reply vel0city 20 hours agorootparentprevI'm wondering if it could be some kind of auth timeout. I've heard from a few people of one person's phone going out, then a bit later the other person's phone finally failing too. reply alephnerd 20 hours agorootparentprevI'm on the latest iPhone and it's SOS for me reply aclindsa 19 hours agorootparentprevYep: my partner's iPhone has service while my Pixel doesn't, both on same plan. reply jeffwask 18 hours agoparentprevThere are some reports that phones with e-sims are less likely to be impacted versus phones with hardware sims. reply harambae 13 hours agorootparentI have eSIM only (iPhone 15) and was impacted the same as physical SIM users on AT&T (Boston area). I suppose I can't speak to likelihood with a sample size of one. reply bombcar 20 hours agoparentprevDoes it still show bars in SOS mode? Or is SOS just “I dunno can’t see no cell towers but maybe it’d work?” I wonder if the MVNOs that piggyback on AT&T are showing down also. If not, it’s some AT&T service authorization system that exploded. reply dcan 20 hours agorootparentSOS mode means it can see towers of other providers you aren’t authenticated with, but no signal to authenticated cell towers reply ok123456 18 hours agorootparentprevEmergency only means 911 calls through whatever provider is available. reply sandinmyjoints 20 hours agorootparentprevMine says SOS Only, shows no bars. reply rixthefox 19 hours agorootparentprevOn the latest iPhones, SOS mode is the emergency fallback to satellite service. It's really meant to be used in situations where you're well outside of any sort of service area but you have a clear, unobstructed view of the sky. Your iPhone will instruct you on where to point and help you track an emergency satellite that is manned by live humans who will take your emergency request and relay it to the proper people. More specific info here: https://support.apple.com/en-us/104992 reply SirMaster 19 hours agorootparentSOS means it has cell service and you can call 911. If there is no cell service then it's SOS with a little picture of a satellite next to it. reply inferiorhuman 17 hours agorootparentExcept SFFD is reporting that (some) AT&T customers are unable to call 911. reply organsnyder 16 hours agorootparentSOS mode typically means that your phone is connected to a carrier other than one you have a contract with. reply Scoundreller 15 hours agorootparentprevI wonder if some devices bungle their failover. The exact failure mode state of AT&Ts network might cause some devices to hang onto AT&T’s RF. reply lxgr 19 hours agorootparentprevI’m pretty sure that “SOS only” can also mean the phone seeing networks it can’t register with but which it could make an emergency call on if required. This predates satellite SOS. reply bombcar 19 hours agorootparentprevWon't that only activate if it can't see or communicate with any towers at all? reply sandinmyjoints 20 hours agoparentprevSame here. Strange. Would love to know the reason! reply SkyPuncher 19 hours agoparentprevMy wife and I were riding in the car next to each other. Took mine about 5 to 10 minutes longer to jump to SOS mode. reply pixl97 19 hours agorootparentI wonder how SIM registration works? For example if it's like a token with an expiry. If some set of registration servers on the network couldn't renew then I could see behavior like this. reply bdcravens 18 hours agoparentprevDitto, mine and my wife's. In my case, the working one is slightly newer (15 Pro Max vs 14 Pro) reply BuckYeah 20 hours agoparentprevDifferent bands could be affected differently if it is solar radiation related. Same exact model of phone? reply oceliker 20 hours agorootparentSame year, different size (13 and 13 Pro Plus) reply sandinmyjoints 20 hours agorootparentprevDifferent models, in my case. reply 171 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AT&T faced a daylong outage affecting numerous users in the US, disrupting calls, texts, and emergency services in major cities.",
      "The cause remains unknown, but AT&T confirmed it wasn't a cyber attack, restored service, and aims to avoid similar incidents in the future.",
      "The FCC is looking into the outage that hindered access to emergency services like 911, impacting not only AT&T users but also those of Verizon and T-Mobile due to high call volumes."
    ],
    "commentSummary": [
      "Recent cellular outages in the U.S. raised concerns about infrastructure reliability, highlighting the importance of redundancy in core systems.",
      "Discussions covered historical telecom system reliability, challenges in network resilience, GPS tech utilization, and limitations of SMS 2FA and TOTP for identity verification.",
      "The conversation emphasized the impact of technology outages on critical services and industries, underlining the significance of offline mapping and a local-first software design approach to prevent future incidents."
    ],
    "points": 450,
    "commentCount": 433,
    "retryCount": 0,
    "time": 1708600505
  },
  {
    "id": 39471568,
    "title": "Paramount Global Celebrates Record Profits with Mass Layoffs",
    "originLink": "https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired",
    "originBody": "February 20, 2024 Our Company Is Doing So Well That You’re All Fired by Andrew Singleton “Paramount Global lays off about 800 employees, a day after announcing record Super Bowl ratings.” — CNBC - - - Thank you for jumping on this last-minute Zoom meeting. As you’re all well aware, this has been a great year for us. Profits are at an all-time high. Our stock price is at an all-time high. Growth projections for the coming year are through the roof. You all have done an amazing job ushering in a new era of success for this company. So amazing, in fact, that we have no choice but to let you all go. A finalist for the National Book Critics Circle Award! I realize this comes as a shock, but I don’t want any of you to worry about the future. Thanks to you, we are in an excellent position to ride out the challenges of the coming year. I mean, those of us who still work here are. But I’m sure you all will be fine too. You’re top-level contributors. You’ll surely help any company you join become successful enough to no longer need you, as long as you don’t do one of the jobs we’re all replacing with AI. About 60 percent of you will be A-OK, in the very short term. You have my non–legally binding word on that. But let’s not linger on the very real problems this most recent evolution of capitalism has put before you. You should take a moment to celebrate yourselves. You crushed it. You’ve helped us grow so much that the only possible way for you to continue to help us grow is by no longer working here, effective immediately. That’s quite an achievement, although I do acknowledge it sounds a little nonsensical. Frankly, I’m not sure I even understand exactly how it works. Maybe one of you can explain it to me before you leave? You’re our best and brightest, after all. Well, you are for the next twenty minutes or so. So keep your heads high. You’ll leave here knowing we couldn’t have been successful enough to fire you without you. That it was your proud and noble contributions that ultimately enabled us to stop giving you a salary and benefits. And at the end of the day, isn’t that more valuable than money? I guess no, not according to our board and shareholders. But they’re not as proud and noble as you, the chosen few (I think 20 percent of our workforce counts as “few”). They may come away from this whole ordeal with a few more points on their shares, but you’ll get to walk away knowing that, for the small sacrifice of your ability to pay your mortgages or buy food, you provided those points. That makes you the big winners in all this, in my eyes. Honestly, if it weren’t for the fact that my equity doesn’t vest for another six months, I might even consider joining you, you noble, fallen warriors. But no, my sad fate is to fail upward and look on with jealousy and admiration as you all succeed downward. I envy you. I really do. “A key barometer of the literary climate.” —The New York Times As for what happens next, be assured that we plan to treat you with the utmost respect— —Sorry, I think you all got booted when IT terminated your access to the company servers. Can’t be too careful, right? Anyway, as I was saying, you’ll all be treated with the utmost respect from now until we get your laptops back. Please try to have those in the mail by the end of the day, by the way. Layoffs Business Capitalism Please help support our writers and keep our site ad-free by becoming a patron today! Become a patron",
    "commentLink": "https://news.ycombinator.com/item?id=39471568",
    "commentBody": "Our Company Is Doing So Well That You're All Fired (mcsweeneys.net)358 points by ohjeez 14 hours agohidepastfavorite281 comments empiko 14 hours agoIf the company fires hundreds or thousands of people and there is no visible dip in the productivity, is it really a bad thing? One thing that is baffling to me is that these companies can fire 10% of their workforce and they just keep on chugging without a hitch. The bullshit job phenomenon is the problem here. reply Aurornis 13 hours agoparentCompanies reduce initiatives and headcount at the same time. It's usually approached as a budget reduction exercise: Company is reducing budgets by X%. Select which projects get cancelled. Now work with managers to identify enough employees to lay off to reduce headcount spend by X%. Now reorganize remaining employees across remaining projects, with the understanding that a few extra people will leave due to future layoff fears. The idea that companies like Twitter/X just removed a lot of employees and then operated exactly the same as before is a myth being propagated online right now, but companies must change their operations as part of layoffs. It's usually not obvious from the outside because the core product always gets attention, but side initiatives, internal tooling, new feature concepts, and even less-visible things (think bot/spam detection on X) lose attention that they may have needed. But no, the idea that companies are making 10% of their workforce disappear at no cost to the business is just fantasy. The only time I saw this happen was when a company hired so fast that they didn't have work for some people to do, but that mistake was quickly recognized. reply tuckerconnelly 13 hours agorootparentWhile X's main use case works, I just tried to run some ads there. When trying to upgrade to verified org: \"An unexpected error occurred\" When trying to change my display name: \"An unexpected error occurred\" When trying to run an ad: \"Awaiting verification\" for a week Contacting support through messages: No response Finally finding a way to submit a ticket: 3 day response time with a standard response that didn't resolve the issue. Responded to that ticket and haven't gotten a response in days. I don't think they're operating at the same level as they used to :) reply jprete 12 hours agorootparentIt's sad that you cannot even get through the workflow that gives them money. They must be in absolutely terrible shape. reply hadlock 13 hours agorootparentprevTwitter/X hasn't seen notable user growth since ~2015/16 and has been absolutely flat since then, coming up on a decade now. It was 325 million active users in 2015 and it's still about 300 million active users today (spring 2024). A decade of no growth. I don't know if twitter is functioning exactly the same as it was before, but if you have to pay the bills, cutting 80% of the workforce and holding the line seems to have worked in this case. Having 5000 engineers on tap for... potential user growth? Makes sense if you're planning on user growth, but Twitter is kind of the poster child of \"we don't need engineers for anticipated growth\" given a near-decade of totally flat userbase number. reply darth_avocado 12 hours agorootparentI worked at Twitter and let me tell you that's not how it works. When you whack most of the engineers out of the company, and if it still continues to work, that doesn't mean you don't need that many engineers. It means the following: 1. The ones remaining are practically working 24x7 every day for essentially the same pay, with the promise of more. 2. You keep falling behind on all the things you need to do to keep things alive and compliant. This eventually comes back in the form of outages or worse, FTC fines. 3. There is very little room for forward looking work, which means you are sacrificing opportunities for growth. E.g. > 325 million active users in 2015 and it's still about 300 million active users today These were two very different numbers. The numbers in 2015 were DAUs and the numbers today are mDAU. mDAU are in layman terms users that can be monetized. This number was ~180M in 2018, so the growth was there. Part of the reason why the ad revenue was growing for Twitter during that phase. 4. Safety and Trust on the platform drops quite a bit when you are understaffed. Regardless of your political beliefs or whether you're team Elon, when there are 10 people banning CP instead of 100, there is no way you could argue objectively that the platform stays the same. There's plenty of other problems, but you would not know they exist unless you're one of the people left after the figurative massacre. reply Ozzie_osman 12 hours agorootparentprevAs a company grows (in size and age), there is definitely more waste and more bureaucracy. No impact in trimming that. But, larger, older companies also spend a lot of effort on defending their business or uncovering future business. Fire the people who do that, and you won't see any immediate impact to revenue or core metrics... But, you've probably reduced the future viability of the product (lower chance of uncovering new growth, higher chance of being disrupted by a competitor, etc). You can see this a lot with companies bought by Private Equity firms. They slash costs and save a lot of short-term costs, and all looks good, but the company is now on a path to death. reply hadlock 12 hours agorootparentI like your optimism, but twitter had already hired the best and brightest for a decade and were completely unable to move the needle, barely maintaining existing user count. You can see the same usercount story for traditional message boards/forums. There's just not that much demand for this kind of social media. > the company is now on a path to death Probably, but it's going to be a very long tail. Forums do not cost that much to run. reply baq 12 hours agorootparentprevIt absolutely isn’t functioning the same. Technically maybe everything works more or less as well as before, after a year. …but the experience is quite bad now: every other thread is full of crypto and/or onlyfans promoting bots. The algorithm displays a lot of obvious attention grabbing posts which have 300 replies, 10 of which are from humans. There’s a constant stream of bots trying to connect with you via follows or likes. It’s a mess and a fantastic example of why anonymous and free communication platforms absolutely need professional moderation. reply itsoktocry 11 hours agorootparent>every other thread is full of crypto and/or onlyfans promoting bots. Remember when we were told Elon was going to unleash AI on the bot/spam problem and solve it in a month? It's worse than it ever was. I hardly ever saw bots or spam. Now I have 100+ spam followers, and on the very rare occasion where I reply to something, it gets 90% spam likes. Ads are also crypto, soft porn games and drop shipping companies. reply importantbrian 12 hours agorootparentprevI don't think it even technically works as well as before. The iOS app seems mostly fine, but on web the feed or tweet fails to load and shows the blue retry button probably 1/3rd of the time for me now. Combined with all the bot issues you mentioned, that has dramatically reduced my Twitter usage. It's become incredibly frustrating to use. I only use it now when I click on a tweet link that's been shared with me. reply darth_avocado 11 hours agorootparentIf you're logged out, you can't see any Tweets unless you're directly viewing a specific Tweet, and even then you cannot see any other Tweet apart from the one you're directly viewing. This was one thing we had tried on Twitter before Elon, and had found that it is detrimental to the platform and engagement. So, had to roll it back. But now it is a thing again. This drives down the popularity and overall usage of the platform, but some people will still claim it works the same (or even better). reply stcroixx 12 hours agorootparentprevOn the other hand, I love it now and hated it before. Guess it depends on what you're looking for. Me - I like little to no moderation and government interference and don't find much offensive and have no problem just ignoring anything I do find objectionable. All worth it to be able to speak honestly. reply itsoktocry 11 hours agorootparent>I like little to no moderation and government interference My personal politics better align with based Elon than the previous regime, but let's not pretend there's no moderation happening. It's just different. reply KittenInABox 12 hours agorootparentprevI don't post a lot and I also find Twitter really annoying these days. I am used to getting pinged only by my friends to talk niche tech stuff, now I get porn/crypto bot spam... Trying to follow technologists I like, but their discussions are also mired in dumb blue checkmark takes and bots... OK maybe those blue checkmarks are speaking honestly but I'd rather have intelligent speech over honest speech at this point reply saagarjha 11 hours agorootparentprevTwitter almost doubled their mDAU between 2016 and 2022. They definitely weren't doing anything. (I have no idea what the recent acquisition has done to the numbers, though.) reply salad-tycoon 13 hours agorootparentprevTwitter shed 80% and still works, is still the go to outside of fringe empty echo chambers of the left and right divide. Not only that but they added a significant amount of features like Grok, which I know nothing about really, and Substack like payments. 80% is a lot and it works just fine. 7,500 to 1,300 in Jan 2023. Source: https://www.cnbc.com/2023/01/20/twitter-is-down-to-fewer-tha... reply NM-Super 12 hours agorootparentTwitter's still working, but it's in a state that I think is worse than before. The spam bots alone seem to be orders of magnitude more prominent. I don't think I'd ever seen a spammer pre-headcount-change, and now \"MY PUSSY IN BIO\" is a running gag on the website because spambots are so prominent. They also changed their content rules to allow drastically more content, which reduces the total amount of work to be done. You could argue that this is a good thing because the moderation was useless and censoring before, but the moderation also made it a friendlier place for major advertisers who have since pulled out—so changing the moderation resulted in a business model change. That's naturally going to reduce headcount. reply JohnMakin 13 hours agorootparentprevThey effectively disabled the developer API via absurd pricing. That alone would cancel a ton of projects/teams due to the load that likely placed on their servers. The user experience is also not even close to the same, while they've not really added any new features that likely weren't already in the pipeline. Not even to mention their tanking revenue and valuation. It's not even close to a \"working the same as before\" company. Your definition of \"working\" needs work. Nothing you said contradicts the parent. reply DiscourseFan 13 hours agorootparentI think OP just meant it was working to strip Twitter of the majority of its operating costs so it can be transformed into a sweatshop profit machine (but, in that case, I disagree, since its not making a profit). reply tick_tock_tick 13 hours agorootparentprevSo what you're saying is everything anyone using Twitter cares about is working just fine. reply JohnMakin 13 hours agorootparentI guess, unless you're also making the assumption that the number of daily users has remained the same from pre-elon acquisition. I promise you that is not the case. There's also that pesky profitability problem. reply watwut 12 hours agorootparentprevFrom my point of view, just not. I went from using Twitter a lot to gaining a lot of free time ... be because overall experience now sux in many small ways. Whereas there used to be interesting exchanges and threads before, algorithm now promotes similar nonsensical short responses again and again. The stuff on top is just not interesting anymore. reply zanellato19 13 hours agorootparentprevTwitter doesn't work if we go by the same definition as before. They have a ton of outages, the app is constantly buggy, the ad network is ridiculous, the moderation is nonexistent. I faced a bug that new tweets wouldn't load _for weeks_. If it was any other company, we would be saying they are done, but since it is Elon at the helm, some people pretend that Twitter still works. reply ryandrake 8 hours agorootparentTwitter is in the \"Wile-E-Coyote just ran off the cliff\" phase where his legs are still running on air. Companies like that have tremendous momentum, but an airplane can only glide so far after you turn off the engine. reply srvaroa 13 hours agorootparentprevIt still is cherry picking one out of many examples where the headcount reductions were by far not that drastic as in Twitter. The parent reply has an important point: a good % of headcount reductions tend to affect r&d and the type of projects where cutting investment doesn’t have a negative short or even medium term effect on the company. Sometimes it can even be positive for operations (e.g. infra teams have to support less product teams). This is mostly why it’s very uncommon to see headcount reductions lead to instability, etc. Long term effects (e.g. stagnation, no innovation..) may manifest, but are harder to see clearly in advance, and when they do happen the connection to those layoffs from N years ago is not obvious/even remembered reply pygar 12 hours agorootparentprevI'm getting a lot of \"tweets aren't loading right now\" type errors. The app keeps jumping to the top when I'm scrolling into someones timeline. Sometimes i see my replies twice. Sometimes things just don't load. It's seems like death by a thousand paper cuts. That said, I'm sure the technical issues can be fixed eventually. I'm also getting a lot of garden variety \"West/Christianity vs East/Islam\" type race baiters and stupid 9gag type memes despite actively trying to indicate to the algorithm that I'm not interested. I think they've skewed it towards controversy to increase engagement but it's just boring. reply importantbrian 12 hours agorootparentIt's so buggy right now. It feels like it's gone back to the old fail whale days. \"It works\" is a pretty low bar to clear. On the upside, it gave me back a lot of free time, because the Twitter experience is just too frustrating to deal with anymore. reply LargeWu 12 hours agorootparentprevIt \"works\". The main app is generally functional, but it's easy to see the deterioration of the smaller subsystems that large platforms like that need for a good user experience. For instance, I have an account that I am unable to log into because their password recovery/change feature doesn't seem send me anything when I click it. I'm sure frequent users have a litany of little things they've noticed that are flaky or outright broken, even if they can still send and read tweets. reply itsoktocry 11 hours agorootparentprev>Not only that but they added a significant amount of features like Grok, which I know nothing about really, and Substack like payments. A premium (as in requires money), also-ran AI with a juvenile \"sense of humor\" and payments? These are significant? reply raydev 12 hours agorootparentprevAn implicit assumption I see often is that all 8000 Twitter employees were software engineers too, but at it's peak engineering only made up 40-ish%. Without question though, despite the appearance of velocity in shipping features post-layoffs, the features were either small changes or if they were big, already partially deployed as experiments to small groups of users. Aside from Grok, Musk's Twitter hasn't shipped anything new except and expanded monetization system (again, partially implemented when he took over anyway). reply watwut 12 hours agorootparentprevThey got rid of stuff working with paying customers - ad companies. And that part is now failing so much, that Elon Musk went on multiple public tirades. Twitter is not able to attract ads, because they don't communicate with companies properly and are chaotic. reply beambot 13 hours agorootparentprevEnterprise Darwinism. reply lp4vn 13 hours agoparentprevIf you have a project/company that's already running well and you fire almost everybody except a skeleton crew, you'd be amused but things don't crumble down instantly. Technical debt piles up and everything goes to maintenance mode, when you need the insight of the good people who left is when you realise that going to the previous state will cost you a lot more time and money than you might possibly predict. reply plorkyeran 7 hours agorootparentOn a much smaller scale, six years ago I inherited a service which previously had two people working on it full time. I've spent about a day a month on doing the bare minimum to keep it working ever since. Clearly we were massively overinvesting in it previously, right? Recently it broke, and I have no clue how to get it working again. Fortunately it's just an internal thing so it's just disrupting work and not causing problems with customers, but now we're having to abruptly scramble to migrate off it and it's going to push back a lot of timelines. We got away with coasting on our upfront investment for six years, but it didn't last forever. reply droptablemain 13 hours agorootparentprevThis perfectly describes a scenario I found myself in several years back, down to a tee. reply throwawayq3423 13 hours agorootparentprevThis is why I think X is worth maybe 1b, not more. reply nebula8804 13 hours agorootparentIt depends. Musk has a great ability of burning his employees out to make up for mistakes he has made: (Early automation efforts at Tesla in 2017/2018 where they repeated the mistakes GM made in the 70s/80s, dumb mistakes in the early days of SpaceX when they tried to replicate what NASA had in the 50s/60s). In the end he succeeds powering through all the dumb decisions he makes and manages to pull ahead:(Tesla production and engineering is now seen as a innovation leader in the industry and not just dismissed as some dumb flash in the pan, SpaceX has no real competitors, maybe the Chinese will eventually catch them but no one else is even in the conversation). I have often wondered why so many people sign up to put themselves though this and I guess the clearest answer I got was when I interviewed some Starlink employees at DEFCON. They just want to be part of a team of excellent players and will overlook Musk's faults to be a part of that team. reply jimbokun 11 hours agorootparentIn Walter Isaacson’s biography of Musk, one of his employees completely burnt out and quit. Then came back years later because he was bored and wanted to be part of something cutting edge again. reply nebula8804 11 hours agorootparentYeah I remember that. Honestly I wish there was some other organization that could rival the Musk companies. We need to spread out the power that he has right now but man I just don't know what the other guys are doing. Just today I saw the Rivian and Lucid financial results and they are not looking good. On the space side I'll take Bezo's Blue Origin but I hear almost nothing good. reply TheHiddenSun 10 hours agorootparentprevHow horrible mismanaged have all other big companies have to be, for someone like Elon Musk to easily overpower and overtake them? It's not like overworking employees is a novel concept no other big corp has left untried. I think attributing Musks success to \"overworking employees\" is either completely wrong or not sufficient to explain his success. You can't say it's by chance, because it's not his first and last successful company either. reply no_wizard 13 hours agoparentprevWhy is anyone so quick to take the company side? For starters, productivity is a broad term, so without defining what it means in context it isn’t doing anyone any favors. You won’t feel the dip or record it until after the event happens, and often not right away. When productivity drops a year from now for example, that could be due to layoffs. It’s not always immediate in its impact. I also question this from the other side: why are executives who set the company priorities not also axed? They are the ones that ultimately messed up as they had poor strategic vision. Their ineffectual leadership is how it got there in the first place, if they aren’t gone how on earth do you know that it wasn’t their responsibility for the decrease in productivity? Logically one would think that axing the executives first and then seeing how productivity increases or decreases with new leadership would make more sense reply peruvian 13 hours agorootparent> Why is anyone so quick to take the company side? HN has a lot of \"wish I was a tech CEO\" types, plus the general dose of contrarianism that runs deep in this place. reply fullshark 12 hours agorootparentWe are all temporarily embarrassed founders here. reply spacemadness 12 hours agorootparentprevAnd don’t forget a nice dose of “I’ve never struggled in my life” libertarianism. reply ta2234234242 7 hours agorootparentprev... who also won the birth lottery, went to stanford, and then golfed with venture capitalists. reply Jensson 13 hours agorootparentprev> They are the ones that ultimately messed up as they had poor strategic vision Everything is a gamble, the right decision can still lead to a bad result due to randomness. So just because a gamble didn't pan out doesn't mean that it was the wrong thing to do. For example, not hiring extra is also a gamble, it means you gamble on the market not growing and you would lose a ton of money if the market didn't stall. reply no_wizard 13 hours agorootparent>For example, not hiring extra is also a gamble, it means you gamble on the market not growing and you would lose a ton of money if the market didn't stall This assumes hiring is the only way to grow the company. More specifically, it assumes ramped up hiring is the only way. There are balances that can be struck, for example, instead of hiring 100 people, maybe hire 20 and see how onboarding and growth pans out? Perhaps evaluate business inefficiencies that serve to slow people down like bad process etc. so people can focus more time on what matters. Automated onboarding, group sales calls and other tactics let you make more with the same amount of people. There are other things you can do, not just hire. And I'm never going to say don't hire people, but what executives did was irresponsible and I knew it even at the time. IMO, this stuff happens because executives know they won't be held responsible if it can't be sustained, and that is really the core issue. Proportions are what matters. I like many am tired of seeing a world that favors oscillation in the extremes like this, and its the average person who takes the brunt when it goes south and the higher ups rake in the brunt of it when things are going well. For what its worth, if we had European style safety nets, this would be a moot point in a way, because we have so much of American society tied to having a job (be it health insurance, ability to afford rent etc) reply Jensson 13 hours agorootparentYou assume they were fired due to inefficiencies, but they were fired since the market went down and the US government made software engineers more expensive short term by changing how those investments were taxed. That means they can no longer maintain as many software engineers. > but what executives did was irresponsible and I knew it even at the time. Lots of people said this during the entire hiring spree from 2010 to 2020, but most of those people are still there paid and working. If they listened to people like you most of us here would be out of a job since the job market wouldn't have grown that fast. reply no_wizard 12 hours agorootparent>Lots of people said this during the entire hiring spree from 2010 to 2020, but most of those people are still there paid and working. If they listened to people like you most of us here would be out of a job since the job market wouldn't have grown that fast. That assumes I'm taking an absolutist position like you shouldn't hire or hire rarely, and I'm not. There needs to be successive indicators and re-evaluation of conditions that need to be balanced. In many (most?) cases, they simply weren't. I would say the accurate time to call this out would have been 2021, not 2009, and that is the timeframe in which I looked at everything and thought to myself \"this won't last more than 5 years at best\". By late 2020 to early 2021 I created a thesis that we crossed into artificial growth rate and too much \"free\" money being pumped into everything around us, to be clear about this. The steady growth from 2009-2019 in tech was relatively organic by comparison, and far more moderate for most of the industry if you compute only non FAANG companies (FAANG companies, and those near that acronym, simply had outsized growth by comparison, by many multiples) All this is to say, that I think if people listened to me, it would have nothing to do with 2010 to the start of 2020. reply HumblyTossed 13 hours agorootparentprev> Why is anyone so quick to take the company side? Corporate and Political propaganda since the Powell Memo. reply jachee 13 hours agorootparentprev“It is difficult to get a man[ager] to understand something when his salary depends upon his not understanding it.” (With apologies to Upton Sinclair.) reply aksss 11 hours agorootparentprevHonestly, I think taking the company's side is the contrarian opinion. The knee-jerk reaction is to judge them as the evil man oppressing working families. Why? The vast majority of people have not worked in managerial positions with high levels of responsibility to understand the decision-making and interest balancing processes required for a large organization to operate. But everyone's been a victim of managers at least once, and certainly seen bad ones. And many people think they can imagine what's entailed in the job. Your questions about ownership and responsibility are not invalid, but they are also the most accessible (easiest) questions. You don't get to see the sausage unless you're on the inside and most people can only imagine what that looks like, and then tend to imagine the worst or most malicious alternatives. Thinking beyond those questions isn't always warranted, but doing so does require a perspective that most people don't have direct experience with. reply no_wizard 11 hours agorootparent>The vast majority of people have not worked in managerial positions with high levels of responsibility to understand the decision-making and interest balancing processes required for a large organization to operate. There are some of us who have and still see the facades and illogical reasoning for many decisions, decisions that if made in the lower wrung of the company would definitely lead to someone getting fired. People ignore good evidence even at the highest levels all the time because their incentives are not aligned, for example. I witnessed this before. I've been in director+ levels multiple times, at small and medium organizations. I'm not convinced that my peers were any smarter than lower level employees when it comes to making decisions given all the available inputs. I will say, that in mid sized organizations I've been apart of, you do see sometimes that lower level employees have no concept outside the department, and that leads to tunnel vision. They don't see the complete story all the time, but I always viewed those as communication failures most of the time. I \"made the sausage\" at my previous job reporting directly to the CTO[0] at a 1000+ person company. It was alot more volume of information, and I talked to alot of different people all the time, but I think you could take any whip smart employee, elevate them to that position, and they could do a reasonable job. That experience has left me really jaded to be honest. Whatever humanity I had left to endure corporate life had sunk by the end, as it were. [0]: This is why I was raising alarm bells in the first place, because I knew it'd be heard. Fact is, it was ignored. reply crazygringo 13 hours agorootparentprev> why are executives who set the company priorities not also axed? They are the ones that ultimately messed up as they had poor strategic vision. Are you fired for any mistake you make in your job? Generally not. And managing staff size is like 1% of their job. They're also busy running their divisions, managing roadmaps and objectives and all sorts of non-staff resource planning, etc. It's entirely reasonable to think economic and competitive conditions will probably be such that we should hire more, and then your competitive situation changes and you have to have some layoffs. That doesn't mean anybody made mistakes or the executives should be fired as well. It just means that nobody has a crystal ball. Very little future estimation that is done in the business world turns out to be perfectly correct. Sometimes executives do mess up in big ways and get fired just like anybody at any level. But that has nothing to do with layoffs in any special way. reply no_wizard 13 hours agorootparent>Are you fired for any mistake you make in your job? Generally not Yeah I think when a layoff is enacted, you made more than a mistake, you fundamentally missed something. Its outside the bounds of needing to be perfect or what have you, something went sideways in a big way, and unless you can actually simply trace it down to market forces beyond your control and you are telling me those people can't be productively moved into other parts of the business, then sure, we have some criteria for layoffs, but then we can be adults and get walked through the failures and how there wasn't anything that could be done. How many situations do you think this actually applies? I have been apart of 2 layoffs (not affected in either, thankfully) in the past 3 years. In the first situation, I and others repeatedly warned that our biggest 3rd party integration, which made the backbone of our business, was going to ban us for abusing their APIs and other business practices that we were doing, and we needed to address that. This was repeated all the way up to the highest levels, I was in the meetings with the CEO, talked about precedent etc. They didn't change anything. The ban eventually happened, and they never recovered, laid off over successive rounds, ultimately ~50% of staff is gone now, because of this very obvious strategic error. In another situation, they tried to blame it on interest rate hikes, but before interest rate hikes, we were sounding the alarm that we were on-boarding way too many iffy customers (its a fintech place for SMBs to manage AP) with out of bounds credit profiles, and that these new sign ups would dry up way too easily, we needed to re-focus on a better core ICP before we try to further expand the business, try and target customer segments with stronger business profiles etc, and this is prior to the majority of the interest rate hikes. I and several others made this case, and yet again, not headed, layoffs hit 6 months later, because all those customers either went belly up or had to downsize themselves, because they were too credit dependent. So what commonality do you think this happens? Because I have a feeling its alot more than is acknowledged. reply bombcar 12 hours agorootparentSometimes a company is moribund much, much earlier than people realize. When you're faced with \"do X or shut down\" you will often chose to do X, even if X is pretty likely to result in shut down because you have a \"die now, maybe die later\" thing. Or you can do a massive pivot, which has successfully worked at times. reply tick_tock_tick 12 hours agorootparentprevI think you just have a fundamentally different view of what a layoff means then the executives, board of directors, and general shareholders. > I and several others made this case, and yet again, not headed, layoffs hit 6 months later, because all those customers either went belly up or had to downsize themselves, because they were too credit dependent. But what if the customers survived? What if interest rates didn't hike as fast? You're suggestion would have been horrible. The risks of over-hiring is a layoff which no one else views as really that bad vs failing behind competitors. reply no_wizard 12 hours agorootparent>I think you just have a fundamentally different view of what a layoff means then the executives, board of directors, and general shareholders. Most likely, and I personally believe that is part of the problem, not the problem. I recognize how damaging layoffs are to people. Its unfair to act like they aren't. Its people's livelihood and we as humans often make decisions around these things, and when its yanked out from underneath them without notice, its very damaging. Its not numbers on a spreadsheet, it is human lives. >But what if the customers survived? What if interest rates didn't hike as fast? You're suggestion would have been horrible. The risks of over-hiring is a layoff which no one else views as really that bad vs failing behind competitors. We would have hired more modestly, and could focus on making the business more generally efficient, cut down on process, and focus on keeping the business nimble and responsive. If I'm wrong, we end up primed to take on more customers while continuing to hire modestly. Ironically, our smaller more nimble competitors ate a bunch of market share from us with much smaller overall staff, because they could move faster, and I can directly trace that to the hiring spree. Now, I will admit, if we had stronger safety nets in the US that made layoffs relatively painless for the unemployed, I'd feel differently about this. reply Jensson 12 hours agorootparent> could focus on making the business more generally efficient, cut down on process, and focus on keeping the business nimble and responsive Every company already tries this as hard as they can, there is no magic button they can press to just make themselves more efficient. reply no_wizard 12 hours agorootparent>Every company already tries this as hard as they can, there is no magic button they can press to just make themselves more efficient. Then why are so many so inefficient? In the Silicon Valley mythology, one of the core tenants is that startups can act more efficiently than big companies simply due to focus & size. If this were inherently true to any reasonable degree, I actually think we would have a differently shaped workforce. I can point out dozens of inefficiencies I experience even today that slow down development, and that is just if we analyze development. I hear from other wings of the company[0] of other things - simple things like consolidating tools so they don't have to bounce between different apps to do things - that also drive alot of real inefficiency. This isn't the first place I worked where all these issues exist either, and its not nitpicky stuff, its all low hanging fruit. Consolidating tools is a complaint I have heard at almost every job I've had sans 1. I don't imagine I'm alone, my peers at other companies often have similar complaints when we talk. I think some things are inherent to simply organizing humans, and I get that, but the obviousness of some improvements that simply aren't undertaken runs counter to the assumptions one makes when they buy into the \"efficient business\" hypothesis [0]: we successfully became more cross functional, that's a huge plus, our solutions are much more holistic now, but its only part of the problem. reply tick_tock_tick 12 hours agorootparentprev> We would have hired more modestly, and could focus on making the business more generally efficient, cut down on process, and focus on keeping the business nimble and responsive. Or more realistically doomed the company to fade from grace as you competitors surge past you eventually ending with a massive layoff. reply no_wizard 11 hours agorootparent>Or more realistically doomed the company to fade from grace as you competitors surge past you eventually ending with a massive layoff. This is a black and white take that doesn't jive, additionally, we have the hindsight of history, both recent history and the history of other boom / bust cycles, that serve to validate. Our competitors wouldn't have zoomed past us. We have strong market presence and we bleed more users due to our own ineffectiveness than strictly from competitive pressure, and I can trace much of that ineffectiveness back to hiring practices from late 2020 to mid 2022. As I noted previously, we have a clear line of how this happened. Generally speaking, a moderate growth strategy would have left us in a better place, and it was easy to see by the end of 2021 that things were going to change sooner rather than later. I'm not that smart, by my own admission, I'm a pretty dumb guy, but I put the pieces together by then by simply looking at where the money was coming from. There was no way the conditions of 2021 were going to continue at most past 2025 if you wanted to be an optimist, but realistically, it was coming down a lot sooner to anyone paying attention to how the money was flooding in, and I felt we would start to see some downturns in 2022 and 2023. As soon as the COVID relief stuff was announced to be ending, it should have been even more so, yet there wasn't a prevalent skepticism in the status quo until much later. reply p1esk 12 hours agorootparentprevHas anyone tried laying off most of the executive team - instead of the workers? reply no_wizard 11 hours agorootparentI always wanted to see an experiment where you take a really knowledgeable about the business front line employee, and hoist them to the C suite with all the same support systems executives have (and there's more than you might expect) and see if they do the same or better in that chair. There's only be limited studies done in this vain, and the results, while limited, seem to suggest that any reasonably smart person with knowledge of the industry could do a good job but the studies are pretty limited, and don't construct the circumstances as I described, so its hard to say its true or not true. reply crazygringo 11 hours agorootparentThey would do terribly. They're completely and utterly different skill sets. It makes as much sense as taking an oboe player and asking them to conduct an orchestra, without any conducting experience. Do you know how to handle investor relations and different factions on the board? Managing sales and marketing strategies? Assessing the regulatory environment in Europe and the legal risks of expansion in China? Can you speak confidently and charmingly at conferences with billionaires? Do you understand how to manage different interest rate environments? Is it obvious to you which law firms to hire for different needs? This isn't just stuff you can learn on the fly. It takes many years of experience, and often a business degree in the middle, to become even moderately competent in this stuff. reply p1esk 5 hours agorootparentAll these tasks you listed - what makes you think an average CEO knows how to do any of them? How do you know if the company is doing well because of the CEO or despite them? reply danaris 6 hours agorootparentprevI have yet to see evidence that what gets done in the C-suite of many (not all, possibly not most—but many) companies is making use of any skill set at all, besides self-aggrandizement and bullshitting. To be charitable, for some of these, what's happening is that rather than leveraging their skills, they're leveraging their relationships. And that can be valuable, sometimes, but it's not a substitute for actually knowing what the hell you're doing. The real problem is that far too many of these people—skills or no skills—primarily bring their egos to the table, and operate mostly based on that. reply ta2234234242 7 hours agorootparentprevWhat's funny is that people assume the executive team must be high performing. In reality, they are able to fire the people with the least amount of power. reply davesque 14 hours agoparentprevFor the sake of argument, does that mean all those employees were useless or does it mean the management isn't doing a good job of allocating them to effective work? I've been at a few companies that were rudderless and yeah, letting go of people wouldn't have made much difference because the leadership didn't know what to do with them anyway. reply cj 13 hours agorootparentA lot of high growth companies simply hire too many people because too many people is usually better for maximizing growth rate than too few people. (If a company's focus shifts from maximizing growth rate to, e.g., optimizing for cash flow or profit instead of growth rate, the equation shifts and lay offs happen) Having extra people on staff who aren't needed can be good as long as you expect to need them in the near future. That obviously creates the risk of needing to terminate the positions if the need for the person never materializes. It takes 2-3 months to hire someone after opening a role, and 6-9 months to onboard them. So 8-12 months to fill a role with someone fully up and running. That's a really long time which is why so many high growth companies fill roles they don't actually need right now. reply sokoloff 13 hours agorootparentAlso, in a company that's growing 30% every year, hiring \"10% too many\" people at any given moment is an extra 4 months' of pay for 9.1% of your payroll, or about a 3% mistake in a 30% growth environment. In a company that's growing slowly or not at all, hiring \"10% too many\" people is a much larger error, possibly even a disaster as it could be a full year or more worth of extra hiring in a slow growth company. reply Jensson 13 hours agorootparentprevThey bet on projects but those projects didn't pan out, every company does this, every hire is a gamble. Even a cafeteria that hires a new person assumes future demand will keep up or increase, if that prediction was wrong the person will not be needed and will be laid off, that is just how business works. This is much better than the \"you get what you use\" resource allocation where organizations never shrinks, we want them to shrink when they don't have any useful work to do. reply matwood 13 hours agorootparentprevBig tech clearly overhired during the pandemic. The layoffs over the last 12-24 months are a return to the mean. reply nytesky 13 hours agorootparentIt actually feels like a semi-coordinated effort to push down wages. With high inflation, workers would have demanded raises, but with the specter of layoffs, they keep their heads down. Musk bought Twitter in April 2022, had huge layoffs and then rest of tech followed in Nov 2022 onwards. Prior to current trends was it common to announce great earnings and then announce layoffs? reply dartos 13 hours agorootparentNothing seems coordinated. Musk made layoff palpable to investors, ZIRP ended, the industry is correcting for over hiring when assuming the 2020s growth would continue forever. Just a bunch of things that happened. As always. “We didn’t start the fire” reply kansface 13 hours agorootparentprevWe went from 0% interest rate to a whole lot real quick. It used to be there was literally no overhead for over hiring if it moved the bar at all. Thats not true anymore. reply nytesky 12 hours agorootparentIf it was really over hiring, and the end of ZIRP, why have their financial results been so strong? It’s not like profits dipped to losses from these “headwinds” — companies have been strong and AI is increasing demand for cloud services, chips, etc. reply baq 12 hours agorootparentprevIn the states it’s a new tax thing going live. reply krainboltgreene 13 hours agorootparentprevFunny you should say that because H1B's have shot back up to pre-pandemic rates. I've never understood why people would believe this overhired narrative, have you ever worked for a company that correctly understood it's own hiring capabilities or measurements? I haven't 15 years in. reply throwawayq3423 13 hours agorootparentIt's cost cutting, not demand for labor. A large headcount used to be a status symbol for a public company (to juice the stock), now if anything it's the opposite. reply pdonis 13 hours agorootparentprevIf the company is rudderless and unable to allocate effective work to employees, it won't be doing well. This is about companies that are doing well. At least, that's the premise of the article. reply davesque 13 hours agorootparentNot necessarily. Companies can be rudderless and have some kind of industry capture or brand recognition and still be doing great. In other words, at one point they had good leadership or built up during times of low regulation or economic prosperity and now are just living off those early successes. A lot of large tech companies are like this. reply smallmancontrov 13 hours agorootparentYeah. The Google Graveyard really shows how far this can go. https://killedbygoogle.com The punchline is that in addition to hundreds of failed hobby projects, their stock is doing great. Monopoly rents are a helluva drug. reply google234123 13 hours agorootparentI never understood the complaints about this. Why shouldn’t a company try and do new things? reply smallmancontrov 12 hours agorootparentMonopoly rents happen at the expense of other economic participants. For every tombstone on that page, I wonder how many mom & pop diners died due to CAC math. reply Jensson 13 hours agorootparentprevYeah, not sure if it would be better if Google put all those people to work on their main products like most other companies. Google maintains their ability to launch new products by those constant launches, they are like constant small exercises, sure they usually aren't new billion dollar products but at least they still know how to launch new products fast unlike many other companies. reply thfuran 13 hours agorootparentprevA company probably should try new things if they have enough extra money. A company definitely shouldn't develop a reputation for abandoning every new thing they try shortly after launch. Why would any company risk depending on a new Google offering for something they actually care about? reply danaris 12 hours agorootparentprevThis is essentially assuming that competence translates directly into high monetary value. That is not true either at the individual level, or at the level of companies. It is particularly untrue with tech behemoths who enjoy near-monopoly status, massive network effects, and/or huge mindshare. At the very simplest level, it is not at all uncommon for good decisions made previously to allow an organization to \"coast\" for a significant amount of time after it starts making bad decisions. This is part of why bad CEOs are so frequently not held accountable for their incompetence: they take over, and for 3 years, everything's going great! It's only after the accumulated \"soft\" resources (customer goodwill, employee loyalty, lack of technical debt, etc) are burned away that the bad decisions actually start being reflected in the balance sheets. reply notahacker 13 hours agorootparentprevNot to mention that those bullshit jobs that don't really impact the company's earnings initially tends to include stuff like fixing the IT systems and handling customer issues within an acceptable time frame... reply adamc 13 hours agorootparent^This. It isn't easy to say, in the short-term, what the costs of cutting those jobs is. You might be underinvesting in future products, or in satisfying current customers. None of that will be immediately reflected. reply RankingMember 14 hours agoparentprevProductivity inertia is a thing. It's common for things to keep rolling in the short term after a big layoff. Over time, though, the little things that so many faceless now-gone people were dutifully keeping going the whole time start to degrade and the cracks get bigger. It's not like these people were all working line jobs where there's an obvious productivity falloff like a now-unmanned conveyer dumping product on the floor. reply makerdiety 13 hours agorootparentThe difference is that now usually the conveyor is smart enough to stop conveying if it notices items falling off its belt. Human labor is becoming superfluous in the face of self-sustaining capital arising or whatever. reply ceejayoz 14 hours agoparentprev“I stopped changing the oil and my car runs fine.” Sometimes problems take time to present themselves. reply kajecounterhack 13 hours agorootparentThis. Attrition often doesn't appear until the next vesting cliff / bonus. If leadership is not careful, they could lose a lot of key people all at once. For a big company like Google this is easy to stomach (they still pay well enough that _someone_ can be made to do the work eventually) but this could well be death to a startup. Even an understaffed team can crawl along with a skeleton crew for a while, but you will burn people out. A burned out team ceases to make forward progress and is going to act like things are in maintenance mode. You're liable to spend more money and SWE-hours looking for backfills vs if you'd just kept your team intact. reply google234123 13 hours agorootparentprevHow about “ I stopped changing my oil every 50 miles and my car runs fine” reply ejb999 12 hours agorootparentthat is more like it. Or we fired 1/2 the scrum masters, so now each project only gets 50% of one instead of a FT one, and BTW, all of a sudden developer meetings get cut in half as a side benefit. Every large company I have ever worked for could have easily fired 1/3 the staff and ran just fine indefinitely - but only if they fired the right third. reply p_l 12 hours agorootparentIn my experience usually the real benefit would have been if they hired the right kind of positions. No, not \"hire better developers/etc\". Hire for positions they don't hire because they seem superfluous. I think in every bigger project over the last 10 years I asked, in otherwise reasonably staffed companies, for a technical writer. Even part time one. Fohgeddaboutit. Sometimes the issue is, as sacrilegious as it sounds even to me, not enough managers. Because sometimes you need people whose only role is to be communication points, or people whose job is to be responsible and accountable. In most of those companies we could probably fire a lot of people too, probably mostly among executives and a lot of those invested in SAFe and the like... reply EchoChamberMan 10 hours agorootparentprevIs Google doing mass layoffs every few weeks? reply sobriquet9 12 hours agoparentprevYou can fire half the commercial pilots, and planes will continue to fly without a hitch. Why have two pilots, when one will do. Trucks have one driver and work just fine. Eventually some planes will crash, but it's ok as long as it happens after effective management gets their bonus and moves on to optimise another company. reply joshjje 12 hours agorootparentAnd make them pay for their own diapers. reply MilStdJunkie 13 hours agoparentprevIf I went into your body and deleted all your DNA, you'd feel fine too. Chugging along. For a while. Here's another thing: the productivity that's being tracked isn't aligned with the value that the consumer is paying for. You can see this in government contracting a lot, where the consumer doesn't even see the product - they just see the paperwork. If you're really good at \"producing\" paperwork, you don't need anything else for a very, very, very long time. reply stetrain 13 hours agoparentprevYou can make pretty significant cuts to R&D and see short term revenue and productivity stay pretty level. In fact it should improve your metrics since you are spending less and selling just as many of your existing products as you were before. Whether that is a good long term strategy is a different question. reply swatcoder 13 hours agoparentprev> no visible dip in the productivity How did you measure that? How long are you tracking it? How are you isolating other factors? While you figure that out, what accountability does your organization have towards the hundreds of thousands of human individuals whose lives were just grossly disrupted and sent into potentially catastrophic strain? Different societies have found different answers to the accountability question. Whether or not the jobs were individually bullshit (almost all jobs are, as are almost all companies) has little to do with the conversation. reply scruple 11 hours agoparentprevMy productivity has been severely impacted, actually, without even going into the mental health / stress / morale effects. I've seen two rounds of layoffs at my current employer and today I am utterly and completely fucking swamped in work. New initiatives showing up in January, directly from corporate sponsors at the SVP and executive level, despite the Great Big Fucking Deal they made about us getting our 2024 \"roadmap\" to them in November of 2023. Bullshit jobs or not, I'm not sure why I'm expected to have to pick up the slack and do more work at the same time. reply kayo_20211030 13 hours agoparentprevThe discrepancy is time i.e. time for things to happen. All organizations have inertia and they will continue as they will until they don't - slowly, until all at once in the end. Maybe that company will retool with a new cadre of recruits, and catch up, or maybe they won't. That's the bet the mgmnt are making. Can I anticipate the next inflection, and if it's later than I expect, will I be somewhere else and outside of the blast radius? reply Delumine 14 hours agoparentprevIf only a handful of people are capturing all the profits, and everyone else is unemployed, homeless, and hungry... that's how you get a decline in society. reply euroderf 14 hours agoparentprevProbably a good way to cultivate your garden of technical debt. reply Spooky23 13 hours agoparentprevOperations is one part of a business. Canning your development and marketing doesn’t affect that right away. Everything gets a little shittier over time. The best tech example is IBM. They descended from industry leader to desiccated husk, only sustained by legacy business and acquisitions. reply occz 13 hours agoparentprevIf you cut off your leg, you'll have enough meat to last for a while. In the long run, you're going to be worse off. reply kaycey2022 1 hour agoparentprevBullshit job phenomenon is inevitable because the game encourages a winner takes all scenario for now. One or two market leaders keep accumulating all the capital and then instead of using their hires to move the needle of progress forward, they force most of them into some pointless hyperspecialisation or stuff like denying competitors of needed talent. All this just means that the balance of society has tilted to one unsustainable direction. Society simply needs much better antitrust (or other) laws to the point that every monopoly/duopoly is broken down and balance is restored. I will use twitter as a hypothetical example. If only twitter can raise capital to the point where it can handle enough traffic at scale and be an arbitor of global news, then it can easily use its resources and reputation to create and staff bullshit jobs without much consequence. To bring back balance: 1. other legal jurisdictions should ban or tax twitter out of existence over there and raise their own monopolies, for which they will likely need a global workforce 2. cost of investing in twitter or the cost of holding large investments in twitter should be increased so that the instead of accumulating capital in the market leader, people are incentivised to let their money be reinvested into competitors. This will be resisted by those who already have enormous sums of money invested into the market leader, which unfortunately also happens to be the people who make the rules. reply jjav 11 hours agoparentprev> can fire 10% of their workforce and they just keep on chugging without a hitch They don't, they just stop doing some things and pile on extra unpaid work on those left. reply throwbadubadu 13 hours agoparentprevHow long do you allow for evaluating if there is a dip? And will times never change? And even if there is really none, then sure it makes sense from the company point of view and pure profit perspective. But if the company is well of, why lay off experienced, trusted and loyal workforce still? You always need slack room, like you need some money put aside. And another level, work puts meaning to our lives.. either we allow for other meaningful things supported by some UBI, or we should rethink if profiting companies really need to ultra-maximize, or how they can also do a little welfare for their employed work force.. imo it's a pity the employee owned model is not more widespread, and it is all for the few. But anyway, different end of a very wide spectrum. Certainly, struggling companies must get rid of the bloat, bullshit jobs, and evil within, if it needs that to survive and is the actual problem ;) reply dagw 13 hours agoparentprevOne thing that is baffling to me is that these companies can fire 10% of their workforce and they just keep on chugging without a hitch. A company I used to work for set up a team to develop a new product for a new market. After 3 years they were still unprofitable, the product was shut down and the entire team was fired. This obviously had no direct effect on the companies overall productivity. Was that team doing a \"bullshit job\"? If you fire the team doing R&D on the next generation of one of your products or the team working on launching your company into an entirely new market or the team the works on internal tooling or some of the people managing key account relationships with certain customers or suppliers or many other similar jobs then the company can absolutely keep going without a hitch, for a while. reply gumby 13 hours agoparentprevWhat's your timescale? If you take medicine for a chronic condition you can often skip a dose without problem, maybe even two, but eventually you presumably revert back to where you were before treatment. I saw a lot of this back in the \"reengineering the corporation\" craze in the 90s (and then the \"lean organization\" and so on). They cut all the slack out of the system so in steady state everything seemed to work. But the world is not consistently in a steady state. So fire that guy doing backups, and save money by not paying for the redundant network connection you've never used. Stock goes up, prepare three envelopes, and retire! reply derefr 12 hours agoparentprevLarge companies reinvest most of their profits as bets on the future — building new arms of the company, that initially do nothing but quiet R&D for months/years — but which might bring the company entirely new revenue-lines years down the line. Much of a large company's headcount at any point in time, is people hired to work on these bets on the future. You can fire all of these people, and nothing will happen to the company's day-to-day operations. But the company's stock will take a nosedive nevertheless — because the promise of future increased returns had already been factored into the share price, and that factor now has to be removed. reply altruios 13 hours agoparentprevThat people have to be employed artificially so they don't starve: is the problem. UBI has the potential to remove a lot of bullshit jobs, companies can run much leaner when they aren't responsible for a citizen's needs. reply gordon_freeman 13 hours agoparentprevHow would you check for the visible dip in the productivity? and in what time frame? Sometimes it is not easy to measure the impact of layoffs on the org in the short term and it may take some time to realize it. reply lazyasciiart 12 hours agoparentprevYou could probably cut every employee’s salary by 10% without noticing a visible dip in productivity. Maybe that means that visible measures of productivity are lagging indicators? reply haswell 14 hours agoparentprev> and there is no visible dip in the productivity, is it really a bad thing? A few issues to consider: 1. Many companies can continue to operate for a time purely based on momentum and existing pipelines. That does not mean that this is sustainable. 2. The lack of a visible dip does not mean the internals are healthy. Often layoffs are accompanied with an expectation to \"do more with less\", which generally means the conditions going south for remaining workers. 3. In an economy that is predicated on everyone having a job or enough income to spend money to support the continued existence of companies, it is not sufficient to look only at the output of the company itself. Bullshit jobs may be a real thing, but much of the system is built on \"bullshit\", all in service of perpetuating the ongoing success of the system. If every company could automate every function and there were no employees anywhere, this would obviously not be sustainable in the long run, and it becomes more apparent that the role of the worker is not just about the company's output, and is intrinsic to the whole system, and that's something we'll have to contend with eventually. I'm not arguing that this is a good thing, or that companies should keep people for no reason, but we're in an era where technology is increasingly disrupting core aspects of the system. reply pdonis 13 hours agorootparent> If every company could automate every function and there were no employees anywhere, this would obviously not be sustainable in the long run Perhaps not, but I'm not sure it would be for the reason you are implying. What is this hypothetical company that automates every function and has no employess anywhere producing? And what price can that thing fetch in the market? If the marginal cost of producing additional units is basically zero, which is what the hypothetical implies, then the products should be basically free, since standard microeconomics says that price = marginal cost. Which means nobody would need jobs to afford these products. Which means this condition would be sustainable in the hypothetical as you state it: sure, there would be no employees, but there would also be no need for the jobs that no longer exist. Of course in the real world there are plenty of ways this could go sideways, but they all basically boil down to: in the real world, governments mess with markets in ways that prevent price from being driven down to marginal cost. That means many things (food, for example) cost more than they should, which means people need more income than they should need if the market were truly free. So what is really not sustainable is governments trying to mess with markets. If that stopped, we could make the necessities of life basically free. That would be a good use of technology. reply haswell 13 hours agorootparentMy point about a lack of sustainability wasn't assigning that issue to companies alone, but broadly to the system as it currently exists and in which those companies operate. > If the marginal cost of producing additional units is basically zero, which is what the hypothetical implies, then the products should be basically free I don't think this is implied unless all productive work across all industries is automated. Practically speaking, this would happen over time, meaning that a fully automated company today could significantly reduce its costs, but would still have to purchase goods and services from upstream providers and pay to market and distribute their products. And since companies essentially operate on greed, the leadership of these companies would carve out space for themselves, and the short term goal would almost certainly still be to maximize profit, meaning goods are not free, even if the marginal cost is low (gotta pay for all of that automation R&D after all). Somewhere there is a tipping point where enough automation would throw the system into chaos without major changes. If it could be achieved overnight, then hypothetically we could all have free stuff. reply ryandrake 7 hours agorootparentprev> 2. The lack of a visible dip does not mean the internals are healthy. Often layoffs are accompanied with an expectation to \"do more with less\", which generally means the conditions going south for remaining workers. I've always wondered: If the layoff is happening because the company over-hired and the roles are no longer needed, then why do existing employees need to step up and do those roles that the company just said are no longer needed? reply haswell 6 hours agorootparentHighly depends on the company and how they select the people to be laid off, but the issue can be that the layoff targets some % of over-hiring, and some teams are effected more than others depending on the makeup of those teams, project load, etc. Some teams are disproportionately impacted. reply insane_dreamer 13 hours agoparentprevI'd rather those profits be spread out among additional employees (who might help the company either produce better stuff or give better service to customers) than to shareholders. reply tamimio 12 hours agoparentprevSimply because the impact never happens instantly. Companies life span is different than humans, and just because you didn’t see it happening in the coming months right after they did, it doesn’t mean there’s no gradual decline, keep in mind this also does not account the gradual hiring after that event, they news will be reported that they terminated 10%, but who’s going to report a 1% hiring every month in the year after? reply johnny_canuck 13 hours agoparentprevIME (having gone through 3 layoffs), you can squeeze a lot out of fearful employees. I haven't be able to see that sustained over the course of 12+ months though. reply theropost 13 hours agoparentprevI think it's this is more of a real point. Honestly if someone who works in a large bureaucratic organization, the amount of people who do next to nothing is astounding. Is that a bad thing? Yes to some degree, however we do need to have jobs for people to do no matter what and if this gives them something to do everyday and they feel like they're contributing something then maybe that's all they need to do. reply xg15 13 hours agoparentprevI guess that depends on what you believe the goal of all of this is - making a great society for companies or a great society for people. reply ike2792 12 hours agoparentprevAs companies get bigger they tend to get more bloated and disorganized. A lot of initiatives end up not being worth continuing on, so they lay off the people working on those initiatives. Better companies will reallocate talent to new, better initiatives so they don't lose the knowledge in their workforce, but that seems rare. reply chrsw 12 hours agoparentprev\"these companies can fire 10% of their workforce and they just keep on chugging without a hitch\" That's not always true though. I've been on both sides of layoffs: laid off and remained. Sometimes the people who remain are asked to do more with less. I was even asked to do more with less pay at one point. reply mathverse 14 hours agoparentprevYes. These companies are faceless entities, literal money printing machines where some people in charge are the gods who get to decide what to do with the money printer. They hold an insane monopolistic position. Often these companies utilize verticals which obliterate entire business segments because they are allowed to give products for free (MS Teams for example). This should not happen. reply brookst 13 hours agoparentprevThat’s one interpretation. Another is that that 10% of work is distributed to the remaining 90%, who now work harder and are more stressed, but well aware that they were lucky to keep their jobs and therefore willing to absorb an extra hour a day. Which, free market and all that, may just be smart business. But I don’t think it’s necessarily good management in the long term. reply droopyEyelids 13 hours agorootparentThis is an interesting thought experiment, because in that situation the company should cut another 10%, and so on, until they get some sort of message they can't cut anymore. reply wubrr 13 hours agoparentprevIn many cases the company can run fine for a while... the systems that engineers already built will continue to function until a certain point. Once they start breaking/failing the company needs to hire additional people for maintenance/improvements. Often these are people who hadn't worked on the original systems and don't have a full understanding of the stack/systems involved this leads to many other issues... knowledge fragmentation, tech debt, maintenance difficulties, increased bugs, etc. I've seen this happen many times in large orgs, and the people making the decisions to cut engineers in certain areas often aren't responsible for what happens down the line and don't need to deal with the fallout. ¯\\_(ツ)_/¯ reply mv4 13 hours agoparentprevProductivity is a lagging indicator. Anxious, scared, overworked people can still be \"productive\" for weeks even months, it's just not sustainable in the long run. Similarly, many systems can continue functioning (seemingly without a hitch) when you cease all maintenance and preventive care. Eventually, they will fail. reply eschneider 13 hours agoparentprevYou can cut costs and cut costs and things seem to operate just fine. Until they don't. See: Boeing. reply datadeft 13 hours agoparentprevDelayed impact is rarely taken into account. You fire people who's impact is going to show up 2-3-6 months from now. Same goes to maintenance. Stop doing maintenance you can save money in the short term. Should you stop doing maintenance? Should you fire this many people? Probably not. reply insane_dreamer 13 hours agoparentprev> that these companies can fire 10% of their workforce and they just keep on chugging without a hitch unless those 10% were working on things which may become important to the company in the future; a company that just \"chuggs along\" will eventually die reply kouru225 13 hours agoparentprevOr those people are doing work that takes a longer time frame to impact the company. Sure it’s fine for the next year, but once you start dealing with issues down the line you’ll wish you had been taking care of all that work. reply nytesky 13 hours agoparentprevIt’s simple. People stop taking vacations and work longer hours and make up the missing 10% of labor. You also no longer have spare capacity for sick days or people leaving, but that doesn’t all come to roost on day one. reply tyingq 12 hours agoparentprevThat feels reductive. Are the people left working 80 hours a week now, maybe? Are they sacrificing long-term goals to keep short-term productivity in place? Etc. reply asow92 13 hours agoparentprevThis is Price's law in action: https://nielsbohrmann.com/prices-law/ reply gilbetron 12 hours agorootparentPrice's law didn't really fit well even within the context in which he created it. Lotka's Law fits better. And they aren't laws, just heuristics, that the tech world likes to propagate as if they are laws, when if fact most tech people that believe they are part of the \"square root people\", but really just lack an appreciation for what is often really going on. reply mankyd 14 hours agoparentprev> is it a really a bad thing? It's great for the company. Not so much for the people. It's a matter of perspective. > The bullshit job phenomenon is the problem here. Perhaps, but who's fault is that? Sounds like the company's problem. Who suffers from the layoffs however? Per your example, not the company. reply ytx 13 hours agorootparentOne relevant question is would the fired person have been better off had they not been hired or the job didn't exist in in the first place? It's far from clear to me, but admittedly doesn't soften the blow either way. reply HumblyTossed 13 hours agoparentprev> The bullshit job phenomenon is the problem here. Which part is bullshit? The part where, in order to survive, everyone* needs a job? Or the part where, because everyone* needs a job, some jobs might not be as important as others? *Y'all know what I mean. reply troupe 13 hours agoparentprevJust laying off 10% of your employees is easy. If a company can identify the 10% of their workforce that isn't contributing anything, that would speak very highly of their management processes. And in any sufficiently large company, there are always 10% that are either adding nothing or are creating negative value. reply corytheboyd 13 hours agoparentprevRedundancy isn’t a bad thing, it’s what lets you go on vacation sometimes. It’s hard to know how much you should have, and it’s no surprise it’s one of the first things to go when companies downsize. Sure, some bullshit is going to slip in, but it’s still a bunch of human beings trying to make a living on this dumb rock. Software itself is all bullshit in the grand scheme of things. reply taurath 12 hours agoparentprevIt’s a bad thing for everyone who depends on the company for their livelihood. The company that decided to hire them, the management that used their headcount to justify a higher valuation. People are burning out, working the jobs of 3 people and are told they’re lucky to have a job. It’s not bullshit if it’s unsustainable. reply boringuser2 12 hours agoparentprevYes. Companies exist at the allowance of a nation-state for the benefit of the people. If the benefit is attenuated, the people, via government, can sanction the corporations. If the corporation is succeeding and paying more into the public till by providing fake jobs, what's the problem? They're not succeeding hard enough? I think the error you've made is that you've mistakenly projected your sense of \"fairness\" on corporations. Corporations aren't people my friend. reply theodric 10 hours agoparentprevThe global team I was in halved while I was at the bank, partly from cuts and partly from people deciding to take their various opportunities to leave and then not being replaced. By the time I departed with severe burnout, I had been consistently putting in 15-hour days for 19 months, and that as a fairly senior member of management (regarded by the business as an exec) just to keep our processes running. In Switzerland, that's not normal. But from the outside, yes, we just kept chugging along, so obviously we were finally the right size. reply tehjoker 13 hours agoparentprevIt could mean the companies are sacrificing long term investment for short term gains. Maybe some of those employees are allocated to failing projects, but it's straight out of the MBA book to kill off stuff that maintains the company position long term or innovates in service of short term bumps in profitability. That guarantees large payouts for execs and shareholders. reply peteradio 13 hours agoparentprevImagine there are 2 groups of people. The first group endeavors to automate and streamline their work as much as possible such that there isn't much left on their plate when the feature well runs dry. Another group has stymied every attempt at automation such that every deploy is a two day long process involving a dozen people. Who do you think management sees as replaceable? reply droopyEyelids 13 hours agoparentprevIf your company did something that made everyone's job 10% more miserable -like maybe all salaried people need to work an extra hour every day to cover the laid off people's work- Productivity per employee would go up, and it would be impossible to really attribute any negative impact to the product to the layoff! But would that have been a wise decision for the company? If it was wise, the company should keep laying off more people, but how can you tell when it has gone too far? reply carlosjobim 13 hours agoparentprevIf the people working for the company has finished making the product, then after that they would have to be tasked with making a new product. So, yes, if the product is delivered then there shouldn't be any dip in revenue if you fire the people who made it. Companies keep these productive people if they want to grow and deliver even more products. They fire these productive people if they're not aiming for more growth at the moment. I use some old software that nobody has been working on for a decade and it still works perfectly, to give an example. reply jimbob45 14 hours agoparentprevYes. Getting fired ruins lives. It causes divorces, suicides, depression, home defaults, and more. Firing and laying people off should be more painful for the company than it should be for the firees. reply runjake 13 hours agorootparent> Firing and laying people off should be more painful for the company than it should be for the firees. Why? How? You start making it painful for the company to lay off people, I anticipate they stop hiring people to minimize risk, and then unemployment shoots up and presents it's own set of (some overlapping) problems. reply PH95VuimJjqBqy 13 hours agoparentprevgenerally they keep chugging along (in tech) because so much of the work has been automated. As long as they can find people to educate themselves on the systems fast enough it's not an issue. I'm sure there's chafe in that 10%, but not all of it is. reply yieldcrv 13 hours agoparentprevI too rely on jobs, right now, but jobs are not something we are entitled to, especially forever jobs, and I cant stand people that act like that Accumulate capital, make it grow If you fail, your life is encumbered in ways that prevent you from doing that, there are limited programs in the US for you, but burdening corporations is a symptom reply paulddraper 12 hours agoparentprevHN: \"Why do you need 1000 people for that? It's a weekend project.\" Also HN: \"You can't have less than 1000 people for that!!\" reply danaris 12 hours agoparentprev\"If hundreds or thousands of people suddenly have no income, putting them in a position of severe hardship and potential homelessness, but it doesn't have any direct negative effects on me personally, is it really a bad thing?\" reply pessimizer 13 hours agoparentprevI agree. We are gradually discarding (again) all possible methods of resource allocation other than market capitalism, so we have to make up pretend jobs for people to do in order to give ourselves the excuse to feed them. For some reason, we think that companies should maintain this insane indirection as a public service. We're not paying people to dig holes and fill them up as a society, instead we're demanding that centers of production and growth willingly disdain productivity in order to subsidize people digging holes and filling them up. We'd be far better off directly subsidizing education, health care, or even customer service and scientific research than demanding that the real sources of all economic growth (and prosperity) have departments printing out reams of paper for another department to shred. Efficiency and technology lower costs and (when properly distributed) give people more time for leisure across the system. You aren't sticking it to the fat cats by telling them not to do layoffs; if you want to stick it to the fat cats, tax their profits and strip layers of economic indirection/middlemen. reply kerkeslager 13 hours agoparentprev> If the company fires hundreds or thousands of people and there is no visible dip in the productivity, is it really a bad thing? How can you be so callous? This is why people don't like startup people. Yes, obviously it's a bad thing, because those people are now without a source of income and often without health insurance. I say \"obviously\" because it's pretty reasonable to expect people aren't sociopaths, and care about the well-being of others. It's only within your sick bubble where maximizing profit at the expense of human suffering is considered a reasonable option. Unfortunately, people like you have a disproportionate power in our society. I'm not saying you're a sociopath, I'm saying that the economic ideology which is commonplace on Hacker News makes normal people who believe it behave like sociopaths. Sure, in a society with a reasonable safety net where losing your job wasn't a potential death sentence, laying off some workers occasionally would be fine. But we don't live in a Keynesian fantasy-world, we live in a world where insurance companies have death panels. reply sokoloff 13 hours agorootparentWould it also be preferable for companies to act as de-facto charities and hire an extra 10% (or an extra hundreds to thousands) of people in order to provide some of the people who are currently out of work a source of income and health insurance? If 100 fewer employees is bad; is 100 more good? reply kerkeslager 5 hours agorootparent> Would it also be preferable for companies to act as de-facto charities and hire an extra 10% (or an extra hundreds to thousands) of people in order to provide some of the people who are currently out of work a source of income and health insurance? No, that's a low-effort straw man argument. It would be preferable for us to stop trusting companies to provide for people's needs, and build effective systems for providing people's needs. A good start might be to stop pretending corporate actions are always morally good as long as they're profitable, and recognize that firing people causes harm to those people. That's not a difficult concept for normal people--again, its obvious--and corporations have spilled a great deal of ink to override basic human empathy on this subject. I'll agree that companies can't be made to work if they're required to act as charities, but I'll also say that if corporations can't exist without causing harm then I'm not sure why we'd want them to exist. The entire purpose of allowing corporations to exist is that they are supposed to benefit society. At the very least, if we're going to allow companies to fire at will, we need a much more effective social safety net to help workers pick up the pieces when they get fired. But of course these corporations will fight against that, because exploiting their workers' dependence is profitable, and besides, they don't want to be asked to pay to fix the harms they cause. reply sadcodemonkey 13 hours agorootparentprevthank you for saying this. the archives of HN comments are going to make a fascinating case study for the next civilization explaining why this one collapsed. reply jader201 14 hours agoprevYou know it's satire from the title (and the first paragraph) because it uses terms like \"fired\" and \"let go\". Nobody uses those anymore. You're \"impacted\". reply esafak 13 hours agoparent\"Poor people used to live in slums. Now the economically disadvantaged occupy substandard housing in the inner cities. And they're broke! They're broke! They don't have a negative cash-flow position. They're ****ing broke! Cause a lot of them were fired. You know, fired. Management wanted to curtail redundancies in the human resources area, so many people are no longer viable members of the workforce.\" George Carlin. https://youtu.be/vuEQixrBKCc?si=cLfi4-hH78PA6ZEg&t=217* reply iteratethis 11 hours agoparentprevThis needs further dehumanization: your \"role\" is impacted. reply xref 9 hours agorootparent“there’s nothing that can’t be solved by another layer of abstraction” reply janalsncm 13 hours agoparentprevA term which I hate because potentially everyone on the team was “impacted” if they have to do the same work with fewer people. It’s just one person who isn’t getting paid. reply preciousoo 13 hours agoparentprevEuphemisms! reply rng99834 13 hours agoparentprevPro tip, it goes both ways. \"Unfortunately, you have been impacted by my resignation\" reply noswi 13 hours agoparentprevAnd the teams are rightsized! reply diego_sandoval 12 hours agoparentprevIf companies started saying \"We have taken the decision to fire you\", people would complain about it being too harsh. If they say impacted, people complain that it's euphemistic. In the end, people just want to complain, and they're going to do it either way. reply saagarjha 11 hours agorootparentYes, because they are getting fired! reply hyperhello 13 hours agoparentprev“Succeeding downwards” reply tairar 13 hours agorootparent\"promoted to customer\" reply ok_vimmer 12 hours agoparentprev\"transitioning\" reply Fin_Code 14 hours agoprevI'm pretty sure a lot of this is connected to the bond market. Companies are running out of operating capitol. When they try and get an operating loan the terms are onerous. Rather than refinance they are electing to perform layoffs to maintain solvency. It does not matter how good you are doing in contrast to previous years. If your loan comes due and you can't afford another one to keep that revolving credit going you need to free up the capitol to pay. reply silverquiet 13 hours agoparentI work for a company that does something that I guess is a bit unusual - they use the revenue from our customers to pay employee salaries. If you would have asked me, this would seem like the obvious way to do things, but I'm told that debt is very important for some reason or other. As Homer Simpson has been known to say, \"I don't know how the economy works\". reply huijzer 13 hours agorootparentIt's about percentages. If the CFO expects that the company can earn 10% on any capital, and they can borrow at 1%, then they will do that. In effect, this means that they will make 9% for free. Even more so, the CFO's bonus probably depends on the net income, so it's a great deal. It's all fun and games until the tide goes out. Then you suddenly might need to rollover the debt at new rates like 6% while the company earnings also go down because everyone can spend a bit less. Suddenly, you might lose a few procent (or more!) on the loans. It's a bit of a double whammy. reply Repulsion9513 10 hours agorootparentIOW (as I've said before and been throughly downvoted for before): short-term profit comes before long-term sustainability. reply rgrieselhuber 13 hours agorootparentprevWhen interest rates are very low, companies are incentivized to borrow more as they can get better returns by investing their own cash flow into assets with predictable returns. When those interest rates go higher and these higher interest rates are coupled with harsher terms, then the ability to invest decreases, companies view reducing headcount as the way to survive to the next phase. reply glitchcrab 13 hours agorootparentprevHow very dare you suggest that organic growth is the way forward? Everyone here knows that unless you've hit your series c and are soaring for unicorn status then you might as well just not exist at all. reply tnel77 13 hours agorootparentprevYou would think that this would be an acceptable approach, but MBAs have decided that this is not the case so I don’t know. reply H8crilA 13 hours agorootparentExcept all tech giants do just that, yet they fire people. Google has an amount of cash in the bank comparable to all the help that has been sent to Ukraine (though there are also many things that Ukraine gets that are not really measurable in money). Sent, not \"promised\" or whatever else some states do to make themselves look better. reply paulddraper 12 hours agorootparentprev> I'm told that debt is very important for some reason or other I can help you out here :) In addition to revenue, debt is a source of funding for investment into growth. For example, if you can get a return of 8% on an investment, it make sense to borrow capital at a 5% rate. But not at a 10% rate. reply abtinf 13 hours agorootparentprevCongress has decided this is unacceptable for engineering salaries. reply leftcenterright 13 hours agoparentprevbut this is not true of the giants like Microsoft and Google right? They chose to fire thousands and yet have sufficient to invest further into markets all over the world. Burn and churn is just much easier and lax labor laws do not prohibit such tendencies! reply tsunamifury 13 hours agorootparentThey elected to do that because the 1 billion dollar write down yield 300 billion in market value -- firing people literally raised their market cap more than what those people could earn doing a real job earning real revenue. To be clear, this is not people being fired for bullshit jobs. This is people being fired because their 1 billion in revenue is a fraction of the 300 billion in stock price opportunity. Ask me how I know... reply aksss 11 hours agorootparentI think both scenarios can be true. Let us not deny that there are worthless employees and worthless jobs, which tend to accumulate in large organizations organically, and warrant periodic correction. reply frfl 12 hours agorootparentprev...how do you know reply makerdiety 10 hours agoparentprevInstead of minimizing costs by performing massive workforce reductions and while not earning profit, corporate officers could just... earn some profit to keep the momentum going. And to go beyond, beyond initial starting conditions. But that's asking for too much from the average corporate leadership. Efficiency and technology acquisition are too much for these agents of the entrenched hegemony. So, what we have right now is a form of nationalist socialist welfare that looks like our current financial-administrative system. There's some momentum and inertia, but it's all being wasted on keeping unproductive fat cats alive. And that's okay. Because smart people will be leaving this oppressive Egypt under a stubborn Pharaoh. For much better lands and pasture. And while that happens, profit will start to look like a heinous crime to these welfare recipients. Who naturally will not be invited to the awesome parties that's coming in the future. Something like Eloi and Morlocks, if you wanna get biological about it. Eloi and Morlocks, though, are just the starting point. H.G. Wells didn't have anime music videos to inspire him to think about the more accurate possibilities of a matrix of biological degradation and environmental niche adaptation. reply toss1 12 hours agoparentprevYup. And this is why smart companies avoid that game. I know one regional company who sells nationally/internationally which has done that for decades. Decades ago, the owner asked the local bank for a growth loan for some equipment and was turned down. The next week, he found out that one of his employees, who depends 100% on his company for income, went to the same bank for a motorcycle loan and was approved. the business owner was so outraged at the bank's stupid decision-making (if the business isn't good for the money, how will the employee be good for it?) that he decided to never use bank debt for growth again. That was in the 1960s, and the company is doing great, focusing on product and service and not financial games, with very expansive and expanding facilities and workforce. The regional bank was absorbed long ago. Remember: no matter how much banks advertise being your partner and friend, they are not. reply floren 12 hours agorootparentThey're not your friend, because they're in the business of making money. Unless that business was the only employer in the area, the owner was basically just having a tantrum over the idea that different kinds of loans for different amounts of money might have different decision-making processes behind them. When my credit union decided to give me a car loan, they didn't look into the solvency of my employer, they just saw that I have good credit and a steady income and took a safe 5-digit gamble. It may have hurt the owner's feelings to essentially be told \"we're pretty sure we can make our money back on this $1000 motorcycle loan which has a very easy-to-deal-with collateral, but we don't want to give you a $100000 for specialized equipment\" reply toss1 10 hours agorootparentEven if you see it that way, the result of his decision was excellent, and likely superior. I've literally dealt with banks (US state/regional scale) as a technology business with a loan that was absolutely current and on-time every one of scores of months, and we were running profitably (small, but definitely positive). Yet, when the bank started having problems in their real-estate sector, they came and called in OUR loan. We had to seriously scramble to have it not put us out of business, cold. There were several other businesses in the area, also non-real-estate, that were caused to fail in this bank's BS moves and made the local papers. It is not just that bank's business is making money. They have a whole bunch of internal incentives that make it perfectly OK in their eyes to fck over anyone for no reason other than to make their personal numbers this month look good. And they don't hesitate to do it. reply yladiz 13 hours agoparentprevI mean, if companies like that didn’t run such a huge deficit, they might be able to get more favorable loan terms. reply lokar 13 hours agorootparentIt’s not about running a deficit (most are free cash flow positive), it’s about improving returns on capital with leverage. reply sterlind 13 hours agorootparentI'm financially illiterate, but what does this mean? Companies have the revenue to pay their employees, but they choose to terminate them instead, because... why? reply WorkerBee28474 13 hours agorootparentHey, SWE who happens to have an MBA here. Theoretically, if a company has money to pay employees it keeps them if the present value of their project is positive, and terminate them if the present value of their projects is negative. Let's say an employee earns $100K this year. The project they're working on this year will generate 40K of revenue in 1 year, 2 years, and 3 years. Is the project worth doing? Let's assume the discount rate is 5%. A dollar now (which we call 'present value (PV)') is worth $1.05 a year from now ('future value (FV)'). And 1.05^2 in 2 years. And 1.05^n in n years. And a dollar in n years (FV) is worth 1/(1+discount)^n dollars today (PV). It's a mechanism similar to inflation, or paying interest. (IRL the discount rate is the WACC + some factor for riskiness, but that's too much to talk about.) Let's say the discount rate is 5%. Is the project worth doing? The present value (PV) is -100K + 40K/1.05 + 40K/1.05^2 + 40K/1.05^3. So if you approve the project, the future profits and expenses are as if you earned $8,930 today. You approve the project. Let's say interest rates go up, and you now need to use 10% as the discount rate. -100K + 40K/1.10 + 40K/1.10^2 + 40K/1.10^3 = $-526. If you approve the project, you will lose money. So you don't approve the project. Now you don't need the employee anymore, and you get rid of the them, by reassignment, reorganization, or layoff. reply sterlind 13 hours agorootparentBut won't the projected value of the project go up with the discount rate? Why would the project still only make $40K on year 3, rather than now making $40K*1.10^3? reply gen220 13 hours agorootparentIn reality, the projected value of many such projects were either never accurately measured, or allowed to differ from the \"measured\" value for a very long time. The projections would go something like $0 two years ago, $5k last year, but then $100k next year, $5mm year after that. One day? $1B, easy. There'd be precious little evidence to support the exponential growth hypothesis, while having 10 assigned engineers earning as much as their peers in more predictable domains. It's easier to justify such ideas in a big, rich company when the risk free rate of return is near-zero and the profit center of the company is compounding (the growth in Ads justifies the negative ROIC everywhere else). But when your profit center is wavering and showing worrying signs of stalling out (as Google and Meta did in 2021/22), the calculus shifts dramatically. When you manage a company not for \"growth\" but for \"value\" (which is an inevitable part of the corporate cycle), these projects and the employees behind them are decreasingly seen as a potential source of future profits and more as a waste of increasingly valuable cash. reply WorkerBee28474 9 hours agorootparentprevThe other 2 replies seem accurate, but I'll also reply. The future earnings should go up with inflation, but not the discount rate. The government adjusts their interest rate to maintain the rate of inflation around 2% or so. The government interest rate is often around 5%, but can vary from 0 to 20%. The discount rate (or technically the cost of capital) is the gov't interest rate plus 4% or so (the higher the better for investors). reply Jensson 12 hours agorootparentprevRent changed in that scenario, not inflation. So the project wouldn't make more since money isn't worth more, you just get more ROI on external investments making internal investments less lucrative in comparison. reply tsunamifury 13 hours agorootparentprevIt's a lot simpler than that, firing people was earning 30x return in stock price. Almost no one's actual revenue to the company was worth that. The market has created negative human incentives. reply gen220 13 hours agorootparentThe person you were replying to is explaining why firing people \"raises the stock price\". And yes, it really is that complicated. Firing people whose salaries have a dramatically positive ROIC for the company does not raise the stock price, it lowers the stock price. The mistake, in retrospect, is the executives': they should not have over-hired in 2020. I know it's a bizarre silver lining, but to the extent there is one, workers got capital (in the form of SBC and cash) that they otherwise \"shouldn't\" have. If those companies had been run more effectively, most would have never been hired in the first place. It's kind of a crap silver lining, because we psychologically feel loss [of a job] 10x more than gain, but the realistic \"market-optimal\" alternative was not \"I have this cushy, high paying job forever\", it was actually \"I have never had a cushy, high paying job\". reply gen220 13 hours agorootparentprevBasically, it's not about piles of cash. It's about the rate at which those piles are changing, and how you can allocate capital to min-max those rates of change. In an environment where the \"risk-free rate of return\" is secularly-higher, the floor of minimal necessary productivity goes up. Let's say the ROIC of investing $300k \"into\" an employee is $310k (3.33% rate of return). If the risk free rate of return is 1%, you take that employee. If it's 5%, you fire that employee. I think the reality is that the typical ROIC was above the typical salary, but that there was an inflection point that they crossed in the hiring spree of 2019-2021. If you're a capital allocator (CEO), your responsibility is to maximize ROIC for shareholders over the long run. In environments where you're not absolutely confident that your eventual, steady-state employee ROIC will trounce (i.e. 2-5x's) the risk free rate of return, you should generally favor returning capital to shareholders (with dividends or buybacks) instead of putting good money after bad. reply yinser 14 hours agoprevI can't decide if I'd prefer to hear a bunch of nonsense from an executive at the moment of being let go, or no explanation and just an email saying I'm part of a layoff. reply EchoChamberMan 13 hours agoparentThere's a third option - no warning at all, your laptop is remotely disabled, and you have to figure it out yourself. reply TehCorwiz 13 hours agorootparentI didn't get my paycheck last Friday. And they took my red stapler. reply toyg 12 hours agorootparentIf you could just go home for the day, thad would be grrrreat. reply silverquiet 11 hours agoparentprevI'll be forever proud that when I got the call from my senior manager, I started with, \"so you got some bad news for me\"? No matter how bad the situation, it seems like my snark will never fail me. reply trey-jones 13 hours agoparentprevT",
    "originSummary": [
      "Paramount Global lays off 800 employees despite record profits, attributing their success as a factor for the terminations.",
      "The company appreciates the employees' contributions but sarcastically lets them go while ensuring respect and urging the swift return of company laptops.",
      "The tone in the communication regarding the layoffs is dismissive and sarcastic towards the affected employees."
    ],
    "commentSummary": [
      "Companies, like Twitter, are laying off employees to boost productivity and reduce expenses, raising worries about the lasting effects, user experience, and executive responsibility.",
      "Discussions highlight decision-making difficulties, productivity and employee welfare post-layoffs, and the necessity for a profit-sustainability equilibrium.",
      "The conversation involves automation, the impact of banks on business expansion, and the significance of efficient management approaches in corporations."
    ],
    "points": 358,
    "commentCount": 281,
    "retryCount": 0,
    "time": 1708628884
  },
  {
    "id": 39474909,
    "title": "Odysseus Mission Marks US Moon Return After 50 Years",
    "originLink": "https://www.theguardian.com/science/live/2024/feb/22/nasa-odysseus-moon-landing-intuitive-machines",
    "originBody": "18.46 EST 'Odysseus has taken the moon,' says Nasa “What a triumph,” said Bill Nelson, Nasa’s administrator in a message following the landing. “Odysseus has taken the moon. This feat is a giant leap forward for all of humanity.” Today for the first time in more than a half century, the US has returned to the moon. Today for the first time in the history of humanity, a commercial company, an American company launched and led the voyage up there. And today is a day that shows the power and promise of NASA’s Commercial partnerships. Congratulations to everyone involved in this great and daring quest at Intuitive Machines, SpaceX and right here at Nasa. Email link Updated at 18.47 EST",
    "commentLink": "https://news.ycombinator.com/item?id=39474909",
    "commentBody": "Intuitive Machines successfully lands on the Moon (theguardian.com)347 points by LorenDB 10 hours agohidepastfavorite121 comments p1mrx 6 hours agohttps://twitter.com/Int_Machines/status/1760838333851148442 > After troubleshooting communications, flight controllers have confirmed Odysseus is upright and starting to send data. Right now, we are working to downlink the first images from the lunar surface. reply dylan604 6 hours agoparentFeels like a bit of jab at JAXA to me. They could have just said landed nominally, but they had to specify upright. Kind of a dick move reply p1mrx 6 hours agorootparentThey were concerned about not landing upright in the live stream: https://www.youtube.com/watch?v=Dg2ffigGcYM&t=5678s \"looks like we had excellent pitch and yaw control throughout, but I did see a little bit of a roll excursion. could it be that we landed off-angle in roll in the final phase?\" reply adastra22 3 hours agorootparentThat would have lined up with the signal issues too. Glad it wasn’t the case. reply surfpel 6 hours agorootparentprevIt’s not a jab. There were suspicions that it landed sideways because communication was lost then became very faint right after touch down. reply tiagod 10 hours agoprevImpressive feat. Got nervous when they couldn't contact the device, but turns out they did made history. I might be wrong, but from my research it seem like they did this with $118 million paid by NASA. India did it with around $74 million, for reference. reply reissbaker 3 hours agoparentGiven how much the cost difference typically is between India and the U.S., doing it in the U.S. at only a 50% premium is pretty impressive IMO. Wages alone are so much higher here that I would've expected a larger price tag. reply colonCapitalDee 6 hours agoparentprevThat's to be expected, a dollar in India goes much further than a dollar in the US. https://data.oecd.org/conversion/purchasing-power-parities-p... says a dollar in India can buy you about 23 times as much stuff as that same dollar could in the US, although I'd expect that number to drop significantly for highly technical projects like a moon landing. reply mmoskal 5 hours agorootparentIt's 23 rupees to a dollar at PPP. The exchange rate is 82, so a dollar goes 3.5 times further in India not 23 :) reply cnity 9 minutes agorootparentThey're not talking about the exchange rate. They're talking about purchasing power. Edit: Ignore me, I can't read. reply Geee 2 hours agorootparentprevPrice difference applies only to wages and other non-portable costs. The prices for products and commodities which can be easily moved are roughly equal on global markets due to arbitrage. You can't get the raw materials or computer chips any cheaper in India. reply jl6 2 hours agorootparent> and other non-portable costs OK, but there are a lot of these. reply FredPret 10 hours agoparentprev$74m surely? reply tiagod 9 hours agorootparentYes, of course. Corrected reply croes 9 hours agorootparentprevhttps://www.businessinsider.com/how-india-moon-landing-cost-... reply justinclift 9 hours agorootparentThe post you're replying to is just pointing out that the quoted amount (they THEY were replying to) is missing the \"million\" abbreviation on the end. So it looks like the original claim was India did it for US$74.00. ;) reply TaylorAlexander 9 hours agorootparentThey are extremely resourceful people. reply dylan604 8 hours agorootparentJust proves why off shoring is so cost effective /s reply throwup238 7 hours agorootparentIt really depends on the rocket’s cultural fit. reply SilasX 6 hours agorootparentprevReminds me of that South Park episode where they go to Mexican counterpart to NASA and get quoted $200 to haul something to the moon. https://www.youtube.com/watch?v=1K_1jqgCjxA&t=40s reply iisan7 7 hours agorootparentprevI heard they did it for $3.50 reply romwell 7 hours agorootparentNo, you're thinking of a Scottish entity; we're talking about India. reply spacebacon 6 hours agorootparentPretty sure it was about tree fiddy. reply LorenDB 10 hours agoprevHuge congratulations to IM. Landing on the Moon is extremely difficult, despite what playing KSP will condition you to think. For them to successfully land on the Moon is a huge milestone. Now to get people back there... maybe someday. reply coconutpalms 9 hours agoparentIt's not even that easy in KSP! reply bigiain 9 hours agorootparentIt's easy enough until your fuel budget gets tight: http://moonlander.seb.ly reply JKCalhoun 8 hours agorootparentBeen loving Lunar Lander ever since I played it in the 1980's. That is a very faithful version. I always loved how ti zoomed in when you got close to the terrain. (And that large throttle you used to control the thrust.) As a kid it was like impossible for me to play. Now it is just ... challenging. My voxel tribute: https://mooncraft2000.com reply skinner927 5 hours agorootparentIt’s fun but the performance plummets after only a few minutes even on a desktop computer. reply LorenDB 9 hours agorootparentprevOnce you get the hang of it, it becomes pretty simple. Just keep pointed retrograde and you'll probably be fine (assuming you start your final landing burn soon enough). reply TeMPOraL 1 hour agorootparentIt's easy when you're playing with a mod that calculates time to suicide burn for you. Otherwise, you'd better overbuild the lander, as you'll need that extra Δv for \"pumping the brakes\", so to speak. reply fnordpiglet 9 hours agorootparentprevI’ll wager once we get a handle on it it’ll be routine and simple. But it’s been quite some time since we really tried and the efforts so far again have been pretty minimal and laced with pork. reply gibolt 8 hours agorootparentOnce Starship is able to reliably get to orbit and refuel, the payloads we'll be able to get the moon will increase significantly, and hopefully with high frequency. Developing custom probes where most of the development+weight is to navigate space and land, May quickly become obsolete. reply TaylorAlexander 9 hours agorootparentprevLike that new Starbucks drink! reply XorNot 9 hours agorootparentprevA KSP mod which requires you to put cameras and altimeters with failure modes on your craft would be...interesting, but probably a lot less fun. reply r2_pilot 5 hours agorootparentIt's called \"OhScrap!\" for the original one and is how I would describe KSP 2 even after the big science update. I would enjoy it a lot more if they could work on adding some precision to the nav points, and I had forgotten how much I enjoyed auto execute mode from MechJeb(not to mention the porkchop plots), but KSP 2 is at least some fun now. reply internetter 10 hours agoparentprevI still just cannot wrap my mind around how we could get people there, and then all this time later have lost that ability. What happened? reply cameldrv 9 hours agorootparentPart of it is cost: In the mid sixties, NASA was about five percent of the federal budget. Another part is safety: Apollo was extremely dangerous. There were a lot of single points of failure, three astronauts died on the ground, three more were very very narrowly saved over the course of only about ten missions. One of the reasons the program was cut short was the political worry after Apollo 13 of having astronauts die on the moon or something similar. This level of danger was considered acceptable given the mission back then, but I don't think it would be today. Who's right is a worthwhile question. Another part was that Apollo had superb, hard working, young, smart, energetic people working on it. The nature and ambition of the project just attracted top people. The context of the Vietnam war also meant that there were a lot of engineers that would otherwise be working on weapons but maybe would rather not, and Apollo was a very nice peaceful project to apply all of that technology to. I think you see a lot of the success of Tesla and SpaceX in the same observations. Both of those companies were super ambitious, and had a goal a lot of people wanted to get behind. Their competition was unattractive to work for, and seemed unlikely to achieve the goal of getting to mars/electric cars. The result was that they were able to hire some of the very smartest, hardest working people in the country. If you get a bunch of people like that together under a common banner, amazing things happen. reply JumpCrisscross 9 hours agorootparent> Part of it is cost The other side to this is we don't build rockets today like we did in the sixties. Yes, some technical know-how here and there was technically \"lost\"--I've been in North Hollywood scrounging around for old NASA parts so that measurements could be taken and worked backwards from. But even if we had that know-how, we wouldn't do it the same again. From novel materials to simulations to communications and onboard computing, modern spacecraft deserved to be redesigned. (For the counterfactuals, see ULA and Arianespace.) So until someone (SpaceX) started eating that node-transition cost, we entered a period of apparent stasis. But that vacuum was energetic, and it wasn't about rediscovering lost knowledge, but inventing new kit. reply cameldrv 4 hours agorootparentYep. One thing about manufacturing is that there are economies of scale everywhere. If we had to build a Saturn V from plans, we'd run into tons of problems. For one, the plans don't actually have everything you need to know. Tons of information is in people's heads. For a lot of the manual work, it's not even necessarily something they can explain. Building something with obsolete methods of fabrication is very difficult, but also the Saturn V was so optimized that switching materials or fabrication techniques would be very difficult. On the F-1 for example, they just barely got combustion to be stable. If you started substituting things, it's very possible it would become unstable again and they'd have to go do all the debugging they did out at the Edwards test stands again. For almost everything, doing something a lot of times over a short period of time is way cheaper than doing something a small number of times, especially if there's a long time between them, and the employees have quit/retired and the tooling/machinery/suppliers have been disposed of, gone out of business, or moved onto other things. reply reachableceo 8 hours agorootparentprevNorth Hollywood? Looking through prop storage from studios or something? Some interesting junk and scrap shops in the area. Spent a few weekend afternoons puttering around looking at random stuff. reply tesseract 4 hours agorootparentThere were/are a couple of aerospace surplus shops in the area. Apex and Norton are the ones I'm familiar with. The supply comes from the SoCal aerospace industrial base and a lot of the demand is indeed for movie props. It's a fortuitous pairing - the prop rental business seems to be keeping them going to some extent whereas a lot of industrial surplus businesses in other parts of the country seem to be struggling these days. reply dylan604 8 hours agorootparentprev>Yes, some technical know-how here and there was technically \"lost\"--I've been in North Hollywood scrounging around for old NASA parts so that measurements could be taken and worked backwards from. Oh, I don't know. I think we've actually made improvements in leaps and bounds over the \"lost\" tech. Our soundstage, green screen, and post tech has definitely improved /s I know it's not what you meant, but the NoHo reference made me laugh at the thought reply JumpCrisscross 8 hours agorootparent> the NoHo reference made me laugh at the thought I can’t remember what it’s called, but it’s a scrapyard with all kinds of space stuff (large and small) that I’m halfway sure isn’t ITAR controlled solely because the guy got it before 1976. reply adastra22 2 hours agorootparentprevThere’s a reason droid parts in OG Star Wars looks like they were pulled off Apollo space suits or something. Practical special effects people raided all the surplus parts after the end of the program. reply foobarian 7 hours agorootparentprev> five percent of the federal budget. It's impressive to compare five percent and some small fraction, but the total budget also expanded a lot since then. While in constant dollars the current NASA budget is indeed smaller than it was then, it is only about half. At the end of the day, the kinds of things they spend that budget on make sense to me for a research organization, and it makes sense to me that retracing the moonshot steps happen in the private sector. reply adastra22 2 hours agorootparentIf you were talking about the science mission directorate you’d be right on. They’re doing good work with the money they’re allocated. The exploration directorate, on the other hand, with their billion dollar rocket to nowhere… reply TeMPOraL 1 hour agorootparentYou mean the billion dollar \"cost of doing science in a democratic country\" / \"jobs program\" Senate Launch System? reply tomrod 6 hours agorootparentprevRe: who is right So long as; the participants are willing, the death rate is in the single percents per mission,the mission value add on terms of new capability is high, the risk is probably worth it. At least in my opinion. reply cameldrv 2 hours agorootparentThis makes a lot of sense in one way, but the weird thing is that it's not just the astronaut's own lives that we have to consider. Personally, I've had some risky hobbies. I accepted the risk, and if I'd died doing them, I might have gotten in the local newspaper, but nothing beyond that. The Challenger explosion was a moment that defined a generation. Some of that was that Christa McAuliffe was supposed to talk to millions of kids once she got into orbit, but some was just what people projected onto the astronauts generally. When the three astronauts died in the Apollo 1 fire, no one shed a tear for the 30,000 man-years of effort that went into making the rocket that was destroyed. The effect was about the men themselves, who were considered national heroes, and also what it might have said about the United States that those men died. Maybe we should treat this differently. Maybe in prior ages of exploration, we did. Back then we didn't have live TV or the internet though that could capture our emotion in the same way, and also in prior ages, not all expeditions were as closely connected with the nation itself. reply adastra22 2 hours agorootparentprevI’m very much a believer in informed consent. The astronauts will tell you, to a person, that they’d take that risk. Who am we to say otherwise? reply stevenjgarner 8 hours agorootparentprevThese are all factors but they pale in comparison with the fact that the US was at war, and was trying to catch up with the phenomenal lead of the USSR. Silicon Valley loves talking about \"moonshots\", but that parlance gets so much wrong. What is important is not the moonshot moment. It is the sputnik moment that is disruptive. reply everfrustrated 5 hours agorootparentInteresting thought. I guess in a way AI is having its \"sputnik\" moment now. reply adastra22 2 hours agorootparentNot in the strategic competition sense though. reply adastra22 3 hours agorootparentprevAre people really that enthused about electric cars? smh reply nicklecompte 10 hours agorootparentprev\"Lost the ability\" is an odd way of putting it. \"Lost the budget\" is better. Ideally we wouldn't have to pick and choose, but since I wasn't US Dictator during the 90s, the Hubble Space Telescope + a lot of probes throughout the solar system was probably more impactful for overall scientific discovery than the counterfactual where NASA invested in continued moon landings. reply ein0p 9 hours agorootparentWe did lose the capability. Read up on the engineering behind Saturn-V. There are books that go into extensive detail. A lot of the things described there required completely novel approaches that would be very difficult to replicate in deindustrialized United States of 2024. Not to mention that cost of all this would be absolutely astronomical, far more so than it was back then. reply JumpCrisscross 9 hours agorootparent> lot of the things described there required completely novel approaches that would be very difficult to replicate in deindustrialized United States of 2024 Give me one example. > cost of all this would be absolutely astronomical, far more so than it was back then Inflation adjusted, the Saturn V was a $45bn project [1]. That's a mulitiple of the total capital SpaceX has raised (or been granted). [1] https://apollo11space.com/the-cost-of-launching-a-saturn-v/ reply ein0p 8 hours agorootparentEg https://en.m.wikipedia.org/wiki/Explosive_forming of upper and lower parts of the fuel and oxidizer tanks with extremely tight restrictions on the weight of the part, and its structural integrity. This is but one example - there’s a great book on this, “Saturn-V, Owner’s workshop manual”. Your mind will be blown literally on every page. Buy it and read it just to see how incredible the engineering was, and how unrealistic it is to reproduce today, when NASA can’t even get humans to LEO on its own. reply JumpCrisscross 8 hours agorootparentWe’re building lighter and stronger pressure vessels—more reliably, at larger scale and, inflation adjusted, probably cheaper—today than those tanks. We’re entering a halcyon of American space flight, with multiple teams doing what others cannot and Apollo-era engineers could only dream of. It’s mind-boggling to see the disconnect between a lunar landing alongside a caricature of American industrial decline. reply ein0p 8 hours agorootparentUntil we successfully land people on the Moon again we by definition haven’t surpassed the Apollo program. And don’t latch onto just this one example. Read the book. There are a thousand more things like that there. reply JumpCrisscross 8 hours agorootparent> Until we successfully land people on the Moon again we by definition haven’t surpassed the Apollo program That’s an application, not a capability. It would be like arguing iPhones never surpassed ENIAC because mine hasn’t simulated a thermonuclear detonation. > Read the book. There are a thousand more things like that there. I asked you to name one. Explosive forming is something (a) we can still do but (b) that nobody does because we have better materials and processes. The engineering of the Apollo era was remarkable. But the problem with promulgating this myth of lost capability is it stunts our ability to get things done today. reply capitainenemo 8 hours agorootparentprevThe reason NASA's current plan is taking a bit longer is it is considerably more ambitious than the Apollo program. If they were only trying to replicate Apollo it would be far easier to do. Smarter Every Day had a rather interesting video on his concerns with the program. https://www.youtube.com/watch?v=OoJsPvmFixU and they were more about a program design that requires over 15 fueling launches for every trip to the moon, and concerns about management and transparency. I don't think anyone is claiming this issues are lost technology though. reply ein0p 7 hours agorootparentThe proof is in the pudding. The pudding is that NASA of 2024 has no human space flight capability at all. reply marssaxman 4 hours agorootparentNASA of 2024 does not need human space flight capability, because that has been successfully commercialized. reply jiggawatts 8 hours agorootparentprevStarship is also the biggest rocket to have ever flown, with twice the thrust of the Saturn V! That’s using a fraction of their budget. reply gibolt 8 hours agorootparentAs a bonus, Starship should be fully reusable, meaning long-term costs drop drastically to continue the program. Each flight could cost single-digit millions, mainly in fuel and employees. reply metal_am 4 hours agorootparentprevI’ll bite. The capability in the US to do large castings and forgings is basically gone. Good luck getting anything with a lead time of less than a couple years. In some cases, it simply can’t be done. That’s why we’re seeing so much investment into large scale additive manufacturing. It’s interesting how this has become such a problem again. Post WW2, the US dismantled massive, basically building size hydraulic presses in former Nazi Germany and shipped them back to the US because that was a capability we didn’t have. reply crazygringo 8 hours agorootparentprev> very difficult to replicate in deindustrialized United States of 2024. While we might have deindustrialized a lot of commodity electronics and small consumer things, I'm pretty sure we're still totally fine when it comes to aerospace. What with our Air Force and NASA and SpaceX and all that. More than fine, even. reply egl2021 7 hours agorootparentRead up on restarting Stinger production. Retired engineers had to pitch in to train the current generation. So I wouldn't say \"totally fine\". reply Solvency 5 hours agorootparentHow is needing to train young people the same as deindustrialization exactly? reply fragmede 50 minutes agorootparentIt's the taking engineers out of retirement that's deindustrialization. A properly industrialized nation would have an active program with educators in the prime of their career to do the educating, and not need to pull someone out of retirement because there wasn't a need for them to train their replacements until after they'd retired. reply gibolt 8 hours agorootparentprevAgreed. For super specialized advanced materials or processes, some Western-aligned country will usually be able to provide it, even if no one in the U.S. can. reply dotnet00 8 hours agorootparentprevThey aren't difficult to replicate because of deindustrialization, they're difficult and pointless to replicate because they were based around handcrafting, while modern rocket manufacturing is all computer controlled. The designs are not optimal for modern production approaches. As an example, a modernized F-1 engine would have far simplified plumbing, much fewer parts, the injector, which contained hundreds of handcrafted and hand-tuned parts to minimize combustion instability would be replaced with one machine crafted part where the instabilities would be properly cancelled out through computer simulation. But, what's the point? The F-1s were designed because computer control systems of the time were not good enough to control a large number of smaller, more efficient engines. Nowadays such control systems exist, with Falcon Heavy flying 27 engines, and Starship doing 33. The fuel they used is also not really being seen as the future, with medium and heavy lifters shifting towards Methane for its better efficiency, cleaner combustion aiding reuse and being liquid at similar temperatures to oxygen, simplifying the cryo-tank design. They simply aren't relevant anymore outside of out of touch Congress critters looking for ways to pour more billions into the dead end of SLS. The other big point is that Saturn V wouldn't even meet the expectations current lunar landers are expected to meet. Both lander proposals are huge, potentially with the habitable volume of the entire ISS. They could, on their own, serve as small long term lunar bases, if it weren't for Orion being unable to spend an extended time in space. reply GeorgeTirebiter 8 hours agorootparentprevI, too, am eager to learn of one example. reply adastra22 2 hours agorootparentprevNASA has spent more on real dollars on Artemis and its dependencies and precursors than the Apollo program costed, and yet it is no where near comparable in return. reply LorenDB 10 hours agorootparentprevIt's largely a matter of incentive. In the sixties, the US had an incentive to prove that it had more advanced technology than the USSR. Once we proved we could land on the moon, the novelty quickly wore off for the public, which influenced Congress's decision to cut funding for the Apollo program. We also haven't returned to the moon yet because of cost, I would imagine. reply rockemsockem 9 hours agorootparentThe public was actually never in favor of the Apollo program except for when we actually landed on the moon. The government was just willing to ignore public sentiment for some reason. And I'm very glad they did. reply JumpCrisscross 9 hours agorootparent> public was actually never in favor of the Apollo program except for when we actually landed on the moon Wow, I wasn't aware that the \"only point at which the opinion surveys demonstrate that more than 50 percent of the public believed Apollo was worth its expense came in 1969 at the time of the Apollo 11 lunar landing\" [1]. [1] https://launiusr.wordpress.com/2010/08/16/exploding-the-myth... reply GeorgeTirebiter 8 hours agorootparentGil Scott-Heron had a very popular song in 1970 https://www.youtube.com/watch?v=goh2x_G0ct4&list=RDgoh2x_G0c... reply Vecr 7 hours agorootparentSomeone should see about making a few versions for various crew compositions/landing times for the Artemis program. So you'd have one ready to go pretty much whatever happens. reply ThisIsMyAltAcct 8 hours agorootparentprevThis is one of the reasons I’m glad the US is a representative democracy and not a direct democracy. reply tintor 10 hours agorootparentprevBut there is plenty of Helium 3 to mine on the Moon, to justify the cost. reply ceejayoz 9 hours agorootparentAssuming we manage to build viable reactors to use it. reply pfdietz 9 hours agorootparentHelion is the closest for that, but it would be easier for them to make their own 3He by DD fusion. It will be very hard to make 3He extraction on the moon energy positive, since it occurs at very low concentration in the regolith. reply dotnet00 9 hours agorootparentprevThere's also specifically the difference that humans being onboard added much more tolerance when it comes to things like landing. Neil Armstrong was able to fly the lander to get to a safer landing area and was able to sanity check the sensor readings in realtime. In comparison, these landers have to do everything autonomously, the computer has to figure out where it's safe to touch down, it has to be able to figure out if a sensor is malfunctioning and if so, which is the one that's wrong. Similarly, previous uncrewed landers were not expected to be that smart, they just aimed for a patch of land believed to be flat and hoped for the best. reply aredox 9 hours agorootparentprevThe best minds of our generation are working on byzantine financial derivatives, making social media more addictive, and turning fake pictures/videos/bots as realistic as possible. reply JumpCrisscross 8 hours agorootparent> best minds of our generation are working on byzantine financial derivatives, making social media more addictive, and turning fake pictures/videos/bots as realistic as possible Ten years ago, maybe. Today, I'm not sure. It's been a long time since I had that \"wow, everyone here is ten times smarter than me\" feeling at one of the big tech firms. And to the extent the system works, it's correcting. We're seeing layoffs on one side. And moon landings on another. reply adastra22 2 hours agorootparentThe vanguard of that just isn’t in FAANG anymore. That’s why you’re not seeing it. reply passwordoops 9 hours agorootparentprevThis comment deserves to be on the front page reply GeorgeTirebiter 8 hours agorootparentIt's only because of a version of Willie Sutton's quote: \"That's where the money is.\" Capital allocation. reply analognoise 9 hours agorootparentprevDon’t forget the parasocial relationships they form with fans who are increasingly lonely, the new frontier in emotionally manipulative AI chatbots, and surveillance capitalism. reply foobarian 10 hours agorootparentprevAnd apparently live video.. reply bombcar 9 hours agorootparentprevI’m not entirely sure that we’ve actually lost it; is it really far fetched to think SpaceX could moon land- if not now but perhaps in a bit? reply justinclift 9 hours agorootparentIn SpaceX's case, they'd probably be the \"but we've regained it\" side of things. reply seydor 4 hours agoparentprevMoon will be the first robot colony. Surely they will be friendly... reply dylan604 8 hours agoparentprevDid you try the simulator from SpaceX for docking the Dragon capsule to the ISS? reply legohead 9 hours agoparentprevSupposedly they want a crewed base by 2030... reply yieldcrv 8 hours agoparentprevIs there a good resource to follow all space efforts? I have no idea whats coming down the pipe and I used to reply JumpCrisscross 8 hours agorootparentSpaceFlightNow is my old go-to for launch schedules [1]. I also like John Holst's Ill-Defined Space [2]. [1] https://spaceflightnow.com/launch-schedule/ [2] https://substack.com/@illdefinedspace reply georgehm 1 hour agoprevI am super curious to know what kind of software magic they did to bring the NDL system as the primary navigation system after their own system failed to function prior to landing .. From what I understood listening in to the broadcast, NDL was only a demonstration project and folks somehow managed to bring it up on demand and use 2 of the sensors on it (?) iirc .. Like how !? Was NDL set up in the same orientation as their original system ? If not, they updated their guidance system on the fly too !?? Simply amazing reply UniverseHacker 7 hours agoprevDoes anyone know anything about the possible video of the landing from EagleCam? I would love to see some video of the moon that is not 1960s quality. reply ChrisArchitect 8 hours agoprevLots of discussion over here: https://news.ycombinator.com/item?id=39465230 reply yinser 7 hours agoprevEagle cam or it didn’t happen On a serious note I’m interested to hear what the measured boil off vs calculated was for their propellant load. Depending which direction it is could mean a huge difference to the Artemis plans. reply sohzm 4 hours agoprevDamn, feeling very happy we're going back to space again. Congratulations IM <3 reply LorenDB 8 hours agoprevA more in-depth article from the Guardian: https://www.theguardian.com/science/2024/feb/22/us-moon-land... reply rmrf100 4 hours agoprevCool, congratulations to IM. reply seatac76 10 hours agoprevGood Stuff reply 1propionyl 9 hours agoprevnext [6 more] [flagged] xanderlewis 9 hours agoparent…what? reply ackbar03 8 hours agorootparentYou heard the man. We're trying to convince people we've faked another landing by landing on the moon reply _factor 5 hours agorootparentI think you misunderstand. We actually had to fake this one to prove the faked ones weren’t fake. reply 1propionyl 5 hours agorootparentNo, poster above you had it right. You've got it backwards. We just can't afford the extensive investment into cinematic technology (cameras, lenses, props, etc) we used to... So now we're reduced to just actually doing it. A shame really. reply dylan604 8 hours agorootparentprevNo, I think you meant Wat! reply yawpitch 3 hours agoprev [–] One small step for private industry, one giant leap for unskippable advertisements on the night sky. reply adastra22 3 hours agoparentThose are outlawed by international treaties. reply yawpitch 2 hours agorootparentSo was the invasion of Ukraine, the usage of chemical weapons in Syria, and the enhanced interrogation techniques at Abu Ghraib. Outlawed by international treaty doesn’t mean all that much. reply adastra22 2 hours agorootparentNone of your examples are commercial. reply RCitronsBroker 1 hour agorootparentUnfortunately, they are. It’s hard to overestimate how scary govt contractors, especially PMIs can get. reply yawpitch 24 minutes agorootparentprevYes they, absolutely, were… but even if that was a coherent counterpoint — your argument seems to now be that the international mechanisms for policing crimes against commercial interests are inherently stronger, more robust, and effective than the international mechanisms for policing crimes against humanity — international treaties also ban copyright infringement, overfishing, and pollution. Clearly, none of those are happening. reply mandmandam 2 hours agorootparentprevTell that to the private companies selling the arms and scooping the resources. Think Halliburton In Space. reply dymk 3 hours agoparentprev [–] Please keep the snide jabs on Reddit reply yawpitch 3 hours agorootparent [–] There’s nothing snide about genuine, deep, heartfelt sadness at the state, and stupidity, of this species. But hey, if you only want to be presented with views that jive with your particular informational and political silo, feel free to ignore me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA's administrator Bill Nelson celebrates the Odysseus mission's triumphant moon landing, signifying the US's return to the moon after 50 years, led by a commercial American company.",
      "The successful mission highlights NASA's strong commercial partnerships' potential, with contributions from Intuitive Machines, SpaceX, and NASA.",
      "The accomplishment signifies a milestone in space exploration and collaboration among different entities in the space industry."
    ],
    "commentSummary": [
      "Intuitive Machines achieves a successful Moon landing with the spacecraft Odysseus, confirming its upright position and data transmission.",
      "The $118 million mission cost is compared to India's $74 million moon landing mission, sparking discussions on space exploration achievements, challenges, modern rocket tech, commercialization, and lunar resource mining potential.",
      "Topics extend to budget constraints on space exploration, international space regulations, and the increasing role of private companies in the space industry."
    ],
    "points": 346,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1708645176
  },
  {
    "id": 39467132,
    "title": "Introducing Pages CMS: Streamlined GitHub Integration",
    "originLink": "https://pagescms.org",
    "originBody": "In a nutshell:1. You log in with your GitHub account.2. You select the GitHub repo where your site&#x2F;app is at (whether it&#x27;s Next.js, 11ty, Hugo, Nuxt... as long as you&#x27;re using flat files for content).3. You add a single config file to your repo to define the content types and other settings (e.g. media folder).4. Congrats: you now have a user friendly CMS to manage content + media BUT all changes are still tracked like regular commits (under your account) on GitHub.I started using Jekyll around 2009 and over the course of the past 10+ years, I&#x27;ve helped build major sites and tiny blogs with Hugo, Gatsby, Next.js and more recently 11ty.I still love it.BUT once you&#x27;re done building, managing content and media can be a bit of a pain. You have a few options:- Edit files directly (on GitHub or your local). Good luck getting your colleagues on the marketing team to do that.- Hook up a headless CMS like Contentful, Sanity, or Strapi. That works, but it&#x27;s one more dependency and (IMHO) overkill in most cases.- OR you could use something like [Decap CMS](https:&#x2F;&#x2F;decapcms.org&#x2F;). Really cool project, but I&#x27;ve never been a fan of the UI&#x2F;UX, and it&#x27;s been a bit of a pain to setup (maybe that&#x27;s just me).I wanted something as simple as possible, preferably with nothing to install or deploy.Back in 2018, I had built a prototype (Jekyll+) [1] with the idea of getting a CMS set up by just adding a single configuration file to your GitHub repository.Pages CMS [2] is a continuation of that idea. It&#x27;s 100% free and Open Source: https:&#x2F;&#x2F;github.com&#x2F;pages-cms&#x2F;pages-cms.If you don&#x27;t want to use the online version because you&#x27;re not comfortable signing up with your GitHub account, consider the following options:- Use a fine-grained personal access token [3], there&#x27;s an option on the login screen. There is still a bug if you try to access a repo that isn&#x27;t part of your token scope, but I&#x27;ll get it fixed in the next couple of days.- Deploy it yourself (for free) on Cloudflare Pages. Literally 5 minutes of work max. I made a video walking you through the process [4].- Check out the intro video on the front page [2] (a bit crap, but I&#x27;ll get a better one up in the next few days).I use it actively with a few other teams, I hope it will be of use to some of you.I&#x27;m already working on adding a few nicer features, like collaborative editing and email invites (to let non-developers login without a GitHub account).PS: I&#x27;ve spent the past 8+ years building a business and only recently got back into coding. I&#x27;d love pointers as to what I could do better (and how I can manage my Powerpoint PTSD).[1]: https:&#x2F;&#x2F;github.com&#x2F;hunvreus&#x2F;jekyllplus&#x2F;[2]: https:&#x2F;&#x2F;pagescms.org[3]: https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;authentication&#x2F;keeping-your-accou...[4]: https:&#x2F;&#x2F;pagescms.org&#x2F;docs&#x2F;development&#x2F;",
    "commentLink": "https://news.ycombinator.com/item?id=39467132",
    "commentBody": "Pages CMS – A CMS for GitHub (pagescms.org)307 points by hunvreus 20 hours agohidepastfavorite103 comments In a nutshell: 1. You log in with your GitHub account. 2. You select the GitHub repo where your site/app is at (whether it's Next.js, 11ty, Hugo, Nuxt... as long as you're using flat files for content). 3. You add a single config file to your repo to define the content types and other settings (e.g. media folder). 4. Congrats: you now have a user friendly CMS to manage content + media BUT all changes are still tracked like regular commits (under your account) on GitHub. I started using Jekyll around 2009 and over the course of the past 10+ years, I've helped build major sites and tiny blogs with Hugo, Gatsby, Next.js and more recently 11ty. I still love it. BUT once you're done building, managing content and media can be a bit of a pain. You have a few options: - Edit files directly (on GitHub or your local). Good luck getting your colleagues on the marketing team to do that. - Hook up a headless CMS like Contentful, Sanity, or Strapi. That works, but it's one more dependency and (IMHO) overkill in most cases. - OR you could use something like [Decap CMS](https://decapcms.org/). Really cool project, but I've never been a fan of the UI/UX, and it's been a bit of a pain to setup (maybe that's just me). I wanted something as simple as possible, preferably with nothing to install or deploy. Back in 2018, I had built a prototype (Jekyll+) [1] with the idea of getting a CMS set up by just adding a single configuration file to your GitHub repository. Pages CMS [2] is a continuation of that idea. It's 100% free and Open Source: https://github.com/pages-cms/pages-cms. If you don't want to use the online version because you're not comfortable signing up with your GitHub account, consider the following options: - Use a fine-grained personal access token [3], there's an option on the login screen. There is still a bug if you try to access a repo that isn't part of your token scope, but I'll get it fixed in the next couple of days. - Deploy it yourself (for free) on Cloudflare Pages. Literally 5 minutes of work max. I made a video walking you through the process [4]. - Check out the intro video on the front page [2] (a bit crap, but I'll get a better one up in the next few days). I use it actively with a few other teams, I hope it will be of use to some of you. I'm already working on adding a few nicer features, like collaborative editing and email invites (to let non-developers login without a GitHub account). PS: I've spent the past 8+ years building a business and only recently got back into coding. I'd love pointers as to what I could do better (and how I can manage my Powerpoint PTSD). [1]: https://github.com/hunvreus/jekyllplus/ [2]: https://pagescms.org [3]: https://docs.github.com/en/authentication/keeping-your-accou... [4]: https://pagescms.org/docs/development/ edtechdev 16 hours agoGood to see further development in this space. Would be interesting to see how it compares to Decap CMS https://decapcms.org/ and Static CMS https://www.staticcms.org/ Me personally I'd like to see something that supports easily creating and using different types of objects besides pages (such as: events, books, recipes, etc.), like content types and fields and views in wordpress or drupal, ideally aligned with schema.org like https://www.drupal.org/project/schemadotorg I think Hugo might support content types in YAML or something. reply hunvreus 16 hours agoparentYou can configure whatever content type you want with nested fields, lists, etc. [1] Disclaimer: I used to work a lot with Drupal 10+ years ago. I more or less wanted the same kinds of features in Pages CMS. [1]: https://pagescms.org/docs/configuration/ reply __jonas 18 hours agoprevNice to see a new entry in the git based CMS realm, I really appreciate that you seem to understand the importance of a good user experience for editing content! Am I gathering correctly that this does not actually require you to host a backend somewhere? The GitHub OAuth will work even if the CMS is just statically hosted on Cloudflare Pages / Netlify? This was something I always found a little strange about Netlify CMS / Decap, the fact that it required you to either use Netlify or self host their git-gateway. Edit: Nevermind, I saw it does the auth through serverless functions https://github.com/pages-cms/pages-cms/tree/main/functions/a... I guess it's impossible to do it frontend-only, looks like a fair compromise to me reply hunvreus 18 hours agoparentThat's the lightest I could figure out to get OAuth rolling, but it does almost nothing and doesn't store your token. You don't have to host anything if you use the online version, but you can self host it fairly easily for free on Cloudflare Pages: https://pagescms.org/docs/development/#deploy-on-cloudflare reply Arelius 16 hours agorootparentYeah, github OAuth doesnt support a web client only flow, so you at least need a backend to forward along the response to the client. It's pretty much just that cross site requests are disabled. reply araes 15 hours agoprevTried it out [1], and with a bit of work got it to function. Media, posts, all seem to upload and be viewable. With the lead-in, kind of thought it was going to be \"click-a-button, you're done.\" However, had to wander around a bit figuring out what format zones in the blog example were, and where they needed to be. Also kind of thought it was going to restyle my GitHub page or something, which did not seem to be the case (probably just false expectations) [1] Uploaded Media using Pages interface: https://github.com/conceptualGabrielPutnam/JAMA4JS/blob/main... Might be kind of nice if it allowed file upload/delete on folders you have not specifically called out for a function. On my desktop at least, the user icon is also in the lower left, and then opens the choice window off the screen to the right. reply hunvreus 15 hours agoparentIf you can send me screenshots at hunvreus@gmail.com or file an issue on GitHub (https://github.com/pages-cms/pages-cms/issues), I'd love to fix it. > Tried it out [1], and with a bit of work got it to function. Media, posts, all seem to upload and be viewable. With the lead-in, kind of thought it was going to be \"click-a-button, you're done.\" However, had to wander around a bit figuring out what format zones in the blog example were, and where they needed to be. Also kind of thought it was going to restyle my GitHub page or something, which did not seem to be the case (probably just false expectations) Agreed. For the 1.0.0 release, I want to have a configuration wizard that does most of it for you: select where your files/collections are and it infers the configuration from existing entries. Hopefully I get it working in the next couple of weeks. reply araes 15 hours agorootparentOpened new issue at https://github.com/pages-cms/pages-cms/issues/3 with documentation. On the second comment, kind of figure it was a WIP currently, hence suggestions. Thanks for the work, as its a fairly light weight way to have a quick little CMS. reply hunvreus 3 hours agorootparentI rolled out a hotfix release, let me know if that fixed it. reply colinramsay 18 hours agoprevEDIT: I didn't have a `media` entry in my pages.yml, which meant the rich text editor wasn't loading (JS error was being thrown). When I added that entry, it started working! Brilliant work! I'm using Jekyll, with yaml frontmatter, and it's not clear how to specify the body. The documentation says that a rich text field could be set up with a \"name\" option (and gives \"body\") as an example but my Markdown files don't specify a name for the body.. it's just... there: https://github.com/colinramsay/colinramsay.github.io/blob/ma... I'm probably missing something but happy to open an issue if not. reply hunvreus 17 hours agoparentThanks a lot for finding that bug. I'm adding a hotfix and will release in the next hour. reply hunvreus 17 hours agoparentprevI rolled out a hotfix, this should be fixed. Thanks for the feedback. reply colinramsay 16 hours agorootparentAmazing, thank you. reply p44v9n 17 hours agoparentprevthis solved my issue, thanks! reply archb 15 hours agoprevThis is very cool! I recently started managing my Astro site content with Notion as a CMS, thanks to `notion-to-md` [1] and `@notionhq/client` [2] but media management is a hassle. I had been planning to re-host Notion media files to Cloudflare R2 and rewrite content, but it might just be simpler to use Pages CMS due to built-in R2 support. But also, I like using Notion apps on the go. Hmm. [1] https://github.com/souvikinator/notion-to-md [2] https://github.com/makenotion/notion-sdk-js reply dewey 6 hours agoprevI randomly found this a few days ago was planning on giving it a shot. Thanks for sharing! I was always looking for a good web interface for Hugo. Static site generators are nice...but sometimes you really just want to drag and drop and image and write a short post without all the extra hassle of moving the file to the post directory, linking it with the file name in the post, then preview is broken because of some path issues etc. reply marc_io 13 hours agoprevThis is great as it is, but I would love to see support for posts in plain HTML. There's a huge potential for static sites generated by Webflow and other platforms like it. Hosting costs are the biggest issue with these platforms. It would help so many designers and marketing teams that don't have access to a developer or simply don't want (or don't know how) to set up themselves a Jekyll, Next.js, Astro, Hugo, or Nuxt website. reply hunvreus 10 hours agoparentYou can save `.html` and `.mdx` single files, I just don't have great support for collections yet (I'll need to sort out a few things as right now it expects a frontmatter to display the table). reply hoofhearted 10 hours agoparentprevSo are you saying that you would appreciate an editor that saves your posts in plain old html? Any preference of storing said posts in a database or .mdx markdown file? reply cjr 9 hours agoprevThis looks and sounds great... I'm currently using contentlayer [https://github.com/contentlayerdev/contentlayer] to manage docs and blog content, mostly .mdx files on urlbox.com. It works well with next.js but unfortunately is abandonware now. I also have a few custom remark/rehype plugins. You're right it is a pain to update articles buried in your repo especially with less technical team members. I already tried out TinaCMS to try and solve the editing issues, but their editor wasn't so nice, and it seemed to implicitly make a commit on every tiny change to any content, so I'm really hoping I could use something like this to edit my already existing content... reply orkj 15 hours agoprevThis made me think of http://prose.io which I remember was a thing 10 years ago. Pleasantly surprised it still is a website, not sure if it still works. But I remember the basic idea being similar, except Jekyll only. reply hunvreus 15 hours agoparentOh yeah, I loved Development Seed (the team that created prose.io) back in the day. They went on to create Mapbox. reply gyanreyer 15 hours agoprevI have been wanting a simple CMS which is effectively just a layer on top of GitHub for a while, I took a stab at it a couple years ago but bounced off so I'm very excited to see this! Definitely going to give it a try. reply tomgs 15 hours agoprevFollowing one of the comments in this thread, I reviewed two other products in this space - https://www.staticcms.org/ and https://decapcms.org/. It looks like the webpages are almost a direct copy of one another, one in dark mode and one in light mode, one with a community strip and professional services and one without I'm a technical product marketer, and I find this type of landing page copying amusing to no end. reply matzf 14 hours agoparentThat's no accident: from the Static CMS readme: > Static CMS is a fork of Decap (previously Netlify CMS) focusing on the core product over adding massive, scope expanding, new features. reply gabeio 12 hours agoprevI love it! I do have a quick question. It seems like you can create files with the correct filename for jekyll but it doesn't seem like there is a way to tell it that the date only exists as the file name? jekyll dates within the files do override the file dates but are actually not required. It would be nice if I could pull the date from the file name. I hope I'm just overlooking something simple. reply hunvreus 10 hours agoparentI had support for that but dropped it as I was trying to get that release out and didn't have time to refactor it properly (I made a lot of changes in the past couple of weeks). I do use Jekyll for a few sites of mine and need this to work, if only to sort my posts. Expect this to be added back in the next few days. I'll drop a line here once it's released. reply 0xferruccio 18 hours agoprevThis is super cool! Tried to set it up for our open source Changelog though and getting some errors, it doesn't seem to work with .mdx files https://github.com/juneHQ/changelog Anyways this looks super promising reply hunvreus 18 hours agoparentI have a very surface understanding of mdx, but basically it's a mix of markdown and JSX syntax, no? Didn't know you could define some sort of fields like you seem to do with: import { MdxLayout } from \"components/mdx-layout.tsx\"; export const meta = { slug: \"all-time-high\", publishedAt: \"2022-09-16T10:00:00.000Z\", title: \"All time high\", headerImage: \"https://june-changelog.s3.eu-central-1.amazonaws.com/changelog_ath_cd256d0719.png\", authors: [ (...) I suppose I should be able to hack it by defining it as a JSON frontmatter and some custom delimiters. If you let me know how you would see this working, I'll try and get something working tomorrow. reply 0xferruccio 18 hours agorootparentHonestly as a v1, I'd just love for you to parse out the whole text file and give me a basic markdown editor! I don't particularly care about having advanced filtering or of parsing my \"meta\" data reply hunvreus 18 hours agorootparentThe editor will kinda work for individual mdx files, just not collections. I guess you could try the following to see: media: public content: - name: changelog label: Changelog type: file path: pages/changelogs/2022-in-review.mdx format: code Obviously not super useful at this stage, especially since I haven't figured out mdx support in Codemirror yet. I can probably get collections to work though in the next few days. I'll let you know. reply reactordev 15 hours agoprevThis is awesome however, I’m hesitant due to the pro pricing. I get it, but I’m definitely looking for something we can host that can do this for our customer support team who aren’t the best at git, or markdown for that matter, but have valuable knowledge of setup and configuration of our apps. Any plans to just go full OSS with it and do sponsorships or OSI model foundation support? If this is yet another SaaS product then we’ll have to stick with our current methods. Very cool stuff though. Keep going! GitHub or GitLab should just buy this for their platform. reply hunvreus 15 hours agoparent- It's 100% Open Source: https://github.com/pages-cms/pages-cms - You can self-host it for free on Cloudflare Pages: https://pagescms.org/docs/development/#deploy-on-cloudflare - The online version is 100% free as well: https://app.pagescms.org There may be a pro plan at some point for some more complex features. From the FAQ on the front page: What's the \"Pro\" plan? I haven't completely figured it out, but there are a few features I'm working on that I believe would only be relevant to larger teams or professional use. Things like real-time collaboration, advanced media management (e.g., image manipulation), or S3 integration. This not only requires a lot more work but also hosting costs. If you're interested, drop me a line: hunvreus@gmail.com (@hunvreus). reply mikae1 15 hours agorootparent> The online version is 100% free > What's the \"Pro\" plan? I haven't completely figured it out Sounds like it's free until it isn't. reply mitchitized 15 hours agoparentprevI feel your frustration, it should either be free or not-free. Stop pretending to be both, while ending up being neither! Maybe we can start to refer to these types of projects as GOSS (Gated Open Source Software) or maybe COSS (Crippled Open Source Software)? :-) reply greenie_beans 15 hours agorootparentyou can download the code and self host it. how is that not open source software? reply fodi 17 hours agoprevVery nice! It looks a bit like Publii [0], but the editor part is cloud hosted instead of running as an app on your machine. [0] https://getpublii.com/ reply hunvreus 16 hours agoparentI've not tried it yet, it looks pretty slick. reply Tomte 3 hours agorootparentIt is quite nice, but since there is no way to configure URLs, it‘s best used for greenfield projects, not moving from some other CMS with a differing URL structure. reply indigodaddy 10 hours agoprevIf you don’t have an existing repo/site, can you start from scratch via this webui/CMS to create a new repo/site via some sort of SSG boilerplate/template etc? If that’s not already there it might be a good feature to add. reply hunvreus 9 hours agoparentI was working on it last week and dropped it to make this release. On the home screen you'll have the ability to choose from a few repos to fork with pre-configured CMS: vanilla, Next.js, Astro, 11ty, Hugo, Jekyll... I'm still curating the projects I'll be using as a starting point, so do let me know if there are templates for your favorite SSG that you'd like me to consider. The more challenging feature is building a configuration wizard that figures out your content structure for you. I think this will adoption a lot as the main friction right now is around getting a working config file up and running. reply _fat_santa 16 hours agoprevI've setup Decap CMS before, back when it was still called Netlify CMS. I loved the core idea of keeping all your files in Git and tracking them normally but like you said, it was just too hard get up and running and was very quirky, always thought to myself: \"I wish someone would make a git based CMS like this but something that's easier to run with a less wired config / UI, seems like you did just that. Congrats! reply FireInsight 15 hours agoparentI found DecapCMS setup quite easy the last time I did it. What roadblocks did you hit? reply yusefnapora 17 hours agoprevI was just thinking this morning how I wanted something like Decap CMS but a bit simpler. Got this running on Cloudflare in ten mins and it seems great so far. Thanks! reply hunvreus 17 hours agoparentGreat to hear, and glad I did not lie about it being easy to deploy. Do get the updates I'll be pushing in the next few days as I'm sure I'll find bugs here and there (I just pushed a hotfix 5 minutes ago [1]). [1]: https://github.com/pages-cms/pages-cms/releases/tag/0.2.1 reply blackhaj7 12 hours agoprevThis looks superb - very timely as I have been looking for something like this recently for a project reply flkiwi 9 hours agoprevIs there a way to configure this with media colocated with posts (i.e., not in a /media folder but in/a/post/subfolder/image.png)? reply hunvreus 8 hours agoparentBoth the content collections and media can filter by file extension so you could very well have something like: media: input: posts output: /posts categories: [ image ] content: - name: posts label: posts type: collection path: posts fields: - { name: title, label: Title, type: string } - { name: body, label: Body, type: rich-text } The collection of posts will only show the `md` files (that's the default, but you could change it by having a `filename` attribute on your content entry) and the media folder will only show images (`jpg`, `jpeg`, `png`, `gif`, `svg`, `bmp`, `tif`, `tiff`). reply FanaHOVA 16 hours agoprevWhy do I have to give access to all my public and private repos instead of selecting the ones I want to give it access to? reply hunvreus 16 hours agoparentI have an explanation in the FAQ section on the front page: Why do you need full access to all of my GitHub repositories? Well, the GitHub API kinda sucks when it comes to OAuth scoping. Pages CMS relies on the OAuth App flow, which doesn't allow for granular permissions. The alternative would be to use the GitHub App flow instead, but: It's a lot more complicated and would require us to store and orchestrate a lot more in the backend. Since we need to impersonate users (for things like commits), we anyway need to request user tokens, which technically would give us the same access as with the OAuth App flow. However, we do not store your GitHub OAuth tokens in the backend. The serverless functions used to facilitate the OAuth login pass the OAuth token to the front-end, allowing it to directly communicate with the GitHub API. And if you still don't trust the online version, you can deploy your own version for free in less than 10 minutes our Cloudflare Pages. Do let me know if I got some of this wrong (@hunvreus), and feel free to suggest improvements in the issue queue. Additionally I've added the support for Fine-grained PATs [1], allowing you to use a repository specific token. You'll see the button that reads \"Sign in with a Fine-Grained PAT\" on the login screen. [1] https://github.blog/2022-10-18-introducing-fine-grained-pers... reply FanaHOVA 15 hours agorootparentGot it; I hadn't heard of \"Fine-Grained PATs\" before so I just ignored it. My personal blog is open source already: https://github.com/FanaHOVA/2024-blog, so I was hoping to just OAuth and try it out, but I understand. Will try the self hosting at some point. Good luck with the project, looks slick. reply robertakarobin 15 hours agorootparent\"Fine-grained pats\" is what a herd of cows produces when their feed contains too much fiber. reply awb 16 hours agoparentprevCheck the FAQs at the bottom. TLDR: > Well, the GitHub API kinda sucks when it comes to OAuth scoping. Pages CMS relies on the OAuth App flow, which doesn't allow for granular permissions. reply sanex 10 hours agoprevThis is the type of thing I wish I would use my skills for. Instead I'm jealous and going to use it. Got it set up in 5 minutes, now to work on my post backlog. reply protomikron 13 hours agoprevThere's also Lektor CMS: https://www.getlektor.com/ It's quite mice solution to the whole problem (simple hosting + simple editing). reply louismerlin 15 hours agoprevThis reminds me of one of my weekend projects from a couple of years ago: a blog based on GitHub issues. https://github.com/louismerlin/blissue reply aleksiy123 16 hours agoprevLooks very nice. Any comparisons against other github based CMS? Personally I've been using Keystatic. reply hunvreus 16 hours agoparentI've not tried it yet, but isn't that running on your local machine? Looks pretty good though. reply aleksiy123 11 hours agorootparentYou can also self host with your blog. I have it running as part of my astro deploy on vercel. reply posterguy 16 hours agorootparentprevthere is a cloud version for free up to three users that supports multiple simultaneous editors. paid beyond that. reply hunvreus 16 hours agorootparentNice, thanks for the tip. Signing up. reply tuktuktuk 12 hours agoprevOne thing have been missing for static CMS for me is the ability to upload my image to 3rd party like uploadcare. reply hunvreus 11 hours agoparentI'm working on support for things like S3 and R2. reply p44v9n 18 hours agoprevThis is really cool! I was sad when Forestry went down and this looks better than TinaCMS. I'm trying to set it up with my Eleventy blog (paavandesign.com/blog) and struggling to get the body field working reply hunvreus 18 hours agoparentIs your repo public? Happy to have a look if it is. Have you also checked the examples: https://pagescms.org/docs/examples/ reply p44v9n 17 hours agorootparentedit: this was the bug which I think you're already on top of! https://news.ycombinator.com/item?id=39468906 --- So 'body' is working if I set the type to text but when I set it to rich-text there's no input generated for body Repo is private (sorry!) but here's the settings YAML file as it stands content: - name: blog label: Blog type: collection path: 'src/blog' view: fields: [ title, date] fields: - name: date label: Date type: date - name: title label: Title type: string - name: description label: Description type: string - name: tags label: Tags type: string - name: body label: Body type: rich-text reply hunvreus 17 hours agorootparentYep, seems to be the same bug as what somebody else reported: if you don't specify a `media` attribute, when the rich-text editor tries to load it fails as it is attempting to load the media related features (to insert images). Publishing a hotfix in the next hour, in the meantime it should work if you add a media attribute (you can set it as `media: \"\"` if you don't have an image folder). reply hunvreus 17 hours agorootparentprevI rolled out a hotfix, this should be fixed. I also found another couple edge cases that are unlikely but that I will patch in the coming days. Thanks for that. reply indigodaddy 18 hours agoprevSo the site has to hosted on GitHub Pages or just the website code needs to be using GitHub? Eg what if I have the site hosted elsewhere that just relies on GitHub commits? reply hunvreus 18 hours agoparentJust the code. For example, https://pagescms.org is an 11ty website: - The code is hosted on GitHub: https://github.com/pages-cms/website - The site is built and hosted by Cloudflare Pages You can have a look at the config there: https://github.com/pages-cms/website/blob/main/.pages.yml reply indigodaddy 18 hours agorootparentPerfect, thanks! Awesome stuff here I think it’s going to be what a large swath of users have been looking for! reply Arelius 16 hours agoprevAny word on S3 media support? I have a few content heavy sites. And that would be essential to be practical to switch to. reply hunvreus 16 hours agoparentI've started looking into the APIs for it, I am rebuilding a site that has a lot of heavy media so kind of need it too. Realistically, 4 to 6 weeks. reply jrdnbwmn 17 hours agoprevThis is awesome! It's exactly what I've been looking for. Can you have multiple types of Jekyll collections (posts)? reply hunvreus 17 hours agoparentYeah, as many collections and single files as you want. reply victorbjorklund 16 hours agoprevNice. Does it support multi-projects? I got a lot of sites and dont like to have to have a seperate CMS for each. reply hunvreus 16 hours agoparentYes: - There's an online version, you can just go to https://app.pagescms.org - You can switch between repos and branches straight from the interface (just click on the repo menu in the top left corner). reply victorbjorklund 12 hours agorootparentPerfect! Will check it out! reply hankchinaski 10 hours agoprevamazing concept, would be even more amazing if you could be able to deploy this on other places besides github pages. (cloudflare pages?) or anything else.. reply hankchinaski 10 hours agoparenti see is supported already, could be neat to setup blogs thank you reply nhggfu 6 hours agoprevgreat work with the explainer video etc. good luck w/ the project @ OP. reply o_____________o 16 hours agoprevTried it out, \"the branch master doesn't exist, redirecting you to the default branch (master)\" master exists reply hunvreus 16 hours agoparentOuch! Is it a public repo? I'd love to have a look. reply Fire-Dragon-DoL 18 hours agoprevNice, i've been looking for something like this in a long time! Will play with it later today reply greenie_beans 15 hours agoprevthis seems real cool at first glance. can't wait to try it out. reply icar 14 hours agoprevAh, damn it. I had this idea in my notes for a while. Too slow. reply posterguy 17 hours agoprevwhy this over keystatic or statamic or (insert upcoming decap replacement)? reply hunvreus 16 hours agoparentThey all look great, but if you were pressing me for a comment: - Keystatic: I want it to run online, not as a local app. - Statamic: I don't want an opinionated, full-stack CMS + SSG. I want to manage content in whatever app/website I'm building whether it's Next.js, Astro, 11ty... - Decap CMS: I mentioned it in my post, I always found Decap's UI/UX pretty lacking, and the DX wasn't that smooth either. With that being said, each one of these projects have been around for much longer, I don't necessarily expect to compete (yet). reply Arelius 16 hours agorootparentThere is als Sveltia cms, based on Decap's backend but with a redone UI https://github.com/sveltia/sveltia-cms reply hunvreus 16 hours agorootparentI saw that but couldn't see an actual demo. I'll have to deploy it and see for myself. reply FireInsight 15 hours agoparentprev> (inset upcoming decap replacement) What's wrong with DecapCMS? reply canadiantim 17 hours agoprevSupremely cool. So this can supplement e.g. my eleventy sites and provide a way for clients of those sites to easily interact with the content and static asset portions of the site? Seems like there's tons of potential here. Kudos, def will give it a whirl! reply hunvreus 17 hours agoparentYes it does HOWEVER for now you need to log in with GitHub. I don't think it's too huge of a hassle with most people, but there is a bit of friction. I am planning to add an \"invite by email\" feature that will allow you to add users by entering their email address. They can then log in without a GitHub account and use it the same way you do (although the commit will be associated to a Pages CMS GitHub app). reply canadiantim 16 hours agorootparentThe invite by email feature sounds great, but as you say setting up a github account for a client isn't that much friction. It's not something I'm bothered by at all, but being able to use just an email is definitely supremely more preferable. Very exciting work. As I'm quite enthused by the project, I hope you don't mind if I offer some hopefully helpful feedback: - For the video on your landing page, ideally the youtube would be embedded so it doesn't open another window - In regards to the video, while I love the content and info, it's not the most marketing friendly. It's over 2 minutes before the background of the video even changes. Ideally the video would get into the demo portion muuuch quicker. - I clicked on this hackernews link, clicked through to your landing page, scrolled around, but really didn't get a grasp of what it was offering or how it worked or who or what it was for (admittedly I was prly lazy reading). Only after I watched the video, heard you specifically mention eleventy (which I personally use a lot) was my interest piqued enough to actually engage in the content and understand it more. Glad I did, but I gotta feel there's some better way of presenting what you're doing. I quite liked your calling it a wordpress-like CMS over top an existing static site stack. Those are just the thoughts that came to me. I absolutely love the project, thrilled it's MIT too. Though I'm not really a javascript dev I could definitely see myself using and contributing! reply hunvreus 16 hours agorootparent> For the video on your landing page, ideally the youtube would be embedded so it doesn't open another window Yep. > In regards to the video, while I love the content and info, it's not the most marketing friendly. It's over 2 minutes before the background of the video even changes. Ideally the video would get into the demo portion muuuch quicker. Yep. I also recorded it when I was sick, and my mic is pretty horrid. > I clicked on this hackernews link, clicked through to your landing page, scrolled around, but really didn't get a grasp of what it was offering or how it worked or who or what it was for (admittedly I was prly lazy reading). Only after I watched the video, heard you specifically mention eleventy (which I personally use a lot) was my interest piqued enough to actually engage in the content and understand it more. Glad I did, but I gotta feel there's some better way of presenting what you're doing. I quite liked your calling it a wordpress-like CMS over top an existing static site stack. I definitely need to level up my marketing game. Thanks a lot for the input. reply invalidname 18 hours agoprevInteresting. https://gdocweb.com/ takes a different path to this by converting Google Docs to Google Pages sites. reply sneak 18 hours agoprevThis looks cool but I moved all my repos to a self-hosted forge when GitHub openly and notoriously refused to stop collaborating with ICE (that runs concentration camps in Texas). It would be nice to have this support generic/arbitrary git servers. I’ve been feeling this pain point for a while and have been considering building something like this myself. reply hunvreus 17 hours agoparentI am looking into GitLab and Bitbucket, but I don't plan on supporting generic Git, unless I can find a way to slap an API in front of it. I'll add that to the backlog. reply irgeek 2 hours agorootparentMaybe you could add support for Gitea? It’s a pretty handy way to slap an API in front of git and it’s much simpler to self host than GitLab. reply hunvreus 2 hours agorootparentAspects of the API look fairly similar to GitHub actually. I'll look into it. reply and0 18 hours agoprev [–] What the hell is going on with the little sentences in that example? I thought it was supposed to be like, a bad VC rap joke song or something? I was looking for rhymes. It's really lame. reply hunvreus 17 hours agoparentSorry for that: the titles are indeed lame. But they're mine. And I kinda like them. reply OJFord 18 hours agoparentprev [–] They're the titles of the author's blog posts: https://ronanberder.com/ (see bottom of page) - i.e. it's a screenshot of them using it for their own blog. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pages CMS is a user-friendly content management system that connects with GitHub repositories, allowing users to manage content and media through regular commits.",
      "Users log in using their GitHub account, choose a repository, add a configuration file, and start managing content effortlessly.",
      "Future updates for Pages CMS involve collaborative editing and email invitations for non-technical users, making it a versatile and accessible tool."
    ],
    "commentSummary": [
      "Users are evaluating different content management systems (CMS) like Pages CMS, Decap CMS, Lektor CMS, focusing on features, user experience, and potential enhancements.",
      "Excitement surrounds new options like PagesCMS for website hosting and content management, including comparisons with platforms like Notion and Keystatic.",
      "Developers are actively improving these open-source CMS projects, aiming for additional features, bug fixes, and integration with various git servers, catering to the demand for lightweight, user-friendly solutions that integrate seamlessly with GitHub."
    ],
    "points": 307,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1708610283
  },
  {
    "id": 39470381,
    "title": "Turning ThinkPad into Programmable USB Device: A Deep Dive",
    "originLink": "https://xairy.io/articles/thinkpad-xdci",
    "originBody": "🤫 Unlocking secret ThinkPad functionality for emulating USB devices Feb 22, 2024 This is the story of how I figured out a way to turn my ThinkPad X1 Carbon 6th Gen laptop into a programmable USB device by enabling the xDCI controller. As a result, the laptop can now be used to emulate arbitrary USB devices such as keyboards or storage drives. Or to fuzz USB hosts with the help of Raw Gadget and syzkaller. Or to even run Facedancer with the help of the Raw Gadget–based backend. And do all this without any external hardware. The journey of enabling xDCI included fiddling with Linux kernel drivers, xHCI, DWC3, ACPI, BIOS/UEFI, Boot Guard, TPM, NVRAM, PCH, PMC, PSF, IOSF, and P2SB, and making a custom USB cable 😱 🎬 Introduction 🧐 Investigation 🐧 Reading kernel code 🔍 Searching online 🚧 Checking PCIe and ACPI 🎁 Checking BIOS 💡 Enabling xDCI ⚙ Enabling xDCI via Advanced settings 🧁 Checking xDCI 🧾 Attempting to enable xDCI via PCH 💾 Enabling xDCI via NVRAM 🚀 Using xDCI 🖱 Legacy gadget drivers 🤖 Raw Gadget 🧰 syzkaller 💃 Facedancer 🗃 Summary 📝 Afterword ⬅ Note the interactive table of contents on the left. 🎬 Introduction One day, I was working on improving Raw Gadget. Raw Gadget and UDCs. Raw Gadget is a Linux kernel module for emulating highly customizable USB devices. This module provides a userspace API for the Linux kernel USB Gadget subsystem. The flexibility of the API allows using Raw Gadget for fuzzing and exploiting USB hosts by providing malformed USB descriptors. UDCs are also sometimes called USB Peripheral Controllers. To emulate USB devices through the Gadget subsystem of the Linux kernel, one needs a special hardware component called the USB Device Controller (UDC). Such components are generally not present on PCs (right? 😉) but are typically embedded into single-board computers like a Raspberry Pi. Raspberry Pi. Thus, I usually used a Raspberry Pi to work with Raw Gadget. However, dealing with a Raspberry Pi is a hassle: plugging in the wires, booting the board, accessing the shell, etc. So, for a while, I dreamt how nice it would be to have a UDC connected directly to my laptop instead. EC3380-AB. At some point, I managed to find a solution for connecting a UDC to a PC: EC3380-AB. Quite a pity modern laptops moved away from using ExpressCard 😢 EC3380-AB is an ExpressCard board based on the USB 3380 Peripheral Controller chip that implements a UDC. With the help of the Sonnet Echo ExpressCard-to-Thunderbolt adapter, EC3380-AB can be plugged into a Thunderbolt port to connect the UDC to a PC without an ExpressCard slot. EC3380-AB with the Sonnet Echo ExpressCard-to-Thunderbolt adapter There are also a few other UDC boards based on USB 3380. But those connect over PCIe and thus require a bulky PCIe-to-Thunderbolt enclosure to connect them conveniently. Two of these boards, USB3380EVB and PP3380-AB, might be familiar to those who worked with DMA attacks. Like EC3380-AB, these boards are powered by USB 3380 and are originally intended to serve as UDCs. However, they can also be reprogrammed to be used as a tool for DMA attacks over PCIe. Working with EC3380-AB. The combination of EC3380-AB and the Sonnet Echo adapter worked mostly fine. However, the net2280 Linux kernel driver used for this UDC would sometimes glitch out. Thus, occasionally, I had to replug the controller into my laptop to reset the driver state. As I was plugging and unplugging EC3380-AB, I would check the contents of /sys/class/udc/ to make sure that EC3380-AB connected successfully. This directory displays the UDCs connected to the system: $ ls /sys/class/udc/ 0000:0a:00.0 $ cat /sys/class/udc/0000\\:0a\\:00.0/uevent USB_UDC_NAME=net2280 Huh. At some point, I typed ls /sys/class/u in the terminal and pressed Tab to let it autocomplete to .../udc. However, the autocomplete showed me another directory: /sys/class/usb_role. Moreover, this directory unexpectedly contained another one named intel_xhci_usb_sw-role-switch. “Hm, what the hell is this?” thought I 🤔 xHCI (eXtensible Host Controller Interface) is a USB HCD (Host Controller Device) used on x86-powered systems. It’s a hardware component that allows the system to act as a USB host. /sys/class/usb_role is an interface for switching the hardware USB component between the host and the device modes. Some computers use the same chip to serve either as an HCD or a UDC, depending on the chosen configuration, and this interface allows reconfiguring that in runtime. xHCI role switch. What surprised me here was seeing a usb_role switch for the xHCI device on my ThinkPad laptop. I wouldn’t be surprised to see it on an x86-powered single-board computer or on an Intel NUC device. But ThinkPad? I thought it didn’t have any capability of being a USB device. This was unexpected. Out of curiosity, I tried writing device to intel_xhci_usb_sw-role-switch/role, which is supposed to switch the USB component on the laptop to the device mode. But nothing happened: no dmesg messages, no new files in /sys/class/udc/. So I decided to dig deeper. 🧐 Investigation 🐧 Reading kernel code I started by looking into how intel_xhci_usb_sw-role-switch ended up appearing on my ThinkPad in the first place. xHCI role switch implementation. A grep for intel_xhci_usb_sw through the Linux kernel code yielded two results: The drivers/usb/roles/intel-xhci-usb-role-switch.c file that contained the platform driver for an intel_xhci_usb_sw device. This driver registered a USB Role Switch, which implemented the handlers for when device or host is written into intel_xhci_usb_sw-role-switch. Both handlers appeared to manipulate the registers of the xHCI device. The drivers/usb/host/xhci-ext-caps.c file that created the intel_xhci_usb_sw virtual platform device, for which the driver from #1 got registered. This device was only created when the xHCI driver set the XHCI_INTEL_USB_ROLE_SW quirk flag. And this only happened for PCIe devices with a few specific device numbers. The list of numbers included PCI_DEVICE_ID_INTEL_SUNRISEPOINT_LP_XHCI with the value 0x9d2f, which matched the xHCI PCIe device ID on my laptop. ThinkPad. So apparently, the xHCI device on my laptop did support the role switching. Or at least its driver believed so. Now the question was: What exactly happened when the driver wrote to the xHCI registers? Did these writes produce any effects? Or did the hardware not support the device role at all? 🔍 Searching online The next thing I tried was looking online for references to intel_xhci_usb_sw-role-switch. Mailing list discussion. This led me to the discussion titled Any example of USB gadget for DRD device mode on Intel Gemini Lake? on the Linux kernel mailing list. There, Dmitry Mikushin was facing the same issue as me: role switching for the xHCI device did not appear to work. Dmitry’s question also noted that after switching the role, one of the ports was successfully recognized as a new USB device when connected with a cable to another laptop. However, initially, I managed to overlook this part of the question. DWC3. Luckily, Heikki Krogerus came to the help: « So, do you have the DWC3 (the USB device controller) PCI device available/visible on your system? What do you get if you run lspci -nngrep USB? The DWC3 PCI device ID on Gemini Lake is 0x31aa (search PCI_DEVICE_ID_INTEL_GLK in drivers/usb/dwc3/dwc3-pci.c). DWC3 stands for the DesignWare Core SuperSpeed USB 3.0 Controller. This is an IP core (aka IP block or IP unit) provided by Synopsys that implements a UDC. Many vendors, including Intel, use this IP core in their systems. In the response, Heikki suggested checking if a DWC3 UDC was present in the list of PCI devices. This made sense: if a UDC is present on a system, it has to be somehow connected. And it looked like Intel connected their DWC3 UDCs over PCI or PCIe. Dmitry responded: « Yes, AFAIK, PCI_DEVICE_ID_INTEL_GLK_XHCI is actually 0x31a8, and I do have it: 00:15.0 USB controller [0c03]: Intel Corporation Device [8086:31a8] (rev 03). Mux. Heikki answered: « That is the xHCI controller, and it is not what you need if you want to use the connector in device mode. The xHCI and DWC3 IPs are separate IPs on GLK. That is why there is a mux between the two. Here, Heikki pointed out that xHCI and DWC3 UDC were different devices. And having xHCI enabled did not imply that the DWC3 UDC could be used. Heikki also gave another valuable piece of information: there is a mux (multiplexer — a kind of switch) between xHCI and the DWC3 UDC. And, apparently, changing the USB role via intel_xhci_usb_sw-role-switch should switch this mux. Oh no. Heikki then continued: « The DWC3 USB Device Controller has device ID 31aa, so you want to see a PCI device with this device ID. It’s not there. So, the DWC3 PCI device is not enabled on your board, which means you do not have a USB Device Controller to deal with. The connector is in host mode only. Sorry. So, if the DWC3 UDC itself was not there, having the mux switched was pointless. Most likely, this is what was happening in my case. xDCI in BIOS. Heikki then also added: « If you can enter the BIOS menu, then you can try to find a setting named xDCI (so that’s “xDCI”, not “xHCI”). It is usually somewhere under some USB menu. If you have that, then enable it, and you should see the DWC3 PCI device in the operating system. This was where I encountered the term xDCI for the first time. An online search for xDCI revealed: « Extensible Device Controller Interface (xDCI) is an interface specification that defines Device Controller for a Universal Serial Bus (USB 3), which is capable of interfacing with USB 1.x, 2.0, and 3.x compatible devices. In the case that the computer is connected as a device to another computer (for example, a tablet connected to a desktop), then the xDCI controller will be activated inside the device and will talk to the Host on the other computer.Extensible Device Controller Interface (xDCI), 12th Generation Intel® Core™ Processors Datasheet So, xDCI is what Intel calls its interface for the DWC3 UDC. Unlike xHCI, xDCI is the interface for the device side of USB, not the host one. In the response, Heikki suggested checking whether xDCI could be enabled in BIOS. This created a spark of hope for me. Perhaps I could indeed enable xDCI in BIOS, and everything would work. For simplicity, I will be using the term “xDCI” to refer to the Intel’s DWC3 UDC itself from now on. Even though technically, the word “BIOS” refers to the legacy firmware used by older IBM PC–compatible systems, I will be saying “BIOS” to refer to the UEFI-compatible platform firmware of modern systems for simplicity. Once I knew the term “xDCI”, I also did an online search for xHCI xDCI. This led me to a diagram from the USB Virtualization documentation page of Project ACRN, which schematically shows xHCI, xDCI, and the mux between them. xDCI in ACPI. In the discussion, Felipe Balbi, who was the maintainer for the USB Gadget subsystem in the Linux kernel up until recently, added: « Also, have a look at acpidump. See if the device even exists in your DSDT but, perhaps, is disabled (look at the _STA method for OTDG or XDCI). Here, Felipe proposed to also make sure that xDCI was enabled in ACPI. « ACPI (Advanced Configuration and Power Interface) is an open standard that operating systems can use to discover and configure computer hardware components. ACPI defines hardware abstraction interfaces between the device’s firmware (e.g., BIOS, UEFI), the computer hardware components, and the operating systems.ACPI, Wikipedia After reading more about ACPI, I found out that _STA method is used to check the device status. The OS will only try to connect the device if its status indicates that the device is enabled and properly configured. If the _STA method is not defined in ACPI at all, the device will not function. Next steps. Finding this discussion was extremely helpful. Now I knew what to do next: Check if the xDCI device shows up in the list of PCIe devices; If not, check if there is a way to enable xDCI through BIOS; Finally, check if ACPI enables xDCI. 🚧 Checking PCIe and ACPI Checking PCIe device. Following Heikki’s guidance, I first checked whether the xDCI device appeared on my laptop’s list of PCIe devices via sudo lspci -vvnnn. The only USB device that I saw there was xHCI with the 9d2f device ID: 00:14.0 USB controller [0c03]: Intel Corporation Sunrise Point-LP USB 3.0 xHCI Controller [8086:9d2f] (rev 21) (prog-if 30 [XHCI])Subsystem: Lenovo Sunrise Point-LP USB 3.0 xHCI Controller [17aa:225c]Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+Status: Cap+ 66MHz- UDF- FastB2B+ ParErr- DEVSEL=medium >TAbort- SERR-acpi.dat $ acpixtract -a acpi.dat $ iasl -d dsdt.dat And grepped dsdt.dsl for XDCI: Device (XDCI) { Name (_ADR, 0x00140001) // _ADR: Address OperationRegion (OTGD, PCI_Config, 0x00, 0x0100) Field (OTGD, DWordAcc, NoLock, Preserve) { DVID, 16, Offset (0x10), XDCB, 64 } Method (_STA, 0, NotSerialized) // _STA: Status { If ((DVID != 0xFFFFFFFF)) { Return (0x0F) } Else { Return (0x00) } } ... } This was good news: the _STA method was present! While this didn’t necessarily mean that ACPI properly set up xDCI, this was already promising. I believe that DVID in the _STA implementation stands for “Device ID”, and it is read from the PCIe Configuration Space of the xDCI device, but I didn’t analyze ACPI for xDCI beyond that. Status result. Out of curiosity, I decided to check what the _STA method returned. After looking around /sys/bus/acpi/devices/, I found out that device:33 was designated to the XDCI ACPI device: $ cat /sys/bus/acpi/devices/device:33/path \\_SB_.PCI0.XDCI And the result of the _STA method was: $ cat /sys/bus/acpi/devices/device:33/status 15 Interestingly, _STA returned 15, which stands for the device being enabled and functioning. However, as I had no xDCI device show up in lspci, this was clearly false. Perhaps everything functioned properly only from the ACPI point of view. I decided that finding a defined _STA method was good enough and proceeded to check BIOS. 🎁 Checking BIOS An online search for combinations of ThinkPad, BIOS, and xDCI yielded no relevant results. Apparently, no one has attempted to figure out whether xDCI can be enabled in Thinkpad BIOS. BIOS settings. I booted into the BIOS settings screen (aka BIOS Setup) and looked around for something related to xDCI or OTG. I found nothing. I knew that many Lenovo laptops had a hidden Advanced page in their BIOS settings. So, perhaps I could find the xDCI setting there. While some Lenovo laptops allow unlocking this page via magic keypresses, I failed to find a way to do this on Thinkpad X1 Carbon. But maybe there was another way. Unpacking BIOS. Before trying to unlock the Advanced page, I decided to check the BIOS image directly for the presence of an xDCI-related setting. I downloaded the BIOS Bootable CD update image from the Lenovo website, unpacked it, and mounted it: $ sudo apt-get install genisoimage $ geteltorito -o n23ur39w.img n23ur39w.iso $ sudo kpartx -av ./n23ur39w.img $ sudo mkdir /mnt/bios $ sudo mount -o ro /dev/mapper/loop0p1 /mnt/bios/ The main BIOS binary was now in /mnt/bios/FLASH/N23ET86W/\\$0AN2300.FL1. For reference, here’s how to unmount the image later: $ sudo umount /mnt/bios/ $ sudo kpartx -d ./n23ur39w.img Also, for reference, here’s how to extract BIOS from the .exe update finary that Lenovo provides: $ sudo apt-get install innoextract $ innoextract n23uj39w_v2.exe $ ls codeGetExtractPath/N23ET86W/\\$0AN2300.FL1 Right in the middle of me meddling with BIOS images, Lenovo removed its Linux *.cab BIOS update packages from their website 🤔 So no more cabextract 😢 Come to think of this, new ThinkPads are also missing Linux kernel drivers for their MIPI web cameras. Eh, Lenovo. UEFITool. I installed UEFITool and imported the BIOS binary: $ sudo apt-get install uefitool $ UEFITool /mnt/bios/FLASH/N23ET86W/\\$0AN2300.FL1 I searched for xDCI and got a hit in GUID E6A7A1CE-5881-4B49-80BE-69C91811685C, corresponding to the Setup module. Searching for xDCI via UEFITool « A GUID is a unique 128-bit number that is a Globally Unique IDentifier. Each time an image, protocol, device, or other item is defined in UEFI, a GUID must be generated for that item.GUIDs, EDK II Driver Writer’s Guide I extracted the binary data from the PE32 image section of the Setup module into Setup.bin via Extract body... from UEFITool and checked for strings that contained xDCI: $ strings -el ./Setup.bingrep xDCI xDCI Support Enable/Disable xDCI (USB OTG Device). Aha! So, the setting for enabling xDCI was there; it was just indeed hidden by default. 💡 Enabling xDCI At this point, I had high hopes that xDCI could be enabled through BIOS. I only had to find a way to unlock the setting to do this. ⚙ Enabling xDCI via Advanced settings The first thing I tried was unlocking the hidden Advanced BIOS page. Luckily, the BIOS modding community has already figured this out. The process is well-documented by the authors of the x1c6-hackintosh project coordinated by Tyler Nguyen. That project aims to run MacOS on ThinkPad X1 Carbon 6th Gen — specifically the laptop that I had. SPI reflashing. Unlocking the Advanced page via the proposed method required reflashing the SPI chip that stores BIOS. The most common approach for reflashing the BIOS SPI chip is attaching a clip to the chip on the laptop’s motherboard and using an SPI programmer to rewrite the chip’s contents. The good thing about this approach is that you can attach the clip once and then reflash BIOS as many times as you’d like. However, I was hesitant to use that approach. A few years ago, I successfully fried the motherboard on another laptop when I tried reflashing BIOS with an SPI clip. I guess I either connected the clip improperly, or the motherboard just didn’t tolerate the SPI chip being powered externally. This time, I wanted to avoid messing this up. A safer approach to reflashing an SPI chip is to unsolder it from the motherboard and directly connect it to an SPI programmer. This is more time-consuming but allows avoiding damaging the motherboard. In the worst case, you would just kill the SPI chip, which is easily replaceable. SPI socket. Instead of unsoldering and soldering the chip back on for each BIOS test, I decided to replace the SPI chip on the motherboard with an SPI socket. With the socket in place, I could take the chip out and put it back in whenever I wanted. My soldering skills are not great. Luckily, Sergey Korablin, who used to design and assemble motherboards for a job, was around to help me with soldering 🤗 Accessing to the SPI chip on the ThinkPad X1 Carbon 6th Gen motherboard required removing the Wi-Fi antenna module. This module was held by a single screw, so this was not hard. We also disconnected the main laptop battery before soldering, just in case (and I also did that whenever I took the chip out of the socket). Left: SPI chip before unsoldering; Right: SPI socket installed Wieson G6179-10. For the SPI socket, I chose Wieson G6179-10 by Adafruit. This socket had a few non-critical problems: It was quite large, so the bottom laptop lid didn’t fully close; The latch on the socket was tight, so opening it required care to avoid tearing it apart from the motherboard (I used a thread to pull on the latch while holding the socket in place); The connection between the chip and the socket was flaky: my laptop would occasionally shut down and make beep noises on the power-on (I had to jiggle the chip in the socket to fix that temporarily). But overall, the socket was good enough for running my experiments. FT2232H Mini Module. For reflashing the SPI chip, I used the FTDI FT2232H Mini Module, which is a USB-to-serial converter that supports working with SPI, UART, I2C, and JTAG. I learned about this module from an article by Dmytro Oleksiuk many years ago and have it in my toolkit since then. For reference, here’s the pinout of this module for working with SPI chips: The module can simultaneously work with two SPI chips through different channels (named 2 and 3). SPI chip pin Boarn pin, ch. 2 Board pin, ch. 3 1 CN2.12 CN3.23 2 CN2.09 CN3.24 3 CN2.14 CN3.21 4 CN2.02 CN3.04 5 CN2.10 CN3.25 6 CN2.07 CN3.26 7 CN2.13 CN3.20 8 CN2.05 CN3.01 To connect the SPI chip pins to the FT2232H module, I used a SOP8 to DIP8 adapter (similar to this one) and a bunch of jump wires. I also connected CN2.1 to CN2.11 and CN3.1 to CN3.3 on the FT2232H module to enable the USB Bus-Powered mode according to the module’s datasheet. FTDI FT2232H Mini Module connected to SOP8 to DIP8 adapter with SPI chip To dump the contents of the SPI chip via the module, I used flashrom: $ sudo flashrom -p ft2232_spi:type=2232H,port=A -r bios.bin Tip: always dump SPI chips twice and make sure the contents in both dumps match. Patching BIOS. The BIOS modding guide suggested patching the BIOS image in two steps: Use the UEFIPatch tool to apply a provided set of patches; Use a hex editor to change the 4C 4E 56 42 42 53 45 43 FB byte sequence to 4C 4E 56 42 42 53 45 43 FF (the last byte is different). Before mindlessly following these steps, I decided to get at least a high-level understanding of what these changes did. First patch. After poking around, I found out that the first step replaced references to the GUID for the Date/Time BIOS page with such for the Advanced one: # SystemFormBrowserCoreDxeenable advance menu Lenovo xx70/xx80 721C8B66-426C-4E86-8E99-3457C46AB0B9 10 P:04320b483cc2e14abb16a73fadda475f:778b1d826d24964e8e103467d56ab1ba 32442D09-1D11-4E27-8AAB-90FE6ACB0489 10 P:04320b483cc2e14abb16a73fadda475f:778b1d826d24964e8e103467d56ab1ba With this patch, the Date/Time BIOS settings page should be replaced with the Advanced one. I applied the patch above via UEFIPatch to my bios.bin: $ sudo apt-get install uefitool-cli $ UEFIPatch bios.bin patch.txt -o bios-patched.bin Second patch. The second step of changing a magical byte sequence was quite cryptic. I suspect this patch switches the TPM-related BIOS setting bit in NVRAM, but I didn’t check it. Eventually, I found out that this change was forcing the TPM (Trusted Platform Module) into the MFG Mode (Manufacturing Mode). This prevented Boot Guard from detecting the BIOS modification from step #1. « Intel Boot Guard is a processor feature that prevents the computer from running firmware (UEFI) images not released by the system manufacturer. When turned on, the processor verifies a digital signature contained in the firmware image before executing it using the public key, which is fused into the system’s Platform Controller Hub (PCH) by the system manufacturer (not by Intel). As a result, Intel Boot Guard makes it impossible for end users to install replacement firmware or modded BIOS.Intel vPro, Wikipedia Normally, disabling Boot Guard should be much harder than simply flipping a bit in the BIOS binary. But I guess ThinkPad X1 Carbon 6th Gen is just old and vulnerable. I did this step on a fairly old BIOS version 1.37. It is possible that this Boot Guard bypass has been fixed in one of the newer versions. However, the NVRAM approach I discuss below should still work. I tried applying only the first change without the second. The laptop made beeping noises, most likely indicating a Boot Guard failure, and refused to boot. To apply the patch from the second step, I simply used the ghex hex editor. Flashing. After applying both patches, I flashed the modified BIOS to the SPI chip: $ sudo flashrom -p ft2232_spi:type=2232H,port=A -w bios-patched.bin And put the chip into the socket. Advanced settings. After booting into BIOS, the Date/Time page disappeared, but the Advanced page appeared instead, as expected 🥳 I haven’t found a way to take BIOS screenshots, so I had to take pictures of the screen 🙃 After following through Intel Advanced Menu, PCI-IO Configuration, and USB Configuration, I found the setting called xDCI Support and switched it to Enabled. I also checked the TPM settings. As expected, the TPM was switched to MFG Mode. Success. After I booted the laptop, I checked lspci once again. Magically, the xDCI device with the expected 9d30 device ID appeared 🥳: 00:14.1 USB controller [0c03]: Intel Corporation Device [8086:9d30] (rev 21) (prog-if fe [USB Device])Subsystem: Lenovo Device [17aa:225c]Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- SERR- Kernel driver in use: dwc3-pciKernel modules: dwc3_pci Moreover, the kernel even automatically loaded the dwc3 UDC driver (remember that xDCI is based on DWC3), and a new entry appeared in the list of UDC devices 🤯: $ ls /sys/class/udc/ dwc3.1.auto The only thing left was checking whether this UDC driver actually worked with xDCI. Shortcomings. This approach of enabling xDCI had two shortcomings. It required: Reflashing the SPI chip via a programmer. This sets a high bar for those wishing to enable xDCI on their machine. Doing this might also be impossible on newer systems due to RPMC; see the Afterword section. Having a Boot Guard bypass. While there is a bypass for the X1 Carbon 6th Gen laptop, other machines might not be vulnerable in the same way. I tried to tackle these problems later; see the sections below. 🧁 Checking xDCI At this point, I had xDCI enabled in BIOS, and the dwc3 UDC driver was loaded. As before, I switched the USB role to the device state: $ echo devicesudo tee /sys/class/usb_role/intel_xchi_usb_sw-role-switch/role And as before, nothing happened; no new dmesg messages appeared, at least. But as I now had the UDC driver loaded, I could attempt emulating a USB device through it and check if that works. Finding port. First, I had to figure out to which USB port xDCI was connected, if to any. On Raspberry Pi–like boards, there is usually a single USB port that can be used for USB device emulation; such port is typically marked as “USB OTG”. But there are no such markings on ThinkPad laptops. Moreover, it could be that the xDCI-enabled port of the xHCI controller was not wired to the external casing of the laptop at all 😟 To find the xDCI-enabled port, I tried plugging in a USB flash drive into all external ports one by one while having the USB role set to device. The second port I tried didn’t work: the OS did not detect the flash drive. But once I switched the USB role back to host, the port started working. So, it looked like I had found the right port 🥳 xDCI-enabled port on ThinkPad X1 Carbon 6th Gen USB cable. Next, I needed a USB cable to connect my laptop to a USB host. First, the cable had to be male-to-male: the xDCI-enabled port of my laptop was type A, not type B, which is typically used on USB devices. So, I needed a cable with a type A connector on both sides. Technically, male-to-male cables are not compliant with USB 2.0. But, oh well, USB is weird. Then, it was reasonable to use a cable with the power VBUS line disconnected. It could be that role switching did not turn off the power on the xDCI-enabled port (and this was indeed the case: I later checked with a USB breakout board and a multimeter). And connecting two devices with slightly different VBUS levels might not be a good idea. A normal USB 2.0 cable has four wires inside: the power VBUS and the ground GND lines that the host uses to supply 5 V and up to 500 mA of power to the device, and the data D- and D+ lines that are used primarily to transmit data. Making USB cable. For the initial testing, I cut open two USB 2.0 cables with type A connectors and used them to make a single male-to-male cable with VBUS disconnected. Later, I switched to using PortaPow USB Power Blocker with a male-to-male USB 2.0 cable as a more production-grade solution. PortaPow USB Power Blocker is a USB adapter that disconnects VBUS while leaving the data D- and D+ and the ground GND lines untouched. Perfect for my use case. Left: DIY male-to-male USB 2.0 cable with VBUS (red wire) disconnected; Right: male-to-male USB 2.0 cable with PortaPow USB Power Block attached I also tried using the USB 3.0 Super-Speed A/A Debugging Cable made by DataPro. This is a male-to-male USB 3.0 cable that leaves VBUS disconnected (along with USB 2.0 D- and D+) and swaps the SuperSpeed receiver/transmitter differentials pairs. However, this cable didn’t work for my use case; I haven’t tried to figure out why. Gadget Zero. Now came the moment of truth. I connected the xDCI-enabled port of my X1 Carbon 6th Gen laptop to a USB port of another computer and loaded the g_zero gadget driver module. g_zero is a gadget driver module that is typically used for testing UDCs. Its configuration entry says: “Make this be the first driver you try using on top of any new USB peripheral controller driver”. Once loaded, this module emulates a testing USB device through the first UDC it finds on the system. And it worked! The g_zero gadget driver successfully emulated a USB device through xDCI for another laptop that I used as the host 🥳 ThinkPad X1 Carbon 6th Gen emulates a USB device for ThinkPad X1 Carbon 10th Gen via xDCI Awesome! 😃 Before moving on to showing a few more interesting usage examples for xDCI, I’ll cover two other approaches to enabling it that I tried. 🧾 Attempting to enable xDCI via PCH After I managed to enable xDCI by reflashing the SPI chip, I was wondering whether it would be possible to do this purely via software. My thinking was that since BIOS somehow enabled xDCI, perhaps I could do the same from the booted OS. For this, I had to figure out how exactly BIOS enabled xDCI. This approach was suggested to me by Maxim Goryachy, who is an expert in the low-level platform security area. Maxim also tremendously helped me by answering questions about BIOS and PCH. For everything in this section, I used the old BIOS version 1.37. I did not check whether it all works the same with newer versions. BIOS source. I’m not a big fan of reverse engineering binaries, so decompiling BIOS to understand how it handled xDCI was not a thing I wanted to do. Luckily, there have been several BIOS source code leaks from various manufacturers over the last few years. Thus, obtaining the source code for the Kaby Lake BIOS was not hard (my ThinkPad X1 Carbon 6th Gen laptop with the i7-8650U CPU is based on the Kaby Lake R architecture). I will not provide a link to the leaked source code nor full snippets of non-public code to limit the possibility of violating some copyrights. But you can find the leaked code yourself if you’re interested; search for KabylakeSiliconPkg. xDCI in BIOS. I grepped the BIOS source code for xDCI and found the code responsible for its bring-up in Pch/Library/Private/PeiPchInitLib/PchXdci.c. The code was roughly structured like this: ConfigureXdci() {// Do stuff.if (XdciConfig.Enable == 0) { // Do stuff to disable xDCI. // Disable xDCI in PSF. // Disable xDCI in PMC.} else { // Do stuff to enable xDCI.} } The parts of the code that I replaced with comments appeared to manipulate various PCH registers. « Platform Controller Hub (PCH) is a family of Intel’s single-chip chipsets. PCH controls certain data paths and support functions used in conjunction with Intel CPUs.Platform Controller Hub, Wikipedia My idea was to try reverting all of the register manipulations done in the disable branch in the reverse order and then apply the manipulations from the enable branch. Datasheet. Before proceeding, I decided to try finding a datasheet for the PCH used on my laptop. The datasheet could hopefully describe the PCH registers, which would be helpful. First, I found out that Sunrise Point (the chipset on my laptop) is the codename for the series 100 PCH chipsets. Even though Kaby Lake does support the 200 series chipsets, ThinkPad X1 Carbon 6th Gen used an older one. After that, I managed to find the Intel 100 Series Chipset Family Platform Controller Hub (PCH) datasheet. However, the datasheet subtitle said “Supporting S and H Platform Register Information”. Later, I discovered that this meant that this specific datasheet applied to Sunrise Point-H chipsets, not Sunrise Point-LP, which I had. But it was still good enough for my purposes. 🔋 Enabling xDCI in PMC The last step in the branch responsible for disabling xDCI in BIOS was disabling it in PMC. PMC (Power Management Controller) is a controller that manages power 🤓 Naturally, disabling xDCI in PMC most likely disconnected the xDCI device from power. These register and bit offsets are public in tianocore/edk2-platforms. xDCI in PMC. To disable xDCI in PMC, BIOS set the B_PCH_PWRM_NST_PG_FDIS_1_XDCI_FDIS_PMC bit (#24) in the R_PCH_PWRM_NST_PG_FDIS_1 register (offset 0x628) of the PMC device. I searched through the PCH datasheet and found out that NST_PG_FDIS_1 was one of the PMC Chipset Initialization Registers, and its bit #24 was responsible for XDCI Function Disable as expected. NST_PG_FDIS_1 register and its xDCI bit in the PCH datasheet PMC registers. To enable xDCI in PMC, I thus wanted to reset the XDCI Function Disable bit in the NST_PG_FDIS_1 register of the PMC device. The question then was: “How do I access this PMC register from the OS?”. This turned out to be not hard. In the datasheet, the NST_PG_FDIS_1 register was listed in the PMC Memory Mapped Registers section. This meant that the register was memory-mapped, and thus, I could access it through a specific address in physical memory. I only had to figure out what that address was. PMC in BIOS. First, I checked how BIOS finds this address. As it turned out, BIOS read the PMC memory-mapped register area address from the R_PCH_PMC_PWRM_BASE register (offset 0x48) within the PCIe Configuration Space of the PMC device: This code is public in tianocore/edk2-platforms. EFI_STATUS EFIAPI PchPwrmBaseGet (OUT UINT32 *Address) { UINTN PmcBase = MmPciBase ( DEFAULT_PCI_BUS_NUMBER_PCH, PCI_DEVICE_NUMBER_PCH_PMC, PCI_FUNCTION_NUMBER_PCH_PMC ); *Address = MmioRead32 (PmcBase + R_PCH_PMC_PWRM_BASE) & B_PCH_PMC_PWRM_BASE_BAR; return EFI_SUCCESS; } Click the switch to see the full source code. Note that the PMC PCIe Configuration Space area (PmcBase in the snippet above) differs from the PMC memory-mapped register area (denoted as PWRMBASE). Both are technically memory-mapped (can be accessed through physical memory), but they serve different purposes and reside at different addresses. PMC device. There indeed was a PMC PCIe device on my laptop: 00:1f.2 Memory controller [0580]: Intel Corporation Sunrise Point-LP PMC [8086:9d21] (rev 21) Subsystem: Lenovo Sunrise Point-LP PMC [17aa:225c] Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx- Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- SERR- regbase + reg_offset); } static inline void pmc_core_reg_write(struct pmc *pmc, int reg_offset,u32 val) {writel(val, pmc->regbase + reg_offset); } Thus, instead of reading out the address myself, I decided to reuse the mapping that the kernel provided. Resetting bit. I wrote a small kernel module that hijacks the PMC memory-mapped register area the kernel mapped and changes the NST_PG_FDIS_1 value: #define SPT_PMC_NST_PG_FDIS_1_OFFSET 0x628 #define SPT_PMC_NST_PG_FDIS_1_BIT_XDCI BIT(24) static void enable_xdci_in_fdis_1(void) {struct pmc *pmc = find_pmc();u32 reg_value = readl(pmc->regbase + SPT_PMC_NST_PG_FDIS_1_OFFSET);pr_err(\"xdci: NST_PG_FDIS_1 is %x\", reg_value);int xdci_bit = !!(reg_value & SPT_PMC_NST_PG_FDIS_1_BIT_XDCI);pr_err(\"xdci: XDCI bit is %d\", xdci_bit);reg_value &= ~SPT_PMC_NST_PG_FDIS_1_BIT_XDCI;pr_err(\"xdci: writing %x to NST_PG_FDIS_1\", reg_value);writel(reg_value, pmc->regbase + SPT_PMC_NST_PG_FDIS_1_OFFSET);reg_value = readl(pmc->regbase + SPT_PMC_NST_PG_FDIS_1_OFFSET);pr_err(\"xdci: NST_PG_FDIS_1 is %x\", reg_value); } Another reason I chose the kernel approach is that I was initially thinking about writing a kernel driver for enabling xDCI. It should, however, be possible to access the PMC memory-mapped registers from userspace via /dev/mem without writing a kernel module. Fail. Unfortunately, resetting the bit failed: xdci: NST_PG_FDIS_1 is 3403ba8 xdci: XDCI bit is 1 xdci: writing 2403ba8 to NST_PG_FDIS_1 xdci: NST_PG_FDIS_1 is 3403ba8 The NST_PG_FDIS_1 value didn’t change after an overwrite attempt. My guess at that point was that the register was locked. This is a typical case with configuration registers: they might be initially writeable, but then they might get locked during boot. Register locked. After searching through the PCH datasheet, I discovered that there was indeed a way to lock the NST_PG_FDIS_1 register via the ST_FDIS_LK bit (Static Function Disable Lock; bit #31) in the ST_PG_FDIS1 register (Static PG Function Disable 1; offset 0x620). Moreover, it was impossible to unlock the register once locked. The bit description said: “Lock control for all ST_PG_FDIS* and NST_PG_FDIS_* registers. Also self-locks when written to 1”. NST_PG_FDIS_1 register and its ST_FDIS_LK bit in the PCH datasheet Checking lock. To confirm that the register was locked, I wrote the code for checking the value of the ST_FDIS_LK bit in ST_PG_FDIS1: #define SPT_PMC_ST_PG_FDIS1_OFFSET 0x620 #define SPT_PMC_ST_PG_FDIS1_BIT_FDIS_LK BIT(31) static void check_fdis_lock(void) {struct pmc *pmc = find_pmc();u32 reg_value = readl(pmc->regbase + SPT_PMC_ST_PG_FDIS1_OFFSET);pr_err(\"xdci: ST_PG_FDIS1 is %x\", reg_value);int lock_bit = !!(reg_value & SPT_PMC_ST_PG_FDIS1_BIT_FDIS_LK);pr_err(\"xdci: FDIS_LK bit is %d\", lock_bit); } The result was: xdci: ST_PG_FDIS1 is 80000040 xdci: FDIS_LK bit is 1 The lock was enabled. Thus, I could not change the value of the NST_PG_FDIS_1 register to enable xDCI in PMC. And I could not remove the lock, as the bit self-locked. This was unfortunate 😢 At that point, I took a break for lunch. Surprise. After coming back to continue, I ran the code again to remember where I was. To my surprise, the lock bit was not set anymore 😮: xdci: ST_PG_FDIS1 is 40 xdci: FDIS_LK bit is 0 “What the hell?” thought I. I rebooted the laptop to see what would happen. The bit was set again 🙃 It took me some time to figure out what was going on 🧐 Suspend. As it turned out, the reason for this was suspend. When I took the lunch break, I closed the lid of my laptop. And the laptop went into suspend. And apparently, the wake-up-from-suspend BIOS code did not set the lock bit; only the boot code did 😅 Do we need a CVE for this or something? 😁 Success. After waking up the laptop from suspend, I ran the code to enable xDCI in PMC again: xdci: NST_PG_FDIS_1 is 3403ba8 xdci: XDCI bit is 1 xdci: writing 2403ba8 to NST_PG_FDIS_1 xdci: NST_PG_FDIS_1 is 2403ba8 And it worked! 😁 Putting the laptop into suspend to enable xDCI in PMC was weird but good enough for me. 🧶 Enabling xDCI in PSF The next step was to enable xDCI in something called PSF. PSF and IOSF. Before reading into the BIOS code responsible for this, I wanted to get at least a high-level idea of what PSF is. This turned out to be non-trivial: the available information about PSF is extremely scarce. The best two sources I managed to find were a talk titled \"Intel VISA: Through the Rabbit Hole\" by Mark Ermolov and Maxim Goryachy and a patent EP2778930A2 titled \"Method and apparatus to trigger and trace on-chip system fabric transactions within the primary scalable fabric\". I spent many hours trying to figure out the relationship between PSF, IOSF-P, and IOSF-SB (these last two are mentioned below), but I cannot say that I fully succeeded. Nevertheless, I tried my best to make this section consistent within my simplified understanding of these terms. « PSF (Primary Scalable Fabric) is an IP unit that interconnects other IP units within the CPU and the PCH chipset. PSF is based on a proprietary Intel specification called IOSF (Intel On-Chip System Fabric).Based on patent EP2778930A2 and Intel VISA: Through the Rabbit Hole Thus, when BIOS disabled xDCI in PSF, the xDCI IP unit likely got disconnected from the system. Also see two articles by Youness Alaoui for notes about IOSF. « According to IOSF, PSF provides two main (non-debug) interfaces: IOSF-P (IOSF Primary) and IOSF-SB (IOSF Sideband). IOSF-P is a fast-speed interface that handles data communication between IP units. IOSF-SB is a low-speed interface used for configuration and error reporting.Based on patent EP2778930A2 and Intel VISA: Through the Rabbit Hole As IOSF-SB is the interface that is used for configuration, BIOS probably disabled xDCI via IOSF-SB. « IOSF-SB can be used to configure PSF and other connected IP units. Each IP unit that can be configured via IOSF-SB is identified by a so-called Port ID.Based on patent EP2778930A2 and Intel VISA: Through the Rabbit Hole Based on this, BIOS likely reconfigured PSF via a corresponding Port ID to disconnect the xDCI IP unit. Having got this high-level understanding, I moved on to reading the BIOS code. In reality, I was going back and forth between reading the BIOS code and trying to find out more details about PSF. But let’s say that everything happened as I describe for a better narrative 😄 xDCI in PSF. To disable xDCI in PSF, BIOS called the PsfDisableXdciDevice function defined in Pch/Library/Private/PeiDxeSmmPchPsfPrivateLib/PchPsfPrivateLib.c: This code is public too. VOID PsfDisableXdciDevice (VOID) { UINT16 RegOffset; if (GetPchSeries () == PchLp) { RegOffset = R_PCH_LP_PCR_PSF2_T0_SHDW_OTG_REG_BASE + R_PCH_PCR_PSFX_T0_SHDW_PCIEN; } else { RegOffset = R_PCH_H_PCR_PSF2_T0_SHDW_OTG_REG_BASE + R_PCH_PCR_PSFX_T0_SHDW_PCIEN; } PchPcrAndThenOr32 ( PID_PSF2, RegOffset, ~0u, B_PCH_PCR_PSFX_T0_SHDW_PCIEN_FUNDIS ); } In PsfDisableXdciDevice, the PchPcrAndThenOr32 call was responsible for reconfiguring PSF via the PID_PSF2 Port ID (0xBB; first argument). Internally, it likely somehow used IOSF-SB. I left figuring out how exactly it does that for later and first looked at the used register and bit offsets. For a Sunrise Point-LP chipset (PchLp series in the snippet), PsfDisableXdciDevice set the B_PCH_PCR_PSFX_T0_SHDW_PCIEN_FUNDIS bit (#8) in the R_PCH_PCR_PSFX_T0_SHDW_PCIEN register (offset 0x1C) from the R_PCH_LP_PCR_PSF2_T0_SHDW_OTG_REG_BASE register area (offset 0x600) within the PID_PSF2 Port ID. Checking datasheet. I looked up this register in the PCH datasheet. As the datasheet was intended for Sunrise Point-H chipsets, the register offset there was 0x21C instead of the expected 0x61C (0x600 + 0x1C). 0x21C did, however, correspond to R_PCH_H_PCR_PSF2_T0_SHDW_OTG_REG_BASE (H, not LP) in the BIOS code. Bit #8 within this register was still described as USB Dual Role (OTG) Function Disable (FunDis), as expected. PSF OTG register and its Function Disable bit in the PCH datasheet PSF2 PCR. Now, I wanted to figure out how PchPcrAndThenOr32 used IOSF-SB to configure the PID_PSF2 Port ID, as I would later need to reproduce this in my kernel module. PchPcrAndThenOr32 turned out to be a wrapper around a PchPcrWrite call, which changed the value of one of the so-called PCRs (Private Chipset Registers): STATIC EFI_STATUS PchPcrWrite ( IN PCH_SBI_PID Pid, IN UINT16 Offset, IN UINTN Size, IN UINT32 InData) { // ... switch (Size) { case 4: MmioWrite32 (PCH_PCR_ADDRESS (Pid, Offset), (UINT32) InData); break; // ... } // ... } Thus, apparently, changing the PSF2 PCR value was what made PCH reconfigure PSF via IOSF-SB. PCR addresses. Based on the BIOS code, the PCRs were memory-mapped, and the address of each of them was calculated as PCH_PCR_ADDRESS(Pid, Offset), where Pid is the Port ID and Offset is the offset to a specific PCR. PCH_PCR_ADDRESS was defined as: #define PCH_PCR_ADDRESS(Pid, Offset) \\(PCH_PCR_BASE_ADDRESS((UINT8)(Pid) << 16)(UINT16)(Offset)) Where PCH_PCR_BASE_ADDRESS was defined as a constant: #define PCH_PCR_BASE_ADDRESS 0xFD000000 ///< SBREG MMIO base address These calculations aligned with the information about the PSF2 PCR registers in the PCH datasheet: « These registers are within the PCH Private Configuration Space, which is accessible through the PCH Sideband Interface. They can be accessed via (SBREG_BAR + Port ID + Register Offset).PSF2 PCR Registers Summary, Intel 100 Series PCH Datasheet, Volume 2 SBREG. Both the BIOS code and the PCH datasheet referred to the memory region that mapped the PCRs as SBREG. Based on the name, I figured that SBREG stands for “Sideband Registers”. Thus, it was logical to assume that overwriting one of the PSF2 PCR registers indeed led to PSF being reconfigured via IOSF-SB; “SB” there stands for “Sideband” too, after all. The next step was to implement changing the xDCI-related PSF2 PCR register in my kernel module. PSF in kernel. I searched through the Linux kernel code to see if it accessed any PSF PCRs. I hoped that I could reuse the kernel mapping of the PSF-related registers, just like I did in the PMC case. Unfortunately, the kernel did not map this area. Thus, I had to implement mapping of the PSF2 PCR register area myself. Mapping PSF2 PCR. The Linux kernel provides an ioremap function for mapping memory-mapped register areas into the kernel virtual address space. This function accepts the physical address and the size of the area to be mapped and returns the virtual address of the mapping. To calculate the physical address of the PSF2 PCR register area, my kernel module needed to know the address of the SBREG region. Here, I had two options: Hardcode the SBREG area address 0xFD000000 taken from the BIOS code. This option had the downside of only working on the systems where SBREG resides at that specific address, like my ThinkPad laptop; Or figure out a way to dynamically determine the SBREG area address. This would be the proper way portable to other systems. I decided to explore the second option first. P2SB. As it turned out, the SBREG area address could be read from the PCIe Configuration Space of the so-called P2SB (Primary-to-Sideband Bridge) device. I failed to find a good explanation of what P2SB is. Based on my understanding, this is a device designed specifically for accessing IOSF-SB. My guess is that its name comes from the fact that accessing memory-mapped I/O or register areas normally happens via IOSF-P, but P2SB converts such accesses to its areas to communication over IOSF-SB. As I learned from the Accessing Intel ICH/PCH GPIOs article by whitequark, P2SB is a PCIe device with the device number 31 (0x1f in hex) and the function number 1. And the address of the SBREG region is stored split across its two Configuration Space registers: SBREG_BAR and SBREG_BARH. By default, the P2SB device is hidden, and all reads from its Configuration Space return 1s. However, it’s possible to unhide it by overwriting the HIDE bit in the P2SBC Configuration Space register. Even when P2SB is hidden, writes to this register still go through. P2SB device and its registers in the PCH datasheet Resetting bit. Thus, to dynamically read out the SBREG area address and calculate the PSF2 PCR address based on that, I had to unhide P2SB first. This seemed like a bit of a hassle, so for now, I decided to just fall back to the simple way of hardcoding the address the same way BIOS did it. Here’s the code I came up with for resetting the xDCI-related bit in the corresponding PSF2 PCR: PCH_PCR_BASE_ADDRESS here is the SBREG address. #define PCH_PCR_BASE_ADDRESS 0xFD000000 #define PCH_PCR_ADDRESS(pid, offset) \\(PCH_PCR_BASE_ADDRESS((u8)(pid) << 16)(u16)(offset)) #define PID_PSF2 0xBB #define PCR_PSF2_T0_SHDW_OTG_REG_BASE0x600 #define PCR_PSFX_T0_SHDW_PCIEN 0x1C #define PCR_PSFX_T0_SHDW_PCIEN_FUNDIS_BIT BIT(8) void enable_xdci_in_psf(void) {void __iomem *regbase = ioremap(PCH_PCR_ADDRESS(PID_PSF2, 0), 0x1000);u32 reg_value = readl(regbase + PCR_PSF2_T0_SHDW_OTG_REG_BASE +PCR_PSFX_T0_SHDW_PCIEN);pr_err(\"xdci: PCR_PSFX_T0_SHDW_PCIEN for OTG is %x\", reg_value);int fundis_bit = !!(reg_value & PCR_PSFX_T0_SHDW_PCIEN_FUNDIS_BIT);pr_err(\"xdci: FUNDIS bit is %d\", fundis_bit);reg_value &= ~PCR_PSFX_T0_SHDW_PCIEN_FUNDIS_BIT;pr_err(\"xdci: writing %x to PCR_PSFX_T0_SHDW_PCIEN\", reg_value);writel(reg_value, regbase + PCR_PSF2_T0_SHDW_OTG_REG_BASE +PCR_PSFX_T0_SHDW_PCIEN);reg_value = readl(regbase + PCR_PSF2_T0_SHDW_OTG_REG_BASE +PCR_PSFX_T0_SHDW_PCIEN);pr_err(\"xdci: PCR_PSFX_T0_SHDW_PCIEN for OTG is %x\", reg_value);iounmap(regbase); } Again, accessing these PCR registers should also be possible from userspace via /dev/mem. When I ran the code, I got: xdci: PCR_PSFX_T0_SHDW_PCIEN for OTG is ffffffff xdci: FUNDIS bit is 1 xdci: writing fffffeff to PCR_PSFX_T0_SHDW_PCIEN xdci: PCR_PSFX_T0_SHDW_PCIEN for OTG is ffffffff Fail. From this, I could see that the kernel likely failed to read the register value. For one thing, the initial value had all bits set, which was suspicious. And then, after I tried to overwrite the value, the read value was still the same. This could mean one of two things. Either the register was completely locked and I could not access it at all. Or the write itself did go through, but I just could not read the register value. Unhiding P2SB. First, I decided to check if unhiding P2SB would make any difference. Even though I wanted to avoid implementing this initially, I had to do it after all. To unhide P2SB, I had to overwrite the HIDE bit (#8) in the P2SBC register (P2SB Control; offset 0xE0) within the PCIe Configuration Space of the P2SB device. P2SB Control register and its Hide Device bit in the PCH datasheet P2SB in kernel. Following the usual path, I checked if the kernel used P2SB to avoid implementing unhiding from scratch myself. Luckily, it did! There were quite a few drivers that unhid P2SB for various purposes. By using the pnd2_edac driver as a reference, I came up with the following code that unhides P2SB and attempts to read the PSF2 PCR: Instead of overwriting just the HIDE bit, this code overwrites the whole byte for simplicity. #define P2SB_DEVFNPCI_DEVFN(31, 1) #define P2SB_HIDE_OFFSET (0xE0 + 1) #define P2SB_READ(size, off, ptr) \\pci_bus_read_config_##size(p2sb_bus, P2SB_DEVFN, off, ptr) #define P2SB_WRITE(size, off, val) \\pci_bus_write_config_##size(p2sb_bus, P2SB_DEVFN, off, val) static void p2sb_unhide(void) {struct pci_bus *p2sb_bus = pci_find_bus(0, 0);P2SB_WRITE(byte, P2SB_HIDE_OFFSET, 0);pr_err(\"xdci: P2SB unhidden\"); } static void p2sb_hide(void) {struct pci_bus *p2sb_bus = pci_find_bus(0, 0);P2SB_WRITE(byte, P2SB_HIDE_OFFSET, 1);pr_err(\"xdci: P2SB hidden\"); } static int __init xdci_enable_init(void) {p2sb_unhide();check_psf();p2sb_hide();return 0; } After running it, I got: xdci: P2SB unhidden xdci: PCR_PSFX_T0_SHDW_PCIEN for OTG is ffffffff xdci: P2SB hidden So, unhiding P2SB did not help: I still could not read the PSF2 PCR. Unhiding is for IOSF-P. Later, I noticed the following note in the description of the HIDE P2SB bit in the datasheet: « When this bit is set, the P2SB will return 1s on any PCI Configuration Read on IOSF-P. All other transactions, including PCI Configuration Writes, are unaffected by this. This does not affect reads performed on the IOSF-SB interface. When I first read this, I didn’t pay attention to the mentions of IOSF-P and IOSF-SB. But now it made sense: unhiding P2SB made no difference, as I was reading the PSF2 PCR through IOSF-SB. Unhiding P2SB was only good for reading the data from the P2SB PCIe Configuration Space, like the SBREG_BAR register or the HIDE bit. Thus, something else prevented me from reading the PSF register. PSF disconnected. After a lot of tinkering, I discovered the reason: PSF was simply disconnected from the Sideband interface. This was done by the RemoveSidebandAccess function defined in Pch/Library/Private/PeiDxeSmmPchInitCommonLib/PchInitCommon.c: This code is public as well. VOID RemoveSidebandAccess (VOID) { UINTNP2sbBase; BOOLEAN P2sbOrgStatus; P2sbBase = MmPciBase (DEFAULT_PCI_BUS_NUMBER_PCH, PCI_DEVICE_NUMBER_PCH_P2SB, PCI_FUNCTION_NUMBER_PCH_P2SB); PchRevealP2sb (P2sbBase, &P2sbOrgStatus); // Disable Sideband access for PSF and other things. MmioOr32 (P2sbBase + R_PCH_P2SB_EPMASK5, BIT29BIT28BIT27 /* PSF */BIT26BIT17BIT16BIT10BIT1); // ... // Lock the EPMASK registers. MmioOr8 (P2sbBase + R_PCH_P2SB_E0 + 2, BIT1); if (!P2sbOrgStatus) { PchHideP2sb (P2sbBase); } } RemoveSidebandAccess did two things besides unhiding and hiding P2SB: Disconnected several Port IDs, including PID_PSF2, via the P2SB EPMASK5 register (offset 0xC4; “Endpoint Mask 5: One hot mask for disabling IOSF-SB endpoint IDs 191-160”); Locked the EPMASK registers via the MASKLOCK bit (“Endpoint Mask Lock: Locks the value of the EPMASK[0-7] registers. Once this value is written to a one, it may only be cleared by a reset.”; bit #17) of the P2SBC register (offset 0xE0). BIT27 in the code corresponds to the 0xBB Port ID, as EPMASK5 counts Port IDs from 160. The MASKLOCK bit wasn’t mentioned in the PCH datasheet that I had, but it was referenced on the Intel website, albeit for a different chipset version. EPMASK5 register in the PCH datasheet Checking disconnect. To confirm my finding, I wrote the code to check the values of the EPMASK5 and P2SBC registers and, just in case, to try overwriting them: #define PID_PSF20xBB #define P2SB_EPMASK5_OFFSET 0xC4 #define P2SB_EPMASK5_PSF2_BIT BIT(PID_PSF2 - 160) #define P2SB_MASKLOCK_OFFSET (0xE0 + 2) #define P2SB_MASKLOCK_BIT BIT(1) void check_epmask(void) {struct pci_bus *p2sb_bus = pci_find_bus(0, 0);u32 epmask5;P2SB_READ(dword, P2SB_EPMASK5_OFFSET, &epmask5);pr_err(\"xdci: EPMASK5 is %x\", epmask5);int epmask5_psf2_bit = !!(epmask5 & P2SB_EPMASK5_PSF2_BIT);pr_err(\"xdci: PSF2 bit is %d\", epmask5_psf2_bit);u8 masklock_byte;P2SB_READ(byte, P2SB_MASKLOCK_OFFSET, &masklock_byte);int masklock_bit = !!(masklock_byte & P2SB_MASKLOCK_BIT);pr_err(\"xdci: MASKLOCK is %d\", masklock_bit);masklock_byte &= ~P2SB_MASKLOCK_BIT;pr_err(\"xdci: writing 0 to MASKLOCK\");P2SB_WRITE(byte, P2SB_MASKLOCK_OFFSET, masklock_byte);P2SB_READ(byte, P2SB_MASKLOCK_OFFSET, &masklock_byte);masklock_bit = !!(masklock_byte & P2SB_MASKLOCK_BIT);pr_err(\"xdci: MASKLOCK is %d\", masklock_bit);epmask5 &= ~P2SB_EPMASK5_PSF2_BIT;pr_err(\"xdci: writing 0 to PSF2 bit\");P2SB_WRITE(dword, P2SB_EPMASK5_OFFSET, epmask5);P2SB_READ(dword, P2SB_EPMASK5_OFFSET, &epmask5);epmask5_psf2_bit = !!(epmask5 & P2SB_EPMASK5_PSF2_BIT);pr_err(\"xdci: PSF2 bit is %d\", epmask5_psf2_bit); } static int __init xdci_enable_init(void) {p2sb_unhide();check_epmask();p2sb_hide();return 0; } The result was: xdci: P2SB unhidden xdci: EPMASK5 is 3c030402 xdci: PSF2 bit is 1 xdci: MASKLOCK is 1 xdci: writing 0 to MASKLOCK xdci: MASKLOCK is 1 xdci: writing 0 to PSF2 bit xdci: PSF2 bit is 1 xdci: P2SB hidden Fail. So, the PID_PSF2 IOSF-SB Port ID was disconnected, the register responsible for this was locked, and I could not unlock it. Thus, I could not enable xDCI in PSF 😥 At that point, I decided that I had sunk enough time into this rabbit hole and moved on to trying a different approach. I nevertheless enjoyed learning about PCH, PMC, and PSF, and I don’t regret the time spent on this 😌 In the BIOS code, RemoveSidebandAccess is called at the end of the PEI stage by PchOnEndOfPei and only if the SbAccessUnlock bit is not set in P2sbConfig. Thus, in theory, it might be possible to unlock IOSF-SB access to PSF2 by somehow changing the value of SbAccessUnlock, putting the laptop into suspend, and then waking it up. I, however, didn’t explore this idea in detail. 💾 Enabling xDCI via NVRAM Having failed to implement a software solution, I wanted to at least figure out a way to enable xDCI without breaking the TPM and, more importantly, without requiring a Boot Guard bypass. Such an approach could be applied to other laptops besides the vulnerable ThinkPad Carbon X1 6th Gen. Avoiding Boot Guard. The change that would piss off Boot Guard was the Advanced page patch I had applied. However, I only needed the Advanced page change the value of the xDCI Support setting. So, perhaps I could somehow enable this setting without unlocking the Advanced page? As the values of BIOS settings are preserved across reboots, they must be stored somewhere. And changing these values through BIOS Setup does not trigger Boot Guard. So, the idea that I had was to try directly changing the value for the xDCI Support setting. For this, I had to figure out where the BIOS settings values are stored. NVRAM. As it turned out, modern UEFI-compatible BIOS stores the values of its settings on the very same SPI chip as the BIOS binary, in the NVRAM region. « NVRAM (Non-Volatile Random-Access Memory) is a random-access memory that retains data without applied power. NVRAM is where BIOS stores its configuration data, including BIOS settings. Legacy BIOS relied on a separate volatile RAM chip that was powered by a CMOS (Complementary Metal–Oxide–Semiconductor) battery when the main power was turned off. In modern UEFI-compatible motherboards, a separate chip is not present. Instead, the configuration data is stored in the so-called NVRAM region on the SPI chip alongside other UEFI data.Based on Non-volatile BIOS memory, Wikipedia Naturally, the value of the xDCI Support setting should be stored in NVRAM as well. So, to change the setting value, I had to figure out where exactly on the SPI chip it was located. IFR. Searching online revealed that a UEFI-compatible BIOS stores the description of its configuration settings in a special format called IFR. « UEFI Internal Form Representation (IFR) is a binary format that the UEFI Human Interface Infrastructure (HII) subsystem uses to store strings, forms, images, animations, and other things that are eventually supposed to end up on the BIOS Setup screen.What is this IFR thing about?, IFRExtractor-RS Thus, my plan was to check IFR to hopefully find the location of the xDCI Support setting. Extracting IFR. I downloaded and built the ifrextractor tool: $ git clone https://github.com/LongSoft/IFRExtractor-RS.git $ cd IFRExtractor-RS $ cargo build And extracted the information about the UEFI configuration settings from the Setup module: $ ./ifrextractor ./Setup.bin all Extracting all UEFI HII form packages using all UEFI HII string packages Inside a produced Setup.bin.1.0.en-US.ifr.txt file, I found the following: VarStoreEfi Guid: 4570B7F1-ADE8-4943-8DC3-406472842384, VarStoreId: 0x5, Attributes: 0x7, Size: 0x75F, Name: \"PchSetup\" OneOf Prompt: \"xDCI Support\", Help: \"Enable/Disable xDCI (USB OTG Device).\", QuestionFlags: 0x10, QuestionId: 0x4D4, VarStoreId: 0x5, VarOffset: 0x40, Flags: 0x10, Size: 8, Min: 0x0, Max: 0x1, Step: 0x0 From this, I could see that the value of the xDCI Support BIOS setting was stored at offset 0x40 within the PchSetup storage area (note the matching VarStoreId). efivarfs. While looking for information about how UEFI stores the values of IFR entries, I stumbled upon efivarfs. « efivarfs is a filesystem in the Linux kernel that enables users to create, delete, and modify UEFI variables. efivarfs is typically and automatically mounted in /sys/firmware/efi/efivars.efivarfs, Gentoo Wiki efivarfs allowed me to verify whether the offset for xDCI Support I had found was correct. Upon checking the efivarfs contents, I saw that there was an entry for each UEFI storage area, including one for PchSetup: $ ls /sys/firmware/efi/efivars/ ... PchSetup-4570b7f1-ade8-4943-8dc3-406472842384 ... Based on the efivars documentation, the first 4 bytes of an efivarfs entry is the header. Thus, the xDCI Support value should have been at the offset 0x44 within the entry for PchSetup. Without xDCI enabled, the value at this offset was 00: $ xxd /sys/firmware/efi/efivars/PchSetup-4570b7f1-ade8-4943-8dc3-406472842384 ... 00000040: 0100 0000 0000 0001 0101 0101 0101 0100 ................ ... But after enabling xDCI via the Advanced page in BIOS, the value changed to 01, as expected: $ xxd /sys/firmware/efi/efivars/PchSetup-4570b7f1-ade8-4943-8dc3-406472842384 ... 00000040: 0100 0000 0100 0001 0101 0101 0101 0100 ................ ... Can’t change from userspace. Out of curiosity, I tried changing the value from userspace by overwriting the /sys/firmware/efi/efivars/PchSetup-... file. The PchSetup-... file had the Immutable attribute set: $ lsattr /sys/firmware/efi/efivars/PchSetup-... ----i--------------- /sys/firmware/efi/efivars/PchSetup-... So, I removed it via sudo chattr -i and then tried to overwrite the file’s contents. Unfortunately, this produced a Read-only file system error. After searching through the Linux kernel code for efivarfs, I found out that this error mapped to EFI_WRITE_PROTECTED being returned from UEFI. Essentially, BIOS rejected the attempt to overwrite PchSetup. Too bad, but not surprising. Interestingly, the Attributes value 0x7 for PchSetup in IFR includes the EFI_VARIABLE_RUNTIME_ACCESS flag (0x4). This should supposedly mean that the PchSetup value can be changed after boot. Thus, it’s unclear to me why changing it via efivarfs fails. I also tried changing the value from EFI Shell via chipsec, but that produced the same error. Patching BIOS. Returning to my initial plan, I wanted to try changing the value of xDCI Support in the NVRAM region without unlocking the Advanced page. Instead of figuring out where the NVRAM region was located on the SPI chip, I took a shortcut. I took the original BIOS image I had read via the SPI programmer (without the Advanced page and the TPM patches) and searched for the 0100 0000 0000 0001 0101 0101 0101 0100 byte sequence (the one at offset 0x40 within PchSetup) with a hex editor. Luckily, this produced only a single hit. Searching for xDCI-related byte sequence I patched the sequence to enable the xDCI-related bit, flashed the image back to the SPI chip, and booted the laptop. Success. Surprisingly, this worked right away! After booting, I could see the xDCI device and the dwc3 driver was loaded 🥳 Done. With this, I concluded my attempts to enable xDCI. The NVRAM patching approach still required reflashing the SPI chip, but at least it didn’t break the TPM and needed no Boot Guard bypass. I decided that this was good enough and moved on. Initially, I patched NVRAM with BIOS version 1.37 and later updated it to 1.61. As the updater preserves the BIOS settings values, I did not have to apply the patch again. 🚀 Using xDCI Once I had xDCI working, I could finally test it with a few USB emulation tools that are based on the Linux kernel USB Gadget subsystem. All of the examples below assume that xDCI is enabled through BIOS, the role is switched to device via intel_xhci_usb_sw-role-switch, and a VBUS-disconnected cable connects the xDCI-enabled port to a USB host. The usage examples listed here are technically not specific to xDCI. Everything would work the same regardless of the used UDC (minding the fact that some UDCs don’t support certain hardware features). Nevertheless, I will show these examples to illustrate the possibilities of using xDCI. 🖱 Legacy gadget drivers The Linux kernel USB Gadget subsystem provides a variety of interfaces for emulating USB devices. Legacy gadget drivers. One of them is the legacy gadget drivers. These are Linux kernel modules that emulate USB devices when loaded. Most of them emulate a USB device of a specific class, like a HID (Human Interface Device) or a mass storage drive. These gadget drivers are called “legacy” because they existed before the introduction of the so-called “composite” framework. Each legacy gadget driver implements a single gadget function (USB class). In turn, the composite framework allows combining multiple gadget functions within the same device. Nowadays, most legacy gadget drivers (except for GadgetFS and Raw Gadget) are just wrappers for the gadget function implementations provided by the composite framework. The g_zero module that I used for the initial xDCI testing is one of the legacy gadget drivers. Mass storage gadget. I decided to try emulating a mass storage drive through xDCI via one of the legacy gadget drivers. For this, the USB Gadget subsystem offers the g_mass_storage module. This module accepts an argument that specifies the filesystem image that will be exposed to the USB host. Once loaded, this module emulates a USB drive through the first UDC it finds on the system (just like g_zero). I prepared a floppy disk–sized FAT image: $ dd if=/dev/zero of=disk.img bs=512 count=2880 $ mkfs.fat ./disk.img $ sudo mkdir /mnt/drive $ sudo mount ./disk.img /mnt/drive -o loop $ echo \"Hi from xDCI\"sudo tee /mnt/drive/file.txt $ sudo umount /mnt/drive And loaded the g_mass_storage module with the created disk.img as the file argument to start emulating a USB mass storage device: $ sudo modprobe g_mass_storage file=./disk.img stall=0 Worked. The emulation worked just as expected. The drive got auto-mounted on the USB host, and I could see the contents of file.txt 🥳 Emulating mass storage drive via xDCI A nifty way to transfer files from an xDCI-enabled laptop in case you forgot to bring a USB drive 😄 To stop emulating the USB drive, unload the module: $ sudo modprobe -r g_mass_storage It should also be possible to emulate a USB device with multiple different classes by using the composite framework with xDCI. However, I didn’t try this myself, so I won’t provide a usage example. 🤖 Raw Gadget The next thing I wanted to test was Raw Gadget. As I mentioned in the introduction, Raw Gadget is a Linux kernel module for emulating highly customizable USB devices. Technically, it’s one of the legacy gadget drivers (as it’s not based on the composite framework), but it provides much greater flexibility than emulating a USB device with a specific class. Running Raw Gadget with xDCI for the first time was very exciting, as my desire to work on Raw Gadget on my laptop without external hardware was what conceived this project 😄 Keyboard. I downloaded, built, and loaded the Raw Gadget module on my laptop: $ git clone https://github.com/xairy/raw-gadget.git $ cd raw-gadget/raw_gadget/ $ make $ ./insmod.sh And then built and ran the example to emulate a keyboard via xDCI: $ cd raw-gadget/examples/ $ make $ sudo ./keyboard dwc3.1.auto dwc3-gadget Fail. Unfortunately, the emulation failed. The emulation code was not able to complete the enumeration process and froze when handling the SET_CONFIGURATION USB request 😢: $ sudo ./keyboard dwc3.1.auto dwc3-gadget event: connect, length: 0 ... event: control, length: 8 bRequestType: 0x0 (OUT), bRequest: 0x9, wValue: 0x1, wIndex: 0x0, wLength: 0 type = USB_TYPE_STANDARD req = USB_REQ_SET_CONFIGURATION ep0: ep_int_in enabled: 1 ep0: spawned ep_int_in thread ep0: transferred 0 bytes (out) # Froze here... Click the switch to see the full log. SET_CONFIGURATION is a USB request that the USB host sends to the USB device once the device completes the enumeration process (the process during which the host asks the device for its descriptors to know what kind of device was plugged in). Issue. After debugging and a lengthy discussion with the Linux kernel maintainers, I figured out the issue. As it turned out, dwc3 and several other UDC drivers incorrectly used the Gadget subsystem API. They assumed that a 0-length control request like SET_CONFIGURATION can be acknowledged immediately without the gadget driver’s confirmation. And Raw Gadget did not expect that. I won’t go deep into explaining the issue, as this would require covering a lot of the USB protocol and the Gadget subsystem internals. If you’re interested in the details, see the discussion and the documentation patch. Fix. A proper fix for this issue would be to change the behavior of all the affected UDC drivers. However, after looking into this, I decided that fixing all of them would require quite a lot of work. Both fixes are now in the mainline kernel. So, instead, I just applied a workaround patch to Raw Gadget. And I noticed that GadgetFS also suffered from this issue, so I fixed it up as well. Success. Once I applied the workaround, the keyboard was emulated successfully 🥳: $ sudo ./keyboard dwc3.1.auto dwc3-gadget event: connect, length: 0 ... event: control, length: 8 bRequestType: 0x0 (OUT), bRequest: 0x9, wValue: 0x1, wIndex: 0x0, wLength: 0 type = USB_TYPE_STANDARD req = USB_REQ_SET_CONFIGURATION ep0: ep_int_in enabled: 1 ep0: spawned ep_int_in thread ep0: transferred 0 bytes (out) event: control, length: 8 bRequestType: 0x80 (IN), bRequest: 0x6, wValue: 0x303, wIndex: 0x409, wLength: 255 type = USB_TYPE_STANDARD req = USB_REQ_GET_DESCRIPTOR desc = USB_DT_STRING ep0: transferred 4 bytes (in) ... event: control, length: 8 bRequestType: 0x21 (OUT), bRequest: 0x9, wValue: 0x200, wIndex: 0x0, wLength: 1 type = USB_TYPE_CLASS req = HID_REQ_SET_REPORT ep0: transferred 1 bytes (out) ep_int_in: key down: 8 ep_int_in: key up: 8 ep_int_in: key down: 8 ep_int_in: key up: 8 ep_int_in: key down: 8 ep_int_in: key up: 8 ... Result of running Raw Gadget keyboard example with xDCI From then on, I could run Raw Gadget directly on my laptop. No more need for Raspberry Pis or EC3380-AB. Yay! 😄 Once I had Raw Gadget working, I moved on to testing a few Raw Gadget–based tools. 🧰 syzkaller One of the tools that rely on Raw Gadget is syzkaller — a coverage-guided kernel fuzzer. You can check out my Looking for Remote Code Execution bugs in the Linux kernel article for its overview. USB fuzzing. syzkaller uses Raw Gadget for externally fuzzing the Linux kernel USB stack. Initially, I developed Raw Gadget specifically for its use in syzkaller. With xDCI, I could run syzkaller directly on my laptop to fuzz an external USB host. However, syzkaller heavily relies on code coverage feedback to guide its fuzzing process. While collecting coverage from an external system would theoretically be possible to implement, I didn’t want to get into this. USB reproducers. Instead, I wanted to try running a reproducer for one of the USB bugs syzkaller found. You can check the syzbot dashboard to view all of the reported and fixed USB bugs that syzkaller discovered in the Linux kernel. Patching syzkaller. By default, syzkaller relies on the Dummy HCD/UDC module for fuzzing USB. « Dummy HCD/UDC is a module that sets up virtual USB Device and Host controllers that are connected to each other inside the kernel. This module allows connecting USB devices emulated from userspace through any of the Gadget subsystem interfaces (Raw Gadget, GadgetFS, etc.) directly to the underlying kernel.Dummy HCD/UDC Kernel Module, Raw Gadget repository As I wanted to make syzkaller use xDCI instead, I patched the code that specifies the used UDC: diff --git a/executor/common_usb_linux.h b/executor/common_usb_linux.h index b706663f8..a0ce15293 100644 --- a/executor/common_usb_linux.h +++ b/executor/common_usb_linux.h @@ -303,9 +303,7 @@ static volatile long syz_usb_connect_impl(uint64 speed, uint64 dev_len, const ch// TODO: consider creating two dummy_udc's per proc to increace the chance of// triggering interaction between multiple USB devices within the same program. - char device[32]; - sprintf(&device[0], \"dummy_udc.%llu\", procid); - int rv = usb_raw_init(fd, speed, \"dummy_udc\", &device[0]); + int rv = usb_raw_init(fd, speed, \"dwc3-gadget\", \"dwc3.1.auto\");if (rv < 0) { debug(\"syz_usb_connect: usb_raw_init failed with %d\", rv); return rv; Running reproducer. I chose to run one of the reproducers for the WARNING in smsusb_start_streaming/usb_submit_urb bug: Program split into multiple lines for readability. syz_usb_connect(0x0, 0x36, &(0x7f0000000000)=ANY=[@ANYBLOB=\" 12010000d39262087f180002311f00000001090224000100000000 09040000024b92160009050ba32b00000000090582ff00ffff0000\"], 0x0) For running reproducers, syzkaller provides a syz-execprog tool. To avoid having to run syz-execprog as root, I changed the permissions on the /dev/raw-gadget virtual device file Raw Gadget exposes to userspace: $ sudo chown user:user /dev/raw-gadget Then, I built syz-execprog and ran the reproducer: $ make execprog executor $ cd ./bin/linux_amd64/ $ ./syz-execprog -enable=usb repro.prog See the syzkaller documentation on how to set it up. Bug triggered. This worked without any issues, and the reproducer triggered the WARNING on the laptop with the 5.15.0-91-generic Ubuntu kernel I used as the host 🥳: ------------[ cut here ]------------ usb 3-9: BOGUS urb xfer, pipe 3 != type 1 WARNING: at drivers/usb/core/urb.c:502 usb_submit_urb+0x473/0x6d0 ... Call Trace: smsusb_submit_urb+0x7e/0xb0 [smsusb] smsusb_start_streaming+0x39/0x82 [smsusb] smsusb_init_device+0x368/0x426 [smsusb] smsusb_probe+0x2ce/0x31f [smsusb] usb_probe_interface+0xeb/0x2b0 ... hub_event+0x1eb/0x430 process_one_work+0x228/0x3d0 worker_thread+0x53/0x420 kthread+0x127/0x150 ret_from_fork+0x1f/0x30 ---[ end trace 6c8fe15158d398c6 ]--- Click the switch to see the full WARNING log. This particular bug is just a WARNING caused by the smsusb driver missing validation for USB endpoint types; nothing harmful. 💃 Facedancer Another thing I wanted to try with Raw Gadget and xDCI is running Facedancer directly on my laptop without any external hardware. Facedancer is a Python framework for emulating USB devices via a range of hardware solutions, including Great FET and not-yet-released-but-highly-awaited Cynthion. Do not confuse the Facedancer software with the Facedancer21 hardware board. The latter is a specific board for emulating USB devices developed many years ago by Travis Goodspeed. Facedancer21 was what enabled me to write my very first Linux kernel exploit and also set me on the path of fiddling with USB security 😌 Originally implemented by Kirill Zhirovsky. Backend. For running Facedancer with Raw Gadget, there’s a prototype of a Raw Gadget–based backend. This prototype relies on a few out-of-tree Raw Gadget patches, but it already works! I’m yet to submit the backend to the mainline Facedancer repository. Before upstreaming, I need to finalize the Raw Gadget patches, and I’ll also likely first wait for the 3.0 Facedancer release. Rubber Ducky. To test Facedancer with Raw Gadget and xDCI, I rebuilt and reloaded the Raw Gadget module with the out-of-tree patches, and ran the rubber-ducky.py example: $ sudo chown user:user /dev/raw-gadget $ export BACKEND=rawgadget $ export RG_UDC_DRIVER=dwc3-gadget $ export RG_UDC_DEVICE=dwc3.1.auto $ ./examples/rubber-ducky.py INFOrubber-duckyBeginning message typing demo... INFOrawgadgetgadget resumed INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=12 blocking=False INFOrawgadgetgadget resumed INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=12 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=9 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=22 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=4 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=30 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=16 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=14 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=0 blocking=False INFOrawgadgetep_enable: rv=5 INFOrawgadgetconfigured INFOrawgadgetsend_on_endpoint: ep_num=3 len(data)=a blocking=False INFOrawgadgetsend_on_endpoint: ep_num=0 len(data)=14 blocking=False INFOrawgadgetsend_on_endpoint: ep_num=3 len(data)=a blocking=False ... The example worked without a hitch 🥳 Result of running Facedancer Rubber Ducky example with Raw Gadget and xDCI Yay. By itself, Raw Gadget allows running Facedancer on Linux-based boards (minding the Raw Gadget limitations), which is quite cool. But combined with xDCI, it’s possible to run Facedancer directly on a PC without any external hardware at all. Insane! 😱 Another Raw Gadget—based tool I successfully ran with xDCI was USB Proxy by Aristo Chen. I won’t provide the instructions here, as the article is already quite long. With xDCI, this tool allows turning the laptop into a USB sniffer or a USB Man-in-the-Middle attack tool. 🗃 Summary ThinkPad X1 Carbon 6th Gen emulates a USB device for ThinkPad X1 Carbon 10th Gen via xDCI Introduction. While working on Raw Gadget, I noticed a suspicious /sys/class/usb_role/intel_xhci_usb_sw-role-switch file on my ThinkPad X1 Carbon 6th Gen laptop. Finding this file seemingly implied that the xHCI device on my laptop could be switched into the device mode and be used to emulate USB devices. Investigation. Upon checking the Linux kernel source code and reading through a related mailing list discussion, I found out that some Intel systems indeed support enabling the so-called xDCI controller for emulating USB devices. On my laptop, this controller was not present in the list of PCIe devices, but there was a hidden xDCI Support setting in BIOS. Enabling xDCI. Then, I took three approaches to try enabling xDCI: Successfully via unlocking the Advanced BIOS settings page and changing the value of xDCI Support. This required reflashing the SPI chip on my laptop and patching BIOS to unlock the Advanced page and bypass Boot Guard. Once I enabled xDCI, I managed to find the xDCI-enabled port on my laptop, make a custom VBUS-disconnected USB cable, and successfully emulate a testing USB device through xDCI; Unsuccessfully via PCH. Within this approach, I tried to reconfigure the PCH registers to enable xDCI. I managed to reconfigure PMC by relying on missing register locking on the wake-up-from-suspend path. However, I failed to reconfigure PSF, as PSF was disconnected from IOSF-SB by BIOS during boot; Successfully via patching NVRAM. For this, I patched the xDCI Support value in NVRAM on the SPI chip directly. This still required reflashing the SPI chip but did not need a Boot Guard bypass. Thus, this approach should be portable to other systems besides my laptop to enable xDCI. Using xDCI. In the last part, I showed a few usage scenarios for xDCI: First, I demonstrated how to use the g_mass_storage legacy gadget driver to emulate a USB mass storage drive; Then, I managed to get Raw Gadget working with xDCI and used them to emulate a USB keyboard. This required applying a workaround patch to Raw Gadget to make it compatible with the buggy dwc3 driver used for xDCI; After that, I successfully ran a reproducer for one of the syzkaller-found bugs to emulate an improper USB device through xDCI and triggered a WARNING on a Linux kernel host; Finally, I showed how to use the Raw Gadget–based Facedancer backend with xDCI to emulate a USB keyboard via the rubber-ducker.py Facedancer example. Emulating USB devices in all shown examples required no hardware besides my laptop. 📝 Afterword Motivation. Even though this project was a bit of a spin-off from my usual expertise areas, I really enjoyed working on it and learning about BIOS and PCH internals in particular 😄 Once I found that USB role-switching file on my laptop, I could not stop thinking about the possibility of it being functional. Luckily, it indeed was 😊 Managing to turn a commonly-used laptop into a programmable USB device feels like a good accomplishment 🥳 Other machines. I specifically targeted my ThinkPad X1 Carbon 6th Gen laptop where I found the file, but I suspect enabling xDCI should also be possible on other PCs. In the simplest case, this might be as easy as turning on xDCI in BIOS settings. This should just work if there’s proper ACPI and role-switching support and the xDCI-enabled port is wired to the external casing. As for other ThinkPads in particular, enabling xDCI should also not be a problem. Although, this will likely require reflashing the SPI chip, as the xDCI BIOS setting is hidden. But at least, with the NVRAM approach, this will not require a Boot Guard bypass. I briefly looked at the ThinkPad X1 Carbon 10th Gen laptop but failed to find the SPI chip. Perhaps the chip is located on the bottom side of the motherboard, which I did not check. Note that the SPI chip on some modern systems might be protected from reflashing via RPMC (Replay Protected Monotonic Counter). The chips that support this feature have “R” in their name, like “W25R128FW”. Software approaches. I also believe it should be possible to enable xDCI purely via software. Even though I failed to do it by reconfiguring PCH, there are other approaches. One of them would be to exploit a BIOS bug to flip the xDCI-related NVRAM variable. For example, this can likely be done by exploiting the LogoFAIL vulnerability found by the Binarly team. Another idea that comes to mind is using DMA to attack BIOS during boot. This way, it might be possible to execute arbitrary code within BIOS and enable xDCI. Even though some modern systems protect BIOS from DMA, others might be vulnerable. Clickbait. Using the word “secret” in the title was a mere attempt at clickbait 😌 I don’t have any reason to believe that Lenovo deliberately keeps the knowledge about xDCI functionality private. It’s likely rather just an undocumented feature. Acknowledgements. Many thanks to those who willingly or unknowingly helped me with this project: to Dmitry Mikushin, Heikki Krogerus, and Felipe Balbi for following through with the mailing list discussion about xHCI/xDCI role switching; to Sergey Korablin for helping me with soldering and taking hardware pictures; to Maxim Goryachy for suggesting the PCH approach, answering my countless questions about BIOS, and giving feedback on the article; to Alan Stern and Thinh Nguyen for looking into the dwc3 issue I reported and reviewing the Raw Gadget workaround; to Alexander Popov for reviewing the article; and to everyone else whose work is referenced in this article 💜 While I failed to implement a software-only approach, I’m very happy with the result. Now, I can run Raw Gadget on my laptop without having to deal with external hardware 😄 💜 Thank you for reading! 🐱 About me I’m a security researcher and a software engineer focusing on the Linux kernel. I contributed to several security-related Linux kernel subsystems and tools, including KASAN — a fast dynamic bug detector, syzkaller — a production-grade kernel fuzzer, and Arm Memory Tagging Extension — an exploit mitigation. I also wrote a few Linux kernel exploits for the bugs I found. Occasionally, I’m having fun with hardware hacking, teaching, and other random stuff. Follow me @andreyknvl on Twitter, @xairy@infosec.exchange on Mastodon, or @xairy on LinkedIn for notifications about new articles, talks, and training sessions.",
    "commentLink": "https://news.ycombinator.com/item?id=39470381",
    "commentBody": "I turned my ThinkPad into a programmable USB device (xairy.io)294 points by true_pk 16 hours agohidepastfavorite79 comments aendruk 14 hours agoOne thing I’ve always wondered is why it’s not common to be able to use a laptop as a generic keyboard and monitor for another computer. For doing maintenance on a headless machine it feels silly to go buy a keyboard and monitor when my laptop already is those. reply mrb 2 hours agoparentAs someone who often deals with headless machines, the best solution I have found for using my laptop as a monitor is this: https://www.amazon.com/dp/B0BJ2YDV7Q It's basically a chip to capture the HDMI signal from a headless machine, and expose an emulated webcam via USB to your laptop. It's so tiny the electronic is basically built into the cable. For the keyboard I haven't found a good solution other than buying a standard wireless keyboard (with integrated trackpad, for the rare cases where a mouse is needed). reply btgeekboy 40 minutes agorootparentThat's basically what I do as well. I combined a similar HDMI dongle with a Logitech K400 Plus wireless keyboard+trackpad combo. It's an absolutely terrible membrane keyboard for everyday use, but having the trackpad and keyboard in the same device, using the same single dongle that has a storage slot in the keyboard makes things really easy for light, occasional use. reply RCitronsBroker 44 minutes agorootparentprevThats genius reply nfriedly 12 hours agoparentprevThe GPD Pocket 3 has a KVM module that lets you do exactly this: it has HDMI & USB-C inputs and then it acts as a display, keyboard, and mouse for whatever machine it's connected to: https://gpd.hk/gpdpocket3 There's also the up-and-coming Minisforum V3 tablet that has a video input so that it can act as an external monitor, but I don't think it'll be able to share it's keyboard and mouse. I agree with you, though, it really would be nice if this were a more common feature. reply bodge5000 9 hours agorootparentNot much of a hardware guy so don't know if this is at all plausible, but it'd be nice to see that as an extension port on the framework laptops. That surely is the advantage of having modular ports. This is a fairly niche application so I can see why it's not a standard option, but for those who want it it could be a real game changer. reply pbronez 8 hours agorootparentThat’s a neat concept. Are there any products that take a video input and USB A connection, combine into USB C and act as a KVM? I can’t even think what that would be called, TBH. It’s hard to search for. reply Zircom 8 hours agorootparentSomething like this maybe? Can use USB C or HDMI+USB A depending https://www.amazon.com/Dopesplay-Wireless-Portable-Touchscre... I've had some success searching \"lapdock\" as well, they're marketed towards using them with smartphones it looks like but can't see why you couldn't plug it into a server or computer as well. reply xeonmc 8 hours agorootparentprevIt would be even better if Framework laptops could do that since they’re all about parts reuse. reply myself248 14 hours agoparentprevSeriously. A friend of mine built a cyberdeck specifically for this -- his mouse and keyboard can be switched to an external port and then cabled to another machine, and there are two HDMI ports on the side, one output and one input, with an internal switch to select either the deck's own machine, or the external input, as the source for the deck's monitor. reply WillAdams 13 hours agorootparentHas your friend done a write-up or parts list or build? I think this would be a big hit on Hackaday or some similar site. reply speed_spread 11 hours agorootparentAn OrangePi 5 Plus would be a good starting platform for this because of the HDMI input port. reply myself248 9 hours agorootparentOooo. reply RockRobotRock 6 hours agorootparentprevwhat?? Can you run PiKVM on it? reply ThrowawayTestr 14 hours agoparentprevThe inability to simply connect one computer to another has always bothered me. Apparently there's a mode in USB 4 which will allow this. reply ssl-3 10 hours agorootparentThere was a time when FireWire did that kind of thing -- not specifically with remote displays, but providing a high-bandwidth (for the time) wired network link between two PCs. At home, in the Ye Olde 10/100 Ethernet days, sometimes I plugged my laptop into my desktop with 400Mbps FireWire to transfer big files. No special cable required; any dumb FireWire cable with the right connectors on each end (\"big\" for the desktop, \"little\" for that particular laptop) worked just fine between any two FireWire devices. reply LoganDark 3 hours agorootparentYou talking about Target Display Mode? Or did that never support FireWire? reply ssl-3 2 hours agorootparentI think Display Target Mode only worked with Thunderbolt, which was different. I just used Firewire (er... maybe IEEE-1394, since it was in PC space) as a temporary and fast network connection for running (eg) IP stuff and file transfers with SMB, but it could have been used permanently. At the time, plugging a Firewire cable in betwixt two Windows machines generated this network automatically, with RFC 3330 addresses for each end. It just worked; each machine could be addressed by name nearly instantly after connection, and whatever file shares they had available could then be accessed with the same mechanisms that would be used on the normal Ethernet LAN. I never did anything particularly complex with it. reply zokier 1 hour agorootparentThat's how it works with USB too these days: https://news.ycombinator.com/item?id=39003469 reply RockRobotRock 6 hours agorootparentprevLinus showed off a really impressive tech demo which accomplishes that: https://www.youtube.com/watch?v=GqCwLjhb4YY I would link another source if I can, but there's almost nothing I could find about Thunderbolt Share. reply rusk 13 hours agorootparentprevRS232 null modem ftw! reply RunSet 5 hours agorootparentprevEthernet has provided it all along. https://en.wikipedia.org/wiki/Ethernet_crossover_cable reply bmicraft 4 hours agorootparentYeah but then you're limited to Ethernet speeds, which (if even present on a modern laptop) is often 1/20th the speed of its fastest USB port. I just want to image the SSD without disassembling the laptop, damnit reply mixmastamyk 6 hours agorootparentprevI just did this but had to buy a special tb4 cable. Just worked, though I had to look up the IP addresses. reply RunSet 5 hours agoparentprevhidclient will let your laptop pretend to be bluetooth keyboard and mouse. https://web.archive.org/web/20180610141436/http://anselm.hof... https://github.com/benizi/hidclient reply boneitis 1 hour agorootparentThanks for posting this. It is pretty much solving a major pain point for me (as is TFA, seemingly on a brief skim), even if with some usability friction. I have a Flipper Zero and have always fantasized about having some sort of VMware/VirtualBox-like application that operates in a capture mode to forward all inputs to another device (similarly as to the guest machine in the VM player) with a keyboard shortcut to release the HIDs back to the host. In the Flipper Zero's case (or any SBC/SOC/microcontroller/whatever-term-is-appropriate), it would be so lovely to be able to operate in all four combinations of: Host - BT - Flipper - BT - Second device Host - USB - Flipper - BT - Second device Host - BT - Flipper - USB - Second device Host - USB/GPIO - Flipper - USB/GPIO - Second device Unfortunately, I don't work so close to the hardware, so I've no idea where to even begin with this. A man can dream. Sigh. reply molave 12 hours agoparentprevMy laptop's monitor went bust and now serves as a glorified CPU. reply justsomehnguy 7 hours agoparentprevCosts, extremely low demand, no standard interconnect and interfaces, anything non-standard wouldn't work with dead OS, costs. reply aaronblohowiak 4 hours agorootparentCan’t it all be acccomplished in Software with usb-c? reply leonheld 14 hours agoparentprevEveryone uses ssh to do that. reply calamari4065 12 hours agorootparentAnd what do you do when your ssh server stops working, or there's a hardware fault, a network fault, a failed OS upgrade? reply filoleg 11 hours agorootparentIdk about all machines ever, but I remember being able to access ssh over a serial port. Though in that specific case for me, it was more of a necessity rather than just a quirky way of using ssh (due to the “machine” just being a dev board with a UART ESP32). reply calamari4065 9 hours agorootparentTo access the serial port remotely you need to add hardware to bridge it to the network. Or you go up to the machine and plug in another computer with a keyboard and monitor to access it directly. reply aendruk 14 hours agorootparentprev…except for when they can’t. The relevant retort would be that I could instead be using hardware with a remote management interface like IPMI, and building a custom OS installation image with remote access preconfigured. But even then I’m not sure how you’d have me troubleshoot surprise network issues. reply dingnuts 14 hours agorootparentprevThat's a completely different use-case. SSH doesn't let me control the GUI on the remote And no, X11 forwarding is not what I mean either reply bee_rider 14 hours agorootparentThere is/was a tool called x2x, which somehow let you send your input (mouse/keyboard) to another machine (I think via X forwarding tricks, although I don’t know how it worked, so I could be wrong). I tried it around a decade ago and at the time it felt a bit abandoned. I vaguely remember that it might not have worked with every window manager? Anyway, I assume it hasn’t got better supported since then. But it did feel magical. reply ssl-3 10 hours agorootparentx2x worked fine for input devices. Later, things like x2vnc made the idea more cross-platform (X on the local unix-like box, VNC on some other platform), but only with two machines. After that, Synergy became a thing, and supported many machines, but then they eventually went to a model that tended to require payment. Later, Barrier forked from Synergy, and it allows much of the same functionality. It's still very free, and it still works, but it's kind of abandoned. Today, there's Input Leap. It is (naturally) a fork of Barrier, and it is still sees regular development. https://github.com/input-leap/input-leap reply jeffbee 14 hours agorootparentprevSo, you'll accept any solution, except the existing ones that work? XDMCP solves this exact issue. You use one device which becomes virtually an attached terminal of another device which is actually headless and with no input devices of its own. We've had this for 30+ years. reply pessimizer 13 hours agorootparentNo, we already use the solutions that actually work, and are speculating why there isn't one with the simple, obvious user experience. Why shouldn't we? reply exe34 11 hours agorootparentprev> XDMCP muh wayland reply pierat 7 hours agorootparentI'm sure we'll see functionality like that in Wayland.... In a decade or 2. reply controversial97 13 hours agoprevThis reminds me of a thing from a few years ago, my vague memory is that it was by Travis Goodspeed. A smart TV would accept a firmware update from a file on a standard USB stick. The TV reads the file start to finish to check the digital signature then reads it again to update. A device that is pretending to be a USB storage device can send a manufacturer firmware file the first time then send your unofficial firmware after. reply zrail 11 hours agoparentNice. This is a class of error with the delightful acronym TOCTOU (Time of Check to Time of Use) which is present in an astonishing number of places. https://en.m.wikipedia.org/wiki/Time-of-check_to_time-of-use reply TeMPOraL 10 hours agorootparentNo surprise, those are near-impossible to avoid. I mean, a simple: if(check(resource)) { use(resource); } is already vulnerable, unless you somehow make the entire piece of code run atomically. reply X-Cubed 9 hours agorootparentOne approach is to copy the data to a location that you control first (eg: RAM) and perform both the check and the use of that data from the trusted location. That can be difficult though on embedded devices with constrained resources. reply crtasm 9 hours agorootparentprevFor a firmware update: read it into memory or copy it to storage you control first. reply hypercube33 9 hours agorootparentprevCorrect me if I'm wrong but I think that's how Gameboy carts show custom logos bypassing the Nintendo copyright check thing - one logo to pass the internal check and another for display. reply steelbrain 14 hours agoprevThis is really interesting. I've been researching some viable options like this. I have a beefy Network Attached Storage (NAS) server that I actively look for excuses to make use of. I have connected with some 40g and 10g interconnects for it across the house. I also have a PS5 & XBox that use USB hard-drives for additional storage. I looked into whether I could expose my NAS storage to PS5 & XBox. Turns out that it's possible! It's possible by mounting the NAS shares over iSCSI or NFS, and then emulating a USB storage device using the g_mass_storage module that exposes said storage to USB hosts. Besides time and cost, one major blocker for me right now is the bandwidth that such a system would provide. It's just not that big of an upgrade. Raspberry Pi very famously supports USB-OTG (similar/same as xDCI), but it only does so with USB 2.0 speeds, and so do all the other SBCs in that class that I found, except for one, RockPi4. RockPi has a Gigabit ethernet port, so if you max out ethernet, you can provide stable HDD speeds to both the PS5 & Xbox. It would be really interesting to explore a solution where you have the ability to plug in a custom PCIe network card (or an express card) that lets you go beyond 1gbe because then you can really saturate the USB 3.0 interface. reply moondev 8 hours agoparentI have a similar homelab setup and have been playing with USB over IP via https://www.virtualhere.com It does require a client to talk to the server but seems to work pretty well. My thought is to setup a \"thin USB client\" on my desk that I can attach USB devices to which then are attached or switched to whatever metal or VM host I want in my lab. reply CTDOCodebases 6 hours agoparentprevHow are you physically connecting the PS5 to the NAS? I’m guessing NAS->SBC->PS5. Or are you connecting the NAS to the PS5 directly with a USB cable? Also how does content play on the PS5? I know the internal NVMe has support a certain speed. Does this not apply to external drives? reply chatmasta 10 hours agoparentprevRaspberry Pi 5 has a gigabit ethernet port, as do a number of other SBCs, like the NanoPi variants from FriendlyElec for example. In fact the NanoPi R6S has two 2.5G ethernet ports in addition to a 1 gigabit port. reply evanjrowley 15 hours agoprevI'm grateful for this extremely detailed article. Great quality here. The fact that this has been achieved gives me hope that a networkless Synergy / Mouse Without Borders setup might be possible using just a modified USB cable. That's great for people like me who need to operate multiple computers simultaneously. reply taskforcegemini 14 hours agoparentthere is kvm switches (or just usb km without the video), but your suggestion sounds intriguing reply erik 14 hours agoprevThis goes to an impressive level of depth. The ability to use a PC as a USB device opens up lots of fun possibilities. It's a little bit tragic that the required xDCI option is there in the hardware on this device, but it's not exposed and requires firmware hacking to access. The hardware is capable, but the vendor has just turned it off and locked away the controls. reply PennRobotics 2 hours agoprevI wonder if Lenovo is using this as a path to debugging one ThinkPad using another: https://www.intel.com/content/www/us/en/developer/articles/t... --- Update: probably. https://www.youtube.com/watch?v=ExUvQa_jB7c (Debugging Intel Firmware using DCI & USB 3 0) reply lathiat 7 hours agoprevI was looking into similar recently just wanting to do 10Gbps between two USB-C machines over USB with just a USB-C cable. Unfortunately most Ryzen boards still don't have Thunderbolt (which supports this natively and easily). And \"dual role\" USB controllers were quite uncommon (or as this article details, possible but not supported) until USB 3.2/4.0 - and even then it seems a bit hit and miss. It's totally crazy to me it's not easier to network two machines with a USB-C cable (without Thunderbolt). I also love the part about the lock bit being cleared after a suspend, that seems to be such a common issue.. super common way to get SATA devices out of the state preventing secure erase for example. reply stragies 10 hours agoprevI have a Celeron N3350/Pentium N4200/Atom E3900 based system, that exposes an xDCI switch in the bios, and upon enabling it, I do get some other options, and upon bootup into linux a PCI device \"USB controller: Intel Corporation Device 5aaa (rev 0b)\" But the UDC device node is not showing up. So it seems I have to dig a bit further using inspiration from the OP reply Topgamer7 14 hours agoprev> However, dealing with a Raspberry Pi is a hassle: plugging in the wires, booting the board, accessing the shell, etc Probably just could have configured USB OTG, so its powered by the device you plug in, enable shell, then scripted this. Then you can configure mounts over ssh, commands over ssh, probably even mount the device. That being said, I encourage people to try to understand their devices, read the kernel, etc. Its something I strive to continually improve. Lets fix our own shit :D. reply lima 13 hours agoparentThe whole point of the exercise is playing with USB OTG, and the Pi has only one OTG port, so you need another way to connect to it (been there, done that) reply k8svet 13 hours agorootparentThe \"Zero\" variants of boards tend to allow power over the OTG port, so you can do some interesting things. (Of course, annoying if your powering-device resets power to it). Particularly, if I didn't suck at DTS/u-boot stuff, you could make a PiKVM for <$75. (You can get $10 1080p/60fps HDMI capture cards, you use the OTG port for kb/mouse). Still, even if you had to external power it, those little boards have pins for power in. It would still be cheap/fairly elegant. And or a USB-C Hub should power it, and give you a place to put more non-host usb devices like a usb-eth dongle. I would like to make one with some of the (Raspberry|Orange) Pi 3, but again, I've lost too much of my life to dts/u-boot stuff. BSP/mainline woes. It's just easier to be a software-only, cloud-machines-only guy and live minimally. This article was exciting because I'd love to imagine cheap old laptops being salvagable as little KVM instances, but it seems like OTG/xDCI is just not a priority. reply freedomben 14 hours agoprevThis reminds me so much of the early internet hacker culture that it's actually triggering nostalgia. What a great write-up! When ESR talked about becoming a hacker (hacker in the Hacker News sense), this is the type of person he was talking about. The combination of curiosity, dedication, and technical ability is exceedingly rare, and majorly impressive. Thanks so much for sharing! reply lights0123 10 hours agoprevI was recently pleasantly surprised by a Linux utility that can list all UEFI variables stored in Hii and change them: https://github.com/linuxboot/uefisettings/ Obviously can't bypass security settings like this laptop has, but it's useful for a few settings in my case. reply urbandw311er 9 hours agoprevWow - this guy REALLY wanted to use his xDCI capability and was not going to stop until he had done so! reply explodingwaffle 7 hours agoprevGreat article. You would hope this feature was more readily exposed on USB-C laptops where it makes a bit more sense… oh well. reply tamimio 14 hours agoprevI quickly skimmed through it but great detailed article, and love the excessive use of emojis, not the typical boring wall of texts! reply urbandw311er 9 hours agoprevWhy did they cripple this capability by hiding it in an undocumented and disabled BIOS setting? reply crote 8 hours agoparentIt's an extremely obscure feature for which the device lacks the hardware to do it properly (no VBUS switching), which is almost certainly untested. If I were designing a mass-market product, I would disable it too. They got the BIOS from a 3rd party vendor, disabling it by default and hiding the setting was probably the easiest way to prevent anyone from accidentally trying to use it. reply zokier 1 hour agorootparentIts under \"advanced\" tab in BIOS. How many people accidentally stumble there? And how many of those people will have problems with it? And of the tiny number of people who end up enabling it and have problems, how many will then complain to Lenovo? That seems like really marginal case! Especially when it needs also things to happen from OS side too, so just flipping the flag in BIOS in itself shouldn't do much. How much trouble would it have been to add warning \"this is not supported, use at own risk\"? reply actionfromafar 14 hours agoprevBeing able to emulate a USB block device is really neat! Talk about easy way to copy files to another computer. :) reply icodefuture 15 hours agoprevGreat article, thanks for sharing. reply imhoguy 12 hours agoprevPity I can't do that with my AMD Ryzen ThinkPad. reply zzz999 6 hours agoprevYou can do that with cheap ESP boards worth about $3 reply extraduder_ire 6 hours agoparentDo you mean by wiring up the GPIO, or via their USB port? Any esp 8266/32 I have used has only had a cheap serial adapter on the USB port. reply zzz999 4 hours agorootparentYou can get USB otg on esp... Like the s2... I did a rubber ducky using those reply JoBrad 6 hours agoparentprevYes, but now you can do it with a laptop! reply zzz999 1 hour agorootparentSure could, but it is almost the most inefficient way... I guess it works for experimentation reply zokier 1 hour agorootparentSure, dinky ESP works if you want to just do trivial gadgets. But for stuff like facedancer embedded solutions start to become actually quite limiting reply rubatuga 14 hours agoprevWoah it's the stuff of dreams. reply luxuryballs 8 hours agoprev [–] So does this mean I could essentially copy and paste between two machines by just feeding standard USB keyboard input into one of them? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author unlocked the xDCI controller on their ThinkPad laptop without external hardware by adjusting Linux kernel drivers, ACPI, and BIOS, enabling USB host fuzzing and Raw Gadget features.",
      "Detailed steps included modifying the BIOS, reflashing the SPI chip, and accessing PSF registers, successfully emulating USB mass storage & a keyboard.",
      "Despite facing obstacles like BIOS locks, the author found the project fulfilling and acknowledged contributors while promoting future security research updates on social media."
    ],
    "commentSummary": [
      "Users are debating using laptops as keyboards and monitors for other devices, considering solutions such as HDMI dongles and wireless keyboards.",
      "Products like the GPD Pocket 3 and Minisforum V3 are mentioned, along with the idea of integrating this feature into modular laptops like Framework laptops.",
      "Discussions encompass remote access options, security risks in firmware updates, and activating hidden xDCI capabilities for file transfers on specific laptops, sharing diverse tools for remote device connection and control while emphasizing leveraging current hardware and exploring innovative functions."
    ],
    "points": 294,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1708623448
  },
  {
    "id": 39471221,
    "title": "Rising JavaScript Bundle Sizes Impact Website Performance",
    "originLink": "https://tonsky.me/blog/js-bloat/",
    "originBody": "JavaScript Bloat in 2024 I was a bit out of touch with modern front-end development. I also remembered articles about web bloat, how the average web page size was approaching several megabytes! So all this time I was living under impression that, for example, if the average web page size is 3 MB, then JavaScript bundle should be around 1 MB. Surely content should still take the majority, no? Well, the only way to find out is to fuck around. Let’s do a reality check! I’m writing this in 2024, so maybe do a sequel in a few years? Method Firefox on macOS (but should be the same in any browser) Not incognito (I want to see numbers inside the app, and there’s a better chance it will resemble actual everyday experience) All extensions disabled JavaScript only Uncompressed Service Workers enabled (again, more real-life) All caching disabled (cold load) Why only JavaScript? Content varies a lot from site to site (surely videos on YouTube are heavier than text messages on Slack), but JavaScript is a universal metric for “complexity of interactions”. The main goal is to evaluate how much work the browser has to do to parse and execute code. To set some baseline, let’s start with this blog: The number here would be 0.004 MB. I also highlighted all the important bits you need to set if you decide to reproduce this at home. Landings Okay, let’s start with something simple, like landing pages/non-interactive apps. A normal slightly interactive page looks like this — Wikipedia, 0.2 MB: Slightly bloated — like this — Linear, 3 MB: Remember: that’s without images, or videos, or even styles! Just JS code. A bad landing page looks like this — Zoom, 6 MB: or like Vercel, 6 MB: Yes, this is just a landing page. No app, no functionality, no calls. 6 MB of JavaScript just for that. You can do a lot worse, though — Gitlab, 13 MB: Still just the landing. Mostly static websites Nothing simpler than showing a static wall of text. Medium needs 3 MB just to do that: Substack needs 4 MB: Progress? Quora, 4.5 MB: Pinterest, 10 MB: Patreon, 11 MB: And all this could’ve been a static page... Search When your app’s interactivity is limited to mostly search. Type the query — show the list of results. How heavy is that? StackOverflow, 3.5 MB: NPM, 4 MB: Airbnb, 7 MB: Booking.com, 12 MB: But Niki, booking is complicated! Look at all this UI! All these filters. All these popups about people near you stealing your vacation! Okay, okay. Something simpler then. Google. How about Google? One text field, list of links. Right? Well, it’ll cost you whooping 9 MB: Just to show a list of links. Simple one-interaction apps Google Translate is just two text boxes. For that, you need 2.5 MB: ChatGPT is one text box. 7 MB: I mean, surely, ChatGPT is complex. But on the server, not in the browser! Videos Loom — 7 MB: YouTube — 12 MB: Compare it to people who really care about performance — Pornhub, 1.4 MB: Audio I guess audio just requires 12 MB no matter what: SoundCloud: Spotify: Email Okay, video and audio are probably heavy stuff (even though we are not measuring content, just JS, remember!). Let’s move to simpler office tasks. Google Mail is just (just!) 20 MB: It’s a freaking mailbox!!! How on earth is it almost as big as Figma, who ships entire custom C++/OpenGL rendering for their app? And if you are thinking: mail is complicated, too. Lots of UI, lots of interactivity. Maybe 20 MB is okay? No! Just no. See, FastMail, same deal, but only 2 MB. 10× less! Productivity Okay, maybe e-mail is too complicated? How about something even simpler? Like a TODO list? Well, meet Todoist, 9 MB: Showing you a list of files in folders requires 10 MB in Dropbox: List of passwords? That’ll be 13 MB on 1Password: Cards? Add 0.5 MB more, up to 13.5 MB. Trello: Okay, maybe TODO lists are too complex, too? How about chatting? Well, Discord needs 21 MB to do that: Document editing Okay, document editing is hard, right? You have to implement cursor movement, synchronization, etc. Google Docs, 13.5 MB: Something simpler? Notion, 16 MB: Social Networks The typical size of code that social networks need for like buttons to go brrr is 12 MB. Twitter, 11 MB: Facebook, 12 MB: TikTok, 12.5 MB: Instagram is somehow bigger than Facebook, despite having like 10× less functions. 16 MB: LinkedIn. Is it a blog? A platform? It has search, it has messaging, it has social functions. Anyways, that’ll be 31 MB: By the way, I'd like to add you to my professional network on LinkedIn. Elephants — its own category Sometimes websites are so stupidly, absurdly large that they deserve their own category. Here, Jira, a task management software. Almost 50 MB! Do they ship the entire Electron compiled WASM or what? But that’s not the limit! Slack adds 5 more MB, up to 55 MB: Yes, it’s a chat. You know, list of users, messages, reactions. Stuff we did on raw HTML, even before JS was invented? That’s 55 MB in today’s world. It’s almost like they are trying to see how much more bullshit can they put in a browser before it breaks. Finally, this blew my mind. Somehow react.dev starts with a modest 2 MB but as you scroll back and forth, it grows indefinitely. Just for fun, I got it to 100 MB (of JavaScript!), but you can go as far as you like: What is going on there? Even if it unloads and downloads parts of that blog post, how is it growing so quickly? The text itself is probably only 50 KB (0.05 MB). How fast are we degrading? Look how cute! In 2015 average web page size was approaching shareware version of Doom 1 (2.5 MB): Source Well, in 2024, Slack pulls up 55 MB, the size of the original Quake 1 with all the resources. But now it’s just in JavaScript alone. For a chat app! How big is 10 MB anyway? To be honest, after typing all these numbers, 10 MB doesn’t even feel that big or special. Seems like shipping 10 MB of code is normal now. If we assume that the average code line is about 65 characters, that would mean we are shipping ~150,000 lines of code. With every website! Sometimes just to show static content! And that code is minified already. So it’s more like 300K+ LoC just for one website. But are modern websites really that complex? The poster child of SPAs, Google Maps, is quite modest by modern standards — is still just 4.5 MB: Somebody at Google is seriously falling behind. Written with modern front-end technologies, it should be at least 20 MB. And if you, like me, thought that “Figma is a really complex front-end app, so it must have a huge javascript download size”, well, that’s correct, but then Gmail is about as complex as Figma, LinkedIn is 1.5× more complex and Slack is 2.5× more ¯\\_(ツ)_/¯ Conclusion It’s not just about download sizes. I welcome high-speed internet as much as the next guy. But code — JavaScript — is something that your browser has to parse, keep in memory, execute. It’s not free. And these people talk about performance and battery life... Call me old-fashioned, but I firmly believe content should outweigh code size. If you are writing a blog post for 10K characters, you don’t need 1000× more JavaScript to render it. This site is doing it right: That’s 0.1 MB. And that’s enough! And yet, on the same internet, in the same timeline, Gitlab needs 13 MB of code, 500K+ LoC of JS, just to display a static landing page. Fuck me. February 22, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39471221",
    "commentBody": "JavaScript Bloat in 2024 (tonsky.me)256 points by cdme 15 hours agohidepastfavorite142 comments djtango 1 hour agoI recently came back from a road trip in New Zealand - a lot of their countryside has little to no cell coverage. Combined with roaming (which seems to add an additional layer of slowness) and boy did it suck to try to use a lot of the web. Also if any spotify PMs are here, please review the Offline UX. Offline is pretty much one of the most critical premium features but actually trying to use the app offline really sucks in so many ways reply Tistron 1 hour agoparentOffline is still miles and miles better than patchy Internet. If spotify thinks you have Internet it calls the server to ask for the contents of every context menu, waiting for a response for seconds before sometimes giving up showing a menu and sometimes falling back to what would have been instant if it was in offline mode. I really loathe their player. reply jve 50 minutes agorootparentHmm, this would also imply they would need more infrastructure at their side when they could just maybe use cached values stored locally. reply meowtimemania 1 hour agorootparentprevI get irritated by this too. When it happens I put my phone on airplane mode to force Spotify to show the offline ui. reply jnsaff2 28 minutes agorootparentprevNot only that, there are many apps with no online aspect to them that have facebook sdk or some other spyware that does a blocking call on app startup and the app won't start without it succeeding, unless you are completely offline. Especially annoying when one is using dns based filtering. reply diggan 1 hour agoparentprev> Also if any spotify PMs are here, please review the Offline UX. Offline is pretty much one of the most critical premium features but actually trying to use the app offline really sucks in so many ways Also, Spotify (at least on iOS) seems to have fallen into the trap of thinking there is only \"Online\" and \"Offline\", so when you're in-between (really high latency, or really lossy connection), Spotify thinks it's online when it really should be thinking it's offline. But to be fair, this is a really common issue and Spotify is in no way alone in failing on this, hard to come up with the right threshold I bet. reply dukeyukey 30 minutes agoparentprevI live in London which typically gets great signal everywhere. Except in the Underground network, where they're rolling out 5G but it's not there yet. Please Spotify, why do I need to wait 30 seconds for the app to load anything when I don't have signal? All I want to do is keep listening to a podcast I downloaded. reply user432678 1 hour agoparentprevRe: Spotify So much agree here, the offline mode is so beyond being annoying so I even started building my own iOS offline first music app. reply jjav 1 hour agorootparent> my own iOS offline first music app Sadly ironic that apple used to sell this, in the shape of an ipod! I hold on to mine, it is perfect in every way that a phone is terrible. It is tiny and 100% offline, just what I need. reply rob74 37 minutes agorootparentWait... iOS doesn't have an offline music app anymore either? Google replaced the \"Play Music\" app (which could also play offline music files) with \"Youtube Music\" a few years ago (not sure if that works with offline files, I switched to a third party app), but I thought iOS still had one (precisely because they used to sell the iPods, specifically the iPod touch which was more or less an iPhone lacking the phone part)? reply okaleniuk 7 minutes agoprevMeanwhile, all the pages on https://wordsandbuttons.online/ with all the animation and interactivity are still below 64 KB. This one, for example, https://wordsandbuttons.online/trippy_polynomials_in_arctang... is 51 KB. And the code is not at all economical. It's 80% copy-paste with little deviations. There is no attempt to save by being clever either, it's all just good old vanilla JS. And no zipping, no space reduction. The code is perfectly readable when opened with the \"View page source\" button. The trick is - zero dependency policy. No third party, no internal. All the code you need, you get along with the HTML file. Paradoxically, in the long run, copy-paste is a bloat preventor, not a bloat cause. reply wruza 4 hours agoprev10MB, 12MB, … Compare it to people who really care about performance — Pornhub, 1.4 MB Porn was always actual web hi-tech with good engineering, not these joke-level “tech” giants. Can’t remember a single time they’d screw up basic ui/ux, content delivery or common sense. reply devjab 2 hours agoparentI never really understood why SPAs became so popular on the web. It’s like we suddenly and collectively became afraid of the page reload on websites just because it’s not a wanted behaviour in actual web applications. I have worked with enterprise applications for two decades, and with some that were build before I was born. And I think the React has been the absolute best frontend for these systems compared to everything that came before. You’re free to insert Angular/Vue/whatever by the way. But these are designed to replace all the various horrible client/server UIs that came before. For a web-page that’s hardly necessary unless you’re g-mail, Facebook or similar, where you need the interactive and live content updates because of how these products work. But for something like pornhub? Well PHP serves them just fine, and this is true for most web sites really. Just look at HN and how many people still vastly prefer the old.reddit.com site to their modern SPA. Hell, many people still would probably still prefer an old.Facebook to the newer much slower version. reply figmert 38 minutes agorootparent> It’s like we suddenly and collectively became afraid of the page reload on websites I used to work at a place where page reloads was constantly an issue brought up as a negative. They couldn't be bothered to fix the slow page loads and instead avoided page changes. I argued several times that we should improve performance instead of caring about page reloads, but never got through to anyone (in fairness, it was probably mostly cos of a senior dev there). At some point a new feature was being developed, and instead of just adding it to our existing product, it was decided to use an iframe with the new feature as a separate product embedded. reply ozim 13 minutes agorootparentOh my god iframes should be removed from browsers so no one should be able to use them ever. reply dudus 24 minutes agorootparentprevWhy SPAs became popular? Because they \"feel\" native on mobile. Now you have page transitions and prefetch which really should kill this use case. IMO the bloat he talks about on the post is not representative of 2024. Pretty much all frontend development of the last 2 years has been moving away from SPAs with smaller builds and faster loading times. Fair enough it's still visible in a lot of sites. But I'd argue it's probably better now than a couple years ago. reply diggan 1 hour agorootparentprev> But for something like pornhub? Well PHP serves them just fine, Kind of fun to make this argument for Pornhub when visiting their website with JavaScript disabled just seems to render a blank page :) > how many people still vastly prefer the old.reddit.com site to their modern SPA Also a fun argument, the times I've seen analytics on it, old.reddit.com seems to hover around/below 10% of the visitors to subs. But I bet this varies a lot by the subreddit. reply ralusek 1 hour agorootparentprevI love SPAs. I love making them, and I love using them. The thing is, they have to be for applications. When I'm using an application, I am willing to eat a slower initial load time. Everything after that is faster, smoother, more dynamic, more responsive. reply ihateolives 1 hour agorootparent> Everything after that is faster, smoother, more dynamic, more responsive. IF and only IF you have at least med- to high-end computer and smartphone. If you have low-end hardware you first have to wait for that 20MB to load AND get to use slow and choppy app afterwards. Worst of both worlds, but hey, it's built according to modern standards! reply thakoppno 3 hours agoparentprev> Can’t remember a single time they’d screw up basic ui/ux, content delivery or common sense. There are many, many cases of porn websites breaking the law. reply yomly 1 hour agorootparentYes - writing PHP in 2024 is a crime that we should hold PH accountable for. reply SebastianKra 10 hours agoprevAny reason why we're looking at uncompressed data? Some of the listed negative examples easily beat GMaps 1.5mb when compressed. Also, I'll give a pass to dynamic apps like Spotify and GMail [1] if (and only if) the navigation after loading the page is fast. I would rather have something like Discord which takes a few seconds to update on startup, than GitLab, which makes me wait up to two seconds for every. single. click. The current prioritisation of cold starts and static rendering is leading to a worse experience on some sites IMO. As an experiment, go to GitHub and navigate through the file tree. On my machine, this feels significantly snappier than the the rest of GitHub. Coincidentally, it's also one of the only parts that is not rendered statically. I click through hundreds of GitHub pages daily. Please, just serve me an unholy amount of JavaScript once, and then cache as much as possible, rather than making me download the entire footer every time I want to view a pipeline. [1]: These are examples. I haven't used GMail and Spotify reply acdha 9 hours agoparentCompression helps transfer but your device still has to parse all of that code. This comes up in discussions about reach because there’s an enormous gap between iOS and Android CPU performance which gets worse when you look at the cheaper devices a lot of the public use where new Android devices sold today perform worse than a 2014 iPhone. If your developers are all using recent iPhones or flagship Android devices, it’s easy to miss how much all of that code bloat affects the median user. https://infrequently.org/2024/01/performance-inequality-gap-... reply SebastianKra 9 hours agorootparentI happen to develop a JS-App that also has to be optimised for an Android Phone from 2017. I don't think the amount of JS is in any way related to performance. You can make 1MB of JS perform just as poorly as 10MB. In our case, the biggest performance issues were: - Rendering too many DOM nodes at once - virtual lists help. - Using reactivity inefficiently. - Random operations in libraries that were poorly optimised. Finding those things was only possible by looking at the profiler. I don't think general statements like \"less JS = better\" help anyone. It helps to examine the size of webpages, but then you have to also put that information into context: how often does this page load new data? once the data is loaded, can you work without further loading? Is the data batched, or do waterfalls occur? Is this a page that users will only visit once, or do they come regularly? ... reply guappa 2 hours agorootparent> You can make 1MB of JS perform just as poorly as 10MB. But can you make a 10MB js perform as good as a good 1MB js? reply hnarn 1 hour agorootparentI'm not a JS developer but I imagine that the amount of JavaScript code isn't the most relevant part if most of it isn't being called. I mean, if you have some particularly heavy code that only runs when you click a button, is that really parsed and causes overhead before the button is clicked? reply guappa 29 minutes agorootparentIt is parsed and loaded in memory, and not executed. reply lifthrasiir 0 minutes agorootparent\"Loaded\" part is no longer true in major JS implementations (for example, [1]). [1] https://v8.dev/blog/preparser lukan 2 hours agorootparentprev\"- Rendering too many DOM nodes at once - virtual lists help\" Yup, despite all the improvements, the DOM is still slow and it is easy to make it behave even slower. Only update what is necessary and be aware of the performance bottleneck forced reflow. Every time you change anything and then get clientWidth for example and then change something else - you will make the DOM calculate(and posibly render) everything twice. I found the chrome dev tools to be really helpful with spotting those and other stuff. But sure, if you update EVERYTHING anyway, including waiting for the whole data transfer, with any click, when you just want some tiny parts refreshed, you have other problems anyway. reply acdha 8 hours agorootparentprev> It helps to examine the size of webpages, but then you have to also put that information into context: how often does this page load new data? once the data is loaded, can you work without further loading? Is the data batched, or do waterfalls occur? Is this a page that users will only visit once, or do they come regularly? I totally agree - my point was simply that people sometimes focus on network bandwidth and forget that a huge JS file can be a problem even if it’s cached. What you’re talking about is the right way to do it - I try to get other developers to use older devices and network traffic shaping to get an idea for those subjective impressions, too, since it’s easy to be more forgiving when you’re focused on a dedicated testing session than, say, if you’re trying to use it while traveling and learning that the number of requests mattered more than the compressed size or that you need more robust error handling when one of the 97 requests fails. reply realusername 2 hours agorootparentprevYou can theoretically have a somewhat fast enough large JS app but it's going to be an uphill battle. You have to make regular bundle analysis otherwise the cache won't work if you deploy too much and package updates and new additions are likely to break the performance analysis you've just made before. Less JS = better performance is a simplified model but very accurate in practice in my opinion, especially on large teams. reply BlueTemplar 2 hours agorootparentprevEven decently powerful phones can have issues with some of these. Substack is particularly infuriating : sometimes it lags so badly that it takes seconds to display scrolled text (and bottom of text references stop working). And that's on a 2016 flagship : Samsung Galaxy S7 ! I shudder to think of the experience for slower phones... (And Substack also manages to slow down to a glitchy crawl when there are a lot of (text only !) comments on my gaming desktop PC.) reply hn_acker 6 hours agoparentprev> Any reason why we're looking at uncompressed data? Some of the listed negative examples easily beat GMaps 1.5mb when compressed. Because for a single page load, decompressing and using the scripts takes time, RAM space, disk space (more scratch space used as more RAM gets used), and power (battery drain from continually executing scripts). Caching can prevent the power and time costs of downloading and decompressing, but not the costs of using. My personal rule of thumb is: the bigger the uncompressed Javascript load, the more code the CPU continually executes as I move my mouse, press any key, scroll, etc. I would be willing to give up a bit of time efficiency for a bit of power efficiency. I'm also willing to give up prettiness for staticness, except where CSS can stand in for JS. Or maybe I'm staring at a scapegoat when the actual/bigger problem is sites which download more files (latent bloat and horrendously bad for archival) when I perform actions other than clicking to different pages corresponding to different URLs. (Please don't have Javascript make different \"pages\" show up with the same URL in the address bar. That's really bad for archival as well.) Tangent: Another rule of thumb I have: the bigger the uncompressed Javascript load, the less likely the archived version of the site will work properly. reply flexagoon 8 hours agoparentprev> go to GitHub and navigate through the file tree. On my machine, this feels significantly snappier than the the rest of GitHub. Coincidentally, it's also one of the only parts that is not rendered statically And it's also the only part of it that doesn't work on slow connections. I've had a slow internet connection for the past week, and GitHub file tree literally doesn't work if you click on it on the website, because it tries to load it through some scripts and fails. However, if, instead of clicking on a file, I copy it's url and paste it into the browser url bar, it loads properly. reply SebastianKra 8 hours agorootparentWow, you're right. I just reproduced that by throttling the network. But actually, that first click from the overview is still an HTML page. Once you're in the master-detail view, it works fast even when throttled. reply rarafael 7 hours agorootparentI have a connection that might be considered slow by most HN readers (1~2MB/s) and the new github file viewer has been a blessing. So snappy compared to everything else reply willsmith72 4 hours agoparentprevgmail is terrible, idk if it's just me but i have to wait 20 seconds are marking an email as read before closing the tab. otherwise it's not saved as read spotify has huge issues with network connectivity, even if i download the album it'll completely freak out as the network changes. plain offline mode would be better than its attempt at staying online reply panstromek 1 hour agoparentprevInteresting that you mention GitHub file tree. I recently encountered a periodic freezing of that whole page. I've profiled for a bit and found out that every few seconds it spends like 5 seconds recomputing relative timestamps on the main thread. reply azangru 49 minutes agorootparent> I recently encountered a periodic freezing of that whole page. Yes; this started happening after they rolled out the new version of their UI built with React several months ago. reply jaredcwhite 9 hours agoparentprevGitHub's probably the worst example of \"Pjax\" or HTMX-style techniques out there at this point…I would definitely not look at that and paint a particular picture of that architecture overall. It's like pointing at a particularly poor example of a SPA and then saying that's why all SPAs suck. reply agos 34 minutes agorootparentis there a good example of reasonably big/complex application using pjax/htmx style that sucks less? Because GitHub isn't making a good case for that technology reply SebastianKra 9 hours agorootparentprevIm inclined to agree, but the same thing happens on GitLab. reply sublinear 10 hours agoprevAny piece of software reflects the organization that built it. The data transferred is going to be almost entirely analytics and miscellaneous 3rd party scripts, not the javascript actually used to make the page work (except for the \"elephant\" category which are lazy loading modules i.e. React). Much of that is driven by marketing teams who don't know or care about any of this. All devs did was paste Google Tag Manager and/or some other script injection service to the page. In some cases the devs don't even do that and the page is modified by some proxy out in the production infrastructure. Maybe the more meaningful concern to have is that marketing has more control over the result than the people actually doing the real work. In the case of the \"elephant\" pages, the bloat is with the organization itself. Not just a few idiots but idiots at scale. reply jve 47 minutes agoparentI remember associating Google with lean and fast: Google the search (vs Yahooo) and Chrome (vs IE/FF (i'm talking about when Chrome was released))... chrome on itself had not much of an UI and it was a feature. reply dieulot 2 hours agoprevFor a more complete view, the HTTP Archive tracks general site weight over time and gives you percentiles (click Show table): https://httparchive.org/reports/page-weight#bytesJs The median JS bundle is 600kB on desktop. p90 (“high 10%”) is 1830kB. reply aembleton 1 minute agoparentWeird spike in March 2021 in the `Other Bytes` graph. Wonder what went on then; or if there was a glitch in their data. reply jspdown 2 hours agoprevThe state of the web is very sad. Most people with a fiber connection don't even notice how slow it became. But when you are still on a 2Mbps connection, this is just plain horrible. I'm in this case, it's terribly painful. Because of this, I can't even consider not using an ad/tracker blocker. Would love to see this test with Ublock origin enabled. reply deliriumchn 1 hour agoparent> But when you are still on a 2Mbps connection What happens when you use modern apps on iphone 3 or first nexus phone? I don't understand, do people think that with better, faster computers and network speed we should focus on smaller and smaller apps and websites? reply pavlov 1 hour agorootparentYour iPhone CPU doesn't suddenly become an iPhone 3G CPU sometimes, but network availability does vary a lot. You may also one day find yourself on a flaky 3G connection needing access to some web app that first loads twenty megabytes of junk before showing the 1 kB of data you need, and then it's clearer what the problem is here. reply bambax 1 hour agoparentprev> Would love to see this test with unblock origin enabled. Me too. I suspect most of this code is for user tracking and ad management. reply panstromek 1 hour agorootparentTracking is a bit heavy, but from what I've looked at, the app code is usually much worse. I've looked at what Instagram and JIRA ship during the initial load and it's kinda crazy. reply threatofrain 58 minutes agoparentprevMost people with a fiber connection? I bet a lot of people with money don't have a fiber connection, certainly not most people here on HN. reply agos 32 minutes agorootparentread it as \"of the people who have a fiber connection, most...\" reply threatofrain 29 minutes agorootparentYeah, I'm saying the relevance of that statement is pretty low because most of us don't experience that, certainly not enough to tip the needle of JS culture. reply vendiddy 19 minutes agoprevI'm inspired to slim down my own app. Any good tools or techniques out there to identify what's causing bloat? reply lifthrasiir 4 minutes agoparentIn general, look for dependencies. Any popular enough bundlers should have a feature or plugin to show the bundle statistics (e.g. webpack-bundle-analyzer or rollup-plugin-analyzer). Audit your dependency to see if that's not requested nor needed, and try to remove them with a finer-grained dependency or leaner library or rewriting, in the order of preference. That alone is enough for usual JS apps, because not much people do so... reply jwr 12 minutes agoprevI have a large and complex (ERP/MRP/inventory system for electronics) ClojureScript application and I was worried about my compiled code size being around 2.45MB. This puts it into perspective. I heard complaints about ClojureScript applications being large, which I think is true if you write a \"Hello, world!\" program, but not true for complex applications. Also, Google Closure compiler in advanced compilation mode is a great tool. Of course, since it is technically superior, it is not in fashion, and since it is hard to use correctly, people pretend it isn't there. reply diimdeep 2 minutes agoprevRemember The Website Obesity Crisis [1] article from 2015, since then [2] things only got worse, and it is been almost 10 years already (at the end of 2024). Is it foolish to say that in 10 more years you wont be able to navigate the web on a circa 2015 PC ? If nothing changes seems like it. My old macbook from 2013 with latest Firefox is already can not handle loading https://civitai.com web page with 23.98 MB of JavaScript, it is just hangs for half a minute while trying to render this disaster of web frontend. [1] https://idlewords.com/talks/website_obesity.htm [2] https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply jakelazaroff 7 hours agoprevI know the implication here is \"too much JavaScript\" but we also need to talk about how much of this is purely tracking junk. reply BandButcher 5 hours agoparentWas going to mention this, almost any company's brand site will have tracking and analytics libraries set in place. Usually to farm marketing and UX feedback. Whats worse is some of them are fetched externally rather than bundled with the host code thus increasing latency and potential security risks reply tadfisher 5 hours agorootparent> Whats worse is some of them are fetched externally rather than bundled with the host code thus increasing latency and potential security risks Some vendor SDKs can be built and bundled from NPM, but most of them explicitly require you fetch their minified/obfuscated bundle from their CDN with a script tag. This is so they don't have to support older versions like most other software in the world, and so they can push updates without requiring customers to update their code. Try to use vendors that distribute open-source SDKs, if you have to use vendors. reply willsmith72 3 hours agorootparentprevim pro privacy, but is it really so bad to get anonymous data about where people clicked and how long they stayed where? it would be almost impossible to measure success without it, whether it's a conversion funnel or tracking usage of a new feature reply MrJohz 3 hours agorootparentIt is when (a) that data collection takes up a significant amount of bandwidth whenever I visit your website, and (b) I don't trust that that data collection really is as anonymous as the website says (or even thinks). The major players here are explicitly not anonymous, they are designed to keep track of people over time so that they can collate habits and preferences across different sites to better target advertising. Yes, your AB test script isn't doing the same thing, but is it really adding any value to be as a consumer, or is it just optimising an extra 0.01% revenue for you? reply kevin_thibedeau 3 hours agorootparentprevThat data can be gathered with self-hosted JS if the devs were allowed to implement it. The infatuation with third party analytics is just a more elaborate version of leftpad. reply ehnto 2 hours agorootparentMarketing snippets are rarely implemented by devs, they get dropped into a text box or added to Google Tags by marketing/SEO peeps. If it is put in by a developer, the budget for that is like an hour to copy paste the code snippet in the right spot. Few are going to pay the hours required for an in house data collection layer that then has to integrate with the third party if that's even an option. At least that is my experience through agency work. Maybe a product owner company could do it. Not to be rude to the industry either, but I don't see why the assumption would bet that an in house dev has the chops to not make the same mistakes a third party does. reply psychoslave 1 hour agorootparentprevHow should we feel if when we learn that due to technical constraints, Stasi wasn’t able to measure how successful it was? reply veeti 1 hour agoparentprevIt's easy to test with adblock in place. For instance, the Gitlab landing page went from 13 megabytes to \"just\" 6 megabytes with tracking scripts blocked. The marketing department will always double the bloat of your software. reply PetitPrince 10 hours agoprevThis compares how much Javascript is loaded from popular sites (cold loaded). Some highlights: - PornHub loads ~10x less JS than YouTube (1.4MB vs 12MB) - Gmail have an incomprehensible large footprint (20MB). Fastmail is 10x ligthter (2MB). Figma is equivalent (20MB) while being a more complex app. - Jira has 58MB (whoa) reply jkoudys 6 hours agoparentPornhub needs to be small. Jira will download once then be loaded locally until it gets updated, just like an offline app. Pornhub will be run in incognito mode, where caching won't help. reply thrwwycbr 4 hours agorootparent> Jira will download once Maybe you should take a look on the Network Tab, because Atlassian sure does have a crappy network stack. reply panstromek 59 minutes agorootparentprevJIRA transfers like 20MB of stuff everytime you open the board, including things like 5MB JSON file with list of all emojis with descriptions (at least the last time I profiled it). reply wruza 3 hours agorootparentprevPornhub will be run in incognito mode It’s not '80s anymore, nobody cares about your porn. I have bookmarks on the bookmarks bar right next to electronics/grocery stores and HN. And if you’re not logged in, how would PH and others know your preferences? reply thiht 1 hour agorootparentI don’t think you’re speaking for the majority here, no one has pornhub in their bookmarks bar reply nox101 1 hour agorootparentWhen I said I didn't watch porn on company laptops, only my personal one, the 5 Apple employees at a party all said they watched porn on company laptops. I was in the minority. reply jjav 1 hour agorootparentprev> I have bookmarks on the bookmarks bar Must be fun at work zoom meeting when you share the browser window! reply mewpmewp2 7 hours agoparentprevYouTube feels really snappy to me, but Figma is consistently the worst experience I have ever felt for web apps. Jira is horrible and slow also though. reply latency-guy2 4 hours agorootparentYouTube does not feel snappy to me anymore, its still one of the better experiences I have on the internet, but quite bad from years before. I just tested my connection to youtube right now, just a tiny bit over 1.2 seconds from not using it for a few days. A fresh, no cache, no cookies, the entire page loaded in 2.8 seconds. A hot reload on either side varied between 0.8s to 1.4 seconds. All done with at most ublock as an extension on desktop chrome with purported gigabit speeds from my ISP. That speed is just OK, but definitely not the 54ms response time I got to hit google's server to send me the HTML document that bears all the dynamic content on youtube Figma is very surprising to me, that bullshit somehow is PREFERRED by people, getting links from designers from that dogshit app screeches my browser to speeds I haven't seen in decades, and I don't think I'm exaggerating at all when I say that reply troupo 2 hours agorootparentprevOn desktop they load 2.5MB of JS and 12 MB of Javascript to show a grid of images. And it still takes them over 5 seconds to show video length in the previews. Youtube hasn't felt snappy in ages reply pkphilip 2 hours agoparentprevNo idea why an email client should have 20 MB of JS. reply Spivak 8 hours agoparentprevHoly god YouTube is 12MB? How!? reply troupo 2 hours agorootparentThey also load 2.5 MB of CSS on desktop :) reply lifthrasiir 8 hours agoprevOne thing completely ignored by this post, especially for actual web applications, is that it doesn't actually break JS files down to see why it is so large. For example, Google Translate is not an one-interaction app once you start to look further; it somehow has dictionaries, alternative suggestions, transliterations, pronunciations, a lot of input methods and more. I still agree that 2.5 MB is too much even after accounting that fact and some optional features can and should be lazily loaded, but as it currently stands, the post is so lazy that it doesn't help any further discussion. reply infensus 1 hour agoparent100% agree. Most of these apps could definitely use some optimization, but trivializing them to something like \"wow few MBs of javascript just to show a text box\" makes this comparison completely useless reply BandButcher 5 hours agoparentprevDon't want to hate on the author's post but the screenshots being slow to load made me chuckle, understandable as images can be big and there were a lot, but just found it a little ironic. reply crooked-v 2 hours agorootparentThese days, slow-loading images usually mean that somebody hasn't bothered to use any of the automatic tooling various frameworks and platforms have for optimized viewport- and pixel density-based image sets, and just stuck in a maximum size 10+ MB image. reply blue_pants 48 minutes agorootparentCould you suggest some of those automatic tools? reply troupo 1 hour agorootparentprevWhich absolutely false for the website in question reply troupo 2 hours agoparentprev> For example, Google Translate is not an one-interaction app once you start to look further; it somehow has dictionaries, alternative suggestions, transliterations, pronunciations, a lot of input methods and more. Almost none of those are loaded in the initial bundle, are they? All those come as data from the server. How much JS do you need for `if data.transliteration show icon with audio embed`? reply lifthrasiir 34 minutes agorootparent> Almost none of those are loaded in the initial bundle, are they? In my testing, at least some input methods are indeed included in the initial requests (!). And that's why I'm stressing it is not \"one-interaction\" app; it is interactive enough that some (but not all) upfront loading might be justifiable. reply w3news 2 hours agoprevSo true, we build large complex frameworks, abstractions over abstractions. Try to make things easy to build and maintain. But I think the problem is that many developers that using these frameworks not even know the Javascript basics. Of course there are smart people at these large companies. But they try to make things easy instead of learn people the basics. We over engineer the web applications, create too much layers to hide the actual language. 20 years ago, every web developer can learn building websites by just check the source code. Now you can see the minified Javascript after a build, and nobody understand how it works, even the developers that build the web application don't recognize the code after the build. I love Javascript, but only pure Javascript, and yes, with all his quirks. Frameworks don't protect you from the quirks, you have to know it so you don't make quirks, and with all the abstraction layers, you not even know what you are really building. Keep it simple, learn Javascript itself instead of frameworks, and you downsize the Javascript codebase a lot. reply wruza 3 minutes agoparentPretty sure the situation wouldn't change if it wasn't minified. Recently I had to add a couple of mechanics into sd-web-ui, but found out that the \"lobe theme\" I used was an insufferable pile of intermingled React nonsense. Returned to sd-web-ui default look that is written in absolutely straightforward js and patched it to my needs easily in half an hour. This is a perfect example based on a medium-size medium-complexity app. Most sites in TFA are less complex. The delusions that frontend guys are preaching are on a different level than everything else in development. reply singularity2001 1 hour agoprevThe static page sites fit the narrative but for word editing and e-mail you should compare with native clients reply flohofwoe 2 hours agoprevAnd here I am feeling bad that my WASM C64 emulator doesn't fit into 64 KBytes anymore ;) (it's grown to 116 KBytes total compressed download size over the years). reply advael 2 hours agoprevI have to also wonder what percentage of each of those web logic figures come from code that's there to silently do telemetry rather than part of e.g. a UX framework reply 420698008 4 hours agoprevMeanwhile the author has a ton of 1440p images while pegging his website width to 560px I like the conversation about web performance, but you should make sure you practice what you preach reply mrighele 2 hours agoparentEven if you are not using an high resolution display like the sibling comment says, the images are reasonably sized (they are a few hundreds KB each). I have seen landing pages with 20-30 MB images for no good reason. reply guappa 1 hour agorootparentDon't forget the ones with background videos of random diversely raced young good looking models having coffee and looking at screens while nodding. reply cyclotron3k 3 hours agoparentprev720px actually, but more importantly, 720px is not actually 720px on a HiDPI screen. I just measured it on my mbp and \"720px\" is actually... 1440 physical pixels. I was surprised too! reply ustad 2 hours agoparentprevThe article is related to js bloat and the images are required for the presentation. The images come to 10MB - not bad. The js? 4.5KB! reply tgv 1 hour agoparentprevSome of my (non-programming) colleagues don't seem to be able to wrap their head around image size. And someone who taught courses to communication/marketing students told me, it took 2 hours to explain resolution and all that to them, and then half even didn't get it. Yeah, I can hear you: \"something that easy? must have been a bad teacher,\" but the concepts are rather weird for non-techies. So those people become responsible for updating the website content, and upload whatever the graphical artists show them. And designers like to zoom in, a lot, so often that's a 20MB png, where a 200kB jpeg would suffice. reply devtailz 3 hours agoparentprevThe images display at 720x720, on a 2x retina screen the image needs to be 1440x1440 to fill that. reply sushirundown 38 minutes agoprevThis also happens with other software, and it's arguably even worse. Some mobile apps take up hundreds of megabytes because of all of the bloat that they feel the need to bundle. reply otabdeveloper4 1 hour agoprevSlack is some of the worst software ever written by mankind. reply solumunus 1 hour agoparentIt's far better than Microsoft Teams, so I must disagree. reply mnau 6 minutes agorootparentTbh, new teams eat only half the memory old one did. Still sucks, but it's 700MB RAM vs 1.4GB just to open a chat. reply xigoi 1 hour agorootparentprevThat’s a really low bar. reply nesarkvechnep 1 hour agorootparentprevBoth are some of the worst software ever. reply ohazi 3 hours agoprevNot surprised at all that JIRA is 50 MB, but still... What the fuck reply ihateolives 47 minutes agoparentWhile Minix 1.1 uncompressed is 5 MB and comes complete with network stack etc. I used to run it on 386 back in the day. reply panstromek 1 hour agoprevInteresting how well does the amount of JS inversely correlates with my experience of using those sites... reply chuckadams 5 hours agoprevJeepers, how does this even happen? I've been developing a fairly complex app with Nuxt, Apollo client, and PrimeVue, and paid no attention to size whatsoever. Yet the most complex page in the app with the most module dependencies loads only 3.8 megs, and that's not even a minified build. Same page from the Nuxt dev server throws 24.4M at me, but I'm pretty sure it's pre-loading everything. Do the big players just not do any code splitting at all? On the other hand, node_modules weighs in at 601M. Sure I've got space to burn on the dev box, but that's reason #1 I'm not doing yarn zero-install and stuffing all that into the repo. reply hdjrudni 4 hours agoparentI'm at 329 kB of JS. I've been building on the same app for 10 years. That's a lot of cruft build up. And I'm still nowhere close to any of these numbers. I've got React, jQuery, lodash, and I don't know what else in there. reply danlugo92 7 hours agoprevI only know that i know nothing, but lately libraries (not my own code) have been getting increasingly bloated... mostly true for big-ticket stuff such as Firebase or Meta's stuff. Anything homemade performs faster than ever though, so engines are getting better but my code has stayed as simplest as ever and finding improved performance. reply l5870uoo9y 1 hour agoprevI suspect ChatGPT is so bloated because they (inefficiently) include entire Markdown parser and code highlighting libraries. Add that various tracking libraries and you have a big bundle. reply pier25 5 hours agoprevWhat's up with the React site? This is embarrassing... reply tjosepo 5 hours agoparentYou can see from the recording that it's downloading the same few files from Codesandbox over and over again, as the iframes used for the examples are being unloaded and reloaded on scrolls and because the author disabled caching. The author could've scrolled forever and the number would've gone up indefinitely. reply makepanic 2 hours agoparentprevCodesandbox is embedded for the code samples. If you not have cache disabled it will fetch from memory cache after the initial load and unload. reply chrismorgan 4 hours agoprev> JavaScript is a universal metric for “complexity of interactions”. I don’t understand this claim in the slightest. It seems trivially falsified by the data that follows. reply gloosx 1 hour agoprevIt would be useful if author sorted the requests by size. Most of this junk anyway is analytics, heatmaps, tracking and all that bullshit. Ofc you can makeThis code likely makes life hard for the developers as well. Well if you have static pages you don't need a team of 10 developers… so there is some self interest there. reply palmfacehn 3 hours agoparentprevTo let us know we are not alone with our disdain. Too many JS enthusiasts paper over this insanity. The remnant lives on. \"But JS is the biggest developer ecosystem in the world!\" reply StressedDev 1 hour agorootparentThis is not a JavaScript issue. It’s a software enginner/people issue. The question is how do you get people to care about performance, security, reliability, etc.? How do you get organizations to care about these issues? These are hard problems and people have been complaining about software size forever. Back in the early 90s, it was bloated C++ code. You will also see that all software continues to use more ram, more disk space, more network bandwidth etc. This trend has been going on for decades. For example, why do we use JSON as an interchange format? It’s relatively slow (i.e. creating it and parsing it is slow), nor is it is not space efficient. Back in the 1980s, the Unix community created RPC and the RPC wire formats were much more efficient because they were binary formats. The reason we use JSON is it makes the developer’s life easier and because developers prefer ease of use to performance. reply shiomiru 1 hour agoparentprev> What's the alternative? Many websites listed on the page don't even need JS.[0] Consider how just a few years ago there used to be an entire suite of alternative frontends to major \"web apps\"/\"social media platforms\", which generally worked without any JS, and were created & ran by volunteers. In general, they all provided superior UX by just not loading megabytes and megabytes of tracking code; this would be the alternative.[1] Now they are slowly evaporating: not because of lack of interest, but because it was affecting the margins of these companies, and they actively blocked the frontends.[2] So I think of it this way: these megabytes and megabytes of JS do not serve me, the user. It's just code designed to fill the pockets of giant corporations that is running on my computer. Which is, indeed, quite infuriating. OK, maybe not even that; after all, you get what you pay for. It's just sad that despite the technological possibilities, this is the norm we have arrived at. [0]: Of course, there is valid use of JS, it's a wonderful technology. I'm talking about cases where you could pretty much write the whole thing in pure HTML without losing core functionality. [1]: Well, only if there existed a viable financing model besides \"selling user data to the highest bidder\" :( technologically at least, it's possible. [2]: See cases of bibliogram, teddit, libreddit, nitter, ... reply xk_id 1 hour agoparentprev> What's the point of it? What's the alternative? An absurd, idiotic situation that is endemic to the whole industry deserves every bit of scorn and ridicule. Developers need to be educated about this so they can make the right choices, instead of remaining complacent agents of a shameful situation. It also strengthens my own resolve to support (through my use) those websites that abide by the original principles of the web: they either deliver actual documents (instead of javascript apps), or offer public APIs to access the plaintext data. There is always an alternative out there. reply cranberryturkey 2 hours agoprevi lived in a trailer and had spotty wifi via at&t hotspot. i also have starlink but trees made it unreliable. You realy don't understand how bad js bloat is until you have a shitty internet connection. reply lukaqq 5 hours agoprevChillin, https://chillin.online/app/editor, integrated complete video editing functions, just for 1.8MB reply jerbear4328 1 hour agoparentYour editor downloads a 32.6MB ffmpeg WASM binary on every page load. Throttling the network to \"Slow 3G\", it took over four minutes of a broken interface before ffmpeg finally loads. (It doesn't cache, either.) A port of the Audacity audio editor to web[1] with WASM takes 2.7 minutes on the same connection, so the binary is totally reasonable, but I think claiming less than 2 MB is disingenuous. [1]: https://wavacity.com/ reply lukaqq 47 minutes agorootparentSorry for that, we just focus on js bundle and don't realize how big the ffmepg.wasm is. Thanks for reminding, next step we will try to rebuild ffmepg.wasm and make it smaller. reply nsonha 3 hours agoprevit's always bothered me that this dogma exists. Somehow web apps need to be super frugal with code size, while apps distributed on other (native) platforms never have such a problem. Somehow it's the bloated web that blocks the access for children in Affrica, but they can download bloated android apps just fine? Maybe, just maybe, the problem isn't the size of the javascript, it's how broken the entire web stack (specifically caching & PWA) is, that makes a trivial thing like code size a problem. reply eitland 3 hours agoparentIf Atlassian did't use a full minute to update a certain roadmap view on my MBP from January I would be one step closer to agreeing with you even if it was still 50 MB. But I am old enough to remember Gannt charts didn't use to take that long on old Pentium processors back in school, way before Git was invented. Another thing is the sheer yuck of it: If a typical web app was lots of business code, maybe. But when you look at the network tab and it feels like you are looking at the hair ball from a sink drain, lots of intertwined trackers to catch everything that passes, that is another story. reply nsonha 1 hour agorootparentyes, the problem with the narrative is that web apps that are logic heavy are lumped together with content based sites who cannot justify their code bloat. reply atraac 2 hours agoparentprev> while apps distributed on other (native) platforms never have such a problem Could you give an example? I was an android app dev over 5 years ago and there was a huge push for lower app size everywhere. Google even made Android App Bundles to fight this issue specifically. reply nsonha 1 hour agorootparenthowever big, you're only required download an app once. Next time there is an update that you cannot download, generally that won't block you from using the app at that point in time. reply atraac 15 minutes agorootparentIt actually depends, developer can block you from using the app until you update, either themselves or through Play Store's 'Immediate updates'. reply tgv 1 hour agoparentprevThere's always a speed/storage tradeoff. Apps should be more economic too, but you download native apps once, and web apps almost every time you open them. So indeed, caching could help, but how large would your cache have to be? Big enough to hold all 50MB downloads for every website you visit? That's an awfully large cache. So I'd say economy is more necessary for web-apps than for native/store apps. I just checked my web usage on my work computer. The last 120k opened URLs were on 8300 unique hosts. 8300 * 50MB is not feasible. reply donkeybeer 3 hours agoparentprevWeb sites should be frugal. If content websites, even more so if they even lack comment sections, are getting huge that's a pure \"skill issue\". It's not our place to speculate why what should have been a content site became a web app. reply troupo 1 hour agoparentprevNo. Native apps are not exempt from this either. Nothing justifies the bloat reply nsonha 1 hour agorootparentare you deliberately not getting the point for some reason? How big is considered bloated in native apps, how big can a native app can get before it hurts accessibility because people cannot download it? Is it a few MB? Web apps are soon going to be if not already matching native apps in terms of complexity yet we are still distracted from the real problem and quibble about some arbitrary and frankly pathetic code size restriction. Fix the root problem with PWA or something. reply gpjanik 14 minutes agoprev [–] Serious question: what is the issue with these paritcular sizes? I know that features/look these websites have are definitely achievable with less JS at a higher engineering cost, but what's the problem with it? 10MB loads in two seconds on an okay-ish desktop connection (correct me if I'm wrong, but most of people don't deploy Vercel apps from their phone from a mountain range with 3G connection). The experience on the websites mentioned is smooth as it can only get; everything is super fast and nice. Every subsequent click is just instant action. That's how web should look like. Is the problem here that they perform poorly on slower computers/connections? Is it even true? Is there an audience of developers who can't use Vercel or GitLab productively because of that? Any metrics to support that? IMHO optimizing against bundle size/JS sent over the network is one of the worst metrics for performance I could imagine. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the growing size of JavaScript bundles in contemporary web development, pointing out the significant amounts of code needed for different websites and applications.",
      "It gives examples of JavaScript bundle sizes for a range of sites, from basic landing pages to intricate social networks and productivity tools.",
      "Emphasizing the importance of prioritizing content for improved performance and battery life, the author expresses worries about the increasing trend towards larger code sizes."
    ],
    "commentSummary": [
      "The discussion delves into various aspects of website and app performance, addressing issues such as large JavaScript files, tracking mechanisms, and unnecessary code dependencies impacting user experience.",
      "Recommendations include optimizing code, reducing app sizes, and emphasizing performance, security, and reliability in software development.",
      "The debate highlights the difficulties non-tech individuals encounter in grasping updates, the preference for faster legacy websites, and the necessity for streamlined web development practices to enhance user experience and performance."
    ],
    "points": 256,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1708627182
  },
  {
    "id": 39467885,
    "title": "The Billion Row Challenge: Java Optimization Techniques Lead to Record Speeds",
    "originLink": "https://questdb.io/blog/billion-row-challenge-step-by-step/",
    "originBody": "QuestDB is a high performance time series database with SQL analytics that can power through data ingestion and analysis. It's open source and integrates with many tools and languages. Give us a try! As I was browsing my timeline on the boring afternoon of the New Year's Day 2024, this tweet by Gunnar Morling jumped out: How fast can YOU aggregate 1B rows using modern #Java? Grab your threads, flex your SIMD, and kick off 2024 true coder style by joining this friendly little competition. Submissions accepted until Jan 31. The challenge was this: Write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. There’s just one caveat: the file has 1,000,000,000 rows! My first thought was, \"Pfft, min, mean and max, that's so simple!\" And the dataset was simple as well: just 413 unique keys, of quite uniform, short lengths. Super-simple data format. A whole month to do it. Where's the challenge?? As is often the case, the devil was in the details. It soon dawned on the contestants that calculating only min, mean and max made the competition harder, not easier. But why? There wasn't one obvious place consuming CPU cycles. Opportunities to make it even faster lay everywhere, even in the darkest corners of the CPU architecture. As a saving grace, the challenge was held out in the open on GitHub. Copying others' ideas wasn't just allowed, it was encouraged. This was to be a learning experience, not a war of secrets. It was also going to be a month-long frenzy of research, ingenuity, and just plain fun. It grabbed the attention and devotion of hundreds of enthusiasts, including at least a dozen or two top Java performance experts. The winning solution is the one submitted by none other than Thomas Wuerthinger (@thomaswue), the lead of the GraalVM project — that's the kind of heavyweights I'm talking about! Along with several other key contestants, he introduced many of the great ideas that everyone else grabbed and incorporated into their own submissions. My submission did quite well, ending up at spot #9. My QuestDB colleague, Jaromir Hamala (@jerrinot), fared even better, placing 3rd — just 73 milliseconds behind the winner! The Official 1BRC Scoreboard What came out on top# Now that you read the rules, can you guess what's the actual challenge? Can you visualize what the winning code would look like? Here, have a look! For that matter, look at any of the top 10 solutions, including mine, and especially at the hot loop in each of them — the part of the code where the program spends almost all of its time. If you ever had the experience of writing a small program in C++ or Rust, and then looking at the optimized machine code the compiler produced, you'll get similar vibes here. Abstractions are spilled open, concerns are criss-crossing and interleaving each other. A ton of utterly alien-looking, bit-twiddling logic. How could a human programmer possibly get to this point? Like in so many other cases, it was people working together and improving step by step. Dozens of Java experts iterated through many tricks and hacks, and as January rolled on, the processing time kept dropping lower and lower. The main thing I'd like to show you in this post is that a good part of that amazing speed comes from easy-to-grasp, reusable tricks that you could apply in your code as well. Towards the end, I'll also show you some of the magical parts that take it beyond that level. The \"normal\" implementation# OK: 1,000,000,000 rows - here we go! For context, this is a sample of the temperature input file. Each line contains a station name and a temperature reading: Hamburg;12.0 Bulawayo;8.9 Palembang;38.8 St. John's;15.2 Cracow;-12.6 Copy And this is the expected output: {Abha=-23.0/18.0/59.2, Abidjan=-16.2/26.0/67.3, Abéché=-10.0/29.4/69.0, ...} Copy As a first take, let's apply some idiomatic Java code that would pass muster with any seasoned Java developer: var allStats = new BufferedReader(new FileReader(\"measurements.txt\")) .lines() .parallel() .collect( groupingBy(line -> line.substring(0, line.indexOf(';')), summarizingDouble(line -> parseDouble(line.substring(line.indexOf(';') + 1))))); var result = allStats.entrySet().stream().collect(Collectors.toMap( Entry::getKey, e -> { var stats = e.getValue(); return String.format(\"%.1f/%.1f/%.1f\", stats.getMin(), stats.getAverage(), stats.getMax()); }, (l, r) -> r, TreeMap::new)); System.out.println(result); Copy This code: uses parallel Java streams, which put all the CPU cores to work doesn't fall into any known performance traps like Java regex leans heavily into all the great building blocks provided by the JDK On a Hetzner CCX33 instance with OpenJDK 21.0.2, it takes 71 seconds to complete. But the best solution takes 1.5 seconds — that's a jaw-dropping 47 times faster! As we said, the way to get there is step by step, so let's start with the first one. Optimization 0: Choose a good VM# Before we touch the code, there's a low effort way to speed up your program: use a modernized JVM. Many production deployments still run on Java 8 or 11, but the pace of progress since those days has been significant. During the 1BRC challenge, we found that GraalVM is one damn fast JVM. It also supports compiling into a native binary, which eliminates the JVM startup cost. By simply downloading GraalVM and making it the default, my solution improved from 71 seconds to 66 — a solid 7.5% improvement for very little effort. When we get deeper into optimizing and bring down the runtime to 2-3 seconds, eliminating the JVM startup provides another 150-200ms in relief. That becomes a big deal. First profile, then optimize# Every successful 1BRC contestant used profiling of one kind or another to guide their optimization efforts. I used a combination of three tools: Good old VisualVM Async Profiler perf command-line tool Many people consider VisualVM outdated, but it harbors a hidden gem: The VisualGC plugin. You have to install it from the Tools→Plugins menu. Once you attach it to a running Java program, VisualGC shows up as the rightmost tab in the window. Be sure to select the shortest refresh period (100 ms), and then enjoy the show. A realtime animation of all the memory regions the garbage collector maintains, along with a graph of JIT compilations and GC runs will appear. I used to spend hours staring at this oddly satisfying, complex dance of GC's cogwheels. For the 1BRC program, I added a while (true) statement to keep processing the input file forever; otherwise things just flash by. The Async Profiler came from following Gunnar's advice on the 1BRC GitHub page. The jbang tool provides very convenient access to it. You run the program once, and get an HTML file with a flamegraph. The flamegraph then tells you which functions/methods your program is spending the most time in. The third tool, perf, has many features, but for Java the most popular choice is perf stat. It doesn't analyze any specific method, but gives you insight into low-level CPU counters. It shows: How many instructions it executed How many branches and memory accesses How many of those were branch/L1 cache misses. To receive these insights, I used the following command: perf stat -e branches,branch-misses,cache-references,cache-misses,cycles,instructions,idle-cycles-backend,idle-cycles-frontend,task-clock -- java --enable-preview -cp src Blog1 Copy VisualGC was the most useful in the initial optimization phase. Then, once I sorted out allocation, the flamegraph proved highly useful to pinpoint the bottlenecks in the code. However, once the runtime went below ~3 seconds, its usefulness declined. At this level we're squeezing out performance not from methods, but from individual CPU instructions. This is where perf stat became the best tool. For reference, here's the perf stat report for our basic implementation: 393,418,510,508 branches 3,112,052,890 branch-misses 26,847,457,554 cache-references 982,409,158 cache-misses 756,818,510,991 cycles 2,031,528,945,161 instructions Copy It's most helpful to interpret the numbers on a per-row basis (dividing everything by 1 billion). We can see that the program spends more than 2,000 instructions on each row. No need to get into more details; initially we'll be driving down just this metric. Optimization 1: Parallelize I/O# A quick profiling run with VisualVM and flamegraph reveals no clear bottleneck for our initial Streams API code. Note: Scroll down the flamegraph page to see the graph! The time divides roughly equally among three main tasks: BufferedReader work that outputs a string for each line Processing these lines Garbage collection (GC) VisualVM shows GC cycles running like crazy, 10 times per second or more. What's worse, there's some spilling to the Old generation, triggering background GC runs as well. We have to decide what to attack first. The first thing most of us at the challenge realized was that we have to parallelize the I/O. In the above code, a single thread does all the file reading and emits a stream of lines. This includes finding the newline and allocating a string for each line. On the other hand, the entire input file fits into the disk cache. It's a great target for parallelization. One tried-and-true approach is to split the file into as many chunks as there are threads, and process each chunk independently. Unfortunately, in order to take that step, we have to say goodbye to the concise Streams API and do everything by hand. We could read the chunks using the API of RandomAccessFile, but since it doesn't natively support buffered reading, it would be a quirky implementation that would involve copying from the native memory to the Java buffer. Instead, everyone went for the mmap approach. This meant that you would work with the file as if it was a large in-memory array. Java has supported mmap for a long time, relying on the ByteBuffer API to read the native memory. It uses int for indexing, limiting the mapped region size to 2 GB. The JDK team is currently introducing a newer API based on long indexing, MemorySegment. In the spirit of 1BRC, which encourages using the latest and greatest Java features, let's go with that: var raf = new RandomAccessFile(file, \"r\"); MemorySegment mappedFile = raf.getChannel().map( MapMode.READ_ONLY, 0, length, Arena.global() ); Copy There are some finicky details involved in finding the exact place to split the files, launch threads, wait on them, and so on. Tending to these details sees our code explode from the initial 17 lines to 120. You can review it here (for now we're using the commented-out \"Variant 1\" at line 84). Let's focus on a few key snippets. First, the hot loop now looks like this: for (var cursor = 0L; cursornew StationStats(name)); var intTemp = (int) Math.round(10 * temp); stats.sum += intTemp; stats.count++; stats.min = Math.min(stats.min, intTemp); stats.max = Math.max(stats.max, intTemp); cursor = newlinePos + 1; } Copy With the Streams API and BufferedReader gone, we run a hand-coded function, findByte(), to find the separator characters. This avoids creating a string for the whole line, but still creates strings for the name and the temperature, using the method named stringAt(). Here are these two methods: private long findByte(long cursor, int b) { for (var i = cursor; i > 3) + 3; findAcc(hash, nameStartOffset, nameLen, nameWord).observe(temperature); break; } hash = hash(hash, nameWord); nameLen += Long.BYTES; } } Copy And wait till you see the methods this calls into: private static final long BROADCAST_SEMICOLON = 0x3B3B3B3B3B3B3B3BL; private static final long BROADCAST_0x01 = 0x0101010101010101L; private static final long BROADCAST_0x80 = 0x8080808080808080L; private static long semicolonMatchBits(long word) { long diff = word ^ BROADCAST_SEMICOLON; return (diff - BROADCAST_0x01) & (~diff & BROADCAST_0x80); } private static int nameLen(long separator) { return (Long.numberOfTrailingZeros(separator) >>> 3) + 1; } // credit: artsiomkorzun private static long maskWord(long word, long matchBits) { long mask = matchBits ^ (matchBits - 1); return word & mask; } private static final long DOT_BITS = 0x10101000; private static final long MAGIC_MULTIPLIER = (100 * 0x1000000 + 10 * 0x10000 + 1); // credit: merykitty private static int dotPos(long word) { return Long.numberOfTrailingZeros(~word & DOT_BITS); } // credit: merykitty and royvanrijn private static int parseTemperature(long numberBytes, int dotPos) { // numberBytes contains the number: X.X, -X.X, XX.X or -XX.X final long invNumberBytes = ~numberBytes; // Calculates the sign final long signed = (invNumberBytes > 63; final int _28MinusDotPos = (dotPos ^ 0b11100); final long minusFilter = ~(signed & 0xFF); // Use the pre-calculated decimal position to adjust the values final long digits = ((numberBytes & minusFilter) >> 32) & 0x3FF; // And apply the sign return (int) ((absValue + signed) ^ signed); } Copy That's a lot of bit-twiddling magic, but note one general thing: there are almost no if statements, and that's the point. We replaced branch instructions with straight-through bitwise calculation. The CPU now tries to predict whether it will go into the \"then\" or the \"else\" branch for each if statement, based on previous iterations. As a result, it starts to decode the appropriate instructions before having all the data ready to evaluate the condition. So, whenever it gets it wrong, it has to discard all that work and start to decode the other instructions. As a rule of thumb, a single branch misprediction costs as much as 10-15 instructions. We also applied the SWAR idea: SIMD Within A Register, which means treating a long number as a vector of 8 byte values, and performing the same operation on each. In our case, semicolonMatchBits() locates the ASCII semicolon byte and returns a long with bits set to one where it was found. Then the method nameLen() turns that bit pattern into the number telling us where it is. This comes from a standard technique, used for example in C to efficiently determine the length of a zero-terminated string. Read a detailed explanation of a very similar approach in this insightful post Finding Null Terminators without Branches by Richard Startin. The method maskWord() takes a long containing 8 bytes of input data and zeroes out all the bytes beyond the semicolon. We need this to perform a fast name equality check. The algorithm in parseTemperature() and dotPos() is a genius creation by @merykitty (Quan Anh Mai), who made it specifically for this challenge. It leverages the properties of the bit patterns of ASCII - and ., as well as several other tricks, and produces the integer value of the two or three temperature digits, accounting for all four possible patterns (X.X, -X.X, XX.X and -XX.X) in one go. If you want to study it in more detail, keep in mind that the number string is stored in the long in little-endian order. For example, this line: long signed = (invNumberBytes > 63; isolates bit number 4 of the first byte – the one where the minus sign may appear – and sign-extends it across the long. This bit is 0 in the - sign, and 1 in all the digits. The operation is done after flipping all the bits (~numberBytes), so this becomes either all 1's if the byte is -, or all 0's otherwise. This parsing code deserves a blog post of its own, and it would distract us too much to explain it in detail here. Instead I've thrown in @merykitty's original code, and expanded his comments a bit more: // credit: merykitty // word contains the number: X.X, -X.X, XX.X or -XX.X private static int parseTemperatureOG(long word, int dotPos) { // signed is -1 if negative, 0 otherwise final long signed = (~word > 63; final long removeSignMask = ~(signed & 0xFF); // Zeroes out the sign character in the word long wordWithoutSign = word & removeSignMask; // Shifts so that the digits come to fixed positions: // 0xUU00TTHH00 (UU: units digit, TT: tens digit, HH: hundreds digit) long digitsAligned = wordWithoutSign >> 32) & 0x3FF; // Apply the sign. It's either all 1's or all 0's. If it's all 1's, // absValue ^ signed flips all bits. In essence, this does the two's // complement operation -a = ~a + 1. (All 1's represents the number -1). return (int) ((absValue ^ signed) - signed); } Copy All these techniques put together result in a 2.8x speedup. From 6.6 seconds, we're now down to 2.4 seconds. Our overall improvement is now 28x. As perf stat reports: 13,612,256,700 branches 656,550,701 branch-misses 3,762,166,084 cache-references 92,058,104 cache-misses 63,244,307,290 cycles 119,581,792,681 instructions Copy There is a huge drop in instruction count, by 3x. Since that's just 120 instructions per row now, we should look into making the same number of instructions execute faster. One thing stands out: there are 0.66 branch-misses per row. Can we do something about that? Optimization 5: Win with statistics# The flamegraph for our current solution indicates that the relative impact of methods hasn't changed much. The CPU spends 45% of its time total inside findAcc(), and just nameEquals() alone takes 19%: boolean nameEquals(long inputBase, long inputNameStart, long inputNameLen, long lastInputWord) { int i = 0; for (; i8 results in 50% branch mispredictions. But if we change the condition in that line of code to nameLen > 16, mispredictions drop to just 2.5%. Informed by this finding, it's clear that we have to write some code to avoid any branch instructions on the condition nameLen > 8, and instead go directly for nameLen > 16. To do that, we have to unroll the semicolon-searching loop along these lines: Perform the first two steps in one go, without checking any conditions Use bit-twiddling logic to combine the results of finding the semicolon in each of the two long words Use the combined result in an if check, which now accounts for all the initial 16 bytes We also need specialized variants of findAcc() and nameEquals() for the cases where the name is up to 16 bytes or more. In my solution, this reduces the time to 1.8 seconds — another 33% improvement, for the new total improvement of ~40x. And perf stat confirms our reasoning: 8,227,092,155 branches 84,323,925 branch-misses 3,219,383,623 cache-references 69,268,236 cache-misses 44,893,388,140 cycles 98,225,209,459 instructions Copy There's only a modest improvement in instructions per row, from 120 to 98. But look at \"branch-misses\"! It dropped almost 8 times, from 0.657 to 0.084 per row. This explains most of the speed improvement. Cache misses dropped as well, from 0.092 to 0.069 per row. This probably comes from the improved memory layout of our stats accumulator, which now stores the first 16 name bytes inside the class instance, and not in the separately-allocated array. Another metric that people like to track is instructions-per-cycle (IPC). We can see that it improved in this last step from 1.89 to 2.19. Reducing branch mispredictions means that the CPU has to do a lot less rework, discarding the speculatively executed instructions. This combines with the drop in instructions-per-row to explain the overall 33% improvement. Optimization 6: Eliminate startup/cleanup costs# If we want to compare our current result of 1.8 seconds to the winner, 1.5 seconds, we have to take into account the measurement methodology. All along this post, we've been reporting inner timings, that the code reports for itself. The outer timing, as measured in the contest, includes both the JVM startup and cleanup at the end. This adds 200 milliseconds - so we're actually at 2.0 seconds compared to 1.5 seconds. @thomaswue realized that around half of this time, 100 ms, is spent on unmapping the memory-mapped file after the output is already produced. He found a way to avoid paying for this with a hack, which was immediately copied by all the other top contenders. He started a subprocess that would actually do the work, so that the parent process could end as soon as it forwarded all the output. It would then leave the subprocess to clean up in the background. For this trick to work, contestants had to eliminate the JVM startup time as well, otherwise they'd pay for it twice. This would cancel out all the improvement! As a result, this forced everyone to also use ahead-of-time compilation into a native binary. With these two tricks added, our outer timing becomes almost identical to the inner timing we've been reporting, which means we are truly getting close! Optimization 7: Use smaller chunks and work stealing# At this point, we're deep into low-level optimization territory. The tricks that further improve the timing are coupled to the detailed specifics of everything, such as: CPU make and model Architecture of the connection to the RAM subsystem Minute details of the compiler Explaining all these tricks would get us deep into the weeds, and wouldn't be reusable knowledge. Instead let me show you one last trick that's kind of cute, and might come in handy in real-life scenarios. The way we divide the work into a single chunk per thread, we can end up with some threads getting \"luckier\" than others and completing sooner. When that happens, the CPU is underutilized for the remaining computation. To address this, we can introduce a small update that changes this to a larger number of small, fixed-size chunks. Up-front we'll only calculate the number of chunks, and then let the threads grab them and calculate their bounds when they're ready. The key element is in ensuring that every chunk gets processed exactly once. And the beauty of it is that it's almost a one-liner: static int chunkCount; static final AtomicInteger chunkSelector = new AtomicInteger(); ... // at program start: chunkCount = (int) ((file.length() / CHUNK_SIZE - 1) + 1); ... // in the worker thread: var selectedChunk = chunkSelector.getAndIncrement(); if (selectedChunk >= chunkCount) { break; } Copy And that's it! All the magic happens in the atomic counter we increment. This trick ekes out one last tenth of a second, down to 1.7 seconds. Conclusion# At the end of our 1BRC speedrun, we managed a 42x improvement over the Parallel Streams implementation on OpenJDK, from 71 seconds down to 1.7. You may notice that my official 1BRC result was quite a bit worse, at 2.3 seconds. The code in this post is different from what I submitted; some of it I wrote just for the post. It turned out that I had to choose between one last round of optimization at 1BRC, or giving full attention to the challenge I got while getting hired for QuestDB. I'm very glad I chose the latter! Performance optimizations we went through are certainly impressive, but do keep in mind that a lot of the gains come from dispensing with all the best practices that apply to production code: validations, bounds checks, hashtable resizing, and so on. The sole purpose of this code was to be fast at one very particularly specified, error-free input file. It has absolutely no tolerance for any kind of deviation, for example a single temperature reading that exceeds the three-digit maximum would cause it to completely lose track, and probably crash. But, coding challenges are meant to be fun — and everybody knows input validation is the opposite of fun! Download QuestDB Open source under Apache 2.0. Blazing fast ingest. SQL analytics.",
    "commentLink": "https://news.ycombinator.com/item?id=39467885",
    "commentBody": "The Billion Row Challenge (1BRC) – Step-by-Step from 71s to 1.7s (questdb.io)225 points by mfiguiere 19 hours agohidepastfavorite35 comments rldjbpin 8 minutes agothis was the most approachable commentary on a 1BRC submission i've read thus far. up to 6.6 seconds variant, i see the point of pushing the envelope of the programming environment and hacks that might help optimize future builds of the language. but beyond that the optimizations seems to be more about overcoming the inherent limitations of the environment, which are due to conciously made tradeoffs. i feel that once you hit that limit, we have reached the upper limit of the competition. reply haxen 17 hours agoprevCool, I see mfiguiere linked to my recent blog post! Let me share a few words about it... I took part in the One Billion Row challenge (1BRC). It was a lot of fun, but also a great learning experience. People came up with some pretty incredible optimization tricks. When you put them all together, it's a huge number, and they are all mingled up in individual solutions. They also happen on many levels -- from quite high, to incredibly low and detailed. In retrospect, I could see there was a good number of tricks that are relatively easy to grasp, and reusable in other projects. I felt the urge to do a writeup that captures this knowledge in one place, isolating and explaining each of the tricks. reply e12e 16 hours agoparentThank you for taking the time to write it up. Really nice to see some modern Java, as well as the ideas with comments - many of which seem to generalize quite well - beyond Java and the jvm. reply fearthetelomere 12 hours agoparentprevIf you do find the time, I'm sure that writeup would be very valuable! reply tristor 8 hours agorootparentThe linked article is that write-up, this is the author replying to someone else posting their blog article. reply scottlamb 14 hours agoprevFun read. I haven't used Java in a long time, so even the \"idiomatic Java code that would pass muster with any seasoned Java developer\" was a bit shocking. I know it's a cliche that someone brings up Rust in any programming thread, but I can't help myself. ;-) Several of the techniques here are easy enough in Rust that I just do them routinely. * Rayon makes it easy to process line-wise in reasonable-sized chunks without allocating a string per line. * The standard integer parsing stuff takes a slice, not an owned string. * The standard hash map doesn't have the same expectation \"that we pass in an instance of the key class that is equal to an existing instance\"; you can call HashMap::get with a &str. (One limitation is that std::collections::HashMap::entry does expect a K, so there's some redundancy if you need to insert afterward, but you could drop to hashbrown::HashMap::raw_entry_mut to avoid that.) reply haxen 14 hours agoparentI actually wrote a Rust version as well, and yes, it was far easier to write, far less code (although not incorporating all the tricks), completely safe, and pretty fast -- but still 2x slower than my end result in Java. HashMap was quite a bottleneck in Rust as well, for many reasons, not just the key allocation problem. But it was very easy to implement the same kind of custom hashtable, which almost by default ends up having a better memory layout than in Java. https://github.com/mtopolnik/rust-1brc/blob/main/src/main.rs I bet I could improve on it just a bit and match the Java time. Not sure about topping it, though. reply scottlamb 13 hours agorootparent> HashMap was quite a bottleneck in Rust as well, for many reasons, not just the key allocation problem. I'd be curious to hear more. Other than the default hash function being quite expensive, and occasionally wanting to use the more expressive hashbrown API, I've been happy with Rust's std::collections::HashMap. reply haxen 13 hours agorootparentThe reason the custom hashtable wins out isn't something generally applicable. For the very specific dataset used in the challenge, the hash function could be radically simplified, to just a single multiply and rotate left. To be fair, I didn't try out the hashbrown API. Maybe that, together with FxHash, would have been enough for the official dataset with 97% of keysFor the very specific dataset used in the challenge, the hash function could be radically simplified, to just a single multiply and rotate left. That's just a custom hash function, though; couldn't you do that with the standard std::collections::HashMap? reply haxen 11 hours agorootparentI didn't bother to try, so not sure. There would probably be some challenges and I don't see how I'd accomplish it without some branch instruction in the custom hasher. reply jasonjmcghee 13 hours agorootparentprevI thought everyone used hashbrown now https://github.com/rust-lang/hashbrown reply IshKebab 12 hours agorootparentThey do because the standard library implementation was changed to use Hashbrown. However the standard library API has a slight limitation in that you can't get say \"I have a reference to a string. If there's an entry for it, give me that. Otherwise clone the string and insert a new entry. Also only do one key lookup.\" You end up either having to do two lookups or always cloning the string even if you ended up not needing it. reply gunnarmorling 13 hours agoprevWhat an excellent post, one of the best on 1BRC I've come across so far. Big shout-out to Marko for participating in the challenge, making a strong push for clarifying corner cases of the rules, and sharing his experiences in this amazing write-up! reply haxen 13 hours agoparentThanks for the praise Gunnar, but we all owe it to you for organizing it, and especially sticking through thick and thin when it took off, and needed lots of attention to evaluate everyone and maintain a level playing field! reply tpmoney 8 hours agorootparentThe last part of the article raises an interesting question for me. What is the fastest, mostly fault tolerant implementation that could be created? So something that runs on say 10 different versions of the input, each of which has had up to 20 characters from the standard ASCII set inserted, replaced or removed randomly in the input file. So we'd be mostly looking for \"data corruption\" fault tolerance vs hardened against malicious input. How much can you still \"out optimize\" the compiler and JVM if you can't throw away all the safety? reply compsciphd 16 hours agoprevI wonder if using a trie instead of a hash would have provided a performance win. if you're parsing the file row by row, iterating over the trie as you process each character (as they argue to calculate the int value) (so what you have to do to hash it anyways), should be similar. What you'd end up in is micro-architectual issues on cache performance. reply o11c 13 hours agoparentThere's a reason everybody uses hashing - if your data is trusted, it does very few operations. Tries have to allocate, walk, and interpret multiple nodes. Perhaps not as bad as trees (though that depends on how many possible trie node representations there are, vs what the data density is at each level), but still worse than the non-colliding hash. That said, with a finite dataset a perfect hash would probably beat a general hash though. reply koliber 11 hours agorootparentWas the list of city names known? Could you statically pre-build the Trie structure? reply haxen 11 hours agorootparentIt was known, but the requirement was that the program keep working for an arbitrary keyset that conforms to the specified rules (up to 10,000 unique keys, each up to 100 bytes in length). reply paulddraper 13 hours agoparentprevTries are rarely the most efficient option. Kind of like linked lists...conceptually pleasant but rarely if ever the best option for real-world perf. reply anonymoushn 12 hours agoparentprevTop solutions tend to use hashes that ignore many input bytes. Even if they did not, they would use hashes that consume 4 or 8 bytes of input at a time (you could do more if the language exposed aesenc) reply haxen 15 hours agoparentprevInterestingly enough, that was my first idea. But when you consider the tiny keyset size, it would be hard to beat two machine instructions to calculate the hash + a single array lookup. reply gunnarmorling 13 hours agoparentprevThere is one entry which uses a trie, but it's not at the top, IIRC (which may or may not be related to using a trie, there's many other relevant design decisions). reply Keyframe 12 hours agoprevI must admit I only glanced at the article and in the past when topic appeared. How does this work, the speed of 1.7s I mean? Taking a look at input, let's say average entry is 16 bytes, there's billion of it; That's ~16GB. Average read speed of SSDs is what, ~500MB/s? Scanning alone at max throughput will take half a minute. This must be relying on DDR4+ read speeds which would probably come in under a second in certain cases. Is that's what's going on, RAM disk? reply haxen 12 hours agoparentI used to benchmark a lot on an enterprise-grade SSD 10 years ago, and that was already at 2 GB/s. Today, even my laptop's SSD supports multiple GB/s. But you're right about the contest -- each program was measured five times in a row, and there was enough RAM to fit the entire file into the page cache. The best time using all 32 cores (64 hyperthreads) on the evaluation machine was 323 milliseconds. reply Keyframe 12 hours agorootparentI went to the original repo, it's indeed a RAM disk. That makes it clear then. I was already almost excited about some IO wizardry going on. Results are determined by running the program on a Hetzner AX161 dedicated server (32 core AMD EPYC™ 7502P (Zen2), 128 GB RAM). Programs are run from a RAM disk (i.o. the IO overhead for loading the file from disk is not relevant), using 8 cores of the machine. reply haxen 12 hours agorootparentRunning without RAM cache would be a great followup to this challenge. I think a time around 2-3 seconds should be achievable. But, it would be highly sensitive to the hardware setup and how well the disk-reading code is placed on cores relative to the connection to the disk. Not sure what it would take to allow hundreds of contestants to benefit from the best arrangement. reply pulse7 12 hours agoparentprevPCIe 3.0 x4 SSD runs at ~3500MB/s, PCIe 4.0 x4 SSD runs at ~7000MB/s and PCIe 5.0 x4 SSD runs at ~14000MB/s (yes - megabytes, not megabits)! reply sroussey 11 hours agorootparent1 KIOXIA SSD on PCI 5 can do that 14000MB/s and 2M IOPS on 4 lanes. Some servers have 100+ lanes. You can have some serious speed if you want! reply Keyframe 12 hours agorootparentprevblazing! I had ye olde SSD SATA bricks in mind though. reply CyberDildonics 8 hours agorootparentprev14000MB/s? That's around 14,000,000 KB/s ! reply yread 1 hour agoprevHere is the .net version https://github.com/noahfalk/1brc a bit faster than fatest java still reply jiehong 6 hours agoprevVery nice! Always use profilers! In reality, though, data always have errors, and you get many validity checks in there, and data is never only ascii, but full of Unicode. I so wish python would come with the equivalent of jvisualvm in its “batteries included” (not even talking about better profilers even). reply teo_zero 3 hours agoprev [–] The bottom line is: of you want performance in Java, code if it was C! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "QuestDB organized a competition for optimizing Java code handling temperature data, won by Thomas Wuerthinger with parallel Java streams and modern JVMs for enhanced performance.",
      "Winning solution highlighted techniques like bit manipulation and efficient string length determination, significantly cutting down processing time.",
      "The event emphasized the significance of reducing branch-misses, enhancing memory layout, and splitting tasks into smaller segments for code optimization and the value of community collaboration in programming."
    ],
    "commentSummary": [
      "The Billion Row Challenge (1BRC) focuses on optimizing code to process significant amounts of data efficiently.",
      "Participants employed custom hash tables, optimized algorithms, and languages like Rust over Java for quicker development.",
      "The challenge showcased the significance of code optimization for performance and the benefits of exploring different languages and techniques for faster processing."
    ],
    "points": 225,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1708613347
  }
]
