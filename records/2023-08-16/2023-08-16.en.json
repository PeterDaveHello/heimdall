[
  {
    "id": 37137110,
    "title": "We reduced the cost of building Mastodon at Twitter-scale by 100x",
    "originLink": "https://blog.redplanetlabs.com/2023/08/15/how-we-reduced-the-cost-of-building-twitter-at-twitter-scale-by-100x/",
    "originBody": "Skip to content Blog HOME OPEN SOURCE ABOUT CAREERS ARCHIVE How we reduced the cost of building Twitter at Twitter-scale by 100x AUGUST 15, 2023 ~ NATHAN MARZ I’m going to cover a lot of ground in this post, so here’s the TLDR: We built a Twitter-scale Mastodon instance from scratch in only 10k lines of code. This is 100x less code than the ~1M lines Twitter wrote to build and scale their original consumer product, which is very similar to Mastodon. Our instance is located at https://mastodon.redplanetlabs.com and open for anyone to use. The instance has 100M bots posting 3,500 times per second at 403 average fanout to demonstrate its scale. Our implementation is built on top of a new platform called Rama that we at Red Planet Labs have developed over the past 10 years. This is the first time we’re talking about Rama publicly. Rama unifies computation and storage into a coherent model capable of building end-to-end backends at any scale in 100x less code than otherwise. Rama integrates and generalizes data ingestion, processing, indexing, and querying. Rama is a generic platform for building application backends, not just for social networks, and is programmed with a pure Java API. I will be exploring Rama in this post through the example of our Mastodon implementation. We spent nine person-months building our scalable Mastodon instance. Twitter spent ~200 person-years to build and scale their original consumer product, and Instagram spent ~25 person-years building Threads, a recently launched Twitter competitor. In their effort Instagram was able to leverage infrastructure already powering similar products. Our scalable Mastodon implementation is also significantly less code than Mastodon’s official implementation, which cannot scale anywhere near Twitter-scale. In one week we will release a version of Rama that anyone can download and use. This version simulates Rama clusters within a single process and can be used to explore the full Rama API and build complete prototypes. We will also release the Rama documentation at that time. In two weeks we will fully open-source our Mastodon implementation. Red Planet Labs will be starting a private beta soon to give companies access to the full version of Rama. We will release more details on the private beta later, but companies can apply here in the meantime. We recognize the way we’re introducing Rama is unusual. We felt that since the 100x cost reduction claim sounds so unbelievable, it wouldn’t do Rama justice to introduce it in the abstract. So we took it upon ourselves to directly demonstrate Rama’s 100x cost reduction by replicating a full application at scale in all its detail. Our Mastodon instance First off, we make no comment about whether Mastodon should be scalable. There are good reasons to limit the size of an individual Mastodon instance. It is our belief, however, that such decisions should be product decisions and not forced by technical limitations. What we are demonstrating with our scalable Mastodon instance is that building a complex application at scale doesn’t have to be a costly endeavor and can instead be easily built and managed by individuals or small teams. There’s no reason the tooling you use to most quickly build your prototype should be different from what you use to build your application at scale. Our Mastodon instance is hosted at https://mastodon.redplanetlabs.com. We’ve implemented every feature of Mastodon from scratch, including: Home timelines, account timelines, local timeline, federated timeline Follow / unfollow Post / delete statuses Lists Boosts / favorites / bookmarks Personalized follow suggestions Hashtag timelines Featured hashtags Notifications Blocking / muting Conversations / direct messages Filters View followers / following in order (paginated) Polls Trending hashtags and links Search (status / people / hashtags) Profiles Image/video attachments Scheduled statuses ActivityPub API to integrate with other Mastodon instances There’s huge variety between these features, and they require very different kinds of implementations for how computations are done and how indexes are structured. Of course, every single aspect of our Mastodon implementation is scalable. To demonstrate the scale of our instance, we’re also running 100M bot accounts which continuously post statuses (Mastodon’s analogue of a “tweet”), replies, boosts (“retweet”), and favorites. 3,500 statuses are posted per second, the average number of followers for each post is 403, and the largest account has over 22M followers. As a comparison, Twitter serves 7,000 tweets per second at 700 average fanout (according to the numbers I could find). With the click of a button we can scale our instance up to handle that load or much larger – it would just cost us more money in server costs. We used the OpenAI API to generate 50,000 statuses for the bots to choose from at random. Since our instance is just meant to demonstrate Rama and costs money to run, we’re not planning to keep it running for that long. So we don’t recommend using this instance for a primary Mastodon account. One feature of Mastodon that needed tweaking because of our high rate of new statuses was global timelines, as it doesn’t make sense to flood the UI with thousands of new statuses per second. So for that feature we instead show a small sample of all the statuses on the platform. The implementation of our instance looks like this: The Mastodon backend is implemented as Rama modules (explained later on this page), which handles all the data processing, data indexing, and most of the product logic. On top of that is our implementation of the Mastodon API using Spring/Reactor. For the most part, the API implementation just handles HTTP requests with simple calls to the Rama modules and serves responses as JSON. We use Soapbox to serve the frontend since it’s built entirely on top of the Mastodon API. S3 is used only for serving pictures and videos. Though we could serve those from Rama, static content like that is better served via a CDN. So we chose to use S3 to mimic that sort of architecture. All other storage is handled by the Rama modules. Our implementation totals 10k lines of code, about half of which is the Rama modules and half of which is the API server. We will be fully open-sourcing the implementation in two weeks. Our implementation is a big reduction in code compared to the official Mastodon implementation, which is built with Ruby on Rails. That codebase doesn’t always have a clear distinction between frontend and backend code, but just adding up the code for clearcut backend portions (models, workers, services, API controllers, ActivityPub) totals 18k lines of Ruby code. That doesn’t include any of the database schema definition code, configurations needed to run Postgres and Redis, or other controller code, so the true line count for the official Mastodon backend is higher than that. And unlike our Rama implementation, it can’t achieve anywhere near Twitter-scale. This isn’t a criticism of the Mastodon codebase – building products with existing technologies is just plain expensive. The reason we’ve worked on Rama for so many years is to enable developers to build applications much faster, with much greater quality, and to never have to worry about scaling ever again. Performance and scalability Here’s a chart showing the scalability of the most intensive part of Mastodon, processing statuses: As you can see, increasing the number of nodes increases the statuses/second that can be processed. Most importantly, the relationship is linear. Processing statuses is so intensive because of fanout – if you have 15M followers, each of your statuses has to be written to 15M timelines. Each status on our instance is written to an average of 403 timelines (plus additional work to handle replies, lists, and conversations). Twitter operates at 7,000 tweets per second at 700 average fanout, which is equivalent to about 12,200 tweets / second at 403 average fanout. So you can see we tested our Mastodon instance well above Twitter-scale. Here’s a chart showing the latency distribution for the time from a status being posted to it being available on follower timelines: These numbers are a bit better than Twitter’s numbers. Because of how unbalanced the social graph is, getting performance this good and this reliable is not easy. For example, when someone with 20M followers posts a status, that creates a huge burst of load which could delay other statuses from fanning out. How we handled this is described more below. Lastly, here’s a chart showing the latency distribution for fetching the data to render a user’s home timeline: Rendering a home timeline requires a lot of data from the backend: a page of statuses to render that aren’t muted/filtered, stats on each status (number of replies, boosts, and favorites), as well as information on the accounts that posted each status (username, display name, profile pic). Getting all this done in an average of 87ms is extremely efficient and a result of Rama being such an integrated system. Rama The numbers I’ve shared here should be hard to believe: a Twitter-scale Mastodon implementation with extremely strong performance numbers in only 10k lines of code, which is less code than Mastodon’s current backend implementation and 100x less code than Twitter’s scalable implementation of a very similar product? How is it possible that we’ve reduced the cost of building scalable applications by multiple orders of magnitude? You can begin to understand this by starting with a simple observation: you can describe Mastodon (or Twitter, Reddit, Slack, Gmail, Uber, etc.) in total detail in a matter of hours. It has profiles, follows, timelines, statuses, replies, boosts, hashtags, search, follow suggestions, and so on. It doesn’t take that long to describe all the actions you can take on Mastodon and what those actions do. So the real question you should be asking is: given that software is entirely abstraction and automation, why does it take so long to build something you can describe in hours? At its core Rama is a coherent set of abstractions for expressing backends end-to-end. All the intricacies of an application backend can be expressed in code that’s much closer to how you describe the application at a high level. Rama’s abstractions allow you to sidestep the mountains of complexity that blow up the cost of existing applications so much. So not only is Rama inherently scalable and fault-tolerant, it’s also far less work to build a backend with Rama than any other technology. Let’s now dive into Rama. We’ll start with a high-level overview of Rama’s concepts. Then we’ll look at how some of the most important parts of our Mastodon instance are implemented in terms of these concepts. Finally, we’ll look at some code from our Mastodon implementation. Rama is programmed entirely with a Java API, and Rama’s programming model has four main concepts: On the left are “depots”, which are distributed, durable, and replicated logs of data. All data coming into Rama comes in through depot appends. Depots are like Apache Kafka except integrated with the rest of Rama. Next are \"ETL\"s, extract-transform-load topologies. These process incoming data from depots as it arrives and produce indexed stores called “partitioned states”. Rama offers two types of ETL, streaming and microbatching, which have different performance characteristics. Most of the time spent programming Rama is spent making ETLs. Rama exposes a Java dataflow API for coding topologies that is extremely expressive. Next are “partitioned states”, which we usually call “PStates”. PStates are how data is indexed in Rama, and just like depots they’re partitioned across many nodes, durable, and replicated. PStates are one of the keys to how Rama is such a general-purpose system. Unlike existing databases, which have rigid indexing models (e.g. “key-value”, “relational”, “column-oriented”, “document”, “graph”, etc.), PStates have a flexible indexing model. In fact, they have an indexing model already familiar to every programmer: data structures. A PState is an arbitrary combination of data structures, and every PState you create can have a different combination. With the “subindexing” feature of PStates, nested data structures can efficiently contain hundreds of millions of elements. For example, a “map of maps” is equivalent to a “document database”, and a “map of subindexed sorted maps” is equivalent to a “column-oriented database”. Any combination of data structures and any amount of nesting is valid – e.g. you can have a “map of lists of subindexed maps of lists of subindexed sets”. I cannot emphasize enough how much interacting with indexes as regular data structures instead of magical “data models” liberates backend programming. The last concept in Rama is “query”. Queries in Rama take advantage of the data structure orientation of PStates with a “path-based” API that allows you to concisely fetch and aggregate data from a single partition. In addition to this, Rama has a feature called “query topologies” which can efficiently do real-time distributed querying and aggregation over an arbitrary collection of PStates. These are the analogue of “predefined queries” in traditional databases, except programmed via the same Java API as used to program ETLs and far more capable. Individually, none of these concepts are new. I’m sure you’ve seen them all before. You may be tempted to dismiss Rama’s programming model as just a combination of event sourcing and materialized views. But what Rama does is integrate and generalize these concepts to such an extent that you can build entire backends end-to-end without any of the impedance mismatches or complexity that characterize and overwhelm existing systems. All these concepts are implemented by Rama in a linearly scalable way. So if your application needs more resources, you can add them at the click of a button. Rama also achieves fault-tolerance by replicating all data and implementing automatic failover. Here’s an example of how these concepts fit together. These are all the depots, ETLs, PStates, and query topologies for the portion of our Mastodon implementation handling profiles, statuses, and timelines: This looks intimidating, but this part of the codebase only totals 1,100 lines of code. And it implements a ton of functionality, all scalably: statuses, timelines, boosts, conversations, favorites, bookmarks, mutes, account registration, profile edits, federation, and more. Notice that the PStates are a diverse collection of data structure combinations, and there are 33 of them here. Many of the ETLs produce multiple PStates and consume multiple depots. Making depots, ETLs, and PStates is inexpensive in Rama and can be done liberally. What you’re seeing in this diagram is a total inversion of control compared to how applications are typically architected today. For example, consider the “fanout” ETL in this diagram which processes incoming statuses and sends them to follower timelines (you’ll see the code for this later in this post). There are a bunch of rules dictating which statuses go to which followers – boosts never go back to the original author, replies only go to followers who also follow the account being replied to, and so on. Traditionally, that’s accomplished with a “database layer” handling storage and a separate “application layer” implementing the product logic. The “application layer” does reads and writes to the “database layer”, and the two layers are deployed, scaled, and managed separately. But with Rama, the product logic exists inside the system doing the indexing. Computation and storage are colocated. Rama does everything a database does, but it also does so much more. When building a backend with Rama, you begin with all the use cases you need to support. For example: fetch the number of followers of a user, fetch a page of a timeline, fetch ten follow suggestions, and so on. Then you determine what PState layouts (data structures) you need to support those queries. One PState could support ten of your queries, and another PState may support just one query. Next you determine what your source data is, and then you make depots to receive that data. Source data usually corresponds to events happening in your application, like “Alice follows Bob”, “James posted the status ‘Hello world’”, or “Bob unfollows Charlie”. You can represent your data however you want, whether Java objects, frameworks like Thrift or Protocol Buffers, or even unstructured formats like JSON (however, we recommend using structured formats as much as possible). The last step is writing the ETL topologies that convert source data from your depots into your PStates. When deployed, the ETLs run continuously keeping your PStates up to date. Rama’s ETL API, though just Java, is like a “distributed programming language” with the computational capabilities of any Turing-complete language along with facilities to easily control on which partition computation happens at any given point. You’ll see many examples of this API later in this post. Clusters and modules Rama is deployed onto a cluster of nodes. There’s a central daemon called the “Conductor” which coordinates deploys, updates, and scaling operations. Every node has a “Supervisor” daemon which manages the launch/teardown of user code. Applications are deployed onto a Rama cluster as “modules”. A “module” contains an arbitrary combination of depots, ETLs, PStates, and query topologies. Unlike traditional architectures, where the corresponding analogues exist in separate processes and usually on separate nodes, in Rama these are colocated in the same set of processes. This colocation enables fantastic efficiency which has never been possible before. Modules can also consume data from depots and PStates in other modules just as easily as they can from their own. A module runs forever, continuously processing new data from depots, unless you choose to destroy it. A module is deployed by giving the Conductor a .jar file with user code. Additionally, configuration is provided for the number of nodes to allocate to the module, replication parameters, as well as any other tuning parameters. A module is updated a similar way: a new .jar is provided with the new code, and the Conductor orchestrates an update sequence that launches new processes and transfers depots and PStates to the new module version. Here’s what a Rama cluster could look like with two modules deployed, “SocialGraphModule” and “TimelineModule”: For testing and development, Rama provides a class called InProcessCluster for simulating Rama clusters within a single process. Mastodon on Rama Let’s look at some of the key parts of how Mastodon is implemented on top of Rama. In this section we’ll focus on the design of Mastodon – what PStates are created, the flow of how data is processed, and how everything is organized. You’ll see how Rama’s capabilities enable some seriously novel ways to architect applications. In the next section we’ll look at some of the code from our implementation. Following hashtags Let’s start with an extremely simple part of the implementation, tracking followers for hashtags. The implementation for this totals 11 lines of code and supports the following queries: Does user A follow hashtag H? Who follows hashtag H (paginated)? How many followers does hashtag H have? Only a single PState is needed for this, called $$hashtagToFollowers (PState names in Rama always begin with $$ ). It is a map from a hashtag to a set of account IDs. Here’s a visualization of what this PState could look like across two partitions. Keep in mind that PStates are distributed across many partitions, with each partition of a PState being the specified data structure: There are two events that change this PState: following a hashtag, and unfollowing a hashtag. In our implementation, these are represented by the types FollowHashtag and RemoveFollowHashtag . A good way to visualize how data is processed to produce this PState is via a dataflow graph, as you can see how data moves from the start of processing (one or more depots) to the end results of processing (updates to PStates): The logic here is trivial, which is why the implementation is only 11 lines of code. You don’t need to worry about things like setting up a database, establishing database connections, handling serialization/deserialization on each database read/write, writing deploys just to handle this one task, or any of the other tasks that pile up when building backend systems. Because Rama is so integrated and so comprehensive, a trivial feature like this has a correspondingly trivial implementation. You may be wondering why the follow and unfollow events go onto the same depot instead of separate depots. Though you could implement it that way, it would be a mistake to do so. Data is processed off a depot partition in the order in which it was received, but there are no ordering guarantees across different depots. So if a user was spamming the “Follow” and “Unfollow” buttons on the UI and those events were appended to different depots, it’s possible a later unfollow could be processed before a prior follow. This would result in the PState ending up in the incorrect state according to the order by which the user made actions. By putting both events in the same depot, the data is processed in the same order in which it was created. As a general rule, Rama guarantees local ordering. Data sent between two points are processed in the order in which they were sent. This is true for processing data off a depot, and it’s also true for intra-ETL processing when your processing jumps around to different partitions as part of computation. This dataflow diagram is literally how you program with Rama, by specifying dataflow graphs in a pure Java API. As you’ll see below, the details of specifying computations like this involve variables, functions, filters, loops, branching, and merging. It also includes fine-grained control over which partitions computation is executing at any given point. Social graph Let’s now look at a slightly more involved part of the implementation, the social graph. The social graph totals 105 lines of code and supports the following queries which power various aspects of Mastodon: Does user A follow user B? How many followers does user A have? How many accounts does user A follow? Who are user A’s followers in the order in which they followed (paginated)? Who does user A follow in the order in which they followed them (paginated)? Who is currently requesting to follow user A in the order in which they requested (paginated)? Does user A block user B? Does user A mute user B? Does user A wish to see boosts from user B? Even though the implementation is so simple with Rama, it’s worth noting that Twitter had to write a custom database from scratch to build their scalable social graph. There are four main PStates produced by our social graph implementation, named $$followerToFollowees , $$followeeToFollowers , $$accountIdToFollowRequests , and $$accountIdToSuppressions . The first three PStates have the same structure, which is a map from account ID to a linked set of account IDs plus additional information needed about the relationship. For example, for $$followeeToFollowers we track whether a follower wants to see boosts from that account in their home timeline (which is one of Mastodon’s features). This PState is also used to compute: whether an account already follows another account, the order in which accounts were followed (which is why a linked set is used rather than a regular set), and the number of followers for an account (which is just the size of the inner set, something Rama computes in fast constant time even if the set has millions of elements). The $$accountIdToSuppressions PState tracks blocks and mutes for each account and has a different structure. It is a map from account ID to a map containing two keys: “blocked” and “muted”. The “blocked” key maps to the set of account IDs the top-level account ID has blocked. The “muted” key is similar but stores a map from account ID to the options for that mute (like expiration time). This PState is used wherever statuses are rendered (e.g. home timeline, notifications) to filter out statuses a user doesn’t want to see. Here’s a visualization of what the $$followeeToFollowers PState could look like in a deployed module: Here you can see that account “1” is followed by accounts “2”, “7”, and “5”, with “2” and “7” having boosts enabled and “5” having boosts disabled. Account “4” is only followed by account “1”, and “1” has boosts enabled for that relationship. The social graph is constructed based on follow, unfollow, block, unblock, mute, and unmute events. In Mastodon’s design, blocking a user also unfollows in both directions (if currently followed). So the block and unblock events go on the same depot as the follow-related events, while the mute and unmute events go on a separate depot. Here’s the dataflow graph showing how these PStates are computed based on the source data: This is more involved than the hashtag follows dataflow graph since this ETL supports so many more features. Yet it’s still only 105 lines of code to implement. You can see how this ETL makes use of conditionals, branching, and merging in its implementation. Here’s a few notes on the logic within this ETL: When a user follows another user, the $$followerToFollowees and $$followeeToFollowers PStates are updated. Unfollowing updates the same PStates but removes the relationship instead of adding it. Blocking is handled specially by implicitly emitting additional unfollow events as part of processing (to unfollow in both directions), as well as tracking who blocks who in the $$accountIdToSuppressions PState. A locked account (where each follower must be individually approved) receives FollowLockedAccount events, and unlocked accounts receive Follow events. Attributes of a relationship (e.g. “show boosts”, “notify”) are updated via appending another Follow or FollowLockedAccount event to the depot. This is why the FollowLockedAccount checks if the follow relationship already exists so it can determine whether to update the follow relationship or add a new follow request. A Follow event also removes the corresponding follow request if it exists. This is why an AcceptFollowRequest event just converts to a Follow event. This ETL interacts with many PStates at the same time. Because of Rama’s integrated nature, these PStates are colocated with one another within the same processes that are executing the ETL logic. So whereas you always have to do a network operation to access most databases, PState operations are local, in-process operations with Rama ETLs. As you’ll see later, you utilize the network in an ETL via “partitioner” operations to get to the right partition of a module, but once you’re on a partition you can perform as many PState operations to as many colocated PStates as you need. This is not only extremely efficient but also liberating due to the total removal of impedance mismatches that characterizes interacting with databases. Computing and rendering home timelines Next let’s look at the core of Mastodon, computing and rendering home timelines. This powers the primary page of the Mastodon experience: This use case is a great example of how to think about building data-intensive systems not just with Rama, but in general. For any backend feature you want to implement, you have to balance what gets precomputed versus what gets computed on the fly at query-time. The more you can precompute, the less work you’ll have to do at query-time and the lower latencies your users will experience. This basic structure looks like this: This of course is the programming model of Rama you’ve already seen, and a big part of designing Rama applications is determining what computation goes in the ETL portion versus what goes in the query portion. Because both the ETL and query portions can be arbitrary distributed computations, and since PStates can be any structure you want, you have total flexibility when it comes to choosing what gets precomputed versus what gets computed on the fly. It’s generally a good idea to work backwards from the needed queries to learn how to best structure things. In the case of querying for a page of a timeline, you need the following information: Content for each status Stats for each status (number of replies, boosts, and favorites) Information about the account that posted each status (username, display name, profile pic) Whether the author of each status is blocked or muted For boosts, the username of the booster ID to use to fetch the next page of statuses Since statuses can be edited and profile information can be changed, almost all this information is dynamic and must be fetched for each render of a timeline page. Let’s consider a typical way to go about this with non-Rama technologies, even scalable ones, which unfortunately is extremely inefficient: Fetch the list of status IDs for a page of the timeline In parallel, send database requests to fetch: Content for each status Stats for each status Information for each author For a page of 20 statuses, this could easily require over 100 database calls with a lot of overhead in the sheer amount of back and forth communication needed to fetch data from each database partition. We handled this use case with Rama by making use of Rama’s facilities for colocation of PStates. The module is organized such that all information for a status and the account who posted it are colocated in the same process. So instead of needing separate requests for status content, status stats, and author information, only one request is needed per status. Structuring the PStates Here are all the depots, PStates, and query topologies in the module implementing timelines and profiles (repeated from above): Let’s look at these PStates in closer detail, as the way they’re structured and partitioned is extremely interesting. Let’s start with $$accountIdToAccount . This is a map from account ID to profile information, including username, display name, and profile pic. Here’s a picture of what this PState could look like across four partitions: This PState is partitioned by the account ID. When I say “partitioned by”, I mean how the Mastodon implementation chooses on which partition to store data for any particular account. The most common way to partition a PState is to hash a partitioning key and modulo the hash by the number of partitions. This deterministically chooses a partition for any particular partitioning key, while evenly spreading out data across all partitions (this “hash/mod” technique is used by most distributed databases). For this PState, the partitioning key is the same as the key in the map, the account ID (as you’ll see soon, it doesn’t have to be). Next is $$accountIdToStatuses . This is a map from account ID to a map from status ID to a list of status content versions. A list of status content versions is stored to capture the edit history of a status, which Mastodon lets you view in its UI. You can visualize this PState like so: Whenever a status is edited, a new version is prepended to its respective inner list. Since this PState and $$accountIdToAccount are in the same module, each partition is colocated on the same thread within the same process. You can visualize this colocation like so: Because of this colocation queries can look at the partitions for both PStates at the same time within the same event instead of needing two roundtrips. You can also have multiple threads per worker process, or multiple worker processes per node. Now let’s see how things get really interesting. $$statusIdToFavoriters is a map from a status ID to a linked set of account IDs. Here’s a visualization of this PState: Similar to the social graph PStates, this PState is used to compute: the order in which accounts favorited a status (paginated), whether a particular account already favorited a status, and the number of favorites for a status. What’s interesting is how this PState is partitioned. It is not partitioned by the status ID, which is the key of the map. It is instead partitioned by the account ID of the user who posted that status, which is not even in the map! This same partitioning scheme is used for all other PStates tracking status information, like $$statusIdToReplies , $$statusIdToBoosters , and $$statusIdToMuters . This means all information for a user and all information for that user’s statuses exist on the same partition, and performing a query to fetch all the information needed to render a status only needs to perform a single roundtrip. Here’s a visualization of how all the PStates described could exist in a module deployment: Notice, for example, how status ID “3” for account ID “10” exists both as a subvalue within $$accountIdToStatuses and also as a top-level key for the status-specific states on the same partition. This use case is a great example of the power of Rama’s integrated approach, achieving incredible efficiency and incredible simplicity. Each of these PStates exists in exactly the perfect shape and partitioning for the use cases it serves. Home timelines Next, let’s explore how home timelines are stored. Materializing home timelines is by far the most intensive part of the application, since statuses are fanned out to all followers. Since the average fanout on our instance is 403, there are over 400x more writes to home timelines than any of the other PStates involved in processing statuses. PStates are durable, replicated, high-performance structures. They are easy to reason about and ideal for most use cases. In our initial implementation of Mastodon, we stored home timelines in a PState and it worked fine. However, writing to the home timelines PState was clearly the overwhelming bottleneck in the application because of the sheer amount of data that had to be written to disk and replicated. We also realized that home timelines are unique in that they are somewhat redundant with other PStates. You can reconstruct a home timeline by looking at the statuses of everyone you follow. This can involve a few hundred PState queries across the cluster, so it isn’t cheap, but it’s also not terribly expensive. As it turns out, what we ended up doing to optimize home timelines is very similar to how Twitter did it for their chronological timelines, and for the exact same reasons. In our revised implementation, we store home timelines in-memory and unreplicated. So instead of needing to persist each timeline write to disk and replicate it to followers, a timeline write is an extremely cheap write to an in-memory buffer. This optimization increased the number of statuses we could process per second by 15x. Solely storing the timeline in-memory is not sufficient though, as it provides no fault-tolerance in the event of a power loss or other node failure. So a new leader for the partition would not have the timeline info since it’s unreplicated and not persisted to disk. To solve this, we reconstruct the timeline on read if it’s missing or incomplete by querying the recent statuses of all follows. This provides the same fault-tolerance as replication, but in a different way. Implementing fault-tolerance this way is a tradeoff. For the benefit of massively reduced cost on timeline write, sometimes reads will be much more expensive due to the cost of reconstructing lost timelines. This tradeoff is overwhelmingly worth it because timeline writes are way, way more frequent than timeline reads and lost partitions are rare. Whereas Twitter stores home timelines in a dedicated in-memory database, in Rama they’re stored in-memory in the same processes executing the ETL for timeline fanout. So instead of having to do network operations, serialization, and deserialization, the reads and writes to home timelines in our implementation are literally just in-memory operations on a hash map. This is dramatically simpler and more efficient than operating a separate in-memory database. The timelines themselves are stored like this: 1 2 3 4 5 public static class Timeline { public long[] buffer; public int startIndex = 0; // index within buffer that contains oldest timeline element public int numElems = 0; // number of elements in this timeline } To minimize memory usage and GC pressure, we use a ring buffer and Java primitives to represent each home timeline. The buffer contains pairs of author ID and status ID. The author ID is stored along with the status ID since it is static information that will never change, and materializing it means that information doesn’t need to be looked up at query time. The home timeline stores the most recent 600 statuses, so the buffer size is 1,200 to accommodate each author ID and status ID pair. The size is fixed since storing full timelines would require a prohibitive amount of memory (the number of statuses times the average number of followers). Each user utilizes about 10kb of memory to represent their home timeline. For a Twitter-scale deployment of 500M users, that requires about 4.7TB of memory total around the cluster, which is easily achievable. The in-memory home timelines and other PStates are put together to render a page of a timeline with the following logic: First, query the in-memory home timeline to fetch a page of [author ID, statusID] pairs. Next, invoke a query topology (a predefined distributed query) that takes in a list of [author ID, statusID] pairs and returns all information needed to render each status – status content, status stats, and author information. The query topology goes to all partitions containing requested status IDs in parallel and fetches all needed information with colocated PState queries. Implementing fanout So that’s the story on how timelines are rendered, but how are home timelines and these various PStates computed? If you look back at the performance numbers, you can see our Mastodon implementation has high throughput that scales linearly while also having great, consistent latencies for delivering statuses to follower timelines. Let’s focus on how statuses are handled, particularly how timelines are materialized. Whenever someone posts a status, that status must be fanned out to all followers and appended to their home timelines. The tricky part is dealing with bursty load arising from how unbalanced the social graph can get. In our Mastodon instance, for example, the average fanout is 403, but the most popular user has over 22M followers. 3,500 statuses are posted each second, meaning that every second the system usually needs to perform 1.4M timeline writes to keep up. But if a user with 20M followers posts a status, then the number of timeline writes blows up by 15x to about 21.4M. With a naive implementation this can significantly delay other statuses from reaching follower timelines. And since the latency from posting a status to it reaching follower timelines is one of the most important metrics for this product, that’s really bad! This is essentially a problem of fairness. You don’t want very popular users to hog all the resources on the system whenever they post a status. The key to solving this issue is to limit the amount of resources a status from a popular user can use before allocating those resources to other statuses. The approach we take in our implementation is: For each iteration of timeline processing, fan out each status to at most 64k followers. If there are more followers left to deliver to for a status, add that status to a PState to continue fanout in the next iteration of processing. With this approach a status from a user with 20M followers will take 312 iterations of processing to complete fanout (about 3 minutes), and fairness is achieved by giving all statuses equal access to resources at any given time. Since the vast majority of users have less than 64k followers, most users will see their statuses delivered in one iteration of processing. Statuses from popular users take longer to deliver to all followers, but that is the tradeoff that has to be made given that the amount of resources is fixed at any given time. As a side note, Twitter also has special handling for users with lots of followers to address the exact same issue. With the basic approach understood, let’s look specifically at how statuses are processed in our implementation to materialize timelines. Here’s a dataflow diagram showing the logic: This dataflow diagram is a little bit different than the social graph one, as this ETL is implemented with microbatching while the social graph is implemented with streaming. Streaming processes data directly off of depots as it arrives, while microbatching processes data in small batches. “Start iteration” in this diagram signifies the beginning of a microbatch. The tradeoffs between streaming and microbatching are too much to cover for this post, but you’ll be able to learn more about that next week when we release the documentation for Rama. In this diagram you can see all the steps involved in delivering statuses to followers. There are many product rules implemented here, such as: only fan out statuses with the correct visibility, only send replies to followers who also follow the account being replied to, respect “show boost” settings, and so on. This dataflow diagram only shows the computation of home timelines, whereas in reality this ETL also handles lists, hashtag timelines, and conversations. Those all work similarly to home timelines and are just additional branches of dataflow computation with slightly differing logic. Processing skew from unbalanced social graph Fairness issues aren’t the only problem caused by an unbalanced social graph. Another problem is skew: a naive implementation that handles all fanout for a single user from a single partition will lead to some partitions of a module having a lot more overall work to do than others. Without balanced processing, throughput is lowered since some resources are idle while others are being overwhelmed. In our Mastodon implementation, we put significant effort into balancing fanout computation regardless of how unbalanced a social graph gets. This is a deep topic, so we will explore this further in a subsequent blog post. The optimizations we did in this area increased throughput by about 15%. Personalized follow suggestions Let’s now look at another part of Mastodon which works completely differently than timelines: personalized follow suggestions. This powers this section on Mastodon: Everything about how data is processed and indexed for follow suggestions is different from what we looked at in the previous sections. Taken together, the implementations of timelines, the social graph, and follow suggestions demonstrate how expressive Rama is as a system. This generality is a result of the total arbitrariness with which you can write ETL computations, structure indexes, and compute queries. Unlike the social graph and timelines, the behavior of personalized follow suggestions could be specified in very different ways. We’ve chosen to determine follow suggestions like so: Rank accounts to suggest based on who’s most followed by the accounts you already follow Don’t suggest accounts you already follow If this doesn’t produce enough suggestions (e.g. because you don’t follow many accounts yet), suggest the most followed accounts on the platform We took a different approach than Mastodon for follow suggestions – the API docs describe follow suggestions as “Accounts that are promoted by staff, or that the user has had past positive interactions with, but is not yet following.” We chose our approach because it’s much more difficult to implement and thus a better demonstration of Rama. Our implementation of personalized follow suggestions totals 141 lines of code. The main PState underlying follow suggestions is called $$whoToFollow , a map from account ID to a list of up to 80 account ID suggestions. The interesting part of the follow suggestions implementation is how this PState is computed and maintained. Follow suggestions can’t be computed fully incrementally, at least not practically. That is, you can’t receive a new follow or unfollow event and incrementally update all the follow suggestions for affected accounts. Computing follow suggestions is a batch operation that needs to look at everyone an account follows, and everyone they follow, at the same time in a single computation. With that in mind, there are a few pieces to our implementation. First, everyone’s follow suggestions are recomputed on a regular basis. The ETL for follow suggestions recomputes the suggestions for 1,280 accounts every 30 seconds. Since there are 100M accounts, this means each account has its suggestions updated every 27 days. In addition to this, we have special handling for new users. When a new user signs up, you want to provide good follow suggestions as soon as possible in order to increase engagement and increase the chance they’ll continue to use the service. So you don’t want to wait 27 days to compute personalized suggestions for a new user. At the same time, you can’t produce good personalized suggestions until the user has followed at least a few accounts. So our implementation tracks milestones which trigger immediate recomputation of follow suggestions: when a user follows 10 accounts and when a user follows 100 accounts. Structuring the PStates Here are all the depots, PStates, and query topologies related to the follow suggestions implementation: Notice that this ETL also consumes followAndBlockAccountDepot , which is the same depot as consumed by the social graph ETL to produce the social graph PStates. Depots are sources of truth that can be consumed by as many topologies as you need. Here’s what each PState for follow suggestions is used for: $$whoToFollow : As described above, this stores a list of up to 80 suggestions for each user. $$nextId : This keeps track of the next group of accounts for which to recompute follow suggestions. This stores a Long on each partition that points to a key within the colocated $$followerToFollowees PState partition. $$followCounts : This is a map from account ID to the number of follow actions that account has taken. This is used to track when a user has passed 10 or 100 follows and trigger an immediate recompute of their follow suggestions. $$forceRecomputeUsers : This is the set of users (a set per partition) that have recently passed the 10 or 100 follows milestone. Accounts chosen for follow suggestion recomputes come from this PState as well as where $$nextId is pointing in $$followerToFollowees . $$topFollowedUsers : This is a global list of the top followed users on the platform. These are used to supplement a user’s follow suggestions when not enough are computed via the personalized method. Notice how some of these PStates are not maps at the top-level, which may feel unusual given that pretty much every database that’s ever existed is map-based (with a “key” being the central concept to identify a record or row). But just as data structures other than maps are useful for everyday programming, data structures other than maps are useful for backend programming as well. Computing follow suggestions Let’s explore in more detail how follow suggestions are recomputed. Unlike the other ETLs we’ve described, this one initiates computations based on time and not on the receipt of data. Every 30 seconds it needs to recompute follow suggestions for a rotating subset of all users and for any users specified in the $$forceRecomputeUsers PState. You can think of time as an infinite stream of events, with each event being an instant in time. Rama exposes a special depot for time called a “tick depot” which emits events according to a specified frequency. For follow suggestions, a tick depot is used to trigger processing every 30 seconds. The subsequent computation then uses the PStates described to recompute follow suggestions for a subset of users. The dataflow looks like this: The key PState is $$followerToFollowees . The ETL selects a subset of the keys in that map for processing, and it stores the last key chosen in $$nextId . The next iteration will start from that key. When it gets to the end of the PState, it starts over again from the beginning. The rest of the processing uses $$followerToFollowees to fetch follows, fetch the follows of those follows, and aggregate a list of candidates along with how many times each candidate is followed among that subset of users. After filtering out candidates the starting account already follows, the $$whoToFollow PState is updated. Every step of this is done in parallel. So when a subset of users is selected from $$followerToFollowees , it’s actually selecting a subset of users from each partition of that PState. In between recomputes, a user may have followed some of the users in their list of suggestions. This is handled with a query topology that filters a user’s follow suggestions to exclude users they already follow. DevOps with Rama Let’s briefly take a look at how we do DevOps with Rama: managing the deployment, monitoring, and operation of modules in production. Since the steps are the same for all modules, we’ll use as an example how we manage the module handling statuses, timelines, and profiles. The module is implemented in the class com.rpl.mastodon.modules.Core and is deployed to a Rama cluster like so: 1 2 3 4 5 6 7 8 9 rama deploy \\ --action launch \\ --jar target/mastodon.jar \\ --module com.rpl.mastodon.modules.Core \\ --tasks 128 \\ --threads 32 \\ --workers 16 \\ --replicationFactor 3 \\ --configOverrides overrides.yaml This submits the module and its code to the cluster with the given parallelism, and Rama then launches worker processes around the cluster to run the module. The same cluster is shared by all modules. Once the workers finish starting up, they start reading from depots, executing ETLs, updating PStates, serving query requests, and so on. The “replication factor” specifies to how many nodes each depot and PState should replicate its data. Replication happens completely behind the scenes and provides automatic failover in case of failures (e.g. hardware issues). Rama provides very strong guarantees with replication – data is not made visible for consumption from depots or PStates until it has been successfully replicated. The referenced overrides.yaml file has only two lines and just registers with Rama how to serialize/deserialize the custom types used by our implementation (defined using Thrift, described more below). After the module finishes launching, the Cluster UI for Rama starts displaying telemetry on the module: Rama tracks and displays telemetry for the module as a whole, as well as specific telemetry for each topology, depot, and PState. The telemetry is extremely useful for understanding the performance of a module and when it needs to be scaled. Rama uses itself to implement telemetry – a built-in module collects telemetry data from all modules into a depot, processes that data with an ETL, and indexes the results into PStates arranged in a time-series structure. When we want to update the module to add a feature (e.g. add a new PState) or fix a bug, we run a command like the following: 1 2 3 4 5 rama deploy \\ --action update \\ --jar target/mastodon.jar \\ --module com.rpl.mastodon.modules.Core \\ --configOverrides overrides.yaml This launches a carefully coordinated automated procedure to launch new worker processes and handoff responsibility for depots and PStates to the new version of the module. Clients of the module doing depot appends and PState queries don’t need to be updated and automatically transition themselves to the new module version. Similarly, when we want to scale the module to have more resources, we run a command like the following: 1 2 3 rama scaleExecutors \\ --module com.rpl.mastodon.modules.Core \\ --workers 24 This launches a similar procedure as module update to transition the module to the new version. And that’s all there is to DevOps with Rama – it’s just a few commands at the terminal to manage everything. You don’t need to invest huge amounts of time writing piles of shell scripts to coordinate changes across dozens of systems. Since Rama is such a cohesive, integrated system it’s able to automate deployment entirely, and it’s able to provide deep and detailed runtime telemetry without needing to lift a finger. Simple Rama code example Let’s look at some code! Before diving into the Mastodon code, let’s look at a simple example of coding with Rama to gain a feeling for what it’s like. I’m not going to explain every last detail in this code – the API is so rich that it’s too much to explain for this post. Instead, I’ll do my best to summarize what the code is doing. In one week we will be releasing all the documentation for Rama, and this includes a six part tutorial that gently introduces everything. We will also be releasing a build of Rama that anyone can download and use. This build will be able to simulate Rama clusters within a single process but will not be able to run distributed clusters. It has the full Rama API and can be used to experiment with Rama. Once we open-source our Mastodon implementation in two weeks, you’ll be able to run it within a single process using this build. With that said, let’s look at a simple example. Here’s the complete definition for a “word count module”, which accepts sentences as input and produces a single PState containing the count of all words in those sentences: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class WordCountModule implements RamaModule { @Override public void define(Setup setup, Topologies topologies) { setup.declareDepot(\"*sentenceDepot\", Depot.random()); StreamTopology wordCount = topologies.stream(\"wordCount\"); wordCount.pstate(\"$$wordCounts\", PState.mapSchema(String.class, Long.class)); wordCount.source(\"*sentenceDepot\").out(\"*sentence\") .each((String sentence, OutputCollector collector) -> { for(String word: sentence.split(\" \")) { collector.emit(word); } }, \"*sentence\").out(\"*word\") .hashPartition(\"*word\") .compoundAgg(\"$$wordCounts\", CompoundAgg.map(\"*word\", Agg.count())); } } This module has one ETL named wordCount , one depot named *sentenceDepot , and one PState named $$wordCounts . The ETL receives new sentences from the depot, tokenizes those sentences into words, and then updates the counts for those words in the PState. The PState partitions are updated within a few milliseconds of appending a sentence to the depot. A module implements the interface RamaModule that has a single method define on it. setup is used to declare depots and any dependencies to depots or PStates in other modules, and topologies is used to declare all ETL and query topologies. The first line of define declares the depot. Depot names always begin with a * . Strings beginning with * are interpreted as variables in Rama code, and they can be passed around and used just like variables in any programming language. The second argument Depot.random() specifies the partitioning scheme of the depot. In this case the partitioning scheme causes appended sentences to go to a random partition of the depot. When local ordering is important, like for follow and unfollow events, the partitioning scheme would be set appropriately so events for the same entity go to the same partition. The next line declares the ETL wordCount as a streaming topology. After that is the declaration of the PState $$wordCounts . The PState is declared with a schema that specifies what it stores and how it stores it. In this case it’s just a simple map, but you can specify whatever structure you want here (e.g. a map of subindexed maps of lists of subindexed sets). Lastly is the definition of the ETL. The line wordCount.source(\"*sentenceDepot\").out(\"*sentence\") subscribes the ETL to *sentenceDepot and binds any new sentences received to the variable *sentence . The next line tokenizes each sentence into words. Java code is inserted with a lambda to split each sentence on whitespace and emit each word individually as the variable *word . Inserting arbitrary Java code into topologies like this is extremely common. The next line .hashPartition(\"*word\") relocates the dataflow to the partition of the module storing the counts for that word. The code before that line and after that line can execute on different machines, and Rama takes care of all the serialization and network transfer involved in moving the computation. Finally, now that the computation is on the correct partition, the last line updates the count for the word in the PState. This PState update is specified in the form of an aggregation template – in this case it says it’s aggregating a map where the key is the word and the value is the count of all events seen for that word. This is such a basic example that it doesn’t really do justice to the expressive power of Rama. However, it does demonstrate the general workflow of declaring modules, depots, PStates, and topologies. Some of the functionality not shown here includes: consuming depots/PStates from other modules, query topologies, microbatching, branching/merging, joins, loops, shadowing variables, conditionals, and decomposing code with macros. Let’s now take a look at interacting with Rama modules as a client outside the cluster, similar to how you interact with a database using a database client. Here’s code that connects to a remote cluster, creates handles to the depot and PState of the module, appends some sentences, and then does some PState queries: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Map config = new HashMap(); config.put(\"conductor.host\", \"1.2.3.4\"); RamaClusterManager manager = RamaClusterManager.open(config); Depot depot = manager.clusterDepot(\"rama.examples.wordcount.WordCountModule\", \"*sentenceDepot\"); depot.append(\"hello world\"); depot.append(\"hello world again\"); depot.append(\"say hello to the planet\"); depot.append(\"red planet labs\"); PState wc = manager.clusterPState(\"rama.examples.wordcount.WordCountModule\", \"$$wordCounts\"); System.out.println(\"'hello' count: \" + wc.selectOne(Path.key(\"hello\"))); System.out.println(\"'world' count: \" + wc.selectOne(Path.key(\"world\"))); System.out.println(\"'planet' count: \" + wc.selectOne(Path.key(\"planet\"))); System.out.println(\"'red' count: \" + wc.selectOne(Path.key(\"red\"))); RamaClusterManager is used to connect to a cluster and retrieve handles to depots and PStates. Depots and PStates are identified by their module name (the class name of the module definition) and their name within the module. By default, depot appends block until all colocated streaming topologies have finished processing the appended data. This is why the PState queries can be executed immediately following the depot appends without further coordination. The PState queries here fetch the values for the specified keys. PStates are queried using Rama’s “Path” API, and this example barely scratches the surface of what you can do with paths. They allow you to easily reach into a PState, regardless of its structure, and retrieve precisely what you need – whether one value, multiple values, or an aggregation of values. They can also be used for updating PStates within topologies. Mastering paths is one of the keys to mastering Rama development. Let’s now take a look at how you would run WordCountModule in a unit test environment: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public void wordCountTest() throws Exception { try (InProcessCluster cluster = InProcessCluster.create()) { cluster.launchModule(new WordCountModule(), new LaunchConfig(4, 2)); String moduleName = WordCountModule.class.getName(); Depot depot = cluster.clusterDepot(moduleName, \"*sentenceDepot\"); depot.append(\"hello world\"); depot.append(\"hello world again\"); depot.append(\"say hello to the planet\"); depot.append(\"red planet labs\"); PState wc = cluster.clusterPState(moduleName, \"$$wordCounts\"); System.out.println(\"'hello' count: \" + wc.selectOne(Path.key(\"hello\"))); System.out.println(\"'world' count: \" + wc.selectOne(Path.key(\"world\"))); System.out.println(\"'planet' count: \" + wc.selectOne(Path.key(\"planet\"))); System.out.println(\"'red' count: \" + wc.selectOne(Path.key(\"red\"))); } } Running this code prints: 1 2 3 4 'hello' count: 3 'world' count: 2 'planet' count: 2 'red' count: 1 InProcessCluster simulates a Rama cluster completely in-process and is ideal for unit testing modules. Here you can see how InProcessCluster is used to launch the module and then fetch depots/PStates just like with RamaClusterManager . There’s no difference in the functionality available with InProcessCluster versus a real cluster, and you’ll be able to try out InProcessCluster next week when we release the non-production build of Rama. Sample code from our Mastodon implementation Now let’s look at some code from our Mastodon implementation. We’ll be looking at bigger code samples in this section utilizing a lot more of the Rama API, so even more than the last section I won’t be able to explain all the details of the code. I’ll summarize what the key parts are, and you’ll be able to learn all the details next week when we release the documentation. Please don’t be too intimidated by this code. There are a lot of concepts and API methods at work here, and no one could possibly understand this code completely at a first glance. This is especially true without the accompanying documentation. I’m showing this code because it ties together the high-level concepts I’ve discussed in this post by making them real instead of abstract. Representing data Let’s start by looking at an example of how data is defined. We chose to represent data using Thrift since it has a nice schema definition language and produces efficient serialization, but you can just as easily use plain Java objects, Protocol Buffers, or anything else you want. We use Thrift-defined objects in both depots and PStates. Here’s how statuses are defined: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 typedef i64 AccountId typedef i64 StatusId typedef i64 Timestamp enum StatusVisibility { Public = 1, Unlisted = 2, Private = 3, Direct = 4 } enum AttachmentKind { Image = 1, Video = 2 } struct StatusPointer { 1: required AccountId authorId; 2: required StatusId statusId; 3: optional Timestamp timestamp; 4: optional bool shouldExclude; } struct PollContent { 1: list choices; 2: required Timestamp expiration; 3: required bool multipleChoice; } struct Attachment { 1: required AttachmentKind kind; 2: required string extension; 3: required string description; } struct AttachmentWithId { 1: required string uuid; 2: required Attachment attachment; } struct NormalStatusContent { 1: required string text; 2: required StatusVisibility visibility; 3: optional PollContent pollContent; 4: optional list attachments; 5: optional string sensitiveWarning; } struct ReplyStatusContent { 1: required string text; 2: required StatusVisibility visibility; 3: required StatusPointer parent; 4: optional PollContent pollContent; 5: optional list attachments; 6: optional string sensitiveWarning; } struct BoostStatusContent { 1: required StatusPointer boosted; } union StatusContent { 1: NormalStatusContent normal; 2: ReplyStatusContent reply; 3: BoostStatusContent boost; } struct Status { 1: required AccountId authorId; 2: required StatusContent content; 3: required Timestamp timestamp; 4: optional string remoteUrl; 5: optional string language; } Every type of status, including boosts, replies, and statuses with polls is represented by this definition. Being able to represent your data using normal programming practices, as opposed to restrictive database environments where you can’t have nested definitions like this, goes a long way in avoiding impedance mismatches and keeping code clean and comprehensible. Following hashtags code Next, let’s look at the entire definition of following hashtags (described earlier in this post). As a reminder, here’s the dataflow diagram for the following hashtags ETL: Here’s the implementation, which is a direct translation of the diagram to code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 setup.declareDepot(\"*followHashtagDepot\", Depot.hashBy(ExtractToken.class)); StreamTopology stream = topologies.stream(\"relationshipsStream\"); stream.pstate(\"$$hashtagToFollowers\", PState.mapSchema(String.class, PState.setSchema(Long.class).subindexed())); stream.source(\"*followHashtagDepot\", StreamSourceOptions.retryAllAfter()).out(\"*data\") .subSource(\"*data\", SubSource.create(FollowHashtag.class) .macro(extractFields(\"*data\", \"*accountId\", \"*token\")) .localTransform(\"$$hashtagToFollowers\", Path.key(\"*token\").voidSetElem().termVal(\"*accountId\")), SubSource.create(RemoveFollowHashtag.class) .macro(extractFields(\"*data\", \"*accountId\", \"*token\")) .localTransform(\"$$hashtagToFollowers\", Path.key(\"*token\").setElem(\"*accountId\").termVoid())); This is extremely simple. subSource branches the dataflow graph based on the type of an object. In this code the object in *data can be one of two types, and there is a separate branch of dataflow for each type. When a FollowHashtag event is received, that account is added to the set of followers for that hashtag. When a RemoveFollowHashtag event is received, that account is removed from the set of followers for that hashtag. Because the nested sets are subindexed, they can efficiently contain hundreds of millions of elements or more. extractFields is a helper function in the Mastodon implementation for extracting fields out of Thrift objects by name and binding them to corresponding Rama variables of the same name. So extractFields(\"*data\", \"*accountId\", \"*token\")) extracts the fields “accountId” and “token” from the Thrift object in *data and binds them to the variables *accountId and *token . extractFields is implemented as a Rama macro, which is a utility for inserting a snippet of dataflow code into another section of dataflow code. It is a mechanism for code reuse that allows the composition of any dataflow elements: functions, filters, aggregation, partitioning, etc. Unlike word count, this code uses paths instead of aggregators to define the writes to the PStates, which is the same API used to read from PStates. You’ll be able to learn more next week when we release Rama’s documentation about the differences between aggregators and paths and when to prefer one over the other. Note that this code defines a parallel computation just like the word count example earlier. The code runs across many nodes to process data off each partition of the depot and update the PState. Any failures (e.g. a node dying) are handled transparently and Rama guarantees all depot data will be fully processed. The partitioning is defined at the depot level ( Depot.hashBy(ExtractToken.class) ), so when the ETL begins processing a piece of data, the computation is already located on the partition of the module storing followers for that hashtag. So no further partitioning is needed in the ETL definition. Social graph code Next, let’s look at the entire definition of the social graph as described earlier. Here was the dataflow diagram for the social graph ETL: Like hashtag follows, the social graph implementation is also a direct translation of the diagram to code. Since the code for this is longer, let’s look at it section by section in the order in which it’s written. The first part is the declaration of the depots, topology, and PStates: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 setup.declareDepot(\"*followAndBlockAccountDepot\", Depot.hashBy(ExtractAccountId.class)); setup.declareDepot(\"*muteAccountDepot\", Depot.hashBy(ExtractAccountId.class)); StreamTopology stream = topologies.stream(\"relationshipsStream\"); KeyToLinkedEntitySetPStateGroup accountIdToFollowRequests = new KeyToLinkedEntitySetPStateGroup(\"$$accountIdToFollowRequests\", Long.class, FollowLockedAccount.class) .entityIdFunction(Long.class, req -> ((FollowLockedAccount) req).requesterId) .descending(); accountIdToFollowRequests.declarePStates(stream); KeyToLinkedEntitySetPStateGroup followerToFollowees = new KeyToLinkedEntitySetPStateGroup(\"$$followerToFollowees\", Long.class, Follower.class) .entityIdFunction(Long.class, new ExtractAccountId()) .descending(); KeyToLinkedEntitySetPStateGroup followeeToFollowers = new KeyToLinkedEntitySetPStateGroup(\"$$followeeToFollowers\", Long.class, Follower.class) .entityIdFunction(Long.class, new ExtractAccountId()) .descending(); followerToFollowees.declarePStates(stream); followeeToFollowers.declarePStates(stream); stream.pstate(\"$$accountIdToSuppressions\", PState.mapSchema(Long.class, PState.fixedKeysSchema(\"muted\", PState.mapSchema(Long.class, MuteAccountOptions.class).subindexed(), \"blocked\", PState.setSchema(Long.class).subindexed()))); Note that both hashtag follows and the social graph are part of the same stream topology. The social graph implementation consumes different depots than hashtag follows does, so the code is otherwise completely independent. The $$followerToFollowees and $$followeeToFollowers PStates are defined with KeyToLinkedEntitySetPStateGroup , which defines the “map to linked set” data structure abstraction as the composition of multiple, more primitive PStates underneath the hood. Its implementation is only 68 lines of code. The next part defines the root of processing where the branching occurs at the start of the dataflow diagram: 1 2 stream.source(\"*followAndBlockAccountDepot\", StreamSourceOptions.retryAllAfter()).out(\"*initialData\") .anchor(\"SocialGraphRoot\") As we build up the code for the social graph, let’s also take a look visually at how the dataflow diagram is filled out. This code starts off the dataflow diagram like this: anchor defines a location in a dataflow graph that can later be hooked onto with hook . The next section defines the first branch of processing: 1 2 .each(Ops.IDENTITY, \"*initialData\").out(\"*data\") .anchor(\"Normal\") This branch passes all data through to the anchor “Normal”, which will later be merged with other branches as you can see in the dataflow diagram. Rama provides the Ops class which has commonly used functions to use within dataflow code. This includes math operations, comparators, and other utilities. Here Ops.IDENTITY is used which emits its input unchanged. The next section defines the branch handling implicit unfollow events for block events: 1 2 3 4 5 6 7 8 .hook(\"SocialGraphRoot\") .keepTrue(new Expr(Ops.IS_INSTANCE_OF, BlockAccount.class, \"*initialData\")) .each((BlockAccount data, OutputCollector collector) -> { collector.emit(new RemoveFollowAccount(data.getAccountId(), data.getTargetId(), data.getTimestamp())); collector.emit(new RemoveFollowAccount(data.getTargetId(), data.getAccountId(), data.getTimestamp())); collector.emit(new RejectFollowRequest(data.getAccountId(), data.getTargetId())); }, \"*initialData\").out(\"*data\") .anchor(\"ImplicitUnfollow\") This uses hook to create a branch off the root of processing for this depot that was defined in the previous code section. The keepTrue line continues processing on this branch only for block events. It then generates implicit events to unfollow in both directions and remove a follow request if it exists. Lastly, the “ImplicitUnfollow” anchor is declared which will later be used to merge this branch together with “Normal” and other branches. The next section defines the branch handling accepting a follow request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 .hook(\"SocialGraphRoot\") .keepTrue(new Expr(Ops.IS_INSTANCE_OF, AcceptFollowRequest.class, \"*initialData\")) .macro(extractFields(\"*initialData\", \"*accountId\", \"*requesterId\")) .localSelect(\"$$accountIdToFollowRequests\", Path.key(\"*accountId\").must(\"*requesterId\")).out(\"*followRequestId\") .localSelect(\"$$accountIdToFollowRequestsById\", Path.key(\"*accountId\", \"*followRequestId\")).out(\"*followRequest\") .hashPartition(\"*requesterId\") .each((AcceptFollowRequest data, FollowLockedAccount req) -> { FollowAccount follow = new FollowAccount(data.getRequesterId(), data.getAccountId(), data.getTimestamp()); if (req.isSetShowBoosts()) follow.setShowBoosts(req.showBoosts); if (req.isSetNotify()) follow.setNotify(req.notify); if (req.isSetLanguages()) follow.setLanguages(req.languages); return follow; } , \"*initialData\", \"*followRequest\").out(\"*data\") .anchor(\"CompleteFollowRequest\") This code is structured just like the previous sections by hooking onto the root and then filtering for the data type of interest. Then, this code checks to see if that follow request still exists since a user could retract their follow request at the same time it was accepted. The must navigator stops this branch of computation if the follow request no longer exists. After that, the code generates the implicit Follow event which will later perform the actual logic of updating the $$followerToFollowees and $$followeeToFollowers PStates. The next section handles follows to a locked account. As a reminder, a locked account requires all followers to be manually approved. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 .hook(\"SocialGraphRoot\") .keepTrue(new Expr(Ops.IS_INSTANCE_OF, FollowLockedAccount.class, \"*initialData\")) .macro(extractFields(\"*initialData\", \"*accountId\", \"*requesterId\")) .localSelect(\"$$followeeToFollowers\", Path.key(\"*accountId\", \"*requesterId\")).out(\"*existingFollowerId\") .ifTrue(new Expr(Ops.IS_NOT_NULL, \"*existingFollowerId\"), Block.each((FollowLockedAccount data) -> { FollowAccount follow = new FollowAccount(data.requesterId, data.accountId, data.timestamp); if(data.isSetShowBoosts()) follow.setShowBoosts(data.isShowBoosts()); if(data.isSetNotify()) follow.setNotify(data.isNotify()); if(data.isSetLanguages()) follow.setLanguages(data.getLanguages()); return follow; }, \"*initialData\").out(\"*data\") .hashPartition(\"*requesterId\") .anchor(\"UpdatePrivateFollow\"), Block.macro(extractFields(\"*initialData\", \"*accountId\", \"*requesterId\")) .macro(accountIdToFollowRequests.addToLinkedSet(\"*accountId\", \"*initialData\"))) When a follow request is done in the UI to a locked account, a FollowLockedAccount event is appended to the depot. Otherwise, a FollowAccount event is appended. Follow relationships contain additional information such as whether the follower wants to see boosts from the followee and whether they only want to see statuses in a certain language from the followee (another one of Mastodon’s features). Updating these settings is done via another Follow or FollowLockedAccount event. This code uses ifTrue to determine if the follower already follows the followee. ifTrue works just like if in any programming language, with a “then” block and an optional “else” block. If the follow relationship exists, it creates an implicit FollowAccount event to update the options on the relationship. The “UpdatePrivateFollow” anchor is used later to merge that branch just like the previous sections. If the follow relationship does not already exist, then the PState tracking follow requests is updated. The next section merges the prior branches together and begins processing for the rest of the events, whether they came directly off the depot or were created implicitly by one of the branches: 1 2 .unify(\"Normal\", \"ImplicitUnfollow\", \"CompleteFollowRequest\", \"UpdatePrivateFollow\") .subSource(\"*data\", unify merges the specified branches together so they share subsequent computation. Any variables that are in scope in all specified branches are in scope in the code following the unify call. The subSource call dispatches subsequent code on the type of the object in *data . The following code defines the handling for FollowAccount events: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 SubSource.create(FollowAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\", \"*followerSharedInboxUrl\", \"*showBoosts\", \"*notify\", \"*languages\")) .localSelect(\"$$followerToFollowees\", Path.key(\"*accountId\").view(Ops.SIZE)).out(\"*followeeCount\") .keepTrue(new Expr(Ops.LESS_THAN, \"*followeeCount\", relationshipCountLimit)) .localSelect(\"$$followerToFollowees\", Path.key(\"*accountId\", \"*targetId\")).out(\"*followeeId\") .localSelect(\"$$followerToFolloweesById\", Path.key(\"*accountId\", \"*followeeId\")).out(\"*existingFollowee\") .each(Relationships::makeFollower, \"*targetId\", \"*showBoosts\", \"*languages\", \"*followerSharedInboxUrl\", \"*existingFollowee\").out(\"*followee\") .macro(followerToFollowees.addToLinkedSet(\"*accountId\", \"*followee\")) .hashPartition(\"*targetId\") .localSelect(\"$$followeeToFollowers\", Path.key(\"*targetId\", \"*accountId\")).out(\"*followerId\") .localSelect(\"$$followeeToFollowersById\", Path.key(\"*targetId\", \"*followerId\")).out(\"*existingFollower\") .each(Relationships::makeFollower, \"*accountId\", \"*showBoosts\", \"*languages\", \"*followerSharedInboxUrl\", \"*existingFollower\").out(\"*follower\") .macro(followeeToFollowers.addToLinkedSet(\"*targetId\", \"*follower\")) .macro(accountIdToFollowRequests.removeFromLinkedSetByEntityId(\"*targetId\", \"*accountId\")), This code adds the relationship to the $$followerToFollowees and $$followerToFollowees PStates. If the relationship already exists, it updates the options on the relationship. It also removes any corresponding follow request from $$accountIdToFollowRequests if it exists. relationshipCountLimit puts an upper limit on the number of follows someone can have and is set to a very conservative limit of 100,000. It exists to prevent abuse of the system. The next section handles RemoveFollowAccount events: 1 2 3 4 5 6 7 SubSource.create(RemoveFollowAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\", \"*followerSharedInboxUrl\")) .hashPartition(\"*accountId\") .macro(followerToFollowees.removeFromLinkedSetByEntityId(\"*accountId\", \"*targetId\")) .hashPartition(\"*targetId\") .macro(followeeToFollowers.removeFromLinkedSetByEntityId(\"*targetId\", \"*accountId\")) .macro(accountIdToFollowRequests.removeFromLinkedSetByEntityId(\"*targetId\", \"*accountId\")), This code just removes the relationship from all relevant PStates. Here is the next section: 1 2 3 SubSource.create(FollowLockedAccount.class), SubSource.create(AcceptFollowRequest.class), This handles events coming off the depot that were handled already in one of the top-level branches we already looked at. subSource requires every data type it sees to have a handler, so this code says to do nothing for those types. The next section handles RejectFollowRequest : 1 2 3 SubSource.create(RejectFollowRequest.class) .macro(extractFields(\"*data\", \"*accountId\", \"*requesterId\")) .macro(accountIdToFollowRequests.removeFromLinkedSetByEntityId(\"*accountId\", \"*requesterId\")), This just removes the follow request from the PState. The next section handles BlockAccount : 1 2 3 4 5 SubSource.create(BlockAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\", \"*timestamp\")) .localSelect(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"blocked\").view(Ops.SIZE)).out(\"*blockeeCount\") .keepTrue(new Expr(Ops.LESS_THAN, \"*blockeeCount\", relationshipCountLimit)) .localTransform(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"blocked\").voidSetElem().termVal(\"*targetId\")), BlockAccount was handled in one of the initial branches to generate the implicit unfollows between the two accounts. Here, BlockAccount is handled again to record the block relationship in the $$accountIdToSuppressions PState. The next section handles RemoveBlockAccount events: 1 2 3 SubSource.create(RemoveBlockAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\")) .localTransform(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"blocked\").setElem(\"*targetId\").termVoid())); All this does is remove the block relationship from the $$accountIdToSuppressions PState. That’s all the code for handling social graph updates from events on followAndBlockAccountDepot . The next section contains all the logic for handling events from muteAccountDepot : 1 2 3 4 5 6 7 8 9 10 stream.source(\"*muteAccountDepot\", StreamSourceOptions.retryAllAfter()).out(\"*data\") .subSource(\"*data\", SubSource.create(MuteAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\", \"*options\")) .localSelect(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"muted\").view(Ops.SIZE)).out(\"*muteeCount\") .keepTrue(new Expr(Ops.LESS_THAN, \"*muteeCount\", relationshipCountLimit)) .localTransform(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"muted\", \"*targetId\").termVal(\"*options\")), SubSource.create(RemoveMuteAccount.class) .macro(extractFields(\"*data\", \"*accountId\", \"*targetId\")) .localTransform(\"$$accountIdToSuppressions\", Path.key(\"*accountId\", \"muted\", \"*targetId\").termVoid())); This is really simple, as all it does is add the mute relationship for MuteAccount events and remove the relationship for RemoveMuteAccount events. That’s the complete implementation of the social graph! As you can see, it’s expressed exactly as you saw in the dataflow diagram with branches, merges, and dispatching on the types of events. It’s worth noting that dataflow code compiles to efficient bytecode when deployed, as efficient as regular Java code. So Rama variables like *accountId and *targetId become actual variables in the generated bytecode. The code for this ETL is also a great example of why it’s so beneficial to interact with your data layer with an API in a general-purpose language instead of a custom language (like SQL). This code makes use of normal programming practices to factor out reusable functionality or to separate code into separate functions to make it easier to read. This code also demonstrates how easy it is to intermix logic written in Java with logic written in Rama’s dataflow API. Method references, lambdas, and macros are facilities for combining the two. Let’s look at some of the Mastodon API implementation related to the social graph so you can see how you interact with a Rama cluster to serve the frontend. Here’s how a new unfollow event is added: 1 2 3 4 5 public CompletableFuture postRemoveFollowAccount(long followerId, long followeeId, String sharedInboxUrl) { RemoveFollowAccount removeFollowAccount = new RemoveFollowAccount(followerId, followeeId, System.currentTimeMillis()); if (sharedInboxUrl != null) removeFollowAccount.setFollowerSharedInboxUrl(sharedInboxUrl); return followAndBlockAccountDepot.appendAsync(removeFollowAccount).thenApply(res -> true); } This uses the async API for depots to append unfollow data to followAndBlockAccountDepot . Rama’s async API is used almost exclusively in the Mastodon API implementation so as not to block any threads (which would be an inefficient use of resources). Here’s how a follow event is handled, conditioning the type of data appended depending on if the account is locked or not: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public CompletableFuture postFollowAccount(long followerId, long followeeId, String sharedInboxUrl, PostFollow params) { return getAccountWithId(followeeId) .thenCompose((followee) -> { if (followee != null && followee.account != null && followee.account.locked) { FollowLockedAccount req = new FollowLockedAccount(followeeId, followerId, System.currentTimeMillis()); if (params != null) { if (params.reblogs != null) req.setShowBoosts(params.reblogs); if (params.notify != null) req.setNotify(params.notify); if (params.languages != null) req.setLanguages(params.languages); } return followAndBlockAccountDepot.appendAsync(req); } else { FollowAccount req = new FollowAccount(followerId, followeeId, System.currentTimeMillis()); if (params != null) { if (params.reblogs != null ) req.setShowBoosts(params.reblogs); if (params.notify != null) req.setNotify(params.notify); if (params.languages != null) req.setLanguages(params.languages); } if (sharedInboxUrl != null) req.setFollowerSharedInboxUrl(sharedInboxUrl); return followAndBlockAccountDepot.appendAsync(req); } }).thenApply(res -> true); } This first calls the helper function getAccountWithId which uses a query topology to get all information about that account. If the account is locked, a FollowLockedAccount is appended with any options appropriately set. Otherwise, a FollowAccount events is appended. The helper function getAccountWithId is implemented like this: 1 2 3 4 5 6 7 8 9 10 11 public CompletableFuture getAccountWithId(Long requestAccountIdMaybe, long accountId) { return getAccountsFromAccountIds.invokeAsync(requestAccountIdMaybe, Arrays.asList(accountId)).thenApply(accounts -> {if (accounts.size() == 0) return null;return accounts.get(0);}); } public CompletableFuture getAccountWithId(long accountId) { return this.getAccountWithId(null, accountId); } The query topology can optionally be invoked with a “requesting account ID”, because in some circumstances an account should not be visible to another account (e.g. the requesting account is blocked by that user). In this case, it just needs to check if the account is locked or not so it passes null for the requesting account ID. The query topology client is fetched like so: 1 QueryTopologyClient getAccountsFromAccountIds = cluster.clusterQuery(\"com.rpl.mastodon.modules.Core\", \"getAccountsFromAccountIds\"); As you can see, a query topology client is fetched just like how you fetch a handle to a depot or PState. Invoking a query topology is like invoking a regular function – you pass it some arguments and you get a result back. Unlike a regular function, a query topology executes on a cluster across potentially many nodes. Here the result is received asynchronously, but you can also do a blocking call with invoke . Timeline fanout code Lastly, let’s look at the code implementing timeline fanout, as described earlier. This implementation is only 51 lines of code. As a reminder, here’s the dataflow diagram for timeline fanout: Once again, since this is a longer piece of code let’s look at it section by section. Let’s start with the subscription to the depot containing all the statuses: 1 2 fan.source(\"*statusWithIdDepot\").out(\"*microbatch\") .anchor(\"FanoutRoot\") Like in the social graph example, let’s see visually how the dataflow diagram gets filled out. This code starts off the dataflow diagram like this: Unlike the previous examples, this topology is implemented with microbatching. Microbatching guarantees exactly-once processing semantics even in the case of failures. That is, even if there are node or network outages and computation needs to be retried, the resulting PState updates will be as if each depot record was processed exactly once. The variable *microbatch represents a batch of data across all partitions of the depot. This code simply binds that variable and marks the root of computation with the label “FanoutRoot”. As you can see in the dataflow diagram, there are two branches off the root of processing. The next section implements the first branch, which handles continuing fanout for statuses with too many followers from the previous iteration: 1 2 3 4 5 6 7 8 9 10 11 .allPartition() .localSelect(\"$$statusIdToLocalFollowerFanouts\", Path.all()).out(\"*keyAndVal\") .each(Ops.EXPAND, \"*keyAndVal\").out(\"*statusId\", \"*followerFanouts\") .localTransform(\"$$statusIdToLocalFollowerFanouts\", Path.key(\"*statusId\").termVoid()) .each(Ops.EXPLODE, \"*followerFanouts\").out(\"*followerFanout\") .macro(extractFields(\"*followerFanout\", \"*authorId\", \"*nextIndex\", \"*fanoutAction\", \"*status\", \"*task\")) .each(FanoutAction::getValue, \"*fanoutAction\").out(\"*fanoutActionValue\") .macro(extractFields(\"*status\", \"*content\", \"*language\")) .each((RamaFunction2) StatusPointer::new, \"*authorId\", \"*statusId\").out(\"*statusPointer\") .directPartition(\"$$partitionedFollowers\", \"*task\") .anchor(\"LocalFollowerFanoutContinue\") Whenever a status has too many followers for one iteration of fanout, it is added to the $$statusIdToLocalFollowerFanouts PState. This PState is a map from status ID to the type FollowerFanout , which contains the information needed to continue fanout starting with the next unhanded follower. As you can see in this code, it reads everything from that PState using Path.all() and then deletes everything from that PState. As you’ll see later, adding to $$statusIdToLocalFollowerFanouts is done on whatever partition followers were read for that status. So this code uses allPartition to access every partition of the PState. allPartition is a partitioner like hashPartition , except instead of the subsequent code executing on one partition, the subsequent code executes on all partitions. This allows the ETL to fetch all statuses that required continued fanout from the last iteration. You have to be careful when using allPartition as you can create non-scalable topologies if you were to use it for every piece of data on a high throughput depot. In this case allPartition is used just once per iteration, so it doesn’t affect the scalability of the topology. At the end of this block of code is some handling related to $$partitionedFollowers . We didn’t mention this PState in the earlier discussion of fanout, but it’s an additional optimization to balance load for handling of users with large amounts of followers. In short, this is an additional view of the social graph where users with more than 1,000 followers have their followers spread among multiple partitions of this PState. This balances the load of processing for fanout by reducing variance among partitions. We will be publishing another blog post in the future exploring this optimization and others. The next section begins the other branch of processing at the root of the dataflow diagram: 1 2 .hook(\"FanoutRoot\") .explodeMicrobatch(\"*microbatch\").out(\"*data\") This creates the branch and reads all new statuses for this iteration from the microbatch. explodeMicrobatch here reads all data from the microbatch across all partitions and binds each piece of data to the variable *data . This operation emits across all partitions of the module. The next section begins processing of new statuses: 1 2 3 4 5 6 7 8 .macro(extractFields(\"*data\", \"*statusId\", \"*status\")) .macro(extractFields(\"*status\", \"*authorId\", \"*content\", \"*language\")) .each(MastodonHelpers::getStatusVisibility, \"*status\").out(\"*visibility\") .keepTrue(new Expr(Ops.NOT_EQUAL, \"*visibility\", StatusVisibility.Direct)) .each(Ops.IDENTITY, -1L).out(\"*nextIndex\") .each(Ops.IDENTITY, FanoutAction.Add.getValue()).out(\"*fanoutActionValue\") .each((RamaFunction2) StatusPointer::new, \"*authorId\", \"*statusId\").out(\"*statusPointer\") .each(HomeTimelines::addTimelineItem, \"*homeTimelines\", \"*authorId\", \"*statusPointer\", new Expr(Ops.CURRENT_MICROBATCH_ID)) This code specifies to only perform fanout for statuses with visibility other than Direct , which is for direct messages and handled elsewhere. It then adds the status to the author’s own home timeline, which implements self-fanout. The next section reads a batch of followers for the status: 1 2 3 4 5 6 7 8 9 10 .select(\"$$partitionedFollowersControl\", Path.key(\"*authorId\")).out(\"*tasks\") .each(Ops.EXPLODE_INDEXED, \"*tasks\").out(\"*i\", \"*task\") .ifTrue(new Expr(Ops.NOT_EQUAL, 0, \"*i\"), Block.directPartition(\"$$partitionedFollowers\", \"*task\")) .anchor(\"NormalFanout\") .unify(\"NormalFanout\", \"LocalFollowerFanoutContinue\") .macro(safeFetchMapLocalFollowers(\"$$partitionedFollowers\", \"*authorId\", \"*nextIndex\", rangeQueryLimit, \"*fetchedFollowers\", \"*nextFollowerId\")) .ifTrue(new Expr(Ops.IS_NOT_NULL, \"*nextFollowerId\"), Block.each((RamaFunction5) FollowerFanout::new, \"*authorId\", \"*nextFollowerId\", new Expr(FanoutAction::findByValue, \"*fanoutActionValue\"), \"*status\", \"*task\").out(\"*followerFanout\") .localTransform(\"$$statusIdToLocalFollowerFanouts\", Path.key(\"*statusId\").nullToList().afterElem().termVal(\"*followerFanout\"))) As mentioned earlier, in a future post we’ll explore the $$partitionedFollowers optimization. As will be explored in that future post, the first section of this code determines from which tasks to read followers in parallel for the status’s author. The unify call merges processing for statuses from both last iteration and new statuses from this iteration. safeFetchMapLocalFollowers is a small helper function reading up to rangeQueryLimit followers from this partition for that author ( rangeQueryLimit is a constant set to 1,000). Since followers are read in parallel across many partitions for users with more than 1,000 followers, and since we deployed this module with 64 partitions, this means up to 64k followers are read per status per iteration. The ifTrue line writes to the $$statusIdToLocalFollowerFanouts PState to continue fanout next iteration if there are still more followers to handle. This is the same PState you saw used in the earlier section. The next section handles follower-specified options on the types of statuses they wish to see from this author: 1 2 3 4 5 6 7 8 9 10 11 .each(Ops.EXPLODE, \"*fetchedFollowers\").out(\"*follower\") .each((Follower follower) -> follower.accountId, \"*follower\").out(\"*followerId\") .each((Follower follower) -> follower.sharedInboxUrl, \"*follower\").out(\"*followerSharedInboxUrl\") .each((Follower follower) -> follower.isShowBoosts(), \"*follower\").out(\"*showBoosts\") .each((Follower follower) -> follower.getLanguages(), \"*follower\").out(\"*languages\") .keepTrue(new Expr(Ops.IS_NULL, \"*followerSharedInboxUrl\")) // skip remote followers .ifTrue(new Expr(Ops.IS_INSTANCE_OF, BoostStatusContent.class, \"*content\"), Block.macro(extractFields(\"*content\", \"*boostedAuthorId\")) .keepTrue(new Expr(Ops.NOT_EQUAL, \"*boostedAuthorId\", \"*followerId\")) .keepTrue(\"*showBoosts\")) .keepTrue(new Expr((List languages, String statusLanguage) -> languages == null || statusLanguage == null || languages.contains(statusLanguage), \"*languages\", \"*language\")) This filters this follower out of fanout for this status if: it’s a boost and they don’t wish to see boosts from this author, it’s a boost and they’re the original author of the status, or they specified they only wish to see certain languages from this author and the status doesn’t match. Notice how all the information needed to do the filtering is on the Follower structure that was retrieved as part of fetching followers for fanout. No extra PState queries need to be done for this information, which is one of the reasons our implementation has such high throughput. The average amount of fanout per status on our instance is 403, so any work post-fanout (after the Ops.EXPLODE call, which emits once per follower in the *fetchedFollowers list) is multiplied by 403 compared to work pre-fanout. This is why we went out of our way to materialize as much information on the follow relationship as possible to minimize the work post-fanout. The next section handles additional filtering required for replies: 1 2 3 4 5 6 .ifTrue(new Expr(Ops.IS_INSTANCE_OF, ReplyStatusContent.class, \"*content\"), Block.macro(extractFields(\"*content\", \"*parentAuthorId\")) .hashPartition(\"*followerId\") .macro(fetchBloomMacro(\"*followerId\", \"*rbloom\")) .keepTrue(new Expr((RBloomFilter rbloom, Long accountId) -> rbloom.bloom.isPresent(\"\" + accountId), \"*rbloom\", \"*parentAuthorId\")) .select(\"$$followerToFollowees\", Path.key(\"*followerId\").must(\"*parentAuthorId\"))) Replies are delivered to a follower only if they also follow the account being replied to. This code queries $$followerToFollowees to perform that check, with the must navigator only emitting if the follow relationship exists. Before the PState query, there’s a bloom filter check to minimize the amount of PState queries done here. This is another optimization that we didn’t mention in the earlier discussion of fanout, and we’ll discuss it more in a future post. In short, a bloom filter is materialized and cached in-memory on this module for each account with all follows for the account. If the bloom filter returns false, the follow relationship definitely does not exist and no PState query is necessary. If it returns true, the PState query is done to weed out false positives. The bloom filter reduces PState queries for replies by 99%. The next section completes this ETL by writing to the home timelines of followers that have passed each of the preceding filters: 1 2 .hashPartition(\"*followerId\") .each(HomeTimelines::addTimelineItem, \"*homeTimelines\", \"*followerId\", \"*statusPointer\", new Expr(Ops.CURRENT_MICROBATCH_ID)); This simply routes to the appropriate partition hosting each follower’s home timeline and then adds to it. That .hashPartition call is actually the most expensive part of this ETL because of the huge volume of messages that flow through it. Due to the average of 403 fanout on our instance and our incoming rate of 3,500 statuses / second, 1.4M messages go across that partitioner every second. When we open-source our Mastodon instance in two weeks, you’ll see that this ETL also handles hashtags, lists, conversations, and federation. We excluded those from this code example since they’re all pretty similar to home timeline fanout, with slightly different rules. They’re just additional branches of computation on this ETL. That’s all we’ll show for now. As mentioned, in two weeks we’ll be open-sourcing our entire Mastodon implementation. Conclusion I’ve covered a lot in this post, but I’ve barely scratched the surface on Rama and our Mastodon implementation. For example, I didn’t mention “fine-grained reactivity”, a new capability provided by Rama that’s never existed before. It allows for true incremental reactivity from the backend up through the frontend. Among other things it will enable UI frameworks to be fully incremental instead of doing expensive diffs to find out what changed. We use reactivity in our Mastodon implementation to power much of Mastodon’s streaming API. I also didn’t mention Rama’s integration API. Because of my description of Rama as being able to build an entire backend on its own, you may have the impression that Rama is an “all-or-nothing” tool. However, just because Rama can do so much doesn’t mean it has to be used to do everything. We’ve designed Rama to be able to seamlessly integrate with any other tool (e.g. databases, queues, monitoring systems, etc.). This allows Rama to be introduced gradually into any architecture. To reiterate what’s to come: in one week we will be releasing the full Rama documentation as well as a build of Rama that exposes the full API for use with InProcessCluster , and in two weeks we will be fully open-sourcing our Mastodon implementation (which can run on InProcessCluster ). Additionally, we will be publishing more posts exploring Rama and our Mastodon implementation in greater depth. You can keep track of developments with Rama by joining our newsletter or following us on Twitter at @redplanetlabs. We’ve also started the Google group rama-user, where you can discuss Rama or ask questions. Lastly, Red Planet Labs will be starting a private beta in the coming months to give companies access to the full version of Rama. We plan to work closely with our private beta users to help them build new systems or reimplement existing systems at massively reduced cost. We will be releasing more details on the private beta later, but you can apply here in the meantime. Discussion on Hacker News. Post navigation ‹ PREVIOUS Tour of our 250k line Clojure codebase 8 thoughts on “How we reduced the cost of building Twitter at Twitter-scale by 100x” Steve Simpson AUGUST 15, 2023 AT 2:20 PM 10 years after: the style is seamlessly-like reading the follow-up to Manning’s “Big Data” book. This is one",
    "commentLink": "https://news.ycombinator.com/item?id=37137110",
    "commentBody": "We reduced the cost of building Mastodon at Twitter-scale by 100xHacker NewspastloginWe reduced the cost of building Mastodon at Twitter-scale by 100x (redplanetlabs.com) 864 points by tekacs 16 hours ago| hidepastfavorite320 comments RomanPushkin 13 hours ago> ...10k lines of code. This is 100x less code than the ~1M lines TwitterI wish I didn&#x27;t see this comparison, which is not fair at all. Everyone in their right mind understands that the number of features is much less, that&#x27;s why you have 10k lines.Add large-scale distributed live video support at the top of that, and you won&#x27;t get any close to 10k lines. It&#x27;s only one of many many examples. I really wish you compare Mastodon to Twitter 0.1 and don&#x27;t do false advertising> 100M bots posting 3,500 times per second... to demonstrate its scaleI&#x27;m wondering why 100M bots post only 3500 times per second? Is it 3500 per second for each bot? Seems like it&#x27;s not, since https termination will consume the most of resources in this case. So I&#x27;m afraid it&#x27;s just not enough.When I worked in Statuspage, we had support of 50-100k requests per second, because this is how it works - you have spikes, and traffic which is not evenly distributed. TBH, if it&#x27;s only 3500 per second total, then I have to admit it is not enough. reply nathanmarz 13 hours agoparentWe&#x27;re comparing just to the original consumer product, which is about the same as Mastodon is today. That&#x27;s why we said \"original consumer product\" and not \"Twitter&#x27;s current consumer product\".Mastodon actually has more features than the original Twitter consumer product like hashtag follows, global timelines, and more sophisticated filtering&#x2F;muting capabilities.Some people argue it&#x27;s not so expensive to build a scalable Twitter with modern tools, which is why we also included the comparison against Threads. That&#x27;s a very recent data point showing how ridiculously expensive it is to build applications like this, and they didn&#x27;t even start from scratch as Instagram&#x2F;Meta already had infrastructure powering similar products. reply doctorpangloss 11 hours agorootparentI work in gaming, so I cannot speak to your specific experiences. Entity Component Systems are extremely performant, really good science, and shipping in middlewares like Unity. However, in order to ship an ECS game, in my experience, you have to have already made your whole game first in a normal approach, in order to have everything be fully specified sufficiently that you can correctly create an ECS implementation. In practice, this means ECS is used to make first person shooters, which have decades of well specified traditions and behavior, and V2 of simulators, like for Cities Skylines 2 and Two Point Campus.So this is not meant to diminish the achievements of what you have built at all, it is more intellectually honest to say that \"any high performance framework is most suitable for projects that are exact clones of pre-existing, mature things with battle-hardened specifications and end user behavior.\" While this might cover some greenfield projects, including the best capitalized ones that may matter to you, it does diminish the appeal of a framework for the vast majority of success stories from small & poorly capitalized teams. Those small & poor teams are very innovation and serendipity driven and hence rarely copying a pre-existing thing. And even if they try to become well-capitalized, they are almost always doing so by having worked on the thing they are copying already (i.e., already shipping version 1.0 for years). reply syntheweave 7 hours agorootparentYes, this is along the lines of what I&#x27;m suspicious of too, as an also-gamedev that has done some ECS.It&#x27;s too easy to study an existing system and, given the resources, create a perfect demo for how to dramatically improve it in certain ways. You can go on to ship the demo, but the demo wasn&#x27;t made by the same kind of organization, with the same kinds of goals, as the original system builders. The extreme example of this is, of course, the demoscene and its hardware-bending tricks that achieve the impossible through a significant modification to the design of a \"production\" equivalent.So it&#x27;s better by performance metrics, better by codesize, but unknown on other metrics. Like, \"do I know how to start building new things with this?\" reply moate 6 hours agorootparentAsking “what are you optimizing for&#x2F;building towards” and getting different answers for different products isn’t proof that either is inferior, just that they may be better at different things.Your comment makes it seem like their post is misleading, when it isn’t it just might be that what they do best isn’t useful to you (or possibly anyone). reply danbolt 6 hours agorootparentprevI can’t go into specifics due to NDAs, but I’ve been a developer on a game that was designed and programmed with the EnTT library from the ground up. [1]I don’t know if I’d suggest ECS for every team as you need the right tooling, culture, and leadership to pull it off. That said, I think the paradigm has unrecognized benefits when it comes to greenfield gameplay programming.Or, your data structures become a way for sub-teams to communicate and share state without stepping on one another’s toes. A group handling pathfinding and a different one handling scenario&#x2F;mission logic can both pay attention to the position data without needing to be highly coupled. It becomes easy to “just add a system” or “an extra component” to build on existing functionality.The Fred Brooks quote about “showing me your tables” comes to mind.[1] https:&#x2F;&#x2F;www.minecraft.net&#x2F;en-us&#x2F;about-legends reply tacotacotaco 6 hours agorootparentprevI only hobby in game dev. I have read some basics of ECS and IMO it seems a lot more intuitive than the “normal approach”. I think this is a case of using familiar tools we are comfortable with, not necessarily better. Unity’s ECS implementation is a hot mess though. I am looking forward to seeing what Bevy delivers as it matures.I agree that copying an existing product will be easier and is usually cheaper and more performant because you can leverage your competitor’s R&D and lessons learned. I presume this is why some tech companies’ product lines are full of clones. reply koonsolo 2 hours agorootparentprev> your whole game first in a normal approachWhat do you mean by normal approach? OO classes with inheritance? reply reissbaker 10 hours agorootparentprevI worked on a previous iteration of Threads at Instagram, and I don&#x27;t think the 25 person-years number you&#x27;re estimating spent on the project was spent primarily on backend engineering. They concurrently launched native iOS and Android apps, and I think you&#x27;re also factoring in the IC count from the linked article to mean full-time ICs dedicated exclusively to Threads, which is probably not how it worked. (Although TBH I&#x27;m not sure how you arrived at 25-person-years from the article, which said the project started with \"dozens\" of engineers and peaked at 50, and was 7 months long in total — a simple average of 24-50 engineers for 7 months, even if you assume they&#x27;re all full time, would be about 21 person-years).When I was at IG, my team was 20-something ICs full-time on our project, bursted to maybe double that as necessary in part-time help from ICs in the wider org. We had a total of three backend engineers, of which I was one, as well as a backend intern for a few months, although we bursted the backend team for a couple of months to four for some ML help.Your project sounds pretty cool! But I don&#x27;t think the comparison to Threads productivity is quite right. The majority of IG engineering is focused on building polished native clients, not generally on backend infrastructure. It&#x27;s true that Meta already has a lot of the backend infra built — if Red Planet Labs can come close to what you get at Meta, that&#x27;s pretty amazing. But I don&#x27;t think the numbers you&#x27;re quoting are apples-to-apples, or mean quite what you think they do. I don&#x27;t know if this version of Threads operated exactly like the one I worked on (a Snapchat competitor), but I&#x27;d be pretty surprised if there was a product team at IG that was majority backend.(Edit: I also think it&#x27;s worth keeping in mind the experience levels of who works on these projects — at IG it&#x27;s usually a smaller number of E6&#x2F;E5s guiding a larger number of E4&#x2F;E3s. Person-years are not all equivalent! If you spent ten years building the Red Planet Labs infrastructure based on your time at Twitter, nine person-months of your team&#x27;s time building a product on your infra might not be the same as nine person-months of someone else&#x27;s.)Anyway — I don&#x27;t mean to downplay your product, and really, if it&#x27;s anything like the backend productivity I experienced at Meta, that would be pretty groundbreaking. Curious to see what you launch :) reply dpweb 10 hours agorootparentprevBuilt some log databases and back end frameworks myself with some of the same concepts. I applaud the creativity in rethinking how back ends should work. Please now do frontends next! :)\"But it&#x27;s not a fully functioning 2023 Twitter!!!\" I think some people miss the point. This is not about hey we built a Twitter clone. This is about a POC for a novel app architecture.We need to be constantly examining and re-examining our thoughts about the best way to deal with distributed systems, scale, developer workflow. Even inventing new ones. reply philipwhiuk 2 hours agorootparent> This is not about hey we built a Twitter clone.Then they shouldn’t have titled it “we built a Twitter clone” reply ehutch79 12 hours agorootparentprevAre you sure about that.With things like twitter, the ui is not the hard part. Things like moderation are the secret sauce. All the corner cases and support for devopsy stuff likely account for a lot. Routing to specific instances for celebrities and such. reply justrealist 12 hours agorootparentNathan worked at Twitter so while he might be wrong, I don&#x27;t think it&#x27;s reasonable to assume he&#x27;s just naive http:&#x2F;&#x2F;nathanmarz.com&#x2F;blog&#x2F;leaving-twitter.html. reply WheelsAtLarge 11 hours agoparentprevWe see this type of post regularly. Something like, \"How I built a betterclone by myself in a month.\" Well, no, usually it&#x27;s just a bare skeleton with the least amount of functionality. Not only that, the software is the least of the functionality. The organizational structure around the app is what matters most to keep it going. It&#x27;s an attention seeking ploy and the whole thing usually disappears real quick. reply gexla 8 hours agorootparentIf you want background, you can go to the blog in his bio and read the first post, which mentioned he worked at Twitter starting in 2011. He mentioned elsewhere that his comparison is early Twitter, as opposed to current Twitter.The background makes this a bit more interesting, because you can imagine how those early days impacted the arc of his work. reply hosh 12 hours agoparentprevHow much of Twitter’s code base is dedicated to things like security, compliance, and moderation?Granted, a decentralized platform would eliminate some of those, just by being decentralized reply asu_thomas 10 hours agorootparentAnd advertising. Twitter is an advertising platform. reply adventured 12 hours agorootparentprevNone of those things get eliminated by decentralization, they get distributed to whatever the point of control &#x2F; ownership is.Mastodon still requires security, compliance and moderation. And those requirements are going to keep getting more challenging by the year. It&#x27;ll end up being another reason nobody will want to host content in a decentralized manner, the burden will become obnoxious. reply hosh 12 hours agorootparentAn organization trying to maintain an ISO certification will have drastically different policies and controls than a small shop, or even a hobbyist group.Everyone (theoretically) would be complying to statues in its broadest sense, but jurisdiction, regulations, industry best practices, reporting requirements, and appetite for risk is going to be different from organization to organization. It’s not one-size-fits-all.So some of these things are eliminated because the people hosting those are not put under the same kind of scrutiny as say, a Twitter. reply jeremyjh 8 hours agorootparentThe scrutiny from investors, legal and compliance exists because the risks are real, and they don&#x27;t go away just because there isn&#x27;t a Serious Business involved. Once someone operating one of these is publicly damaged, the risk will be better understood and marked up accordingly. reply hosh 8 hours agorootparentThey sure are, which brings back to the point: how much did this implementation of Mastadon adequately addressed these risks with the reduced code count? reply jeremyjh 7 hours agorootparentI assumed this is just marketing, so it addressed the risks by never operating a public instance. reply Fomite 11 hours agorootparentprevDuring the first wave of Twitter exodus, several people in my professional circle asked if they should be hosting professional, field-specific Mastadon servers, etc.My answer, born of moderating a modestly sized forum, was \"Absolutely not under any circumstances.\" reply KingOfCoders 5 hours agoparentprevThe 100x less code always reminds me of a hypothetical Half Life 2 game engine. Then your code would be: StartHalfLife2LikeGame()and you&#x27;d replace millions LOC with one line. If there is a perfect match between the framework and your app, there is no code. The more your app diverges from the ideal app the framework was written for, the more code you have. reply lbalazscs 1 hour agorootparentThey claim that \"Reddit, Slack, Gmail, Uber, etc\" could also be done with a similar level of effort. reply KingOfCoders 32 minutes agorootparentYes, all applications that are aligned with the framework (message spreading from one user to many others) - not that I want to put down their effort in any way.But this is the core [0] - a simple event location plattform my wife founded hat easily >200k LOC while the core (edit&#x2F;search&#x2F;detail page event location) hadWhen I worked in Statuspage, we had support of 50-100k requests per secondServing one cached HTML with Nginx and serving dynamically generated content, updating a database is a different thing. reply codedokode 9 hours agoparentprevTwitter has many features, but not all of them are necessary. For example, there is no need to implement popups blocking the page and demanding a user to register, or detailed telemetry collecting user data from 20 different providers. reply otikik 9 hours agoparentprevIndeed. Add a single JavaScript dependency… you will get the banana, the gorilla holding the banana, the tree holding the gorilla, and the whole jungle. reply smcl 1 hour agoparentprevNot to mention the phrase \"x times less than\" doesn&#x27;t really make sense the way it&#x27;s often used. For it to make sense you have to reinterpret it to mean something that it doesn&#x27;t based on being the opposite of \"x times more than\" (which is also often misused). reply anothernewdude 5 hours agoparentprevIt takes more self-control and effort to reduce the number of features to the ones that matter. Twitter having more features is a liability, not a benefit.> Add large-scale distributed live video support at the top of that,Why? For the love of all that is good and efficient, why? Why not have a separate platform for that? Or link to a different federated video service? Why does every platform need to do all the things? reply littlestymaar 11 hours agoparentprev> Add large-scale distributed live video support at the top of that, and you won&#x27;t get any close to 10k lines.But Twitter isn&#x27;t, and was never, about live video support: this is pure feature creep and that&#x27;s how you get headcount inflation and a company that can be run for 17 years without making profit (AKA terrible business).> When I worked in Statuspage, we had support of 50-100k requests per secondHaving served 150kqps in the past as part of a very small team (3 back-end eng.), this isn&#x27;t necessarily as big of a deal as you make it sound: it mostly depends on your workload and whether or not you need consistency (or even persistence at all) in your data.In practice, building scalable system is hard mostly because it&#x27;s hard to get the management forgot their vanity ideas that go against your (their, actually) system&#x27;s scalability. reply MikePlacid 12 hours agoparentprevSo>> 100M bots posting 3,500 times per second...and> We used the OpenAI API to generate 50,000 statuses for the bots to choose from at random.I wonder: 100M OpenAI bots talking to each other continuously and with much vigor - how is this affecting OpenAI’s uhm… intellect? reply sdwr 11 hours agorootparentThey generated 50,000 statuses once, put them in a text file, and pick between them randomly. So not at all. reply dataangel 12 hours agoprevI do C++ backend work in a non-web industry and this entire post is Greek to me. Even though this is targeted at developers, you need a better pitch. I get \"we did this 100x faster\" but the obvious followup question is \"how\" but then the answer seems to be a ton of flow diagrams with way too many nodes that tell me approximately nothing and some handwaving about something called P-States that are basically defined to be entirely nebulous because they are any kind of data structure.I&#x27;m not saying there&#x27;s nothing here, but I am adjacent to your core audience and I have no idea whether there is after reading your post. I think you are strongly assuming a shared basis where everybody has worked on the same kind of large scale web app before; I would find it much more useful to have an overview of, \"This what you would usually do, here are the problems with it, here is what we do instead\" with side by side code comparison of Rama vs what a newbie is likely to hack together with single instance postgres. reply sdwr 12 hours agoparentIn a typical architecture, the DB stores data, and the backend calls the DB to make updates and compile views.Here, the \"views\" are defined formally (the P-states), and incrementally, automatically updated when the underlying data changes.Example problem:Get a list of accounts that follow account 1306\"Classic architecture\":- Naive approach. Search through all accounts follow lists for \"1306\". Super slow, scales terribly with # of accounts.- Normal approach. Create a \"followed by\" table, update it whenever an account follows &#x2F; unfollows &#x2F; is deleted &#x2F; is blocked.Normal sounds good, but add 10x features, or 1000x users, and it gets trickier. You need to make a new table for each feature, and add conditions to the update calls, and they start overlapping... Or you have to split the database up so it scales, but then you have to pay attention to consistency, and watch which order stuff gets updated in.Their solution is separating the \"true\" data tables from the \"view\" tables, formally defining the relationship between the two, and creating the \"view\" tables magically behind the scenes. reply endisneigh 12 hours agorootparentI read their post and honestly it’s not really that much different than just materialized views in a regular database plus async jobs to do the long running tasks.It’s a ridiculous amount of fluff to describe that. Not to mention it’s proprietary and only supports the JVM and doesn’t integrate with the tons of tooling designed about RDBMS unless you stream everything to them, defeating the purpose.What really irks me is that they go on and on bragging about the low LoC count and literally show nothing complete. They should’ve held on this post and released it simultaneously with the code. reply nathanmarz 10 hours agorootparentWe are very open in the post that the core concepts are not new: Individually, none of these concepts are new. I’m sure you’ve seen them all before. You may be tempted to dismiss Rama’s programming model as just a combination of event sourcing and materialized views. But what Rama does is integrate and generalize these concepts to such an extent that you can build entire backends end-to-end without any of the impedance mismatches or complexity that characterize and overwhelm existing systems.Indexes as arbitrary data structures that you shape to perfectly meet your use cases, a powerful computation API that&#x27;s like a \"distributed programming language\", and everything being so integrated make a world of difference.I understand the desire to see all the code, and that&#x27;s coming in two weeks. That said, the code in the post isn&#x27;t trivial as it&#x27;s showing almost the complete implementations of two major parts of Mastodon: the social graph and timeline fanout.Next week you&#x27;ll be able to play with Rama when we release a build of it, and the documentation will help with that. reply Aeolun 10 hours agorootparent> But what Rama does is integrate and generalize these concepts to such an extent that you can build entire backends end-to-end without any of the impedance mismatches or complexityEvery time I hear this the reality turns out to be that building anything with this tech is like building something on top of SAP.But I’m also just allergic to any post that says ‘look how amazing’ in general, so I’m a bit prejudiced. reply Aeolun 8 hours agorootparentAfter reading through the post a bit more, I’m inclined to believe it’s not hot air, but I think most of the innovation where is in the management layer, not the ease of application development.Just looking at the first example tells me that there’s a million ways someone that doesn’t know what they’re doing can mess this up.If the author of the platform implements some service on their own platform it’s always going to seem simple. reply sixo 11 hours agorootparentprevThe difference is that the materialized-view logic lives naturally in the application code; there&#x27;s no step where they go out of the DB to do computations and then reinsert.Once SQL materialized views aren&#x27;t enough, you might do this by replicating your database into Kafka, implementing logic in Flink or something, and reinserting into the same DB&#x2F;Elasticsearch&#x2F;etc. Very common architecture. (Writ small, could also use a queue processor like RabbitMQ.)Their approach is to instead--apparently--make all of these first-class elements of the same ecosystem, not by \"putting it all in the database\", but by putting the database into the application code. Which seems wild, but colocates data, transformation, and view.Seems like it would open up a lot of cans of worms, but if you solve those, sounds great. reply cormacrelf 6 hours agorootparentYou can do all of this with https:&#x2F;&#x2F;materialize.com, and you don’t need to write it in Java. Just connect it to a Postgres instance and start creating materialised views using SQL. These views then auto update. So much so, that you can create a view for the top 10 of something, and let it sit there as the list updates. Otherwise just use normal select statements from your views using any Postgres client. reply leonidasv 11 hours agorootparentprevIIUIC, the most significant difference from a materialized view is that the Rama infrastructure recompute only the changed data by checking the relationship between fields, while a traditional materialized view recomputes the whole table? reply andrenotgiant 6 hours agorootparentincremental view maintenance is the database equivalent of: \"recompute only the changed data by checking the relationship between fields,\"Oracle has decent support for incrementally updated materialized views, redshift has some too. Materialize.com is an entire snowflake-like platform built around incrementally maintained materialized views. reply sdwr 11 hours agorootparentprevThis is all armchair for me, but I think they have containers and sharding built in as well, which is the other half of the puzzle when it comes to scaling. reply endisneigh 11 hours agorootparentYes, but there are plenty of NewSQL that support views and offer all of that too. Yugabyte, Cockroach, TiDB and that’s just off the top of my head and open source. If we count proprietary then you have Fauna, Cloud Spanner and more I’m sure. reply ethbr1 11 hours agorootparentprevSo... at a high level, early React for data? In other words, letting a framework manage update dependency graph tracking, and then cascading updates through its graph in an optimized manner to enhance performance?Obviously, with tons of implementation difficulties and details, and not actual graph structures, but as a top level analogy. reply ricardobeat 9 hours agorootparentNot at all, especially because React doesn’t do much dependency tracking on its own and is built for predictable UI updates and not performance.To be honest any parallel with frontend here is meaningless, reactivity and all the concepts at play have existed long before JS and browsers came along, it’s easier to explain from first principles. reply Aeolun 8 hours agorootparentI think that’s probably not the case for many new developers that don’t have any exposure to anything not React. Of course ‘react for data’ is entirely misleading, but it may give a decent idea if you don’t have an hour to spend on an explanation. reply throwuxiytayq 7 hours agorootparentIn other words, it’s the Dark Souls of application backends, but entirely different. replyldayley 12 hours agoparentprevNathan Marz created Apache Storm, coauthored the book \"Big Data\", and founded an early real-time infrastructure team at Twitter. It&#x27;s likely the &#x27;curse of knowledge&#x27; of working on this specific problem for so long is responsible for the unique and&#x2F;or unfamiliar style of communication here.EDIT: Specifics reply falsandtru 10 hours agoparentprev> Whereas Twitter stores home timelines in a dedicated in-memory database, in Rama they’re stored in-memory in the same processes executing the ETL for timeline fanout. So instead of having to do network operations, serialization, and deserialization, the reads and writes to home timelines in our implementation are literally just in-memory operations on a hash map. This is dramatically simpler and more efficient than operating a separate in-memory database. The timelines themselves are stored like this:> To minimize memory usage and GC pressure, we use a ring buffer and Java primitives to represent each home timeline. The buffer contains pairs of author ID and status ID. The author ID is stored along with the status ID since it is static information that will never change, and materializing it means that information doesn’t need to be looked up at query time. The home timeline stores the most recent 600 statuses, so the buffer size is 1,200 to accommodate each author ID and status ID pair. The size is fixed since storing full timelines would require a prohibitive amount of memory (the number of statuses times the average number of followers).> Each user utilizes about 10kb of memory to represent their home timeline. For a Twitter-scale deployment of 500M users, that requires about 4.7TB of memory total around the cluster, which is easily achievable.Isn&#x27;t this where the most difficult(expensive) part is and Rama has little to do with it? It appears that the other parts also do not have to be Rama. reply nathanmarz 10 hours agorootparentWe&#x27;re storing those in-memory within the Rama modules materializing the home timelines. And the query topologies that refresh home timelines for lost partitions is colocated with that. This is dramatically simpler than operating a separate in-memory database, and Rama has everything to do with that. reply falsandtru 10 hours agorootparentIt appears simpler and better without Rama.> So instead of having to do network operations, serialization, and deserialization, the reads and writes to home timelines in our implementation are literally just in-memory operations on a hash map. This is dramatically simpler and more efficient than operating a separate in-memory database. reply rasz 8 hours agoparentprevHere in video form: Microservices https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=y8OnoxKotPQ reply HaZeust 12 hours agoparentprev... Maybe the post isn&#x27;t targeted to your audience at all? How is \"C++\" and \"non-web work\" adjacent to web work with web launguage audiences? reply slim 12 hours agorootparenthe&#x27;s a developer and curious about the subject. Since it&#x27;s a blog post, not a scientific paper, the fact that he did not understand could be a communication failure. I think he&#x27;s being helpful reply rollcat 12 hours agorootparentprevOP did not specify what their industry actually is. I&#x27;ve been doing \"web work\" for 17 years and I&#x27;m sharing their concern: where&#x27;s the TL;DR for this? If this somehow can make me 100x as productive, how about starting with a \"hello world\" example that shows me how is it different from pip install django, etc? reply buro9 14 hours agoprevMeasuring \"Twitter Scale\" by tweets per second seems to be not how I would measure it.Updates per second to end users who follow the 7K tweets per second seems more realistic, it&#x27;s the timelines and notifications that hurt, not the top of ingest tweets per second prior to the fan out... and then of course it&#x27;s whether you can do that continuously so as not to back up on it. reply nathanmarz 14 hours agoparentThat&#x27;s why we&#x27;re saying \"at 403 fanout\". The bottleneck of Mastodon&#x2F;Twitter is timeline writes, which is posts&#x2F;second multiplied by the average number of followers per post. So our instance is doing 1.4M timeline writes &#x2F; second.Another important metric is \"time to deliver to follower timelines\", which is tricky due to how much variance there can be every second due to the extremely unbalanced social graph. When someone with 20M followers posts, that multiples the number of needed timeline writes by 15x. We went into depth in our post on how we handled that to provide fairness by preventing these big users from hogging all the resources all at once. reply faitswulff 14 hours agorootparentI heard somewhere that one of the particular challenges of Twitter&#x27;s scale is not the average fanout, but the outliers where millions or tens of millions of users follow a single account. Does your simulation take that into account? reply nathanmarz 14 hours agorootparentYes, we discussed this at length in the post. reply mping 15 hours agoprevCongrats on the (kinda) launch. I was curious to see what you guys were up to. The blog post is pretty detailed, and with good insights. Reducing modern app development complexity to mixing data structures sounds like a good abstraction. I&#x27;m sure you thought really hard about the building blocks of Rama and you know your problems better than most of the hn crowd.Now, the really hard part becomes selling. If companies start using your product to get ahead, that will be the real proof, otherwise its \"just\" tech that is good on paper.On a side note, did you guys got any inspiration from clojure? I see lots of interesting projects propping up from clojure people...Best of luck! reply nathanmarz 15 hours agoparentRama is written in Clojure :) reply mark_l_watson 11 hours agorootparentNathan, we guessed that it was written in Clojure :-)Interesting to finally see the announcement. reply cutler 14 hours agorootparentprev\"Rama is programmed entirely with a Java API\". reply nathanmarz 14 hours agorootparentThe customer API in Java, and the implementation of that API is in Clojure. reply fiddlerwoaroof 14 hours agorootparentWill there be a first-party Clojure wrapper? Or, would the expectation be that users would use Java interop? reply nathanmarz 14 hours agorootparentWe&#x27;re not releasing one as we don&#x27;t have the bandwidth right now to maintain and document another API. That said, making a Clojure wrapper around the Java API should be pretty easy. reply stefcoetzee 5 hours agorootparentAnd hopefully, simple, too! ;) replyPxtl 15 hours agoprevI&#x27;ve seen many people describe frameworks like this - you know, first you have the slow back-end event-driven master database that you don&#x27;t query live against, then you&#x27;ve got eventual-consistency flows against the various data-warehouses and data-stores and partitioned sharded databases in useful query-friendly layouts that you actually read live from... and I never see it clearly explained: how do you read a change back to the user literally just after they made the change? How do you say \"other views eventual-consistency is fine but for this view of this bit of info we need it updated now\".This write-up is very detailed but I couldn&#x27;t find that explanation. reply jedberg 15 hours agoparentThe short answer is write-through cache.You write the update directly to the cache closest to the user and into the eventually consistent queue.We did this at reddit. When you make a comment the HTML is rendered and put straight into the cache, and the raw text is put into the queue to go into the database. Same with votes. I suspect they do this client side now, which is now the closest cache to the user, but back then it was the server cache. reply squeaky-clean 13 hours agorootparentIn Nathan Marz&#x27;s (the article author) book, Big Data, he describes this and calls it the Speed Layer. I haven&#x27;t fully finished the article yet, but the components it&#x27;s describing seem to be equivalent to what he calls the Batch Layer and the Serving Layer in his book.But I&#x27;m kind of getting the impression this works without any speed layer and is expected to be fast enough as-is. reply nathanmarz 12 hours agorootparentRama codifies and integrates the concepts I described in my book, with the high level model being: indexes = function(data) and query = function(indexes). These correspond to \"depots\" (data) , \"ETLs\" (functions), \"PStates\" (indexes), and \"queries\" (functions).Rama is not batch-based. That is, PStates are not materialized by recomputing from scratch. They&#x27;re incrementally updated either with stream or microbatch processing. But PStates can be recomputed from the source data on depots if needed. reply FridgeSeal 9 hours agorootparentForgive me if I’m misunderstanding things, but this seems quite similar to what Materialize and ReadySet do, but like “as a library”, because Rama doesn’t use a “separate” layer for the storage stuff. Is that correct-ish? reply Pxtl 11 hours agorootparentprevSo the idea is that you could do1. send event data to depot2. trigger localized ETLs (or put it high-priority in queue) to recalculate just the impacted data into relevant PStates3. await completion of aforementioned ETLs4. run query from updated PStatesMaybe too heavy for an upvote, but very appropriate for a an important transaction like a purchase. reply jitl 13 hours agorootparentprevRama should bundle a write-through cache! Another in-memory JVM cluster thingamabob (Apache Ignite) used to propose write-through caching as it&#x27;s primary selling point: https:&#x2F;&#x2F;ignite.apache.org&#x2F;use-cases&#x2F;in-memory-cache.html#:~:....Or, maybe their pitch is that the streaming bits are so fast, you can just await the downstream commit of some write to a depot and it&#x27;ll be as fast as a normal SQL UPDATE. reply nathanmarz 13 hours agorootparentRama is extremely fast, as you can see for yourself by playing with our Mastodon instance. reply jedberg 13 hours agorootparentIt’s fast until it’s not. Making a post and then hitting reload and not seeing it can be very jarring for the user. Definitely something to think about. reply nathanmarz 13 hours agorootparentWhat do you mean? Every post I do shows up instantly.Reloading the page from scratch can be slow due to Soapbox doing a lot of stuff asynchronously from scratch (Soapbox is the open-source Mastodon interface that we&#x27;re using to serve the frontend). https:&#x2F;&#x2F;soapbox.pub&#x2F; reply squeaky-clean 13 hours agorootparentI think the concern is will this still be true if Mastodon reaches Twitter scale? reply nathanmarz 13 hours agorootparentRama is scalable. So as your usage grows, you add resources to keep up. Scaling a Rama module is a trivial one-line command at the terminal.Rama&#x27;s built-in telemetry provides the information you need to know when it&#x27;s time to scale. reply jitl 13 hours agorootparentprevis there a way to guarantee reading your own writes from a client perspective? reply nathanmarz 13 hours agorootparentYes. Depot appends by default don&#x27;t return success until colocated streaming topologies have completed processing the data. So this is one way to coordinate the frontend with changes on the backend.Within an ETL, when the computations you do on PStates are colocated with them, you always read your own writes. reply teacpde 11 hours agorootparentIt makes sense, but wouldn’t the write be slow? Especially when you have many streaming pipelines. reply nathanmarz 11 hours agorootparentThat&#x27;s part of designing Rama applications. Acking is only coordinated with colocated stream topologies – stream topologies consuming that depot from another module don&#x27;t add any latency.Internally Rama does a lot of dynamic auto-batching for both depot appends and stream ETLs to amortize the cost of things like replication. So additional colocated stream topologies don&#x27;t necessarily add much cost (though that depends on how complex the topology is, of course). replykulahan 14 hours agorootparentprevThis explains so many bugs I came across on Reddit. I guess it works, but man I dislike this implementation. reply reilly3000 14 hours agorootparentprevDynamoDB’s DAX cache espouses the same approach.I have to say in my ~12 years as an active Redditor I can’t recall a time where I saw any real state issues, even with rapidly changing votes, etc. Bravo!? Now that we’re beyond the days of molten servers, I have to say its overall reliability in the face of massive spiky traffic is quite a feat. reply endisneigh 14 hours agorootparentReally? I see this all the time even now. reply sixo 15 hours agoparentprevI imagine you get some UUID back from your write, and effectively \"block\" until you see it committed to the event stream. The intent of such a system is certainly for the read-after-write latency to be not much longer than a traditional RDBMS. (This is roughly what the RDBMS is doing under the hood anyway.) Probably you can isolate latency-critical paths so they don&#x27;t get stuck behind big stream processing jobs.The advantage of the overall architecture is that nearly all application functionality (for something like a social network) can tolerate much higher latency than an RDBMS, so you really want to have architectural building blocks that let you actually use this headroom. reply throw14082020 5 hours agoparentprevIt&#x27;s called \"read-after-write consistency\". Write-through cache is one way, or just use a strongly consistent database, haha.https:&#x2F;&#x2F;avikdas.com&#x2F;2020&#x2F;04&#x2F;13&#x2F;scalability-concepts-read-aft...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cache_%28computing%29#Writing_...https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;databases&#x2F;why-you-sho... reply chubot 13 hours agoparentprevYeah definitely, these ideas always sound very appealing to me, in theory -- I almost wonder why nobody has built it beforee.g. they mention \"event sourcing\" and \"materialized views\" in the post -- sounds goodBut I thought I heard from a few people who were like \"we ripped event sourcing\" out of our codebase and so forthAnd yeah your question is an obvious good one, and the Reddit answer of \"write through cache\" ... is less than satisfying to meI FREQUENTLY have the problem where I reload the page and Reddit shows me stale data. It&#x27;s SUPER buggy.---Anyway I definitely look forward to hearing people try this and what their longer term impressions are !I basically want to know what the tradeoffs are -- it sounds good, but there are always tradeoffsSo is the tradeoff \"eventual consistency\" ? What are the other tradeoffs? reply nathanmarz 13 hours agorootparentWhen you step back and consider the incredible amount of manpower and resources that have been put into these applications, it&#x27;s amazing how buggy these applications are. To put it simply, they&#x27;re buggy because the underlying infrastructure and techniques used to build them are so complex that the implementation is beyond the realm of human understanding.The way applications are built, and have been built since before I was born, is by combining together potentially dozens of narrow tools together: databases, computation systems, caches, monitoring tools, etc. There has never been a cohesive model capable of expressing arbitrary backends end-to-end, and every application built has to be twisted to fit onto the existing narrow pieces.Rama is a lot more than just \"event sourcing\" and \"materialized views\". Those are two concepts at its foundation, but the real breakthrough is being that cohesive model capable of expressing diverse backends in their entirety. It took me more than five years of dedicated research to discover this model, and it was extremely difficult. reply chubot 12 hours agorootparentYes, I 100% agree with you. I would like something like this to succeed, and agree the problem is real.But what are the tradeoffs? There&#x27;s nothing that comes with 100x benefit with no tradeoffs(side note: I worked on Google Code for a short while in 2008, concurrent with Github&#x27;s founding ... I think Github moved a lot faster in a large part because they weren&#x27;t dealing with distributed systems at first -- they had a Rails app, a database, and RAID disks, and grew it from there. We had BigTable and perf reviews :-P )Eventual consistency is probably one?Can I specify that comment editing is correct and ACID, while likes&#x2F;upvotes are eventually consistent? (No is a fine answer, these problems are hard)I read through much of the doc, and don&#x27;t see a mention of the word \"consistency\" at all, which seems like an oversight for something that is unifying what would be in a database with computation. reply nathanmarz 11 hours agorootparentRama is a much broader platform than a database, so the consistency semantics you get depend on how you use it. When using Rama, you&#x27;re not mutating indexes directly like you do with a database, but adding source data that then gets materialized into any number of indexes.You get read-after-write consistency for any PStates in a streaming ETL colocated with the depot you appended to. This is if you do the depot append with \"full acking\", which coordinates its response with the completion of colocated streaming ETLs. If you append at a lower level of acking, then you get eventual consistency on those PStates at the benefit of lower latency appends.Microbatching is always eventually consistent as processing is asynchronous and uncoordinated to depot appends. Microbatching is higher thorughput than streaming and has simpler fault-tolerance semantics.You&#x27;ll be able to read a lot more about this when we release the docs next week. reply datavirtue 10 hours agorootparentI would like your newsletter. reply datavirtue 10 hours agorootparentprevYou should try using Facebook marketplace. It is so rickety. I have to get on a desktop to use it at all. reply chubot 13 hours agorootparentprevHilariously, I went to edit the above comment, and HN was overloaded. Then it served me three or four 500&#x27;s, AND it served me stale data in betweenI was pissed off that I would have to type my comment again, but actually it did save it, and refreshing worked.From what I understand Hacker News is architected more in-memory, on one big box ... Perhaps similar to the event sourcing model(not knocking hacker news -- it&#x27;s generally a very fast site, MUCH better than Reddit. Just that scaling beyond a single machine is difficult and full of gotchas ) reply kaba0 5 hours agorootparentStackOverflow used to run on a single (very beefy) machine also for a long time — databases make efficient use of vertical scaling, horizontal scaling is much harder.Of course, specialist systems can often do much better. reply jokethrowaway 15 hours agoparentprevYou can hack it and optimistically render the data you know about because your client created it - on the frontend, at no additional cost. reply hot_gril 15 hours agorootparentThis is usually what I do. Don&#x27;t even want to wait for an HTTP roundtrip for some of these, e.g. \"liking\" a post should fill in the heart icon or whatever instantly.One famous example of this going to far: Mac Mail app used to play a whoosh sound when your email is actually sent. They changed it to whoosh instantly no matter what. Given how often an email might fail to send or get delayed, this meant an actually useful indication of \"great, your thing was sent, you can close your laptop now\" was rendered useless. reply ceejayoz 15 hours agorootparent> Don&#x27;t even want to wait for an HTTP roundtrip for some of these, e.g. \"liking\" a post should fill in the heart icon or whatever instantly.HN does this, and on slow days, about half of my upvotes don&#x27;t go through. reply hot_gril 15 hours agorootparentMessaging apps often have a checkmark to indicate the message actually went to the server, and maybe another checkmark to indicate it was received on the other end. Maybe HN needs an icon indicating that your vote went through. reply newaccount74 13 hours agorootparentMake the arrows grey to indicate the click registered, make them disappear to indicate the server successfully registered the vote? reply hot_gril 11 hours agorootparentYeah, it&#x27;s easy enough that I was able to do it in the web inspector in a minute (artificial 1s network delay added): https:&#x2F;&#x2F;s11.gifyu.com&#x2F;images&#x2F;ScPMI.gif reply wizofaus 13 hours agorootparentprevYou actually check your list of upvoted comments? reply ceejayoz 13 hours agorootparentNo, I just notice it when I come back to the thread later in the day and a bunch of comments I know I upvoted are back to normal. reply sitzkrieg 14 hours agorootparentprevyea but does hn have any client side js? reply codetrotter 13 hours agorootparentYeah, a very small amount so that clicking the upvote button does not need to reload the whole page reply bmacho 3 hours agorootparentUpvoting does work javascript disabled, it upvotes and reloads the page. It approximately keeps your position by jumping the message anchor. reply hot_gril 11 hours agorootparentprevCan confirm, HN relies on client-side JS for voting and collapsing, but view&#x2F;post&#x2F;edit&#x2F;delete don&#x27;t need it. reply sitzkrieg 9 hours agorootparentmakes sense, thanks for the clarification yall replylossolo 13 hours agoparentprevYou have the option to track the latest update time and, during the minute immediately following this update, direct all reads to come from the leader. Additionally, you could oversee the replication lag among followers and block queries on any follower that lags more than a minute behind the leader.For the client, it&#x27;s feasible to retain the timestamp of its most recent write. In this way, the system can ensure that the replica responsible for any reads related to that user incorporates updates at minimum up to that recorded timestamp. If a replica isn&#x27;t adequately current, the read can either be managed by another replica or the query can wait until the replica catches up. The timestamp might take the form of a logical timestamp, signifying the order of writes (e.g., log sequence number), or it could be based on the actual system clock, where synchronized clocks become vital.When your replicas are spread across multiple datacenters—whether for user proximity or enhanced availability—there&#x27;s an added layer of complexity. Requests requiring the leader&#x27;s involvement must be directed to the datacenter housing the leader. reply softwaredoug 15 hours agoprevIt’s a massive ask, even if the platform was 100x better, for all developers to give up every programming language and database they’ve ever used to depend on a startups closed source platform for all functionality.It’s hard enough trusting Google or Amazons cloud offerings won’t change.It seems that’s what they’re proposing right? What am I missing? reply nathanmarz 15 hours agoparentWe&#x27;re actually not asking anyone to give up anything. First off, it has a simple integration API (which you&#x27;ll be able to see the details of next week) that allows it to seamlessly integrate with any other backend tool (databases, monitoring systems, queues, etc.). So Rama can be incrementally introduced into any existing architecture.Second, Rama has a pure Java API and is not a bespoke language. So no new language needs to be learned. reply erlend_sh 14 hours agorootparentSo Rama-powered apps need to be written in Java? Or will any JVM language work?And the Rama core will remain closed-source? That part seems like the toughest sell of all, at a time when the vast majority of developer tooling and backends are open source or at the very least source-available. reply nathanmarz 14 hours agorootparentAny JVM language should work. We&#x27;ve built modules with Clojure.We&#x27;re keeping it closed-source for now. reply vanviegen 13 hours agorootparent> We&#x27;re keeping it closed-source for now.Rama sounds interesting to me for my &#x27;next big project&#x27;, but I&#x27;d not even consider building it on top of a closed core. I think this is a pretty common sentiment in these circles.I understand building an OSS business is not easy either. But perhaps there is some middle of the road that you can walk?- A contractual obligation to open source all (now current) code a couple of years in the future? - Or an almost-OSS license that makes life difficult for competing cloud providers, like https:&#x2F;&#x2F;www.mongodb.com&#x2F;licensing&#x2F;server-side-public-license... ? reply anonzzzies 3 hours agorootparentYep, not OSS licensed is a nope for us for core dependencies. I am aware many here like to sign away freedom (even future viability) for more productivity now (using some new fangled cloud thingy which might&#x2F;will be gone in a few months to underpin your entire product), but we are not interested in that. reply roguas 14 hours agorootparentprevSince all jvm languages usually have \"ffi\" to javaapis&#x2F;javalibs, I would say yes. reply kpw94 12 hours agorootparentprev> Rama can be incrementally introduced into any existing architectureBig if true (and if the opposite, of incrementally removing it also works). There have been similar platform efforts in past, such as https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20985429 . For that one, the \"massive ask to give up every programming language and database they’ve ever used to depend on a startups closed source platform\" seems like the biggest hindrance to adoption. reply mikae1 15 hours agorootparentprev> Second, Rama has a pure Java API and is not a bespoke language. So no new language needs to be learned.Isn&#x27;t Mastodon a Ruby On Rails application? reply theogravity 15 hours agorootparentThe article says they re-wrote Mastodon from scratch (probably the backend piece). I&#x27;m guessing in Java. reply nathanmarz 14 hours agorootparentYes, it&#x27;s 100% written in Java. reply newlisp 10 hours agorootparentThis looks interesting and a nice weekend read but just skimming through, why is spring needed? reply newlisp 5 hours agorootparentNevermind (it has to be paired with some web server&#x2F;framework). replysoftwaredoug 11 hours agorootparentprevI can imagine this being really useful from the ground up. Because it looks like it wants to be the source of truth, with different views on the data.It’s hard to imagine it for a complex legacy application without having lots of added complexity. It wants to be the unifying programming model for the application. It would seem like running with two RDMS sources of truth simultaneously.It’s like the xkcd “there are 12 ways of doing X, let’s create a standard to unify them” now there are 13 ways reply fragmede 11 hours agorootparentThat&#x27;s xkcd 927.9, which is 3^2, and 27, which is 3^3. Or 900 is Yoda&#x27;s age, and 27 which is the 27 club of musicians who committed suicide. reply mkl 11 hours agorootparentMost members of the 27 Club didn&#x27;t die by suicide: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;27_Club reply russelg 9 hours agorootparentTrue, but perhaps you could call drug overdosing suicide. reply yazzku 15 hours agorootparentprevWhat is the licensing of Rama? Is it libre&#x2F;open? reply masklinn 14 hours agorootparent> We&#x27;re keeping it closed-source for now. reply kennydude 6 minutes agoprevYou didn&#x27;t build Mastodon at Twitter-scale.You built a Mastodon-compatible clone in Spring&#x2F;Reactor. reply ThinkBeat 12 hours agoprevI am confused.This is meant to be hyped to sell your Rama platform&#x2F;product&#x2F;framework? That you have spent 10 years building in secret? During that time you have built a datastore and a Kafke competitor and ?Should not those 10 years be factored into the time it took to develop this technical demo?Is it 100x less code including every LOC in all of Rama?I mean I am sure you picked a use cast that is well suited to creating a Twitterish architecture implementation.If I went off and wrote a ThinkBeat platform for creating Twitterish systems and then created a Twitterish implementation on top if it, its real easy to reach low LOCs. reply nathanmarz 10 hours agoparentYour comment is addressed in another thread by other commenters: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37137653 reply softwaredoug 9 hours agoparentprevKind of reminds me of when FoundationDB came out, they really needed to demonstrate lots of different use cases to prove they were the database storage layer to rule them all... Not just one reply sharms 15 hours agoprevThe performance on the example Mastodon instance is very responsive - almost anywhere I clicked loaded nearly instantly. I created an account and the only thing I found missing was it doesn&#x27;t implement full text search unless my user was tagged, but that might be a Mastodon specific item.I think they have thought a lot about typical hard problems, such as having the timeline processing happen along side the pipeline, taking network &#x2F; storage etc out of the picture. Nice work! reply nathanmarz 15 hours agoparentThat is indeed an intentional part of Mastodon&#x27;s design, which we tried to be faithful to as much as possible. We originally implemented search across all statuses and had to reimplement it when we realized Mastodon is a little different. reply sitzkrieg 14 hours agorootparentdid you ever consider starting from something already technically performant like pleroma or misskey? reply nathanmarz 13 hours agorootparentWell, we didn&#x27;t start from anything as we implemented this completely from scratch. I believe Mastodon is much more widely used than those so it seemed like a better target for this. reply sitzkrieg 13 hours agorootparentyea i misspoke, good distinction lol. certainly makes sense, thanks replyafro88 14 hours agoprevLooks amazing and incredibly smart. But I found the LOC and implementation time comparisons to Twitter and Threads very disingenuous. It makes me wonder what other wool will be pulled over our eyes with Rama in future (or important real world details missed &#x2F; future footguns).Still super impressive. Reminds me of when I discovered Elixir while building a social-ish music discovery app. Switching the backend from Rails to Elixir felt like putting on clothes that actually fit after wearing old sweats. Rama looks like a similar jump, but another layer up, encompassing system architecture. reply StephenAmar 14 hours agoparent+1 the comparisons are not great. How much engineer-hours did it take to build Rama itself?The numbers they got for Twitter likely include the time it took to build their infrastructure, common libraries (like finagle,…) reply jacoblambda 12 hours agorootparentHonestly I&#x27;m willing to accept the number they gave since the author (Nathan Marz) was one of the lead&#x2F;founding devs for twitter&#x27;s streaming compute backend in the past. reply paxys 12 hours agorootparentprevDon&#x27;t forget their entire ads system, data processing&#x2F;analytics, monitoring, customer support, payments, internationalization. They have replicated at most a tiny bit of Twitter&#x27;s core infra for sending Tweets. The company itself does a lot more than that. reply softwaredoug 14 hours agoparentprevIt’s hard to construct a true randomized control trial for software engineering methods. People make many claims about programming paradigms or tools hard to validate.It’s also unsure what we would compare a tool like this to. I doubt you could just say “compare it to Rails” given how frameworks like rails are bound to specific data models, and most realistic applications. You’d have to compare it to some other opinion about how to wire together different data structures. reply clusterhacks 14 hours agoprevI&#x27;m excited to see the docs for Rama. But I am also a little scared of the comment \" I came to suspect a new programming paradigm was needed\" from Nathan.It&#x27;s not so much that I think the comment is wrong or anything, but rather that it seems so similar to what I have heard in the past from power-lisp (or Clojure in this case) super-smart engineers.I feel like we have reached a point in software development where \"better\" paradigms don&#x27;t necessarily gain much adoption. But if Rama wins in the marketplace, that will be interesting. And I am quite excited to see what a smart tech leader and good team have been able to grind out given a years-long timeframe in this programming platform space . . . reply nathanmarz 14 hours agoparentThis is why we exposed Rama as a Java API rather than Clojure or our internal language (which is defined with Clojure macros, so it&#x27;s technically also Clojure). Rama&#x27;s Java dataflow API is effectively a subset of our internal language, with operations like \"partitioners\" being implemented using continuations. reply cutler 14 hours agorootparentJust curious, what advantage over Clojure did reverting to \"pure Java\" give you? Perf or something else? reply mwcampbell 14 hours agorootparentPresumably approachability for programmers that would be scared away by Clojure. Smart marketing move. reply jitl 13 hours agoprevThis architecture seems very similar to existing offerings in the \"in-memory data grid\" category, like Apache Ignite and Hazelcast. I&#x27;m more familiar with Ignite (I built a toy Notion backend with it over a few afternoons in 2020).The way Ignite works overall is similar. You make a cluster of JVM processes, your data partitioned and replicated across the cluster, and you upload some JARs of business logic to the cluster to do things. Your business logic can specify locality so it runs on the same nodes as the relevant data, which ideally makes things a lot faster compared to systems where you need to pull all your data across the wire from a DB. Like Rama, Ignite uses a Java API for everything, including serializing and storing plain &#x27;ol java objects.Ignite&#x27;s architecture isn&#x27;t focused on \"ETL\" into \"PStates\". Instead it&#x27;s more about distributed \"caches\" of data. It does have streaming for ingestion (https:&#x2F;&#x2F;ignite.apache.org&#x2F;docs&#x2F;latest&#x2F;data-streaming), but you can transactionally update the datastore directly (https:&#x2F;&#x2F;ignite.apache.org&#x2F;docs&#x2F;latest&#x2F;key-value-api&#x2F;transact...). It also has a \"continuous query\" feature for those reactive queries to retrieve data (https:&#x2F;&#x2F;ignite.apache.org&#x2F;docs&#x2F;latest&#x2F;key-value-api&#x2F;continuo...).Rama&#x27;s data-structure oriented PState index seems easier to work with than building indexes yourself on top of Ignite&#x27;s KV cache, but Ignite also offers an SQL language, so you can insert your data into the KV cache however, add some custom SQL functions, and then accept more flexible SQL querying of your data compared to the very purpose-built PCache things, but still be able to do lower-level or more performance-oriented logic with data locality.Anyways, if you like some of this stuff but want to use an existing, already battle-tested open source project, you can look for these \"in-memory data grid\", \"distributed cache\", kind of projects. There&#x27;s a few more out there that have similar JVM cluster computing models. reply theptip 13 hours agoparentHazelcast has been on my list to explore for a while. Anyone have pointers to a good sample project &#x2F; deep-dive in the same sort of spirit as the OP here?Also would love to hear folks’ thoughts on the sort of usecase where this data grid excels. reply miki123211 11 hours agoprevIs this just me, or does the code in the post feel like they&#x27;ve implemented what should have been a new programming language on top of Java?Their \"variables\" have names that you have to keep as Java strings and pass to random functions. If you want composable code, you don&#x27;t declare a function, you call .macro(). For control flow and loops, you don&#x27;t use if and for, but a weird abstraction of theirs.I feel like this code could have been a lot simpler if it was written in a specialized language (or a mainstream language with a specialized transpiler and&#x2F;or Macro capabilities.)I&#x27;d quote the old adage about every big program containing a slow and buggy implementation of Common Lisp, but considering that this thing is written in Clojure, the authors have probably heard it before. reply nathanmarz 10 hours agoparentInternally there actually is a new programming language, implemented using Clojure macros (so it&#x27;s also Clojure). The Java dataflow API is exposing a subset of that language. We did it this way rather than expose this new language directly because most people don&#x27;t know Clojure and we don&#x27;t feel it necessary or desirable to require people to have to learn a new language to benefit from this technology. reply skybrian 15 hours agoprevIt sounds like interesting technology for someone, but I wonder more about scaling down. What does a developer instance running on a laptop look like? reply nathanmarz 15 hours agoparentGreat question. There&#x27;s actually two ways to look at this: what does it look like to run Rama in a unit test environment, and what does it look like to run a small-scale single-node Rama application in production?For the former, Rama has a class called \"InProcessCluster\" that works identically to a real cluster. It enables Rama applications to be tested and experimented with end-to-end. There&#x27;s an example of this in the post and this is what we&#x27;re releasing next week.For the latter, Rama can be run on a single node with each daemon and module being a separate process. We made it really easy to launch single-node Rama instances with just a couple commands with the \"rama\" script that comes with the release. That said, we haven&#x27;t spent much time yet optimizing small-scale Rama deployments and there&#x27;s likely things we can do to make it more efficient (e.g. combine the Conductor and Supervisor daemons into a single process). reply joelthelion 12 hours agorootparentFollow up question : do you see Rama as being a good fit for applications that &#x2F;don&#x27;t&#x2F; need Twitter scale? These have simpler requirements, but I feel the integration you propose could still have value there. reply nathanmarz 12 hours agorootparentYes, it&#x27;s a better model for developing backends in general. Our comparison against Mastodon&#x27;s official implementation demonstrates this, being at least 44% less code.It&#x27;s the ability to avoid the impedance mismatches which dominate existing tooling that makes such a difference. With existing databases, including RDBMS&#x27;s, you have to twist your application to fit their data models. The existence of things like ORMs help, but they add their own layers of complexity.With Rama, you mold your indexes to exactly match your application&#x27;s needs. And you&#x27;re always just working with objects represented however you want, whether appending data to depots, processing data in ETLs, or storing data in PStates.That computation and storage are integrated and colocated is another way that Rama simplifies application development and deployment. reply dunk010 14 hours agorootparentprevYou&#x27;re killing it with the replies here +++ reply skybrian 11 hours agorootparentprevInteresting. How about running the cloud? I&#x27;m thinking of the many ways someone who wants to start a blog could install Ghost. [1][1] https:&#x2F;&#x2F;ghost.org&#x2F;docs&#x2F;install&#x2F; reply failuser 14 hours agoprevIs there a breakdown of effort Twitter spent doing the mastodon-level service (serving a feed of the accounts you are subscribed to) vs everything else like ads, algorithmic feed, moderation, fighting spam, copyright claims, localization, GR, PR, safety, etc? reply beefnugs 3 hours agoprevI think the marketing idea of this is amazing : I would probably never even consider learning and reading about such a framework if I heard of it straight up. But if you are really releasing a usable open source implementation of something performant that actually federates properly, that is a huge selling point that buys you a ton of respect up front. reply kyle-rb 13 hours agoprevKinda disappointed by the simulation, where are all the viral posts?I&#x27;ve been digging around for a while and haven&#x27;t found any posts with more than 20 faves. The accounts I&#x27;ve found with ~1 million followers have little to no engagement. I want to see how a post with a million faves holds up to the promises of \"fast constant time\".I&#x27;m especially curious about these queries — fave-count and has-user-faved — since a couple years ago Twitter stopped checking has-user-faved when rendering posts more than a month or so old, so I imagine it was expensive at scale. reply nathanmarz 12 hours agoparentThe load generator generates boosts&#x2F;favorites for a subset of posts that are randomly picked to be \"popular\". However, since the rate of posts is so high even individual posts picked to be \"popular\" are only getting ~70 reactions.Tracking reactions is considerably easier than timeline fanout though, as a favorite does a small handful of things (updates set of users favoriting a status and sending a notification), while fanout has to do an operation on every follower (403 operations on average, sometimes up to 22M).The code getting the favorite count for a status looks like: .localSelect(\"$$statusIdToFavoriters\", Path.key(\"*statusId\").view(Ops.SIZE)).out(\"*numFavorites\")Because the nested set is subindexed, that&#x27;s an extremely fast operation (looking at our telemetry, about 0.05ms).Determining \"has-user-faved\" looks like: .localSelect(\"$$statusIdToFavoriters\", Path.key(\"*statusId\").view(Ops.CONTAINS, \"*accountId\")).out(\"\\*hasFavorited\")The API server doesn&#x27;t do these queries individually, which would be two roundtrips. It does them together in a query topology along with fetching other needed information (like number of boosts, number of replies, \"has boosted\", \"has muted this status\", etc.). reply kyle-rb 11 hours agorootparentThanks for the response, I&#x27;m still curious about the details of the subindexing and how that scales. I&#x27;ll be keeping an eye out for the release! reply nathanmarz 10 hours agorootparentA lot of detail on subindexing and everything else in Rama is coming next week when we release the docs. reply NoraCodes 15 hours agoprevI would argue that this is not \"a Mastodon instance\", since it is not running Mastodon - other than that, very very neat work! I&#x27;m excited for that \"Source Code\" link to be live :) reply nathanmarz 14 hours agoparentWe call it a \"Mastodon instance\" because we implemented the entire Mastodon API (https:&#x2F;&#x2F;docs.joinmastodon.org&#x2F;api&#x2F;). This is in addition to also implementing the ActivityPub API which Mastodon also implements for federation. reply dools 10 hours agorootparent\"Originally, Twitter was one, monolithic application built with Ruby on Rails. But now, it&#x27;s divided into about two hundred self-contained services that talk to each other. Each runs atop the JVM, with most written in Scala and some in Java and Clojure\"[1]So is Twitter not a Twitter instance? Like if it looks, walks and toots like a Mastodon, is it not a Mastodon instance?[1] https:&#x2F;&#x2F;www.wired.com&#x2F;2013&#x2F;09&#x2F;the-second-coming-of-java&#x2F; reply mal-2 5 hours agorootparentprevThat doesn&#x27;t match the way people use the term though. Pleroma and Akkoma implement the Mastodon API but wouldn&#x27;t be called Mastodon instances since they aren&#x27;t running Mastodon. reply legi0nary 10 hours agorootparentprevIf you can do this with Bluesky once it federates we might be able to get away from twitter for good. reply INTPenis 10 hours agorootparent>We spent nine person-months building our scalable Mastodon instanceThey federated this brand new code in 9 months, and bluesky still hasn&#x27;t released anything regarding federation. Don&#x27;t keep your hopes up, it would kill their business model to let anyone run part of the network. People-driven networks are just not compatible with commercially driven ones, name one successful example. reply CharlesW 13 hours agorootparentprev> We call it a \"Mastodon instance\" because we implemented the entire Mastodon API…Except \"Mastodon instance\" means an instance of Mastodon, which is open source. Whether or not it was intended to be deceptive (I&#x27;d think a group of smart people would know better), this personally left a bad taste in my mouth. reply mattl 13 hours agorootparentprevMastodon-compatible would be better.Mastodon is the name of a piece of software as well as an API, a website, etc.Naming this stuff is hard but calling it a Mastodon instance would be more confusing. reply ollien 15 hours agoparentprevYeah, I think this is just an ActivityPub server that supports the Mastodon extensions, right? I think we should embrace the fact that the federated world can be diverse, rather than just call everything \"Mastodon\" reply jauntywundrkind 15 hours agorootparentMastodon has it&#x27;s own API. It basically offers a very limited ActivityPub API too, but it&#x27;s own API is very different.And it&#x27;s a very slim ActivityPub inplementation. For example, I don&#x27;t think you can do basic things like get an individual post in ActivityPub. This should be easy simple json-ld to get but it&#x27;s just 404. https:&#x2F;&#x2F;www.w3.org&#x2F;TR&#x2F;activitypub&#x2F;#retrieving-objects reply colatkinson 14 hours agorootparentMastodon for sure supports fetching individual posts over ActivityPub. For example: curl -L -H &#x27;accept: application&#x2F;activity+json&#x27; &#x27;https:&#x2F;&#x2F;mastodon.social&#x2F;users&#x2F;Gargron&#x2F;statuses&#x2F;18614983&#x27;It does have a bunch of stuff that isn&#x27;t federated though, such as Like counts&#x2F;collections. And of course it only implements the server-to-server (S2S) part of AP, not the client-to-server (C2S) part. reply pmlnr 14 hours agorootparentprevMastodon. o. not a. reply rvnx 15 hours agoparentprevI think it&#x27;s smart from a legal perspective, because the team members seem to partially be coming from companies acquired by Twitter.So I guess, if you say \"it&#x27;s a Mastodon-clone\", you cannot be accused of taking proprietary ideas from Twitter (this is just a guess, they know better).But technically very interesting and refreshing to see. I really like their approach. It feels they are innovative. reply progval 3 hours agorootparentFrom a legal perspective, it&#x27;s against Mastodon&#x27;s trademark policy: \"Only use the Mastodon marks to accurately identify those goods or services that are built using the Mastodon software.\" https:&#x2F;&#x2F;joinmastodon.org&#x2F;trademark reply mikae1 15 hours agoparentprevIt was a good blog post title choice for making it to the front page of HN. reply Pxtl 15 hours agoparentprevYes, a \"mastodon-like Fediverse instance running on our proprietary new application&#x2F;data framework\" sounds like a better description.And either way, I think the source code to their Mastodonlike will not be usable since it will be running on their Rama server framework. reply frandroid 15 hours agoparentprevMastodon-compatible or Mastodon clone... If it quacks and walks like a duck, surely it is part of the aviary family of microblogging services... reply zubairq 4 hours agoprevAs someone who has worked in both startups and Enterprise IT for over 30 years (including large Java based systems) I see a use case for Rama in large companies who have a lot of difficulty glueing many different systems together to achieve scalability. So I think that Red Planet Labs could get several contracts in the 100k USD and over range in large Enterprises. This is for enterprises who have the problem of integrating many systems to achieve scalability and who are already large Java shops.However, I do not see Rama&#x27;s initial market being startups, since they just want the simplest way possible to build UI + backend and want to iterate super fast with tech that their developers already know in the initial stages. reply yayitswei 12 hours agoprevFor context, nathanmarz created what is now Apache Storm, which is used for stream processing at some of the world&#x27;s largest companies, so he knows a thing or two about scale. reply gfodor 15 hours agoprevSomething I&#x27;m immediately thinking about with this is change management and inertia at the early stages of a new, underdefined project. Less code is great, the big question is how such a system compares to the usual hack-and-slash method of getting a v1 up and running as you search for PMF from the perspectives of ops, cost, data migrations, rapid deployments, and so on. Presumably, the idea here is to start from the beginning with Rama, skipping over the usual \"monolith fetches from RDBMS\" happy paths, even for your basic prototype, this way you don&#x27;t slip into a situation like Twitter did where that grew slowly into an unscalable monstrosity requiring a rewrite. So an article focused on the \"easy\" part that&#x27;s required in the beginning of rapid change, as much as it&#x27;s not as important as the \"simple\" part that shines later at scale, seems useful. reply nathanmarz 14 hours agoparentThanks, this is a good idea for a another post.The basic operation Rama provides for evolving an application over time is \"module update\". This lets you update the code for an existing module, including adding new depots, PStates, and topologies. reply beders 9 hours agoprevThis is a lovely and very detailed showcase in how to combine streaming+ETL+materalized-view+query!That said: You need better advisors. Your investors and&#x2F;or the board gave you bad advice on how to publish these accomplishments and talk about them.I hope your go-to-market strategy works out a little better. Hyperbole is fine, but at least on hacker news, the audience is a bit careful with regards to grandiose statements.What might work well on an investor presentation might backfire when you target engineers as audience. reply runeks 3 hours agoprev> To demonstrate the scale of our instance, we’re also running 100M bot accounts which continuously post statuses (Mastodon’s analogue of a “tweet”), replies, boosts (“retweet”), and favorites. 3,500 statuses are posted per second, the average number of followers for each post is 403, and the largest account has over 22M followers. As a comparison, Twitter serves 7,000 tweets per second at 700 average fanout (according to the numbers I could find).Is Twitters 7k tweets per second the average? If so, what’s the peak rate, and have you tested your system under this load? reply wink 1 hour agoprevLook, I don&#x27;t want to defend Twitter but ignoring 15 years of changes and the whole journey of scaling and then using the cost op just building a snapshot of the 15y old version is pretty disingenuous.That&#x27;s a bit like starting an Oracle clone now and summing up what they spent on developer salaries in the last 40 years. You basically can&#x27;t not \"reduce costs\".And no \"the original consumer product\" is not a real cop-out, you probably still have tons of people building iterations. reply ltr_ 13 hours agoprevi always had this question: how realistically is to, having an standard spec and interoperable protocols, for toxic apps of big international tech companies that provides \"\"\"services\"\"\", so instances of implementations can be maintained by municipalities or local tech business and talent with 100x less employees and money? what policies should be in place to achieve that? what would be the challenges? it would be better&#x2F;healthier? is someone researching such things like transition to sustainable digital services? (sustainable in terms of local labor, privacy, economy, accountability, etc...)i mean if you think about this as public services not as a business, profit is secondary, and first is just to make the thing better and better for the users, no need for spying , no advertisement, no need for a rich piece of shit somewhere getting a piece of the money paid in your city for every taxi drive, food delivery or to give up privacy to a soulless&#x2F;faceless entity just because you want to say something publicly or keep in touch with people. there is no disruption from their part, its just an old thing put on the internet, they are just in the middle of everyone&#x27;s life, just sucking everything they can. is the actual state of affairs \"efficient\"?there must be fed up engineers and tech people everywhere with the sad state of IT industry. reply 5636588 13 hours agoparentThe EU already has regulations in place regarding open banking with its Payment Services Directive. I&#x27;d imagine a similar framework could be applied to big social tech. reply duped 15 hours agoprevThis is what they&#x27;ve been hyping on Twitter for a week?FWIW, why hype at all? Why \"We&#x27;ll more in a week. Then more in two weeks.\" Show the code today! reply newaccount74 13 hours agoparentConsidering the length and amount of detail in this blog post, I understand why they would need another week to get the code ready (assuming there will be more docs) reply nathanmarz 12 hours agorootparentWe&#x27;re releasing 100k words of high-quality documentation next week. reply dbish 9 hours agorootparentMeasuring words and loc is not a great way, imho, to share what you’re doing. In fact, I’d love a much shorter set of documentation now to under and this better. Long docs will probably make it less likely to read. reply gexla 7 hours agoprevThis is fascinating, and I&#x27;m glad to have come across it. And your story is inspiring. Thank you!One question, why Google Groups rather than something like Discord? Not sure I would trust Google Groups to be around long. reply cduzz 9 hours agoprevI would hazard the guess that twitter&#x27;s \"show tweets to other people\" is 1&#x2F;40th of the functionality piled into twitter; some other large functions would be things like \"track ad sales\" or \"improve engagement\" or \"allow random law enforcement organizations to engage in whatever access is needed for any particular part of the world\" Each of those is going to be a huge pile of code and all of it working together is going to N! your complexity. reply alexcpn 3 hours agoprevWhy choose Java of all languages. Why not something more modern and less verbose like Go or Rust. Just asking as I have worked enough in Java and then spend a lot of time in GC tunining. Granted the code was not that great and from a diverse team with different skill levels causing all the leaks.. But still reply jillesvangurp 1 hour agoparentThey actually used clojure; which is an interesting choice.GC tuning on the JVM is much less of a topic these days than it used to be. The default garbage collector was changed at some point (G1). It has some configuration options but they come with sane defaults that mostly just work fine and adapt to your memory and cpus. You don&#x27;t spend a lot (or any) time on tuning this typically. I know I haven&#x27;t even looked at GC params in many years now. Never had to. And we run on modestly sized vms of 1 or 2 GB typically. This was different 10 or so years ago when G1 was still newish and not default. ZGC was introduced with Java 11 (I think), and aimed at very large heaps. It trades off additional overhead for guaranteeing very low latency. That tradeoff is why it is not default. For most users, G1 without any tuning whatsoever should be fine. Generally, if you are stressing your heap, you get more hardware. And if you are not, the GC should be keeping up just fine.Anyway, like it or not, the JVM has been a work horse for big data for ages. Things like Hadoop, Kafka, Cassandra, Elasticsearch, etc. all run on it and scale fine (and typically without a lot of GC tuning). The only feasible alternative to the jvm used to be things like C++. Lately, Go and Rust are pretty credible in this space as well and both have had to do a bit of catchup in terms of maturity of tooling, libraries, and language features. Things like generics (Go), async (Rust), etc. are still fairly recent additions and both kind of relevant in a project of this type.In any case, switching languages is hard for teams and these guys have been around for quite some time. When they started, Rust was a lot less mature than it is now and Go was still pretty new as well. Neither was an obvious choice for this stuff at the time. reply endisneigh 12 hours agoprevI don’t really see the point of the comparison. They should show something you could only make with Rama or show how much faster it is to iterate with Rama.Saying this is 100 or even a million times cheaper is like saying taking a picture of Sistine chapel and printing out copies is a trillion times cheaper than making it originally.Many of us on this site could make a number of products very efficiently and cheaply given a static and fixed set of requirements as well as an existing implementation for reference.That being said it was a very detailed post, so kudos for that, but it’s far too vague to be actionable. Why not just release the code and post simultaneously instead of just bragging about how little code was required? reply sixo 15 hours agoprevThe comparisons to Twitter are completely goofy, but the architecture is nothing short of enlightened. Nice work. reply runeks 2 hours agoprevSounds like Rama is also useful for small scale applications (where high scalability isn’t needed), since it simplifies how they’re implemented.Is this the case — ie. would a TodoMVC app implemented in Rama also be much simpler than a traditional frontend&#x2F;backend&#x2F;database CRUD implementation? reply rubiquity 14 hours agoprevNoticeably missing are any details about concurrency control and replication or recovery protocols. A Twitter clone is one thing but any sort of application needing ACID Transactions is a whole other beast. reply nathanmarz 14 hours agoparentAll data on Rama is replicated automatically with a configurable \"replication factor\". Data written to Rama is not made visible until it&#x27;s successfully replicated. The documentation we&#x27;re releasing next week includes a page going into detail in how this works. reply runeks 2 hours agorootparentDoes \"replicated\" here mean that the data is persisted to disk? reply nevi-me 6 hours agoprevIt sounds like the authors spent 10 years building a Twitter factory, and now they can produce a Twitter faster than Twitter could produce it \"by hand\". reply donavanm 5 hours agoprevI appreciate the inversion&#x2F;melding of the data model and compute. Im curious to know your perspective on two parts: How would multitenancy fit in to rama? Using your mastodon example, providing “hosted mastodon instances as a service”, where you _also_ allow for data governance, per customer encryption at rest, user IDP support, etc. Is it multiple single tenant rama deployments, running independent customers? Multitenant rama clusters with shared depos and each pstate includes “tenant id”? Something else?Second whats the product&#x2F;business angle on customer confidence, technical novelty, and your business core competency? A dated example but Im thinking of somewhere like basho with riak. Super cool tool, takes some mental adjustment to “get”, challlenges selling hosting vs software vs pro services. reply primitivesuave 11 hours agoprevThe \"N bots posting X times&#x2F;second\" isn&#x27;t a very meaningful statistic. A system&#x27;s reliability is mostly characterized by its performance under stress. reply RHSman2 3 hours agoprevEasy to copy something that has been done before and knowing what you want, need and responding to expected market traffic. reply doublepg23 11 hours agoprevHN seems to be putting you through the wringer, I for one am excited you guys made this and plan to open source it- it looks like a fantastic project. reply nullspace 5 hours agoparentAdditionally to the “why not” sibling comment, I suspect nathanmarz can take it :). (Early Apache storm user here.)It takes strong conviction to work on something like this for a decade. reply typon 7 hours agoparentprevWhy not? Incredible claims should have equal amount of scrutiny. I am glad that HN has a default skeptical bias - we don&#x27;t want to be swept up in frenzies. If anything, HN is still relatively prone to these frenzies from time to time. reply NoahTheDuke 13 hours agoprevCongrats! This looks super cool.Are there any plans for exposing a Clojure API? Given that it&#x27;s implemented in Clojure, seems like it would be a natural fit. Interop with Java is nice but can be cumbersome compared to the more natural calling conventions and idioms (threading macros instead of `..` builder patterns, etc). reply nathanmarz 11 hours agoparentAnswered this in another comment: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37138526 reply NoahTheDuke 11 hours agorootparentThanks. reply DigitalSea 11 hours agoprevOne of these posts. Dig into the numbers and claims, and you&#x27;ll see that they&#x27;re not building something anywhere near Twitter scale. reply _dwt 15 hours agoprevHmmm, \"Rama is programmed entirely with a Java API – no custom languages or DSLs\" according to the landing page, but this sure looks like an embedded DSL for dataflow graphs to me - Expr and Ops everywhere. Odd angle to take. reply nathanmarz 15 hours agoparentI consider \"DSL\" as something that&#x27;s its own language with it&#x27;s own lexer and parser, like SQL. The Rama API is just Java – classes, interfaces, methods, etc. Everything you do in Rama, from defining indexes, performing queries, or writing dataflow ETLs, is done in Java. reply chc4 15 hours agorootparentThis is usually referred to as an \"embedded DSL\" - you have a DSL embedded in a normal programming language using its first class constructs. reply runeks 2 hours agorootparentMeh. By this definition all libraries expose an \"embedded DSL\" — their API. I&#x27;m honestly not sure this is a useful definition. reply jillesvangurp 25 minutes agorootparentWhether you like it or not; internal DSLs became a thing with Ruby back in the day. And these days things like Kotlin also lend themselves pretty well to creating internal DSLs. Java is not ideal for this. Kotlin and Ruby have a few syntax features that make it very easy. reply gfodor 15 hours agorootparentprevYep the original term DSL was for custom languages, the eventual introduction of using it for these kinds of literate APIs was done later. Using it in the original way unqualified is fine imo. reply goostavos 15 hours agoparentprevOdd thing to split hairs over. reply gkfuhff 15 hours agorootparentWhen someone makes a distinction that you don&#x27;t immediately appreciate, maybe don&#x27;t just dismiss it as splitting hairs, as if the world was a simple place. reply dcre 15 hours agorootparentprevIt’s not a small detail. It’s one of the headline claims! reply Huhuhn 13 hours agoprevWould this framework also be useful for building a Lemmy instance? reply dustingetz 15 hours agoprevSummarizing, now edited down with some editorializing for clarity:What is it? build web-scale reactive backends with an expressive java dataflow API. Instead of a database you develop your own custom app-specific indexes which are reactive, distributed and durable. It&#x27;s like event sourcing and materialized views but integrated in a linearly scalable way.> I cannot emphasize enough how much interacting with indexes as regular data structures instead of magical “data models” liberates backend programming> It allows for true incremental reactivity from the backend up through the frontend. ... enable UI frameworks to be fully incremental instead of doing expensive diffs to find out what changed.Ok, so in my mind I am positioning this against Materialized &#x2F; differential dataflow, whose key primitive is a efficient streaming incremental join that works across very large relational tables. Materialized makes SQL reactive, Rama gives you a java dataflow DSL for developing purpose-built reactive database indexes.How it works? 4 concepts: Depot, ETLs, PState, QueryDepots: \"distributed, durable, and replicated logs of data.\" [Event streams?] \"like Kafka except integrated\" \"All data coming into Rama comes in through depot appends.\"ETLs: data arrives via depots, and is ETLed to PStates via \"a Java dataflow API for coding topologies that is extremely expressive\". \"Most of the time spent programming Rama is spent making ETLs.\"PStates seem like reactive data structures that are also durable&#x2F;replicated, these are meant to supersede your database and indexes, letting you build custom purpose-built indexes that contain 100M elements:> “partitioned states” are how data is indexed in Rama ... Unlike existing databases, which have rigid indexing models (e.g. “key-value”, “relational”, “column-oriented”, “document”, “graph”, etc.), PStates have a flexible indexing model. In fact, they have an indexing model already familiar to every programmer: data structures. A PState is an arbitrary combination of data structures. ... nested data structures can efficiently contain hundreds of millions of elements. For example, a “map of maps” is equivalent to a “document database”, and a “map of subindexed sorted maps” is equivalent to a “column-oriented database”. Any [composition] is valid – e.g. you can have a “map of lists of subindexed maps of lists of subindexed sets”.Query: once you develop PStates to aggregate relevant data into a custom index of the right ... shape?, query seems sorta like GraphQL selectors over your custom index:> Queries in Rama take advantage of the data structure orientation of PStates with a “path-based” API that allows you to concisely fetch and aggregate data from a single partition> “query topologies” ... real-time distributed querying and aggregation over an arbitrary collection of PStates. These are the analogue of “predefined queries” in traditional databases, except programmed via the same Java API as used to program ETLs and far more capable. reply FridgeSeal 12 hours agoprevSemi-related: Their homepage (https:&#x2F;&#x2F;redplanetlabs.com&#x2F;) has to be one of the best looking websites I’ve seen in a while, buttery smooth as well. I love it. reply alberth 12 hours agoparentIf you like the pretty static background look with light text, checkout https:&#x2F;&#x2F;carrd.co&#x2F;It’s a website builder with lots of themes similar in design. reply whateverman23 15 hours agoprevctrl+f \"ads\"ctrl+f \"monetization\"ctrl+f \"moderation\"ctrl+f \"existing infrastructure\"ctrl+f \"personalization\"etc etcYeah about what I expect from a \"we rebuilt twitter for cheap\" post. There&#x27;s no point to the comparisons with the Twitter codebase size&#x2F;cost. It completely distracts from what is probably a perfectly fine project. reply jscottmiller 15 hours agoparentThat&#x27;s a fair criticism - this isn&#x27;t an apples-to-apples comparison. What I find interesting about this is the cost of running the service. Being able to run a twitter-like thing on a hundred or so large aws instances is neat and I&#x27;m sure that many folks here dream of that kind of efficiency at their day jobs, but I&#x27;m more excited about how this scales down. Can you run a community of a thousand or so posters on a micro or nano instance for a few bucks a month or less? At that scale and cost, donations should easily be able to cover hosting fees and you would surely be able to deputize enough mods to keep things civil (for whatever definition of civil your instance lands on). Ads, monetization, personalization are non-issues (well, not major issues) at that scale. reply 10000truths 14 hours agoparentprevThe point is that much of that should be unnecessary to sustain the service because hosting costs are significantly (presumably 100x) cheaper. reply hobofan 4 hours agorootparentThey never claimed that hosting costs would be 100x cheaper, and given what they show with amount of machines in the scaling chart, I doubt it is.It&#x27;s a JVM application with all state duplicated N times, so at least on the memory side it&#x27;s likely going to be a resource hog. reply stuaxo 13 hours agoprevLovely, I could see this paradigm spreading to other languages, something was definitely needed. reply crenwick 9 hours agoprevCongrats on the launch. Great read.Is there any rough infrastructure cost comparison?Excluding the cost of engineering effort, which I understand is the major pitch. reply raverbashing 15 hours agoprevThey deserve congrats for that since they built the load test to prove thisOf course, for actual production use, there&#x27;s probably a lot of things still, but this is a very nice works nonetheless reply nathanmarz 15 hours agoparentI wouldn&#x27;t call our instance a load test, as it&#x27;s a legitimate instance available for anyone to use. It&#x27;s very much production-grade. reply mdaniel 13 hours agorootparentI would not want to speak for raverbashing but I feel the same way: I actually can&#x27;t tell if the bug is with soapbox or with your instance but clicking on the first link from your post practically locks up my browser due to every single Toot getting swapped out \"at twitter scale\"If one clicks quick enough to jump to an actual post, it seems relatively static so it&#x27;s hard to tell if the bots are deleting and recreating their posts or what. In true Xitter clone fashion, trying to view the Posts & replies from any one user is \"sign inAnyway, all of this is not to detract from your framework announcement as much as to have you consider that it&#x27;s perfectly fine to label that instance as a load test, that&#x27;s a fine thing, but calling it a legitimate instance seems to be a potential source of confusion reply nathanmarz 12 hours agorootparentWe did notice on a less powerful machines the browser getting overwhelmed with the rate of new content (even though we&#x27;re only streaming 10&#x2F;s instead of the full 3.5k&#x2F;s actually happening on the backend). I don&#x27;t know if the poor performance in this context is due to Soapbox, the browser, or just the hardware.To get a better feeling of Rama&#x27;s performance on your hardware, I suggest registering an account which will allow you to poke around the whole platform. It takes just a couple seconds to register and we don&#x27;t send any emails. reply raverbashing 15 hours agorootparentprevThis is what I&#x27;m calling load test:> The instance has 100M bots posting 3,500 times per second at 403 average fanout to demonstrate its scale. reply freecodyx 9 hours agoprevLess code excluding dependencies reply boredumb 15 hours agoprevneat read but I was expecting to read about twitter migrating and literally 100x savings being had. reply dunk010 14 hours agoparentYou never know. reply rugina 10 hours agoprevCan you implement a web shop like Amazon using Rama? reply jauntywundrkind 15 hours agoprev> The instance has 100M bots posting 3,500 times per second at 403 average fanout to demonstrate its scale.Mastodon has to send messages to each instance with a recipient. That server can then fan out to all it&#x27;s subscribers. The way this point is worded makes me think all the bits are on just a single instance, meaning all the fan out can be dealt with internally without having to do any server-to-server at all.That is a fair comparison to Twitter, which is single instance. But it sounds like a much reduced ambition versus the task Mastodon has to do. reply nathanmarz 14 hours agoparentWe implemented federation fully exactly as you described. reply jonstewart 7 hours agoprevHow does Rama differ from Flink and timely-&#x2F;differential-dataflow&#x2F;Materialize?I see “microbatching” in the diagram and, maybe this isn’t a fair take, but it feels more 2013 than 2023. reply throwaway892238 9 hours agoprevAre people still thinking social media is a good thing? Hasn&#x27;t every study showed that it&#x27;s terrible for everybody? reply elisbce 8 hours agoprevThe real reason why we can&#x27;t easily replicate Twitter&#x2F;Facebook&#x2F;Google is because we don&#x27;t have the distributed storage&#x2F;caching&#x2F;logging&#x2F;data processing&#x2F;serving&#x2F;job scheduling&#x2F;... infrastructures that they have built internally that are designed to provide some level of guaranteed SLAs for the desired scale, performance, reliability and flexibility, not because it is hard to replicate the application logic like posting to timelines. That&#x27;s also why Threads were built by a small team rather quickly -- they already have the battle-tested infras that can scale.Any attempt to build a simplified version of the ecosystem will face the same fundamental distributed system tradeoffs like consistency&#x2F;reliability&#x2F;flexibility&#x2F;... For example, one of the simplifications may be mixing storage&#x2F;serving&#x2F;ETL workloads on the same node. And the consequence is that without certain level of performance isolation, it could impact the serving latency during expensive ETL workload.For Rama to be adopted successfully, I think it is important to identify areas where it has the most strengths, and low LOCs might not be the only thing that matters. For example, demonstrating why it is much better&#x2F;easier than setting up Kafka&#x2F;Spark and a database and build a Twitter clone on top of that while providing similar&#x2F;better performance&#x2F;reliability&#x2F;extensibility&#x2F;maintainability&#x2F;... is a much stronger argument. reply 70 more comments... GuidelinesFAQListsAPISecurityLega",
    "originSummary": [
      "Red Planet Labs achieved a 100x reduction in cost for building a Twitter-scale Mastodon instance using the Rama platform.",
      "The instance was built with only 10,000 lines of code, compared to Twitter's 1 million lines.",
      "Rama offers scalability, performance, and fault tolerance, simplifying backend development. The team plans to release Rama for public download and open-source their Mastodon implementation. PState in Mastodon is used to track data and improve efficiency. Optimization techniques are discussed, and the release of Rama documentation and API is mentioned."
    ],
    "commentSummary": [
      "The article focuses on Rama, a new backend framework that aims to simplify and reduce the cost of building scalable applications.",
      "Commenters discuss the comparisons made to Twitter and question the scalability of Rama.",
      "The discussions also delve into the challenges of decentralized platforms, the pros and cons of using Entity Component Systems in game development, and the importance of security and moderation."
    ],
    "points": 864,
    "commentCount": 320,
    "retryCount": 0,
    "time": 1692122053
  },
  {
    "id": 37134092,
    "title": "Firefox finally outperforming Google Chrome in SunSpider",
    "originLink": "https://www.phoronix.com/news/Firefox-Faster-SunSpider",
    "originBody": "ARTICLES & REVIEWS NEWS ARCHIVE FORUMS PREMIUM CONTACT CATEGORIES Firefox Finally Outperforming Google Chrome In SunSpider Written by Michael Larabel in Mozilla on 15 August 2023 at 09:53 AM EDT. 36 Comments Mozilla developers are celebrating that they are now faster than Google Chrome with the SunSpider JavaScript benchmark, although that test has been superseded by the JetStream benchmark. Last week a new Firefox Nightly News was published that outlines that \"We’re now apparently beating Chrome on the SunSpider JavaScript benchmark!\" The provided numbers now show Firefox easily beating Chrome in this decade-old JavaScript benchmark. The benchmarks come from AreWeFastYet.com. Meanwhile for the newer and more demanding JetStream 2.0 benchmark, Google Chrome continues to win easily over Firefox: Besides Firefox running the JavaScript SunSpider benchmark much faster over the roughly past month, there's been work on the HTTP/2 upload speed improvements, and various other enhancements. Learn about the latest Firefox Nightly build advancements via the Firefox Nightly News. 36 Comments Related News Firefox Lands Wayland Fractional Scaling Support Mozilla Firefox 116 Now Available - Capable Of Wayland-Only Builds Thunderbird 115 Now Available & It Looks Fantastic Firefox 115 Now Available With Intel GPU Video Decoding On Linux Mozilla Firefox 116 To Allow For Wayland-Only Builds Firefox 114 Available With WebTransport Enabled, Continued DNS Over HTTPS Work About The Author Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com. Popular News This Week Firefox Finally Outperforming Google Chrome In SunSpider Linus Torvalds Reviews The Bcachefs File-System Code Linux Decides To Disable RNG On All AMD fTPMs Linux 6.6 To Finish Gutting Wireless USB & UWB AMD \"INCEPTION\" CPU Vulnerability Disclosed Rust Abstractions Posted For Sockets & Other Fundamental Network Bits Intel Gets Hogwarts Legacy Running On Linux Driver By Pretending Not To Be Intel Graphics Haiku OS Support Upstreamed Into GCC Compiler Latest Linux News Firefox Lands Wayland Fractional Scaling Support Intel Acquisition Of Tower Semiconductor Falls Through Intel XeSS 1.2 Released - Xe Super Sampling Still Driven By Some Windows Binary Blobs Chrome 116 Released With Document Picture-In-Picture API Devuan 5.0 Released For Debian 12 Without systemd Firefox Finally Outperforming Google Chrome In SunSpider Raspberry Pi V3D Driver Gaining Per-Process GPU Usage Stats AMD P-State Preferred Core Patches For Linux Updated, Will Be Enabled By Default Glibc Git Lands Another FMA-Optimized Function - 24% Mean Improvement Intel Making Improvements For CPU Microcode Updating Under Linux Show Your Support, Go Premium Phoronix Premium allows ad-free access to the site, multi-page articles on a single page, and other features while supporting this site's continued operations. Latest Featured Articles Benchmarking The Performance Impact To AMD Inception Mitigations Initial Benchmarks Of The \"NVK\" Open-Source NVIDIA Vulkan Driver NVIDIA GeForce vs. AMD Radeon Linux Gaming Performance For August 2023 Initial Benchmarks Of The Intel Downfall Mitigation Performance Impact Intel DOWNFALL: New Vulnerability Affecting AVX2/AVX-512 With Big Performance Implications Support Phoronix The mission at Phoronix since 2004 has centered around enriching the Linux hardware experience. In addition to supporting our site through advertisements, you can help by subscribing to Phoronix Premium. You can also contribute to Phoronix through a PayPal tip or tip via Stripe. Phoronix Media Contact Michael Larabel OpenBenchmarking.org Phoronix Premium Support Phoronix While Having Ad-Free Browsing, Single-Page Article Viewing Share Facebook Twitter Legal Disclaimer, Privacy Policy, CookiesContact Copyright © 2004 - 2023 by Phoronix Media. All trademarks used are properties of their respective owners. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37134092",
    "commentBody": "Firefox finally outperforming Google Chrome in SunSpiderHacker NewspastloginFirefox finally outperforming Google Chrome in SunSpider (phoronix.com) 803 points by marcodiego 20 hours ago| hidepastfavorite366 comments titzer 19 hours agoSpeaking from the perspective of a (former) V8 engineer, SunSpider is crappy benchmark, and I can&#x27;t believe people still track it. It has misdirected JavaScript performance engineering work for going on 15 years now, leading to all kinds of weird contortions in JavaScript engines that do not benefit real world sites.Years ago, V8 switched away from microbenchmarks for the most part. See:https:&#x2F;&#x2F;benediktmeurer.de&#x2F;2016&#x2F;12&#x2F;16&#x2F;the-truth-about-traditi...edit to add: Don&#x27;t take the above to mean that Firefox&#x27;s performance improvements aren&#x27;t significant, and welcome. It&#x27;s just that SunSpider is the absolute worst benchmark to look at. reply IainIreland 17 hours agoparentTo be clear: we track it in the sense that we still have it running as part of old dashboards. It&#x27;s cheap, and every so often it catches an unintended regression.The SpiderMonkey team is very much on board with ignoring microbenchmarks. We care about Speedometer, because it correlates reasonably well with real-world performance. We pick and choose which parts of Jetstream2 we bother with. Anything older than that is strictly a measuring stick, rather than a target: it&#x27;s nice when the numbers go up, but it will only happen as a side-effect of work we&#x27;re doing for other reasons.Notably, all the SunSpider wins on this graph are side-effects of Speedometer work we&#x27;ve been doing this year. reply titzer 17 hours agorootparentI understand the logic, but I never understood why we (and I mean, collectively, the JS VM engineers across the browser vendors) didn&#x27;t just rewrite the benchmark (or fix it) and name it SunSpider 2. I suggested this many times. It&#x27;s not that much code. reply IainIreland 17 hours agorootparentYeah, I&#x27;ve been making this argument for years too.Speedometer 3 is in the works, and it&#x27;s moving in this direction. On multiple occasions we&#x27;ve done performance analysis of the latest draft version, noticed a particular chunk of code is hot, considered what it would take to optimize it, and then changed it because we don&#x27;t think it reflects real-world usage.EDIT: And I guess the answer for \"why not fix SunSpider specifically?\" is that once you&#x27;ve done the work to turn it into something worth optimizing, it no longer bears any resemblance to the original. It probably looks a lot more like Speedometer, honestly. reply titzer 16 hours agorootparent> EDIT: And I guess the answer for \"why not fix SunSpider specifically?\" is that once you&#x27;ve done the work to turn it into something worth optimizing, it no longer bears any resemblance to the original. It probably looks a lot more like Speedometer, honestly.I get that, yeah, but superseding the old name is probably worth it though; the internet is full of people who never get the memo except via version numbers :)I tongue-in-cheek proposed that we match the Sunspider ASTs in V8 and either make it infinitely fast or infinitely slow. Now that will kill a benchmark once and for all! reply IainIreland 15 hours agorootparentJSOP_SUNSPIDER was apparently a running joke back in the day inside SM. reply londons_explore 17 hours agorootparentprevPresumably because replicating the mix of operations used in real react&#x2F;jquery code all over the web accurately isn&#x27;t trivial... reply hiimkeks 18 hours agoparentprevThe link you posted promotes Speedometer, where Firefox reportedly already surpassed Chrome.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36770883 reply MegaDeKay 18 hours agoparentprevFirefox tracks their performance across many different benchmarks. Here is the performance vs Chrome over the past year on many different ones. It is encouraging to see steady improvements being made on a substantial number of them over time.https:&#x2F;&#x2F;arewefastyet.com&#x2F;win10&#x2F;benchmarks&#x2F;overview?numDays=3... reply yborg 16 hours agorootparentIt appears that Chrome still wins most of these. Even the touted Speedometer win has apparently been trumped by Chrome over the last couple of weeks. The competition doesn&#x27;t stand still. Which is good, imagine if Firefox threw in the towel, Chrome would become IE 6 in short order. reply nwienert 16 hours agorootparentSafari is faster than either across almost all dimensions, lighter on battery, nicer UI, better standards support generally than FF and even matches chrome for what I care about. It’s the clear best browser imo. reply mtlmtlmtlmtl 15 hours agorootparentSafari is only well maintained on MacOS and iOS. That makes it a non-starter for most people. reply Retric 15 hours agorootparentMore than half of US users run iOS or MacOS on one or more of their devices. reply mtlmtlmtlmtl 14 hours agorootparentLess than 5% of people live in the US. Not sure how your statement is relevant to my statement about most people in general. reply Retric 12 hours agorootparentWhich is irrelevant if 95+% of your users are coming from the US…Unless you’re equally interested in non English speaking users then the global average is absolutely meaningless.What matters to organizations is some subset of everyone and what their specific access looks like. reply mtlmtlmtlmtl 11 hours agorootparentWe&#x27;re talking about web browsers. Pretty much everyone with a personal computer(including smartphones) needs one. The US makes no sense as a subset here. If we were discussing what markets for a company to target, then sure. But we&#x27;re not, are we? reply Retric 10 hours agorootparentI and many people here abandoned Chrome, so it’s only relevant to me from a software development perspective.In terms of software development a US specific audience isn’t uncommon. Local utilities and many government agencies etc just don’t care about foreign users. reply pietro72ohboy 4 hours agorootparentprev> Unless you’re equally interested in non English speaking users then the global average is absolutely meaningless.The US is not even the largest English speaking population, it’s India. Additionally, if a subset is to be chosen, why is this subset from the US? Why does it get to be the centre of the universe, especially for a general-use product like a web browser? reply wongarsu 14 hours agorootparentprevThe US is hardly representative. Globally, 25.3% of website visits happen from a device running iOS or MacOS. That&#x27;s impressive, but still a mere quarter. Windows (29%) and Android (40%) each account for more users.https:&#x2F;&#x2F;gs.statcounter.com&#x2F;os-market-share reply fomine3 4 hours agorootparentStatscounter is a popular statistics for platforma usage, but I strongly doubt that it is heavily biased. I&#x27;ve never found that any websites use statcounter (though I haven&#x27;t aggressively investigated), and never found installation guide in Japanese. Perhaps is userbase biased to smaller websites on some countries? reply Retric 12 hours agorootparentprevWhich isn’t actually relevant to any specific website as they all favor either some subset of users and generally a subset of their devices. Ie: Fewer than normal stack overflow users are coming in on cellphones let alone PlayStations.You need to know your target audience, not simply lookup overall statistics. I’ve seen Windows make up less than 4% of all users and I’ve seen it be over 90%. replypmontra 12 hours agorootparentprevI can run Chrome and Firefox on my Debian laptop and on my Android devices. How do I benchmark Safari there? I can&#x27;t, so Safari is barely relevant to me. Nice to know that it&#x27;s faster than the other two browsers on Macs, but I&#x27;m not very surprised. Apple should be able to integrate it with their hardware and OS more than Google and Mozilla. reply isaacremuant 12 hours agorootparentprevCustomization and freedom means Firefox is always the best for me.How&#x27;s the safari world of add-on&#x27;s? Do you have tab containers? What about mobile? Can you block ads? reply NicoJuicy 16 hours agorootparentprevOn Mac? Probably because only Safari has access to the private api&#x27;s reply zuhsetaqi 16 hours agorootparentJust compare the results from Windows and macOS. reply scrum-treats 16 hours agorootparentprevSee also, significant regressions in Firefox&#x27;s memory consumption: https:&#x2F;&#x2F;arewefastyet.com&#x2F;mac&#x2F;memory&#x2F;overview?numDays=365 reply elcritch 16 hours agorootparentIn general I’ve preferred Firefox’s lower memory usage than pure speed. reply Waterluvian 13 hours agoparentprevThis isn’t an agreement or disagreement:Having benchmarks based on real-world sites seems like both a good and a bad idea.Good for obvious reasons. Bad because it creates a feedback loop. People develop based on performance. Performance is improved based on what people develop.Did you&#x2F;your colleagues tackle this in some way? reply titzer 11 hours agorootparentHere&#x27;s one example from 2016: https:&#x2F;&#x2F;v8.dev&#x2F;blog&#x2F;real-world-performance reply tgv 18 hours agoparentprevI agree, but speaking as someone who&#x27;s been using JS for computationally medium-intensive tasks: Firefox and Chrome don&#x27;t differ that much. For most users, the difference is negligible. reply mhh__ 11 hours agoparentprevAs the saying goes any compiler engineer can make their compiler the fastest if they know what the code is. reply olliej 12 hours agoparentprevIn fairness, when sun spider was first released it did accurately reflect the core pain points in JS engines, as present on the web. That applied to V8 as well (though V8 introduced its own benchmark suite as well on release, it was overly focused on property access, and insufficiently on the parts of initial page load that sun spider just happened to cover).Of course all of that was _15 years ago_, no benchmark from back then is really reflective of anything useful, and they are largely no longer valuable for anything other than catching regressions. reply liendolucas 14 hours agoprevI only run Firefox on every computer and phone that I own. I&#x27;m extremely satisfied with its performance so far and sincerely hope they keep improving. My second alternative is Brave, just in case I can&#x27;t use Firefox. My very last choice is Chromium. On Firefox I highly recommend these plugins:AdNauseam: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;adnauseam&#x2F;Consent-O-Matic: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;consent-o-mat...Firefox Multi-Account containers: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;multi-account...I don&#x27;t care about cookies: https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;i-dont-care-a...Edit: As mentioned below, avoid Consent-O-Matic unless you trust Avast. reply takoid 14 hours agoparentJust so you know, \"I don&#x27;t care about cookies\" was sold to Avast last year:https:&#x2F;&#x2F;www.theregister.com&#x2F;2022&#x2F;09&#x2F;21&#x2F;avast_buys_i_dont_car...There is a fork called \"I still don&#x27;t care about cookies\" which is probably worth switching to unless you trust Avast:https:&#x2F;&#x2F;github.com&#x2F;OhMyGuus&#x2F;I-Still-Dont-Care-About-Cookies reply liendolucas 14 hours agorootparentThanks for that and I wasn&#x27;t aware it&#x27;s owned by Avast. I&#x27;ll switch to the one that you suggested! reply kristofferR 13 hours agorootparentYour edit makes no sense though, &#x27;I don&#x27;t care about cookies&#x27; is the tainted one, not Consent-O-Matic. reply liendolucas 12 hours agorootparentOh yes, I made a mistake it&#x27;s the one that you mentioned. I can&#x27;t edit the post anymore :( reply RGBCube 1 hour agorootparent@dang, could you fix it? replyvorticalbox 19 hours agoprev> Firefox easily beating Chrome in this decade-old JavaScript benchmarkwhile also> Meanwhile for the newer and more demanding JetStream 2.0 benchmark, Google Chrome continues to win easily over Firefox reply yborg 16 hours agoparentAnd on macOS, Safari beats the pants off of Chrome by an even larger margin. reply baliex 18 hours agoparentprevBig firefox fan here and it&#x27;s kinda sad to read that reply agumonkey 18 hours agorootparentI wouldn&#x27;t, firefox has less resources and faced an uphill battle for years. The fact that they keep improving regularly and significantly, even though late on some front is applaud worthy to me. reply insanitybit 17 hours agorootparentCan someone tell me how much Chrome has in terms of resources? Firefox, a company with one main product, has hundreds of millions of dollars coming in + millions in donations. That should pay for a sizeable engineering team.Is the Chrome engineering team much larger? Is the V8 team larger than the SpiderMonkey team, significantly so? reply londons_explore 17 hours agorootparentDrawing a precise boundary around \"Chrome\" is hard, since many people work on it part time, or are associated with tangentially related projects, like the skia graphics engine, v8 javascript engine, etc.However, there were 767 unique authors who committed code direct to chromium in the last week:https:&#x2F;&#x2F;github.com&#x2F;chromium&#x2F;chromium&#x2F;pulseAnd I would guess the Chrome team is probably 3-5x that, since many people may not be working on code, may not commit directly, or might be working on big things on branches and didn&#x27;t commit anything this week. Also, that doesn&#x27;t count work on external dependencies, which are counted as \"autoroll bots\". reply insanitybit 15 hours agorootparentI don&#x27;t really think it&#x27;s fair to extrapolate github contributors into FTE employees paid by Google, let alone to throw out a &#x27;3-5x&#x27; number on top. reply londons_explore 1 hour agorootparentManual inspection of todays commits shows that nearly all of them are @chromium.org email addresses, which usually means a google full time engineer. There are some from other companies (microsoft, intel, nvidia). None from random hobbyists that I saw. reply londons_explore 17 hours agorootparentprev3000 engineers * $200k average total comp = $600M. It&#x27;s in the same ballpark as mozilla&#x27;s total revenues. reply smarnach 16 hours agorootparentAverage total comp for an FAANG engineer in the US is more like $450k, and last time I heard numbers, Chrome and adjacent teams were more like 4,000 people. reply londons_explore 1 hour agorootparentI guess Mozilla engineers are paid quite a bit less. They hire the people with a conscience, so don&#x27;t need to pay the &#x27;do evil&#x27; salary premium... reply astrange 14 hours agorootparentprevWhich average is that? Don&#x27;t think it&#x27;s median W2 income. reply tick_tock_tick 13 hours agorootparentYeah it seems a touch low. I&#x27;m not at one of the big boys and $200k is more like base salary then total comp. RSUs show up on a W2. reply lossolo 16 hours agorootparentprev> However, there were 767 unique authors who committed code direct to chromium in the last week:This also includes all people from outside the org (Google), right?I found this but I think it&#x27;s for Chrome? There are also things like v8 engine.> The team consists of around 40 engineers, in addition to a number of PMs, test engineers, UX designers, researchers, and others. We built a lot of features in Chrome that are used by more than a billion people.) Feb 22, 2019 reply tomashubelbauer 15 hours agorootparentprevUnfortunately the money for within Mozilla is not that straightforward in favor of Firefox and I imagine I am not the only Firefox user who is incredibly annoyed with that. I&#x27;d rather send money to individual devs&#x27; Github Sponsors sounds than give Mozilla a donation the devs will never see a cent of. reply tristan957 17 hours agorootparentprevDonations don&#x27;t pay for Firefox development. Chromium is a bit more than Google these days. Mozilla&#x27;s largest failure to date has been their inability to attract other significant corporate interest, which Google has done with Chromium reply fabrice_d 18 hours agorootparentprevfwiw, https:&#x2F;&#x2F;arewefastyet.com&#x2F;win10&#x2F;benchmarks&#x2F;overview?numDays=6... tracks a number of benchmarks. reply dblohm7 17 hours agorootparentprevOptimizing for benchmarks tends to not work all that well for real-world sites. I wouldn&#x27;t worry about it. reply alexisbear 19 hours agoprevWorth noting that SunSpider hasn&#x27;t been updated since 2013. JetStream is more representative of modern JavaScript workloads, but the gap is also getting closer on that benchmark. reply p1mrx 11 hours agoprevGiven all the recent chatter about Chrome turning evil, I spent a few days polishing my Chrome extension (IPvFoo, for observing the IPv6 transition) to run properly on Firefox. Technically it&#x27;s been available since 2017, but there were numerous compatibility bugs that I was hoping Firefox would fix eventually. I finally decided to just hack around the remaining ones, so Chrome&#x2F;Firefox are now at feature parity from a common codebase:https:&#x2F;&#x2F;github.com&#x2F;pmarks-net&#x2F;ipvfoo reply mmastrac 19 hours agoprevI&#x27;m working with the v8 engine and it&#x27;s a minefield of de-optimizations and random slowdowns for things that you would not expect to be slow.Exception handler structures can dramatically change performance characteristics of code. async&#x2F;await is a real crapshoot -- sometimes promises are faster, sometimes async blocks are.We can blame the poor specs for some slowdowns (why does `...` destructuring use iterators??), but I wonder if the v8 team is chasing benchmarks at the expense of real-world code that might not be so perfectly written. reply jbreckmckye 19 hours agoparent> why does `...` use iterators??If you couldn&#x27;t use the iterator protocol, generators in particular would be a lot less powerful reply mmastrac 19 hours agorootparentI think the JS spec would have been better served with a different syntax for generator&#x2F;iterator destructuring. Most code I&#x27;ve seen assumes that: [a, b, ...rest] = arrayis implemented like this: a = array[0] b = array[1] rest = array[2...]When instead it is a giant performance footgun to the point where it should only be used outside of performance critical code.EDIT \"next -> rest\" reply nightpool 18 hours agorootparentIs it not able to be specialized like that when the object is a (non-sparse) array? What is preventing engines from making that optimization? reply IainIreland 17 hours agorootparentIt&#x27;s possible. We have plans to do so. There are a bunch of finicky corner cases, though. For example, destructuring will call the `return` method of the iterator when it&#x27;s done (https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;JavaScript&#x2F;Refe...), so we have to be careful if somebody has defined `return` on the Array iterator prototype. You can supply a default value when destructuring, which is lazily evaluated, so we have to be careful about arbitrary side-effects taking place in the middle of the destructuring. We need to be able to fall back to something less optimized if we suddenly see a generator, or a user-written iterator. And so on.Fundamentally, it&#x27;s the same thing that impedes most optimizations: there are hundreds of different patterns we could optimize, it takes a fair bit of thought to get it right, and we have finite resources, so we have to pick and choose. This particular pattern turns out to be idiomatic in a lot of React code, which is why we&#x27;re looking at it now. (Or so people tell me; most JS I write is gross little testcases.) reply zamadatix 14 hours agorootparentThanks for taking the time to drop these nuggets throughout the thread. reply postalrat 18 hours agorootparentprevI don&#x27;t assume it&#x27;s implemented like \"next = array[2...]\". I don&#x27;t even know what that means. reply mmastrac 18 hours agorootparentSorry, I assumed that most people could understand the pseudocode syntax for a subarray operation. In Javascript, this is:array.slice(2) reply depressedpanda 16 hours agorootparentI understood it, without even thinking about it. In my opinion it&#x27;s more elegant than the real syntax, and I didn&#x27;t even realize it wasn&#x27;t real JavaScript until this comment thread made me look again. replyinsanitybit 18 hours agoparentprev> but I wonder if the v8 team is chasing benchmarks at the expense of real-world code that might not be so perfectly written.They aren&#x27;t. V8&#x27;s benchmark performance for this benchmark has been stagnant because they haven&#x27;t cared about it for a long time. Firefox has an incentive to care about it (unfortunately) because even when a benchmark doesn&#x27;t matter it still looks bad to be behind on it. reply IainIreland 16 hours agorootparentTo be clear, Firefox doesn&#x27;t care about this benchmark either. It just so happens that changes we made for Speedometer also improved our performance here, which is a fun coincidence but not important in the grand scheme of things.(Speedometer is the benchmark that most closely approximates messy real-world code.) reply insanitybit 14 hours agorootparentThanks for clarifying, that&#x27;s interesting + good to know. reply lossolo 16 hours agoparentprev> but I wonder if the v8 team is chasing benchmarks at the expense of real-world code that might not be so perfectly written.Isn&#x27;t it the other way around? https:&#x2F;&#x2F;v8.dev&#x2F;blog&#x2F;real-world-performanceI recall there was some newer blogpost discussing this topic with numbers and examples but can&#x27;t find it atm, maybe it was chromium&#x2F;chrome team blog, maybe someone else can find it. reply mezobeli 18 hours agoprevFirefox with uBO is more than 100% faster than Chromium based browsers. uBO doesn&#x27;t properly work on Chromium based browsers due to their limitations reply ur-whale 13 hours agoparent>Firefox with uBO is more than 100% faster than Chromium based browsers. uBO doesn&#x27;t properly work on Chromium based browsers due to their limitationsTHIS. reply mihaic 16 hours agoprevHonestly, for me Firefox is a second choice until their debugging tools match Chrome&#x27;s and they tighten up their tab UI just a bit.I really want use firefox daily it&#x27;s just that I do feel a development speed hit when using it. reply jeroenhd 14 hours agoparentInteresting, I find Firefox&#x27;s devs tools a lot easier to work with.Firefox&#x27;s design doesn&#x27;t sit right with me, but there are themes and addons to fix that. reply hooch 2 hours agorootparentAgreed. The dev tools in Firefox are beautifully designed and a joy to use. Chrome is a dogs breakfast in comparison. reply mihaic 13 hours agorootparentprevWhat themes and addons do you use? I never changed more than the stock color scheme. reply jeroenhd 10 hours agorootparentGNOME theme for system integration, and the most important window altering changes I&#x27;ve made is to install Tree Style Tabs and hiding the top bar through userChrome.css: @namespace url(\"http:&#x2F;&#x2F;www.mozilla.org&#x2F;keymaster&#x2F;gatekeeper&#x2F;there.is.only.xul\"); &#x2F;* to hide the native tabs *&#x2F; #TabsToolbar { visibility: collapse; } &#x2F;* to hide the sidebar header *&#x2F; #sidebar-header { visibility: collapse; } reply hruzgar 12 hours agoparentprevSame, the UI needs a bit of change. It doesn&#x27;t need to be anything fancy. Just something simple like basic chromium would be nice reply majestic5762 15 hours agoparentprevSame for me reply freediver 16 hours agoprevThe gain is real. Just tested with latest Speedometer 3.0 [1] (unreleased, but open source). Latest versions of the browser were used.Firefox Nightly (Gecko) 11.6Safari TP (WebKit) 11.4Chrome Canary (Blink) 9.9[1] https:&#x2F;&#x2F;github.com&#x2F;WebKit&#x2F;Speedometer reply flas9sd 18 hours agoprevhaving recently looked at a firefox performance regression for filing a bugreport, tooling that tracks performance (quite publicly) sees attention, easy upload to share tracing profiles also helps: https:&#x2F;&#x2F;firefox-source-docs.mozilla.org&#x2F;testing&#x2F;perfdocs&#x2F;ind...Their dedicated blog keeps you posted if firefox perf is your interest https:&#x2F;&#x2F;blog.mozilla.org&#x2F;performance&#x2F;If you have a particular website you notice chromium being significantly faster with, for an easy report, there&#x27;s https:&#x2F;&#x2F;webcompat.com&#x2F; - though bugzilla is better than it seems when coming from github issues reply duringmath 18 hours agoprevBragging about your sunspider score is like bragging about how many myspace followers you have.Besides I&#x27;m not sure how much these benchmark tests are relevant to real world performance anyway. reply bee_rider 18 hours agoparentIt doesn’t look like bragging to me. The only comment Phronix has from a Firefox person is:> We’re now apparently beating Chrome on the SunSpider JavaScript benchmark!“Apparently” seems to indicate that it isn’t something they are putting a ton of stock in. reply duringmath 18 hours agorootparentI think that&#x27;s what you&#x27;d call a \"humble brag\". reply chippiewill 17 hours agorootparentIt would only be a humble brag if it was something that the Firefox developers thought others would be impressed by. It&#x27;s a single line item on their internal dev blog, the bar for entry is not exactly high. reply ErneX 12 hours agoprevUnrelated to this benchmark but Chrome vs Firefox on the same Mac has significantly worse scrolling performance on pages with a reasonable amount of images. Or at least that’s what I have observed.In Chrome while you scroll you get these moments of empty white space while it is probably loading the content in and out. On Firefox I don’t get these empty white chunks at all it is just a smooth content scroll all the way.To me this is more jarring than being faster in JS. reply butz 16 hours agoprevAre there any \"real life\" benchmarks, e.g. comparing how long each browser takes for a typical popup, banner and autoplay video laden website to load and finally become responsive for user? And of course this should be a moving target, as website sizes just keep increasing. reply FractalHQ 18 hours agoprevAs much as I hate monopolies, I hate having to write apps for 5 platforms at once even more. If my app works on Chrome, Brave, Vivaldi, Android, and Edge, and I have to jump through hoops and litter the code with if (isSafari) workarounds to get it working on iPhone, the last thing I want to do with the remainder of my time is start from scratch to fix all of the bugs or missing apis so Firefox can run it for 1% of users.I understand the risk of a chrome monopoly, but boy do I feel a the conflict of interest when these conversations come up… reply no_way 14 hours agoparentFrom my experience, if things work in Chrome, they will likely work with Firefox too, excluding very new apis, it isn&#x27;t much of maitenance burden. With Safari every project I work o I discover new Safari quirk or bug. If you can make it work in Safari you can make it Firefox too. reply makeitdouble 17 hours agoparentprev1% is an interesting number [0]Instinctively it feels small, but in practice increasing your CVR on your registration page by 1% is usually a big deal. What kind of other costs in ads or marketing would you pay to get a 1% increase to your user base ?For most company I worked for, a bug affecting 1% of the users would be a critical show stopper.[0] Setting aside if you really have 1% in firefox vs more % in Brave or Vivaldi, reply amatecha 17 hours agoparentprevwhy checking for specific browser rather than using feature detection to properly handle different browser capabilities? ( Good docs at https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Learn&#x2F;Tools_and_tes... ) reply mattnewton 17 hours agorootparentFor safari, I used to run into a lot of graphical bugs, (especially when pushing things into layers on the gpu with transforms). The second big category would be things that are performant on low end android phones with chrome are not performant on older iPhones and vice versa. reply no_way 14 hours agorootparentSemi related, I was suprised just how many bugs Safari has with animation timing while using Web Animation API. You can&#x27;t possibly feature detect that, Motion one lib even disables GPU acceleration for them in Safari. In that case parsing user agent is the only option.https:&#x2F;&#x2F;motion.dev&#x2F;guides&#x2F;waapi-improvements#webkit-bugfixes reply hinkley 19 hours agoprevI knew NodeJS and Chrome were in for darker times when v8.dev discontinued their engineering blog. That spoke to me of engineering losing steam. Either will or manpower or both.Congratulations to Firefox of course. The hare is napping under a tree and you just keep trucking. reply jbreckmckye 19 hours agoparentYou know. That actually highlights an understated risk of the Chromium monopoly.What happens if Chrome... doesn&#x27;t become evil, it&#x27;s just mismanaged? What happens if the team loses personnel and gains too much tech debt? What happens if Chromium is run into the ground just by happenstance? I don&#x27;t know, maybe they give it to a product manager, or they try Scrum or something.It&#x27;s a lot of influence to put in the hands of a single fallible team. reply VancouverMan 19 hours agorootparentAs we&#x27;ve seen happen before with other formerly-popular browsers like Mosaic, Netscape Navigator, Internet Explorer, and Firefox, the users just move on to whichever other browser works better for them at the time. reply Eji1700 17 hours agorootparentWhich was back when you had a bunch of options. If firefox goes under and chromium throws up, it&#x27;s not like there&#x27;s a large field of choices. reply zamadatix 13 hours agorootparent\"What if nobody wanted to continue developing browsers\" is an unrelated issue. If Firefox goes under, Chromium throws up, and nobody else thinks it&#x27;s worthwhile to do anything with the codebases or make a new separate one then the problem is nobody wants to build a browser not how many wanted to build a browser in the past. reply hinkley 19 hours agorootparentprevInternet Explorer was a lot of wandering in the desert. Let’s not repeat IE 6 mkay? reply pjmlp 18 hours agorootparentWe&#x27;re already there, ironically mostly pushed by the anti-IE generation.If it wasn&#x27;t for the special Safari status on Apple devices, it would be total domination, Web == ChromeOS. reply ebilgenius 18 hours agorootparentprevI&#x27;m seeing this already with how they&#x27;re handling the new right-click contextual Search feature being moved to open the results in the new \"Search sidebar\" rather than a new tab. This was supposedly able to be disabled with the flag \"Search web in side panel\" (there&#x27;s no option for it in the preferences), however if the completely undocumented \"CSC\" flag was left at it&#x27;s default then it overrides any other flag to force enable the feature. You can see the consequences of this decision to this very day in places like &#x2F;r&#x2F;chrome with users complaining that disabling the feature flags isn&#x27;t working. reply kibwen 18 hours agorootparentprev> What happens if Chrome... doesn&#x27;t become evil, it&#x27;s just mismanaged?When executed by an entity with enormous power, incompetence is indistinguishable from malice. reply surajrmal 19 hours agorootparentprevIs chromium hostile towards non Google contributions? It&#x27;s not clear to me why a community of many parties couldn&#x27;t form and prevent this possibility. From what I can tell, the problem is largely a lack of companies who want to participate in chromium development. reply mrguyorama 18 hours agorootparentIf the chromium community are unable to write their own browsing engine, then they are shackled to whatever google implements. When WEI turns out to be so intertwined with deep functionality that you can&#x27;t really excise it, what are these so called \"alternatives\" going to do, just not update the chromium codebase ever? reply fabrice_d 18 hours agorootparentprevThis article from Igalia describe pretty well the situation around contribution to gecko&#x2F;webkit&#x2F;blink : https:&#x2F;&#x2F;bkardell.com&#x2F;blog&#x2F;2023-Mid-Season-Power-Rankings.htm...Very interesting read! reply The_Colonel 17 hours agorootparentprevThat depends if you contribution is conforming with google&#x27;s strategy &#x2F; vision. Very likely google is going to be hostile to the future contribution to re-enable extension manifest v2. reply uluyol 11 hours agoparentprevWhat did they discontinue? https:&#x2F;&#x2F;v8.dev&#x2F;blog&#x2F;speeding-up-v8-heap-snapshots was posted less than a month ago. reply tick_tock_tick 13 hours agoparentprevI feel like you are reading way too much into this. You can look up thread and see both a SpiderMonkey and V8 dev&#x2F;xdev agree SunSpider is basically worthless to almost harmful to benchmark against and that V8 continues to absolutely dominate SpiderMonkey overall. reply makeitdouble 16 hours agoprevI got back to firefox 3 or 4 years ago on mac and it&#x27;s truely excelent.The only chokepoint were occasional Google properties, the same way Safari chokes on them, so I ended up compartimenting them in Chrome. Otherwise it has been pleasant to use.I also use it on android, but surprisingly it was really hard to use on Windows tablets. It performs well enough as a mouse and keyboard browser, but touch support is too accurate and mechanical to be useful.An example: on HN the upvote arrow is a really small target and clicking everywhere else should result in either nothing or opening the link next to the arrow. Chrome \"fuzzies\" the click area to include the upvote arrow even if I click completely on the left of it. Firefox will accurately send the click to text part left of the arrow, making it crazy hard to hit the upvote button.Firefox is technically accurate, but the extra \"magic\" on Chrome&#x27;s side is really helpful as a user. Performance wise I think it&#x27;s already on the right track, from there I hope we can get UX improvements as well. Touch optimizations on the desktop feels like a hard nut to crack, but it would be so nice. reply WorldMaker 15 hours agoparentFirefox has some about:config variables to tweak the touch support. The out of box behaviors should definitely be improved, and maybe even a real settings panel would be nice instead of just about:config hidden things. reply makeitdouble 9 hours agorootparentThanks! I actually tried it a while ago but completely forgot about it.The touch radius can be expanded and adjusted but the behavior really comes down to \"should it let the user set a caret at any text position\", and fuzzing the touch radius will still let the user hit a plain text target and select it, where Chrome (and mobile firefox, coms to think of it) won&#x27;t let you.For instance on HN&#x27;s comment link bar (\"WorldMaker 5 hours agoparentcontextflagon: Firefox finally outperforming Google Chrome in S...\") I can set a text selection carret on the \"|\" between \"parent\" and \"context\" in firefox with a touch radius of 32 mm, largely touching both \"parent\" and \"context\" links.I am actually getting motivated into looking deeper into this and see what they did to have the mobile version work differently. reply kwanbix 18 hours agoprevI used to love Firefox, and I even was part of the organization team that make Asas Dotler&#x27;s presentation in Argentina. I have pictures with him, an original Firefox bag, and an actual Firefox push. I also put money for the NY times add.But Google crushed them with the rapid releases. I remember that to realease firefox 3.0 (from 2.0), it took them like 2 years. Crazy slow.Today is much faster and is really nice, but I don&#x27;t like the UI, i really prefer the one by chrome, it is much slim. reply KORraN 17 hours agoparent> But Google crushed them with the rapid releases. I remember that to realease firefox 3.0 (from 2.0), it took them like 2 years. Crazy slow.Back in the days, increasing major version number meant something. What are the major changes between Chrome 114 and 115? For some reason, Mozilla decided to take part in this race that Google has started. reply kwanbix 1 hour agorootparentBut that is the problem. I rather see progressive improvements each 3&#x2F;6 months, then a big-bang change each 2 years. reply tgv 18 hours agoparentprevI like FF&#x27;s UI better than Chrome&#x27;s, but the small differences can be overcome quickly once you start using it. There is much more at stake than a few pixels here and a button there: do you want Google to control \"the internet\"? reply ur-whale 13 hours agorootparent> do you want Google to control \"the internet\"?Also: do you care at all about Google knowing every single thing you do on the internet? reply lovasoa 18 hours agoprevThat is great news. I hope Mozilla&#x27;s PR team will use this as a chance to bust the old myth that firefox is slow. We need more firefox users if we don&#x27;t want to end up in a world where the web is Google&#x27;s thing. reply hpb42 19 hours agoprevThe article links to the benchmarks in https:&#x2F;&#x2F;arewefastyet.com&#x2F;linux64&#x2F;benchmarks&#x2F;overview?numDays...Some benchmarks evaluate \"execution time\", so the lower the better.Some benchmarks have a \"score\" instead, that the lower the higher in the chart. I&#x27;m not familiar with these benchmarks and don&#x27;t know how to interpret it. Is higher score worse or better? reply alecmg 18 hours agoparentI think that site opted for consistent graphs - points lower on the graph is faster&#x2F;better, indicated by blue down arrow reply snet0 18 hours agoparentprevI&#x27;d also appreciate some kind of notes about what these tests actually represent. What&#x27;s \"Assorted DOM\", and why is Firefox so much better (if the scale is representative of anything useful)? reply ozim 13 hours agoprevI am using Firefox on principle - I don’t care about some benchmarks or that Firefox is only browser that still has connecting to local host so owning developers somewhere on backlog.But at work there is no real way I could use Firefox and I am web dev. Dev tools in Firefox are nowadays also behind what Chrome has. Biggest upside for Firefox was Firebug but now it is only history. Let alone mentioned security issue….I think here it is easy to understand that if all web devs work in chrome we have IE situation back again. reply paulryanrogers 10 hours agoparentDepends on what you need from dev tools. Firefox&#x27;s built in dev tools keeps some response bodies that Chrome&#x27;s tools don&#x27;t. reply WeylandYutani 9 hours agoparentprevGoogle owns internet advertising so I&#x27;m not surprised that they also own web devs. reply monooso 17 hours agoprevI can&#x27;t say I&#x27;ve ever considered switching browsers because of concerns about JS performance.I&#x27;m pretty confident this is the case for most \"average\" users, but I&#x27;m curious whether it makes me an outlier on HN.Anyone else share my indifference? reply putlake 19 hours agoprevI don&#x27;t trust anyone other than Google with my passwords. Google password manager is only available on Chrome. So unfortunately I&#x27;m locked into Chrome. I do like Edge and Firefox but the passwords are an issue. reply est31 18 hours agoparentThen you are one Google account compromise away from disaster: https:&#x2F;&#x2F;www.pcmag.com&#x2F;opinions&#x2F;warning-dont-let-google-manag...Google password manager is not end to end encrypted.LastPass, Bitwarden, and Firefox too, are end to end encrypted with the user chosen password at least. If your password is strong enough, it should be safe.Apple also uses end to end encryption, although there you might also be vulnerable via a hack of your iCloud account, not sure.1Password is the most secure option: here, in addition to your password being encrypted with your master password, it&#x27;s also encrypted via a secret key that is pre-generated. So a leaked encrypted 1Password backup won&#x27;t be brute-forceable. reply sp332 18 hours agorootparentYou can enable \"on-device encryption\" for Google password manager. https:&#x2F;&#x2F;support.google.com&#x2F;accounts&#x2F;answer&#x2F;11350823?sjid=178... reply Grazester 14 hours agorootparentprevLastPass should not even be mentioned. reply jusoren 18 hours agoparentprevWhat happen if google itself is the adversary in the safety of your passwords? That spur of the moment trollish YouTube comment might get flagged, bringing down your google account and all services tied with it; gmail, gdrive, your saved passwords etc. Then trying to rectify the situation is close to impossible since theres no human support for (free) google account.https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2022&#x2F;aug&#x2F;22&#x2F;google-cs... reply cyberlurker 18 hours agoparentprevYou trust Google with your passwords? And you trust them more than 1Password, Apple, etc.?I’d love to hear more about that statement. reply andersa 18 hours agorootparentIs the Apple password manager available on non-Apple devices?It could be argued that Google has significantly more resources to invest in security of the password storage than these smaller companies. We&#x27;ve recently seen LastPass implode, but I&#x27;ve never heard of any successful attack on Google password manager.Still prefer to keep them offline though. reply ComputerGuru 17 hours agorootparent> Is the Apple password manager available on non-Apple devices?As of last month or so, it is now available on Windows as well. reply bee_rider 18 hours agorootparentprevI don’t really get how people keep falling for online password managers. The only thing I can think is there must be some psychological effect: “well, this person is suggesting something that I know to be very dumb. So, I guess they must know something I don’t, better trust them!” reply tristan957 17 hours agorootparentOr we just appreciate convenience. I trust Bitwarden to operate a great secure service. reply tiltowait 17 hours agorootparentprevIt&#x27;s available on Windows. Not sure about Linux&#x2F;Android. reply Propelloni 16 hours agoparentprevOf course this is only my opinion, but I&#x27;d rather trust an open-source password manager such as KeePassXC. They all have browser integration (native or plug-in) if this is important. reply ur-whale 13 hours agoparentprev> I don&#x27;t trust anyone other than Google with my passwords.At this stage in their corporate life-cycle, where greed and complete and utter lack of care towards user happiness reigns supreme, trusting google with anything is a rather large mistake IMO. reply nottorp 18 hours agoprevBut how is which browser is faster relevant?Can you set up a google container in Chrome? :)And even if there is such an extension, can you trust the browser not to send everything you do to Google behind your back? reply ape4 19 hours agoprevCongrats Firefox team! No mention of Rust - but I imagine it helped. reply toyg 18 hours agoparentI wouldn&#x27;t think so. It&#x27;s not like Rust in FF replaces slow Python or Java code, it replaces heavily-optimized C++. If anything, Rust would likely impede speedup efforts (even if it might improve overall code quality and safety).Also, I believe the project to rewrite the entirety of Gecko in Rust has long been abandoned. reply GuB-42 18 hours agoparentprevHow much of SpiderMonkey (JS engine) is Rust? AFAIK, besides the bindings, it is still mostly C++ and my guess is that it would be difficult to get the core parts in Rust without a full rewrite. reply tristan957 17 hours agorootparentChromium is fairly invested in cxx[0]. Firefox could do the same if they haven&#x27;t already.[0]: https:&#x2F;&#x2F;github.com&#x2F;dtolnay&#x2F;cxx reply codedokode 18 hours agoprevI think that benchmarks are made wrong way. They typically calculate how many times per second a browser can run some code. But this causes browser vendors to do trade-offs, for example, increase memory consumption which increases the probability of swapping which causes everything to stall.A more fair way would be to account for memory usage:score = speed ÷ (RAM usage)²Or they should run their tests at 2Gb system with lot of other applications running in background. reply WirelessGigabit 17 hours agoprevI would love to switch to Firefox but I&#x27;m stuck trying to deploy the Firefox Sync server myself in a way that &#x27;just&#x27; works.The old version is not developed anymore, and the new version isn&#x27;t easily deployable without going through 10 separate issues and trial and error (and I&#x27;ve never succeeded in it). reply WorldMaker 15 hours agoparentThat&#x27;s an interesting reason not to use Firefox.Do you just not sign in to Chrome at all? It&#x27;s not like it has ever offered a self-hosted version of Google Accounts.Sync says all data is E2E encrypted. If you trust that (and you&#x27;ve been close enough to source to verify it yourself, it sounds like), it shouldn&#x27;t matter who hosts Firefox Sync for you. reply elashri 17 hours agoprevThat&#x27;s great. Also on a side note regarding performance. Does anyone else use Firefox developer edition as their main browser because they feel that the performance is better than Firefox vanilla itself?I feel that based on a heuristic measurements (that is probably not reproducible) reply rumdz 18 hours agoprevI am pleased with the performance of Firefox over Chrome in every area except GPU-related tasks. I&#x27;ve noticed most (all?) sites that use my GPU (1080 Ti) regularly run around an order of magnitude slower on Firefox than Chrome on my Windows desktop. reply jve 19 hours agoprevPay attention to vertical x axis scale for the graphs - it doesn&#x27;t scale visually. reply postalrat 18 hours agoparentWhat am I suppose to see? reply gadders 17 hours agoprevI don&#x27;t know what version of Firefox this is, but the version I use is so slooooow using Gmail. It takes close to a minute to finish loading if I refresh the screen, compared to seconds with Chrome. reply foob 16 hours agoparentI use Firefox for nearly everything, but I need to keep a Chrome window open because Gmail and Google Meet will max out my CPU and barely work in Firefox. Google properties are literally the only sites where I experience this issue.Does anybody know technical details about why this is the case? The anti-competitive incentives for them doing this are obvious, but I imagine that there must be some technical explanation that makes for a plausible excuse. Something like them using Chrome only APIs with slow polyfills on other browsers? reply liminalsunset 13 hours agorootparentFor what it&#x27;s worth, you can add YouTube to the list of Google sites with CPU consuming bugs - particularly in dark mode.I&#x27;m not sure whether Chrome entirely fixes it, but if you watch YouTube in dark mode, there was a UI upgrade that they added that puts a \"glow\" behind the YouTube video player.It&#x27;s intended to look like a light strip behind your TV (but to me, I originally thought there was just something wrong with the monitor or my eyes), but in reality it&#x27;s just more Canvas&#x2F;GPU bound calculations running at 60fps constantly which would cause the CPU&#x2F;GPU in the laptop to pull tens of watts, killing battery life.There is no way to turn this \"feature\" off, other than disabling dark mode or blocking the \"effects (do not remember exact item name)\" DOM item in uBlock Origin. If you want to replicate this, go to a video on YouTube and use the three dots near the sign in button on the top right to enable the dark theme (may already be enabled). Play a video with bright picture, and after about ten seconds, you should start seeing \"stuff\" show up under the video as if it was a reflection. reply scrivna 8 hours agorootparentThere’s an option to turn these background fx off in the player settings reply kccqzy 16 hours agoparentprevMy Gmail loads just fine on my Firefox even on a low-end Intel NUC. On the modern web Gmail isn&#x27;t that heavy on resources and any modern browser should handle Gmail fine. reply exceptione 17 hours agoparentprevOh no, Google must be really sorry about that!EDIT to clarify: Google is known to make sure the web experience breaks for firefox, same story for youtube, so this is not an accident. Don´t use chrome and don´t use gmail if you want to keep the web open. reply gadders 17 hours agorootparentWell, exactly. I used to work for Lotus when every new release of Windows or Dos broke something in 1-2-3. reply huggingmouth 18 hours agoprevM6 one and only problem with Firefox is its signed add on policy. I want to write my own add ons and istall them on mobile and desktop without creating an account on some random website.With chrome and kiwi, I can do this. reply fulafel 17 hours agoprevThe graph shows a cliff, presumably there&#x27;s a specific code change associated, anyone have a link? reply aidenn0 16 hours agoprevDoing well on benchmarks is fine, but real-world code is what matters and programmers will write their javascript to match what is fast on Chrome and not test on Firefox, meaning there will be some code that is accidentally fast on Firefox, the balance will be purposefully fast on Chrome. reply everdrive 16 hours agoprevI’m really not excited about JavaScript performance. You know what happens when everyone’s JavaScript performance improves? Webpages just add more JavaScript until it becomes just as slow as it was before. People won’t optimize on a large scale until we hit some upper limit. reply omoikane 16 hours agoprevI hope Chrome team send Firefox team a cake for this milestone. reply jasfi 18 hours agoprevIs Firefox + uBlockOrigin close enough to Brave&#x27;s protections? reply eco 18 hours agoparentIt seems like it&#x27;s somewhat close (assuming uBlock Origin grabs most of the tracking query&#x2F;content).https:&#x2F;&#x2F;privacytests.org&#x2F;HTTPS Only can be enabled (it&#x27;s still off by default) to score a few more points.It gets better in Nightly and even more so in Nightly Private. reply thatcherthorn 18 hours agoprevWill Brave ever separate from Google? reply adventured 18 hours agoprevI have been using Firefox on a daily basis since 2005-2006.When Chrome leaped far out ahead in terms of speed, responsiveness, Firefox nearly lost me as a user (as it was quite slow at the time). Since Firefox largely caught up all has been well. I&#x27;d give Firefox an 8.5 out of 10 in terms of how happy I am as a user. They would have to do something atrocious to lose me at this point. I also prefer their engine over Chrome in terms of visual rendering.The sole meaningful complaint I have about Firefox is its memory usage, which still seems high. It wants 1-2gb of ram no matter what I do. reply toastal 17 hours agoparentI remember that time & feeling the absolute need to run Minefield (for Fx v4.x) over stable since the next gen features&#x2F;performance were too important even without add-on compatibility. The 4.0 release took forever in testing & Chrome flew out like a rocket. And even thru the lulls, Fx performance has always been good enough to stick around (tho eventually I moved to LibreWolf due to management of the new browser feature defaults). reply AtNightWeCode 18 hours agoprevSome years ago it was common that some sites did not work or was slow in FF. I can&#x27;t remember that I&#x27;ve had that problem for years. Good work by the FF team regardless of these benchmarks. reply ur-whale 13 hours agoprevWhile that&#x27;s great news, in recent years, AFAIC, the decision to use firefox over Chrome has been entirely unrelated to performance issues.In my book, Chrome is now officially in the same doghouse IE6 was confined to back in the days: a buggy spying machine that is mostly a vehicle designed to shove Google&#x27;s policies down the collective throats of internet users. reply shadowgovt 18 hours agoprevOkay, time to risk a switch back. reply gottorf 19 hours agoprevI use Firefox everywhere I can, warts and all, just to avoid the Chromium monopoly. It was actually a huge bummer for me when Edge gave in and switched to Chromium; having a giant like Microsoft support a third browser engine would really have been a boon to the health of the ecosystem. reply tapoxi 19 hours agoparentFirefox has lost about 20 million users since the announcement of Chrome moving to Manifest v3. It can&#x27;t avoid the monopoly even with the press in their favor.https:&#x2F;&#x2F;data.firefox.com&#x2F;dashboard&#x2F;user-activityCall me a pessimist but I don&#x27;t think Mozilla is managed well enough to actually compete against Chrome. We&#x27;d need the Linux Foundation or some other large neutral standards body to fork Chromium at this point and create a vendor-neutral platform that everyone can rally behind. reply mrguyorama 18 hours agorootparentAh yes, so Firefox is faster than chrome, allows you more latitude for adblockers, and isn&#x27;t pushing garbage hostile web standards, but clearly, mozilla just isn&#x27;t doing enoughCan we stop pretending that this is mozilla&#x27;s fault that the GIANT corp sitting in front of the internet, being the homepage of 90% of computers, that nags users to install chrome \"for a better experience\" on every single web site it can, is able to get onto more systems than firefox?How many average users even KNOW about firefox anymore? reply tapoxi 18 hours agorootparentFaster on one benchmark, but they don&#x27;t have a consistent history of performance wins. The bigger issue is that nobody is embedding Gecko. Every Electron app is another developer targeting Chrome. Applications that embed a webview? Chrome. Any new web browser that wants to deliver a new experience (like Arc, or even Brave) is built atop Chromium.The problem is that \"not pushing hostile web standards\" and \"more flexible ad blocking\" clearly isn&#x27;t stopping the insane bleeding of the userbase. These are people actively migrating away from Firefox in droves. reply ComputerGuru 18 hours agorootparentI would argue that Electron webapps shouldn’t really count towards the browser marketshare. If would be nice to have a Firefox version, but it doesn’t matter precisely because the end user is not choosing the browser - the company pushing the product is. And let’s be honest: nerdy HN users aside, Joe Schmoe has no clue Teams is a web app running in an emasculated browser at all. reply lkbm 17 hours agorootparent> I would argue that Electron webapps shouldn’t really count towards the browser marketshare.Perhaps not, but if you&#x27;re writing your desktop app in Electron, you&#x27;ll be benchmarking your performance optimizations against the Chrome engine. If that same code is used for your web app, you&#x27;re optimizing your website for Chrome.Mozilla can pour resources into making Firefox more performance, but the real-world ecosystem will change behavior to improve Chrome&#x27;s performance on their webapp with little concern as to whether it hurts Firefox&#x27;s performance.(I assume people use largely the same code between Electron and their webapps, and that optimizations for Electron translate reasonably well to optimizations for Chrome.) reply dataangel 17 hours agorootparentprevevery time someone uses electron they buy into the chrome ecosystem -- when they run into problems they write mailing list posts or stack overflow questions that other people can read to help them when they have the same problem, and if they actually want to contribute bugfixes or features upstream they go into chrome not firefox. also if they have a webapp version the webapp version that&#x27;s running in chrome works exactly the same as the same code running in electron without really having to do any separate testing so which browser are they going to push to be used internally? reply SubiculumCode 17 hours agorootparentprev1. I use firefox in Windows, Linux, and Android. I am NOT suffering.2. Most people aren&#x27;t actively choosing their web browser. For example, my wife uses the Samsung web browser on her phone because that is the default.3. Most people think Google is the internet, can&#x27;t distinguish between the browser and the google webpage, etc.4. A dwindling marketshare percentage does not directly imply old userspe are moving from Firefox to another browser, it could imply that new users don&#x27;t know&#x2F;use firefox reply fauigerzigerk 17 hours agorootparent>4. A dwindling marketshare percentage does not directly imply old userspe are moving from Firefox to another browser, it could imply that new users don&#x27;t know&#x2F;use firefoxYou&#x27;re right that it could theoretically mean that, but unfortunately monthly active users have actually fallen by ~20% since 2019 according to Mozilla:https:&#x2F;&#x2F;data.firefox.com&#x2F;dashboard&#x2F;user-activity reply WorldMaker 16 hours agorootparentIt may also just mean more users are opting out of telemetry. Given Mozilla&#x27;s privacy focus in their marketing and their UX, it may not be a shock that a large number of users are also opting out of even Mozilla&#x27;s telemetry. reply SubiculumCode 16 hours agorootparentI do. reply ohgodplsno 17 hours agorootparentprevKeep moving the goalposts, Firefox is going to go real far this way.Google is _forcing_ Chrome, onto everyone. Gecko is fully embeddable on Android, through GeckoView. Nobody uses it, because the default WebView is chrome. androidx.browser is Chrome. And if you want to delegate to the browser while still staying in an app context, the feature is called Chrome Custom Tabs. Everything is made to make people forget about Firefox&#x27;s existence. Google actively harms Firefox performance on their websites, while happily displaying a \"it&#x27;s better on Chrome\" everywhere you browser. They force the WHATWG&#x27;s hand on 90% of features. There&#x27;s articles every month on new-obscure-feature-that-is-only-implemented-by-chrome-and-requires-an-army-to-implement, pushed and paid for by Google, which dumbass webshits are going to go and implement because obviously their VC backed startup _really_ needs WebUSB and WebMIDI, as well as Google&#x27;s shitty, unfinished implementation of WebGPU.Google is a cancer that grows everywhere, killing everything it touches. Firefox smoking a cigarette from time to time is not responsible for that. reply dataangel 17 hours agorootparentmaybe don&#x27;t forget that firefox probably would have died a much sooner death if mozilla hadn&#x27;t been google&#x27;s pet charity project though. mozilla has yet to demonstrate really being a self sustaining organization reply iscream26 15 hours agorootparentCorporations (and the wealthy in general) don&#x27;t do charity. The tacit objetive of every corporation is and will always be profits über alles. Every time you see a corporation donating money to some cause there&#x27;s an ulterior motive that ultimately grows their bottom line. The most common motives are: pay less taxes (or none at all), PR move to improve their public image, and, finally, publicity (sometimes because of controversy).Now, of course, Google&#x27;s financial relationship with Mozilla is no exception. The stated reason is that they give money to Mozilla in exchange for having Google as the default search engine, but its actual purpose is to mitigate claims about Google having a monopoly on the browser market and thereby avoid anti-trust laws.If Firefox were to gain the majority of the marketshare Google would no longer have an incentive to give them money. Mozilla wouldn&#x27;t be happy about that because they&#x27;d lose their biggest source of income. And Google wouldn&#x27;t be happy either because they make money through web ads and harvesting data to sell it to ad companies, and there&#x27;s no better way to go about doing that than creating their own web browser and a whole ecosystem surrounding it, and then making sure it&#x27;s the most popular one. reply cxr 17 hours agorootparentprevGoogle sucks. Your position seems to be, though, that because Google sucks, we can&#x27;t talk about the self-inflicted wounding of Firefox and other buffoonery that happens under the Mozilla Corp umbrella—since Google sucks so much more. There&#x27;s a word for that: whataboutism. reply lostmsu 17 hours agorootparent> self-inflicted wounding of FirefoxThis makes no sense. The parent argument is that Chrome wins due to aggressive bundling and free ads on Google. Neither of those are self-inflicted wounds. The claim is that Mozilla doesn&#x27;t actually have self-inflicted wounds. reply pessimizer 16 hours agorootparentThe claim is that we have to blame Firefox&#x27;s failure on bundling and google.com banners, and can&#x27;t discuss how hostile and bizarre they&#x27;ve been. We also can&#x27;t discuss that 80% of their revenue comes from Google, for nothing, and that the other 20% of their revenue is the entire return on 100% of their investment in the browser. reply cxr 14 hours agorootparentprev> The claim is that Mozilla doesn&#x27;t actually have self-inflicted wounds.Not in the comment I was responding to; you would have to be arguing that \"Firefox smoking a cigarette from time to time\" doesn&#x27;t describe a type of self-inflicted wound—and for you to be right—for that to be true. (Whether you genuinely think that or not is one thing, but you definitely wouldn&#x27;t be right about it, in any case.)The entire comment I responded to belongs to the flavor of apologia that takes the form, \"it doesn&#x27;t matter if X is bad, because Y is worse\". reply ohgodplsno 13 hours agorootparent>it doesn&#x27;t matter if X is bad, because Y is worseIn the case of Mozilla and Google, yes. Mozilla&#x27;s leadership fucking sucks, and I&#x27;ve written at length about how the entire Foundation board is a bunch of useless MBA clowns that are here to suck from the golden teat.But it doesn&#x27;t matter, when the alternative is Google having absolute and total control of the web. Mozilla, for what they&#x27;re still worth today, still have a weight. Write an article titled \"Mozilla opposes API proposal X from Google for privacy reasons\" and people will at the very least give it a listen. RFCs from them are listened to. The WHATWG has to at least pretend to hear them out.Go on. Let Mozilla die. See what the web looks like when Google can push every API they want, unopposed. WEI would look like a walk in the park. Apple could say nuh uh we don&#x27;t want to, and Google would do the exact same thing they did with Firefox: snuff it out. Microsoft wouldn&#x27;t even need to be pressured to work with them, pissing off Apple sounds like a lovely distraction for them. Little by little, websites would stop working on Safari (not that they work quite well currently, most Safari users are doing it because of a complete lack of technical knowledge and it&#x27;s the default browser, with three nerds on HN saying it&#x27;s because it&#x27;s better for their battery life), APIs unsupported because Apple isn&#x27;t interested in the open web.At best, you end up with an Apple Web, and a Google Web, both of them sucking ass in their own right. At worst, Apple becomes irrelevant because of their focus on native apps. Most of which are already Chromium wrappers anyways.So yes. Pinch your nose and go in. Mozilla is worth the support, no matter what. Their leadership already sucks, what&#x27;s the worst that could happen ? They keep sucking, but take back marketshare ? Good. For once, give them support instead of being the worst user in the world, the one the just shits on them for not being good enough, while flocking to Google anyways. reply cxr 13 hours agorootparentYou seem to be replying to an argument no one is making (e.g. \"Don&#x27;t use Firefox\"). Not sure why. replyn0tinventedhere 18 hours agorootparentprev> so Firefox is faster than chromeFaster on sunspider. In practice, on real web pages, as someone who occasionally browses the web using a low end tablet PC (Surface Go 2 with linux. The CPU is downright anemic but good enough for treating it like a reading device&#x2F;video stream platform), I experienced many more stutters on Firefox that I did not on Chrome and it has become one of the reasons I stopped using it (the syncing is also way worse and often breaks. \"Read latest tab from other device\" is almost useless if you don&#x27;t manually trigger a sync because it sure won&#x27;t do it by itself in any reasonable time frame).The performance difference is much less visible on my main computer though. But yall really need to try firefox on low end devices before you make generic statements about its performance. Or the android version.. lord, firefox on android is just unpleasant. reply sundarurfriend 17 hours agorootparentBy stuttering, if you mean video stuttering, I used to experience those a lot; I installed Brave for that very reason, because Netflix, Twitch, etc. were performing pretty poorly on Firefox (on a 4GB RAM low end laptop). But I very rarely experience that these days, to the point I&#x27;ve switched back to Firefox for those sites too.But general performance-wise, Firefox is so easily better than Chromium in my experience, it&#x27;s not even a competition. Firefox with hundreds of tabs open performs better than Chromium with 10-20 tabs. It&#x27;s more responsive to interactions, uses less memory, and is much less likely to freeze up. With Chromium, I have to constantly keep a watch over open tabs, and close them mercilessly, or suffer from overall system slowdown and a browser that&#x27;s almost a slideshow. reply drawfloat 16 hours agorootparentprevIt&#x27;s about 12 months ago now, but I worked on a GL project where Firefox was significantly slower than Chrome – and, agreed, it was often on lower end devices. This was to the point it was easier to just drop features wholesale from Firefox, in order to ensure a steady 30 FPS (with Chrome solidly performing 60). I believe a large amount of it related to WebAudio APIs. reply garciansmith 17 hours agorootparentprevFirefox still runs great on my ThinkPad T420s running Linux, which is 12 years old at this point.And while I never use Firefox on Android without uBlock Origin, I don&#x27;t get how it can be described as unpleasant after the rewrite a few years ago. The combo makes browsing bearable compared to Chrome.The sync tabs thing used to be flaky, I agree, but now sending tabs to and from Linux and Android always works for me. reply seanw444 18 hours agorootparentprevFunny, I use Mull on my Android, which is basically Firefox on Android but even more heavily restricted. Have for at least a year. Didn&#x27;t know people considered it unpleasant. reply onli 18 hours agorootparentI don&#x27;t think they really do. Firefox, at least after the rewrite, is what made browsing on low-powered android machines bearable, in my experience. reply seanw444 16 hours agorootparentEspecially since it supports uBlock Origin and Dark Reader. reply caskstrength 17 hours agorootparentprevI have very similar experience - subjectively I experience more stuttering in FF than Chrome both on Android and Linux laptop. reply Dah00n 16 hours agorootparentI have the exact opposite experience. Streaming video is stuttering and buffering so bad that I can&#x27;t use Chrome at all on those sites (Brave isn&#x27;t any better). It was the reason I switched to Firefox and the performance was so much better. reply caskstrength 16 hours agorootparentYour experience is not \"exact opposite\" since for me video in FF with VAAPI-based hardware decoding also works better than in Chrome. However, FF can stutter while scrolling animation-heavy sites or when open&#x2F;closing tabs. reply vtee44 18 hours agorootparentprevI use Firefox fork as main because anything else is worse on QubesOS. Both WebKit and Chromium have scroll lags, while Librewolf is smooth almost as on bare metal. I don’t know what’s the difference on normal websites, but I can use it and there are almost no lags on it when running few VMs on 8GB RAM.Android variant is actually shit, but it’s getting only better. It’s much faster now than 5 years ago. Hopefully they will improve it further. reply Kuinox 18 hours agorootparentprevNot shipping WebReplay in firefox (instead it&#x27;s a startup called replay.io),Killing the servo project.Running metaverse projects instead.Yes, mozilla could do better. reply mrguyorama 18 hours agorootparentThese are all things that the average user doesn&#x27;t even know exist\"Power users\" aren&#x27;t most of anyone&#x27;s user base. The vast majority of people use chrome because google told them to, because google IS the internet reply pessimizer 16 hours agorootparentSomehow they managed to have a huge market share when Microsoft was fighting tooth and nail in courts and out to tell people to use IE, and Microsoft WAS home computers. reply Kuinox 17 hours agorootparentprevWebsites arent made with firefox, so they are not optimized for firefox. They also can be buggy, firefox is my daily browser, yet, multiple times per year, a website is simply bugged on firefox.Google Chrome was recommended by power user, and that how they took the market lead, not because they are the internet, but because power users told regular users to use chrome. reply 878654Tom 16 hours agorootparentGoogle massively advertised Chrome at the start with banners on Gmail, Search,... telling you you should download it.Powerusers don&#x27;t shift the market. reply ndiddy 16 hours agorootparentGoogle also paid freeware application developers to bundle Chrome in their installers. reply mrguyorama 17 hours agorootparentprevI have never found a website that is broken on firefox that isn&#x27;t also non-functional on chrome. What does firefox break? reply Reubachi 17 hours agorootparentFolks mistakenly associate \"breaking\" with \"not being completely compatible with an illegal streaming website with improper TLS&#x2F;cyphers.\" That&#x27;s just an example but you know what I mean.Those are the types of websites that have issues with firefox, because mozilla maintains it&#x27;s own root trust store held to various standards&#x2F;compliances, and many..less savory web addresses use not HTTPS content on their HTTPS pages.This is a feature&#x2F;value added bonus, but because it \"breaks\" sites for people....back to chrome. reply Kuinox 14 hours agorootparentprevMost are due to the firefox tracking protection. Others (with everything disabled) were for an airline company, some french governements website, and recently an SVG editor that just call to use chrome and doesn&#x27;t works on firefox. replysolardev 17 hours agorootparentprevThat never stopped Phoenix in the early days, or Chrome itself in its early days. Firefox just offers nothing substantial over Chrome and breaks a lot of sites. And over the last decade it kept pushing crapware and spam on you, right in the browser.Mozilla mismanaged it to hell. They don&#x27;t even really care about the browser anymore. reply wintermutestwin 17 hours agorootparent>Firefox just offers nothing substantial over ChromeHah! Let&#x27;s see you try to use vertical tabs in Chrome. If you ever have more than 8 tabs open at once, Horizontal tabs are clearly inferior.Brave and Edge can do vertical tabs natively, but chrome doesn&#x27;t. The extensions I have tried in Chrome are as unusable as Safari&#x27;s native vertical tabs implementation. reply pessimizer 16 hours agorootparent> Hah! Let&#x27;s see you try to use vertical tabs in Chrome. If you ever have more than 8 tabs open at once, Horizontal tabs are clearly inferior.That&#x27;s it, though. If Firefox breaks that again, like they intentionally broke it before, that 3% usage will drop down to 2%. reply radicaldreamer 17 hours agorootparentprevWhich sites don’t work with Firefox?Safari is a huge simply due to iOS but it’s a shame that Apple stopped maintaining the windows port. reply solardev 17 hours agorootparentSadly I don&#x27;t remember anymore. I ran into plenty of them 3 or 4 years ago (every week or so), then I stopped using Firefox altogether because of that. At work, some coworkers were using Firefox last year and it broke various third party libraries (especially graphics heavy ones). Rather than trying to fix the bugs, we just decided to deprecate Firefox support altogether because its usage was down to like 2 or 3 percent. Those resources were better spent on, say, improving mobile usability and performance for everyone.Edit: I wish Mozilla would just fork Chromium and add&#x2F;delete whatever privacy things they want, like Brave. There&#x27;s no reason to maintain Gecko anymore. reply n0tinventedhere 17 hours agorootparentprevI didn&#x27;t have websites outright breaking on FF like solardev, but I had other issues with it, like cloudflare getting me into an infinite loop of asking me if I&#x27;m human and not letting me browse the website. This happened even if I deleted my firefox profile and started fresh with no extension. Note that this behavior of Cloudflare is highly dependent on the settings of their anti-ddos stuff, some websites have it set on a higher level of defensive behavior than others, I didn&#x27;t have this issue everywhere.This wasn&#x27;t due to my computer&#x27;s IP, the problem went away the moment I browsed the same website with Chrome, and this time I wasn&#x27;t even asked to click the checkbox to prove that I&#x27;m human. reply wintermutestwin 17 hours agorootparentprev>Firefox just offers nothing substantial over ChromeHah! Let&#x27;s see you try to use vertical tabs in Chrome. If you ever have more than 8 tabs open at once, Horizontal tabs are clearly inferior. reply pessimizer 16 hours agorootparentFirefox is still hostile to vertical tabs, adding a useless sidebar header that can be hidden with css, but you have to know that and spend the possible hours it could take to figure out how. Then, since that&#x27;s unsupported, you have to prepare for it to possibly be broken after every update. reply solardev 16 hours agorootparentprevAnecdote: I tried that back in the Netcaptor days but it didn&#x27;t really stick (or something like it, anyway). Having to read a long list isn&#x27;t particularly fast for me, personally, so I just use a combination of multiple windows (one per context, and easy to switch between on Mac) and the search engines feature (where you can make custom keyword lookups for Wikipedia, Stackoverflow, etc.) to quickly look things up. Chrome also has collapsible tab groups and pinnable tabs now, but it&#x27;s not particularly great.I never have more than 4 or 5 tabs open at a time, closing them as I&#x27;m done with them. If I need to recover them later they&#x27;re always in the history.I&#x27;m sure Firefox and other browsers offer some power user features that&#x27;s good for some folks. I just don&#x27;t care enough.As a user I just want to see the information I need without tinkering with settings, and Chrome does that well. As a web dev I&#x27;d much rather focus on UX than cross browser compatibility, and the Blink&#x2F;Webkit duopoly makes that possible in a way that standards never did (and still don&#x27;t). So there&#x27;s just no need for Firefox in my personal or work life. YMMV of course. reply madars 17 hours agorootparentprevWhat Firefox definitely offers is uBlock Origin on Android. Augment it with anti-paywall blocklists and the experience is basically a killer feature for Android over iOS. I use Chrome on desktop (mostly for better sandboxing) but on mobile there is no real competition between Firefox and Chrome: Firefox just wins. reply solardev 16 hours agorootparentYou can get the same experience (but across all apps) using Adguard, which acts as a fake VPN and MITM certificate. No more ads in any app, while still keeping your Chrome synced tabs and logins and such between devices. reply madars 16 hours agorootparentIs the experience really equal? E.g. uBlock Origin operates at the DOM layer so it can and does block dynamically generated content using CSS selectors. Does Adguard simply inject the same DOM-aware JS in every page? reply solardev 14 hours agorootparentI dunno, but it&#x27;s good enough. And way more useful to block in app ads than to be limited to the browser only. Of course you can use both if you want, but then you&#x27;d have to use Firefox or another browser. reply Dah00n 16 hours agorootparentprevVPN and MITM?! What a horrible horrible security nightmare! reply solardev 14 hours agorootparentIt sure is. But what a great user experience afterward! Security and convenience are usually tradeoffs, and in this case it&#x27;s totally worth it. reply kilolima 16 hours agorootparentprevExcept there&#x27;s no way to export your bookmarks on android out of Firefox without using their sync service. I think this omission (a simple export!) speaks volumes about Mozilla&#x27;s true goal of browser lock-in and another example of their chronic mismanagement. reply mrguyorama 14 hours agorootparentAh yes browser lock in, from the company that makes zero dollars selling their browser, compared to the company that wants to actively harm the open internet.What&#x27;s wrong with you guys? Could google stab you in the face and you still swear up and down that firefox is negligibly slower than chrome? What will it take? Google is pushing a \"feature\" to completely lock down the internet, take away all meaning from \"user-agent\", and make blocking ads functionally impossible! When will you stop aiding and abetting this behavior because you feel mozilla is \"not perfect\" reply solardev 13 hours agorootparentIn truth I just don&#x27;t really care? We&#x27;ve gone through this cycle so many times with various kinds of DRM, from DVD CSS to Denovo to WideVine to Adobe Cloud to various other schemes that at the end of the day just aren&#x27;t big deals.A lot of the tech echo chamber bandwagons and freaks out about things like this, but I bet in a few years it&#x27;ll turn out either a non issue or else alternate browsers will naturally rise to popularity. No point fussing about it beforehand. In the meantime Chrome just works, Google is whatever, Firefox is annoying, and Mozilla just feels irrelevant.Shrug. I feel way more annoyed by Firefox than anything Google&#x27;s ever done. Whatever their ideology, their product just isn&#x27;t great. replydarklycan51 16 hours agorootparentprevI am a tab hoarder, and on multiple different operating systems, with different installs, etc.Firefox ALWAYS seems to have random leaks after weeks that other browsers don&#x27;t have, once I saw Firefox taking 24GBs of RAM, while Edge and Brave on the same or more tabs and time were taking only ~2-3gb of RAM.This has been going on since at least 2019-2020 or so, when I started dropping Firefox because of this behavior. It is just random websites that seem to cause leaks but this has never happened with Edge or Brave and no website should really be breaking a browser like this, the system becomes unusable after a while.They are seemingly unable to fix the issue or pretend it doesn&#x27;t exist even when its reported so I basically had to stop using FF as my main browser. reply ido 16 hours agorootparentprevIt’s not about speed, compatibility with Firefox is actually getting worse. I switched (back) to FF and DDG a few months ago and FF is having trouble with multiple websites I use on occasion (for an example off the top of my head, if I want to log-in to Haaretz.co.il to get past pay walls I have to use safari as FF doesn’t manage to log in).This is still the small minority of sites but I keep safari and chrome around as it still happens often enough. reply leviathant 17 hours agorootparentprev>Ah yes, so Firefox is faster than chrome, allows you more latitude for adblockers, and isn&#x27;t pushing garbage hostile web standards, but clearly, mozilla just isn&#x27;t doing enough>How many average users even KNOW about firefox anymore?A hard lesson I learned at my last company was that it&#x27;s not enough to have the better product. Better marketing and better relationships will, more often than not, trump a better fitting solution to a problem. It sucks, but it&#x27;s real. reply mrguyorama 14 hours agorootparentYeah! In fact, I would say a large portion of this is every android phone coming with Chrome pre-installed, and you know, that entire decade google spent with literal super bowl advertisements and THE GOOGLE DOT COM WEBSITE URGING YOU TO INSTALL CHROME with a mountain of dark patterns that plenty of smarter people than I have written about.Can you imagine the fucking fit HN would throw if mozilla dared to buy a super bowl ad? reply mda 17 hours agorootparentprevNote that in the Phoronix post it also mentions in the modern and demanding Jetstream benchmark, Firefox is still way behind. While seeing improvements is good, Sunspider is a very old microbenchmark not sure how relevant it is. reply wslh 17 hours agorootparentprev> Can we stop pretending that this is mozilla&#x27;s faultClearly it is Mozilla fault: the world is not fair, if they are where they are they should crunch their resources and win. Google haven&#x27;t started being the home of most computers. It is the nature of competition. Is it hard? Sure. reply mrguyorama 17 hours agorootparent\"The world is not fair\" in part because we refuse to punish Google for their blatantly anti-consumer practice of purposely making their websites work worse on non-chrome browsers and advertising chrome on every google property.How the fuck is mozilla supposed to compete with a GIANT that is nearly everyone&#x27;s homepage? Even if they had comparable budgets (Which they don&#x27;t) mozilla still wouldn&#x27;t get the free advertising that google gives chrome reply pessimizer 16 hours agorootparent> purposely making their websites work worse on non-chrome browsersThis is wild exaggeration that I never hear about except from people defending the mismanagement of Firefox. I&#x27;ve used Firefox since forever, and I don&#x27;t have much trouble anywhere that I can remember. The browser works. This is an article showing that it&#x27;s outperforming Chrome on some metrics. It&#x27;s Firefox&#x27;s business and design decisions that wreck their browser. It went from being awesome to being janky Chrome that calls you abusive when you complain that they put an ad for a tv show in your browser. reply wslh 17 hours agorootparentprevI think the Mozilla foundation has enough money to lobby about monopolistic practices. Did they?If your argument would be 100% true there will be no innovation or market dynamics. Business history has proven that one day you are at the top and another you aren&#x27;t anymore. reply pessimizer 16 hours agorootparentIf firefox sued google over monopolistic practices, that lawsuit would have to be funded completely by google. Google is 80% of firefox&#x27;s revenue and 0% of its expenditure. reply n0tinventedhere 17 hours agorootparentprevI find it amazing how people forget that being a monopolistic power doesn&#x27;t help if your product sucks: Google has, time and time again, failed to gain traction in the messaging app market on Android, the market being entirely occupied by competitors like WhatsApp.Chrome won partly because of the marketing and being preinstalled, sure, but also by being a good product. If nagging people to install Chrome was all it took to win, then why is Edge in such a precarious position on Windows with all the nagging Windows 10 and 11 brought? Microsoft failed to succeed time and time again.People on HN have weird preconceptions about how suggestion and marketing works. reply mrguyorama 14 hours agorootparent>I find it amazing how people forget that being a monopolistic power doesn&#x27;t help if your product sucks:Which is why none of our companies use microsoft teams, right? Or wait, maybe when it comes to products that aren&#x27;t that different, what comes with the package matters. reply Dah00n 16 hours agorootparentprevI don&#x27;t know where you are from but you seem very biased to your own friend or country bubble. For reference I don&#x27;t know anyone at all who use WhatsApp. I use Google Chat and Facebook Messenger with a few but SMS with Google&#x27;s RCS with everyone else, including my younger and much older family members. reply n0tinventedhere 16 hours agorootparentI live in France, and WhatsApp has a pretty big presence. Not a monopoly style one though, there&#x27;s plenty of other apps used: Facebook Messenger, Skype, Snapchat, Discord.. I have friends who have family in other countries and some different apps will be dominant, like Viber, but the common theme is that the only time I&#x27;ve seen people use SMS is when they&#x27;re unsure as to whether the other person has any other means of communication. Most importantly, I&#x27;ve never seen someone use Google&#x27;s own non-RCS chat apps, like Google Chat, Google Hangout, Google Allo, Google Talk.. Google has had many chat apps, sometimes just different names for the same thing, but in the end, they have no users here. Google continuously tried to make a dent in that market and failed.RCS is a non-starter if you need to do group chats with iPhone users or send them pictures without a massive loss in quality, as iPhones do not support RCS. As long as Apple refuses to support it, it will continue being a terrible choice in any country with significant Apple user base. An Android user with WhatsApp shares an identical experience with an iPhone user with WhatsApp. RCS on the other hand will drop you back to what cell phone messaging was many decades ago. reply wslh 16 hours agorootparentprevWhatsApp is used all over Europe and Latin America. Very easy to check. For example [1].[1] https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;1311229&#x2F;whatsapp-usage-m... reply Dah00n 16 hours agorootparentAccording to statista.com 12% use WhatsApp in Denmark. As I said, living in a bubble if one believes everyone uses it.Edit: A quick search said this from the same source:>Using WhatsApp less often than once a month was the most common usage frequency among Danes in 2019, according to 23 percent of the service users. 15 percent used the app weekly and 13 percent used it several times a day. reply tristan957 17 hours agorootparentprevIf you use anti-trust practices to get your foothold, it isn&#x27;t fair. reply TX81Z 17 hours agorootparentprevUh, clearly you don’t know anybody who actually worked there? Management is very very very unpopular and they’ve been facing a massive brain drain for years.Because they do have a great browser, but have a horrible strategy and have wasted money in any number of ways, and made themselves wholly dependent on Google for handouts. reply burkaman 19 hours agorootparentprevI wouldn&#x27;t expect Manifest V3 to have any effect until Manifest V2 is actually disabled and ad blockers stop working. Approximately 0% of Chrome users are following technical browser announcements, they won&#x27;t care until their stuff actually breaks. reply tapoxi 18 hours agorootparentThe Manifest v3 version of uBlock Origin (uBlock Lite) has been out for a long time now, and it still works its just less effective.For people using Opera, Brave, Vivaldi, Arc, etc it won&#x27;t impact them. Their adblockers are all native and not using the extension API. reply comte7092 18 hours agorootparentprevHave to agree here. Most people have no idea what manifest v whatever is and have no reason to switch.Not to mention, google pulls its own underhanded tactics. reply yorwba 18 hours agorootparentprevEven among Firefox users, only about 41% have add-ons installed in the first place: https:&#x2F;&#x2F;data.firefox.com&#x2F;dashboard&#x2F;usage-behavior So 59% of current Firefox users could switch to Chrome without being affected by the Manifest V3 change at all.I don&#x27;t know how many Chrome users use add-ons that would become less useful after the change, so it&#x27;ll definitely be interesting to see whether Firefox&#x27;s numbers will show a sudden uptick at some point. reply arp242 18 hours agorootparentI expect that number to be biased since more tech-savvy users tend to disable&#x2F;block telemetry, and they&#x27;re also the sort of people who install add-ons. I have no idea if it&#x27;s biased by 1%, 10%, or 50% though. reply toastal 17 hours agorootparentEven still, add-ons have always been a Fx selling point so I’m still quite shocked. reply bee_rider 17 hours agorootparentprevFirefox should really (somehow or another) come with ublock origin installed and on by default. reply caskstrength 17 hours agorootparentprev> Even among Firefox users, only about 41% have add-ons installed in the first placeThis seems like surprisingly low percentage. I wonder whether these stats count people who disable telemetry or install add-ons from their distro&#x27;s repository. reply ComputerGuru 17 hours agorootparentprevNot really. Firefox blocks more things out of the box for a better browsing experience than Chrome does. But I admit that there’s a good chance that at least some not-insignificant percentage of those 41% wouldn’t notice and realize that they would need to install uBlock to use Chrome comparably. reply infamia 14 hours agorootparentprevThis isn&#x27;t accurate since Firefox comes with a built-in ad blocker, which Chrome lacks. reply LordDragonfang 18 hours agorootparentprevThere was a huge wave of anti-chrome propaganda about how manifest v3 would prevent all ad-blocking* on Reddit and Tumblr (and probably tiktok) and such. Even if none of them read the actual announcements, the low-info folks still learn about it from the outrage machine.*Which isn&#x27;t actually true; \"cosmetic\" blocking (altering the DOM), the original type and the one most users actually care about, will still work just fine. It&#x27;s only privacy tracking which is going to be totally broken, and frankly, outside of the unreality bubble of HN, most users care much less about \"privacy\" than you&#x27;d think. reply croes 17 hours agorootparentJust because people don&#x27;t care doesn&#x27;t mean it&#x27;s unimportant.So these limitations of manifest V3 don&#x27;t exist?:>One of the main issues with Manifest V3 is its limitation to filter lists: an extension can only include up to 50 static lists, and only 10 of them can be active at the same time.>There are also limitations to the number of filter rules inside these lists: installed extensions cannot collectively exceed 300,000 static filters and it is no longer possible to update reply LordDragonfang 17 hours agorootparentI never said it wasn&#x27;t important, but it is misinformation. Case and point: to answer your question, no, those aren&#x27;t limitations on DOM-manipulation ad removal; those are limitations on the new declarativeNetRequest API, and they ONLY apply to blocked web requests.Also, even that part isn&#x27;t as catastrophic as you&#x27;d think; ad blocking, like most things on the internet, roughly follows the 90-10 rule: 90% of the ads are blocked by 10% of the rules. So even with a tenfold reduction, you should still see the vast majority of the ads blocked by even the intentionally-gimped declarativeNetRequest API.Yes, it&#x27;s going to make ad blockers slightly less effective and make your page loads slightly slower. But that doesn&#x27;t get as many clicks as \"Chrome is banning adblockers!!1\". And it&#x27;s important to note the difference because when v3 rolls around, all those users that were lied to are going to see that their ad blockers still mostly work, and lose even more trust in whistle blowing. reply croes 3 hours agorootparent>Chrome is banning adblockersYou forgot the part where Google postponed the original timetable for the deprecation of manifest v2. Without that postponement, developers would not have had enough time to change the extension so that there would have been a time when adblockers were banned. The outcry prevented that. reply dataangel 17 hours agorootparentprev> It&#x27;s only privacy tracking which is going to be totally broken, and frankly, outside of the unreality bubble of HN, most users care much less about \"privacy\" than you&#x27;d think.explaining this distinction is actually enough to drive me to switch thanks reply hooverd 13 hours agorootparentprevIf that&#x27;s their takeaway about Chrome, good. reply vxNsr 17 hours agorootparentprevNot sure why you’re being downvoted. I agree, I myself was waiting for ublock to stop working before I migrated to Firefox but now that I’m hearing it will still mostly work… I just don’t care that much. So they collect all this info on me… as long as I don’t see ads I just don’t care reply lamontcg 17 hours agorootparentprev> We&#x27;d need the Linux Foundation or some other large neutral standards body to fork ChromiumWhy is the answer always more chromium?And for a userbase nominally so positively biased towards open source it is weird to see how google gets a pass on pretty much everything, while mozilla&#x2F;firefox has this incredibly carefully curated list of offenses that is maintained and spread by the community, doing most of the work at ensuring that firefox never goes anywhere. There&#x27;s a lot of Linux proponents here who are doing all the heavy work of carrying water for Google when it comes to chrome dominance.Yeah, so every time you actually restart firefox after it updates you probably get an ad for mozilla VPN. Okay compare that to Google literally being the central advertising and spyware hub of the entire internet. \"I&#x27;ll just have to use the worst option possible, because the alternative isn&#x27;t 100% ideologically pure\". reply hooverd 13 hours agorootparentNobody hates FOSS more than FOSS users. reply afavour 18 hours agorootparentprevManifest v3 announcement feels like a pretty arbitrary line to draw in the sand.> I don&#x27;t think Mozilla is managed well enough to actually compete against Chrome. We&#x27;d need the Linux Foundation or some other large neutral standards body to fork ChromiumAh, yes, Linux, the OS used on something like 3% of desktop machines! I&#x27;m sorry but I don&#x27;t see the logic at work here at all. Chrome dominates because Google pushes it and they have the means to reach basically every internet user out there. Switching out Mozilla for the Linux Foundation would do absolutely zero to counter that. Case in point: by most measures ChromeOS has a higher desktop market share than vanilla Linux already. reply sam0x17 18 hours agorootparentLinux accounts for:82% of the smartphone market90% of public cloud workloads62% of the embedded market99% of the super-computer marketThe first figure alone makes it more relevant to the general population than windows or any other OS.Let&#x27;s see, 55% of all web traffic is mobile devices so that means roughly 45% of all web traffic comes from linux-based smartphones.And that isn&#x27;t counting things like Chrome OS, or linux desktop browsing, which account for another 2-4%So roughly 49% of all web traffic is literal user-facing linux devices, and 90% of the websites they are browsing are served by linux servers...So yeah, if anything linux is probably the most important OS in existence right now. Definitely not obscure.. reply afavour 17 hours agorootparentAnd how much of that 82% is attributable to the Linux Foundation as opposed to Google? reply Krutonium 17 hours agorootparent100%, because Google is using Linux? reply afavour 17 hours agorootparentThat makes no sense. 82% of the market is using Linux not because of the inherent goodness of the OS, it’s because Google chose to use it. If they change their mind tomorrow (see Project Fuschia and all that) there isn’t anything the Linux Foundation would be able to do about that. reply sam0x17 9 hours agorootparentThis isn&#x27;t true at all. Most people are using containers anyway, and guess what they&#x27;re running in those containers...ASP.net had it&#x27;s day, and everyone wanted to instead run their .NET apps on linux rather than deal with windows licensing and security issues, which is why mono was so popularOther things had their chance too, like MacOS-based hosting, and it never really took off, again because people don&#x27;t want to manage closed-source production systems that they can&#x27;t simply patch themselves. replyrgrieselhuber 18 hours agorootparentprevI’m confused why people would move away from Firefox to Chrome because of Manifest v3. If anything, it should convince one to move even harder in the other direction. reply gattilorenz 17 hours agorootparentIndeed, the point OP is trying to make is that despite Manifest v3, Firefox is nevertheless still losing users (to highlight how dire the situation for Firefox is). reply rgrieselhuber 17 hours agorootparentGot it, thanks for clarifying. Definitely points to a management problem IMO. reply remram 17 hours agorootparentprevWhere does this data come from?I use Firefox because I am privacy-conscious, as a result I have telemetry turned off. Do people like me show up in those stats? reply postalrat 18 hours agorootparentprevWhy does the announcement of manifest v3 mean anything? They keep pushing the date v2 will no longer work back. Why? reply pessimizer 16 hours agorootparentBecause they&#x27;ve already taken the media heat for the switch a half dozen times, each time less intensely. Eventually, when they finally switch it off, there will be one article about it on the Register, and it will be mostly jokes.It&#x27;s a tactic. reply josteink 18 hours agorootparentprev> Call me a pessimist but I don&#x27;t think Mozilla is managed well enoughNo disagreement there.> We&#x27;d need the Linux FoundationWho spends a whooping’ 3% of their total spending on actual Linux? [1]Call me a pessimist, but I might look for a better candidate.[1] https:&#x2F;&#x2F;lunduke.substack.com&#x2F;p&#x2F;linux-foundation-spends-just-... reply gowld 18 hours agorootparentprevExtensions including ad blockers don&#x27;t work at all on Android Chrome.I use Chrome on desktop because websites work properly in Chrome, and Firefox on Android for the adblocker, except when a site is unusable.What is FF marketshare on Android? reply jeffreygoesto 18 hours agorootparentUs two probably? reply toastal 17 hours agorootparentMake it trouble.I really wish the timing of FxOS was better as well as the marketing. I’d certainly choose it over Android, but we’re now sort of locked into the duopoly with terrible decisions from apps like Signal requiring you have a Android or iOS primary device or no service for you. reply yomlica8 19 hours agoparentprevEdge seems like a blunder in general. Prior to the initial Edge introduction IE seemed to have 20-25% of the desktop browser marketshare. Nothing like the crazy 90+% it had at its height but certainly not a browser you can ignore. Now they&#x27;re at about 10% and it seems like they have to use every manipulation tactic in the book to get it.But I agree, that third browser engine kept things interesting even if I didn&#x27;t use it. Selfishly I hope Apple continues using its iron grip on iOS to prevent anything other than Safari&#x27;s engine there. I don&#x27;t use Apple products personally but benefit from the diversity of ecosystem it provides. reply afavour 18 hours agorootparent> Prior to the initial Edge introduction IE seemed to have 20-25% of the desktop browser marketshare.It had been on the decline for years. Statscounter shows 26% in January 2013, 17% January 2014, 13% in January 2015, then Edge was introduced in July of t",
    "originSummary": [
      "Firefox has surpassed Google Chrome in the SunSpider JavaScript benchmark, marking a significant achievement for Mozilla developers.",
      "Despite this success, Chrome continues to outperform Firefox in the more demanding JetStream 2.0 benchmark.",
      "Firefox has focused on improving upload speed and introducing other enhancements to enhance its overall performance."
    ],
    "commentSummary": [
      "The discussion threads focus on various topics related to web browsers such as Firefox, Chrome, and Safari.",
      "Users engage in debates about performance benchmarks, customization options, memory usage, and user interface.",
      "Concerns are raised about security, password managers, and the dominance of Chrome, and discussions surround the decline of Firefox and the potential impact of Google's control over the web."
    ],
    "points": 803,
    "commentCount": 366,
    "retryCount": 0,
    "time": 1692107924
  },
  {
    "id": 37133054,
    "title": "The OpenTF Manifesto",
    "originLink": "https://opentf.org/",
    "originBody": "The OpenTF Manifesto Terraform was open-sourced in 2014 under the Mozilla Public License (v 2.0) (the “MPL”). Over the next ~9 years, it built up a community that included thousands of users, contributors, customers, certified practitioners, vendors, and an ecosystem of open-source modules, plugins, libraries, and extensions. Then, on August 10th, 2023, with little or no advance notice or chance for much, if not all, of the community to have any input, HashiCorp switched the license for Terraform from the MPL to the Business Source License (v1.1) (the “BUSL”), a non-open source license. In our opinion, this change threatens the entire community and ecosystem that’s built up around Terraform over the last 9 years. Our concern: the BUSL license is a poison pill for Terraform. Overnight, tens of thousands of businesses, ranging from one-person shops to the Fortune 500, woke up to a new reality where the underpinnings of their infrastructure suddenly became a potential legal risk. The BUSL and the additional use grant written by the HashiCorp team are vague, and now every company, vendor, and developer using Terraform has to wonder whether what they are doing could be construed as competitive with HashiCorp’s offerings. The FAQ provides some solace for end-customers and systems integrators today, but even if you might be in the clear now, how can you build confidence that your usage won't violate the license terms in the future? What if your products or HashiCorp's products change? What if HashiCorp changes how they interpret competitive? What if they change the license again? As a result, everything that uses Terraform is on shaky ground. It is clear to us that under the new license, the thriving ecosystem built up around the open source Terraform will dwindle and wither. As developers consider what tools to learn and what ecosystems to contribute to, and as companies consider what tools to use to manage their infrastructure, more and more, they'll pick alternatives that are genuinely open-source. Existing Terraform codebases will turn into outdated liabilities, independent tooling will all but disappear, and the community will fracture and disappear. This sort of change also harms all similar open-source projects. Every company and every developer now needs to think twice before adopting and investing in an open-source project in case the creator suddenly decides to change the license. Imagine if the creators of Linux or Kubernetes suddenly switched to a non-open-source license that only permitted non-competitive usage. We believe that the essential building blocks of the modern Internet, such as Linux, Kubernetes, and Terraform need to be truly open source: that is the only way to ensure that we are building our industry on top of solid and predictable underpinnings. Our goal: ensure Terraform remains truly open source—always. Our aim with this manifesto is to return Terraform to a fully open source license. BSL is not open source, so this would mean moving Terraform back to the MPL license, or some other well-known, widely accepted open source license (e.g., Apache License 2.0). Moreover, we want to be confident that Terraform will always remain open source, so you don't have to worry about another sudden license change putting everything at risk. Our request to HashiCorp: switch Terraform back to an open source license. We ask HashiCorp to do the right thing by the community: instead of going forward with the BUSL license change, switch Terraform back to a truly open source license, and commit to keeping it that way forever going forward. That way, instead of fracturing the community, we end up with a single, impartial, reliable home for Terraform where the whole community can unite to keep building this amazing ecosystem. Our fallback plan: fork Terraform into a foundation. If HashiCorp is unwilling to switch Terraform back to an open source license, we propose to fork the legacy MPL-licensed Terraform and maintain the fork in the foundation. This is similar to how Linux and Kubernetes are managed by foundations (the Linux Foundation and the Cloud Native Computing Foundation, respectively), which are run by multiple companies, ensuring the tool stays truly open source and neutral, and not at the whim of any one company. In particular, we want to create a foundation for Terraform that is: Truly open source - under a well-known and widely-accepted license that companies can trust, that won't suddenly change in the future, and isn't subject to the whims of a single vendor Community-driven - so that the community governs the project for the community, where pull requests are regularly reviewed and accepted on their merit Impartial - so that valuable features and fixes are accepted based on their value to the community, regardless of their impact on any particular vendor Layered and modular - with a programmer-friendly project structure to encourage building on top, enabling a new vibrant ecosystem of tools and integrations Backwards-compatible - so that the existing code can drive value for years to come Supporters We acknowledge that maintaining an open source project such as Terraform takes a considerable investment in terms of time, skill, effort, and coordination. We are grateful to HashiCorp for creating Terraform and their leadership in getting it to this point, and to the thousands of community members for their contributions so far. The next step for Terraform must be to remain open source, either by HashiCorp switching it back to a truly open source license or by us forking it into a foundation. Whichever way it turns out, to ensure that there is sufficient investment to grow and evolve Terraform, the signatories below pledge to pool our resources to build a more open, inclusive future for an open source Terraform. Instructions If you’re willing to join our cause, please sign the manifesto as follows: Check out the manifesto repo (instructions). Add a new row to the end of the table below with your details. Open a pull request with your changes (instructions). Contact us If you are a member of the community, a member of the press, an employee of HashiCorp, or anyone else with questions or feedback to share, you can reach the team behind this manifesto by emailing us at pledge@opentf.org. Share Frequently Asked Questions Is OpenTF going to be a foundation? We strongly prefer joining an existing reputable foundation over creating a new one. Stay tuned for additional details in the coming week. Can anyone pledge? Yes, the pledge is open to both all individuals and all companies who care about the future of Terraform. You can also support this initiative by staring this manifesto repository on GitHub and spreading the word via share buttons. HashiCorp deserves to earn a return on their investment. What's wrong with that? When any company releases their tool as open source, the contract with the community is always the same: Anyone can use this code, but we the creators hold a privileged position of being at the epicenter of the ecosystem. Vendors then compete to offer the best solution, and the creators enjoy a unique competitive advantage. We believe that HashiCorp should earn a return by leveraging its unique position in the Terraform ecosystem to build a better product, not by outright preventing others from competing in the first place. Co-signed Name Type How you'd like to help Gruntwork Company Development; open-source community efforts Spacelift Company Cover the cost of 5 FTEs for at least 5 years env0 Company Cover the cost of 5 FTEs for at least 5 years Scalr Company Cover the cost of 3 FTEs for at least 5 years Digger Company Development; open-source community efforts Doppler Company Development; open-source community efforts Gem Agile Company Development; open-source community efforts Massdriver Company Development; open-source community efforts Qovery Company Development; open-source community efforts Rivet Company Development; open-source community efforts Terramate Company Development; open-source community efforts Terrateam Company Development; open-source community efforts Verifa Company Development; open-source community efforts Argonaut Company Development; open-source community efforts Finisterra Company Development; open-source community efforts AutoCloud Company Development; open-source community efforts 35up Company Testing; code reviews; open-source community efforts Cirrus Assessment Company Testing; minor development; open-source community efforts Amach Company Development; open-source community efforts SMS Data Products Company Development; open-source community efforts Cloud Posse Company Development; open-source community efforts RoseSecurity Research Company Development; open-source community efforts CloudDrove Company Development; open-source community efforts Red Queen Dynamics Company Development; open-source community efforts Octo Ventures Company Development; open-source community efforts Oxide Computer Company Company Development; open-source community efforts Vates Company Development; open-source community efforts Coherence Company Development; open-source community efforts Nullstone Company Development; open-source community efforts Hestio Company Testing; documentation; open-source community efforts appCD Company Development; open-source community efforts CloudKnit Company Development; open-source community efforts Code Factory Company Development; open-source community efforts Indeo Solutions Company Development; open-source community efforts 0pass Company Development; open-source community efforts AppsCode Company Development; open-source community efforts Firefly Company Development; open-source community efforts ControlMonkey Company Development; open-source community efforts Labyrinth Labs Company Development; open-source community efforts Wakam Company Development; open-source community efforts Zerodha Tech Company Development; open-source community efforts Ahead Guru Company Development; open-source community efforts; Consultant and Solutions Provider/td> HanaByte Company Development; open-source community efforts OpenTeams Company (Collective) Community Work Orders; Open Source Business Development; OSA Community Support Quansight Company Development; Usage Testing esp. from SciPyData ecosystem; open-source community efforts Veo Technologies Company Development; open-source community efforts ReferrsMe Company Development; open-source community efforts OTF Project Development; open-source community efforts Terrakube Project Development; open-source community efforts Kubestack Project Development; open-source community efforts Elastic2ls Project Development; open-source community efforts Layerform Project Development; open-source community efforts Mariano Rodríguez Individual Development; open-source community efforts Ted Parvu Individual Development; open-source community efforts Mike Hodgkins Individual Development; open-source community efforts Thomas Schuetz Individual Development; open-source community efforts Kelvin Soares Individual Development; open-source community efforts Chris Doyle Individual Development; open-source community efforts Alex Panayi Individual Development; open-source community efforts Sandro Manke Individual Development; open-source community efforts Dave Overall Individual Development; open-source community efforts Jeff Frasca Individual Development; open-source community efforts Jeff Wenzbauer Individual Development; open-source community efforts Alex Levinson Individual Development; open-source community efforts Michael Pursifull Individual Development; open-source community efforts Teodor Kostadinov Individual Development; open-source community efforts Patrick Jain-Taylor Individual Development; open-source community efforts Daniel Ristic Individual Development; open-source community efforts Debasish Mishra Individual Development; open-source community efforts Eddie Herbert Individual Development; open-source community efforts Curtis Vanzandt Individual Development; open-source community efforts Talal Tahir Individual Development; open-source community efforts Kevin Rathbun Individual Development; open-source community efforts David Douglas Individual Development; open-source community efforts Coin Graham Individual Development; open-source community efforts Jim Jagielski Individual Development; open-source community efforts and Open Source foundation experience Maciej Strzelecki Individual Development; open-source community efforts Ioannis Polyzos Individual Development; open-source community efforts Elvis McNeely Individual Development; open-source community efforts Yoaquim Cintron Individual Development; open-source community efforts Viktor Nagornyy Individual Open-source community efforts; Non-profit experience; Fundraising/Open Collective Ronny López Individual Development; open-source community efforts Khrist Hansen Individual Development; open-source community efforts Fatih Tokus Individual Development; open-source community efforts Bill Oberacker Individual Development; open-source community efforts Tiago Rodrigues Individual Development; open-source community efforts Nik Kotov Individual Development; open-source community efforts Nikolay Individual Development; open-source community efforts Simón Ramos Individual Development; open-source community efforts John Walsh Individual Development; open-source community efforts Zoltan Vigh Individual Development; open-source community efforts Ilyas Hamdi Individual Development; open-source community efforts Samuel Phan Individual Development; open-source community efforts Denis Vaumoron Individual Development; open-source community efforts Lawal AbdulLateef Individual Development; open-source community efforts Nils Knieling Individual Development; open-source community efforts Bruno Schaatsbergen Individual Development; open-source community efforts Aymen Segni Individual Development; open-source community efforts Luis M. Gallardo D. Individual Development; open-source community efforts Willi Carlsen Individual Development; open-source community efforts Lucas Tesson Individual Development; open-source community efforts Simon Effenberg Individual Development; open-source community efforts Ofer Chen Individual Development; open-source community efforts Arthur Busser Individual Development; open-source community efforts Ahmed Qazi Individual Development; open-source community efforts Oliver Shaw Individual Development; open-source community efforts Nikul Jain Individual Development; open-source community efforts Alex Torres Individual Development; open-source community efforts Rasmus Rask Individual Development; open-source community efforts Henare Degan Individual Development; open-source community efforts Vineet Pal Singh Rauniwal Individual Development; open-source community efforts Bruno Mattarollo Individual Development; open-source community efforts Mahesh Rijal Individual Testing; Documentation; open-source community efforts Thomas van Latum Individual Development; open-source community efforts Piotr Plenik Individual Development; open-source community efforts Nguyen Duy Phuong Individual Development; open-source community efforts Diego Cristóbal Individual Development; open-source community efforts and Open Source foundation experience Yasha Prikhodko Individual Development; open-source community efforts Allie Coleman Individual Development; open-source community efforts Scott A. Williams Individual Development; open-source community efforts Kevin Zheng Individual Development; open-source community efforts Gayan Hewa Individual Development; open-source community efforts Yadav Lamichhane Individual Development; open-source community efforts Wan Azlan Wan Mansor Individual Development; open-source community efforts Ben McNicholl Individual Development; open-source community efforts Minchul Joh Individual Development; open-source community efforts Karan Sharma Individual Development; open-source community efforts Colin Wilson Individual Development; open-source community efforts Dylan Hitt Individual Development; open-source community efforts Thomas Senay Individual Testing; Documentation Alik Khilazhev Individual Development; open-source community efforts Gary Mclean Individual Development; open-source community efforts David Jones Individual Development; Consultancy; Leveraging OS tools on behalf of clients Bob Rohan Individual Development; open-source community efforts Javier Ruiz Jimenez Individual Development; open-source community efforts Igor Rodionov Individual Development; open-source community efforts Sumeet Ninawe Individual Development; open-source community efforts Ravish Tiwari Individual Development; open-source community efforts; Helping teams adopt scalable Open Source IaC tools Alexander Sharov Individual Development; open-source community efforts Webert Lima Individual Development; open-source community efforts Mahsoud Badalbaev Individual Development; Research; Testing; Helping teams adopt scalable Open Source IaC tools August 15th, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=37133054",
    "commentBody": "The OpenTF ManifestoHacker NewspastloginThe OpenTF Manifesto (opentf.org) 554 points by CathalMullan 21 hours ago| hidepastfavorite317 comments dbingham 18 hours agoI appreciate the letter and trying to work with Hashicorp -- I used to have a ton of respect for Hashicorp. But honestly... at this point......just fork it into a foundation. Don&#x27;t wait for Hashicorp&#x27;s response. I get wanting to have the appearance of working with Hashicorp, but we&#x27;ve been shown again, and again, and again, and a-fucking-gain that private corporations cannot be trusted to maintain public goods. Only community governed non-profit foundations can do that.Private corporations will put the bottom line first every single time. And in the case of investor funded enterprises, the bottom line is never ending exponential growth or bust. reply fishnchips 18 hours agoparentMarcin here, co-founder of Spacelift.Even though I strongly believe the OpenTF fork could open up incredible possibilities for the community (I could go on and on about it), it is an equivalent of a civil war. It doesn&#x27;t serve the community and our only interest is in the continued strength of the community that we continue to build for.Based on my immense respect for what&#x27;s been built under Hashi&#x27;s umbrella I&#x27;d rather see a change of mind, and an opportunity to honor our pledge of resources (5 FTEs for 5 years) to the common rather than partisan cause. reply tedivm 18 hours agorootparentI really appreciate that, and I do think it&#x27;s right of you to at least make the attempt.That being said, I don&#x27;t expect this attempt to work and I fully believe that a fork is going to be inevitable. I also think a fork is an amazing opportunity to standardize the language and prioritize the features developers want.It isn&#x27;t just about the license, but the way that Hashicorp has maintained the Terraform project. The github insights show that they don&#x27;t have nearly as many people working on it as I would expect, and most of them are split into also working on Terraform Cloud. At the same time they don&#x27;t work with the community that well- there are open issues and pull requests that just get ignored as Hashicorp clearly doesn&#x27;t see value in open source contributors. This isn&#x27;t just a Terraform issue either- my company had to move off of nomad due to the lack of development and support (as well as broken features).I have strong concerns about the future of these projects in general beyond just the licensing. An open foundation that had multiple companies involved would by definition need to find a way for those people to collaborate together, and once they do that it makes it easier for them to invite community collaboration. So while I do appreciate that it is a drastic step, I think it&#x27;s one that would also be far better for the ecosystem and project as a whole.That said, maybe this is the wake up call hashicorp needs to fix these problems. If you provide five FTEs that basically doubles the size of their Terraform development team (they have more people working on it than five, but those people are split into other projects), and once they start working with other groups maybe they&#x27;ll work with the community more as well. I&#x27;m not holding my breath though. reply lamontcg 17 hours agorootparentSmells like the end of Chef. Management doesn&#x27;t understand how much it takes to maintain the open source project and is just pouring resources into sales and marketing and products that they can charge for, and don&#x27;t see how that erodes goodwill and the technological foundation of the company. reply miah_ 15 hours agorootparentI also saw that parallel with Chef. I think its the story of all VC funded software that attempts to be \"Open Source\". For them, Open Source means \"You can read the source code, and potentially fix a bug\", for us, it means community, transparency, and fixing bugs beyond those your paying customer has.I looked at github &#x2F;chef&#x2F;chef and github &#x2F;inspec&#x2F;inspec and its the same as it was shortly after I left. The only changes are from the one person who carried over after the sale to Progress, and the contracting team out of India, with dozens are unanswered queries and pull requests from the community.What really ruffles my feathers was when they had us define oss-practices (https:&#x2F;&#x2F;github.com&#x2F;chef&#x2F;chef-oss-practices), clearly nobody outside our small team read (or understood) those words and goals. It feels like it was work to make us look better in OSS in order to bolster the company sale. reply lamontcg 14 hours agorootparentThere was a whole lot of community window dressing going on. I still wonder if they weren&#x27;t trying to ship maintenance of the open source code off onto the community thinking that if all that worked appeared (or thinking that it was actually going on--believing their own bullshit about how involved the community was) that they could just leach that work.There&#x27;s probably some manager at Hashi right now trying to argue that they should offload TF maintenance entirely onto the community and they should pivot to hosting services and consulting and making money off of all that free work. reply hdaz0017 14 hours agorootparentprevchef said, did and tried some really dumb stuff and lots of it failed for obvious reasons, it&#x27;s like docker took a chunk of their playbook and their business went the same way.It&#x27;s all good and \"fun\" on the IPO up.(in x months we will rewrite the whole universe). reply FrenchTouch42 13 hours agorootparentprevThank you Miah for all your contributions (beyond InSpec) reply busterarm 17 hours agorootparentprevHashicorp isn&#x27;t going to budge here. The same argument that you&#x27;ve made about Terraform being the underpinnings and needing to be open-sourced can be applied to their other important products like Vault, Consul and Nomad as well. The ecosystem of those three is plainly a direct competitor to Kubernetes which is open-source.There&#x27;s really no move for them to make here. It&#x27;s unfortunate. reply fishpen0 15 hours agorootparentTons of organizations run vault and consul as part of their k8s ecosystem so they don&#x27;t directly compete. The vault CSI driver might be the single most installed CSI driver across all the orgs I&#x27;ve worked for reply busterarm 15 hours agorootparentYou&#x27;re completely missing it.If you are running Nomad as your orchestrator, because of the tight integrations you are almost certainly running vault for secrets and consul for service discovery&#x2F;service mesh. The ecosystem of the three is the competitor to K8s.s&#x2F; someone who runs both ecosystems at scale. reply twunde 14 hours agorootparentWhile the Nomad stack is a direct competitor to k8s, Consul and Vault are both heavily used alongside k8s. In fact, Consul had features that were only for k8s the last time I checked reply busterarm 14 hours agorootparentWhile these facts are true, that&#x27;s totally not the fucking point and has nothing to do with the argument. I say again.You can have software that both supports and competes with the k8s ecosystem. That&#x27;s even the same type of problem all of these companies have with Hashicorp software now under the BSL.Gruntwork builds tools that everyone using Terraform uses but they also offer services that Hashicorp would prefer that you pay them for instead.You telling me \"but people running K8s also use vault&#x2F;consul\" is like telling me that Gruntwork makes terragrunt which terraform users use. It doesn&#x27;t mean that Hashicorp doesn&#x27;t view them as a threat. reply hdaz0017 13 hours agorootparentThere is a big difference though Terraform is the out and out winner in its market.All their other products are at best small x% share of a crowded market or dominated by another product. reply ghshephard 12 hours agorootparentGenuinely curious - other than Vault - what other product is there for secret management in the cloud infrastructure space. I get that CyberArk Conjur is big in the enterprise space, but I thought cloud users, even with k8s, mostly went with vault. reply candiddevmike 11 hours agorootparentMost companies are probably using the secret manager provided by their cloud platform instead of Vault, especially after they tried to buy Vault. reply Alupis 10 hours agorootparentWhich is more than likely Vault + new carpets and paint. reply xyzzy_plugh 7 hours agorootparentNo, it&#x27;s astronomically unlikely to be Vault at all. reply oneplane 11 hours agorootparentprevYou generally only run Vault if your secrets don&#x27;t&#x2F;won&#x27;t&#x2F;can&#x27;t reside on a public cloud service. reply Alupis 9 hours agorootparentThere&#x27;s a lot more reasons to run Vault than those.Having a standardized way to \"do secrets\" for any team, any service, any app within the organization is very nice. Becoming cloud-agnostic for your secrets (connecting your local Vault with the cloud provider&#x27;s vault) is another great benefit. Automatic secret rotation is also another great benefit. Secret versioning and auditing... etc.It&#x27;s not just \"can&#x27;t have this secret in VCS or viewable via kubectl\". replykragen 12 hours agorootparentprevit&#x27;s not warhashicorp has decided they don&#x27;t want to contribute to open source any morethey&#x27;re totally within their rights to do so, and it doesn&#x27;t harm anybody; it&#x27;s not the equivalent of going around blowing up buildings, raping women, and napalming children. at most we can wish they had continued doing the beneficial things they were previously doingmaybe they&#x27;ll change their minds, as you say, but that&#x27;s no reason for the community to sit around twiddling its thumbs hoping for such a change. what&#x27;s important now is that the people who are still willing to cooperate can do so successfully, and that&#x27;s what opentf is aboutthat&#x27;s even more obviously not the equivalent of going around blowing up buildings, raping women, and napalming children. it&#x27;s very much the opposite, in factit&#x27;s unclear to me which of the parties you intend to accuse of doing the moral equivalent of burning innocent people alive en masse, but either way, maybe you should think about walking back that rhetoric a bit reply fishnchips 11 hours agorootparentI apologize if I appeared to downplay the horror of actual war. I&#x27;m lucky to never have experienced it first-hand.I mainly wanted to focus on the \"civil\" than \"war\" aspect of the possible schism. reply anyoneamous 11 hours agorootparentFWIW there are still people (dozens of us!) who don&#x27;t feel the need to be artificially offended by a bit of hyperbole on an internet forum. reply kragen 5 hours agorootparentmy objection is not that mass graves, piles of mangled bodies, your close friends unexpectedly disappearing into pink mist, and terrible stenches are too sacred to be used as a metaphor for something elsemy objection is that warfare involves people intentionally harming each other, and that doesn&#x27;t seem to be what&#x27;s going on here. it&#x27;s not that war is a more extreme version of the situation; it&#x27;s that it&#x27;s directionally differentrather, hashicorp is struggling to not go bankrupt, so they&#x27;ve decided to switch to making a proprietary product instead of an open-source products; and terraform users, naturally enough, are reluctant to make their infrastructure vulnerable to a proprietary software license. hashicorp is not intentionally harming terraform users, and terraform users are not intentionally harming hashicorpthey&#x27;re just not continuing their previous cooperation reply bifrost 10 hours agorootparentprevI concurr. reply kragen 11 hours agorootparentprevit&#x27;s not a possible schism. hashicorp has clearly and unmistakably abandoned the open source community. conceivably they&#x27;ll change their minds, but their communication doesn&#x27;t have any ambiguity in iti have no idea what you could possibly mean by &#x27;focus on the \"civil\"&#x27;. it&#x27;s good that people are being civil to one another, isn&#x27;t it? then why are you criticizing them? reply tedivm 10 hours agorootparentYou&#x27;re missing the point- the OpenTF group wants to mend the schism if possible, by getting Hashicorp to change their licensing back. If they immediately fork then that&#x27;s not likely to happen, so they&#x27;re attempting this first.I don&#x27;t think it will work, but I think it&#x27;s good of them to try. reply kragen 9 hours agorootparentmending the schism would be great, but we probably can&#x27;t do that by pretending it doesn&#x27;t exist like https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37139929, or analogizing it to blowing thousands of children&#x27;s extremities off, or analogizing acknowledging its existence to blowing thousands of children&#x27;s extremities offI&#x27;m still not sure which of the latter two was the intent of the comment I was responding to replyPeterZaitsev 16 hours agorootparentprevHi Marcin,I recognize you&#x27;re in interesting position in Spacelift. Per your recent analyses you may not be impacted and in this case you probably do not want to pissoff Hashicorp folks :)In reality though force better be responded with force and showing Hashcorp what what was Terraform will be successful as Open Source project with or without them is best way to get them to reconsider. reply jeffchao 12 hours agorootparentprevAs a user and collaborator of TACOS, agreed that it could open up opportunities. Though I echo trade-offs (as in other replies) that it could start a civil war that makes it difficult for end users and collaborators -- reminds me of Python2 -> 3, Presto&#x2F;Trino, and many other stories. Pledging resources is a great approach. reply RcouF1uZ4gsC 13 hours agorootparentprev> it is an equivalent of a civil war.Why. It is open source. A fork should be no big deal, and definitely not a “civil war”. I think the community should be quicker to fork open source projects that are not serving the needs of the community.The corporations are trying to have the benefits of open source without the responsibility. Forking is a normal, acceptable part of open source and we should normalize it. reply enneff 12 hours agorootparentWhat would it mean to “normalise” forking? The costs of maintaining a fork are significant, and if one group of programmers are being funded to work on the project then it can be very difficult to fork a project in any meaningful way without significant resources behind it.Also IIUC most of the parties in this conversation are corporations. They’re all trying to enjoy the benefits of open source development for a variety of reasons. reply lmm 9 hours agorootparent> The costs of maintaining a fork are significantWhy? If we have tooling or workflows that assume forks don&#x27;t happen, maybe we can fix those. Forking should be cheap, easy and frequent. reply throwaway2037 37 minutes agorootparentI assume OP meant cost == community. That is the hard part of a fork. reply waynesonfire 5 hours agorootparentprevRight below your comment was another comment that started with,\"The truth is that a fork hurts everyone.\"So, which is it? reply lmm 3 hours agorootparentCurrently forks are painful, because they aren&#x27;t normalized i.e. our tools and workflows don&#x27;t expect them. I&#x27;m saying rather than discouraging forks we should adapt our tools and workflows to expect them. reply ericb 46 minutes agorootparentSure, tooling could be better.But the real work is all the hard work that goes into a fork. I&#x27;ve watched open forks die all the time--all it takes is no one to step up and do&#x2F;pay for the work, which is basically the default, because it is in everyone&#x27;s interest if someone else is the one to do that.I think that&#x27;s really the crux of the problem--there are plenty of folks willing to maintain software for money, and a whole lot of people who&#x27;d rather it not cost money and if it does, not their money.If the tooling is better, who is going to maintain this? replybrikis98 18 hours agoparentprevThe truth is that a fork hurts everyone.Imagine a future CTO trying to pick the IaC tools for their company. They see Terraform as an option, but then learn there are multiple forks, licensing questions, and a big battle happening in the community. What do they do? They are now way more likely to pick a different tool that is genuinely open source. The same is true of every dev considering where to build their career, every hobbyist, every open source enthusiast, every vendor, etc. In the end, no matter which fork wins, everyone will be worse off: the community will be smaller and more splintered.So we opted to ask HashiCorp do the right thing first. If they choose to do the right thing, we can avoid a fork, and avoid splintering the community. We still think that&#x27;s the best option. But if that doesn&#x27;t work, then a foundation + fork it is. reply i_am_jl 16 hours agorootparentnext [–]Imagine a future CTO trying to pick the IaC tools for their company. They see Terraform as an option, but then learn there are multiple forks, licensing questions, and a big battle happening in the community. What do they do?I truly believe that a CTO who sees Terraform as an option and who isn&#x27;t scared off by the BSL, but then has all of these other concerns, exists only in fantasy. reply verdverm 16 hours agorootparentLots of people still using elastic, mongo, and redis.What&#x27;s different about this one? reply i_am_jl 14 hours agorootparentnext [–]You may make production use of the Licensed Work, provided such use does not include offering the Licensed Work to third parties on a hosted or embedded basis which is competitive with HashiCorp&#x27;s products.Read benevolently it&#x27;s a prohibition from spinning up a service based on HashiCorp&#x27;s code and undercutting HashiCorp&#x27;s pricing.On the other hand, if I build a product with HashiCorp-owned BSL&#x27;d code, then HashiCorp releases&#x2F;acquires a product that competes with mine, then my license is void. reply verdverm 13 hours agorootparentMy understanding is that the aforementioned companies&#x27; licenses are to the same effect, so what is the difference? reply i_am_jl 12 hours agorootparentRedis is 3-clause BSD, BSD does not have a \"your license is void if you sell a product that competes with us\" clause. Redis does have enterprise products that are licensed in a manner similar to BSL, but Redis itself is not.MongoDB and Elastic are SSPL. SSPL approaches the problem like the AGPL; it compels licensees who sell a service derived from the software to make available under the SSPL the source of all supporting tooling and software so that a user could spin up their own version of the service.There&#x27;s an argument to be made that SSPL is de facto \"you can&#x27;t compete with us\" since it would be more challenging to make a competitive SaaS offering if your whole stack is source available. I don&#x27;t disagree. However, as distasteful as SSPL is, at least it doesn&#x27;t grant licensing to a product conditionally on the unknowable future product offerings of HashiCorp. reply verdverm 12 hours agorootparentthanks for the explanation, my understanding is that they are all after limiting competition in various ways, while still trying to maintain the mantle of open sourceWe are certainly in interesting times around the monetization &#x2F; financial sustainability of open source reply bit_flipper 12 hours agorootparentprevSSPL has no provision even close to the reach of the \"anti-competition\" clause Hashicorp is using. While SSPL is not considered open source, it isn&#x27;t that far off from the AGPL. The difference between SSPL and AGPL is that SSPL (1) is in effect regardless of modification of the service and (2) extends copy left virality to all programs which support running the service, including those that interact with the software over a network.MongoDB, Elastic, etc. cannot stop you from running a competitor based on the terms of their licenses, they just ask that you publish the source code for whatever service you&#x27;re running in its entirety (I acknowledge there are disagreements about how far \"entirety\" extends). The clause in Hashicorp&#x27;s license actually revokes the right to use their software at all if you&#x27;re a direct competitor.OK, no one is going to build an open source competitor to Elastic or MongoDB because then you have no moat and your business will probably fail, I get it, but it&#x27;s still possible to do without repercussion. It&#x27;s not like the AGPL is that far off in terms of limitation, either, which is why you don&#x27;t see many copyleft services run by large corporations unless they&#x27;ve been dual-licensed. reply vosper 15 hours agorootparentprevJust went with Elastic cloud after evaluating both Elasticsearch and OpenSearch. It was an easy choice to stick with the incumbent&#x2F;creator that I was familiar with. No complaints so far. reply verdverm 15 hours agorootparentWe just went back to TF after giving Pulumi a try. Prefer declarative syntax for infra and more abuse of Yaml (\"fn::...\" here) is not what I&#x27;m after.We are working on wrapping TF in CUE since you can CUE->JSON->TFhttps:&#x2F;&#x2F;github.com&#x2F;hofstadter-io&#x2F;cuelmMany more CUE experiments are going on in the devops space reply AaronFriel 12 hours agorootparentPulumi has a few languages other than YAML and Pulumi is declarative[1], and the programs you write are only as complex as you want them to be. This python program declares an S3 bucket and declares ten objects to exist in it. from pulumi_aws import s3 bucket = s3.Bucket(&#x27;bucket&#x27;) for i in range(10): s3.BucketObject( f&#x27;object-{i}&#x27;, s3.BucketObjectArgs( bucket=bucket.id, key=str(i), ) )Even so, Pulumi YAML has a \"compiler\" option, so if you want to write CUE or jsonnet[1], or other[2] languages, it definitely supports that.Disclaimer: I led the YAML project and added the compiler feature at the request of some folks internally looking for CUE support :)[1] https:&#x2F;&#x2F;www.pulumi.com&#x2F;blog&#x2F;pulumi-is-imperative-declarative...[2] https:&#x2F;&#x2F;www.pulumi.com&#x2F;blog&#x2F;extending-pulumi-languages-with-...[3] https:&#x2F;&#x2F;leebriggs.co.uk&#x2F;blog&#x2F;2022&#x2F;05&#x2F;04&#x2F;deploying-kubernetes... reply verdverm 11 hours agorootparentI&#x27;m aware of the SDKs, but we don&#x27;t want them because they are an imperative interface, no matter how you want to spin it as \"declarative\". I have access to all the imperative constructs in the underlying language and can create conditional execution without restriction.Even if I use the Yaml compiler for CUE (which we did) I still have to write `fn::` strings as keys, which is ugly and not the direction our industry should go. Let&#x27;s stop putting imperative constructs into string, let&#x27;s use a better language for configuration, something purpose built, not an SDK in an imperative language. These \"fn::\" strings are just bringing imperative constructs back into what could have been an actual declarative interface. Note, Pulumi is not alone here, there are lots of people hacking Yaml because they don&#x27;t know what else there is to do. CEL making it&#x27;s way to k8s is another specific example.This cannot be the state-of-art in ops, we can do much better, but I get that Pulumi is trying to reach a different set of users than devops and will end up with different choices and tradeoffs(I maintain https:&#x2F;&#x2F;cuetorials.com and am very active in the CUE community) reply lmm 9 hours agorootparentprevAn imperative for loop is somehow declarative now? Lol. reply jjnoakes 8 hours agorootparentThis seems extremely dismissive and shallow.The imperative part of that code appears to be analogous to templating. The actual work done under the covers is not imperative, but is based on the difference between the result of the template execution and the current state of the system. That&#x27;s what makes it declarative. reply verdverm 8 hours agorootparentIt really depends on the interaction between the user&#x27;s Pulumi script and the Pulumi engine.If there is more than one back and forth, you become declarative, even if you imperatively generate a \"declarative\" intermediate representation (not really sure what state file at a point in time could ever be imperative), you then would get back some data from the engine, then make choices about what to send off to the engine in the next request.It&#x27;s important to understand that with Pulumi, you can end up in either situation. You have to be careful to not become imperative overall is probably the better way to consider this.https:&#x2F;&#x2F;www.pulumi.com&#x2F;docs&#x2F;languages-sdks&#x2F;javascript&#x2F;#entry...Another way this can break down is if the user writes code to call the same APIs in the middle of a Pulumi script. I meant to try this myself to verify it works, but I would assume that Pulumi is not stopping me from doing something like this. reply jjnoakes 8 hours agorootparentIn general maybe, but in the specific context above, I think calling that loop declarative is accurate, and laughing at that classification is a poor response rooted in a deep misunderstanding. reply verdverm 7 hours agorootparentnext [–]import pulumi from pulumi_gcp import storage bucket = \"hof-io--develop-internal\" name = \"pulumi&#x2F;hack&#x2F;condition.txt\" cond = False msg = \"running\" cnt = 0 while not cond: cnt += 1 key = storage.get_bucket_object_content(name=name, bucket=bucket) print(cnt, key.content) if key.content == \"exit\": msg = \"hallo!\" break pulumi.export(&#x27;msg&#x27;, msg) pulumi.export(&#x27;cnt&#x27;, cnt)--- 769 exit 770 exit 771 exit 772 exit 773 exit 774 exit 775 exit Outputs: cnt: 775 msg: \"hallo!\" Resources: + 1 to create info: There are no resources in your stack (other than the stack resource). Do you want to perform this update? [Use arrows to move, type to filter] yes > no details----Of note, all but the last exit had a newline, until I `echo -n` the file I copied up---ooo... 348 what?!?! 349 what?!?! 350 what?!?! 351 what?!?! 352 what?!?! 353 what?!?! 354 what?!?! 355 what?!?! 356 what?!?! 357 what?!?! 358 what?!?! 359 exit Outputs: cnt: 359 msg: \"hallo!\" Resources: + 1 created Duration: 27s---I uploaded a different file while waiting to be asked to continue, and then proceeded to get different outputsNote, while I can get the contents of a bucket in TF, I cannot build a loop around it as I have abovehttps:&#x2F;&#x2F;registry.terraform.io&#x2F;providers&#x2F;hashicorp&#x2F;aws&#x2F;latest...TF might be susceptible to the same file contents manipulation between plan & apply as well, but then again, you can save a plan to a file and then run it later, so maybe not? Another experiment seems to be in order reply AaronFriel 4 hours agorootparentI think this is an advantage of Pulumi, here are two use cases:1. Creating a resource where created is not the same as ready. This is extraordinarily common with compute resources (a virtual machine, a container, an HTTP server, a process) where attempting to create follow-up resources can result in costly retry-back-off loops. Even when creating Kubernetes resources, Pulumi will stand up an internet-connected deployment more quickly than many other tools because you can ensure the image is published before a pod references it, the pod is up before a service references it, and so on. (The Kubernetes provider bakes some of these awaits in by default.)2. Resources graphs that are dynamic, reflecting external data sources at the moment of creation. Whether you want to write a Kubernetes operator, synchronize an LDAP directory to a SaaS product, or one of my favorite examples. When I set up demos, I often configure the authorized public IPs dynamically: import * as publicIp from &#x27;public-ip&#x27;; new someProvider.Kubernetes.Cluster(&#x27;cluster&#x27;, { apiServerAccessProfile: { authorizedIPRanges: [await publicIp.v4()], enablePrivateCluster: false, }, } reply verdverm 7 hours agorootparentprevI was just wondering what stops me from reading and writing to a cloud bucket like an infinite tape?https:&#x2F;&#x2F;www.pulumi.com&#x2F;registry&#x2F;packages&#x2F;gcp&#x2F;api-docs&#x2F;storag... reply lmm 8 hours agorootparentprev> This seems extremely dismissive and shallow.When someone tries to make a sophisticated argument that up is down and white is black, dismissive and shallow is the right response.> The actual work done under the covers is not imperativeHaving a declarative layer somewhere in the stack doesn&#x27;t make something declarative, if that&#x27;s not the layer you actually use to work on and reason about the system. See the famous \"the C language is purely functional\" post. reply Pet_Ant 3 hours agorootparentI’m guessing this is the reference?http:&#x2F;&#x2F;conal.net&#x2F;blog&#x2F;posts&#x2F;the-c-language-is-purely-functio... reply verdverm 9 hours agorootparentprevyou can have loops and still be declarative, CUE has loops, though they are considered comprehensions more technically, but there is no assignment or stack in CUEOne of the interesting aspects of CUE is that it gives us many of the programming constructs we are used to, but remains Turing incomplete, so no general recursion or user defined functions. There is a scripting layer where you can get more real world stuff done tooThe CUE language is super interesting, has a very unique take on things and comes from the same heritage as Go, containers, and Kuberneteshttps:&#x2F;&#x2F;cuelang.orghttps:&#x2F;&#x2F;cuetorials.com replyhdaz0017 13 hours agorootparentprevHopefully it&#x27;s not down to CTOs to be picking tools for their company but a process within DevOps&#x2F;Engineering teams etc.Does anyone else see this as the Nagios Effect all over again, there must be lots to learn from history? reply Atotalnoob 11 hours agorootparentWhat is the nagios effect? reply pxc 7 hours agorootparentI didn&#x27;t know either, so I did some Googling and found an old announcement[1] from 2009:> A group of leading Nagios protagonists including members of the Nagios Community Advisory board and creators of multiple Nagios Addons have launched Icinga – a fork of Nagios, the prevalent open source monitoring system. This independent project [is based upon a] broader developer community. [...] Icinga takes all the great features of Nagios and combines it with the feature requests and patches of the user community.It also looks like in 2014, Nagios centralized and appropriated a domain name and website used for hosting Nagios plugins, away from the community (its plugin developers)[2]:> In the past, the domain \"nagios-plugins.org\" pointed to a server maintained by us, the Nagios Plugins Development Team. The domain itself had been transferred to Nagios Enterprises a few years ago, but we had an agreement that the project would continue to be independently run by the actual plugin maintainers.¹ Yesterday, the DNS records were modified to point to web space controlled by Nagios Enterprises instead. This change was done without prior notice.> To make things worse, large parts of our web site were copied and are now served (with slight modifications²) by . Again, this was done without contacting us, and without our permission.> This means we cannot use the name \"Nagios Plugins\" any longer.There&#x27;s some previous discussion of those controversies on HN here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9452013From that article[3]:> [Icinga developer]: \"Six months before the fork, there was a bit of unrest among Nagios&#x27; extension developers [...] Community patches went unapplied for a long time[.]\"> [...]> Two years ago, more or less when the split happened, [Nagios author] was having problems resolving [trademark] issues with a company called \"Netways\".I&#x27;m still not sure what the effect is supposed to be tbh.--1: https:&#x2F;&#x2F;icinga.com&#x2F;blog&#x2F;2009&#x2F;05&#x2F;06&#x2F;announcing-icinga&#x2F;2: https:&#x2F;&#x2F;www.monitoring-plugins.org&#x2F;archive&#x2F;devel&#x2F;2014-Januar...3: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20160314090137&#x2F;http:&#x2F;&#x2F;www.freeso... reply Spivak 14 hours agorootparentprevI don&#x27;t get this one, you pick OpenTerraform and get on with your life. It&#x27;s the same with picking OpenSearch over Elastic. I can use the proprietary version that locks me into a single profit-seeking vendor and doesn&#x27;t have community backing or the one run by a foundation made up of companies that use and are heavily invested in Terraform. reply linuxdude314 17 hours agorootparentprevI doubt that’s what would happen if they could afford a license from Hashicorp.Avoiding proprietary licenses has its place but if you aren’t using terraform to build a product this really shouldn’t impact you much. reply aldarisbm 14 hours agorootparentShouldnt impact you much. _yet_ reply PeterZaitsev 16 hours agoparentprevI totally agree. I do not think pleading with Hashicorp to reconsider will result in changing back the licenseDoing the Fork and showing it IS sustainable and has broad community support can encourage Hashicorp to make concessions.After taking this unilateral hostile step I do not think Hashicorp deserves the community trust and what industry needs is \"Foundation Governed\" Terraform like solution, whatever name this solution will have.You can see example in Confluenct which builds proprietary solutions around Kafka, where Kafka itself is Apache project. reply xinayder 3 hours agoparentprevWhy not get it under the umbrella of either the Linux Foundation or CNCF? Things like this and Ansible should be really kept under neutral companies and not companies like Red Hat and HashiCorp that have shown that all they care about open source is the free work they get from contributors. reply VectorLock 10 hours agoparentprevI don&#x27;t think you can just \"fork it.\" Hashicorp already owns all that code because they make contributors sign it away to them. reply pxc 7 hours agorootparentThey can&#x27;t retroactively take source code away from people who they already granted access to it under the MPL. The old code is still available under the MPL forever- even if they take down all of their own public copies of it, anyone with the old Terraform code is still free to upload their copy for the creation of a new fork. That&#x27;s kinda the whole idea with these open-source licenses :) reply VectorLock 4 hours agorootparentI&#x27;ve heard some people discuss that the contribution agreement that Hashicorp makes people sign gives them the right to change the license for existing contributions, but I&#x27;m not a lawyer so I really couldn&#x27;t say for certain either way. reply linuxandrew 1 hour agorootparentHashiCorp makes its external contributors sign a CLA to basically hand over the copyright.However the MPL and licensing in general is irrevocable. They have irrevocably licensed Terraform 1.5.5 under the MPL and an enterprise license (dual license). Anyone can use, modify and distribute version 1.5.5 under the terms of the MPL.Since HashiCorp retains full copyright they can release the next version under the BSL.Note that many free software projects (like Linux) don&#x27;t have a CLA which makes relicensing impractical since every contributor would have to agree to it. reply nprateem 15 hours agoparentprevIt was never a public good. reply dbingham 15 hours agorootparent> In economics, a public good (also referred to as a social good or collective good)[1] is a good that is both non-excludable and non-rivalrous. For such goods, users cannot be barred from accessing or using them for failing to pay for them. Also, use by one person neither prevents access of other people nor does it reduce availability to others.Any free open source product qualifies as a public good. It is free for all to use, and one person using it does not exclude anyone else from using. reply candiddevmike 18 hours agoparentprevSpeaking of nuclear options, need to get the providers to pledge&#x2F;follow the fork, maybe via some kind of API incompatibility. Terraform is useless if the providers don&#x27;t work with it and only the fork. Focus on disrupting the ecosystem. reply bickfordb 14 hours agorootparentThis is the interesting part of all of this. The meat of Terraform is in its provider ecosystem. Anyone can make a new frontend (or even fork the existing one?), get rid of all the warts, add the missing encryption features gated under enterprise and have a much better tool. reply tedivm 17 hours agorootparentprevHashicorp left the provider frameworks under the original licenses, probably because they don&#x27;t want to scare provider developers off. So for now both Terraform and a potential fork can continue sharing the same providers without issue. reply dividedbyzero 17 hours agorootparentAre they even able to unilaterally relicense third party providers? reply throwaway1101z 16 hours agorootparentIndirectly, by changing the license for the SDK. replybcantrill 16 hours agoprevWe at Oxide were honored to be asked to add our name to OpenTF Manifesto. Our statement:At Oxide, our vision has been that on-premises infrastructure is deserving of a system consisting of both hardware and software, at once integrated and open. Ensuring Terraform users can easily deploy to Oxide has been essential for realizing this vision: we want customers of an Oxide rack to be able to use the tools that they know and love! And while HashiCorp&#x27;s move to the BSL does not immediately affect Oxide (our Terraform provider is and remains MPLv2), we recognize that the ambiguity in both the license and HashCorp&#x27;s language has created widespread concern that gives customers pause. We support the OpenTF efforts to assure an open source Terraform. It is our preference to see an MPLv2 Terraform as led by HashiCorp, and we join the call from the OpenTF signatories for HashiCorp to renew its social contract with the community by reverting the change of Terraform to the BSL. That said, we also agree with OpenTF&#x27;s fallback position: a BSL-licensed Terraform is not in fact tenable; if Terraform must be forked into a foundation to assure its future, we will support these efforts. Open source comprises the foundation of the modern Internet, and is bigger than one company: it is the power of us all together to determine our fate. But we cannot take that foundation for granted -- and we must be willing to work to exercise that power to assure an open source future.Thank you to the consortium here that is coming together to guide Hashi to see the wisdom in an open source Terraform! reply diarrhea 14 hours agoparentJust scrolled through your open job postings, and the Control Plane opening is jaw-droppingly interesting. Sounds like a marvelous mission you&#x27;re on. I&#x27;m wishing you all the best and will make sure to check back in with Oxide&#x27;s progress! reply hbogert 14 hours agoparentprevI can rest easy now that my take on this aligns with Oxide&#x27;s and I&#x27;m not being sarcastic. I&#x27;ve been following the podcast for 2 years now, and you guys are spot on with your stance on opening up and keeping things Open.It would be ironic though if Hubris ever gets relicensed reply lawnchair 15 hours agoparentprevThanks Bryan! reply sausagefeet 15 hours agoparentprevAs a huge Oxide fan, thank you for the support! reply bloopernova 14 hours agoprevIf any Hashicorp people are reading, can you please tell your middle and senior management that this decision has deeply soured my entire DevOps cohort on continuing to use Terraform in the future.We&#x27;re already exploring alternatives. Future client projects may not use Terraform at all.Languages and frameworks must remain open or they will wither and die. reply bayindirh 14 hours agoparentAs a Free Software advocate and supporter, I&#x27;m thinking about the answer to this question:- MPL is a weak-copyleft license, which allows companies to grab and run Terraform codebase, provide it as-is (as Terraform), or as white-labeled Terraform compatible feature&#x2F;layer. This is alright (because license allows this).- These people also contribute their own fixes upstream, which is great, and maintain their own patches if Hashicorp decides to reject them (which is fine, too, this is how ecosystem works).But, HashiCorp says that, the thing we develop (i.e. Terraform) is used by others and generate revenue for them, this is great, but we can&#x27;t generate enough revenue from it to keep the company afloat and continue providing TerraForm development, and sell it as a product at the same time.What should they do? I&#x27;d advocate AGPL, but xGPL licenses are avoided like a plague, because Free Software is not \"closed forks\" friendly, and companies hate to open everything like that.BSL is neither Free nor Open, which we all hate, but it allows HashiCorp to survive to a degree (this is not a fact, this is what HashiCorp is thinking).So, just because people adapted it, and HashiCorp cannot survive, should they say, we&#x27;re closing shop, it&#x27;s all MIT now, do whatever you want, bye!?Weak copyleft licenses are not designed for software that big. Or they assume that the developing party is untouchable. Strong copyleft solves this, but companies hate it because its unrelenting transparency.What should we do?P.S.: I neither endorse, nor support BSL, or HashiCorp&#x27;s decision (or any company treads the same path).Edit: I mistyped MPL as permissive instead of weak-copyleft. Corrected, sorry. reply jaggederest 14 hours agorootparent> But, HashiCorp says that, the thing we develop (i.e. Terraform) is used by others and generate revenue for them, this is great, but we can&#x27;t generate enough revenue from it to keep the company afloat and continue providing TerraForm development, and sell it as a product at the same time.> What should they do?Suck it up, open source Terraform under a non profit foundation, find a new source of revenue. Or stop developing Terraform, cut expenditures, and move on with life.There&#x27;s no universe where \"bait and switch customers who wanted open source into paying us by switching licenses\" is a viable option.> So, just because people adapted it, and HashiCorp cannot survive, should they say, we&#x27;re closing shop, it&#x27;s all MIT now, do whatever you want, bye!?Exactly, you know the answer, you just don&#x27;t like the implications. People somehow think that a business which started an open source project \"deserves\" to profit from it. They do not. Open source is a great way to get people to know who you are and build things that are interoperable with your (proprietary, closed source) SaaS offerings. It is not in itself a revenue source.If the viability of your business is predicated on being the only one able to provide your project as a service and earn that service revenue gravy, just leave it closed source and proprietary. Sure, you won&#x27;t get adoption at anywhere near the rate, but that&#x27;s the tradeoff you make. reply stu2b50 13 hours agorootparent> Or stop developing Terraform, cut expenditures, and move on with life.How would that be an improvement to anyone in any way? If you want, you can just pretend that Terraform is dead and Hashicorp will never push another commit for it. The people who can make the compromises BSL has can continue to use it.> If the viability of your business is predicated on being the only one able to provide your project as a service and earn that service revenue gravy, just leave it closed source and proprietary. Sure, you won&#x27;t get adoption at anywhere near the rate, but that&#x27;s the tradeoff you make.I don’t understand why this is a binary. If the conditions of the BSL are unacceptable to YOU that’s fine, just pretend it’s closed source if you wish. For others, that the BSL isn’t completely proprietary is useful for them - let it be useful. Your wishes need not dictate everyone else’s. reply bayindirh 13 hours agorootparentprev> Exactly, you know the answer,No. It was not a rhetorical device.> you just don&#x27;t like the implications.I don&#x27;t know which implications you&#x27;re talking about, it was a question without prejudice or load.> People somehow think that a business which started an open source project \"deserves\" to profit from it.I do not agree. I don&#x27;t hold a position stating that \"TF should stay open, and companies should profit from it while giving it patches if you feel like it\". I&#x27;m the opposite, and I find the approach to permissive licenses as \"Ooo... Free tool to build a new product on and profit\" as unethical to begin with. I put anything and everything I put out as A&#x2F;GPLv3 or GFDL, because I produce that code for myself, on my free time, and I don&#x27;t have a secret desire for it to be forked and closed down for internet cookie points.If you want to use my tool for any reason (which are not very sophisticated to begin with), comply with GPL, or roll your own. I. Don&#x27;t. Care.I also pay for tons of things. Docker, cloud storage, programming fonts I use, anything I deem worth the money they&#x27;re asking for.I also use Vagrant a lot, share my Vagrantfiles (again under GPLv3), and if they begin to charge like Docker, I&#x27;ll pay for it, if I deem it&#x27;s worth the money they ask for.However, at the end of the day, I&#x27;m a Free Software advocate. I use Free Software to the extent possible, and develop my software as Free Software. Not Open Source software under some permissive license to be grabbed and forked to death. reply spencerflem 13 hours agorootparentprevHow is them shutting down and them continuing under a closed source licence any different for you? reply pxc 10 hours agorootparentprev> What should they do? I&#x27;d advocate AGPL, but xGPL licenses are avoided like a plague, because Free Software is not \"closed forks\" friendly, and companies hate to open everything like that.They can offer it under more than one license if they want. I, for one, would be pretty happy if they offered it under both BuSL and AGPL. - business users who are afraid of copyleft could use Terraform under the terms of the BuSL - the F&#x2F;OSS community could distribute it under the terms of the AGPL and freely build on it - competitors to Hashicorp would have a choice: + open their whole stack and compete in the market based on the quality of their services and support alone (admirable but very tough) + negotiate and pay HashiCorp to license Terraform under special, proprietary termsProbably, many of the same companies who want to fork Terraform now would still want to. But I&#x27;d be satisfied and it would likely shift the conversation in a way beneficial to Hashicorp&#x27;s reputation. reply smarx007 13 hours agorootparentprevMPL is a copyleft license, actually, just like EPL and EUPL. What it is not is a viral copyleft license. Anyone making changes to TF code and productionizing it is expected to contribute back but only the direct changes to TF itself, not the extensions around it. That&#x27;s my reading of EPL&#x2F;MPL&#x2F;EUPL. Does that match your reading? reply bayindirh 13 hours agorootparentYou&#x27;re right. MPL is a weak-copyleft license. I mistyped it, I don&#x27;t know why. Fixed my comment with a note, thanks.However, it still allows your code to be bundled inside a bigger work. The bigger work can be in any license (maybe not GPL, need to check), but MPL stays MPL. This doesn&#x27;t prevent white-labeling the codebase, though.As far as I read the MPL, the contribution back part is not mandatory, but encouraged by not affecting the larger work. It keeps the MPL part open, but doesn&#x27;t enforce a \"send patches back\" policy. reply smarx007 13 hours agorootparent> As far as I read the MPL, the contribution back part is not mandatoryThat&#x27;s not how I read it.> the contribution back part is not mandatoryIn that case weak copyleft would not be any different from MIT&#x2F;BSD.First, assuming you are distributing only the larger work:\"3.3. Distribution of a Larger WorkYou may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. [...]\"Assuming the covered (OSS) work is distributed as an executable:\"3.2. Distribution of Executable FormIf You distribute Covered Software in Executable Form then: (a) such Covered Software must also be made available in Source Code Form, as described in Section 3.1 [...]\"Finally, 3.1:\"3.1. Distribution of Source FormAll distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License.\"The only wiggle room I see is not triggering the distribution clause by a SaaS-only offering. By the way, EUPL is a non-viral copyleft license that closes the SaaS loophole in MPL&#x2F;EPL&#x2F;LGPL. reply bayindirh 13 hours agorootparentBy contributing back, I mean rolling up a patchset (or a tar of your source tree) and sending back to upstream.Otherwise, I think I clearly noted that the MPL part&#x27;s source stays available whatever you do by saying \"MPL part stays open\".It&#x27;s not the earliest hour here, so sorry if I can&#x27;t articulate my thoughts clearly.You need to keep MPL part&#x27;s source open and accessible, yet MPL doesn&#x27;t prevent abuse like permissive licenses, much. reply smarx007 12 hours agorootparentTrue, sending back to upstream is not required by the license. But even AGPL does not require this.Rolling up a tar of your source tree, however, will be required if you get a request from one of your customers by email. The difference from GPL is that you will only have to tarball a portion of the tree that was MPL-licensed. That tar must include your patches, as §3.1 requires.As I said before, many companies argue that using a private fork of an MPL&#x2F;EPL&#x2F;LGPL software in a SaaS does not trigger the distribution clause as customers never get a source or a binary of the program that runs in the cloud. EUPL closes that loophole.It&#x27;s also possible that many companies violate EPL&#x2F;MPL&#x2F;LGPL terms (knowingly or unknowingly).One good example of non-viral copyleft working as intended is the Eclipse IDE. There are many closed-source tools that use Eclipse IDE under the hood (the part that Eclipse calls RCP), but there are no commercial forks of the IDE itself because any such company would have to open-source their changes to the fork. replyecnahc515 14 hours agorootparentprevIMO: HashiCorp is in this situation because they want to grow too big, too fast. A lot of their software is tooling, tooling that doesn&#x27;t necessarily make sense to sell in a SaaS offering, or if it does: it&#x27;s going to be commoditized. However, SaaS is where the $ is. They have investors they must please, and they want a big return on investment. It&#x27;s probably too late to fix this, but IMO if they took the slow and steady route of providing support and professional services HashiCorp could easily be profitable while maintaining all of their products, but perhaps to a lesser degree. reply skywhopper 12 hours agorootparentprevHashiCorp has plenty of revenue and that revenue is growing fast. They are losing money, but that&#x27;s to be expected during the high-growth phase of their lifecycle. If they are having trouble growing as fast as their investors want it to, changing their licensing is not the way to fix it. This change is unfortunate because it won&#x27;t bring serious new revenue to the company but it is a major blow to the reputation they had built. reply johnbellone 13 hours agoparentprevJust stop using their products. Stop giving them free advertisements. Stop integrating your software and services with them.It is harsh, but they’re a public company now, not Mitchell. reply bshacklett 10 hours agorootparentFar easier said than done. reply thdn 13 hours agoparentprevWould you mind to share, those alternatives? reply AtNightWeCode 13 hours agoparentprevThe fantastic cost of running TF in the cloud is painful. Several years ago it was very clear that it would be difficult for HC to survive once they became a company registered at stock markets. reply jen20 12 hours agorootparent> The fantastic cost of running TF in the cloud is painful.Citation needed. I don&#x27;t think it costs very much at all. reply tootie 14 hours agoparentprevI&#x27;m not Hashicorp, but I mean look at Docker. They built the most valuable devops tool of the last generation and can barely muster a viable business. Why would Hashicorp give the slightest worry to losing thousands and thousands of non-paying customers? The upside is lots of money and the downside is loss of halo. Honestly, it&#x27;s an unfortunate game of expectation setting. If I wrote an open letter decrying Salesforce for not open sourcing their codebase, nobody would take me seriously. But we expect better from Hashicorp for some reason. reply i_am_jl 14 hours agorootparentnext [–]Why would Hashicorp give the slightest worry to losing thousands and thousands of non-paying customers?Because it goes hand in hand with losing hundreds of unpaid developers, testers, bug-reporters and evangelists. If I wrote an open letter decrying Salesforce for not open sourcing their codebase, nobody would take me seriously. But we expect better from Hashicorp for some reason.Because the community contributed to the codebase. Salesforce was never open source, Terraform was and took community contributions, that&#x27;s the reason we have different expectations. [Docker] can barely muster a viable businessDocker split their development and enterprise offerings into two companies years ago and both are making money. reply smarx007 13 hours agorootparentI think that once you have more than X customers, you don&#x27;t really need OSS testers and bug-reporters that much – your customers will be the first ones to file a ticket if anything is wrong. And as we saw with CentOS Stream, OSS users are not generally keen to be beta-testers.Ditto for earnings: once your company is publicly traded or at least lands in a Gartner report, you don&#x27;t need evangelists that badly anymore.Free pull requests are always welcome, but I guess a calculation was made in this case. reply i_am_jl 13 hours agorootparent>I think that once you have more than X customers, you don&#x27;t really need OSS testers and bug-reporters that much – your customers will be the first ones to file a ticket if anything is wrong. And as we saw with CentOS Stream, OSS users are not generally keen to be beta-testers.In this case the customers they&#x27;re losing (as well as the devs, testers, reporters, etc) aren&#x27;t end users but companies who have built software offerings on top of Terraform. That&#x27;s the class of user principally impacted by the change to the license. OSS users aren&#x27;t stoked about testing, but developers who build on top of Terraform in order to eat will submit bug reports and PRs all day. reply Matl 13 hours agorootparentprev> They built the most valuable devops tool of the last generation&#x27;They&#x27; as in Docker Inc.? They made it accessible for the masses, which counts for a lot, but they built on a lot of pre-existing Linux kernel tech that other people who envisioned containers put in place before Docker came in and seized on the opportunity.> can barely muster a viable business.They perused what proved to be the wrong business model until 2019, nowdays they&#x27;re more than &#x27;barely&#x27; viable. reply pxc 7 hours agorootparent> &#x27;They&#x27; as in Docker Inc.? They made it accessible for the masses, which counts for a lot, but they built on a lot of pre-existing Linux kernel tech that other people who envisioned containers put in place before Docker came in and seized on the opportunity.100%. Docker&#x27;s UX has always been its killer feature, and it counts for a lot. That&#x27;s a very real contribution. But this is absolutely still a &#x27;shoulders of giants&#x27;, &#x27;it takes a village&#x27; situation. reply Kamq 6 hours agorootparentprev> Why would Hashicorp give the slightest worry to losing thousands and thousands of non-paying customers?The photoshop effect*. That is to say, devs that are familiar with it will push their workplaces, because it&#x27;s a pain to pick up yet another tool when you know one that will work.*A largely hypothesized reason photoshop was so easy to crack back in the day, was that Adobe knew that if kids grew up using photoshop, businesses wouldn&#x27;t be willing to spend the money re-training their employees, and would just buy a photoshop license. reply reidrac 14 hours agorootparentprevIt is all about the change. HashiCorp model was open source, and now is not. Salesforce has never been open source.Would terraform have been as popular it if had never been open source? That we won&#x27;t know, but we can guess. reply YetAnotherNick 14 hours agoparentprev> Languages and frameworks must remain open or they will wither and die.Terraform is neither a language nor a framework, and I certainly don&#x27;t think it will wither and die if they transition to BSL. Case in point, docker&#x27;s revenue grew by 12x once they started taking control of their code and stopped caring about community. Same with postman or nginx or many other companies. reply mardifoufs 13 hours agorootparentWhat? Docker is still completely open source apart from the desktop GUI. The engine and (I&#x27;m pretty sure) all components are completely free and if anything, they have pushed for the standardization of the container runtime. Buildkit is free, compose is free, no feature is paywalled apart from Mirantis-centric stuff (not part of docker inc)You can absolutely bet that they would get dropped like a rock if they moved to changing the engine&#x27;s license. Even the docker desktop code wasn&#x27;t ever open to begin with anyways. reply scrum-treats 13 hours agorootparentDidn&#x27;t Docker actually try it earlier this year, e.g., https:&#x2F;&#x2F;blog.alexellis.io&#x2F;docker-is-deleting-open-source-ima...? reply mardifoufs 12 hours agorootparentThat&#x27;s just hosting though iirc. Docker hub is very important, but it&#x27;s not really part of Docker the software. As in, you could deploy your own container registry with 0 licensing issues. They just didn&#x27;t want to pay the bandwidth costs anymore, though I think they walked back on that for open source images. reply scrum-treats 9 hours agorootparentGot it. Thanks for the clarity. reply flanked-evergl 12 hours agorootparentprevNot sure what \"it\" refers to here, but what Docker did is in no way similar to what HashiCorp did. reply coolspot 12 hours agorootparentprevNginx is FreeBSD License, The Docker Engine is licensed under the Apache License 2.0 reply new23d 14 hours agoprevAs an end-user, not competing with HashiCorp, this change doesn&#x27;t worry me. According to their FAQ [1]: 10. What are the usage limitations for HashiCorp’s products under BSL? All non-production uses are permitted. All production uses are allowed other than hosting or embedding the software in an offering competitive with HashiCorp commercial products, hosted or self-managed. 24. Can I host the HashiCorp products as a service internal to my organization? Yes. The terms of the BSL allow for all non-production and production usage, except for providing competitive offerings to third parties that embed or host our software. Hosting the products for your internal use of your organization is permitted.[1] https:&#x2F;&#x2F;www.hashicorp.com&#x2F;license-faq reply JeremyNT 13 hours agoparentEven if you don&#x27;t mind abiding by the terms of the BSL, the licensing change is a signal that Hashicorp is in dire straits and doesn&#x27;t know how to operate as a sustainable business. They&#x27;re flailing about trying to increase revenue, and in so doing they&#x27;re removing one of the core components (the open source licensing) that made their tools ubiquitous to begin with. And what will their next cash grab be?Here&#x27;s the kicker though... before the change to BSL, the future of Hashicorp didn&#x27;t really matter as much, since somebody could fork their projects and keep them going. But with this licensing change, if Hashicorp shuts down one day, nobody could create a fork for several years.So to me, whether or not I can use the software as currently licensed isn&#x27;t the biggest issue. I want the ability to have an \"escape hatch\" should Hashicorp continue its downward trajectory or shut down completely. reply colechristensen 12 hours agorootparentGiving away most of your product for free and selling commercial services on top when it&#x27;s very easy to compete with you on that front is.... well it&#x27;s not a sustainable business model.It would be troublesome if any of the vendors at $work past or present went bankrupt, this is the nature of having external vendors. I am not particularly concerned.I was not the biggest fan of Terraform in the first place, I don&#x27;t like some of the language choices, but it works better than anything else that exists out there. reply ryanisnan 13 hours agoparentprevI think it should, to some extent. A really quick example comes to mind. Some of the best documentation on how to use Terraform properly comes from folks who provide competitive offerings.I could also see someome like Amazon eventually launching a CloudFormation like tool that works natively with Terraform, but now that&#x27;s off the table and I think a net negative.It also sounds like projects like Atlantis also would be against the BSL, including self-managed installations of the tool. reply orochimaaru 8 hours agoparentprevIt won&#x27;t affect you unless you&#x27;re selling tooling that embeds TF in some form. That, unfortunately, covers too wide a space and there is no telling when your offering is going to be in competition with Hashi&#x27;s. reply diarrhea 13 hours agoparentprevIt’s not about individual usage but the ecosystem around and on top of Terraform, whose foundation just got a lot more shaky. reply nikolay 13 hours agoparentprevIt should worry you - it hurts the ecosystem. Terraform is just a tool. The providers, modules, not supported by HashiCorp, is what makes Terraform useful. Ige the ecosystem dies, Terraform becomes useless. reply jen20 12 hours agorootparentThe ecosystem outside of providers is far less important than people like to claim. Open source modules are almost all poorly scoped, often just wrapping a single resource completely unnecessarily - simultaneously over- and under-abstracted. It&#x27;s also a huge security risk to pull them in. reply colechristensen 12 hours agorootparentThe only providers I have ever used in production, or would likely ever consider using would be published by Hashicorp or the software vendor for the resource being managed (for example [1]). Much would need to be done to trust any other third party without good reason.I have had similar experiences poking around other tf providers which were of apparently low quality.[1] https:&#x2F;&#x2F;registry.terraform.io&#x2F;providers&#x2F;elastic&#x2F;ec&#x2F;latest&#x2F;do... reply bloopernova 11 hours agoparentprevIt&#x27;s more that hashicorp leadership has shown themselves to be untrustworthy custodians of an infrastructure tool. reply aftbit 19 hours agoprev>Imagine if the creators of Linux [] suddenly switched to a non-open-source license that only permitted non-competitive usage.Linux cannot even successfully switch from GPL2 to GPL3 because of the sheer number of contributors and the fact that not all of them have transferred their copyright ownership to any given organization. This patchwork of different copyright owners has historically been seen as a potential weakness for Linux, but it seems like perhaps license inflexibility is a strength for open source. reply cies 18 hours agoparentI thought Linus and other believed GPLv2 was fine and the improvements of GPLv3 did not outweigh the potential problems introduced by it. It never came to a point where all authors were asked to agree, or sign away their ownership. reply aftbit 16 hours agorootparentMy understanding was that some people in the community believed that GPLv3 was better, and one of Linus&#x27;s criticisms was that it was essentially impossible to switch even if it were better. I also believe Linus was opposed to the switch, which would make it unlikely anyway, but even if he had approved, I still think it would be practically impossible. reply rwmj 15 hours agorootparentprevWe changed the license[1] of a project which had 10 contributors, and we got every single one of them to do an Acked-by (by email) which took some weeks. That was on the advice of our lawyers. Can&#x27;t imagine the impossible hassle of doing the same for something like Linux.[1] https:&#x2F;&#x2F;gitlab.com&#x2F;nbdkit&#x2F;nbdkit&#x2F;-&#x2F;commit&#x2F;952ffe0fc7685ea775... reply justincormack 14 hours agorootparentIt has been done for some fairly large projects, eg openssl managed to swith to Apache for 3.0. It is a lot of work. reply maccard 14 hours agorootparentprevCan&#x27;t speak for Linux, but got a few projects I&#x27;ve contributed to I&#x27;ve had to sign a CLA which negates that problem (but causes the one in this thread) reply kmeisthax 16 hours agorootparentprevTorvalds considered the anti-TiVo clause to be changing the deal and he didn&#x27;t want to do that, and there&#x27;s no way in GPLv3 to opt-out of the clause[0].This is less \"locking down devices is a human right\" and more him being angry that the FSF was trying to butt into his project&#x27;s affairs. He&#x27;s also similarly angry about \"GNU&#x2F;Linux\" as it sounds an awful lot like Stallman just demanding everyone stick \"GNU\" onto the name of Linus&#x27;s kernel project.Anyway all of this is going to seem really quaint in 2027 when Broadcom gets sued under DMCA 1201 by a rogue kernel contributor for evading the Linux linker&#x27;s license checks[1] and they have to hurriedly rewrite them out of the kernel and relicense anyway.[0] Granting a blanket exception doesn&#x27;t work because others can just remove the exception. \"No further restrictions\" is an ironclad law of copyleft.[1] The Linux kernel checks the declared license of loaded modules and refuses to link non-GPL-compatible code against any kernel symbol not marked as a user-space equivalent. The reason why this works this way is because Linux ships under GPLv2 plus an exception that says user-space APIs don&#x27;t trip copyleft, so you can legally load code built to those APIs into the kernel, but anything else might violate GPL.Since this is enforcing an interpretation of the GPL, this is a DMCA 1201 technical protection measure. You absolutely could make a DMCA 1201 anticircumvention claim in court against a proprietary driver developer that tried to evade the checks. Though Linus usually just bans their modules in the next kernel revision since he&#x27;s mainly worried about keeping proprietary modules from generating spurious bug reports in Linux. But the lawsuit is still possible, since they&#x27;re on GPLv2. If they had relicensed to GPLv3, this wouldn&#x27;t be an issue. reply comex 10 hours agorootparentAre you arguing that the GPL check “effectively controls access to a work” or “effectively protects a right of a copyright owner [..] in a work or a portion thereof”? Either way, the bar of how “effective” a measure needs to be to count may be low, but probably not that low. reply nabakin 16 hours agorootparentprevYes, I believe I saw a video of Linus stating exactly this reply sausagefeet 20 hours agoprevWe, Terrateam, do not believe we violate the new license but we support Terraform being open due to how important it is to the ecosystem. Unlike Vault or Waypoint, Terraform is closer to a language compiler like Go or Java and benefits from a robust community that can build on top of a stable ecosystem. As such, we have announced our support of the OpenTF Manifest[0][0] https:&#x2F;&#x2F;terrateam.io&#x2F;blog&#x2F;opentf-pledge reply marcinzm 18 hours agoprevAs I see it Hashicorp has failed to create a viable business model in an environment where there isn&#x27;t unlimited perpetual VC money. Now they&#x27;re at the stage of giving up and simply trying to shake down those who have managed to make better business models.It&#x27;s usually not a good idea to be near a company flailing like this since who knows what their next rent seeking approach will be. A company with nothing to lose is a dangerous partner to have. reply tedivm 17 hours agoparentThe worst part is they did create a viable business model. They were profitable when they had their IPO. They then pretended that the IPO was just another Series X investment, blew all the money, and went negative on their cashflow.Hashicorps problem isn&#x27;t that their business model doesn&#x27;t work, it&#x27;s that they are really bad at their jobs. They ignore customer feedback, laid off support people, and then jacked their prices up. It&#x27;s a self inflicted wound, and instead of trying to fix it they just keep making it worse. reply fishpen0 16 hours agorootparentI&#x27;ve worked for three companies now that went to hashicorp to buy TFE and left with a quote equal to a quarter or more of revenue and the sales rep acting like they are the second coming and obviously we are stupid for not thinking they bring that much value to our org. No org invests 1&#x2F;4 of their revenue on a single tool. So we used atlantis or spacelift or hand rolled GHA scripts and saved a fortune.We are trying to pay them and they are being so unreasonable with pricing that we can&#x27;t give them our money reply johnbellone 14 hours agorootparentIt’s been that way for years with them. It’ll be interesting to see if they go the road if a private equity buyout, or they’re acquired by a technology company for the copyright&#x2F;trademark. reply marcinzm 17 hours agorootparentprevDefinitely, the fact they&#x27;re rent seeking against similar small companies clearly shows that. Sad that rather than looking inward to improve themselves they&#x27;ve decided to just attack others. reply glenngillen 7 hours agorootparentprevBlew all that money? They still had about $1B in the bank at the last earnings, and they only raised $1.2B in the IPO iirc reply jen20 17 hours agorootparentprevHave you actually read the financial statements? None of what you just wrote is remotely true. reply paulgb 14 hours agorootparentI looked it up and you&#x27;re right, but it&#x27;s also striking to me how much they spend on sales and marketing: almost 75% of their gross revenue in 2023! reply brikis98 19 hours agoprevGruntwork here. You can find our statement here: The Future of Terraform must be open—our plan and pledge to keep Terraform open source. https:&#x2F;&#x2F;blog.gruntwork.io&#x2F;the-future-of-terraform-must-be-op...If you want to help us keep Terraform open source, please show your support at https:&#x2F;&#x2F;opentf.org&#x2F;! reply DrRobinson 14 hours agoparentGreat to see your commitment but I&#x27;m also curious why you, unlike some other companies, have chosen not to support with any full time employees? It seems your business is largely based on Terraform and saying pretty much \"we&#x27;ll contribute code\" doesn&#x27;t signal too much commitment.I realize my comment might sound like an accusation but that&#x27;s not my intention, I want to hear your reasoning about it! reply Brian_K_White 17 hours agoprevWe need a new word for this.We say OpenTF is (or will be) a fork, and forks are bad, nuclear option, etc, but really, Hashicorp are the ones who made a breaking change, and the \"fork\" merely maintains that which already was, but for reasons, are not allowed to continue using their own name.We need for the shortest sound-bite 3-word sentence to the non-technical to somehow use terminology that says that the entity that caused the problem is the one who did some action.OpenTF did not (or is not prepareing to) fork this project, Hashicorp did.If it was me and I wasn&#x27;t legally prevented by something actually binding in writing with signatures, I&#x27;d even keep using the original name and duke that out. reply dijit 16 hours agoparentThe issue with that is the ownership of the name.Name is identity, and thus the canonical representation of \"Terraform\" is now BSL.However, if you don&#x27;t have an identifier such as the trademarked name and you look at the project itself then I think you&#x27;re right. reply hadlock 14 hours agorootparentIt can be rebranded, that&#x27;s pretty straightforward for something that&#x27;s such an industry standard. \"Oh yeah? Earthworks? That&#x27;s the open source fork of Terraform\" pretty simple. If it were a lesser known technology it would be an issue but most of the (modern) internet runs on it, whatever they name the fork will be well known pretty much instantly reply Brian_K_White 7 hours agorootparentNo one asked \"How does one rebrand?\"The whole point of this thread is the premise that it&#x27;s not a fork.Hashicorp&#x27;s copy is the fork, and it&#x27;s backwards that the fork gets to keep the name and the original must rename itself. reply PeterZaitsev 16 hours agorootparentprevDo not mix source code license with Trademark. Trademark use is focused by Trademark policy which is here for Hashicorp https:&#x2F;&#x2F;www.hashicorp.com&#x2F;trademark-policy reply Fawlty 16 hours agoparentprevafaik you can&#x27;t use terraform in the name, it&#x27;s trademarked by Hashicorp reply Brian_K_White 16 hours agorootparentThat would be what I meant by legally prevented. reply time0ut 14 hours agoprevAs a long time Gruntwork customer, contributor, and fan, it is really nice to see them stepping up as thought leaders here. They run a great open source community already. Our DevOps team has been buzzing all day with what we are going to do. For now, we are staying pinned to the last open source version of Terraform and will likely follow Gruntwork&#x27;s lead when the time comes. reply linuxdude314 8 hours agoparentIf your company isn&#x27;t building a product based on terraform, why are you doing anything? reply time0ut 5 hours agorootparentWe use Terragrunt to manage thousands of Terraform configurations. If it and Terraform drift apart, we will have to go one way or the other eventually. Separately, a new license for Terraform means its gotta go back through legal and compliance so we will be paused for months anyway. reply lijok 20 hours agoprevI love it. The list of \"pledged companies\" is literally just a list of all the offenders that Hashicorp are trying to shake off. reply tedivm 19 hours agoparentThese people have seriously contributed back to the Terraform community. Terraform doesn&#x27;t have a test suite- Grunt made Terratest, as well as many other tools. These people have seriously contributed back to the ecosystem, in many ways beyond what Hashicorp has done.Beyond that, I know some of these companies tried to be contributors to Terraform itself but were ghosted by Hashicorp.At the same time there&#x27;s only a handful of regular contributors to Terraform[1]. It would not be hard for these companies to provide more resources to Terraform than Hashicorp is.https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;terraform&#x2F;pulse&#x2F;monthly reply rjbwork 18 hours agorootparentProbably not difficult for these competitor organizations to fund an OpenTF team to hire some of these people away from HashiCorp and continue it on as FOSS either. I can&#x27;t imagine Liam turning down .5M&#x2F;year to do so. reply fishnchips 18 hours agorootparentMarcin here, co-founder at Spacelift. We are open to fund 5 FTEs, feel free to reach out to us via the pledge page if you&#x27;re interested in OpenTF becoming your full-time job. reply tedivm 17 hours agorootparentThis is the first time I&#x27;ve been disappointed that I really like my current job, as that sounds like it would be pretty awesome. reply DrRobinson 13 hours agorootparentprev> Terraform doesn&#x27;t have a test suite- Grunt made TerratestThey have experimental support: https:&#x2F;&#x2F;developer.hashicorp.com&#x2F;terraform&#x2F;language&#x2F;modules&#x2F;t... reply lijok 17 hours agorootparentprevGruntwork is the only one of these companies that I genuinely cannot understand Hashicorp having any beef with. reply jen20 17 hours agorootparentprev> Terraform doesn&#x27;t have a test suitePatently false. Terraform has had an excellent test suite since 2014. reply tedivm 17 hours agorootparentSorry, this may be a miscommunication. Terraform itself has a test suite, yeah, but it doesn&#x27;t have a testing framework that users of the language can use to test their own code.To make a python metaphor, if cpython had a testing suite but pytest didn&#x27;t exist then people wouldn&#x27;t be able to test their own python code. That&#x27;s kind of the situation with Terraform right now- you can&#x27;t test your code using just the Terraform tools, you have to rely on Terratest which was written by Gruntwork. Hashicorp has spent years relying on the open source community to fill those gaps, which Gruntwork has done very nicely.Hashicorp even recommends Terratest for testing: https:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;testing-hashicorp-terraform reply empressplay 15 hours agorootparentThis is not testing?https:&#x2F;&#x2F;developer.hashicorp.com&#x2F;terraform&#x2F;cli&#x2F;commands&#x2F;plan reply candiddevmike 15 hours agorootparentNo, that&#x27;s a rough guesstimate from Terrafrom on what actions it will be doing. Terratest allows you to write actual tests and mocks. reply pseg134 17 hours agorootparentprevIt isn’t a miscommunication, jen20 is astroturfing for hashicorp. Look at all of their recent posts. reply jen20 16 hours agorootparent\"Astroturfing\"... lol OK. I don&#x27;t think I&#x27;ve ever made it a secret that I worked for HashiCorp in the early days (leaving in 2017), and have been critical to the point of being banned by the CEO from speaking at HashiConf.Saying \"Terraform doesn&#x27;t have a test suite\" is not miscommunication, it is misinformation, plain and simple - the same as most of the other things I have been correcting this week (not least from the same poster in this thread - someone who&#x27;s clearly has an axe to grind). reply candiddevmike 14 hours agorootparentYou and the OP are referring to different things. Terraform the codebase has a test suite. Terraform the app does not have a test suite&#x2F;runner as in a way to run tests against your Terraform files. reply jen20 13 hours agorootparentIt doesn’t have a testing tool built in. No one legitimately understands the phrase “terraform doesn’t have a test suite” to mean anything except “there are no tests”. A runner and a suite are quite different. reply tedivm 10 hours agorootparentI can understand the miscommunication in the first post, but I clarified in a direct comment to you what I meant. I was even comparing it to Terratest, which is not used for testing terraform core. At this point you&#x27;re just being belligerent for the sake of being belligerent. reply jen20 10 hours agorootparentI&#x27;m not arguing with you, your clarifying comment was fine. I&#x27;m arguing with the person who replied to you. replyPeterZaitsev 16 hours agoparentprevThis is reality of any successful Open Source ecosystem - folks who contribute the most (code, bug reports, marketing) in the project tend to be those who are making money on the project and these are the same folks who compete with youCoopetition is name of the game in Open Source and too bad increasing number of the companies want to focus on capturing all economic value from ecosystem they have created with help from so many others reply jsiepkes 20 hours agoprevPart of me hopes a fork comes out of this. I mean maybe features like this PR[1] for local state file encryption can then finally get merged.[1] https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;terraform&#x2F;pull&#x2F;28603 reply Fawlty 16 hours agoparentHashi got really good at ignoring PRs if they weren&#x27;t their own. They even ignored the PRs coming from the dev teams of their own customers (ie users of TF Cloud and Enterprise) which speaks volumes about their willingness to listen to the community... reply LVB 19 hours agoparentprevI did see https:&#x2F;&#x2F;github.com&#x2F;diggerhq&#x2F;open-terraform but have no idea if it is related. And I’m sure there are others. What I’ll be interested to see with the forks is how they will practically be maintained. All the bug and security fixes that HashiCorp is writing can’t just be cherry-picked into these forks (I think?), so what exactly are they supposed to do?Update: and in the past hour this repo is gone. reply JeremyNT 18 hours agorootparent> All the bug and security fixes that HashiCorp is writing can’t just be cherry-picked into these forks (I think?), so what exactly are they supposed to do?Indeed, any fork will need to implement their own bug fixes.Ideally they should do this \"clean room\" and not even look at the BSL&#x27;d code, to help defend against any accusations of copyright infringement. reply pawelpiwosz 19 hours agorootparentprevHi! OpenTF is not connected with a single company. This is an united, community-driven effort. You can check on the manifesto side who is behind. And we welcome all support!The fork you mentioned is no longer existing. reply fishnchips 18 hours agoparentprevThat would be one of the 1st things I&#x27;d love to see myself. Plus opening of \"internal\" to let folks build new stuff on top of Terraform elements. reply cattown 14 hours agoprevI like Terraform and will continue to use it. I&#x27;m just an end user that isn&#x27;t involved in building other product offerings on it or a user of other derivative products.Even though this really doesn&#x27;t affect my use case it does feel like kind of a dirty bait and switch. I do hope for a future where there&#x27;s a version (and Terraform provider module versions) that are actively maintained under a true open source license. I&#x27;ll favor using those over the official BSL version as much as possible.I guess it&#x27;s the CLA that all of the contributors signed that allows this to happen? I wonder if there&#x27;s a way for open source licenses to address this, and disallow the use of CLAs, or require some CLA clause that doesn&#x27;t allow sudden switches to non-permissive licenses? reply igorzij 20 hours agoprevDigger hereOur statement: https:&#x2F;&#x2F;medium.com&#x2F;@DiggerHQ&#x2F;diggers-statement-on-the-hashic... reply ddon 15 hours agoparentImpossible to read, behind a paywall... why people post stuff to Medium is a big mystery to me :)Here is how your post looks like: https:&#x2F;&#x2F;postimg.cc&#x2F;Pvwdw8D3 reply mdaniel 14 hours agorootparentLet me introduce you to my go-to for that bullshit: https:&#x2F;&#x2F;scribe.rip&#x2F;@DiggerHQ&#x2F;diggers-statement-on-the-hashic...courtesy of: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28838053 reply oars 11 hours agorootparentThanks for sharing Scribe which can help me read Medium articles with an alternative front end that bypasses the paywall. reply onedr0p 20 hours agoprevThere is zero chance of Hashicorp donating Terraform to an open source foundation. If there was they would have never even considered this change in license. Honestly it&#x27;s not a bad thing, maybe the maintainers of the Terraform fork will actually listen to feedback from the community of people who use it instead of ignoring them. reply PeterZaitsev 17 hours agoprevOne thing I particularly hate about license change is lack of notice - If you operate in good faith you probably would want to give time to community to make arrangement, whenever it is negotiating agreement with you or looking for alternatives. Lack of notice this means everyone who embedded Terraform put their customers at risk immediately as in case any discovered CVEs they will not be able to ship security fixes to their customers. reply cube2222 17 hours agoparentTheir FAQ states they&#x27;ll backport security fixes under the old license until the end of 2023.Disclaimer: Work at Spacelift reply PeterZaitsev 17 hours agorootparentAh, this is great when. MongoDB did not do it in their switch to SSPL reply cube2222 20 hours agoprevAlso, update from Spacelift, we believe that we are not in violation of the new license, you can find more details in our today&#x27;s announcement[0].We nevertheless support this initiative, though, as written in the article itself.[0]: https:&#x2F;&#x2F;spacelift.io&#x2F;blog&#x2F;spacelift-latest-statement-on-hash...Disclaimer: Work at Spacelift reply empressplay 16 hours agoparentYou guys effectively sell cloud-based TF instances though, don&#x27;t you? Pretty sure that&#x27;s a no-no...? reply cube2222 11 hours agorootparent> cloud-based TF instancesNot sure what specifically you mean with this.Anyway, the devil’s in the details, of both the license as well as the internal architecture of our system. I can’t share more here, but if you’d like to learn more please reach out via our chat or email. You can also expect more updates on our blog. reply cyberax 16 hours agoprevTerraform core is kinda crappy. The language is awful, and the module infrastructure sucks.I would support (with my own money) a fork that would re-use the Terraform providers, and reimplement the language as something not so insane. reply diarrhea 14 hours agoparentHaving recently picked up Rust (yes, sorry for mentioning it, I promise it&#x27;s relevant), I picked up Terraform the other day. I was shocked by how weak its language-level developer experience story is.I am working in VSCode, which by and large tends to be the editor supported best, with the most mindshare. Terraform has static and mostly strong typing, yet some testing revealed I was able to pass an argument of the wrong type to some variable I declared. This is type safety 101: variable declared `str` shouldn&#x27;t accept `int`, ever. Yet `tf validate` was silent, so was all IDE-integrated tooling (whatever the VSCode TF extension does).Jumping to&#x2F;from symbol definitions&#x2F;usages was also flaky (but not entirely absent).Really disappointing! My excitement of diving into TF went poof. Maybe I&#x27;m overly sensitive, but I was so excited to escape YAML hell (Ansible).Now I&#x27;m even firmer in the boat of just using a regular old language, like Pulumi with Python (with full typing).Did I do something wrong or can anyone confirm my findings? reply spion 11 hours agorootparentNo need to be sorry about mentioning Rust :)Yes, can confirm the terraform language server is kinda unreliable. We could both be doing something wrong, but it seems like that&#x27;s a common outcome. reply jen20 7 hours agorootparentprevThe IntelliJ support for Terraform is far more complete.FWIW, the Rust support is also better in CLion than VS Code. reply verdverm 16 hours agoparentprevThe CUE community has some experiments, we were chatting about it recently, given the upheavalImagine if that language covered more of the stack then just one tool too! reply rank0 12 hours agoparentprevAny reason why you don’t like cloudFormation or the SDKs? reply cyberax 9 hours agorootparentThey are EVEN MORE crappy than TF. reply jen20 12 hours agoparentprevWell, if that&#x27;s something you _actually_ want, take a look at Pulumi, which does precisely what you ask. reply cyberax 4 hours agorootparentPulumi looks great. Why aren&#x27;t they more popular?!?They still need the state (boo!), but otherwise they&#x27;re great. reply ms4720 19 hours agoprevI think the problem is that if hashicorp thinks you are a competitor you and your clients now have legal&#x2F;operational issues. Ie you are now a competitor because we are releasing a product just like yours, here is a letter from a lawyer telling you to stop using terraform. reply brikis98 18 hours agoparent> if hashicorp thinks you are a competitorThis is precisely the problem with the new BSL license. Whether your usage of Terraform complies with the license isn’t determined by the legal terms, but instead is entirely at the whim of HashiCorp. And they can change their mind at any time. It makes it impossible to build anything on top of Terraform.I talk about that more here: https:&#x2F;&#x2F;blog.gruntwork.io&#x2F;the-future-of-terraform-must-be-op... reply cstejerean 14 hours agorootparentThis covers really well why I think the BSL license is a non-starter for things like TF. I get trying to prevent AWS from competing with you using your own open source code, but it creates this ambiguity where it&#x27;s not clear whether lots of uses are or are not competing with HashiCorp.> For example, if you’re an independent software vendor (ISV) or managed service provider (MSP) in the DevOps space, and you use Terraform with your customers (but not necessarily Terraform Cloud&#x2F;Enterprise), are you a competitor? If your company creates a CI &#x2F; CD product, is that competitive with Terraform Cloud or Waypoint? If your CI &#x2F; CD product natively supports running Terraform as part of your CI &#x2F; CD builds, is that embedding or hosting? If you built a wrapper for Terraform, is that a competitor? Is it embedding only if you include the source code or does using the Terraform CLI count as embedding? What if the CLI is installed by the customer? Is it hosting if the customer runs your product on their own servers?The answer is at the whim of HashiCorp and subject to change at any point in the future. Even ignoring the attempt to dilute the meaning of \"open source\", the practical implications of the BSL license are more than enough reason to coalesce around a truly open source fork IMO. reply unethical_ban 17 hours agorootparentprevI worked at a financial institution that heavily utilized terraform. Their business is banking and they do not offer automation, orchestration or IaC as a service. They&#x27;re fine.This seems to affect only those places that attempt to build a business off terraform.I am not saying those businesses can&#x27;t be mad at the rug getting pulled out from under them, but it&#x27;s important to be accurate that this doesn&#x27;t affect end users of TF directly. reply ig1 17 hours agorootparentIs the financial institution made up of separate legal entities which bill each other for services, and does one of those entities provide tech infra for the other legal entities? reply unethical_ban 16 hours agorootparentGood point, but no. Also I think they pay Hashicorp for support. reply ig1 16 hours agorootparentThe messiness of the real-world unfortunately doesn&#x27;t play well with ambiguity in licences :)It&#x27;ll be a headache for every large company which now has to send the licence to their legal teams who have to ask these kind of questions (another interesting one is \"can contractors touch our terraform setup?\") - in fairness to Hashicorp they&#x27;ve tried to address some of these issues in their FAQ, but the FAQ isn&#x27;t legally binding so legal teams have to go on what&#x27;s actually written in the licence. replymousetree 14 hours agoprevAs a regular end-user of Terraform, what difference does BSL vs MPL make to me? From reading this article it seems not very much? Perhaps I&#x27;m misreading this. reply dmattia 14 hours agoparentYou can continue to use plain Terraform forever. It would only affect you if you use a tool like Env0, spacelift, Gruntwork pipelines, etc instead of something like the Terraform Cloud.These tools are not going to be able to be used with users using new Terraform versions (though they can always use the current or any previous versions, or can use their fork these companies are jointly supporting).Then there are open source tools that don&#x27;t directly compete with Hashicorp that are in a bit of a gray area, but I&#x27;ve seen Atlantis, Pulumi, OTF, and other tools all claim that this does not affect them. I would presume this could also apply to things like Terratest, Terragrunt, etc. but I don&#x27;t know. I am not a lawyer.And if none of these company&#x2F;product names are familiar to you, then you shouldn&#x27;t have any noticeable difference :) reply cube2222 14 hours agorootparent> These tools are not going to be able to be used with users using new Terraform versionsAs discussed in the other thread, we believe that we are not in violation of the new license, you can find more details in our today&#x27;s announcement[0].Disclaimer: Work at Spacelift.[0]: https:&#x2F;&#x2F;spacelift.io&#x2F;blog&#x2F;spacelift-latest-statement-on-hash... reply dmattia 14 hours agorootparentOh neat, thanks for sharing this! I had read https:&#x2F;&#x2F;spacelift.io&#x2F;blog&#x2F;hashicorps-license-change last week, but hadn&#x27;t seen this blog yet.Best of luck with Spacelift :) reply cube2222 14 hours agorootparentThanks, very appreciated! reply mirzap 14 hours agoparentprevIt doesn&#x27;t. And it will not make any difference. The same way as Sentry license, Elasticsearch, etc., doesn&#x27;t have any difference for regular users. Those changes target cloud providers like Amazon, Google, etc. reply robbintt 14 hours agoparentprevIt will affect categories of business users (programmers) who currently embrace terraform: amazon, google, microsoft, oracle, alibaba cloud.Like it or not, cohorts of engineering organizations like the above (cloud providers) have a very outsized weight and already have contender products they can choose to vigorously fund tomorrow.From the article:The license does not allow you to use Terraform if you meet both of the following conditions: You are building a product that is competitive with HashiCorp. You embed or host Terraform in your product.My $0.02: the management of hashicorp is following a stupid trend and should have thought about their customers more.It will come out to what lawyers think, I guess. Lawyers usually say no to things with poorly established precedent. reply cbo100 14 hours agorootparent> have contender products they can choose to vigorously fund tomorrow.Then why didn’t they “choose” to fund hashicorp yesterday and avoid this?As usual in OSS-goes-private events these all just sound like “keep building our critical infrastructure tool for free or we will go elsewhere”.What is hashicorp or any other company in their position to do?This is not sustainable. reply i_am_jl 13 hours agorootparent> As usual in OSS-goes-private events these all just sound like “keep building our critical infrastructure tool for free or we will go elsewhere”.If Terraform was 100% developed by HashiCorp employees that would be fair description. It&#x27;s more like \"We&#x27;re the only company allowed to make money off of the codebase you contributed to.\"> What is hashicorp or any other company in their position to do?I&#x27;d suggest paying developers to write proprietary code. That way they could just sell a product they fully own instead of having to pull this bullshit re-licensing of an open source codebase. reply LinXitoW 10 hours agoparentprevSomeone correct me if I&#x27;m wrong, but if I use TF in a Petstore-as-a-Service to provision new machines for my users, does that not count as embedding TF? So if HashiCorp decides to do Petstore-as-a-Service tomorrow, no matter how shitty the offering, no matter how insincere, even if it&#x27;s just a single intern working on it, I would have to, overnight, rip TF out of my entire offering, no? reply vrosas 14 hours agoparentprevIANAL but it doesn’t seem like much, unless you’re planning on building a terraform-as-a-service company to compete directly with hashicorp. Honestly I’m not surprised given hashicorp’s enterprise offering are basically just… hosting the .tfstate file for you? I still can’t figure out what their upsell is. reply pknomad 14 hours agorootparentHosting the statefile in a secure way, state-locking, injecting the secrets so you&#x27;re not keeping the secrets locally or in env var, and pre-built integration with other Hashicorp suite.I see the use case for it if you don&#x27;t want to use a 3rd party or open source tool (Atlantis) but the pricing seems prohibitive. reply pknomad 14 hours agoparentprevNot much, right now.Long term? Possibly less adoption (teams may elect to go with Pulumi or some other alternatives), less 3rd party tooling available (what if Hashicorp decides your tool is their competitor?), etc.It seems very similar to the spat that community had with Red Hat with how 3rd party captures too much value from their own internal offering and leadership responds by changing the license model and makes things less open-source-y. Perhaps this will become the new normal for OSS&#x2F;former OSS? IDK. reply lacker 14 hours agoparentprevAs a regular end-user, the main difference in the licenses is that it forks the ecosystem. If the fork goes ahead, and you were using some HashiCorp products and some software that is moving to OpenTF, eventually you won&#x27;t be able to use that combination of tools any more. So you will have to pick what license you are going with, even if you don&#x27;t care about the license directly. reply rst 14 hours agoparentprevIt depends on what you&#x27;re using it for, and whether it competes with anything Hashicorp does -- or might do in the future. If you can guarantee that&#x27;s \"none\", you&#x27;re in the clear. But as the man said, prediction is hard, especially about the future. reply hnav 14 hours agoparentprevthis is similar to other opencore debacles, Hashicorp wants you to use their managed Terraform rather than someone else&#x27;s integration. This means that if you choose TF today for you managing your IaC you&#x27;ll potentially be locked into using Hashi products down the road. The community is all but guaranteed to fork TF which means that over time the two forks will diverge and you&#x27;ll have a bad time when trying to read docs, debug, contribute fixes. reply yonixwm 14 hours agoparentprevI think you will be affected by the bigger picture. Mongo did this move also but there they were mostly in control before and after. Here there is a huge community of plugins. If before AWS shared a provider without hesitating, now they will ask themselves why contribute to a closed and possibly competitor garden. reply VectorLock 10 hours agorootparentAWS has had a Terraform competitor for a long time; CloudFormation. I&#x27;m not sure how much they contribute to Terraform, some I think, but most of the AWS provider for Terraform to my knowledge is developed and owned by Hashicorp. If AWS just stopped contributing to Terraform development, I think it would have effectively zero difference. reply fidotron 15 hours agoprevThe weaponisation of open source by the cloud vendors combined with devops culture encouraging only paying for operations and commoditising development is going to lead to constant pointless migrations of this kind (such as Docker&#x2F;podman etc.)Devops people need to find a viable way to reward the developers of the tools they make a living from operating. Failing that they will wake up finding no one is willing to make them or that those that do have an ulterior motive. reply rank0 14 hours agoparentI actually love the weaponization of OSS. It eats away at the technical gap between proprietary systems and their FOSS equivalents. See elasticsearch + openAI (although open models are still quite far b",
    "originSummary": [
      "The OpenTF Manifesto raises concerns about a license change in Terraform, an open-source platform, and advocates for either reverting the change or establishing a foundation to maintain an open-source version.",
      "Various companies and individuals actively participate in open-source community initiatives to enhance projects and foster collaboration.",
      "On August 15th, 2023, a group of individuals contributed to individual development and open-source community efforts, such as testing, documentation, consultancy, and utilizing open-source tools."
    ],
    "commentSummary": [
      "The OpenTF Manifesto is a discussion by developers advocating for a fork of Terraform into a non-profit foundation due to concerns about lack of maintenance and support from Hashicorp, the current owner.",
      "Some developers argue for a fork to improve the language and prioritize community needs, while others hope for a change of mind from Hashicorp.",
      "The discussion also addresses secret management products, limitations of declarative programming, impact on the open-source community, provider ecosystem, revenue generation by Hashicorp, alternative licensing options, renaming and rebranding projects, lack of testing framework in Terraform, ownership issues, and concerns with the new license.",
      "Users express support for an open-source Terraform and explore alternative solutions.",
      "Concerns about switching licenses, impact on revenue growth, limitations, compatibility, and influence of cloud vendors on open source tools are also discussed."
    ],
    "points": 554,
    "commentCount": 317,
    "retryCount": 0,
    "time": 1692101973
  },
  {
    "id": 37140013,
    "title": "How Is LLaMa.cpp Possible?",
    "originLink": "https://finbarr.ca/how-is-llama-cpp-possible/",
    "originBody": "Finbarr Timbers Blog Books Advice Prompt engineering How is LLaMa.cpp possible? If you want to read more of my writing, I have a Substack. Articles will be posted simultaneously to both places. Recently, a project rewrote the LLaMa inference code in raw C++. With some optimizations and quantizing the weights, this allows running a LLM locally on a wild variety of hardware: On a Pixel5, you can run the 7B parameter model at 1 tokens/s. On a M2 Macbook Pro, you can get ~16 tokens/s with the 7B parameter model You can even run the 7B model on a 4GB RAM Raspberry Pi, albeit at 0.1 tokens/s. If you are like me, you saw this and thought: What? How is this possible? Don’t large models require expensive GPUs? I took my confusion and dove into the math surrounding inference requirements to understand the constraints we’re dealing with. Let’s start with GPUs. GPUs have two main benefits for deep learning: They have a large amount of memory bandwidth (A100: 1935 GB/s, 4090: 1008 GB/s) They have a large amount of compute (A100: 312 TFLOPS of FP16, 4090: 82.6 TFLOPS of FP16) When we talk about memory bandwidth, we’re talking about how long it takes to move things from the HBM memory (i.e. the RAM) into the on-chip memory. To actually do math with the GPU, we need to move the matrices in question into the on-chip memory, which is quite small (40MB on an A100, compared to 40-80GB of RAM). Note that the memory bandwidth is ~2 orders of magnitude smaller than the compute performance— this will matter later, as the memory bandwidth tends to be the bottleneck for inference. What does this mean in the context of serving LLaMa? Let’s start with some inference arithmetic. We can do some rough calculations on the inference performance of a LLM using Kipply’s article1. First, some notation on the dimensions of the model: The 𝑄 , 𝐾 , and 𝑉 weight matrices are all shape [ 𝑑 model , 𝑑 head ], and we have 𝑛 heads of them per layer; the attention output matrix has the same shape, for a total of 4 × [ 𝑑 model , 𝑛 heads ⋅ 𝑑 head ]. By convention, GPT-style networks have 𝑑 head ⋅ 𝑛 heads = 𝑑 model . The MLP has two weight matrices, of shape [ 𝑑 model , 4 ⋅ 𝑑 model ] and [ 4 ⋅ 𝑑 model , 𝑑 model ] The embeddings matrix is of size [ 𝑑 vocab , 𝑑 model ]. This gives us a handy equation for the number of parameters in a GPT-style model:2 𝑃 = 𝑛 blocks ( 4 ⋅ 𝑑 model 2 + 2 ⋅ 4 ⋅ 𝑑 model 2 ) + 𝑛 vocab ⋅ 𝑑 model For the duration of the post, I’m going to focus on the case where we’re running a ChatGPT style service locally, which is what LLaMa.cpp does, letting me assume a batch size of 1. For efficient inference, the KV cache has to be stored in memory; the KV cache requires storing the KV values for every layer, which is equal to storing: 𝑛 bytes ⋅ 2 ⋅ 𝑑 model I use 𝑛 bytes here to indicate the number of bytes per param; for float32s, this is 4, for float16s, this is 2, etc. The 2 in the middle is because we have to store one set of weights for the K values, and one for the Vs. Given a model with n layers, the total memory for the KV cache is: 𝑛 blocks ⋅ 𝑛 bytes ⋅ 2 ⋅ 𝑑 model In addition to storing the KV cache in memory, we also need to store the weights themselves in memory; this requires 𝑛 bytes ⋅ 𝑃 bytes. This is the advantage of quantization. By using less precision, we can radically decrease the amount of memory needed to store our models in memory. Note that, with int4 precision, all of these models fit into memory on an A100 (which is the standard datacenter GPU right now), and all of them, except for the biggest model, fit into memory on high-end consumer GPUs (3090s/4090s, which have 24GB of RAM). It takes approximately 2 𝑃 FLOPS to run inference on our model for a single token, because we are doing a bunch of matmuls with a total of 𝑃 parameters, and multiplying a matrix of size ( 𝑚 , 𝑛 ) with a vector of size ( 𝑛 , ) has a cost of 2 𝑚 𝑛 .3 With all that math out of the way, let’s calculate the requirements for running inference with LLaMa. The main requirements when it comes to sampling are: Keep the KV cache in memory, in addition to all the parameters. Read all the weights from HBM into the on-chip memory. Because we sample auto-regressively, we have to repeat this for each token we sample. Do the actual matmuls to calculate the output of our network. The latency is the maximum of either the compute or the memory latency, as reading parameters into on-chip memory happens asynchronously in all modern tensor programming libraries. As a result, we write: latency model = max ( latency compute , latency memory ) latency memory = 2 ⋅ 𝑃 ⋅ 𝑛 bytes ⋅ 𝐵 𝑛 memory bandwidth , latency compute = 2 ⋅ 𝑃 𝑛 flops , where 𝐵 is the batch size. As 𝑛 memory bandwidth = 1.935 𝑒 12 , and 𝑛 flops = 3.12 𝑒 14 , as long as the batch size is less than 161, the model is memory-bound.4 With a batch size of 1, this is the same equation, as on most hardware (e.g. Nvidia GPUs), there is a linear speedup as you decrease the precision (you get twice the FLOPS when using fp16 vs fp32, which doubles again as you go to int8, and doubles once more as you go to int4s). As LLaMa.cpp uses int4s, the RAM requirements are reduced to 1.33GB of memory for the KV cache, and 16.25GB of VRAM for the model parameters. That’s pretty good! As the memory bandwidth is almost always5 much smaller than the number of FLOPS, memory bandwidth is the binding constraint. Running LLaMa on an A100 On an A100 (80GB PCIe), the memory bandwidth is 1935GB/s. The int4 compute is 1248 TOPS. As such, the model is (heavily) memory-bound. We should expect to see inferences as given in the table; roughly 30 tokens/s with the 65B model, and 277 tokens/s with the 7B model. Running LLaMa on a M1 Macbook Air The M1 GPU has a bandwidth of 68.25 GB/s, while the M1 GPU can do up to 5.5 TFLOPS of fp16 compute. As such, we should expect a ceiling of ~1 tokens/s for sampling from the 65B model with int4s, and 10 tokens/s with the 7B model. As the M2 Pro has 200 GB/s of bandwidth, and the M2 Max has 400 GB/s of bandwidth, we should expect massive improvements with them, going up to 6 tokens/s with the M2 Max with the 65B model. That’s pretty darn good for a laptop. Running LLaMa on a Raspberry Pi 4 A Raspberry Pi 4 has 13.5 GFLOPS of compute, and ~4GB/s of memory bandwidth. Given this, we’d expect to see ~2 tokens/s with the 7B model if it was memory bound. Given that we’re currently seeing ~0.1 tokens/s, I suspect we’re actually compute-bound (although this is a stab in the dark— I can’t find enough information about the specs for a Raspberry Pi to determine this with any precision). Summary Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them much easier to serve— like quantization! This is yet another reason why distillation, or just training smaller models for longer, is really important. Note: I’m not an expert in CUDA, so I probably have errors in my math. If so, please shoot me an email and let me know- I’d love to hear from you so I can learn more about how this works and update this post. Resources on transformer inference performance: Large Transformer Model Inference Optimization Transformer inference arithmetic LLM parameter counting Efficient Transformers Thank you to Kaushik Patnaik, Arthur Allshire, Stanislav Fort, and Tom McGrath for reading early drafts of this. Almost all of this math is taken from their article; they deserve full credit. ↩ Although it’s an open question of how much Mac specific optimization is being done with LLaMa, or indeed, any of the tensor programming frameworks. ↩ For a more detailed discussion showing that this is the case, check out kipply’s article. ↩ If you’re following along with kipply’s post, there’s a slight discrepancy here, as I assume a 80GB A100 PCIe, which is what I see as the standard GPU. Their post assumes a 40GB A100 PCIe, which has slightly lower memory bandwidth. ↩ I hedge with “almost” here, but I’m not aware of any counterexamples. ↩ PS if you want to read more of my writing, subscribe to my Substack.",
    "commentLink": "https://news.ycombinator.com/item?id=37140013",
    "commentBody": "How Is LLaMa.cpp Possible?Hacker NewspastloginHow Is LLaMa.cpp Possible? (finbarr.ca) 500 points by birriel 11 hours ago| hidepastfavorite172 comments tomohelix 11 hours agoIn case anyone is wondering, yes, there is a cost when a model is quantized.https:&#x2F;&#x2F;oobabooga.github.io&#x2F;blog&#x2F;posts&#x2F;perplexities&#x2F;Essentially, you lose some accuracy and there might be some weird answers and probably more likely to go off the rail and hallucinate. But the quality loss is lower the more parameters you have. So for very large model sizes the differences might be negligible. Also, this is the cost of inference only. Training is a whole other beast and requires much more power.Still, we are looking at GPT3 level of performance on one server rack. That says something when less than a year ago, such AI was literally magic and only run on a massive datacenter. Bandwidth and memory size are probably, in my ignorance mind, easier to increase than raw compute so maybe we will soon actually have \"smart\" devices. reply simonw 10 hours agoparentI was hoping that link would answer the question that&#x27;s been bugging me for months: what are the penalties that you pay for using a quantized model?Sadly it didn&#x27;t. It talked about \"perplexities\" and showed some floating point numbers.I want to see examples like \"here&#x27;s a prompt against a model and the same prompt against a quantized version of that model, see how they differ.\" reply mikeravkine 6 hours agorootparentI have several sets of quant comparisons posted on my HF spaces, the caveat is my prompts are all \"English to code\": https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mike-ravkine&#x2F;can-ai-code-compa...The dropdown at the top selects which comparison: Falcon compares GGML, Vicuna compares bits and bytes. I have some more comparisons planned, feel free to open an issue if you&#x27;d like to see something specific: https:&#x2F;&#x2F;github.com&#x2F;the-crypt-keeper&#x2F;can-ai-code reply brucethemoose2 7 hours agorootparentprevIt makes the model dumber.That seems simplistic, but its really simple as that. Naive 3 bit quantization will turn llama 7B into blubbering nonsense.But llama.cpp quantization is good! I recommend checking out the graphs ikawrakow made for their K-quants implementation:https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684Basically, the more you quantize with K-quant, the dumber the model gets. 2 bit llama 13B quant, for instance, is about as dumb as 7B F16, but the dropoff is not nearly as severe from 3-6 bits. reply version_five 10 hours agorootparentprevnext [–]I want to see examples like \"here&#x27;s a prompt against a model and the same prompt against a quantized version of that model, see how they differ.\"We suck at evaluating and comparing models imo. There are metrics and evaluation task, but it&#x27;s still very subjective.The closer we get to assessing human like performance, the tougher it is, because it becomes more subjective and less deterministic by the nature of the task. I don&#x27;t know the answer, but I know that for the metrics we have it&#x27;s not so easy to translate them into any idea about the kind of performance on some specific thing you might want to do with the model. reply tysam_and 7 hours agorootparentNot mathematically, at the very least. Perplexity is a translation of the best measure we have for informing us how a model is doing empirically over a test dataset (both pre and post). It is enough to be, usably, at least the final word on how different quantization methods perform.Subjective ratings are different, but for compression things are quite well defined. reply cj 8 hours agorootparentprev> some specific thing you might want to do with the model.I think this right here is the answer to measuring and comparing model performance.Instead of trying to compare models holistically, we should be comparing them for specific problem sets and use cases... the same as we compare humans against one another.Using people as an example, a hiring manager doesn&#x27;t compare 2 people holistically, they compare 2 people based on how well they&#x27;re expected to perform a certain task or set of tasks.We should be measuring and comparing models discriminately rather than holistically. reply mr_toad 8 hours agorootparentYou could have two models answer 100 questions the same way, and differ on the 101st. They’re unpredictable by nature - if we could accurately predict them we’d just use the predictions instead. reply cj 7 hours agorootparent(Stupid question) are models still non-deterministic if you set the temperature to zero?Would setting the temperature to zero degrade the quality of response? reply version_five 51 minutes agorootparentEven at T=0 and run deterministically, the answers still have \"randomness\" with respect to the exact prompt used. Change wording slightly and you&#x27;ve introduced randomness again even if the meaning doesn&#x27;t change. It would be the same for a person.For an llm, a trivial change in wording could produce a big change in answer, same as running it again with a new random seed. \"Prompt engineering\" is basically overfitting if not approached methodically. For example, it would be interesting to try deliberate permutations of an input that don&#x27;t change the meaning and see how the answer changes as part of an evaluation. reply visarga 5 hours agorootparentprevSome GPU operations give different results depending on the order they are done. This happens because floating point numbers are approximations and lose associativity. Requiring a strict order causes a big slowdown. replysharms 6 hours agorootparentprevI have been using nat.dev to compare quantized models and it works great. reply dustymcp 3 hours agorootparentprevit will be different for every usecase, the only way to find out is spinning one up.. reply hackernewds 5 hours agorootparentprevwould an answer \"there aren&#x27;t much significant penalties\" suffice? reply moffkalast 10 hours agorootparentprevThe problem is that it&#x27;s not consistent enough for a good demo. Not even two different models, but even two different fine tunes of the same base model may be wildly differently affected by quantization. It can range from making hardly a difference to complete garbage output. reply riezebos 36 minutes agoparentprevCould this be why people recently say they see more weird results in ChatGPT? Maybe OpenAI is trying out different quantization methods for the GPT4 model(s) to reduce resource usage of ChatGPT. reply pocketarc 31 minutes agorootparentI&#x27;d be more inclined to believe that they&#x27;re dropping down to gpt-3.5-turbo based on some heuristic, and that&#x27;s why sometimes it gives you \"dumber\" responses. If you can serve 5&#x2F;10 requests with 3.5 by swapping only the \"easy\" messages out, you&#x27;ve just cut your costs by nearly half (3.5 is like 5% of the cost of 4). reply redox99 10 hours agoparentprev>Still, we are looking at GPT3 level of performance on one server rack. That says something when less than a year ago, such AI was literally magic and only run on a massive datacenter.I&#x27;m not sure what you mean by this. You&#x27;ve always been able to run GPT3 on a single server (your typical 8xA100). reply brucethemoose2 7 hours agorootparent8xA100 is technically a single server, but I think OP is talking about affordable and plentiful CPU hosts, or even relatively modest single GPU instances.DGX boxes do not grow on trees, especially these days reply blovescoffee 9 hours agorootparentprevAm I missing something or how do you know this? Also I think the OP was talking about a single card not multiple but that was just my reading. reply redox99 9 hours agorootparentBecause 175B parameters (350GB for the weights FP16, let&#x27;s say a bit over 400GB for actual inference), fit very comfortably on 8xA100 (640GB VRAM total).And basically all servers will have 8xA100 (maybe 4xA100). Nobody bothers with a single A100 (of course in a VM you might have access to only one) reply axiom92 9 hours agorootparent> And basically all servers will have 8xA100for those wondering: no this is not the norm. My lab at CMU doesn&#x27;t own any A100s (we have A6000s). reply doctorpangloss 9 hours agorootparentThe servers the commenter is talking about are DGX machines from NVIDIA.It doesn’t really make sense to BTO. What you gain economically you lose in the science you can do.But nobody could have anticipated this. reply _zoltan_ 1 hour agorootparentyou could also get HGX from any of the vendors. reply _zoltan_ 1 hour agorootparentprevwho&#x27;s norm? I assure you it&#x27;s the norm. :) replyYetAnotherNick 3 hours agoparentprevThe effect is lesser than you think. 5 bit quantization has negligible performance loss compared to 16 bits: https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684 reply astrange 2 hours agorootparentThis paper from last month has a method for acceptable 3-bit quantization and a start at 2-bit.https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13304 reply smcleod 4 hours agoparentprevGood blog post, shame the site has no RSS feed! reply tysam_and 10 hours agoparentprevYes, there is a logarithmically-bound (or exponential if you&#x27;re viewing it from another angle) falloff in the information lost in quantization. This comes from the non-uniform \"value\" of different weights. We can try to get around them with different methods, but at the end of the day, some parameters just hurt more to squeeze.What is insane though is how far we&#x27;ve taken it. I remember when INT8 from NVIDIA seemed like a nigh-pipedream! reply WithinReason 2 hours agoparentprevThis is not generally true, sometimes quantisation can improve accuracy. I haven&#x27;t seen that with LLMs yet though. reply arijun 1 hour agorootparentInteresting, how would that work? Are there any well-known examples?Is it: the weights all happen to be where float is sparse, so quantization ends up increasing fidelity? Or is it more of a “worse is better” dropout-type situation? reply WithinReason 1 hour agorootparentI suspect it works as regularisation of the network. It usually happens when you train with quantisation instead of post-training quantisation, an I haven&#x27;t seen that done with LLMs yet. reply matsemann 1 hour agorootparentprevFor image recognition it can sometimes be like that. My gut feeling is that lowering from fp32 to fp16 can get rid of some kind of overfitting or so. reply ripvanwinkle 10 hours agoparentprevThank you! Is there a sweet spot with quantization. how much can you quantize for given model type and size and still be useful. reply pseudonom- 10 hours agorootparentTim Dettmers recently (https:&#x2F;&#x2F;www.manifold1.com&#x2F;episodes&#x2F;ai-on-your-phone-tim-dett...):\"But what we found with these neural networks is, if you use 32 bits, they&#x27;re just fine. And then you use 16 bits, and they&#x27;re just fine. And then with eight bits, you need to use a couple of tricks and then it&#x27;s just fine.And now we find if you can go to four bits, and for some networks, that&#x27;s much easier. For some networks, it&#x27;s much more difficult, but then you need a couple more tricks. And so it seems they&#x27;re much more robust.\" reply KirillPanov 59 minutes agorootparent> And now we find if you can go to four bitsThat will be really interesting for FPGAs, because the current ones are basically oceans of 4-bit computers.Yes, you can gang together a pair of 4LUTs to make a 5LUT, and a pair of 5LUTs to make a 6LUT, but you halve your parallelism each time you do that. OTOH you can&#x27;t turn a 4LUT into a pair of 3LUTs on any currently-manufactured FPGA. It&#x27;s simply the \"quantum unit\" of currently-available hardware -- and it&#x27;s been that way for at least 15 years (Altera had 3LUTs back in the 2000s). There&#x27;s no fundamental reason for the number 4 -- but it is a very, very deep local minimum for the current (non-AI) customers of FPGA vendors. reply prvc 4 hours agoparentprevAny use case for using the 7B model over the 13B, quantized? reply clarionbell 2 hours agorootparentSBC reply brucethemoose2 7 hours agoprevThis leaves a ton of stuff out.- Token generation is serial and bandwidth bound, but prompt ingestion is not and runs in batches of 512+. Short tests are fast on pure CPU llama.cpp, but long prompting (such as with ongoing conversation) is extremely slow compared to other backends.- Llama.cpp now has very good ~4 bit quantization that doesn&#x27;t affect perplexity much. Q6_K almost has the same perplexity as FP16, but is still massively smaller.- Batching is a big thing to ignore outside of personal deployments.- The real magic of llama.cpp is model splitting. A small discrete GPU can completely offload prompt ingestion and part of the model inference. And it doesn&#x27;t have to be an Nvidia GPU! There is no other backend that will do that so efficiently in the generative AI space.- Hence the GPU backends (OpenCL, Metal, CUDA, soon ROCm and Vulkan) are the defacto way to run llama.cpp these days. Without them, I couldn&#x27;t even run 70B on my desktop, or 33B on my (16GB RAM) laptop. reply throwing_away 31 minutes agoparentROCm works now! I just set it up tonight on a 6900xt with 16gb vram running wayland at the same time. The trick was using the opencl-amd package (somehow rocm packages don&#x27;t depend on opencl, but llama does, idk).I&#x27;m astonished at the results I can get from the q6_K models. reply PoignardAzur 1 hour agoparentprevWhat&#x27;s prompt ingestion? reply ant6n 1 hour agoparentprevWhat’s perplexity? reply regularfry 1 hour agorootparentPerplexity is a measure of how certain the model is of the next token. It&#x27;s calculated by looking at the probabilities that the model calculates for the next token in a stream. If there are several choices for the next token with similar probabilities, that&#x27;s telling you that the model is having a hard time telling what the right answer should be: the model is more perplexed, perplexity is higher. If there&#x27;s a single option with a much higher implied probability than any other, that means the model is more certain, and perplexity is lower.Note that this has nothing to do with whether the answer is objectively correct. It&#x27;s just measuring how confident it is. reply ant6n 32 minutes agorootparentThank you! reply jmorgan 11 hours agoprevThis project&#x27;s been a blast to work with. While it&#x27;s written in C++, it provides a C interface to compile against which makes it especially easy to extend with Go, Python and other runtimes.A few folks and I have been building a tool with it in Go for pulling & running multiple models, and serving them on a REST API: https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollamaIn similar light, you haven&#x27;t checked it out, llama.cpp also has a pretty extensive \"server\" tool (in its examples directory in the repo) with a web ui and support for grammar (e.g. forcing the output to be JSON) reply jay-barronville 10 hours agoparentThank you for the link to your project! I&#x27;ll be playing around with it. reply jcims 9 hours agoparentprevJust wanted to say thanks homie for making something so hairy so accessible. Love it!!! reply wfurney 8 hours agoparentprevI&#x27;ve also been experimenting with the C# implementationhttps:&#x2F;&#x2F;github.com&#x2F;trrahul&#x2F;llama2.cs reply pjmlp 3 hours agorootparentWhy Parallel.For() instead of SIMD types?I would expect a much better performance. reply lovelyviking 1 hour agoparentprev> A few folks and I have been building a tool with it in Go for pulling & running multiple models, and serving them on a REST API: https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollamaWhy this tool was created? What was the reason for creating such tool? Why it was named this way? reply oblio 1 hour agorootparentBecause.On a more serious node, your questions seem very... aggressive.Especially the one about the name, is the name offensive or what? To me it sounds quite benign. reply Roark66 4 hours agoprevIt is useful to mention running inference on modern cpus that have AVX2 is not that bad. Sure it is slower than on the gpu, but you get the benefit of having a single long continuous region of ram.But there is one huge problem why this is not that popular on x86_64. Having to run in fp32. As far as I know our most common ml libraries (pytorch, tf, onnx etc) do not have an option to quantize to 4 bits and they don&#x27;t have an option to run inference at anything other than fp32 on the x86_64 cpus.It is a huge shame. There is openvino which supports int8, but if you can&#x27;t easily quantize large models without a gpu, what use is it? (For small models I suppose).So if anyone figured out a way to quantize a transformer model to 4&#x2F;8 bit and run it on the x86_64 cpu platform I&#x27;m very interested in hearing about it. reply _nalply 2 hours agoparentSubjective experience: AVX512 helps a lot. I would have liked to read more about this. It seems that AVX512 supports fp16 in hardware and allows 32 fused multiplication-add operations per core. So I imagine on a Ryzen 9 with 12 cores you can have 384 simultaneous fused multiplication-add operations. I am not sure whether my estimation is off. Anyone know more than me? reply cameron_b 10 hours agoprevI’ve been working through that repo and managed the 13B dataset on a single Pi4 8gigI’ve also replicated the work in OpenMPI ( from a thread on the llama.cpp GitHub repo ) and today I managed to get the 65B dataset operational on three pi4 nodes.I’m not saying this as any achievement of mine, but as a comment on the current reality of reproducible LLM At home on anything you’ve got.It really feels like this technique has arrived.https:&#x2F;&#x2F;github.com&#x2F;cameronbunce&#x2F;ClusterConfig reply loxias 7 hours agoparent> I’ve also replicated the work in OpenMPI...Oh cool! How did it perform?I wonder if this would be an exciting test for Amazon&#x27;s SRD protocol which appears to be built for HPC. I&#x27;m looking for an excuse to play with it... reply eurekin 10 hours agoparentprevHow many tokens a second? reply cameron_b 9 hours agorootparentThe other way around is whole number math. I added the 3-node output from the 13B model to github, the timings are below. The 3-node 65B job hasn&#x27;t finished yet.llama_print_timings: load time = 17766.29 ms llama_print_timings: sample time = 264.42 ms &#x2F; 128 runs ( 2.07 ms per token, 484.07 tokens per second) llama_print_timings: prompt eval time = 10146.71 ms &#x2F; 8 tokens ( 1268.34 ms per token, 0.79 tokens per second) llama_print_timings: eval time = 287157.12 ms &#x2F; 127 runs ( 2261.08 ms per token, 0.44 tokens per second) llama_print_timings: total time = 297598.22 ms reply eurekin 9 hours agorootparentThis is very interesting and actually in the usable realm, for some use cases reply cameron_b 9 hours agorootparentMy networking setup is not optimal, but it was quite surprising how easy it was to get it all to work. replytysam_and 10 hours agoprevIn my best estimation, Finbarr makes pretty great content, he and I have had a number of positive interactions on Twitter. I tend to have a pretty grumpy disposition towards a lot of modern ML and such as I feel it&#x27;s shovelware, but whenever Finbarr puts out work, I tend to set aside some time to give it a good gander, as I feel like it&#x27;s generally pretty \"meaty\" (which I honestly find pretty hard to do past a certain pace). Well worth the subscribe if you have not done so already (I&#x27;m not affiliated with him, I just really like his work!). reply djmips 49 minutes agoprevAnd BTW this analysis holds true for many workloads. Memory bandwidth is often at the core of performance. reply gautamcgoel 10 hours agoprevI enjoyed this article, but it seems to me that the latency numbers should have units of nanoseconds or maybe CPU cycles. I feel like the article was a bit sloppy with units.Another question that occurs to me is: why do chipmakers even bother putting so many functional units on the chip if almost all workloads are memory bound? Based of the calculations in this article, you could decrease the number of teraflops a modern GPU can perform by a factor of 2 and not notice any appreciable difference in ML performance. reply reissbaker 10 hours agoparent1. I think nanosecond-scale latency numbers on operations taking dozens to hundreds of ms are probably overkill?2. Inference is only one aspect of what GPUs are used for. Many other workloads are compute-bound. That being said, given the recent rise of these kinds of open-source, pre-trained large language models, I wouldn&#x27;t be surprised if future Nvidia product launches offered variants with significantly more VRAM. There would probably be a lot of interest in \"3080-equivalent compute, but 48GB VRAM\" these days — certainly I would take one over a 4090 with 24GB VRAM. (Then again, that&#x27;s basically an A6000, and those go for nearly $7k...) reply djmips 41 minutes agorootparentCompromise on microseconds reply sbierwagen 9 hours agorootparentprevYeah, Nvidia won&#x27;t do big VRAM consumer cards until AMD forces them to. They&#x27;re running flat out just trying to keep up with demand for H100s at forty thousand USD each. reply ColonelPhantom 41 minutes agorootparentOr Intel! They&#x27;re not making any high-end cards currently, but the A770 16GB is pretty decent if you&#x27;re on a budget. The software support isn&#x27;t really amazing yet, but their GPUs have some quite decent matmul acceleration.Unfortunately support for their GPUs is not upstream in Tensorflow or Pytorch, and I don&#x27;t think they&#x27;re well-supported by Llama.cpp either, but Intel does look quite promising. I also believe that Intel oneMKL works on Arc GPUs these days, which in CPU land is amongst the fastest BLAS libraries out there. Also their matrix hardware is accessible using OpenCL extensions, which means that rolling a custom kernel for things related to quantization should be quite possible.(For Tensorflow and PyTorch, you need to install a custom package called Intel Extension for $FRAMEWORK. The PyTorch one got updated to PyTorch 2.0 recently, which is promising.)Currently rumors seem to indicate that their second generation GPUs will go from 32 to 64 Xe cores for the top model, and keep the 256 bit bus. If Intel were to double the VRAM to 32 GB as well (at least as an option, just like the A770 comes in both 8GB and 16GB variants), it&#x27;d immediately make them the crown of consumer VRAM size, which I&#x27;d wager would drive a lot of interest from the ML community. reply zamadatix 10 hours agoparentprevThe primary use case for GPUs in devices, graphics, is not as often memory bound. Even for other general data compute purposes that may not always be the case. It&#x27;s specifically neural nets that have extremely wide batches of extremely simple operations occurring across extremely large chunks of memory where being memory bound is the default case. reply oars 10 hours agoprevGreat article. Don&#x27;t see content like this anywhere else outside of HN. reply redox99 9 hours agoparentYou can find content like this on Twitter if you follow the right people. In fact I read this article before it was even posted here because @karpathy tweeted about it. reply notuseful 9 hours agorootparentTwitter no longer exists. It&#x27;s called X now and it&#x27;s a little shameful to continue to use it. reply DiggyJohnson 4 hours agorootparentI’ve never been a regular Twitter user, and don&#x27;t really enjoy the platform, but this comment of yours is either abusing the word “shameful” or betraying a major lack of understanding that you can’t expect other people to care deeply about the things you care deeply about.There’s a lot of shameful things in this world, GP using twitter isn’t one of them. Not even a “little.” reply class4behavior 3 hours agorootparentIt&#x27;s betraying the word shameful in that it&#x27;s an utter understatement.If people don&#x27;t care about supporting companies that enable and spread far-right content and groups, it&#x27;s they who are a problem.To say nothing of the pure disregard of the human right to privacy (and with AI now IP as well) that is forced on the the rest of the world by the dominance of the US market. reply Joe_Boogz 8 hours agorootparentprevShameful? Just because you dislike it doesn’t mean everyone has to. reply PKop 8 hours agorootparentprevSays you. I&#x27;d prefer to take advantage of the resources available there. Your silly moralism is easy to ignore reply gdiamos 6 hours agoprevMemory bound token generation is a limitation of transformer decoders.In the past, hardware has motivation algorithm innovations.I’m curious how long it will take until we see more hardware friendly models. reply marmaduke 1 hour agoparentThe Rwkv family of models qualifies, since it computes like a recurrent network at inference time. reply aargh_aargh 3 hours agoprevOriginally posted here:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35192102 reply pedrovhb 9 hours agoprevGiven the massive imbalance in the memory bandwidth bottleneck, I wonder why specialized hardware is the way it is. Is there some use case in which processing is the bottleneck, or at least it&#x27;s more even? Are we expecting some software paradigm shift which will change the balance? Why couldn&#x27;t they just make a cheaper, more rounded card which isn&#x27;t heavily underutilized because of a large bottleneck? reply brucethemoose2 7 hours agoparentSee: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Random-access_memory#Memory_...But llama.cpp, and llms in general, are very atypically memory bound, even for AI workloads. Other models&#x2F;backends will typically make more use of compute and cache.For llms specifically, cloud hosts will batch requests to make better use of GPUs.But you are not far off. There is a proposition to pipe chips with very fast memory (aka no external memory at all) together: https:&#x2F;&#x2F;www.nextplatform.com&#x2F;2023&#x2F;07&#x2F;12&#x2F;microsofts-chiplet-c... reply andrewstuart 9 hours agoprev>> Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers.So how about using an APU - a CPU with GPU built in. The GPU shares the CPU memory, so if you want you can have 128GB RAM and allocate 100GB to the GPU.Sure the GPU i not fast, but if memory is important..... reply brucethemoose2 7 hours agoparentYou can, right now, with the OpenCL backend.And for the moment, its slower than pure CPU. Optimizing for IGPs is not trivial.MLC&#x27;s Vulkan backend is actually quite good on my AMD APU, but unfortunately it won&#x27;t split the model to a dGPU. reply ladberg 6 hours agoparentprevYou&#x27;re thinking about the wrong bandwidth here. The article is talking about going from the GPU&#x27;s RAMGPU cores (i.e. through load&#x2F;store instructions in a cuda kernel), not from CPU&#x27;s RAMGPU&#x27;s RAM. That kind of bandwidth is still important but usually not the bottleneck on most ML workloads. reply thrdbndndn 6 hours agorootparentI&#x27;m still confused.The author (correctly) made a distinction about the two like you said, but at the end when talking about Raspberry Pi 4 they use a number (~4GB&#x2F;s of memory bandwidth) from an article [1] which I can only assume is NOT about graphics memory or its bandwidth (do Raspberry PIs even have it?).And how exactly is the bandwidth counted if I use integrated GPU (like i5 13600K)? Or pure CPU?[1] https:&#x2F;&#x2F;forums.raspberrypi.com&#x2F;viewtopic.php?t=281183 reply ColonelPhantom 59 minutes agorootparentCPUs and GPUs both interface with their own memory, and those memories have a certain bandwidth. Generally, CPU memory has relatively little bandwidth, but relatively good latency. (For example, an i9-13900K supports memory up to around 100 GB&#x2F;s, while even my previous GPU, a midrange Radeon HD 7850 from 2012, has over 150 GB&#x2F;s of bandwidth).An integrated GPU shares memory with the CPU, so at best it gets the same amount of bandwidth assuming the CPU is not using any (which is rather unlikely).A dedicated GPU has its own private high-bandwidth memory (an RTX 4090 has a memory bandwidth of over 1000 GB&#x2F;s), but to get anything in there it needs to be loaded over the PCIe bus (which has a measly 32 GB&#x2F;s bandwidth for PCIe 4.0 x16).That said, CPU memory does have one big advantage: it tends to be much larger. You can pair up to 192 GB with a regular Ryzen 7000 CPU, while consumer GPUs don&#x27;t go above 24 GB of memory (RTX 4090, RX 7900 XTX). (There are bigger GPUs out there, but those are generally intended for datacenters, and if you go that route, an Epyc or Xeon CPU can also support much more memory than a plain desktop Ryzen, although you can also slot multiple GPUs into a single server.)I believe that for LLM performance, memory bandwidth is key, because all the neural network layers need to be streamed from memory, and very little work is done with it each time, although I guess batching operations could help if you&#x27;re working at scale, since each weight would be applied multiple times then. reply regularfry 39 minutes agorootparentprevRaspberry Pi&#x27;s do have a usable GPU, but using it for computation is not a particularly well-traveled path. I think that&#x27;s a shame. The pre-4 models have a different Broadcom graphics core to the 4, and it looks like you can get useful work out of both, but they are different enough that it&#x27;s a rebuild between the generations. reply BobbyJo 9 hours agoparentprevMost CPU RAM is much slower than GPU RAM. GPUs typically pack RAM 2 generations ahead with a wider bus than anything you&#x27;d find on a consumer motherboard. reply winwang 9 hours agorootparentFor reference, DDR4-3200 in quad channel is ~100 GB&#x2F;s while a 3090&#x27;s VRAM is 960 GB&#x2F;s. Of course, most consumers only have dual channel.M1 Pro is 200 and M1 Max is 400. Which is slow for GPU memory, but incredible for main memory -- although I&#x27;m not sure how much of that a single core can actually pull. reply KolenCh 8 hours agorootparentThe Anand tech article has profiled this. IIRC the CPU cores has access to half that band width only, which is still quite a lot. reply andrewstuart 8 hours agorootparentprevThe AMD Ryzen™ 9 7940HS uses DDR5-5600, which I understand to be about 89.6GB&#x2F;s in a dual channel setup. reply mhh__ 8 hours agoparentprevThis is basically the appeal of the apple chips in this domain. Apple have fuck-you money so they have a bunch of high-bandwidth decent-latency soldered onto the chip. reply ggm 9 hours agoprev80&#x2F;20 rule. Approximations have smaller, less accurate approximates which do in many contexts. If your goal is to reach America, a crude compass and speed reasoning works. If you want to target an ICBM you need better positional accuracy. reply TMWNN 11 hours agoprevBah. We still haven&#x27;t equaled the rude and hateful AI achieved in a microcomputer in 1981.reply RosanaAnaDana 11 hours agoparentWe can keep reaching for that rainbow. reply ggm 10 hours agorootparentA good analogy: as you approach the rainbow moves off. Others see you in it, but you can confirm it&#x27;s somewhere else. It&#x27;s an effect, a side effect, it&#x27;s pretty and we value it. There&#x27;s no pot of gold in literal sense, it&#x27;s ephemeral value in other products. reply Havoc 11 hours agoprevWhat I find more stunning is what this implies going forward. If tech advances as it tends to do then having a 200bn model fit into consumer hardware isn&#x27;t that far away.Might not be AGI but I think cliched as it is that would \"change everything\". If not at 200 then 400 or whatever. Doesn&#x27;t matter - the direction of travel seems certain. reply gct 11 hours agoparentBasically Ray Kurzweil&#x27;s argument, he&#x27;s been saying $1000 worth of compute will be able to match human performance around 2029 for decades now. reply KerrAvon 10 hours agorootparentFirst, there has to be something capable of matching human performance at a much higher cost. This is still just spicy autocomplete. reply Waterluvian 10 hours agorootparentHumans just do spicy autocomplete too. reply not2b 10 hours agorootparentNo, a human isn&#x27;t born with a set of knowledge like a freshly trained LLM, keeping the model fixed and responding to input. The analog to the model changes based on the human&#x27;s experience. Just making bigger and bigger LLMs won&#x27;t give you this. reply mlboss 8 hours agorootparentHumans are born with millions of years of evolutionary training embedded in their dna and brain. We are not born with nothing. reply KirillPanov 51 minutes agorootparentYeah but chimpanzees and cats and mice have all the same million-year-old stuff that we do.The million-year-old stuff is not what makes humans interesting. reply regularfry 37 minutes agorootparentIt&#x27;s what makes humans possible. \"Necessary but not sufficient\" is the phrase that pays. reply tysam_and 10 hours agorootparentprevAh, but I believe you forget the implicit biases of genetic programming. Instincts in my experience are the skeleton, and in a sense the default basis functions for the structure of how we live, see, do, and learn. reply not2b 9 hours agorootparentNo, I don&#x27;t forget that. There&#x27;s obviously a starting point, behaviors and abilities that newborns already have. The point is that the model is not static. reply Waterluvian 10 hours agorootparentprevSo a human is different because it keeps training its neural network? reply eurekin 10 hours agorootparentWhoa, imagine you get a good base LLM model and save all conversations with it. Run a batch process every night to fine tune a LORA on convo dataset. If I ever came across such a chat bot I&#x27;d probably freak out as to why it remembers things outside of the context window, without summarisation reply MichaelZuo 9 hours agorootparentThat&#x27;s a pretty neat idea, I would be surprised if no one is already working on that. reply notuseful 9 hours agorootparentI did it years ago on a lark with a seq2seq model in a matrix chat room. reply eurekin 7 hours agorootparentHow did it perform? Was it well received by members? reply notuseful 5 hours agorootparentPoorly! It was a small seq2seq and was gibberish to start with. Although it did tell my friend that it loved him which was nice. replyPaulDavisThe1st 8 hours agorootparentprevone reason why a human is different: just based on word count alone, most LLM&#x27;s are trained on 3-5 orders of magnitude more input.could be a difference that makes no difference, or ... reply dTal 7 hours agorootparentBit of an unfair comparison when humans also have a bunch of senses that LLMs don&#x27;t have. They might be trained on orders of magnitude more words, but more data? Doubtful. reply gerdesj 10 hours agorootparentprevReally? I can guess at what spicy autocomplete might actually mean but I doubt a LLM ... OK ChatGPT did a pretty good job of it (I&#x27;ve just asked it), whilst sidestepping the definition of spicy in this context. It is after all a very good next word guesser, given a context, and its not ... me! I am capable of hallucinating but it was 30 odd years ago since I hunted out certain fungi on Dartmoor, or smoked hemp.To be fair, we humans do often interrupt each other to second guess a sentence completion. Done correctly it is a brief satisfying collaboration. Done wrong ... I&#x27;ve been married for 18 years and know when to bite my tongue, but I still get it wrong from time to time - sometimes deliberately. Despite that, me and the wiff can autocomplete each other&#x27;s sentences with uncanny accuracy and end up with perfect harmony or a cough slight disagreement as a result.We are getting some phenomenal slide rules these days but the darleks are not going to be flying up the stairwell just yet, nor will SkyNet be taking over tomorrow.That said, you just know that some noddy is trying to sell a nuclear \"deterrent\" LLM AI thingie somewhere. Thankfully, production military equipment takes quite a while to get to deployment. There is a good chance that we will get to grips with all this stuff before SkyNet is let loose for real 8) reply goatlover 10 hours agorootparentprevNo they don&#x27;t. You&#x27;re \"just\" doing what everyone else in the past has done with the brain&#x2F;human intelligence and using the latest technology as a metaphor without realizing it. reply Waterluvian 10 hours agorootparentWe want to think we’re exceptional but all we can do is say “human consciousness is special” without having any way of measuring it or disproving the assertion that we’re just really fancy pattern matchers.Take any metaphor you want, it’s the same outcome: we may all be philosophical zombies. reply roarcher 10 hours agorootparentWe may \"just\" be neural networks that run on meat instead of silicon, but that does not mean that we&#x27;re LLMs. reply Waterluvian 10 hours agorootparentWhy doesn’t it? reply roarcher 6 hours agorootparentBecause not all neural networks are LLMs.A GAN is a neural network, does that make it an LLM? reply marginalia_nu 10 hours agorootparentprevIt&#x27;s a formal logical error. One does not follow from the other without affirming the consequent. reply hkt 5 hours agorootparentprevWe have inputs other than words, for a start reply goatlover 8 hours agorootparentprevI’m conscious, maybe you’re not? Not that I really believe that. I think you probably see colors and hear sounds, even in your dreams! But engineering types tend to be persuaded by a particular view of the world, failing to understand that it is a view and not nature itself. reply catchnear4321 10 hours agorootparentprevmonkeys make monkeys accidentally.monkeys make meseeks on purpose.there is a difference, but will it be fun? reply Waterluvian 10 hours agorootparentI am 100% invested in how much ridiculous fun this era is going to be. Right up until the moment when it becomes a horror. reply ggm 10 hours agorootparentprevThe irony in your statement is immense. Yes, Kurzeweil has been saying this for decades. No it doesn&#x27;t mean AGI is close. These llms do nothing to advance AGI. There is no theoretical basis to the belief in emergent intelligence from statistical language models and the answers are amazingly good, highly unreliable and parrot meaning at best. There is no inductance, and no inteospection and no understanding of the deep semantic meaning of the language presented. There&#x27;s no intelligence. reply kimixa 9 hours agorootparentThe lack of concept of \"knowledge\" is a big one for me - if that&#x27;s an emergent thing it hasn&#x27;t even shown hints of this yet. This to me seems a pretty hard line right now, as it limits their capability of things even inexperienced humans can do - namely decide if they actually know something, and identify when they don&#x27;t know something and attempt to fix that - e.g. asking for clarification on vague inputs, or deciding if something is actually truth or fiction.That then ties into another limitation right now - how after a training the model is pretty static, so cannot learn and has no state outside it&#x27;s context buffer. This could just be another point where a few orders of magnitude more computing power can \"fix\" it, doing whole training steps between each input to actually incorporate new information and corrections into itself instead of relying on a fixed size small context.But I&#x27;m not deep enough into things to say if they&#x27;re fundamental issues, or current techniques will start displaying those effects as emergent characteristics as the complexity and training increases. There&#x27;s been a few other examples when \"known\" techniques start to show unexpected characteristics down the line as they are scaled up, so can&#x27;t really say for sure they&#x27;ll &#x2F;never&#x2F; be shown, just that the current examples don&#x27;t seem to show even the beginnings of that sort of thing. reply coolspot 10 hours agorootparentprev> the answers are amazingly good, highly unreliable and parrot meaning at best. There is no inductance, and no inteospection and no understanding of the deep semantic meaning of the language presented. There&#x27;s no intelligence.Can say the same about a half of population, tbh reply binary132 10 hours agorootparentSo what you’re saying is.... we’re going to have human-level AI, and it’s going to be incredibly stupid reply bugglebeetle 8 hours agorootparentBAAGI (Below Average Artificial General Intelligence). reply djmips 31 minutes agorootparentOr more simply not AGI but AGS reply glimshe 10 hours agorootparentprevWhy do you say they do nothing to advance AGI? Do you know what it takes to advance AGI? It&#x27;s hard to state that without knowing how AGI would work yourself.LLMs would be considered magic just a couple years ago. Sure, not AGI but behaves just like one for certain workloads. I find hard to believe we&#x27;re not a bit closer now - or maybe even a lot closer. reply shrimpx 10 hours agorootparentAGI should have morals, opinions, self-reflection, learn continuously from sensor data, reason, realize when they’re proven wrong and update their model of the world, and be creative. So far LLMs exhibit none of those. But LLMs exhibit a digestible distillation of a very large body of data which may be a component of an AGI.But you can have an AGI that doesn’t have encyclopedic knowledge but it’s still highly intelligent, so I don’t think LLMs have to be an intrinsic component. reply zarzavat 9 hours agorootparentThat is not what AGI means. AGI = Artificial General Intelligence.1. Artificial = we made it2. General = it can solve problems in any field3. Intelligence = the ability to solve problemsA chess engine is a very strong Artificial Intelligence. But it’s not very General, it can only evaluate chess positions.GPT-4 is very General, you can ask it about any question and get a somewhat reasonable answer. But it’s not very intelligent, often the answer is wrong.You’re talking about an Artificial Human. That’s a different problem. Intelligence is not species dependent. Dolphins are intelligent (a bit), aliens can be intelligent and have zero emotions or conception of self. There’s certainly plenty of amoral intelligent serial killers. reply shrimpx 1 hour agorootparent> That’s a different problem.That&#x27;s news to me. AGI (or strong AI) is typically defined as \"human-level intelligence\", or \"perform any task that a human or animal can.\" Humans and animals often perform tasks that are critically reliant on being conscious, emoting, reading body language, reasoning, etc.Not only that but prominent thinkers who have carved out the notion of AGI (or Strong AI) tend to have consciousness, mental states, and emotions at the core of it.I think what you&#x27;re talking about is a multi-task AI, not an AGI. reply ggm 8 hours agorootparentprevWe don&#x27;t have a good computer model of dolphin intelligence and the llms are not even remotely close to consciousness or dolphins, dogs, parrots on the intelligence Front. reply zarzavat 7 hours agorootparentConsciousness ≠ intelligence.Consciousness: being “awake” and perceiving the world.Intelligence: solving problems, finding the truth.Consciousness is perceiving the world, whereas intelligence is understanding it. reply PaulDavisThe1st 8 hours agorootparentprevGPT-4 solves only 1 problem: what is the most likely stream of tokens to follow what we already have.It is remarkably good at this. But there&#x27;s absolutely zero reason to believe it can solve any other problem at all. reply zarzavat 6 hours agorootparentHow do humans write if not by intuiting what word comes after another?Intelligence is the ability of that next word decision procedure to determine a next word that is aligned with our human intuition and model of truth.I believe what you’re getting at is modality, that GPT-4 only provides responses in text. You can’t ask it to drive a car, or paint like Dall-e. And that’s a fair criticism, but it’s mostly just because it would make the models too large and slow, not because we don’t know how to do it. The thing we don’t know how to do is make a model reason as well as a human, and it makes sense to try to solve that in the text domain first rather than making highly multimodal models that reason poorly in all domains. reply PaulDavisThe1st 6 hours agorootparent> How do humans write if not by intuiting what word comes after another?We don&#x27;t know. It may turn out that we use mechanisms similar to LLMs, or it might be something entirely different.As for the rest: nobody knows how to make ChatGPT butter a piece of toast, let alone drive a car.ChatGPT does not reason about text, either. reply og_kalu 5 hours agorootparent>nobody knows how to make ChatGPT butter a piece of toastThere is plenty of research on LLMs successfully piloting robots.It&#x27;s but no means a solved problem but \"Nobody knows how\" is a stretch.https:&#x2F;&#x2F;tidybot.cs.princeton.edu&#x2F; https:&#x2F;&#x2F;innermonologue.github.io&#x2F;>ChatGPT does not reason about text, either.It does and there&#x27;s plenty of output to demonstrate that. reply astrange 2 hours agorootparentprevThat&#x27;s an interface, not an implementation.(\"The most likely\" out of what distribution? The model&#x27;s distribution. So that just means \"what the model thinks the answer to your question is\".) replypanarky 9 hours agorootparentprevHis prediction was that one human brain&#x27;s worth of computing power could be acquired for $1000 by 2029. That still seems reasonable.That&#x27;s not the same as AGI or the singularity. reply ggm 8 hours agorootparentYou have a metric for human brains worth of computing power which hasn&#x27;t already been exceeded? I can&#x27;t do infinite precision arithmetic or the RSA algorithm in my head, or index a billion strings into lexical sort order.But I am human, I am conscious and no visible VLSI work or algorithmic model will lead to AGI or a human equivalent computing power by 2029. Let alone for $1000. reply dekhn 7 hours agorootparentwell, you could just be hallucinating your own consciousness. By 2029 it seems not unreasonable to expect that the most sophisticated models will carry out visual and auditory interactions which could fool even the most sophisticated viewer. At that point, what really does consciousness mean? If the robot insisted to me it was conscious, how can I really say no? reply ggm 5 hours agorootparent> it seems not unreasonableThis is where we differ. reply circuit10 10 hours agorootparentprevBy most measures you could think of for intelligence languages models are improving, so I don’t see why you think this wouldn’t lead to something at least almost human-level if you scaled it up enoughOf course there could be some wall somewhere but I don’t see why there would be reply croes 9 hours agorootparentBecause you need more training data for better results and they are running out of new training data. reply ddingus 6 hours agorootparentI don&#x27;t think so.While it may be true that new data is coming in at a trickle these days, due to things like Discord, Slack, et al. all locking conversation and context up, as well as the daily volume of chapter is small relative to what is out there now.The fact is that training data can be used in many different ways and I bet you we see the products of that fairly quickly as those who see this same as I do reach a point where they want to show n tell and test. reply croes 3 hours agorootparent>The fact is that training data can be used in many different ways and I bet you we see the products of that fairly quickly as those who see this same as I do reach a point where they want to show n tell and test.Sounds like wishful thinking to overcome the limitations of LLMs.At the same time we get more and more texts generated by LLMs so it gets harder to get actual man made texts. reply ggm 8 hours agorootparentprevThat&#x27;s \"we need a larger cowbell\" thinking. It&#x27;s not a theory of mind, it&#x27;s wishful thinking that it will.. emerge. Absent theory I don&#x27;t think moar will make it emerge, no. reply Aerroon 10 hours agoparentprevA 200b 4-bit quantized model could potentially fit into 128 GB of RAM. The inference would just be really slow.Ie you could technically run something like that today.I think more VRAM on GPUs isn&#x27;t necessarily a technical limitation either. I think GPU manufacturers could add a lot more VRAM to their cards if they wanted to. The question is whether it would be worth the price increase. reply rootusrootus 10 hours agorootparent> Ie you could technically run something like that today.Yep, on higher end machines it should already be feasible. I can do 2.5-3 tok&#x2F;sec on a 70B model quantized at 4 bit today with my MacBook Pro M2 MAX w&#x2F;96GB. It&#x27;s a little slower than a 30B, but the difference is less than I had guessed it would be. That&#x27;s not super fast, but it&#x27;s usable.And that&#x27;s on a machine that isn&#x27;t designed for this workload. Over the next few years things should improve quite a bit. 200B does not seem like a reach. reply eurekin 10 hours agorootparentprevAbout the RAM. I doubt they wanted to do that, since basic gpu function is to render a frame in as little ms as possible. Currently VRAM is latency optimized on consumer gpus and all memory chips are an inch away from the gpu. Light only travels as far in the gigahertz realm. Thats why they started mounting vram chips on both sides of the board, cause there was no more place left on the first side.Just checked: light travels 30cm in one nanosecond. So if the gpu is running at 4GHz it goes only 7.5 cm. reply redox99 10 hours agorootparentVRAM is not latency optimized. VRAM has worse latency than your CPU RAM. The reason why it&#x27;s mounted closer is because of signal integrity because of higher frequencies, not because of latency. reply eurekin 9 hours agorootparentInteresting. Where can I read more about that? reply redox99 9 hours agorootparentSorry can&#x27;t provide any resources right now. If you search a bit I&#x27;m sure you&#x27;ll find some latency comparisons between DDR and GDDR.But basically GPU memory (GDDR5&#x2F;6&#x2F;6X&#x2F;etc) is optimized for bandwidth (because GPUs need to move a lot of data, have few branches, few unknown data dependencies, high spatial locality). CPU memory is more optimized for latency (because of branchy code). replycsjh 11 hours agoparentprevIMO the direction we&#x27;re going seems more like having a few small models in a MoE that are equivalent to a current 200bn model reply Mertax 9 hours agoparentprevAnd then things like neural implants and BCIs -- seems like your dog could have language capabilities sooner than you&#x27;d think ;) reply ant6n 1 hour agoprevThis article would probably be useful for a lot more people if it spent just a couple of sentences introducing the various parameters, rather than just throwing variable names at the reader. Interestingly, a whole paragraph is spent on explaining n_bytes. reply thomasfromcdnjs 10 hours agoprevThank you very much for writing this out! reply fnbr 3 hours agoparentYou’re welcome :) reply fouc 5 hours agoprevDid anyone notice that apparently a M2 Macbook Pro is only 16x faster than a Pixel 5? Not sure that makes sense. reply sharkjacobs 5 hours agoparentI’m not an expert in this but my “does that feel right wrong” sense isn’t going offPixel 5 is 2-3 years old. CPUs aren’t doubling in speed every 2 years, but let’s very generously say we expect current designs to be 4x faster than 2-3 year old equivalent.Apple silicon is faster than other ARM chips, so if we imagine that’s another 2x we’re up to 8xCommon wisdom is that “real computers” are faster than phones, but the difference between the A16 and M2 is less than 2x for multi core, and much much less for single core benchmarks. Rounding up, another 2x is 16xMaybe there are characteristics of the two devices which make this more surprising, I’d be interested to learn more. reply astrange 2 hours agoparentprevThe phone slows down as it heats up, and then slows down infinitely when the battery dies.(With some workloads and chargers, it&#x27;s possible running on a phone would drain the battery faster than it can recharge even if you plugged it in.) reply Animats 10 hours agoprevIs this single-thread? Or are they putting all available CPUs on the problem? reply sharms 9 hours agoparentThe problem is memory bandwidth rather than CPU cores: \"Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them much easier to serve\" reply brucethemoose2 7 hours agoparentprevIts complicated.If you dont have a GPU, prompt ingestion is totally threaded. The more cores the better.For generating tokens, more cores helps to a point, but then:- You start saturating the memory bus, and performance plateaus.- There is some overhead from the threading implementation, and too many threads hurts performance.The ideal number of threads varys per CPU. For instance, using hyperthreaded cores or Apple&#x2F;Intel e cores typically hurts performance... But not always. You just have to test and see. reply __loam 11 hours agoprevWill be interesting to see what people can do with local models, particularly for open source programming tools and PCG models for video games. reply cratermoon 10 hours agoparentThey&#x27;ll probably write fanfic and some Harry Potter ships. reply m3kw9 9 hours agoprevIs there a Llama2 65b quantized version for Mac M2? reply eurekin 9 hours agoparentI&#x27;d look for llama2 thebloke 70b GGML on hugginface.Update, this might work: https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-70B-GGMLExample command mentions the 4bit variant:.&#x2F;main -m llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p \"Llamas are\" reply kitanata 7 hours agoprev [3 more] [flagged] tysam_and 7 hours agoparent [–] I&#x27;m feeling a sense of very wry irony will approach us soon.... reply kitanata 7 hours agorootparent [–] Let’s hope so. Sad I had to spam the site to get my voice heard.We have a right to be forgotten no matter what Paul Graham and the other greedy fucks at ycombinator think. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author examines the feasibility of running the LLaMa inference code in raw C++ on different hardware, including devices with limited RAM like the Raspberry Pi.",
      "The significance of memory bandwidth in deep learning is discussed, along with the benefits of using lower precision models to decrease memory usage.",
      "Calculations and examples are provided for running LLaMa on various hardware, with an emphasis on the importance of memory bandwidth when sampling from transformers. Additionally, the author highlights the importance of distillation and extended training for smaller models to improve efficiency. However, they acknowledge the possibility of errors in their calculations and encourage feedback."
    ],
    "commentSummary": [
      "The discussions revolve around quantizing models, Llama language model's capabilities, CPU-based inference, memory bandwidth limitations, AI's potential to match human performance, limitations and potential of language models for AGI, and challenges in obtaining human-made text.",
      "Participants cover specific platforms, methods, and hardware configurations.",
      "Differing opinions are expressed on the feasibility and future advancements of AI models."
    ],
    "points": 497,
    "commentCount": 172,
    "retryCount": 0,
    "time": 1692137897
  },
  {
    "id": 37136898,
    "title": "Opendream: A layer-based UI for Stable Diffusion",
    "originLink": "https://github.com/varunshenoy/opendream",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up varunshenoy / opendream Public Notifications Fork 29 Star 734 Code Issues 9 Pull requests 1 Actions Projects Security Insights varunshenoy/opendream main 1 branch 0 tags Go to file Code Latest commit varunshenoy Update README.md 00deb93 Git stats 119 commits Files Type Name Latest commit message Commit time images edited opendream make controlnets clearer webapp/opendream-ui added textarea for prompt .gitignore remove workflows LICENSE done README.md Update README.md requirements.txt Update requirements.txt run_opendream.sh use venv in script README.md Opendream: A Web UI For the Rest of Us 💭 🎨 Opendream brings much needed and familiar features, such as layering, non-destructive editing, portability, and easy-to-write extensions, to your Stable Diffusion workflows. Check out our demo video. Getting started Prerequisites: Make sure you have Node installed. You can download it here. Clone this repository. Navigate to this project within your terminal and run sh ./run_opendream.sh. After ~30 seconds, both the frontend and backend of the Opendream system should be up and running. Features Diffusion models have emerged as powerful tools in the world of image generation and manipulation. While they offer significant benefits, these models are often considered black boxes due to their inherent complexity. The current diffusion image generation ecosystem is defined by tools that allow one-off image manipulation tasks to control these models - text2img, in-painting, pix2pix, among others. For example, popular interfaces like Automatic1111, Midjourney, and Stability.AI's DreamStudio only support destructive editing: each edit \"consumes\" the previous image. This means users cannot easily build off of previous images or run multiple experiments on the same image, limiting their options for creative exploration. Layering and Non-destructive Editing Non-destructive editing is a method of image manipulation that preserves the original image data while allowing users to make adjustments and modifications without overwriting previous work. This approach facilitates experimentation and provides more control over the editing process by using layers and masks. When you delete a layer, all layers after it also get deleted. This guarantees that all layers currently on the canvas are a product of other existing layers. This also allows one to deterministically \"replay\" a workflow. Like Photoshop, Opendream supports non-destructive editing out of the box. Learn more about the principles of non-destructive editing in Photoshop here. Save and Share Workflows Users can also save their current workflows into a portable file format that can be opened up at a later time or shared with collaborators. In this context, a \"state\" is just a JSON file describing all of the current layers and how they were created. Support Simple to Write, Easy to Install Extensions As the open-source ecosystem flourishes around these models and tools, extensibility has also become a major concern. While Automatic1111 does offer extensions, they are often difficult to program, use, and install. It is far from being as full-featured as an application like Adobe Photoshop. As new features for Stable Diffusion, like ControlNet, are released, users should be able to seamlessly integrate them into their artistic workflows with minimal overload and time. Opendream makes writing and using new diffusion features as simple as writing a Python function. Keep reading to learn how. Extensions From the get-go, Opendream supports two key primitive operations baked into the core system: dream and mask_and_inpaint. In this repository, extensions for instruct_pix2pix, controlnet_canny, controlnet_openpose, and sam (Segment Anything) are provided. Any image manipulation logic can be easily written as an extension. With extensions, you can also decide how certain operations work. For example, you can override the dream operation to use OpenAI's DALL-E instead or call a serverless endpoint on a service like AWS or Replicate. Here's an example using Baseten. Loading an Existing Extension There are two ways to load extensions. Install a pre-written one through the Web UI. (Manual) Download a valid extension file (or write one yourself!) and add it to the opendream/extensions folder. Instructions for writing your own extension are below. Here is a sampling of currently supported extensions. You can use the links to install any given extension through the Web UI. Extension Link OpenAI's DALL-E File Serverless Stable Diffusion File Instruct Pix2Pix File ControlNet Canny File ControlNet Openpose File Segment Anything File PhotoshopGPT Gist Note that extensions may have their own requirements you would need to include in the requirements.txt file. For example, you would need to add openai if you want to use the DALL-E extension. Feel free to make a PR if you create a useful extension! Writing Your Own Extension Users can write their own extensions as follows: Create a new Python file in the opendream/extensions folder. Write a method with type hints and a @opendream.define_op decorator. This decorator registers this method with the Opendream backend. The method has a few requirements: Parameters must have type hints. These enable the backend to generate a schema for the input which is parsed into form components on the frontend. Valid types include: str, int, float, Layer, MaskLayer, or ImageLayer. The only valid return types are a Layer or a list of Layer objects. Contributions and Licensing Opendream was built by Varun Shenoy, Eric Lou, Shashank Rammoorthy, and Rahul Shiv as a part of Stanford's CS 348K. Feel free to provide any contibutions you deem necessary or useful. This project is licensed under the MIT License. About An extensible, easy-to-use, and portable diffusion web UI 👨🎨 Topics ai image-generation diffusion stable-diffusion automatic-1111 Resources Readme License MIT license Activity Stars 734 stars Watchers 7 watching Forks 29 forks Report repository Releases No releases published Packages No packages published Contributors 5 Languages JavaScript 58.3% Python 37.1% HTML 2.7% CSS 1.5% Shell 0.4% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37136898",
    "commentBody": "Opendream: A layer-based UI for Stable DiffusionHacker NewspastloginOpendream: A layer-based UI for Stable Diffusion (github.com/varunshenoy) 407 points by varunshenoy 16 hours ago| hidepastfavorite110 comments cateye 3 hours agoIt makes more sense to embed stable diffusion capabilities into well-established image editors such as Gimp, Photoshop, Krita, or Figma, which come with layered, non-destructive functionalities, rather than attempting the opposite approach.https:&#x2F;&#x2F;github.com&#x2F;Interpause&#x2F;auto-sd-paint-ext https:&#x2F;&#x2F;github.com&#x2F;thndrbrrr&#x2F;gimp-stable-boy https:&#x2F;&#x2F;www.magicbrushai.com&#x2F; reply Timon3 1 hour agoparentI got briefly very excited for non-destructive editing in GIMP, but the website still says this is slated for 3.2. Which functionality were you referring to? reply j-a-a-p 2 hours agoparentprevDepends on where the &#x27;opposite approach&#x27; is aimed to end. If the result is a totally new creative workflow then what is the point of carrying all the ballast of a legacy tool? reply tavavex 14 hours agoprevVery exciting. The \"first-generation\" Stable Diffusion frontends seem to have settled on a specific design philosophy, so it&#x27;s interesting to see new tools (like this or ComfyUI) shake up the way people work with this tool. I hope that in a few years, we&#x27;ll know which philosophy works best. reply TillE 12 hours agoparentOut of all the AI-related tools, generative art frontends are probably the thing most likely to radically change and improve in the next few years.It&#x27;s specifically why I&#x27;ve avoided diving too deep into \"prompt engineering\", because the kind of incantations required today just aren&#x27;t going to be the way most people interact with this stuff for very long. reply orbital-decay 11 hours agorootparent> Out of all the AI-related tools, generative art frontends are probably the thing most likely to radically change and improve in the next few years.The difference between UIs is actually not very relevant today; by now the generic workflow for complex scenes is more or less obvious to anyone who spent time with SD.- Draw basic composition guides. Use them with controlnets or any other generic guidance method to enforce the environment composition you want. Train your own controlnet if you need something specific. (lots of untapped potential here)- Finetune the checkpoint on your reference pictures or use other style transfer methods to enforce the consistent style.- Use manual brush masking, manually guided segmentation (ex. SAM), or prompted segmentation (ex ClipSEG) to select the parts to be replaced with other objects. The choice depends on your case and need to do it procedurally.- Photobash and add detail to the elements of your scene using any composition methods you have (noisy latent composition, inpainting etc) with the masks you created in the previous step. Use advanced guidance (controlnets, t2i adapters etc)- Don&#x27;t bother with any prompts beyond very basic descriptions, as \"prompt engineering\" is slow and unreliable. Don&#x27;t overwhelm the model by trying to fit lots of detail in one pass; use separate passes for separate objects or regions.- Alternative 3D version: build a primitive 3D scene from basic props (shapes, rigs). Render the backdrop and separate objects into separate layers as guides. Use them with controlnets & co to render the scene in a guided manner, combining the objects by latent composition, inpainting, or any other means. This can be used for procedural scenes and animation (although current models lack temporal stability).As long as your tool has all that in one place, it&#x27;s a breeze, regardless of the UI paradigm (admittedly auto1111&#x27;s overloaded gradio looks straight out of a trash compactor nowadays). I expect 2D&#x2F;3D software integrations being the most successful in the future, as they already offer proven UIs and most desirable side features. The problem is that in the current state SD can&#x27;t do much in the production setting, it&#x27;s not a finished product - so there&#x27;s not a lot of interest in software integrations just yet. reply logicallee 10 hours agorootparentThanks for sharing this detailed guide. Can you share an example of the type of resulting image you’ve generated using the above approach?I’ve only just used Dall-E or SD with basic prompts, or sometimes using photoshop afterward. I’m curious what you’ve been able to come up with using your more complex pipeline. reply kadokaelan 10 hours agorootparentprevvizcom.ai ;) reply chefandy 3 hours agorootparentprevAs a commercial artist that&#x27;s worked in several professional creative industries, I find the current textual methods of interacting with generative image AI to be unusable for the vast majority of professional tasks. I think they&#x27;re great for a lot of laypeople because they abstract away things that laypeople don&#x27;t want to have to think about— but in professional workflows, you need specificity at pixel-level granularity, predictability, and repeatability. Those things are all difficult with purpose-built tools and impossible through text prompts. I haven&#x27;t spoken to a single colleague that doesn&#x27;t work in high-volume, low-effort end of their disks&#x2F;fields that disagree. Most commercial artists selling point is deciding exactly what should go into a piece, and implementing it is the easy part.The pro tools that have incorporated generative AI into their workflows are not at all textual. The environment that popularizes this among the general public will look a lot more like canva or maybe Instagram than what&#x27;s popular now. reply fnordpiglet 6 hours agorootparentprevAt some level I agree that the prompt engineering done today to break ChatGPT guard rails are things that barely rise to “interesting hack” levels, but I think that manipulating language to induce specific behavior by an LLM is a powerful skill, and requires a very facile understanding of language in the semantic context of the training corpus. By varying the tone, vocabulary, style, pacing, and obviously the semantics of the original inducing language you can dramatically change the behavior of the LLM. This is less about prompt engineering and being a masterful manipulator of language - and why I don’t fear that LLMs make language skill irrelevant. Those with the most language skill will produce the most compelling and tailored LLM output for a purpose. reply greggsy 11 hours agorootparentprevIt’s entirely likely that there’s much more effort going into generative text - any perceived advancement of generative images is going to be disproportionately skewed due the richness of information that they hold. reply bobboies 12 hours agorootparentprevIncantations are fun! reply fassssst 6 hours agoparentprevPhotoshop Beta does it best. The generative features are just new tools that work as you’d expect with all the existing tools. For example, if you want to do outpainting, just make your canvas bigger and you get a contextual menu where you can (optionally) type a prompt. Inpainting, just make a selection however you want and type a prompt. reply DrSiemer 3 hours agorootparentThe control that offers is extremely limited versus SD in A1111 with all it&#x27;s different models, LoRA&#x27;s, embeddings, extensions and ControlNet types. reply bavell 11 hours agoparentprevI wrote a typescript API generator for ComfyUI, works great - hopefully will have time to release it soon.I think there&#x27;s so much unexplored potential in UI and workflows around generative AI, we&#x27;ve barely scratched the surface. Very exciting times ahead! reply ssalka 11 hours agoparentprevI bet this will be available as an Automatic1111 extension by end of month. reply tavavex 5 hours agorootparentI&#x27;m doubtful about that. A1111 is what I called a \"first-generation frontend\". Both it and all of its extensions follow a specific model for its usage - in general, every tool is contained on its own tab, with each tab having buttons to transfer the outputs into other tools. Radically changing this model would require rewriting so much that it&#x27;d just make sense to use a different frontend in the first place. reply tomalaci 12 hours agoprevI haven&#x27;t followed diffusion image generation development for a while. Where do you find information on what models you can use in the model_ckpt field? Do I need to import them from somewhere? What are the main differences between them and which are more modern or better? reply smusamashah 10 hours agoparentCivitai.com is the current most popular resource for models. Also ckpt format is discourage for security concerns and saftensors is now used instead. reply nickstinemates 11 hours agoparentprevYou can find them on huggingface, or you can reverse engineer which ckpt you want to use based on an image you&#x27;ve seen generated (like at majin[1] - beware, there&#x27;s a lot of NSFW&#x2F;controversial stuff here.)1: https:&#x2F;&#x2F;majinai.art&#x2F; reply CSSer 11 hours agorootparentSome of this is straight up soft-core child porn. This is fucked up. reply jay-barronville 10 hours agorootparentAgreed! I just clicked the link and did a double take. I don&#x27;t care if it&#x27;s AI. This is child porn material, and in my opinion, it should be shut down. reply jvanderbot 9 hours agorootparentprevWhat can someone do about it? reply greggsy 11 hours agorootparentprevI believe illustrations have been deemed to be abuse material, so I wouldn’t be surprised if LE have started looking into it. reply kleiba 11 hours agorootparentWho exactly is being abused here?I for one would much rather give pedophiles an opportunity to fulfil their sexual desires through AI-generated pictures than real ones.Of course, we can talk about the training material. Are there actual child porn images in there? I seriously doubt it but who knows?And perhaps a case could be made that AI-generated child porn could be a gateway to invite people who then seek out non-generated material.But I think these are separate discussions to be had. reply gochi 11 hours agorootparentThey aren&#x27;t separate discussions, they&#x27;re directly tied to determining abuse material. Revenge porn is an example of abusive material, despite the subject not being abused in the material usually, they&#x27;re considered abusive material due to the intent to cause abuse through distribution.So if either case applies, whether it&#x27;s training based on certain images, or it becomes a gateway, these are discussions to be had directly relating to whether or not it should be classified as abuse material.Additionally, I&#x27;m not sure if the recommended help methods by professionals who deal with pedophiles is to let them fulfill their specific fantasies without a care.There are lots of really important discussions to be had, but they&#x27;re all tied to each other basically. We can&#x27;t separate them out, nor should we aim to. reply GaggiX 10 hours agorootparentReading the parent post I think he would believe that revenge porn is abusive material, because there is a person who is getting abused with the distribution of the content, the person didn&#x27;t consent the distribution. CP is abusive because a child was abused in the creation of the content. The doubt that the author of the parent post has is (from what I understand): who is getting abused with the creation and distribution of a generated image? reply gochi 10 hours agorootparentThey&#x27;ve brought up 3 ways in which it can be abusive (trained on abusive materials, created for the intent to abuse, or continuing self abuse rather than seeking help), but argued those should be separate discussions vs my thinking which is that these are linked to determining if such material is abusive. reply kleiba 6 hours agorootparentFirst of all, thanks very much for these comments - all too often, threads quickly deteriorate into flame wars or emotional finger pointing, and I&#x27;m quite happy that this exchange of (opposing) opinions has remained very civil on such a hot topic.I just wanted to clarify that I did not mean that these topics are all unrelated. When I wrote that these are separate discussions to be had, I was rather trying to imply that these questions are important enough to deserve an own treatment. However, I do agree with you that in the end, they all contribute to the question whether or not artificially generated child porn is abusive or not.I do appreciate another sibling comment that points out the relation to other fictional child porn, such as literary works.Additionally, I would like to add another dimension to the topic, namely that IMO, there is often an unspoken underlying assumption that portraits consumers of child porn as (potential) predators. However, unlike a juvenile delinquent who might find it cool to break into a local corner shop at night to steal some cans of beer, pedophiles are usually not attracted to child porn as a matter of choice. Like many sexual preferences, it is often innate, and can also be a burden to them: imagine you know that what gets you on is morally wrong, even a crime, and for most of your life you are forced to suppress your real sexuality as a consequence.I&#x27;m thinking that fictional child porn, even when it&#x27;s not AI-based but perhaps created with photoshop or in form of stories, could potentially help pedophiles to find ways of somehow dealing with their sexuality without actually preying on innocent children.However, all of these thoughts come from a very naive understanding of the subject matter. Neither am I a pedophile myself, nor do I know anyone who is, nor am I a psychologist or something who works in the field. So I am very interested in corrections or additional options - especially, as I pointed out before, if they are done in equally civil ways as they have been so far in this thread. reply sangnoir 10 hours agorootparentprev> Reading the parent post I think he would believe that revenge porn is abusive material, because there is a person who is getting abused with the distribution of the content, the person didn&#x27;t consent the distributionSo would it be okay to distribute \"revenge porn\" imagery after the subject has died? reply GaggiX 9 hours agorootparentwell no, the person didn&#x27;t consent, where is the gotcha? reply HeyImAlex 7 hours agorootparentprevThis discussion isn&#x27;t new[1], and I&#x27;m not sure re-hashing it here will be useful unless you think AI generated child porn is significantly different from any other form of fictional child porn. Photoshop has existed for thirty years, pen and paper for even longer.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Legal_status_of_fictional_porn... reply CSSer 11 hours agorootparentprevGeez that’s disturbing. I clicked having no qualms with nudes, artistic or otherwise. I’m not a prude. I’ve seen my fair share of anime girls and AI nudes. Hell, I was raised on the internet before parental settings were a thing, but I didn’t expect that. It’s so gross how it toes a line too. reply dingnuts 11 hours agorootparentthe Fediverse has a big problem with this, too, and I never hear anyone talking seriously about it reply unshavedyak 10 hours agorootparentWhat is a serious talk, though? Ie what can be done? Isn&#x27;t it a lot like the internet as a whole; report the offenders if you like then move on?To me it&#x27;s akin to encryption being used for illicit&#x2F;illegal activities. Any tool that gives power to people can and will be abused by people you&#x27;d want nothing to do with.What did you have in mind? reply GaggiX 11 hours agorootparentprevcontext: there are several instances that let you upload illustrations without a restriction on what content the image can contain, it can contain minors, rape, incest. These are instances are between if not the biggest and more active Maastodon instances. The reason why they are so active is because platform like Twitter while having a lot of these stuff too sometimes they will ban your account or at least shadowban you. Many Mastodon instances have banned these ones.In my opinion instances should let the user decide what they find problematic or not and unless it&#x27;s just spam they shouldn&#x27;t ban instances. reply GaggiX 11 hours agorootparentprevillustrations are not a problem under the law in the United States, but it has to be seen for generated images indistinguishable from reality or almost. reply samstave 10 hours agorootparentprevYep.There needs to be a REALLY FUCKING STRONG effort to kill all CP AI anything. Full stop.AI should automagically report any attempt at CP. reply idiotsecant 7 hours agorootparentSerious question - Why? Assuming no actual CP was used in the training of the model who is being harmed? Ickiness should not imply illegality unless the ickiness is at the expense of someone else. Swing your fist as much as you want so long as you avoid my nose, and all. reply Timon3 1 hour agorootparent> Serious question - Why? Assuming no actual CP was used in the training of the model who is being harmedI don&#x27;t think an AI model could generate realistic CP without being trained on examples, which would mean there is literally no way for this assumption to be true. reply obvious_thrwawy 8 hours agorootparentprevShould they check the IDs of the models to verify?Imagine getting reported because you generated an image of an anime girl deemed to be only 17.I&#x27;d personally rather live in a world where people generate distasteful images with an AI and have that AI unconstrained than the inevitable one where everything gets locked down and run by large corporations who will ultimately create more harm than someone generating some lolicon. reply nomel 9 hours agorootparentprevI&#x27;ve always assumed this is what will be used to justify the regulation of AI. reply SV_BubbleTime 7 hours agorootparentThe four horsemen of internet censorship.Money laundering, CSAM, terrorism, drugs. reply colechristensen 9 hours agorootparentprevIf you’re running a service you should have automatic filtering, detection, and such.A model by itself though… you might as well ask a pencil to report someone for drawing graffiti. It does not make sense. reply eVeechu7 9 hours agorootparentModels have been trained on something though. They are not analogous to pencils or brushes. reply BHSPitMonkey 9 hours agorootparentThey are responding specifically to the \"AI should automatically report\" suggestion. An AI model on its own (without a service built up around it) would not have any mechanisms to use the network, reach out to an FBI hotline, or do anything like this. Protections &#x2F; limits would have to be added by the operator somehow (or the distributor of models themselves). reply GaggiX 9 hours agorootparentprevIf you have 4 GB of VRAM, you can finetune a model to whatever you want. replyZuiii 7 hours agorootparentprev> https:&#x2F;&#x2F;majinai.art&#x2F;Thank you so much for sharing this. Civitai keeps bugging me to create an account. This doesn&#x27;t seem to suffer from the same flaw. reply bavell 11 hours agorootparentprevAlso CivitAI but beware the NSFWhttps:&#x2F;&#x2F;civitai.com&#x2F; reply lopatin 7 hours agorootparentprevControversial is one way to put it reply Zuiii 6 hours agorootparentIt always amuses me when people who think they&#x27;re the center of the world discover that there are other people with moral takes different than theirs.If no real children were harmed to produce this stuff than it should be treated like any other extreme works of fiction (e.g. violence in video games, graphical descriptions in certain books).Being disgusted is not grounds for banning something lol. reply orbital-decay 11 hours agoparentprev>Where do you find information on what models you can use in the model_ckpt field? Do I need to import them from somewhere?You can train (finetune) your own on your reference material. reply cwkoss 15 hours agoprevVery cool. Would be interesting to train a model on images with alpha channels so outputs would be automatically masked and more easily composable. But maybe masking is so good these days that would be futile?When a user does img-2-img on a layer does it use the context from other visible layers in the generation? reply Zetobal 15 hours agoparentSegmentation is solved... https:&#x2F;&#x2F;github.com&#x2F;RockeyCoss&#x2F;Prompt-Segment-Anything reply michaelt 13 hours agorootparentSegment Anything is neat, but segmentation is far from solved.If the user generates a picture of a horse and rider to add onto another composition - they probably want to include the saddle. reply GaggiX 13 hours agorootparentSAM is also conditioned on points, if it&#x27;s ambiguous what you want to mask you can add a point on the saddle and the model will add it without a problem, segmentation is pretty much solved, I agree with the parent post. reply bavell 11 hours agorootparentIME I haven&#x27;t gotten great results using SAM, maybe it was just the images I was using? They weren&#x27;t great quality and it seemed to struggle with low contrast areas reply Zetobal 57 minutes agorootparentIf it&#x27;s audio, images, cg or video it&#x27;s almost always GiGo. replymdp2021 15 hours agoparentprev> Would be interesting to train a model on images with alpha channelsWould be even more interesting to get an ANN middle system of ontology of the (finally) represented content in order to change the single items.An internal representation of qualified structured items in space as part of the chain. Prompt > accessible internal representation > render. reply dheera 15 hours agoparentprevFor composing this approach works pretty well, maybe the author should consider making a UI for ithttps:&#x2F;&#x2F;multidiffusion.github.io&#x2F; reply mottiden 14 hours agorootparentThanks for posting. Really interesting reply brianjking 14 hours agoprevIs it possible to add SD XL support for this?I&#x27;d love a colab notebook if anyone has the skill and time to do so. reply varunshenoy 12 hours agoparentIf anyone wants to add SDXL support, all you have to do is create a new extension with the correct SDXL logic (loading from HF diffusers, etc.). You could parameterize `num_inference_steps`, for example, to delegate decisions to the user of the extension.If anyone gets to making one before me, please leave a PR! reply magic_hamster 3 hours agoprevWhile automatic1111 is cumbersome and takes s while to learn, it seems far more capable. The layers here are just inpainting (as noted in the repository readme as well). reply antman 12 hours agoprevCan you add a layer with e.g. an image of yourself? reply ttul 12 hours agoparentPretty sure you can do this. Diffusion models by default start with noise, but you can start with any data, including an existing image. For instance, you could import a photo of yourself, mask the eyes and then ask the model to make them green. reply asynchronous 13 hours agoprevVery cool honestly, seems like a much needed improvement over Automatic. Does it support LoRa&#x2F;will it support in near future? reply varunshenoy 10 hours agoparentYou can write an extension to support LoRA (~10 lines of Python HF Diffusers code).If you get to this before me, please create a PR! reply HeartStrings 1 hour agoprevHow is this better than A1111? reply _sys49152 9 hours agoprevits gonna be breathtaking when this technology gets close enough to make legit cartoons and animations. layers is a step closer to getting there. reply etra0 7 hours agoparentCorridor Crew did a some sort of anime using this technique [1] and then they did two videos [2, 3] explaining the technology behind. Quite interesting if you ask me!There still are some issues with the eyes and a bit of flickering but at the speed everything is moving I wouldn&#x27;t be surprised if this improves in a year or two.Needless to say, there&#x27;s still a lot of artistry involved in such a process so anything is yet to be completely automated.[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tWZOEFvczzA[2] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=FQ6z90MuURM[3] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mUFlOynaUyk reply synapticpaint 8 hours agoparentprevThis technology is already close to making animation. Check out some of my experiments with text to video here:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=CgKNTAjQpkkhttps:&#x2F;&#x2F;youtu.be&#x2F;X0AhqMhEe-c reply ryukoposting 11 hours agoprevIf it can handle LoRAs, I&#x27;ll be sure to try it out this weekend. reply varunshenoy 10 hours agoparentLoRAs can be handled as a straight-forward Python extension! reply toenail 13 hours agoprevFirst thoughts, how do I bind to an ip, and where can I install models? reply gatane 12 hours agoprevIs this related to Melondream? reply smrtinsert 13 hours agoprevThere&#x27;s great articles on how layered uis are a lot easier to use than node based uis. Really excited to see a layered approach to SD. Its definitely time to break out of gradio. reply TeMPOraL 13 hours agoparentMaybe if they&#x27;re talking about layered UIs with layer groups, which turn a flat stack into something resembling a tree. But even these UIs don&#x27;t give you proper non-destructive editing - anything more complex requires you to duplicate parts of layer stack to feed as inputs, which is a destructive operation with respect to structure (those pasted layers won&#x27;t update if you make changes to copied source). Doing this properly requires a DAG, at which point you&#x27;re at node-based UIs (or some idiosyncratic mess of an UI that pretends it&#x27;s not modelling a DAG).It&#x27;s all moot though, because as far as I know, there is no proper 2D graphics editing software that uses DAGs and nodes. Everyone just copies Photoshop. Especially Affinity, which is grating, given their recent focus on non-destructive editing. For some reason, node-based UIs ended up being a mainstay of VFX, 3D graphics, and VFX & gamedev support tooling. But general 2D graphics - photo editing, raster and vector creation? Nodes are surprisingly absent. reply danwills 1 hour agorootparentWhat about ancient &#x27;Illusion&#x27;, old &#x27;Shake&#x27; or current &#x27;Nuke&#x27; VFX compositing softwares that totally have been supporting node-based (ie DAG-based) comp-workflows since the early 2000s? Guess this is just a very different (much smaller) realm than your usual Photoshop&#x27;s and so on? reply orbital-decay 10 hours agorootparentprevFor some reason, node-based UIs ended up being a mainstay of VFX, 3D graphics, and VFX & gamedev support tooling. But general 2D graphics - photo editing, raster and vector creation? Nodes are surprisingly absent.That&#x27;s because non-destructive editing is mostly useful for animation, image series&#x2F;sequences, and asset reuse, which are the most common in these fields. 2D artists have a different mental model, which is additionally set in stone by Photoshop and other software imitating it. Photographers use non-destructive editing, but mostly in simple cases because advanced things (retouching, creative compositing) can&#x27;t and don&#x27;t need to be done procedurally anyway. reply dragonwriter 10 hours agoparentprev> There&#x27;s great articles on how layered uis are a lot easier to use than node based uisI can see that being sensible for simple linear flows from one step to the next, with no branching merging, or connections that skip steps.Seems to me that with any of those other things, a layered UI is going to start to break down a lot faster. reply rytill 9 hours agoparentprevCan you share such articles? reply Hamcha 14 hours agoprevWhat&#x27;s up with names nowadays? Not only there&#x27;s already an OpenDream[1] on GitHub, but there&#x27;s also a Stable Diffusion service also called OpenDream[2]!1. https:&#x2F;&#x2F;github.com&#x2F;OpenDreamProject&#x2F;OpenDream 2. https:&#x2F;&#x2F;opendream.ai&#x2F; reply smallerfish 15 hours agoprevSlap a virtualenv setup into that install script please. A system wide pip install is a bad pattern. reply varunshenoy 14 hours agoparentdone :) reply noman-land 14 hours agorootparentNow that&#x27;s agile. reply __loam 15 hours agoprevnext [19 more] [flagged] dang 13 hours agoparentMaybe so, but please don&#x27;t post unsubstantive comments to Hacker News. reply __loam 10 hours agorootparentI&#x27;m honored to be flagged for this comment in a sea of approval for this highly unethical technology.To make a more substantial argument, I think these AI art devs are really shooting themselves in the foot by continuing to develop a lot of this stuff. Art being exposed to the public does not mean that there isn&#x27;t a copyright associated with that art, and frankly the developers of the base model, Stability, really ought to have at least talked to some of the artists whose art they used to train this thing. If this continues, it&#x27;s going to have a horrifically cooling effect on artists being able to make a living on their own work or being willing to share that work with the public, in the apparent absence of protections for that work, legal or otherwise. Eventually, this is really going to limit the amount of training data available to those AI companies, which will end up hurting them and their ability to train new models. And that&#x27;s without talking about whether this stuff will actually be found to be fair use. Many people here seem to be focused on the idea that all transformative work is fair use by default, but that&#x27;s not really the case, especially if the transformative work threatens the market for the original, which this clearly does.I think it&#x27;s important to bring these points up ad nauseum on any discussions involving this technology rather than blithely accept this as the new status quo, or just some harmless technical innovation. reply orbital-decay 3 hours agorootparentConsider that maybe, just maybe, you are in your own bubble, being blind to what is actually being done. Look at another comment in this thread. [1] Six guys produced 23 minutes of rotoscoping in a short time with just an experimental colab notebook, something not possible before. They made the concept art and transferred it to their live play. What did they steal as per your comment above? The concept of having a body, limbs, and a head?Several styles in animation already converged to the same playbook to cut the production costs. They can massively benefit from this, as it enables things that were not possible before due to the high manual labor requirements. This is a major limiting factor.Maybe, just maybe, the point about replacing the skilled artists with prompt-writing monkeys is moot, because this is an emerging highly technical creative field akin to 3D CGI and nobody is actually doing it like the panicking crowd with zero understanding likes to imagine? If anything, the industry is perfectly capable of exploiting underpaid labor and drowning itself in shit without using any AI.Might it also be that the entire moral panic and the ethics angle was just a viral fluke of a twitter horde amplifying the info noise without giving it any thought or having a look into it?[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37142433 reply __loam 1 hour agorootparentNice of the corridor crew to rip off work like vampire hunter d and still have it look like garbage. It&#x27;s pretty easy to understand what they did because they released an entire video about the process. reply dragonwriter 6 hours agorootparentprev> I think it’s important to bring these points up ad nauseum on any discussions involving this technologyYeah, hijacking and derailing every distantly-related discussion to your pet political issue has been recognized as bad netiquette since, at least, the early Usenet days, and its definitely not appropriate here. There is definitely an interesting and important discussion to be had about the practical, ethical, and legal issues around AI training and use, intellectual property, and the market for works of the types that models are trained on, but your overtly stated crusade to hijack all AI image generation conversations onto that topic is wildly inappropriate for this forum. reply Zuiii 6 hours agorootparentprevIt&#x27;s comments like this that really highlight the extent of damage copyright has caused. It has conditioned people to think that they can own information once released, and that they can treat it like property. It&#x27;s a notion that&#x27;s ridiculous and silly that it gets threatened each time we make a leap in technology (tapes, computers, internet and streaming, and now generative AI).I wonder how long society will tolerate the nonsensical idea of copyright before it&#x27;s had enough.Intellectual property is nonsense and I&#x27;m glad it keeps getting exposed. reply __loam 1 hour agorootparentUntil we have an economy where people don&#x27;t starve to death from lack of food or die of frostbite from being homeless, we have to figure out ways for people who actually make things to benefit from that work. Copyright is currently protecting people who rely on the product of their labor to make ends meet. Until we have the fairy tale economy you envision where artists can have the work stolen and still live, this is the best we&#x27;ve got.And yeah, wild to think people feel entitled to own their own work thanks to silly things like the entire body of copyright law. reply joemi 14 hours agoparentprevDoesn&#x27;t this entirely depend both on what its been trained on and what style is being output? But also, philosophically, is it even \"theft\" to make something in-the-style-of someone?I believe these questions and their complex answers are the reason you&#x27;ve been downvoted. reply coding123 14 hours agorootparentI understand why the person was downvoted, but not why the person was flagged. It doesn&#x27;t make sense for someone to flag \"AI art is theft.\"Downvotes because you didn&#x27;t back up what you mean.Flagged because there are AI fanboys that want to sensor speech perhaps? reply stale2002 14 hours agorootparentI would say that it absolutely deserved to be flagged because it was a comment of little engagement.It both isn&#x27;t directly related to the original post, and also didn&#x27;t even make any particular arguments. It was just a 5 word declaration of fact, that is borderline offtopic. reply cercatrova 11 hours agorootparentprevFlagged because it was an unsubstantive comment as dang mentioned and also that it&#x27;s increasingly a flame bait topic on HN, same as with Copilot and licensing. reply __loam 11 hours agorootparentprevDownvotes because the userbase here is rabidly supportive of any new \"AI\" tech regardless of the consequences, damage, or ethics surrounding it. But of course, we can never take a critical view of technology on Hackernews. reply valine 14 hours agoparentprevTheft takes the original, piracy makes a copy, AI art remixes the original. I’m not sure how to classify AI art but it definitely isn’t theft. reply slowmovintarget 14 hours agorootparentSo piracy may be involved in training the model, but the rest does not follow.Art inspired by other art has been the way of things for as long as we&#x27;ve been creating images. There&#x27;s no such thing as a \"clean-room painting.\" reply __loam 11 hours agorootparentUsing unlicensed copies of other people&#x27;s work in training is the problem, along with what that does to the market for original works. Using people&#x27;s labor for AI training without permission or compensation will discourage people from sharing that work and ultimately make the AI models worse too. reply yieldcrv 14 hours agorootparentprevDerivative works with zero copyright protection due to the predominance of machine assistsNo way to quantify though, for or against copy protectionBut thats a convenient compromise for now reply mcclux 14 hours agoparentprev\"This pixel right here officer; clearly stolen.\" reply visarga 14 hours agoparentprevWhy, did you lose your art because of AI? reply kitanata 7 hours agoprevnext [5 more] [flagged] renewiltord 7 hours agoparentI get that you&#x27;re spamming out of outrage, but they allowed me to disentangle my comments from my username, which is the same unless you mentioned something you don&#x27;t want to mention. reply kitanata 7 hours agorootparentI do not want to disentangle my name from my comments. I want to delete my comments. They are MY comments. I have a right to have them deleted. reply Zuiii 6 hours agorootparentIf you&#x27;re in the EU or are willing to stay in the EU for an extended time (>6 months?), then you may be able to compel them to delete the comments using EU laws. If they refuse, escalate and let the entirety of the EU take care of the rest.I get why hn is against deleting comments, and sure, make it really hard to delete comments if necessary, but you should honor the request of users who&#x27;s unpaid contributions make your site what it is. reply kitanata 6 hours agorootparentMaybe this is the start of a movement here to get us our own GDPR?You know what’s way more dangerous than me spamming this site with LLM hallucinations? Me walking into Congress with a fucking bill.How far do you want this to go HN? Delete my comments and then delete my account or maybe I start talking to Senators. It’s a nice site you have here. It would a shame to see your data go “poof”, wouldn’t it?You don’t delete my stuff? Maybe I just burn it all down in a nice fire led by Congress and a stroke of a pen. How would you like that?(Also thank you for the support Zuiii. You’re alright with me. :) ) replyadventured 12 hours agoprev [–] Not a bad start. One quick suggestion: avoid the temptation to make it overly complex.Stable Diffusion needs to go out to the masses to a greater degree. The unnecessary garbage complexity (eg Comfy&#x27;s ridiculous noodlescape) that developers keep including into the UIs is holding Stable Diffusion back significantly from a greater mass adoption. reply bavell 11 hours agoparent [–] Node based workflows with little DRY capability (i.e. ComfyUI) do get painful as the workflow grows. That said, an http server capabable of executing ML DAGs is extremely useful and a great building block for other tools and UIs to be built upon.I wrote a typescript API generator for ComfyUI recently and having programmatic access to let you build and send the execution graphs is a game changer. Hoping to have time to release it soon. Same can easily be done for any other language. Exciting stuff! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Opendream is a web UI tool that introduces features like layering, non-destructive editing, and portability to image generation and manipulation through diffusion models.",
      "Users can build upon existing images, conduct multiple experiments on the same image, and save and share their workflows.",
      "Opendream supports extensions and offers a user-friendly method to write and utilize new diffusion features."
    ],
    "commentSummary": [
      "The discussion centers around the application of AI technology in image editing, art, and animation, highlighting its potential benefits and challenges in these fields.",
      "Topics covered include the development of a layer-based UI, the potential harm of AI-generated controversial content, concerns about copyright infringement, and the ethical and legal implications of AI training and use.",
      "The debate also explores the responsibility of service providers, the necessity for regulation, and the impact on artists' livelihoods, raising important questions about the future of AI in these industries."
    ],
    "points": 407,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1692121090
  },
  {
    "id": 37138681,
    "title": "ISPs complain that listing every fee is too hard, urge FCC to scrap new rule",
    "originLink": "https://arstechnica.com/tech-policy/2023/08/isps-complain-that-listing-every-fee-is-too-hard-urge-fcc-to-scrap-new-rule/",
    "originBody": "SKIP TO MAIN CONTENT BIZ & IT TECH SCIENCE POLICY CARS GAMING & CULTURE STORE FORUMS SUBSCRIBE SIGN IN TO FEE OR NOT TO FEE — ISPs complain that listing every fee is too hard, urge FCC to scrap new rule Broadband industry fights requirement to \"list all recurring monthly fees.\" JON BRODKIN - 8/15/2023, 1:01 PM Enlarge Getty Imagesanyaberkut 175 WITH The US broadband industry is united in opposition to a requirement that Internet service providers list all of their monthly fees. Five lobby groups representing cable companies, fiber and DSL providers, and mobile operators have repeatedly urged the Federal Communications Commission to eliminate the requirement before new broadband labeling rules take effect. The trade associations petitioned the FCC in January to change the rules and renewed their call last week in a filing and in a meeting with FCC officials. The requirement that ISPs list all their monthly fees \"would add unnecessary complexity and burdens to the label for consumers and providers and could result in some providers having to create many labels for any given plan,\" the groups said in the filing on Friday. Enter your email to get the Ars Technica newsletter Join Ars Technica and Get Our Best Tech Stories DELIVERED STRAIGHT TO YOUR INBOX. SIGN ME UP By signing up, you agree to our user agreement (including the class action waiver and arbitration provisions), our privacy policy and cookie statement, and to receive marketing and account-related emails from Ars Technica. You can unsubscribe at any time. The trade groups said the FCC should instead \"require providers to include an explanatory statement that such fees may apply and that they vary by jurisdiction, similar to the Commission's treatment of government-imposed taxes,\" or require \"the display of the maximum level of government-imposed fees that might be passed through, so that consumers would not experience bill shock with respect to such fees.\" The filing was submitted by NCTA-The Internet & Television Association, which represents Comcast, Charter, Cox, and other cable companies. The NCTA's ex parte filing described a meeting with FCC officials that also included wireless industry trade group CTIA and USTelecom, which represents telcos including AT&T, Verizon, Lumen (formerly CenturyLink), Frontier, and Windstream. The meeting was attended by two other groups representing smaller ISPs: NTCA-The Rural Broadband Association and ACA Connects-America's Communications Association. The trade groups met on Wednesday with the legal advisors to FCC Chairwoman Jessica Rosenworcel and Commissioner Brendan Carr, according to the filing. Advertisement Comcast accused of “trying to create loopholes” Comcast submitted its own filing urging the FCC to scrap the rules in June. The calls to weaken the FCC's truth-in-billing rules angered consumer advocates, as we wrote at the time. \"The label hasn't even reached consumers yet, but Comcast is already trying to create loopholes. This request would allow the big ISPs to continue hiding the true cost of service and frustrating customers with poor service,\" Joshua Stager, policy director at media advocacy group Free Press, told Ars. Congress required the FCC to implement broadband labels with exact prices for Internet service plans in a 2021 law, but gave the FCC some leeway in how to structure the rules. The FCC adopted specific label rules in November 2022. The labels must be displayed to consumers at the point of sale and include monthly price, additional charges, speeds, data caps, additional charges for data, and other information. The FCC rules aren't in force yet because they are subject to a federal Office of Management and Budget (OMB) review under the US Paperwork Reduction Act. ISPs object to a portion of the FCC order that says, \"providers must list all recurring monthly fees\" including \"all charges that providers impose at their discretion, i.e., charges not mandated by a government.\" The five trade groups complain that this would require ISPs \"to display the pass-through of fees imposed by federal, state, or local government agencies on the consumer broadband label.\" But just because an ISP says a fee is related to a government charge doesn't mean that ISPs have to break them out separately. ISPs could instead include all costs in their advertised rates to give potential customers a clearer idea of how much they would have to pay each month. \"A provider that opts to combine all of its monthly discretionary fees with its base monthly price may do so and list that total price. In that case, the provider need not separately itemize those fees in the label,\" the FCC order said. Page: 1 2 Next → READER COMMENTS 175 WITH JON BRODKIN Jon has been a reporter for Ars Technica since 2011 and covers a wide array of telecom and tech policy topics. Jon graduated from Boston University with a degree in journalism and has been a full-time journalist for over 20 years. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 The F-35's next tech upgrade US Navy Gets an Italian Accent SITREP: DOD Resets Ballistic Missile Interceptor program SITREP: DOD's New Long-Range Air-to-Air Missile Aims to \"Outstick\" China Army's New Pistol Has Had Some Misfires Army's Next (Vertical) Lift En Route SITREP: President Trump's Missile Defense Strategy Hybrid Options for US's Next Top Fighter The Air Force’s Senior Citizen Chopper Can’t Retire Yet Ars Live #23: The History and Future of Tech Law Police re-creation of body camera evidence - Pueblo, COArs Technica Visual Labs body camera software with the Dos Palos PDArs Technica He knew his rights; he got tased anyway More videos ← PREVIOUS STORY NEXT STORY → Related Stories by Taboola Sponsored Links Heart Surgeon Begs Americans: “Stop Doing This To Your Blueberries\" Gundry MD Can Dental Implants for Seniors Be Paid For By Medicare? (See How) StuffAnswered The New BMW X7 Is A Jaw Dropper New BMW X7 Deals Volvo's Gorgeous New Lineup Might Leave You Speechless New Volvo Deals Bezos's Private Jet Puts Air Force One To Shame investing.com California College Named Worst College In The US BrainSharper CCPA Notice Today on Ars STORE SUBSCRIBE ABOUT US RSS FEEDS VIEW MOBILE SITE CONTACT US STAFF ADVERTISE WITH US REPRINTS NEWSLETTER SIGNUP Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox. Sign me up → CNMN Collection WIRED Media Group © 2023 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy. Your California Privacy RightsYour Privacy Choices The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",
    "commentLink": "https://news.ycombinator.com/item?id=37138681",
    "commentBody": "ISPs complain that listing every fee is too hard, urge FCC to scrap new ruleHacker NewspastloginISPs complain that listing every fee is too hard, urge FCC to scrap new rule (arstechnica.com) 408 points by LinuxBender 14 hours ago| hidepastfavorite304 comments NickC25 13 hours agoIf they can charge for the fee, they should be by law mandated to tell me what it&#x27;s for, and where it&#x27;s going.Sick of these TelCos getting billions in subsidies, not spending it on infrastructure as promised (somehow they are allowed to act as a for-profit corporation!?), lobbying to get more taxpayer money, and then whining when we find out that half the fees on the bills they send out are just to pad profits. Greedy bastards, the lot of them. Time to cut the crap and just regulate them as utilities. reply eli 11 hours agoparentThey&#x27;ve been getting away for decades with labeling all sorts of their own costs as \"taxes\" and \"fees\" to customers. It&#x27;s very misleading and IMHO should be illegal. reply darth_avocado 11 hours agorootparentExactly my thought. This problem wouldn’t have existed if the ISPs didn’t for decades advertise a plan for $50 and then charge $100 by adding on items like “just because we can fee” and “fee for processing your fee”. reply perfectstorm 9 hours agorootparentthis was my gripe with Sonic which people love over here in the Bay Area. their trucks and billboards advertise $40&#x2F;month for gigabit speed (this was a while back when i had them). they then charge you $10&#x2F;mo for a phone line which i never installed but it was part of the package and there was no way to avoid it. the phone line brings in additional fees like 911 and other local taxes + the taxes on internet service and my bill was more like $62&#x2F;month. $22 more than their own advertised price.I would have gladly paid $62&#x2F;mo for gigabit speeds but i felt like i was overpaying them by $22 because of their ads. reply tptacek 10 hours agorootparentprevPer the NCTA&#x27;s Ex Parte filing with FCC: they are objecting only to pass-through fees applied by federal, state, and local authorities. They don&#x27;t have any choice about any of these fees, which are really just taxes. reply Gabrys1 10 hours agorootparentThe choice they have (as is mentioned near the end of the article) is to pay them themselves (cost of running business) and include them in the advertised price (like it probably is anywhere in the world outside the US) reply lamontcg 9 hours agorootparentFees should just be illegal for any service, across the board. Cost of internet should just be what it is, plus any actual sales tax. Price should just be the advertised price. Anyone that tacks on fees should just get immediately sued for the fees. Fuck &#x27;em. Tired of the bullshit we put up with. reply CamelCaseName 9 hours agorootparentEurope is a wonderful place.It was very weird going to a restaurant for the first time and paying the price listed on the menu, and not 30% more (sales tax + tip) reply vladvasiliu 2 hours agorootparentSure. But have you tried paying for electricity here?In France, if you look at your EDF (national electricity company) invoice, you&#x27;ll see that you pay VAT (a tax) on top of other taxes.They also levy VAT on the \"Greffier\"&#x27;s fees. That&#x27;s a clerk at the \"commerce tribunal\", which is basically a part of the state administration with which you interact whenever you need any administrative changes for a company. Say, you need to declare a change of address. reply lostlogin 8 hours agorootparentprevWe have that system down here in New Zealand. However we have only two supermarket chains and a near monopoly for many consumer good and building supplies.You can buy New Zealand produce around the world for less than it costs us in New Zealand. So going for a meal costs what it says on the menu and no more, but that cost is steep. reply jackson1442 7 hours agorootparentI don’t think inclusive pricing and the cost of goods are necessarily linked, there are many other reasons goods and services could be more expensive in NZ. reply lostlogin 7 hours agorootparentOh there are, rules around employees, consumer guarantees act, acc levies and various other things. But it seems unlikely that have just 2 food suppliers isn’t a root cause of food price issues. reply lmpdev 2 hours agorootparentAustralian hereIsn&#x27;t it mostly just economies of scale plus transport costs?Or are our industrial relations laws really not as good? reply lozenge 1 hour agorootparentIt&#x27;s more about farming subsidies and tax levels. Some governments choose to make food cheap by encouraging overproduction. reply renonce 6 hours agorootparentprevIt looks like New Zealand charges a 15% VAT which is applied when New Zealand residents buy anything in Europe but not when New Zealand goods are exported, maybe that could be where the cost is? reply lostlogin 5 hours agorootparentThere is VAT when you buy in the UK too though. New Zealanders pay way more. It&#x27;s seems to be 50-100% what those in the UK pay for groceries, though this varies depending on how you measure or what you read.https:&#x2F;&#x2F;simplenewzealand.com&#x2F;cost-of-living-in-new-zealand-v....https:&#x2F;&#x2F;www.newshub.co.nz&#x2F;home&#x2F;money&#x2F;2021&#x2F;09&#x2F;shopper-s-recei...https:&#x2F;&#x2F;thespinoff.co.nz&#x2F;money&#x2F;12-08-2021&#x2F;how-do-nz-grocery-... reply lamontcg 9 hours agorootparentprevI literally can&#x27;t even imagine that utopia, because I still reflexively included a carve out for sales tax. reply romafirst3 3 hours agorootparentprevincidentally the price of broadband is also much cheaper, you can get 1Gig in Dublin for 50 euros per month (about $54). For that price in LA I can get 50Mbps.https:&#x2F;&#x2F;switcher.ie&#x2F;broadband&#x2F;ireland&#x2F;dublin&#x2F;?eircode=&tv_in... reply Thedarkb 6 minutes agorootparentThat&#x27;s not really consistent across Ireland though, let alone Europe. I was paying €50 a month for two megabit DSL in Kilkenny five years ago as that was all my ISP&#x27;s backhaul was good for. midoridensha 8 hours agorootparentprevIn Europe, it&#x27;s still common to give a small tip. In Germany, it&#x27;s called \"trinkgeld\" (literally \"drink gold\"). Usually, it&#x27;s just rounding up to the nearest Euro or 5 Euros.Japan is better: there&#x27;s absolutely no tipping, at all. The price you see advertised is the price you pay, and they won&#x27;t even take your tip. (There are a few exceptions: some American-style cafes have been putting tipping jars at the register; very annoying.) The only caveat is the tax: that&#x27;s not always counted in the price, so many restaurants show two prices, the pre-tax price and the post-tax price just below it (usually in a smaller font).The other thing that&#x27;s MUCH better about Japan vs. Europe is that water is free. In Europe, if you want any drink with your meal at all, you have to ask for it, and if you ask for water, they&#x27;ll bring a fancy glass bottle of very expensive bottled water. If you ask for tap water they&#x27;ll give you dirty looks. In Japan, it&#x27;s more like America: water (tap water, but it&#x27;s really good) is free, and usually given without asking. Some places put a giant carafe of iced water on your table, which is great during this ridiculously hot summer.The other thing that&#x27;s much better about Japan (this isn&#x27;t really about restaurants, though it does apply to the food court restaurants in malls) is that public bathrooms are free. In Europe, they usually expect you to leave a tip or some kind of fee for the cleaning people. Japan is again more like America: public bathrooms are just that: open to the public, for free. The big differences are that 1) there&#x27;s a washlet on most toilets, and 2) they&#x27;re actually clean. reply kergonath 3 hours agorootparent> In Europe, it&#x27;s still common to give a small tip.No, it’s not. Europe is more than one country, and there are local differences. And even in several places where it is common, nobody is going to be upset if you don’t tip, because they have their salary regardless.> The other thing that&#x27;s MUCH better about Japan vs. Europe is that water is free. In Europe, if you want any drink with your meal at all, you have to ask for it, and if you ask for water, they&#x27;ll bring a fancy glass bottle of very expensive bottled water.Depends on the country, I assume, but in most of those I know they are happy to bring you some tap water. In France, they are required by law to offer tap water for free but IIRC that’s not the case in the Netherlands or Germany, for example. So generalisations are not helpful. reply OfSanguineFire 4 hours agorootparentprevI think that one of the reasons you have been downvoted is because you are generalizing too much about Europe. Neither the tip culture nor ordering water works the same across the continent, not even across all Western European countries. There are European countries where one does not even round up the bill unless one really wants to, and countries where it is totally commonplace to order a big pitcher of tap water. reply midoridensha 2 hours agorootparentFair enough. But I was replying to a comment that also generalized all of Europe as having no tipping culture. It just isn&#x27;t true. I&#x27;ve been presented with credit card slips asking for a tip in multiple EU countries, exactly like when I go to a restaurant in the US. This is something you will never find in Japan.Sure, you can say that you aren&#x27;t \"required\" to tip in these cases, or that they&#x27;re taking advantage of tourists who don&#x27;t know better, but still, they&#x27;re asking for a tip. To me, \"no tipping culture\" means that tipping simply does not exist. Europe has a tipping culture; it&#x27;s just different than the US&#x27;s. reply vladvasiliu 10 minutes agorootparent> I&#x27;ve been presented with credit card slips asking for a tip in multiple EU countries, exactly like when I go to a restaurant in the US.I haven&#x27;t seen it in this form in France (yet?). But I do see more and more payment terminals that are asking for a tip when they&#x27;re presented to you. The amounts usually seem fixed, and you have the option of picking 0. I&#x27;ve mostly seen this in bars and similar situations.Some stores may also ask you to \"round up\" to give to some charity. Fun fact: these didn&#x27;t use to work with Apple Pay, so if you said yes, the assistant would have to cancel and restart the payment process. adhesive_wombat 1 hour agorootparentprevEurope is a broad brush. Tap water is free (by law) in the UK, and there are only two places I&#x27;ve ever paid for a public toilet: a handful of stations cost 20-50p and a handful of nightclubs might have a bouncer who you appear to be expected to tip £1 in exchange for a spritz of cologne.I&#x27;m not sure I&#x27;d say either of those chargeable toilet scenarios is well correlated to clean toilets, but I assume mostly they do that to try to avoid various kinds of mischief. reply bmicraft 8 hours agorootparentprev> literally \"drink gold\"\"Geld\" is not gold, its money. \"Gold\" is gold. reply Shaanie 5 hours agorootparentprevTap water is expected to be included for free in Sweden, and I would be slightly surprised if that didn&#x27;t also apply to our neighbors (Norway, Denmark and Finland). Getting charged for water is just as big a cultural shock for us as it is for you. reply LeonidasXIV 1 hour agorootparentTap water is unfortunately not included in Denmark; getting charged 25kr per person is not uncommon. Its a bit of a disgrace, honestly.But at least public toilets tend to be free whereas I believe in Sweden these tend to cost money. reply midoridensha 5 hours agorootparentprevInteresting; I guess Germany is just different that way. I haven&#x27;t been to Scandinavia yet. reply johnnyanmac 4 hours agorootparentprevyeah, public bathrooms have to be free. But as always it tends to come with an asterisk: for various reasons they may lock the doors and require a customer to ask for a key. IDK how legal it is but it&#x27;s not uncommon to require purchasing food before using some public restrooms (not necessarily for fast food&#x2F;grocery chains, but not unusual in either higher class dining or smaller mom and pop shops). reply LelouBil 8 hours agorootparentprevIn France, lots of restaurant give you a carafe, and they are obligated by law to give you free water if you ask (and you ordered something). reply 6510 7 hours agorootparentIn the Netherlands, if a stranger shows up at your door and asks for water you are obligated to give it to them. reply Thiez 3 hours agorootparentReally? Could you point me to that law? I think it&#x27;s a cool rule but can&#x27;t find it. Unless you mean in emergencies? reply r0b1n 1 hour agorootparentIn Germany, there is \"case law\" to that effect. Letting someone suffer of thirst is \"unterlassene Hilfeleistung\", roughly translated \"neglecting to give aid\". Not giving someone suffering of thirst any water is considered as an example in legal literature, however, I know of no real case where someone was convicted solely for that. And it isn&#x27;t really straightforward to argue that a restaurant not serving tap water would fulfil the criteria of \"unterlassene Hilfeleistung\", at least because the usual restaurant-goer isn&#x27;t suffering that badly.Also, aforementioned aid doesn&#x27;t have to be free, the person benefiting from your aid owes you the cost of that aid. reply sneak 4 hours agorootparentprevThis is true everywhere on Earth, just perhaps not codified into law. reply NicoJuicy 2 hours agorootparentprev> Usually, it&#x27;s just rounding up to the nearest Euro or 5 Euros.It&#x27;s not exactly common. In Belgium, the behavior is like when they thought it&#x27;s really good or if people worked on a restaurant before.But nobody expects that anyone would tip.Last time I went to Hungary it was added to the bill thoughPs. A bottle of water is free in France and some other places.Giving 50 cent for a restaurant when only using the bathroom is just a sign of good behavior in my view. Depends if I&#x27;m hurried, but I&#x27;ll usually buy a glass of water. Then I&#x27;m a customer ;) reply midoridensha 1 hour agorootparent>Giving 50 cent for a restaurant when only using the bathroom is just a sign of good behavior in my view.Why? If you&#x27;re already eating there, why should you pay to use the toilet? And if you&#x27;re not eating there, then why are you there to use the toilet? Just use a public one. If there aren&#x27;t sufficient public toilets, that sounds like a failure of the municipal government. Any good city should have lots of public toilets outside, in train stations, or in large shared buildings. reply NicoJuicy 1 hour agorootparentRestaurants don&#x27;t charge you when you&#x27;re already there as a customer ( as mentioned in my last example)? They charge you when you need to go and use them as a public toilet sometimes ( eg. when it&#x27;s urgent). reply midoridensha 1 hour agorootparentAgain, why would you do this? Just go to one of the countless public toilets outside. How often are people trying to use the toilets in a restaurant without eating there? It seems insane they would even worry about this.Also, my experience in Germany at a mall food court is that they DO charge you to use the toilet there, even though I was eating there. reply LeonidasXIV 1 hour agorootparent> Just go to one of the countless public toilets outside.If there even are public toilets, if they are open, if they are in a state to be used (and not just an uncleaned mess), if they are free... plenty of reasons to go to a restaurant to use their toilet. replyjjav 3 hours agorootparentprev> Fees should just be illegal for any service, across the board.Very much this. The law should be that you must advertise the price you will charge, full stop. Same goes with tipping mess. Just compute the total price internally and tell me what it is. Nothing additional allowed. reply Bluecobra 6 hours agorootparentprevI agree, and somehow I am lucky enough to have 1G AT&T Fiber Internet for exactly $100&#x2F;month with no other fees. At my previous home I had 500mb microwave&#x2F;millimeter wave Internet and that was not taxed either. Seems like local&#x2F;state&#x2F;federal government hasn’t caught onto these mediums yet. reply tptacek 10 hours agorootparentprevHow is that better? You&#x27;re asking for a world where the list price of Comcast Internet service is just quietly lower or higher depending on which suburb you live in. That&#x27;s strictly worse than the current situation.You should look at all these fees on your ISP bill and get pissed off. The issue here is that the taxing body&#x27;s ploy worked: you got pissed at Comcast and AT&T instead of them. reply Gabrys1 10 hours agorootparentIt&#x27;s better in exactly one way: you understand upfront what your internet is going to cost.The same argument goes for including the sale tax or many gas taxes in the displayed price, (or displaying the price including VAT in EU).FTR, I do get that by listing the tax explicitly you get to appreciate your local tax is lower than your neighbor&#x27;s or get annoyed that it&#x27;s higher (and then vote to change that).My internet provider in Poland offered me the internet service for 60PLN&#x2F;mo. Guess how much I&#x27;m paying for it? 60PLN&#x2F;mo. They obviously pay many taxes and fees to different organizations but I don&#x27;t need to care about it. reply jkaplowitz 9 hours agorootparentRegarding VAT in Europe and similar systems in Canada and Mexico where it’s included in the listed price: in all such countries which I’ve personally experienced, the receipts still generally include a breakdown of which tax applies to which item, including correctly distinguishing when different taxes or tax rates apply to different items, and often a total of each tax or all taxes per receipt. You do get to care about those if you want to merely by looking at the details on your receipt, or not if you don’t, but the sticker price is the final price either way.(Tangent: the above is discussing consumer-targeted prices only. Business-targeted prices often don’t include VAT in the advertised price, even in the EU, because most EU business can get a credit for VAT paid on inputs to a product or service in which they themselves charge VAT. Of course, receipts and invoices still break down all the details of which VAT rate is charged for which item, and sometimes why.)The situation for disclosure of multiple overlapping sales taxes in the US (as is usually the case in NYC for example) is much less detailed, sadly, but even there it’s still usually clear which items are taxed or tax-free. reply extraduder_ire 8 hours agorootparentI didn&#x27;t know Canada&#x2F;Mexico mandated showing the total price, I thought Canada at least was in the same boat as the US.Very much a fan of the EU&#x27;s Consumer Rights Directive, especially where it pertains to advertised prices. Even if some things have now skirted it, like sugar-tax and pfand in certain countries.In my country at least, any business with a VAT number buys most things VAT-free and the end consumer is charged the whole amount, for the sake of doing less paperwork. Don&#x27;t know how exactly it works when something is transformed into a lower-VAT output. reply doubled112 8 hours agorootparentIf we did, I didn&#x27;t know. All of the price tags in Canada (or Ontario at least) are pre-tax. reply tptacek 10 hours agorootparentprevNCTA proposed informing customers of the maximum amount of pass-through fees&#x2F;taxes from governments customers might face, to avoid the \"sticker shock\" problem with bills. reply jkaplowitz 9 hours agorootparentHaving the individual fees listed feels important to me for two reasons: one, to allow customers who are so inclined to check for any calculation errors, as a safeguard against accidental or intentional overcharging by the ISP; and two, so that customers who want to lobby their elected officials to reduce or eliminate any of the charges can identify the specific charges applicable and each of their amounts, without which no meaningful political activism is possible. reply kergonath 2 hours agorootparent> Having the individual fees listed feels important to me for two reasons: one, to allow customers who are so inclined to check for any calculation errors, as a safeguard against accidental or intentional overcharging by the ISPAren’t there stories in the news regularly about ISPs adding additional “fuck you” fees just because they can? So this does not seem to be working. At the end of the day, when I buy a beer I don’t care that €0.20 is going to their administrative overhead and €1.28 to their installations’ maintenance. That is not supposed to be the customer’s problem.> and two, so that customers who want to lobby their elected officials to reduce or eliminate any of the charges can identify the specific charges applicable and each of their amounts, without which no meaningful political activism is possible.This does not seem to be working well either. And at the same time, other countries without this issue seem to be doing fine with their broadband prices. It looks very ridiculous from the outside, in any case. reply Gabrys1 10 hours agorootparentprevYes, why not, that seems reasonable. Your price is $60 plus up to $20 additional fees.I think I got an offer like this from Sonic on the Bay Area. They would eat up all fees exceeding the declared maximum. (Or I&#x27;m making this up, was in 2018) reply fivre 9 hours agorootparentprevThat&#x27;s already the case. ISPs have the freedom to charge prices that vary by region and available service tiers.Most consumers aren&#x27;t going to care that a portion of the price is Comcast structuring prices so that it can cover mandated costs of doing business. I&#x27;ve never encountered a company listing, say, that it&#x27;s charging additional pennies so that it can meet its Social Security tax obligations, for example. That level of granularity simply isn&#x27;t that interesting for consumer-level prices.If Comcast thinks these fees are unreasonable and wants to lobby voters and politicians to work against them, it is free to do so via other means. reply rtpg 7 hours agorootparentprev> The FCC order said the requirement to list \"all charges that providers impose at their discretion\" is meant to help broadband users \"understand which charges are part of the provider&#x27;s rate structure, and which derive from government assessments or programs.\" These fees must have \"simple, accurate, [and] easy-to-understand name[s],\" the FCC order said.The point is that the fees are charged to the ISPs, and then the ISPs want to pass it onto the consumer but not actually say that that&#x27;s what is going on.Yes, some localities will charge more things. Hell, listing them out would add pressure to make these fees go away! The FCC isn&#x27;t the one imposing these fees. But ISPs are choosing to charge the users, but also not advertising these fees ahead of time! Advertising \"this service costs X\" but actually costing X + Y is something that feels pretty unambiguously bad.Then again the US is the land of \"every locality sets its own sales tax\" so.... reply tptacek 7 hours agorootparentNo, these fees are devised by governments to be passed on to consumers. Not that it especially matters --- it&#x27;s also the case that taxes levied directly to ISPs are passed to consumers, because money is fungible. But here, it&#x27;s especially overt that this is money governments are collecting these fees from their constituents; they&#x27;re just using the ISPs as bill collectors. reply rtpg 7 hours agorootparentI am assuming here that the ISP is who would get in trouble for not sending the money to the locality, not the N individual customers. I&#x27;m also assuming that if I sent my ISP money for the internet cost, minus the passthrough costs, the ISP would act as if I owe them money.I get what you&#x27;re saying about the fee being for the consumer. It&#x27;s just like... OK then, well tell the consumer how much they are going to pay. You mentioned that the lobbying is to get rid of these locality&#x27;s taxes, and I&#x27;m not pro-federalism so hey why not.But all of these places have to collect the money anyways, so there is _a_ logic to how much to charge. Many places in the world, the price of internet is \"type your zip code into a box and then we tell you\". This seems eminently reasonable. This precludes a nationwide campaign to say exactly how much the service costs (unless ISPs just decided to eat the costs themselves!). But wouldn&#x27;t it be good for people to know how much something costs? reply woobar 10 hours agorootparentprevSomehow T-Mobile figured that out. And charge me a nice round numbers that include all their fees and taxes. They are also able to advertise their prices nationwide. reply intellirogue 1 hour agorootparentThey&#x27;re European (Deutsche Telekom), which might be part of why they prefer to work that way. reply bradleybuda 9 hours agorootparentprevAnd as a consumer you can choose to use them if you like that model. The market works! reply lmm 7 hours agorootparentOnly if you magically know for free which companies are using that model and which aren&#x27;t. reply gamblor956 10 hours agorootparentprevMy small regional ISP doesn&#x27;t charge the fees that Comcast charges my neighbors in my building, those are included in its list price because they&#x27;re a cost of the ISP doing business.You should look at all these fees on your ISP bill and get pissed off.You&#x27;re right, we should be pissed at Comcast and AT&T fraudulently misrepresenting the cost of service. They chose to pass these fees along (conveniently marked up for their \"costs\" associated with administering this fee to their customers) and are pretending that they don&#x27;t have a choice.The thing is, \"service fees\" aren&#x27;t government-imposed. They&#x27;re fees entirely made up by Comcast and AT&T for the privilege of providing the service that the customer is already paying them for. Since they&#x27;re not optional, they can and should be rolled into the advertised price. reply tptacek 9 hours agorootparentAs we both read the NCTA filing, we&#x27;re both aware that NCTA is objecting to pass-through fees from federal, state, and local authorities, not to service fees they themselves make up. reply lozenge 1 hour agorootparentprevBut the FCC order itself explicitly says that government imposed fees won&#x27;t be affected. The quote is in the article. reply m463 6 hours agorootparentprevThat said, those fees are the reason \"add free tv to your internet\" is a huge scam on their part (and they make money on the advertising). reply grecy 7 hours agorootparentprevI worked for a very large telco.Marketing were tasked with always increasing revenue, so they would often sit around with their feet on the table while they came up with such brilliant fees as \"you downgraded your internet plan? Fee\" & \"You want more data on your cellphone plan? Fee.\"It was sickening, but also very enlightening to realize it really was all made up. reply tptacek 11 hours agorootparentprevI don&#x27;t know what this means, but local franchise fees are in fact taxes. reply lokar 10 hours agorootparentWhen you eat at a restaurant do you see property tax on the bill? It is a cost of doing business, everyone has them. reply tptacek 10 hours agorootparentWhen I eat at a restaurant, that business is not paying a per-seating fee to the city it&#x27;s sited in. But that&#x27;s what the ISPs are doing. reply xp84 10 hours agorootparentSure, but they are paying those fees in exchange for the incredibly valuable privilege of (usually) an exclusive monopoly on broadband in an entire city.This was the contract they accepted when they eagerly signed such a deal, because the last thing those companies want is to have a bunch of little fiber outfits going around wiring up all the richest and easiest-to-deploy neighborhoods with quality fiber internet service. Or -- g-d forbid -- municipalities cutting them out and laying fiber themselves. Their franchise fee protects them from either of those horrors.Pretending they&#x27;re the victim for covering the costs they agreed to, and then acting like consumers are being unreasonable when we don&#x27;t pay what was forced on us by our one choice of ISP is the boldest hypocrite move I&#x27;ve heard of yet from telcos. reply tptacek 10 hours agorootparentFranchise agreements usually do not grant exclusivity to providers. And they&#x27;re not the only fees we&#x27;re talking about; counties, states, and the federal government apply pass-through taxes to ISP customers as well.Go look yours up. I just did, to make sure (I have opinions on this weird issue because I&#x27;m on my town&#x27;s governing commission for this stuff --- go join yours too, I&#x27;ve gotten to do all sorts of cool stuff that I wouldn&#x27;t have predicted from the nerdy charter of the commission --- and after 18 months of falling asleep every time the topic of the franchise agreement came up, I finally stayed awake and learned we were making a fairly huge amount of money by soaking residents for their ISP connections). reply lokar 10 hours agorootparentprevWhy does that matter? reply tptacek 10 hours agorootparentBecause property taxes are a predictable, fixed fee, and ISP fees are usage taxes applied per-customer.Let&#x27;s make this imaginary restaurant more fun: not only is it required to charge a per-seating fee, but it&#x27;s not the same fee for each customer. Instead, they have to inquire about the residential address of each guest, and compute a locality-specific seating fee. reply ryandrake 8 hours agorootparentThis imaginary restaurant could just pay all these fees itself as a cost of doing business and transparently charge a slightly higher whole number to the average customer. reply ta8645 8 hours agorootparentEither way, it really drives home the point that businesses don&#x27;t pay taxes, the consumer does. reply tptacek 8 hours agorootparentprevNo, it can&#x27;t! reply ryandrake 8 hours agorootparentWhy not?If my bill looks like this:$50 Steak Meal$10.54 Municipal We Hate You tax$5.87 Local ditch digging tax$2.15 State paperwork shredding tax$0.04 Federal fuck you taxWhy can&#x27;t instead the restaurant just write on the menu:$70 Steak ($50 + taxes and fees)...and eat the costs itself? It&#x27;s entirely within the businesses&#x27; power to do this. They choose not to. reply tptacek 7 hours agorootparentSorry, I thought you were implying that ISPs could opt instead to pay some lump sum to the various governments charging these fees, and not break them out per customer. They can&#x27;t do that. Nor can they charge \"$70 steak ($50 + taxes and fees)\" --- that&#x27;s what they want to do. It&#x27;s the FCC that&#x27;s saying the bill has to look the way you&#x27;re complaining about. reply BHSPitMonkey 2 hours agorootparentPer the article, the FCC says ISPs are free to eat those costs behind the scenes and charge customers of the same plan the same flat rate (thus avoiding the need to itemize any taxes). The FCC is essentially creating an incentive structure that favors simplicity for consumers (possibly at the cost of making customers in low-tax areas pay a little more while those in high-tax areas pay a little less). replywoobar 10 hours agorootparentprevIsn&#x27;t fee based on location of where the service is provided? My ISP at the rental property doesn&#x27;t care about my residential address. They only care about the address where their service is consumed. You can stop worrying about that restaurant, they will charge same fee all their customers as long as they don&#x27;t deliver. reply tptacek 9 hours agorootparentThis is about labeling requirements at the point of sale, not about about the details of your bill, which already list these fees. reply woobar 9 hours agorootparentAre you saying that ISPs already have all this information and there is no problem for them to calculate these (checking the thread...) \"unpredictable\" fees? reply lokar 8 hours agorootparentprevThey are allowed to list the maximum total price reply tptacek 8 hours agorootparentNo, in their objection they&#x27;re specifically asking as an alternative to be allowed to do that. replyeli 11 hours agorootparentprevI just pulled up an old comcast bill and it has: \"Federal Cost Recovery Fee\" and \"Universal Service Fund Surcharge (State)\" and \"Utility User’s Tax &#x2F;Business\" and \"Universal Connectivity Charge\" and \"911 Line Tax (State)\" reply ljm 10 hours agorootparentIn the UK, for most bills, it is line rental + provider fee. Line rental is to pay for the copper wiring that is still used for landline phones and all of the infra for it (via local loop unbundling). It goes to OpenReach and every ISP resells it unless they plant their own infra (Virgin, Hyperoptic) reply Jochim 3 minutes agorootparentMost ISPs sell broadband only plans now, even on copper installations. tptacek 11 hours agorootparentprevThose are all pass-through fees. They are just taxes. reply eli 10 hours agorootparentThey are not just taxes. Reimbursing the telco for their compliance costs makes about as much sense and reimbursing a restaurant for complying with local health code.If they were actually taxes Comcast wouldn’t be fighting this rule, which does not apply to government mandated taxes. reply tptacek 10 hours agorootparentI tracked down the FCC filing this article is about and read it. They are just taxes. There may be other things in the FCC rule that aren&#x27;t about taxes, but the NCTA&#x27;s objection is exclusively to the taxes. reply deelowe 10 hours agorootparentLike most things, the government creates a problem then convinces everyone they aren&#x27;t the reason.I can&#x27;t get broadband at my home despite being less than a mile from multiple providers in 3 different directions. I literally have an att fiber running down my row to a neighborhood less than a mile a way.After being pissed at att for several years I finally looked into it and realized the county won&#x27;t approve att putting in the service for anything but developer projects. Basically crony capitalism... reply sangnoir 10 hours agorootparentDo you think ATT is for or against not having to compete on every block? reply tptacek 9 hours agorootparentMost of these fees have nothing to do with exclusivity, and for the most part AT&T and Comcast don&#x27;t get exclusivity in their franchise agreements. Go look yours up (your city or township should make it very simple to find, and if it doesn&#x27;t, there&#x27;s a reason for that!) and see if they got exclusivity. reply deelowe 9 hours agorootparentprevI don&#x27;t follow. In my specific case there has been one developer locked up already for collusion with the county so I think I&#x27;m fair in assuming the issue is not ATT in this case.Plus what&#x27;s stopping Comcast from coming down my street other than the countys rules about new developments only? replym463 6 hours agorootparentprevThe ISPs allow you to add \"free television\" with your internet service, then it turns out to be very NOT free. They do get lots of ad revenue even though they \"don&#x27;t charge\" for the TV. reply gamblor956 9 hours agorootparentprevAs a tax lawyer, I must rebut your ridiculous claim that \"franchise fees are in fact taxes.\"Franchise fees are fees paid by an ISP for the right to provide their service in a territory; paying the franchise fee entitles the ISP to access the public right of way (such as by installing lines under roads, in sewers, etc.). That they happen to commonly be calculated as a % of gross charges paid by ISP customers is irrelevant; royalties are calculated the same way and nobody calls a royalty a tax. reply tptacek 9 hours agorootparentAs a tax lawyer, you may not be allowed to call them taxes. Since I&#x27;m not one, I can apply the common sense definition, rather than the technical one. reply zamfi 9 hours agorootparentprevA royalty, paid to a government, as a condition of business?Yes, I’d call it a sales tax, property tax, income tax, etc. based on what it was grounded in. reply AmericanChopper 9 hours agorootparentprevSo it’s a compulsory contribution to state revenue levied by the government and added to the cost of the service? Because that’s what a tax is…> a compulsory contribution to state revenue, levied by the government on workers&#x27; income and business profits, or added to the cost of some goods, services, and transactions. reply lmm 7 hours agorootparentSounds like it&#x27;s not compulsory though? The ISP is perfectly welcome to lay their lines via private lands (having negotiated permission with each landowner, naturally) if they&#x27;d prefer. reply mlyle 6 hours agorootparent> Sounds like it&#x27;s not compulsory though? The ISP is perfectly welcome to lay their lines via private lands (having negotiated permission with each landowner, naturally) if they&#x27;d prefer.Streets cut cities into small islands of private lands. Therefore, you cannot reach anyone without crossing the public right of way. reply lmm 4 hours agorootparentPerhaps mere crossing should be treated as de minimis. But when you&#x27;re closing and digging up the public street for days at a time, or perhaps even running your lines through publicly-maintained conduits and accessing them through publicly-maintained manholes, you&#x27;re taking a lot more from the commons than that. reply AmericanChopper 7 hours agorootparentprevWell it’s not compulsory to pay your vehicle registration tax either, if you only want to drive your vehicle on private land. reply lmm 6 hours agorootparentI wouldn&#x27;t consider that a \"tax\" either if that&#x27;s all it&#x27;s for. Do you consider national park entrance passes to be a tax? reply AmericanChopper 6 hours agorootparentContrived arguments about whether something is a tax or a user fee has been a feature of politically motivated spin since basically forever. I would say any essential service (like access to internet, roads, healthcare, etc…) that requires for money to be paid to the government is taxes. There is no way to get access to the internet without paying these taxes to the government, and your suggestion to implement a private-land-only infrastructure is silly. There is no contiguous area of private land between my house and an ISP POP that doesn’t cross over publicly owned land. That’s almost certainly true for you too, and anybody else who’s reading this. reply lmm 5 hours agorootparent> I would say any essential service (like access to internet, roads, healthcare, etc…) that requires for money to be paid to the government is taxes. There is no way to get access to the internet without paying these taxes to the government, and your suggestion to implement a private-land-only infrastructure is silly. There is no contiguous area of private land between my house and an ISP POP that doesn’t cross over publicly owned land.If it&#x27;s both an essential service and effectively impossible to implement without making use of public land (and I don&#x27;t disagree), then there&#x27;s no reasonable basis for it to be a private industry operation at all. These ISPs can&#x27;t have it both ways. reply chriscappuccio 5 hours agorootparentprevMost taxes and fees from an ISP or telco have absolutely nothing to do with passing over public property. Many telco fees are in portion or in full a profit for the provider! replydfxm12 11 hours agorootparentprevIf I understand what you&#x27;re saying correctly, I think the Junk Fee Prevention Act, which Joe Biden championed in this last SOTU address, is being marketed as doing this, more or less. reply ClumsyPilot 8 hours agorootparentprev> labeling all sorts of their own costs as \"taxes\"This literally sounds like fraud to me. If I did that, i think i would get charged reply germinalphrase 13 hours agoparentprevRemoving laws that enforce non-competition by municipalities would be a fine start. reply NickC25 12 hours agorootparentBanning them from lobbying would also be a welcome change. How can a utility with a monopoly on services be allowed to buy favorable laws in government? reply robertlagrant 11 hours agorootparentIt&#x27;s not lobbying that&#x27;s the problem. It&#x27;s government corruption. reply mulmen 12 hours agorootparentprevYou can&#x27;t ban lobbying in a representative democracy. reply spicebox 12 hours agorootparentYes you can, even if you don’t want to restrict lobbying on free speech ground you can place donation caps and increase transparency requirements which would both limit the power of corporate lobbying reply pessimizer 11 hours agorootparentYou can end donations entirely, and disqualify candidates who accept them. The government can put up a website where you can get information about candidates, send everyone bundles of information, schedule debates in a standard format with clear rules for participation, and throw events where candidates can give speeches, and distribute those speeches to everybody who wants them. You can give everybody a day off to vote.The reason government is corrupt is because we want it to be corrupt. If it weren&#x27;t corrupt, nobody would pick this endless shower of dynastic creeps.There is no good democratic outcome for $175K a year jobs that cost half a billion dollars to apply for. reply spicebox 11 hours agorootparent> There is no good democratic outcome for $175K a year jobs that cost half a billion dollars to apply for.I would argue that this is a reason to reform our electoral system rather than just resign ourselves to a corrupt system. If it didn’t cost half a billion dollars to become a politician and the pay better reflected the job responsibilities (along with other reforms) we’d probably see less corruption reply sdenton4 10 hours agorootparentIs there a word for the situation where incumbents produced by a system try to maintain that system even though it&#x27;s shite for everyone else?Politicians hate electoral reform. reply skykooler 8 hours agorootparentprev> The reason government is corrupt is because we want it to be corrupt.The reason government is corrupt is because it is corrupt, and corrupt governments don&#x27;t give their citizens an option for \"stop being corrupt\" in the ballot. reply wredue 11 hours agorootparentprevThey only cost half a billion to apply for due to lobbying… reply TylerE 11 hours agorootparentprevThey&#x27;ll just add another layer of rat finkery on top, making the whole mess EVEN LESS transparent.PACs were limited lobbying.That wasn&#x27;t good enough...so we got super PACs with virtually no oversight what-so-ever. reply mulmen 11 hours agorootparentprevDonation caps and transparency aren’t bans. Our system is based on representatives acting on behalf of constituents. When a constituent contacts their representative that’s lobbying. Lobbying is the system. So you can’t ban it. reply spicebox 11 hours agorootparentThe issue is that lobbying has two different meanings. The literal meaning is trying to influence politicians but the common usage refers to when companies and special interest groups spend hundreds of millions of dollars on donations and advertising to get politicians to pass laws that benefit them. The first meaning, which is what you’re talking about, is fine. But the second meaning, which is what people mean when they talk about banning lobbying, is the opposite of democracy. reply wredue 11 hours agorootparentHundreds of millions?In most cases, a vote that ends regulatory bodies and enables businesses to dump toxic sludge in your drinking water costs a couple grand.It’s actually embarrassing how cheap votes are. reply xp84 10 hours agorootparentprevTechnically you can do whatever those representatives will vote for. So far, it&#x27;s mostly been \"whatever the lobbyists want\" though. reply PrimeMcFly 12 hours agorootparentprevLobbying isn&#x27;t the issue. The corrupt politicians who just implement whatever telcos request are. reply UncleMeat 11 hours agorootparent\"Quid-pro-quo bribery isn&#x27;t the issue. The corrupt politicians who accept bribes are the issue.\"This isn&#x27;t how we do legal policy. We wouldn&#x27;t say \"all bribery laws are gone, we&#x27;ll just make sure that we don&#x27;t have any politicians that accept bribes some other way.\" reply PrimeMcFly 11 hours agorootparentWell, you&#x27;re kind of right. Bribery disguised as lobbying is the issue, not lobbying itself. reply pessimizer 11 hours agorootparentprevSo instead of regulating lobbying, it would be easier to eradicate all potentially corruptible people from the planet. reply robertlagrant 11 hours agorootparentIf you think regulation is magically excluded from your own logic, you need to get another think. reply zakki 11 hours agorootparentprevThe politicians corrupted because of the lobbying. How can we put them in separate basket? reply PrimeMcFly 11 hours agorootparentThey&#x27;re not corrupt because of lobbying, they are already corrupt and take advantage of lobbying.There are things that could be done but the people in office making the laws are not interested, and the population is too apathetic&#x2F;indoctrinated to make the correct choices. replybriandear 12 hours agorootparentprevBanning lobbying is banning free speech. Anyone should be able to lobby the government. reply spicebox 12 hours agorootparentWhy? What is the benefit to the public good of allowing corporations to use their massive resources to shape legislation to benefit them at the expense of the general public? reply dan-robertson 11 hours agorootparentGenerally industries are much better than random politicians at predicting the consequences of rules. Obviously it isn’t good if the rules being made are entirely in the interests of incumbents, but it’s also bad if the rules don’t make sense or ignore the realities of some industry or technology. Even ignoring lobbying, you see regulators reaching out for comments from industry about new rules.Though perhaps lobbying (as practiced or in general) should still be considered inappropriate. reply spicebox 11 hours agorootparent> Generally industries are much better than random politicians at predicting the consequences of rulesThis is true but when people are talking about restricting lobbying the kind of lobbying they want to restrict isn’t companies saying “hey this policy isn’t good for us”. It’s companies spending millions of dollars on donations and advertising to force politicians to make laws that benefit them. It’s possible to restrict the second kind without impacting the first kind, for example by implementing spending limits. reply robertlagrant 1 hour agorootparent> It’s companies spending millions of dollars on donations and advertising to force politicians to make laws that benefit themThey aren&#x27;t forcing. They&#x27;d just rather take the money than not. If politicians weren&#x27;t corrupt, we&#x27;d only have the first kind of lobbying, as no one will spend money on something that doesn&#x27;t work. reply dan-robertson 11 hours agorootparentprevThe article linked at the top of this thread is the former thing. But maybe that’s not relevant.Aren’t the rules on donations made by companies pretty restricted in the US? Isn’t it normally that companies persuade their employees to give to some company pac (as a deduction from their paycheck) and that pac then makes maximum campaign contributions to various politicians. (The limits on both contributions to the pac and to campaigns are pretty small. On the order of $5k. It seems unlikely to matter much to a politician but maybe they do care).Maybe you’re not talking about the US or maybe I’m wrong or there is some other mechanism I don’t understand reply hnbad 1 hour agorootparentprevIndustries are also better with regard to conflict of interest in that there is no conflict, they just have interests which are entirely separate from those of the public.> Even ignoring lobbying, you see regulators reaching out for comments from industry about new rules.Yet somehow the only times those \"comments from industry\" are taken into consideration in a meaningful way is when they come from large businesses with deep pockets or astroturf consumer interest groups funded by large businesses with deep pockets. Genuine grassroots consumer interest groups rarely leave a mark. reply refurb 10 hours agorootparentprevBecause individuals have a Constitutional right to petition their representatives and that right doesn&#x27;t magically disappear when they decide to do it as a group of individuals? reply pessimizer 11 hours agorootparentprevPeople should be able to say what they want. They shouldn&#x27;t be able to pay politicians for favorable treatment. If that&#x27;s somehow required for free speech, we should get rid of free speech. reply refurb 10 hours agorootparentI think the problem here is that people don&#x27;t know what lobbying means.It doesn&#x27;t mean paying politicians.Lobbying is simply that - petitioning your representative for change (or no change). No money changes hands, you literally just get time with your representative to argue your case.When a company spends $1M on lobbying, it&#x27;s $1M given to a lobbying firm to put together a strategy and a pitch to representatives.If you want fund raising laws to change, then say that. reply derefr 12 hours agorootparentprevWho&#x27;s this \"anyone\"? Why are monopolies suddenly people? reply throw0101c 12 hours agorootparent> Who&#x27;s this \"anyone\"? Why are monopolies suddenly people?Corporations have always been legal persons, going back to the Middle Ages (guilds, chartered cities, universities) and even Ancient Rome:* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Legal_person reply dghlsakjg 11 hours agorootparentCorporations are legal entities separate from people. They do not have the same rights and responsibilities as humans. reply throw0101c 9 hours agorootparent> They do not have the same rights and responsibilities as humans.Opinions and precedents vary:* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Legal_person#Rights_and_respon... reply dghlsakjg 9 hours agorootparentFrom the article you linked: \"There are therefore two kinds of legal entities: human and non-human. In law, a human person is called a natural person (sometimes also a physical person), and a non-human person is called a juridical person\"No where in that article does it cite a jurisdiction where a non-human legal entity has the same rights and responsibilities as a human. My point still stands that a human has a different, albeit overlapping, set of rights and responsibilities in comparison to other legal entities. reply PrimeMcFly 12 hours agorootparentprevWhat&#x27;s amazing is how many people misunderstand this, and think the government treats a legal person as a full flesh and blood person. reply dragonwriter 12 hours agorootparentprev> Why are monopolies suddenly people?Who else besides people would control a monopoly? reply tarboreus 11 hours agorootparentIf the people running the monopoly want to lobby the government on their own time, uncompensated, that certainly sounds fine. It&#x27;s lobbying on behalf of the monopoly that is an issue. replytptacek 11 hours agoparentprevNo, this is backwards.From the NCTA&#x27;s Ex Parte filing: they&#x27;re objecting to reporting requirements for pass-through fees from federal, state, and local governments. Importantly, none of these are fees ISPs \"can\" charge; they&#x27;re taxes that public bodies collect through the ISP&#x27;s billing system. In some cases, those \"fees\" are added by statutory mandate; in others, they&#x27;re a condition of access to municipal last-mile infrastructure (as is the case with franchise fees).I generally think most middle-class people aren&#x27;t taxed enough (yell at me somewhere else about this). But these taxes are frustrating. They&#x27;re hidden on ISP bills, so you can&#x27;t easily tell that they represent your local municipality milking you for fee revenue. And they&#x27;re not even consistent; for instance, because Comcast runs copper television service, they&#x27;ve got a different history with many municipalities and different contract stipulations. In other words: your local government can tax you specifically for using (or not using) Comcast. That&#x27;s messed up.It should be the responsibility of public bodies that levy fees to make sure that people are made aware of the nature of those fees. The ISPs aren&#x27;t responsible for this stuff, and shouldn&#x27;t be asked to do more work to further conceal decisions our elected officials are making for us. reply kerkeslager 2 hours agorootparentYou&#x27;re one of the few people here I&#x27;ve generally respected and you&#x27;re better than this. You&#x27;re just repeating the ISPs&#x27; rhetoric here without thinking about it critically.Consider: what incentive do ISPs have to hide municipal taxes? Why would they do that? Wouldn&#x27;t it behoove them to advertise these taxes to consumers so that consumers know it&#x27;s not them adding this cost? There&#x27;s obviously going to be very little cost simply reporting these taxes given they already have to calculate them to pay them.The obvious answer is that, maybe this happens occasionally, but the majority of these fees aren&#x27;t what you&#x27;re describing: this is just a foil that ISPs are using to cover up their own greed.> It should be the responsibility of public bodies that levy fees to make sure that people are made aware of the nature of those fees. The ISPs aren&#x27;t responsible for this stuff, and shouldn&#x27;t be asked to do more work to further conceal decisions our elected officials are making for us.How does this make sense, given that ISPs are lobbying to be allowed to continue concealing these supposed decisions? reply Konnstann 10 hours agorootparentprevIf the fees they charge are due to specific levies the government places, and those fees depend on where the customer is located, why is it that much work to demote that in bills, considering they are already calculating who to charge what fee where? If my Internet bill had a bunch of line items that said \"local city tax\" or some such I can&#x27;t really be mad at that can I?Sure every government body could make their own website listing telecom fees and then I&#x27;d have to go to at least 3 (local, state, federal) to figure out how much I&#x27;d be paying or the ISP could tell me since they already aggregate that info. reply tptacek 10 hours agorootparentIt is listed in bills. The objection NCTA has is the FCC&#x27;s requirement that the fees be catalogued to consumers at the point of sale, before a bill has been incurred. I don&#x27;t blame them for objecting to this. These aren&#x27;t Comcast&#x27;s fees; they&#x27;re municipal taxes. reply nulbyte 9 hours agorootparent> I don&#x27;t blame them for objecting to this. These aren&#x27;t Comcast&#x27;s fees; they&#x27;re municipal taxes.I do. If they are obligated to collect it, they ought to be obligated to inform the customer at the time of purchase. It&#x27;s unreasonable to expect customers to agree to vague terms that leave them not knowing how much they are going to pay.If I go shopping online, I know how much I&#x27;m going to pay in shipping and sales tax before I agree to the charge. This doesn&#x27;t seem that different to me. The ISP knows how much they are going to charge. They just don&#x27;t want to tell their customers until it&#x27;s too late. I&#x27;ll blame &#x27;em for it all day long, it&#x27;s their fault they don&#x27;t want to tell me up front, not my government&#x27;s. reply kerkeslager 2 hours agorootparentI&#x27;d actually go further and say that taxes should always be included in prices, even before checkout.A common anti-poor talking point is that poor people don&#x27;t make and stick to budgets, but corporate refusal to include tax in prices means that you can&#x27;t reasonably tell how a given purchase will affect your budget beforehand. A simple trip to the grocery store means that you have to know what taxes apply to every item in your cart before you go to checkout to be able to know how much you&#x27;re spending. That&#x27;s not reasonable for the average consumer. If you really think it is, go ahead and try calculating your total of 20+ different grocery store items before you check out--you&#x27;ll fail. reply hnbad 1 hour agorootparentprevHow can you consent to a contract if you don&#x27;t know what the contract actually entails? I realize a lot of places in the US don&#x27;t even spell out sales tax on items before they&#x27;re rung up at the cashier but this level of intransparency seems like it contradicts the entire premise of legal contract theory.If an ISP collects these fees and taxes, it&#x27;s their obligation to inform the customer about them and to include them in price listings, at least in addition to the raw price. For example in Germany b2b price listings will often exclude VAT (which means VAT will still be shown but not included in the listed item price) but for b2c prices will always match exactly what you pay. The only example I&#x27;ve ever seen of a \"surprise tax\" in my entire life both in private and professionally is artists&#x27; social security insurance contributions, which are owed by any business or organisation paying a contractor or employee defined as an \"artist or editor\" (which includes most creative and media work): the fee is not collected by the person you pay but has to be paid to the social security organization directly and it&#x27;s something you&#x27;re simply expected to know if you run a business or organization. reply lmm 7 hours agorootparentprevPaying another organisation in exchange for use of their infrastructure isn&#x27;t a tax, and doesn&#x27;t become one just because that organisation happens to be a governmental body. Should these ISPs be able to \"pass through\" their upstream transit costs as a fee on top of their advertised prices too? Or maybe their peering fees with government-operated ISPs but not with privately owned ISPs? reply tptacek 7 hours agorootparentThat&#x27;s not how these fees work. They&#x27;re usually specifically levied per-customer, with the intent that they be passed through as fees to customers. reply lmm 6 hours agorootparentSurely the infrastructure owner can offer whatever pricing structure they like. Per-seat fees aren&#x27;t exactly rare in private industry contracts either. reply tombakt 10 hours agorootparentprev> It should be the responsibility of public bodies that levy fees to make sure that people are made aware of the nature of those fees. The ISPs aren&#x27;t responsible for this stuff, and shouldn&#x27;t be asked to do more work to further conceal decisions our elected officials are making for us.What are your thoughts on businesses incorporating and listing the amount of sales tax paid on receipts of transactions at your local grocery&#x2F;convenience store?It appears to me that the least surprising place for these things to be listed is where it is most relevant, which is alongside the primary transaction presented as an invoice or receipt. How would you improve on this UX assuming that the fee is definitely going to be incorporated into the cost?> I generally think most middle-class people aren&#x27;t taxed enough (yell at me somewhere else about this)I&#x27;ll refrain from yelling. Can you expound on this since you thought to mention it? reply hnbad 1 hour agorootparentprevThat&#x27;s funny because when renting in Germany your landlord does pass on various fees and taxes like this but they still have to give you an itemized bill spelling out every single fee and tax individually and for shared utilities they have to spell out the before and after readings of the meter so you can double-check with the previous bill.Likewise on every price increase my German electricity provider gives me a listing of each fee and tax that goes into the price. This is routinely used to point out \"hey, we&#x27;re not being greedy, most of this actually goes to your government so maybe deregulating us would save you money, wink-wink, nudge-nudge\". If e.g. Comcast thinks its taxation is particularly punishing and anti-competitive, increasing transparency about this would actually help make their argument for them. That they still prefer to be intransparent and pretend this is unreasonably difficult when they literally have to do all the same accounting internally anyway suggests the intransparency is giving them an advantage. reply gamblor956 9 hours agorootparentprevImportantly, none of these are fees ISPs \"can\" charge; they&#x27;re taxes that public bodies collect through the ISP&#x27;s billing system. This is false. The ex parte filing itself clearly states that these passed-thru fees are not taxes and that part of the burden ISPs are trying to avoid is having to break out these fees the way they already are required to break out taxes.The truly bizarre thing is that the ISPs already have the breakdown of these fees available to them when they create the bill. They&#x27;re actually doing more work to obfuscate this information by summing those amounts together.The ISPs aren&#x27;t responsible for this stuff, and shouldn&#x27;t be asked to do more work to further conceal decisions our elected officials are making for us.But that&#x27;s exactly what ISPs are trying to do. If you truly understood what you appear to be arguing for (accountability for elected officials), you should be in favor of requiring ISPs to break out these fees separately...because then customers would be able to see exactly what fees are making their bills so large.The only reason ISPs don&#x27;t want to do this is because they impose their own, wholly-made up \"service fees\" and include this in the fee amount they \"pass along\" to the customers. If they had to break out fees separately, it would be immediately apparent that government-imposed fees are a substantially smaller portion of the bill than ISPs (and their sycophants) claim. reply tptacek 9 hours agorootparentThis is money that is, by design, collected from customers of the ISPs through the ISPs (that&#x27;s why they&#x27;re called \"pass-through fees\"). When you read your local municipal budget, you&#x27;ll see a line item for the revenue collected from these fees, which, again, are levied by local governments against their residents. Each dollar in that line item literally offsets a dollar of property tax levy. I&#x27;m sure there&#x27;s some technical reason why I&#x27;m not allowed in court to call these \"taxes\", which is why I&#x27;m glad I&#x27;m a citizen involved in local government and not a tax lawyer, because that means I don&#x27;t have to care about this distinction: they are taxes. reply TuringNYC 12 hours agoparentprevThere were rumors Amazon was entering the space. It would be awesome for incumbents to be disrupted and for the system to move towards actual competition.I wish Google was able to achieve this with their GrandCentral acquisition, but as usual they Google let their acquisition wither away. reply 2023throwawayy 12 hours agorootparentPlease no.I agree disruption in the space would be great, but don&#x27;t let Amazon enter the ISP space. They have enough money, and power over the internet, as is.I would have to think that would be an antitrust violation anyway. reply NickC25 12 hours agorootparentSeriously...don&#x27;t give Bezos any ideas. Amazon in the ISP space would be an absolute disaster for consumers. reply crazygringo 12 hours agorootparentISP&#x27;s are already a disaster for consumers, with their totally deceptive pricing and invented fees.Any new entrant can only increase competition and drive down prices.Amazon as an ISP would be amazing for me. There&#x27;s only one ISP that serves my neighborhood already, so any additional one would be a godsend.If it&#x27;s between no new ISP and a new Amazon ISP, I&#x27;ll take Amazon any day of the week. reply dotnet00 10 hours agorootparentKind of like Google Fi in the mobile service provider space, anyone, even if someone as otherwise bad as Google or Amazon, can potentially disrupt things and improve prices and functionality. reply belval 12 hours agorootparentprev(I work at AWS)I know this is close to whataboutism so I will try to choose my words carefully, but does Amazon really have that much power over the Internet? Realistically Amazon does not have a large advert network, footing in mobile, operating systems or web browsers?They have some control in the form of AWS&#x2F;Route 53 but that&#x27;s a far cry from some larger DNS, domain registrar and CDN providers.Not denying that they have a lot of money, but even when using that money to try and buy a user base (Alexa or Fire Tablet&#x2F;TV) their success has been pretty limited overall. reply thewildginger 11 hours agorootparentWhen you control the most common way to buy and sell goods personal goods in the western world, you control a lot more than the internet. You control everything else. reply KerrAvon 11 hours agorootparentprevThere are different forms of power. Pretend you could switch off AWS completely tomorrow. How much of the internet would disappear? reply est31 11 hours agorootparentprevComparing with Google isn&#x27;t fair, they are probably the company with the single most power over the internet out there. reply krono 11 hours agorootparentprevWithout meaning to imply anything, I do suppose that powering much of the western internet does indeed equal having much power over the internet. reply mr_toad 7 hours agorootparentI think you overestimate how much of the internet runs on AWS. In terms of websites it’s less than 10%. Their share of the overall cloud is about 35%. reply extraduder_ire 8 hours agorootparentprevWould those project kuiper satellites count as entering the space? It&#x27;s their starlink-alike that hasn&#x27;t quite gotten off the ground yet. reply philipwhiuk 28 minutes agorootparentIronically entering space is the exact problem they&#x27;ve been having. They keep picking launchers that aren&#x27;t ready. reply mcronce 7 hours agoparentprevIf charging for it isn&#x27;t too hard, it sure as fuck isn&#x27;t too hard to list it reply luma 12 hours agoprevSomehow it&#x27;s _really_hard_ to list all the fees ahead of time, yet it&#x27;s still _really_easy_ to list them all when it comes time to bill their millions of subscribers each and every month. The largest of these companies manage to compute this number more than a billion times a year.Any regulator who takes this line of reasoning seriously is immediately suspect. reply YeBanKo 9 hours agoparentWhile you are right, it’s not limited to telcos or isps, seems like in general consumer pricing must be required to be transparent. reply ryandrake 8 hours agorootparentThere&#x27;s nothing quite as American as not knowing what you&#x27;re going to pay until the moment you have to pay it. It happens in retail, hotels, especially healthcare, car shopping, everything. We have this weird culture of just accepting terrible status-quo. This is one of the many nationwide problems that we have convinced ourselves there&#x27;s no way to fix, despite many other countries in the world having fixed it a long time ago. reply csomar 7 hours agorootparentNo country has fixed this a long time ago. This problem never existed in the first place. In some countries not having a clear fee schedule will get you in troubles that you don&#x27;t want with your clientele. reply CameronNemo 8 hours agorootparentprevDon&#x27;t forget restaurants that tack on \"service fees\" that often range from 1-18%. reply ryandrake 7 hours agorootparentAnother person in a different thread[1] mentioned his city where businesses are tacking \"living wage surcharges\" onto their bills! So now the customer is paying your payroll?I&#x27;m convinced this is the inevitable end-state of American business: Advertise $0 for all products, and then opaquely \"pass through\" everything as fees when the customer goes to check out.1: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37141091 reply m463 5 hours agorootparentSounds like how free apps work.maybe a \"free food\" restaurant with upcharges for calories? more for protein calories? expensive diet plans? reply WesolyKubeczek 4 hours agorootparentI’ve been to a running sushi restaurant where you pay for your time at the table, and if your stomach is large enough and if you don’t waste your time, the food gets ridiculously cheap.Suffice to say it didn’t help my BMI at all. reply YeBanKo 4 hours agorootparentprevSF does it. They justified it as fee to offset cost of doing business. reply crote 8 hours agorootparentprevThe US can&#x27;t even get companies to include sales tax on price tags shown in physical stores. If they don&#x27;t even manage to do something that basic properly, don&#x27;t hold your breath trying to get them to fix telco&#x2F;ISP pricing. reply csomar 7 hours agorootparentI have mixed feelings about this one since it can help reminding people how much of their purchasing power is being extracted from the state. That being said, it could be fixed by having the two prices at the same time. reply YeBanKo 4 hours agorootparentIf it must be included in the price, business will go out of the way to demonstrate the part of the price a tax is responsible for. Pretty much everywhere where it’s the case, somewhere on the page with price there is a note explaining that tax is included. reply hnbad 49 minutes agorootparentprevIn Germany all listed prices everywhere (except in strict b2b spaces) include VAT. On invoices and bills prices are usually listed as both \"raw\", VAT and combined, often per item. The only exception is deposit on packaging (cans, bottles and bottle crates) where the deposit is always listed separately on price tags and listed as a separate item on the bill. But these deposits are standardized by law and can be refunded at any place that sells these goods.This doesn&#x27;t seem to have any particular effect (tho Germans are notoriously uninterested in political activism) except that any person on the street can likely tell you exactly what the VAT rate is (it&#x27;s currently 19% on most goods, 7% on some specific items and 0% on a much smaller set of items -- most people likely won&#x27;t know specifically which items are in those two categories tho as it&#x27;s somewhat arbitrary thanks to lobbying).Shops generally don&#x27;t break out what exactly goes into the \"raw\" price tho as that&#x27;s usually not in the brand&#x27;s best interest (because it spells out their profit margin, which may be surprisingly high). reply YeBanKo 4 hours agorootparentprevI actually do think it’s possible and easy to fix. We are few propositions away from forcing it everywhere in the country. reply Terr_ 13 hours agoprev> ISPs object to a portion of the FCC order that says, \"providers must list all recurring monthly fees\" including \"all charges that providers impose at their discretion, i.e., charges not mandated by a government.\" The five trade groups complain that this would require ISPs \"to display the pass-through of fees imposed by federal, state, or local government agencies on the consumer broadband label.\"OK, so their problem is solely with pass-through 1:1 fees from local governments, and the ISPs totally agree that there&#x27;s no problem showing their own fees that aren&#x27;t one-to-one pass-through amounts, right?... Riiiight?> Comcast said the non-mandatory fees also include pass-through of state and local government fees.Sounds like they&#x27;re choosing to mix fees [are &#x2F; aren&#x27;t] their fault together, and then whining that it&#x27;s \"unfair\" to make them list any of them. reply whaleofatw2022 11 hours agoparent\"Local government fees\" are often fees the provider pays for a certain level of exclusivity...They don&#x27;t want you to see the recurring bribes to the local politicians... reply phendrenad2 12 hours agoparentprevNo, because this article is only focusing on one objection here. You&#x27;ll have to do some googling. reply nonethewiser 12 hours agoparentprevThey should want to show pass through fees. I had no idea there were any. reply thomas-st 12 hours agoprevA while ago I was helping a friend pick a cell phone plan with T-Mobile USA. If you study their plans, the \"Essentials\" plan does not include \"taxes and fees\", but their \"Magenta\" plan does. When contacting T-Mobile, they could not tell me what the fees were even after providing the specific ZIP code. They said I would have to sign up for the plan first, and could then see the fees on the bill. Even when I told them that the choice of plan would depend on the amount of taxes and fees, they were not able to tell me and said that I could look at the current cell phone bill with the current carrier, and that the taxes and fees should be similar.It is crazy they can&#x27;t tell you how much you&#x27;ll be paying before signing up. reply Terr_ 12 hours agoparent> It is crazy they can&#x27;t tell you how much you&#x27;ll be paying before signing up.Even further along the dystopic spectrum: Imagine if it worked like health care insurance. Even monthly bills would be only guesses subject to arbitrary revision. reply MiddleEndian 12 hours agorootparentI&#x27;ve tried to pay a healthcare bill for an operation and a followup that had been completed months prior, and they still would not tell me how much I owed. I just got sporadic bills in the mail and there was a single website where I could enter how much I wanted to pay them in total. I waited a couple months, walked to the hospital, and asked them for the sum. They told me they had no way to know. I paid what I thought I owed and then I guess somebody figured it out without telling me, so I ended up in collections for a two figure sum. reply wildzzz 9 hours agorootparentIt&#x27;s because insurance companies tell hospitals how much they&#x27;ll pay for things but when you ask how much they actually cost, the hospital shrugs because no one is breaking down the prices. The various specialists may be independent of the hospital in terms of billing which only complicates things as you now have more than one bill (and they may be out of network for your insurance). The non-profit hospitals will usually work with people without insurance to figure how much they can afford to pay and just charge that. The actual costs are likely much more but no one really knows since insurance is likely overpaying to compensate the hospital&#x27;s losses on caring for the poor. reply MiddleEndian 8 hours agorootparentIt wasn&#x27;t just future bills though. I waited months. I had received a bunch of bills, from them, on paper, and apparently missed some. I knew I probably missed some. Why not just tell me, in person or online, what the sum of the bills they&#x27;ve sent me so far is? They&#x27;re calculating it at some point.It was a decade ago, and I&#x27;m still salty about it lol. I think the hospital has a functional website now, because it&#x27;s in their interest to get all of the money from customers instead of a much smaller percent from a debt collector anyway.Regardless, I think if an organization can&#x27;t tell people what they owe, it clearly doesn&#x27;t need that money and should forfeit it. reply gwright 12 hours agorootparentprevMy favorite example in this space is college tuition&#x2F;room&#x2F;board. This seems to be the only example of a service in which you have to share all your financial details with the vendor and then they will tell you how much it is going to cost. reply actionablefiber 11 hours agorootparentMany types of loans you can take out, where approval and interest are dependent on your credit history and assets, and also your tax burden to the government, are kind of like that too. Really anything where the amount charged varies with your ability to pay. reply lotsofpulp 12 hours agorootparentprevHealth insurance premiums are set annually. I don’t think insurance companies can legally change premiums after the insurance regular approved them. reply Terr_ 12 hours agorootparentNot premiums, but claims.\"It was recently discovered that your procedure involved a duck, however the insurance company will only cover geese, so here is the revised bill... Er, an invoice, not the animal&#x27;s.\" replynocoiner 9 hours agoparentprevVerizon has a tool online to estimate your fees for a given zip code. It’s hilariously broken - like it will return a list of the same city and county taxes listed dozens of times. It’s completely unusable. But as other posters have noted, it somehow doesn’t stop them from calculating and charging those fees every month. reply extraduder_ire 8 hours agorootparentAt the very least the support agent didn&#x27;t try to run a calculation, and cause a phantom bill you&#x27;d never know about until you get a call from collections or discover a lien on your house.That&#x27;s about the level of dystopia I&#x27;d expect. reply throitallaway 12 hours agoparentprevI recently signed up for a business TMobile tablet plan. I had the option to choose taxes&#x2F;fees as included or extra. The plans are identical. I have no clue why that&#x27;s even an option, but I&#x27;m glad I get to pay a nice round number. reply AlotOfReading 12 hours agorootparentThat exists because somewhere out there in the world is a category of businesses have to deal with the cost and taxes separately for legal or tax reasons. I&#x27;ve run into it before and it&#x27;s incredibly annoying when all you have is a single line item on the bill. It&#x27;s even more fun when you add currency conversions on top. reply abwizz 2 hours agoparentprev> they can&#x27;t tell you how much you&#x27;ll be paying before signing upseems unreasonable for a consumer facing product, how did it come to that? reply figassis 6 hours agoparentprevMy reasoning here is this: it should not be legal to provide pricing when the customer already owes and has no choice to go elsewhere. That price should be zero.Capitalism is playing with supply and demand. Holding people hostage is not it.Example: I went to Hawai this month with my wife and the fires broke out. So I went to United’s website to move up my return flight. They said in order to have the option to change the flight free of charge I needed to upgrade from Economy Basic to Economy for $90 ($45&#x2F;person). After I did, the site said there were no flights, however I could see the flights on kayak. So I call United, and indeed, they had a flight, but it would cost $1000 ($500&#x2F;person). The return flight cost more than the entire 2 way flight as originally booked. Plus the $90 upgrade for no fee changes.Either the $90 or the $1000 should be illegal. reply Ylpertnodi 10 hours agoparentprev>It is crazy they can&#x27;t tell you how much you&#x27;ll be paying before signing up.Crazy? 2023? I would consider it &#x27;expected&#x27;. reply hnbad 43 minutes agorootparentIt&#x27;s both, relative to the US. The US is uniquely horrible in this because it has no real consumer protection mechanisms. Instead a lot of US business practices are the result of litigation rather than policy.It may be expected in the US but it seems ridiculous in Germany because we have a consumer protection agency and it has teeth. On the other hand, suing a company for damages won&#x27;t get you nearly as much money here. But of course most of your medical expenses would be covered by public health insurance and you normally don&#x27;t have to worry about something being \"out of network\" or requiring a copay etc. reply tguvot 11 hours agoparentprevused to work in a company that build and implemented BSS&#x2F;OSS system for major telcos (including the one that you mentioned).I can totally see that high level pricing for a packages is modeled globally and exposed to sales team while taxes are implemented only in billing system, because its \"zip code\" specific. reply TuringNYC 12 hours agoprevMy favorite thing is when Telcos say they cannot tell you how much their plan will cost when you sign the contract, but magically, they are able to figure it out when you get your first bill. reply throitallaway 12 hours agoparentThis is why I like (for cell plans) prepaid carriers. Their prices are all-inclusive. reply Terr_ 10 hours agorootparentYeah, I recently switched service and prepaid a full year. Not just fewer surprises, but often cheaper.I still live in a kind of dismayed disbelief at how much money the major cell-phone carriers want for plans they consider normal&#x2F;budget. This seems to be because they include what is (to me) a crazy amount of cellular data, amounts I&#x27;ll never use because I don&#x27;t stream full-length movies on the bus or whatever.For context, a major local network&#x27;s \"essentials\" plan costs $60&#x2F;month for 50GB of \"premium\" mobile data and unlimited not-so-premium data. In contrast, a reseller&#x27;s prepaid plan (on the same infrastructure) comes out to $15&#x2F;month for 5GB (5G speeds) and a 4gb trickle (LTE) after that.Since I&#x27;m always near Wi-Fi and almost never exceed 2GB&#x2F;month, it&#x27;s a no-brainer. reply fuomag9 1 hour agorootparentMeanwhile here in Italy rechargeable sims have 10 euros per month plans with unlimited data (which is actually ~1TB but it&#x27;s not advertised since it&#x27;s more of a fair use thing) or very high caps (150-200GB) reply Shank 5 hours agorootparentprevThe biggest issue with being on an MVNO is that your data priority is below all other customers. It’s not even that bad most of the time, but if you go to a congested area like a concert you’ll definitely feel the speed decrease. reply Spivak 7 hours agoparentprevIt&#x27;s because the people who make the website aren&#x27;t the people who manage the billing which a labyrinthine black box of custom hand-rolled rules. That poor customer service rep has no idea. reply nonethewiser 12 hours agoparentprevWell is it usage based? There are potentially real reasons they might not be able to tell you. reply ender341341 11 hours agorootparentWith usage based they can tell you it will be \"base cost + cost&#x2F;unit\", but some carriers will tell you they can&#x27;t determine the taxes&#x2F;fees you&#x27;ll be subject to until you&#x27;re billed.It likely wouldn&#x27;t hold up in court very well, but also you&#x27;d have to be able afford that reply laurencerowe 10 hours agoprevThe US desperately needs to institute the EU rule that all advertised prices are inclusive of all taxes and fees.It’s not just sales tax (which admittedly varies by city) but you also see absurd things like ‘SF health mandate’ on a receipt at a restaurant. What are they going to break out next, rent? reply breadwinner 9 hours agoparentIn Seattle when they increased minimum wage to $15, this airport parking company called MasterPark started adding a \"living wage surcharge\" to their bill [1]. Employee wages are a \"surcharge\" that&#x27;s added on to the bill!! I don&#x27;t think anything can top this.[1] https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;kellyphillipserb&#x2F;2014&#x2F;06&#x2F;12&#x2F;sea... reply expertentipp 2 hours agoparentprevSorry but you have some idealistic view of some EU regulations. In every country where ISPs form a oligopoly, predatory practices and billing ensue. reply yladiz 21 minutes agorootparentEven if the prices aren&#x27;t great depending on the provider, at least in EU countries the price you see is what you pay. My internet bill is not €30 + hidden fees + taxes + executive coffee fund, it&#x27;s just €30. You may be charged additionally in specific circumstances like ending a contract early without cause, but in the general case you aren&#x27;t charged on top of what you see, so I don&#x27;t know what you mean by \"in every country [...] predatory practices and billing ensue\" because it&#x27;s not necessarily the case. reply himinlomax 1 hour agorootparentprevIn France we have one operator (SFR Red) that&#x27;s known for being shit and messing with billing and having predatory practices, but the other three major operators are generally fine and you typically pay exactly what was advertised. reply sterwill 13 hours agoprevGoogle Fiber gets this right. They say my Internet package will cost $70 and every month the total I get charged is $70. reply sschueller 12 hours agoparentSame here. CHF 777 per year for my 25gbit internet and TV service [1] . Any extra fees from interconnects etc are already included.Same goes for sales tax. It must be included in the price of the product when displaying the price. No surprises at the cash register.[1] https:&#x2F;&#x2F;www.init7.net&#x2F;en&#x2F;internet&#x2F;fiber7&#x2F; reply Blammar 11 hours agorootparentNow you&#x27;re making me want to move to Switzerland, even though I hate living in the mountains... reply petepete 10 hours agorootparentI wouldn&#x27;t even know what to do with 25Gbps internet.How many people in Switzerland even have home networks capable of taking full advantage of those speeds? reply avidiax 3 hours agorootparentI&#x27;m also on Init7. 25gbps is pure nerd cred.The only hosts you can talk to with that speed are other 25gbps customers, with custom hardware, and an ultra fast NAS or sending random bits.I have 10gbps, also for nerd cred. 1&#x2F;10&#x2F;25gbps from this provider is all the same monthly cost, differing setup fees.The advantage to 10gbps is I have a silent router (Mikrotik RB5009), and every 1gbps port is entirely independent. You will never have 1 machines activities slow another&#x27;s.Do I ever actually get 1gbps speeds? Yes, sometimes, for large downloads on XBOX. BitTorrent doesn&#x27;t achieve this because your peers don&#x27;t have that upload. Google drive doesn&#x27;t achieve that because their VMs are probably shared 1gbps each.But, Init7 has excellent peering vs other 1gbps fiber providers and no traffic shaping at all. reply _zoltan_ 4 hours agorootparentprevI see init7, I upvote. reply hsbauauvhabzb 12 hours agoparentprevIn Australia all prices are written on the box. Do US quoted prices not include tax or something? How many additional fees are there? reply db48x 10 hours agorootparentIt depends on the locality. The city, county, and state may all impose taxes. In particular, cities sometimes opt to generate revenue by taxing specific services, such as cable TV. So a resident in City X pays $5 per month extra for cable TV, while someone in City Y pays $7.44 extra. Both of these cities allow just a single cable operator, and require that operator to collect the tax and remit it to them. But if you have satellite TV then you don’t pay the tax. And maybe City Y never updated the law to apply the tax to cable internet service, but City X did. So someone living in City X pays $5 per month if they have both cable TV and internet service (or either one alone), while someone in City Y only pays the tax if they have TV service. If all they have is internet service, then they pay nothing extra.And if the service includes telephone service, then there will be a Federal Universal Service Fee, which pays for reduced–cost land–line telephone service for the poor. It’s like $0.11 per month per telephone line, IIRC.The average customer might see two or three of these things on their bill. But it’s not uncommon to see a dozen either.So it definitely depends on where the subscriber wants the service, and what service or services they subscribe to. None of this is really all that difficult to figure out, and these companies have already automated it. They are just complaining because they will have to raise their advertised prices, which naturally will lead to reduced sales. Currently they just lie through their teeth about their prices, and count on people being too lazy to cancel their service once they see the first month’s bill. reply hsbauauvhabzb 6 hours agorootparentInteresting, for context in Australia almost all goods and services that aren’t billed based on usage must clearly outline total cost - if you order something you can expect the marked price to match the reciept (unless the receipt is less, common in retail discount offers)We also don’t have tipping culture, you can tip, but it’s not an expectation. Though, the latter is slowly creeping in. reply db48x 2 hours agorootparentYes, I understand that. In fact I would like to see us move in that direction. I think these new rules are definitely a step in the right direction.However, put yourself in the position of someone selling internet service in the whole country of Australia. If you put out a television ad that will be seen by everybody, then you either cannot quote a price because every viewer could end up paying different taxes, or you must quote a price but put a footnote that mentions local taxes, or some other dodge. The new rules actually cover this case as well. In any situation where they show a generic price not including local taxes or other fees, the ISP must instead direct the consumer to a source that gives them a correct customized price (presumably a web page). They must also document the event, which is something else the ISPs are objecting to. I don’t know enough about it to say if that would be a good thing or not.It’s the same with sales taxes too, btw. Every town and county has their own sales taxes, so advertised prices never include them. At least in that case each store will only ever need to charge a small number of tax rates so in principle it would be easy enough for the stores to print price tags that show how much tax would be collected for each item. Still, it was historically easier to leave the tax information off of the price tags and rely on the customer to simply remember the local sales tax rate (usually between 2% and 10%, except on tax–exempt products) so that’s what we still do.On the other hand, gas prices always include the taxes. Historically gas taxes have had a much higher percentage rate (because they are taxed a fixed number of cents per gallon, rather than as a percentage of the price), so maybe gas stations just didn’t want customers to argue with the clerks. reply rajamaka 10 hours agorootparentprevAlso Australian, when I went to the US and ordered a burger at McDonalds for $2 - I was so confused when they asked for more money after I handed them $2. Turns out they don&#x27;t include taxes in the advertised price for some reason. reply delvinj 13 hours agoparentprevSame with US Internet in Minneapolis. I couldn&#x27;t be more pleased with the service, coming off of 15 years with Comcast. reply jvanderbot 12 hours agorootparentWhat kind of speed do you get? is this municipal wifi? reply remexre 12 hours agorootparentI&#x27;m a happy customer until the end of the month (moving out of their service area, sadly); I had symmetric gigabit. No IPv6, though. reply jvanderbot 9 hours agorootparentI live in the area, and loved Centurylink for this reason (symmetric gigabit, extremely low latency), but am stuck with Xfinity. reply tzs 12 hours agoparentprevThat&#x27;s how Comcast works here in western Washington.If you have non-internet service from them (such as TV, phone, or home security) there are plenty of fees on your bill which makes it more than the advertised price of your package, but if you just have internet the bill matches the advertised price. reply fn-mote 12 hours agorootparent> if you just have internet the bill matches the advertised price.Until your fixed-rate introductory period ends. After that, watch the price creep up month by month. I&#x27;ve even seen very low-speed service become more pricey than high-speed service (possibly because it&#x27;s no longer being advertised, so the rate doesn&#x27;t have to look good).Source: experience with a non-Comcast cable provider. reply tzs 11 hours agorootparent> Until your fixed-rate introductory period ends.If you are on a contract then when the contract ends your fixed rate will jump from the contract fixed rate to the advertised fixed rate for month to month service.If you are not on a contract internet is fixed rate at the advertised plan rate.For example if I go to their site and pretend do be someone moving to my area and look at their plans for my neighborhood my current plan without a 2 year contract is $73&#x2F;month, or $63&#x2F;month if you go paperless and use use autopay. That&#x27;s the same price I am paying as a long time customer on that plan.With a 2 year contract it is $45&#x2F;month or $35&#x2F;month if you go paperless and use autopay for years 1 and 2, and then goes to up to the prices from the previous paragraph.It really is that simple. Plan price, minus $10&#x2F;month for paperless and autopay, minus promotional discount if on a contract.When you add TV or voice then you get things that won&#x27;t necessarily be fixed rate. There&#x27;s a broadcast TV fee and if you TV package includes sports channels a regional sports fee. If you have voice then there are state fees. There are also state taxes on TV and voice. reply Arrath 8 hours agorootparentprevYup, I&#x27;ve seen this myself. My introductory plan at 69.99&#x2F;mo (or 79 I can&#x27;t recall) was good for the first 6 months or maybe a year, and then it steadily crept up in $10 increments every quarter or so for the next 4 years I had their service.To be fair, the plan also went from 200 megs to 1 gig over that timeframe in similar incremental increases, so that was kinda nice. reply notnmeyer 8 hours agoparentprevZiply Fiber is the same way. $70 exactly every month, 1 page bill. Trivially easy to understand. Symmetrical gigabit, no data caps. No ipv6 though which sucks.Left Comcast&#x2F;Xfinity the moment I had a better alternative. I still get door to door Xfinity folks trying to sell me more expensive, shittier service. reply Cerium 13 hours agoparentprevATT and Sonic fiber too. Both listed a price and charged it (90 and 50 respectfully). reply bsimpson 12 hours agorootparentAlso MonkeyBrains in SF: they charge $105 every 3 months. reply nottorp 34 minutes agoprevI still don&#x27;t understand how it can be legal to charge more than the price written in the contract...Feeling like... if i ever moved to the US... i&#x27;d have to declare bankruptcy in a month because I&#x27;ll spend triple what I think I&#x27;m spending due to the \"fees\". reply muppetman 9 hours agoprevWorking at an ISP I&#x27;ve never understood all these fees&#x2F;random charges, and this \"fear my ISP is watching what I&#x27;m doing\" Then I realise it&#x27;s a US thing. In NZ we don&#x27;t log&#x2F;care what DNS sites you hit. You&#x27;re better off to use our DNS because it&#x27;s faster&#x2F;closer to you than 1.1.1.1&#x2F;8.8.8.8. We don&#x27;t charge any fees, short of the $90 a month for unlimited Gigabit Ethernet speeds fibre. I feel so bad for you all. As a tech behemoth you bring amazing technology to the lives of everyone on the planet (well, within reason). But it seems your personal lives have to suffer as a result, it&#x27;s so screwed up. I hope all and any stupid additional \"fees\" are shot down. The \"Service costs you $X a month\" should be all you see, it&#x27;s up the ISP to account for all their outgoings before setting that price.Edit: To be clear, the ISP I work for. I don&#x27;t think any of the others do either, but I obviously can&#x27;t vouch for what they do&#x2F;don&#x27;t do. I know the Mobile Carriers here aggregate and sell locations data from the cell towers. reply zer8k 6 hours agoparent> and this \"fear my ISP is watching what I&#x27;m doing\" Then I realise it&#x27;s a US thingI&#x27;m not sure about it being a US thing. But I don&#x27;t personally have a privacy fear with my ISP because I can&#x27;t really do anything about it. At some point a clear request has to hit the edge of my ISP and at that point, no matter what I do, they will know what I&#x27;m doing. It&#x27;s better to mitigate the deep packet inspection part of it rather than worry about DNS requests leaking (that&#x27;s what coffee shops are for).The real problem I have with it is I pay $280&#x2F;mo. for a \"gigabit\" line from Cox. It has a data cap. On top of having a data cap if I don&#x27;t use my own DNS Cox will inject ads and other data into my requests. There are several blog posts on this happening and I am not on my computer with screenshots. It suffices to say that they, and other ISPs in the US (at least), regularly poison requests with advertisements, warnings, alerts, and other non-sense. During COVID, despite paying for a top tier plan, my connection was throttled to 10 Mbps 5&#x2F;7 days a week because so many people were watching Netflix. It doesn&#x27;t even matter if you pay more they&#x27;ll QoS your ass into the ground at a whim. Billions of dollars and they can&#x27;t even spend the time to install better hardware.The combination of last-mile control and a resulting veritable monopoly on internet in most localities means ISPs can more or less do what they want. Sucks. I try not to blame the employees but they, too, are complicit in this nonsense. ISPs are a legitimate cancer. reply RajT88 7 hours agoparentprevProfit is the almighty motivator in the US.For reasons I cannot fathom, most outages in US ISP&#x27;s are due to their DNS servers going down.The reason ISP&#x27;s log all your traffic is so they can sell that data to data brokers.The sale of a data set is not a one-time thing. You can sell it over and over again. Different brokers or end customers or even government agencies (yes the government is buying that data, reported here even) or you can sell time slices of that data or demographic slices of that data.I could not tell you how profitable that data sales is. It is a closely guarded secret. This could mean it is very high or very low, either sparking much outrage.But for sure, ISP&#x27;s in the US are spying on their users for profit.Why the US and not (say) NZ? Lots of money, and lots of people. I would trust an ISP in Vanuatu not to spy on me. reply abigail95 6 hours agoparentprevYou aren&#x27;t high enough on the ladder at your ISP.NZSIS has direct access to your network and 100% has a copy of that DNS traffic, including any and all metadata for every single person.Domestic SIGINT in all five eyes countries is drag net, it&#x27;s all collected, facilitated by ISPs. reply muppetman 2 hours agorootparentAre you suggesting the CEO configures the core upstream routers &#x2F; BNGs without telling me? This is certainly a shock.Regardless - I was talking about us as an ISP collecting&#x2F;selling that data - not the data we must provide to government agencies to fulfil our legal obligations. reply 52 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The US broadband industry is opposing a requirement by the FCC to disclose all monthly fees, arguing that it would add complexity and burden for both consumers and providers.",
      "Lobby groups suggest including an explanatory statement about fees, similar to government-imposed taxes, instead of listing all fees.",
      "Consumer advocates criticize this proposal, stating that it allows ISPs to hide the true cost of service.",
      "The FCC's broadband labeling rules, including pricing and additional charges, are currently being reviewed by the US Office of Management and Budget."
    ],
    "commentSummary": [
      "The discussion covers multiple topics concerning ISPs, such as fee transparency, regulation as utilities, tipping culture, tap water availability, pricing transparency, and fees charged by ISPs.",
      "It also includes concerns about lobbying, government corruption, deceptive pricing, competition from new entrants, and ISP surveillance and data collection practices.",
      "The overall theme is the call for more transparency, consumer protection, and regulation in the ISP industry."
    ],
    "points": 406,
    "commentCount": 302,
    "retryCount": 0,
    "time": 1692129943
  },
  {
    "id": 37132754,
    "title": "CSS Selectors: A Visual Guide",
    "originLink": "https://fffuel.co/css-selectors/",
    "originBody": "🪄 Cool social media templates CSS Selectors: A Visual Guide ✨ Here's a visual guide to the most popular CSS selectors. CSS selectors are patterns used in CSS to select and style HTML elements on a page, allowing us to dictate how styles apply to specific HTML elements. Along with traditional CSS selectors, we also have pseudo-classes and pseudo-elements. Pseudo-classes allow us to define styles based on an element's state or its relation to other elements. Think of things like hovering over a button or selecting the first item in a list. Pesudo-classes start with a colon character (:). On the other hand, pseudo-elements give you the power to target and style specific parts of an element, such as its first line or before and after its content. Pseudo-elements start with two colons (::). This guide serves as your map, detailing and visualizing these essential selectors. ↑ * element .class #id .class.class-2 .class,.class-2 .class .class-2 .class + class-2 .class > class-2 .class ~ class-2 * + * [attr] [attr=val] [attr~=val] [attr*=val] [attr^=val] [attr$=val] :link :active :visited :hover :focus :checked :disabled :enabled :valid :invalid :required :optional :first-child :last-child :nth-child :nth-last-child :only-child :first-of-type :last-of-type :nth-of-type :nth-last-of-type :only-of-type :target :not() :has() ::before ::after ::first-letter ::first-line ::placeholder ::selection ::marker ... * universal selector Select all elements: div * { background: coral; } element element selector Select element(s): p { background: deeppink; } .class class selector Select all elements with the specified class name: .my-class { background: royalblue; } #id ID selector Select the element with the specified ID: #my-id { background: aquamarine; } .class.class-2 multiple selectors Chain two or more classes or IDs to select the element(s) that have all the specified classes/IDs: .my-class.special { background: royalblue; } .class, .class-2 comma combinator Separate multiple selector declarations using a comma. This makes it easy to apply the same styles to multiple selector declarations: .item-1, .item-2 { background: sandybrown; } .class .class-2 descendant selector Leave a space (descendant combinator) between selectors to select element(s) that are descendant of another element: .wrapper .card { background: lightblue; } .class + .class-2 adjacent selector Use a plus sign (adjacent combinator) to select an element that is a direct sibling to a first element: .item-1 + div { background: yellowgreen; } .class > .class-2 child selector Use a > sign (child combinator) to select element(s) that are direct children to another element: .wrapper > div { background: olive; } .class ~ .class-2 general sibling selector Use a tilde sign (general sibling combinator) to select every element that is preceded by the first element, without having to be a direct sibling to the first element: .item-1 ~ div { background: lightcoral; } * + * lobotomized owl A selector pattern known as the lobotomized owl where all elements that have a preceding sibling are selected. Use it for example to add spacing to elements within a container except for the first element, which has no preceding sibling. Note that now that we have the :not() pseudo-class, we can select the same elements using :not(:first-child): * + * { background: khaki; } [attr] attribute selector Select element(s) that have a specified attribute: [data-text] { background: deepskyblue; } [attr=val] attribute & attribute value Select element(s) that have the specified attribute and attribute value: [data-text=\"hello\"] { background: lemonchiffon; } [attr~=val] attribute & one of the attribute's values Select element(s) that have the specified attribute with one of it's space-separated values matching the value: [title~=\"old\"] { background: crimson; } [attr*=val] attribute & partial value Select element(s) that have the specified attribute with val being included in the attribute value: [title*=\"saur\"] { background: darkgoldenrod; } [attr^=val] attribute & starting value Select element(s) that have the specified attribute with a value that starts with val: a[href^='http'] { background: Aquamarine; } a[href^='#'] { background: AntiqueWhite; } a[href^='/'] { background: LavenderBlush; } External link Internal link Relative link [attr$=val] attribute & ending value Select element(s) that have the specified attribute with a value that ends with val: a[href$='.jpg'], a[href$='.png'] { background: greenyellow; } Page link Image link Another page link :link :visited :hover & :active link pseudo-class selectors These 4 pseudo-classes are useful to select elements such as links in various states. These 4 are most often used with links, but :active is also useful for buttons and :hover can be used on all kinds of elements: :link - Targets unvisited links. It allows you to style hyperlinks that the user hasn't clicked on yet. :visited - Targets links that have already been visited by the user. This pseudo-class lets you apply styles to previously clicked hyperlinks. :hover - Targets elements (commonly links) when they are being hovered over by the user's pointer, such as a mouse cursor. :active - Targets elements (typically links or buttons) during the moment they are being activated, like when a user clicks on them. a:link { background: aliceblue; } a:visited { background: blanchedalmond; } a:hover { background: honeydew; } a:active { background: lavenderblush; } Tyrannosaurus :focus focused input element(s) The :focus pseudo-class targets an element when it receives focus, such as when a user clicks on an input field or navigates to it using the keyboard: input:focus { border: 2px solid deepskyblue; background: lightcyan; outline: none; box-shadow: 0 0 8px rgba(0, 191, 255, 0.5); } Your name: :checked checked input element(s) The :checked pseudo-class targets radio buttons, checkboxes, or options in a select element that are currently selected/checked. In the following example I make use of appearance: none to remove the default styling of the checkbox input, then use the :checked pseudo-class to style the sibling label, while also adding styling on the label when the checkbox is focused: input[type='checkbox'] { /* remove default checkbox styles */ all: unset; -webkit-appearance: none; appearance: none; margin: 0; } input[type='checkbox']:checked + label { background: mediumseagreen; } input[type='checkbox']:focus + label { box-shadow: 0 0 0 2px yellow; } I am a label :disabled disabled input element(s) The :disabled pseudo-class matches form elements like buttons or text inputs that are disabled: input[type=\"text\"] { padding: 10px; border: 2px solid mediumslateblue; margin-right: 10px; } input[type=\"text\"]:disabled { background: lightgray; border: 2px solid darkgray; color: darkgray; } :enabled enabled input element(s) The :enabled pseudo-class matches form elements that are interactive and can receive input: input[type=\"text\"] { padding: 10px; border: 1px solid gray; margin-right: 10px; } input[type=\"text\"]:enabled { background: lightgreen; border: 1px solid green; } :valid valid input element(s) The :valid pseudo-class is used to target an input element that has content that matches the requirements as specified by its attributes (like pattern, type, etc.): input[type=\"email\"]:valid { border: 2px solid limegreen; background: honeydew; } Email: :invalid invalid input element(s) The :invalid pseudo-class is used to target input elements that have content that doesn't match the requirements: input[type=\"email\"]:invalid { border: 2px solid tomato; background: mistyrose; } Email: :required required input element(s) The :required pseudo-class targets input elements that have the required attribute, indicating that they must be filled out before the form can be submitted: input:required { border: 2px dotted orangered; background: floralwhite; box-shadow: 0 0 10px rgba(255, 69, 0, 0.2); } Name (required): Optional Field: :optional optional input element(s) The :optional pseudo-class targets input elements that do not have the required attribute, implying that they're not mandatory to fill out: input:optional { border: 2px dotted darkgray; background: whitesmoke; box-shadow: 0 0 10px rgba(105, 105, 105, 0.2); } Name (required): Optional Field: :first-child first child element The :first-child pseudo-class targets the first child element within its parent: div { border: 1px dotted gray; } div:first-child { background: lightblue; border-color: deepskyblue; border-style: solid; } First Child Second Child Third Child :last-child last child element The :last-child pseudo-class targets the last child element within its parent: div { border: 1px dotted gray; } div:last-child { background: lightblue; border-color: deepskyblue; border-style: solid; } First Child Second Child Last Child :nth-child nth child element The :nth-child pseudo-class targets elements based on their position within their parent, allowing for a wide variety of selections. :nth-child also lets you select elements in patterns: :nth-child(odd) or :nth-child(2n+1) selects every odd-numbered child :nth-child(even) or :nth-child(2n) selects every even-numbered child The 'n' in the formula is like a counter, letting you target elements in repeating cycles. For instance, :nth-child(3n) would target every third element. div { border: 1px dotted gray; } div:nth-child(2) { background: lightcoral; border-color: darkred; border-style: solid; } First Child Second Child Last Child :nth-last-child nth child element, counting backwards from last The :nth-last-child pseudo-class is similar to :nth-child, but it counts from the last child backwards: div { border: 1px dotted gray; } div:nth-last-child(2) { background: darkorchid; border-color: indigo; color: white; border-style: solid; } First Child Second Child Third Child Last Child :only-child only child of an element The :only-child pseudo-class targets an element if it's the only child element of its parent: div { border: 1px dotted gray; } div:only-child { background: lightsalmon; border-color: darksalmon; border-style: solid; } 1st Child Inner child 1 Inner child 2 2nd Child Only child of '2nd Child' :first-of-type first element of a type The :first-of-type pseudo-class targets the first element of its type within its parent: p, div { border: 1px dotted gray; } div:first-of-type { background: lightyellow; border-color: gold; color: darkorange; border-style: solid; } First paragraph Second paragraph First div Second div :last-of-type last element of a type The :last-of-type pseudo-class targets the last element of its type within a parent: p, div { border: 1px dotted gray; } p:last-of-type { background: lightyellow; border-color: gold; color: darkorange; border-style: solid; } First paragraph Second paragraph First div Second div :nth-of-type nth element of a type The :nth-of-type pseudo-class matches elements based on their type and position among siblings: p, div { border: 1px dotted gray; } p:nth-of-type(3) { background: lightyellow; border-color: gold; color: darkorange; border-style: solid; } First paragraph Second paragraph Third paragraph First div Second div Third div :nth-last-of-type nth element of type, counting backwards The :nth-last-of-type pseudo-class matches elements based on their type and position among siblings, but counting from the end: p, div { border: 1px dotted gray; } div:nth-last-of-type(3) { background: lightyellow; border-color: gold; color: darkorange; border-style: solid; } First paragraph Second paragraph Third paragraph First div Second div Third div :only-of-type only element of its type The :only-of-type pseudo-class targets an element that is the only element of its type among its siblings: p, div { border: 1px dotted gray; } :only-of-type { background: lightyellow; border-color: gold; color: darkorange; border-style: solid; } First paragraph Second paragraph First div Paragraph :target target element selector The :target pseudo-class selects an element with an ID attribute matching the URL fragment (eg: https://example.com/#fragment). :target is often used to style sections of a page that are directly linked to, typically used with in-page links: div:target { border: 3px solid deepskyblue; background-color: lightcyan; transform: scale(1.05); } Go to Section 1 Go to Section 2 Section 1 content Section 2 content :not() negation pseudo-class The :not() functional pseudo-class allows you to target elements that do not match a specified selector or condition. It's essentially an exclusion filter: div:not(.exclude) { border: 2px solid royalblue; background: aliceblue; } This div is targeted This div has the '.exclude' class Another targeted div :has() parent selector The :has() functional pseudo-class allows to style an element if it contains a certain element or another selector: div:has(p.special) { border: 2px solid darkkhaki; background-color: peachpuff; } Regular paragraph. This paragraph has a the '.special' class, so its parent div is styled! Another regular paragraph. ::before first child pseudo-element The ::before pseudo-element is used to insert content before the content of an element. It can be used to add decorative content, icons, or other elements that don't need to be in the actual DOM: .alert::before { content: '⚠ '; margin-right: 0.25rem; } This is an alert message ::after last child pseudo-element The ::after pseudo-element is similar to ::before and is used to insert content after the content of an element: .info::after { content: ''; display: inline-block; width: 0.75rem; height: 0.75rem; border-radius: 35%; background: darkseagreen; margin-left: 0.35rem; rotate: 45deg; vertical-align: middle; } My message ::first-letter first letter pseudo-element The ::first-letter pseudo-element is used to style the first letter of a block-level element, allowing for design elements like drop caps: p::first-letter { font-size: 2em; font-weight: bold; float: left; color: crimson; } Once upon a time, in a land far, far away, there lived a curious coder on a quest for knowledge. ::first-line first line pseudo-element The ::first-line pseudo-element is used to style the first line of a block-level element. This allows for typographic effects that can adapt dynamically based on the size of the containing element and the font size: p::first-line { font-weight: bold; color: darkslategray; text-transform: uppercase; } It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness... ::placeholder text input placeholder The ::placeholder pseudo-element is used to style the placeholder text of form fields likeand : input::placeholder { font-style: italic; color: thistle; opacity: 0.7; } ::selection style highlighted box The ::placeholder pseudo-element is used to style the portion of an element that has been highlighted or selected by the user. For instance, when a user clicks and drags to select text, the ::selection pseudo-element can be used to modify the background color, text color, and other styles of that selection: ::selection { background-color: gold; color: darkblue; } Click and drag to select this text to see the custom selection styling in action. ::marker list marker pseudo-element The ::marker pseudo-element is used to style marker boxes in list items, which typically contain bullets (for unordered lists) or numbers/letters (for ordered lists). Before the introduction of the ::marker pseudo-element, customizing these markers often required workarounds, but this pseudo-element gives us a bit more control: li::marker { color: LightSeaGreen; font-size: 1.5em; font-weight: bold; } Apples 🍎 Bananas 🍌 Cherries 🍒 More pseudo-classes Here's some additional pseudo-classes that are available in CSS: :root: Targets the highest-level parent element in a document, typically theelement in HTML documents. Useful to define CSS variables that will be available to all elements within the page. :is(): Matches elements that can be one of several selectors, making long selector lists shorter and easier to read. For example, :is(h1, h2, h3) would match any of those three heading elements. :where(): Similar to :is(), but allows for selecting elements based on conditions without affecting the specificity of the selector. :default: Matches UI elements (like radio buttons or checkboxes) that are set to their default selection state. :empty: Selects elements that have no children (including text nodes). :fullscreen: Targets elements that are currently displayed in fullscreen mode. :in-range: Matches form elements with a value that is within the specified range (using attributes like min and max). :out-of-range: Matches form elements with a value that is outside the specified range. :indeterminate: Targets form elements whose state is uncertain, such as a checkbox that's neither checked nor unchecked (often seen in tree-view structures). :read-only: Matches form elements that are not editable by the user, due to the readonly attribute. :read-write: Targets form elements that are editable by the user, implying they are not readonly. :lang(): Matches elements based on their language attribute. E.g., :lang(en) selects elements defined in English. I'm creating a collection of awesome-looking social media templates. Subscribe to be among the first to know when it goes live ↓ Powered by Buttondown (affiliate link) SVG GENERATORS, TOOLS & DESIGN RESOURCES ↓ cccolor HEX, RGB & HSL color picker pppalette color palette generator hhhue curated color palettes ssshape SVG blob generator llline SVG line generator dddepth AI-generated 3D shapes sssvg SVG reference ffflux fluid SVG gradients nnnoise noise texture generator aaabstract abstract backgrounds rrrepeat repeating SVG shapes wwwatercolor watercolor backgrounds hhholographic holographic backgrounds mmmotif SVG patterns with a 3D feel gggrain grainy gradient generator uuunion mesh-like gradient generator nnneon glowing SVG shapes ccclaymoji emoji maker iiisometric isometric design builder sssurf SVG wave generator qqquad generative patterns dddraw freehand SVG drawing tool pppointed SVG arrow maker cccloud cloud SVGs ssstar star SVGs lllove heart SVGs bbblurry blurry background shapes uuundulate make some SVG ripples ccchaos wild wavy SVG shapes oooscillate curvy line patterns ffflurry make it rain, SVG-style cccoil SVG spiral waves ssspiral SVG spiral patterns cccircular SVG gradient circle patterns rrreplicate SVG line pattern generator tttwinkle bursting line patterns ssscales gradient SVG patterns rrrainbow colorful background patterns ttten 10 Print-inspired pattern generator ssscribble make SVG scribbles pppixelate pixel art patterns sssquiggly wavy line patterns ooorganize SVG grid patterns vvvortex spinning circle patterns gggyrate spinning geometric patterns llleaves organic SVG backgrounds ggglitch glitchy shapes ppperspective shapes that vanish sssurface SVG circles that feel 3D hhhorizon Cyberpunk-style backgrounds sssplatter make funky SVG shapes ssspill generate melting SVGs ssspot organic speck patterns wwwhirl twisting line patterns rrreflection SVG circle patterns bbburst confetti generator dddynamite decorative SVG shapes dddivided split SVG shapes lllook fun line icons dddoodle hand-drawn doodles pppsychedelic psychedelic backgrounds rrrasterize convert SVG to PNG eeencode encode SVG to Base64 rrready? support chart for web features 👋 Hey, I'm Seb and I'm creating fffuel If the tools and resources on here have sparked some creative joy for you, how about dropping a few dollars my way? Your support keeps this space vibrant with fresh new things. Thanks so much for helping out! 💡✨ 👏 Make a small donation 🔥 Landing Page Hot Tips A Rich-Media Ebook by Rob Hope with 100 snackable landing page lessons. This affiliate link applies a coupon for $10 off: 🙌 Check out the Ebook ✉ Buttondown Email Buttondown is the service I use to email people who subscribe on this site. They do email marketing right by being privacy-first and by being aimed at individual creators. This affiliate link helps support fffuel and gives you $9 off on your first month: 💌 Check out Buttondown Fathom Analytics Fathom is what I use for analytics on this site. It's a simple privacy-first web analytics service. This affiliate link gives you $10 off on your first invoice: ✅ Check out Fathom Analytics ©2023 Sébastien Noël home ⟡ about ⟡ contact ⟡ 👏 support fffuel ⟡ privacy ⟡ terms ⟡ license 💖 share it ↓",
    "commentLink": "https://news.ycombinator.com/item?id=37132754",
    "commentBody": "CSS Selectors: A Visual GuideHacker NewspastloginCSS Selectors: A Visual Guide (fffuel.co) 288 points by CosmosSeb 22 hours ago| hidepastfavorite55 comments myfonj 3 hours agoAnd maybe some folks could be interested about little known fact that each selector has even a namespace part on the left; the void implicit one having effect depending on presence or absence of `@namespace` prologue in given stylesheet: - In stylesheet without explicit unnamed namespace it is like &#x27;*|&#x27;, so everything is like `*|`; - In stylesheet with explicit unnamed namespace (`@namespace \"\");`) all element selectors without `|` match only elements from that namespace. - In stylesheet with explicit named namespace (`@namespace\"\";`) all element selectors with `|` match only elements from that namespace. @namespace \"bogus\"; &#x2F;* unnamed *&#x2F; @namespace aitchtee \"http:&#x2F;&#x2F;www.w3.org&#x2F;1999&#x2F;xhtml\"; @namespace esvee \"http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg\"; * { &#x2F;* will not match anything, because \"bogus\" NS *&#x2F; } html { &#x2F;* also nothing, because \"bogus\" NS *&#x2F; } *|* { &#x2F;* will match everything *&#x2F; } aitchtee|title { &#x2F;* will not match SVG title element, only the HTML one *&#x2F; } aitchtee|*:root { &#x2F;* will match html *&#x2F; } esvee|* { &#x2F;* all SVG elements *&#x2F; } reply anilakar 21 hours agoprevYou can make nth-child repeat by using an arithmetic expression inside the parentheses, like nth-child(3n). They also stack, so you can create pseudorandom transformations to a sequence of elements by adding styles for the first few prime numbers. reply rafram 20 hours agoparentOr nth-child(even), or nth-child(odd). (Keeping in mind that children are 1-indexed in CSS.) reply ataylor32 20 hours agoprevHere are a couple quotes from https:&#x2F;&#x2F;developer.chrome.com&#x2F;articles&#x2F;css-nth-child-of-s&#x2F;#pr... :New in CSS Selectors Level 4 is the ability to optionally pass a selector list into :nth-child() and :nth-last-child().For example, :nth-child(2 of .highlight) selects the second matching element that has the .highlight class. Put differently: out of all children with the class .highlight, select the second one. reply keepamovin 20 hours agoprev10 years ago I created an algorithm to generalize CSS selectors based on applying the \"sequence alignment\" algorithm (from bioinformatics &#x2F; string matching) to objects (bags of attribute &#x2F; key values, ie, HTML elements). This let you select two or more examples and produce a selector that captures what you meant.More here: https:&#x2F;&#x2F;github.com&#x2F;00000o1&#x2F;selector-generalizationThere&#x27;ve been multiple attempts at such a thing over the years, but I like this approach for its essentially simple foundation in solid algorithms.Be warned tho this code is old! I haven&#x27;t yet got around to updating it to make it more \"modern\" etc!! reply lazylion2 3 hours agoparentUpdated link for the live example:https:&#x2F;&#x2F;dosyago.github.io&#x2F;selector-generalization&#x2F; reply keepamovin 3 hours agorootparentTHank you lazylion2! That&#x27;s very kind of you! reply hk1337 21 hours agoprevVery cool. It seems to be missing the begins with and ends with selectors for the attr-val.[title^=“old”]and[title$=“old”] reply reaperducer 21 hours agoparent[title^=“old”]and[title$=“old”]Once you get used to using these, and their relatives, you find all kinds of uses for them, like applying styles only to telephone links: a[href^=\"tel:\"] reply hk1337 20 hours agorootparentDefinitely, also, they&#x27;re called CSS selectors but I use them a lot in javascript for query selection too. reply 6510 20 hours agorootparentprevYou can do [data-foo*=\"banana\"] and trigger it by changing the dataset to something that contains banana.https:&#x2F;&#x2F;jsfiddle.net&#x2F;gaby_de_wilde&#x2F;s729uk61&#x2F; reply ttfkam 14 hours agoparentprevI&#x27;m constantly amused by how often regular expression syntax can be found in all sorts of unexpected places. It really is foundational knowledge these days. ^ means match input start $ means match input endMakes remembering the CSS selector syntax easier once you know that. (Or makes the regex syntax easier to remember?) reply Technotroll 21 hours agoparentprevRegular selectors... (pun intended) reply HughG 20 hours agoprevVery nice, but some of the examples could do with more elements to indicate what is _not_ selected. For example, the \"child selector\" image could have some more \"div\"s which are descendants but not direct children of the \".wrapper\" element. reply mminer237 20 hours agoprev`* + *` works, but I think `:not(:first-child)` is a much clearer way of doing this. (Unless you need to support IE8 for some horrific reason?) reply lcnPylGDnU4H9OF 20 hours agoparentI assume the author is alluding to a certain blog post. https:&#x2F;&#x2F;alistapart.com&#x2F;article&#x2F;axiomatic-css-and-lobotomized...To the point, the :not(:first-child) option should similarly be able to minimize the total number of rules. reply myfonj 17 hours agorootparent`:not(:first-child)` has specificity 0.1.0 that is some orders higher than 0.0.0 of `* + *` (since `:first-child` is a pseudoclass), so you&#x27;d have to adjust anything that you&#x27;d need to override it with.Strict equivalent of * + *would be :where(:not(:first-child)) reply cantSpellSober 19 hours agoprevThe ~ selector should be labeled \"general sibling combinator\" instead of \"subsequent selector\" (thanks for sharing!) reply aiunboxed 21 hours agoprevAmazing, nice curation. Had a look at the other blog posts, great work. reply zvr 14 hours agoprevNice!Minor typo nit: the ::placeholder bold selector name has been copy-pasted in the sections for ::selection and ::marker. reply CosmosSeb 13 hours agoparentThanks for pointing that out, fixed it. reply newfie_bullet 21 hours agoprevNice resource - very handy. I&#x27;ve shared with some slack groups I frequent and mentor in! reply bloopernova 21 hours agoprevThis is really cool and useful! I don&#x27;t know a huge amount of CSS beyond the basics of tweaking sites via ublock origin or devtools, but this explained things clearly. Kudos! reply Timwi 21 hours agoprevWhat makes scrolling on this page so sluggish? (FF mobile) reply oneeyedpigeon 20 hours agoparentIt might be using a &#x27;fuck-with-the-scrolling&#x27; JS library, like a few poor sites do here and there. Does it get better if you disable JS? reply CosmosSeb 12 hours agoparentprevReally not sure what does that. Not using any JS scrolling library. Perhaps a performance hit from using backdrop-filter on the table of contents section for a background blur effect. reply thih9 21 hours agoparentprevAre you on iOS or on Android? Are you using any ad filtering extension?I checked on both FF and FF Focus on iOS and got regular scrolling behavior. reply mcdonje 21 hours agorootparentI&#x27;m using FF on android and had no issues scrolling. reply aiunboxed 21 hours agoparentprev+1 reply Vincenius 21 hours agoprevA great overview! Thanks for creating this :) reply tniemi 20 hours agoprevNow, create a similar page for XPath... reply gozzoo 20 hours agoparentdoes anyone still use XPath nowdays? reply account-5 20 hours agorootparentDoes anyone still use XML? I would imagine the answer to both is yes. I often wish JSON had something as good as xpath. reply djbusby 19 hours agorootparentThe `jq` command line tool has some neat expressions for selecting things from JSON. Not sure if it&#x27;s a standard though. reply rhdunn 19 hours agorootparentprevXPath and XSLT are still active, especially in the publishing space where documents are not stored in HTML, but in other formats like JATS or DocBook and need converting to HTML when displaying to the user, or converting to other XML formats when interfacing with other formats&#x2F;vendors like crossref. It&#x27;s also still used for things like processing the US bill XML data (e.g. https:&#x2F;&#x2F;www.govinfo.gov&#x2F;bulkdata&#x2F;BILLS&#x2F;resources). reply djbusby 21 hours agoprevSome typos regarding :placeholder pseudo selector - in the sections after (:marker)Scrolling seems fine. Firefox on Pixel3 reply seanplusplus 18 hours agoprevThis is absolutely wonderful! reply rado 22 hours agoprevWonderful. Note that :target works only after the whole page is loaded, including any JS. :has() is revolutionary, but unsupported by FF. reply lovegrenoble 20 hours agoprevWill be so useful for me reply russellbeattie 15 hours agoprevI either forgot or never knew there&#x27;s a difference between \".class1.class2\" and \".class1 .class2\" (without and with a space). I didn&#x27;t even think the first is valid. That bit alone was worth the entire article for me. reply Kiro 21 hours agoprev [–] Cool but also reminds me why I use inline styles for everything. I don&#x27;t want to fiddle with selectors. I want to put the styling right on the element. reply hinkley 21 hours agoparentIn a large app you often can’t easily find all of the html. It’s less work by far to ensure you can find all of the CSS though.And when the company decides to rebrand and change the color scheme for the whole app, which they will at least half the time, you’ll be glad not to be playing whack-a-mole. Problems that aren’t done when you think they’re done create a lot of friction with the rest of the org. Open ended problems cause engineering to look incompetent. The cost of that problem is immeasurable, and doubly so in a bad economy. reply oneeyedpigeon 20 hours agorootparent> In a large app you often can’t easily find all of the html.Which is often a warning sign of a bad design. The times I&#x27;ve dealt with systems that bury a certain piece of text in either a content file or a template or the database...> It’s less work by far to ensure you can find all of the CSS though.Hmm... depends. I&#x27;ve seen things like React embedding the CSS directly in with the markup and component logic etc. So this hasn&#x27;t necessarily gotten any better — in fact, a site consisting JUST of html files would probably be easier to find CSS in even if it&#x27;s all added via inline style attributes. reply hinkley 20 hours agorootparentThere are no clean designs on a large team. Anyone who tells you otherwise is selling something.Zealots provide solutions that cannot possibly survive contact with actual humans. The rest of the team talks about them behind their backs.> I&#x27;ve seen things like React embedding the CSS directly in with the markup and component logic etc.You were talking about bad designs? Embedded styling is not Cascading Style Sheets. It’s embedded styling. There’s no sheet, and no cascade.Nothing is free, and few things are easy. There are lines that are easier to maintain in practice than others, and separation of CSS is one of the former. reply oneeyedpigeon 20 hours agorootparent> Embedded styling is not Cascading Style SheetsI think that&#x27;s needless pedantry; the contents of the `style` attribute still needs to be valid CSS. reply hinkley 19 hours agorootparentIt’s not needless pedantry. Calling it CSS is using a crescent wrench as a hammer and calling it a hammer (a running joke on r&#x2F;tools). You’ve missed the entire point and someone needs to remind you - and everyone listening in - what the point was.We had in-line style before we had CSS. We are going back and nobody remembers why we’ve had CSS for twenty years? Read some history. reply oneeyedpigeon 19 hours agorootparentWe&#x27;re not in 2003, though. If you&#x27;re working on a project with somebody and you insist they refrain from calling the contents of a style attribute CSS because it wasn&#x27;t always technically CSS, nobody will thank you.Edit: actually, even when I was working with CSS in 2003 (IIRC, we absolutely were using separate CSS files then), this attitude would have been unhelpful, to say the least. replyailef 21 hours agoparentprevHow do you create reusable styles this way? reply Kiro 20 hours agorootparentWith components. Same thing as when isolating styles to a component in various frameworks (e.g. Single-File Components in Vue) but instead of its own style tags I just put the styling directly in the HTML. reply bryanrasmussen 21 hours agorootparentprevmodern JS has a bunch of half-assed scenarios for solving this problem that allows you to solve almost everything CSS did a few years ago and helps keep people stuck in ineffective legacy ways of doing it because all their code would have to be rewritten to use straight css.I know because every place I&#x27;ve worked in the last few years uses some form of CSS in JS. reply hinkley 20 hours agorootparentEvery five years or so prior to CSS 3, there was a push to purge styling and animation decisions from JS. We are meant to be past that now.Calling it a feature that you have CSS in your JS is tantamount to saying you’re writing legacy code. Which happens, but don’t be cheerful about it. reply hoherd 20 hours agoparentprevFWIW, these are useful for more than styling. I don&#x27;t do any front-end dev work, but I use these regularly in selenium to find components in the page to interact with. reply froddd 21 hours agoparentprevSo you’re using S, not CSS reply oneeyedpigeon 20 hours agoparentprev [–] Presumably you never use hover styles, media queries, or quite a lot of other CSS features? reply Kiro 20 hours agorootparent [–] I use Tailwind when I need that. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This visual guide provides an explanation of various CSS selectors, including pseudo-classes and pseudo-elements, with examples of their usage.",
      "The guide covers selectors for elements, classes, IDs, attributes, and different states of input elements.",
      "It also mentions additional CSS pseudo-classes and pseudo-elements for targeting and styling specific elements within a webpage and provides resources such as SVG generators, tools, and design resources. Please note that it includes affiliate links for related services."
    ],
    "commentSummary": [
      "The article delves into the functionalities of CSS selectors, explaining their various uses and providing examples.",
      "Advanced features such as nth-child and selectors with specific values at the start or end are discussed.",
      "The article also explores a tool for generalizing CSS selectors and highlights the usefulness of regular expression syntax in CSS."
    ],
    "points": 288,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1692100064
  },
  {
    "id": 37138807,
    "title": "How should I read type system notation?",
    "originLink": "https://langdev.stackexchange.com/questions/2692/how-should-i-read-type-system-notation",
    "originBody": "Log in Sign up Programming Language Design and Implementation Stack Exchange is a question and answer site for designers and implementers of computer programming languages. It only takes a minute to sign up. Sign up to join this community Anybody can ask a question Anybody can answer The best answers are voted up and rise to the top Programming Language Design and Implementation Beta Home PUBLIC Questions Tags Users Unanswered TEAMS Stack Overflow for Teams – Start collaborating and sharing organizational knowledge. Create a free Team Why Teams? How should I read type system notation? Ask Question Asked 3 days ago Modified yesterday Viewed 30k times 108 I sometimes read articles or papers about type systems that use some funny-looking, two-dimensional notation with lots of unfamiliar symbols and Greek letters. The notation seems mathematical, but it’s entirely unlike any mathematics I learned in school. It’s quite intimidating, and I don’t even know where to start. How should I read this notation? What does it mean? typestype-systemsnotation Share Follow edited yesterday asked Aug 13 at 2:18 Alexis King♦ 5,3015 5 gold badges 19 19 silver badges 52 52 bronze badges 1 Related: stackoverflow.com/questions/12532552/… – leftaroundabout 2 days ago Nice question. I learned a lot of this stuff in my university language design course, but that was nearly 30 years ago now so a refresher is definitely helpful :) – occipita 16 hours ago Add a comment 1 Answer Sorted by: Highest score (default) Date modified (newest first) Date created (oldest first) 124 The notation used to describe type systems varies from presentation to presentation, so giving a comprehensive overview is impossible. However, most presentations share a large, common subset, so this answer will attempt to provide a foundation of enough of the basics to understand variations on the common theme. Syntax and grammars Type systems as applied to programming language are syntactic systems. That is, a type system is a set of rules that operate on the (abstract) syntax of a programming language. For this reason, comprehensive treatments of type systems begin by providing the grammar of all the syntactic constructs considered by the type system using BNF notation. In the simplest typed languages, syntax is needed for precisely two things: expressions and types. For example, let’s consider the grammar for an extremely simple language of booleans and integers: e τ ::=|| ::=true|false 0|1|−1|2|−2|… if e then e else e e+e|e−e|e×e e=e|ee Bool Int boolean literal integer literal conditional arithmetic comparison booleans integers 𝑒 ::= 𝑡 𝑟 𝑢 𝑒𝑓 𝑎 𝑙 𝑠 𝑒 boolean literal01− 12− 2… integer literal𝑖 𝑓 𝑒 𝑡 ℎ 𝑒 𝑛 𝑒 𝑒 𝑙 𝑠 𝑒 𝑒 conditional𝑒 + 𝑒𝑒 − 𝑒𝑒 × 𝑒 arithmetic𝑒 = 𝑒𝑒𝑒 comparison 𝜏 ::= 𝐵 𝑜 𝑜 𝑙 booleans𝐼 𝑛 𝑡 integers Here, e 𝑒 corresponds to an expression and τ 𝜏 corresponds to a type, which is a standard notational convention. Some presentations use other symbols for types, such as t 𝑡 , T 𝑇 , σ 𝜎 , or other lowercase Greek letters, but the overall structure will look roughly the same. More complex languages will naturally have more complex grammars: imperative languages will include the grammar of statements, languages with pattern-matching will include the grammar of patterns, and so on. This language is so simple that it doesn’t even have variables! However, this core syntactic division between terms (things that have types) and types is essential, as defining the relationship between them is what type systems are all about. Relations, judgments, axioms, and inference rules Once the grammar has been specified, the next step is to define the typing relation, which is generally written e:τ 𝑒 : 𝜏 and can be read “ e 𝑒 has type τ 𝜏 ”. Intuitively, we understand that some statements of this form “make sense” and others do not: 1+2:Int 1 + 2 : 𝐼 𝑛 𝑡 means “ 1+2 1 + 2 has type Int 𝐼 𝑛 𝑡 ”, which certainly makes sense. 1+2:Bool 1 + 2 : 𝐵 𝑜 𝑜 𝑙 means “ 1+2 1 + 2 has type Bool 𝐵 𝑜 𝑜 𝑙 ”, which does not make sense. true+2:Int 𝑡 𝑟 𝑢 𝑒 + 2 : 𝐼 𝑛 𝑡 means “ true+2 𝑡 𝑟 𝑢 𝑒 + 2 has type Int 𝐼 𝑛 𝑡 ”, which makes even less sense, as the expression true+2 𝑡 𝑟 𝑢 𝑒 + 2 is nonsense and does not have any type at all. We’d like to write down some rules that precisely capture our intuitions about which statements “make sense” and which do not. To do this, we’ll define the typing judgment, which is written using the following notation: ⊢e:τ ⊢ 𝑒 : 𝜏 Here, the ⊢ ⊢ can be read to mean “the following statement is true”. (The ⊢ ⊢ might seem a bit unnecessary, and indeed it is sometimes omitted in simple systems like this one, but it will play a more significant role later.) Using this notation, we can write down the some of the typing rules for our type system: ⊢true:Bool ⊢false:Bool ⊢ 𝑡 𝑟 𝑢 𝑒 : 𝐵 𝑜 𝑜 𝑙 ⊢ 𝑓 𝑎 𝑙 𝑠 𝑒 : 𝐵 𝑜 𝑜 𝑙 The horizontal bar over each of these rules with nothing on top means that they are always true, which makes them axioms. We also have an infinite number of similar axioms for integer literals: ⊢0:Int ⊢1:Int ⊢−1:Int ⊢2:Int ⋯ ⊢ 0 : 𝐼 𝑛 𝑡 ⊢ 1 : 𝐼 𝑛 𝑡 ⊢ − 1 : 𝐼 𝑛 𝑡 ⊢ 2 : 𝐼 𝑛 𝑡 ⋯ Of course, the typing rules for literals are fairly boring. Things get much more interesting when we consider rules for expressions that have subexpressions! Here are the typing rules for + + and> are nearly identical to the ones given above, but the rule for if…then…else 𝑖 𝑓 … 𝑡 ℎ 𝑒 𝑛 … 𝑒 𝑙 𝑠 𝑒 must be slightly more complex. This is because the branches of these expressions can be any type, as long as they agree. That is, both if true then 1 else 2 𝑖 𝑓 𝑡 𝑟 𝑢 𝑒 𝑡 ℎ 𝑒 𝑛 1 𝑒 𝑙 𝑠 𝑒 2 and if true then false else true 𝑖 𝑓 𝑡 𝑟 𝑢 𝑒 𝑡 ℎ 𝑒 𝑛 𝑓 𝑎 𝑙 𝑠 𝑒 𝑒 𝑙 𝑠 𝑒 𝑡 𝑟 𝑢 𝑒 are legal, but if true then 1 else true 𝑖 𝑓 𝑡 𝑟 𝑢 𝑒 𝑡 ℎ 𝑒 𝑛 1 𝑒 𝑙 𝑠 𝑒 𝑡 𝑟 𝑢 𝑒 is not. To capture this, the typing rule uses a variable to stand for the type of the branches: ⊢ e 1 :Bool ⊢ e 2 :τ ⊢ e 3 :τ ⊢if e 1 then e 2 else e 3 :τ ⊢ 𝑒 1 : 𝐵 𝑜 𝑜 𝑙 ⊢ 𝑒 2 : 𝜏 ⊢ 𝑒 3 : 𝜏 ⊢ 𝑖 𝑓 𝑒 1 𝑡 ℎ 𝑒 𝑛 𝑒 2 𝑒 𝑙 𝑠 𝑒 𝑒 3 : 𝜏 When the rule is applied, any type may be picked for τ 𝜏 as long as the choice is used consistently. This notation originates in formal logic, and in particular, the style used to specify type systems most closely resembles natural deduction. Though I will not go into the formal details of the notation in this answer, rules expressed in this way can be used to construct formal proofs about the system’s properties, which is important for proving things like type soundness. Judgments as an algorithmic specification So far, I’ve intentionally refrained from saying anything about the computational interpretation of typing judgments. In general, judgments are just logical rules, and some type systems specified this way do not directly correspond to a decidable typechecking algorithm. However, if you are not used to thinking about proof systems, this perspective can be extremely difficult to wrap your head around. Fortunately, in many cases, there is a way to read typing rules that directly yields a typechecking algorithm: it’s possible to interpret ⊢e:τ ⊢ 𝑒 : 𝜏 as a function from an expression e 𝑒 to its type τ 𝜏 . This is because there is usually exactly one rule defined for each case in the grammar of expressions, which makes it possible to think of each typing rule as a distinct case in a recursive typechecking function. For example, consider the rules for our little language given above. They correspond directly to an infer i n f e r function with the following shape: infer:Expr→Type infer(e)=match e where truefalse 01−12… e 1 + e 2 e 1e in TypeScript, and f x 𝑓 𝑥 corresponds to f(x). With these additions to our grammar, the notation used for our typing relation does not change—it’s still just e:τ 𝑒 : 𝜏 —but the structure of our typing judgment must be extended. The trouble appears when we attempt to write a typing rule for variables: ⊢x:??? ⊢ 𝑥 : ??? The problem is that the type of a variable depends on the context it appears in. Therefore, we need to extend the typing judgment to keep track of the types of all variables that happen to be in scope, which we do using the following notation: Γ⊢e:τ Γ ⊢ 𝑒 : 𝜏 Γ Γ is called “the context” or “the type environment”, and the role of the ⊢ ⊢ now becomes clearer: it separates contextual assumptions from the statement to be proved. The extended judgment can therefore be pronounced “under the context Γ Γ , the expression e 𝑒 has type τ 𝜏 ”, and algorithmically, Γ Γ can be considered an additional “input” to the judgment of type Map. However, formally speaking, every typing rule must be defined syntactically, so contexts are explicitly represented in typing rules as syntactic constructs with the following shape: Γ ::=∅ Γ,x:τ empty context variable binding Γ ::= ∅ empty contextΓ , 𝑥 : 𝜏 variable binding Sometimes ∙ ∙ is used instead of ∅ ∅ to represent the empty context. Under this representation, a context is essentially an association list mapping variable names to types. Most typing rules have no reason to care about the context. Most inference rules just pass it along unaltered, and most axioms ignore it altogether. For example, here are a couple typing rules from above adapted to our new judgment: Γ⊢true:Bool Γ⊢ e 1 :Int Γ⊢ e 2 :Int Γ⊢ e 1 + e 2 :Int Γ ⊢ 𝑡 𝑟 𝑢 𝑒 : 𝐵 𝑜 𝑜 𝑙 Γ ⊢ 𝑒 1 : 𝐼 𝑛 𝑡 Γ ⊢ 𝑒 2 : 𝐼 𝑛 𝑡 Γ ⊢ 𝑒 1 + 𝑒 2 : 𝐼 𝑛 𝑡 However, the context is essential for two new typing rules, which handle variable uses and lambda expressions: x:τ∈Γ Γ⊢x:τ Γ,x: τ 1 ⊢e: τ 2 Γ⊢(λx: τ 1 .e): τ 1 → τ 2 𝑥 : 𝜏 ∈ Γ Γ ⊢ 𝑥 : 𝜏 Γ , 𝑥 : 𝜏 1 ⊢ 𝑒 : 𝜏 2 Γ ⊢ ( λ 𝑥 : 𝜏 1 . 𝑒 ) : 𝜏 1 → 𝜏 2 The second of these two rules is the more complex one, as it’s where all the magic happens: while typechecking the body e 𝑒 of the lambda expression, the context is extended with a new binding x: τ 1 𝑥 : 𝜏 1 . This information is then utilized by the first rule, which says that if there is a variable binding x 𝑥 with type τ 𝜏 in the current context (and therefore in scope), then x 𝑥 has type τ 𝜏 . In other words, the context is used as a communication mechanism to propagate information between these two rules. (Observant readers may note that this model does not handle variable shadowing. As a simplifying assumption, type systems specified this way usually assume that all variables have already been resolved and made unique.) If this still seems a bit confusing to you, it may help to consider the way these additions affect our infer i n f e r function from earlier: infer:(Context,Expr)→Type infer(Γ,e)=match e where … x (λx: τ 1 . e ′ ) ↦lookup(Γ,x) ↦let Γ ′ =extend(Γ,x, τ 1 ); let τ 2 =infer( Γ ′ , e ′ ); τ 1 → τ 2 i n f e r : ( 𝐶 𝑜 𝑛 𝑡 𝑒 𝑥 𝑡 , 𝐸 𝑥 𝑝 𝑟 ) → 𝑇 𝑦 𝑝 𝑒 i n f e r ( Γ , 𝑒 ) = 𝑚 𝑎 𝑡 𝑐 ℎ 𝑒 𝑤 ℎ 𝑒 𝑟 𝑒 … 𝑥↦ l o o k u p ( Γ , 𝑥 ) ( λ 𝑥 : 𝜏 1 . 𝑒 ′ )↦ 𝑙 𝑒 𝑡 Γ ′ = e x t e n d ( Γ , 𝑥 , 𝜏 1 ) ; 𝑙 𝑒 𝑡 𝜏 2 = i n f e r ( Γ ′ , 𝑒 ′ ) ; 𝜏 1 → 𝜏 2 The only remaining rule we need to add to cope with the addition of functions is a rule for function application: Γ⊢ e 1 : τ 1 → τ 2 Γ⊢ e 2 : τ 1 Γ⊢ e 1 e 2 : τ 2 Γ ⊢ 𝑒 1 : 𝜏 1 → 𝜏 2 Γ ⊢ 𝑒 2 : 𝜏 1 Γ ⊢ 𝑒 1 𝑒 2 : 𝜏 2 That’s it! Other common notation and considerations The above describes the large majority of notation used to specify type systems if measured by volume, but modifications and extensions to this foundation are extremely common. It would be impossible to cover all of them, but fortunately, good papers usually explain whatever nonstandard notation they choose to introduce. However, some conventions are common enough that they are often used without explanation; this section attempts to provide a basic survey and describe a few notational quirks. Since this is not and can never be an exhaustive list, please open a separate question if you find some notation not covered here! Inference rule layout So far, all of the examples in this answer have laid out inference rules in a very regular, vertical way. However, placing each condition on its own line is not by any means required, so several conditions may appear side-by-side: Γ⊢ e 1 :IntΓ⊢ e 2 :Int Γ⊢ e 1 + e 2 :Int Γ ⊢ 𝑒 1 : 𝐼 𝑛 𝑡 Γ ⊢ 𝑒 2 : 𝐼 𝑛 𝑡 Γ ⊢ 𝑒 1 + 𝑒 2 : 𝐼 𝑛 𝑡 Vertical and horizontal arrangement may even be combined within the same rule. Side conditions Usually, the conditions that appear above the horizontal bar in an inference rule are themselves judgments that must be satisfied by some combination of inference rules and axioms. However, this is not always the case: rules may also include arbitrary boolean expressions known as side conditions, which must all be true in order for the rule to be applied. The x:τ∈Γ 𝑥 : 𝜏 ∈ Γ condition in our typing rule for variables is an example of a side condition. A special type of side condition that sometimes appears in algorithmic type systems is written α fresh 𝛼 𝑓 𝑟 𝑒 𝑠 ℎ . This means that α 𝛼 should be a fresh type variable, i.e. a type variable distinct from all other type variables. Subtyping Subtyping introduces a weaker notion of consistency between types than strict equality, and the subtyping relation must be explicitly defined. It is usually denoted τ 1low/high-byte in 8-bit assembler? SSH host key not recognized I don’t understand Noether’s theorem… there is nothing to prove? Why is the UKSA not called the Ministry of Space? Retaining full manual for scrolling in SSH (Putty) terminal Count N-Rich Permutations of an Integer Sequence Exterior outlet from interior main breaker Solving inequation with parameter How to reject a postdoc offer from ex-co-author? Are there always 2 teams such that they have together defeated every other team Could a jet engine theoretically ingest not only oxidizer but fuel (on planets like jupiter) Homeomorphism vs Homotopy Equivalence Distinguishing between adjective and adverb for word inserted between article and noun for Lückentext Question feed PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION Tour Help Chat Contact Feedback COMPANY Stack Overflow Teams Advertising Collectives Talent About Press Legal Privacy Policy Terms of Service Cookie Settings Cookie Policy STACK EXCHANGE NETWORK Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.8.15.43579 Your privacy By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy. Accept all cookies Necessary cookies only Customize settings",
    "commentLink": "https://news.ycombinator.com/item?id=37138807",
    "commentBody": "How should I read type system notation?Hacker NewspastloginHow should I read type system notation? (langdev.stackexchange.com) 278 points by mpweiher 14 hours ago| hidepastfavorite64 comments dan-robertson 11 hours agoThere was a Guy Steele talk a while ago on this topic. He also gives searchable names for some of the things (eg the ‘2d inference rule diagrams’). He calls it ‘computer science metanotation’, though I think it’s more of a PLT thing. https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=dCuZkaaou0Q reply dang 5 hours agoparentDiscussed at the time:It&#x27;s Time for a New Old Language – Guy Steele [video] - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15473199 - Oct 2017 (45 comments) reply carapace 9 hours agoparentprevYou beat me to it.> The most popular programming language in computer science has no compiler, interpreter, or complete specification. It is remarkably concise. It grew over decades; today, dozens of variations are in use. Its complexity has reached the point where it needs to be re-explained every time it is used—but too often it is not; instead, each variant is more or less taken for granted. This has led to ambiguities and inconsistencies. Much effort has been spent in hand-translating, in both directions, between this language and other languages that do have compilers, primarily for the purpose of implementing yet other languages.=> https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Guy_Steele Guy Steele=> https:&#x2F;&#x2F;www.codemesh.io&#x2F;codemesh2017&#x2F;guy-l-steele \"A Cobbler&#x27;s Child\" talk at Code Mesh 2017=> https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qNPlDnX6Mio \"A Cobbler&#x27;s Child\" (video on youtube)=> https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dCuZkaaou0Q \"It&#x27;s Time for a New Old Language\" (video on youtube)=> https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15473199 Discussion on HN=> https:&#x2F;&#x2F;labs.oracle.com&#x2F;pls&#x2F;apex&#x2F;f?p=94065:40150:0::::P40150... SlidesTo me this seems like a pretty sorry state of affairs. The folks trying the hardest to speak precisely about the most precise human art use ambiguous and inconsistent notation. wat? reply nextaccountic 7 hours agorootparentThis is also the notation that is used in logic as well.The thing about it is that, while every paper can have a slightly different variant of this metalanguage, all texts are supposed to contain an introduction defining exactly what this means. That&#x27;s because it&#x27;s common that, if you are talking about a new logic, you will have new syntax as well, or a slightly different way to make inference reply nextaccountic 6 hours agoparentprevI can&#x27;t find the slides in PDF, was it posted somewhere? reply otikik 13 minutes agoprevAmazing answer, starting from fundamentals and evolving from there. reply mbwgh 3 hours agoprevThe notation goes back to Frege. It&#x27;s not easy to google if you don&#x27;t know what you&#x27;re looking for, but this seems like a reasonable summary: https:&#x2F;&#x2F;plato.stanford.edu&#x2F;entries&#x2F;frege-logicYou can see that the turnstile symbol |- was already used, and the horizontal line (which was called \"Fregescher Schlussstrich\", Frege&#x27;s Conclusion Stroke) in my class, was apparently somehow part of the turnstile itself and only became a separated element in modern notation it seems. reply the_af 12 hours agoprevTo anyone hesitating to read this: it&#x27;s an explanation of type system notation in Computer Science papers, basically a primer on BNF notation, inference rules, etc for type systems.It looks like a good summary. reply ljm 11 hours agoparentIt’s a testament to computer science that this has been so thoroughly abstracted over the years reply mathisfun123 7 hours agorootparentOne of those rare comments that I can&#x27;t intuit the meaning of even though I know the definitions of all the words. In so far as this is math, this is completely, perfectly, thoroughly, abstract.So in reality did you mean to say reified ie made concrete? Or did you mean to say obviated ie made irrelevant? And you think computer science did this? The thing where the point of the game is to build more and more abstract representations of things?Like what do you think the word \"abstracted\" actually means? reply JadeNB 7 hours agorootparentIt seems clear enough to me what the GP might have meant, at least clear enough that it need not be responded to with remarks like \"Like what do you think the word \"abstracted\" actually means?\" that reads to me as saying that, whatever the GP thinks, it is wrong.Type notation presumably started off as a very concrete thing—probably just about anyone can read something like BOOL = TRUEFALSE. But then—I&#x27;m speaking here from a general perspective of computer-theoretic notation, so it&#x27;s entirely possible that these are not actual notations that occur in type theory—things were added like syntactic sugar, say, for character classes, about whose abstract-ness we can argue but which definitely adds a decoding layer for people not used to it; and things like parametrized types&#x2F;higher types that very definitely do add abstraction. It&#x27;s very likely that, almost any time a definition benefits from an extra side remark, \"For example, …\", it is illustrating an additional abstraction. reply mathisfun123 5 hours agorootparent>that reads to me as saying that, whatever the GP thinks, it is wrong.let me give you an example sentence and you tell me if the word \"abstracted\" makes sense, in context:\"the accounting spreadsheet abstracts away all of the calculus because all the numbers are written down exhaustively\" reply kodra 6 hours agorootparentprevIs it tho? Lambda calculus looks as abstract as these to me. And it was from about a century ago. reply guerrilla 2 hours agorootparentLambda calculus is arguably far more abstract because often type theories imply a lot of implrmentation constraints, which makes sense since that&#x27;s kind of the piount. reply lenkite 1 hour agorootparentprevReads like Gobbledygook to be honest. Why couldn&#x27;t they express stuff as functions and s-expressions instead of this un-readable mess? reply dclowd9901 11 hours agoparentprevHonestly, I just need a cheat sheet for how to read the symbols using english words. I understand the concepts of the logical application of typing, but I don&#x27;t read computer science dissertations regularly enough to make them the mapping of symbols to meaning stick. reply yawpitch 3 hours agorootparent> Honestly, I just need a cheat sheet for how to read the symbols using english words.Amen… however I believe the existence of such a cheat sheet would represent an existential economic risk for PLT researchers, and they’d be incentivized to add more Greek (and, eventually, Cyrillic, then Czech, then eventually Xhosa and probably Klingon) symbols in order to retain access to funding. It’s only a matter of time before someone reinvents the paper town inside of PLT papers, if they haven’t already.** the above is meant to be taken facetiously> I understand the concepts of the logical application of typing, but I don&#x27;t read computer science dissertations regularly enough to make them the mapping of symbols to meaning stick.I just wish CS (and, indeed, mathematics) was even remotely internally consistent… cross the boundaries between any two extremely narrow domains and ω can mean anything. It’s (very) vaguely consistent within PLT, but even there getting lowercase omega “stuck” in your head as having one English translation is just a pathway to pain. reply armchairhacker 12 hours agoprevThis is a nice explanation, but why is it written on stack exchange and not (also?) https:&#x2F;&#x2F;lexi-lambda.github.io&#x2F;? reply the_af 12 hours agoparentThe person who wrote this [1] seems to be a Haskell (GHC) contributor and someone hailing from lexi-lambda.----[1] https:&#x2F;&#x2F;langdev.stackexchange.com&#x2F;users&#x2F;861&#x2F;alexis-king reply bloppe 11 hours agorootparentI, too, hail from myself. reply the_af 10 hours agorootparentOops. For some reason I confused lexi-lambda (which I didn&#x27;t know) with lambda-the-ultimate (http:&#x2F;&#x2F;lambda-the-ultimate.org). reply IshKebab 12 hours agoparentprevI guess because StackExchange has higher Google ranking and allows comments, easy edits, supports latex etc. It&#x27;s a great platform for stuff like this. reply DylanSp 11 hours agorootparentAlso, the langdev SE is still in beta, and pretty early beta at that; posting useful material there helps get it off the ground. That said, I wouldn&#x27;t be surprised if Alexis also posts it to her blog at some point. reply eirikbakke 10 hours agoprev\"Types and Programming Languages\" by Benjamin C. Pierce is a good textbook that covers this kind of stuff. reply thechao 8 hours agoparentTAPL is remarkably unclear on the basic meaning of the syntax it uses; which I find ironic. This answer is orders-of-magnitude clearer than TAPL. reply OJFord 1 hour agorootparentAgreed, I stumbled through a course on essentially TAPL at university, and bought & read Pierce&#x27;s TAPL to help. It&#x27;s a great book, but the submission would&#x27;ve helped me a lot as an additional primer. I didn&#x27;t have the background in mathematical logic for it frankly.(It was a third of fourth year CS course, but where everyone else would&#x27;ve had more formal logic in first and second, I&#x27;d had the electronics part of my programme, which was actually based in EE. I naïvely thought I&#x27;d read enough to have the requisite background and I absolutely hadn&#x27;t, or at least not to understand & follow fast enough.) reply imrehg 8 hours agoprevFrom the examples:> 𝗍𝗋𝗎𝖾+2:𝖨𝗇𝗍 means “𝗍𝗋𝗎𝖾+2 has type 𝖨𝗇𝗍”, which makes even less sense, as the expression 𝗍𝗋𝗎𝖾+2 is nonsense and does not have any type at all.Well, Python called where `True + 2` is actually an int and is equal to 3. :) This doesn&#x27;t say anything about whether this should be the case or not, just whether it is. reply Tainnor 3 hours agoparentIf you think that True + 2 makes sense, you can just define your own judgments that allow it (*). Logic, and the theory behind type systems, don&#x27;t care about your axioms and inference rules, they just let you reason about them, and their interactions.(*) Something like |- True : Bool |- True : Intor, if you only want to allow it in certain expressions |- x : Int ----------- |- True + x: Intetc. reply dreamcompiler 6 hours agoparentprevEven though &#x27;True + 2&#x27; doesn&#x27;t throw an error in Python or C, it&#x27;s still stupid because it makes the semantics of the language hard to reason about, in order to give programmers a little syntactic sugar. reply anttihaapala 5 hours agorootparentThis is for backward-compatibility reasons. The comparison operators in Python produced 1 or 0 before boolean was a thing, and programs existed that exploited this, i.e. you can do something like `(ab)` in a sorting algorithm. Same thing in C, or even more so, as the comparison operators produce an `int` instead of a `bool`. reply edgyquant 6 hours agoparentprevIsn’t this language specific? For instance in C doesn’t true map to 1, thus true+1=2 reply Tainnor 2 hours agorootparentThe runtime values don&#x27;t matter to type theory, only the types of expressions. reply ajdude 7 hours agoprevWhen I was reading the Ada Reference Manual[1], I immediately recognized this kind of syntax. I didn&#x27;t actually know what it was called, but it&#x27;s interesting to see it in practical use. The whole language is defined in that kind of notation.[1] For example: https:&#x2F;&#x2F;ada-lang.io&#x2F;docs&#x2F;arm&#x2F;AA-3&#x2F;AA-3.7#syntax reply Jtsummers 6 hours agoparenthttp:&#x2F;&#x2F;www.ada-auth.org&#x2F;standards&#x2F;22rm&#x2F;html&#x2F;RM-1-1-4.htmlThe Ada Reference Manual specifies the notation they use. They&#x27;re using a variant of Backus-Naur Form and they describe their particular variant in the section I linked above. reply idreyn 12 hours agoprevLovely. I&#x27;ve wondered about this for years but never knew quite how to frame a query to learn more. reply agalunar 12 hours agoprevThis seems like maybe a right place to evangelize [1] the only hill I&#x27;ve decided to die on: the formatting of type annotations that use a colon in their syntax. Specifically, there should be an equal amount of space on both sides of the colon. I keep meaning to write a proper essay, but this is the essential argument.I think of it this way: there are two symbols that by coincidence are written the same (two aligned dots). I mean that genuinely (as in, I actually think of them as separate things), but it also helps as an explanative device.The first I&#x27;ll call the prefatory or label(ing) colon. Its usage matches usage in English [2], where the text preceding the colon introduces the text after the colon, or the thing on the left is a label or name for the thing on the right. For example, it is used to start a block (as in Python), or to define a key-value pair, or define a name-value pair in a struct (as in C or Rust).The second is the type annotation. This syntax is borrowed from mathematics. It is a binary relation, and binary relations are written with equal space on the left and right. Just as you&#x27;d never write \"x= 1\" (another relation), \"x> y\" (relation), or \"x+ z\" (operator), you&#x27;d never write \"x: X\". Instead you write \"x = 1\", \"x > y\", and \"x + z\" or \"x+z\" instead.Whenever I see \"a: b\", I immediately think labeling colon. Despite having seen it thousands of times, I always have to perform an additional mental step (however trivial) when it turns out to be a type annotation; it takes a small amount of mental energy to dismiss each instance of syntax that looks wrong.[Truthfully, I&#x27;m somewhat baffled how it would ever even cross anyone&#x27;s mind when designing a language to write type annotations like \"x: X\" given the long-established, pre-existing precedent in mathematics and the way its semantics seem backwards (if you had to use a labeling colon, \"X: x\" would make much more sense to me).]edit: I should be explicit that I&#x27;m referring to programming language syntax, and that I much prefer \"x : X\" over \"X x\" (there are good reasons I think for the type to be on the right-hand side, but other people have already written about that).[1] \"Evangelion\" is a really lovely word, from εὐαγγέλιον good news = εὐ- good + ἄγγελος messenger, the double gamma pronounced even in ancient Greek as \"ŋg\" (\"ng\") rather than \"ːg\" (\"g\").[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Colon_(punctuation)#Usage_in_E... reply contravariant 11 hours agoparentYou may be operating under several misconceptions. Firstly the notation f: X->Y, putting more space to the right of the colon does occur in mathematical writing (admittedly only 1 of the 3 books I checked used that notation exclusively). Secondly that is still a labelling, you&#x27;re labelling a map of a specific form (it&#x27;s almost never used to label anything else, except when dealing with types but that&#x27;s begging the question).I suppose it&#x27;s hard to tell the difference between labelling and stating a property, if I had to give an argument I&#x27;d say that you rarely see a formula on the left hand side, except when defining new notations (e.g. let f+g : X -> Y be the map defined by (f+g)(x) = f(x) + g(x)). Compare this to something like &#x27;let x^2 + y^2 = 1&#x27; which kind of functions as a labelling (or omits it really) but is really more stating a proposition.One alternative meaning in mathematics for the colon that does truly differ is using &#x27;:&#x27; as shorthand for &#x27;such that&#x27;, though sometimes people use a . or no symbol at all. This is commonly used when defining a set by some proposition e.g. { x : x \\in IN and x2}, but also sometimes when using quantifiers. reply kian 8 hours agorootparentI think f: X->Y could be fairly thought of as labeling the function X->Y with f. reply OJFord 1 hour agorootparentf: X->Y means f maps an input from domain X (set of all real numbers, say) to output range Y (set of all positive real numbers, say, f could be &#x27;abs&#x27;).I think you might be thinking it&#x27;s LX.Y (L for lambda)? But consider X=Y, LX.X is the identity function, some f: X->X could be the identity function, but it could also be anything else that returns a value of the same type as its argument - it could be &#x27;add1&#x27; in this example of X being the reals, for example. reply yakubin 1 hour agorootparentprevX->Y is not a function. f is. reply stefncb 33 minutes agoparentprevInterestingly, the labeling colon is written with spaces on both sides in French. reply dvt 12 hours agoparentprevPretty sure it&#x27;s standard practice to always do \"t[space]:[space]T\" with equidistant spaces between the colon. In spite of Type Theory generally being an inconsistent mess, this is a rare case where everyone seems to be fairly consistent. I was actually curious how I did it in undergrad, and even I had the common-sense of making it look nicely symmetrical[1] :)[1] https:&#x2F;&#x2F;dvt.name&#x2F;logic&#x2F;horse2.pdf reply agalunar 12 hours agorootparentAh, I should&#x27;ve been more explicit that I was referring to programming language syntax. For example, this is unfortunately not the convention in Python, Rust, Swift, or TypeScript. reply dvt 11 hours agorootparentAh gotcha, I actually 100% agree here. I think both the parser (and my brain) would probably be better served by space on both sides of a variable : type (or type : variable) declaration. reply chrisweekly 11 hours agorootparentFor my part, while I have some mixed feelings about introducing whitespace signifigance, I sympathize w the OP and in TypeScript projects, have sometimes wished for more intuitive disambiguation. As for conventions, I might preferkey: value(trailing space for eg JS props), vsfoo :type(leading space for types) reply EarthLaunch 9 hours agorootparentI try to do this at home [eg 0] and it flabbergasted some coworkers when one such notation accidentally slipped into a day job PR last year. I didn&#x27;t try to argue for it- but I like it personally, mainly for making control flow easier to skim.0: https:&#x2F;&#x2F;github.com&#x2F;Suncapped&#x2F;babs&#x2F;blob&#x2F;prod&#x2F;src&#x2F;shared&#x2F;Share... replyweinzierl 11 hours agoparentprevThat is an interesting view. What you said about the extra mental step is true for me when I read the common \"X x\" notation, which is astonishing because I saw it vastly more often than \"x: X\". The later reads very naturally for me and I suspect that at least for my brain it is close to how the colon is used in natural language. Often you have a proposition and what follows after the colon adds detail to proposition or describes it in more detail - exactly like a type is additional information about the thing to its left. reply dvdkon 11 hours agoparentprev> Truthfully, I&#x27;m somewhat baffled how it would ever even cross anyone&#x27;s mind when designing a language to write type annotations like \"x: X\" given the long-established, pre-existing precedent in mathematics and the way its semantics seem backwards (if you had to use a labeling colon, \"X: x\" would make much more sense to me).What maths precedent are you refering to? The thing I would consider most similar to today&#x27;s type annotations are statements like \"x ∈ ℝ\" or \"let x be a set of integers\". In both cases the variable comes first.Besides, it&#x27;s much more practical this way. An identifier is very simple to parse and usually short, while types can be long monsters. reply agalunar 11 hours agorootparentTo clarify, I&#x27;m not arguing for \"X x\" over \"x : X\", I&#x27;m arguing for \"x : X\" over \"x: X\".If you had to write it in the form \"a: b\", however, \"X: x\" would make as much or more sense to me as \"x: X\". reply rocqua 3 hours agorootparentPutting the type first would be weird in languages that do type inference. Because there you sometimes leave out the type info, but need to add it occasionally. Then it makes sense to append it, rather than pre-pend. reply dan-robertson 11 hours agoparentprevI had assumed ocaml had the space because you put spaces on both sides of the colon in French.I don’t particularly disagree with you but one thing I think about is the symmetry of a relation or operator as written and if it’s meaning. Eg I prefer an assignment operator to not have a vertical line of symmetry. The other operator examples you give like Colon used before a description> Bertha is so desperate that she&#x27;ll date anyone, even William: he&#x27;s uglier than a squashed toad on the highway, and that&#x27;s on his good days.As in:varible x: Its an X.(edit: formatting) reply agalunar 11 hours agorootparentThat doesn&#x27;t match my experience with the usage of a labeling or prefatory colon (continuing with my ad hoc terminology). I&#x27;d read it asHere&#x27;s an x: X.– which is nonsensical (the type X is not an example of the instance x; indeed, it&#x27;s the other way round), or asIt&#x27;s the case that x: more detailed things that explain or follow from x are X.– which is also backward (since a type is a more general thing than one of many specific instances of the type), or asHere&#x27;s a quick premise x: the main point is X.– which is wrong because x is more relevant (because it&#x27;s a local variable, whereas X is likely global and shows up in many places; indeed, within a particular scope many variables may simultaneously have the same type, but a name refers unambiguously to a particular variable), or asLabel x: thing X.– which is wrong because x is not a name for X. reply teo_zero 10 hours agorootparentI always read the colon as separating a term (introduced for the first time) and its definition. Just compare the characters list in literature: Hercule Poirot: an investigator Linnet Doyle: a rich heiress... with the list of variables at the beginning of a function: x: TypeX y: TypeYIt makes perfect sense to me. reply agalunar 8 hours agorootparentI think I would completely agree for you if it were a definition and not a type annotation!That is, \"x: 5\" instead of \"x = 5\" (or \"x ← 5\", &c) looks fine to my eyes. And of course, that&#x27;s how values for the members of a struct are in fact defined in many languages (such as Rust).I suppose the underlying point is that has-type is a relation, not a definition. Asymmetrical space around a binary relation violates (my) experience, but it&#x27;s also a bit confusing to use the same syntax for definition and type annotation. reply teo_zero 33 minutes agorootparentYou may have a broader scenario in mind, but my experience is limited to languages like Rust and C where the type is linked to an object the moment that object is declared for the first time, and can&#x27;t be changed later. These languages do use the same syntax for definition and type annotation.I will remember this conversation when I encounter a language that allows this (declaration in the first line, type annotations follow): var x x : Integer x = 5 ... x : String x = \"hello\" reply sophacles 10 hours agorootparentprevYou are telling the compiler:Set aside a chunk of memory x. Use the details you call X for operations with it.The computer itself doesn&#x27;t know anything about types or anything else - its just some detail for the compiler to use while generating instructions.A type isn&#x27;t a thing - it&#x27;s not an object. It&#x27;s just a set of constraints that detail valid and invalid uses for the variable. reply agalunar 7 hours agorootparentWe may simply differ here – for me, types are as real as objects. Or, perhaps more accurately, types are as illusory as objects ^_^ replyguerrilla 1 hour agoprevSometimes I just hate it when somone gives away my hard-earned arcane knowledge for free ;) I sure wish this post existed when I learned this stuff. I hope increased accessibility leads to less shit languages though. reply kfixjviv 9 hours agoprevSee also: https:&#x2F;&#x2F;www.hedonisticlearning.com&#x2F;posts&#x2F;understanding-typin... reply lcnPylGDnU4H9OF 12 hours agoprev [–] Where could one find an example of this notation “in the wild”? reply ratmice 2 hours agoparentIt is used in The definition of Standard ML to define the syntax and semantics of the language. That is the most \"in the wild\" usage I&#x27;m familiar with in the sense that it gives a formal specification of the language, which has been used across multiple implementations of the language successfully...https:&#x2F;&#x2F;smlfamily.github.io&#x2F;sml90-defn.pdf reply UncleMeat 12 hours agoparentprev [–] Proceedings in POPL or PLDI will have plenty of papers with this stuff. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Programming Language Design and Implementation Stack Exchange is a platform for programming language designers and implementers to exchange questions and answers.",
      "A recent question on the site sought guidance on reading type system notation.",
      "The site provides an explanation of the basics, including syntax, grammars, relations, judgments, axioms, and inference rules. It also discusses the role of the context or type environment in determining variable types and other considerations in specifying type systems."
    ],
    "commentSummary": [
      "Understanding and using type system notation is crucial in computer science and programming languages.",
      "The article explores the historical development of type system notation and its use in logic.",
      "Different viewpoints and experiences regarding the placement and convention of type annotations in programming languages are discussed."
    ],
    "points": 276,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1692130527
  },
  {
    "id": 37133504,
    "title": "Why host your own LLM?",
    "originLink": "http://marble.onl/posts/why_host_your_own_llm.html",
    "originBody": "Why host your own LLM? Andrew Marble marble.onl andrew@willows.ai August 13, 2023 In the Terminator movies, good relationships beat technological superiority. Kyle Reese and Sarah Connor outwit the advanced T-800 who in turn helps Sarah and John beat the ultra-advanced T-1000. OpenAI’s GPT-4 is currently the most advanced publicly available language model. There are also analyses showing it’s generally cheaper to run than self-hosting comparable models. I want to argue that despite everything OpenAI’s models have going for them, it’s worth considering self-hosting anyway, especially if you’re building a product or an internal capability. If you’re using language models for custom applications, you can use an API from companies like OpenAI or Anthropic, where you submit your prompt, get a response, and pay usage based fees. Or you can configure your own model and host it locally or in the cloud. There are many models available for self-hosting. A few recent analyses have made a case for using OpenAI’s API based on cost and performance1,2. Very detailed cost calculations are possible, but the obvious cost advantage of a usage based API is that you only pay for the hardware when you use it. Most self-hosted applications will be challenged to get good utilization from dedicated GPUs and so are paying a lot for idle time. There’s a lot of subtlety in gauging performance – personally I think there’s not a 1:1 relationship between rankings in the various benchmarks and “leaderboards”3 and performance on specific commercially relevant tasks. But GPT-4 is unequivocally better than the rest across a wide range of skills and only the best publicly available models compete with Claude (Anthropic’s model) and GPT-3.5. Despite the advantages, there is still a compelling case for working with publicly available models. (Note I don’t say open source, many have other limitations that disqualify them from being labeled as such4, but I won’t dwell on that here.) To me it boils down to a the “relationship”. Using APIs means you’re a taker of whatever OpenAI et al. are offering. Model features, customizations, values (censorship and world view) etc. are all dictated by those companies. You can only build a front-end. This also means you don’t have access to internal states and so are limited, for example in applying advanced accountability techniques or guardrails on top. All of this could be good, it means you don’t have to worry about it. But it also makes whatever you build utterly dependent on a start-up company. For “relationship-based” development, there are good reasons to use self-hosted models. Having control over the model architecture and weights removes uncertainty about future changes, and means you don’t have to take what OpenAI decides to give you. There is a rich ecosystem of different models to experiment with, as well as the ability to customize – for example by fine-tuning on your own terms. The construct ultimately lets you build a long-term relationship with your AI model and adapt your product around it, having clarity that what you build is going to keep working with the model that you’ve chosen and giving you control over when and if you decide to make changes. It lets you build something that isn’t just a front-end on somebody else’s language model but is deeply integrated. Also, for many applications, the well-rounded superiority of a GPT-like model is not what’s driving value. Running a model as big as GPT-4 is potentially $10,000’s per month. But it’s possible to run 7B and 13B models (models with 7 and 13 billion parameters, common sizes for LLaMA and other public models) on a laptop. These models are big enough to perform many tasks competently and can be cost-effective as part of local systems. “Responsible” use of AI has many meanings. Tech companies have often focused on political correctness and superficial notions of bias, largely to avoid controversy in broadly capable public models like chat-GPT. For many applications, particularly specialized knowledge work5, those concerns are mostly irrelevant and give way to real issues about factual accuracy, completeness, or simply staying on-topic. Many techniques for “keeping models in line” require access to internal states, gradients, and intermediate outputs6. Using an API-based model limits the kind of experimentation and augmentation that is possible. The same holds true for various optimizations such as caching internal model states, as well as model fine-tuning. APIs offer options, but they are limited compared to what is available. The technology is evolving so quickly still that new models and techniques are becoming available every day. For those that are using the LLM as a tightly integrated part of a product or tool, the only way to have the flexibility to evolve with the technology is to have a self-hosted model. An additional aspect of the fast pace of change in language models right now is that the skills and knowledge required to work with the technology are evolving quickly. Working with self-hosted models gives institutional and individual experience in this evolving landscape, in a way that APIs don’t. For professional development of employees as well as adaptability to change, keeping “AI” at a deeper technical level is important for many companies, particularly those that are building applications. It’s not a mature technology, and part of the “moat” that we practitioners have is simply knowing what’s going on. I’ll actually go further and say that any organization making nontrivial use of AI should internally or through advisers have access to some deep knowledge of the technology, not just the API reference, to be able to understand what it fundamentally does best. As AI gets commodified and hyped-up, there often ends up being a big disconnect between what it can do and what it’s used or proposed for. In a few years, I expect the landscape will look very different – there will be agreed upon things that are critical to be able to do with a model, and APIs will support this. For a new, still experimental, and rapidly evolving technology, real participation requires deep access to the models and code. This doesn’t mean that all companies or products require such access – there are many valuable things that can be built on top of an API and would probably be a waste of time to self-host. But these are different kinds of products. Back to the Terminator, Reese and the T-800 both built strong relationships that led to their successful completion of their missions. The Skynet-tasked Terminators just went around flexing their superior technological prowess, and it wasn’t enough to win the day. Part of building the relationships is access. I know it’s a silly analogy, but I believe the same is true with these models, it’s about being able to deeply understand the strengths of the tool and built something tightly integrated, and you can’t do that with an API. https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526↩︎ https://www.cursor.so/blog/llama-inference↩︎ https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard↩︎ http://marble.onl/posts/software-licenses-masquerading-as-open-source.html↩︎ https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/↩︎ There is a wide range of literature and research into model accountability. For a narrow example, see “The Internal State of an LLM Knows When its Lying” https://arxiv.org/pdf/2304.13734.pdf but also https://arxiv.org/pdf/2307.00175.pdf↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=37133504",
    "commentBody": "Why host your own LLM?Hacker NewspastloginWhy host your own LLM? (marble.onl) 248 points by andy99 21 hours ago| hidepastfavorite128 comments waffletower 18 hours agoI chose to self-host for a variety of reasons. One, I had been using gpt-3.5-turbo for a hobby project and had an abysmal accessibility experience -- I received repeated 429s with serial requests far more sparse than the limits, at most frequent, 10 times less often than the recommended threshold. When I adjusted further the 429s kept returning. It reached a point where even llama.cpp models (on 20 cores) were definitely more performant. I received absolutely no response from customer support despite being an actual paying customer. I imagine OpenAI affiliates here will downvote this, but I found OpenAI&#x27;s API accessibility to be one of the most terrible I have ever used. reply phillipcarter 17 hours agoparentFWIW, we&#x27;ve been live with GPT-3.5 turbo since May and it&#x27;s improved a LOT.Latency is down consistently across the board and we haven&#x27;t seen a single \"429 model overloaded\" error in the past month: https:&#x2F;&#x2F;twitter.com&#x2F;_cartermp&#x2F;status&#x2F;1686894576202907651&#x2F; reply waffletower 16 hours agorootparentIt is worth noting that I had the perception, incorrect or not, that larger customers were being given priority to OpenAI APIs. It is good to see that you are reporting improvements. reply phillipcarter 16 hours agorootparentOh, they undoubtedly were. We have decent volume, but worked very hard in our prompting to minimize input and output tokens, so we don&#x27;t have a big bill. I just think that in the past months they&#x27;ve added significantly more capacity and done a lot of work on their inference servers to make things run better. reply waffletower 16 hours agorootparentGiven Meta&#x27;s obvious, and most welcome, open source play with LLaMA 2, it behooves OpenAI to be performant and accessible to everyone. For my application, I am finding that 70b LLaMA 2 models are very close to the inference quality of gpt-3.5-turbo. reply hooande 15 hours agorootparentprevIt&#x27;s gotten better for everyone in the last few months. It used to be a nightmare, but I haven&#x27;t seen a timeout or rate limit error in a long time. reply justanotherunit 15 hours agoparentprevWhere do you host your model? I am looking around on where I can deploy one without ruining me financially. reply Mernit 13 hours agorootparentYou could easily host your model on https:&#x2F;&#x2F;beam.cloud (I&#x27;m a founder). You just add a decorator to your existing Python code: from beam import App, Runtime app = App(name=\"gpu-app\", runtime=Runtime(gpu=\"T4\")) @app.rest_api() def inference(): print(\"This is running on a GPU\")Then run beam deploy {your-app}.py and boom, it&#x27;s running on the cloud reply jokethrowaway 10 hours agorootparentAn A10G for 1200$ per month will ruin me financially reply coder543 7 hours agorootparentI think the Beam website should be a lot clearer about how things work[0], but I think Beam is offering to bill you for your actual usage, in a serverless fashion. So, unless you&#x27;re continuously running computations for the entire month, it won&#x27;t cost $1200&#x2F;mo.If it works the way I think it does, it sounds appealing, but the GPUs also feel a bit small. The A10G only has 24GB of VRAM. They say they&#x27;re planning to add an A100 option, but... only the 40GB model? Nvidia has offered an 80GB A100 for several years now, which seems like it would be far more useful for pushing the limits of today&#x27;s 70B+ parameter models. Quantization can get a 70B parameter model running on less VRAM, but it&#x27;s definitely a trade-off, and I&#x27;m not sure how the training side of things works with regards to quantized models.Beam&#x27;s focus on Python apps makes a lot of sense, but what if I want to run `llama.cpp`?Anyways, Beam is obviously a very small team, so they can&#x27;t solve every problem for every person.[0]: what is the \"time to idle\" for serverless functions? is it instant? \"Pay for what you use, down to the second\" sounds good in theory, but AWS also uses per-second billing on tons of stuff... EC2 instances don&#x27;t just stop billing you when they go idle, though, you have to manually shut them down and start them up. So, making the lifecycle clearer would be great. Even a quick example of how you would be billed might be helpful. reply bananapub 11 hours agorootparentprevwhy did you decide to make such a bad pitch for your product like this? reply wongarsu 14 hours agorootparentprevThe easy answer here would be either pure CPU at Hetzner (e.g. a 24 core i9 with 64GB RAM for €84&#x2F;month) or GPU at lambdalabs (starting at $360&#x2F;month). Or maybe vast.ai, if you find a trustworthy offering with good uptime.Though GPU workloads are still a point where building your own server and running it from your basement or putting it in colo can be very attractive. reply waffletower 9 hours agorootparentprevLast year I did invest in a dual RTX 3090 Ryzen self-build tower. It runs fairly cool in the basement. So I literally self-host. I am confident that I have or soon will reach the cheaper to self host point of the cost curve, particularly as the two GPUs see very consistent use. reply Zetaphor 8 hours agorootparentprevI use Runpod, an A4000 is $0.31&#x2F;hr reply jokethrowaway 10 hours agorootparentprevI found gpu-mart.com but haven&#x27;t tested yet.An A4000 for 139$ x 12 is not terrible reply visarga 15 hours agoparentprevI have had similar issues with the Azure GPT-3.5, it would respond fast sometimes, and just hang for minutes other times, not even a 429. Just blocked randomly. reply pmontra 12 hours agoparentprevHow much are you spending to self host a LLM? reply rmason 18 hours agoprevJust listened to Lex Fridman&#x27;s three hour interview with George Hotz this weekend. He spoke about his new company, Tiny Corp.Tiny Corp. will be producing the Tiny Box that lets you host your own LLM at home using TinyGrad software.The tinybox738 FP16 TFLOPS144 GB GPU RAM5.76 TB&#x2F;s RAM bandwidth30 GB&#x2F;s model load bandwidth (big llama loads in around 4 seconds)AMD EPYC CPU1600W (one 120V outlet)Runs 65B FP16 LLaMA out of the box (using tinygrad, subject to software development risks)$15,000Hardware startups are extremely difficult. But Hotz&#x27;s other company Comma.ai is already profitable so it is possible. I find the guy extremely encouraging and he is always doing interesting stuff. reply LoganDark 18 hours agoparentIs $15,000 really an \"at home\" sort of price?(If money is no object, why not grab an oxide.computer rack? Assuming you have three-phase power, of course...) reply Me1000 16 hours agorootparentI don&#x27;t think the $15k price tag is intended for regular consumers. I didn&#x27;t listen to the Lex Fridman interview (my patience starts to wane after the first two hours), but I did listen to another of Hotz&#x27;s interview on another podcast, and I believe he referred to this their \"Tiny Box\" as more of a dev box. It&#x27;s for the engineers and tinkerers who want a capable machine for AI. So while expensive, it fills a niche for those who don&#x27;t want to deal with linking multiple Nvidia GPUs together and deal with the power and cooling setup that come with that. reply Havoc 15 hours agorootparentHe&#x27;s definitely thinking bigger than just dev boxes. Lately he&#x27;s repeatedly mentioned building own chips to be somewhere in the future. How plausible that is is another matter reply Me1000 14 hours agorootparentRight, that’s the long term plan. I’m speaking specifically about the $15k first release of the tiny box. reply natsucks 17 hours agorootparentprevRight, especially when it&#x27;s not clear if such a machine will be needed a year or two from now to run a model with the equivalent performance. reply hereonout2 13 hours agorootparentAt this rate it may not be needed in a few months to run a model of equivalent performance.At work I&#x27;ve been successfully running llama2 variants on Mac M2 systems with more than adequate throughput for small experiments and examples. Can also boot a 4x Nvidia T4 instance on AWS (g4dn.12xlarge) to larger models.I feel you&#x27;d need to be doing inference on a 24&#x2F;7 basis for at least a year to justify the price of this machine. reply d_sem 15 hours agorootparentprevThat&#x27;s roughly the price of two original Macintosh computers adjusted for inflation. reply Havoc 15 hours agorootparentprevDoes oxide do GPU builds? I thought they&#x27;re classic server racks (CPU)> three-phase power,Think for the tiny grad it&#x27;s run off single, but for the US the GPUs are power limited given the 110V grid. reply steveklabnik 10 hours agorootparentWe (oxide) aren&#x27;t doing a GPU focused product yet, that&#x27;s right. reply MuffinFlavored 11 hours agorootparentprev> Is $15,000 really an \"at home\" sort of price?For something that is... going to hallucinate? I don&#x27;t get it? reply bavell 11 hours agorootparentI mean, people spend way more than that on random art paintings and sculptures right?With this, it could always be repurposed if something new and shiny comes along reply LoganDark 6 hours agorootparent> With this, it could always be repurposed if something new and shiny comes alongI mean, the oxide.computer specs make me go \"this would be the most killer gaming machine in all of existence\" ... though it&#x27;s a server rack meant to host an entire private cloud for an entire corporation, I still would love to glom together as much resources as can fit into a single VM for gaming purposes. reply phone8675309 14 hours agorootparentprevA new car, especially electric, costs significantly more than $15,000, yet people buy them and bring them home daily. reply paulryanrogers 9 hours agorootparentTransportation is a universal need, and the time savings over other forms are often worth the premium. Doubtful the same can be said for these. reply doctorpangloss 15 hours agoparentprevGeorge Hotz misread the NVIDIA Inception pricing as per unit instead of as a rebate, believing the GPUs are 80% cheaper than they actually are. reply lawn 17 hours agoparentprevWhy buy this instead of building your own setup with X 4090s? reply thatcherthorn 15 hours agorootparentI think he mentions this in the podcast. One of the challenging problems they&#x27;re attempting to tackle is enabling the build to work with standard 120 volt wall sockets. He mentions throttling the system (GPUs specifically) to use less power at cost to performance.That and setting up the software to work w&#x2F; an array of GPUs rather than one seems difficult. reply Aerroon 14 hours agorootparentI don&#x27;t know if he actually said this, but power limits on gpus do work. I&#x27;ve been limiting my gpu to about 60% max power during long AI workloads and I lost about 10% of performance. A 40% reduction in power usage seems worthwhile.It probably depends on your gpu though.Undervolting would probably be even better. reply robotnikman 12 hours agorootparentsame here with the 3090 I use. Power limited to 60% and I barely notice a difference when experimenting around with AI or playing games. Really good to use in a SFF PC reply bavell 10 hours agorootparentWhat OS&#x2F;software are you using to throttle the cards? I&#x27;m interested in doing this on Arch reply zten 9 hours agorootparentnext [–]~ > sudo nvidia-smi -pl 300 Power limit for GPU 00000000:09:00.0 was set to 300.00 W from 450.00 W. replygmerc 16 hours agorootparentprev144GB Vram vs 48? reply wongarsu 14 hours agorootparentAnd once you buy 6 4090s for 6x24=144, and add the other components, you are not that far off from 15k. The 4090s are still a bit cheaper and with a more standard setup, but it&#x27;s close enough to consider the alternative. reply gmerc 4 hours agorootparentGood luck running that on 1600W and keeping it cool. reply bananapub 11 hours agoparentprevwhy are you valuing George Hotz (or Lex Fridman&#x27;s) opinions in this at all? reply psyclobe 16 hours agoparentprevThink of it like an ac unit, they are expensive but indispensable. I have always envisioned locally trained ai and now it is happening! reply Aerroon 13 hours agorootparentYou probably need an AC unit too if you&#x27;re going to be dumping 2000W of heat into your living space. reply naillo 17 hours agoparentprevImagine how many 4090s you could buy and run in a cluster for $15,000 though reply J_Shelby_J 17 hours agorootparentI hate to burst your bubble, but more than two 4090s is going to put you at BoM + labor costs around $15k. Especially if you have to upgrade your electrical and hvac. reply elorant 16 hours agorootparentprevThey seem to have six, either 4090s or 3090s judging by the total amount of VRAM. How many more do you think you could get with $15k considering all the other hardware costs? I doubt you could make it more efficient at this price point. reply heyitsguay 15 hours agorootparentIt might be tough to make more efficient, but $15k seems exactly about the price of \"stick 6 4090s in a decent box and throw in a couple grand for my troubles\", versus any revolutionary hardware configuration. The way it advertises running fp16 Llama 70B feels a bit contrived too, given the prevelance of quantizing to 8 bit at minimum. reply elorant 15 hours agorootparentIn my opinion the best hardware to run big models is to go and get a mac studio ultra. You have 192GB of unified RAM which can run pretty much every available model without losing performance. And it would cost you half that price. reply qwytw 14 hours agorootparent> without losing performanceBut isn&#x27;t M2 ULTRA over 20x slower than this thing? ~30 TFlops vs 738. reply gsuuon 5 hours agorootparentAccording to this [1] article (current top of hn) memory bandwidth is typically the limiting factor, so as long as your batch size isn&#x27;t huge you probably aren&#x27;t losing too much in performance.https:&#x2F;&#x2F;finbarr.ca&#x2F;how-is-llama-cpp-possible&#x2F; reply elorant 14 hours agorootparentprevBy losing performance I meant you don’t need to quantize the model a lot since it fits in RAM. My bad for not clarifying it. replyoutside1234 18 hours agoparentprevWhat podcast (or otherwise) was this?Was it this one? https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dNrTrx42DGQ reply rmason 18 hours agorootparentYes, that&#x27;s the podcast I watched. He also talks a lot about comma.ai which is now profitable on $5 million of VC funding. I think that he has raised a similar amount of funding for Tiny Corp. reply garciasn 20 hours agoprevWe host our own LLM because:1. We are not permitted, by rule, to send client data to unapproved third parties.2. I generally do not trust third parties with our data, even if it falls outside of #1. Just look at the hoopla with Zoom; do you really want OpenAI further solidifying their grip on the industry with your data?3. We have the opportunity to refine the models ourselves to get better results than offered out of the box.4. It&#x27;s fun work to do and there&#x27;s a confluence of a ton of new and existing nerdy technology to learn and use. reply zzleeper 8 hours agoparentStupid question from an outsider, but is it possible to grab a pretrained model (most likely one of the \"camelids\"), feed it your own data (I have about 1k documents that cannot leave my network) and use it for fast-but-sometimes-wrong information retrieval? reply weinzierl 15 hours agoparentprevWhat is your experience with quality? Even if you don&#x27;t have the option to use a third-party LLM the question is if your self-hosted solution is good enough so your users (employees) will accept it. While you can forbid external solutions in the end you can&#x27;t force them to use your own solution - at least not in the long run.I&#x27;m very curious what your experience is? Do you think to self-host is good enough so users will accept it? reply garciasn 13 hours agorootparent1. We can and do disallow sharing of client and company data with third party LLMs by policy and, yes, we can ‘force’ them to use our product capabilities.2. We tested 13 and 30b parameter pretrained LLMs and found them “good enough” within the confines of the original scope.That said, we are expanding the use cases and with that comes the need for increasing levels of quality. This is where the self-training comes in.We’re fairly confident it’ll meet our needs for now. reply weinzierl 13 hours agorootparentThanks for your answer. I think I have to research this further. I disagree with your point about forcing your employees. Everyone has GPT as a comparison, if the internal solution is a little bit worse it might fly. If it is too bad people will not put up with it and if you force them they will either ignore the internal solution or leave.EDIT: Or in the worst case ignore the policy and get fired. reply garciasn 13 hours agorootparentGetting fired could be the least of their concerns depending on the nature of the data shared with third parties. reply weinzierl 12 hours agorootparentI think a good example is Microsoft Source Safe. Microsoft had the strongest incentive to use their own product internally and yet even a company this size could not resist the pressure in the long run to use a marginally better product.We will see how this turns out with LLMs and while I wish companies were more concerned about what data they share with third parties my experience is that apart from military&#x2F;intelligence related data everything can and will be outsourced. replyrig666 19 hours agoprevI host an LLM because it&#x27;s cheaper for my use case. To many people focus on how an LLM interfaces with users but I believe the best most reliable use for an LLM is for analyzing free form text and having it put that data into quantifiable fields or tagging. Things like this would have taken an interns or overseas laborers weeks to months to do can now finally be automated. reply unshavedyak 19 hours agoparentI&#x27;ve had a similar thought. I want to feed LLMs (and friends) messy data from my house and let it un-mess as best it can. A big hurdle in managing home data (chat logs, emails, browser history, etc) is making use of it. i don&#x27;t want to have to tag all of my data. LLMs seem really attractive for that to me.I have this urge to toy with the idea but i also find \"Prompt Engineering\" to be very unattractive. It feels like something i&#x27;d have to re-tailor towards any new model i change to. Not very re-usable and difficult to make \"just work\". reply rig666 19 hours agorootparentYa prompt engineering can be a more difficult than it looks. Especially when dealing with less intelligent models. That&#x27;s why I recommend having an error checking stage were the model gives a model should be able to return a simple \"True\" or \"Yes\" when presented it&#x27;s last response. This eats up more GPU time but the signal to noise ratio improves drastically. reply unshavedyak 16 hours agorootparent> That&#x27;s why I recommend having an error checking stage were the model gives a model should be able to return a simple \"True\" or \"Yes\" when presented it&#x27;s last response.Mind elaborating on that? Looks like a typo but i&#x27;m having difficulty knowing for sure. Thanks! reply siquick 12 hours agorootparentprevWould also be interested in learning more about this reply vilaca 18 hours agorootparentprevThank you for this hint. I have saved a history file from a remote session i did yesterday, some interesting stuff there but also a lot of clutter from all the mistakes I did and asking the free chatgpt to unclutter it for me just made it a lot easier to read and went from 133 lines to 39 lines with added comments and line spacing.It&#x27;s not perfect but it removed a lot of useless stuff like all the times i mistyped &#x27;--recurse-submodules&#x27; or the times I went into a folder only to realize it was not the correct folder for what I was trying to do. reply eterps 19 hours agorootparentprev> I want to feed LLMs (and friends) messy data from my house and let it un-mess as best it can.What would be your goal for doing that? reply unshavedyak 18 hours agorootparentTo then be able to have secondary steps of lookup. I often want a search engine for \"my life\". Most commonly from text messages where i discussed something. In a perfect world it would be able to link context between emails, browser history, chat conversations, etc. I&#x27;d love a flexible system that could record what i have in boxes, in the fridge, etc.Sounds a bit silly, but of course it&#x27;s mostly just for fun. However on the more practical side, i do often find myself needing to dig through old text conversations trying to find that one message. Not having a flexible, deep search behind it sucks. I often find myself wanting to do the same with my browser history. Find that one website i visited, etc.I have the thought that it would be great to make my data points more rich. Don&#x27;t just tag my browser history with isolated tags, such as Programming, Rust, etc - but infer meaning from my searching. Be able to see that i&#x27;m working on ProjectX actively via CLI Git activity, and that i&#x27;m searching for Y. Be able to correlate commit Z with search Y. etcetcIt feels to me there&#x27;s a ton of small, edge case utility that can be gained by dumping everything to a local server and having it link the data. But i don&#x27;t want to do any of that manually.Likewise, i&#x27;ve wanted to manage \"Home Inventory\" before - what&#x27;s in boxes, etc. Managing that myself is tedious, though. LLMs seem ripe for figuring out associations - even dumb LLMs. My hope is that i eventually can start wiring things together and having the LLMs start making rich data out of messy untagged data.Would be neat, &#x2F;shrug reply LTL_FTC 12 hours agorootparentThis would be truly great. I have been pushing back the task of itemizing all of my belongings into a spreadsheet in the event of a natural disaster&#x2F;fire&#x2F;theft and having an assistant that I could say, \"I have this road bike I built\" and have it look through my emails and gather all the components and associated costs, then add it to a spreadsheet, would be a boon to many people. Of course, having it do it automagically from my purchases would be even better. reply BillyTheKing 17 hours agorootparentprevhave you heard of rewind.ai? it sounds like it might be a possible solution for what you&#x27;re looking for (not affiliated with it though, and also don&#x27;t have it on my Mac, so not sure how well it works in reality) reply mickelsen 11 hours agorootparentJust leaving this, there was an effort to build a truly open source one: https:&#x2F;&#x2F;github.com&#x2F;dhamaniasad&#x2F;cytev2 reply unshavedyak 17 hours agorootparentprevYea, though it&#x27;s not local. They claim it is, but then use ChatGPT .. which is odd.Personally i want to build a fairly dumb system though. Ie make a system which can be useful with LLama2 13B or w&#x2F;e. Something that doesn&#x27;t require state of the art GPT4+.If that means compromising on some features that&#x27;s fine, but at least then it can be truly and fully local. replycelestialcheese 18 hours agoparentprevExactly this. It&#x27;s so fast to spin up classifiers now when it used to take weeks to get something working.What LLMs are you using? reply hubraumhugo 19 hours agoparentprevAbsolutely, just look at the number of manual data entry jobs on Upwork. IMO one of the superpowers of LLMs is not generating text or images, it&#x27;s understanding and transforming unstructured data. reply victorbjorklund 19 hours agoparentprevWhat type of analysis do you do on the text? And how is the performance&#x2F;cost of running vs more specialized models trained for the task? reply rig666 18 hours agorootparentThis isn&#x27;t our field but its something similar. So say some of your clients are old publications. Some have articles dating back to the 1800s. Nearly all the work is digitized but searching for something in the great categorized mess is a nightmare. As most old publications are downsizing they don&#x27;t have the man power to curate there archives but are inundated with research requests nearly 24&#x2F;7. As a service to help these publications maintain there image as an organized informative keeper of historical records you could do the following. 1. have an LLM make a series of tags for all the articles. 2. make a summary for all the articles to improve search results. 3. provide a service to them or up sold to their clients were a question&#x2F;prompt can be ran across every article or a section of articles.> how is the performance&#x2F;cost of running vs more specialized models trained for the task. most models are GNU licensed so thats not an issue. But I imagine you meant the age old question of hosting yourself vs using openAI. Truth is as of now it currently is not foretasted to beat using one of the less intelligent models on openAI. hardware cost alone yes but Dev time is very expensive. Lucky were a small company & our CEO sees this as training. Because LLMs are so new there really isn&#x27;t a large labor market for it yet. If our devs and engineers get in this early then we can beat others to market as the technology develops and new opportunities come to light. on top of having possible HIPPA, GDPR, or other security laws to follow that OpenAI has been very shooty about, we do not want be at the whim of OpenAI or another SaaS provider on a mission critical part of a vertical. They have talked about depreciating old model. As well they have had content changes in there models to placate political critics, well not realizing that this pulls the rug out from under developers that need any sense of stability from there product. reply zzleeper 8 hours agorootparentDo you have any suggestions about how to start implementing something like this in-house? I&#x27;m sitting on thousands of PDFs (that can be trivially turned into text) and it would be really useful to train an LLM on them for information retrieval.But the dev and computing cost of this feels so huge that I&#x27;m not even sure where to start. reply rig666 5 hours agorootparentmy first way of showcasing this was by taking a spare computer sitting around the office then writing a little python script that used and LLM to parse information out of file names that our finance team would use to label rebilling invoices. the invoices included the client, payment date, amount, late payment status, etc write in a concluded an completely non consistent file name. the little office PC had 16gb of ram so it was usable for an LLM via the CPU and I just let it run for like 2 days. I continued with my normal work and when it finished I had an intern spend 1 whole day validating just 6% of the data and found it to be 97 percent accurate. I made some obvious changes an was able to fill in that 3% gap. (later we did find a hand full of errors but over all you could consider the validation 99% accurate)While it really resonated with my management I felt worried I wouldn&#x27;t be able to replicate these kind of results on other projects.THE ONLY REAL ADVICE I CAN GIVE ON AI PROJECTS IS . . . don&#x27;t let your managements expectation of LLMs out weigh its capabilities.I&#x27;m sure I speak for many people here when your non-tech fluent directors get together and think GPT4 is some sort of deity. GPT4 smart (or used to be at least) ill give it that, but small locally hosted 7b&#x2F;13b LLMs are very limited and people for whatever reason get AI infatuation the second they finally see you show direct value in it they will lose there shit in its assumed capabilities. you got to be direct with them that no matter what dumb video they saw on Sam Altman, what your are proposing is not that. Be very clear in its possible scope because there is some idiot in our organization that will assume assume you can programmatically answer prayers. I actually had this guy from our networking team try and raise a concern about the LLM going sentient and us having a \"Skynet\" problem. granted this was back in march&#x2F;2023 so AI histira was a little more rampant but still.tl;dr my recommendation for your pdf project is run https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui. if your can get a 30 series GPU in your company Then run a 13B 4bit model that can pull info, assign tags, run minor analysis on your text. else find a spare 16gb machine and do the same but but over a longer time scale.run a prompt that checks for hallucinations. \"does the following text make sense? previous prompt + text if yes then keep else make intern do it.GPT-j-7b is still one of the best models because it has indexing & categorizing at the main prosperous. other models are great but core idea behind LLMs is that its just a high level auto complete reply wcedmisten 16 hours agoparentprevHow accurate is an LLM for this task? I was thinking of using one for analyzing free form PDF text to find a specific element, but I was worried about hallucinations. reply halflings 16 hours agorootparentExtractive tasks are part of where LLMs shine, and where you get the least amount of hallucination as long as you fine-tune your model.By fine-tuning the model to extract a specific desired output from the text you give it, it learns that the output always comes from the input, and so you get less random outputs than just by prompting an instruction-tuned model (which was fine-tuned to find the answer in its weights, instead of copying it from the input). reply wcedmisten 14 hours agorootparentI&#x27;m pretty ignorant on which is the best self hosted LLM for such a task or how to fine-tune it. Do you know of any resources on how to set that up?It seems like llama2 is the biggest name on HN when it comes to self hosting but I have no idea how it actually performs. reply Aerroon 13 hours agorootparentprevWon&#x27;t you run out of context size though? The older models only went up to 2000 tokens, newer ones up to 16k.Ie how do you feed the LLM the text along with your question without it forgetting most of the text? I assume the text you want to feed it is longer than 16,000 words. reply wcedmisten 9 hours agorootparentFor my use-case the PDFs are only a few pages long generally, so I think the 16k word limit would be well within my needs. I&#x27;m trying to find a list of device names from an FDA 510k summary (for medical device clearances). Currently I&#x27;m doing this manually and it&#x27;s quite time consuming. I have around 15,000 PDFs to get through manually, but it&#x27;s pretty slow work. reply jackthetab 15 hours agoparentprevI assume asking for \"quantifiable fields\" is akin to requesting \"return the data in JSON format\", yes?How do you do the tagging bits, though? reply andai 19 hours agoparentprevCheaper than GPT-3? Can you give a comparison of the costs? reply dbish 18 hours agorootparentGPt-3 is very expensive if you use it frequently compared to just running in a desktop machine you already have. Of course, if you’re buying new hardware just to run a model for yourself locally, that’s a different cost analysis, but for me I had other reasons to have a decent gpu.If you have a product that uses an LLM and can get away with one of the open source ones, it’s probably cheaper (and def lower latency&#x2F;response time) to host yourself too somewhere like azure or aws. reply moneywoes 19 hours agoparentprevMay I ask what your stack is? reply rig666 19 hours agorootparentNothing complex actually but it is a little messy and cobbled togetherI run oobabooga&#x27;s API on docker with a 13B 4bit quantized model. https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webuiWe use GTX 3060s because there the best bang for there buck in terms of VRAM. Our current set up is mostly proof of concept or used of inner office work well we work on scaling to get a fluid handler built so it can distribute workloads around the multiple GPUs.Lucky the crypto mining community laid the ground work for some of the hardware. reply grep_name 16 hours agorootparent> Lucky the crypto mining community laid the ground work for some of the hardware.Are you using riser cards to connect the GPU&#x27;s to the motherboards then? I thought about trying a setup like yours, but was worried that the riser card interfaces would create a bottleneck. Ideally I&#x27;d like to run some cards in a separate box and connect them to my main computer through some kind of cable interface, but I&#x27;m not sure if that&#x27;s possible without seriously affecting performance. reply neilv 20 hours agoprevIt takes searching and experimenting to figure out what works, and to avoid some of the sketchier stuff (and to lean towards things you could legally use for a startup), but I&#x27;m pretty happy with my current home setup, on an old PC with RTX 3090 and 64GB main RAM.8-bit quantized uncensored Llama 2 13B, doing 50 generated tokens&#x2F;second, using CPU+GPU including 17GB of 3090&#x27;s 24GB VRAM.I also have quantized 70B running currently CPU-only, but I might later be able to speed that up with some CUDA or OpenCL offloading.This is on Debian Stable (like usual), albeit currently with closed Nvidia CUDA stack, and necessarily with the closed Llama 2 that I can only fine-tune atop. (I&#x27;m hoping that some scientific&#x2F;academic non-profit&#x2F;govt effort will be able to muster fully open models in the future.)One of the main reasons I picked Llama 2 was the relatively friendly licensing (and Meta is earning lots of goodwill with that). With this licensing, and the performance I&#x27;m getting, in theory, I could even shoestring bootstrap an indie startup with low online LLM demands, from a single consumer hardware box in the proverbial startup garage or kitchen table. (Though I&#x27;d try to get affordable cloud compute first.) reply thrwayaistartup 20 hours agoparentI am about to start working on a non-profit project -- not a startup, but similar in terms of resources dedicated to the project and how we hope it will scale.One of our big questions is whether it makes sense to rent or to buy for training&#x2F;finetuning&#x2F;RLHF. The advantage of renting is obvious: I don&#x27;t think that this phase of the project will last very long, and if it turns out that the idea is a success we&#x27;ll have no problem securing funding for perma-improvement infra.The possible advantage of buying is that we would then have the hardware available for inference hosting. We do expect some amount of demand in perpetuity. Having that ongoing cost as small as possible would allow us to continue serving the \"clients\" we KNOW would benefit a lot from our service with minimal recurring revenue. reply rig666 19 hours agoparentprevJust a suggestion but they have 4bit quantified models that are even smaller and faster that the 8 bit. Your average 13B 4bit model is only about 8-9gb of VRAM. Using this I bet you can get a much higher perimeter model on the 3090. reply neilv 19 hours agorootparentI was using various 4-bit quantized earlier, but decided to go back to 8-bit for 13B, since I had the VRAM anyway, and (at the time, for other reasons) was seeing some quirky behavior.70B is currently 4-bit on this box, and once I have GPU accel for 70B, I&#x27;ll see how the quality compares to 13B 8-bit. reply dealuromanet 7 hours agoparentprevWhoa, 50 tokens&#x2F;second locally sounds amazing. Any recommendations on guides or documentation for setting up the stack to run on hardware like that? reply easygenes 15 hours agoprevI’m pretty amazed by how good 13B models are since they’ve gotten the orca treatment. This new one released today has the best evaluation performance of all so far and is in some ways comparable or better than the original LLaMA-65b… a bit shocked by that.https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;OpenOrca-Platypus2-13Bhttps:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;Open-Orca&#x2F;OpenOrca-Platypus2-1... reply Timon3 13 hours agoparentThe model has also been quantized if you have less than 26GB of RAM: https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;OpenOrca-Platypus2-13B-GGML reply elorant 19 hours agoprevI self host an LLM (Vicuna 13b) for two reasons. One is cost, and the second is privacy. I don’t want OpenAI or any other provider knowing what I&#x27;m working on because they could replicate it. I&#x27;m not saying that they would considering that there could be thousand of business cases for using an LLM, but why risk it. By running it locally I have one less thing to worry about. reply flemhans 12 hours agoprevHow could you _not_?I&#x27;m only waiting to have decent LLMs able locally to be able to start really using them.No way I&#x27;d feed my code, customer data, personal info, secrets, emails, etc., to some dubious cloud machine which is already excellent at exctracting valuable or juicy bits from what I&#x27;m feeding it. reply jonahbenton 12 hours agoparent+1I have a little quip I use to troll people who say in effect- but but contract law, your contract says your data is yours even when it is on someone&#x27;s cloud servers- I say- I know, I know, but remember \"possession is 9&#x2F;10ths of the law.\"I do believe in 99.(many 9s) cases no one admin with visibility cares about any given customer&#x27;s stuff but if they do and if it matters, by then it&#x27;s too late. reply edfletcher_t137 17 hours agoprevI built a simple, asynchronous, serialized API on top of llama.cpp for exactly this reason. https:&#x2F;&#x2F;github.com&#x2F;edfletcher&#x2F;llama.http&#x2F;tree&#x2F;master&#x2F;example... It can run on low-resource VPSes even, if you have the patience for CPU inference (which will take awhile)! reply jmorgan 19 hours agoprevOne benefit of self-hosting LLMs is the wide range of fine-tuned models available, including uncensored models. A popular one over the last weeks was Llama 2 Uncensored by George Sung: https:&#x2F;&#x2F;ollama.ai&#x2F;blog&#x2F;run-llama2-uncensored-locallyA few more:- Wizard Vicuna 13B uncensored- Nous Hermes Llama 2- WizardLM Uncensored llama2 reply _pdp_ 14 hours agoprevYou should run your own LLM if you can. Just keep in mind that many hobby users simply cannot do that. They represent the majority of LLM users - not the majority of power users. These people will struggle to use LLMs without some technical support, not because they cannot learn, of course they can, mostly because it is not their priority. LLMs as a technology needs to be made more widely accessible by making it open-source, so folks can run their own instances if they decided to do so, but also hosting it and providing it as a service for those who simply do not have the skills or desire to run them themselves. reply ilaksh 12 hours agoprevA few things holding me back for now:- I use LLMs for code generation for a startup and they are not competitive for that yet.- Most of the popular open models are non-commercial.- The only practical way I know of to get large custom datasets for training is to have OpenAI&#x27;s models generate them, and they forbid this in their terms of service.Having something that&#x27;s truly open and closer to GPT-4 for code generation will probably happen within less than a year (I hope) and will be a game changer for self-hosting. reply thatcherthorn 20 hours agoprevI haven&#x27;t tried self-hosting due to the hesitation around the general drab I&#x27;ve experienced in the past trying trying to host other ML models.Find a repo. follow the install instructions. What is this weird error? A library issue..? Maybe it&#x27;s my OS..?It always seems to be tedious compared to open projects in other domains. Maybe that can&#x27;t be solved. reply smcleod 20 hours agoparentI replied to a comment in another post yesterday on this - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37120346Honestly the easiest way that “just works” is to use LM Studio which you can run locally https:&#x2F;&#x2F;lmstudio.ai&#x2F;Obviously you’ll have faster results if you have a fancy gaming GPU or something like the M2 Max&#x2F;Ultra but you don’t need those to have a play and see if it interests you. reply vorticalbox 20 hours agoparentprevI just use GTP4all [0] both the GUI and the python bindings[1][0] https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html [1] https:&#x2F;&#x2F;docs.gpt4all.io&#x2F; reply vorpalhex 20 hours agoparentprevLlama models have been pretty easy to host. StableDiffusion was a real nightmare when it came out (and still is at times).Using docker has an initial threshold you have to get over but once you do, everything becomes very easy in it. How you end up using docker matters very little once you get the concepts. reply bheadmaster 20 hours agoparentprevIn my experience, people specialzed in machine learning are usually researchers and mathematicians, not engineers. Writing a package that will work on any random person&#x27;s hardware and system is a non-trivial engineering task. reply d_sem 15 hours agoprevWhether or not its \"better\" to host your own, the positive effects from the open source community at trimming down the parts of state-of-the-art LLM that matter and improving efficiency will be good for the community. reply sourcecodeplz 18 hours agoprevI run Llama 7b with CPU only. It is fun when my Internet goes down and I have nothing else to do. reply kordlessagain 16 hours agoparentThey are a bit of an \"offline\" network, in a way. reply blackcat201 20 hours agoprevI own my LLM not because I need it now but having the luxury to fall back if openai ran out of money reply gdsdfe 18 hours agoprevI&#x27;ve been thinking about hosting my own LLM to see if I can hyper customized it to me basically, kinda like an AI companion. My main issue is building the hardware, there so much fluff in that space, it&#x27;s hard to know what to get and what works well together reply ajcp 14 hours agoparent- Intel Core i9-11900KF 3.5 GHz 8-Core Processor- Corsair H150i PRO 47.3 CFM Liquid CPU Cooler- MSI MPG Z590 GAMING EDGE WIFI ATX LGA1200 Motherboard- G.Skill Ripjaws V 64 GB (4 x 16 GB) DDR4-3600 CL18 Memory- Samsung 970 Evo Plus 1 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive- MSI GeForce RTX 3090 TI SUPRIM X 24G GeForce RTX 3090 Ti 24 GB Video Card- Corsair Carbide Series 275R ATX Mid Tower Case- Corsair RM1000x (2021) 1000 W 80+ Gold Certified Fully Modular ATX Power Supply- Microsoft Windows 11 ProOn this setup I&#x27;ve been able to run every model 13B and below with 0 issue. Even been able to fine-tune Llama 2 13B using my own data (emails, SMS, FB messages, WhatsApp, etc.) with pretty fun results! reply davidklemke 11 hours agorootparentWhat was your stack for doing the fine tuning with your own data? I&#x27;ve been looking around at various different approaches and I think I have a general idea of how to approach it (generating embeddings, putting them in a vector DB, somehow linking that to a useable UI, etc.). Would definitely be keen to understand what your solution looks like though! reply dealuromanet 7 hours agorootparentI am interested in this as well. Pretty please!!! reply kwerk 9 hours agorootparentprev> On this setup I&#x27;ve been able to run every model 13B and below with 0 issue. Even been able to fine-tune Llama 2 13B using my own data (emails, SMS, FB messages, WhatsApp, etc.) with pretty fun results!Is it useful? Would you let it draft responses at you? Curious about the fun results :) reply talham 15 hours agoprevthanks for writing the article: any recommended links on HOW to host your own LLM? reply talham 15 hours agoprevthanks for this article: any recommended links on HOW to host your own LLM? reply Bostonian 20 hours agoprev [–] ChatGPT is powerful, but it gives you different answers to the same question from one session to the next. And research found that overall performance can vary over time, sometimes for the worse. So you may host your own LLM for reproducibility.I have not tried public LLMs myself. Do they give reproducible results? reply TacticalCoder 20 hours agoparent> I have not tried public LLMs myself. Do they give reproducible results?Public LLMs I don&#x27;t know but images generated using StableDiffusion are, of course, totally deterministic.There really is no reason a LLM cannot be deterministic and if it isn&#x27;t: fix it (even if this comes at a tiny performance cost). reply mcbuilder 17 hours agorootparentIf you fix the random number seed virtually all LLMs should be deterministic. However, just 1 token difference in the input could produce a very different output, depending on the sampler, model, etc. So, LLMs can be deterministic, but in practice they are pure alchemy. reply embwbam 20 hours agoparentprevThey do that on purpose. The API gives you a setting you can change to 0 or 1 for maximum creativity or maximum reproducibility reply sp332 19 hours agorootparentThat doesn&#x27;t work for GPT-4 though. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006224 reply Makhini 19 hours agorootparentprevExcept it&#x27;s never reproducible. It&#x27;s a bug probably. reply varispeed 16 hours agoparentprev [–] > it gives you different answers to the same questionSometimes answer is wrong and then right.If it is deterministic, then what if gets \"stuck\" on wrong answer? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author presents an argument in favor of self-hosting language models (LLMs) instead of relying on publicly available models like OpenAI's GPT-4.",
      "Self-hosting offers greater control over model architecture and weights, customization and fine-tuning capabilities, and a long-term relationship with the AI model.",
      "Self-hosting allows for more experimentation, optimization, and staying up-to-date with evolving technology, while APIs may not provide the same level of access and understanding."
    ],
    "commentSummary": [
      "The article explores the reasons and challenges of self-hosting Language Model Models (LLMs), instead of relying on OpenAI's API.",
      "Alternative options for hosting LLMs, such as Beam, personal hardware, and George Hotz's Tiny Box, are discussed.",
      "Feasibility and benefits of self-hosting LLMs, including cost-effectiveness and privacy concerns, are explored."
    ],
    "points": 248,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1692104791
  },
  {
    "id": 37140159,
    "title": "Modern CSV version 2",
    "originLink": "https://www.moderncsv.com/modern-csv-2-is-now-available/",
    "originBody": "Hi, I&#x27;m Evan and I developed Modern CSV. Last year, the beta version of version 2 was posted here on Hacker News. As a follow-up, I&#x27;m letting you guys know that the beta period is over and Modern CSV 2 is now available.Modern CSV is a tabular file editor&#x2F;viewer for Windows, Mac, and Linux. I developed it out of frustration with how spreadsheet programs handle CSV files. Plain text editors, on the other job, do a poor job of handling columns. With Modern CSV, I attempt to combine the best of both worlds.With version 2, you can expect to see an improved UI&#x2F;UX, better performance, more useful features, updated documentation, and for Mac users, native Apple Silicon support.If you haven&#x27;t tried it yet, give it a shot and let me know what you think!",
    "commentLink": "https://news.ycombinator.com/item?id=37140159",
    "commentBody": "Modern CSV version 2Hacker NewspastloginModern CSV version 2 (moderncsv.com) 251 points by evanem 11 hours ago| hidepastfavorite107 comments Hi, I&#x27;m Evan and I developed Modern CSV. Last year, the beta version of version 2 was posted here on Hacker News. As a follow-up, I&#x27;m letting you guys know that the beta period is over and Modern CSV 2 is now available.Modern CSV is a tabular file editor&#x2F;viewer for Windows, Mac, and Linux. I developed it out of frustration with how spreadsheet programs handle CSV files. Plain text editors, on the other job, do a poor job of handling columns. With Modern CSV, I attempt to combine the best of both worlds.With version 2, you can expect to see an improved UI&#x2F;UX, better performance, more useful features, updated documentation, and for Mac users, native Apple Silicon support.If you haven&#x27;t tried it yet, give it a shot and let me know what you think! hn_throwaway_99 10 hours agoJust wanted to say I haven&#x27;t used your product yet, but there are a couple things I really like about your site that led me to download it:1. The first question I had was \"How is this better than Excel?\" The Google result for your site directly pulls up this very informative blog post, https:&#x2F;&#x2F;www.moderncsv.com&#x2F;why-excel-sucks-and-modern-csv-is-... . Note my 2 cents is that I would more prominently link this, or at least have a more prominent section, from your home page.2. Simple AF pricing. So rare these days. Kudos! reply haspok 1 hour agoparentThanks for linking that post. I would summarize this tool more as a log viewer and editor, not necessarily a CSV editor. I mean, this: \"What if you’re running a script that writes to a CSV file and you want to see it updated?\" I have never experienced in my life, and I had my fair share of CSV munging... reply stevage 2 hours agoparentprevYeah, Excel&#x27;s data munging is a massive problem for a lot of CSV stuff. reply simonw 10 hours agoprevYou should add that \"Modern CSV is a tabular file editor&#x2F;viewer for Windows, Mac, and Linux\" line to the announcement page on https:&#x2F;&#x2F;www.moderncsv.com&#x2F;modern-csv-2-is-now-available&#x2F; - for this kind of post it&#x27;s always best to assume that people landing on the page don&#x27;t know what the software is yet.(It&#x27;s easy to assume people will click the link from that page to https:&#x2F;&#x2F;www.moderncsv.com&#x2F; in order to find out more, but I&#x27;m not sure that&#x27;s always a safe assumption!) reply wodenokoto 5 hours agoparentYeah, the HN title lead me to believe this would be version to of a modern cab specification.A bit surprised to see that a specification had improved UI before it dawned on me that it’s a CSV reading. reply topato 3 hours agorootparentHaha, I had the same thought \"Oh wow, I wonder what sorts of modern data storage techniques they managed to backport into simple CSV\" And then.... disappoint. At least until I check out the software. It still sounds like the same column functions I get from notepad++ or VS:C reply nullindividual 10 hours agoparentprevTo expand upon this... the name is a problem.\"How can one make a _modern_ version of the CSV file format, let alone a second?\" reply evanem 9 hours agorootparentThe name only refers to the app&#x27;s features and appearance. Well before I started development, I was looking for a CSV editor for my own use and everything I tried had a 90s look and feel. I figured just because the format is ancient doesn&#x27;t mean the applications for it should be. reply coldtea 3 hours agorootparent>The name only refers to the app&#x27;s features and appearancePerhaps, but it&#x27;s very easy to confuse with it being about a modern version of the CSV standard.In fact that would be probably the default interpretation. reply kwanbix 1 hour agorootparentYeah, that is the first thing I thought.Modern CSV Editor will probably be a much better name. reply coldtea 1 minute agorootparentYeah, and perhaps sticking the word together: ModernCSV Editor simondotau 8 hours agorootparentprevI think it’s a great name. Keep it. Even if it confuses some people at first, it’s the sort of confusion which will inspire many people — especially those who deal with text files all day and get pedantic about this stuff — to satisfy their incredulity and thus be exposed to your program.This is perfect for a tool which most people, including those who would derive the most benefit from it, won’t even hypothesise the existence of. reply pbhjpbhj 2 hours agorootparentThis type of marketing by deception is absolute anathema to me.People will \"dig deeper\" because you tricked then into thinking what you&#x27;ve made of something else, something that person knows they want. Bleurgh.Yes, that&#x27;s consistent with modern marketing: make people get your product even if they don&#x27;t want it and&#x2F;or don&#x27;t need it. But, really, are you so desperate. You might save a 1000 hours by making it clear what the subject is; choosing to waste others time [which I&#x27;m not at all claiming the OP has done] is contemptible. reply coldtea 2 hours agorootparentprev>it’s the sort of confusion which will inspire many people...to skip it, because while they could use a CSV file editor, they don&#x27;t care to read about yet another \"modern CSV\" format.Confusion can only inspire if potential users pass from hearing about this, to the next level (of checking what it is about).I, for one, almost didn&#x27;t click the HN submission because of this reason. reply ChrisNorstrom 3 hours agorootparentprevIgnore them, ModernCSV is perfect and memorable. That&#x27;s how I remembered it and found it. reply coldtea 2 hours agorootparentOf course those who skipped it because of the name wouldn&#x27;t be posting...https:&#x2F;&#x2F;medium.com&#x2F;@penguinpress&#x2F;an-excerpt-from-how-not-to-... reply bombcar 10 hours agorootparentprevSomething playing on CSV could work - CSVlad or CSViper or CSVi reply agar 9 hours agorootparentCSVi is quite clever. It immediately conjures the idea that its an editor for CSVs.I assumed from the HN headline that Modern CSV version 2 was an iteration on some new CSV specification. reply another-dave 2 hours agorootparentIt is clever but I can imagine the comments the OP would&#x27;ve gotten if posting \"CSVi\" and it didn&#x27;t use vi-like keyboard shortcuts! reply sdenton4 8 hours agorootparentprevSpoiler: it&#x27;s Parquet. reply __del__ 7 hours agorootparentprevi agree with the nibling comment that this name works precisely because it provokes curiosity about its relevance.i can&#x27;t be the only dev who saw this headline and wondered whether some entity was pushing for a csv standardization (e.g. guarantees about encoding, escaping, etc.) reply bee_rider 6 hours agorootparentprevI don’t really see the problem. Of course, CSV is more of a vague description of a family of pseudo-standards, rather than an actual standard. But Modern CSV seems like a fine name for an actual standard that takes inspiration from them. reply coldtea 2 hours agorootparentThat&#x27;s exactly the problem.This isn&#x27;t about a name \"for an actual standard that takes inspiration from them\" but for a CSV editing software. reply shmoe 6 hours agorootparentprevThat sort of thing usually breeds curiosity and leads someone to dig in deeper about the product though. reply d-k-bo 2 hours agoprevI think you should release the Linux version as a Flatpak [1] to Flathub [2]. That way it could be install using a (graphical) software manager on many distros and it would increase the discoveribility of your program. It shouldn&#x27;t be too complicated to create a package from the provided tarball.[1]: https:&#x2F;&#x2F;flatpak.org&#x2F; [2]: https:&#x2F;&#x2F;flathub.org&#x2F; reply danmur 1 hour agoparent+1 for flatpak (and modern csv looks great! I will most likely buy) reply tacker2000 24 minutes agoprevCan your app auto detect the CSV delimiters and also detect decimal point vs comma for numbers?This is a huge problem when dealing with European and UK&#x2F;US CSV formats (or whatever you mau call them) for me daily.Excel only auto opens the format supported by the current locale, so anything else will just not be detected. reply ptman 19 minutes agoparentlibreoffice calc is better with csv import IIRC reply kasperset 8 hours agoprevNice to see software for such usage. Also reminds me to use this Visidata which is not GUI like modernCSV but TUI https:&#x2F;&#x2F;www.visidata.org reply albert_e 6 hours agoprevWhen opening mobile site from India (Android,Chrome) -- the \"buy\" page lists prices like 19 and 29 without any currency symbol. You mean rupees, right? Take my money! ;) reply billrobertson42 5 hours agoparentGood news! It&#x27;s in rubles. reply vogon_laureate 1 hour agorootparentI&#x27;m so glad I put all my rubles into PutinCoin. reply davgoldin 9 hours agoprevI share the frustration! I have to open lots of small and large CSV files daily, with multiple encodings. Usually I&#x27;m juggling between Excel, Numbers, and Sublime text editor - depending on which one would open that file faster.Naturally, I installed Modern CSV seconds after seeing this post.I&#x27;m on MacBook M2, using trackpad 1) the vertical scroll is slow and un-mac-like, unnatural; and 2) horizontal scroll basically doesn&#x27;t work, it scrolls a column or a half on a full swipe, but it does improve somewhat when there&#x27;s less content in the cells.I admire your work, please keep on going, there&#x27;s a huge need! reply evanem 9 hours agoparentYou&#x27;re right, Mac trackpad handling is something I need to address. It&#x27;s on my to-do list. reply oneeyedpigeon 42 minutes agorootparentLooks like it&#x27;s scrolling in general, not just trackpad. It appears to limit the maximum scroll speed when I&#x27;m using a mouse. I take it this is because the app is programmed using some kind of non-native controls? reply nuc1e0n 50 minutes agoprevNice work. This&#x27;ll make things easier for non technical people to not mess up data entry by abusing Excel. Is there a means to configure and enforce validation functions&#x2F;regex for columns? Like some kind of python api perhaps? It would also be cool if data could be streamed over http(s). reply gregwebs 9 hours agoprevHaven’t tried ModernCSV. Recently used TAD: https:&#x2F;&#x2F;www.tadviewer.com&#x2F; And clickhouse-local reply codedokode 9 hours agoprevJudging by screenshots, it seems that there are things that can be improved in UI design, for example, in \"Solarized Dark Theme\", the contrast in selected cell is so low (gray on purple) that the text is barely readable. To be honest, I don&#x27;t like the colors from any of the themes. All the themes use grey color a lot and have low contrast.The widgets like inputs or selects also look weird, sometimes they don&#x27;t have enough padding and content touches the edges, sometimes there is too much padding. In a numeric input there are icrement&#x2F;decrement buttons, but they are so tiny that I doubt they are usable even with an Apple touchpad. The text in inputs is sometimes aligned to the left and sometimes centered without obvious reasons.The command search popup is transparent and the background seen behind it makes reading the text harder. reply Exuma 10 hours agoprev.... one huge thing that I&#x27;m assuming theres a fix for. How do i enable light mode? This is insane on the eyesEdit: I found it under &#x27;set theme&#x27;I REALLY recommend doing light mode as the default, not dark mode. I imagine like 1% of people would actually want to look at big data as white text on black. reply TylerE 7 hours agoparentReally the default should be \"match system\". Both Mac and Windows (at the very least) have this baked into the DE at this point. reply jer0me 10 hours agoparentprevScreenshot 2 shows an option to change the theme reply andy800 9 hours agoprevModernCSV is a good tool. However, from the viewpoint of a paying customer, it feels too much like a side&#x2F;hobby project and that you aren&#x27;t taking it all that seriously as a business. The v2 beta was announced in May 2022 with indications that the final v2 was just a few weeks&#x2F;months away, as you announced the beta expiring June 25, 2022. Obviously v2 was not close to ready and paying users had to revert to v1 or to repeatedly request updated betas, each of which expired after a brief period. Your web site was not regularly updated with news or progress reports and the project appeared to be abandoned.This is all fine for an open source project but once people are paying for a tool, using it daily, and are promised certain updates coming soon, it&#x27;s not great to have that update delayed by a full year.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31576554 reply hn_throwaway_99 8 hours agoparentI agree with your general sentiment, and it&#x27;s important for businesses to have good communication.At the same time, the \"Premium Personal\" price is a one-time payment of $29. My basic chicken sandwich meal at Whataburger was $11 dollars.Point being, it&#x27;s obvious to me that this is 1 dude who is developing this, so I&#x27;m not going to expect flawless communication for the price of 2.5 Whataburger meals. I&#x27;d much prefer to deal with that than have something like a monthly subscription SaaS charge. reply andy800 8 hours agorootparentThat&#x27;s your opinion, and it&#x27;s ok, but my employer bought the business version (4.5 whataburgers), and it bought multiple licenses (about 20 or 25 whataburgers, where&#x27;s the tipping point?), plus I personally spent a lot of time (based on my compensation, likely 50+ whataburgers) vouching for a new vendor and then all the corporate b.s. like establishing purchase orders, creating new budget line items, etc, as well as justifying the purchase to IT dept which believes that Excel handles CSV files just fine.Doesn&#x27;t have to be flawless but a full year delay is a little much. I&#x27;m not even with that employer anymore. reply TylerE 7 hours agorootparentThe real dropped ball isn&#x27;t the delay, but the communication (or lack there of) of same. reply totoglazer 7 hours agorootparentprev~$1000 USD based on the quoted Whatsburger&#x2F;USD cross rate. reply Sytten 7 hours agoparentprevHN: Subscriptions are evil, one time payment is the only way software should be soldAlso HN: Why is the one developper that work on my software when he can because he can&#x27;t make a living out of my one time 30$ payment doesn&#x27;t ship all the features and versions I expect in a timely mannerSupport contracts start at 500-1000$ a month these days, if you depend on it for your day job maybe invest in your tooling correctly... reply andy800 6 hours agorootparent- we spent at least 6x of your estimate- we would have been fine continuing to use a non-expiring v2 beta, but the developer set it up differently (eventually, a non-expiring version was provided)- we never asked for support, just to not have the software expire without a replacement available> maybe invest in your tooling correctly...So your suggestion is that businesses should avoid independent developers altogether? That&#x27;s a great way to be supportive of the little guy. reply nuc1e0n 22 minutes agorootparentprevI disagree with you there. For organisations that expect support for software over a longer period a recurring support contract is better. Otherwise you can end up supporting their never ending requests for free. Or worse they just choose something else. reply skrebbel 3 hours agorootparentprevAlmost seems like HN doesn’t have a single unified opinion on things huh! reply evanem 8 hours agoparentprevThat&#x27;s a fair critique. While the app last year was mostly ready to go (aside from a few features I later decided to add), most of the issues I&#x27;ve had to deal with since have been related to other things (sales form, website, etc.). The license keys I gave you should work on this latest version. reply Aeolun 8 hours agoparentprevAre you saying you are upset that you couldn’t use an unreleased new version, and that you had to resort to using the version you had actually purchased? Or did you purchase V2 before it was released as a sort of pre-purchase? reply andy800 7 hours agorootparentOur initial purchase was made while v2 was in beta and with a promise of a free upgrade to the final v2, which was expected soon. Except it was never released, until now. reply evanem 7 hours agorootparentJust to clarify, I did give you version 2 license keys that worked on those beta releases and this version 2 release at no extra charge, as I promised. I also changed the beta releases to not have a time limit and accept license keys instead. The beta releases I provided had nearly the same capability and quality as this one. reply andy800 6 hours agorootparentThere was a gap between the beta expiring and me reaching out and requesting an update, which you supplied. I think the first replacement also expired and we had to make another request but I could be wrong, I&#x27;m no longer at that company and have no way to verify. Maybe not.I am not claiming you ripped us off or that your product is no good. Pretty sure we eventually purchased 4 licenses, because it is a good product. But better communication, taking the whole thing a bit more seriously, would go a long way. You knew what day the betas were expiring, instead of leaving all the customers hanging, you could have proactively sent everyone license keys or a longer-expiring version. You could have posted a quick status update on your web site. As I mentioned in another comment, I vouched for you internally and when asked about it months later (budget reviews, etc) with still just a beta version, I didn&#x27;t have any answers. reply shmoe 6 hours agorootparentprevYou certainly did the right thing, it sounds like you could&#x27;ve been more responsive though. Learning experience! replyjdthedisciple 1 hour agoprevLooks great, definitely bookmarked for the next time I&#x27;m gonna work w large csv files.On a side note: Howcome MS failed so hard at solving this problem with Excel?Considering it&#x27;s old age and the budget behind it it would seem like Excel should be THE solution to anything to do with tabular data by now. reply haspok 1 hour agoparentMaybe there is no real need for supporting large CSV files? Typically large amounts of data will be stored in a database (in which case you can query with SQL), or you will be using large-data oriented file formats like parquet. Excel&#x27;s CSV support is just good enough for 99% of the real world use cases. reply nuc1e0n 10 minutes agorootparentLarge CSV files do occur &#x27;in the wild&#x27;. Whether they should or not is beside the point. Sometimes CSV is the only option to import or export data from ancient &#x27;Enterprise&#x27; horror systems. Excel&#x27;s CSV support has been demonstrated to not be fit for the purpose, as one of the other commenters here points out. reply oneeyedpigeon 40 minutes agorootparentprevYou&#x27;d hope so, but the UK government used Excel to manage some COVID data which it then lost because there were too many rows (65k+) for the format to handle. reply Exuma 10 hours agoprevI&#x27;ll give this a shot.Heres a link to a big CSV if anyone else looking for one to test: https:&#x2F;&#x2F;github.com&#x2F;datablist&#x2F;sample-csv-files&#x2F;raw&#x2F;main&#x2F;files... reply tboerstad 2 hours agoparentHere&#x27;s a complementary tool for creating (scatter) plots of the CSV-file, note that this the 100k rows example from your link:https:&#x2F;&#x2F;csvplot.com&#x2F;remote_file.html?url=https:&#x2F;&#x2F;media.githu... reply jay-barronville 9 hours agoparentprevThank you, but would you mind updating your link to one that doesn&#x27;t automatically trigger a ZIP file download? reply jay-barronville 9 hours agorootparentActually, nevermind, here&#x27;s the link: https:&#x2F;&#x2F;github.com&#x2F;datablist&#x2F;sample-csv-files&#x2F;blob&#x2F;main&#x2F;file... reply cameldrv 9 hours agoprevLooks interesting. Also worth checking out (which I use) is Visidata, similar to this but TUI. reply juunpp 8 hours agoprevWhy does the personal license not have statistics and analysis? Those are useful features currently tied to a business license, but a personal user may not care much for having their license transferable. reply another-dave 2 hours agoparentIn fairness the business pricing isn&#x27;t really out of reach for an individual (assuming US-based tech worker).Think it&#x27;s perfectly fine to tier features and make some stuff gated at a higher price.Especially when you consider that it&#x27;s a one time purchase and also it&#x27;s an editor for a standard that will never change (you won&#x27;t have to upgrade in a year to support features in the latest version of CSV) reply MichaelZuo 5 hours agoprevFYI there may be a bug with the mac version on Mojave.I tried testing it out just now by copying a 5 million row by 14 column selection of random data, ~620MB, and it completely freezes up right after hitting command-c on my 2019 macbook pro 15. Even after waiting minutes, it&#x27;s still frozen. reply ryzvonusef 4 hours agoprevWhy does Excel autoformat data from a CSV? what is the rationale behind this? How hard it is to have an option that says \"open with no formatting\"? reply Simran-B 2 hours agoparentDepending on whether you open a CSV file with Excel or import it into it, different things work and don&#x27;t work. This has been a problem especially on a Windows with a locale that uses a different separator, e.g. semicolon instead of comma on a German Windows because the comma is used in numbers as decimal separator.Unicode encoding or rather anything non-ASCII is also prone to cause trouble. The automatic conversion of quoted(!) digit strings to numbers can be worked around by appending a tab character. Line breaks within a cell are a big headache, however, and I believe the U+2028 code point can still not be used instead. reply mbreese 3 hours agoparentprevYou can import the CSV file instead of opening it, which lets you choose the data type for each column. It&#x27;s clunky, but it does what you want (if you set the types correctly).Excel is a known issue for many text data files. My least favorite Excel-ism is when it changes the names of genes to dates. Many genes recently changed names to avoid Excel-related issues. As an example: OCT4 became POU5F1, and SEPT1 became SEPTIN1. Otherwise, if you&#x27;re looking at gene related data, you&#x27;d have to be very careful when looking at gene names that you didn&#x27;t accidentally save the file with a few \"dates\" as opposed to gene names. (Which is even more confusing when you realize that dates are stored as integers in Excel). reply ryzvonusef 2 hours agorootparentI was doing a coursera project and it kept raising duplicates from what I thought were random strings of numbers and characters and therefore should be unique, took me a while to figure out what was happening:https:&#x2F;&#x2F;sites.google.com&#x2F;view&#x2F;ryzvonusef&#x2F;processWasted a week&#x27;s worth of effort, but I learnt a valuable lesson. reply pbhjpbhj 2 hours agorootparentSpoiler: Data read into Excel&#x2F;Google sheets was interpreted as scientific notation, this caused uniqueness errors. reply blitzar 2 hours agorootparentprevMy least favorite Excel-ism is when it changes correct dates to american dates.https:&#x2F;&#x2F;xkcd.com&#x2F;1179&#x2F; reply m_0x 9 hours agoprevIf I buy a personal license can I install it multiple times in different computers? Or do I need to transfer my license from computer to computer? reply evanem 9 hours agoparentYes, you can use it on as many machines as you want and there&#x27;s no limitation on platforms either. reply tambourine_man 8 hours agoprevInteresting. Where does it save column size changes, since you can&#x27;t add that to the absurdly simple CSV “format” (specification)? In metadata? Does it lose this settings if you change the file name behind its back? Or it doesn&#x27;t try to save such things at all? reply evanem 8 hours agoparentThere&#x27;s a setting to automatically fit the column widths based on the contents and it&#x27;s turned on by default. You can turn it off and manually change the sizes, but they&#x27;ll be lost once you close the program. If you change the file name externally, it&#x27;ll assume the current file has been deleted. You can change the file name within the program with the Rename File command. reply simondotau 8 hours agoparentprevSurely all of those settings can be inferred from the file upon opening? Presumably the only settings you actually care about are ones that have predictable consequences for the saved document. reply lockyc 8 hours agoprevI didn&#x27;t click with ModernCSV and it lacked features. I&#x27;ve since found Easy CSV Editor https:&#x2F;&#x2F;vdt-labs.com&#x2F;easy-csv-editor&#x2F; It has been very good reply lemper 5 hours agoprevlately i&#x27;ve been doing things on aws cloudfront. one annoying thing, though, the unmodified format of the file cannot be opened as is by many csv viewers. for example, in excel, import from csv file won&#x27;t detect the columns properly because of the first line (#version1.0).when i open it using modern csv, the header is detected as a single column while the rest of the data is being read just fine.using it on windows, though. reply xeromal 10 hours agoprevI&#x27;ve used the modern csv freeware to open 3Gb+ size CSVs and it does wonderfully well. I might just buy a license now. lol reply SamuelAdams 9 hours agoparentThis is good to hear, I usually use VI to open anything > 2 GB. A modern UI that doesn’t hang or crash on those files would be nice. reply xeromal 8 hours agorootparentIt really is good and you can actually have multiple large files loaded into it. Not sure how it manages that but very very good.Just to give you some info, I just opened a csv that&#x27;s 3.5 GB and has about 1.6 million rows. It took about 30-40 seconds to load nad it&#x27;s only using 215 MB of ram. I have managed to crash it a few times but I think it&#x27;s understandable considering the huge amount of data it&#x27;s dealing with. reply chambers 10 hours agoprevLove the idea of this. Is there a feature to support views or linked records from other CSV files?Basically, my team is curious if there&#x27;s a smaller, lighter, local-alternative to Airtable, which we can collaborate on with Git. It&#x27;s a big ask and maybe not on your roadmap, but something that is on our minds. reply djbusby 10 hours agoparentHave you tried the trick where you put CSV into SQLite?I do that. Many, maybe linked, CSVs. Load all into SQLIte, query, filter, sort, etc.In the repos to process we have only scripts to load and filter. The SQLite things are short lived.Also, memory only SQLite FTW! reply TristanBall 9 hours agorootparentDuckdb does this straight from csv. You can treat them pretty much just like un-indexed tables, including queries, joins and output back to csv or parquet. itsReally nice. reply evanem 9 hours agoparentprevThat has not been on my roadmap, but I&#x27;ll think about it. It sounds like it may have to be a separate product, not that that&#x27;s a bad thing. reply jdoconnor 10 hours agoparentprevhttps:&#x2F;&#x2F;support.getgrist.com&#x2F;self-managed&#x2F; may suit your needs. reply chambers 9 hours agorootparentGrist was one of the first tools we looked at but it wasn&#x27;t the desktop-first app my team was hoping for. Self-hosting a service felt like a little too much upkeep, on top of all the other systems we own. I recall the data was also not git diff-able. reply jdoconnor 22 minutes agorootparentThere&#x27;s an electron app that doesn&#x27;t require hosting at all. It&#x27;s a desktop solution. https:&#x2F;&#x2F;github.com&#x2F;gristlabs&#x2F;grist-electron reply bnj 9 hours agorootparentprevDesktop first made me think of Superintendant[0] which I’ve enjoyed using0: https:&#x2F;&#x2F;superintendent.app&#x2F; reply alanvillalobos 10 hours agoparentprevGit based database? Sounds super interesting. Are you guys actively looking for something or is it a nice to have? reply TylerE 7 hours agorootparentMany moons and a few jobs ago, I rolled an in-house Wiki in python, that was basically a very thin web UI over mercurial. Each page was a checked-in plain text file (I think some sort of markdown flavor, but this was circa 2010, a totally different era!), each edit was a revision&#x2F;commit. Things like article history just shelled out to mercurial. \"Querying\" was down via filesystem operations. Rolled the whole thing in a day and a half or so - less than I&#x27;d already spent trying to get a MediaWiki instance stood up! reply chambers 10 hours agorootparentprevMore like a git-based log than database. We use it for requests and questions that come up on our slack. We figured out how to extract threads from slack, transform them into CSV files and then load them into Airtable.Airtable has a great UI but the data becomes isolated from our ETL script and it means we have to re-download and export to CSV to use in other contexts.It&#x27;d be great if we could load a CSV into a desktop editor, and then have it store certain views&#x2F;linked records to other CSV files in a local \"db.json\" file. reply caseyf7 11 hours agoprevThis is a great tool. I’ve found it to be a great way to open large csv files that crash most gui programs. It’s now my favorite way to open csvs outside of the terminal. reply justinator 8 hours agoprevOh God I thought you were writing, \"Modern CVS\" and I thought: Noooo just let it die!Good luck on your project! reply emadda 9 hours agoprevIt looks cool.What are the benefits over converting the CSV to a SQLite DB and using a DB GUI? reply winrid 9 hours agoprevWhat UI framework are you using? It kind of reminds me of libgdx + vis-ui. reply evanem 9 hours agoparentQt 6 with C++. reply ChrisNorstrom 3 hours agoprevPaying customer here. I LOVE Modern CSV, I&#x27;ve been using it for massive CSV files that have 15+ million rows of data. I (same username Chris Norstrom) was the guy that emailed a bug fix on a large CSV with UTF-8 faulty characters I was working with.I was wondering if you had added a feature to make selecting and deleting massive amounts of rows&#x2F;collumns easier.Currently: I have a CSV with 5 million rows, I want to delete all rows that have \"&#x2F;gallery&#x2F;\" in collumn B. I have to filter that row then I have to select ALL 2 million rows and delete them. But that&#x27;s so many rows that ModernCSV gets stuck so I have to delete 5,000 or 10,000 at a time if I can remember correctly. And once it starts deleting there is no message, progress bar, or sound notification when it gets done. On very large CSVs this takes forever.Request: I wish I can somehow ask ModernCSV to \"delete all rows\" if \"collumn B\" contains the string \"&#x2F;gallery&#x2F;\" and just click \"RUN\" then I can walk away and go work on something else until a sound plays and I know it&#x27;s done.Maybe you have something similar already implemented? reply SamuelAdams 9 hours agoprevIs this downloadable via HomeBrew? reply Terretta 7 hours agoparentnext [–]brew install --cask modern-csv reply frederikb 1 hour agorootparentUnfortunately this fails for me: curl: (6) Could not resolve host: t6a3m9g6.rocketcdn.me reply greazy 10 hours agoprevHow does it handle dates? reply evanem 9 hours agoparentIt can convert date and time formats. It can also write down the current date&#x2F;time in whatever format you want. reply 11 hours agoprev[deleted] dheera 9 hours agoprev [–] While we&#x27;re inventing a CSV 2 standard, can we please get rid of the weird quirks like allowing newline literals inside quoted values? It makes parsing about 5 billion times harder because you cannot use the readline() functions available in various languages to parse CSV.Newlines in values should be explicitly escaped, e.g. foo,2.0,bar,this\\nis\\na\\ntest,blah replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The developer of Modern CSV, Evan, has released version 2 of the tabular file editor/viewer for Windows, Mac, and Linux.",
      "This new version introduces several improvements including an enhanced UI/UX, improved performance, additional features, updated documentation, and native support for Apple Silicon.",
      "Users are encouraged to try out the new version and provide feedback to the developer."
    ],
    "commentSummary": [
      "Modern CSV version 2 has been released for Windows, Mac, and Linux, offering a powerful tabular file editor/viewer that enhances CSV file handling.",
      "The update comes with an improved UI/UX, better performance, more features, updated documentation, and support for Apple Silicon.",
      "Users have recommended rebranding and providing clearer software purpose while discussing alternative options, expressing frustrations with Excel, and raising concerns about communication and delayed updates."
    ],
    "points": 247,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1692138909
  }
]
