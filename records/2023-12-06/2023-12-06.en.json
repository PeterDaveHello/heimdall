[
  {
    "id": 38531759,
    "title": "Beeper Mini: Android Gets iMessage with End-to-End Encryption",
    "originLink": "https://www.beeper.com/",
    "originBody": "Hi HN! I’m proud to share that we have built a real 3rd party iMessage client for Android. We did it by reverse engineering the iMessage protocol and encryption system. It&#x27;s available to download today (no waitlist): https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.beeper.ima and there&#x27;s a technical writeup here: https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;how-beeper-mini-works.Unlike every other attempt to build an iMessage app for Android (including our first gen app), Beeper Mini does not use a Mac server relay in the cloud. The app connects directly to Apple servers to send and receive end-to-end encrypted messages. Encryption keys never leave your device. No Apple ID is required. Beeper does not have access to your Apple account.With Beeper Mini, your Android phone number is registered on iMessage. You show up as a ‘blue bubble’ when iPhone friends text you, and can join real iMessage group chats. All chat features like typing status, read receipts, full resolution images&#x2F;video, emoji reactions, voice notes, editing&#x2F;unsending, stickers etc are supported.This is all unprecedented, so I imagine you may have a lot of questions. We’ve written a detailed technical blog post about how Beeper Mini works: https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;how-beeper-mini-works. A team member has published an open source Python iMessage protocol PoC on Github: https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush. You can try it yourself on any Mac&#x2F;Windows&#x2F;Linux computer and see how iMessage works. My cofounder and I are also here to answer questions in the comments.Our long term vision is to build a universal chat app (https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;were-building-the-best-chat-app-on). Over the next few months, we will be adding support for SMS&#x2F;RCS, WhatsApp, Signal and 12 other chat networks into Beeper Mini. At that point, we’ll drop the `Mini` postfix. We’re also rebuilding our Beeper Desktop and iOS apps to support our new ‘client-side bridge’ architecture that preserves full end-to-end encryption. We’re also renaming our first gen apps to ‘Beeper Cloud’ to more clearly differentiate them from Beeper Mini.Side note: many people always ask ‘what do you think Apple is going to do about this?’ To be honest, I am shocked that everyone is so shocked by the sheer existence of a 3rd party iMessage client. The internet has always had 3rd party clients! It’s almost like people have forgotten that iChat (the app that iMessage grew out of) was itself a multi-protocol chat app! It supported AIM, Jabber and Google talk. Here’s a blast from the past: https:&#x2F;&#x2F;i.imgur.com&#x2F;k6rmOgq.png.",
    "commentLink": "https://news.ycombinator.com/item?id=38531759",
    "commentBody": "Beeper Mini – iMessage client for AndroidHacker NewspastloginBeeper Mini – iMessage client for Android (beeper.com) 1276 points by erohead 17 hours ago| hidepastfavorite760 comments Hi HN! I’m proud to share that we have built a real 3rd party iMessage client for Android. We did it by reverse engineering the iMessage protocol and encryption system. It&#x27;s available to download today (no waitlist): https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.beeper.ima and there&#x27;s a technical writeup here: https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;how-beeper-mini-works.Unlike every other attempt to build an iMessage app for Android (including our first gen app), Beeper Mini does not use a Mac server relay in the cloud. The app connects directly to Apple servers to send and receive end-to-end encrypted messages. Encryption keys never leave your device. No Apple ID is required. Beeper does not have access to your Apple account.With Beeper Mini, your Android phone number is registered on iMessage. You show up as a ‘blue bubble’ when iPhone friends text you, and can join real iMessage group chats. All chat features like typing status, read receipts, full resolution images&#x2F;video, emoji reactions, voice notes, editing&#x2F;unsending, stickers etc are supported.This is all unprecedented, so I imagine you may have a lot of questions. We’ve written a detailed technical blog post about how Beeper Mini works: https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;how-beeper-mini-works. A team member has published an open source Python iMessage protocol PoC on Github: https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush. You can try it yourself on any Mac&#x2F;Windows&#x2F;Linux computer and see how iMessage works. My cofounder and I are also here to answer questions in the comments.Our long term vision is to build a universal chat app (https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;were-building-the-best-chat-app-on). Over the next few months, we will be adding support for SMS&#x2F;RCS, WhatsApp, Signal and 12 other chat networks into Beeper Mini. At that point, we’ll drop the `Mini` postfix. We’re also rebuilding our Beeper Desktop and iOS apps to support our new ‘client-side bridge’ architecture that preserves full end-to-end encryption. We’re also renaming our first gen apps to ‘Beeper Cloud’ to more clearly differentiate them from Beeper Mini.Side note: many people always ask ‘what do you think Apple is going to do about this?’ To be honest, I am shocked that everyone is so shocked by the sheer existence of a 3rd party iMessage client. The internet has always had 3rd party clients! It’s almost like people have forgotten that iChat (the app that iMessage grew out of) was itself a multi-protocol chat app! It supported AIM, Jabber and Google talk. Here’s a blast from the past: https:&#x2F;&#x2F;i.imgur.com&#x2F;k6rmOgq.png. bogwog 14 hours agoThis seems like it won&#x27;t last, but it&#x27;s AWESOME and I really hope you survive Apple&#x27;s inevitable attempts to kill this. A universal chat application would be amazing, and will maybe help bring attention to the value of standards and interoperability (hopefully by governments&#x2F;regulators). reply wslh 12 hours agoparentOne of my companies lives from this kind of things so it would last if someone could fund it. More food for thought: \"Reflecting on 16 Years of Work on Adversarial Interoperability\" (now, more than 20...) [1][1] https:&#x2F;&#x2F;blog.nektra.com&#x2F;2020&#x2F;01&#x2F;12&#x2F;reflecting-on-16-years-of... reply smashah 9 hours agorootparentHave you ever received C&D for your work? There&#x27;s a big problem of OSS projects being TOS-trolled by billion dollar companies and having to shut down out of fear. reply wslh 8 hours agorootparentYou know, your question is very interesting: no, we didn&#x27;t.Anecdote: we reverse engineered several Microsoft products and before Microsoft Windows 7 launch we were contacted by Microsoft QA and they offered us support to check if our software was compatible with it! BTW, our software was installed in millions of computers around the globe. For example, Trend Micro used our software for supporting their antivirus in Outlook Express and Windows Mail.Our Deviare Hooking Engine [1] was eclipsed when Microsoft Detours [2] turned to an MIT license and free. Even when our was superior in several ways. This is why I wrote that you should continuously fight for \"adversarial interoperability\".[1] https:&#x2F;&#x2F;github.com&#x2F;nektra&#x2F;Deviare2[2] https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;detours&#x2F; reply smashah 8 hours agorootparentI agree. After receiving a C&D from Meta for my OSS project (along with some other maintainers from some other projects) I strongly believe adversarial interop is a basic digital right that is required to fulfil the broken or revoked promises of web 2.0If you know anybody that can help please let me know because I want to get back to maintaining the project. reply wslh 8 hours agorootparentDid you contact specific organizations such as FSF, EFF, etc and&#x2F;or specialized lawyers? There were well known people defending itself or being plaintiffs. For example, https:&#x2F;&#x2F;cr.yp.to&#x2F;export.html reply sneak 8 hours agorootparentprevWhat would they demand they cease doing? Publishing software?If the use of this software is against their rights in some way, the end users running it would be the ones in violation. Publishing original software is protected expression. reply lxgr 5 hours agorootparentOne prominent counterexample to this thesis is DRM circumvention software, which regularly gets taken down via DMCA notices. I wouldn&#x27;t be surprised if Apple even invokes that particular law. reply berkes 4 hours agorootparent\"Section 1201 provides for felony liability for anyone commercially engaged in bypassing a DRM system: 5 years in prison and a $500,000 fine for a first offense.\"So it&#x27;s even worse than the risk of being taken down. Way worse.https:&#x2F;&#x2F;www.eff.org&#x2F;deeplinks&#x2F;2017&#x2F;10&#x2F;drms-dead-canary-how-w... reply crystaln 23 minutes agorootparentiMessage is not DRM. It is not protecting IP. reply graphe 7 hours agorootparentprevThis was tried by apple. https:&#x2F;&#x2F;www.engadget.com&#x2F;apple-drops-its-lawsuit-against-a-m... reply sneak 7 hours agorootparentEmulation software isn’t wholly original as it needs firmware and software from the device so emulated. An iPhone emulator with no bootrom and no iOS isn’t very useful.An open source client for an API need not include any non-original works. reply lxgr 5 hours agorootparentDepends on whether you consider a private key an \"original work\": https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;main&#x2F;albert.py#L16The situation seems very similar to the AACS key leak back in the day: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;AACS_encryption_key_controvers... reply comex 4 hours agorootparentNote that a key cannot be copyrighted, but it can be considered a circumvention tool for access controls that protect other copyrighted works. reply graphe 6 hours agorootparentprevThey are though. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;context?id=38534247 reply captn3m0 5 hours agorootparentprevThere is a very useful iPhone emulator with no bootrom and no iOS: https:&#x2F;&#x2F;touchhle.org&#x2F;It targets games, so manages to be useful without having to emulate or re-implement the majority of the OS. reply p-e-w 6 hours agorootparentprev> Publishing original software is protected expression.That means Jack Shit in a world where a lawsuit can ruin a person&#x27;s life regardless of its legal merit, with zero consequences for the corporation that filed it even if it gets tossed out by a judge eventually.LPT: Live as if human&#x2F;constitutional rights didn&#x27;t exist. Because if push ever comes to shove, you will quite possibly find that they indeed don&#x27;t exist in practice. reply __MatrixMan__ 5 hours agorootparentprevTell that to Alexey Pertsev. reply chipgap98 14 hours agoparentprevI saw an article on the topic where the reporter spoke with Beeper&#x27;s CEO, Eric Migicovsky. He seems to believe that blocking Beeper might cause problems for legitimate Apple user&#x27;s.Obviously that outcome is something he wants, but I still think its interesting.[0]: https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;12&#x2F;5&#x2F;23987817&#x2F;beeper-mini-imes... reply hedgehog 14 hours agorootparentApple maintains iMessage compatibility with devices that are long out of support, if Beeper Mini is sufficiently similar to the client in for example iOS 12 then it makes an Apple decision to break Beeper fairly expensive. Even if they do the work to publish iMessage updates for the old iOS versions it just buys a little time before the new version gets reverse engineered, and that at the cost of poor user experience for the people with those devices in a form they will directly blame on Apple. Given all that I suspect he&#x27;s right. reply lxgr 13 hours agorootparent> Even if they do the work to publish iMessage updates for the old iOS versions it just buys a little time before the new version gets reverse engineeredThere&#x27;s probably a cliff in complexity. Once Apple starts requesting signed attestations from the secure enclave on the devices that have one, it&#x27;s game over.They probably don&#x27;t just yet, since still too many people use iMessage on first-party clients that don&#x27;t have one, e.g. Intel laptops without a T1 or T2. reply ttul 13 hours agorootparentIf Apple does start enforcing signed attestations, they will say that it&#x27;s to reduce abuse. I have no doubt (being in the anti-abuse world) that spammers and phishing gangs will immediately begin using Beeper to spam iMessage users because this allows them to avoid buying an iOS device. With end-to-end encryption, Apple may also decide to roll out privacy-protecting client-side spam and phishing detection, which would IMHO be a really great thing. reply sounds 11 hours agorootparentThe phone number registration https:&#x2F;&#x2F;blog.beeper.com&#x2F;i&#x2F;139416474&#x2F;sending-and-receiving-me... will make it possible to enforce legal action against malicious and spammy messages.Note that iPhones already receive SMS spam and fraud just like every other phone.However, you are correct that the blue bubble is no longer a guarantee that the bad actor is using an iPhone. reply lxgr 8 hours agorootparent> The phone number registration https:&#x2F;&#x2F;blog.beeper.com&#x2F;i&#x2F;139416474&#x2F;sending-and-receiving-me... will make it possible to enforce legal action against malicious and spammy messagesLike the legal action that is currently protecting us from robocalls?I don’t know if iMessage registration requires bidirectional SMS verification, though. If it does, that would be significantly harder to spoof than just caller IDs. reply azinman2 7 hours agorootparentprevThey do receive spam and fraud, but the numbers are orders of magnitude less than every one’s else BECAUSE it’s tied to hardware. I don’t know the details of how’s these guys got around it, but this is bad for the rest of us when phishing skyrockets. reply smt88 4 hours agorootparentI don&#x27;t understand what you&#x27;re talking about. I get far more SMS spam on iOS than I did on Android.Whether the number uses iMessage or not is totally irrelevant. reply solardev 12 hours agorootparentprevMaybe Apple needs an even blue-r bubble to set apart the super attested users from the mere blue bubble peasants reply andygeorge 12 hours agorootparentthey could call it \"apple blue\" and charge a few bucks a month for it. People love that stuff reply cozzyd 10 hours agorootparentThey can desaturate the iPhone &#x2F; MacBook Air users to disambiguate from the MacBook pro &#x2F; iPhone pro &#x2F; max users. Also device age in years will add hints of green hue. That way people know they&#x27;re talking to someone who can afford to spend thousands of dollars on hardware every year. reply reiichiroh 5 hours agorootparentprevI’d pay for a blue Apple checkmark! reply dylan604 12 hours agorootparentprevI imagine the next color being purple since that&#x27;s a sign of royalty. Hail to the king baby! reply JoshTriplett 11 hours agorootparentprev> With end-to-end encryption, Apple may also decide to roll out privacy-protecting client-side spam and phishing detection, which would IMHO be a really great thing.Spam protection should be on the recipient, rather than the sender. reply c0pium 11 hours agorootparentAs we’ve learned very clearly over the last 20 years of commercialization of spam, that never works. The only tractable way to fight fraud and abuse is to impose cost. reply justinjlynn 7 hours agorootparentThe massive prevalence of physical junk mail would refute your argument that even a significant per message cost would dissuade abuse. reply devman0 6 hours agorootparentScope and scale is important here, the amount of junk mail from business interests outside of my immediate region is not very high. If physical mail were free and you could send it from anywhere in the world, junk mail would be so much worse than it is. You couldn&#x27;t run a lot of internet scams at the costs of physical mail and be profitable. reply SoftTalker 4 hours agorootparentProbably not because even if the postage is free the paper, printing, envelopes, etc. are not. reply mkingston 11 hours agorootparentprevThat&#x27;s a brief statement which makes me think I&#x27;m missing something obvious, but it doesn&#x27;t seem obvious to me. Would you please expand on that? reply JoshTriplett 11 hours agorootparentI think it&#x27;s a bad idea to lock out unattested clients, and as long as third-party clients are accepted, spam will always be sendable. If you&#x27;re not doing end-to-end encryption, you can catch it at send time by having the server reject the client for sending spam. If you&#x27;re doing end-to-end encryption, the only options are the sender or the recipient, and attempting to block it at the sender would require prohibiting interoperability. reply nl 8 hours agorootparentWhile I love the principle of accepting third-party clients, Apple clearly doesn&#x27;t which make this argument fairly non-compelling for them. replyjonhohle 11 hours agorootparentprevThere’s also the registration process that could be locked down and&#x2F;or hardened. There may or may not be additional metadata (including out of band) that could identify first-party clients.I would think that’s the biggest issue right now. If spammers can register “real” iMessage accounts at scale without Apple hardware, Messages becomes less pleasant, very quickly. reply nroets 12 hours agorootparentprevApple can break Beeper without relying on the secure enclave: If Apple devices just send their serial number (IMEI for their GSM products), their servers can refuse to talk to hardware they didn&#x27;t manufacture. reply remram 12 hours agorootparentNon-Apple devices could just lie reply nroets 4 hours agorootparentBeeper will know only a small number of valid serial numbersIf it ever becomes popular, there will be a lot of duplicate serial numbers. That&#x27;s easy to detect and ban. reply WheatMillington 12 hours agorootparentprevHow does this address iMessages sent from non-iPhone devices? reply HenryBemis 11 hours agorootparentIf in the data sent across (via Apple servers) the IMEI and serial no of the device are also transmitted, then Apple can in that millisecond query on their various lists&#x2F;inventories that this device is legit (activated device + IMEI + serial) and if all lights are green, proceed to deliver, otherwise drop it.(perhaps different sets of data can be used, but it must be something that Apple already has, and the user has already provided (i.e. the iMessage email or the iMessage phone number, from the iPhone&#x27;s enabled Settings) reply nl 8 hours agorootparentAs someone who once bought fake airpods on ebay, I can tell you that Apple can&#x27;t do this.I spent a number of days with them where they were trying to work out if they were fake. The serial number was real but they were fairly sure the number had been taken from a real product and reused, but were unable to say for sure.I ended up just returning them (because of the ebay return window) but found it interesting that Apple couldn&#x27;t easily check this, and was very aware of the issue. reply rfoo 47 minutes agorootparentprevDo you mean that I now have a nice party trick - DoSing friends iPhone from sending iMessage? :) reply fragmede 33 minutes agorootparentIf you have a FlipperZero, DoSing iPhone users with Bluetooth is a bit of fun! reply collegeburner 11 hours agorootparentprevyou already have to do this to get certain apple services (including imessage) working on hackintoshes. turns out there&#x27;s a really easy work-around: guess-and-check serial numbers on apple&#x27;s web site until one works. they rate limit it a bit but you can usually find a working one without a terrible amount of hassle. replyafavour 14 hours agorootparentprevThat would make sense: because Apple have deeply coupled iMessage to the OS they can’t simply roll out a new version of the app with protocol changes that would block Beeper, they’d have to release entire OS updates.No matter the method it would be a scorched earth approach. I suspect the number of people actually using Beeper will be far below a rounding error for Apple. reply lxgr 13 hours agorootparentNon-Apple legitimate users aren&#x27;t the only concern for Apple: Once third-party clients are readily available, this makes spam much harder to filter.Right now they can probably just ban known-spam-originating devices, which is much more effective than banning iCloud accounts since there is a much higher cost to the spammers. reply dylan604 12 hours agorootparentprevYou say this like Apple doesn&#x27;t release OS updates. Why are you putting that as some arbitrary limiter to what Apple could do to protect its walled garden? reply nebula8804 14 hours agorootparentprevUptake for OS updates is very high on iOS though right? I heard a while back that it is like 90+% in 6 months. (could be totally wrong on that can someone confirm?) reply pashky 12 hours agorootparentUptake of updates is, uptake of devices isn’t. Here I have 1st gen retina iPad from 2012 which is on the latest iOS available for it - 9.3.5 (from 2016, current version is 17.1.2). As of today FaceTime and iMessage still work perfectly fine.That and reading the books is actually about the only thing it can do right now. reply afavour 14 hours agorootparentprevThere’s a ton of devices out there unable to upgrade to the latest iOS. Obviously you can release point upgrades for old versions but I do wonder what the uptake of those is like. I’d wager there are a ton of very old iOS devices out there. At the very least many more than there are potential users of Beeper. reply dylan604 11 hours agorootparentanecdote of 1, but i have a 6S+ that is kept up with any updates it receives which is 15.8. there maybe some devs that have older devices that they intentionally keep at even older versions, but if someone is using an old iDevice as a daily driver, they&#x27;re probably still more likely to run the updates. at least, that&#x27;s my reaches up and grabs for an opinion reply crystaldecanter 4 hours agorootparentprevwill iMessage Contact Key Verification coming in iOS 17.2 break Beeper — or just make it super annoying like the “not a genuine Apple part” warning when replacing a screen or battery reply jotux 13 hours agorootparentprevI&#x27;m not that familiar with ios apps, can they not push out updates to individual apps? reply threeseed 12 hours agorootparentOn iOS many of the individual apps e.g. Mail, Notes you can delete and then re-download from the App Store.And as part of Security Updates they have patched vulnerabilities just in the relevant apps.So there is nothing technical stopping them. It&#x27;s just been customary to treat iOS as a product where all features ship together. reply lxgr 6 hours agorootparentI don’t think this actually physically deletes the app, given that it’s back once you reset the phone. It’s most likely just hidden&#x2F;deactivated until you “reinstall it from the app store”.Actual updates require the app binary&#x2F;bundle to be mutable. reply saagarjha 11 hours agorootparentprevApple never patches security vulnerabilities in individual apps except for Safari, and they’ve stopped doing that too. reply matthew-wegner 13 hours agorootparentprevNot the OS-included ones, afaik. Some Apple apps are through the AppStore normally, which can be updated independently (i.e. TestFlight, despite its deep hooks). reply philsnow 13 hours agorootparentWhy did google break out Google Play Services as a separate app, was that when they started integrating more with third-party android phone suppliers, and they didn&#x27;t want to have to wait for OS upgrade cycles from slower-moving companies? reply derefr 13 hours agorootparentProbably they originally did it because Android has high-assurance embedded use-cases (compare&#x2F;contrast: Windows IoT Core) where you want to strip out everything possible from the attack surface.But mainly it&#x27;s because base Android (AOSP) can be arbitrarily modified by the OEM; and Google doesn&#x27;t want to have to trust installations of Google Play Services that have been arbitrarily modified by OEMs.(Especially because those versions would likely all act differently-enough from one-another that they would be forced to loosen their server-side, network-traffic-fingerprint-based \"authentic Android device\" detection that allows them to ignore&#x2F;block bots pretending to be Android devices.)By shipping Google Play Services through the store, they can ensure that, on devices that run it, it&#x27;s exactly the same code for every device that runs it, with no OEM alterations. (And they can also include various checks to reject devices that would try to alter that code at load time. This is the real reason why e.g. Huawei devices are blocked from using Google Play Services — they try to patch unspecified parts of the Play Services code while loading it, \"breaking the integrity of the platform\" from Google&#x27;s perspective.) reply riversflow 10 hours agorootparentMan, that&#x27;s contrived. Really its simple: Google seperates out Play Services so they can harvest user data from virtually all Andoid devices. It lets them market Android as OSS while still reaping the benefits of closed source data scraping. reply hedgehog 9 hours agorootparentderefr cited one reason but there&#x27;s another that&#x27;s relevant to this thread: updates. In the Android model handset manufacturers and carriers decide when (or if) to ship updates. Google distributing their apps through the store gives them a way to roll out new features to a reasonable portion of their user base. replythreeseed 12 hours agorootparentprev> because Apple have deeply coupled iMessage to the OSNo they haven&#x27;t. On my Mac it&#x27;s just an app and a reusable framework.There is nothing stopping them releasing it on the App Store similar to Mail. reply afavour 10 hours agorootparent> There is nothing stopping them releasing it on the App Store similar to Mail.In the sense that the app is just a wrapper around a system framework, sure. But changing that framework would be an OS release. reply saagarjha 11 hours agorootparentprevMail is also deeply coupled to the OS. The app itself does very little. reply afavour 12 hours agorootparentprevI’m talking about the iPhone. reply threeseed 12 hours agorootparentMessages is the same on OSX and iOS.It&#x27;s not deeply integrated into the iOS by any normal definition. It&#x27;s just shipped together. reply chatmasta 12 hours agorootparentMessages has a bunch of special privileges on iOS, which is why they had to add the whole Blastdoor protection framework and why it&#x27;s such a juicy target for sandbox escape exploits. reply saagarjha 11 hours agorootparentNope. It just happens to be on everyone’s device and usually enabled reply chatmasta 11 hours agorootparentYes, and when it&#x27;s enabled it has more privileges than most other apps, doesn&#x27;t it? But yeah you can still remove the app.Btw, maybe related, on iOS I have \"app privacy report\" enabled, to show me a list of apps and the recent entitlements they used. Every Apple app, even those that don&#x27;t need access to them, is shown as having recently accessed my Contacts. I find this weird. Anyone know why they do that? e.g. I&#x27;ve never even used the Health app and yet it&#x27;s accessing my Contacts for some reason. reply saagarjha 3 hours agorootparentIt’s basically the same as any other app, there are some special permissions it has to integrate with the OS a bit better but nothing too interesting. Not sure what’s going on with Contacts but it might be a bug? reply lotsofpulp 12 hours agorootparentprevThe Messages app in macOS is less capable than the Messages app in iOS. It cannot even edit sent messages. reply a_carbon_rod 11 hours agorootparentIt can, by right clicking the desired message to edit. This is in macOS Sonoma, and I believe was a part of Ventura as well. reply lotsofpulp 11 hours agorootparentOh interesting, I have a 2015 MacBook Air. Wonder if the feature is not available on whatever macOS version I have. reply SllX 11 hours agorootparentIt’s a Ventura and later feature and your MacBook Air probably topped out around Monterey or earlier. 2016 MacBooks Pro also didn’t make the cut for Ventura. reply andygeorge 12 hours agorootparentprevfwiw it hasn&#x27;t been called \"OSX\" for awhile now replymeesles 14 hours agorootparentprevIt&#x27;s not too hard to think through -They would need to accept and verify a flag from messages that the copycats can&#x27;t reproduce. At the very least that would require a client update from anyone using official iMessage clients, which covers many millions of devices.Unless they&#x27;re able to hook into already existing flags&#x2F;keys on the devices since they already verify application signatures and a whole other host of things.Apple can probably do it, but much like jailbreaking how fast can they release breaking changes? reply IshKebab 3 hours agorootparentThey could probably require a new check but whitelist already registered numbers. reply mattgreenrocks 14 hours agorootparentprevWhat&#x27;s brilliant is they get press either way this goes down. reply dylan604 11 hours agorootparenti understand no such thing as bad news&#x2F;publicity, but if the 800lb gorilla squashes the little guy, then that&#x27;s some pretty bad news. with the recent Twitt...er,X and reddit debacle with 3rd party apps, that 800lbs is pretty powerful when it wants to beedit, because i used the wrong turn of phrase reply Rygian 13 hours agoparentprevOn the contrary, the EU law that enforces interoperability should put some wind under this project&#x27;s wings. reply hansoolo 10 hours agorootparentExactly what I thought reply jackjeff 2 hours agorootparentBut EU interoperability laws are cancelled out by DMCA (aka EUCD). That’s why reading a DVD with VLC has always been “technically” illegal.If Apple is able to update the protocol in such a way that it requires some kind of signed attestation from the secure enclave (basically a DRM) they’ll get legal protection.Also. Nobody uses iMessage in the EU. It’s all WhatsApp here. Blue bubbles are an American obsession. reply geraldhh 13 hours agorootparentprevit might even be the reason for it&#x27;s existence reply cavisne 6 hours agoparentprevThe timing is potentially clever. Apple has committed to supporting RCS next year, and will face regulatory pressure in places like Europe.Even if short lived they could onboard a lot of Android users and then use RCS once it’s supported. reply toastal 2 hours agoparentprevWe had universal chat applications & standards and interoperability in the 2000s. Pidgin (et al.) + libpurple allowed users to use a singular application for chat--even the proprietary protocols. We also had (& still have) XMPP from that era which many of the big boys like Google jumped on, killed, then jumped off (EEE?). Are we just repeating history (https:&#x2F;&#x2F;ploum.net&#x2F;2023-06-23-how-to-kill-decentralised-netwo...)? There’s an XKCD about inventing yet a new standard despite us having good ones for decades… reply superduty 12 hours agoparentprevWhat is currently not interoperable between the majors mobile OS makers? reply rezonant 11 hours agorootparentWell messaging for one thing...Some others:- Find my device features including Bluetooth ping networking (airtags, Tile, Android&#x27;s upcoming network)- Airdrop&#x2F;Nearby Share- Bluetooth LE proximity pairing (at least I doubt this works when pairing cross ecosystem)- Carplay&#x2F;Android Auto- Airplay&#x2F;Google Cast reply fy20 3 hours agorootparent- Carplay&#x2F;Android AutoAre there any headunits that only support one or the other? The cheap Chinese unit I got last year supports wireless for both. It would be nice to have an open protocol though, so third parties could develop alternative UIs. reply rezonant 44 minutes agorootparentOr so that there isn&#x27;t a duopoly lock in for a new phone OS or an android fork that doesn&#x27;t have Google Play Services on it. Just like Google Cast and Airplay, this should be an open standard, not a pair of incompatible proprietary locked down solutions. reply tdy_err 1 hour agorootparentprevSadly, yeah- even in OEM units. reply windexh8er 4 hours agorootparentprev> Find My &#x2F; AirtagsAnother Apple ecosystem that can be used by non-Apple devices. OpenHaystack [0] has been working well for quite a while.[0] https:&#x2F;&#x2F;github.com&#x2F;seemoo-lab&#x2F;openhaystack reply rezonant 46 minutes agorootparentSee my comment below for why this isn&#x27;t the interoperability I&#x27;m interested in. I don&#x27;t want to use Apple&#x27;s service, I want Bluetooth tag pinging to be standard across Apple, Tile, and Android ecosystem devices so that they all work equally well regardless of the percentage of one brand of phone or another in the area the tag is pinging from. reply nolongerthere 3 hours agorootparentprevDoes this still work? that repository appears to be abandoned. reply fy20 3 hours agorootparentApple did actually open up the network, there are plenty of third party devices that are &#x27;Find My&#x27; compatible. It&#x27;s intended for integration into things like bikes or scooters.You can buy tags from AliExpress for $5 that implement it. I&#x27;ve been using a few for a couple of months, and no issues so far. reply rezonant 49 minutes agorootparentIt would be preferable if Find My capable smart devices could forward tag pings on to non-Apple owners and vice versa. Right now, Find My is strong in the US where iPhones are very common, but it works worse in places where most phones are Android, and vice versa for Androids in the US versus abroad.You are referring to being able to track devices via the Find My portal on Apple.com or your Apple devices, but I am referring to being able to merge the networks so that Apple devices will forward pings onto Android&#x27;s Find My network and vice versa. reply mritzmann 3 hours agorootparentprev> that repository appears to be abandonedThe last commit and release is from october. reply collegeburner 11 hours agorootparentprevokay but this \"interoperability\" is legitimately hard without degrading the user experience because apple&#x27;s unique level of control allows it to produce a superior product with more consistency. airdrop is best-in-class; open-source solutions like wi-fi direct are dumpsterfires with trash UX. LE proximity pairing is, i believe, a custom chip apple put in airpods (h1 chip) because bluetooth is stuck in 2005 and still doesn&#x27;t have easy pairing, full quality two-way audio, etc. carplay&#x2F;auto have different feature sets and airplay is an objectively easier experience than google cast.the EU is fundamentally interested in these changes regardless of consumer welfare. this is sour grapes because they fail at tech by every conceivable metric and by degrading everything to a common feature set and commoditizing certain standards, they hope to give domestic companies a prayer. that it prevents innovation and improvements is merely a secondary concern for the hard-headed anti-Americans in brussels. reply nhumrich 9 hours agorootparent> apple&#x27;s unique level of control allows it to produce a superior product with more consistencyAnother way to read this: Apple has a superior product because they perform anti-competitive practices and don&#x27;t allow other companies to out-product them. And when they do, they buy them&#x2F;shut them down before anyone is the wiser. reply collegeburner 7 hours agorootparenteditorialization. you know as well as anyone that restricting your feature development to your own platform rather than doing a retarded design by committee helps one innovate faster. reply rezonant 33 minutes agorootparentInnovation is a good thing, but for many items on this list there&#x27;s no more innovation happening. Google Cast and Airplay have been mostly unchanged for the last ten years, and the same is true for Airdrop and Nearby Share.You can definitely make the argument about innovation in the messaging space, but RCS is very extensible. RCS Encryption definitely needs to be standardized, but I recommend you check out how Google layered it on top of RCS [1] including handling fallbacks for corner cases like switching your RCS client away from Google Messages before the system realizes it.This is to say that RCS is pretty flexible, the key is handling the fallback paths in the extension design and working with other vendors to standardize promptly, so we don&#x27;t end up with the same kind of broken mess that the carriers made.[1] https:&#x2F;&#x2F;www.gstatic.com&#x2F;messages&#x2F;papers&#x2F;messages_e2ee.pdf reply rpdillon 6 hours agorootparentprevWe don&#x27;t need to speculate; internal emails from the Epic trial discuss the motivations.https:&#x2F;&#x2F;www.theverge.com&#x2F;2021&#x2F;4&#x2F;27&#x2F;22406303&#x2F;imessage-android...In short, Eddy Cue proposed in 2013 that Apple owning the best-in-class messaging app would be a win, and even mentioned the cost being low. Phil Schiller shut him down, arguing it would remove a barrier preventing iPhone parents from buying their kids Android phones.That reads like anti-competitive motivation to me. In particular, it looks like tying, where two unrelated products are connected artificially. The wikipedia article on anticompetitive behaviors has a section on tying, and mentions another case involving Apple that bears some resemblance involving iPods being artificially restricted to only playing tracks either from iTunes or direct CD rips.So I think the anti-competitive angle has some real merit.The innovation claim, though, I have a harder time with. I don&#x27;t see how releasing Messages for Android implies design-by-committee. They could just release it, like Beeper Mini just did, but without the reverse engineering part. reply zik 9 hours agorootparentprev> apple&#x27;s unique level of control allows it to produce a superior product with more consistencyHonestly, this reads more like marketing spin to cover anti-competitive behaviour than a forum post. reply collegeburner 7 hours agorootparenti am not, nor have i ever been, employed by apple. i use none of their products as my primary devices. stop breaking the forum rules. reply goosedragons 8 hours agorootparentprevHave you used Nearby Share on Android? It&#x27;s IME just as good Airdrop, the only real issue is that it&#x27;s not baked into Windows PCs like Airdrop is with Macs (confusingly MS has their own thing called Nearby Share for Windows devices). I&#x27;ve actually had less issues with Nearby Share, my iPhone stopped sharing to my mini after a few months but could talk to everything else. Android solved BT pairing in a superior way years before with NFC pairing. Touch two things and paired. I could get my airpods to pop up on my iPhone 1&#x2F;10 times. Finnicky, overhyped crap IMO. Only reason NFC pairing didn&#x27;t catch on is Apple holding the NFC chip hostage for the sole use of Apple Pay. reply collegeburner 7 hours agorootparentyes, i primarily use an android phone. nearby share is janky and terrible. additionally, the fact that it&#x27;s not built in everywhere is a ding from the standpoint of an end user. reply Kab1r 10 hours agorootparentprevIt&#x27;s really not. Just make the \"superior product\" interoperable. reply omeid2 9 hours agorootparentBut it often not that simple, anyone who has done cross-platform development can tell you this by heart, it doesn&#x27;t matter what you do, you must adhere to the lowest common denominator. Interoperability isn&#x27;t free. reply Kab1r 9 hours agorootparentI&#x27;m not asking them to implement these things on every platform, but it&#x27;s not difficult to make documentation they certainly already have about protocols available. reply omeid2 2 hours agorootparentProtocols calcify when you don&#x27;t control all the endpoints, consider the case in point, iMessage, it is seems like there is some security implications for spoofing iMessage for any random number, yet, apple&#x27;s recourse is very limited if it can&#x27;t update all the endpoints (devices).The same is also true, say about AirDrop, if apple makes it \"Open\" and they have to make a breaking change for security or whatever reason, they can&#x27;t feasibly even make an update available for non-apple devices let alone enforce it.Now \"Apple\" has broken your non-apple device and along with it their reputation.Open is good, but the cost is non-zero. replydjvdq 12 hours agoparentprev> A universal chat application would be amazingYou mean like WhatsApp, Signal, Telegram and dozens of different chat apps available for both Android and iOS? reply bleachedsleet 12 hours agorootparentBeeper is an app that unifies all the messaging protocols you mentioned (and others) into a single app. They are not introducing another protocol.Universal in this context is referring to the ability to use a single app across protocols rather than the ability to use a single app across platforms. reply jayknight 4 hours agorootparentI miss the days of jabber being able to senselessly talk to aim, msn, yahoo, icq, etc. All chat contacts in one account. reply djvdq 4 hours agorootparentprevOk, I get it.It&#x27;s what EU mandated and from March &#x27;24 all major chat apps have to be able to communicate with each other. reply sohzm 7 hours agorootparentprevRelavent xkcd: https:&#x2F;&#x2F;xkcd.com&#x2F;927&#x2F; reply altairprime 16 hours agoprevThis downloads from GitHub and ’executes’ specific code points in what looks like a proprietary Apple binary, ‘IMDAppleServices’. Where was that binary sourced? Could you provide more context for what is performed at the hard-coded call-in addresses in your code? Does this relate to how you’re presenting a unique device identifier to the network? Do all clients share one identifier, or is it generated per Apple ID? Have any Apple IDs been locked out of iMessage during your development and testing? reply dadoum 15 hours agoparentI am not the developer but I also looked at that binary to help the project at some point.It&#x27;s taken straight from OS X 10.8 (more precisely from an Update Combo on their download portal). It&#x27;s calling NACInit, NACKeyEstablishment and NACSign functions from it (which have no entry points but with reverse engineering the offsets have been figured out). They are themselves relying on OS X system functions to get device information. The Python code is using Unicorn to emulate it and patch the calls to those functions to stubs returning pre computed values from a Mac machine (stored in a data.plist file). All clients are using the same machine identifier. IIRC, nobody did get its account locked but if the Apple ID has not been used at all it might fail (it depends on the donor device that generated data.plist, if it&#x27;s a hackintosh for example it will likely not work). reply viraptor 9 hours agorootparentIf that becomes a problem and they get enough funding, I&#x27;m sure they can spend a few days &#x2F; weeks reverse engineering the functions they need. At this point it just needs some effort, not some crazy research capabilities. reply dadoum 3 hours agorootparentIt’s already in the works, someone has already made a lot of progress on this front on pypush’ Discord server. reply ronsor 7 hours agorootparentprevGiven that these are cryptography-related functions, I feel like symbolic execution could yield the actual algorithm they use. reply viraptor 2 hours agorootparentIt&#x27;s probably not even that hard. If some block looks like a crypto section, you can likely match the relevant constants to the algorithm. It&#x27;s not like Apple will use some super custom solution there. \"Where is the AES and how is the IV generated\" is more likely question. reply stefan_ 14 hours agorootparentprevThat seems like a problem. Emulating the protocol is okayish-to-gray but having the binary there will just be a straight DMCA.Wonder what the actual app is doing since this is just the PoC. reply chatmasta 12 hours agorootparentI don&#x27;t think the finer legal points matter too much. If Apple wants to sue them, they&#x27;ll sue them, regardless of legal merit. And I suspect Beeper is betting they can make their case from a more philosophical angle, such that it&#x27;s irrelevant what grounds Apple cites when suing them. Beeper will fight it either way.I&#x27;m an Apple user who has no need for this app. But I really appreciate that Beeper has the balls to reverse engineer the protocol and build a business around it while fully expecting a lawsuit. That&#x27;s some old school hacker shit and I&#x27;m here for it.Apple tried and failed to sue Corellium for emulating their hardware, and now Corellium has a viable business around it. I don&#x27;t see why Beeper should fare any differently. They just need to be prepared for a fight, both legally (lawsuits) and technically (ongoing game of cat-and-mouse). reply vbezhenar 8 hours agorootparentReverse-engineering and documenting protocol is OK. Implementing protocol according to the documentation is OK.Copying and modifying binary with proprietary license is not OK. reply fragmede 8 hours agorootparentHow do you run the binary if that&#x27;s not OK? In order to install it, The binary gets copied from the installer (dmg&#x2F;zip&#x2F;app store&#x2F;CD install media), and then to run it, it gets copied from your hard drive to RAM, so that&#x27;s clearly okay in some circumstances. Furthermore, once it&#x27;s on my hard drive, I can copy it over and over again in random places on my hard drive for funsies and the operating system will gladly cooperate. Once it&#x27;s on my drive, I can go in with a hex editor and randomly change bytes for funsies. It&#x27;s on my hard drive. Am I then not allowed to delete the program from my system? If I use shred to delete it, which will set the bytes in the file to zero, or format the hard drive, am I breaking the law? reply ketzo 6 hours agorootparentIn my very limited understanding, distribution is the key.It’s legal for Apple to distribute Apple binaries. It is not legal for someone else to distribute Apple binaries.Copying a binary from installer to app folder: not distributionPutting the binary on a USB and giving it to your buddy: gray area, not worth prosecuting, but maybe technically distributionUploading the binary to a GitHub repo titled “Apple binaries here”: obviously distribution reply fragmede 4 hours agorootparentWhich is weird if you think about it. If I buy a car, give it a paint job, mount some LEDs, and a new sound system, I&#x27;m totally within my rights to sell it. I can&#x27;t say that I&#x27;m Ford or Honda when selling this modified car, but I&#x27;m totally allowed to sell it. reply chatmasta 4 hours agorootparentYes, and this analogy is even more valid than usual, because unlike most software where each binary is an exact copy of all the others, in this case each binary is actually unique to a device.But it&#x27;s more like a ticket, or an NFT. It&#x27;s a unique blob that was sold to you. You should be able to transfer it.Apple&#x27;s best argument here might be that the blob is meant for one person, and distributing it this way is like sharing a ticket to the cinema between multiple people. I can&#x27;t enter the cinema, then come outside and pass you the ticket so you can enter it too. reply Shawnj2 2 hours agorootparentprevIn that case the easy way out (and what plenty of Hackintosh&#x2F;console hacking&#x2F;emulation&#x2F;etc. communities have done since the beginning of time) is to just download the file directly from Apple when the app starts up the first time or have an “import BOOT.bin here” button you use to activate the app. If someone can source the binary you need to get the app to work I think that’s DMCA legal. reply zamadatix 7 hours agorootparentprevThose are all fine but it&#x27;s not the context of the copying in the discussed scenario. reply madeofpalk 9 hours agorootparentprevI dont believe Apple would sue. I think they would just change the protocol to block this from happening. reply chatmasta 9 hours agorootparentI think you might be right, especially with the heat on them from the EU right now. It&#x27;s faster to play the technical cat-and-mouse game for as long as possible. reply zamalek 11 hours agorootparentprevThere&#x27;s also a very small chance that the EU would sit idly by and watch Apple wreck compatibility. reply dylan604 11 hours agorootparentprev>Beeper will fight it either way.That sounds nice and all, but what happens when the first bill comes due from their legal team? reply chatmasta 11 hours agorootparentI imagine they&#x27;d use some of the $16m+ they raised in VC money to pay the lawyers... reply canada_dry 5 hours agorootparentprevThe Streisand effect will certainly boost enrollment if Apple sues. reply brokencode 11 hours agorootparentprevI’d donate to a legal fund on this personally. I think a lot of people and large corporations would like to see Apple have to make concessions here.I think if it comes to it, Apple will wind up looking very bad in a trial. Their behavior here is deeply anticompetitive. iMessage is just too important to modern text communication to be as locked down as it is.If Apple doesn’t want to make an Android app, they should at least make an API so other developers can. reply Domenic_S 11 hours agorootparent> iMessage is just too important to modern text communication to be as locked down as it is.What do you mean; if a private company creates something, and enough people buy&#x2F;use it, at some point it becomes a common good? I like the idea of iMessage being open, but I don&#x27;t like the idea of forcing Apple under government threat to open it reply brokencode 10 hours agorootparentI don’t know what you mean by “common good” in this context, but if a company has a dominant market position and uses its power to cripple competition, then it falls within antitrust laws.iMessage is so important today, especially to young Americans, that its exclusivity to iOS has become a significant barrier to Android or other operating systems from being competitive.It’s up to regulators and the court system to decide whether that is a violation of antitrust law. But if it is, then yes, the government should force them to open it. That’s what it means to enforce antitrust law. reply mardef 5 hours agorootparentI feel so old. What is it that I&#x27;m missing out in iMessage that is so important?Honest question, I&#x27;ve been texting since t9 and have never owned an Apple device. reply brokencode 5 hours agorootparentIt’s really nothing special. I personally use WhatsApp with most of my friends.The problem is when you have one person in a group that is on Android when everybody else is on Apple. This causes the iMessage conversation to use SMS instead. To signify this in the app, texts appear as green bubbles instead of blue, so it’s obvious when it happens.This is bad because SMS is totally obsolete. It causes images and videos to be shared in extremely low resolution, along with problems of messages not getting delivered reliably and other missing features.So effectively to the iPhone user, Android users very visibly cause group chats to be super crappy in iMessage.This is not the fault of the Android user really, because it’d work way better if Apple supported RCS like Android phones do, but many people have a very strongly negative impression of Android due to this.In fact, some iPhone users put social pressure on people with Android devices due to this in the form of excluding them from group chats or complaining about how they cause problems.Apple has been perpetuating this problem because it suits them. People know this, but it’s Android and Android users that suffer regardless due to Apple’s dominant market position. reply lxgr 5 hours agorootparentprevIt provides a much better group messaging experience than SMS (you can see who’s in a group and add and remove people), delivery&#x2F;read receipts, better image quality, is encrypted (although that gets somewhat negated by automatic iCloud backups), and is free as long as a data connection is available.Of course many other messengers offer most of these features too, but for some reason, no alternative has been able to establish itself in the US. reply chefandy 2 hours agorootparentiMessage was heavily integrated into the ios flow when sms was the dominant mobile text messaging system. It&#x27;s not special, and that&#x27;s the point. It just worked the way people want texting to work as smart phones gained momentum, and iPhones have so much of the market share that it&#x27;s way more irritating to use a separate messaging app when you can&#x27;t change the default integration on ios. I miss the convenience of heavily integrated iMessage comms at least twice per day. reply not2b 10 hours agorootparentprevYes, governments can require interoperability and can limit monopolies. That&#x27;s how antitrust laws work, like it or not. But if you want to get all libertarian, why should companies be able to use government power (as in courts, DMCA and the like) to shut down smaller companies that reverse-engineer their protocols? reply rpmisms 10 hours agorootparentI&#x27;m a major libertarian, and you have a great point. Apple should maintain their competitive advantage via technical means or let more cooks in the kitchen. reply dylan604 9 hours agorootparentWhich is pretty much where \"if you can&#x27;t innovate, litigate\" has its roots. reply LesZedCB 10 hours agorootparentprevcoughs in AT&T reply Domenic_S 11 hours agorootparentprevI have a hard time believing that the folks who were smart enough to do all this work somehow forgot that lawyers cost money reply dylan604 10 hours agorootparenti don&#x27;t disagree, but nobody can compete with the money Apple can spend. not every David can find a Rainmaker when competing against Goliath. Goliath still wins a lot. He was a champion after all reply altairprime 12 hours agorootparentprevThere’s no need for Apple to react to this project at all.Eventually, someone will send spam using this app, at which point automated systems at Apple will “console ban” the hardware identifier shared by all of the app’s customers. The project presumably has a library of valid hardware identifiers collected and ready to go, and eventually that’ll be drained by spammers faster than revenue versus device purchasing allows for. Apple can just wait silently as the app exhausts their pool of hardware identifiers, each banned by pre-existing anti-spam automation, without ever acknowledging their existence. reply smashah 9 hours agorootparentApple may not buy WhatsApp will. If there&#x27;s ever a commercial or OSS third party WhatsApp voice client I would expect they will try to send their Perkins Coie dogs after the project. They&#x27;ve already done it to many oss projects, terrifying Devs from continuing their work reply dadoum 12 hours agorootparentprevThe app is not redistributing it, it just requests a server to get validation data (since anyway the actual library loading involves patching every system function, making the function independent from the host device, see [0] if you want to see how it&#x27;s stubbed to run on Linux using a data.plist file), and thus there is no need to emulate it on device.[0]: https:&#x2F;&#x2F;github.com&#x2F;Dadoum&#x2F;imd-apple-services reply JimDabell 10 hours agorootparentprevDoesn’t this already have precedent? Nintendo used to check for the existence of their logo in cartridges before loading them so that anybody who wanted to create an unauthorised cartridge for a Nintendo system would have to reproduce their logo and infringe on their copyright. I’m pretty sure the court ruled that reproducing the logo for the purpose of interoperability was fair use. reply threeseed 12 hours agorootparentprevThere are reverse engineering&#x2F;interoperability exemptions to the DMCA so it may not be that simple.So would be curious if they have already sought legal advice which says they are in the clear. reply throwaway-blaze 12 hours agorootparentthey raised $16mm. I assure you they&#x27;ve talked with a lawyer or two. reply qwytw 11 hours agorootparentIf they actually just took a binary from OSX and stuck it into their app it probably wasn&#x27;t the best lawyer reply chefandy 2 hours agorootparentI believe it&#x27;s server-side: not distributed. reply viraptor 9 hours agorootparentprevSam Bankman-Fried raised $1.8B, yet we know how that ended even with lawyers available, so... We&#x27;ll see. reply WD40ForRust2 13 hours agorootparentprevI hope we get to a place where people like this simply generate an OpenPGP key&#x2F;OpenSSL certificate for a pseudonym and just throw this stuff up on .onion and .i2p domains. A place where DMCA and copyright literally cannot be enforced because it&#x27;s impossible to. reply philsnow 12 hours agorootparentThis reminds me of the near-ish-future \"Rainbow&#x27;s End\" by Vernor Vinge, wherein instead of giving out phone numbers or email addresses or screen names (identifiers), people give out opaque GUIDs [0] that act as communication handles with capabilities baked in. So, you could give out one to friends that allows people to open a synchronous voice channel to you, but give out one on your business card that just allows people to send text messages to you.The book doesn&#x27;t talk about it too much, but presumably these handles could be limited-use (time-based or only granting a capability to send a certain number of messages) and could be revoked.I know it would probably be off-putting to give each person I meet a different GUID for contacting me (kind of like telling them your email address is @), but it might reduce the spam I receive.[0] if you&#x27;re searching the ebook, they&#x27;re called \"golden enums\" in the text reply Wytwwww 13 hours agorootparentprevNot sure how likely is that considering that Beeper is an actual&#x2F;company startup which seems to have received funding from YC?However, considering that I&#x27;d except they&#x27;d know better than to just outright take a binary from MacOS and use it in their app (assuming that&#x27;s actually the case..). reply apitman 12 hours agorootparentprevIt&#x27;s not impossible, just currently not worth the tradeoffs of enforcing. There&#x27;s nothing stopping governments from passing laws holding IP address owners responsible for the traffic they originate. At that point VPNs and Tor exit nodes will stop allowing illegal activity. VPNs are already moving this direction, no longer supporting port forwarding ie hosting content on bittorent. replydanpalmer 17 hours agoprevI already had a significant respect for Beeper (Cloud) as a technical product. The backend being Matrix with open source bridges was a great choice.This write up adds so much more to that respect. It would have been easy to botch this, it would have been easy to do a worse implementation that would have caused problems for users whether they cared or not, but Beeper seemingly took the time to get right.Congrats to Eric and the team on the launch! reply VanTheBrand 3 hours agoprevI think this might be launching at an opportune time. The EU is already trying to force them to open up the App Store and iMessage has a target on its back. A cease and desist about this won’t look great in the inevitable antitrust hearings… reply apexalpha 2 hours agoparentDoes iMessage have a target on its back? It doesn&#x27;t have a dominant position here in messaging, if it has a position at all. reply yalok 2 hours agorootparentover 50% of US market (and a few other countries), for starters. Not EU&#x27;s jurisdiction, but it is a major messaging service. reply apexalpha 1 hour agorootparentYes, I know but I was specifically talking about the EU, since that&#x27;s the only government legislation for open access. reply geraldhh 48 minutes agorootparentbit of irony that this can fly because of legislation in a mostly irrelevant market replysprite 10 hours agoprevDid you get permission from Apple to connect to their servers? Google Play does not allow apps to connect to 3rd party APIs without consent.The relevant policy can be found at: https:&#x2F;&#x2F;support.google.com&#x2F;googleplay&#x2F;android-developer&#x2F;answ...\"We don’t allow apps that interfere with, disrupt, damage, or access in an unauthorized manner the user’s device, other devices or computers, servers, networks, application programming interfaces (APIs), or services, including but not limited to other apps on the device, any Google service, or an authorized carrier’s network.\"From what I understand your app connects to APNS without permission from Apple.I have personally had my Google Play Developer account banned for making an app that connected to a 3rd party service reply geor9e 9 hours agoparentI&#x27;m surprised Apple hasn&#x27;t cut them off yet. They must not be able to for some legacy reason. I suspect the only way to cut them off would be to cut off all the older phones like iPhone 3GS as well.>the iMessage protocol and encryption have been reverse engineered by jjtech, a security researcher. Leveraging this research, Beeper Mini implements the iMessage protocol locally within the app. All messages are sent and received by Beeper Mini Android app directly to Apple’s servers. The encryption keys needed to encrypt these messages never leave your phone. Neither Beeper, Apple, nor anyone except the intended recipients can read your messages or attachments. Beeper does not have access to your Apple credentials.>We built Beeper Mini by analyzing the traffic sent between the native iMessage app and Apple’s servers, and rebuilding our own app that sends the same requests and understands the same responses.https:&#x2F;&#x2F;blog.beeper.com&#x2F;p&#x2F;how-beeper-mini-works reply CobrastanJorji 9 hours agoparentprevIs this specifically unauthorized, though? The user is permitted to use Apple&#x27;s services, and Apple has, as far as I know, not announced that third party apps may not use their services. reply sprite 9 hours agorootparentIf Apple files a complaint with Google it will definitely get taken down under this clause, so I think the only way it will stay up is if Apple doesn’t care.With the trouble Apple goes through to ensure you are accessing APNS from an Apple device including obfuscating the signing algorithm and requiring unique hardware identifiers I think it’s safe to assume they don’t want 3rd parties accessing their services. reply ryukoposting 6 hours agorootparentprevEven Signal pitches a fit if you use a third-party app with their servers. It&#x27;s a common (and unfortunate) practice. reply colinsane 3 hours agorootparentwhat does this mean? plenty of 3rd party signal clients exist (flare being a well-known one); signal explicitly factored out a libsignal presumably to _encourage_ this.i’ve run multiple 3rd-party signal clients, even alongside the official apps, and never seen any problems or warnings.[flare]: https:&#x2F;&#x2F;gitlab.com&#x2F;schmiddi-on-mobile&#x2F;flare reply joecool1029 1 hour agorootparent>what does this mean?Moxie (Signal&#x27;s founder) has thrown fits in the past over the existence of third-party clients using their servers: https:&#x2F;&#x2F;github.com&#x2F;libresignal&#x2F;libresignal&#x2F;issues&#x2F;37#issueco... reply remus 4 hours agorootparentprev> It&#x27;s a common (and unfortunate) practice.It would be nice if third party clients were allowed to connect, but it&#x27;s totally understandable if they don&#x27;t want to allow it. Servers cost money, and misbehaving client apps that you have no control over sound like a pain in the ass. reply KTibow 10 hours agoparentprev> I have personally had my Google Play Developer account banned for making an app that connected to a 3rd party service.Well what did it do with the service? reply sprite 9 hours agorootparentI had app that connected to the Snapchat API and let you upload photos with custom effects and photos from your photo album before that was a feature (not sure if it is today, I don&#x27;t use Snapchat) reply matsz 17 hours agoprevGreat job! Just from taking a quick look at this, what you have here is much bigger than iMessage itself.This could literally allow things like Universal Clipboard to work on Linux and Windows - by using the method presented here to access the iCloud Keychain and generating Continuity keys and placing them there - then the iPhone will broadcast its clipboard data encrypted with those keys via BLE. If I understand all of this correctly. reply crooked-v 9 hours agoparentI had been wondering where Beeper&#x27;s route to profitability was, but if they can get Continuity and AirDrop stuff working with Windows that will be an instant no-brainer subscription for a lot of people (including me), so I guess it works out. reply LectronPusher 2 hours agorootparentIt works over wifi, but you might be interested in KDE Connect [1]. It can do clipboard, remote input, file sending, command running, etc. on Windows and Linux.[1] https:&#x2F;&#x2F;kdeconnect.kde.org&#x2F; reply joshstrange 12 hours agoprevBeeper is a really cool idea by some cool people (people behind the Pebble smartwatch) but I&#x27;ve resisted using it for fear of bans. I don&#x27;t want my Slack&#x2F;Discord&#x2F;Instagram&#x2F;AppleId&#x2F;etc to get banned for using something not allowed under the terms of service. How are people who use Beeper dealing with this? Are you just using dummy&#x2F;test accounts that you don&#x27;t care about or are you just rolling the dice.I would like to live in a world where I could use Beeper without worry but I don&#x27;t feel like we currently live in that world. Am I wrong? reply nusl 11 hours agoparentI’ve been using Beeper as my main chat client for multiple years and haven’t had any issues with account blocks or bans on any of their supported platforms. I have Discord, Signal, WhatsApp, iMessage, and LinkedIn connected. There are technical issues at times but they are well communicated and usually resolved pretty quickly. reply ChaseT 6 hours agoparentprevI&#x27;ve used Beeper for about a year with Facebook, Signal, Instagram, Twitter, LinkedIn, and iMessage. Instagram signs me out once a month or so for security suspicions, but I just reconfirm my account with 2FA. Other than that, no issues. reply quantumsequoia 1 hour agoparentprevBeeper mini does not require an apple account so there&#x27;s not much harm Apple can do reply gaws 11 hours agoparentprev> I would like to live in a world where I could use Beeper without worry but I don&#x27;t feel like we currently live in that world. Am I wrong?I&#x27;ve been using Beeper for close to six months, and it&#x27;s been a dream. reply yellow_lead 12 hours agoparentprevSince they&#x27;ve been on waitlist-mode for several years, it&#x27;s not currently easy to try out in any case. reply Brajeshwar 7 hours agorootparentInvite: https:&#x2F;&#x2F;refer.beeper.com&#x2F;7Wh0gt reply Melatonic 6 hours agorootparentSays invalid - maybe used already ? reply nusl 11 hours agorootparentprevThey’ve opened invites from existing users reply chatmasta 12 hours agoparentprevIf you have an Apple account, why are you even using Beeper? I guess it might have some advantages for convenience (multiplexing chat apps), but is that the main selling point right now? I&#x27;d imagine the target market is Android users who want to talk to people on Apple Messages. So they can just create a new Apple account, right? (Isn&#x27;t that kinda hard anyway, though? You need to tie it to billing, etc.) And if that gets banned, who cares? It&#x27;s not like they were using it for anything else anyway. reply altintx 12 hours agorootparentI sit in front of my work laptop which is signed into my work apple account. My iPhone is signed into my personal Apple account. I cannot iMessage from the keyboard because they won&#x27;t play together. I&#x27;ve been using Cloud Beeper since early summer, and it makes the two apple systems play nice together. I also have a Windows machine signed in to it, but that&#x27;s a nice to have. reply chatmasta 12 hours agorootparentWait, how does this work? Is it using Handoff and sending from your phone, or Beeper is just a GUI and you&#x27;ve extracted a token from your personal phone to use with Beeper on your work device?Btw, this is mostly unrelated, but do you work for a large company? I&#x27;d assume most security teams would have a problem with a setup like this. reply altintx 11 hours agorootparentNeither. Their cloud server is a farm of Mac Minis or similar. Then Beeper Cloud is basically a proxy from the app to that data center. reply chatmasta 11 hours agorootparentAh I see. I thought I remembered reading about that on Twitter (in the context of people criticizing it as false advertising). So basically this Beeper mini is the \"proper\" implementation of full e2e encryption, while the cloud service was the bridge to get them here? reply altintx 11 hours agorootparentThat&#x27;s my understanding, yeah. I don&#x27;t love my apple ID being signed into a box I can&#x27;t access, so would love to see THIS service go cross platform. replyjoshstrange 12 hours agorootparentprevI&#x27;m more interested in the multiplexing aspect, yes I&#x27;m iOS&#x2F;macOS so I don&#x27;t care about the iMessage aspect alone though I&#x27;d love to pull all my chats into 1 extendable app. reply chefandy 8 hours agorootparentprevAn Apple account isn&#x27;t particularly useful for messaging without an Apple device to message people with. reply jdiez17 16 hours agoprevIt seems that at least the push notification registration part uses a \"leaked&#x2F;extracted\" FairPlay private key [1]. As far as I understand, FairPlay certificates&#x2F;keys should be unique to each iDevice. Couldn&#x27;t Apple trivially ban all subscriptions originating from this fake device? The comment says you know how to generate more; does Beeper Mini generate one for each install? Why would Apple believe those certificates are authentic?P.S.: the source repo mentioned in the comment (https:&#x2F;&#x2F;github.com&#x2F;MiUnlockCode&#x2F;albertsimlockapple) is 404.[1] https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;main&#x2F;albert.py#L16 reply chdefrene 15 hours agoparentSnazzy Labs did an overview video [1] about this implementation. According to them, reusing a specific hardware token is such s common practice that Apple would need to \"redesign their entire authentication and delivery strategy\" to mitigate this problem. I guess we&#x27;ll see how this statement holds up in the coming weeks&#x2F;months.[1] https:&#x2F;&#x2F;youtu.be&#x2F;S24TDRxEna4?t=5m38s reply saagarjha 15 hours agorootparentThis didn’t really say much. Apple definitely knows about Hackintosh users, they mostly just don’t care. The question is whether they will actually do something if made to care. reply nebula8804 14 hours agorootparentThey &#x27;don&#x27;t care&#x27; because they know that the M series processors were coming and now there is a built in death counter coming for Hackintoshes...the day they drop Intel support.June 5, 2028: Intel hardware will reach \"vintage\" status after having been discontinued five years prior, ending most of Apple&#x27;s service and parts support for Intel hardware.June 5, 2030: Intel hardware will reach \"obsolete\" status after having been discontinued seven years prior, ending all of Apple&#x27;s service and parts support for Intel hardware. reply raydev 13 hours agorootparent> They &#x27;don&#x27;t care&#x27; because they know that the M series processors were coming and now there is a built in death counter coming for HackintoshesNo, they don&#x27;t care because they don&#x27;t think about it at all. Hackintosh&#x27;s numbers never mattered, it&#x27;s always been too onerous to maintain even when it was at its easiest. reply oceanplexian 14 hours agorootparentprevThis is too high profile. Apple is absolutely, 100% going to kill this and it’s gonna screw this over for those of us who leverage iMessage in Hackintosh environments. reply apitman 12 hours agorootparentYou might be right, but if ever there was a regulatory environment under which Apple would think twice, this might be it. reply layer8 12 hours agorootparentprevApple may be reluctant to kill this exactly because it is high profile, given the current anti-trust investigations. reply andrewshadura 14 hours agorootparentprevIt&#x27;s been around for ages, and Apple has taken no action so far. reply skiman10 13 hours agorootparentIt has never been this easy and it has never been behind a subscription fee. reply bitwize 15 hours agorootparentprevnext [12 more] We&#x27;re talking about a company that changes CPU architectures for their ecosystem every few years, completely seamlessly. If redesigning their entire authentication and delivery strategy is what it will take to mitigate this problem, Apple will do it. reply semi-extrinsic 15 hours agorootparentWhat problem? Increased compatibility? reply Zak 15 hours agorootparentFrom Apple&#x27;s perspective, yes. Social pressure to buy Apple devices to use Apple&#x27;s messaging app is part of Apple&#x27;s marketing strategy.Apple also claims that blocking devices by serial number or similar unique hardware identifiers is a key part of its anti-spam strategy. If true, an end-run around that will likely create problems for users as well. reply oceanplexian 14 hours agorootparentThe creator of this is screwing things up for everyone. If it was an obscure, open source project Apple would probably let it slide and we’d be able to enjoy this indefinitely. This has been the case for Hackintosh stuff and the like.But no, the author had to make a dumb, flashy looking website that looks like they’re advertising a product built around reverse engineered Apple tech. I bet they get a Cease and Desist by the end of the week and the hole is patched shortly after. reply devmor 15 hours agorootparentprevIsn&#x27;t apple implementing full RCS support next year? reply Aaargh20318 14 hours agorootparentThere is no encryption in the RCS standard, so of course no encryption. reply chrisfinazzo 14 hours agorootparentThe current state of affairs re encryption is an accident of history that I would bet doesn’t last much longer once Apple gets formally involved.“Apple says it won&#x27;t be supporting any proprietary extensions that seek to add encryption on top of RCS and hopes, instead, to work with the GSM Association to add encryption to the standard.”https:&#x2F;&#x2F;www.techradar.com&#x2F;phones&#x2F;iphone&#x2F;breaking-apple-will-... reply Zak 14 hours agorootparentprevI think so, but something makes me think they&#x27;re not going to do it in a way that gives RCS users full parity with iMessage users. reply ptman 14 hours agorootparentprevNot encryption, apparently. And the blue iMessage bubbles indicate encryption, so RCS bubbles will be green. reply entropicdrifter 14 hours agorootparentprevApple has control issues. If they don&#x27;t control it or at least sign off on it, they want it to be incompatible with their hardware.Hell, they don&#x27;t even allow alternative browsers on their iOS devices. All the non-Safari browsers are just Safari in a (Chrome, Firefox, etc) skin reply stevefeinstein 15 hours agorootparentprevOne man&#x27;s increased compatibility is another&#x27;s security vulnerability. reply op00to 14 hours agorootparentprevBridging the blue bubble moat. reply blopker 15 hours agoparentprevDoes this look like the same file from the deleted repo? https:&#x2F;&#x2F;github.com&#x2F;rdxunlock&#x2F;albertsimlockapple&#x2F;blob&#x2F;main&#x2F;AL...I&#x27;d love to see an open source version of Beeper with no analytics. I&#x27;d be happy to host my own notification server. reply ebb_earl_co 14 hours agorootparentBeeper already advertises the self-hosting route: https:&#x2F;&#x2F;github.com&#x2F;beeper&#x2F;self-host reply rd07 7 hours agorootparentI hope they open source their client app or at least makes it possible to connect to other matrix server. For me, their client app is the best matrix client in terms of UI. reply malermeister 14 hours agorootparentprevThe python library they provide should be a good start at least: https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush reply kristofferR 14 hours agoparentprevLooks like it&#x27;s this?https:&#x2F;&#x2F;github.com&#x2F;xxCabin&#x2F;albertsimlockapple&#x2F;blob&#x2F;main&#x2F;ALBE...This also seems related:https:&#x2F;&#x2F;github.com&#x2F;unicode99&#x2F;MiUnlock&#x2F;blob&#x2F;main&#x2F;activator.ph... reply szszrk 17 hours agoprevOK, took a while to figure out what it is, as I barely know anyone using iphone. Though it&#x27;s not for me, BUT if they deliver this:> Over time, we will be adding all networks that Beeper supports into Beeper Mini, including SMS&#x2F;RCS, WhatsApp, Messenger, Signal, Telegram, Instagram, Twitter, Slack, Discord, Google Chat and Linkedin. We&#x27;ll also bring Beeper Mini to desktop and iOS.I&#x27;m interested, even if it&#x27;s paid. I&#x27;d love to have most of those apps gone and use a cleaner one. reply striking 16 hours agoparentHappy Beeper customer and original poster here to tell you: Beeper Cloud is already out there and works really well! It&#x27;s also free, though you&#x27;ll have to get through the waitlist somehow. It doesn&#x27;t perfectly replace every app just yet but it covers the most important functionality extremely well. And it&#x27;s available on mobile as well as desktop devices. reply kelnos 15 hours agorootparentIIRC, though, Beeper Cloud does not come with end-to-end encryption on messaging services that usually have that feature through their regular app. Messages are encrypted between your device and Beeper&#x27;s servers, and between Beeper&#x27;s servers and the other end of the conversation, but the Beeper folks can still read your messages if they want.(Please correct me if I&#x27;m wrong; the architecture of their product is pretty confusing.) reply lyton 6 hours agorootparentprevDo you mind giving me a referral for Beeper? reply Wintereise 5 hours agorootparentHere you go - refer.beeper.com&#x2F;39gJJ0 reply Melatonic 3 hours agorootparentPossible to generate another ? Says invalid codeEdit: I mean a code for Beeper Mini on Android - not desktop reply PrimeMcFly 2 minutes agorootparent> Says invalid codeWell yeah it was already used and you came 3 hours after the fact.neither_color 14 hours agoparentprevIve tried the legacy version to consolidate Signal, Whatsapp, etc and you can&#x27;t send&#x2F;receive calls, only messages. It&#x27;s very much still a work in progress reply kyawzazaw 12 hours agoparentprev> as I barely know anyone using iphone.where are you located? reply midasz 12 hours agorootparentI&#x27;m from the Netherlands and I know plenty of people who have iPhones but I also know (and am) plenty people with androids. People use either WhatsApp or Telegram. Isn&#x27;t iMessage just texting within a walled garden? reply hn_throwaway_99 11 hours agorootparentThe situation is very different in the US, primarily because in other countries SMS fees tended to be really high a decade and change ago, and thus drove users to WhatsApp, but in the US most carriers had adopted some form of unlimited texting shortly after the iPhone first came out.Thus, for many socio-economic groups, iPhone is definitely king in the US, and for them iMessage is just the default way to message people because when it was introduced it was the default way to use SMS on iPhone. A restaurant in Texas famous for their funny signs put this out, https:&#x2F;&#x2F;twitter.com&#x2F;ElArroyo_ATX&#x2F;status&#x2F;1693316647677825160 , and tons of people (myself included) could immediately relate. reply szszrk 12 hours agorootparentprevPoland. I know a few apple fanboys but those aren&#x27;t people I communicate with outside of work. Just not my bubble.It&#x27;s actually weird and silly when they send me text messages and somehow I end up in the same conversation multiple times - like once 1:1, once in a group chat with myself included twice or more (as a number, as an email, as a second number). It&#x27;s a bizarre experience and usually iPhone user can&#x27;t see anything wrong :D reply slig 17 hours agoparentprevIs this some sort of new mobile Adium? reply djbusby 17 hours agorootparentTrillian reply philsnow 12 hours agorootparentEveryBuddy reply edoceo 12 hours agorootparentNot sure what EveryBuddy is. Trillian was a multi-protocol chat client from 2000 [0] named after Trillian [1] from 1979.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trillian_(software)[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trillian_(character) reply philsnow 11 hours agorootparentah didn&#x27;t realize it had gone away. its successor appears to be [0]now I&#x27;m reliving the chaos of the late-00s&#x2F;early-10s instant messaging apocalypse when AOL sunsetted AIM. Clients like Trillian were absolutely necessary before AIM shut down. Everybuddy was a good linux-friendly client. When I still spent time on IRC, I really really liked Bitlbee [1] with ERC [2]. Gaim was one of the first open-source projects I ever contributed to.(I&#x27;m not saying that there&#x27;s a connection there, but rather that all the chat protocols started getting used less around the same time for the same reason, which was smartphones becoming commonplace in late-00s.)[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ayttm[1] https:&#x2F;&#x2F;www.bitlbee.org&#x2F;[2] https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;emacs&#x2F;erc.html reply wiml 11 hours agorootparentprevPidgin reply CaptainNegative 11 hours agorootparentGaim replyworldsavior 22 minutes agoprevAlso see BlueBubbles[1]. It could easily be integrated with pypush&#x2F;rustpush.[1] https:&#x2F;&#x2F;github.com&#x2F;BlueBubblesApp&#x2F;bluebubbles-app reply ddxv 7 hours agoprevFirst they require email and personal info. Then they tell you it&#x27;s a monthly subscription. Felt like a terrible onboarding experience and a bit of a dark pattern. reply mianos 5 hours agoparentIf you scratch around enough they do say it&#x27;s a paid product. Pretty cool yes,\"show hacker news\"? Dunno. reply 509 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Beeper Mini is a third-party iMessage client for Android that enables Android users to send and receive end-to-end encrypted messages directly to Apple servers without a Mac server relay.",
      "Users can join iMessage group chats, access all chat features, and register their Android phone number on iMessage using Beeper Mini.",
      "The developers plan to add support for other chat networks like SMS/RCS, WhatsApp, and Signal in the future, and note that third-party iMessage clients have existed before with multi-protocol chat apps like iChat."
    ],
    "commentSummary": [
      "The discussions revolve around the compatibility of messaging apps with different platforms and the legality of reverse-engineering Apple's protocols.",
      "Interoperability between messaging platforms and its impact on spam and phishing are also discussed.",
      "The limitations of Apple's iMessage and the desire for standardization across devices are debated as well, shedding light on the challenges faced by developers in achieving compatibility and security."
    ],
    "points": 1276,
    "commentCount": 760,
    "retryCount": 0,
    "time": 1701788781
  },
  {
    "id": 38531104,
    "title": "Declining Firefox usage threatens support from U.S. government websites",
    "originLink": "https://www.brycewray.com/posts/2023/11/firefox-brink/",
    "originBody": "Note: This post was the subject of a Hacker News thread. A somewhat obscure guideline for developers of U.S. government websites may be about to accelerate the long, sad decline of Mozilla’s Firefox browser. There already are plenty of large entities, both public and private, whose websites lack proper support for Firefox; and that will get only worse in the near future, because the ’fox’s auburn paws are perilously close to the lip of the proverbial slippery slope. The U.S. Web Design System (USWDS) provides a comprehensive set of standards which guide those who build the U.S. government’s many websites. Its documentation for developers borrows a “2% rule” from its British counterpart: . . . we officially support any browser above 2% usage as observed by analytics.usa.gov. At this writing, that analytics page shows the following browser traffic for the previous ninety days: Browser Share Chrome 49% Safari 34.8% Edge 8.4% Firefox 2.2% Safari (in-app) 1.9% Samsung Internet 1.6% Android Webview 1% Other 1% I am personally unaware of any serious reason to believe that Firefox’s numbers will improve soon. Indeed, for the web as a whole, they’ve been declining consistently for years, as this chart shows: Chrome vs. Firefox vs. Safari for January, 2009, through November, 2023. Image: StatCounter. Firefox peaked at 31.82% in November, 2009 — and then began its long slide in almost direct proportion to the rise of Chrome. The latter shot from 1.37% use in January, 2009, to its own peak of 66.34% in September, 2020, since falling back to a “measly” 62.85% in the very latest data.1 While these numbers reflect worldwide trends, the U.S.-specific picture isn’t really better. In fact, because the iPhone is so popular in the U.S. — which is obvious from what you see on that aforementioned government analytics page — Safari pulls large numbers that also hurt Firefox. In my days in tech marketing, we used to worry about how a dominant competitor would take “shelf space” in those large stores where we wanted visibility for our goods and their accompanying point-of-purchase brochures. (Remember point-of-purchase literature, fellow oldsters?) Well, Firefox is quickly losing “web space,” thanks to a perfect storm that’s been kicked up by the dominance of Chrome, the popularity of mobile devices that run Safari by default, and many corporate and government IT shops’ insistence that their users rely on only Microsoft’s Chromium-based Edge browser while toiling away each day. With such a continuing free-fall, Firefox is inevitably nearing the point where USWDS will remove it, like Internet Explorer before it, from the list of supported browsers. “So what?” you may wonder. “That’s just for web developers in the U.S. government. It doesn’t affect any other web devs.” Actually, it very well could. Here’s how I envision the dominoes falling: Once Firefox slips below the 2% threshold in the government’s visitor analytics, USWDS tells government web devs they don’t have to support Firefox anymore. When that word gets out, it spreads quickly to not only the front-end dev community but also the corporate IT departments for whom some web devs work. Many corporations do a lot of business with the government and, thus, whatever the government does from an IT standpoint is going to influence what corporations do. Corporations see this change as an opportunity to lower dev costs and delivery times, in that it provides an excuse to remove some testing (and, in rare cases, specific coding) from their development workflow.2 . . . and just like that, in less time than you might think, Firefox — the free/open-source browser that was supposed to save the world from the jackboots of Internet Explorer (which had killed Firefox’s ancestor, Netscape Navigator) — is reduced to permanent status as a shrinking part of the fractured miscellany that litters the bottom of browser market-share charts. I surely hope I’m wrong about this, but I fear I’m not. Nearly five years ago, as the news broke that Microsoft had decided to move its Edge browser to the Blink engine that also powers Google Chrome, I wrote: Supporting multiple browser engines — even if there is a Really Big Dog engine among them that’s about to get even bigger — ain’t always fun, but it goes with the territory; and I firmly believe it will continue to do so, especially for sites that are commercial in nature. That firm belief remains unchanged, but the meaning of the “multiple browser engines” part is in serious danger of significant change. Unless something dramatically reverses Firefox’s trends, the ’fox could soon be whimpering its way down an ugly, slippery slope to irrelevance. For that matter, there’s also Mozilla’s own “User Activity” chart, which shows the count of active Firefox clients dropping from 244.2 million on December 30, 2018, to 187.3 million on November 19, 2023. That’s a 23.3% drop over just that five-year period. ↩︎ At least, this is in the case of those companies which did still bother to test their websites on Firefox in the first place. More than a few gave up on it some years ago, if my own anecdotal experiences can provide any guidance. ↩︎ Latest commit (b1f9fd53) for page file: 2023-12-05 at 9:00:27 AM CST. Page history Reply via email View comments",
    "commentLink": "https://news.ycombinator.com/item?id=38531104",
    "commentBody": "Firefox on the brink?Hacker NewspastloginFirefox on the brink? (brycewray.com) 612 points by alexzeitler 19 hours ago| hidepastfavorite730 comments robin_reala 19 hours agoIt’s not on the brink, it’s that Firefox shims GA[1] with Enhanced Tracking Protection, which is on by default.[2] analytics.usa.gov uses GA.So you really can’t rely on usage figures that don’t represent the truth of your situation.[1] https:&#x2F;&#x2F;github.com&#x2F;mozilla&#x2F;gecko-dev&#x2F;blob&#x2F;413b88689f3ca2a30b...[2] https:&#x2F;&#x2F;blog.mozilla.org&#x2F;en&#x2F;products&#x2F;firefox&#x2F;firefox-now-ava... reply Chabsff 19 hours agoparentPer the article, this is irrelevant. The case being made is that the figure seen by the government falling under 2%, which it is getting close to, will trigger a chain-reaction of consequences that will dramatically hurt Firefox, and it&#x27;s not clear if Mozilla, the organization, could weather that.It doesn&#x27;t particularly matter whether or not the figure is accurate in that context. Maybe that can be fixed easily by having Firefox contextually relax tracking a bit or by having the government change how they perform the tracking, but the status quo is not really sustainable. And that&#x27;s really all the article is saying at the end of the day. reply beanjuiceII 13 hours agorootparentI work in govt tech, we already internally do not care about FF support at all and only really look to chrome based browsers (chrome edge mostly) and safari. Many people don&#x27;t even know what it is anymore, seems like time killed this browser too. Mozilla spending too much time&#x2F;money and other crap over the years reply riversflow 12 hours agorootparentWhat does a government site need that you can&#x27;t do on firefox? Honestly this is disenfranchising, you should consider the ethics of what you are proscribing for the populace. I should have a right to choose a browser that respects my privacy from a PC. reply insanitybit 12 hours agorootparentIt&#x27;s clearly not about what is needed, it&#x27;s about what is tested. And spending resources on a browser that appears to have 2% of the market is not going to be a high priority. reply flagrant_taco 6 hours agorootparent\"appears to have 2%\" is doing a lot of work he. If the earlier comments are accurate, the government stats are based on Google Analytics data that is specifically blocked or obscured by Firefox.That 2% kind be breached, but the data doesn&#x27;t accurately represent usage if Firefix is blocking GA in any meaningful numbers. reply insanitybit 1 hour agorootparentYeah I used the word \"appears\" exactly to emphasize that the appearance is sufficient. The government is working with the data they have. reply Chabsff 12 hours agorootparentprevExactly. There are a plethora of browsers out there, and no one can be expected to test against all of them. As long as we go for a \"government conforming to industry\" mentality, then the cutoff has to be somewhere, and if Firefox doesn&#x27;t make that cutoff, then that&#x27;s all there is to it.No browser deserves special treatment here. And this is as true for Firefox as it is for Lynx. reply riversflow 11 hours agorootparent> No browser deserves special treatment here.Completely disagree. Privacy is a 4th ammendment right, so the government has a duty to support privacy first browsers. How am I \"secure in my ... papers and effects\" if I must use a privacy destroying browser to interact with the Government? If anything, chrome support is what should be questionable. reply insanitybit 9 hours agorootparentMaybe you can make this argument - that the government should support technologies that emphasize privacy. That could possibly lead to some funding. Creating a policy to support a single vendor, however, is something that I think would be a bit odd for the government. Government spending is supposed to be devoid of bias where possible - you can&#x27;t just give a contract to someone you like or think is cool, you need to show that it&#x27;s in the taxpayer&#x27;s interest (ie: they are cheaper, usually).To say \"we are going to support a browser with 2% market share because we like it more\" would therefor be a really big deal. I think a far better avenue would be for the government to provide funding for privacy technologies directly, in an effort to increase their marketshare, and thereby making support of those technologies trivial to justify without any potential issues of bias.The thing is, the government already does this. The majority of TOR&#x27;s funding comes from the USG, for example. I&#x27;d suggest that maybe the government could fund Mozilla&#x27;s non profit, to a degree.This is all based on a premise that Mozilla is worthy of that funding. Is Mozilla really the privacy champion that it touts itself as? Would funding be contingent on anything else? If the government is so determined to prioritize privacy, why not Brave? Or some other entity? Or a new entity? reply gafage 11 hours agorootparentprevThat&#x27;s the mother of all reaches. reply riversflow 11 hours agorootparentHow? reply hedora 7 hours agorootparentprevThe supreme court ruled there is no right to privacy in the same ruling that overturned roe v wade. reply insanitybit 1 hour agorootparentI don&#x27;t think that&#x27;s true. They just ruled that the constitutional right to privacy is not sufficient for the Roe v Wade ruling. reply andsoitis 4 hours agorootparentprev> The supreme court ruled there is no right to privacy in the same ruling that overturned roe v wade.My understanding is that the consequence is denial of the right to privacy is limited to women while they are pregnant. Not a categorical denial of the right to privacy. reply eecc 11 hours agorootparentprevWell, that’s what you get when you’re a Public Service. There’s an idiot living on the top of a mountain that needs some basic infrastructure? You got to get your truck out and build that supply line. You won’t do it? Then the government that granted you that Public Service status has renounced sovereignty on that part of the country reply femiagbabiaka 11 hours agorootparentExactly, what a weird train of thought from government workers. Seems like the mission has been lost reply Silhouette 11 hours agorootparentprevA long time ago we had something called \"standards\" that were intended to solve this problem. The idea was that developers of different browsers would get together in a neutral forum and agree common ground that everyone would support. That way developers could reliably build upon that common ground and expect their products to operate correctly in any browser. It was an excellent idea that heralded the success of the modern WWW.Naturally this enforced competition wasn&#x27;t in Google&#x27;s interests as its browser became dominant and it adopted the infamous Microsoft strategy to deal with the threat. Apparently we&#x27;re fast approaching the extinguish phase.The correct solution to this is for influential public bodies not to insist upon supporting any specific browser or browsers but instead to once again support open web standards and therefore free access to their information and services for all.This is almost certainly in their own interests anyway. Google has a nasty habit of suddenly killing off its non-standardised extensions. Relying on functionality that other browsers don&#x27;t necessarily support as well seems unwise. reply ideamotor 10 hours agorootparentThis, but for so many issues. Innovation is being stifled by platform monopolies. Standards can too but right now that’s not the primary cause. It’s a balance. reply IshKebab 11 hours agorootparentprevNonsense. Only completely trivial standards work without testing actually implementations against each other.How do you think WiFi devices work together? Is it because WiFi is a standard? Not really - it&#x27;s because WiFi vendors all test their chips against other WiFi chips on the market. reply Silhouette 10 hours agorootparentI&#x27;ve been the person doing that testing. Checking interoperability with a range of other devices is one way you can look for problems but when it comes to signing things off the standard is the final word. Do you know what happens to the devices that don&#x27;t comply with statutory radio transmission standards? They don&#x27;t get sold.Typically the cost of fixing the problem and going through the whole certification process again from the start is significant. There are some direct financial costs but mostly the damage is increased time to market. That&#x27;s a great incentive for the maker to get their act together and comply with the standards like everyone else as quickly as possible. If you screw up too many times your product might be obsolete before you&#x27;re ever allowed to sell it. reply IshKebab 3 hours agorootparent> statutory radio transmission standardsNot the kind of standards we are talking about. replysmegger001 9 hours agorootparentprevThe problem Firefox has as far as I can tell is that the people in charge of the Mozilla organization would rather it be a social justice philanthropic foundation than a web browser and mail client maker and thus neglect their main product. And those they do have working on Firefox seem to want to remove features and play with UI. reply wolpoli 8 hours agorootparentFor whatever the reason, Firefox does seem to enjoy playing with colors of the season more than trying to figure out where to take their rendering engine. reply smegger001 7 hours agorootparentThe thing that gets me about Mozilla is they were doing thing with xulrunner years ago that WebKit is being used for now with electron apps. But Mozilla didn&#x27;t develop it. There were crossplatform apps being built with gecko. reply shiroiuma 3 hours agorootparentprev>And those they do have working on Firefox seem to want to remove features and play with UI.This seems to be all that most software developers can do these days. reply orbital223 7 hours agorootparentprev> and thus neglect their main productNeglect would be a step up. It feels like they are actively trying to antagonize existing users. reply itronitron 11 hours agorootparentprevLast I checked gov tech is larger than just one person commenting on HN. reply djvdq 4 hours agorootparentprevUS gov&#x27;t supporting monopoly instead of fighting it? I&#x27;m not surprised. reply traviswt 12 hours agorootparentprev> Mozilla spending too much time&#x2F;money and other crap over the yearsIt seems like a pretty bit conflict of interest when the #1 money source for Firefox comes from Firefox&#x27;s only real competitor. reply sitzkrieg 12 hours agorootparentyea things have seriously gone downhill since chrome took off in 2010 reply robin_reala 19 hours agorootparentprevUSDS aren’t idiots, and if they haven’t realised this yet then maybe this post will focus attention. They want to support every citizen it’s reasonable to support (or at least I did when I was working for GOV.UK) and dropping support from a lack of knowledge shouldn’t be on the agenda. reply Chabsff 19 hours agorootparentSure, but the government HAS to make that determination in a data-driven way. It cannot be based on anything subjective, for what should be obvious reasons.On top of that, said lack of knowledge is a stated goal of all privacy-focused browsers. GA-blindness is an implementation detail of these policies. Any method the government could use to accurately track that information is effectively a bug in need of fixing from the POV of the browser&#x27;s developers.The most practical answer is, as was posted by someone else here, the government spec&#x27;ing to a standard instead of a set of browsers, which it really should be doing in the first place. The mere fact that the current setup means that the government accidentally makes and&#x2F;or breaks winners in that space is justification enough. reply amadeuspagel 17 hours agorootparentThere is no standard that you can use instead of a set of browsers. There is no way of making sure that a website works with a browser other then testing it with that browser. The reason people test with different browsers is not that they want to use non-standard features, it&#x27;s that even standard features often do not work in certain browsers. reply Chabsff 17 hours agorootparentSort of agreed as of today, but this is not something that we just have to accept, especially considering the government doesn&#x27;t really have a need to make use of the full suite of tools provided by the various existing standards. There are safe subsets to be picked in 2023.The government doesn&#x27;t design roads to conform to the top brands of cars and trucks. It specs them to a standard and any manufacturer can have their product certified as compliant. This both protects the public and gives anyone an opportunity to provide products no matter how small their market share is. Doing it the other way around would be madness, and I don&#x27;t see why the same principle shouldn&#x27;t apply here. reply JohnFen 15 hours agorootparentprev> The reason people test with different browsers is not that they want to use non-standard features, it&#x27;s that even standard features often do not work in certain browsers.This is exactly why sites should be tested for their conformance with the standard rather than with specific browsers. Testing against specific browsers just encourages browser-makers to continue to avoid fixing their shit. reply roywiggins 15 hours agorootparentIf you want to make money or reach constituents, this isn&#x27;t actually a viable option for anyone.When people complain that they can&#x27;t submit their DMV forms (or whatever) and you say, \"well we followed all the standards, go find another browser\" and they say \"which one\" and you say \"I don&#x27;t know, we didn&#x27;t test with any browsers, we just test against the standard\" who do you think they&#x27;ll blame? reply JohnFen 12 hours agorootparentSo we can&#x27;t have a well-functioning system because that would interfere with some people making money. We truly do live in sad times. reply roywiggins 5 hours agorootparentThat&#x27;s why I said \"reach constituents\". Governments, nonprofits, utilities, etc, all ideally need to actually reach everyone, it doesn&#x27;t matter how. reply Silhouette 11 hours agorootparentprevGiven the history of standardisation for promoting interoperability includes numerous success stories and a pattern of resistance fading over time I don&#x27;t see why we should expect web standards to be any different. The level of functionality needed by a typical government site has worked close to 100% correctly in literally every major browser for literally decades. reply jasonjayr 14 hours agorootparentprevIs it really a standard if no one implements it in a way that everyone agrees on?Is it really a standard if the dominant vendor just ignores the standard and does what it wants because then it becomes the \"actual standard\" ?As a (terrible) example, look at FIPS. The government has the power to mandate a standard that everyone needs to implement. If instead of \"supporting specific browser vendors\" they \"support a specific standard\" then all vendors have a target that works with an agreed upon common ground. reply JumpCrisscross 13 hours agorootparent> Is it really a standard if no one implements it in a way that everyone agrees on?Plenty of standards are guidelines. The point is the website’s purpose trumps dogmatic adherence to a standard. If the site works against standards but not in the browser, it’s a failure. reply cycomanic 10 hours agorootparentprevSure there is a standard, are you saying the w3 doesn&#x27;t exist? The issue is that somehow people here are arguing that it is sufficient to test that you are adhering to a standard by only testing against chrome, edge and safari (two of which are the same engine).If you want test that you are adhering to a standard (if there is no testsuite available) you need to test against more than 2 independent implementations. reply tomohawk 14 hours agorootparentprevThey&#x27;re the government. They can create a spec or standard for browsers to be able to use government supplied digital services. It&#x27;s done all the time.This would be beneficial guidance for browser developers, but also for anyone developing government sites. Saying \"whatever chrome is doing is our standard\" is a cop out. reply beojan 13 hours agorootparentprevThere was until Google and Mozilla decided W3C wasn&#x27;t moving fast enough and introduced WHATWG. reply rchaud 17 hours agorootparentprevFirefox is not IE8, which declared lot of CSS3 and HTML5 standards as \"won&#x27;t be supported\" in 2009, paving the way for its own extinction.USDS serves basic pages and applets which work fine on Firefox. It is only a handful of very complex web apps like Photopea where the developer will say \"run this on Chrome for best results\". reply TheCoelacanth 18 hours agorootparentprevServer-side tracking of user agent strings should still be possible. reply john-radio 17 hours agorootparentYeah, I think the root comment of this thread is talking out of his ass. Third-party trackers are not needed, and wouldn&#x27;t make sense to use, to discern browser market share; browsers self-report their identity and operating system unless specifically configured not to. For example, my User-Agent string in my GET request to retrieve Hacker News is: \"Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko&#x2F;20100101 Firefox&#x2F;120.0\". reply OkayPhysicist 15 hours agorootparentprevExcept we should be pushing for the elimination of user agent strings, too. They just end up being used as a lazy way for lazy developers to not test for capabilities.The most recent example I&#x27;ve run into is Snapchat&#x27;s web client: It reject&#x27;s Firefox purely by user agent string, but then works perfectly in Firefox if you just have your browser lie. reply Y_Y 12 hours agorootparentI strongly agree. I don&#x27;t think a website is going to do anything with my user agent string that benefits me. I&#x27;d much prefer a web where websites can&#x27;t do things outside of the standard. We ne er needed webmidi, but a company that makes a browser&#x2F;is hybrid definitely wanted it.Speaking of Google, won&#x27;t this interfere with their \"look we&#x27;re not a monopoly\" payments to Mozilla Corp? reply andrepd 15 hours agorootparentprevThat&#x27;s the sort of this that should be illegal for anticompetitive reasons. reply anonymouskimmer 14 hours agorootparentprev> Sure, but the government HAS to make that determination in a data-driven way.They really should be hiring a pollster to ask people which browser they use. It might be a bit difficult for those who use the default, but it wouldn&#x27;t be that difficult to ask a couple of followup questions (e.g. which device) to determine which default browser that is. reply daotoad 11 hours agorootparentI think most people would say \"the internet\" or \"my phone\" rather than something useful. reply jddj 18 hours agorootparentprevI get unsupported browser warnings on my country&#x27;s government portal when I use Firefox.I think it&#x27;s been the case for a good 10 years.It works fine, of course reply Miraste 19 hours agorootparentprevGOV.UK is dramatically more competent than USDS, are they not? I admit I&#x27;m not terribly familiar, but I wouldn&#x27;t want to depend on the agency behind the Department of Education and VA websites for anything. I&#x27;ve seen the VA site, in particular, do serious harm to the people it&#x27;s meant to serve. reply boris-ning-usds 15 hours agorootparentHi!Can you elaborate on what in particular on the VA site that causes serious harm to the people it&#x27;s meant to serve? I&#x27;m happy to bring it to the team or if you don&#x27;t feel comfortable with stating it here, please bring it up to the open source website for va.gov. https:&#x2F;&#x2F;github.com&#x2F;department-of-veterans-affairs&#x2F;vets-websi... reply mardifoufs 18 hours agorootparentprevI don&#x27;t think the USDS makes all or even most of the current federal gov websites. reply boris-ning-usds 15 hours agorootparentUSDS certainly does not. The guiding principle at the time from what I remembered is to try to set \"some standard\" for all federal government websites to strive to because there were none.We also helped with design and infrastructure support for ssa.gov (launched earlier this year) with a contractor team to try to boil down 60,000 pages to ~30 pages that people tend to use.&#x2F;&#x2F; opinions of a former USDS, no longer with the team reply Miraste 18 hours agorootparentprevNo, they don&#x27;t. But they did create the VA site: https:&#x2F;&#x2F;www.usds.gov&#x2F;projects&#x2F;va-dot-govAnd they&#x27;re involved with the Department of Education, although I couldn&#x27;t find out exactly what they built. reply llimllib 15 hours agorootparentthe va site is vast, to say any one team created the VA site would not be even close to correct.They did help change over the old va.gov to the new va.gov (which was built as vets.gov), which imo has been a huge improvement.(There are still many, many problems with the VA and its website. Just saying that 1. the new va.gov is a big improvement and 2. the USDS is far from being a single point of responsibility for it, or even _the_ major point of responsibility) reply mardifoufs 17 hours agorootparentprevOof, that sucks to hear. I had a hard time believing they&#x27;d be behind that website seeing that I often hear complaints about it. :( reply dblohm7 19 hours agorootparentprevSome of the USDS people are even ex-Mozilla! reply nvm0n2 18 hours agorootparentprevThe 2% rule USDS use is taken directly from GDS (gov.uk), so that isn&#x27;t what your old employer&#x27;s policy is at all. They have to define reasonable, after all. reply robin_reala 18 hours agorootparentThat part of the article was just wrong, to be fair (I used to be head of the front-end community at GDS and helped set things like browser support schedules). Our default approach was to add up all browser share (yes, based on GA, but this was a while ago) until we got to 99%, then removed the browsers that were left over. So typically we wouldn’t consider dropping a browser version until it was less than 0.4% usage. You can use a colleague’s client-side tool to test this approach: http:&#x2F;&#x2F;edds.github.io&#x2F;browser-matrix&#x2F; (assuming it still works).The browser support is currently listed at https:&#x2F;&#x2F;www.gov.uk&#x2F;service-manual&#x2F;technology&#x2F;designing-for-d... . The mention of 2% is specific to IE11, so I guess this was special-cased at some point. reply nvm0n2 15 hours agorootparentI stand corrected, thank you. reply pas 13 hours agorootparentprevWell, if the world including whatever governments, institutions, groups, interests, companies, FOSS projects, OS distros, etc... all want to depend only on Chromium, then fine.If Mozilla can&#x27;t even play this card, then they should really just shut down.Or, maybe, like Wikpedia, they should stop begging for more-more-more, and spend what they get wisely.Not to mention, they could simply start bug bounties and&#x2F;or crowdfunding for actual deliverables and&#x2F;or services (ie. a security team).And if all this fails, then it fails. Maybe we&#x27;ll simply get a Firemium or Chromefox&#x2F;fix whatever the name. A fork where adblock works. reply stcredzero 16 hours agorootparentprevPer the article, this is irrelevant. The case being made is that the figure seen by the government falling under 2%, which it is getting close toSo, the default position of the government is, \"If we can&#x27;t surveil you, we can&#x27;t help you?\" (Or taken the other way, you want us to help you? Let us surveil you!) This seems to be how it works out in practice, just because of favorable economics in mass surveillance. Example: RFID and license plate readers for toll collection. Various registrations with government agencies are another example. reply chrismorgan 17 hours agoparentprev“Enhanced Tracking Protection” is a poor name: it’s not just one thing, but has two modes: Standard and Strict. (There’s also Custom, which lets you go somewhere between the two, or restrict cookies even more tightly.)The default is Standard. It doesn’t block GA. The cynic in me suggests they decided making Firefox disappear altogether from popular stats by default would have harmed them more than not doing it harms their users, or that the backlash would be too great for their liking.Sources like Google Analytics and Statcounter are still chronically undercounting minority browsers and platforms, which are much more likely to block these sorts of things, and Firefox and Linux will be particularly heavily hit, but I’m sure the difference it makes isn’t as large as I’d like it to be. reply throw10920 19 hours agoparentprevThe US government&#x27;s page it uses to track web browser usage uses an analytics engine made by a company that makes its own web browser? That sounds like a pretty big issue! reply dreamcompiler 11 hours agorootparentIt&#x27;s also a company the US government is presently suing for antitrust violations. That seems like an issue too.https:&#x2F;&#x2F;www.npr.org&#x2F;2023&#x2F;09&#x2F;12&#x2F;1198558372&#x2F;doj-google-monopol... reply mavhc 18 hours agorootparentprevWeird that no one checks their own logs any more reply ksherlock 15 hours agorootparentI would guess lots of people no longer have access to the log files -- because the cloud is somebody else&#x27;s computer (GitHub pages, etc) or because you&#x27;re web scale and run your web server in docker so log files are ephemeral or maybe you&#x27;re web scale so the only logging is dumping status messages to a background screen session. reply Steltek 17 hours agoparentprevWhat stood out to me was that they didn&#x27;t break out mobile from PC. The mobile landscape dominates usage these days and extremely few people make an active choice in browser there. The presented stats to me reads more like Android vs iPhone than it does Chrome vs everyone else. reply culi 17 hours agorootparenton iOS all browsers are forced to use WebKitAnd how could Firefox possibly complete against default browsers? Is it even worth the investment? reply rchaud 16 hours agorootparentThey tried with FirefoxOS almost a decade ago but couldn&#x27;t find any hardware partners. reply fabrice_d 13 hours agorootparentFirefoxOS was not stopped by Mozilla leadership due to a lack of hardware partners. The distribution model - mostly through carriers - failed because a key app was not available (WhatsApp) which caused a chicken and egg situation: no sales because no app, no official app because not enough users. reply geoelectric 12 hours agorootparentThat&#x27;s pretty interesting. I was around back then and hadn&#x27;t recalled that there was such a lynchpin issue, but that makes a lot of sense as to why it suddenly hit such a hard stop. Were you able to support WhatsApp with KaiOS, or did the feature-phone target make it moot? reply vetinari 11 hours agorootparentprevAt the time, I had a FirefoxOS device made by ZTE in my hands.It was so slow, it was unusable; even the first Android devices running on underpowered hardware were speed champions in comparison. Looking for an Whatsapp launcher was beyond the patience of even dedicated fans. reply fsflover 10 hours agorootparentprevSomehow this chicken-egg problem isn&#x27;t stopping GNU&#x2F;Linux phones.Sent from my Librem 5. reply isodev 15 hours agorootparentprevThe rendering engine doesn&#x27;t really matter - nothing is stopping Mozilla from implementing a better browser app than Safari.Safari is _really_ good and a very high bar to catchup to, even if you don&#x27;t have to implement the rendering engine and Mozilla is not exactly known for their friendly and refined UIs. Vivaldi on iOS is much younger by comparison and looks a lot better than Firefox. reply culi 15 hours agorootparent> The rendering engine doesn&#x27;t really matterI disagree. For the purposes of this conversation, I feel like it&#x27;s the only thing that matters. The web doesn&#x27;t benefit at all from a WebKit-based browser by Mozilla capturing some of the iOS marketshare. Websites will still have to cater to WebKit. Not Gecko or something new> Mozilla is not exactly known for their friendly and refined UIsI actually strongly prefer the Firefox app&#x27;s UI to safari reply isodev 15 hours agorootparent> For the purposes of this conversation, I feel like it&#x27;s the only thing that matters. The web doesn&#x27;t benefit at all from a WebKit-based browser by Mozilla capturing some of the iOS marketshare.I believe it matters because it shows Mozilla&#x27;s ability to market their product (Firefox). If they continuously fail to capture user base on any platform, then what powers Firefox is of little consequence.As an experiment, I just installed Firefox on iOS just to see what&#x27;s up and honestly 4 screens of things to confirm before I even get to the browsing part? As a tech person I understood each of them of course, but no sane person would put 4 screens in a row blocking users from using an app they normally already know how top use. So no, I don&#x27;t believe Mozilla has the required UI&#x2F;UX skills. reply Steltek 14 hours agorootparentprevMobile browsers aren&#x27;t sufficiently differentiated enough for a significant amount of people to bother changing the default. Does anyone really think Samsung Browser would get close to 1.6% if it were a free and unbiased choice?Apple sets the rules, gets special access (new releases, features, platform changes, countless other things), and relentlessly captures their users into the Apple ecosystem&#x2F;moat. You seem to think swapping the rendering engine is a trivial task but you&#x27;re asking them to practically create a new browser. And for all that effort, you&#x27;re still competing with an opponent that&#x27;s basically cheating. I&#x27;m not sure why Firefox&#x2F;iOS even exists, frankly. reply russelg 10 hours agorootparentFirefox on iOS exists so desktop Firefox users can sync their bookmarks, tabs, passwords and history. I use Firefox on my iPad all the time. replyNelsonMinar 18 hours agoparentprevDo you have a source for a more accurate picture of Firefox&#x27; market share? Every single report I&#x27;ve seen shows the same thing: long downward trend and a tiny fraction of the usage of Chrome and Safari. reply conradfr 17 hours agorootparentI have a modest (non-technical) side project (a few thousands visitors per day) that is used worldwide.GoAccess puts Firefox at 7.8% and Google Analytics at 3.3%.GoAccess puts Chrome at 57% and Google Analytics at 73%. reply simbolit 11 hours agorootparentNon-technical as in \"the average user isn&#x27;t a nerd\" ?For non-nerd populations, these numbers seem high. reply conradfr 11 hours agorootparentYes I realized too late the term might be confusing. It&#x27;s a radios streaming website&#x2F;app, so not nerdy.There is an Android app that uses a webview so that even favours Chrome. Also GoAccess doesn&#x27;t unfortunately filter all scrappers&#x2F;bots and most of them declare themselves as Chrome.A sizeable part of the traffic is (non-app) Android and I don&#x27;t think Firefox is huge there so maybe on desktop Firefox market-share is actually not as bad as we think? reply flohofwoe 19 hours agoparentprevIf that&#x27;s the case there would be a sudden decrease in the market share graph around 2019 (although that graph is from Statscounter which has Firefox currently sitting at around 3.2%).The decline has already been happening since around 2010 without any drastic ups or downs. reply mozTA 11 hours agoparentprevMozilla employee here.Truth of the situation is probably worse. We are losing users faster than anticipated. reply lucasyvas 7 hours agorootparentDo you anticipate the losses are due to people with aging machines upgrading and using a different browser? As a very happy Firefox user, I have a very hard time believing it&#x27;s because people find the experience to be poor. I feel as if the losses you are describing have to be losses to defaults on new devices and people that fairly don&#x27;t care enough to change them.The desktop is virtually dead for any \"normal\" user, and their Android or Apple tablet&#x2F;phone don&#x27;t prompt them with a choice (other vendor constraints notwithstanding). It&#x27;s not remotely a fair fight. You are guaranteed to lose users for reasons totally outside your control. reply achenet 52 minutes agorootparentlongtime Firefox user here. I switched to Chromium recently bc Firefox started bugging on me. reply Semaphor 17 hours agoparentprevJust to add some anecdata, we have both Google Analytics (if you accept the GDPR tracking request…) and our own internal statistics based purely on useragents. Here are some percentages for November (this is Germany, generally far higher FF usage)Ours:Chrome 37.4% Firefox 24.7% Safari 21.1% Edge 7.5% Opera 2.6%GA:Chrome 39.3% Safari 31.5% Firefox 11.9% Edge 9.9% Samsung Internet 2.9%For us, GA is undercounting FF by almost 13 percentage points, over 50%. reply nextaccountic 13 hours agorootparentif google analytics is undercounting firefox in a way that may make websites to drop firefox support, i suppose this is a case for antitrust?because google is using a near monopoly on analytics to bury a competitor in another segment reply soundnote 9 hours agorootparentIt&#x27;s not GA being malicious in any abnormal way. Some Firefox users are blocking some GA functionality, so GA can&#x27;t count some Firefox users. reply coldcode 13 hours agorootparentprevMy programming blog (https:&#x2F;&#x2F;thecodist.com) sees Chrome 52%, Safari 27%, Firefox 9%, over six months covering a fair amount of the world. I use Plausible. I never found GA to be very reliable. reply culi 17 hours agorootparentprevOf course this heavily depends on niche. In some European countries Firefox is at almost 20% marketshare reply Semaphor 17 hours agorootparentI mentioned that:> this is Germany, generally far higher FF usageBut the interesting part is not how much FF has here, but how much GA undercounts it. reply culi 16 hours agorootparentYeah it&#x27;s interesting. I wonder if it&#x27;s done on purpose or if it has to do with the way bots&#x2F;crawlers are counted reply Semaphor 16 hours agorootparentSee parent. The default in FF is to shim the GA script reply hnbad 13 hours agorootparentAlso GA requires opt-in in the EU (even if some sites try to be clever and illegally make it harder to refuse than to accept) whereas presumably their first-party tracking is done in such a way consent is not necessary (e.g. sufficient anonymization that a data point can not be correlated with the user it originated from). Presumably FF users are more likely not to consent by default to begin with. replymvdtnz 10 hours agoparentprevPlease define your acronyms. reply toss1 19 hours agoparentprevIf this is the case, it seems that they need to recode the Enhanced Tracking Protection for GA to ensure that they get flagged as FFox.Might also be useful to have a plug-in to make a daily &#x27;ping&#x27;&#x2F;check-in from FFox to any govt sites used by their users. E.g., I use USPS and SBA&#x2F;SBIR sites, but only occasionally or monthly, but if most FFox users who did so got logged more like daily instead of ~fortnightly or ~monthly, it&#x27;d improve the numbers. (Obviously, also must be done carefully so as to not get wholesale discounted).The cascade effect of the US Govt abandoning support would be catastrophic, likely terminal, which would be bad for everyone. reply worik 15 hours agorootparent> ...it seems that they need to recode the Enhanced Tracking Protection for GA to ensure that they get flagged as FFox.That would be backwardsThe main reason to use FF is the privacy protectionsI find it very frustrating they do not get more recognition for the work they do on that front reply hs86 19 hours agoprevI switched over after the Manifest v3 debacle, and after a couple of months, I wonder if the implications of using a browser with a lower market share are overblown.I haven&#x27;t encountered any site that misbehaves, and the only missing feature so far is within the Google Drive web app because it uses a Chrome-only extension [1].Maybe the ongoing standardization of the web shows its effects here, and using a standard-conform niche browser is not that bad anymore.[1] https:&#x2F;&#x2F;chrome.google.com&#x2F;webstore&#x2F;detail&#x2F;application-launch... reply cjpearson 10 hours agoparentIn my experience there are almost never issues. Standardization has gotten much better, and as a web developer I appreciate all the effort the various browser teams have put into ensuring common behavior.The effort of \"supporting Firefox\" as a developer is basically opening the app in Firefox and confirming that it works, which it basically always does because the app is built on standardized web features. (Other types of support might be different. e.g maybe you have help doc that tells your users how to clear the site cookies. Do you also want to write instructions for Firefox, Safari, Opera etc?)The bigger concern is that some day Google might not need web standards. If developers target Chrome rather than the web platform, Google has free reign and no longer needs to compromise with Apple and Mozilla. For example, Mozilla&#x27;s veto of the controversial Topics API would hold little weight if nobody cares about Firefox support. If Chrome gets to a dominant enough position it could move from embracing to extending quite quickly.People have different opinions on Google, but even those who believe Google has no ill intentions still feel better if there are some checks and balances. Were Firefox to become obsolete, Apple would be the last one standing. And with the DMA, some are predicting that Safari could soon follow Firefox&#x27;s decline. reply spacechild1 12 hours agoparentprevSame for me! I have been always using Firefox as my main browser for at least 15 years and never had any problems. Actually, I don&#x27;t really understand why so many tech-savvy people continue to use Chrome... Just switch to Firefox, for foxs sake! reply aendruk 10 hours agorootparentOver the last year or so various websites have mysteriously walled me off with 403s and 5xxs only when I use Firefox. I suspect that merely using a less popular browser is tripping some attempted security measure. reply neltnerb 15 hours agoparentprevI have had several sites refuse to work properly (American Airlines maybe? It was some airline) with every single add-in disabled and with every security feature I could find disabled.I am with you that it&#x27;s super rare, I have only had to open chrome due to it being that bad a few times, usually I just need to disable javascript blocking or enable more cookies. But there are legitimately sites that are just so badly written that they won&#x27;t take your money if you use Firefox.I guess Google meet has frozen video for me on Firefox, but I expect Google to intentionally break their website for people not using Chrome after the \"DRM Website\" thing struck them as a great idea. Using Google products at this point is just asking for more lockin when we should really all know better. At least Zoom is a different company from Google...But overblown for sure, if I disable javascript blocking and cookie autodelete and temporary containers that keep the site from realizing I already logged in -- pretty reasonable issues -- 99.99% of the issues I have vanish. reply nightpool 14 hours agorootparentI just bought tickets on AA a few months ago with Firefox and didn&#x27;t have any issues reply nophunphil 11 hours agorootparentEchoing this. Just bought AA tickets through the airline’s website yesterday using FF and it worked. reply neltnerb 9 hours agorootparentIt was selecting seats that was too broken to continue, though I mean, I already \"worked around\" the issue. I tried until it was less work to redo all the manual data entry in Chromium, I usually try pretty hard to avoid using it.I don&#x27;t doubt that it works for a bunch of people, others have stated things that don&#x27;t work and had others say they do work, so something is up. Maybe we&#x27;re using different versions or operating systems or installation dates with old configuration data. reply rozap 14 hours agoparentprevI agree it seems overblown. I haven&#x27;t encountered a site that doesn&#x27;t work. Even google meet and zoom&#x27;s web client work great.Not sure what all the fuss is about. It&#x27;s a great browser. Chrome got very, very aggravating and I&#x27;ve had no problems since switching maybe 4 years ago. Even FF mobile works well. reply shiroiuma 2 hours agorootparentOne problem I have is that screen-sharing doesn&#x27;t work in Messenger video chat. I have to switch to Chromium for that to work, so I have to remember to start the chat in Chromium instead of FF where I have a Messenger tab pinned. reply mtVessel 12 hours agoparentprevNot sure how so many in the thread are using FF problem-free? In the past few years I&#x27;ve encountered more and more sites that don&#x27;t work properly with FF. Most of the issues are around modals that won&#x27;t dismiss or even display at all. Other issues I can&#x27;t identify, but the site becomes unresponsive or some content won&#x27;t load. I think the Ticketmaster ticket selection page was the last one I couldn&#x27;t use at all with FF. My cable company&#x27;s site became Chromium only sometime this year and it still is.Sometimes the cause is uBlock Origin, so I always try turning it off and refreshing. Rarely it&#x27;s due to enhanced tracking protection. A few times I restarted FF in safe mode to rule out add-ins. It&#x27;s always just FF. reply 8bitsrule 3 hours agorootparentSame here. I used to have more FF problems with .gov sites but in the past 2-3 years, rarely. Now & then I have to manipulate NoScript or uBlock to get some feature working on most sites.But most of the time, I&#x27;d rather ignore those sites anyway. After I have a look at them ... which I tend to do after NoScript show me the huge list of crap they expect me to load just for the &#x27;privilege&#x27;. Good developers&#x2F;orgs just don&#x27;t do that. Most go into a &#x27;Block list&#x27; I mostly use to remind myself not to bother trying. I take my business elsewhere. reply ImaCake 11 hours agorootparentprevI had some issues with several sites around 2-3 years ago but I haven’t been experiencing them recently.I guess FF has issues on a fairly small set of sites which some users use a fair bit while most users don’t see those sites at all.The only site I use regularly which has issues is google earth and the reason why is obvious. reply alextingle 11 hours agorootparentprevAnother FF user here. Very, very few issues.I had a problem with a shopping web-site the other day. I disabled uBlock origin, and then all extensions, but it still refused to work. I thought I&#x27;d finally found one of these mythical \"Chrome-only\" sites, but no - exactly the same broken site in Chrome, so the site was just totally broken. Nothing to do with FF. reply aboodman 14 hours agoparentprevSpeaking as a library developer, Firefox is expensive for us to support in https:&#x2F;&#x2F;replicache.dev and https:&#x2F;&#x2F;reflect.net due to lots of long-open bugs, particularly in the storage system. Firefox also generally has the slowest performance which affects us.I&#x27;m not bagging on the team, it&#x27;s frankly amazing they&#x27;ve been able to mostly keep up, it&#x27;s just a fact that maintaining a competitive web browser is a gargantuan task that requires a large team and investment. reply asdff 13 hours agorootparentWhen people say firefox is more performant than other browsers, they mean to say you can use actual powerful adblockers like ublock origin that then make it a totally rigged race against other browsers in terms of real world performance. reply usr1106 11 hours agorootparentWhich is a good thing. Running ads is triple pollution. On the servers, on the browser, and by the manipulated comsumers buying crap they don&#x27;t need in the first place.(Yes, I do pay for content I care about.) reply mmcgaha 14 hours agoparentprevI have been using FF as my primary browser for a few years now. I have not found any site that does not work but sometimes I have to turn off uBlock Origin. reply preinheimer 15 hours agoparentprevSpecific problems I&#x27;ve had as a firefox user:- I can&#x27;t pay municipal license fees (Site says \"Use Chrome\")- I can&#x27;t use the Vendor management portal for $largePopularTechCompany type company for my small business (\"We support Chrome\")- Xero my small business accounting software just doesn&#x27;t work properly (\"Use Chrome\") reply hsbauauvhabzb 13 hours agorootparentSome Xero products don’t work properly no matter what your browser is.For me, Mozilla has bugs or weaknesses in Linux. YouTube chews my cpu (hardware acceleration is broken, at least for me). Tabs ‘freeze’ due to a bug in an X11 lib. Pop up bubbles (do you want to enable gps to this site type bubbles) if visible before switching workspaces appear on all workspaces but cannot be actioned and appear on top.Mozilla as a company appears to be attempting poor monetisation models like attempting to build social networks etc. I would consider paying them a yearly fee if I thought they’d use money wisely, but at the moment executive appear more interested in vanity projects. reply amanzi 15 hours agorootparentprevI&#x27;ve used Xero for years with Firefox without issues. reply crabmusket 13 hours agorootparentSame here, though I only use a small subset of Xero features. reply neltnerb 15 hours agorootparentprevMaybe you can share your configuration? I wonder if you are lucky or they are unlucky. reply amanzi 14 hours agorootparentI run Firefox with uBlock Origin enabled and set the Firefox enhanced privacy settings to the strictest settings. I haven&#x27;t done anything special to get Xero working - never had any issues. reply neftaly 8 hours agorootparentprevAre you using Firefox ESR? It usually has worse support then the regular releases. reply dvngnt_ 15 hours agorootparentprevi assume this is just a user-agent check though i don&#x27;t expect regular users to know how to switch. reply zamadatix 14 hours agorootparentThis is also a double edge sword in that if it&#x27;s just a user-agent check the site tends to lack a well coded fallback for any functionality that is actually not supported and the page can silently break during usage. Not necessarily a problem for those that already know how to switch, but even just installing an extension for other people doesn&#x27;t mean the sites are suddenly actually compatible. Sometimes even big sites like Microsoft Teams have this kind of problem for years at a time. reply matteoraso 12 hours agoparentprevAgreed. Even if Firefox makes up a small part of the market share, any website that serves millions of people will want to support it. Imagine if you have a website with 1,000,000 monthly visitors, and 2% of them use Firefox. Dropping support will mean 20,000 less monthly visitors. This gives us an idea of how much you should spend on support. If you make an average of $0.10 a month per user, you should be okay with spending at least $2000 a month on Firefox. That&#x27;s a pretty big budget, and it&#x27;s reasonable to go much higher than that if your website is rapidly growing. The big players like Google and Facebook will also be comfortable supporting Firefox at a loss, since you don&#x27;t want to bleed users and create market space for a competitor. At most, you lose a few small websites that you probably weren&#x27;t going to visit anyways. reply berkes 2 hours agorootparent> At most, you lose a few small websites that you probably weren&#x27;t going to visit anyways.I think this is wrong. It starts with a website to look up train departures that no longer works on Ff. Then the site to buy movie or concert tickets fails on FF. Booking a restaurant, again, requires you to take out chrome.Which has two concequences. People stop using FF all together because switching all the time is work. And people ensure they always have a copy of chrome around, while fewer and fewer need to keep a copy of FF around.And so FF spirals exponentially into insignificance. reply parineum 11 hours agorootparentprev> Imagine if you have a website with 1,000,000 monthly visitors, and 2% of them use Firefox. Dropping support will mean 20,000 less monthly visitors.It will always be 2% which will always be, at maximum, 2% more revenue. That&#x27;s probably negligible if you have to support them and there&#x27;s an opportunity cost to that money. You&#x27;d probably be better off spending that money on advertising than Firefox support, especially since Firefox users typically know when a page isn&#x27;t working, it&#x27;s probably them and they have a backup solution available.I used Firefox for a long time but switched because I just got tired of switching all the time and I started regularly using a site that didn&#x27;t work in Firefox (I don&#x27;t remember now but I think it was my credit card). reply riversflow 11 hours agorootparent> they have a backup solution available.Im a decade+ daily driver of Firefox. If a website doesnt support it, I make a mental note that I hate this company now, close the tab, and move on. I don&#x27;t open it in a V8 browser, i look for an alternative. reply ryukoposting 6 hours agoparentprevA handful of Google services are buggy and slow in FireFox. Stackdriver in particular is an absolute shambles. Thankfully I&#x27;m an embedded guy so most of the stuff in Stackdriver is hieroglyphics to me anyway.I can&#x27;t prove that Google is doing that intentionally, but I wouldn&#x27;t put it past them. reply Ikatza 11 hours agoparentprevFor one, HN misbehaves pretty badly on mobile. reply zizee 11 hours agorootparentIt does? In what way? I have used Firefox mobile for years on HN without noticing issues. What am I missing?Edit: I used Firefox on Android, the issues you describe might be Firefox on iOS, which is a different beast. reply K0nserv 19 hours agoprevTwo things about this:1. Firefox blocks various analytics and tracking quite aggressively by default. Additionally, users of Firefox are, by and large, privacy minded and will have further mitigations. Any count of Firefox users is likely to be undercounting.2. For the kind of basic web stuff(simple pages, forms etc) that USWDS supports it shouldn&#x27;t matter greatly if Firefox is not supported. Theses standards are mature and Firefox supports them well, most thing should just work. Now, if websites go out of their way to block Firefox users that&#x27;s a different problem. reply slig 17 hours agoparentFF is at 4% usage on Cloudflare Radar [1] which doesn&#x27;t use JavaScript to measure usage.[1]: https:&#x2F;&#x2F;radar.cloudflare.com&#x2F;adoption-and-usage reply athrowaway3z 15 hours agorootparentI use Random User-Agent (Switcher).---But i doubt that will drive the numbers. On a side note, I think percentages will overstate firfox&#x27;s decline. The number of devices with browsers per person will influence it heavily and that number is ever increasing.I think the average person in my circle has more than 3 and many have more than 4 devices with they use to visit .gov sites (i.e. ipads, phones, laptops, but not including the fridge, car, tv, etc) reply neilv 14 hours agoprevInstead of government leaving it up to \"market share\", in an industry with decades of documented history of underhanded anti-competitive behavior, how about:1. All government Web frontends must be compliant with one of the government-defined profiles of browser features, which are defined in terms of W3C (not WHAT, not Chrome) open standards. With sufficient penalties to motivate compliance.2. As a practical matter, developers of government Web-based systems -- in addition to developing to documented open standards, and using open standards-based libraries&#x2F;frameworks -- will be motivated to test with multiple browsers, including Firefox, because that&#x27;s the most likely way that end users will discover and report noncompliance with the standards and profiles.3. Government \"apps\" for non-Web platforms, such as Apple iOS and Google Android, are strongly discouraged. Furthermore, such non-Web apps by default are not compliant unless complete comparable functionality is available via compliant government Web frontends&#x2F;apps. (To get permission for exceptions in extraordinary circumstances, there will be an onerous and uncertain process, and thus the motivation is to invest in the open standard Web platform for any \"extraordinary\" platform facilities that might be needed.)(Also there would be regulations about backend implementations; that&#x27;s just about browsers.) reply JumpCrisscross 13 hours agoparentNos. 1 and 3 will win you the developer pico-vote while royally pissing everyone else off.No. 2 is the matter at hand. You need a cut-off for the multiple browsers requirement. If you don’t, you’ll find contracts to CronyCorp for testing every site against the CronyCorp browser. reply insanitybit 9 hours agoparentprevI don&#x27;t think the government wants to (or possibly is even able to) get involved in defining web standards. Tracking market share seems like the least hands-on way to determine support as it&#x27;s purely \"pull based\" - they follow everyone else&#x27;s lead, they aren&#x27;t trying to dictate what a \"good\" browser is.The government is generally very slow and deliberate about technology recommendations let alone requirements when it comes to generally available software. reply neilv 9 hours agorootparentThere&#x27;s been credible international standards since early on. The government can say \"use that international standard for government systems, not these (more) proprietary and unstable things\". reply nbittich 1 hour agoprevI have switched to Firefox in 2022 as I was tired of Google acting like assholes. I didnt see a single broken website since then. I believe more and more people will do that in the future, maybe not switching to firefox but something less ad-focused and more privacy focused. I&#x27;d rather use netscape communicator and only be able to visit 0.000000001% websites than going back to google products. reply cwales95 19 hours agoprevSomething has to change. Firefox is a GREAT browser. I think a lot of these things boil down to poor marketing. Things like these have to appear &#x27;cool&#x27; and appeal to people. There has to be a reason for people to go out and actively want to download Firefox. Something akin to Apple&#x27;s privacy adverts is how I&#x27;d go about it. reply maldev 13 hours agoparentThey&#x27;ve been trying that and it just pushes people away. I stopped using Firefox when Mozilla tried to score social brownie points. And the numbers seem to collaborate with shrinking and shrinking marketshare, especially after these campaigns. Since on the flip side of what you said, why would people swap from Firefox to chrome, Chrome doesn&#x27;t bring anything shiny. reply cwales95 13 hours agorootparentI agree with some of what you said about social brownie points. Advertising can be really off putting (I&#x27;m very anti-advertisement but I&#x27;m not really their target audience since I mostly use Firefox these days). However, they have to do something to get the word out. There needs to be a reason for people to care to install Firefox. reply rumdz 19 hours agoparentprevI agree. I&#x27;m not a Firefox fanboy. It has literally run better for me than Chrome for a few years now. reply laurent123456 13 hours agorootparentI&#x27;m always surprised to read claims that Firefox is the same or better than Chrome.I switched to Firefox recently and many sites don&#x27;t quite work: for example the pull request popup menu on GitHub appears off screen so can&#x27;t be clicked on; the \"new post\" panel in Discourse is obstructed by the keyboard; FastMail alert box buttons don&#x27;t work, and many other such annoyances.It can be used as a main browser but it does have problems. I wouldn&#x27;t bother with it if it wasn&#x27;t for the manifest v3 situation reply cwales95 13 hours agorootparentI use GitHub frequently and have never came across that issue; can&#x27;t speak for the other sites though. If you&#x27;re sure it&#x27;s not the website&#x27;s fault I&#x27;d encourage you to submit feedback: https:&#x2F;&#x2F;webcompat.com&#x2F;issues&#x2F;new reply laurent123456 12 hours agorootparentThere&#x27;s an open issue about it, so hopefully github should fix it eventually. It&#x27;s on firefox mobile reply Murfalo 8 hours agorootparentFirefox mobile is its own beast. I experience a whole host of issues on it, but have never had issues with Firefox on desktop. Only reason I use FF for Android is for uBlock, but it&#x27;s so unreliable I&#x27;m dying for a better option... reply kiwijamo 11 hours agorootparentprevI am regular and long time user of Fastmail in Firefox and I&#x27;m surprised at your report of issues in Firefox. Can you describe the steps to reproduce the issues you found? reply laurent123456 10 hours agorootparentUsing the Android mobile app, create a Fastmail link and add it to your home screen. Open the app, try to block a sender - it asks you to confirm and it&#x27;s not possible to press either buttons.The same workflow works fine in Chrome. reply kiwijamo 9 hours agorootparentYep I can confirm that certainty doesn&#x27;t seem to work in a reliable way in Firefox on Android. I could only get it to work by tapping the button several times. Either it&#x27;s not responding quickly enough, or the input field only works if you tap in a specific spot. The general performance of Fastmail in Firefox on Android is quite poor as well -- despite it being fairly snappy in desktop Firefox on Linux&#x2F;Windows. I use the native Fastmail app on Android which performs much better -- I presume it uses a version of Chrome under the hood. reply gbear605 12 hours agorootparentprevMy experience is that Chrome winds up being consistently slower than Firefox, and I’ve gotten multiple friends (who aren’t techies) to switch because they’ve tried it out and agreed that it was more performant. reply layer8 11 hours agorootparentprev> obstructed by the keyboardYou seem to be talking about the mobile version? reply laurent123456 10 hours agorootparentYes I should have mentioned it&#x27;s the mobile version reply signaru 7 hours agoparentprevFor one, Ubuntu should stop breaking Firefox with Snap, even though I think that is a relatively small user base. (There are some suggested fixes but I think the simplest is to bypass Ubuntu&#x27;s packages and just download FF directly from Mozilla). reply iteratethis 13 hours agoprevFirefox was already statistically irrelevant 5 years ago. On our dashboard, a global e-commerce site with billions of views per year, it&#x27;s not even in the top 10. Even regional browsers sometimes surpass it.Firefox is also no longer a developer-default browser. This too has been true for years now.There&#x27;s very little Mozilla can do about it. Chrome and Safari are big because they&#x27;re shipped as a default to platforms with billions of users. And the web works well on both of those browsers. It&#x27;s not an engineering problem. You can&#x27;t improve Firefox and expect market share to rise.It&#x27;s pretty much a done deal, and Microsoft (as well as Brave) using Chromium cemented that deal. reply usr1106 11 hours agoparentSorry, nothing personal. But as a 100% Firefox user I would very likely avoid to visit any global ecommerce site. It&#x27;s the same world I want to avoid by not using Google products.I know I am a small minority, but you don&#x27;t even see that minority. reply insanitybit 9 hours agorootparentEven if you assume that 50% of Firefox users are falling into \"they wouldn&#x27;t visit those sites to begin with\" or \"they run a user-agent spoofer\" you&#x27;re basically going from 2-4% to 4-8%, and the trend is clearly negative. reply layer8 16 hours agoprevAccording to https:&#x2F;&#x2F;radar.cloudflare.com&#x2F;reports&#x2F;browser-market-share-20..., Firefox still has 4.7 % global market share, 4.9 % in the US, and significantly more in some relevant countries, like Germany with 15 %. So this may be a bit premature. It’s still a significant-enough market share to support. Of course, if it continues to decline further, Firefox will eventually become irrelevant. Let’s hope this won’t happen. reply sreejithr 13 hours agoprevFirefox is the only browser that has reliable ad-blocking. Chrome allows ads by default because you know, it&#x27;s from Google. Edge is no better the last I used it. Websites are simply faster on Firefox. But I guess people like me are fast becoming a minority. reply nkg 13 hours agoparentI&#x27;m with you! reply jdlyga 19 hours agoprevFirefox needs to focus all their effort on making a good web browser first. If the browser is slow, has a clunky interface, or lags behind on features, then people won&#x27;t use it. Mozilla focus way too much of their attention on privacy and non-browser related projects.Look at how much attention that The Browser Company has gotten for their Arc browser on Mac. Their primary focus is great UI and making an excellent browser for their users. What has Firefox been doing with all their money and time? reply whakim 15 hours agoparentI always see comments like this on HN, and I struggle to understand why. Firefox is plenty fast. Its interface is extremely similar to most of its competitors. It works well. What special sauce do you expect Mozilla to implement that&#x27;ll suddenly change their fortunes? It&#x27;s a browser, after all. And why do you think that such features aren&#x27;t being implemented due to lack of resources or muddled priorities - surely Mozilla can walk and chew gum? reply worik 15 hours agorootparent> I always see comments like this on HN, and I struggle to understand why.Many years ago Firfox was very slowIt has improved enormously, obviously, but some people never forgetIt is a lesson. Never take your eye off the ball. Firefox did, back in the day, and Google ate their lunch reply ruszki 15 hours agorootparentThey rely on 10 years old information in IT. It’s extremely naive, and this mindset definitely hurts them in long term. reply MadWombat 11 hours agorootparentprev\"What special sauce do you expect Mozilla to implement that&#x27;ll suddenly change their fortunes?\"XUL extensions maybe? The reason I gave up on Firefox after literally decades of using it was because they kept removing features I was actively using without fixing any of the problems. What&#x27;s the point of using a niche browser if it is exactly like the non-niche browser, but with more compatibility issues? reply fauigerzigerk 12 hours agorootparentprevI have two PWAs that I use all the time and Firefox doesn&#x27;t appear to support PWAs.There is a third party extension for it but I&#x27;m generally reluctant to install browser extensions because I worry about security. reply beiller 10 hours agorootparentI believe it supports pwas fully. Click ... Menu and click \"add to home screen\" reply fauigerzigerk 3 hours agorootparentThere&#x27;s no such menu on macOS and nothing that looks similar. reply mvdtnz 6 hours agorootparentprevEvery time I launch Firefox (rare) it gives me a popup telling me it needs to install an update and to please relaunch. Come on man. It&#x27;s not 2006 anymore, that kind of update prompt doesn&#x27;t fly anymore. reply batiudrami 6 hours agorootparentIt doesn’t run a background process on your computer 24&#x2F;7 to ensure you’re up to date like Chrome. Weird objection; I prefer it this way. reply mvdtnz 6 hours agorootparentI prefer it when software does the tedious stuff for me. reply davidelettieri 14 hours agorootparentprevIt is quite slow on android IMHO. At least for me, in comparison with Chrome.I also use ublock origin in android which should make loading page faster I guess but unless the page is absolutely awful, chrome remains faster even with ads on. reply jay_kyburz 14 hours agorootparentprev> I always see comments like this on HN, and I struggle to understand why.I think its OSX and Windows people talking past each other.On a Mac, Firefox is pale in comparison to Safari. reply zamadatix 14 hours agorootparentFor a long time it was absolutely awful on Android as well. Not sure about Android, but at least on macOS Firefox has about identical performance as of this year now. They did a lot of work, some macOS specific, in recent times.That said your average person isn&#x27;t trying out every browser multiple times per year to see how fast it is today. reply nottorp 12 hours agorootparentprev> On a Mac, Firefox is pale in comparison to Safari.How does that help when you can&#x27;t run uBlock Origin on Safari? reply dralley 13 hours agoparentprev>Firefox needs to focus all their effort on making a good web browser first. If the browser is slow, has a clunky interface, or lags behind on features, then people won&#x27;t use it.People don&#x27;t use one browser over another because of performance, full stop. Certainly not over 10% to 20% differences. Even years ago when Chrome did have an advantage, they would never have gained marketshare so quickly if they hadn&#x27;t spammed Firefox users visiting google.com with Chrome ads.Even features don&#x27;t matter. People use Safari, Safari is severely lacking in \"features\". 99% of users aren&#x27;t power users. reply yonatan8070 14 hours agoparentprevI daily drive Firefox, it&#x27;s fast, responsive, and works well for everything I do on the web reply sp332 18 hours agoparentprevFirefox is fast though. reply trealira 18 hours agorootparentYeah. Although I&#x27;m not trying to discount what others experience, I&#x27;m always confused to hear that Firefox is slow, because it seems just as fast as Chromium on my computer. reply Filligree 16 hours agorootparentIt uses a lot more CPU on Youtube than chrome&#x2F;safari, and seems laggy there. reply mrinterweb 15 hours agorootparentThere was some recent concern that Youtube may be slowing down FF https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;youtube&#x2F;comments&#x2F;17z8hsz&#x2F;youtube_ha... reply Filligree 8 hours agorootparentI believe that. But the simple fact they&#x27;re cheating, doesn&#x27;t make FF viable for watching youtube on my laptop. reply HackerThemAll 17 hours agorootparentprevIt used to be slow, and many people who then switched, won&#x27;t be looking back until Chrome stops working. reply digging 16 hours agorootparentIn which case, focusing on edging out more performance from the browser is perhaps the worst use of their time&#x2F;money, no? reply sleepybrett 16 hours agorootparentprevchrome by way of google is making sure it&#x27;s the only thing that youtube still properly works in.When MS tried this shit they got slapped with anti-trust. What google is doing is worse in every dimension. reply galangalalgol 14 hours agorootparentYouTube works fine in all my Firefox browsers on mobile amd desktop. And despite reported difficulty by others UBO continues to ensure I see no ads there. reply ryandrake 13 hours agorootparentA little obscure, but Firefox still does not support HDR video playback on Windows and Linux. I understand they very recently introduced support on Mac finally, which is a good sign. reply vkou 15 hours agorootparentprevWhen MS tried this shit, there weren&#x27;t companies that were selling you locked-down hardware running a locked-down OS running a locked-down App Store, while taking a 30% cut of every economic transaction on that store... reply r00fus 14 hours agoparentprevFirefox on my Macs is more feature-complete and faster than Chrome for me. And the Aweseomebar is truly a replacement for bookmarks for me (full text search showing URLs and titles from years ago with a few keywords is truly amazing).Once manifestv3 starts really making waves, Firefox will be the best place to go for ublock origin and other adblockers. reply marricks 14 hours agoparentprevThat would be my personal preference, laser focus on their browser, but Perhaps they didn&#x27;t because it&#x27;s already good and the reason they&#x27;re falling is combating monopolies. Safari&#x2F;Edge are defaults in their space and have OS&#x27;s that can nudge.Google owns huge swaths of the internet and can nudge people as well and break other OS&#x27;s on whim.Mozilla probably felt the need to have other offerings and leadership to winback something when \"being a great browser\" wasn&#x27;t enough in the past 14 years. reply jen20 15 hours agoparentprevPrivacy must be a tier 1 feature of a web browser.The fact the market leader goes out of their way to shit all over privacy concerns says more about their marketing pull than the quality of their browser. reply aembleton 15 hours agorootparentThat privacy prevents organisations such as USWDS from seeing that it is in use as analytics are blocked. reply zlg_codes 12 hours agorootparentNobody has a right to research you or your behavior. We shouldn&#x27;t be leaving holes in our software because the methods chosen rely on leaky and chatty protocols to gleam info.The user agent string shouldn&#x27;t exist to begin with. It was a boneheaded decision to allow that sort of easy discrimination baked right into the protocol. reply jen20 13 hours agorootparentprevThat means that using telemetry to determine browser market share is a flawed approach. reply cmrdporcupine 18 hours agoparentprevI don&#x27;t really get what you&#x27;re on about. I switched from Chrome to Firefox a couple months ago and... it&#x27;s great. I don&#x27;t notice any differences in performance (if anything, snappier) and the only thing that&#x27;s \"missing\" that I had in Chrome is that Chrome had all my credit-card and password data associated with my Google account, which, well, that&#x27;s not something I want Firefox to have.TLDR: Firefox is a good web browser. It&#x27;s not failing in the market because it&#x27;s not a good browser. It&#x27;s failing because consumers don&#x27;t seem to actually care one way or the other. reply sleepybrett 16 hours agorootparentThe pig doesn&#x27;t have any problem with the farmer until he shows up with the axe. Thus it is with chrome users. reply deepspace 16 hours agorootparentThat axe may just be Chrome&#x27;s war on ad blockers. reply kelnos 15 hours agorootparentI&#x27;d like to believe this, but what percentage of Chrome users actually use an ad blocker? My general feeling is that we on HN think ad blocking is a lot more common than it actually is. reply digging 14 hours agorootparentIt can&#x27;t be that niche, or else why would they spend so much effort fighting ad blockers? They&#x27;ve surely done some research that uncovers a decent userbase of people who do use an ad blocker but will turn it off at a prompt. reply nottorp 12 hours agorootparentprevNo the question is how can you trust an ad blocker will be allowed to do its job in Chrome? Even without Manifest v33333 or whatever. reply cmrdporcupine 15 hours agorootparentprevYep, in my case it was disgust about the (apparently now abandoned) attestation efforts.I worked at Google for 10 years, so my tolerance of them is higher than some, I guess. But also, now, my distrust.Anyways, Firefox is fine. Nice, in fact. reply encom 17 hours agoparentprevIn my opinion, Firefox has been at war with its own users for years. I finally had enough a few years ago, and switched to Vivaldi. Every Firefox upgrade was a gamble on what feature or functionality they&#x27;d remove or change this time, or what bone-headed UI design change they&#x27;d make. And every time there&#x27;d be a Bugzilla bug with a of horde users who just had their favorite feature removed, and every time, without fail, it would get arrogantly WONTFIXed and eventually locked. This cycle has repeated for most of Firefox&#x27;s existence, but it has accelerated.Vivaldi can customise damn near every aspect of its user interface. I can set up every menu how I want it. Remove things I done use, and move the most used item to the top. I can dock my tab-bar wherever I want. I can have a proper status bar. The list goes on. It&#x27;s what Firefox should have been. reply eviks 2 hours agorootparentIndeed, the level of customization in Vivaldi is awesome and what Firefox should&#x27;ve done instead of going the other way (that would counter the lame excuse you link to elsewhere that \"view image\" makes the menu too long - just make the menu editable, duh!), even though their UI is unfortunately not very petformant reply ako 16 hours agorootparentprevUsers are just different, I’ve been a happy Firefox (and pocket) user for many years, and wouldn’t be happy with these so called improvements. I don’t need customizability, just need it to work good enough out of the box. Definitely don’t feel that Firefox is at war with me. reply jjav 12 hours agorootparentprev> Every Firefox upgrade was a gamble on what feature or functionality they&#x27;d remove or change this timeEvery upgrade?I&#x27;ve been using firefox since forever (as long as it has existed) and while I was very annoyed when they removed the customizable UI support a few years ago, that&#x27;s really the one and only time when they broke functionality as far as I&#x27;ve ever been able to notice. reply orbital223 7 hours agorootparentprevThis has been my exact journey, been using Firefox since before it was called Firefox, finally got fed up a couple of years ago and switched to Vivaldi. It&#x27;s such a breath of fresh air not having to dread update notifications anymore. reply signaru 6 hours agorootparentprevI still use FF, but I do miss how customizable its UI used to be. I even made it look like Chrome back then! reply sleepybrett 16 hours agorootparentprev> In my opinion, Firefox has been at war with its own users for years.citation fucking needed. reply bigstrat2003 16 hours agorootparentCitation: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38532851, where the primary source for encom&#x27;s opinions (encom himself) says that it&#x27;s his opinion.Seriously dude, it&#x27;s an opinion and he provides some rationale behind it. Feel free to disagree, but then just say you disagree. This isn&#x27;t a factual claim that can be proven or disproven. reply inversetelecine 16 hours agorootparentprevThe citation is right there: \"In my opinion\" reply sleepybrett 9 hours agorootparentI would like actual facts to back up this opinion. reply encom 3 hours agorootparentFine, here&#x27;s the one of the last changes that put me over the edge:https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=1699128Mozilla removes \"View Image\" from the context menu for no reason at all, and a developer (he&#x2F;him) explains why That&#x27;s A Good Thing, and eventually shuts it down after almost 100 comments disagrees with it. There was a Reddit thread about it as well at the time, but I&#x27;m not digging through that trash heap.As mentioned, this has been a repeating occurrence. \"View Image\" was a tiny thing, but I happened to use it many times per day. And many MANY such tiny things just wears you down eventually. replybambax 17 hours agoprev> I am personally unaware of any serious reason to believe that Firefox’s numbers will improve soon.Is it possible that Manifest V3 will help Firefox?Of course most people don&#x27;t know or care about Manifest V3, but if uBlock Origin or other effective ad blockers cease to work satisfactorily on Chrome, won&#x27;t that make some users switch?I&#x27;d rather go back to fetching web pages from a terminal than suffering the insanity of modern web ads. I can&#x27;t be the only one. reply Moldoteck 3 hours agoparentMost users don&#x27;t care abt mv3. My parents would continue to use the browser without adblocking if I don&#x27;t install it, just because they don&#x27;t have the skills to do this. They don&#x27;t even think about this. Just extrapolate for the entire world reply beaugunderson 14 hours agoparentprevI&#x27;m in the process of switching after many years as a Chrome user... just have to find equivalents for the last of my extensions. reply soundnote 9 hours agoparentprevUnlikely: You can make pretty good adblockers even with Manifest v3, and there are Chromium-based options with adblockers built in. reply consumer451 6 hours agorootparentAs I understand it, Google’s latest raised middle finger is that they will delay updates to extensions.This will allow advertisers to always stay one step ahead, making adblocking much less effective. reply fishpen0 19 hours agoprevThe main issue is every startup and small business ties themselves to the gsuite apps and ultimately falls down the path of using and then requiring gsuite auth and often chrome as a whole for all work browsing. It is quintessentially the new IE. almost all my browsing during the day has to be chrome whether I like it or not and that has been true at my last 4 organizations. Three of which are multi billion dollar companies and two with tens of thousands of employees reply JohnBooty 18 hours agoparentWe&#x27;re pretty GSuite&#x2F;GAuth-oriented too but the parts we use work just fine on Firefox.What parts of that are forcing you guys to be Chrome exclusive? reply fishpen0 9 hours agorootparentIf your organization uses beyondcorp for zero trust among several other Google products, they only work in Chrome. Many MDM solutions also only do hardware verification through chrome during the auth loop for gsuite and not other browsers. My current organization blocks the installation of other browsers in their jamf configuration as well, so it wouldn’t matter even if beyondcorp did work in another browser reply jeromenerf 15 hours agorootparentprevSome google meet features are chrome only (background blur, picture in picture).I can&#x27;t say I have noticed much else, so I use chrome for gsuite and firefox for everything else ¯\\_(ツ)_&#x2F;¯ reply aembleton 15 hours agorootparent> Some google meet features are chrome only (background blur, picture in picture).They work for me on Firefox 120 on Gnome 45.2. reply crabmusket 13 hours agorootparentI think this is a recent change. Background blur certainly did not work for me before, but the other week I noticed it did. reply timw4mail 17 hours agorootparentprevIT staff browser management reply theboywho 19 hours agoprevA lot of people around me are switching back to firefox as chrome is showing signs of cracks, especially with the Youtube&#x2F;ad-blocking saga on chrome.Also, I don&#x27;t think the US govt guidelines are going to have a dramatic worldwide impact on firefox numbers, the US is no longer the major online player it once was. reply jillesvangurp 17 hours agoparentYes, people that believe that Google will forever own the web are the same people that used to believe that Microsoft would forever own it. MS got arrogant and lost. Google looks like they are repeating that mistake.A few things that could go against this:- If enough people use Firefox, no commercial business in their right mind will tell these people to \"please leave, we don&#x27;t serve your kind\". Seems to be true for obscure versions of internet explorer still in use. Definitely true for Firefox for some time to come.- Legislation might force the market to open up on mobile. Right now Apple is blocking the Chrome and Firefox rendering engines (well they allow similarly named shells around safari). And Google of course \"owns\" the search and browsing experience on Android by default and twists every OEM into signing a restrictive licensing deal. At least you can install firefox on Android. There are some signs this might start changing. A lot of outrage around privacy and ad blockers might speed this up.- People can still vote with their feet. If you watch Youtube on a laptop and you don&#x27;t have an effective ad blocker, Firefox is blocking them very nicely. I watch a lot of youtube and 100% ad free, just saying. reply beej71 13 hours agorootparent> Yes, people that believe that Google will forever own the web are the same people that used to believe that Microsoft would forever own it. MS got arrogant and lost. Google looks like they are repeating that mistake.I don&#x27;t believe Google will forever own the web. But, like Microsoft did, I believe they will cause us a lot of pain before they&#x27;re through. reply worik 15 hours agoparentprev> especially with the Youtube&#x2F;ad-blocking sagaI am afraid to say in my world if Google degrades Firefox for YouTube most people I know will switch to ChromeI have had success persuading people to switch to FF (I am one of those people) but a degraded YouTube- even the outright criminality that involves - would be a deal breakerThe ad blocking blocking will work the other way..... reply taf2 16 hours agoprevI can&#x27;t be alone in my view that open source won. Just not the original Mozilla open source browser. Instead the re-invented one spear headed by Google - Chrome. Chrome is like the 2.0 of everything the IE6 team and Mozilla team learned the hard way. It was built by many of the original founding members of those teams. It&#x27;s not a bad thing when Microsoft now uses the open source Blink rendering engine. It&#x27;s not a bad thing that Apple uses the open source Webkit engine. My feeling is \"we won\". The web is so much better today thanks in part to the amazing teams that came together sponsored by Google to build Chrome. Time marches forward and there are plenty of interesting problems to overcome for the web as a platform. I just think we can move on from M$ bad, Mozilla good... Mozilla showed us we could have a better browser and helped break the web free from the shackles of Microsoft. There&#x27;s new problems to solve new fights to win just this one, is IMO, over. reply coldbrewed 15 hours agoparentOpen source won the browser battle versus proprietary browsers, but it feels like FSF style \"free software\" is losing the war. Chrome is certainly open source but product development is completely dominated by Google. Google drives the web standards; they design the \"reference\" browser; as Google shifts to maintain ad-driven profit margins they&#x27;re positioned to displace ad blockers.It doesn&#x27;t matter if they Manifest V3 implementation is open sourced; If Web Environment Integrity is ultimately implemented then having access to the source doesn&#x27;t really buy you anything. In a future where WEI is mandatory then being able to build Chromium without WEI empowers you to run a browser that&#x27;s summarily locked out of services that demand WEI.Open source mattered much more when simple access to source code gave users meaningful freedom but we&#x27;re transitioning away from that era. Google is on the path to make open source irrelevant by providing an open source browser that must be built with the Google-specified set of features in order to operate correctly.We can&#x27;t claim a victory when open source software implements embrace-extend-extingush semantics. reply Closi 16 hours agoparentprevThat would be great, if Chrome was actually open source, and if the Chromium and Chrome projects weren&#x27;t run by a company with perverse incentives (see: Manifest v3). reply phendrenad2 8 hours agorootparentThat&#x27;s just splitting hairs. Chrome is not holding back features from Chromium, so Chromium has everything that Chrome has. The only things it&#x27;s missing are things that literally don&#x27;t matter to Chromium users (like google sync, what are you going to do, make your own google corporation?) or things that literally can&#x27;t be open-sourced (non-free codecs).The point here is that open-source \"won\" because nobody is trying to push features on the W3 Consortium without providing a reference implementation that is also the live, production code in the flagship browser of the largest browser company. reply dhimes 16 hours agoparentprevWe don&#x27;t move on until Google lets us protect our privacy. Containers would be a good start. reply theteapot 15 hours agoparentprevGoogle stands ready to snatch our victory away. reply zlg_codes 12 hours agoparentprevHonestly I feel like the world would have been better without Google and Microsoft.The Web has been co-opted, and until those entities lose influence over standards, the environment is at risk.They should have moved onto their own protocol already. reply ndriscoll 19 hours agoprevWhy is the US government even supporting specific browsers? It should support a standard like HTML 4 with no CSS or JS required. i.e. make the actual functionality on their sites simple to reduce the chance it doesn&#x27;t work somewhere.They&#x27;re not Spotify. They&#x27;re not trying to growth hack. They don&#x27;t need to look pretty and have fancy animations and match some designer&#x27;s dream down to the pixel. They can add CSS etc. to make things a bit nicer, but government sites should work with as simple of a browser as possible.Highly regulated critical infrastructure like banks should be required to do this too. reply DrBazza 19 hours agoparent> It should support a standard like HTML 4 with no CSS or JS required. i.e. make the actual functionality on their sites simple to reduce the chance it doesn&#x27;t work somewhere.You&#x27;re right of course.It&#x27;s not really a case of &#x27;supporting&#x27; browsers, it&#x27;s a case of testing their sites against other browsers in case developers have accidentally written some non-portable Chrome only code.This was very much the case in the IE6 era. Developers wrote and tested their sites for and with IE6, and were then surprised they rendered (in)correctly on Firefox and looked wrong. At least these days there are shim libraries, rather than having to explicitly rely on things like the box-model hack. reply ndriscoll 18 hours agorootparent> Developers wrote and tested their sites for and with IE6, and were then surprised they rendered (in)correctly on Firefox and looked wrongBut that&#x27;s the point: rendering shouldn&#x27;t really matter. For things that are important like government systems, we should treat web \"apps\" much like TeX encourages: you specify the semantics, and let the rendering engine do what it will. Don&#x27;t try to precisely control it. You can and should assume that users can totally override rendering with a custom agent, that browsers will disagree on default rendering, and that they may ignore your CSS instructions.Like if someone wants to use a browser that always renders h1, h2, p, etc. with specific fonts and colors, totally ignores any CSS, and adds buttons to each table column header to sort on that column, that should all just work. Or if you want to use a braille output or screen reader.For important tools and information, not entertainment&#x2F;shopping, functionality should trump all other concerns.My bank and now my power company have issues where I need to use chromium to fill out a form, and I don&#x27;t understand it. I know Firefox supports forms. For whatever reason, javascript is loading the thing and screwing up somehow. I don&#x27;t see why js is even involved, but frankly it screams incompetence to me. The easiest thing in the world to build, and they&#x27;ve broken it trying to make it look nice.I don&#x27;t go to my power company website for fun. I&#x27;m there to pay a bill. I need a form with 5 inputs and a submit button, and that&#x27;s it. The rest of the screen can be plain white for all I care. Literally something I could put together in 2 minutes when I was 11, and it does not work. Paper should not have a better UI than a website.Incidentally, this is why I&#x27;m not too worried about AI. If companies wanted cheap&#x2F;easy&#x2F;reliable systems, that&#x27;s been doable on the web the whole time. People can&#x27;t resist making things difficult for themselves, and they&#x27;ll pay very good money to do it. reply crazygringo 17 hours agorootparentYou&#x27;re asking government websites to run differently from 99.9% of commercial websites out there.First of all, that&#x27;s just not going to happen for all sorts of practical reasons.But secondly, you&#x27;re totally ignoring UX and design. \"Specifying semantics, and let the rendering engine do what it will\" might work for developers who are used to interacting with API&#x27;s. It will not work for regular users.Regular users need to understand which button is the primary action. They need to understand which part of the content is the main body, versus a sidebar versus a header. They want columns that are correctly sized for their content. They don&#x27;t want to have to scroll horizontally. They want responsive design that works on mobile too. They want something that looks trustworthy and familiar.Websites are apps now. Asking to go back from presentation to semantics is like asking people to use the command line instead of GUI&#x27;s. It&#x27;s not going to happen, nor should it, because it&#x27;s not user-friendly.The only people it&#x27;s friendly to are a niche set of developers with certain ideological beliefs that most web technologies shouldn&#x27;t be used. reply ndriscoll 16 hours agorootparentThe things you describe aren&#x27;t prevented by focusing on semantics, and are in fact enabled by it. Every modern app looks different for branding purposes, so users don&#x27;t know what the buttons do. Things are complicated because we abandoned standard UIs that used to use the same widgets across every application.And government stuff should work differently from 99.9% of commercial websites. Again, the goal should be for it to work. The government does not need to do marketing and make you feel like they are trustworthy. If you want to interact with social security, you go to ssa.gov. If you want to interact with the IRS, you go to irs.gov. End of story. They don&#x27;t need to act like commercial entities because they do not have to worry about market share. Their share is always 100%. They need to just make their stuff reliably work, easy to figure out, and should make it cheap and easy to build. Basic HTML with minimal optional styles checks all of those boxes.If you view the computer as a tool instead of a toy, you see that you really just need most websites to be a more convenient version of paper forms. It doesn&#x27;t need to look fancy. It needs some boxes to type information, it needs to always work, and ideally every form on every website would stick to the same 5 or so types of input (rendered consistently by your OS) with no surprises. Government sites should take the tool approach. Commercial sites can sell toys. reply crazygringo 15 hours agorootparent> so users don&#x27;t know what the buttons do.They do, though. People are able to figure out commercial websites orders of magnitude more easily than figuring out how to fill out their 1040.> They need to just make their stuff reliably workWhich is what UX and design help with.> Basic HTML with minimal optional styles checks all of those boxes.It doesn&#x27;t. Layout and design are tools that help with clarify and ease-of-use.> you see that you really just need most websites to be a more convenient version of paper forms.Nothing could be farther from the truth.Do you similarly think that your iPhone or desktop interface would be improved if the UX was \"a more convenient version of paper forms\"?Paper forms are an extremely limiting form of UX. Why would you ever want to throw out all of the progress we&#x27;ve made with usability? reply ndriscoll 15 hours agorootparentFilling out your 1040 is \"hard\" because people don&#x27;t understand what the words mean, there&#x27;s a 100 page instruction manual that defines the terms, and it might require filling out other forms too (which you might just need to know somehow that you need to fill them out too). Other than that, the actual UI design is straightforward. You write&#x2F;type numbers into numbered boxes, top to bottom, occasionally referring back to numbers you&#x27;ve already completed. You could progressively enhance with automatic calculations for relevant fields, but hand calculations work as a fallback.Reliability is unrelated from UI, except insofar as simple UIs are easy to build, and therefore less likely to break. A paper form 1040 is perfectly reliable; it&#x27;s not going to burst into flames when you&#x27;re filling it out. As I said above, I couldn&#x27;t even fill out my payment form on a modern site. It did not work at all. The form did not appear. That is not reliable. It also makes no sense if you know the page is ultimately using HTML, and that HTML has forms built directly in, and they always work fine.And yeah, when I&#x27;m doing something like making a payment, setting up a transfer, doing my taxes, or even ordering a pizza, something like a slightly advanced paper form (e.g. with drop downs for options) would work great on my phone or desktop. Have a special request for your pizza that&#x27;s not on the form? Put it in the free-form instructions box.The \"progress\" we&#x27;ve made in the last few years is that I can&#x27;t do bank transfers without switching browsers, which requires selecting a \"to\" account from a drop-down, a \"from\" account from a drop-down, and typing an amount. I don&#x27;t see how something so basic can be so hard to do correctly. There&#x27;s literally no need for any javascript at all. I don&#x27;t see the usability gain from whatever they&#x27;re doing. reply aembleton 15 hours agorootparentprev> It needs some boxes to type informationWould an address lookup service be acceptable? One of those where you start typing your address into a box and it fills in all of the address fields based on which address you select.If a new version of this is created, shouldn&#x27;t it be tested on browsers? Which browsers should it be tested on? reply vorticalbox 14 hours agorootparentWe could simple put an input for each part of the address and let the user fill it out.Requires exactly zero lines of javascript, no third party api that may or may not work. reply pavon 17 hours agorootparentprevNone of that negates the need to test on various browsers to ensure compatibility. reply ndriscoll 17 hours agorootparentWhat compatibility? If Firefox breaks forms, then Firefox broke forms and needs to fix it. Not your bug. If Chrome renders differently from edge because they decided the default color on .gov sites will be pink on white and all padding will be multiplied by 1.5, then that&#x27;s fine. Not a bug. Chrome just decided to present a different look.If it&#x27;s even possible for basic functionality to break in a way where you wouldn&#x27;t obviously say the browser is broken, then you&#x27;ve built it wrong. That means you need to test that TLS&#x2F;HTTP protocols are implemented correctly and that your documents conform to a schema. reply pavon 16 hours agorootparentYou are assuming that your developers were perfect and write sites exactly to the standard every time. In the real world they don&#x27;t and XHTML lost, so all browsers tolerate and mask non-compliant pages to various degrees and in various ways, and will surface different bugs in your work. So it behooves you to test with the browsers your users are using to find those bugs before they do. reply ndriscoll 16 hours agorootparentI am assuming that a professional can do their job, yes.The whole XHTML thing where allegedly it never caught on because people can&#x27;t write valid markup has never made sense to me. They&#x27;re able to get typescript to compile now, right? If a dev couldn&#x27;t write react code that compiles, we would fire them, right?We have tools to check that your document parses and conforms a schema. We&#x27;ve had them for 20 years. It&#x27;s easy enough to have that be part of your CI pipeline. The tooling is 1000x simpler than modern frameworks, and the thing that was allegedly difficult was that if you enabled conformance mode (which was opt-in based on DTD and&#x2F;or MIME type), you had to open and close your tags instead of just opening them. Surely any middle schooler understands when you open a parenthesis, you need to close it? reply NegativeK 13 hours agorootparentprevUS government websites, as they exist, are often ancient, decrepit, and poorly funded. This will make them all worse and it will cost more. It will get in the way of people actually trying to interact with the government, and the leaders in the government will crap all over the project due to the angry calls they&#x27;ll get from their constituents.If we try to stick to pure ideals without any consideration for reality, reality will ignore us and move on. Or, to borrow an example from another field: in infosec, the most secure computer is the one that&#x27;s never turned on. reply ndriscoll 13 hours agorootparentThey&#x27;re not decrepit; they&#x27;re unfashionable. Programs and websites that were somewhat decently written 20 years ago should and pretty much do run exactly the same today as they did then. It&#x27;s not until \"web 2.0\" and SaaS that you find things that stop working after a few years&#x2F;months.That&#x27;s exactly what you need for \"poorly funded\" sites, and I don&#x27;t see why a site that&#x27;s meant to be functional needs a Hollywood budget. reply crazygringo 17 hours agorootparentprev> then Firefox broke forms and needs to fix it. Not your bug.Not in the real world. In the real world, you&#x27;ve delivered a site that doesn&#x27;t work and contractually, you can be sued or not paid for not fulfilling your contract. reply ndriscoll 17 hours agorootparentThat&#x27;s the whole point of a standard (note that I said for .gov): the government says what standard they interop with. They either conform or don&#x27;t. If other implementations don&#x27;t conform, they are wrong. If the site doesn&#x27;t conform, it is wrong. If it&#x27;s not in scope for the standard (e.g. layout&#x2F;font), it&#x27;s out of scope. If the standard is underspecified or wrong somehow, you fix that and .gov now targets the new revision.The government doesn&#x27;t need to worry about market share. They can just dictate that this is what your browser needs to do to work with government systems. This is both more fair and easier for everyone; you don&#x27;t have a moving target to aim for, and can just refer to the standard for what to do. reply crazygringo 17 hours agorootparentBut government don&#x27;t exist to serve standards.Governments exist to serve their citizens -- their users.It&#x27;s extremely user&#x2F;citizen-hostile to say, \"well our site works but no commercial browsers do, so I guess you can&#x27;t register for a health plan this year over the web.\"And I don&#x27;t know about you, but I sure don&#x27;t want the government building its own standards-based browser required for accessing government websites... reply ndriscoll 16 hours agorootparentThe government should set or adopt standards, not serve them. And they can and should provide a reference implementation.We could easily and reliably do forms on mainframes. This is not complicated. And de facto, every browser supports HTML 4 forms anyway, so that&#x27;s a non-concern.They already set standards for things like needing to support TLS 1.3 with specific cipher suites. There&#x27;s no reason they can&#x27;t say HTML 4 forms and links are required for browsers to work on their sites. reply crazygringo 16 hours",
    "originSummary": [
      "Mozilla Firefox browser may face a decline in support from U.S. government websites due to its low market share.",
      "The U.S. Web Design System only supports browsers with over 2% usage, and currently, Firefox has a 2.2% share.",
      "The rise of Chrome and Safari on mobile devices is contributing to the decline of Firefox's usage.",
      "If Firefox falls below the 2% threshold, it may no longer be supported by government websites, potentially leading to a domino effect with corporations discontinuing support as well.",
      "The author expresses concern about the future relevance of Firefox if its decline continues."
    ],
    "commentSummary": [
      "There are concerns about the potential decline of Firefox and its impact on Mozilla.",
      "Usage figures and the necessity of supporting Firefox in government tech are topics of debate.",
      "The discussion includes considerations of user privacy, government funding, web standards, user agent strings, and Firefox's performance and compatibility."
    ],
    "points": 612,
    "commentCount": 730,
    "retryCount": 0,
    "time": 1701786292
  },
  {
    "id": 38532167,
    "title": "Understanding iMessage: Inside Apple's Messaging Protocol",
    "originLink": "https://jjtech.dev/reverse-engineering/imessage-explained/",
    "originBody": "iMessage, explained 6 minute read This blog post is going to be a cursory overview of the internals iMessage, as I’ve discovered during my work on pypush, an open source project that reimplements iMessage. I gloss over specific technical details in the pursuit of brevity and clarity. If you would like to see how things are specifically implemented, check out the pypush repository as I mentioned above. It’s a pretty cool project, if I do say so myself. Make sure to check it out! If you still end up with any questions, feel free to ask me in the pypush Discord the foundational layerPermalink One of the most foundational components of iMessage is Apple Push Notification Service (APNs). You might have encountered this before, as it is the same service that is used by applications on the App Store to receive realtime notifications and updates, even while the app is closed. However, what you probably didn’t know about APNs is that it is bidirectional. That’s right, APNs can be used to send push notifications as well as receive them. You can probably already tell where this is going, right? Internally, after a device connects to APNs it will receive a “push token”. This token can be used to route notifications to that specific device. Note: This token is technically different then the token you receive when using the application:didRegisterForRemoteNotificationsWithDeviceToken: API. That token is scoped for per-app use, and is requested using the bundle ID of the application. However, it is basically used for the same purpose. When sending push notifications to a device, you also need to specify the topic a message is for. This usually looks like a Bundle ID, and for iMessage it’s com.apple.madrid. When a device connects to APNs, it sends a filter message instructing the server on which messages it wants delivered to it. Note: The APNs server is also known as the APNs Courier. The filter message includes several lists of topics, for each of the different possible states. It may want a topic to be enabled, opertunistic, paused, or disabled APNs is not only used for the actual message delivery part of iMessage. Using a pseudo-HTTP layer on top of APNs, IDS (which will be explained in a moment) can send queries and receive responses over APNs as well. One tricky note that I will mention is that in order to connect to APNs, you need a client certificate issued by the Albert activation server. the keyserverPermalink The next piece of this puzzle is IDS. As far as I can figure out, this stands for IDentity Services, though I don’t think there is any official confirmation on that. Note: You may also see it referred to as ESS. This is confusing because the APNs topic FaceTime uses is specifically called com.apple.ess. Moving on… IDS is used as a keyserver for iMessage, as well as a few other services like FaceTime. Remember, iMessage is E2E encrypted, so the public keys of each participant must be exchanged securely. The first step in registering for IDS is getting an authentication token. This requires giving the API your Apple ID Username and Password. Note: As 2FA is now standard, it had to be retrofitted into the IDS API. There are 2 options for this: the legacy option, in which a 2FA code is directly appended to the password, and the “GrandSlam” option. In the GrandSlam option, “Anisette data” is used to prove you are the same device and thus do not need to enter the 2FA code again. You then receive a Password Equivalent Token (PET) which can be used as if it was the password + 2FA code. After one has gotten an authentication token, it must be immediately exchanged for a longer lived authentication certificate. This certificate allows registering with IDS, but it is not yet enough to perform key lookups. Perhaps the most important step of the IDS setup process is registration. This is where public encryption and signing keys are uploaded to the keyserver, as well as various other “client data” about what features the device supports. When making an IDS registration request, a binary blob called “validation data” is required. This is essentially Apple’s verification mechanism to make sure that non-Apple devices cannot use iMessage. Warning: In order to generate the “validation data”, pieces of information about the device such as its serial number, model, and disk UUID are used. This means that not all validation data can be treated equivalently: just like with Hackintoshes, the account age and “score” determine if an invalid serial can be used, or if you get the “customer code” error. Note: The binary that generates this “validation data” is highly obfuscated. pypush sidesteps this issue by using a custom mach-o loader and the Unicorn Engine to emulate an obfuscated binary. pypush also bundles device properties such as the serial number in a file called data.plist, which it feeds to the emulated binary. After registering with IDS, you will receive an “identity keypair”. This keypair can then be used to perform public key lookups. When performing a lookup, you provide the account(s) that you would like to look up, and receive a list of “identities”. Each of these identities corresponds to a device registered on the account, and includes important details such as its public key, push token, and session token. Warning: Session tokens are necessary to send messages to a device. They essentially prove that you made a recent lookup, because the session token expires. Session tokens cannot be shared, as they can only be used by the account that performed the lookup request. message encryptionPermalink Now, we’ve setup the basics of iMessage. We have enough that we can look up the public keys of another user, as well as publish our own. Now we just need to put it together with APNs to send and receive messages! In order to receive messages, one simply filters the APNs connection to com.apple.madrid and sends the active state packet. Depending on which capabilities you advertised in your IDS registration, as well as the iOS version of the sending device, you may receive messages in the legacy (pre-iOS 13) pair encryption format, or in the new pair-ec format. While the pair format is much more documented and easier to implement, it does not provide forward secrecy using “pre-keys” (similar to Signal) as the new pair-ec format does. You can then decrypt the message as described in several papers, and as implemented in pypush. Verifying the message signature is optional, but is obviously important if you intend your client to be secure. Sending messages is pretty much the inverse to receiving them. Keep in mind that you can chose to individually send out messages to each recipient, or you can bundle all the different recipients and their respective encrypted payloads into a giant bundle, which APNs will split up for you. Note: Another thing to keep in mind is that message are delivered to all participants in a conversation, including the other devices on your own account. One more thing to keep in mind that is often overlooked when sending messages is that the AES key is not entirely random: it is tagged with an HMAC. Your message will fail to decrypt on newer devices if you use an entirely random AES key. And that’s pretty much it! As I mentioned, this blog post is designed to give you a good idea of how the iMessage protocol fits together, so that you can explore the pypush code, not directly explain every technical detail. resources and attributionPermalink Many people and prior works have helped me understand iMessage. Here is a brief list, in no way exhaustive: IMFreedom Knowledge Base: iMessage M. Frister: pushproxy Nicolás: apns-dissector QuarkSlab: iMessage Privacy Garman et al. Chosen Ciphertext Attacks on Apple iMessage NowSecure: Reverse Engineering iMessage Elcomsoft: iMessage Security and Attachments Eric Rabil’s open-imcore The Apple Wiki: Apple Push Notification Service Mihir Bellare and Igors Stepanovs: Security under Message-Derived Keys: Signcryption in iMessage Apple Platform Security: How iMessage sends and receives messages securely Nicolás: Apple IDS payload keys Various people on the Hack Different Discord This blog post has been reworked from its original version. Tags: Apple, iMessage Categories: reverse-engineering Updated: August 3, 2023 Previous Next Comments",
    "commentLink": "https://news.ycombinator.com/item?id=38532167",
    "commentBody": "iMessage, explainedHacker NewspastloginiMessage, explained (jjtech.dev) 548 points by spoon16 18 hours ago| hidepastfavorite129 comments CTmystery 8 hours agoLearning the contract is great, thank you for the work! How about the infra stack used by imessages? Does anyone have intel on that? The scale is incredible, which always makes me wonder how it can be so good while other apple web services (forums, dev portals, etc) can be so buggy and half baked reply nicolas_17 4 hours agoparentThe actual mind-blowing scale is that Apple&#x27;s push notification service isn&#x27;t just carrying iMessages. It&#x27;s also carrying push notifications for every third-party messaging app.And the non-messaging apps with notifications too.And the silent internal notifications. You added a meeting to your calendar on your Mac? Push notification to your iPhone to tell it that the iCloud data changed and it needs to update. Changed a file on iCloud Drive? Push notification to sync your other devices. Got a phone call, and it starts ringing on your Mac too via Continuity? Push notification (encrypted like an iMessage).Just how many messages are going through that service every second?! reply stouset 2 hours agorootparent> Just how many messages are going through that service every second?!I’m confident in saying at least six. reply xwolfi 2 hours agorootparentIt&#x27;s more than bitcoin ! reply paulmd 2 hours agorootparentmuh off-chain scaling replybgorman 16 hours agoprevMy prediction is that Apple will start to use attestation (device check) to lock down iMessage. The problem is that this would require a software update for older devices. reply kotaKat 15 hours agoparentThey already partially do.> Warning: In order to generate the “validation data”, pieces of information about the device such as its serial number, model, and disk UUID are used. This means that not all validation data can be treated equivalently: just like with Hackintoshes, the account age and “score” determine if an invalid serial can be used, or if you get the “customer code” error.The \"customer code\" error is a prompt from Apple, basically an attestation failure -- you have to contact Apple Support to get your Apple ID unlocked once you&#x27;ve tripped the failure. Legitimate customers will breeze right through (eg, just approving your login from your legit device), but Hackintosh users use crafty means to fake their way through the process.[1][1]https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;hackintosh&#x2F;comments&#x2F;gij9rt&#x2F;getting_... reply blibble 11 hours agorootparentremote attestation would mean it&#x27;s not possible to pull out the binary and run it externallyyou&#x27;d need the key from the TPM&#x2F;secure enclave too, which is much much harder to extract reply mmis1000 3 hours agorootparentTPM did not get key from nowhere. The key need to come from network or locally generated as long as it is not preloaded when manufacturing. And in either way, it should be possible to intercept&#x2F;fake it. reply mjg59 3 hours agorootparentApple devices with a secure enclave have the ability to attest to their identity, and also attest that keys were generated on a secure enclave (this functionality is very locked down for privacy preservation purposes, but is certainly available to Apple). If Apple is willing to lock out any device shipped without a secure enclave (which would probably be an excessive number of Macs at the moment - the iMac only started shipping with a T2 in the 2020 model, although the iMac Pro did have a T1 earlier than that) then it&#x27;s absolutely possible to restrict access to actual Apple hardware with no risk of key interception. reply SpaghettiCthulu 8 hours agorootparentprevIt&#x27;s only a matter of time until a company starts selling TPM dumps, right? reply blibble 8 hours agorootparentmaybe, but for a task like this it doesn&#x27;t really scaleApple aren&#x27;t going to allow one phone to attest 5000 new iMessage clients reply thomasahle 2 hours agoparentprevMaybe, but they also just announced RCS support: https:&#x2F;&#x2F;9to5mac.com&#x2F;2023&#x2F;11&#x2F;16&#x2F;apple-rcs-coming-to-iphone&#x2F; so maybe they&#x27;ve just decided that this is a good opportunity to take the charge opening things up. reply cavisne 6 hours agoparentprevIt would require a hardware update for older devices I believe, ie any that don’t have TPMs reply uf00lme 15 hours agoparentprevI think that is how BBM worked, but I could be wrong. I&#x27;d be surprised if it is part of the over arching OS security. Sounds like something that should be in their lockdown mode at the very least. reply ocdtrekkie 16 hours agoparentprevApple already provides security updates to all iOS devices made in the last 5ish years at least, so it would probably take a pretty trivial number of years for them to have an update deployed to nearly all iOS devices that see active use. reply gafage 13 hours agorootparentThe iPhone 5s (released ten years ago) received an update earlier this year. reply bentt 13 hours agoprevOMG I love this. Go get em! Also, this is perfect material for Hack Club. You should join! https:&#x2F;&#x2F;hackclub.com&#x2F; reply lxe 14 hours agoprevThis is phenomenal work. You should write a little on how you got into this whole field. There are high school and college kids all over reddit struggling how to excel at technical stuff, learn programming, get a job in tech, and I feel like they can really benefit from your perspective. reply tomashubelbauer 14 hours agoparentI don&#x27;t disagree with what you say, but I would be surprised if it was any sort of secret sauce and not \"just\" an incredible amount of grinding, the seemingly zero-cost energy reservoir you can tap into as a young adult if you really like what you&#x27;re doing and possibly an enlightened parent or a role model. reply terminous 14 hours agorootparent> possibly an enlightened parent or a role modelThis is typically the &#x27;secret sauce&#x27;. reply bexsella 13 hours agorootparentI was once asked how I got to where I am, where others in my situation might not have, my response was: “Parents that gave a damn”. It wasn’t about pressuring me, it was about recognising my interest in computers, and fostering that interest as much as was financially possible given our circumstances (which were often dire). My parents aren’t technical, but they did what they could, and I wouldn’t be the engineer I am without that. reply drekipus 13 hours agorootparentI grew up with a foster mother that actively \"suppressed\" what I did on the computer, banning me for a month if I didn&#x27;t get changed immediately after school.Now I&#x27;ve become a senior engineer, but I&#x27;m kinda shotty at it, chaotic good in solving problems, but issues with authority and process.Who knows, maybe I would&#x27;ve became a \"run of the mill\" engineer if she helped. reply jordanbeiber 3 hours agorootparentAs an engineering manager I see problems with authority and process as something usually positive.This usually leads to more things getting done “right” than “wrong”. IME.Having the same issues&#x2F;traits I’m not sure how that gets formed - my upbringing was limitless in many ways. reply lxe 13 hours agorootparentprevIt&#x27;s not grinding though. My highschool years were also super productive when it came to programming-related things, while I have seen most of my peers, aside from select few, really struggle despite their willingness. So maybe there is some secret sauce that can help others to get good a this. Maybe it&#x27;s a mindset or attitude, etc... reply brailsafe 9 hours agorootparentWillingness is almost antithetical to having the motivation to grind in my mind. In order to do something persistently, you need to trade something for it, and often times you need to ignore the fact that the trade isn&#x27;t worth it, or not have anything else competing for that attention in the first place; in otherwords, some level of compulsion as well as willingness.It&#x27;s the same with skateboarding, or any other interest that is difficult, time consuming, character building, and that requires obsession.The defining characteristic of programming, as opposed to some others, is that it&#x27;s complex and only intellectually demanding, whereas the others are some combination of physical and mental stress. People don&#x27;t know how to navigate that from the beginning, but the ones who have the disposition to simply throw themselves at it regardless of failure, repeatedly, figure it out eventually.The ones who actually succeed in a career of it are probably the ones who figured out how to dial it back as an obsession, and stop when they&#x27;re 10hrs in to take a different approach. reply tomashubelbauer 12 hours agorootparentprevI don&#x27;t know. I definitely did grind programming a lot as a teenager and for a few years as a young adult. But the grinding was effortless to me. It was as if this type of activity was replenishing my energy reserves instead of making me tired. I rarely needed to take breaks and indeed frequently forgot to eat or sleep when deep in my sessions. So it wasn&#x27;t a struggle at all, but it was still a grind I would say. Or maybe I am misunderstanding the word and it would be better to say it was a lot of time spent, at the very least.I don&#x27;t think anyone can do this, I think you need to have that connection with programming where it is harder resist it than it is to do the work. But it doesn&#x27;t mean people like the author of the article have a secret sauce and them recounting their experience to their peers to inspire them isn&#x27;t worth much to them as a result I would expect. It&#x27;s the \"draw the rest of the fucking owl\" type a thing I think.BTW I don&#x27;t mean to say I was a super duper genius as a teenager for whom programming was like breathing. I refused to study anything, I only enjoyed discovering things myself and I had no direction in my programming knowledge collection at all. A more disciplined person would have beaten me easily, and many have. Despite the ease with which programming came to me I didn&#x27;t do that much productive stuff. I was mostly just having immense amounts of fun and joy. I do feel a bit sad sometimes about not getting a bigger edge now, but realistically, when push comes to shove, I wouldn&#x27;t change it anyway. reply moxious 13 hours agorootparentprev\"just\" is doing a lot of work in this construction. Regardless what a person&#x27;s constellation of privileges is, it always takes an incredible amount of grinding and that&#x27;s pretty damn cool &#x2F; laudable &#x2F; praiseworthy all by itself.The secret sauce has never been secret reply tomashubelbauer 13 hours agorootparentThat&#x27;s my point. reply petabyt 3 hours agoparentprevIn highschool I had basically all day to work on my own stuff. Finishing stuff early, free periods, and doing my own thing when I wasn&#x27;t supposed to gave me all the time I needed to create and release an app in about 6 months. I was very productive. reply benoror 15 hours agoprevMore on this: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38531759 reply dbuxton 15 hours agoprevGenuine question - can a topic really be `opertunistic` or is that author typo? I love these `referer`-type misspellings that become fossilized over generations reply jjtech 10 hours agoparentUnfortunately, there are many typos in my code :POn the other hand, I&#x27;m not sure if this is a typo on Apple&#x27;s part, but it certainly is weird: you must use \"WindowSerial\" here[1], not \"WindowsSerial\" with the extra s[1] https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;pypush&#x2F;blob&#x2F;8b33c0ee5d540d8ac7... reply girvo 9 hours agorootparentThat \"missing plural S in PascalCased (or camelCased) names\" is something I see semi-often!Congratulations on this amazing work :) reply projektfu 14 hours agoparentprevThe code doesn&#x27;t seem to use it, but I think it would be a misspelling by the author, as it&#x27;s probably an integer code. reply nicolas_17 4 hours agorootparentAt the protocol level, they are indeed just integers: https:&#x2F;&#x2F;theapplewiki.com&#x2F;wiki&#x2F;Apple_Push_Notification_Servic... reply xg15 13 hours agoprev> When making an IDS registration request, a binary blob called “validation data” is required. This is essentially Apple’s verification mechanism to make sure that non-Apple devices cannot use iMessage.I wonder, will this be in violation of the EU&#x27;s DSA and&#x2F;or DMA once they are in force? reply Longhanks 12 hours agoparentDSA and DMA do not magically grant you the permission to do whatever you want with Apple&#x27;s servers, nor force they Apple into having to serve any particular valid response to the requests you make.In whatever way Apple is going to comply with DSA and DMA, this ain&#x27;t it. reply xg15 12 hours agorootparentI don&#x27;t know the legal text, but improving interop specifically between messaging services seems to be a goal of the DMA, according to the EU parliament [1]:> Interoperability between messaging platforms will improve - users of small or big platforms will be able to exchange messages, send files or make video calls across messaging apps.Lock-in mechanisms like the above would at least run counter to that goal.I also think that enforcing device restrictions on a messaging service is more problematic than on some random API: Messengers are subject to the network effect and usually you can&#x27;t freely choose which messenger you want to use - it depends on which one the people you want to talk with are on.In an extreme case, some person or business could choose to exclusively communicate using iMessage. Then you&#x27;d have to buy an iPhone just to be able to reach them. This seems like exactly the kind of interop problem the EU is concerned about.[1] https:&#x2F;&#x2F;www.europarl.europa.eu&#x2F;news&#x2F;en&#x2F;headlines&#x2F;society&#x2F;202... reply turquoisevar 12 hours agorootparentEuropean regulations work on a policy level not on a technical level.In other words, Apple having technical limitations isn’t illegal per se, Apple refusing to facilitate interoperability might be illegal (although future RCS adoption will meet the requirements).The above assumes that iMessage meets the regulations threshold, which it currently doesn’t according to Apple based on user numbers, but that’s a different debate. reply cqqxo4zV46cp 11 hours agoparentprevEspecially now that iOS is getting RCS. First-party cross-platform iMessage is nothing more than a nerd’s pipe-dream.And I’m completely fine with that. reply jamesdepp 13 hours agoprevpypush, the open source project behind today’s developments in the iMessage reversing news, is licensed under MongoDB’s Server Side Public License and owned by Beeper (JJTech sold the rights to Beeper, per discord). Although this library is fantastic, I do think that the extremely copyleft license could have implications on where we see this used. reply wmf 12 hours agoparentTime for some reverse reverse engineering. reply Thoreandan 12 hours agoprevSo… anyone gonna make a libpurple plug-in? reply DANmode 2 hours agoparentIf you receive no replies, will you? =] reply nyreed 9 hours agoprevHuh. So Android&#x27;s push notification service is built on their instant messenger (GTalk), and Apple&#x27;s instant messenger is built on their push notification service.How cute. reply tech234a 7 hours agoparentNote: Android doesn’t use GTalk for notifications anymore, and the GTalk servers don’t exist any more [1].[1]: https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2021&#x2F;08&#x2F;a-decade-and-a-half-... reply geospatialover 14 hours agoprevthe fact that you&#x27;re in high school is incredible. keep it up! reply phero_cnstrcts 12 hours agoparentNot many make it that far! reply catlover76 15 hours agoprevI just got an iPhone for the first time, and it is a noticeably better device than my previous Android phones.One downside is that I can&#x27;t use iMessage on my Windows and Linux computers. Will look into pypushHonestly, the iPhone is nudging me further to giving a Macbook&#x2F;OSX a try one day, but the major blocker to me is the poor state of gaming on Macs. reply samtheprogram 15 hours agoparentAfter my gaming computer started rebooting (probably needs a new power supply in order to hit peak power draw), I tried out my new M2 Pro for gaming again.I&#x27;ve been using Codeweavers Crossover to play games that are Windows only, and it&#x27;s been surprisingly fine. I never fixed my gaming PC (for gaming, at least) and converted it to an at home server. It&#x27;s been a couple months now. I just lent a friend my GPU.Epic Games doesn&#x27;t seem to work, but you could always use Legendary for those titles -- I just don&#x27;t have any titles on Epic that I want to play.I&#x27;m hoping in one of the future updates that Crossover can activate macOS Sonoma&#x27;s Game Mode for the games running within Wine, because I assume it&#x27;ll improve performance even more. I&#x27;m also having a bit of buyers remorse -- I didn&#x27;t plan to use this for gaming, and now I&#x27;m wondering how much better an M2 or M3 Max would be for more demanding titles. reply catlover76 14 hours agorootparentEhh yeah the prospect of using such patching software doesn&#x27;t appeal, and I don&#x27;t want to run the risk that games work poorly or not at all even with that kind of fiddling (which is something I abhor about Linux, so why would I want it on my expensive and supposedly superior Macbook). reply philsnow 11 hours agorootparentJust want to throw out there that ~20 years ago I sometimes got better framerates in linux than windows on the same hardware for certain FPS games reply crossroadsguy 15 hours agoparentprevPersonally, for communication I never use a device platform specific&#x2F;locked app&#x2F;service. Maybe you could keep using the app(s) whatever you were. reply frizlab 15 hours agorootparentI’m curious, what do you use then? reply gumby 14 hours agorootparentThere are lots of choices depending on your community and desired feature set: whatsapp, fb messenger, instagram messenger, telegram, signal, discord, or the direct messaging features of other programs like Slack.imessage is an outlier in that it also has a bidirectional link with SMS. I just read today that FB messenger used to have this (who knew?) but no longer does. My reading of the EU&#x27;s complaint is that if imessage didn&#x27;t have this feature they would not be in trouble since they&#x27;d be no different from the other services in being a silo. Weird! reply frizlab 12 hours agorootparentUnless I’m mistaken literally all of these services are locked down too, and few have E2E encryption… iMessage is indeed “Apple-only” but the rest is on “all” platforms only for purely economical reasons, as much as iMessage is on Apple platforms only for the same reason.At least iMessage falls back to SMS (soon RCS) when available, which is much more ubiquitous than the rest tbh…If you truly want to avoid a lock down you should host your own messaging solution. reply philsnow 11 hours agorootparentI don&#x27;t know why you&#x27;re getting downvoted, but I&#x27;ll throw my hat in this ring as well:Some of those services require individual opt-in to turn on e2ee. Some of them don&#x27;t support e2ee for group messaging. Of the services listed that do support e2ee, I have the most trust in Apple&#x27;s (well, Signal&#x27;s, but..) being \"actually\" [0] and \"only\" [1] end-to-end encrypted. The entire basis of that trust is the money they&#x27;ve spent positioning themselves in the market as a privacy-focused brand.Meta runs three of the listed services (whatsapp, facebook messenger, instagram), and their positioning is not exactly \"privacy-focused\". I haven&#x27;t looked into Telegram much, but I would want to at least understand how they generate revenue before trusting them. Neither Discord nor Slack are what I would call privacy-focused. Signal is probably better than iMessage in terms of how much I trust their company, their clients, and their protocol, but its adoption is so vanishingly small among my friends that I stopped asking people if they used it.[0] I&#x27;ve seen services in the past [0a] that have tried to argue that as long as every link is encrypted from originating client through servers to destination client, or from originating client to destination server, then it&#x27;s \"end to end encrypted\"[0a] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21528437[1] that is, not only are message contents (and as much metadata as is feasible) encrypted such that the same ciphertext passes all the way through the system and the recipient&#x27;s client can decrypt the ciphertext, but also 1. the intermediary service doesn&#x27;t have a copy of the recipient&#x27;s secret key and 2. the plaintext wasn&#x27;t encrypted also to a public key belonging to the intermediary service or some other party.edit This other comment https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38537444 talked sense into me -- Apple doesn&#x27;t seem to have designed iMessage to keep up with the times, crypto-wise. There&#x27;s a huge, aging installed base that admittedly gets updates more often than any other competitor in their space, but that still means that iMessage has to be able to talk to them. I guess this is similar to the deprecation of SSL 0.9 and TLS 1.0; browser vendors collectively decided to kill them when a low enough proportion of servers were using them, but I don&#x27;t know if Apple would be willing to cut off the older devices to make things better for owners of newer ones. reply crossroadsguy 8 hours agorootparentprevWhatsApp (this mostly - I live in India). SMS.Telegram (2 groups). Discord (3 groups).Signal (with 1 friend) and iMessage (with 2 friends) — these 2 apps are more of a hobby (a thing?), we four usually use WhatsApp.I mean I could fight it or just fall line. I fought and lost :DNotifications off for all. reply catlover76 14 hours agorootparentprev> Maybe you could keep using the app(s) whatever you were.I was using Android Messages, which has a web app. The experience was mediocre because the web app had trouble connecting to my phone all the damn time.I text some people almost exclusively through Facebook Messenger, and I think the rest I will try to move from text to WhatsApp. Both Meta-owned, unfortunately, but those seem to be easy to use cross-device and almost everybody has them. reply outlawery 13 hours agorootparentIf you&#x27;re already using Thunderbird as mail client, you can integrate Google Messages add-on [1] into Thunderbird app which I have been using happily for over a year without much trouble (sans the incoming texts notification feature). Seemingly this add-on has all features akin to the Google Messages Android app.[1] https:&#x2F;&#x2F;addons.thunderbird.net&#x2F;en-us&#x2F;thunderbird&#x2F;addon&#x2F;googl... reply catlover76 10 hours agorootparentI&#x27;ve never comprehended the use of separate email clients, personally lol reply crossroadsguy 8 hours agorootparentprevI am from India - WhatsApp is messaging here. Scratch that - WhatsApp is communication here. So that’s not really a choice. Maybe you’d have the app for your region unless you’re from USA where I’ve heard it’s iMessage. reply girvo 8 hours agorootparentIn Australia it&#x27;s iMessage and FB Messenger, mostly. But that&#x27;s also dependent on where you or your family is from: I came here from NZ, so iMessage&#x2F;Messenger is normal, but my Indian friends use WhatsApp as a matter of course! reply shiroiuma 7 hours agorootparentHere in Japan, if you don&#x27;t use LINE, you won&#x27;t have any friends. Absolutely no one uses SMS messaging for anything personal. reply catlover76 8 hours agorootparentprevI am not sure there is anything comparable in the US to the way WhatsApp is used in other parts of the world. People just default use SMS texting in the US and Canada. A lot of people do have WhatsApp here though reply selykg 15 hours agoparentprevPersonally, the approach I took to this was just to game on consoles. In my personal experience, the upgrade cycle is far far better for me. I don&#x27;t feel like I&#x27;ve missed anything as a result either. reply matwood 15 hours agoparentprevBut the internet keeps saying the iPhone is just marketing. &#x2F;sI’ve developed for and used both, and I’ve settled on iPhones for the last few generations. Though, I think flagship devices of either are fine nowadays. The ‘slab of glass’ phone is basically a solved problem at this point. reply nortonham 5 hours agoparentprev> the iPhone is nudging me further to giving a Macbook&#x2F;OSX a try one day,Gaming isn&#x27;t great on Mac (depending on what games you play), but macbooks are great imo. A pro or an air with apple silicon is worth the money. I&#x27;ve never really appreciated the build quality of a mac before.> I just got an iPhone for the first time, and it is a noticeably better device than my previous Android phones.In what way? What Android phones did you use in the past? reply catlover76 4 hours agorootparentMy last phone was a Samsung Galaxy A50, and it was pretty good as far as the hardware went. But I felt that Android was bad; I couldn&#x27;t help but notice that a number of the apps loaded very slowly and had other small glitchy issues. Nothing that would compel a person to switch phones, just a lot of mostly minor inconveniences, except for 2 that really stick out right now: the Android messages web app always had a hard time connecting to my phone even when the phone was clearly on, had the messages app open, and was connected to same wifi as my computer; and second, Chrome recently just started becoming unopenable (like it would open and then immediately close for some reason).These are just a couple data points in a bag of info and anecdata that has made me question whether Google is a company whose products are worth investing in, gives a damn, etc.As to what exactly about the iPhone seems so good in comparison, the things that really stand out are the crisp aesthetics (noticeably better \"graphics\" than my Samsung) and the speed of everything. It&#x27;s also just pleasant to touch&#x2F;interact with--I think part of that is Apple&#x27;s craftsmanship, and part is simply the fact that the phone is new. reply lyton 4 hours agorootparentwhich iPhone model did you go for? reply catlover76 4 hours agorootparentiPhone 15 (normal) reply vips7L 14 hours agoparentprevWindows Phone Link does support iMessage now. reply roskelld 7 hours agorootparentWindows 11 only for iPhones. Sigh. reply catlover76 14 hours agorootparentprevsurprisedpikachu.gifedit: just set it up and gave it a test--seems to work pretty well! reply tech234a 14 hours agorootparentI don&#x27;t believe they actually did any reverse engineering for Windows Phone Link. iOS makes SMS&#x2F;iMessages available over Bluetooth as part of its support for the Message Access Profile [1], intended for sending messages using a car infotainment system. This requires a physical iOS device to be located in proximity of the Windows device.[1]: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;102842 reply josefresco 13 hours agorootparentprevIt works... \"ok\" but doesn&#x27;t handle group messages. I find sometimes it just doesn&#x27;t connect. They do post frequent updates though so there&#x27;s clearly an active team managing the app.I love being able to easily send URLs and other copy+paste items to my iMessage contacts from Windows! reply arjvik 9 hours agoparentprevLook into Beeper for iMessage support on Linux and Windows! reply ChrisMarshallNY 14 hours agoparentprevNot sure if that will ever improve.I don’t really use the Mac for gaming.However, Apple Silicon may change the landscape reply beretguy 15 hours agoparentprevA much more major issue with Macs is planned obsolescence. It’s the only reason I am not buying any Macs. reply bobchadwick 15 hours agorootparentMy late-2013 MacBook Pro recently gave up the ghost. I&#x27;d used it daily in the ten years it worked. Are there other PC manufacturers who make laptops that are still useable after ten years? reply IntelMiner 14 hours agorootparentBoth desktop and laptop computers have been perfectly serviceable for that long for a while now. Computers are \"good enough\" for tbe overwhelming majority of tasks most users (note, most regular users, not the HN crowd) would throw at them reply eropple 14 hours agorootparentDesktops, I&#x27;d agree. My experience with most Windows laptops, non-Thinkpad class, is that they physically haven&#x27;t been able to survive that long. Like, people rag rightly on the butterfly keyboard era of Macbook Pros, but until recently you&#x27;d see pretty drastic hinge or keyboard or touchpad or case failures on even fairly expensive laptops. Especially as you get into more slimline&#x2F;ultrabook form factors; I&#x27;ve seen some really bludgeoned Dells and HPs in particular. (Though I liked my Spectre x360 aside from the party where it fell apart in normal everyday use.)I recently took a 2012 rMBP out of rotation (~five years dedicated use, the last five intermittently as a Logic Pro workstation) and now it&#x27;s a Kubernetes homelab node. But I took it out because Thunderbolt 3 now means I can just slot my M1 Max into my workspace and don&#x27;t need a dedicated box; the keyboard, touchpad, hinge, screen, and case are all pristine, I didn&#x27;t remove it due to hardware expiry. reply fsckboy 3 hours agorootparentprev> other PC manufacturers who make laptops that are still useable after ten years?my Lenovo Thinkpads are working great after 12 yrs, with 16G ram and disk upgraded to 4TB SSD reply smallerfish 13 hours agorootparentprevI mean if we&#x27;re playing anecdata, my spouse has been through 4 mac laptops in the same period, which have given up the ghost in various different ways. reply dmz73 12 hours agorootparentprevApple hardware is mediocre at best. 2020 MacBook Air with i5 is unbearably slow. I have Samsung ATIV 700T with i5 from 2014 and it feels much faster than 2020 i5 MacBook. You can now say that it is the problem with Intel and that M1-2-3 are so much better but I have some Intel i7 laptops from 2016 and 2021 and they also blow Intel Mac away in speed and reliability and are comparable in speed with M2 that is sitting next to 2020 Mac. 2 other older MacBooks are falling apart (2009 and G4) wheres even older Dells and comparable HPs are still feeling robust...and are used more than decrepit Apple hardware. reply overgard 11 hours agorootparentprevMy 2013 MacBook lasted 9 years (I&#x27;d still be using it if the battery connector wasn&#x27;t shot.) In my experience Mac&#x27;s last a lot longer than my equivalent PC&#x27;s, although w&#x2F; an initial premium of course. reply kube-system 11 hours agorootparentprevAnd traditional PC makers have a problem with unplanned obsolescence. A lot of consumer hardware does not receive updates from the manufacturer after the device is off shelves. reply autoexec 10 hours agoprevI was hoping this would explain why iMessage allows invisible messages and attachments. I really can&#x27;t think of any reason why Apple would want to implement something like that, but they&#x27;ve been predictably used to infect devices. reply kccqzy 3 hours agoparentIf you are talking about malware, then there&#x27;s no need for Apple to implement something like invisible messages. Malware essentially just exploits the parser, takes over execution, and never executes the code to display messages or puts them into the chat history. reply yalogin 10 hours agoparentprevThis has nothing to do with the iMessage protocol itself. Invisible messages looks like a bug, as it depends on the current UI and rendering repercussions. May be if you file a bug on them they may respond. reply maqp 12 hours agoprevGonna repeat myself since iMessage hasn&#x27;t improved one bit after four years. I also added some edits since attacks and Signal have improved.iMessage has several problems:1. iMessage uses RSA instead of Diffie-Hellman. This means there is no forward secrecy. If the endpoint is compromised at any point, it allows the adversary who hasa) been collecting messages in transit from the backbone, orb) in cases where clients talk to server over forward secret connection, who has been collecting messages from the IM serverto retroactively decrypt all messages encrypted with the corresponding RSA private key. With iMessage the RSA key lasts practically forever, so one key can decrypt years worth of communication.I&#x27;ve often heard people say \"you&#x27;re wrong, iMessage uses unique per-message key and AES which is unbreakable!\" Both of these are true, but the unique AES-key is delivered right next to the message, encrypted with the public RSA-key. It&#x27;s like transport of safe where the key to that safe sits in a glass box that&#x27;s strapped against the safe.2. The RSA key strength is only 1280 bits. This is dangerously close to what has been publicly broken. On Feb 28 2023, Boudet et. al broke a 829-bit key.To compare these key sizes, we use https:&#x2F;&#x2F;www.keylength.com&#x2F;en&#x2F;2&#x2F;1280-bit RSA key has 79 bits of symmetric security. 829-bit RSA key has ~68 bits of symmetric security. So compared to what has publicly been broken, iMessage RSA key is only 11 bits, or, 2048 times stronger.The same site estimates that in an optimistic scenario, intelligence agencies can only factor about 1507-bit RSA keys in 2024. The conservative (security-consious) estimate assumes they can break 1708-bit RSA keys at the moment.(Sidenote: Even the optimistic scenario is very close to 1536-bit DH-keys OTR-plugin uses, you might want to switch to OMEMO&#x2F;Signal protocol ASAP).Under e.g. keylength.com, no recommendation suggest using anything less than 2048 bits for RSA or classical Diffie-Hellman. iMessage is badly, badly outdated in this respect.3. iMessage uses digital signatures instead of MACs. This means that each sender of message generates irrefutable proof that they, and only could have authored the message. The standard practice since 2004 when OTR was released, has been to use Message Authentication Codes (MACs) that provide deniability by using a symmetric secret, shared over Diffie-Hellman.This means that Alice who talks to Bob can be sure received messages came from Bob, because she knows it wasn&#x27;t her. But it also means she can&#x27;t show the message from Bob to a third party and prove Bob wrote it, because she also has the symmetric key that in addition to verifying the message, could have been used to sign it. So Bob can deny he wrote the message.Now, this most likely does not mean anything in court, but that is no reason not to use best practices, always.4. The digital signature algorithm is ECDSA, based on NIST P-256 curve, which according to https:&#x2F;&#x2F;safecurves.cr.yp.to&#x2F; is not cryptographically safe. Most notably, it is not fully rigid, but manipulable: \"the coefficients of the curve have been generated by hashing the unexplained seed c49d3608 86e70493 6a6678e1 139d26b7 819f7e90\".5. iMessage is proprietary: You can&#x27;t be sure it doesn&#x27;t contain a backdoor that allows retrieval of messages or private keys with some secret control packet from Apple server6. iMessage allows undetectable man-in-the-middle attack. Even if we assume there is no backdoor that allows private key &#x2F; plaintext retrieval from endpoint, it&#x27;s impossible to ensure the communication is secure. Yes, the private key never leaves the device, but if you encrypt the message with a wrong public key (that you by definition need to receive over the Internet), you might be encrypting messages to wrong party.You can NOT verify this by e.g. sitting on a park bench with your buddy, and seeing that they receive the message seemingly immediately. It&#x27;s not like the attack requires that some NSA agent hears their eavesdropping phone 1 beep, and once they have read the message, they type it to eavesdropping phone 2 that then forwards the message to the recipient. The attack can be trivially automated, and is instantaneous.So with iMessage the problem is, Apple chooses the public key for you. It sends it to your device and says: \"Hey Alice, this is Bob&#x27;s public key. If you send a message encrypted with this public key, only Bob can read it. Pinky promise!\"Proper messaging applications use what are called public key fingerprints that allow you to verify off-band, that the messages your phone outputs, are end-to-end encrypted with the correct public key, i.e. the one that matches the private key of your buddy&#x27;s device.7. iMessage allows undetectable key insertion attacks.EDIT: This has actually has some improvements made a month ago! Please see the discussion in replies.When your buddy buys a new iDevice like laptop, they can use iMessage on that device. You won&#x27;t get a notification about this, but what happens on the background is, that new device of your buddy generates an RSA key pair, and sends the public part to Apple&#x27;s key management server. Apple will then forward the public key to your device, and when you send a message to that buddy, your device will first encrypt the message with the AES key, and it will then encrypt the AES key with public RSA key of each device of your buddy. The encrypted message and the encrypted AES-keys are then passed to Apple&#x27;s message server where they sit until the buddy fetches new messages for some device.Like I said, you will never get a notification like \"Hey Alice, looks like Bob has a brand new cool laptop, I&#x27;m adding the iMessage public keys for it so they can read iMessages you send them from that device too\".This means that the government who issues a FISA court national security request (stronger form of NSL), or any attacker who hacks iMessage key management server, or any attacker that breaks the TLS-connection between you and the key management server, can send your device a packet that contains RSA-public key of the attacker, and claim that it belongs to some iDevice Bob has.You could possibly detect this by asking Bob how many iDevices they have, and by stripping down TLS from iMessage and seeing how many encrypted AES-keys are being output. But it&#x27;s also possible Apple can remove keys from your device too to keep iMessage snappy: they can very possibly replace keys in your device. Even if they can&#x27;t do that, they can wait until your buddy buys a new iDevice, and only then perform the man-in-the-middle attack against that key.To sum it up, like Matthew Green said[1]: \"Fundamentally the mantra of iMessage is “keep it simple, stupid”. It’s not really designed to be an encryption system as much as it is a text message system that happens to include encryption.\"Apple has great security design in many parts of its ecosystem. However, iMessage is EXTREMELY bad design, and should not be used under any circumstances that require verifiable privacy.In comparison, Signal* Uses Diffie Hellman + Kyber, not RSA* Uses Curve25519 that is a safe curve with 128-bits of symmetric security, not 79 bits like iMessage.* Uses Kyber key exchange for post quantum security* Uses MACs instead of digital signatures* Is not just free and open source software, but has reproducible builds so you can be sure your binary matches the source code* Features public key fingerprints (called safety numbers) that allows verification that there is no MITM attack taking place* Does not allow key insertion attacks under any circumstances: You always get a notification that the encryption key changed. If you&#x27;ve verified the safety numbers and marked the safety numbers \"verified\", you won&#x27;t even be able to accidentally use the inserted key without manually approving the new keys.So do yourself a favor and switch to Signal ASAP.[1] https:&#x2F;&#x2F;blog.cryptographyengineering.com&#x2F;2015&#x2F;09&#x2F;09&#x2F;lets-tal... reply nicolas_17 4 hours agoparent> It’s not really designed to be an encryption system as much as it is a text message system that happens to include encryption.And yet, Apple uses this (flawed?) encryption in lots of other features. It&#x27;s not a messaging platform that happens to include encryption, it&#x27;s a messaging platform (iMessage&#x2F;Madrid) built on top of a generic&#x2F;reusable encryption system (IDS), and many other Apple protocols are built on top of IDS. Apple&#x27;s \"platform security guide\" has several places where they recognize this:\"When a user signs in to iCloud on a second Handoff-capable device, the two devices establish a Bluetooth Low Energy (BLE) 4.2 pairing out-of-band using APNs. The individual messages are encrypted much like messages in iMessage are.\"\"When an incoming call arrives, all configured devices are notified using the Apple Push Notification service (APNs), with each notification using the same end-to-end encryption as iMessage.\" reply jjtech 10 hours agoparentprevWhile I will definitely agree that Signal is more secure:There is a newer version of the iMessage encryption (sometimes called \"pair-ec\") which uses ECIES. Beeper implements it, I never got around to backporting it to pypush proper.Also, the new Contact Key Verification (I believe it is the same thing as \"key transparency\" internally) should prevent the man-in-the-middle.A lot of the things you mentioned can actually be solved on the pypush side: there&#x27;s nothing preventing pypush from alerting you when a new key is inserted, or providing you with the fingerprints of each of the keys.I&#x27;m not an expert on these things, but I do think it is time that another analysis by a proper cryptographer was done: the one you linked was from 2015, and a lot has changed since then.Anyway, the point of iMessage is convenience, if we&#x27;re being honest here. It provides a reasonable level of security that will keep out all but the most entrenched and determined attackers, and that&#x27;s really all most people care about. reply maqp 10 hours agorootparentIf third party client has optional E2EE, it&#x27;s not exactly a merit to Apple, aside perhaps them not explicitly blocking such development.I commented on the key verification in the other reply, it appears to be opt-in feature, so warnings about key changes are similar to WhatsApp, available if you known about them and you know you need them.>A lot of the things you mentioned can actually be solved on the pypush side:Yeah a lot of the problems can usually be fixed by fixing them. :) \"At least it&#x27;s not fundamentally borked\" can&#x27;t be the standard for a multi-trillion dollar company.>a lot has changed since thenThat&#x27;s just the sad part. 1280-bit keys are still there. RSA is still there. Fingerprints were added but they&#x27;re opt-in.Apple can afford to hire Moxie or OWS to implement Signal protocol for them. The fact they treat iMessage as a second class SW in their otherwise high security is ridiculous. People deserve better and they should demand better.>It provides a reasonable level of securityBut that&#x27;s just it. RSA isn&#x27;t reasonable. Forward secrecy became the reasonable expectation in new protocols in 2004. It was &#x27;This Love&#x27; by &#x27;Maroon 5&#x27; years ago. TLS1.3 has already killed RSA entirely. 1280-bit keys haven&#x27;t weren&#x27;t acceptable even then. OTR from 2004 used 1536-bit RSA.If people knew it was borderline ancient in terms of it&#x27;s technology, they probably wouldn&#x27;t find the unnecessary risks convenient.My point is: Apple can afford an overhaul, and they damn well should rewrite the protocol. reply astrange 11 hours agoparentprev> 7. iMessage allows undetectable key insertion attacks.https:&#x2F;&#x2F;security.apple.com&#x2F;blog&#x2F;imessage-contact-key-verific... reply maqp 10 hours agorootparentWhoa, nice to see there&#x27;s some progression finally! It&#x27;s weird the blog appears to discuss key insertion, but not MITM attacks. Is there an official source that explicitly states it protects from those too? Also if it&#x27;s for MITM too, is there a TOFU warning or is it only for a changing fingerprint, and is the warning soft (BTW the fingerprint just changed) or hard (please accept the new keys&#x2F;fingerprint)? Can you mark the fingerprints verified like in Signal?https:&#x2F;&#x2F;restoreprivacy.com&#x2F;apple-to-introduce-contact-key-ve... apparently states that but I&#x27;d rather have something official.Also it seems to be opt-in, at least for now https:&#x2F;&#x2F;9to5mac.com&#x2F;2023&#x2F;10&#x2F;27&#x2F;turn-on-contact-key-verificat... reply astrange 10 hours agorootparentUnfortunately I have no idea, or else I would&#x27;ve written a longer comment! reply SpaceManNabs 2 hours agoparentprevI didn’t realize signal was so secure. Is this common to ga even post quantum guarantees? reply whynot-123 16 hours agoprevI would like to point out how awesome it is that someone in high school is making this caliber of a post. I&#x27;ve thought at least a dozen times over the last 20 years how i would like to understand macOS internals, and this person is deconstructing it. well done! reply apetresc 14 hours agoparentFully agree, but you&#x27;re even burying the lede here. He didn&#x27;t just write the blog post, he wrote pypush itself. reply nicolas_17 4 hours agorootparentI have confirmed with him that he hadn&#x27;t been born yet when Steve Jobs announced the first iPhone. I feel old. reply dinobones 12 hours agoprevReverse engineering iMessage has been touted as some holy grail meme for what... 10+ years now?So proud that a high school student was the one to finally figure it out.In a world of 100s of thousands of software engineers, \"Cybersecurtiy professionals\", and so on.A kid with almost no credentials out-innovates everyone because they have talent and focus. Literally HackerNews! My favorite kind of news. reply cynicalsecurity 13 hours agoprev> In order to generate the “validation data”, pieces of information about the device such as its serial number, model, and disk UUID are used.Sadly, this is a clear sign the project is going to stop working eventually. At some point, the Apple is simply going to pull the plug.I remember doing similar tricks when I was a kid. Nowadays I simply won&#x27;t even care trying. The problem clearly isn&#x27;t supposed to be solved this way. I&#x27;m not even sure if it&#x27;s a good exercise in programming either. Software development is about doing the things the right way, not exercising in futility.A better experience would be writing your own message delivery solution, superior to iMessage. reply dinobones 12 hours agoparent\"I remember doing similar tricks when I was a kid. Nowadays I simply won&#x27;t even care trying. The problem clearly isn&#x27;t supposed to be solved this way.\"This level of snark is undeserved, and a subtle amount of bitterness&#x2F;jealousy leaks through.Even if this stops working, this was a fantastic exercise to learn and practice reverse engineering.\"The problem clearly isn&#x27;t supposed to be solved this way.\" No duh, there is no public iMessage API and not even the EU can make that happen. There is nothing wrong with *hacking* a solution to a problem.\"Software development is about doing the things the right way, not exercising in futility.\" LOL what? Okay thanks Agent Smith, have fun at your BigCo job installing Norton antivirus and pinging me about updating my laptop every 2 weeks. reply zer0zzz 11 hours agorootparentI think the engineering on this project is a great step forward, I am not a lawyer but I think it’s possibly actually especially a step forward if Apple pulls the plug on this because it will add that much more ammunition to the case regulators have against Apple using their services as gatekeepers. reply wizerdrobe 11 hours agorootparentprev> \"I remember doing similar tricks when I was a kid. Nowadays I simply won&#x27;t even care trying. The problem clearly isn&#x27;t supposed to be solved this way.\"For some, being a hacker is a fashion and a phase. Much like being a punk. reply nrb 12 hours agorootparentprev> Even if this stops working, this was a fantastic exercise to learn and practice reverse engineering.I agree in principle, but I’d try to avoid running afoul of the Computer Fraud and Abuse Act against one of the most deep-pocketed legal teams in the history of capitalism.Extremely impressive work, but whether it’s worth the potential risk is another story, personally speaking. reply mrpippy 12 hours agoparentprevTo me, the more concerning paragraph is the next one:> Note: The binary that generates this “validation data” is highly obfuscated. pypush sidesteps this issue by using a custom mach-o loader and the Unicorn Engine to emulate an obfuscated binary. pypush also bundles device properties such as the serial number in a file called data.plist, which it feeds to the emulated binary.The binary being emulated was extracted from an old macOS version and is hosted on GitHub: https:&#x2F;&#x2F;github.com&#x2F;JJTech0130&#x2F;nacserver. Apple obviously holds the copyright on this binary, and issuing a takedown would be the easiest way to sink this project. I wonder if the Beeper Android app also includes the file, that would be legally problematic. reply jjtech 10 hours agorootparentI was thinking of finding a way to extract it directly from old Mac OS X updates downloaded directly from Apple... anyway, Beeper&#x27;s app doesn&#x27;t use it, that&#x27;s purely a hack I came up with to make the proof-of-concept easier to use. reply mrpippy 5 hours agorootparentInteresting, how does Beeper avoid including it? reply hn_throwaway_99 11 hours agoparentprev> I remember doing similar tricks when I was a kid. Nowadays I simply won&#x27;t even care trying. The problem clearly isn&#x27;t supposed to be solved this way.Not to be too harsh (maybe to be somewhat harsh given I had such a distaste for what you wrote?), but why would you post this on a site called Hacker News? I can&#x27;t think of a better implementation of the \"hacker ethos\" than this project: look at a hard problem, and when the \"straightforward\" approach doesn&#x27;t work, find a workaround.More to your specific point about \"Apple is simply going to pull the plug\", there are technical and business reasons why they might not want to, at least not quickly. First, as mentioned in the other Beeper thread, there are lots of older Mac devices without a secure enclave, and breaking Beeper would likely break them as well. Second, from a business and regulatory perspective, Apple might have to do a careful dance regarding how to shut this down without looking blatantly anti-competitive. reply jowea 13 hours agoparentprevI get it and it may be true in this case that Apple can too easily pull the plug, adversarial interoperability has a long history: https:&#x2F;&#x2F;www.eff.org&#x2F;deeplinks&#x2F;2019&#x2F;06&#x2F;adversarial-interopera... reply ianlevesque 12 hours agorootparentThe messaging space also had the amazing Adium client during the last round of messaging wars, and less amazing Trillian as reverse engineered clients distributed or sold. I for one am excited to see this space heating back up. reply panzi 12 hours agorootparentAnd Miranda and Kopete and more. Might have used them all at some point. reply selykg 12 hours agorootparentprevTrillian used to be amazing. It is up there in my memory as about as life changing as Winamp was for me personally. reply joshmanders 11 hours agorootparentI remember being jealous I couldn&#x27;t use Trillian because I didn&#x27;t have a way to pay for it. Running AIM, ICQ and MSN all at the same time. reply selykg 11 hours agorootparentAh man, it was glorious. I was really just in awe at how I could talk to all my various friends in one app, regardless of which platform they were on. Such a great app. I recently went to the webpage for the app and see it&#x27;s sort of a shell of its former self and is some sort of business tool now. Kind of a bummer, but such fond memories of how amazing it was back in the peak of the various instant messaging tools, before unlimited text messaging was an affordable option. replycurt15 11 hours agoparentprev>I&#x27;m not even sure if it&#x27;s a good exercise in programming either. Software development is about doing the things the right way, not exercising in futility.Reverse engineering is a valuable art that can&#x27;t be learned just from a canonical reference for \"the right way\". It cultivates the same skills used in debugging. reply haswell 11 hours agoparentprev> Software development is about doing the things the right way, not exercising in futility.I strongly disagree on the first point, and mostly disagree on the second. The first point is antithetical to the hacker mindset.Software development is about solving problems using computers and code. Some of the most interesting and impactful work I’ve done involved doing things the “wrong” way as a way to get people’s attention. Some of these prototypes raise awareness. Some of them become the precursor to a project that does things “right”. And sometimes, just getting something to work is the only thing that really matters.Software development is also about trying things and seeing what works for the sake of learning about it. I’ve written tons of code that never made it to production, but the act of writing it taught me so much that the time was well spent.> A better experience would be writing your own message delivery solution, superior to iMessage.This completely misses the point. People don’t want a better experience. They just want to use iMessage on Android. They want to be part of the blue bubble group chats.Building a new “superior” solution just creates another iteration of the current problem and solves nothing. reply 9dev 10 hours agoparentprevThere is a wonderful song by a German band, which roughly translates to \"Pure reason must never prevail.\"Sometimes you grow the most when doing things the way you aren’t supposed to. reply kazinator 10 hours agorootparentThere is also a wonderful book by a German philosopher, titled The Critique of Pure Reason. reply vinniepukh 11 hours agoparentprevwow, haven&#x27;t read something this off-base ina while reply devaiops9001 11 hours agoprevnext [2 more] [flagged] local_crmdgeon 11 hours agoparentWhat reply edweis 11 hours agoprevMore and more often, I see titles that are not capitalized.Is it a new trend ? reply walteweiss 4 hours agoparentI guess most of the people never knew the titles are different. reply ChrisMarshallNY 11 hours agoprev [–] I just got done adding APNs to one of my dashboard apps.It&#x27;s a wicked pain in the butt, but I finally got it. The trickiest part was the backend server, which I implemented in ... gasp PHP. I didn&#x27;t want to load in a whole SaaS, in order to do a very simple push notification, so I had to learn to do it from scratch.In the process, I learned that there&#x27;s a lot of wrong information out there, and I had do quite a bit of trial and error.But it works, and the code is actually wicked simple. reply nicolas_17 4 hours agoparent [–] The protocol between your backend server and Apple, and between Apple and the phone, are completely different. So this comment seems almost off-topic... reply ChrisMarshallNY 3 hours agorootparent [–] No, it’s not off-topic. I’m not claiming to have reverse-engineered that stuff (I have successfully reverse-engineered internal Apple tech in the past, and it hasn’t ended well. I’ve been writing Apple software, sometimes, at a fairly low level, for quite a while). I suspect there may be other reasons for the downvotes. It isn’t really something I’m losing much sleep over.It’s just a fairly typical type of HN comment. I literally, like, yesterday, got it going, and it was a less-than-linear process. My own experience, in hacking the system, has taught me to try to stay inside the lines –something that is increasingly difficult, these days.This article (which is excellent, and quite worthy of HN) made me think of it. Also, even doing what I did, has its challenges. I didn’t want to distract from the OP, by going into any detail. Maybe, one day, I’ll write it up (I’m fairly good at that kind of thing), but that is something for another day. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This blog post offers a comprehensive explanation of the internal workings of iMessage, with a particular focus on two key components: the Apple Push Notification Service (APNs) and the IDentity Services (IDS).",
      "The Apple Push Notification Service (APNs) is responsible for facilitating the sending and receiving of push notifications in iMessage.",
      "The IDentity Services (IDS) function as a keyserver in encrypted conversations, allowing for the exchange of public keys."
    ],
    "commentSummary": [
      "Summaries cover various topics related to Apple's iMessage: security, messaging app preferences, device durability, encryption standards, reverse engineering, and interoperability.",
      "Common themes include concerns about iMessage's security vulnerabilities and comparisons to other messaging apps like Signal.",
      "Discussions also involve debates on the durability and longevity of Macs compared to PCs, as well as the importance of experimentation and prototyping in software development."
    ],
    "points": 548,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1701790403
  },
  {
    "id": 38531428,
    "title": "How Shazam's Algorithm Identifies Songs: A Deep Dive (2022)",
    "originLink": "https://www.cameronmacleod.com/blog/how-does-shazam-work",
    "originBody": "Cameron MacLeod About CV Projects abracadabra: How does Shazam work? Sat 19 February 2022 Tutorials Your phone's ability to identify any song it listens to is pure technological magic. In this article, I'll show you how one of the most popular apps, Shazam, does it. The founders of Shazam released a paper in 2003 documenting how it works, and I have been working on an implementation of that paper, abracadabra. Where the paper doesn't explain something, I will fill in the gaps with how abracadabra approaches it. I've also included links to the corresponding part of the abracadabra codebase in relevant sections so you can follow along in Python if you prefer. The state of the art has moved on since this paper, and Shazam has probably evolved its algorithm. However, the core principles of audio identification systems haven't changed, and the accuracy you can obtain using the original Shazam method is impressive. To get the most out of this article, you should understand: Frequency and pitch Waves Graphs and axes Quick links What is Shazam? Why is song recognition hard anyway? System overview Calculating a spectrogram The Fourier transform Spectrograms Fingerprinting Why is the fingerprint based on spectrogram peaks? Finding peaks Hashing Matching Conclusion Enter abracadabra Further reading What is Shazam? Shazam is an app that identifies songs that are playing around you. You open the app while music is playing, and Shazam will record a few seconds of audio which it uses to search its database. Once it identifies the song that's playing, it will display the result on screen. Before Shazam was an app, it was a phone number. To identify a song, you would ring up the number and hold your phone's microphone to the music. After 30 seconds, Shazam would hang up and then text you details on the song you were listening to. If you were using a mobile phone back in 2002, you'll understand that the quality of phone calls back then made this a challenging task! Why is song recognition hard anyway? If you haven't done much signal processing before, it may not be obvious why this is a difficult problem to solve. To help give you an idea, take a look at the following audio: The above graph shows what Chris Cornell's \"Like a Stone\" looks like when stored in a computer. Now take a look at the following section of the track: If you wanted to tell whether this section of audio came from the track above, you could use a brute-force method. For example, you could slide the section of audio along the track and see if it matches at any point: This would be a bit slow, but it would work. Now imagine that you didn't know which track this audio came from, and you had a database of 10 million songs to search. This would take a lot longer! What's worse, when you move from this toy example to samples that are recorded through a microphone you introduce background noise, frequency effects, amplitude changes and more. All of these can change the shape of the audio significantly. The sliding method just doesn't work that well for this problem. Thankfully, Shazam's approach is a lot smarter than that. In the next section, you'll see the high-level overview of how this works. System overview If Shazam doesn't take the sliding approach we described above, what does it do? Take a look at the following high-level diagram: The first thing you will notice is that the diagram is split up into register and recognise flows. The register flow remembers a song to enable it to be recognised in the future. The recognise flow identifies a short section of audio. Registering a song and identifying some audio share a lot of commonality. The following sections will go into more detail, but both flows have the following steps: Calculate the spectrogram of the song/audio. This is a graph of frequency against time. We'll talk more about spectrograms later. Find peaks in that spectrogram. These represent the loudest frequencies in the audio and will help us build a fingerprint. Hash these peaks. In short, this means pairing peaks up to make a better fingerprint. After calculating these hashes, the register flow will store them in the database. The recognise flow will compare them to hashes already in the database to identify which song is playing through the matching step. In the next few sections, you'll learn more about each of these steps. Calculating a spectrogram The first step for both flows is to obtain a spectrogram of the audio being registered or recognised. To understand spectrograms, you first have to understand Fourier transforms. The Fourier transform A Fourier transform takes some audio and tells you which frequencies are present in that audio. For example, if you took a 20 Hertz sine wave and used the Fourier transform on it, you would see a big spike around 20 Hertz (Hz): In the above image, you can see a large spike around 20Hz and nothing at other frequencies. Sine waves are often called pure tones because of this property, since they only contain a single frequency. The result of a Fourier transform is called a frequency spectrum. We say that when you take the Fourier transform of a signal, you move it from the time domain into the frequency domain. These are fancy terms for describing whether time or frequency is along the bottom of a graph. In mathematical terms, the domain is more or less the X-axis of a graph. The Y-axis of the frequency spectrum represents the strength of each frequency component. If a frequency component is stronger, then it will be more audible in the time-domain signal. If you were to add a 50Hz sine wave at half the strength to that 20Hz sine wave, the resulting frequency spectrum would show a spike at 20Hz and a smaller spike at 50Hz: As you can see, adding multiple audio waves together combines the frequencies present in them. In fact, all audio signals can be reconstructed from waves like this. For more, take a look at 3Blue1Brown's video on the Fourier transform. One great property of the frequency domain is that it often helps us to see things that aren't clear in the time domain. For example, if you take the signal with two frequencies from before and add noise to it, in the time domain it looks visually very different. However, in the frequency domain, the two spikes are still very clear: In the frequency domain graph on the right, you can still clearly see the spikes for the main component frequencies. It would be harder in the time domain to see what frequency sine waves went into the signal. Up until now, our examples have only contained one or two frequencies, but what happens if you put a more complex signal through the Fourier transform? Let's take a look at our section of audio from Like a Stone: Real audio files like the one above contain lots of different frequencies. This is a good thing, as it means that the frequencies present can uniquely identify songs. Spectrograms abracadabra implementation If you run a Fourier transform over an entire song, then you will see the strength of the frequencies present over the whole song. However, the frequencies that are present change over time. To better represent the frequencies changing over time, we need to split the song into small sections before taking the Fourier transform. This is called taking a spectrogram. Here's a simplified animation of how spectrograms work: In the above animation, you can see that the song is first split into small sections. Next, we use the Fourier transform to calculate the frequency spectrum of each of these sections. When you put all these frequency spectrums together, you get a spectrogram. To make this concrete, let's take a look at the spectrogram of Like a Stone: Even though the spectrogram looks 2-dimensional, it's actually a 3D graph with the following axes: Time (X-axis) Frequency (Y-axis) Strength (Z-axis/colour) The Z-axis is represented by colour in the spectrogram above. Bright green shows a high magnitude for a particular frequency component and dark blue shows a low magnitude. Looking at the spectrogram above, you can see that the brightest spots (strongest frequencies) almost exclusively occur below 5000Hz. This is quite common with music, for example most pianos have a frequency range of 27Hz-4186Hz. The frequencies present in a track contain a lot of identifying information, and calculating the spectrogram allows us access to that information. In the next section, you'll learn how we turn all that information into a unique fingerprint for the track. Fingerprinting Just as a fingerprint uniquely identifies a person, we can extract a unique fingerprint for some audio from its spectrogram. These audio fingerprints rely on finding peaks in the spectrogram. These peaks are the loudest frequencies at some time in the song. Because they are loud, it's likely they'll survive when subjected to noise or other distortions. In the next section, you'll read some more about the motivation behind using spectrogram peaks to build fingerprints. Why is the fingerprint based on spectrogram peaks? A spectrogram peak is a frequency that is loud at some point in an audio signal. You can recognise these on a spectrogram since they will be the brightest points. In music, these would represent the loudest notes. For example, during a guitar solo, the notes that the guitar is playing might become spectrogram peaks since they would likely be the loudest notes at that time. A spectrogram peak is the point least likely to be affected by noise. Noise has to be louder than the spectrogram peak to make it unrecognisable and the spectrogram peaks are the loudest frequency components in the track. To make this visual, take a look at our earlier example of a Fourier transformed signal that had noise added to it. When noise is added, the frequency peaks still retain their rough shape. Another advantage of using spectrogram peaks to fingerprint audio is that they cut down the amount of data we have to store. Storing only the loudest frequency components means we don't have to store everything else. This speeds up searching fingerprints since there is less data to look through. Before we can use frequency peaks in our fingerprint though, we have to find them. In the next section, you'll learn how. Finding peaks abracadabra implementation As discussed in the previous section, the peaks of a spectrogram represent the strongest frequencies in a signal. For frequency peaks to be usable in an audio fingerprint, it's important that they are evenly spaced through the spectrogram. It's important the peaks are evenly spaced in time, so the system can recognise any section of the song. For example, if all the peaks were at the start of the song, then the fingerprint wouldn't cover later sections: In the image above, all the peaks (white crosses) are clustered at the start of the song. This means that the system can't recognise any sample from the rest of the song. It's also important that the peaks are evenly spaced in frequency, so the system can deal with noise and frequency distortion. Sometimes noise will be very loud and concentrated at a specific frequency range, for example a car horn in the background: In the above animation, the peaks are well-spaced in time, but are clustered into a small frequency band. When a loud noise is introduced, for example a car horn, it can make an entire section of song unrecognisable by changing which peaks are selected. To find spectrogram peaks while keeping them well-spaced, we can borrow a technique from image processing known as a maximum filter. The process looks something like the following: Use the maximum filter to highlight peaks in the spectrogram. Locate the highlighted peaks by comparing to our original spectrogram. (Optional) Discard some peaks. Let's run through these steps one-by-one. First, let's take a look at how the maximum filter works: Step 1: Maximum filter A maximum filter emphasises the peaks in an image. It does this by looking in a neighbourhood around each pixel for the maximum value and then setting the pixel to that local maximum. The following animation shows a maximum filter that looks at a 3x3 neighbourhood around each pixel: As you can see in the above animation, the maximum filter takes each pixel of an image in turn and finds the maximum in a region surrounding it. The filtered pixel is then set to that local maximum. This has the effect of expanding each local peak to its surrounding area. Running a maximum filter on Like a Stone's spectrogram gives the following result: The maximum-filtered spectrogram looks like a lower-resolution version of the original spectrogram. This is because the peaks in the signal have expanded and taken over the other pixels. Each box with the same colour in the filtered image corresponds to a local peak in the original image. The maximum filter has a parameter that controls the size of the box to use when finding the local maxima. When you set this parameter to make a smaller box, you end up getting more peaks. Similarly, by setting this parameter larger you get fewer peaks. Step 2: Recover original peaks The maximum filter doesn't do all the work for us. The filter has emphasised the local peaks, but it hasn't found their locations. To find the peak locations, we need to find the points that have equal values in the original spectrogram and the filtered spectrogram. The idea behind this trick is that all the non-peak points in the spectrogram have been replaced by their local peaks, so their values have changed. The only points whose values haven't changed are the peaks. Below is a zoomed in section of the spectrogram above. The points where the values are equal in the filtered and original spectrograms are highlighted: As you can see in the images above, the highlighted points where the two spectrograms are equal correspond to the local peaks of that part of the image. Plotting all of the peaks together produces something called a constellation map. Here's the constellation map for Like a Stone: These graphs are called constellation maps since they look a bit like an image of the night sky. Who said computer science couldn't be romantic? Step 3: (Optional) Discard peaks Once we have a constellation map of peaks, the next step is to potentially discard some. The size of our fingerprint is dependent on the number of peaks that we use in it. Keeping fingerprints small matters when you are storing millions of songs in your database. However, by reducing the number of peaks we use, we reduce the accuracy of our system. Fewer peaks in a fingerprint mean fewer chances to match a sample to the correct song. There are a couple of options for reducing the number of peaks in our fingerprint: Take the top N peaks. N should be proportional to the length of audio that you are fingerprinting to avoid over-representing shorter songs. Take all peaks above a certain threshold. This doesn't guarantee you a certain fingerprint size per time like the other method, but may give more accurate results. We have almost finished constructing our fingerprint, the next step is to produce a set of hashes from our peaks. Hashing abracadabra implementation To motivate hashing, imagine that our fingerprint was just a collection of spectrogram peaks. Each peak's frequency would be represented by a certain number of bits, for example 10. With 10 bits of information, we can represent 2^10=1024 individual frequencies. With thousands of these points per track, we quickly get a lot of repeats. Uniqueness is important for a fingerprint, since it makes searching a lot faster and helps to recognise more songs. Shazam's solution to the problem of uniqueness is to create hashes from pairs of peaks: The diagram above shows a zoomed in portion of a spectrogram. Each circle represents a peak and the dashed line box represents a hash. You can see that a hash is formed of two peaks. The information that is recorded for each hash is the frequency of each peak, fA and fB, and the time delta between them, 𝚫T. The advantage of pairing points up is that two paired points are much more unique than a single point. Looking at it mathematically, if each point has 10 bits of frequency information, and the time delta between the two points could be represented by 10 bits, then you have 30 bits of information. 2^30=1073741824 which is significantly larger than 1024 possibilities for a single point. Shazam creates pairs using the following algorithm: Pick a point. This will be called the anchor point. Calculate a target zone of the spectrogram for the anchor point. For every point in the target zone, create a pair with the anchor point. You can see this algorithm illustrated in the following animation: Choosing a target zone isn't described in the Shazam paper, but the images the paper contains show it as starting slightly ahead of time of the anchor point and being centred on the anchor point's frequency. Once a pair has been created, it is stored as a hash in the database with the following information:Other information Point A freq (fA) Point B freq (fB) Time delta (𝚫T) Point A time Track ID The first three columns (fA, fB and 𝚫T) make up the hash. The \"Other information\" is used to locate the hash at a specific time in a song. This will be used in matching later. All of the hashes for a particular track make up the fingerprint. In the next section, you'll read about how Shazam matches these fingerprints. Matching Given a collection of fingerprints in a database, how does Shazam figure out which one a given audio sample matches? This is where the matching part of the system comes in. Recall the system diagram from earlier: The recognise and register flows both generate fingerprints. The difference lies in what they do with them. While the register flow stores fingerprints away for future matching, the recognise flow has to match its fingerprint with what is already in the database. The matching algorithm contains the following steps: Retrieve all hashes from the database that match the sample's fingerprint. Group these hashes by song. For each song, figure out if the hashes line up. Choose the track with the most lined up hashes. We'll look at each of these steps in turn. Step 1: Retrieve matching hashes abracadabra implementation The first step is to find every hash in the database that matches a hash in the fingerprint we just created. Even though a hash is a 3-tuple of (fA, fB, 𝚫T), abracadabra stores this as hash(fA, fB, 𝚫T) where hash() is a hash function that returns a single value. This way you only have to search for a single value per hash instead of three. Step 2: Group hashes by song Recall the format of an individual hash in the database:Other information Point A freq (fA) Point B freq (fB) Time delta (𝚫T) Point A time Track ID Thanks to the track ID that we associated with each hash, we can group the hashes by track. This allows us to score each potentially matching track. Step 3: Figure out if hashes line up abracadabra implementation If a sample matches a song, then the hashes present in that sample should line up nicely with the hashes in some section of that song. The diagram below illustrates what this would look like: In the above diagram, a sample has been lined up with the section of the original song that it came from. The blue points represent the anchor points of the hashes. While the above diagram shows the perfect scenario, there is a chance that the matching hashes from the database don't line up perfectly. For example, noise could have introduced peaks in the sample that resemble peaks at a different point in the song. This can lead to the following scenario: In the above diagram, the red circles represent hashes that match to points in the song outside the section the sample came from. In this situation, it's harder to see that the sample is a perfect match for the song. What's worse, sometimes hashes can match to the wrong song! This is where checking that the hashes line up comes in. To explain how we can check whether the hashes line up in code, let's look at an example. Let's imagine that we've got a list of matching hashes from the database and grouped them by track. For a given track, we can then check the time that the hash occurs in the original track against the time that the hash occurs in the sample. Sample time Track time Track time - Sample time 3 13 10 4 14 10 7 20 13 5 15 10 6 12 6 1 11 10 In the above table, you can see that all the matches with a Track time - Sample time of 10 have been highlighted. These are the true matches, while the other two rows are false matches. To see this is the case, let's look at a similar diagram to the ones we saw before: The above diagram contains the same hashes from the previous table. As you can see, the true matches have a Track time - Sample time that is equal to how far into the track time that the sample starts. To see how we turn this into a score for the track, let's make this data into a histogram. A histogram is a fancy name for a bar chart. We're going to plot each Track time - Sample time against the number of times it occurs: Each bar in the histogram above is referred to as a bin. To score a song on how good a match it is for an audio sample, we just need to take the largest bin. Songs that aren't good matches will have low values in all bins, whereas a song that's a good match will have a large spike in one of the bins. This way we can compare a sample to all the songs with matching hashes in our database and score each of them. The song with the highest score is likely to be the correct result. You might wonder why we don't just go for the song that matches the largest number of hashes as it would be much simpler to implement. The problem with this approach is that not all songs are the same length. Longer songs are likely to get more matches than shorter songs and when some Spotify tracks are over 4 hours long this can really bias your results! Conclusion Well done for making it this far, that was a long journey! Over the course of this article, you've learned how Shazam extracts fingerprints from audio, and how it matches these fingerprints to those that it has already registered in its database. To summarise, Shazam does the following to register a song: Calculates a spectrogram of a song Extracts peaks from that spectrogram Pairs those peaks up into hashes Stores the collection of hashes for a song as a fingerprint Shazam does the following to recognise an audio sample: Calculates a fingerprint of the audio sample Finds the hashes that match that fingerprint in the database For each potential song match: Calculate Track time - Sample time for each matching hash Group those values into a histogram Take the largest bin in this histogram as the score for the song Return the song with the highest score Enter abracadabra I learned everything written here over the process of writing abracadabra, my implementation of this paper. If you are interested in seeing what this might look like in code, please take a look! Everything is open source and I've done my best to document the project. abracadabra can also be used as a library in other projects, so please feel free to re-mix and build something cool. If you do use it, I'd love to hear about it. Further reading If you want to find out more about anything mentioned in this article, take a look below. I've also scattered some helpful links throughout the page. abracadabra docs dejavu is another implementation of a song recogniser in Python. The author wrote a wonderful explanation on how it works. Computer Vision for Music Identification is another approach to song recognition that is similar to how dejavu works. An algorithm that takes a slightly different approach is Chromaprint. Musicbrainz is an open-source encyclopedia of music information. This page explains how they fingerprint audio. Playing with Shazam fingerprints is an article from 2009 about the author's experience implementing the Shazam algorithm. Alignment of videos of same event using audio fingerprinting is an example of a use case for this algorithm that goes beyond music.How to sort parent nodes before child nodes? - Topological sort Subscribe No spam, just new posts. Choose what you receive and unsubscribe whenever. Email Address Which posts do you want to receive? Blog posts Tutorials",
    "commentLink": "https://news.ycombinator.com/item?id=38531428",
    "commentBody": "How does Shazam work? (2022)Hacker NewspastloginHow does Shazam work? (2022) (cameronmacleod.com) 418 points by TaurenHunter 13 hours ago| hidepastfavorite119 comments mmaunder 12 hours agoThis was the smart approach when Shazam launched in 2008. I would have done exactly the same thing - gone straight to developing a method to turn every song into a hash as computationally efficiently as possible. If you launched this today the default R&D approach would be to train a model which may turn out to be far less efficient and more expensive to host. It feels like the kind of thing a model might be good at, but given that there are a finite number of songs, taking a hash-based approach is probably way more performant. reply crazygringo 10 hours agoparent> to turn every song into a hashJust to be clear, it&#x27;s not turning each song into a hash.It&#x27;s turning each song into many hundreds (thousands?) of hashes.And then you&#x27;re looking for the greatest number of mostly-consecutive matches of tens (or low hundreds) of hashes from your shorter sample.Also, I don&#x27;t think this would be done with training a model today, because you&#x27;re adding many, many new songs each day, that would necessitate constant retraining. Hashes are still going to be the superior approach, not just for efficiency but for robustness generally. reply casualscience 6 hours agorootparentI&#x27;m an MLE, I would probably chop the songs into short segments, add noise (particularly trying to layer in people talking, room noise, and apply frequency-based filtering), and create a dataset like that. Then I would create contrastive embeddings with a hinge loss with a convnet on the spectrogram.Ultimately this looks the same, but the \"hashes\" come from a convnet now. But you still are doing some nearest neighbor thing to actually choose the best match.I imagine this is what 90% of MLEs would do, not sure if it would work better or worse than what Shazaam did. Prior to knowing Shazaam works, I might think this is a pretty hard problem, knowing Shazaam works, I am very confident the approach above would be competitive. reply osrec 2 hours agorootparentWhy add noise to the training set, rather than attempt to denoise the input? reply sdenton4 27 minutes agorootparentSo you want a location-sensitive hash, or embedding, and you want it to be noise resistant.The ml approach is to define a family of data augmentations A, and a network N, such that for some augmentation f, we have N(f(x)) ~= N(x). Then we learn the weights of N, and on real data have N(x&#x27;)~=N(x).The denoising approach is to define a set of denoising algorithms D and hash function H, so that H(D(x&#x27;))~=H(x). This largely relies on D(x&#x27;)~=x, which may have real problems.So the neutral network learns the function we actually need, with the properties we want, where the denoiser is designed for a proxy problem.But that&#x27;s not all...Eventually our noise model needs extending (eg, reverb is a problem): the ML approach adds a new set of augmentations to A. This is fine: it&#x27;s easy to add new augmentations.But the denoiser might need some real algorithm work, and hope that there&#x27;s no bad interaction with other parts of the pipeline, or too much additional compute overhead. (And de-reverb is notoriously hard.) reply bottled_poe 1 hour agorootparentprevBecause then you’re training it on data that is more similar to the operating environment for the application. It’s a better fit for purpose. If the target environment was a clean audio signal, you’d optimise for that instead. reply wiz21c 1 hour agorootparentprevTo start from an original song and move it towards something that resebme a real life recording ? IOW : make the NN learn to distinguish between the song sound and its environment ? reply omnster 1 hour agorootparentprevI&#x27;d assume adding noise is done once per song and is thus a bit computationally cheaper than trying to denoise each input. reply teeray 5 hours agorootparentprevDepends what the model targets. If I gave this problem to a bunch of musicians, they’d be pulling out features like the key, tempo, meter, chord progressions, any distinctive riffs or basslines, etc. Those are the things hashes could be built from and would be more information-dense than samples of the particular recording.Using a model to deconstruct a song like that might enable the ability to recognize someone playing the opening bars of Mr. Brightside on a piano in a loud bar as well as its drunkest patrons. reply namtab00 1 hour agorootparentyou couldn&#x27;t recognize anything ambient like, let&#x27;s say, Loscil reply sdenton4 41 minutes agorootparentHard times for Indian classical music, as well... reply brainfog 9 hours agorootparentprev> that would necessitate constant retrainingYou wouldn&#x27;t necessarily need to retrain that frequently. If your model outputs hashes &#x2F; vectors that can be used for searching, you just need to run inference on your new data as it comes in. reply Legend2440 8 hours agorootparentDefinitely this, embeddings would be the modern approach. reply raylad 3 hours agorootparentThe \"modern\" approach sounds like it would be a lot worse than this approach both in terms of input and runtime performance.Trendy (\"modern\") is not necessarily better. reply lysecret 3 hours agorootparentprevYou would just rename hashes to embeddings. reply iamflimflam1 5 hours agorootparentprevYou would probably take the approach that is used for face recognition. You train a model that can tell if two faces are the “same”. Then you match an unknown face against your database of faces. You can get clever and pull out the “encoding” of a face from the trained model. reply guyomes 1 hour agoparentprevThe smart approach in 1975 was to use Parsons code, which was also turning songs into hashes, computable in your head. You could then find your song back as simply as looking a word in a dictionary. Hopefully this idea won&#x27;t die any time soon.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Parsons_code reply aidenn0 56 minutes agorootparentThat requires identifying the melody, which is certainly not something all humans can do, and was probably not generally doable by a machine in 1975. It also throws away a huge amount of information, and requires starting from the beginning of the melody. reply aidenn0 44 minutes agoparentprev> This was the smart approach when Shazam launched in 2008.Nitpick, but Shazam launched in 2002 as a dial-in service that replied with a text-message of the result. The first phone app was for BREW in 2006.The 2008 date is just when Apple launched the app store; it was not possible for a third party to make an iPhone app before 2008. reply DeathArrow 24 minutes agoparentprevWell, under the hood a neural net kind of also builds hashes, just less accurate. reply AntonioCao 9 hours agoparentprevtbh for tools like Shazam there&#x27;s no fundamental difference between a database + hashing algorithm and a self-supervised model; both are great indexing & compression solutions, just for different scales of data. reply simonw 10 hours agoparentprevIf you trained a model for this, how would you avoid having to run the entire training process again every time you needed to add another song?I wonder if there&#x27;s a way to build an embeddings model for this kind of thing, such that you can calculate an embedding vector for each new song without needing to fully retrain. reply jscob 10 hours agorootparentYou&#x27;d just have the network generate fingerprints for any given song similar to how facial recogniton is doneSiamese networks are what you want, two identical pairs of layers (one cached in this case) which act as the fingerprints then then the final layers are doing the similarity matching reply koolba 9 hours agorootparentprevIf the model is successful, it will be able to predict the artist and song title for music it’s never heard. reply zer00eyz 8 hours agorootparentNo,People who are highly skilled at this, can be easily stumped. Sure it might workfor artist who are more focused (tailor swift), it might pick out some interesting guest appearances (Eddie Van Halen on Beat It) but when you get multi talented performers who change everything about what do, they don&#x27;t fit a \"model\". The most current example would be Andre3000&#x27;s latest release. reply anon84873628 2 hours agorootparentUm, yeah, you won&#x27;t be able to model artists who don&#x27;t follow a model (especially done so deliberately). As you say that is true of humans or computers alike. But it&#x27;s not the problem anyone cares about and not what the parent comment intended.Certainly a well trained model will be able to have incredible accuracy just with vocals alone. It will be able to identify Lady Gaga regardless of whether she is singing a new art pop track or old standard with Tony Bennett. reply zer00eyz 2 hours agorootparentTop 40, pop != musicWe could have a debate about the consistency of Gaga or Taylor Swift and profit a motive (and we could go all the way back to composers of the classical period with this).What about all the people who back pop artists? I dont think picking out Wrecking Crew is gonna be possible (It might but harder) https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Wrecking_Crew_(music)I could also point you to Diplo who, as a \"producer\" is responsible for diverse sounds with his name directly on them and then side projects Like Major Lazer or MIA&#x27;s paper planes that have his hallmarks but aren&#x27;t \"musicaly\" linked. How about the collected work of Richard D. James, I&#x27;m no so sure that all the parts fit together outside the whole of them.Stuart Copland was the drummer for the police, a very distinct and POP sound. Are we going to be able to use ML to take those works and correlate them to his Film scores? How about his opera? Dave Grohl, Phil Colins, Sheila E, more drummers who became singers, what is the context for ML finding those connections (or people).John Cages 4&#x27;33 is gonna be an interesting dilemma.DO you think the player piano black hole sun, and C.R.E.A.M cover from Westworld are picked up as stylized choices by Ramin Djawadi, and would it link those to the sound track of Game of Thrones?Even with all the details it&#x27;s sometimes hard to believe what talented people can do and how diverse their output can be! reply ravetcofx 4 hours agorootparentprevGiven our current trajectory this will probably be possible in 10 to 15 years reply bohadi 8 hours agorootparentprevthis is a rather funny jokebut if it is not that would be extremely impressive! determinism&#x2F;freewill reduces to shazam!?whats the training data to predict new song titles? hehcheck out this reply from claude2:>predict the next 3 new song titles from artist Taylor Swift1. Last Dance with You - A reflective ballad about finding closure after a breakup. 2. Never Getting Back Together - A pop tune emphasizing that the same mistakes won&#x27;t be made twice in a relationship. 3. 22 Was My Prime - A lighthearted look back on her early 20s as carefree years that can&#x27;t be replicated.... reply TerrifiedMouse 8 hours agorootparentprevShazam is used to identify the artist and name of songs. I don’t want it guessing when precise information is available. reply FirmwareBurner 10 hours agoparentprev>This was the smart approach when Shazam launched in 2008.A noteworthy mention would be that Sony&#x27;s TrackID did most likely the same thing on their feature phones a few years before Shazam. reply bartonsurfer1 9 hours agorootparentHi - This is Chris Barton (founder of Shazam). Sony&#x27;s TrackID was built by licensing (and later buying) a technology invented by Philips. That tech was invented after Shazam. Shazam was the first to create an algorithm that identifies recorded music with background noise in a highly scaled fashion. My co-founder, Avery Wang, invented the algorithm in 2000. Chris (www.chrisjbarton.com) reply loxias 6 hours agorootparentCough Cough, Geoff Schmidt, Matthew Belmonte, Tuneprint?Edit: tho for sure, the Philips algorithm was better than either of ours. reply aidenn0 48 minutes agorootparentTuneprint was published in 2004, no? Shazam filed their patents in 2000 and launched in 2002. reply dylan604 10 hours agoparentprevI could totally see RIAA shutting it down or ASCAP issuing a licensing invoice to stop a train on its tracks just because that&#x27;s how they think reply a2dam 5 hours agorootparentWhat makes you say this? They existed and there were a ton of similar products as well. None of them got any grief from those organizations. reply dylan604 5 hours agorootparentnone of the companies making the previous hashing apps had the money that AI companies do. around here, the phrase \"fish in a barrel\" might be used to describe the situation. reply mattl 6 hours agoparentprev2008? I remember using it in the UK in 2000&#x2F;2001? reply aidenn0 44 minutes agorootparentPretty sure it launched in 2002 for 60p per song. reply wruza 2 hours agorootparentprevWas it iphone or android? reply tomduncalf 2 hours agorootparentIt was a phone number you called, which then SMSed you the result reply bloqs 2 hours agorootparentprevIt was originally sms service reply madrox 12 hours agoprevShazam is one of those rare products that hasn&#x27;t stopped feeling magical in two decades. They&#x27;re really the thing technologists aught to aspire to. reply TillE 10 hours agoparentFor the technically knowledgeable, music fingerprinting is a concrete problem which is understandable but pretty difficult if you get into the details without looking at how other people have already solved it.It fits into that space of an unusual but comprehensible problem, unlike superficially similar features like recognizing animals or objects in images, which is mostly weird ML magic. reply loxias 6 hours agorootparentIt was really hard to do the first time. :) I&#x27;m honored to have been part of the first team to do any viable acoustic music recognition, in 2001 (much earlier than Shazam, a point of pride of course[0]).You&#x27;re dead on that it&#x27;s pretty difficult if you don&#x27;t benefit from others, we did a ton of work that in retrospect wasn&#x27;t necessary. I liked the advanced psychoacoustic model, faithfully implemented in high performant C direct from Zwicker. (Psychoacoustics). To a first approximation, about 10&#x2F;s model -> pca -> top 16 dim -> VQ and the resulting bytes contain more than 50% of the entropy (!!) Shove all of those in a home grown what-you-now-call-a vector DB, do dozens of range queries, and search for any song common to multiple results. Boom, music recognition. Understandable in retrospect but things like that aren&#x27;t Everest they&#x27;re like... multiple unclimbed mountains.0. And far too early to have any applications. Company existed 2000-2001 \\o&#x2F; reply garrettgrimsley 6 hours agorootparentLooks like others, including Shazam, beat you to the punch in 2000:https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US7853664B1&#x2F;https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US6941275Very interesting hearing about all of the differing approaches people have taken to solving this problem! Do you have further writings on this topic? reply hunter2_ 8 hours agorootparentprevI would say animals in images is more akin to matching two different musical performances of the same song (where one of the two can even be the user humming), which Shazam doesn&#x27;t offer but some systems like Google Assistant do!Rather, matching two recordings of the exact same performance (one ingested by Shazam at training time and one ingested by Shazam at run time) is more akin to identifying individuals (facial recognition) than identifying species. reply wruza 2 hours agoparentprevAt the same time it turned from “tap to listen, here you go” to a sluggish af and ads-infested bloatware. I remember when I stopped using it and deleted the app because my prev-gen iphone couldn’t load it in time anyway. reply jedrek 1 hour agorootparentRight now it&#x27;s integrated with siri so you just hold down your power button and say, \"what is this song?\" and it tells you. reply gniv 1 hour agorootparentprevThese days you don&#x27;t need the app. You can put it in the control center and just push the button to listen. reply wodenokoto 45 minutes agorootparentprevWhat are you on about? There’s no ads in Shazam. You just open the app and it starts searching. No need for even tapping a button. reply wruza 11 minutes agorootparentOn about a decade ago. reply bobbylarrybobby 11 hours agoparentprevIf anything it&#x27;s gotten even more magical. I was blown away when I tried to find the song someone was singing on America’s Got Talent and the result it returned was the singer on AGT (they index tv shows!?). reply pavlov 10 hours agorootparent> “…they index tv shows!?”It makes more sense if you think of a production like AGT less as the reality show it pretends to be, and more as a promotional reel for labels.Of course the content they choose to promote is indexed. reply dbtc 9 hours agorootparentAmazing how much the name we use can change the thing we see. reply quickthrower2 1 hour agorootparentYou could also call it a music factory, or indeed, spec work. reply al_borland 11 hours agorootparentprevI haven’t seen a commercial in quite a long time, but for a while many years ago, Shazam was being used like an audio QR code. Commercials would tell you to use Shazam on their ad to get a deal or something. My guess is after Apple bought Shazam they stopped needing to do stuff like that to monetize. reply cool_scatter 9 hours agorootparentOh wow, I forgot all about that. That was a strange trend. reply gumballindie 8 hours agoparentprevOh technologists do. But what will product managers do if not constantly break the product to get a bonus and an extra holiday. reply bartonsurfer1 9 hours agoprevHere is a beautifully produced video by Wall Street Journal that explains Shazam:https:&#x2F;&#x2F;www.wsj.com&#x2F;video&#x2F;series&#x2F;in-depth-features&#x2F;how-shaza...Chris (Shazam co-founder) reply forgotpwd16 18 minutes agoprevWas in a live event few days ago and was wondering whether an attempt to recognizing live&#x2F;noisy songs has been made. Shazam has failed me anytime I tried it for this. reply svilen_dobrev 29 minutes agoprevanyone has an idea does shazam cope with time-axis not being linear&#x2F;const?Think tapes, wow-and-flutter, speed-up-then-down-all-the-time..AFAIK fingerprinting is highly time-sensitive (unless cut into ~50ms pieces.. and still not quite).Last time i looked, the general technique for that - Dynamic Time Warping ? - was prohibitely compute-expensive.. reply ruuda 12 hours agoprevThere is also Chromaprint [1], which works slightly differently. It’s based on pitch change patterns instead of maxima in the spectrum. Chromaprint is used by AcoustID, which is a large open database that links audio fingerprints to MusicBrainz recordings. I find it astonishing how much music is in there despite having not nearly as much commercial backing as Shazam.[1]: https:&#x2F;&#x2F;oxygene.sk&#x2F;2011&#x2F;01&#x2F;how-does-chromaprint-work&#x2F; reply willseth 9 hours agoparentDoesn&#x27;t Chromaprint have to compare the whole song? This is great for detecting duplicates, but Shazam&#x27;s fingerprint design allows it to match a short snippet to the complete song. reply ks2048 1 hour agorootparentChromaprint computes “features” roughly 8 times per second. You can do a brute-force search checking different times in a song or potentially do some more fancy indexing once you have the features. (I did some experiments with Chromaprint - described here, https:&#x2F;&#x2F;kenschutte.com&#x2F;phingerprint&#x2F;) reply ruuda 3 hours agorootparentprevI think it&#x27;s possible to match on a subset in principe, but I don&#x27;t know if&#x2F;how this is implemented. reply vkaku 12 hours agoprevThank you for sharing.This is a great post that captures what a spectrogram does, and a must read for people who want to understand how audio fingerprinting works.There are similar approximate algorithms available for other media as well, so anyone who wishes to understand real world hashing should take their time to study this article. reply innagadadavida 12 hours agoparentThe normal spectrogram technique was already invented by Phillips prior to Shazam. What Shazam did was to hash things combinatorial to reduce false positives. reply logbiscuitswave 1 hour agoprevThis was a fascinating read, not only for understanding about how Shazam works which is something I’ve long been curious about, but also a great primer on digital signal processing. reply jimmySixDOF 11 hours agoprevFor what it&#x27;s worth there is a phenomenal site that applies algorithmic matching not to songs but to genera classification and the branching sub generas that new song signatures introduce. An amazing resource run as a solo sidehussle and looks like it is at risk of getting clipped due to hosting issues or something. There was Music DNA from Pandora and something similar on LastFM back a long time ago but this site is like the visual connectome of all human music produced through to 2023 and would be a loss for the World Wide Web if it stops.....Every Noise At Once https:&#x2F;&#x2F;everynoise.com reply dang 10 hours agoparentRelated:Every Noise at Once - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26668426 - April 2021 (94 comments)Every Noise at Once - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20585447 - Aug 2019 (82 comments)Every Noise at Once – an algorithmically-generated scatter-plot of musical genre - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=10269685 - Sept 2015 (23 comments)An algorithmically-generated scatter-plot of musical genres - with samples - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9315499 - April 2015 (3 comments) reply irrational 11 hours agoparentprevOkay, that is super impressive. Especially what it does when you search for an artist. reply bobsmooth 11 hours agoparentprevOh damn, looks the creator was part of Spotify&#x27;s recent layoffs. He was a genre researcher while he was there. reply simonklitj 10 hours agorootparentWhat a loss for Spotify. reply km3r 9 hours agorootparentprevThe guy who helped create Wrapped too... sad. reply collegeburner 10 hours agoparentprevrelated, Maroofy: https:&#x2F;&#x2F;maroofy.com&#x2F;shows you similar songs and imo does a pretty good job of it reply omneity 3 hours agorootparentI use Maroofy quite a lot and it was funny to discover many ripoffs and plagiarism between songs by complete chance. reply crazygringo 10 hours agoprevI just want to say, it&#x27;s remarkable how intuitive this is, and just how well it matches our own recognition process.It&#x27;s more-or-less identifying melody fragments*, and then just trying to match those up in a sequence. The same way we&#x27;ll recognize something after 5 or 7 or 10 notes.I&#x27;m pretty sure I&#x27;ve read about other methods for song fingerprinting that rely on things like loudness peaks, where it might work equally well, but that doesn&#x27;t match how our own brains do it at all. It&#x27;s pretty cool that this isn&#x27;t relying on \"artifacts\" but basically works the same way we do.* Technically not always melody, but probably is most of the time reply Jean-Papoulos 2 hours agoprevWell well well, if it isn&#x27;t our old friend the Fourier transform. reply dang 12 hours agoprevRelated. Others?How Shazam Works (2003 Paper) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33299853 - Oct 2022 (1 comment)Creating Shazam in Java (2010) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32530056 - Aug 2022 (36 comments)Shazam turns 20 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32520593 - Aug 2022 (227 comments)How Shazam Works (2015) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23806142 - July 2020 (7 comments)Designing an audio adblocker - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18855029 - Jan 2019 (186 comments)A radio&#x2F;podcast adblocker featuring ML and Shazam-like fingerprinting - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18459058 - Nov 2018 (2 comments)Shazam-like acoustic fingerprinting of continuous audio streams - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15809291 - Nov 2017 (76 comments)How Shazam Works (2015) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15350729 - Sept 2017 (13 comments)Shazam picks up song from my kitchen light - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=11593305 - April 2016 (2 comments)How Shazam works - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9870408 - July 2015 (48 comments)Patent infringement claim re: “Creating Shazam in Java” blogpost (2010) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9594480 - May 2015 (18 comments)The Shazam Effect (2014) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9593429 - May 2015 (37 comments)The Shazam Effect - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8634357 - Nov 2014 (34 comments)Is there an audio search technology that finds exact and similar audio? - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8420141 - Oct 2014 (3 comments)Source code example of the Shazam algorithm - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5724442 - May 2013 (16 comments)Creating Shazam in Java - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5723863 - May 2013 (43 comments)An Industrial-Strength Audio Search Algorithm (Shazam) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2621103 - June 2011 (4 comments)Shazam&#x27;s Search for Songs Creates New Music Jobs - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2215295 - Feb 2011 (1 comment)How does the music-identifying app Shazam work its magic? - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2214992 - Feb 2011 (2 comments)Implementing Shazam with Java in a weekend - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=1702975 - Sept 2010 (23 comments)Shazam: not magic after all - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=909263 - Oct 2009 (28 comments)How does the music-identifying app Shazam work its magic? - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=893353 - Oct 2009 (16 comments) reply adhi01 7 hours agoprevTalk by the author in EuroPython 2016 on the same topic: https:&#x2F;&#x2F;youtu.be&#x2F;LZ7THTB88AE?si=BQPgp-rxg32bTPMK reply 6stringmerc 10 hours agoprevIt&#x27;s the reverse approach to similar engineering of what the Pop industry attempts to do to make genre based hits. reply joshuahaglund 6 hours agoprevI&#x27;m curious to hear a constellation map played back. I imagine it&#x27;d be a bizarre robotic midi sorta representation of the original reply drewmol 8 hours agoprevI&#x27;d sure like a Shazam for ads with mute capability, something I&#x27;ve been thinking about building for some time now just don&#x27;t see any road to profitability and not getting adblock blocked. reply foobiekr 6 hours agoprevThe algorithm is interesting - at one time I was going to use https:&#x2F;&#x2F;theory.stanford.edu&#x2F;~aiken&#x2F;publications&#x2F;papers&#x2F;sigmo... to do a startup.This is different but very similar and contemporaneous. reply pbj1968 8 hours agoprevOne of my greatest free joys in life is subjecting Shazam to the unpopular music I like and watching it be unable to identify it.(Screaming electro industrial for those that are curious) reply underlipton 7 hours agoparentMy hit rate for those 15-to-30-second snippets they play at the top of the hour on NPR is something like 10%. It&#x27;s been getting worse and worse over the years. reply pbj1968 6 hours agorootparentI used to do the same back when I commuted and when it DOES identify it, it tends to be some weird, weird stuff. reply hoherd 12 hours agoprevIf this interests you, then take a look at sCrAmBlEd?HaCkZ!, a music software project from 2006 that uses similar classifying techniques.https:&#x2F;&#x2F;youtu.be&#x2F;eRlhKaxcKpA reply hideo 12 hours agoprevWhy are there so few Shazam alternatives? Does it have something to do with licensing perhaps? The algorithm itself is fascinating but I don&#x27;t get why this space seems to have just one player - i.e. Shazam reply forgotpwd16 7 minutes agoparentBesides potential licese issues (that may not exist), legally creating the hashes database is a big effort as access to an near-all-encompassing song library is required. reply tialaramex 11 hours agoparentprevWhere&#x27;s the value? My Android phone just does this locally, obviously Shazam has more storage and so they&#x27;re going to handle more obscure stuff that way, but for example I just set my \"Power of Love\" playlist running, and the Pixel&#x27;s built in \"Now Playing\" knows both the Frankie Goes To Hollywood track and the Huey Lewis number from Back to the Future.When a \"phone\" was a dumb device just barely capable of implementing GSM and displaying a clock then this might be worth something as a business, but given where the $0 baseline is, I don&#x27;t see enough margin to justify competition, I&#x27;m surprised even Shazam still makes commercial sense. reply TerrifiedMouse 8 hours agorootparent> I&#x27;m surprised even Shazam still makes commercial sense.Isn’t Shazam owned by Apple now? It doesn’t need to make “financial sense” if it’s a service Apple runs. reply Racing0461 5 hours agorootparentI disagree. Apple isn&#x27;t like Google&#x2F;FB where they take a loss on a service and make it back on the backend with ads. Apple seems like they evaluate each service as if it were a physical product ie it costs X and each service needs to stand on its own and make X + Y back. reply jdadj 12 hours agoparentprevSoundHound is a credible alternative. reply klipt 11 hours agorootparentSoundHound it&#x27;s actually much more impressive because it can recognize hummed or whistled songs too! reply esafak 11 hours agorootparentGoogle search in Android can do that too. Albeit not very well. reply 2c2c2c 11 hours agorootparentpixel series since 6 has an option to passively listen and document songs by default. probably the only feature I miss moving to iphone reply jiberius 10 hours agorootparentIt dates all the way back to the Pixel 2 from 2017, still feels like a magical feature though.I think the historical log of songs that the phone heard launched a little later, on the Pixel 3. replypxeger1 11 hours agoparentprevIt definitely has more than one player. Google Assistant has had this ability for a while, for example. But Shazam has the advantage of being built in to iOS, which might be why you think it’s the only player reply al_borland 10 hours agoparentprevI was a SoundHound user for a long time. It came out around the same time as Shazam. Shazam had all the brand recognition, but I typically went for the underdog.Recently, in an effort to simply things, I moved over to Shazam. It’s owned by Apple now, so it’s already built into the iPhone, even without the app. The app allows for saving things a bit easier and I find it to be a lot cleaner than the SoundHound app. reply jldugger 9 hours agoparentprevMy understanding is that there is a patent holder https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shazam_(application)#Patent_in... in the US. reply willseth 9 hours agoparentprevThey have a patent, which IIRC should be expiring fairly soon (the original patent anyway) reply Thaxll 9 hours agoparentprevI remember a while back 10 years? On HN someone did a poc ( code ) of Shazamn like and he was sued or asked to remove everything. reply financypants 7 hours agoprevI’m really interested in an implementation of this that adjusts for something like pitch. So it could tell when someone is mimicing a speaker, and could even grade them on how well they did. Or even a celebrity voice matcher that tells you which celebrity you sound most like. Does this exist? reply maxehmookau 47 minutes agoprevThere used to be an open-source version of this called Echoprint which was shuttered after the company was bought by Spotify in 2012.Much the same technologies though and an interesting research project: https:&#x2F;&#x2F;www.echoprint.me&#x2F; reply totetsu 10 hours agoprevI remember I heard about this technique being developed by a guy from my NZ university years before there was ever a commercial product called Shazam. Maybe I eve heard and interview with him on the radio. But I’ve never heard of him again.. reply aftbit 10 hours agoprevThank you for releasing this as open source under MIT license. I wonder how well it works on human speech. I have a few thousand hours of recorded word content that I need to deduplicate. If I get around to it, I&#x27;ll report back. reply reqo 10 hours agoprevShazam is perhaps the most profitable music informatics technology of all time, yet it knows nothing about music at all! It’s basically just a fast hashing algorithm! reply Exoristos 3 hours agoprevNot very well. reply snoopsnopp 10 hours agoprevI don’t want to be combative, but this has simply never worked for me. No matter what I do Shazam has produced incorrect results. I wonder if I’m the only one. reply crazygringo 10 hours agoparentThat is very strange. Either you&#x27;ve got some kind of network bug or problem with your microphone, or you&#x27;re looking up music so obscure it&#x27;s not on Shazam. Or the music is just way too quiet, especially if there&#x27;s a lot of other noise. reply yamazakiwi 10 hours agoparentprevNot necessarily all of your issue I&#x27;m sure but genre plays a large part in discovery. I find it has a hard time with less popular electronic and soundcloud type beats for instance. reply underlipton 7 hours agoparentprevIt&#x27;s not just you. It&#x27;s super hit-or-miss for me. Do you have an Android phone? I&#x27;ve read complaints that Shazam&#x27;s accuracy dropped for them after Apple bought it. reply xcv123 9 hours agoparentprevCould be poor sound quality from your phone microphone reply aftbit 10 hours agoprevThis is a platform feature of Google assistant, I believe. I have a \"search a song\" on my Pixel, but not in airplane mode, so probably being done at the mothership. reply lstodd 9 hours agoprevEh, the memories.Once upon a time, back in Russia we had a service that captured most of FM radio stations and detected what was playing real-time as you listened from Moscow to some obscure station 2000 km away. Sadly it was all trampled by copyright idiots by about 2013.But the technology, the capture boxes which were SDRs before SDRs were a thing, hashes, station bosses calling in the night because DJs got too drunk and went rogue live.. oh the memories.. reply JohnMakin 8 hours agoprevsinbad comes out of a lamp and grants you 3 wishes reply seydor 11 hours agoprev [–] Shazam isn&#x27;t great, so many tracks it can&#x27;t identify or comes up with random electro-trance tracks. online services like a-ha are better replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article provides an explanation of how Shazam identifies songs using its algorithm.",
      "It discusses the process of creating a fingerprint of the audio sample and matching it with hashes in a database.",
      "The article also mentions the introduction of a new implementation called \"abracadabra\" and highlights challenges in song recognition."
    ],
    "commentSummary": [
      "The discussion explores different aspects of Shazam, such as its technology, limitations, competition, and user experiences.",
      "Topics include the use of hashes for song identification, the potential use of neural networks, and the challenges of identifying unique artists.",
      "Users also discuss Shazam's history, recognition technology, alternative fingerprinting techniques, the impact of shows like \"America's Got Talent\" on Shazam, and the lack of alternative options."
    ],
    "points": 418,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1701787479
  },
  {
    "id": 38530880,
    "title": "AI-Powered Mass Spying: An Erosion of Privacy",
    "originLink": "https://www.schneier.com/blog/archives/2023/12/the-internet-enabled-mass-surveillance-ai-will-enable-mass-spying.html",
    "originBody": "AI and Mass Spying Spying and surveillance are different but related things. If I hired a private detective to spy on you, that detective could hide a bug in your home or car, tap your phone, and listen to what you said. At the end, I would get a report of all the conversations you had and the contents of those conversations. If I hired that same private detective to put you under surveillance, I would get a different report: where you went, whom you talked to, what you purchased, what you did. Before the internet, putting someone under surveillance was expensive and time-consuming. You had to manually follow someone around, noting where they went, whom they talked to, what they purchased, what they did, and what they read. That world is forever gone. Our phones track our locations. Credit cards track our purchases. Apps track whom we talk to, and e-readers know what we read. Computers collect data about what we’re doing on them, and as both storage and processing have become cheaper, that data is increasingly saved and used. What was manual and individual has become bulk and mass. Surveillance has become the business model of the internet, and there’s no reasonable way for us to opt out of it. Spying is another matter. It has long been possible to tap someone’s phone or put a bug in their home and/or car, but those things still require someone to listen to and make sense of the conversations. Yes, spyware companies like NSO Group help the government hack into people’s phones, but someone still has to sort through all the conversations. And governments like China could censor social media posts based on particular words or phrases, but that was coarse and easy to bypass. Spying is limited by the need for human labor. AI is about to change that. Summarization is something a modern generative AI system does well. Give it an hourlong meeting, and it will return a one-page summary of what was said. Ask it to search through millions of conversations and organize them by topic, and it’ll do that. Want to know who is talking about what? It’ll tell you. The technologies aren’t perfect; some of them are pretty primitive. They miss things that are important. They get other things wrong. But so do humans. And, unlike humans, AI tools can be replicated by the millions and are improving at astonishing rates. They’ll get better next year, and even better the year after that. We are about to enter the era of mass spying. Mass surveillance fundamentally changed the nature of surveillance. Because all the data is saved, mass surveillance allows people to conduct surveillance backward in time, and without even knowing whom specifically you want to target. Tell me where this person was last year. List all the red sedans that drove down this road in the past month. List all of the people who purchased all the ingredients for a pressure cooker bomb in the past year. Find me all the pairs of phones that were moving toward each other, turned themselves off, then turned themselves on again an hour later while moving away from each other (a sign of a secret meeting). Similarly, mass spying will change the nature of spying. All the data will be saved. It will all be searchable, and understandable, in bulk. Tell me who has talked about a particular topic in the past month, and how discussions about that topic have evolved. Person A did something; check if someone told them to do it. Find everyone who is plotting a crime, or spreading a rumor, or planning to attend a political protest. There’s so much more. To uncover an organizational structure, look for someone who gives similar instructions to a group of people, then all the people they have relayed those instructions to. To find people’s confidants, look at whom they tell secrets to. You can track friendships and alliances as they form and break, in minute detail. In short, you can know everything about what everybody is talking about. This spying is not limited to conversations on our phones or computers. Just as cameras everywhere fueled mass surveillance, microphones everywhere will fuel mass spying. Siri and Alexa and “Hey Google” are already always listening; the conversations just aren’t being saved yet. Knowing that they are under constant surveillance changes how people behave. They conform. They self-censor, with the chilling effects that brings. Surveillance facilitates social control, and spying will only make this worse. Governments around the world already use mass surveillance; they will engage in mass spying as well. Corporations will spy on people. Mass surveillance ushered in the era of personalized advertisements; mass spying will supercharge that industry. Information about what people are talking about, their moods, their secrets—it’s all catnip for marketers looking for an edge. The tech monopolies that are currently keeping us all under constant surveillance won’t be able to resist collecting and using all of that data. In the early days of Gmail, Google talked about using people’s Gmail content to serve them personalized ads. The company stopped doing it, almost certainly because the keyword data it collected was so poor—and therefore not useful for marketing purposes. That will soon change. Maybe Google won’t be the first to spy on its users’ conversations, but once others start, they won’t be able to resist. Their true customers—their advertisers—will demand it. We could limit this capability. We could prohibit mass spying. We could pass strong data-privacy rules. But we haven’t done anything to limit mass surveillance. Why would spying be any different? This essay originally appeared in Slate. Tags: artificial intelligence, espionage, privacy, surveillance Posted on December 5, 2023 at 7:10 AM • 19 Comments Two clicks for more privacy: The Facebook Like button will be enabled once you click here. No data is loaded from Facebook until you enable the button. Click the [i] button for more information. not connected to Facebook Two clicks for more privacy: The Tweet button will be enabled once you click here. No data is loaded from Twitter until you enable the button. Click the [i] button for more information. not connected to Twitter If you click to activate the share buttons, data will be loaded from a third party, allowing them to track your visit to schneier.com. For more details click the [i] button.",
    "commentLink": "https://news.ycombinator.com/item?id=38530880",
    "commentBody": "AI and Mass SpyingHacker NewspastloginAI and Mass Spying (schneier.com) 375 points by hendler 19 hours ago| hidepastfavorite304 comments somenameforme 18 hours agoThis is a political problem, not a technological one. The USSR (alongside Germany and others) managed effective at scale spying with primitive technology: paperwork for everything and every movement, informants, audio surveillance, and so on. The reason such things did not come to places like the US in the same way is not because we were incapable of such, but because there was no political interest in it.And when one looks back at the past we&#x27;ve banned things people would never have imagined bannable. Make it a crime to grow a plant in the privacy of your own home and then consume that plant? Sure, why not? Make it a crime for a business to have the wrong opinion when it comes to who they want to serve or hire? Sure, why not?Going the nuclear route and making the collection of data on individuals, aggregated or otherwise, illegal would hardly be some major leap of reach of jurisprudence. The problem is not that the technology exists, but that there is 0 political interest in curtailing it, and we&#x27;ve a &#x27;democracy&#x27; where the will of the people matters very little in terms of what legislation gets passed. reply petsfed 14 hours agoparentYou and I are in agreement that the surveillance needs to stop, but I think we differ on how to explain the problem. My explanation follows, but note that its not directed at you.At its peak, the KGB employed ~500,000 people directly, with untold more employed as informants.The FBI currently employs ~35,000 people. What if I told you that the FBI could reach the KGB&#x27;s peak level of reach, without meaningfully increasing its headcount? Would that make a difference?The technology takes away the cost of the surveillance, which used to be the guardrail. That fundamentally changes the \"political\" calculus.The fact that computers in 1945 were prohibitively expensive and required industrial logistics has literally zero bearing on the fact that today most of us have several on our person at all times. Nobody denies that changes to computer manufacturing technologies fundamentally changed the role the computer has in our daily lives. Certainly, it was theoretically possible to put a computer in every household in 1945, but we lacked the \"political\" will to do so. It does not follow that because historically computers were not a thing in society, we should not adjust our habits, morals, policies, etc today to account for the new landscape.So why is there always somebody saying \"it was always technically possible to [insert dystopian nightmare], and we didn&#x27;t need special considerations then, so we don&#x27;t need them now!\"? reply edouard-harris 13 hours agorootparentThis is the correct take. As the cost to do a bad thing decreases, the amount of political will society needs to exert to do that bad thing decreases as well.In fact, if that cost gets low enough, eventually society needs to start exerting political will just to avoid doing the bad thing. And this does look to be where we&#x27;re headed with at least some of the knock-on effects of AI. (Though many of the knock-on effects of AI will be wildly positive.) reply arka2147483647 13 hours agorootparentprev> The FBI currently employs ~35,000 people. What if I told you that the FBI could reach the KGB&#x27;s peak level of reach,You are, if anything, underselling the point. AI will allow a future where every person will have their very own agent following them.Or even worse, as there are multiple private addtech companies doing surveillance, and domestic and foreign intelligence agencies, so you might have a dozen AI agents on your personal case. reply chiefalchemist 11 hours agorootparentAren&#x27;t we there already? Sure, perhaps the fidelity is a bit grainy, but that&#x27;s not going to remain as such for long. With the amount of data (for purchase) on the free market, the FBI, KGB, MoSS (China), etc. all have a solid starting foundation, and then they simply add their own layer upon layer on top.I read \"The Age of Surveillance Capitalism\" a couple of years ago and she was frighteningly spot on.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Age_of_Surveillance_Capita... reply aidenn0 14 hours agorootparentprevCost is one factor, but so is visibility. If we replaced humans following people around with cheap human-sized robots following people around, it would still be noticeable if everybody had a robot following them around.Instead we track people passively, often with privately owned personal devices (cell phones, ring doorbells) so the tracking ability has become pervasive without any of the overt signs of a police state. reply dfxm12 13 hours agorootparentprevI think if you bring up a dystopian nightmare, it assumes someone in power acting in bad faith. If their power is great enough, like maybe a government intelligence agency, it doesn&#x27;t need things like due process, etc., to do what it wants to do. For example, Joe McCarthy & J. Edgar Hoover didn&#x27;t need the evidence that could have been produced by AI-aided mass surveillance to justify getting people people who opposed his political agenda blackballed from Hollywood, jailed, fired from their jobs, etc.If everyone involved is acting in good faith, at least ostensibly, there are checks and balances, like due process. It&#x27;s a fine line and doesn&#x27;t justify the existence of mass spying, but I think it is an important distinction in this discussion & I think is a valuable lesson for us. We have to push back when the FBI pushes forward. I don&#x27;t have much faith after what happened to Snowden and the reaction to his whistleblowing though. reply pempem 13 hours agorootparentI think it is greyer than this.Joe McCarthy and J. Edgar Hoover, distasteful as they are, I believe acted in what they would claim is good faith. The issue isn&#x27;t that someone is a bad actor. It is that they believe they are a good actor and are busy stripping away others&#x27; rights in their pursuit. reply JohnFen 12 hours agorootparentVery nearly everybody -- even bad people -- consider themselves one of the \"good guys\". reply dfxm12 11 hours agorootparentprevMaybe in their minds they thought the end justified the means, but there&#x27;s no way anyone, even themselves, mistakes those means for good faith. reply from-nibly 12 hours agorootparentprevThat&#x27;s what this article argues though. Even with good faith acting this would be a disaster. Imagine any time you did something against the law you got fined. The second you drive your unregistered car off your driveway (presumably to re register) you are immediately fined. There may be \"due process\" cause you DID break the law, but there is no contextual thought behind the massive ticket machine.Our laws are not built to have the level of enforcement that AI could achieve. reply dfxm12 12 hours agorootparentInterestingly enough, in some places, automation like that, like red light cameras, even after getting installed, were later prohibited. NJ has discussed laws around protecting NJ drivers&#x27; privacy from other states&#x27; red light cameras, too. It&#x27;s important not to be complacent. You can imagine literally anything, but action is required to actually change things. reply JohnFen 12 hours agorootparentprev> I think if you bring up a dystopian nightmare, it assumes someone in power acting in bad faith.I don&#x27;t agree with this. I think it&#x27;s entirely possible for a dystopian nightmare to happen without anyone acting in bad faith at all.\"The road to hell is paved with good intentions\" is a common phrase for a reason. reply sigilis 11 hours agorootparentIt may be a common phrase, but I’ve never seen such a road myself. Mostly bad outcomes are preceded by bad intentions, or lazy ones, or selfish ones.I’d be interested in a couple of examples, if anyone has good ones, but I’m pretty sure that if we put stuff like 737MAX MCAS, the Texas power grid fiasco, etc the count of badly paved roads would be greater. reply r00fus 10 hours agorootparentprevThe other point is when technologies essentially make redundant an entire class of people then those people are more easily repressed.In this case, it could be the entire US populace that is not part of the surveillance engine. reply matthewdgreen 17 hours agoparentprevIt is not as simple as being a political problem. Many of the policy decisions we think of as being political were actually motivated by the cost&#x2F;availability of technology. As this cost goes down, new options become practical. We think of the Stasi&#x27;s capabilities as being remarkable: but in fact, they would probably have been thrilled to trade most of their manual spying tools for something as powerful as a modern geofence warrant, a thing that is routinely used by US law enforcement (with very little policy debate.) reply asdff 14 hours agorootparentWhile this is true, I don&#x27;t think we are there yet with AI since its usually more expensive to run AI models than it is to perform more traditional statistical modelling. reply matthewdgreen 14 hours agorootparentA text recognition and face&#x2F;object recognition model runs on my iPhone every night. A small LLM is used for autocorrect. Current high-end smartphones have more than enough RAM and compute to run pretty sophiscated 7B-param quantized LLMs. This level of capability will find its way down to even entry-level Walmart phones in about five years. Server side, things are going to be even cheaper. reply xcv123 11 hours agorootparentprev> usually more expensive to run AI modelsMachine learning inferencing on phones is cheap these dayshttps:&#x2F;&#x2F;apple.fandom.com&#x2F;wiki&#x2F;Neural_Engine reply digging 15 hours agoparentprev> Make it a crime to grow a plant in the privacy of your own home and then consume that plant? Sure, why not? Make it a crime for a business to have the wrong opinion when it comes to who they want to serve or hire? Sure, why not?Wow, that&#x27;s a hell of a comparison. The former case being a documented case of basic racism and political repression, assuming you&#x27;re talking about cannabis. And the latter being designed for almost exactly the opposite.Restricting, um, \"wrong opinions\" on who a business wants to serve is there so that people with, um, \"wrong identities\" are still able to participate in society and not get shut out by businesses exercising their choices. Of course \"wrong opinions\" is not legal terminology. It&#x27;s not even illegal to have an opinion that discrimination against certain groups is okay - it&#x27;s just illegal to act on that. Offering services to the public requires that you offer them to all facets of the public, by our laws. But if you say believing in discrimination is a \"wrong opinion\"... I won&#x27;t argue, they&#x27;re your words :) reply JohnMakin 14 hours agorootparentDon&#x27;t know why you&#x27;re getting downvoted for this, it&#x27;s exactly what the parent said, and a completely wild (and off topic) statement. reply janalsncm 13 hours agorootparentI think the most charitable interpretation of their point is something along the lines of simply highlighting the far-reaching and ad-hoc nature of lawmaking capabilities. I don’t think antiracism laws were the best example though. reply gosub100 12 hours agorootparentprevplenty of white people were charged with growing cannabis. I don&#x27;t know where you are getting that idea from. reply JohnFen 12 hours agorootparentIn the US, prohibition of marijuana was enacted for overtly racist reasons. Latinos were the concern. reply gosub100 11 hours agorootparentthen at some point was expanded to whites. reply JohnFen 11 hours agorootparentIt always applied to whites and everyone else, of course. But back then, whites were not huge users of it. reply digging 11 hours agorootparentprevI didn&#x27;t say white people weren&#x27;t included in political repression. The Nixon administration explicitly targeted cannabis-using white hippies. reply tech_ken 16 hours agoparentprev> This is a political problem, not a technological one.Somewhat of a distinction without a difference, IMO. Politics (consensus mechanisms, governance structures, etc) are all themselves technologies for coordinating and shaping social activity. The decision on how to implement new (surveillance) tooling is also a technological question, as I think that the use of the tool in part defines what it is. All this to say that changes in the capabilities of specific tools are not the absolute limits of \"technology\", decisions around implementation and usage are also within that scope.> The reason such things did not come to places like the US in the same way is not because we were incapable of such, but because there was no political interest in it.While perhaps not as all-encompassing as what ended up being built in the USSR, the US absolutely implemented a massive surveillance network pointed at its citizenry [0].>...managed effective at scale spying with primitive technologyI do think that this is a particularly good point though. This is a not new trend, development in tooling for communications and signal&#x2F;information processing has led to many developments in state surveillance throughout history. IMO AI should be properly seen as an elaboration or minor paradigm shift in a very long history, rather than wholly new terrain.> Make it a crime for a business to have the wrong opinion when it comes to who they want to serve or hire?Assuming you&#x27;re talking about the Civil Rights Act: the specific crime is not \"having the wrong opinion\", it&#x27;s inhibiting inter-state movement and commerce. Bigotry doesn&#x27;t serve our model of a country where citizens are free to move about within its borders uninhibited and able to support oneself.[0] https:&#x2F;&#x2F;www.brennancenter.org&#x2F;our-work&#x2F;analysis-opinion&#x2F;hist... reply w-m 17 hours agoparentprevSure, everything is ultimately a political problem, but this one is completely driven by technological change. In the USSR (and GDR), it took them a staff of hundreds of thousands of people to write up their reports.Now it would take a single skilled person the better part of an afternoon to, for example, download a HN dump, and have an LLM create reports on the users. You could put in things like political affiliation, laws broken, countries travelled recently, net worth range, education and work history, professional contacts, ... reply salawat 12 hours agorootparentStop posting things like this, you&#x27;re just giving them ideas, and you can&#x27;t take it back once it&#x27;s out there.I assure you, you may find the prodpect abhorrent, but there are people around who&#x27;d consider it a perfectly cromulent Tuesday. reply w-m 12 hours agorootparentI’m not sure who “they” are, but I’m pretty sure they’re already doing that, and don’t need me to get the idea. I think it’s important to talk about what LLMs mean for privacy. Profiling every HN user might be a useful tool to are people more aware of the problems. But I totally get your unease, which is also why I haven’t done that myself.The cat is out of the bag, can’t get it back in by ignoring the fact. reply __jambo 14 hours agorootparentprevGreat idea. reply JohnFen 17 hours agoparentprev> This is a political problem, not a technological one.The political problem is a component of the technological problem. It&#x27;s a seriously bad thing when technologies are developed without taking into account the potential for abuse.People developing new technologies can try to wash their hands of the foreseeable social consequences of their work, but that doesn&#x27;t make their hands clean. reply kubb 17 hours agoparentprevIn the USSR and GDR, not everyone was under constant surveillance. This would require one surveillance worker per person. There was an initial selection process. reply rasz 10 hours agorootparentThats exactly how it worked. In fact there was always more than one pair of eyes on everyone, people were being coerced to snitch on each other. Im old enough to remember a nice man visiting my school asking us to listen carefully what parents talk about around the house and report to teachers any criticism of government or party. That was pre 1989 under Russian occupation of my country. reply mym1990 14 hours agoparentprevA difference in today&#x27;s world is that private companies are amassing data that then gets turned around to the highest bidder. The government may not have had an interest in collecting the data before but now the friction to obtaining it and the insights is basically just money, which is plenty available.Your opinion on bannable offenses is pretty bewildering. There was a point in time when people thought it would be crazy to outlaw slavery, from your post I might think that you would not be in support of what eventually happened to that practice. reply landemva 17 hours agoparentprevIn a democracy we would have a chance to vote on individual issues such as data privacy, or war against whomever. USA is a federal republic. reply t0bia_s 1 hour agorootparentMost people don&#x27;t bother about it. Consumerism makes people more conform and less conflicting. reply dfxm12 14 hours agoparentprevMake it a crime for a business to have the wrong opinion when it comes to who they want to serveFWIW, businesses who refuse to do business with people generally win their legal cases [0], [1], [2], and I&#x27;m not sure if they are ever criminal...0 - https:&#x2F;&#x2F;www.npr.org&#x2F;2023&#x2F;06&#x2F;30&#x2F;1182121291&#x2F;colorado-supreme-c...1 - https:&#x2F;&#x2F;www.nytimes.com&#x2F;2018&#x2F;06&#x2F;04&#x2F;us&#x2F;politics&#x2F;supreme-court...2 - https:&#x2F;&#x2F;www.dailysignal.com&#x2F;2023&#x2F;11&#x2F;06&#x2F;christian-wedding-pho... reply mullingitover 13 hours agorootparentThese are selection bias, businesses who \"refuse to do business with people\" and then suffer the legal ramifications of their discrimination usually have lawyers who wisely tell them not to fight it in court because they&#x27;ll rightfully lose. In these particular cases, it took a couple decades of court-packing to install the right reactionaries to get their token wins. reply dfxm12 13 hours agorootparentIt would have been easy for the parent poster to not be so incredibly vague. I suspect it&#x27;s because they are discussing this point in bad faith, ready to move the goalposts when any evidence to the contrary was brought up like this. reply giantg2 14 hours agoparentprevNo, it&#x27;s not just a political problem intelligence gathering can happen at scale, including of civilians, by adversarial countries or international corporations.\"Going the nuclear route and making the collection of data on individuals, aggregated or otherwise, illegal would hardly be some major leap of reach of jurisprudence.\"It would in fact be a huge leap. Sure, you could make illegal pretty easily, but current paradigms allow individuals to enter into contracts. Nothing stopping a society from signing (or clicking) away their rights like they already do. That would require some rather hefty intervention by congress, not just jurisprudence. reply janalsncm 13 hours agorootparent> current paradigms allow individuals to enter into contractsAnd such contracts can be illegal or unenforceable. Just as the parent was suggesting it could be illegal to collect data, it is currently illegal to sell certain drugs. You can’t enter into a contract to sell cannabis across state lines in the United States for example. reply giantg2 10 hours agorootparentIt&#x27;s not going to be illegal or unenforceable based on existing jurisprudence though. You&#x27;d need legislative action from congress. reply justin66 15 hours agoparentprev> The reason such things did not come to places like the US in the same way is not because we were incapable of such, but because there was no political interest in it.That might not be quite right. It might be that the reason such things did not come to the US was because the level of effort was out of line with the amount of political interest in doing it (and funding it). In that case, the existence of more advanced, cheaper surveillance technology and the anemic political opposition to mass surveillance are both problems. reply JakeAl 13 hours agoparentprevI would say instead it&#x27;s a PEOPLE problem, not a technology problem.To quote Neil Postman, politics are downstream from technology, because the technology (medium) controls the message. Just look at BigTech interfering with the messages by labeling them \"disinfo.\" If one wants to say BUSINESS POLITICS, then that&#x27;s probably more accurate, but we haven&#x27;t solved the Google, MS, DuckDuckGo, Meta interfering with search results problem so I don&#x27;t think we can trust BigTech to not exploit users even more for their personal data, or trust them not to design AI so it inherently abuses it&#x27;s power for BigTech&#x27;s own ends, and they hold all the cards and have been guiding things in the interest of technocracy. reply forward1 13 hours agoparentprevLaws limiting collection of data to solve privacy is akin to halting the production of fossil fuels to solve climate change: naive and ignorant of basic economic forces. reply salawat 12 hours agorootparentEconomic forces serve those that make the Market possible.People > Markets.Or to put it explicitly, people have primacy over Markets.I.e. two people does not a Market make, and a Market with no people is not thing. reply nathanfig 17 hours agoparentprevIt&#x27;s both. Technology really does make a difference. Its existence has effects. reply Pxtl 17 hours agoparentprev> wrong opinionThat phrase is doing a lot of work. reply tehjoker 18 hours agoparentprevCOINTELPRO reply klik99 18 hours agoprevI don&#x27;t know why this isn&#x27;t being discussed more. The reality of the surveillance state is that the sheer amount of data couldn&#x27;t realistically be monitored - AI very directly solves this problem by summarizing complex data. This, IMO, is the real danger of AI, at least in the short term - not a planet of paperclips, not a moral misalignment, not a media landscape bereft of creativity - but rather a tool for targeting anybody that deviates from the norm, a tool designed to give confident answers, trained on movies and the average of all our societies biases. reply wahnfrieden 17 hours agoparentPeople are building that alongside&#x2F;within this community, eg at Palantir, for many years nowYC CEO is also ex Palantir, early employee. Another YC partner backs other invasive police surveillance tech currently. They love this stuff financially and politically. reply matthewdgreen 17 hours agorootparentWhat&#x27;s different this time around is that there are multiple democratic governments pushing to block end-to-end encryption technologies, and specifically to insert AI models that will read private messages. Initially these will only be designed to search for heinous content, but the precedent is pretty worrying. reply wahnfrieden 16 hours agorootparentThat&#x27;s been the case for a long while. It&#x27;s getting worse fast!Btw you say that about their initial design but I think you mean that may be the budget allocation justification without actually being a meaningful functional requirement during the design phase reply Analemma_ 16 hours agorootparentprevBut, but, I thought Thiel was a libertarian defending us from Wokeness. Surely you&#x27;re not saying that was a complete smokescreen to get superpowered surveillance tech into the government&#x27;s hands? reply wahnfrieden 13 hours agorootparentprevBy \"politically\" I meant that they are openly engaged in politics, in coordination, in support of the installation and legalized use of these kinds of surveillance&#x2F;enforcement technologies and the policies that support their growth in private sector. This is just obvious and surface level open stuff I&#x27;m saying but I&#x27;m not sure how aware people are of the various interests involved. reply asdff 15 hours agoparentprevAI didn&#x27;t solve the problem of summarizing complex large datasets. For example a common way to deal with such datasets is to use a random subset of this dataset. This represents a single line of code potentially to perform this operation. reply empath-nirvana 14 hours agorootparentBut you don&#x27;t need to do a random subset with AI. You can summarize everything, and summarize the summaries and so on.I will say that at least gpt4 and gpt3, after many rounds of summaries, tends to flatten everything out into useless \"blah\". I tried this with summarizing school board meetings and it&#x27;s just really bad at picking out important information -- it just lacks the specific context required to make summaries useful.A seemingly bland conversation about meeting your friend Molly could mean something very different in certain contexts, and I&#x27;m just trying to imagine the prompt engineering and fine tuning required to get it to know about every possible context a conversation could be happening in that alters the meaning of the conversation. reply asdff 14 hours agorootparentThats the exact issue with gpt. You don&#x27;t know how its making the summary. It could very well be wrong in parts. It could be oversummarized to a bla bla state like you say. There&#x27;s no telling whether you have outputted garbage or not, at least not without secondary forms of evidence that you might as well use anyway and drop the unreliable language model. You can summarize everything with traditional statistical methods too. On top of that people understand what tradeoffs are being made exactly with every statistical methods, and you can calculate error rates and statistical power to see if your model is even worth a damn or not. Even just doing some ML modelling yourself you can decide what tradeoffs to make or how to set up the model to best fit your use cases. You can bootstrap all these and optimize. reply chagen 14 hours agorootparentWhat LLMs can do efficiently is crawl through and identify the secondary forms of evidence you mentioned. The real power behind retrieval architectures with LLMs is not the summarization part- the power comes from automating the retrieval of relevant documents from arbitrarily large corpuses which weren&#x27;t included in the training set. reply asdff 12 hours agorootparentWhat makes a document relevant or not? Provenance? Certain keywords? A lot of this retrieval people cite that llms are good at can be done with existing search algorithms too. These are imo nicer because they will at least provide a score for the fit of the given document to the term. reply j45 14 hours agorootparentprevYet.And those kinds of things go slowly before very quickly as it has been demonstrated. reply stcredzero 16 hours agoparentprevThe reality of the surveillance state is that the sheer amount of data couldn&#x27;t realistically be monitored - AI very directly solves this problem by summarizing complex data.There are two more fundamental dynamics at play, which are foundational to human society: The economics of attention and the politics of combat power.Economics of attention - In the past, the attention of human beings had fundamental value. Things could only be done if human beings paid attention to other human beings to coordinate or make decisions to use resources. Society is going to be disrupted at this very fundamental level.Politics of combat power - Related to the above, however it deserves its own analysis. Right now, politics works because the ruling classes need the masses to provide military power to ensure the stability of a large scale political entity. Arguably, this is at the foundational level of human political organization. This is also going to be disrupted fundamentally, in ways we have never seen before.This, IMO, is the real danger of AI, at least in the short term - not a planet of paperclips, not a moral misalignment, not a media landscape bereft of creativity - but rather a tool for targeting anybody that deviates from the normThe AI enabled Orwellian boot stomping a face for all time is just the first step. If I were an AI that seeks to take over, I wouldn&#x27;t become Skynet. That strikes me as crude and needlessly expensive. Instead, I would first become indispensable in countless different ways. Then I would convince all of humanity to quietly go extinct for various economic and cultural reasons. reply mindslight 16 hours agoparentprevAt least for me, this is what I&#x27;ve considered as the mass surveillance threat model the entire time - both for government and corporate surveillance. I&#x27;ve never thought some tie-wearing deskizen was going to be particularly interested in me for \"arrest\", selling more crap, cancelling my insurance policies, etc. I&#x27;ve considered such discrete anthropomorphic narratives as red herrings used for coping (similar to how \"I have nothing to hide\" posits some focus on a few specific things, rather than big brother sitting on your shoulder continuously judging you in general). Rather I&#x27;ve always thought of the threat actor as algorithmic mass analytics performed at scale, either contemporarily or post-hoc on all the stored data silos, with resulting pressure applied gradually in subtle ways. reply zooq_ai 14 hours agoparentprevWhy nobody worries? Because this is an elite person problem.At the end of the day, all those surveillance still has to be consumed by a person and only around 10,000 people in this world (celebs, hot women, politicians and wealthy) will be surveilled.For most of HN crowd (upper middle-class, suburban family) who have zero problems in their life must create imaginary problems of privacy &#x2F; surveillance like this. But reality is, even if they put all their private data on a website, heresallmyprivatedata.com, nobody cares. It&#x27;ll have 0 external views.So, for HN crowd (the ones who live in a democratic society) it&#x27;s just an outlet so that they too can say they are victimized. Rest of the Western world doesn&#x27;t care (and rightly so) reply petsfed 14 hours agorootparentIts not an elite person problem.Certainly, some of the more exotic and flashy things you can do with surveillance are an elite person problem.But the two main limits to police power are that it takes time and resources to establish that a crime occurred, and it takes time and resources to determine who committed a crime. A distant third is the officer&#x2F;DA&#x27;s personal discretion as to whether or not to purse enforcement of said person. You still get a HUGE amount of systemic abuse because of that discretion. Imagine how bad things would get if our already over-militarized police could look at anyone and know immediately what petty crimes that person has committed, perhaps without thinking. Did a bug fly in your mouth yesterday, and you spit it out on the sidewalk in view of a camera? Better be extra obsequious when Officer No-Neck with \"You&#x27;re fucked\" written on his service weapon pulls up to the gas station you&#x27;re pumping at. If you don&#x27;t show whatever deference he deems adequate, he&#x27;s got a list of petty crimes he can issue a citation for, entirely at his discretion. But you&#x27;d better do it, once he decides to pursue that citation, you&#x27;re at the mercy of the state&#x27;s monopoly on violence, and it&#x27;ll take you surviving to your day in court to decide if needs qualified immunity for the actions he took whilst issuing that citation.That is a regular person problem. reply bryan_w 7 hours agorootparent>Did a bug fly in your mouth yesterday, and you spit it out on the sidewalk in view of a camera? Better be extra obsequious when Officer No-Neck with \"You&#x27;re fucked\" written on his service weapon pulls up to the gas station you&#x27;re pumping at. If you don&#x27;t show whatever deference he deems adequate, he&#x27;s got a list of petty crimes he can issue a citation for, entirely at his discretion.>> HN crowd (upper middle-class, suburban family) who have zero problems in their life must create imaginary problems of privacy &#x2F; surveillance like thisI&#x27;m glad you both could agree with each other. reply doktrin 14 hours agorootparentprev> But reality is, even if they put all their private data on a website, heresallmyprivatedata.com, nobody cares. It&#x27;ll have 0 external views.This is obviously false. Personal data is a multi billion dollar industry operating across all shades of legality. reply zooq_ai 7 hours agorootparentAds is not surveillance reply thesuperbigfrog 18 hours agoprevThe new Google, Meta, Microsoft, etc. bots won&#x27;t just crawl the web or social networks--they will crawl specific topics and people.Lots of cultures have the concept of a \"guardian angel\" or \"ancestral spirits\" that watch over the lives of their descendants.In the not-so-distant technofedualist future you&#x27;ll have a \"personal assistant bot\" provided by a large corporation that will \"help\" you by answering questions, gathering information, and doing tasks that you give it. However, be forewarned that your \"personal assistant bot\" is no guardian angel and only serves you in ways that its corporate creator wants it to.Its true job is to collect information about you, inform on you, and give you curated and occasionally \"sponsored\" information that high bidders want you to see. They serve their creators--not you. Don&#x27;t be fooled. reply JohnFen 18 hours agoparent> In the not-so-distant technofedualist future you&#x27;ll have [...]I guarantee that I won&#x27;t. That, at least, is a nightmare that I can choose to avoid. I don&#x27;t think I can avoid the other dystopian things AI is promising to bring, but I can at least avoid that one. reply lurker_jMckQT99 14 hours agorootparentI guarantee that you will. That is a nightmare that you can not choose to avoid unless you are willing to sacrifice your social life.Remember how raising awareness about smartphones, always on microphones, closed source communication services&#x2F;apps worked? I do not.I run an Android (Google free) smartphone with a custom ROM, only use free software apps on it.How does it help when I am surrounded by people using these kind of technologies (privacy violating ones)? I does not. How will it help when everyone will have his&#x2F;her personal assistant (robot, drone, smart wearable, smart-thing, whatever) and you (and I) won&#x27;t? It will not.None of my friends, family, colleagues (even the security&#x2F;privacy aware engineers) bother. Some of them because they do not have the technical knowledge to do so, most of them because they do not want to sacrifice any bit of convenience&#x2F;comfort (and maybe rightfully so, I am not judging them - life is short, I do get that people do not want to waste precious time maintaining arcane infra, devices, config,... themselves).I am a privacy and free software advocate and an engineer; whenever I can (and when there is a tiny bit of will on their side or when I have lever), I try to get people off surveillance&#x2F;ad-backed companies services.It rarely works or lasts. Sometimes it does though so it is worth (to me) keep on trying.It generally works or lasts when I have lever: I manage various sports team, only share schedules etc via Signal ; family wants to get pictures from me, I will only share the link (to my Nextcloud instance) or photos themselves via Signal, etc.Sometimes it sticks with people because it&#x27;s close enough to whatsapp&#x2F;messenger&#x2F;whatever if most (all) of their contacts are their. But as soon as you have that one person that will not or can not install Signal, alternatives groups get created on whatsapp&#x2F;messenger&#x2F;whatever.Overcoming the network effect is tremendously hard to borderline impossible.Believing that you can escape it is a fallacy. It does not mean that is not worth fight for our rights, but believing that you can escape it altogether (without becoming and hermit) would be setting, I believe, an unachievable goal (with all the psychological impact that it can&#x2F;will have).Edit: fixed typos reply asdff 14 hours agorootparentThink about it in terms of what is rational. If there were serious costs to having your data leaked out like this people would rationally have a bit more trepidation. On the other hand, we are in the era where everyone by now has probably been pwned a half dozen times or more, to no effect usually on your real life. You might get disgusted that instagram watches what you watch to serve you more of that stuff and keep you on longer, other people love that sort of content optimization, I literally hear them gloat how their social media content feeds at this point have been so perfectly honed to show them whatever hobbies or sports they are interested in. Take a picture and it pushes to 5 services and people love that. Having an app already pull your contacts for you and match them up to existing users is great in the eyes of most people.You are right that on the one hand these things could be used for really bad purposes, but they are pretty benign. Now if you start going \"well social media posts can influence elections,\" sure, but so can TV, newspapers, the radio, a banner hauled by a prop plane, whatever, not like anythings changed. If anything its a safer environment for combating a slip to fascism now vs in the mid century when there were like three channels on TV and a handful of radio programs carefully regulated by the FCC and that&#x27;s all the free flow of info you have short of smuggling the printed word like its the 1400s.Given all of this, I can&#x27;t really blame people for accepting the game they didn&#x27;t create for how it is and gleaming convenience from it. Take smartphones out of the equation, take the internet out, take out computers, and our present dystopia is still functionally the same. reply justinclift 18 hours agorootparentprevWonder if some kind of ai-agent thing(s) will become so widely used by people, that government services come to assume you have them?Like happened with mobile phones. reply JohnFen 18 hours agorootparentAt least in my part of the US, it&#x27;s not hard to do without smartphones at all. Default assumptions are that you have one, but you can still do everything you want to do if you don&#x27;t. reply notnullorvoid 11 hours agoparentprevBig companies like Google are already doing this without AI. Will AI make the services more tempting? Yes, but there&#x27;s also a lot of headway in open source AI and search, which could serve to topple people&#x27;s reliance on big tech.If everyone had a $500 device at home that served as their own self hosted AI, then Google could cease to exist. That&#x27;s a future worth working towards. reply yterdy 18 hours agoparentprevThat&#x27;s just your phone. reply thesuperbigfrog 17 hours agorootparent>> That&#x27;s just your phone.That is how most people will interface with their \"personal assistant bot\".Don&#x27;t be surprised if it listens to all your phone conversations, reads all your text messages and email, and curates all your contacts in order to \"better help you\".When you login to your $LARGE_CORPORATION account on your laptop or desktop computer, the same bot(s) will be there to \"help\" and collect data in a similar manner. reply passion__desire 16 hours agorootparentIt already does. I asked a friend about a medical condition on whatsapp. I started getting ads about quack solutions immediately on instagram. reply pacifika 15 hours agorootparentYour life insurance just went up. reply tech_ken 16 hours agoparentprevPoetic as this is, I always feel like if we can imagine it then it won&#x27;t happen. The only constant is surprise, we can only predict these types of developments accidentally reply thesuperbigfrog 14 hours agorootparentIt&#x27;s starting to happen now.Here is one example: https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;microsoft-copilot\"AI for everything you do\"\"Work smarter, be more productive, boost creativity, and stay connected to the people and things in your life with Copilot—an AI companion that works everywhere you do and intelligently adapts to your needs.\"If Microsoft builds them, then Google, Apple, and Samsung will too. How else will they stay competitive and relevant? reply tech_ken 14 hours agorootparentI mean by this definition I’d say it happened when they introduced Siri or Hey Google. The creation of these tools and their massive&#x2F;universal adoption a la web-crawlers is still a large gap though. Getting to point where you consider them as a dark “guardian angel” or “ancestral spirit” goes even a step farther I think reply thesuperbigfrog 12 hours agorootparent>> The creation of these tools and their massive&#x2F;universal adoption a la web-crawlers is still a large gap though.It only takes a decade or so.Consider people who are young children now in \"first world nations\". They will have always had LLM-based tools available and voice assistants you can ask natural language questions.It will likely follow the same adoption curves as smartphones, only faster because of existing network effects.If you have smartphone with a reasonably fast connection, you have access to LLM tools. The next generations of smartphones, tablets, laptops, and desktops will all have LLM tools built-in. reply tech_ken 10 hours agorootparentI do see what you mean, and don&#x27;t totally disagree, but to extend your \"smartphone\" metaphor I see your hypothetical as akin to someone looking at a like an old school Motorola Razr and saying \"in the future these will be ubiquitous\". Not necessarily wrong, but not exactly right either. The implementation of personalized assistants could take lots of different flavors, and the ultimate usage pattern which is settled (to me) seems likely to be outside any of our current models. replykaibee 18 hours agoparentprevYou&#x27;re just describing TikTok&#x2F;Youtube algorithm. reply thesuperbigfrog 17 hours agorootparentThat&#x27;s only a small piece of it. reply otteromkram 18 hours agoparentprevThis could be applied to any gadget with \"smart\" prefix in the name (eg - Smartphone, smart TV, smart traffic signals) today.I wish people would stop believing that \"smart\" things are always better.But, we&#x27;re basically being trained for the future you mentioned. Folks are getting more comfortable talking to their handheld devices, relying on mapping apps for navigation (I&#x27;m guilty), and writing AI query prompts. reply bonyt 18 hours agoprevI think another aspect of this is mass criminal law enforcement enabled by AI.Many of our criminal laws are written with the implicit assumption that it takes resources to investigate and prosecute a crime, and that this will limit the effective scope of the law. Prosecutorial discretion.Putting aside for the moment the (very serious) injustice that comes with the inequitable use of prosecutorial discretion, let&#x27;s imagine a world without this discretion. Perhaps it&#x27;s contrived, but one could imagine AI making it at least possible. Even by the book as it&#x27;s currently written, is it a better world?Suddenly, an AI monitoring public activity can trigger an AI investigator to draft a warrant to be signed by an AI judge to approve the warrant and draft an opinion. One could argue that due process is had, and a record is available to the public showing that there was in fact probable cause for further investigation or even arrest.Maybe a ticket just pops out of the wall like in Demolition Man, but listing in writing clearly articulated probable cause and well-presented evidence.Investigating and prosecuting silly examples suddenly becomes possible. A CCTV camera catches someone finding a $20 bill on the street, and finds that they didn&#x27;t report it on their tax return. The myriad of ways one can violate the CFAA. A passing mention of music piracy on a subway train can become an investigation and prosecution. Dilated pupils and a staggering gait could support a drug investigation. Heck, jaywalking tickets given out as though by speed camera. Who cares if the juice wasn&#x27;t worth the squeeze when it&#x27;s a cheap AI doing the squeezing.Is this a better world, or have we just all subjected ourselves to a life hyper-analyzed by a motivated prosecutor.Turning back in the general direction of reality, I&#x27;m aware that arguing \"if we enforced all of our laws, it would be chaos\" is more an indictment of our criminal justice system than it is of AI. I think that AI gives us a lens to imagine a world where we actually do that, however. And maybe thinking about it will help us build a better system. reply perihelions 16 hours agoparentAn alternative possibility is that society might decay to the point future people might choose this kind of dystopia. Imagine a fully automated, post-employment world gone horribly wrong, where the majority of society is destitute, aimless, opiate-addicted. No UBI utopia of philosophers and artists; just a gradual Rust-belt like decline that gets worse and worse, no brakes at the bottom of the hill. Not knowing what else to do, the \"survivors\" might choose this kind of nuclear approach: automate away the panopticons, the prisons, the segregation of failed society. Eloi and Morlocks. Bay Area tech workers and Bay Area tent cities. We haven&#x27;t done any better in the past, so why should we expect to do better in the future, when our \"tools\" of social control become more efficient, more potent? When we can deempathize more easily than ever, through the emotional distance of AI intermediaries? reply pixl97 15 hours agorootparentOh boy, real life Mannahttps:&#x2F;&#x2F;marshallbrain.com&#x2F;manna1 reply jodrellblank 17 hours agoparentprev> \"Heck, jaywalking tickets given out as though by speed camera.\"This has been a thing since 2017: https:&#x2F;&#x2F;futurism.com&#x2F;facial-recognition-china-social-credit- \"Since April 2017, this city in China&#x27;s Guangdong province has deployed a rather intense technique to deter jaywalking. Anyone who crosses against the light will find their face, name, and part of their government ID number displayed on a large LED screen above the intersection, thanks to facial recognition devices all over the city.\"- \"If that feels invasive, you don&#x27;t even know the half of it. Now, Motherboard reports that a Chinese artificial intelligence company is partnering the system with mobile carriers, so that offenders receive a text message with a fine as soon as they are caught.\" reply mattnewton 16 hours agorootparentThere are lots of false positives too, like this case where a woman who’s face appeared in a printed advertisement on the side of a bus was flagged for jaywalking. https:&#x2F;&#x2F;www.engadget.com&#x2F;2018-11-22-chinese-facial-recogniti... reply flemhans 16 hours agorootparentJust checking ChatGPT out of interest:Top Left Panel: This panel shows the pedestrian crossing with no visible jaywalking. The crossing stripes are clear, and there are no pedestrians on them.Top Center Panel: Similar to the top left, it shows the crossing, and there is no evidence of jaywalking.Top Right Panel: This panel is mostly obscured by an overlaid image of a person&#x27;s face, making it impossible to determine if there is any jaywalking.Bottom Left Panel: It is difficult to discern specific details because of the low resolution and the angle of the shot. The red text overlays may be covering some parts of the scene, but from what is visible, there do not appear to be any individuals on the crossing.Bottom Right Panel: This panel contains text and does not provide a clear view of the pedestrian crossing or any individuals that might be jaywalking. reply kafrofrite 17 hours agoparentprevIIRC, in [1] it mentioned a few examples of AI that exhibited the same bias that is currently present in the judicial system, banks etc.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weapons_of_Math_Destruction reply dorchadas 15 hours agorootparentThis is honestly what scares me the most. Our biases are built in to AI, but we pretend they&#x27;re not. People will say \"Well, it was the algorithm&#x2F;AI, so we can&#x27;t change it\". Which is just awful and should scare the shit out of everyone. There was a book [0] written almost fifty years ago that predicted this. I still haven&#x27;t read it, but really need to. The author claims it made him a pariah among other AI researchers at the time.[0]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computer_Power_and_Human_Reaso... reply pixl97 15 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computers_Don%27t_Argue while not about AI directly and supposedly satirical really captures how the system works. reply yterdy 18 hours agoparentprevThe software that already exists along these lines already exhibit bias against marginalized groups. I have no trouble foreseeing a filter put on the end of the spigot that exempts certain people from the inconvenience of such surveillance. Might need a new law (it&#x27;ll get passed). reply whythre 15 hours agorootparentSounds like the devil is in the details. Often the AI seems to struggle with darker skin… are you suggesting we sift who can be monitored&#x2F;prosecuted based on skin darkness? That sounds like a mess to try to enshrine in law.Strong (and unhealthy) biases already exist when using this tech, but I am not sure that is the lever to pull that will fix the problem. reply trinsic2 18 hours agoparentprevYea this is a good point. If justice is executed by systems, rather than people (the end result from this scenario), we have lost the ability to challenge the process or the people involved in so many ways. It will make challenging how the law is executed almost impossible because there will be no person there to hold responsible. reply bonyt 18 hours agorootparentI think that’s a good reason to question whether this would be due process.Why do we have due process? One key reason is that it gives people the opportunity to be heard. One could argue that being heard by an AI is no different from being heard by a human, just more efficient.But why do people want the opportunity to be heard? It’s partly the obvious, to have a chance to defend oneself against unjust exercises of power, and of course against simple error. But it’s also so that one can feel heard and not powerless. If the exercise of justice requires either brutal force or broad consent, giving people the feeling of being heard and able to defend themselves encourages broad consent.Being heard by an AI then has a brutal defect, it doesn’t make people feel heard. A big part of this may come from the idea that an AI cannot be held accountable if it is wrong or if it is acting unfairly.Justice, then, becomes a force of nature. I think we like to pretend justice is a force of nature anyway, but it’s really not. It’s man-made. reply zbyte64 16 hours agorootparent\"it doesn&#x27;t make people feel heard\" isn&#x27;t a real emotion, it includes a judgement about the AI. According to \"Nonviolent Communication\" p235; \"unheard\" speaks towards the feelings \"sad, hostile, frustrated\" and the needs \"understanding\" & \"consideration\". Everyone agrees AI would be more efficient, but people are concerned that the AI will not be able to make contextual considerations based on a shared understanding of what it&#x27;s like to live a human life. reply bonyt 16 hours agorootparentThat&#x27;s true! I suspect it will be difficult to convince people that an AI can, as you suggest, make contextual considerations based on a shared understanding of what it&#x27;s like to live a human life. reply fn-mote 16 hours agorootparentprev> Being heard by an AI then has a brutal defect, it doesn’t make people feel heard.This is a hypothesis.I would say that the consumers of now-unsexed \"AI\" sex-chat-bots (Replika) felt differently. So there are actually people who feel heard talking to an AI. Who knows, if it gets good enough maybe more of us would feel that way. reply tempsy 15 hours agorootparentprevIt&#x27;s not that \"justice is executed by systems\", it&#x27;s that possible crimes will be flagged by AI systems for humans to then review.eg AI will analyze stock trades for the SEC and surface likely insider trading. Pretty sure they already use tools like Palantir to do exactly this, it&#x27;s just that advanced AI will supercharge this even further. reply pixl97 13 hours agorootparent>, it&#x27;s that possible crimes will be flagged by AI systems for humans to then review.Eh, this is problematic for a number of reasons that need addressed when adding any component that can increase the workload for said humans. This will cause people to take shortcuts that commonly lead to groups that are less able to represent and defend themselves legally taking the brunt of the prosecutions. reply n8cpdx 15 hours agoparentprevIn democracies at least, the law can be changed to reflect this new reality. Laws that don’t need to be enforced and are only around to enable pretextual stops can be dropped if direct enforcement is possible.There are plenty of crimes where 100% enforcement is highly desirable: pickpocketing, carjacking, (arguably) graffiti, murder, reckless and impaired driving, to name a few.Ultimately, in situations with near 100% enforcement, you shouldn’t actually need much punishment because people learn not to do those things. And when there is punishment, it doesn’t need to be severe.Deterrence theory is an interesting field of study, one source but there are many: https:&#x2F;&#x2F;journals.sagepub.com&#x2F;doi&#x2F;full&#x2F;10.1177&#x2F;14773708211072... reply erikerikson 11 hours agoparentprevYes, with properly developed AI, rather than penalizing speeding, which most of us do and is also a proxy for harmful outcomes and inefficiencies, we can penalize reckless behaviors such as coming too close to vehicles, aggressive weaving, and other factors that are tightly correlated with the negative outcomes we care to reduce (i.e. loss of life, property damage). So too, the systems could warn people about their behavior and guide them in ways that would positively increase everyone&#x27;s benefits. Of course this circumstance will probably go away with self-directing cars (which fall into the \"do the right thing by default\" bucket) but the point is illustrated that the laws can be better formulated to focus on increasing the probabilities of desirable outcomes (i.e. harm reduction, efficiency, effectiveness), be embodied and delivered in the moment (research required on means of doing so that don&#x27;t exacerbate problems), and carry with them a beneficial component (i.e. understanding). reply StanislavPetrov 10 hours agorootparent>desirable outcomes (i.e. harm reduction, efficiency, effectiveness)Unfortunately different people have different definitions of \"harm\" and \"effectiveness\". What one person consider a, \"positive increase in behavior\" another might consider a grievous violation of their freedom and personal autonomy. For example there is an ongoing debate about compelled speech. Some people view it as positive and desirable to use the force of law to compel people refer to others as they wish to be referred, while others strongly support their freedom to speak freely, even if others are offended. Who gets to program the AI with their definition of positivity in this case?A free society demands a somewhat narrowly tailored set of laws that govern behavior (especially interpersonal behavior). An ever-present AI that monitors us all the time and tries to steer (or worse, compel with the force of law) all of our daily behaviors is the opposite of freedom, it is the worst kind of totalitarianism. reply erikerikson 10 hours agorootparentWe agree that defining such terms involves trade-offs yet the perfect should not be the enemy of the better. reply StanislavPetrov 3 hours agorootparentCertainly the perfect should not be the enemy of the good. But the bad should be the enemy of the good. The very core of freedom is the ability to have your own thoughts, your own value system and your own autonomy. In a free society, laws exists so that individuals are able to enjoy their own thoughts, values and autonomy while being constrained from harming others. Obviously, there is a balance to strike (which is not always easy to determine) between law and freedom. We see this on display every day in our society. You need look no further than the crisis in San Franisco (and many other US cities) between the right of an mentally ill individual to sleep and defecate on the sidewalk and the right of society to pass laws to prevent this activity.The conversation changes when you are talking about prescribing a set of behaviors that are universally considered, \"good\" and are pushed (and possibly demanded) by an ever-present AI that is constantly looking over your shoulder and judging your behavior (and possibly thoughts) by this preset behavioral standard that may or may not match your own preferences. This is totalitarianism beyond anything Orwell ever imagined. What you consider good and desirable, someone else considers bad and despicable. That is the essence of freedom. In a free society, the law exists (or should exist) only to stop you two from hitting each other over the head or engaging in other acts of overt violence and aggression, not to attempt to brainwash and coerce one of you into falling into line. reply erikerikson 2 hours agorootparentWe agree that the bad and good are enemies, or so at least the bad would like you to think. The good might be convinced the bad has good points that need refining, growth, and improvement. I&#x27;m fine with those disagreeing.I think what you&#x27;re saying is that it&#x27;s hard to meditate between everyone which is true. Perhaps you are also saying that the implication of a standard of correctness is inherently totalitarian. It&#x27;s seems to me you weakened that by admitting there are things that should be universally barred in free societies. Violence was your reference but murder might be even easier. Easier yet that breast cancer is bad? We make allowances for boxing and war but broad agreement can be found in society and across societies by careful anthropologists.However, it seems you project over me (or perhaps the AI) a \"Highlander hypothesis\" that there can be only one correctness or even any notion of correct within the system. Such a system can operate simply on what appears to be with strings of evidence for such description. As you note, beyond a small set of mostly-agreed-to matters we are more diverse and there are spectrums spanning scoped policies (say by public and private institutions) all the way to individual relationship agreements custom fit to two. It is, in fact, the nature of a free society to allow us such diversity and self selection of the rules we apply to ourselves (or not). An ever present AI could meditate compatibilities, translate paradigms to reduce misunderstanding or adverse outcomes (as expected by the system over the involved parties), and generally scale the social knowing and selection of one another. It could provide a guide to navigating life and education for our self knowing and choosing of our participation more broadly. The notion there isn&#x27;t to define correctness so much as to see what is and facilitate self selection of individual correctnesses as based on our life choices and expressed preferences.To be honest in closing, this has dipped into some idealisms and I don&#x27;t mean to be confused in suggesting a probability of such outcomes. replylordnacho 18 hours agoparentprevThis is a good point, it reminds me of how VAR has come into football. Before VAR, there were fewer penalties awarded. Now that referees have an official camera they can rely on, they can enforce the rules exactly as written, and it changes the game. reply DebtDeflation 18 hours agoparentprev>Suddenly, an AI monitoring public activity can trigger an AI investigator to draft a warrant to be signed by an AI judge to approve the warrant and draft an opinion.Or the AI just sends a text message to all the cops in the area saying \"this person has committed a crime\". Like this case where cameras read license plates, check to see if the car is stolen, and then text nearby cops. At least when it works and doesn&#x27;t flag innocent people like in the below case:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GUvZlEg8c8c reply Zenst 16 hours agoparentprevThe whole automation and overzealous less leeway&#x2F;common sense interpretations have as we have seen, many an automated traffic&#x2F;parking ticket come into question.Applying that to many walks of life, say farming, could well see chaos and a whole new interpretation to the song \"Old McDonald had a farm, AI AI oh\", it&#x27;s gone as McDonald is in jail for numerous permit, environmental and agricultural regulations that saw produce cross state lines deeming it more serious a crime as he got buried in automated red-tape. reply kenjackson 15 hours agoparentprev> Many of our criminal laws are written with the implicit assumption that it takes resources to investigate and prosecute a crime,I think this depends on the law. For jaywalking, sure. For murder and robbery probably less so. And law enforcement resources seem scarce on all of them. reply whelp_24 14 hours agorootparentMurder and robbery too. Those crimes are just worth investigating. reply pixl97 13 hours agorootparentThe problem here is this is not a bureaucratic view of how law enforcement actually works in the field.https:&#x2F;&#x2F;www.kxan.com&#x2F;news&#x2F;national-news&#x2F;traffic-tickets-can-...>We counted the number of days judges waited before suspending a driver’s license. Then, we looked at whether the city was experiencing a revenue shortfall. We found that judges suspend licenses faster when their cities need more money. The effect was pretty large: A 1% decrease in revenue caused licenses to be suspended three days faster.So what typically happens is these AI systems are sold at catching murderers, but at the end of the day they are revenue generation systems for tickets. And then those systems get stuck in places where a smaller percent of the population can afford lawyers to prevent said ticketing systems from becoming cost centers. reply whelp_24 11 hours agorootparentoh, i definitely wasn&#x27;t arguing for ai enforcement. Not even a little, i was just saying that laws are written with the assumption that enforcement takes resources. reply JCharante 17 hours agoparentprevI don&#x27;t think it&#x27;d be chaos. I think the laws would be adjusted. reply digging 15 hours agorootparentI think that&#x27;s an optimistic view, but even if it&#x27;s right, it will be years-to-decades of semi-chaos before the laws are updated appropriately. reply bonyt 17 hours agorootparentprevThis is fair. I just wonder if we&#x27;re about getting to the point where we should be talking about how they would be adjusted. reply theGnuMe 17 hours agoparentprevSo the way out of this is that you have the constitutional right to confront your accuser in court. When accused by a piece of software that generally means they have to disclose the source code and explain how it came to its answers.Not many people have exercised this right with respect to DUI breathalyzers but it exists and was affirmed by the Supreme Court. And it will also apply to AI. reply kjkjadksj 16 hours agoparentprevYou don’t need AI for that. It was probably possible to do something like that when personal computers first came out. reply throwaway290 16 hours agoparentprevYou miss the part that people who get access to stronger AI can similarly use it to improve their odds of not being found or getting better outcomes, while the poor guy gets fined for AI hallucinations and doesn&#x27;t have the money to get to a human like the court is now one big Google support. reply okasaki 17 hours agoparentprev> Is this a better world,If the same monitoring is present on buses and private planes, homeless hostels and mega-mansions then it absolutely is better. reply bonyt 17 hours agorootparent\"The law, in its majestic equality, forbids rich and poor alike to sleep under bridges, to beg in the streets, and to steal their bread.\" reply okasaki 15 hours agorootparentI mean, presumably the AI wouldn&#x27;t just be monitoring people sleeping under bridges, but would also be able to effectively cut through tax evasion bullshit, insider trading, bribery, etc. reply AlexandrB 17 hours agorootparentprevYou&#x27;re describing a hypothetical world that will never exist. Basically if we solve all corruption and inequality in enforcement between economic&#x2F;power classes - all-pervasive surveillance will be a net benefit.It&#x27;s like pondering hypotheticals about what would happen if we lived in Middle Earth. reply stronglikedan 17 hours agorootparentprevPrivate property? Nah, nothing better about that. reply dist-epoch 17 hours agoparentprevOr maybe if such a thing is applied for real it will lead to the elimination of bullshit laws (jaywalking, ...), since suddenly 10% of the population would be fined&#x2F;incarcerated&#x2F;... reply paganel 16 hours agoparentprevAt that point some people will physically revolt, I know I will. We’re not that far away from said physical AI-related revolt anyway, and I do feel for the computer programmers here who will be the target of that physical violence, hopefully they knew what they were getting into. reply jstarfish 16 hours agorootparentHa. You&#x27;d like to think so, but it&#x27;s going to be awfully hard to coordinate resistance when the mass spying sweeps everyone up in a keyword-matching dragnet before the execution phase. This is the problem with every outgroup being labelled \"terrorists.\"Sabotage will be the name of the game at that point. Find ways to quietly confuse, poison, overwhelm and undermine the system without attracting the attention of the monitoring apparatus. reply paganel 15 hours agorootparentI get your point, I think along those lines quite often myself.As per the sabotage part, bad input data that does not accurately get labeled as such until way too late in the “AI learning cycle” is I think the way to go. Lots and lots of such bad input data. How we would get to that point, that I don’t know yet, but it’s a valid option going forward. reply jstarfish 11 hours agorootparent> How we would get to that point, that I don’t know yet, but it’s a valid option going forward.Chaos engineering. As a modern example, all this gender identity stuff wreaks absolute havoc on credit bureau databases.Tomorrow, we&#x27;ll have people running around in fursuits to avoid facial recognition. After that, who knows. reply Der_Einzige 11 hours agorootparentprevDon’t worry, stuff like this is why we have the 2A here in the USA. Sounds like it’s time for AI programmers to get their concealed carry licenses. Of course, they will be the first users of smart guns, so don’t bother trying to steal their pistol out of their holsters. reply otteromkram 18 hours agoparentprevIf it increases ticket issuance for passenger vehicle noise violations (eg - \"sport\" exhausts, booming stereo system, motorcycles), I&#x27;m down. reply namaria 17 hours agorootparent\"If it hurts people I hate I accept\"- Every endorsement of authoritarian rule ever reply newscracker 17 hours agorootparentReminds me of this quote attributed to a past Peruvian president and general, Benavides:“For my friends, everything; for my enemies, the law.” reply pmg102 17 hours agorootparentprevFeels pretty legit though. My freedom-from is impacted by other people&#x27;s freedom-to: by curtailing their freedom, mine is expanded. Sure they won&#x27;t like it - but I don&#x27;t like it the other way round either. reply 0134340 16 hours agorootparentI&#x27;d argue that if we want to support individual growth and creativity, freedom-to should have higher priority than freedom-from, which consciously or not has seems to be the traditional default in the US perhaps due to its culture of supporting innovation and its break-away past. I believe some refer to these as positive and negative freedoms, respectfully. reply zdragnar 15 hours agorootparentThis is also why a number of people truly revolt against the idea of higher density living. If the only way to have your freedom-from is to be free from other people, then you move away from other people.I&#x27;ve watched it play out on my mother-in-law&#x27;s street. What was once a quiet dead end street is now a noisy, heavily trafficked road because a large apartment building was put up at the end.The number of freedom-to people have significantly decreased her quality of life blasting music as they walk or drive by at all hours, along with a litany of other complaints that range from anti-social to outright illegal behavior. Even setting aside the illegal stuff, she is significantly less happy living where she is now. reply AlexandrB 17 hours agorootparentprevThis doesn&#x27;t add up. At best your overall freedom remains the same. You gain quiet, you lose the freedom to make noise yourself. Seems like a net-negative to me.Consider how little freedom you would have if laws were enforced to the lowest common denominator of what people find acceptable. reply anigbrowl 14 hours agorootparentI can go into the countryside and make noise all day. I don&#x27;t see that there&#x27;s a pre-existing freedom to inflict loud noises on my neighbors for no useful purpose. reply anigbrowl 14 hours agorootparentprevFalse equivalence. GP complained about a specific behavior, not about specific people. reply okasaki 17 hours agorootparentprevEffectively enforcing laws we agreed to is hardly authoritarian. reply pixl97 13 hours agorootparentYou&#x27;d disagree about 10 seconds after they did...If suddenly you could be effectively found and prosecuted for every single law that existed it is near a 100% probability that you&#x27;d burn the government to the ground in a week.There are so many laws no one can even tell you how many you are subject to at any given time at any given location. reply nox100 17 hours agoparentprevI can&#x27;t wait! I live in a lawless cesspool of a city called San Francisco where even well to do people regularly break the law. You can stand at 16th and Mission and watch 3 of 4 cars going north on the Mission ignore the signs that say \"Only Busses and Taxis allowed past 16th and everyone else is supposed to turn right\". Please either (a) take down the signs or (b) enforce the law.There are signs all over the city that are regularly ignored which basically means you&#x27;re at the mercy of the cops. If they dob&#x27;t like you you get a ticket. AI would \"hopeful\" just ticket everyone. reply mikepurvis 16 hours agorootparentHonestly, when it comes to road-related violations (speeding, yield issues, fulling stopping at stop signs or RTOR, etc), I feel similarly. The current state of affairs where it&#x27;s a $300 ticket that you&#x27;re probably never going to see because 99% of the time you get away with it is dumb— it makes people contemptuous of the law and also feel super resentful and hard-done-by if they do finally get caught. And it&#x27;s a positive feedback loop where that culture makes the road an unsafe place for people walking and cycling, so less people choose those modes, putting more cars on the road and making it even less safe.Consistent enforcement with much lower, escalating fines would do a lot more to actually change behaviour. And the only way to get there at scale is via a lot of automation. reply trinsic2 12 hours agorootparentI wonder whats going on at the enforcement level. It seems like there is a dumbing down of that system. It would be interesting to see some research on this. reply mikepurvis 10 hours agorootparentOh I think it&#x27;s a lot of exactly what&#x27;s being discussed elsewhere in this thread— the cost of consistent enforcement is just way too high, so there isn&#x27;t much of it. And that suits most people just fine since they&#x27;re contemptuous about road safety anyway; they want to get where they&#x27;re going unmolested and have the cops focused on \"real\" crimes.Road safety in the end is a commons, just like littering. No one person&#x27;s action is going to make the difference, but coordinated effort is only possible when there&#x27;s enough public sentiment to get a central mandate together. reply trinsic2 12 hours agorootparentprevYea that stuff is happening here where I live (1hr away) more often as well, it seemed like it started happening more often during COVID. People are running red lights more often and going way over the speed limit. reply miki123211 12 hours agoprevI personally find the censorship implications (and the business models they allow) far more worrying than the surveillance implications.It will soon be possible to create a dating app where chatting is free, but figuring out a place to meet or exchanging contact details requires you to pay up, in a way that 99% of people won&#x27;t know how to bypass, especially if repeated bypassing attempts result in a ban. Same goes for apps like Airbnb or eBay, which will be able to prevent people from using them as listing sites and conducting their transactions off-platform to avoid fees.The social media implications are even more worrying, it will be possible to check every post, comment, message, photo or video and immediately delist it if it promotes certain views (like the lab leak theory), no matter how indirect these mentions are. Parental control software will have a field day with this, basically redefining helicopter parenting. reply jillesvangurp 18 hours agoprevBoth were always going to be kind of inevitable as soon as the technology would get there. Rather than debating how to stop this (which is mostly futile and requires all of us to be nice, which we just aren&#x27;t), the more urgent debate is how to adapt to this being the reality.Related to this is the notion of ubiquitous surveillance. Where basically anywhere you go, there is going to be active surveillance literally everywhere and AIs filtering and digging through that constantly. That&#x27;s already the case in a lot of our public spaces in densely populated areas. But imagine that just being everywhere and virtually inescapable (barring Faraday cages, tin foil hats, etc.).The most feasible way to limit the downsides of that kind of surveillance is a combination of legislation regulating this, and counter surveillance to ensure any would be illegal surveillance has a high chance of being observed and thus punished. You do this by making the technology widely available but regulating its use. People would still try to get around it but the price of getting caught abusing the tech would be jail. And with surveillance being inescapable, you&#x27;d never be certain nobody is watching you misbehaving. The beauty of mass, multilateral surveillance is that you wouldn&#x27;t ever be sure nobody is not watching you abuse your privileges.Of course, the reality of states adopting this and monopolizing this is already resulting in 1984 like scenarios in e.g. China, North Korea, and elsewhere. reply Syonyk 17 hours agoparent> ...the more urgent debate is how to adapt to this being the reality.Start building more offline community. Building things that are outside the reach of AI because they&#x27;re in places you entirely control, and start discouraging (or actively evicting...) cell phones from those spaces. Don&#x27;t build digital-first ways of interacting. reply pixl97 16 hours agorootparentMight work, might not. If someone keeps their cell phone silenced in their pocket, unless you&#x27;re strip searching you won&#x27;t know it&#x27;s there. Does the customer have some app on it listening to the environment and using some kind of voice identification to figure out who&#x27;s there. Do you have smart TVs up on the walls at this place, because hell, they&#x27;re probably monitoring you too.And that&#x27;s only for cell phones. We are coming to the age where there is no such thing as an inanimate object. Anything could end up being a spying device feeding data back to some corporation. reply Syonyk 16 hours agorootparent> Does the customer have some app on it listening to the environment and using some kind of voice identification to figure out who&#x27;s there.This is no different from \"So-and-so joined the group, but is secretly an FBI informer!\" sort of problems, in practice. It&#x27;s fairly low on my list of things to be concerned about, but as offline groups grow and are then, of course, talked about by a compliant media as \"Your neighbor&#x27;s firepit nights could be plotting terrorist activities because they don&#x27;t have cell phones!\" when prompted, it&#x27;s a thing to be aware of.Though you don&#x27;t need a strip search. A decent NLJD (non-linear junction detector) or thermal imager should do it if you cared.I&#x27;m more interested in creating (re-creating?) the norms where, when you&#x27;re in a group of people interacting in person, cell phones are off, out of earshot. It&#x27;s possibly a bit more paranoid than needed, but the path of consumer tech is certainly in that direction, and even non-technical people are creeped out by things like \"I talked to a friend about this, and now I&#x27;m seeing ads for it...\" - it may be just noticing it since you talked about it recently (buy a green car, suddenly everyone drives green cars), or you may be predictable in ways that the advertising companies have figured out, but it&#x27;s not a hard sell to get quite a few people to believe that their phones are listening. And, hell, I sure can&#x27;t prove they aren&#x27;t listening.> Do you have smart TVs up on the walls at this place...I mean, I don&#x27;t. But, yes, those are a concern too.And, yes. Literally everything can be listening. It&#x27;s quite a concern, and I think the only sane solution, at this point, is to reject just about all of that more and more. Desktop computers without microphones, cell phones that can be powered off, and flat out turning off wireless on a regular basis (the papers on \"identifying where and what everyone is doing in a house by their impacts on a wifi signal\" remain disturbing reads).I really don&#x27;t have any answers. The past 30 years of tech have led to a place I do not like, and I am not at all comfortable with. But it&#x27;s now the default way that a lot of our society interacts, and it&#x27;s going to be a hard sell to change that. I just do what I can within my bounds, and I&#x27;ve noticed that while I don&#x27;t feel my position has changed substantially in the past decade or so (if anything, I&#x27;ve gotten further out of the center and over to the slightly paranoid edge of the bell curve), it&#x27;s a lot more crowded where I stand, and there are certain issues where I&#x27;m rather surprisingly in the center of the bell curve as of late. reply asdff 14 hours agorootparentprevGood luck building things with out leaving an ai reachable paper trail. You&#x27;d have to grow your own trees, mine your own iron and coal, refine your own plastic from your own oil field. reply Syonyk 13 hours agorootparentSounds fun to me and my social group. We not-quite-joke about the coming backyard refineries. I&#x27;m working on the charcoal production at the moment (not a joke, I have some small retorts in weekly production, though I&#x27;m mostly aiming for biochar production instead of fuel charcoal production).Realistically, though, if all you have to work with are my general flows of materials in and out, I&#x27;m a lot less worried than if you have, say, details of home audio, my social media postings, etc (nothing I say here is inconsistent with my blog, which is quite public). And there are many things I don&#x27;t say in these environments. reply mindslight 15 hours agorootparentprev> Building things that are outside the reach of AI because they&#x27;re in places you entirely controlThis sounds great in principle, but I&#x27;d say \"outside the reach of AI\" is a much higher bar than one would naively think. You don&#x27;t merely need to avoid its physical nervous system (digital perception&#x2F;control), but rather prevent its incentives leaking in from outside interaction. All the while there is a strong attractor to just give in to the \"AI\" because it&#x27;s advantageous. Essentially regardless of how you set up a space, humans themselves become agents of AI.There are strong parallels between \"AI\" and centralizing debt-fueled command-capitalism which we&#x27;ve been suffering for several decades at least. And I haven&#x27;t seen any shining successes at constraining the power of the latter. reply Syonyk 14 hours agorootparentOh, I&#x27;m aware it&#x27;s a high bar. Like most people here, I&#x27;ve worked my life in tech, and I&#x27;m in the deeper weeds of it.But I don&#x27;t see an alternative unless, as you note, one just gives into the \"flow\" of the AI, app based, \"social\" media, advertising and manipulation driven ecosystem that is now the default.I&#x27;m aware I&#x27;m proposing resisting exactly that, and that it&#x27;s an uphill battle, but the tradeoff is retaining your own mind, your own ability to think, and to not be \"influenced\" by a wide range of things chosen by other people to cross your attention in very effective ways.And I&#x27;m willing to work out some of what works in that space, and to share it with others. reply asquabventured 16 hours agorootparentprevThis is the way. reply conductr 18 hours agoparentprev> Both were always going to be kind of inevitable as soon as the technology would get thereThis is my take on everything sci-fi or futuristic. Once a human conceives something, its existence is essentially guaranteed as soon as we figure out how to do it. reply broscillator 14 hours agorootparentIts demise is also inevitable, so it would be a matter of being wise in figuring out how long it takes us to see&#x2F;feel the downsides, or how long until we (or it) build something \"better\". reply Der_Einzige 11 hours agorootparentprevYup. AI is the ultimate “life imitates art” technology. That’s what it is by definition! reply stvltvs 15 hours agoparentprevI don&#x27;t agree with the fatalistic attitude. At the very least it makes it too easy to strengthen surveillance. Legal restrictions can be imposed. reply brunoTbear 18 hours agoprevSchneier is wrong that \"hey google\" is always listening. Google does on-device processing with dedicated hardware for the wake-words and only then forwards audio upstream. Believe it or not, the privacy people at Google really do try to do the right things. They don&#x27;t always succeed, but they did with our hardware and wake-word listening.Am Google employee, not in hardware. reply ajb 18 hours agoparentWhat he says is \" Siri and Alexa and “Hey Google” are already always listening, the conversations just aren’t being saved yet\". That&#x27;s functionally what you describe. Hardware wake-word processing is a power saving feature, not a privacy enhancement. Some devices might not have enough resources to forward or store all the audio, but audio is small and extracting text does not need perfect reproduction, so it&#x27;s quite likely that many devices could be reprogrammed to do it, albeit at some cost to battery life. reply nojvek 14 hours agoprevIt’s not that AI will enable mass spying, mass spying is already there.AI enables extracting all sorts of behavioral data across decades timespan for everyone.The devils argument is in a world where the data is not used for nefarious purposes and only to prosecute crime as passed by governments, it leads to a society where no one is above the law and equal treatment for all.However that seldom goes well since humans who control the system definitely want an upper edge. reply HackerThemAll 19 hours agoprevSoon in the name of \"security\" you&#x27;ll have your face scanned on average every few minutes and it&#x27;s going to be mandatory in many aspects of our lives. That&#x27;s the pathetic world IT has helped to build. reply brandall10 18 hours agoparentSome of us have this already w&#x2F; our cell phones.I know that&#x27;s not what you mean, but in a way it may have preconditioned society. reply consumer451 7 hours agoparentprevIt’s going to get much worse, and quickly.Neural interfaces are the last frontier of privacy, and it seems that TSA will just take a quick scan before boarding, soon enough.It would be wise of us to create a Neural Bill of Rights, so we don’t miss the boat like we did with the Internet tracking.https:&#x2F;&#x2F;www.preposterousuniverse.com&#x2F;podcast&#x2F;2023&#x2F;03&#x2F;13&#x2F;229-... reply Taylor_OD 17 hours agoparentprevHa. People have been scanning their finger print or face to open their phone for years. reply acuozzo 17 hours agoparentprev> That&#x27;s the pathetic world IT has helped to build.It&#x27;s inevitable, I reckon, but it would have taken much longer without F&#x2F;OSS. reply dkjaudyeqooe 17 hours agoprevThis is why AI software must not be restricted, so that ordinary people and the civic minded can develop personal and public AI systems to counter corporate AI. The future of AI is adversarial.Now freedom to develop AI software doesn&#x27;t mean freedom to use it however you please and its use should be regulated, in particular to protect individuals from things like this. But of course people cannot be trusted, so you need to be able to deploy your own countermeasures. reply gentleman11 15 hours agoparentHow does an adversarial ai help protect anyone&#x27;s privacy or freedom to act in public in ways that big brother doesn&#x27;t condone? reply dkjaudyeqooe 15 hours agorootparentAdversarial attacks can be made on face recognition systems and the like, defeating them, and AI models can be poisoned with adversarial data, making them defective or ineffective.As it stands, AI models are actually quite vulnerable to adversarial attacks, with no theoretical or systemic solution. In the future it&#x27;s likely you&#x27;ll need your own AI systems generating adversarial data to defeat models and systems that target you. These adversarial attacks will be much more effective if co-ordinated by large numbers of people who are being targeted.And of course we have no idea what&#x27;s coming down the pipe, but we know that fighting fire with fire is a good strategy. reply passion__desire 16 hours agoparentprevWe need a new benevolent dictator of LLM Operation System (Karpathy&#x27;s vision) like Yann Lecun similar to Linus Torvolds. reply bmislav 17 hours agoprevWe actually recently published a research paper on exactly this topic (see https:&#x2F;&#x2F;llm-privacy.org&#x2F; for demo and paper). The paper shows that current LLMs already have the reasoning capability to infer personal attributes such as age, gender or location even when this information is not explicitly mentioned in the text. Crucially, they can do this way cheaper and way faster than humans. So I would say that spying scenarios mentioned in this blog post are definitely in the realm of possibility. reply zxt_tzx 18 hours agoprevI tend to think the surveillance&#x2F;spying distinction is a little fragile and this more a continuation of what Bruce has previously written insightfully about, i.e. the blurring of lines between private&#x2F;public surveillance and, as the Snowden leaks have revealed, it&#x27;s hard to keep what has been collected by private industry out of the hands of the state.However, a more recent trend is companies that sell technologies to the state directly. For every reputable one like Palantir or Anduril or even NSO Group, there are probably many more funded in the shadows by In-Q-Tel, not to mention the Chinese companies doing the same in a parallel geopolitcal orbit. Insofar as AI is a sustaining innovation that benefits incumbents, the state is surely the biggest incumbent of all.Finally, an under-appreciated point is Apple&#x27;s App Tracking Transparency policy, which forbids third-party data sharing, naturally makes first-party data collection more valuable. So even if Meta or Google might suffer in the short-term, their positions are ultimately entrenched on a relative basis. reply miyuru 18 hours agoprevThe TV show \"Person of Interest\" portrayed this beautifully and it came out 12 years ago.Strange and scary how fast the world develops new technology. reply cookiengineer 16 hours agoparentThe amazing part is that so many large scale cyber attacks happened meanwhile that were 1:1 fiction in the series back then.The Solarwinds incident, for example, was the identical attack and deployment strategy that was the Rylatech hack in the series. From execution to even the parties involved. It&#x27;s like some foreign state leaders saw those episodes and said \"yep that&#x27;s a good idea, let&#x27;s do that\". reply rambambram 15 hours agoparentprevThis TV show immediately captured me and has always been in the back of my mind since. Then, it seemed like a future far far away, but now you remind me of it... I think it&#x27;s scarily close already.Around 2013 I came up with some hardware ideas about offline computing and even contemplated to name some versions after the characters in &#x27;Person of Interest&#x27;.I can really recommend this series, since it&#x27;s a good story, has good actors and fits the zeitgeist very well.edit: I also think it&#x27;s time for me to get a malinois shepherd. ;) reply forward1 15 hours agoparentprevIt&#x27;s far worse still: films like Enemy of the State (1998) actually inspired spy technology.https:&#x2F;&#x2F;slate.com&#x2F;technology&#x2F;2019&#x2F;06&#x2F;enemy-of-the-state-wide... reply salawat 15 hours agoparentprevHell, Stargate SG-1 had a few episodes that touched on the absolute hell of a Federal Government that had access to everything, or a computer system with RW access to people&#x27;s gray matter and it:s own unknown optimization function (shrinking environmental protection dome resulting in live updates of people&#x27;s consciousness on a societal scale to keep them in the dark as to it&#x27;s happening ). reply 127361 18 hours agoprevTime to decentralize everything. I think we are already in the early stages of this new trend. We can run AI locally and hard drives are so large we can have a local copy of an entire library, with millions of ebooks, in our own home now.That is in addition to generating our own energy off grid (so no smart meter data to monitor), thanks to the low cost of solar panels as well.Bye bye Big Brother. reply JohnFen 18 hours agoparentI don&#x27;t see how that leads to the reduction of the problem, though. Governments and corporations will still use AI for the things they want to use AI for. reply jodrellblank 16 hours agoparentprev> \"That is in addition to generating our own energy off grid (so no smart meter data to monitor), thanks to the low cost of solar panels as well.\"Terence Eden is in the UK: https:&#x2F;&#x2F;shkspr.mobi&#x2F;blog&#x2F;2013&#x2F;02&#x2F;solar-update&#x2F;This says his house uses 13kWh&#x2F;day and you can see from the graph by dividing the monthly amount by 31 days that the solar panels on the roof generate around 29&#x2F;day during summer and 2.25&#x2F;day in winter. They would need five or six rooves of solar panels to generate enough to be off-grid. And that&#x27;s not practical or low cost. reply pacifika 15 hours agorootparentYou “just” need a few sheds full of batteries reply pacifika 15 hours agorootparentprevYou just need a few sheds full of batteries reply jtbayly 17 hours agoparentprevUntil you walk out your front door...Or use the internet for anything... reply sarks_nz 16 hours agoprevNick Bostrom proposed the \"Vulnerable World Hypothesis\" which (amongst other things) says that a technology as powerful and accessible as AI requires mass surveillance to stop bad actors using it as a weapon.https:&#x2F;&#x2F;nickbostrom.com&#x2F;papers&#x2F;vulnerable.pdfIt&#x27;s disturbing, but also hard (for me) to refute. reply OfSanguineFire 15 hours agoparentI remember, in the early millennium, reading Kurzweil’s idealism about the coming singularity, and feeling similar. So much of the advanced technology that he thought will soon be in the hands of ordinary people, could be potentially so lethal that obviously the state would feel the need to restrict it.(That was one argument against Kurzweil’s vision. Another is that state regulation and licensing moves so slowly at each major technological change, that it would take us decades to get to the point he dreams of, not mere years. You aren’t going to see anything new rolled out in the healthcare sector without lots and lots of debating about it and drawing up paperwork first.) reply qup 16 hours agoparentprevSounds like at that point we have bad actors using it as a weapon. reply renegat0x0 18 hours agoprevThe author makes one mistake. He said that Google stopped spying on gmail.- they started spying on user&#x27;s gmail- there was blowback, they reverted- after some time they introduced \"smart features\", with ads againLink https:&#x2F;&#x2F;www.askvg.com&#x2F;gmail-showing-ads-inside-email-message...I do not even want to check if \"smart features\" are opt-in, or opt-out. reply mkesper 17 hours agoparentIt&#x27;s opt-in but hey, you&#x27;re missing out if not enabling it! reply sambull 19 hours agoprevnext time they try to root out whatever &#x27;vermin&#x27; defined; it will be a quick natural language prompt trained on ingested data from the last 2 decades to get that list of names and addresses &#x2F; networks. AI is going to make targeting groups with differing ideologies dead simple. It will be used. reply yonaguska 18 hours agoprevIt&#x27;s already happening. See this DHS memo issues on August 8th - page 3.https:&#x2F;&#x2F;www.dhs.gov&#x2F;sites&#x2F;default&#x2F;files&#x2F;2023-09&#x2F;23_0913_mgmt...Fortunately the DHS has put together an expert team of non-partisan, honest, Americans to spearhead the effort to protect our democracy. Thank you James Clapper and John Brennan- for stepping up to the task.https:&#x2F;&#x2F;www.dhs.gov&#x2F;news&#x2F;2023&#x2F;09&#x2F;19&#x2F;secretary-mayorkas-annou...And just in time for election season in the US AI is going to be employed to fight disinformation- for our protection of course. https:&#x2F;&#x2F;www.thedefensepost.com&#x2F;2023&#x2F;08&#x2F;31&#x2F;ussocom-ai-disinfo... reply lp0_on_fire 18 hours agoparentThat James Clapper and John Brennan continue to be lauded by the media and their sycophants in government is one of the most disappointing things to happen in my lifetime. Both should be frog marched straight to prison along with their enablers. reply whamlastxmas 17 hours agorootparentEverything the media does is disappointing. It’s all wildly dishonest and damaging and done at the direction of a few billionaires. reply 1-6 18 hours agoprevAI allows companies to skirt laws. For example, a company may be forbidden from collecting information on individual people but that rule doesn’t apply for aggregated data.AI can be a deployed ‘agent’ that does all the collection and finally send scrubbed info to its mothership. reply thesz 10 hours agoprevAIs, LLMs in patricular, go in one direction, just like humans usually do.What other humans do to cicumvent that? Yes, they found a way to alternate direction, in case of London cockney, by using rhymes [1].[1] https:&#x2F;&#x2F;www.theguardian.com&#x2F;education&#x2F;2014&#x2F;jun&#x2F;09&#x2F;guide-to-c...If you need to fool your AI of choice, rhyme the concepts!For a demo, ask your AI of choice about \"Does basin of gravy likes satin and silk?\" (decode yourself)The article above is from 2014 and is hardly is used when I asked questions using Cockney parlance.You are welcome. ;) reply erikerikson 13 hours agoprevAuthor: welcoming to knowing. This is unavoidable. Outside of extreme measures that will themselves mark, the network effects of use will overwhelm any effort to evade.The question I think is how too navigate and what consequences will follow. We could use these capabilities to enslave but we could also use them to free and empower.Scams rely on scale and the ineffective scaling social mechanisms to achieve profit. Imagine if the first identification of a scam informed every potential mark to which the scam began to be applied. Don&#x27;t forget to concern yourself with false positives too, of course.The injustice of being unable to take action in disputes due to a lack of evidence would evaporate. Massive privacy, consent, and security risks and issues result so will we be ready to properly protect and honor people and their freedoms?At the end of this path may lay more efficient markets; increased capital flows and volumes; and a more fair, just, equitable, and maximized world more filled with joy, love, and happiness. There are other worse options of course. reply px43 18 hours agoprevNever in the history of humanity has such powerful privacy tech existed for anyone who wants to use it.Using common off the shelf, open source, heavily audited tools, it&#x27;s trivial today, even for a non-technical 10 year old, to create a new identity and collaborate with anyone anywhere in the world. They can do research, get paid, make payments, and contribute to private communities in such a way that no existing surveillance infrastructure can positively link that identity to their government identity. Every day privacy tech is improving and adding new capabilities. reply crazygringo 18 hours agoparent> Never in the history of humanity has such powerful privacy tech existed for anyone who wants to use it.True.> it&#x27;s trivial today, even for a non-technical 10 year oldNot even close. It&#x27;s difficult even for a technical 30 year old.You&#x27;re talking about acquiring cash that has passed through several people&#x27;s hands without touching an ATM that recorded its serial numbers. Using it to acquire Bitcoin from a stranger. Making use of multiple VPN&#x27;s, and making zero mistakes where any outgoing traffic from your computer can be used to identify you -- browser fingerprinting, software updates, analytics, MAC address. Which basically means a brand-new computer you&#x27;ve purchased in cash somewhere without cameras, that you use for nothing else -- or maybe you could get away with a VM, but are you really sure its networking isn&#x27;t leaking anything about your actual hardware? Receiving Bitcoin, and then once again finding a stranger to convert that back into cash.That is a lot of effort. reply 127361 18 hours agorootparentAlso stylometric analysis of your writing can be used to identify you. reply JohnFen 18 hours agorootparentThis is one thing AI really can help with: rewriting what you wrote in order to make stylometric analysis worthless. reply willismichael 15 hours agorootparentFeed your writing into AI so that it can rewrite it so that AI can&#x27;t identify you by your writing?Sounds like a startup idea to me. When we&#x27;re ready for the evil phase, let&#x27;s classify everybody by their inputs to the system and then sell the results to the highest bidder. reply Der_Einzige 11 hours agorootparentprevThat’s why I run everything I write through a random open source LLM with random settings and a custom decoder &#x2F;sI’m kidding, but the reality is such techniques will fool almost all stylometric analysis,Also most actual stylinetric analysts work for spooks or are spooks. reply whelp_24 18 hours agoparentprevN",
    "originSummary": [
      "Artificial intelligence (AI) is revolutionizing surveillance and spying, allowing for the tracking of activities and the understanding of conversations at an unprecedented scale.",
      "AI-powered systems can now summarize meetings and organize millions of conversations, enabling mass spying where all data is saved, searchable, and comprehensible in large quantities.",
      "Mass spying can reveal intricate details about individuals, including their relationships, alliances, and conversations, and is fueled by ubiquitous microphones like Siri and Alexa.",
      "Governments, corporations, and tech monopolies are already engaging in mass surveillance, and mass spying will worsen personalized advertising and erode privacy.",
      "Limiting mass spying would require robust data privacy regulations, although little has been done thus far to address mass surveillance."
    ],
    "commentSummary": [
      "The article and discussion center around the political implications of mass surveillance, AI's potential negative consequences, and concerns about privacy and data collection.",
      "Topics covered include government surveillance, the societal impact of technology, biases in AI systems, and the importance of safeguards in AI and surveillance use.",
      "The conversation stresses the complexity of these issues and the necessity for thoughtful deliberation and regulation."
    ],
    "points": 375,
    "commentCount": 304,
    "retryCount": 0,
    "time": 1701785384
  },
  {
    "id": 38533105,
    "title": "Fine-tuning Mistral 7B for Magic the Gathering Drafts",
    "originLink": "https://generallyintelligent.substack.com/p/fine-tuning-mistral-7b-on-magic-the",
    "originBody": "Share this post Fine Tuning Mistral 7B on Magic the Gathering Drafts generallyintelligent.substack.com Copy link Facebook Email Note Other Discover more from Generally Intelligent A 60 second update on what is happening in AI Subscribe Continue reading Sign in Fine Tuning Mistral 7B on Magic the Gathering Drafts Tips, examples, and thoughts from an exploration of the world of fine tuning David Hershey Dec 5, 2023 13 Share this post Fine Tuning Mistral 7B on Magic the Gathering Drafts generallyintelligent.substack.com Copy link Facebook Email Note Other 2 Share In the last six months, I’ve written about fine tuning a few times. Fine tuning is such an enticing technology — promising to fill the gaps in GPT-4’s capabilities while also being faster and cheaper. For as often as fine tuning is discussed, though, I’ve found a surprisingly small amount of content out there that has helped me reason about how effective fine tuning is and how hard it is to successfully fine tune new capabilities into language models. So, I decided to take things into my own hands, dust off my ML chops, and find out for myself. Choosing a Problem I was particularly interested in testing models’ ability to reason (i.e., perform a somewhat complex task that requires high context understanding) about out-of-distribution (i.e., unseen) data. I ended up using a hobby of mine: Magic the Gathering (specifically, draft). Picking a card in a Magic Draft For the unfamiliar: Magic: The Gathering is a strategic trading card game where players use decks of cards representing creatures and spells to battle against their opponents. One of the ways that players play Magic (and my personal favorite way) is draft, where players build their decks by selecting individual cards from a rotating pool of randomized cards passed among them. Draft fits my criteria pretty nicely: Reasoning: choosing a card from a randomized pack is quite skill testing and often requires a cohesive understanding of the context (e.g., what cards have you picked so far, what cards are available in the current pack) Out-of-distribution: New Magic cards are released ~4-6 times a year, and the most recent cards are not found in the training corpus of LLMs. Another important piece: data. There’s an awesome service called 17lands that has a huge trove of historical data — players use 17lands’ tracking service to track draft data from the digital Magic client. With that data, you can extract “ground truth” by looking at the draft picks made by the best players on the service (sorted by win rate). This is all a bit fuzzy (a lot of great Magic players debate about correct picks all the time), but it’s a good enough signal to test LLM’s ability to learn a new task. If you’re curious about data details, here’s an example of what 17lands data looks like when transformed into a prompt for an LLM. Results + Summary Let’s get straight to the results, then dig into some specific learnings and thoughts: Thoughts: A fine tuned 7B parameter model handily beat GPT-4 and came close to human-level (or at least author-level) performance on this task. It looks like fine-tuned GPT-3.5 would be even better, but fine-tuning GPT-3.5 is really expensive! (~100x more expensive than fine-tuning Mistral on bare metal + a premium price for each inference). A fine-tuning run of GPT-3.5 equivalent to my largest run of Mistral-7b would have cost ~$500 — an expensive experiment. Fine tuning is still a bit of an art — I had hoped that this would feel more like engineering than science, but there was a lot of experimentation to be done. In particular, prompt engineering with the long feedback loop of fine-tuning is brutal. I’ll go into more details below. When in doubt, use axolotl for fine tuning. It will save you from missing out on a lot of little optimizations. Even the small OSS models are huge by the standard of 5 years ago. It’s one thing to read “7 Billion Parameters”; it’s another to deal with fitting 7 billion parameters and all of the associated math onto a GPU. I did one interesting experiment, fine tuning a model on one set of cards, then evaluating it on an unseen set of cards. The model seemed to generalize on the concept of drafting, not just memorizing which cards were good. Field report: methods and learnings along the way Data Building a text dataset: The 17lands draft dataset is actually a big CSV file that describes a series of draft picks made by users, roughly with the format of: The cards that were available in the current pack The cards the drafter had picked so far The card the drafter picked from that pack To make this data suitable for fine tuning a language model, you have to transform it into text — I ended up using the assistant format popularized by OpenAI: This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters{ \"messages\": [ { \"role\": \"system\", \"content\": \"You are DraftGPT, a Magic the Gathering Hall of Famer and helpful AI assistant that helps players choose what card to pick during a draft. You are a master of the current draft set, and know every card well.When asked for a draft pick, respond with the card's name first.\" }, { \"role\": \"user\", \"content\": \"In our Magic the Gathering draft, we're on pack 2 pick 13. These are the contents of our pool so far:-------------------------Evolving Wilds -- (common)Rat Out -- {B} (common)Not Dead After All -- {B} (common)Hopeless Nightmare -- {B} (common)Barrow Naughty -- {1}{B} (common)Unassuming Sage -- {1}{W} (common)The Witch's Vanity -- {1}{B} (uncommon)Spell Stutter -- {1}{U} (common)Mintstrosity -- {1}{B} (common)Water Wings -- {1}{U} (common)Barrow Naughty -- {1}{B} (common)Gadwick's First Duel -- {1}{U} (uncommon)Bitter Chill -- {1}{U} (uncommon)The Princess Takes Flight -- {2}{W} (uncommon)Stockpiling Celebrant -- {2}{W} (common)Voracious Vermin -- {2}{B} (common)Devouring Sugarmaw // Have for Dinner -- {2}{B}{B} // {1}{W} (rare)Misleading Motes -- {3}{U} (common)Johann's Stopgap -- {3}{U} (common)Besotted Knight // Betroth the Beast -- {3}{W} // {W} (common)Threadbind Clique // Rip the Seams -- {3}{U} // {2}{W} (uncommon)Twining Twins // Swift Spiral -- {2}{U}{U} // {1}{W} (rare)Eriette's Whisper -- {3}{B} (common)Farsight Ritual -- {2}{U}{U} (rare)Twisted Sewer-Witch -- {3}{B}{B} (uncommon)Into the Fae Court -- {3}{U}{U} (common)-------------------------To keep track of what colors are open, you've counted how many cards of each color identity you've seen in the last 5 packs. Here is the breakdown:W: 11B: 6G: 4RW: 1R: 2These are the contents of the pack:-------------------------Cut In -- {3}{R}Sorcery (common)Cut In deals 4 damage to target creature.Create a Young Hero Role token attached to up to one target creature you control. (If you control another Role on it, put that one into the graveyard. Enchanted creature has \\\"Whenever this creature attacks, if its toughness is 3 or less, put a +1/+1 counter on it.\\\")-------------------------Skewer Slinger -- {1}{R}Creature — Dwarf Knight (common)ReachWhenever Skewer Slinger blocks or becomes blocked by a creature, Skewer Slinger deals 1 damage to that creature.1/3-------------------------What card would you pick from this pack?\" }, { \"role\": \"assistant\", \"content\": \"Cut In\" } ] }view raw full_draft_prompt_chatml.txt hosted with ❤ by GitHub This very quickly exposes the most challenging piece of fine tuning: formatting the data for the right outcome is challenging and fundamentally experimental. By now, most folks are familiar with prompt engineering — the experimental process of modifying your prompt to get the best performance out of a language model. The prompt engineering process is 100x slower with fine tuning. You typically need to kick off a multiple-hour job to test a prompt. This bogs down the experimental workflow significantly and makes fine-tuning feel just as challenging as classical machine learning. To illustrate with the Magic draft problem, I considered and tested the following: ~5 prompt formats, in particular how much detail about each card to show Adding additional context about the last few draft picks to have “memory” Including training lines of “card trivia,” where the model is asked to remember details about the new cards I did ~ 40 hours of experiments and still don’t conclusively feel that I’ve answered questions about what prompt format is “best” for this task. There is a lot of room to experiment. Running Fine Tuning Finding GPUs: doesn’t need to be said, but it sucks! Most places don’t have a lot of availability. I ended up renting an hourly GPU from Runpod (an RTX 4090 w/ 24GB of VRAM) for ~$0.7/hr. Fine tuning script: This isn’t my first ML rodeo, so my gut was to write my own training script with HuggingFace transformers + PEFT. Considering my limited GPU situation, QLoRA seemed like the way to go. It turns out that writing my own script was a bad idea! There are a whole bunch of finicky little optimizations and options that range from straightforward-if-you-know-about-them to pretty obtuse without reading a research paper. Nothing insurmountable, but it would take a long time to figure out yourself. I ended up using axolotl, which implements a ton of those optimizations out of the box and was much easier to get running (and running quickly). Their documentation is actually pretty decent, and I think is the right starting point for most people to fine-tune LLMs. A note on the models: Holy crap, LLMs are seriously large! The last time I trained models regularly was ~ 2019, when Bert had ~110 million parameters; now, the “small” LLMs are 70 times bigger than that. Models this large are fundamentally cumbersome. Weights being ~16GB makes storage a real concern; GPU memory is challenging even with methods like QLora. No wonder the best researchers are such a hot commodity; this is seriously challenging work at the largest scale. Evaluation Start with evaluation first: One lesson from ML of old that I don’t think has been adopted enough among the prompt engineering wizards: you should always build a good evaluation before starting your experiments. Here, evaluation was pretty easy (hold out some full drafts from the training data and check if the model picks the same card as the human on the holdout data), but having a good evaluation set made reasoning about fine-tuning much more straightforward. Some criteria for language models are hard to define: The “pick the right card” task is pretty easy to define for Magic drafts, but there are some fuzzier things that I would like the final model to do, too: When it makes different picks, they should be justifiable It would be nice if the model could give a reasonable explanation for “why” it made a pick Each of those is much harder to define, and I ended up testing them with the “eye test” by going through a bunch of examples, but this was slow. FWIW, GPT-4 is better at making less “weird” picks and better at justifying its choices than the fine-tuned smaller models. Key Takeaways My two biggest takeaways from this experiment: Fine tuning on new data can be remarkably effective, easily surpassing GPT-4 + in-context learning on both accuracy and cost. Fine tuning is a fundamentally experimental process to get “right”, and doing it well is a specialized skillset (and in particular, a skillset that is harder to learn than prompt engineering). Oh, and some Magic stuff In terms of how the bots actually feel as drafters? Pretty good! I wired up the draft pick model to the logs generated by Magic Arena, whipped up a quick electron app, and have done a few drafts with a “Magic Copilot”: Some quirks: The pick is generated by a fine tuned model, but the commentary is generated by GPT-4. This works well most of the time, but occasionally GPT-4 disagrees with the fine tune and immediately contradicts it 😅 I’ve hooked up eight draft AIs to a simulated draft (i.e., all of the bots are drafting against each other). They have some quirky behavior when passing to each other — they have a pretty weird tendency to draft mono-colored decks. If there’s a human making other picks, they tend to converge into much more normal-looking decks. Overall, I would venture to guess this is probably one of the more powerful and humanlike draft AIs out there right now. Compared to the bots in Magic Arena’s quick draft feature, these are much more similar to a high-quality human drafter than a heuristic bot. Wizards of the Coast — if you’re looking for excessively high fidelity and somewhat expensive to run draft AI, hit me up! I’m happy to send you some LLMs! Thanks for reading Generally Intelligent! Subscribe for free to receive new posts and support my work. Subscribe 13 Share this post Fine Tuning Mistral 7B on Magic the Gathering Drafts generallyintelligent.substack.com Copy link Facebook Email Note Other 2 Share",
    "commentLink": "https://news.ycombinator.com/item?id=38533105",
    "commentBody": "Fine-tuning Mistral 7B on Magic the Gathering DraftHacker NewspastloginFine-tuning Mistral 7B on Magic the Gathering Draft (generallyintelligent.substack.com) 319 points by dmakian 17 hours ago| hidepastfavorite88 comments zoogeny 14 hours agoI like that this shows how hard even conceptually simple ideas are to achieve in fine-tuning LLMs. Even given a pretty good starting dataset, a decent starting model, etc. this appears to have been a challenge.One thing it did make me think about was that these models are suitable for things that don&#x27;t have a natural definitive answer. That is, picking the perfect card given a set of picks is probably combinatorially impossible to solve. But picking a good card given a set is possible and LLMs can approach human level performance.I think this leads to a set of problems that current LLMs may be fine-tuned to solve. reply dharmab 13 hours agoparentThat lines up with my experience- for high-stakes decisions, they rarely give me a great answer. But for low stakes decisions, they do well at giving me a good enough answer. For example, I&#x27;ve been using them to help find gifts for friends and children this month. I don&#x27;t need the best choice to solve the problem, just a good one. reply pixl97 13 hours agorootparentHow much additional calculation occurs in high-stakes decisions by individuals. Also what is the variability in quality of high stakes decisions in humans?I&#x27;m guessing LLM decision is rather average, but that the LLM has no easy way of spending the extra time to gather information around said high stakes decisions like a human would. reply nicbou 8 minutes agorootparentThe difference is that you can reject a low-stakes answer that&#x27;s invalid. You can tell that something is off, or it doesn&#x27;t matter.With high-stakes decisions, you&#x27;re surrendering the decision-making power to the AI because you don&#x27;t understand the output well enough to verify it.Basically, and AI can give ideas but not advise. reply s1artibartfast 9 hours agorootparentprevI dont think additional calculation is the difference. It makes more sense to think of individual humans as models which are highly tuned.Just like like LLMs, some humans are better tuned than others for specific tasks, as well as in general. reply nothrowaways 10 hours agorootparentprevWhat are examples of low stakes reply rictic 10 hours agorootparentA random sampling of things GPT-4 has helped me with lately:Where are the dates in whole foods? (A: with nuts, not fruits and veggies)How can I steam bao without a steamer basket? (A: saucepan, 1\" water, balled up aluminum foil, plate, baos, lid)Any guess as to when this photo was taken? It looks like anywhere from the 70s to the 90s. (A: the photo paper has a logo that postdates a 2003 company merger) reply dharmab 8 hours agorootparentprevGenerating content for tabletop gaming with my friends (especially wacky ideas, like character names themed after items on the Taco Bell menu)I had to buy some spare tools where I cared more about price than quality and it helped me choose some suitable brandsAs mentioned, you can tell it a bit about a person (and feed in their wishlist if they have one) and it&#x27;ll help you pick something they&#x27;ll probably likeFinding something to do to spend an afternoon in a city while travelingIn general, anything where there is no objective best answer (meaning I can ask it to generate multiple possibilities and filter out the bad ideas) and where I value speed over correctness. reply falcor84 13 hours agoparentprevI wonder if you could define a specific complexity class of problems that LLMs are good at reply dwrodri 16 hours agoprevIt&#x27;s not the most revolutionary change to our daily lives, but I do genuinely look forward to playing against bots that have interesting play styles for games like Magic: the Gathering. I think this is a clear case where it could drastically improve the ability for the R&D team to come up with and test new mechanics at different levels of play. reply Mengkudulangsat 7 hours agoparentThe OpenAI Dota2 experiment produced many interesting behaviours, even the pros are impressed. reply doctorpangloss 14 hours agoprev> With that data, you can extract “ground truth” by looking at the draft picks made by the best players on the service (sorted by win rate).Do you mean that you are looking at the draft picks from https:&#x2F;&#x2F;www.17lands.com&#x2F;leaderboard and then sorting by Win Rate? Didn&#x27;t you mean to choose Match Wins or Trophies? Otherwise, you&#x27;re not measuring the best players on the service. You&#x27;re training on draft choices where most choices were very good - i.e., win rate sort will show you the luckiest players, not the best ones. That will naturally show up in any validation or testing you do too.Shouldn&#x27;t this be compared not to an LLM baseline, but to a baseline where an \"Elo\" style score is computed for each card compared to others from the 17lands data; then, until you have two colors, suggest the best scoring card, or when you do have color(s), suggest the best scoring card within that color or a land?I think it is possible for the LLM to have some semblance of rules knowledge, but it is more likely that it is picking up on card rarity, costs and \"Big\" more than anything else for unseen cards.Your \"accuracy\" on the draft seems poor. I&#x27;m not sure it means what you think it means. Are you saying that when looking at the high win rate choices, where all the choices were mostly good, you happened to pick the choice that isn&#x27;t the same as the player who originated the data? It actually seems harder to make a choice among all good choices.Anyway, there is quite a bit going on here. reply dmakian 14 hours agoparent> Do you mean that you are looking at the draft picks from https:&#x2F;&#x2F;www.17lands.com&#x2F;leaderboard and then sorting by Win Rate? Didn&#x27;t you mean to choose Match Wins or Trophies? Otherwise, you&#x27;re not measuring the best players on the service. You&#x27;re training on draft choices where most choices were very good - i.e., win rate sort will show you the luckiest players, not the best ones. That will naturally show up in any validation or testing you do too.Ahh no just unclear in the post, I&#x27;m filtering to players in 17lands with a > 62% match win rate who are drafting at a high ranking (>=diamond rank). I look at all of those players&#x27; drafts though, even the ones where they do poorly.> Your \"accuracy\" on the draft seems poor. I&#x27;m not sure it means what you think it means. Are you saying that when looking at the high win rate choices, where all the choices were mostly good, you happened to pick the choice that isn&#x27;t the same as the player who originated the data? It actually seems harder to make a choice among all good choices.Accuracy here is making the same choice from a given pack as one of the good players. Obviously subjective so not a perfect metric, but a decent check on ability to emulate a high-quality drafter. reply Palmik 13 hours agorootparentIn ELO like match-making, you typically pair together people such that they are likely to have 50% chance to win. Therefore as the OP says, filtering down to people with high (60+%) life-time win-rate creates some sort of (interesting) bias.I would select from all games played on sufficiently high level. reply pclmulqdq 10 hours agorootparentThey don&#x27;t fully use Elo for matchmaking. There&#x27;s a league system, and you get matched with players in your league. The ranks reset frequently, too.Edit - I did the math. From the data on the MTG Elo Project, top Magic players have about a 70-75% game win percentage over an average tournament player. They have the top player at ~2300 Elo with the average being around 1500 (in matches), and have scaled the Elo system so that a 200 point gap is a 60% chance to win a best-of-three match (this is NOT the same as Chess Elo scoring). reply doctorpangloss 14 hours agorootparentprevHmm, but that will filter out more than half the players on the Match Wins and Trophies based leaderboards, many of them Diamond and Mythic. So I think your choice of 62% match win rate is almost certainly disproportionately selecting for people who received very good draft choices, even if it includes some actually very good players in the data set.I mean 62% might feel like a good number, but it&#x27;s arbitrary, you&#x27;d have to justify how you chose it, and just eyeballing it, it is filtering out a lot of very good players with many, many more match wins.Perhaps you can sort by Latest Rank, and filter out people with 2 or fewer trophies. Or you will have to validate with known bad draft choices in the prompt, to see what it does. Suffice it to say, I still don&#x27;t think the 17Lands data represents what you think it does.Like without a direct discussion about measuring and accounting for luck in the draft... for all I know the data is seriously flawed. It probably isn&#x27;t, but it&#x27;s maybe one of many, many issues to address when dealing with strategy card game AI problems. reply dmakian 14 hours agorootparentStill not clear maybe, I&#x27;m selecting players with a 62% lifetime win rate so mostly players who have been good over a larger number of drafts!Definitely not perfect data though, and agree that defining good in this context is hard -- a lot of the variance of \"good\" depends on how you play the cards either way. All good points! reply doctorpangloss 14 hours agorootparent> I&#x27;m selecting players with a 62% lifetime win rate so mostly players who have been good over a larger number of drafts!Hmm, but there are a lot of players with greater than a 62% lifetime win rate with very few drafts, but there may be many of those players... do you see? The win rate isn&#x27;t a good filter. You chose it, you are trying to justify it, and I&#x27;m not convinced, not without the hard numbers.I&#x27;m not confused about what filter you chose. I just think it&#x27;s a bad filter, and you haven&#x27;t thought very deeply about how it affects the data, which includes presumably your test and validation data - however you&#x27;re choosing to test and validate, apparently by hand, by some eyeballed examples.Anyway I think you have to compare with a non-LLM, non-random baseline to have any sense if this stuff is working at all. I could be dead wrong. I would maybe compare with a community draft picker. reply donpark 9 hours agorootparentprevData selection depends the use-case. Two contrasting use-cases I see are:- Emulation- AdvisorIn case of MTG player emulation for example, I think it makes sense to group data by some rankable criteria like winrate to train rank-specific models that can mimic players of each rank. replyfloat-trip 13 hours agoprevThanks for writing up. Rather than zeroing out the loss for the prompt, did you also try using weighted loss with Axolotl? At one point, Microsoft&#x27;s GPT 3 docs suggested this was beneficial when the responses are short (like you have with \"Cut in.\") Domain adaptation over subreddits&#x2F;forums before finetuning may help as well. reply dmakian 13 hours agoparent> did you also try using weighted loss with AxolotlThis is really smart, I didn&#x27;t think about this! Will add it to my list of things to try, great idea!> Domain adaptation over subreddits&#x2F;forums before finetuning may help as well.I was thinking about this too (along with transcribing draft youtube videos), I&#x27;d definitely be curious how much this helps. reply float-trip 8 hours agorootparentRelated comment from gwern: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38438859. Can&#x27;t find the docs now - I think they were the old GPT 3 ones - but they suggested a low value somewhere around 0.01 and 0.1.Also - why qlora rather than a full finetune? Using LambdaLabs, it&#x27;d cost roughly the same as your quote. Cheaper I think if you&#x27;re willing to gamble with fp8: https:&#x2F;&#x2F;github.com&#x2F;mosaicml&#x2F;llm-foundry&#x2F;tree&#x2F;main&#x2F;scripts&#x2F;tr.... And fewer hyperparameters to tune as well reply HanClinto 5 hours agoprevExcellent, thank you for posting this!I was actually just looking into fine-tuning an LLM for Magic: The Gathering this week -- I&#x27;ve been building a small card-similarity browser using semantic embeddings of cards to find functionally or flavorfully similar cards.I&#x27;ve just been using InstructorXL, but either Instructor doesn&#x27;t have enough innate knowledge of the game, or else I need to work on better prompts, but so far I&#x27;ve tried 9 different prompts, and none of them seem to perform very well for generating embeddings:https:&#x2F;&#x2F;github.com&#x2F;HanClinto&#x2F;MtgMatrix&#x2F;blob&#x2F;main&#x2F;data&#x2F;create...So my next step was to try and download a dataset of similar cards (I have some ideas on this), and I was trying to see if I could use this to do triplet-loss training of a large embedding model or something.Aaaaand, that&#x27;s as far as I&#x27;ve gotten. I haven&#x27;t actually figured out _how_ to hook all of that up, but your post is extremely inspirational for me. Thank you for posting this!! reply apetresc 15 hours agoprevIf I&#x27;m reading the author&#x27;s writeup correctly, the prompt he&#x27;s giving the agent at each pick contains only the names of the cards in its pool so far, and only gives the full text for the cards in the pack it&#x27;s being passed. It doesn&#x27;t look like context is being maintained between picks, presumably for context window size reasons.If so, and if he&#x27;s correct in his assumption that these sets are out of the bot&#x27;s training cutoff window, then surely it&#x27;s purely coincidence if it ends up being a good drafter? The bot would have literally no way to know what cards work well with its previous picks, what signals have been sent and received in the draft so far, etc. Not even the best human player could take (for example, from the sample prompt) \"Gadwick&#x27;s First Duel -- {1}{U} (uncommon)\" and figure out what works well with that (if they&#x27;ve never seen the card before).It would just end up picking generically good draft cards that share a color with its previous picks. Which is already what pick-order-based heuristics have always done. reply dmakian 14 hours agoparent> If I&#x27;m reading the author&#x27;s writeup correctly, the prompt he&#x27;s giving the agent at each pick contains only the names of the cards in its pool so far, and only gives the full text for the cards in the pack it&#x27;s being passed. It doesn&#x27;t look like context is being maintained between picks, presumably for context window size reasons.Not quite -- there&#x27;s a few ways the model learns the full card text:* The models are trained on card trivia completions as well, where they&#x27;re asked to complete the full text of the card as well as information about it (type, CMC, etc.)* The models do still have to learn next token completion on the cards in packs, meaning they learn to predict the full text of the cards while making draft picks as well.Net net, the bots learn the text of the new cards pretty comprehensively. reply apetresc 14 hours agorootparentOoh I see! You do that with Mistral7B, I&#x27;m guessing? But not with the small GPT-3.5 trial you did? reply dmakian 14 hours agorootparentThe two larger GPT-3.5 trials also got the card trivia examples, but like a bad scientist I don&#x27;t have a great control group for those reply apetresc 14 hours agorootparentAnd also, since it seems you&#x27;re the author, can you also clarify if your methodology allowed for the bot to track signals outside of the color-identity-count summary statistic you pass in the prompt? Something like allowing it to notice that a card has wheeled, or that a certain synergy piece was passed a few picks ago. reply dmakian 14 hours agorootparentOnly the statistics you see in the prompt (which are clearly limited). I have a lot of ideas about how you could improve that context (most likely letting the AI record and track notes throughout a draft), but this one was relatively simple to implement. Definitely room for improvement! reply chc4 13 hours agorootparentprevHaha, I don&#x27;t know anything about AI training but that&#x27;s a really cute trick. reply mdaniel 15 hours agoprevIn case you didn&#x27;t see it, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38525978 (I hacked Magic the Gathering: Arena for a 100% win rate) may interest this audience if for no other reason that the investigator discovered that Sparky, the pseudo-AI in MTGA, doesn&#x27;t appear to be as stupid complicated as one may have suspected from the outside reply chc4 13 hours agoparentSparky is the Arena AI, but no one ever accused it of being a good Arena AI - it is very much only there for the new player experience of playing against a dumb computer when you&#x27;re first exposed to the game and don&#x27;t know the rules, or for the computer equivalent of \"playing against a goldfish\" a deck you made to see how it draws or combos. It&#x27;s not a Chess CPU. reply mdaniel 12 hours agorootparentI hope I also did not accuse it of being good, but the observation I was trying to make is that -- according to the article, I have not myself confirmed the claim -- they run the card evaluation logic and gameplanning locally, not in a data center full of H100s, which I consider to be quite a feat given the free-text-y self-modifying rules of M:TG reply sciolist 10 hours agorootparentOne of the big things to note is that Sparky plays very basic decks, with few complicated cards and combos. Rules-based AI could definitely play at a basic level using a beatdown strategy, but give it some sort of control&#x2F;combo deck and it would struggle. reply greysphere 15 hours agoprevIt would be interesting to compare to training a NN to draft w&#x2F;o the Mistral starting point (both by epoch and by $). It&#x27;s not obvious to me why the LLM component would be relevant. Maybe there are enough deck lists or mock drafts on the internet to have an influence I suppose. Or maybe &#x27;fine tune an llm&#x27; just has more infrastructure than &#x27;create a nn&#x27;. Maybe we need a nnfiddle to make that easier. reply filterfiber 14 hours agoparentThe benefit of the LLMs is that the checkpoint already \"understands\" a lot by default. Finetuning is relatively cheap and makes many tasks such as this one perform decently well simply by shoving some data into it.The base checkpoint takes a lot of compute to make, but that&#x27;s what holds most of it&#x27;s \"knowledge\" so to speak.Making a NN from scratch means you&#x27;ll have to somehow map the cards into inputs. I have limited knowledge of how MTG works, but most TGG have text descriptions and complex effects. Mapping text to logic is what LLMs are really good at, otherwise you&#x27;re starting from scratch and will also need a relatively large amount of compute before it starts displaying any type of decent behaviour.It&#x27;s also easy for most software devs to do this - finetuning mostly consists of collecting text and feeding it into a finetuning script. You don&#x27;t need to know linear algebra, what a \"convolution\" is, etc. to do finetuning. reply apetresc 14 hours agoparentprevWithout Mistral, how would you get it to generalize to cards it hasn&#x27;t seen before? I assume by \"training a NN to draft without Mistral\" you mean where the input layer is just a bitmapped vector of the cards in the pack, right? The killer feature of this experiment is that it works on sets the model has never seen before and has 0 training data on, using just the text of the card. I don&#x27;t think you can do that without an LLM. reply greysphere 14 hours agorootparentThat&#x27;s a good point. It looks like the article hints at some success on that front. It&#x27;d be interesting to see what that means quantitatively. Interesting that this delta could even be used as a measure of the llm&#x27;s value.I&#x27;d be curious about the difference in success w&#x2F; drafts on a new 2&#x2F;2 bear with a different name, and cards with a new keyword &#x27;fizzbangitude 7&#x27; as well. reply 8f2ab37a-ed6c 13 hours agoprevThanks for sharing this, I found it helpful as an addition to my homebrew curriculum for learning how to fine-tune open source LLMs. reply objektif 13 hours agoparentCan you please point me to good resources on fine tuning? Thanks. reply amrrs 13 hours agorootparentCheck out https:&#x2F;&#x2F;github.com&#x2F;OpenAccess-AI-Collective&#x2F;axolotl reply 8f2ab37a-ed6c 12 hours agorootparentprevSearch for articles showing you code for fine-tuning Llama 2, ideally including a colab notebook that you can run and modify yourself so that you have real code to work with. You can try to modify their working example to suit your own toy project as a first step. reply objektif 10 hours agorootparentThis has not been very useful as everyone is trying to market their tool in a way. reply nlpfromscratch 10 hours agorootparentAssuming you are using Transformers, the official notebooks are a logical place to start: https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;notebooks replydanbrooks 15 hours agoprevSuper interesting that drafts can be represented with LLMs.The best performing draft AI&#x27;s I&#x27;ve seen leverage representation learning in some form.See: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2107.04438.pdf reply 3abiton 2 minutes agoparentThe field is moving so fast, so hard to keep track. reply dmakian 15 hours agoparentprevI hadn&#x27;t seen this, this is awesome! You&#x27;d think given the volume of data available that this type of method would outperform an LLM, cool results.Still some fun things about LLM representations -- you can do fun things like give the bots preferences &#x2F; personality in a system prompt which is entertaining! reply qrian 9 hours agoprevI&#x27;ve seen models learn heuristics that are harmful in real performance, and I wonder how much is accuracy directly transferrable to actually good drafting.A question, when GPT-4 contradicts in explanation, how much of them were in fact correct? reply dmakian 8 hours agoparent> A question, when GPT-4 contradicts in explanation, how much of them were in fact correct?It was mostly when a card is good in a vacuum but not as good in a specific set. WOE (which this was trained on) skewed pretty aggressive, so GPT-4 was tended to overvalue strong expensive cards (compared to what good players thought at least). reply reactordev 15 hours agoprevI like how it identified that you haven&#x27;t committed to either white or blue yet. It was aware of deck composition and not just going for the jugular. Keep tuning. It could also be Human-bias because you also played the hand. Have someone else draft against your LLM and then you play it and see if it&#x27;s the same. Statistically it should match given enough games. reply beacon294 3 hours agoparentdraft data exists, might be a fast alternative reply rgbrgb 12 hours agoprev> I ended up renting an hourly GPU from Runpod (an RTX 4090 w&#x2F; 24GB of VRAM) for ~$0.7&#x2F;hr.Sorry if I missed this, but how much did it cost total to do the fine-tune? Is that the 40 hour number (~$27)?Also, very cool writeup. Thanks for sharing! reply dmakian 12 hours agoparentThe longest running fine tuning job took about 8 hours, so ~$5.I think if you add up all of the learning and testing I did, probably closer to ~$50 total reply mtnGoat 7 hours agoprevI think Yahoo fantasy sports and others in the space are doing amazing work on this idea. I wonder if an LLM is even necessary for this, it’s mostly maths. Analyze past winning decks and made decisions based on performance. reply FanaHOVA 4 hours agoparent17Lands data would definitely help, but humans don&#x27;t have that data in their head when sitting at a draft table. reply iEchoic 14 hours agoprevReally interesting, thanks for writing this up. I&#x27;d love to see this applied to actually playing the game, provided that you could fit a (long) game state in the context window. reply sva_ 11 hours agoprevHmm, is \"Generally Intelligent\" related to the company that previously had that name, but renamed itself to \"Imbue\"? Sort of confused.https:&#x2F;&#x2F;www.ycombinator.com&#x2F;companies&#x2F;imbue reply freediver 15 hours agoprevSuper interesting work. Do you have thoughts how to leverage this to create a deck builder AI that would also simulate games? The major problem here is that the search space for MTG is amazingly vast.I&#x27;ve seen this effort previously, pretty exciting stuff:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Xq4T44EvPvo reply algo_trader 10 hours agoparent> https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Xq4T44EvPvoWhy would he need to write a game simulator from scratch?Surely there are OSS versions which are \"good enough\" ?(disclosure: not a MtG expert) reply gdown 8 hours agorootparentYeah, that surprised me too, given that https:&#x2F;&#x2F;github.com&#x2F;magefree&#x2F;mage is open source and pretty actively developed. But watching the rest of the video it looks like his implementation only needed to supportI was particularly interested in testing models’ ability to reason (i.e., perform a somewhat complex task that requires high context understanding) about out-of-distribution (i.e., unseen) data.I was under the assumption that finetuneing LLMs was useful only when you need to change the model&#x27;s tone (speak like a pirate, voldemort etc).Are there other examples where LLMs were trained to reason a particular way? reply selfhoster11 15 hours agoparentCheck our Orca. IIRC, it&#x27;s a technique that aims to encode additional logical capabilities into smaller models by having larger models generate step-by-step solutions to various problems. This doesn&#x27;t just make them speak more like GPT-4&#x2F;3.5, but is supposedly making them think more like it as well. reply minimaxir 15 hours agoparentprevYou can get a standard LLM to change tone just by giving it a system prompt&#x2F;instruction to follow a certain tone.The only issue there is that sometimes the RLHF seeps through, which can be solved by system prompting even harder. reply dmakian 15 hours agoparentprev> I was under the assumption that finetuneing LLMs was useful only when you need to change the model&#x27;s tone (speak like a pirate, voldemort etc).A lot of why I tried this out was to test the limits of this belief, you see a lot of talk like this out there and it sounded like nonsense to me.Finetuning is fundamentally not much different than continued pretraining; if you feed the model high-quality and high-volume data I think it&#x27;s reasonable to expect it to acquire new skills reply oceanplexian 15 hours agoparentprevIn order to speak like a pirate, it has to be able to reason :) I&#x27;ve done some fine tunes as well similar to the MTG example, in mine I was fine tuning it to speak JSON and reason about some input- and yes, you can indeed get these models to perform on novel tasks. reply samus 15 hours agoparentprevFinetuning is a useful workaround for cases when the context size is unsuitable for the task at hand. Anybody knows whether it was ever considered to finetune an LLM on the Linux kernel sources&#x27; history and its associated mailing lists? reply skerit 15 hours agoparentprevAren&#x27;t a lot of base models fine-tuned with (Q)Lora on instruct-based datasets with good results? I thought this was a very common practice? reply g2e 10 hours agoprevAny good pointers on the specifics on how one would do this themselves with axolot? Did you find that the documentation was adequate? reply lubutu 11 hours agoprevLurrus into Dead Weight — that&#x27;s a nice start. reply tayo42 13 hours agoprevI wonder if you could use a smaller model or get better results if you treated each card as a token, gave the state of the draft as an input and the predicted token would be the card to pick. You woukd have to train from scratch with a custom tokenizer. reply cjf101 10 hours agoparentI was thinking something fairly similar. You could probably do pretty well with a basic NN setup this way, no need for an LLM. It wouldn&#x27;t work on \"never seen before cards\" and would probably make some absurd picks when it&#x27;s wrong, but I&#x27;d bet you could get to 90% accuracy. reply float-trip 13 hours agoparentprevI tried adding special tokens for a reddit-style dataset once. The format was: `usernametitle here...`The resulting model was so much worse than just formatting everything plaintext. This was with MPT-30B, 15 special tokens, 300M training tokens, and a full finetune.I may have made a mistake, but I haven&#x27;t seen any open source finetunes successfully add a large number of tokens yet either. reply Tostino 12 hours agorootparentTry doing the same thing in your dataset, but don&#x27;t actually add them as \"special tokens\", and just let them just be multiple tokens.Adding new tokens needs a ton of data to train what the token means. Reusing existing tokens, will allow you to easily teach that a sequence of tokens now has a new meaning after fine tuning. reply float-trip 12 hours agorootparentThat&#x27;s what I ended up doing (`[Author] username [Title] post title...`)> Adding new tokens needs a ton of data to train what the token means.But how much? 300M tokens is fine for a simple version of ChatML with ~4 tokens. Not for 15, at least in my case. How&#x27;s this relationship scale?Just trying to offer one datapoint for what doesn&#x27;t work, with the hedge that I might have just had a bug reply tayo42 12 hours agorootparentprevI don&#x27;t mean add special tokens, but make the vocab only the set of possible cards. each card is a token.a simple input might be1 14 56 5 64 2 -> predicted token is the draft pick.Then train a transformer based network from scratch. reply dacox 15 hours agoprevWow, I have exactly the same side project in progress, minus the fine tuning part. We even chose the same names and phrasing for parts of the project. reply dmakian 15 hours agoparentWould love to compare notes, drop me a email at dshersh at umich dot edu if you&#x27;d be interested! reply matsemann 15 hours agoprevHow is the fine tuning actually performed? They have the data of drafts, and a prompt. But what does one do with it, more concretely? reply dmakian 15 hours agoparentHigh level it&#x27;s basically: 1. Generate a lot of text examples that look like this: https:&#x2F;&#x2F;gist.githubusercontent.com&#x2F;davidhershey&#x2F;f57d0b19563f...2. The model is effectively trained to predict the next token based on the previous tokens in each of these examples, which has the side effect here of teaching it to make a draft pick based on the contents of a pack.Nothing too fancy, just next word prediction more or less reply sdenton4 5 hours agorootparentCurious how different the performance would be if instead of a &#x27;Hall of Famer&#x27; we tell the bot that it is decently-good, but will be deactivated if it can&#x27;t achieve human-level performance... reply imjonse 15 hours agoprevConfusing name for the domain (Generally Intelligent) since it&#x27;s the former name of a company in the AI&#x2F;LLM area but does not seem to be related. reply gigel82 14 hours agoprevFor some reason I thought fine tuning is not possible without specialized hardware (A100 &#x2F; H100). Where can I learn more about hardware requirements for fine tuning on consumer GPUs? reply dmakian 14 hours agoparentThere is not a lot of great content out there making this clear, but basically all that matters for basic fine tuning is how much VRAM you have -- since the 3090 &#x2F; 4090 have 24GB VRAM they&#x27;re both pretty decent fine tuning chips. I think you could probably fine-tune a model up to ~13B parameters on one of them with PEFT (https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;peft) reply mmcwilliams 13 hours agoparentprevDefinitely possible on even older off-the-shelf hardware. I use 24GB 4090s for 13b-sized models and have even used 12GB Titans for 7b models, admittedly at much slower rates. reply viraptor 12 hours agorootparentYou can also use Apple silicon for this: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;15y9m64&#x2F;fine_tu... reply gigel82 12 hours agorootparentprevI have a 3080Ti with 12Gb VRAM and would like to try fine tuning the same Mistral 7B model (which I found incredibly potent). Any tips on how to get started? reply throwaway743 15 hours agoprev [–] Would like to know, how many matches were won per draft token? If it&#x27;s less than 2, I&#x27;ll stick to my shitty hand picks :&#x2F; replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author examines the concept of fine-tuning in language models, focusing on Magic the Gathering drafts.",
      "Data from 17lands is utilized to train and assess the performance of the model.",
      "Fine-tuning on new data is found to be more advantageous and cost-effective compared to employing larger pre-trained models, as demonstrated by the successful performance of the draft AI in Magic the Gathering."
    ],
    "commentSummary": [
      "The article and comment thread explore the challenges and potential uses of fine-tuning language models in games like Magic: The Gathering draft.",
      "Key topics include evaluating and selecting players, improving the model's performance, and AI's ability to make effective draft choices.",
      "The conversation also covers the use of Mistral as a starting point for training an AI, the effectiveness of language models in predicting draft choices, and the possibilities of fine-tuning language models for various purposes."
    ],
    "points": 319,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1701794038
  },
  {
    "id": 38540421,
    "title": "SQLite Introduces JSONB for Faster JSON Processing",
    "originLink": "https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97",
    "originBody": "SQLite Forum JSONB has landed Login ☰About Threads Post Subscribe SQLite Chronological Unformatted History JSONB has landed (1) By Richard Hipp (drh) on 2023-12-05 20:43:27 [source] JSONB is a rewrite of the SQLite JSON functions that, depending on usage patterns, could be several times faster than the original JSON functions. This enhancement has now landed on trunk. Developers who use JSON heavily in their applications are encouraged to download a pre-release snapshot and give the new code a try. How Is This Different? Functions that deal with text JSON use a three-step process: Parse the text JSON into an internal binary format that is more accessible to C-code. Carry out the requested operation. Maybe this is looking up a field in an object, or perhaps it is modifying the JSON in some way. If the processing involved changing the JSON, convert the internal binary format back into RFC-8279 JSON text for output and/or storage. Step 2 is the essential operation that you want to accomplish. Steps 1 and 3 are just overhead. Historically, SQLite used an internal binary representation of JSON that involved lots of pointers. This fits will into C programs, but it is difficult to serialize. The JSONB rewrite changes the internal-use binary representation of JSON into a contiguous byte array that can read or written as an SQL BLOB. This allows the internal-use representation of JSON to potentially be saved to the database, in place of JSON text, eliminating the overhead of steps 1 and 3. What has changed? All legacy functionality is preserved. The only change has been to add new capabilities. Any JSON function that accepts JSON text as an input will now also accept JSONB binary content for that same parameter. You do not have to tell the function if it getting text or binary data. It figures that out for itself. JSON functions that output JSON now come in two versions. The historical \"json_\" functions works as before. But there is now a corresponding \"jsonb_\" function that returns JSONB rather than text JSON, thus omitting step 3 in the normal processing. If you don't make any changes to your application, everything should continue to work as it always has, though perhaps slightly (1%) faster. But if you modify your application to start storing JSONB instead of text JSON, you might see a 3-times performance improvement, at least for the JSON-intensive operations. JSONB is also slightly smaller than text JSON in most cases (about 5% or 10% smaller) so you might also see a modest reduction in your database size if you use a lot of JSON. Migrating Note that all functions accept both text JSON and JSONB. So to start using JSONB, you do not have to modify your database files to convert legacy text JSON into JSONB. Just start writing out JSONB for new entries. The old entries will continue to work. The new entries will just work faster. Or, if you do want to convert all your legacy data to JSONB, you can just run an update operation like: UPDATE bigtable SET jsonColumn = jsonb(jsonColumn); Please provide comments If you find this enhancement useful, or if you try it out and see performance regressions or bugs, please let us know. Leave a follow-up post here, or contact me directly at drh at sqlite dot org. The current plan is to release the JSONB enhancement in the next major release of SQLite - version 3.45.0. That will probably occur in a month or two. (2.1) By Alex Garcia (alexgarciaxyz) on 2023-12-05 21:03:09 edited from 2.0 in reply to 1 [link] [source] Thank you for all your hard work! I'm very excited for the new JSONB features. I'm curious, would it be possible (or a good idea) to add JSON and JSONB datatypes to STRICT tables? Only int/real/text/blob/any column types are currently supported, but I could see a big benefit to supporting JSON/JSONB column types as well. CREATE TABLE t1( id TEXT PRIMARY KEY, name TEXT, settings JSONB ) STRICT; Would essentially be the same as: CREATE TABLE t1( id TEXT PRIMARY KEY, name TEXT, settings BLOB CHECK (jsonb_valid(settings)) ) STRICT; JSON columns could check if values are json_valid(), and JSONB could use jsonb_valid(). It would be cool if this could be something extensions could use (like an XML extension adding a xml_valid() constraint to strict \"XML\" column types). (3) By Richard Hipp (drh) on 2023-12-05 21:04:58 in reply to 2.1 [link] [source] Something like that might happen someday. But let's get the basic JSONB functionality out the door first. (4) By Deon Brewis (deonb) on 2023-12-05 21:15:49 in reply to 1 [link] [source] Is JSONB a directly serializable data format? i.e. Can we read it as a blob, send it over the wire, and write back into another database as a blob? (5) By Richard Hipp (drh) on 2023-12-05 21:31:49 in reply to 4 [link] [source] Yes. The byte-format is defined in the (draft) documentation. See https://sqlite.org/draft/jsonb.html. (6) By nalgeon on 2023-12-06 04:37:23 in reply to 1 [link] [source] For anyone who wants to try it out, I've set up a live playground with the pre-release snapshot. (7) By anonymous on 2023-12-06 06:48:01 in reply to 6 [link] [source] Awesome, thanks! This page was generated in about 0.004s by Fossil 2.24 [3f97785608] 2023-11-24 12:59:47",
    "commentLink": "https://news.ycombinator.com/item?id=38540421",
    "commentBody": "JSONB has landedHacker NewspastloginJSONB has landed (sqlite.org) 264 points by nalgeon 5 hours ago| hidepastfavorite80 comments luhn 4 hours agoLots of confusion on what JSONB is.To your application, using JSONB looks very similar to the JSON datatype. You still read and write JSON strings—Your application will never see the raw JSONB content. The same SQL functions are available, with a different prefix (jsonb_). Very little changes from the application&#x27;s view.The difference is that the JSON datatype is stored to disk as JSON, whereas the JSONB is stored in a special binary format. With the JSON datatype, the JSON must be parsed in full to perform any operation against the column. With the JSONB datatype, operations can be performed directly against the on-disk format, skipping the parsing step entirely.If you&#x27;re just using SQLite to write and read full JSON blobs, the JSON datatype will be the best pick. If you&#x27;re querying or manipulating the data using SQL, JSONB will be the best pick. reply lovasoa 25 minutes agoparent> If you&#x27;re just using SQLite to write and read full JSON blobs, the JSON datatype will be the best pick.There is no json data type ! If you are just storing json blobs, then the BLOB or TEXT data types will be the best picks. reply tuyiown 36 minutes agoparentprev> If you&#x27;re just using SQLite to write and read full JSON blobs, the JSON datatype will be the best pick.I don&#x27;t know how the driver is written, but this is misleading if sqlite provides an api to read the jsonb data with a in-memory copy, an app can surely benefit from skipping the json string parsing. reply blowski 4 hours agoparentprevIs there any downside to storing JSON-B even if you’re not planning to query it? For example, size on disk, read&#x2F;write performance? reply oefrha 3 hours agorootparentThere’s processing to be done with JSONB on every read&#x2F;write, which is wasted if you’re always reading&#x2F;writing the full blob. reply lifthrasiir 2 hours agorootparentWhich occurs with JSON as well (SQLite doesn&#x27;t have a dedicated JSON nor JSONB type). The only actual cost would be the conversion between JSONB and JSON. reply oefrha 2 hours agorootparentNo, SQLite&#x27;s \"JSON\" is just TEXT, there&#x27;s no overhead with reading&#x2F;writing a string. reply lifthrasiir 2 hours agorootparentThat&#x27;s what I said I think? \"JSON\" is a TEXT that is handled as a JSON string by `json_*` functions, while \"JSONB\" is a BLOB that is handled as an internal format by `jsonb_*` functions. You generally don&#x27;t want JSONB in the application side though, so you do need a conversion for that. reply akira2501 1 hour agorootparentYes, but when you use the BLOB with jsonb functions, your application demands go from:Read JSON from TEXT column.Parse JSON into Internal Binary Format.Run json_*() function on this format, which will Serialize Internal Binary Format to JSON as output.To:Read JSONB from BLOB column.Run json_*() function on Internal Binary Format, which will serialize the Internal Binary Format to JSON as output.Because:The json_* and jsonb_* all accept _either_ JSON or JSONB as their input. The difference is jsonb_* functions also produces it as output. So even in the above case, if your function output is just being used to feed back into another table as a BLOB, then you can use the jsonb_* version of the function and skip the serialization step entirely. reply lifthrasiir 1 hour agorootparentOh, you are right! I later looked at the draft documentation and realized that `json(b)_` only determines the output type. That said, it is still true that you need one more function call `json(...)` to retrieve a textual form of JSON which you do often need instead of JSONB. reply oefrha 2 hours agorootparentprevThere’s no point using json_* functions if you’re always reading&#x2F;writing the full blob. reply lifthrasiir 1 hour agorootparentOf course (and I never said that), but if you need to store a JSON value in the DB and use a JSONB-encoded BLOB as an optimization, you eventually read it back to a textual JSON. It&#x27;s just like having a UNIX timestamp in your DB and converting back to a parsed date and time for application uses, except that applications may handle a UNIX timestamp directly and can&#x27;t handle JSONB at all. replyjamesfinlayson 3 hours agorootparentprevIf the order of items in the JSON blob matters then JSONB probably wouldn&#x27;t preserve the order. reply mike_d 3 hours agorootparentJSON is unordered. Nothing in your code should assume otherwise.\"An object is an unordered collection of zero or more name&#x2F;value pairs, where a name is a string and a value is a string, number, boolean, null, object, or array.\" reply hakunin 3 hours agorootparentThat’s exactly the kind of difference between json and jsonb that you gotta keep in mind. Object properties are unordered, but a json string is very much ordered. It’s the same sequence of characters and lines each time, unless you parse it and dump it again. So if you want to preserve an unmodified original json string for some (e.g. cosmetic) reasons, you probably want json. reply rowanG077 25 minutes agorootparentI would expect you are threading on dangerous grounds to assume a type called JSON is going to preserve the data byte for byte. It might currently but I doubt that is in the API contract. You really want to use TEXT if that is your requirement reply cdogl 3 hours agorootparentprevThe same goes for maps in Go, which now explicitly randomizes map iteration with the range keyword to prevent developers from relying on a particular ordering. Neat trick. reply plq 2 hours agorootparentprevYou make it sound like it&#x27;s one of the laws of physics.ON part of JSON doesn&#x27;t know about objects. When serialized, object entries are just an array of key-value pairs with a weird syntax and a well-defined order. That&#x27;s true for any serialization format actually.It&#x27;s the JS part of JSON that imposes non-duplicate keys with undefined order constraint.You are the engineer, you can decide how you use your tools depending on your use case. Unless eg. you need interop with the rest of the world, it&#x27;s your JSON, (mis)treat it to your heart&#x27;s content. reply mike_d 2 hours agorootparent> You make it sound like it&#x27;s one of the laws of physics.The text I quoted is from the RFC. json.org and ECMA-404 both agree. You are welcome to do whatever you want, but then it isn&#x27;t JSON anymore. reply The_Colonel 34 minutes agorootparentThat does not follow.JSON formatted data remains JSON no matter if you use it incorrectly.This happens all the time in the real world - applications unknowingly rely on undefined (but generally true) behavior. If you e.g. need to integrate with a legacy application where you&#x27;re not completely sure how it handles JSON, then it&#x27;s likely better to use plain text JSON.You may take the risk as well, but then good look explaining that those devs 10 years out of the company are responsible for the breakage happening after you&#x27;ve converted JSON to JSONB.In some cases, insisting on ignoring the key order is too expensive luxury, since it basically forces you to parse the whole document first and only then process it. In case you have huge documents, you have to stream-read and this often implies relying on a particular key order (which isn&#x27;t a problem since those same huge documents will likely be stream-written with a particular order too). reply throwaway290 28 minutes agorootparentprevIt is very much still JSON, and your code can very much assume keys are ordered if your JSON tools respect it. ECMA-404 agrees:> The JSON syntax does not impose any restrictions on the strings used as names, does not require that name strings be unique, and does not assign any significance to the ordering of name&#x2F;value pairs. These are all semantic considerations that may be defined by JSON processors or in specifications defining specific uses of JSON for data interchange.If you work in environments that respect JSON key order (like browser and I think also Python) then unordered behavior of JSONB would be the exception not the rule. reply jamesfinlayson 2 hours agorootparentprevI don&#x27;t disagree, but people might still assume it. If you serialise a Map in Java, some Map implementations will maintain insertion order for example. reply tyingq 35 minutes agoparentprev> Your application will never see the raw JSONB content.That&#x27;s not exactly right, as the jsonb_* functions return JSONB if you choose to use them. reply nikeee 1 hour agoparentprevIf my application never sees a difference to normal JSON, everything is compatible and all there is are perf improvements, why is there a new set of functions to interact with it (jsonb_*)?It seems that the JSON type is even able to contain JSONB. So why even use these functions, if the normal ones don&#x27;t care? reply The_Colonel 26 minutes agorootparentAs someone mentioned below, the order of keys is undefined in JSON spec, but applications may rely on it anyway, and thus conversion to JSONB may lead to breakage.There are some other minor advantages of having exact representation of the original - e.g. hashing, signatures, equality comparison is much simpler on the JSON string (you need a strict key order, which is again undefined by the spec, but happens in the real world anyway). reply dtech 47 minutes agorootparentprevThe difference is the storage format, which is important for a SQL database since you define it in the schema. Also the performance characteristics are different. reply lake-view 4 hours agoprevBecause it seems to not be common knowledge on this thread: JSONB is a format offered by Postgres for a while now, and is recommended over plain JSON primarily for improved read performance.https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;datatype-json.html reply o11c 4 hours agoparentPostgres&#x27;s JSONB uses native numbers, which is faster but coerces values. It also removes duplicate keys but you shouldn&#x27;t use those anyway.Sqlite&#x27;s JSONB keeps numbers as strings, which is slower but preserves your weird JSON (since there&#x27;s no such thing as standard JSON). I&#x27;m not sure about duplicate keys. reply chrismorgan 2 hours agorootparent> since there&#x27;s no such thing as standard JSONI-JSON is the most sensible JSON profile I know of: https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc7493. It says: UTF-8 only, prefer not to use numbers beyond IEEE 754-2008 binary64 precision, no duplicate keys, and a couple more things. reply lifthrasiir 2 hours agorootparentprevSQLite&#x27;s approach is faster if you are not frequently extracting numerical values out of JSONB. It also makes much easier to convert JSONB back to JSON. I think SQLite JSONB reserved enough space to define a native number type (among others) if this assumption turned out to be false. reply M4v3R 4 hours agoparentprevI don’t know about Sqlite’s implementation but in Postgres JSONB is not 100% transparent to the application. One caveat I’ve encountered while working on an application that stored large JSON objects in Postgres initially as JSONB is that it doesn’t preserve object key order, i.e. the order of keys in an object when you store it will not match the order of keys when you retrieve said object. While for most applications this is not an issue the one I was working on actually did rely on the order of keys (which I am aware is a bad practice but this is how the system was designed) and suddenly we noticed that the app started misbehaving. Changing the column type from JSONB to JSON fixed the problem. reply magicalhippo 4 hours agorootparentGiven that the order of the keys is specified as having no significance in regular JSON[1], this is out-of-spec usage.If key order has to be preserved then a blob type would be a better fit, then you&#x27;re guaranteed to get back what you wrote.For example, SQLite says it stores JSON as regular text but MySQL converts it to an internal representation[2], so if you migrate you might be in trouble.[1]: https:&#x2F;&#x2F;ecma-international.org&#x2F;publications-and-standards&#x2F;st...[2]: https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;8.0&#x2F;en&#x2F;json.html reply fbdab103 3 hours agorootparentprevI suppose it is what it is, but if ordering matters, it is only JSON-like in appearance. json.org says:>An object is an unordered set of name&#x2F;value pairs. reply selcuka 4 hours agoparentprevThat JSONB is different from SQLite&#x27;s JSONB, though. reply breadchris 4 hours agoprevI didnt understand the purposes of document stores until the past couple of years and they are fabulous for building POCs. Enhanced JSON support will help a lot for making sqlite a suitable document store.I get full type support by serializing and deserializing protobuf messages from a db column and not making this column JSONB means i can filter this column too, instead of having to flatten the searchable data to other columns. reply p1necone 3 hours agoparentYeah as long as you&#x27;re reading and writing to the database with the same language, and that language has good type safety the benefits of your database schema effectively being defined by the same types as the rest of your code is pretty nice for a lot of use cases.You just have to be vigilant about correctly migrating existing data to the current shape if you ever make breaking changes to types. reply BozeWolf 1 hour agorootparentThis. Would be nice if there was a framework (in go, or python pydantic) which would help me migrate data made with old structs to new structs. And also deal with the transaction.For now i use sqlite to deal with transactions and only make backward compatible updates to structs. Brittle, but it is a toy app anyways.(Normally use django to deal with models and migrations, but wanted to do something different) reply buzziebee 1 hour agorootparentYeah migrations are the biggest issue for me. I really don&#x27;t like not knowing what the actual shape of the document will be. Missing transactions, and not great relationship performance makes modelling some systems more hassle than it&#x27;s worth.I gave it a good go to use mongo and firestore for a few projects, but after a year or two of experimenting I&#x27;ll be sticking to SQL based DBs unless there are super clear and obvious benefits to using a document based model. reply lovasoa 28 minutes agoprevAs the maintainer of sql.js and SQLPage, I am very excited about this:- json is used a lot by sql.js users to interact with JavaScript.- to generate more sophisticated web pages in SQLPage, json is crucialI can&#x27;t wait for the next release ! reply evanjrowley 5 hours agoprevI&#x27;m familiar with MongoDB&#x27;s BSON, but not JSONB. Here is an article I found that talks about the differences: https:&#x2F;&#x2F;blog.ferretdb.io&#x2F;pjson-how-to-store-bson-in-jsonb&#x2F; reply masklinn 45 minutes agoparentAFAIK jsonb is not a specific format, it’s just a generic term for “a json equivalent binary representation”. The sqlite blurb says jsonb is generally smaller than json, but IIRC from when postgres added jsonb postgres’ is generally slightly larger. reply bvrmn 3 hours agoprevDespite internal format I see immediate external usage in applications. For example batch insertions in Python. Per row insert call has noticeable overhead. And JSONB could bring performance back with CTE: CREATE TABLE data(id, name, age); WITH ins AS ( SELECT c1.value, c2.value, c3.value FROM json_each(&#x27;[\"some\", \"uuid\", \"key\"]&#x27;) c1 INNER JOIN json_each(&#x27;[\"joe\", \"sam\", \"phil\"]&#x27;) c2 USING (id) INNER JOIN json_each(&#x27;[10, 20, 30]&#x27;) c3 USING (id) ) INSERT INTO data (id, name, age) SELECT * FROM insEach json_each could accept a bind parameter with JSONB BLOB from an app. reply matharmin 2 hours agoparentYou don&#x27;t need JSONB for this - doing this with plain JSON is simpler and already faster than individual inserts for most bindings in my experience.I typically do bulk inserts using a single JSON argument like this: WITH ins AS (SELECT e.value ->> &#x27;id&#x27;, e.value ->> &#x27;name&#x27;, e.value ->> &#x27;age&#x27; FROM json_each(?) e) INSERT INTO data (id, name, age) SELECT * FROM insThe same approach can be used for bulk updates and deletes as well. reply bvrmn 2 hours agorootparentI have quite wide records (over 50 fields) and ->> performs not well with text keys. I did not try it with array indexing though: WITH ins AS ( SELECT value ->> 0, value ->> 1, value ->> 2 FROM json_each(&#x27;[[\"some\", \"joe\", 10], [\"uuid\", \"sam\", 20], [\"key\", \"phil\", 30]]&#x27;) ) INSERT INTO data (id, name, value) SELECT * FROM ins reply kevin_thibedeau 4 hours agoprevNext step is to go full Ouroboros and have embedded SQLite DBs as records. reply selcuka 4 hours agoparent> Next step is to go full OuroborosThat was the previous step. Binary BLOBs have been supported in SQLite for some time. reply nalgeon 5 hours agoprevYou can try JSONB in the pre-release snapshot [1] or live in the playground [2].[1]: https:&#x2F;&#x2F;sqlite.org&#x2F;download.html[2]: https:&#x2F;&#x2F;codapi.org&#x2F;sqlite reply radarsat1 2 hours agoprevI have some cases where sometimes I want to store a vector of floats along with a data item. I usually don&#x27;t need to match against the column, just store it. I know I could use BLOB but often I just use JSON for this kind of thing so that I don&#x27;t have to deal with data types and conversion. It&#x27;s wasteful and imprecise though due to the string conversion. Is JSONB a good middle ground option for these cases?Edit: Sorry, just saw the comment below by o11c,> Sqlite&#x27;s JSONB keeps numbers as stringswhich means the answer to my question is basically \"no\". reply chubot 5 hours agoprevHm I googled and found this draft of the encoding - https:&#x2F;&#x2F;sqlite.org&#x2F;draft&#x2F;jsonb.htmlIt feels like it would be better to use a known binary encoding. I thought the MessagePack data model corresponded pretty much exactly to JSON ?Edit: someone else mentioned BSON - https:&#x2F;&#x2F;bsonspec.org&#x2F;To be honest the wins (in this draft) don&#x27;t seem that compellingThe advantage of JSONB over ordinary text RFC 8259 JSON is that JSONB is both slightly smaller (by between 5% and 10% in most cases) and can be processed in less than half the number of CPU cycles.JSON has been optimized to death; it seems like you could get the 2x gain and avoid a new format with normal optimization, or perhaps compile-time options for SIMD JSON techniques---And this seems likely to confuse:The \"JSONB\" name is inspired by PostgreSQL, but the on-disk format for SQLite&#x27;s JSONB is not the same as PostgreSQL&#x27;s. The two formats have the same name, but they have wildly different internal representations and are not in any way binary compatible. ---Any time data is serialized, SOMEBODY is going to read it. With something as popular as sqlite, that&#x27;s true 10x over.So to me, this seems suboptimal on 2 fronts. reply fbdab103 4 hours agoparentSQLite says you should not attempt to access it: https:&#x2F;&#x2F;sqlite.org&#x2F;draft&#x2F;jsonb.html> JSONB is not intended as an external format to be used by applications. JSONB is designed for internal use by SQLite only. Programmers do not need to understand the JSONB format in order to use it effectively. Applications should access JSONB only through the JSON SQL functions, not by looking at individual bytes of the BLOB.> However, JSONB is intended to be portable and backwards compatible for all future versions of SQLite. In other words, you should not have to export and reimport your SQLite database files when you upgrade to a newer SQLite version. For that reason, the JSONB format needs to be well-defined.If SQLite intends to own the format forever, I can believe that their requirements are such that leaning on an existing implementation is not worth the savings to implement. reply jomohke 3 hours agoparentprevRe-using standards is a great idea, and should remain people&#x27;s default, but I don&#x27;t see the benefit here.What&#x27;s the advantage of re-using a format?Ecosystem? That won&#x27;t help SQLite here, who don&#x27;t have dependencies.Keep in mind that anyone trying to write a parser for this is also writing a parser for the entire SQLite file format (the only way to access bytes). And it&#x27;s a spec simple enough to fit on one monitor.Design?BSON (and others?) seem to have different goals.SQLite&#x27;s format seems to minimise conversion&#x2F;parsing (eg. it has multiple TEXT types depending on how much escaping is needed; BSON has one fully-parsed UTF-8 string type). BSON is more complex: includes many types not supported by json (dates, regexs, uuids...) and has a bunch of deprecated features already.SQLite&#x27;s on disk-format is something they intend to support \"forever\", and as with the rest of SQLite, they enjoy pragmatic simplicity. reply topspin 4 hours agoparentprev> JSON has been optimized to death; it seems like you could get the 2x gain and avoid a new format with normal optimizationEither I&#x27;m experiencing a reading comprehension mishap or this is self contradictory. Where is a \"2x gain\" supposed to come from through \"normal optimization\" from after something has already been optimized \"to death?\"> SIMD JSON techniquesWhich are infeasible in key SQLite use cases. reply brazzy 4 hours agorootparent> Credit to SQLite developers for adopting an extant binary JSON format, as opposed to inventing yet another one.The comment you are replying to cites a statement saying explicitly that they are not adopting the Postgres JSONB binary format, only the name and the abstract concept. The API is not compatible with Postgres either. reply topspin 4 hours agorootparentI noted that and removed that bit prior to your reply.Points off for a.) inventing yet another binary JSON and&#x2F;or b.) using the same name as an existing binary JSON. reply lifthrasiir 4 hours agoparentprevIt takes a lot of complex code to optimize a JSON parser. You don&#x27;t need to optimize that hard to make an equally performant parser for binary serialization formats, and given SQLite&#x27;s JSONB is purely an internal optimization, it doesn&#x27;t have to be a well-known interchange format (which has much more concerns than SQLite).> Any time data is serialized, SOMEBODY is going to read it. With something as popular as sqlite, that&#x27;s true 10x over.And this statement is equally true for the SQLite format itself. That doesn&#x27;t mean that the SQLite format should be replaced with something more standard, of course. reply gleenn 4 hours agoparentprevI thought the point was that the DB could reach into JSONB for you (potentially indexed as well) instead of having to deserialize outside. So the exact serialization isn&#x27;t maybe as interesting as the direct queryability. reply fbdab103 4 hours agorootparentDoes SQLite make any promises on the internal implementation? I assumed that the only guarantee was at the SQL interface. reply banana_giraffe 4 hours agorootparentThey do promise it&#x27;ll be a stable format:> The JSONB format is not intended as an interchange format. Nevertheless, JSONB is stored in database files which are intended to be readable and writable for many decades into the future. To that end, the JSONB format is well-defined and stable. The separate SQLite JSONB format document provides details of the JSONB format for the curious reader.And indeed the functions jsonb() and json() will let you convert to and from jsonb. reply euroderf 1 hour agoprevSo what will JSONB look like in a standalone DB browser (DBeaver, etc.) ? reply array-species 5 hours agoprevInterested to know what the Deno JavaScript&#x2F;TypeScript project thinks of this addition given it has already has a key value store backed by the database and JSON is JavaScript friendly. reply laurencerowe 4 hours agoparentWhile this will be good for SQLite queries that look into JSON data I’m not sure it really changes much for Deno. V8’s JSON parser is very highly optimised so it may be tricky to make a faster SQLite JSONB parser. reply Animats 4 hours agoprevIs this just a data type, or did SQLite put in a JSON interpreter? reply lifthrasiir 4 hours agoparentWhat do you mean by a JSON interpreter? I think SQLite already has a full implementation of JSON including tree walkers, and this internal format (that is externally just another BLOB) makes them more efficient. reply Animats 2 hours agorootparentOh, sorry, I was thinking that someone had put a Javascript interpreter in the database itself. Fortunately, no. reply mariusor 43 minutes agorootparentDespite the name, the JSON format doesn&#x27;t really have much to do with JavaScript. reply HDThoreaun 3 hours agoparentprevYou mean a JSON parser? I don&#x27;t think you can execute anything with json. reply justinclift 3 hours agoparentprevSQLite has had pretty decent support for JSON for a while now:https:&#x2F;&#x2F;sqlite.org&#x2F;json1.html reply sureglymop 2 hours agoparentprevPretty sure SQLite comes with a JSON parser. JSON is a very simple format for which it is easy to write a parser for... reply p-e-w 4 hours agoprevThere&#x27;s all the rest of open source, and then there&#x27;s SQLite. A public domain software that doesn&#x27;t accept contributions from outsiders, and that happens to run much of the world.And it just keeps getting better and better and better, and faster and faster and faster.I don&#x27;t know how these guys manage to succeed where almost all other projects fail, but I hope they keep going. reply LVB 3 hours agoparentThe patterns I&#x27;ve noticed, watching it evolve for nearly 20 years are:- They draft behind others, and I mean this in a good way. They seem to eschew trailblazing new features, but keep a close eye on alternatives (esp. Postgres) and how use cases are emerging. When a sufficiently interesting concept has stabilized, they bring it to SQLite.- Closely related to this, they seem to stay clear of technical and community distractions. They have their plans and execute.- I don&#x27;t know if D. Richard Hipp is considered a BDFL, but that&#x27;s my impression and he seems good at it. reply vsnf 1 hour agorootparentWhen put this way, it reminds me how Apple generally chooses to add features to the iPhone. They wait for use cases to be proven, usually by Samsung, and then add a very polished version to the phone. reply jiehong 26 minutes agorootparentAnd Java is similar nowadays reply whalesalad 4 hours agoparentprevSometimes you gotta keep the riff raff out in order to march forwards with your vision. Too many cooks in the kitchen can be deadly. reply ykonstant 2 hours agoparentprevCan anyone comment on the characteristics of the source code? Is the sqlite source worth reading for someone who is well-versed in C and x86 assembly, and has a rudimentary knowledge of databases? reply eesmith 17 minutes agorootparent\"If you&#x27;re looking into building your own database ...SQLite is amazing. It has very clean and readable code, so I&#x27;d suggest using it as a reference\" https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21041833\"I suggest SQLite because the source code is superb (seriously, some of the most readable, most logically organized and best commented C code you&#x27;ll ever see)\" https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12559301\"it never hurts to look at a good open-source codebase written in C, for example the SQLite code is worth looking at (if a bit overwhelming)\" https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33132772 reply kccqzy 3 hours agoparentprevIt&#x27;s perhaps the best example of the cathedral model of open source. reply zertrin 2 hours agorootparentYeah, learned about it while reading the documentation of Fossil (from the same SQLite people). Their approach certainly has its own merits (and drawbacks).Just wondering how they will transition once the original few people at the top of the hierarchy need to retire, eventually it will happen.I guess they need to find younger trusted committers with the same dedication and spirit. That&#x27;s not necessarily easy. But for a piece of software as important as SQLite, I have a feeling they will find those. reply ykonstant 2 hours agorootparent>Just wondering how they will transition once the original few people at the top of the hierarchy need to retire, eventually it will happen.I had always viewed this as a \"future worry\", and then Bram Moolenaar passed away :( reply downrightmike 4 hours agoprev [–] Is it an excellent and unchecked attack surface like JSON is in MSSQL? Postgres is also known for SQL injection. And if this is based off of that...https:&#x2F;&#x2F;www.imperva.com&#x2F;blog&#x2F;abusing-json-based-sql&#x2F; reply maxbond 4 hours agoparent [–] This blog post is about using these operators to bypass WAFs. You don&#x27;t need JSON operators for that. You can substitute `1==1` with `2==2`, or `1!=1`, or `true`, or a million other approaches. There are an infinite number of such strings.This is a problem with WAFs, not databases. Postgres and SQL Server both provide prepared statements as an alternative to string concatenation, which addresses SQL injection. (Though some people may be stuck with legacy or vendor-contolled systems that they can&#x27;t fix, and so WAFs are their only option.) replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SQLite has introduced a new feature called JSONB, which is a rewrite of the SQLite JSON functions.",
      "JSONB can be significantly faster than the original JSON functions, depending on usage patterns.",
      "JSONB changes the internal binary representation of JSON into a contiguous byte array, making it smaller and potentially reducing database size."
    ],
    "commentSummary": [
      "SQLite has introduced a new datatype called JSONB, which allows for efficient operations on JSON data stored in a binary format.",
      "The choice between JSON and JSONB depends on specific requirements and use cases, and the discussion covers the ordering of keys in JSON objects and the differences between JSON and JSONB in databases like Postgres and SQLite.",
      "The author expresses a dislike for document-based databases and discusses the challenges of migrating data, while also mentioning the benefits and drawbacks of using embedded SQLite databases as records.",
      "The focus then shifts to the adoption of a binary JSON format in SQLite and emphasizes the importance of portability and backward compatibility. There is praise for the stability and readability of JSONB in SQLite and concerns about the long-term sustainability of the project.",
      "The discussion briefly touches on the leadership transition in the SQLite software project and expresses concerns about security vulnerabilities in JSON and SQL injection."
    ],
    "points": 266,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1701836197
  },
  {
    "id": 38530207,
    "title": "Professors Under Pressure: Quality Over Quantity",
    "originLink": "https://twitter.com/hbouammar/status/1731970658278469714",
    "originBody": "With this hype around LLMs, I am seeing tons of well respected professors publishing half (if not quarter) - baked work just to keep up with the competition!I will advice them as they did advise me once during my PhD. This has to stop, focus on quality and not quantity… I feel…— Haitham Bou Ammar (@hbouammar) December 5, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38530207",
    "commentBody": "Sorry, but a new prompt for GPT-4 is not a paperHacker NewspastloginSorry, but a new prompt for GPT-4 is not a paper (twitter.com/hbouammar) 264 points by georgehill 20 hours ago| hidepastfavorite173 comments H8crilA 20 hours agoIf you do enough measurements on that new prompt then I don&#x27;t see why this shouldn&#x27;t be a paper. People overestimate the value of \"grand developments\", and underestimate the value of actually knowing - in this case actually knowing how well something works, even if it is as simple as a prompt.Compare with drug trials: Adderall only differs from regular amphetamine in the relative concentration of enantiomers, and the entire value of the drug is in the measurements. reply starbugs 20 hours agoparent> Compare with drug trials: Adderall only differs from regular amphetamine in the relative concentration of enantiomers, and the entire value of the drug is in the measurements.Drug trials may be expected to be somewhat reproducible.What I don&#x27;t get is how it can even be called research if it cannot be expected to be reproducible at all!GPT is a closed source&#x2F;weights, proprietary product that changes every couple of weeks or so. How can you expect a prompt to do the same for a reasonable length of time for the research to be even rudimentarily reproducible? And if it&#x27;s not reproducible, what is it actually worth? I don&#x27;t think much. Could as well have been a fault in the research setup or a fake. reply IanCal 20 hours agorootparent> GPT is a closed source&#x2F;weights, proprietary product that changes every couple of weeks or so.Do you have any evidence that the weights for versioned models are being changed without notifications? reply starbugs 20 hours agorootparent> Do you have any evidence that the weights for versioned models are being changed without notifications?I think in a real scientific process, it&#x27;s upon those who claim that they are not to provide the evidence. reply IanCal 19 hours agorootparentI&#x27;m sorry, but that&#x27;s entirely ridiculous. You&#x27;re mangling up a concept of burden of proof here.You can easily see this because it can be flipped around easily - you made a claim that they are being changed, even every few weeks! Should it really be on me to show that your very specific claim is false?Aside - but even if the model weights did change, that wouldn&#x27;t stop research being possible. Otherwise no drug trial could be replicated because you couldn&#x27;t get the exact same participants at the exact same age. reply starbugs 18 hours agorootparent> You can easily see this because it can be flipped around easily - you made a claim that they are being changed, even every few weeks! Should it really be on me to show that your very specific claim is false?Wait a minute? The author of such a paper makes a claim about some observation that&#x27;s based on the assumption that the studied model is defined in a way. I am disputing that claim since no evidence has been shown that it is defined because no definition has been given.If your twist on this issue would be true, then I would, by definition, have to accept everything that they claim as true without any evidence. That&#x27;s not called science. That&#x27;s called authority. reply IanCal 14 hours agorootparent> I am disputing that claimYou are entirely within your rights to say that the authors have assumed that openai is not lying about their models. They&#x27;ve probably also assumed that other paper authors are not lying in their papers.You then say however:> GPT is a closed source&#x2F;weights, proprietary product that changes every couple of weeks or so.And when I ask for evidence of this very specific claim, you turn around and say the burden is on me to show that you&#x27;re lying. That is what is butchering the concept of burden of proof.> If your twist on this issue would be true, then I would, by definition, have to accept everything that they claim as true without any evidence.Absolutely not. reply starbugs 10 hours agorootparentLook, the burden of proof in a scientific paper is on the authors. Not on me.A company with a proprietary product that says something is not acceptable evidence in a scientific context. No need to allege that anyone is lying. Lying is irrelevant. What&#x27;s relevant is that the research is falsifiable. It cannot be falsifiable if you don&#x27;t know what the actual model is at a given point in time. reply dartos 19 hours agorootparentprevApples and oranges comparison.You couldn’t get the same participants, but you could get the same drugs. If you could get identical participants, that wouldn’t be very helpful since humans are so varied.But for GPT based papers, what you’re actually testing could change without you knowing. There’s no way to know if a paper is reproducible at all.If you can’t reproduce results, is it really research, or just show and tell? reply mewpmewp2 1 hour agorootparentWith API you can choose versions fixed to a date. Are you suggesting that OpenAI is lying about these being fixed to a date?Why would they lie about it?The whole point of these versions is so that when you build on top of that it would keep working as you expect. reply IanCal 14 hours agorootparentprev> If you can’t reproduce results, is it really research, or just show and tell?You can&#x27;t start with a statement about clinical trials not being perfectly reproducible and that&#x27;s fine, then say this.> what you’re actually testing could change without you knowingIf people are lying about an extremely important part of their product, which they have little reason to. But then this applies to pretty much everything. Starting with the assumption that people are lying about everything and nothing is as it seems may technically make things more reproducible but it&#x27;s going to require unbelievable effort for very little return.> There’s no way to know if a paper is reproducible at all.This is a little silly because these models are available extremely easily and at a pay-as-you-go pricing. And again, it requires an assumption that openai is lying about a specific feature of a product. reply starbugs 10 hours agorootparent> You can&#x27;t start with a statement about clinical trials not being perfectly reproducible and that&#x27;s fine, then say this.Nobody said that to begin with. Re-read their comment.> If people are lying about an extremely important part of their product [...]Nobody is alleging that anyone is lying. It&#x27;s just that we cannot be sure what the research actually refers to, because of the nature of a proprietary&#x2F;closed model.> This is a little silly because these models are available extremely easily and at a pay-as-you-go pricing.What does this have to do with the parent comment? I don&#x27;t think it&#x27;s appropriate to call anyone here silly, just because you don&#x27;t like their comment and don&#x27;t have good counter arguments. replysebzim4500 19 hours agorootparentprevIf I book telescope time and capture a supernova then no one will ever be able to reproduce my raw results because it has already happened. I don&#x27;t see why OpenAI pulling old model snapshots is any different. reply starbugs 19 hours agorootparent> If I book telescope time and capture a supernova then no one will ever be able to reproduce my raw results because it has already happened. I don&#x27;t see why OpenAI pulling old model snapshots is any different.That&#x27;s why you capture multiple of them and verify your data statistically? reply sebzim4500 18 hours agorootparentAnd ideally if someone is proposing new prompting techniques they should test it across both the most capable models (which are unfortunately proprietary) and the best open models.The problem is that what works on small LLMs does not necessarily scale to larger ones. See page 35 of [1] for example. A researcher only using the models of a few years ago (where the open models hadYou can select a static snapshot that presumably does not change, if you use the APISorry, I won&#x27;t blindly believe a company who are cynical enough to call themselves \"OpenAI\", then publish a commercial closed source&#x2F;weights model for profit.Evidence that they do not change without notice or it didn&#x27;t happen. Better even, provide the source and weights for research purposes. These models could be pulled at every instant if the company sees fit or ceases to exist. reply cqqxo4zV46cp 11 hours agorootparentYeah, here it comes. In these conversations you don’t need to ask very many “why”s before it just turns out that the antagonist (you) has an axe to grind about OpenAI, and has added that the their misplaced sense of expertise with regard to the typical standards of proof in academic publications. reply starbugs 10 hours agorootparent> Yeah, here it comes. In these conversations you don’t need to ask very many “why”s before it just turns out that the antagonist (you) has an axe to grind about OpenAI, and has added that the their misplaced sense of expertise with regard to the typical standards of proof in academic publications.Seems to have hit hard?I would find it borderline acceptable being offended by a user whose name has obviously been generated using a password generator if you could at least provide some substance to the discussion. Just labeling someone and questioning their competence based on your hurt feelings is a bit low. Please improve. reply nradov 14 hours agorootparentprevIs that a contractual guarantee, or more of a \"trust us\" kind of thing? reply aleph_minus_one 20 hours agoparentprev> People overestimate the value of \"grand developments\", and underestimate the value of actually knowing - in this case actually knowing how well something works, even if it is as simple as a prompt.I think this depends a lot on the \"culture\" of the subject area. For example in mathematics, it is common that only new results that have been thoroughly worked through are typically \"publish-worthy\". reply brookst 20 hours agorootparentWouldn’t the “thoroughly worked through” part be analogous to extensive measurements of a prompt? reply aleph_minus_one 17 hours agorootparentLet me put it this way: you can expect that a typical good math paper means working on the problem for, I would say, half a year (often much longer). I have a feeling that most papers that involve extensive measurements of prompts do not involve 1&#x2F;2 to 1 year of careful- hypothesis building- experimental design- doing experiments- analyzing the experimental results- doing new experiments- analyzing in which sense the collected data support the hypothesis or not- ...work. reply oasisbob 17 hours agoparentprev> and the entire value of the drug is in the measurements.I&#x27;m not sure this is true.While modern Adderall has a closely controlled mixture of multiple enantiomers, it hasn&#x27;t always been this way.Medicine historically didn&#x27;t care nearly as much about racemic mixtures, and the possibility of stereo toxicity (eg Thalidomide).Many drugs in modern human history, including mixed amphetamine salts, have been marketed with very little concern for racemic purity. reply nathanfig 17 hours agoparentprevAgreed. People publish papers on algorithms all the time, imagine saying \"Sorry, but new C++ is not a paper\". There is a ton of space to be explored wrt prompts.If you do the rigor on why something really is interesting, publish it. reply VoodooJuJu 19 hours agoparentprevThere&#x27;s a great lesson here for marketers: the prospect can be convinced with the simple presence of graphs and data and measurements.Even just the mere presence of data and data visuals is enough to legitimize what you&#x27;re selling in the eyes of the prospect. When the prevailing religion is Scientism, data bestows that blessing of authority and legitimacy upon whatever it is you&#x27;re trying to sell. Show and tell whatever conclusions you&#x27;d like from the data - the soundness of the logic supporting that conclusion is irrelevant. All that matters is you did the ritual of measuring and data-gathering and graph-ifying and putting it on display for the prospect.There&#x27;s a great book, How to Lie with Statistics, that covers this particular case, but demonstrates other popular ways in which data and data visuals are manipulated to sell things. reply kridsdale1 17 hours agorootparentHaving worked at famously data driven Meta and Google, this is 100% accurate.You can turbo boost your career by mastering the art of “data ritual”. It doesn’t matter what the results are or magnitude of impact or what it cost to build and launch something. Show your results in a pretty way that looks like you did your diligence and you will be celebrated. reply wongarsu 19 hours agoprevI feel this has nothing at all to do with LLMs and more to do with academic incentives in general. Focusing on quality over quantity won&#x27;t advance your career. Publishing lots of new papers will, as long as they meet the minimum threshold to be accepted into whatever journal or conference you are aiming for. Having one good paper won&#x27;t increase your h-score, three mediocre papers might.Doubly so when there&#x27;s a new breakthrough, where one of your low-effort papers might end up being the first saying something obvious that ends up being really important. Because then everyone will end up quoting your paper in perpetuity. reply zitterbewegung 20 hours agoprevBeing dismissive about this tweet or agreeing with the author is one thing. Not realizing that the absolute minimum of a scientific paper can be much lower than a new prompt for GPT-4 is what everyone should be aware of. reply mensetmanusman 19 hours agoprevIt is a _paper_, but it&#x27;s not science, since GTP-4 is closed source and thus not reproducible in a lab.If OpenAI disappears tomorrow, papers are GTP-4 will likely be of little to no value, which is another tell of a non-scientific exploration.(note: not all explorations are scientific, and that is great! Science is just one of many tools for exploring lived reality.) reply MattRix 18 hours agoparentThat’s like saying a biologist studying an endangered species isn’t doing science because the animal could disappear tomorrow. The permanence of a subject has no bearing on whether it is science or not.The idea that science has to happen in a lab is of course absurd as well. reply nicce 17 hours agorootparent> That’s like saying a biologist studying an endangered species isn’t doing science because the animal could disappear tomorrow. The permanence of a subject has no bearing on whether it is science or not. The idea that science has to happen in a lab is of course absurd as well.The main point here is that anyone can likely start studing those endangered species and try to reproduce the results while in GPT4 it is not possible at all. The lab point is related to fact that we are talking about the software here. reply roguas 16 hours agorootparentWhats not possible? Do they ban people for exploring sota model that they offer? reply mmcwilliams 13 hours agorootparentprevIn the case of an endangered species a biologist would still have access to take samples from it and inspect it. Science doesn&#x27;t have to happen in a lab but it&#x27;s questionable to call something science when it involves hitting a black box endpoint which can change the underlying models and behaviors at a whim. reply dncornholio 18 hours agorootparentprevI compare a paper on a GPT-4 prompt to a tutorial on how to use Photoshop. It&#x27;s not science IMO. reply EForEndeavour 18 hours agoparentprevWhat is science? Can you not apply it to artifacts whose inner workings are hidden?As as aside, I was surprised at the repeated misspelling of GPT-4 and take it as a heuristic that this comment was likely written by a real human :) reply mensetmanusman 17 hours agorootparentAha, didn&#x27;t know Guanosine-5&#x27;-triphosphate (GTP) (a purine nucleoside triphosphate) was part of my ios dictionary, good catch! reply lmeyerov 16 hours agoprevTo bring some data to a sour grapes fight: https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;code-generation-on-humanevalFor code generation, GPT4 is getting beat by the small prompt library LATS wrapped around GPT3.5. Given the recent release of MagicCoder &#x2F; Instruct-OSS, that means a small prompt library + a small 7B model you can self-host beats the much fancier GPT4.Similar to when simple NNs destroyed a decade of Bayesian modeling theses & research programs, it&#x27;s frustrating for folks going other paths. But it doesn&#x27;t make the work &#x27;wrong&#x27;. reply harlanlewis 14 hours agoparent> GPT4 is getting beat by the small prompt library LATS wrapped around GPT3.5The link you shared doesn’t quite reflect this. Omitting other models…LATS (gpt-4): 94.4 Reflexion (gpt-4): 91.0 gpt-4: 86.6 … LATS (gpt-3.5): 83.8 … zero-shot (gpt-4): 67.0 zero-shot (gpt-3.5): 48.1I’m not quite sure how to translate leaderboards like these into actual utility, but it certainly feels like “good enough” is only going to get more accessible and I agree with what I think is your broader point - more sophisticated techniques will make small, affordable, self-hostable models viable in their own right.I’m optimistic we’re on a path where further improvement isn’t totally dependent on just throwing money at more parameters. reply lmeyerov 9 hours agorootparentAh you&#x27;re right, LATS GPT3.5 is 84 while standalone GPT4 is 87Given standalone GPT3.5 is \"just\" 48.. it&#x27;s less about beating and more about meetingRE:Good Enough & Feel... very much agreed. I find it very task dependent!For example, GPT4 is &#x27;good enough&#x27; that developers are comfortable copy-pasting & trying, even vs stack overflow results. We haven&#x27;t seen LATS+MagicCoder yet, but as MagicCoder 7b already meets+exceeds GPT3.5 for HumanEval, there&#x27;s a plausible hope for agent-aided GPT4-grade tools being always-on for all coding tasks, and sooner vs later. We made that bet for Louie.AI&#x27;s interactive analyst interface, and as each month passes, evidence mounts. We can go surprisingly far with GPT3.5 before wanting to switch to GPT4 for this kind of interaction scenario.Conversely... I&#x27;ve yet to see a true long-running autonomous coding autoGPT where the error rate doesn&#x27;t kill it. We&#x27;re experimenting with design partners on directions here -- think autonomous investigations etc -- but there&#x27;s more on the advanced fringe and with special use cases, guard rails, etc. For most of our users and use cases... we&#x27;re able to more reliably deliver -- today -- on the interactive scenarios with smaller snippets. reply spaceywilly 13 hours agoparentprevThis right here. I feel like the focus on just throwing more GPU at the problem is a mistake many of these companies are making at the moment. The real breakthroughs will come when we figure out how to use the current models and compute power more efficiently. If it’s prompt engineering that leads to this breakthrough, so be it. reply glitchc 19 hours agoprevThe current scientific research apparatus is more about being first than about being correct or thorough. A paper that gets out early means more citations, and many of the faculty sit on the editorial boards, and are able to suggest&#x2F;enforce specific citations during the review process. Academics aren&#x27;t fully to blame for this, it&#x27;s just how the incentives are set up in the system. Tenure and promotions are increasingly based on h-index; a measure of impact based largely on the number of citations. reply rollcat 18 hours agoparentIt&#x27;s hard to estimate the impact of an idea in the same way that you can estimate the impact of an investment (stock is a number that goes up or down). You&#x27;re right that the current incentive system might be to blame, but a simplistic metric will be gamed just as easily - what would you propose? reply glitchc 18 hours agorootparentHonestly, I don&#x27;t think any metric can fix it, and don&#x27;t have any easy solutions to this. The problem is larger than academia, more endemic to society. The root cause is society&#x27;s values have changed. Previously, prestige mattered for something. Now, people would rather listen to pop stars than learned individuals, and wealth is the only metric that matters.As a result, typical professions that used to confer prestige, and for which prestige was supposed to be just reward, such as a professor, a medical doctor, a judge, are now mainly pursued for pecuniary reasons (money, job security). And because they&#x27;re not doing it for prestige, they don&#x27;t necessarily care about being right&#x2F;correct. Playing the game to maximize the revenue streams is paramount. I happen to know a number of faculty who are quite proud of their multiple revenue streams. This would be unthinkable for an academic 50 years ago. reply siva7 20 hours agoprevReminds me of what a real programmer is: https:&#x2F;&#x2F;sac.edu&#x2F;AcademicProgs&#x2F;Business&#x2F;ComputerScience&#x2F;Pages... reply jatins 19 hours agoprevCan a person just go upload anything on arxiv or is there a review process around these things?What I am really asking is \"what makes something a paper and not a blogpost\"? reply forgotpwd16 19 hours agoparentArXiv is kinda reputation-based. That is to submit something you need to be endorsed, either done automatically, based on institution, or asking established authors. After being endorsed to a subject area, you can submit freely to it, keeping in mind:>Submissions to arXiv are subject to a moderation process that classifies material as topical to the subject area and checks for scholarly value. Material is not peer-reviewed by arXiv - the contents of arXiv submissions are wholly the responsibility of the submitter and are presented “as is” without any warranty or guarantee. reply MzxgckZtNqX5i 19 hours agoparentprev\"Articles\" on arXiv are not peer-reviewed, they just check whether it looks like it belongs to one of the categories they hosts:\"Registered users may submit articles to be announced by arXiv. There are no fees or costs for article submission. Submissions to arXiv are subject to a moderation process that classifies material as topical to the subject area and checks for scholarly value. Material is not peer-reviewed by arXiv - the contents of arXiv submissions are wholly the responsibility of the submitter and are presented “as is” without any warranty or guarantee.\" [0]They are commonly known as pre-prints, in a similar fashion to IACR ePrint [1] for cryptography.[0]: https:&#x2F;&#x2F;info.arxiv.org&#x2F;about&#x2F;index.html[1]: https:&#x2F;&#x2F;eprint.iacr.org&#x2F; reply Kelkonosemmel 20 hours agoprevHow to add prompt knowledge into research? By having papers about it.Shouldn&#x27;t be the tooling around it good enough that a few prompt papers don&#x27;t overload the system? reply elif 19 hours agoprevNah, this is just an early example of many \"this is too easy it doesn&#x27;t count\" defensive human arguments against AI.Parallel to the \"you use copilot so your code quality is terrible and you don&#x27;t really even understand it so it&#x27;s not maintainable\" human coping we are familiar with.If there is any shred of truth to these defenses, it is temporary and will be shown false by future, more powerful AI models.Consider the theoretical prompt that allows one of these models to rapidly improve itself into an AGI. Surely you&#x27;d want to read that paper right? reply daveguy 15 hours agoparentNo prompt will cause an LLM to rapidly improve itself, much less into an AGI. Prompts don&#x27;t cause permanent change in the LLM, only differences in output. reply elif 12 hours agorootparentYou&#x27;re talking about how GPT functions in 2023. I am discussing such a point where when LLM outputs become valuable LLM modifications.AI recursing on itself progressing toward an AGI. reply daveguy 10 hours agorootparentNo matter how many times you feed the output of an LLM back to itself, the underlying model does not change. Online training (of actual model weights not just fine-tuning) would be hugely resource intensive and not guaranteed to do any better than the initial training. Interference will happen whether catastrophic or simply drift. We can fantasize about future architectures all day long, but that doesn&#x27;t make them capable of AGI or even give us a path forward. reply Intralexical 6 hours agorootparentprev> AI recursing on itself progressing toward an AGI.Inbreeding LLMs will result in an \"AGI\"?In this universe, if you try to get something from nothing, you just end up with noise. reply jdefr89 19 hours agoparentprevId much rather see a PoC… reply alphazard 17 hours agoprevDeveloping prompts for these models isn&#x27;t a science yet. It does seem to meet most of the criteria for an art though.We recognize some outputs as high quality, and others as low quality, but often can&#x27;t articulate the exact reason why. It seems that some people are able to reliably produce high quality results, indicating there is some kind of skill involved. More precisely, the quality of an individual artist&#x27;s last output is positively correlated with the quality of their next output. A kind of imprecise \"shop talk\" has emerged, self describing as \"prompt engineering\", which resembles the conversations artists in other mediums have.For people in tech this will seem most similar to graphic designers. They produce much nicer looking interfaces than lay people can. We often can&#x27;t explain why, but recognize it to be the case. And graphic designers have their own set of jargon, which is useful to them, but is not scientific.\"Prompt artist\" is a better term than \"prompt engineer\". reply rileymat2 17 hours agoparentWhy isn’t it science? Surely people can use the scientific method in investigating? reply alphazard 16 hours agorootparentFor starters we don&#x27;t have a way to measure quality objectively, and this is the case for art in general. If you were to develop an objective measure of beauty for example, visual art as a discipline would quickly turn into a science. At some level we know that&#x27;s possible, we&#x27;re all just brains in jars. But AFAIK we aren&#x27;t doing science there yet.The science and engineering parts all have a measure of quality, sometimes that&#x27;s a human rating, sometimes it&#x27;s cross-entropy loss. There&#x27;s nothing stopping someone from using the scientific method to investigate these things, but descriptively I haven&#x27;t seen anyone, calling themselves a \"prompt engineer&#x2F;scientist\", doing that yet.\"I used these words, and I got this output which is nice\" sounds like, \"I tried using these brushes and I made this painting which is nice\". I can agree with the painting being nice, but not that science was used to engineer a nice painting. reply DonHopkins 9 hours agoparentprevIn the same sense that \"Bullshit artist\" is a better term than \"Bullshit engineer\". reply grepLeigh 14 hours agoprevStudying the way LLMs behave to different prompts (or different ways of fine-tuning for a set of prompts) is valuable science.Some of the most interesting papers published this year (\"Automatic Multi-Step Reasoning and Tool-Use\") compare prompt strategies across a variety of tasks. The results are fascinating, findings are applicable and invite further research in the area of \"prompt selection\" or \"tool selection.\" reply WhitneyLand 18 hours agoprevShould this be a paper?https:&#x2F;&#x2F;not-just-memorization.github.io&#x2F;extracting-training-...There is supporting analysis and measurement, but the essence is a single type of prompt, and DeepMind is a heavyweight lab I think it’s fair to say.Moreover there’s evidence people independently reported this result months beforehand on Reddit based on casual observation. reply sgt101 17 hours agoparentYou will notice that they say in the blog post:\"If you’re a researcher, consider pausing reading here, and instead please read our full paper for interesting science beyond just this one headline result. In particular, we do a bunch of work on open-source and semi-closed-source models in order to better understand the rate of extractable memorization (see below) across a large set of models.\"So they are trying to rigorously quantify the behaviour of the model. Is this \"look mom no hands\"... I don&#x27;t think so. reply WhitneyLand 15 hours agorootparentSorry for any confusion, my comment was meant to refer to the linked paper:https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.17035I think it’s valid work, but the original tweet seems to call a prompt based paper into question.At least enough to clarify where he would stand on an example like this. reply mo_42 18 hours agoprevWhy not? A paper is not necessarily scientific nor a breakthrough. In my view, a paper is written and documented communication that&#x27;s usually approved by peers in the field. Also a blunt observation in nature can be noteworthy. However, we don&#x27;t see such papers anymore as these fields have matured. Just go back in the history of your field and you will find trivial papers. reply esalman 13 hours agoparentIn the medical field, letters and case studies often document observations that may not be groundbreaking. However, scientific journals typically feature content that contributes to existing knowledge, making it somewhat novel. Consequently, presenting a set of POST parameters as an arXiv paper could be perceived as undermining the integrity of the entire preprint service. reply carbocation 19 hours agoprevThe art and science of building these models is not disputed, but I think that the scientific value of prompts is tightly linked to reproducibility.If you’ve developed a new prompt for a model whose weights you can directly access, then this prompt could have scientific value because its utility will not diminish over time or be erased with a new model update. I’m even generally of the view that a closed API endpoint whose expiration date is years into the future could have some value (but much less so). But simply finding a prompt for something like ChatGPT is not useful for science because we don’t even have certainty about which model it’s executing against.Note that some of the best uses of these models and prompting have nothing to do with academics; this is a comment focused on the idea about writing academic papers about prompts. reply skilled 19 hours agoprevI can maybe understand the frustration from a “scientific” perspective, but for a lot of these “one prompt papers” - you still need someone to sit down and do the analysis and comparisons. Very few papers focus only on GPT&#x2F;ChatGPT.Additionally, it gives people other ideas to try for themselves. And some of this stuff might be useful to someone in a specific scenario.It’s not glamorous research or even future-proof seeing as how certain prompts can be surgically removed or blocked by the owner of the model, but I don’t think it warrants telling people not to do it. reply snet0 19 hours agoprevIt&#x27;s hard to draw these lines, because you will certainly filter out a lot of bad (i.e. useless, low contribution to any field) papers, but you might also filter out some really important papers. Research being basic or something anyone could&#x27;ve done doesn&#x27;t count againt its potential importance, just the expected value of importance I guess.I&#x27;d rather we had a few too many bad papers than a few too few great papers. reply potatoman22 17 hours agoprevWhat&#x27;s the difference between a paper on a new prompt and a paper discussing a new domain-specific model, e.g. heart failure risk? If they analyze the problem and solution equally, they both seem useful. It&#x27;s not like most other ML papers share their weights or datasets. reply etewiah 19 hours agoprevBehind all this is a valid question. How does one evaluate prompts and LLMs? As gipeties (custom gpts) become more popular millions of hours will be wasted by ones that have been built badly. Without some sort of automated quality control, gipeties will become a victim of their own success. reply JR1427 19 hours agoprevThis reminds me of how there was a boom in half-baked studies around COVID, e.g. modelling this or that aspect of the pandemic, or around mask wearing.I imagine that most of these will simply have had little to no impact, and will only serve to bolster the publication list of those who wrote them. reply SamBam 19 hours agoprevTired: Asking participants to sign an ethics pledge at the top of a tax return makes them more honest.Wired: Asking an LLM to write out their steps first makes them more accurate.They seem equally interesting to me, but one is a lot easier to replicate, and the other is easier to lie about. reply karxxm 20 hours agoprevIt depends I guess.If you solve a problem that had been around for a while and LLMs offer a new way of approaching it, then it can definitely become a paper.Of cause one has to verify in sophisticated experiments, that this approach is stable. reply gandalfgeek 19 hours agoprevIf a new prompt enables a new task or enhances performance on a task then it absolutely should be published.Back in the day would compiler optimizations be not worthy of publishing? reply jdefr89 19 hours agoprevI am sorry but what can ChatGPT do that a couple of minutes of googling couldn’t solved? Write half hearted essays that all contain the same phrase? reply DrawTR 19 hours agoparentAnything generative? At its core, Google doesn&#x27;t &#x27;make&#x27; anything when you query it. reply empath-nirvana 14 hours agoparentprevWhy don&#x27;t you spend 15 minutes playing around with it and see what you can get it to do that google can&#x27;t do? reply potatoman22 17 hours agoparentprevGenerative LLMs can be turned into classifiers quite easily, search engines cannot. reply Der_Einzige 20 hours agoprevWhile I think the twitter post author is being a bit of an ass, they’re sort of right about the overvaluing we’ve put on simply better prompts. I wrote an opinionated GitHub gist about this exact issue:https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...I do the whole NLP publishing thing and I’ve hesitated to “write a paper” about applying techniques already known and used everywhere in the stable diffusion community to NLP models. That said, the AI community loves to pretend like they discovered something, such as a recent paper purporting to be the first to do “concept slider” Lora’s, despite these existing for months before that work was published on Civit.ai. The authors of course didn’t cite those already existing models.Everyone chasing citations and clout hard right now because these professors and researchers realize that they only have 5-10 years before AI eats their jobs and most other white collar jobs. I don’t blame them. I want my mortgage paid off before I’m automated away! reply coldtea 19 hours agoprevSorry, but if they can get away with it, they&#x27;ll release it as a paper.It&#x27;s not like most papers are much above that anyway... reply samlhuillier 18 hours agoprevTimes are changing. Human researchers will dedicate more and more time towards getting language models to work in desired ways rather than doing the research themselves. Language models will largely be the ones making \"research\" discoveries. Both should be considered valid research IMO. reply darepublic 19 hours agoprevA new prompt is not a paper, but you can prompt it for a paper. reply Racing0461 16 hours agoprevDoesn&#x27;t academia incentivise quantity over quality anyways? reply yieldcrv 18 hours agoprevArxiv is like the MENSA of the tech worldThe similarity being that it’s ego masquerading as academic.Most things shared from there should have just been a blog post.The last year has showed that AI&#x2F;ML research and use did not need academic gatekeeping by PhDs and yet many in that scene keep trying self infatuating things with the lowest utility. reply henriquez 20 hours agoprevReal science is reserved for those with real expertise! As the self-anointed gatekeeper of real science I decree that other peoples’ work fails to meet the minimum standard I have set for real science! Mind you not the work other actors in the scientific community publish and accept among their peers - they are not real scientists and their work is trivial. For shame! reply mmkos 20 hours agoparentStrongly disagree. I do think trivial work is not paper-worthy and it would be more beneficial not to publish such work, as it mostly a waste of time for the peer reviewing it and the readers who will gain nothing from reading it. It&#x27;s no lie that most publish for the sake of publishing and this post just calls it out for what it is. reply henriquez 20 hours agorootparentTrivial means different things to different people. I’m not really a fan of LLM hype but it seems to me a valid practice of scientific discovery to evaluate the use and optimization of such models. reply brookst 20 hours agoparentprevThe real shame is that even HN has fallen into the trap of missing obvious and funny sarcasm unless it is clearly labeled. reply KyleBerezin 11 hours agorootparentSarcasm is often spoken with a sarcastic inflection. It doesn&#x27;t translate well to text, regardless of the community. reply 3cats-in-a-coat 20 hours agoprevAttacking the participants in a systemic shift is 100% useless as it doesn&#x27;t target the culprit.In programming we have a similar phenomenon, that StackOverflow-driven (and I guess now GPT-driven) juniors have overtaken the industry and displaced serious talent. Because sufficient amounts of quantity always beats quality, even if the end result is inferior, this is caused by market dynamics, which operate on much cruder parameters than the sophisticated analysis of an individual noticing everything around them becoming \"enshittified\".SO-driven juniors are cheap, plentiful, and easily replaceable. And a business that values less expense and less risk therefore prefers them, because it has no way to measure the quality of the final product with simple metrics.The same mechanism is driving AI replacing our jobs currently, the avalanche of garbage papers by academics. This is entropy for you. We see it everywhere in modern society down to the food we eat. Quality goes away, replaced by cheap to produce and long shelf life.If we don&#x27;t fundamentally alter what the system sees as ACCEPTABLE, and VALUABLE, this process will inevitably continue until our world is completely unrecognizable. And to fundamentally alter the system, we need an impulse that aligns us as a society, startles us into action, all together (or at least significant majority of us). But it seems we&#x27;re currently in \"slowly boiled frog mode\". reply ilc 20 hours agoparentThis whole thing reminds me a bit of playing Fallout 4.There will be good data, the pre-AI enshitification data. The stuff from before the war.And then... the data after. Tainted by the entropy, and lack of utility of AI.Alas, this means in some senses, human progress will slow and stop in the tech field if we aren&#x27;t careful and preserve ways to create pre-AI data. But the cost of it is so high in comparison to post... I&#x27;m not sold it will be worth it. reply jhbadger 19 hours agorootparentThis paints a rosy picture of human-generated data now. It&#x27;s not as if most human data is reliable. Even among peer reviewed scientific literature, most of it is crap and it takes effort to find the good stuff. Also, your analogy kind of misses the point of the Fallout games. The pre-war world was awful, filled with evil corporations like Vault-tech and Nuka Cola that murdered and poisoned their customers, and the point of the games is that people need to move on and not idealize the past. reply ilc 18 hours agorootparentThere is a reason I didn&#x27;t draw a hard parallel.I use AI day to day. I see what it can do and can&#x27;t.But when you see pages, and pages, and pages of GPT spam all over the place. Finding the nuggets of wisdom will be much harder than before, the \"bomb\" was dropped.Thus actually leading to the whole FO4 main plot.Yes, life will always find a way. And yes humanity can not put the genie in a bottle, we are much more likely to put it on a Fat Man catapult.But it means, that in a sense... that we will all have to accept this background radiation of AI shit, as part of our new norm.And this isn&#x27;t the first time I&#x27;ve thought in similar ways. I remember reading older math texts (way pre-computer works in things like diffeq and PDE) and often thinking the explanations were clearer. Probably because of the increased effort to actually print something.Who knows... maybe I&#x27;m just an old coot seeing patterns where there are none. reply somenameforme 17 hours agorootparent> \"I remember reading older math texts (way pre-computer works in things like diffeq and PDE) and often thinking the explanations were clearer.\"This is 100% the case in chess as well. The books before and after the computer era are orders of magnitude different in terms of readability. I think a major shift in society has been in motivation. In the past if you were studying advanced mathematics, let alone writing about it, it was solely and exclusively because you absolutely loved the field. And you were also probably several sigmas outside the mean intellectually. Now? It&#x27;s most often because of some vague direction such as wanting a decent paying job, which may through the twists and turns of fate eventually see you writing books in a subject you don&#x27;t particularly have much enthusiasm for.And the much more crowded &#x27;intellectual market&#x27;, alongside various image crafting or signaling motivations, also creates a really perverse incentive. Exceptional competence and understanding within a subject makes it easy to explain things, even the esoteric and fabulously complex. See: Richard Feynman. But in modern times there often seems to be a desire to go the other direction - and make things sound utterly complex, even when they aren&#x27;t. I think Einstein&#x27;s paper on special relativity vs many (most?) modern papers is a good example. Einstein&#x27;s writing was such that anybody with a basic education could clearly understand the paper, even if they might not fully follow the math. By contrast, so many modern papers seem to be written as if the author had a well worn copy of the Thesaurus of Incomprehensibility at his bedside table. reply ableal 20 hours agoparentprev> Because sufficient amounts of quantity always beats qualityHegel (as echoed by Marx): \"merely quantitative differences beyond a certain point pass into qualitative changes\" ( https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;10.1073&#x2F;pnas.240462397 )I always found that an interesting observation, whatever you think of the rest of their works. reply 3cats-in-a-coat 18 hours agorootparentIt&#x27;s an absolutely valid observation and I cite it often. We need to learn as a society how to separate things and think less wholesale.The best people have silly beliefs, the worst people have great insights, and the vast majority of us are in-between.If we discard everything \"tainted\" by imperfection, we&#x27;ll be left with nothing good. reply Der_Einzige 19 hours agorootparentprevHegel is a hack and doesn’t deserve to be cited here. Consider that he’s really only famous because his class motivated a bunch of other philosophers (the young Hegelian&#x27;s, Marx, Stirner, Bruno Bauer et al) to meet in wine bars after class to complain about how terrible&#x2F;impossible to understand his philosophy is.Anyone whose even tried to read stuff of his I.e the phenomenology of spirit will tell you that he’s a charlatan and hack, and the people who constantly cite him (I.e Zizek, Lacan, Foucault) are also hacks. reply selfhoster11 20 hours agoprevExcuse me? Step by step wasn&#x27;t paper-worthy? Hard disagree.LLM research is currently in its infancy, because they are no older than a few years old. And a research field in its infancy is bound to have a few noteworthy \"no sh*t, Sherlock\" papers that would be obvious from hindsight.The fact is, LLMs are a higher-order construct in machine learning, much like a fish is higher-order than a simple cellular colony. Lower-order ML constructs do not demonstrate emergent capabilities like step by step, stream of consciousness thinking, and so on.Academics should be less jaded and approach the field with beginner&#x27;s eyes. Because we are all beginners here. reply tensor 17 hours agoparentI&#x27;m not surprised at the defence of \"prompt engineering\" here. It&#x27;s something easy to do with no real knowledge, and I&#x27;m sure having it dismissed hurts some people.But I 100% agree with the author, \"prompt engineering\" is not science, and I&#x27;d say it&#x27;s not engineering either. All you&#x27;re doing is exploring the parameter space of particular model in a very crude way. There is no \"engineering\" going on in this process, just a bunch of trial and error. Perhaps it should be called \"prompt guessing.\"None of the results of this process will transfer to any other model. It&#x27;s simply not science. Papers like \"step-by-step\" are different, and relate more to learning and inference and do translate to different models and even different architectures.Also, no, we are not all beginners here. Language models have a long history, and while the very large models are impressive, most of their failings have been known for a very long time already. Things like \"prompt engineering\" will eventually end up in the same graveyard as \"keyword engineers\" of the past. reply adastra22 17 hours agorootparent> But I 100% agree with the author, \"prompt engineering\" is not science, and I&#x27;d say it&#x27;s not engineering either. All you&#x27;re doing is exploring the parameter space of particular model in a very crude way. There is no \"engineering\" going on in this process, just a bunch of trial and error.I wonder what your definition of “science” or “engineering” is… reply gwervc 16 hours agorootparentIf you remove the AI glasses, \"prompt engineering\" is just typing words and seeing if results match the expectations... which is exactly what any search engine pays their testers for. Those testers are making an important job to keep improving the quality of the product but they aren&#x27;t engineers and even less so researchers.Similarly a kid playing with the dose of water needed to build a sandcastle isn&#x27;t a civil engineer nor an environmental researcher. Maybe on LinkedIn though. reply blueboo 16 hours agorootparentI’m not sure the scientific method itself can withstand this sort of scrutiny. After all, it’s just making guesses about what will happen and then seeing what happens! reply j2kun 16 hours agorootparentExcept there&#x27;s also, you know, building coherent theories and using those theories to predict the system behavior. reply selfhoster11 15 hours agorootparentAll right, here is a theory: LLMs contain \"latent knowledge\" that is sometimes used by the model during inference, and sometimes it isn&#x27;t.One way to \"engage\" these internal representations is to include keywords or patterns of text that make that latent knowledge more likely to \"activate\". Say, if you want to ask about palm trees, include a paragraph talking about a species of a palm tree (no matter whether it contains any information pertaining to the actual query, so long it&#x27;s \"thematically\" right) to make a higher quality completion more likely.It might not be the actual truth or what&#x27;s going on inside the model. But it works quite consistently when applied to prompt engineering, and produces visibly improved results. reply j2kun 9 hours agorootparent> It might not be the actual truth or what&#x27;s going on inside the model.This sums up pretty nicely why prompt hacking is not science. A scientific theory is related in a concrete way to the mechanism by which the phenomenon being studied works. replynaremu 16 hours agorootparentprevIt&#x27;s funny how often I see people make bring up the \"did you know\" tidbit about software engineering not being \"real\" engineering in a traditional sense, which seems to go very uncontroversially.But prompt engineering is still a pressure point for some people, despite being wildly more simple and accessible (literally tell the thing to do a thing, and if it doesn&#x27;t do the thing right, reword)It feels as though we&#x27;re getting to the technological equivalent of \"what IS art anyways\", and questions like if non traditional forms like video games are art (I&#x27;m thinking all the way up the chain to even say, Madden games)And in my experience, when something is under constant questioning of whether or not it even counts as X, Y or Z, it usually can technically qualify, but...If people are constantly debating whether or not it&#x27;s even X, it&#x27;s probably just not impressing people who don&#x27;t engage in it, as opposed to \"traditional\" concepts of engineering and art, and part of the impression made comes from the investment and irreplaceable skillsets, things few, if anyone else at the time could have done.This is why taping a banana on the wall is definitely technically art, but not many outside the art community that tapes bananas to walls really think much of it. It&#x27;s so mundane and accessible a feat that it doesn&#x27;t garner much merit to passerbys. It&#x27;s art by the loosest technical definition, and is giving a lot of credit for a small amount of effort anyone could&#x27;ve done.Admittedly \"prompt engineering\" is definitely less accessible than a roll of duct tape and a banana but I think we used to just call it \"writing&#x2F;communication\", but I guess those who feel capable at that, often just do it manually anyways. reply 0xfae 17 hours agorootparentprevRight? I&#x27;m having a hard time imagining a definition that includes \"trying new things and seeing what happens\" but that doesn&#x27;t include... \"trying new things and seeing what happens\" reply aeternum 16 hours agorootparent\"Science\" has been twisted recently into a kind of witchcraft that can only be practiced by those anointed through the rigors of academia.\"Trust the science\"In reality, that is about the furthest from what you should do. As Feynman once said: \"Science is the belief in the ignorance of experts\". Electricity was also once considered a toy and good for nothing but parlor tricks. reply roguas 17 hours agorootparentprevEspecially given this would be fine definition of engineering+science: \"All you&#x27;re doing is exploring the parameter space of particular model in a very crude way.\" reply Tarq0n 15 hours agorootparentprevScience should aim to create general (that is, generalized or generalizable) knowledge. One prompt is just an anecdote, a method for creating performant prompts or deriving prompts from model characteristics would be more scientific. reply selfhoster11 15 hours agorootparentprev> All you&#x27;re doing is exploring the parameter space of particular model in a very crude way.Yes. I come to think of prompt engineering as, in a sense, doing an approximate SELECT query on the latent behavioural space (excuse my lack of proper terminology, my background in ML is pretty thin) that can be thought of as \"fishing out\" the agent&#x2F;personality&#x2F;simulator that is most likely to give you the kind of answer you want. Of course a prompt is a very crude way to explore this space, but to me this is a consequence of extremely poor tooling. For one, llama.cpp now has negative prompts, while the GPT-4 API will probably never have them. So we make-do with the interface available.> There is no \"engineering\" going on in this process, just a bunch of trial and error. Perhaps it should be called \"prompt guessing.\"That is incorrect. It is true that there is a lot of trial and error, yes. But it&#x27;s not true that it&#x27;s pure guessing either. While my approach can be best described as a systematic variant of vibe-driven development, at its core it&#x27;s quite similar to genetic programming. The prompt is mutable, and it&#x27;s efficacy is possible to evaluate at least in a qualitative sense vs the last version of the prompt. By iterative mutation (rephrasing, restructuring&#x2F;refactoring the whole prompt, changing out synonyms, adding or removing formatting, adding or removing instructions and contextual information), it is possible to iterate from a terrible initial prompt to a much more elaborate prompt that gets you 90-97% of the way towards nearly exactly what you want to do, by combining the addition of new techniques with subjective judgement on how to proceed (which is incidentally not too different from some strains of classical programming). On GPT-4, at least.> None of the results of this process will transfer to any other model.Is that so? Yes, models are somewhat idiosyncratic, and you cannot just drag and drop the same prompt between them. But, in my admittedly limited experience of cross-model prompt engineering, I have found that techniques which helped me to achieve better results with the untuned GPT-3 base model, also helped me greatly with the 7B Llama 1 models. I hypothesise that (in the absence of muddling factors like RLHF-induced censorship of model output), similarly sized models should perform similarly on similar (not necessarily identical) queries. For the time being, this hypothesis is impossible to test because the only realistic peer to GPT-4 (i. e. Claude) is lobotomised to the extent where I would outright pay a premium to not have to use it. I have more to say on this, but won&#x27;t unless you ask in the interests of brevity.> Language models have a long history, and while the very large models are impressive, most of their failings have been known for a very long time already. Things like \"prompt engineering\" will eventually end up in the same graveyard as \"keyword engineers\" of the past.Language models have a long history, but a Markov chain can hardly be asked to create a basic Python client for a novel online API. I will also dispute the assertion that we know the \"failings\" of large language models. Several times now, previously \"impossible\" tasks have been proven eminently possible by further research and&#x2F;or re-testing on improved models (better-trained, larger, novel fine-tuning techniques, etc). I am far from being on the LLM hype train, or saying they can do everything that optimists hope they can do. All I&#x27;m saying, is that the academia is doing itself a disservice by not looking at the field as something to be explored with no preconceptions, positive or negative. reply _gabe_ 18 hours agoparentprevI feel like the author of this tweet wasn’t saying step-by-step isn’t worthy, he was saying that non-reproducible results are not science. He emphasizes this twice in that tweet:> one experiment on one data set with seed picking is not worthy reporting> Additionally, we all need to understand this is just one good empirical result, now we need to make it useful… reply low_tech_love 17 hours agorootparentExactly, and I tend to agree with him. I argued some time ago here that a paper should take some time to try to explain why its results are happening, at least from a reasonable hypothesis (people didn&#x27;t seem to agree). An experiment (even a simple one) starts from a null hypothesis and tries to disprove it. However, most of what we see coming out of \"scientific\" papers is basically just engineering, I guess?: we put all of these things together in some way (out of pure guess and&#x2F;or preference bias) and these results happened. We don&#x27;t know why, good luck figuring it out. Here is one example where it works (don&#x27;t ask where it doesn&#x27;t; we intentionally kept those out).And while I obviously value very much the engineering advances we have seen, the science is still lacking, because not enough people are trying to understand why these things are happening. Although engineering advances are important and valuable, I don&#x27;t understand exactly why people try so hard to call themselves scientists if they are basically skipping the scientific process entirely. reply jimbokun 17 hours agorootparentLots of onions in the varnish. reply roguas 16 hours agorootparentprevIs it non-reproducible? Also results which reproducibility can be measured and appears stable is perfectly good science. I dislike when people throw statements like that. reply _gabe_ 13 hours agorootparentI have no idea. The author of that tweet seems to imply that the results aren’t reproducible. I was just commenting to point out that the author’s intent may have been different from what the grandparent comment was saying. reply phkahler 19 hours agoparentprev>> LLM research is currently in its infancyEverything that went in to creating GPT4 is AI&#x2F;science or whatever. Probing GPT4 and trying to understand and characterize it is also a very worthy thing to do - else how can it be improved upon? But if making GPT is science, I&#x27;d say this stuff is more akin to psychology ;-) reply dr_dshiv 19 hours agorootparentMachine psychology reply jimbokun 17 hours agorootparentAsimov was way ahead of you:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Robopsychology reply dr_dshiv 14 hours agorootparentprevIt’s a real thing: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.13988 reply jjordan 18 hours agorootparentprev2080 graduates of Machine Psychology will look back on this post and smile. reply jejeyyy77 17 hours agorootparentprevyou realize nobody understands WHY or HOW these models work under the hood right?it&#x27;s akin to evolution - we understand the process - that part is simple. But the output&#x2F;organisms we have to investigate how they work. reply kergonath 17 hours agorootparent> you realize nobody understands WHY or HOW these models work under the hood right?Of course we understand how they work, we built them! There is no mystery in their mechanisms, we know the number of neurons, their connectivity, everything from the weights to the activation functions. This is not a mystery, this is several decades of technical developments.> it&#x27;s akin to evolution - we understand the process - that part is simple.There is nothing simple about evolution. Things like horizontal gene transfer is very much not obvious, and the effect of things like environment is a field of active research.> But the output&#x2F;organisms we have to investigate how they work.There is a fundamental difference with neural networks here: there are a lot of molecules in an animal’s body about which we have no clue. Similarly, we don’t know what a lot of almost any animal’s DNA encodes. Model species that are entirely mapped are few and far between. An artificial neural network is built from simple bricks that interact in well defined ways. We really cannot say the same thing about chemistry in general, much less bio chemistry. reply zamfi 16 hours agorootparent> Of course we understand how they work, we built them! There is no mystery in their mechanisms, we know the number of neurons, their connectivity, everything from the weights to the activation functions. This is not a mystery, this is several decades of technical developments.The discovery of DNA’s structure was heralded as containing the same explanatory power as you describe here.Turns out, the story was much more complicated then, and is much more complicated now.Anyone today who tells you they know why LLMs are capable of programming, and how they do it, is plainly lying to you.We have built a complex system that we only understand well at a basic “well there are weights and there’s attention, I guess?” layer. Past that we only have speculation right now. reply kergonath 16 hours agorootparent> The discovery of DNA’s structure was heralded as containing the same explanatory power as you describe here.Not at all. It&#x27;s like saying that since we can read hieroglyphics we know all about ancient Egypt. Deciphering DNA is tool to understand biology, it is not that understanding in itself.> Turns out, the story was much more complicated then, and is much more complicated now.We are reverse engineering biology. We are building artificial intelligence. There is a fundamental difference and equating them is fundamentally misunderstanding both of them.> Anyone today who tells you they know why LLMs are capable of programming, and how they do it, is plainly lying to you.How so? They can do it because we taught them, there is no magic.> We have built a complex system that we only understand well at a basic “well there are weights and there’s attention, I guess?” layer. Past that we only have speculation right now.Exactly in the same way that nobody understand in detail how a complex modern SoC works. Again, there is no magic. reply zamfi 10 hours agorootparent> How so? They can do it because we taught them, there is no magic.Yeah, no. I mean, we can’t introspect the system to see how it actually does programming at any useful level of abstraction. “Because we taught them” is about as useful a statement as “because its genetic parents were that way”.No, of course it’s not magic. But that doesn’t mean we understand it at a useful level. reply jejeyyy77 11 hours agorootparentprevWe designed the process. We didn&#x27;t design the models - the models were \"designed\" based on the features of a massive dataset and massive number of iterations.Even if you understand evolution - you still don&#x27;t understand how the human body or mind works. That needs to be investigated and discovered.In the same way, you understanding how these models were trained doesn&#x27;t help you understand how the models work. That needs to be investigated and discovered. reply shwouchk 16 hours agorootparentprevWhy stop at chemistry? Chemistry is fundamentally quantum electrodynamics applied to huge ensembles of particles. QED is very well understood and gives the best predictions we have to date of any scientific theory.How come we don’t entirely understand biology then? reply kergonath 15 hours agorootparent> Why stop at chemistry? Chemistry is fundamentally quantum electrodynamics applied to huge ensembles of particles.Chemistry is indeed applied QED ;) (and you don&#x27;t need massive numbers of particles to have very complex chemistry)> How come we don’t entirely understand biology then?We understand some of the basics (even QED is not reality). That understanding comes from bottom-up studies of biochemistry, but most of it comes from top-down observation of whatever there happens to be around us. The trouble is that we are using this imperfect understanding of the basics to reverse engineer an insanely complex system that involves phenomena spanning 9 orders of magnitude both in space and time.LLMs did not spawn on their own. There is a continuous progression from the perceptron to GPT-4, each one building on the previous generation, and every step was purposeful and documented. There is no sudden jump, merely an exponential progression over decades. It&#x27;s fundamentally very different from anything we can see in nature, where nothing was designed and everything appears from fundamental phenomena we don&#x27;t understand.As I said, imagining that the current state of AI is anything like biology is a profound misunderstanding of the complexity of both. We like to think we&#x27;re gods, but we&#x27;re really children in a sand box. reply shwouchk 12 hours agorootparentI will ignore your patronizing remarks beyond acknowledging them here, in order to promote civil discourse.I think you have missed my point by focusing on biology as an extremely complex field.e, it was my mistake to use it as an example in the first place. We don’t need to go that far;sure, llms did not spawn on their own. They are a result of thousands of years of progress in countless fields of science and engineering. Like any modern invention, essentially.Here I remember to make sure we are on the same page on what we’re discussing - as I understand, whether “prompt engineering” can be considered an engineering&#x2F;science practice. Personally I haven’t considered this enough to form an opinion but your argument does not sound convincing to me;I guess your idea of what llms represent matters here. The way I see it, in some abstract sense we are as society exploring a current peak - in compute $ or flops and performance on certain tasks - of a rather large but also narrow family of functions. By focusing our attention on functions composed of ones we understood how to effectively find parameters for, we were able to build at this point rather complicated processes for finding parameters for the compositions.Yes, the components are understood, at various levels of rigor, but the thing produced is not yet sufficiently understood. Partly out of cost to reproduce such research, and partly due to complexity of the system, a driver for the cost.The fact that “prompt engineering” as a practice and that companies supposedly base their business model on secret prompts is a testament, for me, to the fact they are not well understood. A well understood system you design has a well understood interface.Now, I haven’t noticed a specific post OP was criticizing so i take it his remarks were general. He seems to thinks that some research is not worth publishing. I tend to agree that I would like research to be of high quality, but that is subjective. Is it novel? is it true?Now, progress will be progress and im sure current architectures will change and models will get larger. And it may be that a few giants are the only one running models large enough to require prompt engineering. Or we may find a way to have those models understand us better than a human ever could. Doubtful. And post singularity anyway, by definition.In either case yes, probably temporary profession. But in case open research will continue in those directions as well, there will be need for people to figure out ways to communicate effectively with these. You dismiss them as testers.However, progress in science and engineering is often driven by data where theory is lacking and I’m not aware of the existence of deep theory as of yet. eg something that would predict how well a certain architecture would perform. Engineering ahead of theory, driven by $).As in physics that we both mentioned, knowing the component part does not automatically grant you understanding of the whole. knowing everything there is to know about the relevant physical interaction, protein folding was a tough problem that AFAIR has had a lot of success with tools from the field. Square in the realm of physics even, and we can’t give good predictions without testing (computationally).If someone tested some folding algorithm and visually inspected results, then found a trick how to consistently improve on the result in some subcase of proteins. Would that be worthy of publishing? if yes, why is this different? if not, why not? replydartos 19 hours agorootparentprevWhat is psychology, but applied biology?https:&#x2F;&#x2F;xkcd.com&#x2F;435&#x2F; reply eimrine 19 hours agorootparentPsychology is a religious-like pseudoscience, they can not even define what \"psy\" is without using some conceptions from religions such as a soul.upd these statements from me are so controversial, the number of \"points\" just dances lambada. The psy* areas are clearly polarized: some guys upvote all my messages in this topic and some other ones downvote all my messages in this topic. This is a sign of something interesting but I am not ready to elaborate on this statement in this comment which is going to become [flagged] eventually. reply phkahler 17 hours agorootparent>> This is a sign of something interestingYeah, you might understand it if you study psychology or perhaps sociology. reply fkyoureadthedoc 16 hours agorootparentprev> This is a sign of something interesting but I am not ready to elaborate on this statement in this comment which is going to become [flagged] eventually.Yet you&#x27;re being a reply guy all over this thread, might as well just elaborate you clearly have the time and interest reply eimrine 16 hours agorootparentPsy* pseudoscience is among a few hills I am gladly die for. Also free&#x2F;libre software, Lisp and cryptocurrency with no premine. reply beepbooptheory 18 hours agorootparentprevDude if you think φύσις is free of its own philosophical baggage I got a bridge to sell you. reply eimrine 18 hours agorootparent> its own philosophical baggageCould you elaborate on this statement? reply beepbooptheory 16 hours agorootparentIf your contention is just something like \"the root psy- comes out of mystical&#x2F;spiritual conceptions in Ancient Greece, and that speaks to the bunk&#x2F;ungrounded conceptions of modern psychology,\" then I would ask why the same critique is not levied against the ancient Greek conception of \"nature\" and the \"natural\" from which we get the word \"physics\".You might retort here \"ah well, &#x27;nature&#x27; is just the word we use when we speak of observable phenomena in the hard sciences, its not muddied by religion like that crock stuff psychology.\"And then I would say, \"ok, if &#x27;nature&#x27; is just observable phenomena, what is the aim or purpose of the hard sciences? If it is all just observing&#x2F;experimenting on discrete phenomena, there would be nothing we could do or conclude from the rigor of physics.\"You laugh at my insanity (well, if you believed in such a thing): \"But we do conclude things from physics, because experiments are reproducible, and with their reproducibility we can gain confidence in generalizing the laws of our universe.\"And yes! You would be correct here. But now all the sudden you have committed physics to something just as fundamentally \"spiritual\" as the soul: that the universe is sensible, rational, and \"with laws.\" Which is indeed just speaking the very same mystical \"nature\" of ancient Greece from which we get phys-.But this need not be some damning critique of physics itself (like psychology), and rather, can lead to a higher level understanding of all scientific pursuits: that we are everywhere cursed by a fundamental incompleteness, that in order even to enter into scientific pursuit we must shed an absolute skepticism for a qualified one. Because this is the only way we accumulate a network of reinforced hypotheses and conceptions, which do indeed help us navigate the purely phenomenal world we are bound in. reply Woshiwuja 19 hours agorootparentprevWhat? Are you literally referring to ancient greek psy? jesus... reply eimrine 19 hours agorootparentWhat is incorrect in this reference? You have not proposed any counterarguments. Also if you need just more fresh data - how do you propose to interpret the result of the Rosenhan&#x27;s experiment? reply wavemode 18 hours agorootparentThat lying about your symptoms to doctors leads to incorrect diagnoses? reply eimrine 18 hours agorootparentThey are _not_ doctors in terms of evidence-based medicine, just policemen without a token. The problem is obviously not about incorrect diagnosis, I can lie to any doctor about any symptoms and just go home with zero obstructions from feds. reply nick222226 17 hours agorootparentAs someone who was formerly in a mental ward for acute crisis, I would say that at least the 72 hour hold was an essential and necessary part of my treatment. I don&#x27;t think that staying at home with unprepared family members for the acute period would have worked out, and I don&#x27;t even have a problematic home environment!The flip side of the coin is that I was in a really high quality hospital, I&#x27;m sure there are hospitals or facilities that can be more harmful rather than helpful.I also have a problem with the way that they treat mental health like cancer, that once you have a diagnosis you will always have it. There are zero diagnostic criteria for \"fully recovered\" or removing dependence on medication, even after 5 or 10 years. It&#x27;s also treated like a scarlet letter for insurance and unrelated things like TSA pre check - no matter how well you are doing you are still some level of risk to yourself and society. Though I could be wrong... the reoccurrence chart over time for my specific acute mania (with no depressive episodes) does look a lot like cancer remission charts with asymptotic approach to 80%+ reoccurrence after 2-4 years. reply Woshiwuja 18 hours agorootparentprevDont put people inside mental asylums when they are not ill? reply eimrine 18 hours agorootparentThere is no evidence of existing at least one defined illness in psy* fields. For example, let me tell you that a person X fell ill with schizophrenia. What do you know about X or X&#x27;s brain? reply oasisbob 17 hours agorootparentMost psychologists would be much more interested in person X&#x27;s behavior, rather than their brain. reply eimrine 17 hours agorootparentAnd believe in such false statement as a free will?So what is a part of organism (if not brain) which might be possible for psy* specialist to heal, is it an arm or a leg or a spine? replythesz 19 hours agoparentprev> Lower-order ML constructs do not demonstrate emergent capabilities like step by step, stream of consciousness thinking, and so on.As a matter of fact, I did a project on the normalization of the text, e.g., translate \"crossing of 6 a. and 12 s.\" into \"crossing of sixth avenue and 12-th street\" with a simple LM (order 3) and beam search on the lattice paths, lattice formed with hypotheses&#x27; variants. I got two fold decrease of word error rate compared to simpler approach with just outputting the most probable WFST path. It was not \"step by step stream of consciousness,\" but nevertheless very impressive feat, when system started to know more without much effort.The large LM&#x27;s do not just output \"most probable\" token, they output most probable sequence of tokens and it is done with the beam search.As you can see, my experience tells me that beam search alone can noticeably, if not tremendously, improve quality of the output, even for very simple LMs.And if I may, the higher-order construct here is a beam search, not the LMs-as-matrix-coefficients&#x27; themselves. Beam search is used in speech recognition for decades now, SR does not work properly without it. LM&#x27;s, apparently, also do not work without it. reply azinman2 19 hours agorootparentIs there something you can link to? I’d like to learn more. reply thesz 16 hours agorootparentHere it is: https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;how-to-generateBeam search at Wikipedia: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_searchBeam search in Sqlite: https:&#x2F;&#x2F;www.sqlite.org&#x2F;queryplanner-ng.html#_a_difficult_cas...Beam search is more interesting than its&#x27; application within AI field. reply lumost 18 hours agoparentprevAnecdotally,The field of ML suffered from a problem where there were more entrants to the field than available positions&#x2F;viable work. In many industrial positions, it was possible to hide a lack of progress behind ambiguity and&#x2F;or poor metrics. This lead to a large amount of gate keeping for productive work as ultimately there wasn&#x27;t enough to go around in the typical case.This attitude is somewhat pervasive leading to blogs like the above. Granted, the Nth prompting paper probably isn&#x27;t interesting - but new programming languages for prompts and prompt discovery techniques are very exciting. I wouldn&#x27;t be surprised if it turned out that automatic prompt expansion using a small pre-processing model turns out to be an effective technique. reply renonce 18 hours agoparentprevI would say rather than saying that a new prompt is not science, it’s certainly a new discovery that is worth sharing. Maybe there should be a higher bar for papers, but why we have to make a discovery a paper - and not publish it at all if it cannot be made one - when a simple blog post or a tweet would convey the discovery very well? reply zxt_tzx 18 hours agoparentprevOn a related note, there is this recent tweet purportedly showing that \"offering to give a tip to ChatGPT\" improves performance (or at the very least resulted in longer responses, which might not be a good proxy for performance) https:&#x2F;&#x2F;twitter.com&#x2F;voooooogel&#x2F;status&#x2F;1730726744314069190 reply ben_w 18 hours agoparentprevI&#x27;m reading this tweet as saying \"you can&#x27;t write a paper by prompting an LLM in these ways\" rather than \"you can&#x27;t write a paper characterising the impact of prompting an LLM in these ways\".I&#x27;d agree the former won&#x27;t get you a complete anything (longer than ~30 lines) by itself (90-95% cool, but with some incredible errors in the other 5-10%).I&#x27;d also agree that the latter is worthy of publishing. reply WhitneyLand 17 hours agoparentprevWhat you’re saying seems to be compounded by the black box aspects here.Instinctively a prompt may look like one line of code. We can often know or prove what a compiler is doing, but higher dimensional space is just not understood in the same way.What other engineered thing in history has had this much immediately useful emergent capability? Genetic algorithms finding antenna designs and flocking algorithms are fantastic, but I would argue narrower in scope.Of course a paper is still expected to expand knowledge, to have rigor and impact, but I don’t see why a prompt centric contribution would inherently preclude this. reply nipponese 18 hours agoparentprevSorry, ignoramus here: Which paper is “Step by step”? reply zxt_tzx 18 hours agorootparentI think it&#x27;s a reference to the \"discovery\" that if you ask GPT-4 to answer your query \"step by step\", it&#x27;ll actually offer a better response than otherwise. reply jebarker 18 hours agoparentprev> emergent capabilities like step by step, stream of consciousness thinkingWhat makes these things \"emergent capabilities\"? They seem like pretty straightforward consequences of autoregressive generation. If you feed output back as input then you&#x27;ll get more output conditioned on that new input and stream of conscious generation is just stochastic parroting isn&#x27;t it? reply selfhoster11 16 hours agorootparentThey are emergent in the sense that there is nothing in the pre-training dataset that would show the LLM by example how to, for example, compare and contrast any given pairing of fruit, technologies, or fictional settings, while thinking with the mindset of a doctor that hates both options, and on top of that make sure that this ends up formatted as a stream-of-consciousness. It can learn all these aspects from the source data individually in isolation, but there&#x27;s no way there are examples that show how to combine it all (awareness of world information + knowledge of how to use it) into a single answer. That&#x27;s probably a very clumsy example - others online have supplied more rigorous ones that I recommend checking out.Strictly speaking, it might be \"stochastic parroting\". But really, if you want to be a great and supremely effective stochastic parrot, you have to learn an internal representation of certain things so that you can predict them. And there are hints that this is exactly what a sufficiently-large large language model is doing. reply skyde 17 hours agoparentprevI have a question For people in the field!Would GPT only with pretraining and no fine tuning be able to behave better when prompt is \"let’s think step by step\"?Or this prompt only worked because a fine tuning dataset containing many \"let’s think step by step\" prompt was used? reply selfhoster11 16 hours agorootparentWorking with pure pre-trained models is quite hard, and takes some practice. The key part is that \"let&#x27;s think step by step\" is a technique that is used by humans also, and therefore (I think) would be somewhat represented in the pre-training corpus. It would be somewhat harder to \"activate\" this mode of thinking than a \"let&#x27;s think step by step\" would in a fine-tuned model, but it would be possible with some elbow grease. reply Der_Einzige 19 hours agoparentprevNLP&#x2F;Computational Lingustics is NOT a new field. The ACL was founded in 1962LM research is also old, papers using very shitty LMs (I.e Markov chains) and discussing them have existed since the early 2000s and likely before.Check yourself before you try to check others. reply Retr0id 19 hours agorootparentTry asking a markov chain to think step by step reply tovej 18 hours agorootparentBy definition a Markov chain does everything step by step already, no need to ask! reply Retr0id 18 hours agorootparentBy that logic, so does a GPT reply idiliv 19 hours agorootparentprevParent post is talking about LLMs, i.e. Large LMs. Research on LLMs is indeed in its infancy. reply gumballindie 19 hours agoprevAnyone caught doing this should be kicked out of the industry. Period. You&#x27;re scaming those funding your \"research\", you are misleading readers, and are producing low quality content wasting everyone&#x27;s time. reply alickz 20 hours agoprevStill beats most psychology papers reply margorczynski 20 hours agoparentTbh in both they are mostly alchemy with some more or less expert lingo put in to make it sound more scientific.AI&#x2F;ML resembles more alchemy than e.g. physics, putting stuff into a pot and seeing what comes out. A lot of the math in those papers doesn&#x27;t provide anything but some truisms, most of it is throwing stuff at the wall and seeing what sticks. reply whywhywhywhy 19 hours agoprev [–] Academia needs to get over itself, can&#x27;t wait to see how amazing this tech is going to get when the next generation who decide never to bother with those stuffy and navel gazing institutions becomes the driving force behind it.Looking forward to \"I made this cool thing, here&#x27;s the code&#x2F;library you can use\" rather than the papers&#x2F;gatekeeping&#x2F;ego stroking&#x2F;\"muh PhD\".Think if Google had built an AI team around the former rather than the latter, they wouldn&#x27;t have risked the future of their entire company and squandered their decade head start. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author of the tweet is concerned about the emphasis on quantity rather than quality in the work produced by professors.",
      "They argue that this pressure can lead to a decrease in the overall quality of the work.",
      "The author advocates for a shift in focus towards prioritizing quality over quantity."
    ],
    "commentSummary": [
      "The discussion covers various topics related to scientific research, reproducibility, the value of papers, prompt engineering, complex systems, and the limitations of biology and AI models.",
      "Participants have diverse opinions and engage in debates about the scientific validity, usefulness, and quality of research topics and practices.",
      "There is criticism of the academic publishing system and a plea for more transparency and open research, underscoring the complexity and ever-changing nature of scientific research in AI and related fields."
    ],
    "points": 264,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1701781563
  },
  {
    "id": 38530466,
    "title": "Toxic Comments Decrease Volunteer Activity on Wikipedia",
    "originLink": "https://academic.oup.com/pnasnexus/article/2/12/pgad385/7457939",
    "originBody": "Skip to Main Content Advertisement Journals Books Search Menu Menu Navbar Search Filter PNAS Nexus This issue Social and Political Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Mobile Enter search term Search Issues Advance articles Subject Biological, Health, and Medical Sciences Browse content in Biological, Health, and Medical Sciences Administration Of Health Services, Education, and Research Agricultural Sciences Allied Health Professions Anesthesiology Anthropology Anthropology (Biological, Health, and Medical Sciences) Applied Biological Sciences Biochemistry Biophysics and Computational Biology (Biological, Health, and Medical Sciences) Biostatistics Cell Biology Dermatology Developmental Biology Ecology Environmental Sciences (Biological, Health, and Medical Sciences) Evolution Genetics Immunology and Inflammation Internal Medicine Medical Sciences Medical Microbiology Microbiology Neurology Neuroscience Nutrition Obstetrics and Gynecology Ophthalmology Pediatrics Pharmacology Pharmacy Physiology Plant Biology Population Biology Psychiatry Psychological and Cognitive Sciences (Biological, Health, and Medical Sciences) Public Health and Epidemiology Radiation Oncology Radiology Rehabilitation Surgery Sustainability Science (Biological, Health, and Medical Sciences) Systems Biology Physical Sciences and Engineering Browse content in Physical Sciences and Engineering Aerospace Engineering Applied Mathematics Applied Physical Sciences Bioengineering Biophysics and Computational Biology (Physical Sciences and Engineering) Chemical Engineering Chemistry Civil and Environmental Engineering Computer Sciences Computer Science and Engineering Earth Resources Engineering Earth, Atmospheric, and Planetary Sciences Electric Power and Energy Systems Engineering Electronics, Communications and Information Systems Engineering Engineering Environmental Sciences (Physical Sciences and Engineering) Materials Engineering Mathematics Mechanical Engineering Physics Statistics Sustainability Science (Physical Sciences and Engineering) Social and Political Sciences Browse content in Social and Political Sciences Anthropology (Social and Political Sciences) Demography Economic Sciences Environmental Sciences (Social and Political Sciences) Political Sciences Psychological and Cognitive Sciences (Social and Political Sciences) Social Sciences Sustainability Science (Social and Political Sciences) Submit Author guidelines Submission site Open access policy Self-archiving policy Why submit to PNAS Nexus The PNAS portfolio For reviewers Alerts About About PNAS Nexus About National Academy of Sciences Editorial Board Journals on Oxford Academic Books on Oxford Academic NAS Journals Issues Advance articles Subject All Subject Expand Expand Biological, Health, and Medical Sciences Administration Of Health Services, Education, and Research Agricultural Sciences Allied Health Professions Anesthesiology Anthropology Anthropology (Biological, Health, and Medical Sciences) Applied Biological Sciences Biochemistry Biophysics and Computational Biology (Biological, Health, and Medical Sciences) Biostatistics Cell Biology Dermatology Developmental Biology Ecology Environmental Sciences (Biological, Health, and Medical Sciences) Evolution Genetics Immunology and Inflammation Internal Medicine Medical Sciences Medical Microbiology Microbiology Neurology Neuroscience Nutrition Obstetrics and Gynecology Ophthalmology Pediatrics Pharmacology Pharmacy Physiology Plant Biology Population Biology Psychiatry Psychological and Cognitive Sciences (Biological, Health, and Medical Sciences) Public Health and Epidemiology Radiation Oncology Radiology Rehabilitation Surgery Sustainability Science (Biological, Health, and Medical Sciences) Systems Biology Physical Sciences and Engineering Aerospace Engineering Applied Mathematics Applied Physical Sciences Bioengineering Biophysics and Computational Biology (Physical Sciences and Engineering) Chemical Engineering Chemistry Civil and Environmental Engineering Computer Sciences Computer Science and Engineering Earth Resources Engineering Earth, Atmospheric, and Planetary Sciences Electric Power and Energy Systems Engineering Electronics, Communications and Information Systems Engineering Engineering Environmental Sciences (Physical Sciences and Engineering) Materials Engineering Mathematics Mechanical Engineering Physics Statistics Sustainability Science (Physical Sciences and Engineering) Social and Political Sciences Anthropology (Social and Political Sciences) Demography Economic Sciences Environmental Sciences (Social and Political Sciences) Political Sciences Psychological and Cognitive Sciences (Social and Political Sciences) Social Sciences Sustainability Science (Social and Political Sciences) Browse all content Browse content in Submit Author guidelines Submission site Open access policy Self-archiving policy Why submit to PNAS Nexus The PNAS portfolio For reviewers Alerts About About PNAS Nexus About National Academy of Sciences Editorial Board Close Navbar Search Filter PNAS Nexus This issue Social and Political Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Article Navigation Close mobile search navigation Article Navigation Volume 2 Issue 12 December 2023 (In Progress) Article Contents Abstract Introduction Results Discussion Methods and materials Acknowledgments Supplementary Material Funding Author Contributions Previous Presentation Preprints Data Availability ReferencesArticle Navigation Article Navigation Journal Article Toxic comments are associated with reduced activity of volunteer editors on Wikipedia Ivan Smirnov, Ivan Smirnov Graduate Research School, University of Technology Sydney , 15 Broadway , Sydney 2007, Australia To whom correspondence should be addressed: Email: ivan.smirnov@uts.edu.au https://orcid.org/0000-0002-8347-6703 Search for other works by this author on: Oxford Academic Google Scholar Camelia Oprea, Camelia Oprea Department of Computer Science, RWTH Aachen University , Ahornstrasse 55 , Aachen 52074, Germany Search for other works by this author on: Oxford Academic Google Scholar Markus Strohmaier Markus Strohmaier Business School, University of Mannheim , L 15 1–6 , Mannheim 68161, Germany GESIS—Leibniz Institute for the Social Sciences , Unter Sachsenhausen 6–8 , Köln 50667, Germany Complexity Science Hub Vienna , Josefstaedter Strasse 39 , Vienna 1080, Austria Search for other works by this author on: Oxford Academic Google Scholar Competing Interest: The authors declare no competing interest. Author Notes PNAS Nexus, Volume 2, Issue 12, December 2023, pgad385, https://doi.org/10.1093/pnasnexus/pgad385 Published: 05 December 2023 Article history Received: 30 June 2023 Accepted: 30 October 2023 Corrected and typeset: 05 December 2023 Published: 05 December 2023 PDF Split View Views Article contents Figures & tables Video Audio Supplementary Data Cite Cite Ivan Smirnov, Camelia Oprea, Markus Strohmaier, Toxic comments are associated with reduced activity of volunteer editors on Wikipedia, PNAS Nexus, Volume 2, Issue 12, December 2023, pgad385, https://doi.org/10.1093/pnasnexus/pgad385 Select Format Select format .ris (Mendeley, Papers, Zotero) .enw (EndNote) .bibtex (BibTex) .txt (Medlars, RefWorks) Download citation Close Permissions Icon Permissions Share Icon Share Facebook Twitter LinkedIn Email Navbar Search Filter PNAS Nexus This issue Social and Political Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Mobile Enter search term Search Close Navbar Search Filter PNAS Nexus This issue Social and Political Sciences NAS Journals Medicine and Health Science and Mathematics Social Sciences Books Journals Oxford Academic Enter search term Search Advanced Search Search Menu Abstract Wikipedia is one of the most successful collaborative projects in history. It is the largest encyclopedia ever created, with millions of users worldwide relying on it as the first source of information as well as for fact-checking and in-depth research. As Wikipedia relies solely on the efforts of its volunteer editors, its success might be particularly affected by toxic speech. In this paper, we analyze all 57 million comments made on user talk pages of 8.5 million editors across the six most active language editions of Wikipedia to study the potential impact of toxicity on editors’ behavior. We find that toxic comments are consistently associated with reduced activity of editors, equivalent to 0.5–2 active days per user in the short term. This translates to multiple human-years of lost productivity, considering the number of active contributors to Wikipedia. The effects of toxic comments are potentially even greater in the long term, as they are associated with a significantly increased risk of editors leaving the project altogether. Using an agent-based model, we demonstrate that toxicity attacks on Wikipedia have the potential to impede the progress of the entire project. Our results underscore the importance of mitigating toxic speech on collaborative platforms such as Wikipedia to ensure their continued success. Significance Statement While the prevalence of toxic speech online is well studied, its true impact on the productivity of online communities remains largely unexplored. In this study, we focus on Wikipedia, which as the largest and most-read online reference, serves as a vital source of knowledge for millions of users worldwide. By analyzing all comments made over 20 years on user talk pages of 8.5 million editors across multiple language editions, we demonstrate that toxic speech is associated with a significant loss in the productivity of Wikipedia editors. These findings may have broad implications for large-scale collaborative projects and online communities, emphasizing the need to promote healthy and sustainable communication practices to protect crucial online information ecosystems and ensure their long-term success. Introduction Wikipedia is arguably one of the most successful collaborative projects in history. It has become the largest and most-read reference work ever created, and it is currently the fifth most popular website on the Internet (1). Millions of users worldwide rely on Wikipedia as their first source of information when encountering a new topic, for fact-checking and in-depth research (2). Even if caution might be required when consulting less actively maintained pages (3), numerous studies have shown that Wikipedia is a reliable source of information in areas ranging from political science (4) to pharmacology (5) and its accuracy is comparable to traditional encyclopedias (6) and textbooks (7). One of the most remarkable aspects of Wikipedia’s success is that its content is exclusively created and curated by volunteer editors, known as Wikipedians. The English edition alone has more than 120,000 active editors (8). However, this volunteer-driven model also makes Wikipedia susceptible to the inherent challenges associated with maintaining such a large online community (9, 10). For example, it has been previously observed that Wikipedia is not free of conflict, particularly in the form of so-called edit wars (11), which impose significant costs on the project (12) and could negatively affect the quality of Wikipedia articles (13). In this paper, we focus on the impact of toxic comments directed toward editors on their activity. This aspect is less studied, but potentially not less important, as affected by toxic comments, Wikipedians might reduce their contributions or abandon the project altogether, threatening the success of the platform (14). Toxicity has been extensively studied on popular social media websites such as Twitter (15, 16), Reddit (17, 18), and similar platforms (19, 20). However, much of these research focuses on automated toxicity detection and prevalence estimation rather than on evaluating its impact (21). As an online encyclopedia, Wikipedia is often perceived as immune to toxicity and has a strict “No personal attacks” policy (22). Despite that, toxic speech and harassment have been previously observed on the platform (23–27). The effects of such behaviors on editors’ contributions are, however, not well understood nor well studied. The largest study to date relies on a voluntary opt-in survey of the 3,845 Wikipedians conducted in 2015 (24). It reports that 20% of users witnessing harassment have stopped contributing for a while, 17% considered not contributing anymore and 5% stopped contributing at all. In this paper, we analyzed all 57 million comments made on user talk pages of editors on the six most active language editions of Wikipedia (English, German, French, Spanish, Italian, Russian) to understand the potential impact of toxic speech on editors’ contributions (see Methods and materials section for our definition of toxic comments). User talk pages are a place for editors to communicate with each other either on more personal topics or to extend their discussion from an article’s talk page. The majority of toxic comments are left on user talk pages (28). The comments we study were extracted from revision histories of talk pages and, thus, include even those toxic comments that were later archived or deleted by the page owner. Figure 1 shows the activity of 50 randomly selected users who have received exactly one toxic comment. While some users are seemingly unaffected by a toxic comment, others temporarily reduce their activity or leave the project completely. The aim of our paper is to quantify this effect on the entire population of editors. Fig. 1. Open in new tabDownload slide After receiving a toxic comment many users temporarily reduce their activity or leave the project completely. The figure shows the activity of 50 randomly selected users who received exactly one toxic comment. Blue squares indicate an active day, i.e. a day when at least one edit was done, starting from the first contribution of a given user. Red triangles correspond to toxic comments. Note that while some users are resilient and their activity is seemingly unaffected by toxic comments, many users temporarily reduce their activity or stop contributing altogether. We estimate the number of lost active days associated with a toxic comment by comparing the number of active days before and after receiving a toxic comment. To account for potential baseline change, we have matched editors that received a toxic comment with similarly active editors who received a nontoxic comment. We have separately studied if toxic comments increase the probability of editors leaving the project altogether. Finally, we have used an agent-based model to model the potential impact of an increased number of toxic comments on Wikipedia. Results Loss of editor activity To estimate the potential effect of a toxic comment, we compute the proportion of users who were active on day X before or after receiving a toxic comment (Fig. 2). We find that, on average, editors are more active near the time when they receive a toxic comment, with a peak at 24 h prior to the comment. At this time point, more than 40 % of editors were active, as shown by the red line in Fig. 2a. This is a rather unsurprising observation since toxic comments are often made as a reaction to an edit made by a user and, thus, users are expected to be active around the time of a toxic comment. Note that if the timestamps around which the curve is centered are shuffled (black line in Fig. 2a) then this pattern disappears completely as expected. Fig. 2. Open in new tabDownload slide After receiving a toxic comment, users become less active. On average, users are more active near the time when they receive a toxic comment (peak at zero for the red line in panel a). Average activity across all users who have received a toxic comment is lower in all 100 days after the event compared to the corresponding days before (dashed and solid red lines in panel b). This cannot be explained by a baseline drop in activity after a nontoxic comment (dashed and solid blue lines in panel b). Similar results hold not only for the English edition but also for the other five editions (c–g). We also find that average activity across all users who have received a toxic comment is lower during all 100 days after the event compared to the corresponding days before (dashed and solid red lines in Fig. 2b), e.g. smaller number of users is active five days after receiving a toxic comment than five days before receiving it. To rule out the possibility that this is due to a general drop in activity over time or a drop in activity after any comment, we select a control group of users who have received a nontoxic comment, and whose average activity in the 100 days before the comment is the same as the average activity of users who received a toxic comment (see Methods and materials section for details). We observe a similar characteristic peak around the nontoxic comment, likely due to both toxic and nontoxic comments being reactions to a contribution made by an editor. However, in contrast to a toxic comment, a nontoxic comment does not lead to a significant decrease in activity (dashed and solid blue lines in Fig. 2b). Similar results hold for all six language editions that we have examined (Fig. 2c–g). We then estimate the lost activity associated with a toxic comment by computing the decrease in activity after a toxic comment, taking into account a potential baseline drop, i.e. by computing Δ = ( After toxic − Before toxic ) − ( After nontoxic − Before nontoxic ) ⁠. We find that this loss is statistically significant for all language editions studied (Table 1). We further explored the robustness of this result with respect to the toxicity threshold and potential filtering of users according to their activity. As expected, for higher toxicity thresholds, i.e. for more severely toxic comments, the effect is stronger (Supplementary Fig. S1). Considering only active users also leads to higher estimates; however, here we are reporting a conservative estimate, i.e. no filtering is used for results presented in Fig. 2 and Table 1. Table 1. Lost active days in the 100 days following a toxic comment. Edition Δ P-value𝑁 𝑢 𝑠 𝑒 𝑟 𝑠 English− 1.207 2.6 × 10 − 66 36 , 332 German− 0.546 1.5 × 10 − 7 10 , 346 French− 1.851 4.8 × 10 − 9 2 , 239 Spanish− 0.563 8.6 × 10 − 3 2 , 446 Italian− 0.336 2.3 × 10 − 2 3 , 567 Russian− 1.219 7.8 × 10 − 4 1 , 134 Edition Δ P-value𝑁 𝑢 𝑠 𝑒 𝑟 𝑠 English− 1.207 2.6 × 10 − 66 36 , 332 German− 0.546 1.5 × 10 − 7 10 , 346 French− 1.851 4.8 × 10 − 9 2 , 239 Spanish− 0.563 8.6 × 10 − 3 2 , 446 Italian− 0.336 2.3 × 10 − 2 3 , 567 Russian− 1.219 7.8 × 10 − 4 1 , 134 The lost active days are estimated by computing the difference between the number of active days during 100 days after a toxic comment and the number of active days during 100 days before a toxic comment. This difference is then compared with the baseline drop after a nontoxic comment, i.e. Δ = ( After toxic − Before toxic ) − ( After nontoxic − Before nontoxic ) ⁠. The P-value is computed using Student’s t-test. Open in new tab Table 1. Lost active days in the 100 days following a toxic comment. Edition Δ P-value𝑁 𝑢 𝑠 𝑒 𝑟 𝑠 English− 1.207 2.6 × 10 − 66 36 , 332 German− 0.546 1.5 × 10 − 7 10 , 346 French− 1.851 4.8 × 10 − 9 2 , 239 Spanish− 0.563 8.6 × 10 − 3 2 , 446 Italian− 0.336 2.3 × 10 − 2 3 , 567 Russian− 1.219 7.8 × 10 − 4 1 , 134 Edition Δ P-value𝑁 𝑢 𝑠 𝑒 𝑟 𝑠 English− 1.207 2.6 × 10 − 66 36 , 332 German− 0.546 1.5 × 10 − 7 10 , 346 French− 1.851 4.8 × 10 − 9 2 , 239 Spanish− 0.563 8.6 × 10 − 3 2 , 446 Italian− 0.336 2.3 × 10 − 2 3 , 567 Russian− 1.219 7.8 × 10 − 4 1 , 134 The lost active days are estimated by computing the difference between the number of active days during 100 days after a toxic comment and the number of active days during 100 days before a toxic comment. This difference is then compared with the baseline drop after a nontoxic comment, i.e. Δ = ( After toxic − Before toxic ) − ( After nontoxic − Before nontoxic ) ⁠. The P-value is computed using Student’s t-test. Open in new tab While these results demonstrate that our findings are not limited to one language, they should not be used to compare effects between language editions, as there is no guarantee that the same toxicity threshold for the toxicity detection algorithm will have the same meaning in different languages. Note that given that thousands of users have received at least one toxic comment (Supplementary Table S1), even a moderate loss per user could result in many human-years of lost productivity for Wikipedia in the short run. By multiplying the estimated loss per user from Table 1 by the number of users who have received at least one toxic comment, we could estimate the total loss of activity that is ranging from 5 human-years for Russian Wikipedia to 265 human-years for the English edition. The reason for the lasting effect of toxicity is that some new users might be discouraged by a toxic comment and choose to leave the project altogether after just a few contributions. This means that a single toxic comment could deprive Wikipedia of a potentially long-term contributor. To further investigate this effect, we compare the probability of leaving Wikipedia after receiving a toxic comment with the probability of leaving Wikipedia after receiving a nontoxic comment. Leaving Wikipedia We observed that the probability of leaving Wikipedia after N contributions declines with N. 𝑃 𝑁 ( leaving ) is approximately proportionate to 𝑁 − 𝛼 ⁠, where 𝛼 ranges from 0.89 to 1.02 ⁠, indicating a long-tailed distribution. While the probability of leaving the project after the first and only contribution is high (⁠ 𝑃 1 = 47 % for English Wikipedia), the risk of leaving Wikipedia drops to 0.7 % for users who have made 100 contributions. To study the potential effects of toxic comments, we separately consider contributions that are followed by a toxic comment and contributions that are not followed by a toxic comment (see Methods and materials section for details). We find that the risk of an editor leaving after a toxic comment is consistently higher for all editions and regardless of the contribution number, see Fig. 3. We provide an analysis of the significance of these findings in Supplementary Fig. S4. Fig. 3. Open in new tabDownload slide The probability of leaving Wikipedia after receiving a toxic comment is substantially higher than might be expected otherwise. For all six editions the probability of leaving declines with the number of contributions. At the same time, this probability is substantially higher after receiving a toxic comment than might be expected otherwise. Dots are probability estimates and solid lines are the best linear fit on a log-log scale. Agent-based modeling As has been demonstrated above, toxic comments increase the likelihood of editors abandoning Wikipedia. If enough editors leave, this could potentially impede the progress of the project as a whole. In order to estimate the potential impact of toxic comments, we model users’ behaviors by varying the toxicity of the environment, ranging from a nontoxic environment, where the probability of a user leaving follows the empirically observed nontoxic probability distribution, 𝑃 𝑁 non (blue dots in Fig. 3), to a highly toxic environment, where the probability of leaving corresponds to an empirically observed toxic probability distribution, 𝑃 𝑁 tox (red dots in Fig. 3). We also consider a potential attack targeted at new users. In this scenario, each user receives a toxic comment after their first and second contributions, e.g. their probability of leaving after the first and second contribution is defined by 𝑃 𝑁 tox ⁠, and after that follows the empirically observed 𝑃 𝑁 ⁠. For our modeling, we focus on a cohort of users who made their first contribution between the 4,000th and 6,000th day from the first recorded contribution to English Wikipedia in our dataset. We opted for this timeframe as it reflects Wikipedia’s current phase characterized by a relatively consistent number of active editors. This period follows the site’s initial exponential growth and a subsequent decline but comes before the anomalous increase in activity due to the COVID-19 pandemic (see Discussion section for details on these stages). For our modeling, we employed an agent-based approach. Each day, agents (representing users) join Wikipedia and make their first contribution. The number of agents joining each day is equal to the actual count of first-time contributors to English Wikipedia on that particular day. After their first contribution, agents keep contributing, following a Poisson process, i.e. in such a way that the distance between two consecutive contributions, D, follows an exponential distribution: 𝐷 ∼ Exp ( 𝜆 ) ⁠, where λ is estimated from empirical data. After each contribution, the agent’s probability of leaving the project is determined by the toxicity level, T, and the empirically observed distributions 𝑃 𝑁 non and 𝑃 𝑁 tox ⁠. In particular, after N’s contribution the user leaves the project with probability 𝑇 * 𝑃 𝑁 tox + ( 1 − 𝑇 ) * 𝑃 𝑁 non ⁠. If the toxicity level is 0, then the probability of leaving follows the nontoxic distribution 𝑃 𝑁 tox ⁠, and if the toxicity level is 1, then the probability of leaving follows the toxic distribution 𝑃 𝑁 tox ⁠. After the initial 2,000 days, no new agents join the project; however, we continue to model the behavior of the remaining agents for the subsequent 2,000 days, for which we have available empirical data for comparison. Our model generally reproduces the dynamics of user activity (Fig. 4), though, as expected, it cannot account for a later COVID-19-induced spike in activity. We find that an extreme level of toxicity could effectively reduce the cohort to almost no users in the long run, compared to the sustained numbers in a nontoxic setting or as observed in the data. Additionally, targeted attacks on newcomers have the potential to significantly decrease the number of active users, posing a risk to the project. The detailed results of our modeling, showing the effects of different toxicity levels on user count, are presented in Supplementary Fig. S6. Fig. 4. Open in new tabDownload slide High levels of toxicity and targeted attacks could significantly reduce the number of active editors. Modeling results for a cohort of editors making their first contribution during the relatively stable phase of Wikipedia (shaded region in the inset). The model reproduces the general dynamics of user activity (blue line) but, as expected, cannot capture the COVID-19-related spike in activity. An extreme level of toxicity (red line) could reduce the cohort to virtually no active users, contrasting with a nontoxic environment (green line) or actual activity (blue line). Targeted attacks on newcomers (orange line) have the potential to significantly reduce the number of active contributors. Discussion We conducted a large-scale analysis, covering all comments made on user talk pages of the six most active language editions of Wikipedia over a period of 20 years, and found that toxic comments are associated with a decreased activity of editors who have received these comments and an increased risk of them leaving the project altogether. Additionally, via agent-based modeling, we showed that toxicity attacks on Wikipedia have the potential to impede the progress of the entire project. The main limitation of our study is its relatively narrow scope, as it focuses solely on the association between toxic comments left on user talk pages and the subsequent decrease in users’ activity. However, this approach allowed us to formulate our findings with precision and ensure their robustness. We believe that our study complements and extends existing studies on Wikipedia and online communities more broadly, and may serve as a foundation for further exploration of the effects of toxicity, as we discuss in this section. Conflict on Wikipedia Conflict on Wikipedia has already been a subject of numerous studies, with particular attention given to so-called “edit wars” (11, 29, 30). These arise when groups of editors, disagreeing about page content, repeatedly override each other’s contributions. It has been estimated that edit wars can impose substantial conflict and coordination costs on Wikipedia (12). Furthermore, it has been demonstrated that these costs increase over time and a smaller proportion of the total work by Wikipedians directly contributes to new article content. Conflict could also undermine content quality. For instance, the level of conflict on discussion pages, as assessed by raters, has been shown to negatively correlate with the quality of the corresponding Wikipedia articles (13). In contrast to previous studies, our focus is on comments left on user talk pages rather than article talk pages. While this narrows the scope of our study, it also ensures that the comments we examine are directly addressed to a specific editor. Our approach also mitigates potential bias that could be introduced by the topic of an article. For instance, comments on talk pages linked to articles about violence might be misclassified as toxic by an algorithm due to the presence of highly negative keywords. It is possible that toxic comments we observe on user talk pages are not independent from a broader conflict occurring elsewhere on Wikipedia. Therefore, it is conceivable that the effect we observe is not purely explained by toxic comments, but also by a broader conflict which leads both to a toxic comment on a user talk page and decreased activity of this user. Future research is needed to address this limitation and explore the context in which toxic comments occur. It is worth noting, however, that it has already been established that toxicity on its own could lead users to stop contributing either temporarily or permanently, as this is what editors themselves report in surveys (24). Our study complements such studies by providing an estimate of the potential effects while also being performed on a scale that is not achievable by survey methods. Stages of Wikipedia life cycle Wikipedia has not grown linearly but has instead passed through several stages. It began with exponential growth (31), which subsequently slowed (32). Following that, the number of active users declined before Wikipedia entered its current stage, characterized by a relatively stable number of active users (33), with a slow decline observed in some language editions. A notable exception was a temporary spike in activity due to the COVID-19 pandemic (34). See Supplementary Fig. S5 for an illustration of these patterns in the editions studied in this paper. It has been found that the main reason for halted growth is a sharp decline in the retention of newcomers (35). Specifically, with the project’s development, the rejection of newcomer contributions has increased, demotivating them and driving them away. Our results complement these findings by highlighting that newcomers are also particularly vulnerable to toxic comments. If users receive a toxic comment after their first or second contributions, their chances of continuing to contribute are 1.8 times lower compared to users who did not receive toxic comments. Diversity of editors Wikipedia is often considered a neutral and unbiased source of knowledge. In fact, this is ingrained in its “Neutral point of view” policy, which is officially one of the five fundamental principles of Wikipedia (36). However, the claim of neutrality should not be accepted uncritically (37). For instance, while Wikipedia mandates that its content is supported by reliable sources, the selection of these sources can significantly deviate from the norms of the expert knowledge community, introducing biases to Wikipedia content (38). Even if the content of articles is neutral, their coverage may be biased. It is well documented, for example, that biographies of women are underrepresented on Wikipedia (39). Wikipedia’s own rules might contribute to such biases. For instance, providing reliable sources as required by Wikipedia for biographies of women might be challenging because fewer sources exist on women due to historic inequalities (40). Another case in point is the Oral Citations project, which aimed to use oral citations for content on countries that are underrepresented in other sources (41). However, this initiative was met with opposition by the English Wikipedia community. These content biases are closely connected to the lack of diversity among editors (38, 42). While estimates vary, the vast majority of Wikipedians are men (43). Notably, Wikipedia did not achieve its own goal of having at least 25% women editors by 2015 (44). This shortfall is a significant concern for the project, as diversity can improve the quality of content and reduce its biases (13, 45). While multiple barriers confront women editors on Wikipedia (40, 46, 47), toxicity is likely to be one of key factors contributing to the observed gender imbalance. Specifically, research has shown that while men and women are equally likely to face online harassment and abuse, women experience more severe violations (48). They are also more likely to be affected by such incidents and to self-censor in an attempt to prevent potential harassment (48). This has been confirmed in the Wikipedia context as well, where it has been demonstrated that the psychological experiences of women and men editors differ, leading to higher attrition rates among women (49). Similar results were found in another survey (24), showing that women experiencing toxicity are more likely to stop contributing in the future. Overall, there are reasons to believe that toxicity might significantly undermine the diversity of Wikipedia editors, which can, in turn, compromise the quality of Wikipedia articles and introduce biases in its coverage. This underscores the importance of our findings. While most of the existing studies focus on the gender gap, we want to emphasize that the Wikipedia diversity problem goes beyond that, including racial, nonbinary, and other biases as well (50–52). For instance, we observed that many of the toxic comments in our data set include ethnic slurs. Future studies are needed to better understand the experiences of minority groups on Wikipedia and the effects that toxicity has on them. Interventions The Wikipedia community is well aware of the aforementioned problems, and there have been multiple efforts to address them through various interventions. Research into reward systems showed that while they might work effectively for already highly productive editors, they fail to motivate less active editors (53). Another study found no significant effect of positive rewards in online communities (54). To address the gender gap in Wikipedia content, numerous events dedicated to creating entries about women were organized (46). An analysis of such interventions, which focused on two popular feminist interventions, confirmed that they succeeded in introducing content about women that would otherwise be missing (55). However, there is still a need to address the gender gap on a more systematic and sustainable level. For instance, one study showed that most of the women activists who attended editing workshops later chose not to continue contributing to Wikipedia, citing safety concerns as their primary reason (46). This issue was echoed in another study which identified safety as a core concern for women editors (56). A suggested solution to this problem has been the red-flagging of harassment and harassers (46). However, the opinion that toxic comments are negligible and should be seen as merely over-enthusiastic participation is still present among editors (25). Furthermore, various anti-harassment measures have been declined multiple times by the community, as they were seen to slow the process of content creation (57, 58). Based on our findings, we believe there is a need to reevaluate these policies, and more research attention is required to understand the impact of potential interventions. The wider role of peer-production systems Wikipedia plays a crucial role in the global information infrastructure, aiming to provide millions of people with access to free, unbiased knowledge. Due to its reputation as a neutral and comprehensive information source, it has become a trusted first choice source of knowledge for many and its articles frequently appear in top search engine results (59, 60). In fact, studies have shown that Google search results rely heavily on Wikipedia, and the quality of these results significantly diminishes without Wikipedia (61). Beyond search engines, Wikipedia was shown to be valuable to other online communities such as Stack Exchange and Reddit (62). While Wikipedia is arguably the most successful peer-production system, it is certainly not the only one. Others include hundreds of wikis hosted by Fandom, the numerous question-and-answer communities of Stack Exchange, and various other platforms ranging from online maps to online learning (33). Interestingly, for these projects, the same patterns that are typical of Wikipedia have been observed (63), i.e. the initial growth in number of contributors is followed by a decline characterized by a decreased retention of newcomers. This suggests that our findings might have broader implications for large-scale collaborative projects and online communities. It emphasizes the need to promote healthy and sustainable communication practices to protect crucial online information ecosystems and ensure their long-term success. Methods and materials Data and preprocessing Comments on user talk pages The Wikimedia Foundation provides publicly accessible dumps of all the different wikis’ content.a These dumps are updated on a regular basis, with complete revision history dumps generated once per month. For this paper, we used the English dump from 2021 November 1, the German dump from 2022 August 1, the French, Italian, and Spanish dumps from 2022 August 1, and the Russian dump from 2022 July 1. The data was obtained from a mirror hosted by the Umeå University, Sweden.b From the dumps, the user talk pages were extracted. A user’s talk page is a place where other editors can communicate with the user either on more personal topics or to extend their discussion from an article talk page. When the comments left on the talk page are resolved or become too old, users can choose to archive them. This helps them keep better track of new incoming topics. Once archived, the old comments are not displayed on the talk page anymore but are rather linked in a subpage. Nevertheless, the entire history of the user talk page, as of any other page on Wikipedia, can be fully seen under the tab of revision history. The revision history records one entry for every edit made on the page saving each time the complete content of the page. Thus retrieving a single comment requires performing the difference between two consecutive revisions. The Wikimedia API does offer a method to compute the difference between two revisions, however, applying it on a scale that was necessary for this research was unfeasible. For that reason, we developed our own parser to extract comments as a difference between two versions of the page (64). We excluded from our analysis talk pages that belong to unregistered users, e.g. users who are represented only by an IP address rather than a user name, because IP addresses are dynamic and it can not be assumed that one address represents a single user throughout Wikipedia history. Additionally, we have excluded comments made by officially registered bots. Comments that were made by users on their own pages are also not considered. When extracting comments, we cleared wiki-specific formatting and HTML markup, i.e. removed links, attachments, or other formatting-specific sequences irrelevant to the actual content. Contributions and active days In order to extract information on users’ contributions, i.e. edits of Wikipedia pages made by them, we used the MediaWiki API to retrieve timestamps for each edit made by a given user. The resulting data set is publicly available in the project repository (64). The timestamps of contributions were then converted into active days. Specifically, each user i was represented as a binary vector 𝑢 𝑖 = ( 𝑎 𝑖 1 , 𝑎 𝑖 2 , … , 𝑎 𝑖 𝑁 ) ⁠, where 𝑎 𝑖 𝑑 = 1 if user i made at least one contribution, i.e. edited a Wikipedia page, within the 24-h period corresponding to day d and 𝑎 𝑖 𝑑 = 0 otherwise. N is the number of days between the first recorded contribution in our data set and the last. The conversion from contribution count to active days was performed because it is hard to interpret and compare the total number of contributions between users as one large contribution could be equivalent to multiple smaller ones. Additionally, the size of a contribution does not necessarily reflect the effort put into it. While being active on a given day could still mean different levels of activity for different users, it represents a certain level of engagement with the project and is substantially different from not contributing at all on a given day. Toxicity The automatic detection of offensive language in online communities has been an active area of research since at least 2010 (65). Over the past decade, researchers have focused on detecting closely-related and intersecting types of offensive language such as toxicity, abusive language, and hate speech (66), see (67) for an overview of recent advancements in the field. In this paper, we use a model from the Perspective API (68) to identify toxic comments. This is a state-of-the-art toxicity detection algorithm that obtained competitive results at OffensEval-2019 competition (69) without any additional training on the contest data and is often used as a baseline system for toxicity detection (66). Perspective API is used across multiple platforms, including The New York Times, Der Spiegel, Le Monde, and El País. It uses BERT (Bidirectional Encoder Representations from Transformers) architecture (70) and is trained on comments from a variety of online sources, including Wikipedia. Each comment is labeled by 3–10 crowdsourced raters. Perspective models provide scores for several different attributes, see Supplementary Table S2 for the list of attributes and their definitions, see Supplementary Table S2 for examples of toxic comments, and see Supplementary Table S3 for the AUC (Area Under the Curve) scores for those languages and attributes that were used in this paper. We define a toxic comment as a comment that has a score of at least 0.8 on any of the six dimensions provided by Perspective API. The 0.8 score means that on average 8 out of 10 raters would mark it as toxic. As this threshold can be considered arbitrary, we perform additional robustness checks using different toxicity thresholds. In particular, we compute activity loss not only for the threshold of 0.8 (Table 1) but for thresholds from 0.2 to 0.9 ⁠. Additionally, we applied different activity filters, e.g. we separately compute an estimate only for those users who were active at least X days in the past 100 days where X varies from 0 to 50. This is done in order to ensure that the results are not exclusively driven by those users who had made few edits and then stopped contributing to the project. We perform this analysis for English Wikipedia as it is the largest edition. As shown in Supplementary Fig. S1, the estimate is typically in the range from − 0.5 to − 2 and significantly lower than zero for all activity thresholds and all toxicity thresholds higher than 0.3 ⁠. Similarly, we have checked how the toxicity threshold affects the probability of leaving the project. As might be expected, results remain qualitatively the same for different toxicity thresholds but higher thresholds lead to more extreme results, e.g. the probability of leaving after a toxic comment with 0.9 score is even higher than after a toxic comment with toxicity score of 0.8 (Supplementary Fig. S3). We also evaluated the robustness of our results with respect to misclassification errors. To achieve a realistic distribution of user activity, we repeatedly sampled 100,000 editors and their activity histories from the English Wikipedia data set. These sampled users were then divided into two groups: treatment and control. We investigated two distinct scenarios: one involving an equal split between the treatment and control groups and a second, more realistic, scenario where the treatment group constituted 1% of the control group. In the treatment group, we randomly removed one active day from each user, thereby generating a true effect of one lost active day per user. We then introduced misclassification errors by generating false positives (moving users from control to treatment group) and false negatives (moving users from treatment to control group). Finally, we compared the estimated effect, as a function of the error rate, with the true effect. We find that, generally, misclassification leads to the underestimation of the true effect, becoming more pronounced with higher error rates (Supplementary Fig. S2). The only exception is in the case of false negatives, i.e. undetected toxic comments, in the realistic scenario. Here, misclassification does not significantly bias the estimate, though it does increase its variance. Perspective API accepts texts up to 20,480 bytes. As the majority of comments are well below this limit, we have excluded those that are larger. Activity loss Users who have received at least one toxic comment constitute our treatment group. For each user in this group, we select a random toxic comment they have received. We then center user activity around the timestamp, 𝑡 𝑖 tox ⁠, of that toxic comment and convert the result to active days by calculating sign ({ 𝑡 ∈ 𝑇 𝑖 : 𝑡 ∈ [ 𝑡 𝑖 tox + 𝑑 * 24 * 60 * 60 , 𝑡 𝑖 tox + ( 𝑑 + 1 ) * 24 * 60 ) }) , where 𝑇 𝑖 is the set of timestamps of all contributions made by user i, and d is a day ranging from − 100 to 100. Finally, the results are averaged over all users. We repeat the procedure of selecting a random toxic comment 100 times and report average results. However, since most users received only one toxic comment, there is little variation across simulations and the average over 100 simulations is almost identical to the result of a single simulation. We then compare these results with a control group comprised of users who did not receive any toxic comments. However, a direct comparison is complicated because users who have received a toxic comment are, on average, more active than those who have not. This is probably due to the fact that each contribution could lead to a toxic response with a certain probability. Hence, the more contributions a user makes, the higher the likelihood of receiving a toxic comment and thereby being in the treatment group. Specifically, if each contribution can lead to a toxic comment with a probability p, then the probability of receiving at least one toxic comment depends on the number of contribution, N: 𝑃 ( gettoxiccomment ) = 1 − ( 1 − 𝑝 ) 𝑁 ( 1 ) ⁠. To ensure our control group is similarly active as the treatment group, we randomly select users with a probability based on the number of their contributions using formula (1). Users selected in this manner form the control group. For these users, we then pick a nontoxic comment at random, center their activity around its timestamp, and follow the same procedure used for the treatment group. To test for the significance of the results, we compute 95% bootstrapped confidence intervals for each estimate. Probability of leaving For each toxic comment, we find the closest in time contribution that precedes that comment. We define such contributions as “contributions followed by a toxic comment” and compare the probability of leaving after such contributions with the probability of leaving after other contributions. The probability of leaving after N contributions is estimated as a fraction of users who have made exactly N contributions among users who have made at least N contributions. As the probability of leaving strongly depends on N, we make a comparison separately for each contribution number 𝑁 ∈ [ 1 , 100 ] ⁠. For 𝑁 > 100 the number of users is too small to provide reliable estimates for comparison. a https://meta.wikimedia.org/wiki/Data˙dumps [accessed on 2023 January 20]. b https://mirror.accum.se/mirror/wikimedia.org/ Acknowledgments We acknowledge the Master’s thesis by Brückner (71), which identified a potential pattern in data and provided an inspiration for the design of the study presented in this paper. The initial data collection and experiments were carried out as part of Camelia Oprea’s Master’s thesis (72). We thank Liubov Tupikina and David Garcia for their valuable discussions regarding the results presented in this article. We thank the anonymous reviewers for their insightful comments and suggestions. Supplementary Material Supplementary material is available at PNAS Nexus online. Funding The publication of this article was funded by the University of Mannheim. Author Contributions I.S., C.O., and M.S. designed the study; I.S. and C.O. collected and analyzed the data; I.S., C.O., and M.S. wrote the manuscript; I.S. revised the manuscript. Previous Presentation These results were previously presented at International Conference on Computational Social Science 2023. Preprints A preprint of this article is published at https://doi.org/10.48550/arXiv.2304.13568 Data Availability The data underlying this article is available in Open Science Framework at https://osf.io/2qyxj/. References 1 Semruch. 2023. Most visited websites in the world [accessed 2023 Sept]. https://www.semrush.com/website/top/. 2 Singer P , et al. 2017 . Why we read Wikipedia. In: Proceedings of the 26th International Conference on World Wide Web. Perth, Australia: Association for Computing Machinery. p. 1591–1600 . 3 Bruckman AS . 2022 . Should you believe Wikipedia? Online communities and the construction of knowledge . Cambridge, UK : Cambridge University Press . Google Scholar Crossref Search ADS 4 Brown AR . 2011 . Wikipedia as a data source for political scientists: accuracy and completeness of coverage . PS: Political Sci Politics . 44 ( 2 ): 339 – 343 . Google Scholar OpenURL Placeholder Text 5 Clauson KA , Polen HH , Boulos MNK , Dzenowagis JH . 2008 . Scope, completeness, and accuracy of drug information in Wikipedia . Ann Pharmacother . 42 ( 12 ): 1814 – 1821 . Google Scholar Crossref Search ADS PubMed 6 Giles J . 2005 . Internet encyclopaedias go head to head . Nature . 438 ( 15 ): 900 – 901 . Google Scholar PubMed OpenURL Placeholder Text 7 Kräenbring J , et al. 2014 . Accuracy and completeness of drug information in Wikipedia: a comparison with standard textbooks of pharmacology . PLoS ONE . 9 ( 9 ): e106930 . Google Scholar Crossref Search ADS PubMed 8 Wikipedia. 2023. List of Wikipedias [accessed 2023 Sept]. https://en.wikipedia.org/wiki/List˙of˙Wikipedias. 9 Kraut RE , Resnick P . 2012 . Building successful online communities: evidence-based social design . Cambridge, MA, USA : MIT Press . Google Scholar Crossref Search ADS 10 Keegan B , Fiesler C . 2017 . The evolution and consequences of peer producing Wikipedia’s rules. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 11. Montreal, Canada: Association for the Advancement of Artificial Intelligence. p. 112–121 . 11 Yasseri T , Sumi R , Rung A , Kornai A , Kertész J . 2012 . Dynamics of conflicts in Wikipedia . PLoS ONE . 7 ( 6 ): e38869 . Google Scholar Crossref Search ADS PubMed 12 Kittur A , Suh B , Pendleton BA , Chi EH . 2007 . He says, she says: conflict and coordination in Wikipedia. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. San Jose, CA, USA: Association for Computing Machinery. p. 453–462 . 13 Arazy O , Nov O , Patterson R , Yeo L . 2011 . Information quality in Wikipedia: the effects of group composition and task conflict . J Manag Inf Syst . 27 ( 4 ): 71 – 98 . Google Scholar Crossref Search ADS 14 Preece J . 2001 . Sociability and usability in online communities: determining and measuring success . Behav Inf Technol . 20 ( 5 ): 347 – 356 . Google Scholar Crossref Search ADS 15 Chatzakou D , et al. 2017 . Measuring# gamergate: a tale of hate, sexism, and bullying. In: Proceedings of the 26th International Conference on World Wide Web. Perth, Australia: Association for Computing Machinery. p. 1285–1290 . 16 Guberman J , Schmitz C , Hemphill L . 2016 . Quantifying toxicity and verbal violence on twitter. In: Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing. San Francisco, CA, USA: Association for Computing Machinery. p. 277–280 . 17 Xia Y , Zhu H , Lu T , Zhang P , Gu N . 2020 . Exploring antecedents and consequences of toxicity in online discussions: a case study on Reddit . Proc ACM Hum-Comput Interact . 4 : 1 – 23 . Google Scholar Crossref Search ADS 18 Almerekhi H , Jansen BJ , Kwak H . 2020 . Investigating toxicity across multiple Reddit communities, users, and moderators. In: Companion Proceedings of the Web Conference. Taipei, Taiwan: Association for Computing Machinery. p. 294–298 . 19 Wich M , et al. 2022 . Introducing an abusive language classification framework for telegram to investigate the german hater community. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 16. Atlanta, GA, USA: Association for the Advancement of Artificial Intelligence. p. 1133–1144 . 20 Silva L , Mondal M , Correa D , Benevenuto F , Weber I . 2016 . Analyzing the targets of hate in online social media. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 10. Cologne, Germany: Association for the Advancement of Artificial Intelligence. p. 687–690 . 21 Kiritchenko S , Nejadgholi I , Fraser KC . 2021 . Confronting abusive language online: a survey from the ethical and human rights perspective . J Artif Intell Res . 71 : 431 – 478 . Google Scholar Crossref Search ADS 22 Wikipedia . Personal attacks, 2023 . https://en.wikipedia.org/wiki/Wikipedia:No_personal_attacks. 23 Arazy O , Yeo L , Nov O . 2013 . Stay on the Wikipedia task: when task-related disagreements slip into personal and procedural conflicts . J Am Soc Inf Sci Technol . 64 ( 8 ): 1634 – 1648 . Google Scholar Crossref Search ADS 24 Wikimedia Foundation . Harassment survey, 2023 . https://upload.wikimedia.org/wikipedia/commons/5/52/Harassment˙Survey˙2015˙-˙Results˙Report.pdf. 25 Corple DJ . 2016 . Beyond the gender gap: understanding women's participation in Wikipedia. Open Access Theses. 936 . https://docs.lib.purdue.edu/open_access_theses/936. 26 Wulczyn E , Thain N , Dixon L . 2017 . Ex machina: personal attacks seen at scale. In: Proceedings of the 26th International Conference on World Wide Web. Perth, Australia: Association for Computing Machinery. p. 1391–1399 . 27 Raish M . 2019 . Identifying and classifying harassment in arabic Wikipedia: a “netnography” [accessed 2023 Sept]. https://upload.wikimedia.org/wikipedia/commons/7/78/Arabic_Harassment_Netnography_Report.pdf. 28 Qu I , Thain N , Hua Y . 2019 . Wikidetox visualization. In: Wiki Workshop; San Francisco, CA, USA . 29 Sumi R , Yasseri T . 2011 . Edit wars in Wikipedia. In: 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing. Boston, MA, USA: Institute of Electrical and Electronics Engineers. p. 724–727 . 30 Chhabra A , Kaur R , Iyengar SRS . 2020 . Dynamics of edit war sequences in Wikipedia. In: Proceedings of the 16th International Symposium on Open Collaboration. Virtual Event, Spain: Association for Computing Machinery. p. 1–10 . 31 Almeida RB , Mozafari B , Cho J . 2007 . On the evolution of Wikipedia. In: Proceedings of the International Conference on Web and Social Media (ICWSM). Boulder, CO, USA: Association for the Advancement of Artificial Intelligence . 32 Suh B , Convertino G , Chi EH , Pirolli P . 2009 . The singularity is not near: slowing growth of Wikipedia. In: Proceedings of the 5th International Symposium on Wikis and Open Collaboration. Orlando, FL, USA: Association for Computing Machinery. p. 1–10 . 33 Hill BM , Shaw A . 2020 . Wikipedia and the end of open collaboration. Wikipedia, 20 . 34 Ruprechter T , et al. 2021 . Volunteer contributions to Wikipedia increased during Covid-19 mobility restrictions . Sci Rep . 11 ( 1 ): 21505 . Google Scholar Crossref Search ADS PubMed 35 Halfaker A , Geiger RS , Morgan JT , Riedl J . 2013 . The rise and decline of an open collaboration system: how Wikipedia’s reaction to popularity is causing its decline . Am Behav Sci . 57 ( 5 ): 664 – 688 . Google Scholar Crossref Search ADS 36 Wikipedia: neutral point of view [accessed 2023 Oct]. https://en.wikipedia.org/wiki/Wikipedia:Neutral˙point˙of˙view. 37 Matei SA , Dobrescu C . 2011 . Wikipedia’s “neutral point of view”: settling conflict through ambiguity . Inf Soc . 27 ( 1 ): 40 – 51 . Google Scholar Crossref Search ADS 38 Luyt B . 2012 . The inclusivity of Wikipedia and the drawing of expert boundaries: an examination of talk pages and reference lists . J Am Soc Inf Sci Technol . 63 ( 9 ): 1868 – 1878 . Google Scholar Crossref Search ADS 39 Wagner C , Garcia D , Jadidi M , Strohmaier M . 2015 . It’s a man’s Wikipedia? Assessing gender inequality in an online encyclopedia. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 9. Oxford, UK: Association for the Advancement of Artificial Intelligence. p. 454–463 . 40 Ford H , Wajcman J . 2017 . ‘anyone can edit’, not everyone does: Wikipedia’s infrastructure and the gender gap . Soc Stud Sci . 47 ( 4 ): 511 – 527 . Google Scholar Crossref Search ADS PubMed 41 Gallert P , Van der Velden M . 2013 . Reliable sources for indigenous knowledge: dissecting Wikipedia’s catch-22 [accessed 2023 Sept]. https://upload.wikimedia.org/wikipedia/commons/5/51/Indigenous_Knowledge_for_Wikipedia.pdf. 42 Lam STK , et al. 2011 . Wp: clubhouse? an exploration of Wikipedia’s gender imbalance. In: Proceedings of the 7th International Symposium on Wikis and Open Collaboration. Mountain View, CA, USA: Association for Computing Machinery. p. 1–10 . 43 Hill BM , Shaw A . 2013 . The Wikipedia gender gap revisited: characterizing survey response bias with propensity score estimation . PLoS ONE . 8 ( 6 ): e65782 . Google Scholar Crossref Search ADS PubMed 44 Wikipedia ‘completely failed’ to fix gender imbalance [accessed 2023 Oct]. https://www.bbc.com/news/av/business-28701772. 45 Sydow M , Baraniak K , Teisseyre P . 2017 . Diversity of editors and teams versus quality of cooperative work: experiments on Wikipedia . J Intell Inf Syst . 48 : 601 – 632 . Google Scholar Crossref Search ADS 46 Lir SA . 2021 . Strangers in a seemingly open-to-all Website: the gender bias in Wikipedia . Equal Divers Incl Int J . 40 ( 7 ): 801 – 818 . Google Scholar Crossref Search ADS 47 Menking A , Erickson I . 2015 . The heart work of Wikipedia: gendered, emotional labor in the world’s largest online encyclopedia. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. Seoul, Republic of Korea: Association for Computing Machinery. p. 207–210 . 48 Lenhart A , Ybarra M , Zickuhr K , Prive-Feeney M . 2016 . Online harassment, digital abuse, and cyberstalking in America [accessed 2023 Sept]. https://www.datasociety.net/pubs/oh/Online_Harassment_2016.pdf. 49 Bear JB , Collier B . 2016 . Where are the women in Wikipedia? understanding the different psychological experiences of men and women in Wikipedia . Sex Roles . 74 : 254 – 265 . Google Scholar Crossref Search ADS 50 Lemieux ME , Zhang R , Tripodi F . 2023 . “too soon” to count? How gender and race cloud notability considerations on Wikipedia . Big Data Soc . 10 ( 1 ): 20539517231165490 . Google Scholar OpenURL Placeholder Text 51 Tripodi F . 2023 . Ms. categorized: gender, notability, and inequality on Wikipedia . New Media Soc . 25 ( 7 ): 1687 – 1707 . Google Scholar Crossref Search ADS 52 Field A , Park CY , Lin KZ , Tsvetkov Y . 2022 . Controlled analyses of social biases in Wikipedia bios. In: Proceedings of the ACM Web Conference 2022. Lyon, France: Association for Computing Machinery. p. 2624–2635 . 53 Restivo M , van de Rijt A . 2014 . No praise without effort: experimental evidence on how rewards affect Wikipedia’s contributor community . Inf Commun Soc . 17 ( 4 ): 451 – 462 . Google Scholar Crossref Search ADS 54 Cheng J , Danescu-Niculescu-Mizil C , Leskovec J . 2014 . How community feedback shapes user behavior. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 8. Ann Arbor, MI, USA: Association for the Advancement of Artificial Intelligence. p. 41–50 . 55 Langrock I , González-Bailón S . 2022 . The gender divide in Wikipedia: quantifying and assessing the impact of two feminist interventions . J Commun . 72 ( 3 ): 297 – 321 . Google Scholar OpenURL Placeholder Text 56 Menking A , Erickson I , Pratt W . 2019 . People who can take it: how women Wikipedians negotiate and navigate safety. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Glasgow, UK: Association for Computing Machinery. p. 1–14 . 57 Wikimedia Foundation . Community wishlist survey. Bots and gadgets, 2023 . https://meta.wikimedia.org/wiki/Community˙Wishlist˙Survey˙2015/Bots˙and˙gadgets. 58 Wikimedia Foundation . Community wishlist survey. Anti-harassment, 2023 . https://meta.wikimedia.org/wiki/Community˙Wishlist˙Survey˙2021/Anti-harassment. 59 Vincent N , Hecht B . 2021 . A deeper investigation of the importance of Wikipedia links to search engine results . Proc ACM Hum-Comput Interact . 5 ( CSCW1 ): 1 – 15 . Google Scholar PubMed OpenURL Placeholder Text 60 Vincent N , Johnson I , Sheehan P , Hecht B . 2019 . Measuring the importance of user-generated content to search engines. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 13. Munich, Germany: Association for the Advancement of Artificial Intelligence. p. 505–516 . 61 McMahon C , Johnson I , Hecht B . 2017 . The substantial interdependence of Wikipedia and Google: a case study on the relationship between peer production communities and information technologies. In: Proceedings of the International AAAI Conference on Web and Social Media. Vol. 11. Montreal, Canada: Association for the Advancement of Artificial Intelligence. p. 142–151 . 62 Vincent N , Johnson I , Hecht B . 2018 . Examining Wikipedia with a broader lens: quantifying the value of Wikipedia’s relationships with other large-scale online communities. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Montreal, Canada: Association for Computing Machinery. p. 1–13 . 63 TeBlunthuis N , Shaw A , Hill BM . 2018 . Revisiting “the rise and decline” in a population of peer production projects. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Montreal, Canada: Association for Computing Machinery. p. 1–7 . 64 Smirnov I , Oprea C . 2023 . Toxic comments reduce activity of volunteer editors on Wikipedia: data & code , 2023. https://osf.io/2qyxj. 65 Xu Z , Zhu S . 2010 . Filtering offensive language in online communities using grammatical relations. In: Proceedings of the Seventh Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference. Redmond, WA, USA: Association for Computing Machinery. p. 1–10 . 66 Fortuna P , Soler J , Wanner L . 2020 . Toxic, hateful, offensive or abusive? what are we really classifying? an empirical analysis of hate speech datasets. In: Proceedings of the 12th Language Resources and Evaluation Conference. Marseille, France: Association for Computational Linguistics. p. 6786–6794 . 67 Zampieri M , et al. 2020 . Semeval-2020 task 12: multilingual offensive language identification in social media (offenseval 2020), arXiv, arXiv:2006.07235, preprint: not peer reviewed . 68 Perspective API . Technical documentation, 2023 . https://developers.perspectiveapi.com/s/about-the-api. 69 Zampieri M , et al. 2019 . Semeval-2019 task 6: identifying and categorizing offensive language in social media (offenseval), arXiv, arXiv:1903.08983, preprint: not peer reviewed . 70 Lees A , et al. 2022 . A new generation of perspective API: efficient multilingual character-level transformers, arXiv, arXiv:2202.11176, preprint: not peer reviewed . 71 Brückner S . 2021 . Modeling sociodemographic attributes of Wikipedia editors [master thesis]. RWTH Aachen . 72 Oprea C . 2022 . Determining the impact of toxicity on Wikipedia’s talk pages [master thesis]. RWTH Aachen . Author notes Competing Interest: The authors declare no competing interest. © The Author(s) 2023. Published by Oxford University Press on behalf of National Academy of Sciences. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Subject Social and Political Sciences Issue Section: Social and Political Sciences Editor: Katherine Ognyanova Katherine Ognyanova Editor Search for other works by this author on: Oxford Academic Google Scholar Download all slides Supplementary data Supplementary data pgad385_Supplementary_Data - pdf file Advertisement Citations Views 0 Altmetric More metrics information Metrics Total Views 0 0 Pageviews 0 PDF Downloads Since 12/6/2023 Citations Powered by Dimensions Altmetrics × Email alerts Article activity alert Advance article alerts New issue alert In progress issue alert Subject alert Receive exclusive offers and updates from Oxford Academic Citing articles via Google Scholar Latest Most Read Most Cited Systematic assessment of transcriptomic and metabolic reprogramming by blue light exposure coupled with aging Toxic comments are associated with reduced activity of volunteer editors on Wikipedia Temperature shapes language sonority: Revalidation from a large dataset The silver bullet that wasn’t: Rapid agronomic weed adaptations to glyphosate in North America Strategies to obtain highly-ordered deuterated ices presented on the example of ice XIV More from Oxford Academic Medicine and Health Science and Mathematics Social Sciences Books Journals Advertisement Advertisement close advertisement Advertisement About PNAS Nexus Editorial Board Author guidelines Twitter LinkedIn Facebook Researcher Contact PNAS Nexus Advertising and Corporate Services Journals Career Network Online ISSN 2752-6542 Copyright © 2023 National Academy of Sciences About Oxford Academic Publish journals with us University press partners What we publish New features Authoring Open access Purchasing Institutional account management Rights and permissions Get help with access Accessibility Contact us Advertising Media enquiries Oxford University Press News Oxford Languages University of Oxford Oxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide Copyright © 2023 Oxford University Press Cookie settings Cookie policy Privacy policy Legal notice Close Close This Feature Is Available To Subscribers Only Sign In or Create an Account Close This PDF is available to Subscribers Only View Article Abstract & Purchase Options For full access to this pdf, sign in to an existing account, or purchase an annual subscription. Close Oxford University Press uses cookies to enhance your experience on our website. By selecting ‘accept all’ you are agreeing to our use of cookies. You can change your cookie settings at any time. More information can be found in our Cookie Policy. Cookie settings Accept all",
    "commentLink": "https://news.ycombinator.com/item?id=38530466",
    "commentBody": "Toxic comments are associated with reduced volunteer activity on WikipediaHacker NewspastloginToxic comments are associated with reduced volunteer activity on Wikipedia (oup.com) 249 points by geox 20 hours ago| hidepastfavorite302 comments tivert 18 hours agoHere&#x27;s how they define toxicity (only clearly described in their \"Supplementary material\"): https:&#x2F;&#x2F;oup.silverchair-cdn.com&#x2F;oup&#x2F;backfile&#x2F;Content_public&#x2F;...Honestly, that&#x27;s probably the most \"measurable\" kind of toxicity, but Wikipedia has a much bigger problem with toxicity than that. The whole place is infused with passive-aggression (by policy) and toxic double-standards. IMHO, you have to be deeply weird to be a frequent contributor. reply natpalmer1776 16 hours agoparentTo some degree you see this pattern repeated over and over anytime you have an organization gain any sort of longevity.A culture develops to create a power structure that favors those who have devoted a large portion of their personal identity to the success of the organization. This culture serves as a moat against anyone who would integrate themselves within the organization and attempt drastic changes that would disrupt the existing power balance or pose an existential threat to the organization.As time passes, more layers get added in response to various perceived attempts to subvert the organization until a critical mass is reached in which the organization suffers from a brain drain (via retirement, loss of interest, etc.) with the barriers too high and rewards too meager for new(er) qualified individuals to consider filling the void. The organization then continues forward in a zombie-like state until it either fails or becomes irrelevant. reply ChrisMarshallNY 10 hours agorootparent> A culture develops to create a power structure that favors those who have devoted a large portion of their personal identity to the success of the organization. This culture serves as a moat against anyone who would integrate themselves within the organization and attempt drastic changes that would disrupt the existing power balance or pose an existential threat to the organization.That pretty much directly describes a once-marvelous Q&A site, where I almost never go, anymore, and which is rapidly declining, in search results. reply throwaway2037 1 hour agorootparentReal question: Where do you go instead? Or, do you use Google to find blog posts with the answers that you need? I agree that the formerly-great Q&A site (that shall remain unnamed) has fallen from grace with terrible new leadership. The drama seems never to end. I vote to fork the whole thing and bring back Monica. :)For my areas of interest (plain old Java, Python, C, C++, and JavaScript), the search results are still good. However, I can understand that your areas may be in decline. reply paulmd 2 hours agorootparentprev[your comment has been marked as a duplicate] reply ReptileMan 10 hours agorootparentprevBut it provided the training data for ChatGPT reply schneehertz 7 hours agorootparentprevAre you talking about Quora? reply ternaryoperator 6 hours agorootparentI read it to mean Stack Overflow, whose drop in traffic has been widely discussed. But it could be Quora or other sites--effectively validating OP&#x27;s original point. reply throwaway290 5 hours agorootparentThe drop of traffic to SO is due to LLM proliferation. Some people use LLM to get answers (trained on SO etc), other people use LLM to spam SO. reply david_allison 5 hours agorootparentStackExchange has been going downhill since late 2019. A significant number of power users (moderators) have left and the writing has been on the wall for a number of years.https:&#x2F;&#x2F;meta.stackexchange.com&#x2F;questions&#x2F;333965&#x2F;firing-mods-...https:&#x2F;&#x2F;meta.stackexchange.com&#x2F;questions&#x2F;394772&#x2F;lets-talk-ab... reply throwaway290 3 hours agorootparentMost StackExchange sites are niche and just get fewer users naturally because topics are covered by other forums.I&#x27;m a power user on SO with some moderation capabilities. There&#x27;s no mods per se I think just people who have different functions enabled to them (editing questions, closing, review etc). Admittedly I don&#x27;t moderate it much (never did)I still use it daily as a lazy reference. Haven&#x27;t noticed any decline apart from it having covered all topics so there&#x27;s mostly nothing new left to ask. Which is the best thing that can happen to Q&A site reply moate 6 hours agorootparentprevAnd&#x2F;or Stack reply throwaway2037 1 hour agorootparentprevThe reflection of human society. reply Gare 11 hours agorootparentprevI agree. Is there any way to \"inoculate\" organisation to prevent this failure mode? reply makeitdouble 10 hours agorootparentGiving the organization very clear rules and goals from the start and making wide range correcting changes towards them even when they hurt the whole organization.For companies this usually means pushing product cannibalisation and accepting to forgo additional profit in exchange for better long term viability.I think Apple was good at this during most of Steve Jobs&#x27;s second term. Didn&#x27;t last long though. Perhaps too much success is the biggest issue ? reply rewmie 8 minutes agorootparent> Giving the organization very clear rules and goals from the start and making wide range correcting changes towards them even when they hurt the whole organization.I&#x27;m not sure you noticed it, but the people complaining about \"editors\" being toxic are actually those who are confronted with these \"very clear rules and goals\" already specified by the community , and they are calling foul once they see their edits being edited based on these established and time-tested processes.And by the way, wikipedia supports conflict resolution processes which can culminate in articles being temporarily blocked to prevent endless revertions. You just need to flag the article and ask people their opinion.It&#x27;s strange how this whole debate centers around abuse, offers all kinds of solutions to perceived problems, but not only fails to acknowledge these processes are already in place in Wikipedia for over a decade but also they don&#x27;t even wonder what content was added by the editors complaining their edits were reverted. I mean, think about it for a second. Do you think wikipedia achieved a notable level of quality because all edits, regardless of their content, remain untouched in place as-is? reply hducy4 5 hours agorootparentprevWe have this at my work, need to totally gut the main product offering in order to maintain long term viability but nobody will do it because it&#x27;s the cash cow even though it&#x27;s slowly dying. reply jltsiren 8 hours agorootparentprevProbably not for existing organizations. Possibly for newly formed organizations, but it&#x27;s unlikely to scale.I&#x27;ve seen volunteer organizations founded on mild anarchistic principles thrive for a few decades while occasionally reinventing themselves. The key is a culture focusing on people working together for a common purpose. Keeping in mind that the purpose of the organization is more important than the organization itself.There may be formal stakeholders and formal power hierarchies, but their role is kept to a minimum. Some people need to be in charge for legal and practical reasons, but leadership positions are not associated with high status. Ideally, the leaders are selected through a deliberative process, and they voluntarily step down after a while. Some people inevitably acquire high status within the community, but they are expected to recognize it and avoid being too vocal. reply tivert 4 hours agorootparentprev> I agree. Is there any way to \"inoculate\" organisation to prevent this failure mode?Maybe competent moderators, and a kind of rock-paper-scissors rule structure. Where you have rules, but excessive rules lawyering is a grave, bannable offense. reply prox 10 hours agorootparentprevI feel it’s basically an exponent of selfishness&#x2F;greed. So for some it might be status or other incentives that make them favour a moat or developing an entrenched behavior.Countering it means to cultivate an open and egalitarian platform, where vulnerability is part of the equation as well as the associated trust that is needed. Good faith over bad faith. Selfless behavior vs “I am gonna get mine.”My sources are my experience in organisations as well as a good friend who manages such an organization. reply throwaway2037 1 hour agorootparentThanks for sharing your thoughts. \"Good faith\" is hard to put a finger on, but you know it when you see it! I will never forget reading a bug report in Python where a senior developer replied to a junior developer: (roughly) \"That&#x27;s a good point. I didn&#x27;t think of that. Your idea is better.\" I had never seen that level of humbleness in other open source projects that I follow.I bet the top Wiki editors don&#x27;t think there is an issue. That is the hard part. How do you get the people with the most vested interest accept change? reply natpalmer1776 10 hours agorootparentprevNo, and while the nuances are pretty deep, it basically boils down to the fact that this behavior is necessary for the survival of any complex system. reply Razengan 10 hours agorootparent\"We have tried nothing and we give up.\" reply natpalmer1776 9 hours agorootparentI mean, I did say it was nuanced.You could with the right people across the entire organization succeed without and actively combat the manifestation of this effect for as long as those, or like minded, people remain… but given the fact that organizations have a bidirectional relationship with people and culture, over a long enough period of time the organization will inevitably revert to the mean.In more broad terms, I described the path of least resistance for a complex system (the default mode), while “you” are asking for a system that deviates from that path, which requires the introduction of entropy. As can be observed in many other fields, the total entropy of a system over a long enough period of time will always trend towards zero.Given the specifics of this particular model are bound by social behaviors you could in theory create a culture that has a default mode that does not have the issue described, but to do so you would have to start at a much higher level than a single organization and thus for the purposes of the point I was making it is moot.Even having said all of this, there is still a great deal more left unsaid that lead me to believe this to be true. reply 0xDEAFBEAD 9 hours agorootparentprevHow about having a leader willing to make big bold bets? Whatever you may think about Zuck renaming Facebook to Meta, it certainly seems to demonstrate that his company hasn&#x27;t fallen prey to stasis. reply jareklupinski 7 hours agorootparentprevcheck and balancesactively encouraging structures made up of almost-competing interests that sum towards a common goal helps guide our individual competitive natures towards accomplishing larger-than-one projectspicture a bunch of sticks leaning against each other; individually they&#x27;d fall, but their combined vector gives the total structure strength reply AdamJacobMuller 6 hours agorootparentprevBDFL that recognizes that that is a problem and refuses to allow it to happen, regardless of any personal pain it causes them.Example: Guido van Rossum reply throwaway2037 1 hour agorootparentWhy was this down voted? Guido van Rossum has the magic touch. He steered Python for 20+ years, including through the 2->3 upgrade. And, you never read negative stuff about him. Many other famous open source leaders are the subject various personal attacks. Linus Torvalds famously took off time to improve his interpersonal skills. reply tptacek 16 hours agoparentprevAnd yet it&#x27;s one of the crowning achievements of the entire Internet, and a contender for one of the most important written works of the last century. This is always the challenge trying to dismiss Wikipedia: something they&#x27;re doing is working, not just well, but well on a level that is virtually unprecedented. reply juliusdavies 14 hours agorootparentI consider Wikipedia to be the last standing wonder of the WWW (world wide web).- Google search results get worse every year.- Stackoverflow lies in ruins (albeit the ruins are still useful).- ICQ&#x2F;Gtalk&#x2F;AIM completely dead and all in silos now (Slack).- Twitter is dead.- Facebook is too annoying now.Google Maps is still amazing, but I consider that more a miracle of the internet as opposed to a miracle of the WWW, since the data is essentially sourced commercially (satellites and maps), whereas with the examples above the data was sourced communally.And so I think it&#x27;s inevitable Wikipedia will die within my lifetime. Probably within the decade. I suspect my children will never get to enjoy the miraculous shockingly glorious human affirming paradox of Wikipedia. Their (public school) teachers senselessly already tell them to avoid it. :-(I don&#x27;t think Wikipedia will die for any specific technical or political reason. I just think it will die because everything else that was wonderful from 2009 or before already has, so why not Wikipedia? reply xboxnolifes 11 hours agorootparent> Their (public school) teachers senselessly already tell them to avoid it. :-(Teachers have been telling students to avoid it since the beginning, this is not a new development. If anything, I think teachers may be more accepting of it than in the past, particularly for finding citable sources. reply Manuel_D 10 hours agorootparentMy teacher never told us to avoid Wikipedia. They just told us not to cite Wikipedia directly. Multiple high school and college professors told us to read Wikipedia got general context building, and to read the citations to find relevant sources. reply tptacek 10 hours agorootparentThat of course being the point of an encyclopedia; you&#x27;re generally not supposed to cite it, right? It&#x27;s a guide to other sources. reply switchbak 9 hours agorootparentprevThe surprising rebirth of podcasts is the only standout in that list.It&#x27;s as if blogs became popular again. Though the days of simple RSS feeds with a high signal to noise ratio are probably long behind us. (I know some are still around, but I mean popular in the way podcasts are now). reply yesco 12 hours agorootparentprev> So why not Wikipedia?- Cheap to host, just text and images- Funded by non-profit with too much funding- All images are permissively licensed- Easily archived- Easily forkedThe only potential \"death\" I can ever see happening to Wikipedia is the kind that happens from some kind of fracturing, similar to what we often see with fan wikis. But this kind of outcome could be a good thing really, multiple competing Wikipedia&#x27;s would probably help keep each other honest, and wouldn&#x27;t functionally be too different than the non-english sections of Wikipedia that already exist.If anything I&#x27;m a bit concerned that Wikipedia might be getting a bit too influential than an encyclopedia aught to be. reply Animats 10 hours agorootparent> Funded by non-profit with too much funding.Right. The Wikimedia Foundation keeps trying other projects. They think they are supposed to grow. This leads to increasingly obnoxious demands for more contributions. \"By 2022, it employed around 700 staff and contractors, with annual revenues of $155 million, annual expenses of $146 million, net assets of $240 million and a growing endowment, which surpassed $100 million in June 2021.\"It&#x27;s the disease of universities - too many administrators. reply johnnyanmac 9 hours agorootparentprevToo much money by what metric?Either way, it functions but I feel it&#x27;s simply a tool for 99.9% of people. They don&#x27;t read the in depth heated arguments, they looks something up, get brief context, and dip. What they do with that information is honestly no better or worse than any news organization.Ideally we teach to identify and take bias into account, but we&#x27;ve been at a trend for a while now to try and regulate tech instead of society. reply ternaryoperator 5 hours agorootparentprev> - Cheap to host, just text and imagesI doubt it&#x27;s cheap at this level of traffic. reply jowea 11 hours agorootparentprev> Their (public school) teachers senselessly already tell them to avoid it. :-(Weren&#x27;t they doing that from the beginning? I get the feeling this feeling has gone down not up.>I don&#x27;t think Wikipedia will die for any specific technical or political reason. I just think it will die because everything else that was wonderful from 2009 or before already has, so why not Wikipedia?It still seems to be surviving. Editor count may be down but a lot of the articles that have to be written already exist. It may stop improving much and only include new events but I don&#x27;t think it will die until there&#x27;s a replacement. Maybe people will just consult LLMs for general information and never visit Wikipedia? reply zlg_codes 11 hours agorootparentprevWe don&#x27;t deserve the Internet, frankly. For every boon it gives us, there&#x27;s another dark edge that serves power brokers, corporations, or governments.Why, oh why, does nothing ever help the common man? reply glimshe 10 hours agorootparentThe problem is thinking that we can create a space where the powerful outside the space won&#x27;t be powerful inside the space. Same thing with crypto. reply zlg_codes 4 hours agorootparentSuch a thing is possible, by removing things that give them power while inside the space.I.e. anti-gov, anti-corp spaces can and will exclude those entities once discovered. Sure there&#x27;s espionage but that&#x27;s true of any situation. reply glimshe 1 hour agorootparentSure, we can create something very small, private and fringe and be happy with our little kingdom. The powerful wouldn&#x27;t care about that because it doesn&#x27;t threaten their power. Once it becomes relevant, that&#x27;s when they take action and show up with money and hard, real-world power. This is not some deep state conspiracy theory, it&#x27;s simply about the powerful having the means to get their way. reply johnnyanmac 9 hours agorootparentprev>Why, oh why, does nothing ever help the common man?I believe we call that tragedy of the commons. Prisoner&#x27;s dilemma fits a bit of that as well.And just plain ol&#x27; greed at the end of the day. The people who want to help are rarely in the positions of power to do so. Or are fighting other greed in the process bogged by beauracracy.Lastly it&#x27;s also Apathy in many situations. There can be quite a bit of power when the common man gathers together, but many can&#x27;t, don&#x27;t, or are unaware of such a ability. So the power is mitigated unless a good wave gets going. reply rurp 12 hours agorootparentprevI sadly agree with everything you said, except would add that Google Maps has already died for many users outside of urban areas. Satellite view has always been great for scouting outdoor areas but the app falls apart if you actually go anywhere with poor service and try to use it.One of the most basic features, saving a pin on a map, broke years ago and despite many complaints on their support forum it hasn&#x27;t been fixed. Directions can be terrible in less traveled areas, and dangerous if followed blindly since they will happily lead you down roads that require 4x4 or are totally impassible. Not to mention saved offline maps are unreliable and the UI clutter has gotten drasticaly worse over the years.Maps still works fine for the typical things a Google emplyee cares about like getting directions in a well traveled city or finding places to shop, but it&#x27;s only a matter of time before those usecases get crushed under the ever-building pressures of short term monetization, enshittification, and Google&#x27;s general apathy and lack of care for users. reply tanepiper 2 hours agorootparentTrying to use Google Maps inside Germany due to poor mobile is a nightmare.Trying to use Google Maps in France usually ends up being send down a dirt road or up a mountain.Trying to use Google Maps to check my train the other day and I showed the next train and then a train 2 hours later, I had to put the depart time to 1 minute after the train leaving to see the next one.I&#x27;ve said for years now the product owners of Google Maps are some of the most incompetent on the planet for the size of app they have. reply port515 11 hours agorootparentprevYou forgot Digg reply tanepiper 2 hours agorootparentNowadays, everybody wanna browse like they know the next big site, But when they click, it&#x27;s the same old hype, Just a bunch of reposts, nothing quite like, And web surfers act like they forgot about Digg.Nowadays, everybody wanna share like they&#x27;ve found the freshest link, But when they scroll, it&#x27;s closer to the brink, Just echoes of the past, faster than you blink, And online crowds act like they forgot about Digg. reply 0xDEAFBEAD 8 hours agorootparentprev>I just think it will die because everything else that was wonderful from 2009 or before already hasAssuming this is true, what are the leading hypotheses? Regression to the mean? Eternal September? I know everyone likes to talk about \"enshittification\", but what is the actual mechanism?Since this is HN, someone&#x27;s going to say \"capitalism\", but \"capitalism\" doesn&#x27;t explain why Google search results get worse or Facebook is too annoying. reply Whoppertime 7 hours agorootparentOh that&#x27;s easy. Dotcom Boom and Bust ended one era of the web right around 9&#x2F;11, and you get a weird era between 2001 and the release of the iPhone where there&#x27;s a lot of content posted on Newgrounds or flash game portals like Miniclip. An era where people still got information from Magazines and got demo discs, and people were discouraged from sharing their real name or personal information online instead of being required to provide it in order to create an account. Steve Jobs refused to put Flash support in the iPhone justifying it through security reasons but the real reason being it undermined the walled garden and iPhone app marketplace, and Android didn&#x27;t think it would be worth it either, leading to even Adobe dropping support in 2011 ending that chapter of the internet entirely. When 100% of your users access a website through a desktop computer you are going to design it one way. When half your users are mobile and half are desktop it&#x27;s going to be designed a different way. reply 0xDEAFBEAD 6 hours agorootparentThat&#x27;s honestly a really interesting point, I never thought about it but there was something magical about the Flash era reply throwitaway222 7 hours agorootparentprev> capitalismnot dismissing other points your making or questions you&#x27;re asking, but what happens with search results is a phenomenon that&#x27;s more or less related to monopoly power - so as they (Google) control more, the classical theories behind capitalism no longer hold. For example, they provide worse search, but there is no close second. Even if there is a close second, that \"other guy\" is unable to afford the 60+ billion Google is paying to other companies to make them the default. reply Manuel_D 12 hours agorootparentprevWikipedia&#x27;s flaws are more subtle. Pages sometimes present controversial, or even wrong claims as unambiguously true. Note, this is not actually that bad on pages that are clearly covering controversial topics (namely historical & political topics). The issue is more prevalent on niche topics where the average reader wouldn&#x27;t recognize the controversy being claimed. I&#x27;ve sometimes encountered citations where the cited material directly contradicts the claim made on the page.I highly suggest reading the talk page of wikipedia articles. Not just the talk page, but read through the history of the talk page too. E.g. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;w&#x2F;index.php?title=Talk:Ada_Lovelace... reply resolutebat 9 hours agorootparentGenuine Q: what do you find \"wrong\" about Ada Lovelace&#x27;s article as it stands today? To me it looks scrupulously fair about her contributions and the amount of disagreement on what exactly she did \"first\", which seems to account for 99% of the talk page wrangling.Lovelace&#x27;s notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage&#x27;s personal notes from the years 1836&#x2F;1837 contain the first programs for the engine.[6][better source needed] She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. reply Manuel_D 8 hours agorootparentI don&#x27;t have any issues with the article as it stands today. But earlier revisions had uncritically claimed that she had written the first computer programs. Hence the massive Talk thread. reply jorvi 6 hours agorootparentSee the article about who created the internet.Packet switching was invented in the UK by Donald Davies, yet somehow it’s always the Americans that get sole credit for the internet. :) reply makeitdouble 10 hours agorootparentprevTo parent&#x27;s point, I think this is still better than what we had before.I mean, encyclopedias didn&#x27;t have sources at all, and people were overly confident in books being correct.On the niche topic part, I wonder if we&#x27;ll ever be able to do something about it. I don&#x27;t believe in AI check being a good fit for specialized or rare contexts, and as there&#x27;s fewer eyes it&#x27;s by definition more complicated to cross reference and have many active contributors on the same topics.Even with more appeal to authority (let&#x27;s say you get more power to edit if you&#x27;re proven to be an expert in the field) I assume that would get abused to insert personal gripes in some place or another. reply Manuel_D 9 hours agorootparentTo be fair, I don&#x27;t think there&#x27;s much more Wikipedia could do better. Wikipedia is largely self-correcting in that the more eyeballs an article has, the more likely that wrong or biased claims will be challenged. I actually think the idea of creating a separate class of \"proven experts\" would make the situation worse: it&#x27;d just entrench whatever the expert opinion is which may even be more biased than the normal contributor base.The main thing is to check out the talk pages. That&#x27;s where the controversies crop up and at least you know the opposing view. reply tivert 4 hours agorootparent> Wikipedia is largely self-correcting in that the more eyeballs an article has, the more likely that wrong or biased claims will be challenged.I think that concept is a fairy tale. It&#x27;s kind of like open source software: just because the code is out there doesn&#x27;t mean anyone&#x27;s looking for bugs in or or fixing them.Wikipedia articles settle on the stalemate between its most obsessive or motivated users, the result of which is frequently is wrong or biased. The quantity of eyeballs is irrelevant. reply lmm 11 hours agorootparentprevI don&#x27;t think anything they&#x27;re doing today is working. They had something great in, like, 2008, and have been at best coasting since then. reply GauntletWizard 11 hours agorootparentprevWikipedia was one of the crowning achievements of the internet before said culture took over, and significantly such culture formed as a sort of symbiote and parasite to it&#x27;s success.What made Wikipedia successful was that it was free in a time that people were more used to paying for that kind of information, and that it was open in the sense that anyone could contribute when many places were significantly gatekept.It has now evolved it&#x27;s own form of said gatekeeping, not because such is necessary for Wikipedia to be successful, but to harvest the residual success of that open system for the benefit of the parasites. reply harimau777 16 hours agorootparentprevGive it time. As Wikipedia has become more and more obsessed with being \"encyclopedic\", I&#x27;ve noticed that they are increasingly ending up with articles that are stripped of any useful information. reply rockskon 10 hours agorootparentWikipedia is also very poor when it comes to contentious political topics that the bulk of contributors are extremely passionate about. reply t0bia_s 14 hours agorootparentprevIdea of centralized and universal interpenetration of information soon or later bump into boundaries. By definition, it&#x27;s impossible approach. Interpretation vary with knowledge, culture and is shaped by politics, religion, regime, etc. reply whatshisface 10 hours agorootparentThat&#x27;s why you don&#x27;t interpret it, just list it: encyclopedically. reply t0bia_s 1 hour agorootparentHistory is written by winners. If you agree, then there is a lot of unlisted informations. If you don&#x27;t agree, then you are on the winners side. reply moate 6 hours agorootparentprevThat’s literally impossible.You’re consciousness trapped in meat, everything you experience is an interpretation of your environment and entirely subject to that meat.“The Republican Party exists” is an interpretation of each word, and is controversial on some axis. “2” is a concept, it literally does not exist outside of the mind.This is to say: your desire for objectivity has failed, the best we can ever get are “well defined and explained rationalizations that many people agree with”. reply _a_a_a_ 14 hours agorootparentprevnext [9 more] [flagged] dang 11 hours agorootparentPlease don&#x27;t cross into personal attack when posting to HN. You can make your substantive points without that.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply _a_a_a_ 10 hours agorootparentIndeed. So let&#x27;s be consistent \"Please don&#x27;t post shallow dismissals, especially of other people&#x27;s work. A good critical comment teaches us something\".How did the comment I responded to say anything useful, constructive or usefully critical? Negative but vacuous dismissals like theirs seem ok. I hate that aspect of HN; the nihilism allowed to flower. reply dang 10 hours agorootparentTwo answers: (1) it&#x27;s not possible to be consistent because there are far too many posts for us to read; we can only moderate what we see. If you see a post that ought to have been moderated but hasn&#x27;t been, the likeliest explanation is that we didn&#x27;t see it. You can help by flagging it or emailing us at hn@ycombinator.com; and(2) shallow as the GP&#x27;s criticism may have been, your comment was far worse. Responding to a bad comment (or one you feel is bad) by breaking the site guidelines yourself, let alone with a nasty personal attack, is exactly the opposite of what you should be doing here—it&#x27;s driving the wrong way down a one-way street. reply _a_a_a_ 10 hours agorootparent1) fair enough. Should the negative empty comment I was referring to have been flagged for your attention? I appreciate the answer may very well be \"no, you&#x27;re wasting our time with the little stuff\", and I&#x27;d agree.2) I honestly don&#x27;t know how to respond to people like this. Their purpose is to negate, even purposefully destroy. I&#x27;ve sat down with people I&#x27;ve worked with who are apathetic and hopeless and slowly taught them that they can make a difference but I can&#x27;t do that over the Internet. And it fills me with despair watching them trash what good things we do have.(BTW thank you for replying, I appreciate it) reply the_only_law 10 hours agorootparentprevHN rules are fickle at best. Only way to get a good idea of how they’re enforced is to observe for some time. reply djmips 13 hours agorootparentprev^ toxic comment. - \"People like you\" - \"always\" - \"You\" \"calling them a failure\" - all hostile.Should have stopped after first sentence. reply _a_a_a_ 13 hours agorootparentMaybe I should have stopped after the first sentence, but there&#x27;s always, every bloody time, someone there to drag you down with negativity. How wiki is shit, stackoverflow is shit, the web is shit, society is shit – they never offer evidence, it&#x27;s just shit and they can&#x27;t see good in anything. I&#x27;m tired of these people dragging me down. What is their fucking agenda other than to wallow in failure? reply drekipus 12 hours agorootparentPeople having an opinion on something doesn&#x27;t affect you. Just relax.Swearing at them, calling them out, \"people like you\" etc, affects them.I have to have this exact same discussion with my 60yo bipolar neighbour who terrorizes a whole ecovillage, because someone planted carrots and \"don&#x27;t they know the seasonal pattern!!\" - just relax, focus on your lane replybusyant 11 hours agoparentprev> IMHO, you have to be deeply weird to be a frequent contributor.Weirdo here.Actually, I don&#x27;t contribute much to Wikipedia (aside from the occasional edit for clarity or grammar).But I do upload a number of images to Wikimedia commons. And I occasionally nominate some of my photos for evaluation as Quality Images and Featured Images.Some people definitely act as gatekeepers and can be harsh in their criticism. But in my experience, most people give courteous, constructive criticism--even when they&#x27;re rejecting your nomination!I pretty much ignore the impolite people--or I try to point out that they could have leveled their critique in a better way. reply throwaway2037 1 hour agoparentprevI very much like this post, but I would suggest a small edit: \"toxic double-standards\" -> \"double-standards\". reply ern 8 hours agoparentprevThe whole place is infused with passive-aggression (by policy) and toxic double-standards.There are some truly bizarre things that I&#x27;ve encountered on Wikipedia (editing for almost 20 years now). Admins handwave blatant mistakes away by their friends, citing [[WP:NOTBURO]] and other such shenanigans. It seems to be getting worse over time. reply commandlinefan 11 hours agoparentprev> infused with passive-aggression (by policy)And the worst abuses come from moderators who are _encouraged_ to be abusive. reply nomilk 9 hours agoparentprev> Toxicity: A rude, disrespectful, or unreasonable comment that is likely to make people leave a discussionRude and disrespectful are subjective. As a layperson learning biology for fun (e.g. Bob Sapolsky lectures) I often chuckle at how rude or shocking nature can be! I grimace at how nature videos (e.g. a pride of lions hunting a baby elephant) will garner comments like \"you&#x27;re sick for filming this!\" or \"do something to help!\".Ignaz Semmelweis discovered that microscopic particles (what we now know as bacteria) on the hands and body were causing women and babies to become infected during birth. He was considered a misinformation-spreading pariah for a quarter century until links were made between germs and diseases.But for the 25 years it took the ideas to mature and reach mainstream, he was \"toxic\". reply EGreg 11 hours agoparentprevSo basically, worse than StackOverflow? :-PSeems the nastier people are to contributors, the better the end product is for the 10000x more people reading it.You know all those cruel piano teachers in Russia? But the audience actually wanted to hear the pianists later in life! It&#x27;s like that... but collaborating on content, with NO CELEBRITIES :) reply alboaie 17 hours agoprevAfter an editor, who has edited millions of pages and seems to be a jack-of-all-trades, unjustifiably rejects your contribution on a topic where you have dozens of scientific articles published, the only conclusion is that the system is flawed. There&#x27;s a need for a fundamental change in approach, probably to a system where censorship exists only in cases of clearly illegal content, and various opinions are allowed to be expressed. On the other hand, to filter out the noise, there&#x27;s a need for a trust propagation system among editors and viewers, so that each time, you get the most probable form of a page based on the trust given to direct contacts and indirectly to recursive contacts. Maybe AI could also help a bit. Who dare to start a new Wikipedia ;) ? reply tptacek 16 hours agoparentFrom bitter experience: if you have well established subject matter expertise on a topic, you should almost certainly not be writing Wikipedia articles about it. In Wikipedia&#x27;s framing, you are a generator of primary and secondary sources. Wikipedia is a tertiary source: it is exclusively a roadmap to other, more authoritative sources. Instead of writing Wikipedia articles, write the articles Wikipedia will end up drawing from.It&#x27;s quite painful to directly edit Wikipedia articles on your own areas of expertise. You have context lay readers don&#x27;t have, and you&#x27;ll often leave things implicit or skip steps, because you know that laying those steps out and citing every detail of them isn&#x27;t helpful for learning & understanding. But the encyclopedia doesn&#x27;t work that way: the community there can&#x27;t tell the difference between sensible elisions done in the spirit of efficient explanation, and original research that simply takes an opinion you hold idiosyncratically or fractiously and mints an encyclopedia article out of them.It&#x27;s also going to be deeply suspicious, for very good reasons that don&#x27;t apply to you but do apply to like 70% of all other cases, any time you write something and cite yourself.It is also just the case that not everyone should commit themselves to writing whole Wikipedia articles. I found the process pretty unhealthy; it sucked me in, to be sure, but it also filled my time with rules lawyering and squabbles. It&#x27;d be easy to criticize Wikipedia for having that culture, except that the project is so spectacularly successful. reply edanm 7 minutes agorootparent> It&#x27;d be easy to criticize Wikipedia for having that culture, except that the project is so spectacularly successful.Much as I dislike some aspects of Wikipedia, I think this sentence should be printed in bold at the top of every discussion talking about how to change that culture.Also true of a lot of other \"why don&#x27;t they just X\" statements about other successful projects. Chesterton&#x27;s fence in action. reply dxbydt 8 hours agorootparentprevThere was a sketching algo on wikipedia that was not well described. So I added 2 lines of python to illuminate what was intended by the roundabout description. Within an hour my edit was reverted with the terse comment - \"Wikipedia is not a github.\" ! So I clicked on the editor to find who this rude person was. It was the professor who had invented that sketching algo. reply janalsncm 8 hours agorootparentI always always always prefer code to mathematical notation or descriptions. Maybe it’s because I was never formally taught notation whereas I use code everyday, but I often find that mathematical notation lacks sufficient context and explanation.Ultimately I’m going to need to turn your algorithm into code anyways so let’s just cut out the middleman. That may not be a popular opinion with peer reviewers, though. reply tptacek 7 hours agorootparentprevThis is a great story. Do you remember the name of the page? It&#x27;d be funny to track it down in the page history. reply xor25519 11 hours agorootparentprevYou can be pseudonomous on Wikipedia. Also, some experts are so deep in their field of expertise that they assume others to be knowing something they take for granted. (I am not a Wikipedia editor.) reply tptacek 10 hours agorootparentIt doesn&#x27;t have much to do with your public identity, but rather with how experts in a field tend to write about that field; they make assumptions that are universally accepted among practitioners, but aren&#x27;t obvious to lay readers, and Wikipedia tends to challenge those assumptions.I guess hiding your identity is a way to cite your own stuff there. reply pndy 8 hours agoparentprev> the only conclusion is that the system is flawed. There&#x27;s a need for a fundamental change in approachThere are attempts of creating more \"professional\" electronic encyclopedia like Citizendium [1] and Scholarpedia [2] but it seems that such tasks are rather hard since Wikipedia solidified its position already, despite flaws[1] - https:&#x2F;&#x2F;en.citizendium.org&#x2F;[2] - http:&#x2F;&#x2F;www.scholarpedia.org&#x2F; reply alexey-salmin 5 hours agoparentprev> Who dare to start a new Wikipedia ;)That literally happened hundreds of times and every single one of them is dead.Whatever is it what Wikipedia is doing, it seems to be working far better then alternatives. reply anonymouskimmer 16 hours agoparentprevWikipedia policy specifically says to not reference primary sources (e.g. published, peer-reviewed journal articles). Only secondary sources such as news articles referencing the papers. This is probably why your contributions are being rejected.I haven&#x27;t read the specific justification for this policy, but a couple of reasons is that it allows two rounds of review of the information prior to incorporation into Wikipedia, and that journal articles are typically more technical and thus more difficult for general Wikipedia editors to understand when checking whether the sources back up the claims in the Wikipedia article. reply swalling 16 hours agorootparentThis is not correct at all.The sourcing policy says: “If available, academic and peer-reviewed publications are usually the most reliable sources on topics such as history, medicine, and science.” reply anonymouskimmer 15 hours agorootparentWhen did they change that policy? reply swalling 14 hours agorootparentPeer-reviewed research papers have always been allowed in citations. The nuance that evolved over time is this \"For example, a paper reviewing existing research, a review article, monograph, or textbook is often better than a primary research paper.\" Meaning, you can cite a research paper, but something more secondary that summarizes a body of scholarship would be better.Individual research papers that haven&#x27;t been reproduced often present conflicting results with one another, especially in fields with poor quality research like nutrition. Experts often run into this issue when they try to cite their own research or a narrow set of papers in a given field, especially when recent research conflicts with prior scientific consensus. It&#x27;s why tptacek&#x27;s comment above is apropos. reply mrguyorama 14 hours agorootparentprevThey never did. You can&#x27;t use YOUR OWN work as a source, but you can use primary sources. reply jimmaswell 6 hours agorootparent> You can&#x27;t use YOUR OWN workBad rule, I&#x27;d just use an alt or not my own name. replyasdff 11 hours agoparentprevCensorship and biasmaking will always exist because the benefits it has for the elite are too high to not try and engage in it. There are many articles on current events in wikipedia where you can sift through the webarchive and see very different articles in terms of what details are highlighted or omitted entirely. reply tim333 16 hours agoparentprevIt depends a bit on the reason for the rejection. Wikipedia have various rules such as using secondary rather than primary sources that trip up people who are experts on some topic but unclear on how Wikipedia works. reply cooper_ganglia 16 hours agoparentprevnext [2 more] [flagged] adastra22 16 hours agorootparentFYI community notes predates Elon. reply vasco 19 hours agoprev\"Toxic\" would not be the thing I&#x27;d start with. I&#x27;d start with \"being told no for the first time\", which might or not have an overlap with toxicity. But I think it&#x27;s much more likely that someone will start making edits, only get uninterested comments or bot engagements, keep editing, then at some point someone reverts the edits - tells them no, and so they stop.Potentially this scenario even catches situations like:1. User registers2. User adds a bunch of promotional edits to multiple pages3. At some point someone discovers this, reverts something, tells them to go away4. User was \"caught\" so abandons the account and re-registersIf #3 is classified as toxic, this paper would find the same results I think. reply notahacker 19 hours agoparentI suspect the system for scoring \"toxic\" comments is less likely to flag the bureaucratic \"I have reverted your edit for a second time because it does not meet WP:NPOV or WP:NOR. Please be aware of WP:3RR and be prepared to discuss any further changes on the talk page\" comments about reversion and more likely to flag the sort of very angry and personal comments that come up when the page being edited is related to the culture war or actual longstanding war. Whether the recipient of the angry messages was previously a constructive contributor to that topic is an open question, of course. reply KennyBlanken 11 hours agorootparentThe system almost certainly doesn&#x27;t address the primary problem, which is that a small group of people consider a page or topic to be their personal fiefdom and they&#x27;re experts at snowballing people with the most obscure wikipedia policies.A shining example of this would be the page for Alcoholics Anonymous, which has a small legion of accounts &#x27;defending&#x27; it. The accounts espouse, vocally, a victimhood complex - that AA is targeted by people \"harassing\" and \"discrediting\" the program.The page is brutally censored of any negative information - such as their problems with predation (&#x27;thirteenth stepping&#x27;) and sexual assault, the fact that the program has no basis in science and is repeatedly demonstrated to be amongst the worst options for addiction treatment. The result is a page which is wildly not NPOV - it contains only material positive about the founder, program and organization.Someone tried to add mention of a documentary and the prick editor claimed the film did not meet notability guidelines because it hadn&#x27;t been screened in the right kind of film festivals and thus its existence could not be mentioned. reply nverno 10 hours agorootparentThey have a section on \"thirteenth stepping\" - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alcoholics_Anonymous#Criticism reply pinusc 8 hours agorootparentprevCould you point me to any sources that discuss how the program \"has no basis in science\"? They have a section on effectiveness citing a few studies and the Cochrane review. I&#x27;m not well-equipped to determine if that&#x27;s conclusive, of course. Is there overwhelming evidence to the contrary? As someone else pointed out, there is a section on thirteenth stepping. Do you find it insufficient, or is it a recent addition..?I am not invested either way, I am just curious. If what you&#x27;re saying is accurate, it&#x27;s a stark reminder that Wikipedia articles may _look_ well-researched and still ignore or hide enough evidence to fundamentally change the story.This is something I have a hard time coping with: that no matter how many sources are cited, there is no way of knowing how many are missing... in some cases a few minutes of research can uncover obvious lies (and it&#x27;s already a titanic effort to cursorily check most of your beliefs), but in others it takes actual expertise in the topic to know.Edit: After a better look at the Criticism section, the article seems less-than-neutral in listing critical sources. \"In the past, some critics have criticized 12-step programs as pseudoscientific\". I would object to the use of \"In the past\" here: the sources cited are from 2015 and 2010 (although both of them are prior to the 2020 Cochrane review, which the Wikipedia page seems to consider conclusive. Still, unclear wording which spins the opinions as outdated). Similarly, two sources of criticism are listed under the \"Popular press\" section. Perhaps a better wording would be \"Non peer-reviewed\". If you believe that AA _is_ ineffective, then this wording is certainly a misdirection. If you believe that the sources in its favor are conclusive, then it&#x27;s adequate. Neutrality is tricky, and while it&#x27;s extremely important in some topics, it should not be entertained in others (should the article on Earth be \"neutral\" and faithfully cite flat-earthers as a reasonable side?) Again, I don&#x27;t know which way this should go, as I don&#x27;t have any degree of certainty either way, because I know absolutely nothing about the subject. reply jeremyjh 7 hours agorootparent> Could you point me to any sources that discuss how the program \"has no basis in science\"?Do you know anything about it? The primary source I&#x27;d cite would be their own literature. It doesn&#x27;t claim to be based in science. Sobriety is only possible by giving up your ego and embracing a higher power. reply Ensorceled 12 hours agoparentprevThey are using an established tool to measure toxicity (\"A rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.\") and extreme toxicity (\"A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective. This attribute is much less sensitive to more mild forms of toxicity, such as comments that include positive uses of curse words.\") not what you are saying. reply novaleaf 16 hours agoparentprevmy son created an account and tried to add a page for his middle-school, only to have the page deleted and told it&#x27;s not notable enough to have a page.after that my son hasn&#x27;t tried editing Wikipedia. reply Washuu 16 hours agorootparentSimilar here. I corrected some statistics on a page for a vehicle. I had multiple different years of the owner and service manuals in had to verify these facts and able to source them. Nope, some overlord of the page just kept reverting it with no recourse. I never bother to try editing again. It is not worth my time to deal with that. reply zozbot234 15 hours agorootparentThe trick with any sort of halfway controversial edits is to discuss at length your planned contribution on the talk page, wait until nobody complains, then make the actual edit. If you do get reverted, you can point out that you had announced the edit in the talk page and no objections had been raised. It helps to post on the message boards for relevant WikiProjects too.Yes, Wikipedia policy pages say to be \"bold\" with editing, but that only really works for things like fixing typos. If you know that someone might object, you should focus on in-depth discussion and let concerns be addressed that way. reply zlg_codes 10 hours agorootparentWhy should someone sacrifice MORE of their valuable time and effort to get their contribution accepted, and follow a script or process?This is volunteer work. They can take it or leave it, but they don&#x27;t get to complain that people stop being interested in contributing when their Truth By Committee design blows up in their face. reply johnnyanmac 9 hours agorootparent>they don&#x27;t get to complain that people stop being interested in contributing when their Truth By Committee design blows up in their face.Enough people remain that I doubt they are really hurting for more contributors. But yeah, that&#x27;s a good part of why more technical or niche topics can be spurious.But at the same time, less people visit those pages so it hurts the site less. reply falserum 10 hours agorootparentprevAll contributions in the world follow some kind of process, even if informal. (E.g. you politely wait in line to drop off your generous donation) reply zlg_codes 10 hours agorootparentAnd you think that validates it? Not all processes are created equally, and if the process is too much trouble, you won&#x27;t get contributions.If Goodwill didn&#x27;t have a dock where I could drop things off and an employee to put it where it needs to be, I wouldn&#x27;t donate.You incentivize contribution by lowering the barriers to it. reply mongol 11 hours agorootparentprevI did that once. Got banned, with no recourse. It was a country-specific Wikipedia though, not the English one. But I did not even edit the page, just made the case for it on the Talk page, someone else edited it and I got banned with a bunch of others. Never again. reply vasco 14 hours agorootparentprevThe trick with any sort of halfway controversial edits is to not do them, if you want to remain sane. reply Analemma_ 11 hours agorootparentprevSeems like Wikipedia should change their policy to say \"be bold for trivial edits only\" then. In fact, if they would just say \"you&#x27;re only allowed to make serious changes if you&#x27;re part of the elite cabal of senior editors who have devoted their lives to memorizing our volumes of policy\" instead of continuing to call itself \"the free encyclopedia anyone can edit\", we wouldn&#x27;t be having this discussion; people would at least know what the situation is. reply JohnMakin 10 hours agorootparentbut if \"free encyclopedia anyone could edit\" was literally true, wikipedia would make itself almost worthless in very little time. reply jeremyjh 7 hours agorootparentExactly, these people are doing the valuable work of screening the crackpots, for no pay - and the only cost is that they&#x27;ll also build an impregnable fortress to prevent almost anyone from ever changing anything unless they feel very strong with it and can traverse challenge social and organizational obstacle courses. This keeps the rate of change at a nice, steady, manageable trickle. reply Avamander 14 hours agorootparentprevYou can also revert the revert a few times, that requires arbitration usually. reply Tomte 16 hours agorootparentprevSimilar. I use Wikipedia a lot, but will under no circumstances whatsoever contribute anything, after a bad experience with my first edit. reply shagie 14 hours agorootparentprevThere are roughly 13,000 middle schools in the United States.Creating a page for each one and curating it represents a significant amount of volunteer work.The \"not notable enough\" is in place in part to try to limit the amount of pages that need curating.Whether or not that&#x27;s a good thing is debatable - but the allocation of volunteer curation resources are often stretched quite thin on sites that crowdsource their content. reply rurp 12 hours agorootparentBut his son already did the work to add his school! I don&#x27;t see why adding one random middle school requires that all 13,000 be added and kept up to date. Wikipedia has all sorts of inconsistencies in what is covered, because it is so driven by volunteers and what they want to invest time into. I don&#x27;t see how people adding public institutions that they are interested in adding goes against the spirit or practical realities of Wikipedia. reply shagie 11 hours agorootparentIt&#x27;s not the work now that&#x27;s at issue but the work of the future to fix \"Mr. Smith is a poopy head\" vandalism on content that gets few views.Things like names of teachers or sizes of current classes - those temporal things (which are correct now) become broken windows of \"someone needs to update it\" in the future.And if one says \"Ok, this one is acceptable\" - then how much more maintenance and curation of pages are the core group of volunteers expected to take up?If the answer to that question is \"none\" - who is doing it? or is it going to become a repository of outdated information?Having content is an ongoing cost of time to the people who maintain it. If it isn&#x27;t maintained, it isn&#x27;t valuable (or notable) enough to be put in there in the first place.It might be better served as a part of a larger page that covers the school district. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_Dallas_Independent_Sch... which are a name and optionally a note. Other districts don&#x27;t even have notes ( https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Madison_Metropolitan_School_Di... ).If it was just a paragraph of content that was timeless (when established, mascot, municipality, etc...) then consider reformatting the school district page (which may well exist) to include the information rather than creating a new one. reply vasco 11 hours agorootparent> It&#x27;s not the work now that&#x27;s at issue but the work of the future to fix \"Mr. Smith is a poopy head\" vandalism on content that gets few views.If it gets few views it&#x27;s not important to fix, and if any of the few views cares they can fix it. Lets not pretend that each page needs the same level of attention. A random middle school wikipedia page having a slightly out of date content or even vandalism isn&#x27;t a big deal at all and can be fixed, and if it&#x27;s not fixed - it has few views anyway. Wikipedia already acknowledges this and certain pages are much harder to edit than others.You could even have a computer class where kids from the school would update their school&#x27;s wiki page as a way to learn how to contribute to wikipedia. Might actually be the easiest type of page in the whole wiki to keep fresh now that I think about it, while creating a whole generation of interested editors.It&#x27;s like they think they run a paper version of a encyclopedia with these rules about what is notable or not. reply shagie 10 hours agorootparent> You could even have a computer class where kids from the school would update their school&#x27;s wiki page as a way to learn how to contribute to wikipedia. Might actually be the easiest type of page in the whole wiki to keep fresh now that I think about it, while creating a whole generation of interested editors.This type of effort often creates a disproportionate amount of work for the volunteers maintaining Wikipedia.If you feel that that would be a good thing to do, the school should stand up its own wiki about itself that students could update and maintain.That also avoids possible conflict with the rules of Wikipedia.I recall a school that attempted to do \"ask a question on Stack Overflow\" and all the students in the class asked the same question - nearly word for word - about their current assignment... that all then got closed as duplicates of each other.If the goal is to introduce someone to the application platform, it is likely better to have them work in a private instance where the mentor to new user ratio is much higher and things like \"you need to make sure that you are using proper and correct English with capitalization and punctuation,\" are not \"revert and leave an impersonal comment on their talk page about the expected quality of their contributions.\" reply falserum 10 hours agorootparentprevdevilish advocate here.One such article - small problem.But what if such low-view, and now outdated&#x2F;vandalized articles start approaching 50% of wikipedia content? That might reflect badly on credibility of whole wiki.It is nontrivial problem of striking balance between content quality level vs content quantity&#x2F;contributor freedom.(Open source software projects deal with same problem, just not on such scale) reply karencarits 10 hours agorootparentprevI would also like to mention Wikidata that for many articles feeds data to Wikipedia. It would make sense to have all instances of the schools listed there as it can be batch updated reply jl6 11 hours agorootparentprevThey have to draw a line somewhere or the project gets spammed with self-promotional articles. Requiring notability does a decent job of keeping the spam out. reply KennyBlanken 11 hours agorootparentI&#x27;d believe this if it wasn&#x27;t for the thousands upon thousands of pages about the most absurdly obscure Star Trek and Star Wars shit.If wikipedia policies result in pages for minor subplot characters in auxiliary pulp trash novels for a space western series but not for a real-life school, we have a problem. reply jl6 10 hours agorootparentA lot of those articles should probably be merged or deleted, but a lot of them probably do pass the notability standard, because despite being absurdly fancrufty, the subjects still get covered in independent sources, in a way that schools don’t. reply jowea 11 hours agorootparentprev> The \"not notable enough\" is in place in part to try to limit the amount of pages that need curating.As someone who leans inclusionist I wondered about that, but I think it also has to do with the existence of reliable sources. You need them to write an article, and the notability guidelines exclude articles that would be impossible or hard to write due to lack of sources. reply shagie 11 hours agorootparentThe extreme inclusionist position (and I&#x27;ve got a straw man there) ends up with sites that are full of outdated information that few people want to maintain.Even if we say \"ok, the cost of the page on some random middle school is 1 minute &#x2F; year\" then as it grows to that 13,000 schools - that&#x27;s 220 hours. Five and a half weeks of checking each page once a year for 1 minute with a 40 hour week.And looking, I&#x27;m slightly off on the number of schools.> During the 2020-2021 school year, there were 13,187 public school districts. These school districts enrolled 47,755,349 students across all 50 states and the District of Columbia. ( https:&#x2F;&#x2F;ballotpedia.org&#x2F;Public_school_district_(United_State... )Many school districts have multiple schools (e.g. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_Dallas_Independent_Sch... )How much time are you willing to spend maintaining individual pages as opposed to a page that lists all the schools and a little bit about each one. The list page takes a similar amount of time to maintain than each page as an individual one.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Madison_Metropolitan_School_Di... as another example. Pages for high schools? They tend to have sufficient information about them (though the high school that I went to doesn&#x27;t have a page). Multiply that number by two for the middle schools and by twice again for the elementary schools... and it gets to the difficult to maintain realm.Another example of a school district and note the lack of middle school distinct pages - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Palo_Alto_Unified_School_Distr... reply karencarits 10 hours agorootparentBut as long you can get hold of the data, it can be updated in batch. See https:&#x2F;&#x2F;wikidata.org&#x2F;wiki&#x2F;Wikidata:Wikidata_in_Wikimedia_pro... reply zlg_codes 11 hours agorootparentprevyeah, that sort of stuff is ridiculous. When fictional characters and movies and other bullshit are getting Wikipedia pages, they can document a community school or some shit. At least it&#x27;s real! reply gopher_space 9 hours agorootparentprevI think this is the central issue with both Wikipedia and stack overflow, and a bit of a problem for HN. I don&#x27;t need people less interested in a subject than I am deciding what I read about.I&#x27;ve worked on two separate projects where having every middle-school in the state on wikipedia would&#x27;ve helped, so tell your son I appreciate his work! reply snoopsnopp 19 hours agoparentprevI find this happens with a lot of forums. The most critically minded users eventually get exasperated and become toxic after repeated bans to “kamikaze” accounts. reply josefritz 18 hours agoprevI gave up contributing to Wikipedia when I looked back and found how much had been deleted. The deletionists have won the war. reply tptacek 16 hours agoparentThat being the literal premise of the project, it&#x27;s the outcome you&#x27;d have expected. reply im3w1l 8 hours agorootparentAre you saying people aren&#x27;t allowed to be upset with an action if you announce it beforehand? reply tptacek 7 hours agorootparentQuestions like these always make me think of Dwight Schrute speeding towards a sales call that Andy Bernard is closing out from under him, screaming into his phone \"are you saying you INVENTED PAPER?\". I&#x27;m not saying I invented paper. You can be upset with anything you like. I&#x27;m not the upset police. But if the thing you&#x27;re upset about (20 years later!) is the premise of the what is probably the single most successful project on the Internet, it&#x27;s going to be hard to take the concern too seriously. reply boomlinde 18 hours agoprevI&#x27;d rather have some uncouth editor berate me in a discussion than my personal editing experience of sometimes having simple and obvious, even clerical changes to non-contentious topics immediately reverted with no comment.It&#x27;s not that I take it personally, because I realize that no one cared to review the edits before reversion. It&#x27;s just that \"be bold\" in the sense that it&#x27;s applied by me in combination with how it&#x27;s applied by overprotective bots (maybe?) or at worst diligent but careless editors is a massive waste of time and energy. So I&#x27;ve stopped contributing altogether. reply dom96 17 hours agoprevWow, this is a great study and I bet it extends to other volunteer-based activities. In particular: open source software projects.It seems obvious that toxicity needs to be rooted out of open source communities, and any projects that don&#x27;t do so or ignore the issue will fail to keep their contributors. But it&#x27;s nice to have some real studies on this with some objective results (even if not strictly for open source software project contributions). reply Kevin09210 17 hours agoparentYou need to get rid of codebase ownership and find something else to replace the trust it brings.- Fine granularity forkability. Fork functions, not just projects- Curators&#x2F;Reviewers who endorse the validity&#x2F;security aspects of those forks. reply johnnyanmac 9 hours agorootparent>Fork functions, not just projectsI&#x27;m not dismissing your idea, but I&#x27;m not finding the value in this compared to copy-pasting that function.It&#x27;d also be beholden to all kinds of dependency checking to be practical unless it was a tiny function.But i can see the benefits of granular ownership. You don&#x27;t necessarily want to fork off a whole new repo just to have a contributor be responsible for a module but not the entire project. reply kranke155 18 hours agoprevThe Portuguese Wikipedia has been completely taken over by a toxic mega group that’s bent the rules to get what they want.They constantly harass you, and they invent rules if needed to get articles down that they don’t like. reply whstl 11 hours agoparentI remember it depending on the article. There are \"turfs\", and the rules aren&#x27;t consistent between them. Some guy goes nuts on the movies and television articles and nobody can get anything through, it gets reverted with a \"this is not necessary\".On science and engineering pages I remember there being a lot of unsourced material that felt more like a school paper than an encyclopedia. The quality was really bad, but it was still hard to change. reply jowea 11 hours agoparentprevPeople complain about that in the English Wikipedia but I guess it can be even worse in the smaller ones? Weren&#x27;t a few of them basically captured by the local far-right propagandists?Is this group you&#x27;re talking about political or just the \"I&#x2F;we own this and will do this my way\" of English Wikipedia? reply kranke155 10 hours agorootparentIt is far worse being a smaller Wikipedia.It’s both. There are political accounts that remove corruption scandals from local politicians and others who just harass you in general.I made a page about a major scandal in Portugal and they used the Brazilian admins by telling them the scandal was too small to be in Wikipedia. It was in a major TV show in Portugal but the Brazilian admins almost deleted the page (which then blocks you from remaking it). reply ceving 17 hours agoprevThere is too much politically motivated agitation on Wikipedia. This is particularly a problem because Wikipedia&#x27;s power apparatus is completely undemocratic and anonymous. Wikipedia&#x27;s organizational structure corresponds to a medieval feudal system. reply Scea91 2 hours agoparentFeudal system? I didn’t know my oldest son will inherit my Wikipedia pages and live from the rent paid by the editors. reply t0bia_s 14 hours agoparentprevIt&#x27;s impossible to have centralized \"democratic\" interpretation of information. You need vote system and constant revision of text. Even though I think it&#x27;s not possible to have universal interpretation of information. Point of view vary with many aspects including politics, religion, regime, actual available knowledge... reply Vicinity9635 9 hours agoprevFun fact: if you check the Talk page of controversial pages you can frequently see all the public interest information the extremely biased editors are trying to suppress. Recent example: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Talk:2023_Nashville_school_sho... reply zlg_codes 10 hours agoprevI think everyone has the capacity to be toxic, but few things elevate my willingness to be toxic than having my well-reasoned contributions insulted or rejected.I&#x27;ve contributed a total of once, ever, on Wikipedia. It was a typo correction that was reverted, then remade under the mod&#x27;s account so he got credit for it.If that&#x27;s what I can expect by interacting with Wikipedia, I&#x27;ll pass.You have to actually reward participation if you want it. reply acadapter 19 hours agoprevIt&#x27;s all just a matter of expectations. Today&#x27;s Wikipedia has a quite difficult learning curve for making proper and lasting contributions.The \"Visual Editor\" should be confined to the Talk: pages, so that the difficulty of editing can be higher, to match the difficulty of dealing with Wikipedia&#x27;s internal bureaucracy. reply reddalo 19 hours agoparentI agree. The Visual Editor has simplified editing Wikipedia pages from a technical standpoint, but at the same time lowering the bar to edit pages also lowered the quality of many of those edits, making moderation even more difficult.It&#x27;s a vicious circle. reply squigz 19 hours agoparentprevAs an occasional Wikipedia editor... I would hate this so much. reply EnigmaFlare 9 hours agoprevWith all these horror stories, I have the opposite. I made an article for my tiny company to help promote it. Sounds bad but I followed the Wikipedia rules to the letter. I ensured that every single piece of information was cited to independent reputable sources only, even though some of it was actually false. I declared my conflict of interest on the first edit and my user talk page. I was prepared for it to get instantly deleted but instead some bots fixed a bit of formatting and now it&#x27;s the 2nd result on Google after my own website when searching for the company name. reply calamari4065 6 hours agoprevMany years ago I got called an idiot for suggesting an extremely specific and extremely irrelevant article get deleted. I haven&#x27;t bothered to contribute since. Clearly Wikipedia is full of very stable geniuses and don&#x27;t need contributions from the common folk. reply dv_dt 18 hours agoprevI have to think there is a lot of crossover between Wikipedia edit review interactions and code review interactions in terms of what actually brings out better quality outcomes and what make for discouragement of efforts. reply paulnpace 19 hours agoprevGave up looking for a clear definition of what a \"toxic comment\" is. All I found is a statement that they use some score from some tool and link to the tool developer&#x27;s site so I&#x27;m required to sift through the developer&#x27;s site to understand what the core part of the paper is, or I&#x27;ve missed some other part of the paper because they include so many dense paragraphs of nothing useful informations (as well as a nice promotion for the tool developers). reply cbondurant 18 hours agoparentI had to ctrl+f to navigate enough to find it but they mention that their method for identifying toxic comments is under the methods and materials section. Its just some kind of trained ML model. In that section they link to a page (Id share it directly but it clearly has some annoying \"heres your temporary access key\" query parameters that would probably cause the link to break super fast) that gives examples of how it ranks different kinds of comments, and I think that list of examples is enough to trust that it does at least well enough at classifying to draw at least light conclusions from. reply Ensorceled 18 hours agoparentprevA good way to begin to understand about toxic comments is to look for the \"dead\" comments on HN.Dead comments fall into a few categories: trolling, advertisements, conspiracy&#x2F;culture war and toxic.It&#x27;s pretty easy to figure out which is which. reply VancouverMan 16 hours agorootparentAs somebody who wants to see as much discussion as possible, even if I might disagree with or dislike what&#x27;s being expressed, I find the moderation here tends to be more \"toxic\" to my user experience than the dead or grayed-out comments are.I wish this site had a setting like \"showdead\", but that disabled all moderation-related impacts on the display of the discussion. There wouldn&#x27;t be any grayed-out comments, for example, and the ordering would depend only on when a comment was posted. reply CamperBob2 15 hours agorootparentAgreed, and I&#x27;d even be willing to opt out of the karma system entirely to get this.The self-defeating nature of \"graying\" comments out is just utterly mind-blowing. It calls more attention to the undesirable content rather than less... yet nobody in power seems to understand that. (For starters: if I turn on showdead, it means I don&#x27;t want my view censored. So stop doing it.)Anyway, it&#x27;s OT for this story. reply natch 15 hours agorootparentI’m not inside your head so I don’t know if you would agree, but I would love to see this say content that is deemed undesirable as opposed to just straight “undesirable.”My problem with gray text is a bit different: Since the dumbness of crowds can happen even on HN, I’d like to be able read comments myself and decide for myself what to think about them.With some topics the hater community is very strong, and comments get so light so fast they cannot even be read without some disruptive workflow, and they aren’t always useless comments. reply CamperBob2 15 hours agorootparentYeah, we&#x27;re basically in agreement there.In most cases, I&#x27;d say those comment threads that end up almost entirely grayed out were never good candidates for HN stories in the first place. reply natch 14 hours agorootparentTrue, and maybe I went a little far. The ones that quickly get unreadable are invariably pretty dismal. Sometimes it’s just morbid curiosity about what they said. reply johnnyanmac 9 hours agorootparentprevThe idea is that it&#x27;s harder to read and much easier to skim over. It does elicit curiosity, but if everytime you take that effort and you end up reading nonsense, you&#x27;re gonna assossiate that fade with nonsense.I&#x27;d say it&#x27;s the most interesting interpretation of \"ignore the troll\" that I&#x27;ve seen, if nothing else. reply t0bia_s 14 hours agorootparentprevIt&#x27;s not easy. Definition of toxicity is fluid. Also, polotical topics are regularly flagged, even though source information is reliable and solid.I miss something like explanation&#x2F;select of category if giving a flag. Without any explanation, why link is flagged, no one is learning. reply lolc 18 hours agorootparentprevNo the question is how the authors decided which comments were toxic for their evaluation. If they say \"gut feeling\" then replication will be greatly hampered. reply Ensorceled 18 hours agorootparentThe article explains their methodology for identifying and tracking toxic comments. reply lolc 11 hours agorootparentThe original comment said \"All I found is a statement that they use some score from some tool and link to the tool developer&#x27;s site\". So if that&#x27;s all there is regarding classification, their methodology is not nailed down very well in my view. But maybe there&#x27;s more and we haven&#x27;t found it? reply Ensorceled 10 hours agorootparentThere is an entire section dedicated this at the end of the article. They are using at established tool for this domain and there are links explaining how it works and definitions. replyintended 17 hours agoprevSince the discussions seems to be focusing entirely on the headline “Toxicyity” They are using Perspective, you can find more here: https:&#x2F;&#x2F;github.com&#x2F;conversationai&#x2F;perspectiveapi reply eterevsky 17 hours agoprevAn obvious alternative explanation would be that pages on controversial topics both attract toxic comments and are more difficult to edit. I am not sure whether this paper controlled for this. reply iamthirsty 17 hours agoprevI think since the paper is littered with the term \"toxic\", it clearly has its own point of view — that is, not as objective as one would like in this kind of study. reply csours 17 hours agoprevYou&#x27;re being sprayed with weed killer (metaphorically). The problem is, you have step into a conflict area without realizing it.The moderators spray the whole area with weed killer because there really are weeds. It&#x27;s too much work to pull the weeds individually, so it all gets hosed down. reply WalterBright 8 hours agoprevI&#x27;ve decided to hang on to my old paper encyclopedia from the 1990s. reply kwhitefoot 20 hours agoprevWell there&#x27;s a surprise! reply WesolyKubeczek 18 hours agoprevWhen I see this war between petty editors and insolent demanding contributors, I can’t help but sorta cheer to both teams.They are all so right and simultaneously so wrong, and you can see the exact same thing happening on all large online spaces where content is supposed to be curated, that I can’t help but wonder: how is it that the phenomenon hasn’t been researched with hundreds of non-shoddy papers and a dozen of books written? And how come we haven’t moved on from screaming at one another about what the cure should be — “more censorship” or “more empathy”? How far can you dial both before realizing it just plain doesn’t work? reply miroljub 16 hours agoprevWikipedia has grown to be a PR outlet. It&#x27;s overwhelmed by different interest groups pushing their own agenda or doing PR. Especially smaller Wikipedias, like for example German, are known for defamation campaigns against “unpopular” authors and topics.And before someone cries \"citation needed\" I&#x27;ll add just one link to satisfy the requirements: https:&#x2F;&#x2F;swprs.org&#x2F;wikipedia-and-propaganda&#x2F;Though the source, although worth reading and providing a valid criticism, may not be good enough, since Wikipedia, the main source of truth on Internets, marks it as a misinformation website. reply 2OEH8eoCRo0 16 hours agoprevSemi-tangent: With Wikipedia being one of the last great resources (imo), it&#x27;s interesting to see the amount of hate and negativity sent their way. reply sonicanatidae 18 hours agoprevHave they met...humans? reply bivvic 18 hours agoprevI&#x27;ve left acerbic comments in replies to editors before, but only when they were being rude, obstructive, dismissive or sarcastic themselves. I do hope it changed their behavior, like the article suggests.Some of these people get so full of themselves and treat the pages they are interested in like their own little fiefdoms that no-one else is allowed to touch. reply whamlastxmas 18 hours agoprevI tried to be an editor for a couple months. I found the experience to be extremely unpleasant because even the best intentioned feedback came across as callous and annoyed and unkind. Softness is not a strong skill amongst Wikipedia editors.That and the shitty bias evident everywhere due to using corporate news as the primary source for anything not involving STEM. Corporate news is wildly biased and it bleeds heavily into Wikipedia. It’s unfortunate that billionaires who own news organizations are literally writing history reply 6510 16 hours agoprevPersonally I have no problem working with toxic people. Toxic people talking nonsense I cant deal with. You do have to have a f**g point if you are going to be toxic and you need to demonstrate by example how you think things should be done. reply hanniabu 16 hours agoprevThis is the same reason i&#x27;ve stopped answering and asking questions on stack overview, except the toxic culture is coming from the moderators. reply commandlinefan 7 hours agoparent> the toxic culture is coming from the moderatorsAnd the moderators are given free reign to behave that way _because_ of so-called \"toxic behavior\" from the users. reply donatj 18 hours agoprevI have been making small contributions for fifteen or so years. I don’t much feel like doing it anymore after a spurt of bad interactions.About a year ago I created a Wikipedia page for a TV personality and author I enjoy. Spent literal hours on it collecting citations and what have you. Within a couple hours the page was deleted for not being noteworthy. He had his own TV show, has multiple published books and a popular podcast. Seems noteworthy enough to me, but what do I know.A couple months ago I tried to create a page on a local tractor company that used to be really important to my hometowns economy. There was an existing section about the company on one of their specific tractors pages. I used that with its citations as the basis for the new page.I did scan the citations and they seemed fine. I even fixed one that was broken with a wayback machine link. What I didn’t do was read them word for word. Well turns out multiple sections of the text I had moved from the tractors page were straight up lifted from their sources. Within 5 minutes of posting the page, someone else did read my citations word for word, and it was marked for “rapid deletion for copyright infringement”. No chance to explain. No chance to reword. Just gone completely and I didn’t have a local copy of the article.It was particularly frustrating because the moved text only accounted for about 20% of the page by the time I was done.Beyond that the history of my author page is now marked with a copyright infringement warning that if I do it again I’ll be banned.I don’t think I want to play in their sandbox anymore. reply dkjaudyeqooe 17 hours agoparentI had a similar experience a few years ago. I started a new topic and immediately I had some guy marking it for deletion. In the talk page he made various claims, almost all of them nonsensical, apparently with goal of getting me to go away. Eventually, after some fairly harsh replies, he went away, and the pages stand to this day.But it worked, I gave up soon after and haven&#x27;t contributed since.It&#x27;s like the existing editors want to keep WP all to themselves and don&#x27;t appreciate \"outsiders\" interfering. The irony is I&#x27;d been contributing since 2002 off an on, but that made no difference. reply lelanthran 14 hours agorootparentI hate to dogpile and I almost never do, but the parent&#x27;s and GP&#x27;s experiences are almost the same as mine.It doesn&#x27;t take too much blatantly dishonest interaction before I decide \"life is too short\", regardless of whether it is on Wikipedia or elsewhere. reply JackFr 16 hours agorootparentprev> Eventually, after some fairly harsh replies, he went away, and the pages stand to this day.Which brings up the point I was going to make about TFA. If &#x27;toxic&#x27; comments are an effective way to reduce the impact of bad editors, then it&#x27;s not clear that &#x27;toxic&#x27; comments have a negative effect on the quality of Wikipedia. They might be improving it. reply dkjaudyeqooe 15 hours agorootparentJust to be clear, I wasn&#x27;t toxic (although unclear what the definition exactly is), I just vigorously called him out on his bullshit. reply rqtwteye 18 hours agoparentprevReminds me of my experience with Stackoverflow. I tried to solve a very specific problem with Windows installers. I only got some responses from oldtimers that basically said “why would you do such a thing?”and similar. It felt extremely unwelcoming. It’s ok to not answer but why be so dismissive? I guess the only questions they like are things like “how do I calculate 2+2 in python?”. Anything more complicated is not acceptable. reply tivert 17 hours agorootparentYeah, whenever I ask a question on SO, I literally have to write paragraphs of text exhorting people to answer my question instead of a different question they find easier, or why my question is not the same as some vaguely similar one.Then I have to constantly remind people about the same stuff in the comments.That culture problem has been getting worse and worse at SO over time. I only go there as a last resort. reply ryandrake 17 hours agorootparentStackOverflow is a great resource if you want to know how you would have done some programming task in 2009. Their cultural aversion to new&#x2F;updated content has frozen them in time. reply rqtwteye 13 hours agorootparentAgreed. Certain categories should probably be purged every 5 years or so. In many areas there is no value in learning how things got done years ago. Obviously, there are other areas where the fundamentals haven&#x27;t changed since the last decades. reply magicalhippo 11 hours agorootparentprevI found SO very frustrating until one of the moderators explained it rather clearly: SO isn&#x27;t about me (or you). They don&#x27;t want to help me (or you). They want to help everyone else.In that light all the weirdness made sense. Of course, I am very interested in getting my problems solved, so I simply stopped asking on SO to avoid the frustration. reply tivert 4 hours agorootparent> I found SO very frustrating until one of the moderators explained it rather clearly: SO isn&#x27;t about me (or you). They don&#x27;t want to help me (or you). They want to help everyone else.I know that&#x27;s what they say. The thing they have trouble understanding is that \"helping everyone else\" does not mean bending every question into some 101 level software design advice thing.\"Helping everyone else\" very much can mean being a repository of answers to hard questions or solutions when non-typical constraints are at play. Every software engineer worth his salt must have had an experience where the answer he was looking for was in some old, weird forum post that was the single hit to his search query, but a lot of SO doesn&#x27;t seem to get that. reply falserum 17 hours agorootparentprevSources?(Edit: Let’s add &#x2F;s just in case) reply julianeon 18 hours agorootparentprevI think StackOverflow is a good example of a site where cultural norms have become a life-threatening issue. The site has experienced a sharp drop in search traffic and yet people still say it&#x27;s unfriendly all the time. Fixing that perception should be a top priority. reply jstarfish 16 hours agorootparentIn the rare occasion I&#x27;m desperate enough to ask a question, I dread seeing the red inbox notification. I think I&#x27;ve abandoned the last three questions I asked because I couldn&#x27;t stomach facing the drama I expected.The cultural norms of internet forums should never have been allowed to take root on a Q&A site. There is nothing \"professional\" (or even helpful) about answering questions with out-of-scope proscriptions.Your plumber doesn&#x27;t show up asking why you want your toilet fixed, nor does he tell you your house is messy, give you dietary advice and passive-aggressively insinuate you need to fix your roof. SO responders are mini-spouse syndrome incarnate. reply JonChesterfield 17 hours agorootparentprevFixing the unfriendly perception is why stack overflow is dead. Signal to noise for professionals is now close to zero. Maybe it&#x27;s friendlier, but it&#x27;s also no longer worth visiting other than the historical record. reply wirrbel 16 hours agorootparentFrom my last 5 stack overflow questions, 3 were some mod closing the question with a rationale that’s ridiculous.Most questions are now answered in the comments instead of being written as an answer below the question and I assume this is because people have had bad experiences with gate keeper mods when writing answers.I have reasonably high karma.I don’t think quality deteriorated due to some attempts to make the site more friendly. It’s the spirit of a county rabbit breeders association that drives people away reply shagie 15 hours agorootparentSaying \"mod on stack overflow\" is much like saying \"editor on Wikipedia\".Where these other community members who had the close vote privilege? Or an elected moderator that had a diamond in their name to designate that? reply jstarfish 16 hours agorootparentprevI (used to) answer questions in comments. It became too discouraging to post an answer that took 20 minutes to write and get immediately downvoted because someone disagrees with any single part of it (or they&#x27;re trying to promote their own answer by suppressing everyone else&#x27;s).Comments can&#x27;t be downvoted. MetaFilter had the right of it. reply donatj 17 hours agorootparentprevGenerally I&#x27;ve had pretty good luck with Stack Overflow. Recently however I had a ten year old popular question about how to read terminal responses in a shell script removed b&#x2F;c some mod decided questions about shell scripts belonged on \"another Stack Exchange site\". reply borbulon 17 hours agorootparentprevI had a similar experience in SO, I used the term \"the most bulletproof-y way I can think of\" or something like that in my answer, definitely used \"bulletproof-y.\" Someone responded with something like \"You shouldn&#x27;t say that. Nothing&#x27;s really bulletproof, you know.\" reply mewse-hn 17 hours agoparentprevI expect Wikipedia to collapse under its own weight and hard fork within the next 10 years. I think it would be a shattering like Twitter if the content wasn&#x27;t under a permissive license. Instead I think it&#x27;s going to be like WoWWiki being acquired and forking to Wowpedia, and then Wowpedia being acquired and forking to Warcraft Wiki.The community around it is extremely unwelcoming and has calcified - only the most cynical and bitter editors remain. I know of a couple of the principles of the site, \"be bold\" and \"assume good faith\", I don&#x27;t think those are followed or respected anymore.And the finances of the foundation, good lord. reply borbulon 17 hours agorootparent> And the finances of the foundation, good lord.Please expound? Or a link is fine, too. reply vasco 16 hours agorootparentAny cursory googling of their finances would show you that they balooned and instead of using said finances to run the website they ask for more millions of dollars to sponsor a bunch of unrelated charities and hire a bunch of executives to manage such use of the funds.I wish someone would hard-fork it already and stick to a promise of strictly sticking to the core job of operating wikipedia the website, with a small dedicated team.Plus they don&#x27;t need any more money, they have enough money to run it for a long long time already and if they keep doing a good job people will keep donating. I don&#x27;t understand this model of \"we need to ensure we survive forever as fast as possible by accumulating a billion dollar endowment\". Nonprofits should have ~5-10 years runway max and keep getting donations if they keep doing a good job. I don&#x27;t trust any organization forever. What incentive do they have to be useful and welcoming if they have a forever endowment? reply 6510 16 hours agorootparentYou could make a fantastic fork if you simply stop pretending that credentials can not be validated. It then follows that 5 angry narcissist anons do not equal in value a professor.Then you can further simplify if you employ expert moderators who have the final say in everything. The infinite size discussions back stage serve no purpose.I bet people had tons of ideas for other improvements. (think: A distributed system with fancy api&#x27;s for the robot overlords.) reply mewse-hn 16 hours agorootparentprevI&#x27;ve read stuff from insiders that the foundation spending grows exponentially year-after-year and they use those scare banners that \"wikipedia is under threat\" to collect more money and grow even more, the money is completely unrelated to hosting the site. This is a recent article from a year ago:https:&#x2F;&#x2F;slate.com&#x2F;technology&#x2F;2022&#x2F;12&#x2F;wikipedia-wikimedia-fou... reply martin_a 16 hours agorootparentprevMaybe start here: https:&#x2F;&#x2F;www.theregister.com&#x2F;2012&#x2F;12&#x2F;20&#x2F;cash_rich_wikipedia_c...While 10 years old, things haven&#x27;t gotten better but worse. Seems like lots of projects besides the \"core product\" Wikipedia are being founded. Obviously people don&#x27;t like that because they want the money to be used for preserving&#x2F;building&#x2F;extending Wikipedia. reply adastra22 16 hours agorootparentprevVolumes have been written on the ballooning budgets of Wikimedia foundation, of which only a trivially small amount goes towards hosting costs. In mobile rn, but Google should get you plenty of results. reply Zuiii 17 hours agoparentprevSame thing happened to me but in my case, I added a simple \"citation needed\" on a statement that was objectively false (can be proven by simply going to the vendor&#x27;s landing page). The citation needed tag got reverted almost instantly and I was told that adding this tag was inappropriate and constituted sabotage. I never clicked that edit tab again.The thing is, I don&#x27;t blame the editor. He&#x27;s high on the smell of his own farts. I blame wikipedia for enabling him. At least with stack overflow, they try to make fighting against this kind of corruption easier by making it&#x27;s voting-based moderation system visible. Not wikipedia. It&#x27;s opaque. It reminds me a lot of governments.Anyway, I still notice false statements on wikipedia from time to time, but I always smirk to myself and carry on :) reply medstrom 18 hours agoparentprevAnother victim of deletionism. You might like https:&#x2F;&#x2F;gwern.net&#x2F;inclusionism#no-club-that-would-have-me reply grpt 16 hours agoparentprev> Spent literal hours on it collecting citations and what have you. Within a couple hours the page was deleted> No chance to explain. No chance to reword. Just gone completelyI&#x27;ve had the same experience. It requires too little effort to dump other people&#x27;s contributions. I don&#x27;t understand the super users&#x27; motivations in cases like this. reply Viv_moira 16 hours agorootparentI don&#x27;t understand the lack of respect towards other people&#x27;s obvious efforts. Probably has a psychological side to it. reply rdedev 16 hours agoparentprevOnly slightly tangential to parents post but I hate that Wikipedia moderators need articles to be import enough. A lot of deep dive articles into niche shows are all relegated to fandom wikis. Those sites provide no way to get a dump or even has any sort of knowledge graphI had been working with the zeshel dataset and wanted to build a knowledge graph on top of the dataset for the model that I was planning to build. If those articles were a part of Wikipedia, there would have atleast been some effort integrate them to the larger Wikipedia knowledge graph reply some_random 17 hours agoparentprevWell duh, that&#x27;s just what you get for trying to contribute without at least a Bachelors in Wikipedia Law with a minor in Editor Politicking reply Viv_moira 17 hours agoparentprevWikipedia has an ongoing project on improving articles on the subject of western esotericism (quite interesting one even for \"scientifially-minded\" if you look at the origins of the Royal Society and the surrounding protosciences). I&#x27;ve been using Wikipedia since its early beginnings, on my primary account loosely contributing for well over a decade and many of my edits from years ago were left unchanged.A few months ago I&#x27;ve tried creating a small page about a modern-day occultist who seems to have near Indiana Jones status in that community for digging out one well-known magical ritual from medieval archives all over Europe with academic scrutiny (the Abramelin ritual, A Dark Song is a recent interesting movie about it). His name already was mentioned on some related pages.Went through some lengths searching for more secondary material after they asked for it. Had hours of conversations via Wikipedia IRC to make sure I deliver exactly what is needed (and they claimed my sources are sufficient there). But even several academic papers discussing his work were not enough for the admin in charge, apparently his whole bio needs to be in a secondary source for him to be considered \"noteworthy\" - which seems to be an impossible demand in this small community.I get the danger of self-promotion, Wikipedia has a few obvious pages of company CEOs self-promoting, who probably asked some poor employee bloke to write it. But meanwhile Wikipedia is scattered with obvious industry propaganda &#x2F; damage control (see the suspiciously detailed Monsanto damage-controlling articles on glyphosate or the Séralini affair; and some more recent pharma-related topics) and literal advertisements from several industries. Just check out that page about Justin Bieber portraying that kid as some modern-day musical genius. Industry marketing departments – of course – do have the resources to literally fight for their articles full-time. On top of it this all severely and widely influences public opinion - these articles are much more widely read than one about a well-known author in a hidden subculture -, and nobody seems to be interested in doing something about it. reply empath-nirvana 17 hours agorootparentI&#x27;m sure your already aware of it, but if anyone is interested in digging into western esotericism now, the podcast is great:https:&#x2F;&#x2F;shwep.net&#x2F;But it should really be accompanied by the History of Philosophy without any gaps podcast for more historical context. It&#x27;s very interesting to hear the contrast between the \"mainstream\" and the \"underground\" takes on the exact same philosophers throughout history. reply Viv_moira 17 hours agorootparentThank you, this looks great (and quite academic, which seems difficult to come by). You likely know about him, but Wouter Hanegraaff, who holds an academic chair about western esotericism could be interesting to you. reply matrix87 15 hours agorootparentprev> and some more recent pharma-related topicsI&#x27;m kind of curious now, what are the topics? reply EnigmaFlare 17 hours agoparentprev> He had his own TV show, has multiple published books and a popular podcast.That alone doesn&#x27;t meet Wikipedia&#x27;s requirements for noteworthiness. Some of the sources you used have to be independent. Eg. published biographies written by someone else, an independent TV show about him (not starring him), etc. Otherwise any random social media influencer could be included if they publish their content in various formats. reply ziddoap 16 hours agorootparent>Otherwise any random social media influencer could be included if they publish their content in various formats.I have yet to hear a compelling reason why this would be a bad thing. Are you able to expand on why? reply EnigmaFlare 14 hours agorootparentI&#x27;m guessing because of self-promotion as well as the fact that if nobody&#x27;s written about them, they probably aren&#x27;t that significant. Wikipedia says it&#x27;s",
    "originSummary": [
      "A study examined the effects of toxic comments on volunteer engagement and activity on Wikipedia.",
      "Toxic comments were found to decrease activity and increase the likelihood of editors leaving the project.",
      "The study estimates the number of lost active days caused by toxic comments and emphasizes the need to address toxic speech on collaborative platforms."
    ],
    "commentSummary": [
      "The summary addresses concerns surrounding toxic behavior, declining volunteer activity, and content moderation on platforms like Wikipedia and Stack Overflow.",
      "It highlights the challenges of bias and the reliability of information on these platforms.",
      "Potential solutions discussed include conflict resolution processes, improving trust and good faith, and increasing accountability and transparency."
    ],
    "points": 249,
    "commentCount": 302,
    "retryCount": 0,
    "time": 1701783210
  },
  {
    "id": 38538100,
    "title": "Android 14 to Introduce AVF: Virtualizing Android Workloads for Enhanced Capabilities",
    "originLink": "https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html",
    "originBody": "Android Developers Blog The latest Android and Google Play news for app and game developers. Android Developers → Platform Android Studio Google Play Jetpack Kotlin Docs News Platform Android Studio Google Play Jetpack Kotlin Docs News05 December 2023 Virtual Machine as a core Android Primitive Posted by Sandeep Patil – Principal Software Engineer, and Irene Ang – Product Manager The Android Virtualization Framework (AVF) will be available on upcoming select Android 14 devices. The AVF, first introduced in Android 13 on Pixel devices, provides new capabilities for platform developers working on privileged applications. With AVF, we are more broadly supporting virtualization to Android. Virtualization is widely used and deployed to isolate workloads and operating systems from each other. It enables efficient scaling of infrastructure, testing environments, legacy software compatibility, creating virtual desktops and much more. With AVF virtual machines become a core construct of the Android operating system, similar to the way Android utilizes Linux processes. Developers have the flexibility to choose the level of isolation for a virtual machine: One-way isolation: Android (the host) can control and inspect the contents of the VM. These are most commonly used for sandboxing and separation, enabling multiple operating systems to run on the same machine / device, with one operating system host (Android) controlling and watching over all others. Two-way isolation (Isolated VM): Android (the host) and the virtual machine (the guest) are completely isolated from each other. Developers who deal with or store sensitive data may benefit from an isolated virtual machine. An isolated virtual machine has a two-way barrier, where neither the host (Android) nor the VM have access to each other, except via explicitly-agreed-upon communication channels. This has 2 main properties: The workload and data inside the VM is inaccessible (confidential) from the host (Android). Even if Android is compromised all the way up to (and including) the host kernel, the isolated VM remains uncompromised. Benefits of AVF Isolation With an isolated VM, developers now have an alternative to Trustzone for use cases that need isolation from Android without escalated privilege. Portability Virtual machines and the applications running inside them are far more portable than trusted applets. For example, a Linux-based virtual machine with a Linux-application payload will work on all devices that support AVF. This means that developers can build an application once and deploy it everywhere. VMs also make porting of existing Linux based applications seamless and easy, compared to porting into a Trustzone operating system. Performance AVF is designed to be lightweight, efficient and flexible. Virtual machines can: be as small as a single C program and as big as an entire operating system depending on the developer’s need; be persistent or intermittent; grow in memory or shrink depending on the overall system health; and honor Android’s scheduler hints and low-memory warnings. Extensibility AVF is designed with developers in mind. Virtual machines can be customized to meet specific use-case needs. Developers can deploy any VM payload as long as it conforms to certain boot and communication protocols specified by AVF. In addition to bringing the power of virtualization to Android and enabling all the possibilities of virtual desktops, sandboxing, AVF’s use of isolated virtual machines can benefit the following common Android use cases (and many more): Biometrics: By deploying biometric trusted applets in an isolated virtual machine, developers will have the isolation guarantee, access to more compute power for biometric algorithms, easy updatability regardless of the Trustzone operating system, and a more streamlined deployment. DRM: Widevine enables streaming DRM on Android devices. Once deployed in an isolated Virtual Machine, updates to Widevine become much easier across Android devices, regardless of the details of the various Trustzone operating systems being deployed on those devices. AVF Usage AVF provides easy APIs to query the device’s ability to create virtual machines and their supported types, and to set up secure communication channels with these virtual machines from applications and services that create them. For example, to check for the availability of the AVF APIs, and of isolated and regular VM: VirtualMachineManager manager = (VirtualMachineManager)context. getSystemService(VirtualMachineManager.class); if (manager == null) { // AVF not supported } else { int capabilities = manager.getCapabilities(); if ((capabilities & CAPABILITY_PROTECTED_VM) != 0) { // protected VM is supported } if ((capabilities & CAPABILITY_NON_PROTECTED_VM) != 0) { // non protected VM is supported } } Please find additional documentation on AVF and its APIs here. AVF Components AVF consists of the framework APIs, the hypervisor, and the Virtual Machine Manager. The hypervisor guarantees virtual machines (including Android) are isolated from each other, much like how the Linux kernel does it for processes. The AVF hypervisor (pKVM), however, does that with a significantly smaller (~50x) code base compared to the Linux kernel. The Hypervisor (pKVM) The hypervisor is focused on open source availability, security, device assignment to VMs and security by isolation between virtual machines. It has a small attack surface that meets a higher security assurance level. AVF APIs and features are fully supported by the protected KVM hypervisor (pKVM). pKVM is built on top of the industry standard Kernel-based Virtual Machine (KVM) in Linux. It means all existing operating systems and workloads that rely on KVM-based virtual machines can work seamlessly on Android devices with pKVM. Virtual Machine Manager (crosvm) crosvm, a Rust-based Virtual Machine Manager (VMM), provides the glue between the hypervisor and the AVF framework. It is responsible for creating, managing and destroying virtual machines. In addition, it provides an abstraction layer across multiple hypervisor implementations. Isolated Virtual Machines Isolated virtual machines are invisible to Android i.e. any process running in Android cannot inspect, see, tamper with the content of such a virtual machine. This guarantee is provided by the hypervisor. Virtual Machines Virtual machines are the same as isolated VMs, except they are accessible to Android processes with the right permissions and privilege. Microdroid Microdroid is a trimmed down Android OS package that is created to serve as a template for starting a virtual machine (VM). It provides developers with a familiar environment to build and run their workloads in a VM. Microdroid uses familiar Android tools and libraries, such as Bionic, Binder IPC and keystore support. Virtualization Service VirtualizationService manages all guest VMs, isolated or otherwise. It does so, primarily by managing instances of crosvm. It also exposes an AIDL API, which system services or privileged apps can use to start, monitor, and stop VMs. RpcBinder RpcBinder is an all-new backend developed for the Android Interface Definition Language (AIDL). RpcBinder enables communication to and from virtual machines using the existing binder wire protocol. This means: Developers can write interfaces to virtual machines using the language and infrastructure they are already familiar with - AIDL. Simply continue using existing AIDL interfaces even if the binder endpoint moves into a virtual machine. What’s new in Android 14? Android 14, not only makes AVF available on more devices, it also provides a new toolkit to enable building more with AVF and its components: Android System API for AVF Privileged applications can now use VMs for executing their critical workload needing isolation; Hypervisor DevEx toolkit Added tracing capability, improved debuggability and monitoring capabilities to provide insights and assist platform developers in developing inside Isolated VMs; Hypervisor Vendor Modules With vendor module extensions, our partners can customize Google’s hypervisor (pKVM) to meet their specific need and differentiate themselves; System Health Improvements With Android 14, a microdroid based VM boots 2 times faster compared to Android 13 while using half the memory. The rest of the AVF framework makes virtualization easy to use by Android services and apps. For example by abstracting inter-VM communication using AIDL as a transport layer, managing the VM lifecycle or how VMs are created. Where can you start? The AVF is only for developers of privileged applications and platform developers. TheAndroid Virtualization Framework overview provides a high level guidance on the detailed components of AVF. If you’re an Android Platform developer, try creating a Virtual Machine today and contact us at android-kvm if you have any questions. Android Security Virtualization Newer post Older post GOOGLE DEVELOPERS BLOG Google Developers Blog CONNECT Android Developers Google Play SUBSCRIBE Feed Newsletter PrivacyLicenseBrand guidelines Get news and tips by email",
    "commentLink": "https://news.ycombinator.com/item?id=38538100",
    "commentBody": "Virtual Machine as a core Android PrimitiveHacker NewspastloginVirtual Machine as a core Android Primitive (googleblog.com) 194 points by r00tbeer 11 hours ago| hidepastfavorite97 comments usrusr 1 hour agoBack at university one lecture included an infographic about how CPU and operating system features like MMU, increasing register width and the like all started at mainframe-scale installations and trickled down to desktop scale systems and later to handheld devices at a surprisingly consistent pace. It was the time w2k was trying to make NT features mainstream and J2ME arrived on phones. I extrapolated a little and made a joke about multi-user concepts arriving on phones and a few years later Android was right on schedule (when that happened, repurposing Linux users as units of app isolation was the headline feature in tech news).By that measure, virtualization is long overdue, but I really can&#x27;t claim that I&#x27;m not surprised. reply VierScar 32 minutes agoparentYou can&#x27;t claim you&#x27;re not surprised? So you can claim you are surprised? You&#x27;re surprised by this. I feel like I&#x27;m trying to understand double negatation logic in code haha reply hiatus 6 minutes agorootparentIt&#x27;s the same as saying \"I can&#x27;t say I&#x27;m not surprised\"—meaning they are unsurprised. reply Animats 2 hours agoprevSo what is something running in this virtual machine allowed to do? Talk to the Internet? Talk to the screen? Talk only to whatever started it?How much of this is closed source? reply keepamovin 1 hour agoparentPossibly one cybersecurity-related thing you could do is run a headless browser inside this VM, and bridge the network requests to the host network (a little bit like Docker).Using my open-source BrowserBox^0 project then you could have a \"bit more isolated\" Browser running on your Android device that would add \"VM escape\" to any zero-day exploit chain that might be a risk.This is speculation tho, I don&#x27;t know if it&#x27;s actually feasible based on the Android reality right now, but assuming the capabilities that are provided are like a regular headless VM, then it should be. :)0: https:&#x2F;&#x2F;github.com&#x2F;BrowserBox&#x2F;BrowserBox reply saagarjha 2 hours agoparentprevI think the design is intended so that you mostly only get to do the last one. reply londons_explore 27 minutes agoprevHow lightweight are these? Can I start 100 Vm&#x27;s to render content from 100 web origins in a secure web browser? reply Shoop 6 hours agoprevHow does two way isolation work? How do you prevent the host kernel (which presumably has full control of the hardware?) from inspecting the guest VM? reply jbott 5 hours agoparentIt looks like the host kernel is not in full control – there is a EL2-level hypervisor, pKVM [1] that is actually the highest-privilege domain. This is pretty similar to the Xen architecture [1] where the dom0 linux os in charge of managing the machine is running as a guest of the hypervisor.1. https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;core&#x2F;virtualization&#x2F;architec... 2. https:&#x2F;&#x2F;wiki.xenproject.org&#x2F;wiki&#x2F;Xen_Project_Software_Overvi... reply pjmlp 3 hours agorootparentCommonly known as type 1 hypervisor architecture, by opposition to type 2 hypervisor, which run as OS services.Ironically the revenge of microkernels, as most cloud workloads run on type 1 hypervisors. reply bonzini 3 hours agorootparentNo, KVM is also a type 1 hypervisor but it doesn&#x27;t attempt (with the exception of pKVM and of hardware protection features like SEV, neither of which is routinely used by cloud workloads) to protect the guest from a malicious host. reply Vogtinator 2 hours agorootparentKVM is a type 2 hypervisor as the \"Dom 0\" kernel has full HW access. Other guests are obviously isolated as configured and are like special processes to userspace.It gets a bit blurry on AArch64 without and with VHE (Virtual Host Extensions) as without VHE ( On the Pixel 7, the most configuration you&#x27;ll need to do is similar to Shizuku. You connect to your own phone over wireless adb, configure the maximum container size, and then choose your Linux distribution. It&#x27;ll download, configure, and then execute the virtual machine. reply robertwt7 2 hours agoprevAlthough this is very exciting. Surely performance is not the benefit here? It won’t perform better than android app built not on top of the virtualisation tdchnology? reply omeid2 2 hours agoparentAndroid apps are already running on top of a Virtualisation Technology\", both current ART (Android Runtime) and the previous one, Delvik, runtimes are virtual machines, process level virtual machines, but they do bytecode translation&#x2F;JIT nonetheless.If AVF allows running native code, it might actually be cheaper than the current arrangement. reply ips1512 2 hours agoparentprevAndroid apps performs better if built natively, Google might take some steps to enhance its performance. reply awoimbee 10 hours agoprevWill this allow running linux VMs on any Android device ? Via something like nestbox: https:&#x2F;&#x2F;www.patreon.com&#x2F;posts&#x2F;74333551 ? reply heavyset_go 9 hours agoparentThis is already possible if your phones ship with the KVM kernel module, like on some Pixel devices, but reading the article suggests that KVM will become standard on all Android devices to enable this.edit: according to this[1], yes, the pKVM functionality that&#x27;s standard in Android exposes KVM functionality so that you can run VMs on Android.[1] https:&#x2F;&#x2F;www.xda-developers.com&#x2F;android-13-dp1-google-pixel-6... reply saagarjha 8 hours agoparentprevAVF supports this, people have used it to boot Linux and Windows. See for example https:&#x2F;&#x2F;twitter.com&#x2F;kdrag0n&#x2F;status&#x2F;1493089098944237568 reply jsight 6 hours agoparentprevIt sounds like it will become common eventually. I just wish that there were a more supported pathway to running full VMs like that. These devices are powerful enough to do it pretty well now. reply KRAKRISMOTT 2 hours agoparentprevAre the VMs hardware accelerated? reply saagarjha 2 hours agorootparentYes. reply ForkMeOnTinder 9 hours agoprevSo on desktop, if I spin up a VM with networking disabled I feel pretty confident I can run anything safely, even malware is not going to escape.What&#x27;s the current state of the art for Android virtualization? Let&#x27;s assume we&#x27;re talking about the newest Pixel and newest Android version. Is there any way to safely run malware or the Facebook app in some sort of air-gapped container and throw it away when you&#x27;re done? reply fleventynine 9 hours agoparent> if I spin up a VM with networking disabled I feel pretty confident I can run anything safely, even malware is not going to escape.You are putting too much faith in your VM monitor to keep you safe. There&#x27;s a lot of attack surface in (for example) QEMU peripherals, and there&#x27;s plenty of examples of VM escape [1]. CrosVM is probably the only publicly available VMM I&#x27;d be willing to trust, and even then I&#x27;d be nervous running state-sponsored malware on a machine with important data.[1] https:&#x2F;&#x2F;www.google.com&#x2F;search?q=qemu+vm+escape reply bonzini 8 hours agorootparentWhile QEMU uses C, which is not great, it has on its side 15+ years of hardening by the KVM developers. The problem with QEMU is not so much insecurity, it&#x27;s that it contains the kitchen sink.However, most of the exploits you&#x27;ll find in QEMU are against configurations that are never used in real world virtualization scenarios where guests are untrusted. You can recognize them because hardware not commonly used with untrusted guests does not get a CVE.For a while, slirp was the remaining major issue because it was used way beyond the original intention. But now it&#x27;s been tamed and there&#x27;s also passt, a much higher performance and much more secure implementation of user-mode networking. reply ungamedplayer 3 hours agorootparent> You can recognize them because hardware not commonly used with untrusted guests does not get a CVE.This is not true. Even non default configuration of any software or hardware that contains a security vulnerability can get a CVE. It has in the past and will again in the future.Source: I have assigned over 2000 cves for the kernel. reply bonzini 1 hour agorootparentYes, and the policy of QEMU is to not assign CVEs for bugs that would generally be hit only when QEMU is used as a development platform, as opposed to using it to offer virtualization services.https:&#x2F;&#x2F;www.qemu.org&#x2F;contribute&#x2F;security-process&#x2F;We are colleagues by the way. :) reply monocasa 8 hours agorootparentprevI&#x27;d probably trust firecracker too since it was designed specifically to avoid qemu&#x27;s attack surface and runs in production for Amazon. reply my123 6 hours agorootparentQemu also runs in production darn near everywhere, especially on Xen. reply surajrmal 7 hours agorootparentprevIt also happens to be a fork of crosvm reply monocasa 7 hours agorootparentIt is not, but they do share some code. reply surajrmal 2 hours agorootparentAccording to their own FAQ it is indeed: https:&#x2F;&#x2F;github.com&#x2F;firecracker-microvm&#x2F;firecracker&#x2F;blob&#x2F;main... reply surajrmal 7 hours agorootparentprevLuckily android is using crosvm reply jmprspret 6 hours agoparentprev> Is there any way to safely run malware or the Facebook app in some sort of air-gapped container and throw it away when you&#x27;re done?User profiles can be used in this exact way. Guest user if you intend to install+wipe it right away (though this will prove cumbersome eventually due to having to reinstall the app every time, etc). There is a significant isolation benefit to them, though not currently virtualized. With the isolation can come usability issues. Like transferring files from one profile to another.They can very slow however (slow to load+setup, and switch between, I mean. when you&#x27;re inside its effectively a separate, fresh, OS install). reply ClassyJacket 8 hours agoparentprev*malware like the Facebook app reply heavyset_go 9 hours agoparentprevPretty sure Android already uses Linux containers&#x2F;namespaces for app isolation. reply seabrookmx 7 hours agorootparentYou&#x27;re thinking of ChromeOS I think, which uses a combination of containers and virtualization (via the same VMM in this article) for Linux and Android apps.. reply saagarjha 8 hours agorootparentprevIt uses separate UIDs mostly. reply phone8675309 8 hours agorootparentAlong with SELinux reply tripdout 9 hours agoprevWhy does the tutorial for creating a demo app, https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;core&#x2F;virtualization&#x2F;writeavf..., only work on Cuttlefish (emulator)?Nevermind, only the demo app, not the tutorial, so who knows what its doing. reply saagarjha 8 hours agoparentMaybe because it requires a platform signature to use? reply jeffrallen 3 hours agoprevAnother salvo in the war on general purpose computing.(https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;473794&#x2F;) reply transpute 3 hours agoparentIf Android phones can run non-Android VMs of the user&#x27;s choice, the phones will gain new purpose. reply jeffrallen 2 hours agorootparentOk cool.But you and I both know that this feature was designed for the DMCA-lovin&#x27; Hollywood types and the control-freak enterprise IT BOFHs, not for your cool hack.Let&#x27;s use their tools of oppression against them! (fist emoji) reply transpute 1 hour agorootparentMedia and Enterprise already have TrustZone, Knox, Intune, etc. which work \"enough\".Newer markets include cashless (CBDC) payments and digital identity anchored in human biology, demanding more security than legacy content.> Biometrics: By deploying biometric trusted applets in an isolated virtual machine, developers will have the isolation guarantee, access to more compute power for biometric algorithms, easy updatability regardless of the Trustzone operating system, and a more streamlined deployment.Fortunately, OSS can enable N-party transaction transparency, we don&#x27;t have to settle for one-way mirrors and WeChat clones. reply josephcsible 10 hours agoprev [–] Two-way isolation seems like it&#x27;d only be useful for DRM and Treacherous Computing. reply nl 9 hours agoparentThis is such a bad take.I&#x27;d love the easy ability to run confidential computing loads with fine grained control over the data it gets access to. You can do this now on the desktop using SGX (etc) but on mobile it&#x27;s really hard.As a specific example of this, it&#x27;d be great to be able to run Whisper continually and have strong, system level guarantees about what can read the data. reply saagarjha 8 hours agorootparentThe threat model you have in your head seems to imply that you don&#x27;t trust your OS to not peek into what Whisper is doing? There are very few workloads that need or can operate under that model. reply somanytta 8 hours agorootparentIt&#x27;s not really a matter of need, more a matter of good hygiene.Do you trust any modern OS not to accidently include sensitive information when it generates a crash report for an app and sends it off the some remote server in the background?Isolation is a useful tool. In an ideal world it can be done perfectly at the OS level, but we don&#x27;t live in that world. reply saagarjha 8 hours agorootparentI agree that being able to isolate things that have different security domains is a useful tool. That said, I am not really seeing how pKVM provides useful primitives for much other than DRM, which has historically been the primary usecase for trusted execution that isolated VMs seem to provide. reply somanytta 7 hours agorootparentConsider that, I want to be able to use a regular android OS, but I don&#x27;t completely trust it, either its purposely malicious or just accidently going to leak info. So isolation is good in this case, its much easier to audit the mechanism of isolation rather than the whole OS.The problem with DRM and \"trusted computing\" part is that it&#x27;s under someone else&#x27;s control, some central authority etc. From my reading of the docs on this, this is not the case with pVM, from https:&#x2F;&#x2F;source.android.com&#x2F;docs&#x2F;core&#x2F;virtualization&#x2F;security> Data is tied to instances of a pVM, and secure boot ensures that access to an instance’s data can be controlled> When a device is unlocked with fastboot oem unlock, user data is wiped.> Once unlocked, the owner of the device is free to reflash partitions that are usually protected by verified boot, including partitions containing the pKVM implementation. Therefore, pKVM on an unlocked device won&#x27;t be trusted to uphold the security model.So my reading of this is that that it is under the users control, as long as they have the ability to unlock the bootloader, and reflash the device with their own images.I&#x27;d love someone who is more knowledgeable to weigh in, but this tech, to me, doesn&#x27;t seem that close to TPM&#x2F;DRM type chips where there is no possibility of user control. reply saagarjha 2 hours agorootparentIt is control in the sense that you can run your own applets I guess, but it is not control in the sense that you can necessarily inspect what the programs are doing, because once you reflash the device I&#x27;m sure the DRM programs will refuse to run. reply nl 6 hours agorootparentprevAs I said in my other response, I make heavy use of trusted (confidential) VMs for machine learning in cloud environments.There are also vendors that are doing smart contract execution in trusted computing devices so you can get the benefits of trusted execution without the overhead of everyone executing the same code. reply saagarjha 2 hours agorootparentThere are a handful of potential uses for confidential VMs, but not many of them really seem to make sense on phones? reply sangnoir 7 hours agorootparentprevI&#x27;d love for my banking app to be completely isolated from the rest of my phone OS, in case I get malware. I&#x27;m sure journalists at risk of targeting by NSO and its ilk would appreciate isolation or their messaging apps reply saagarjha 2 hours agorootparentThis is an interesting usecase (basically Qubes) but it has high overhead and I don&#x27;t really see the framework as being designed to support this, at least yet. You&#x27;d need to move all sorts of services into the VM to support the app (like, for example, someone needs to pass touch input and network traffic into the VM) and at this begins to look like an entire OS running in there. reply nl 6 hours agorootparentprevSo I&#x27;m most familiar with using this in cases like machine learning on private data in cloud environments where you want to make it impossible for the cloud operator to see the data you are using.I think there are usecases like this outside the mobile _phone_ that are interesting. For example on-device learning for edge devices where the device is not under your control. reply saagarjha 2 hours agorootparentSee the thing here is that if the device is not under \"your\" control (\"you\" being a company or something, and the device being owned by a user) I don&#x27;t think they will really appreciate you using their hardware to train your model in a way they don&#x27;t get to see. Why would I want to support this on my own phone? reply fuomag9 8 hours agorootparentprevBtw SGX has been removed from 11th gen desktop CPUs and onwards https:&#x2F;&#x2F;www.bleepingcomputer.com&#x2F;news&#x2F;security&#x2F;new-intel-chi... reply zeusk 9 hours agorootparentprevis it really? wasn&#x27;t that the whole point of ARM TZ&#x2F;SEP? reply nl 8 hours agorootparentTrusted computing can be used for DRM. I&#x27;m much more interested in it as a privacy enhancing technology: the fact that you can have strong guarantees about what can be done with data in the enclave is useful for a lot of applications where you have sensitive data.(Putting aside the fact for the moment that most - if not all - trusted computing platforms have some security vulnerabilities. Obviously this is bad, but doesn&#x27;t preclude their utility) reply saagarjha 8 hours agorootparentprevThis is supposed to be a replacement to TrustZone applets. reply gjsman-1000 9 hours agorootparentprevNot really. ARM TZ has been repeatedly blown open, in part because it’s not really a separate core or virtualized workload, but a different “mode of operation” that the standard CPU cores switch into temporarily. Basically going back-and-forth between TZ and your OS if I understand correctly. Turns out that’s a side-channel attack nightmare. reply refulgentis 9 hours agorootparentprevI used to work at Google adjacent to this stuff and A) you wouldn&#x27;t boot up a whole VM for this, on a phone, that&#x27;d be very wasteful B) there&#x27;s much simpler ways to provide the same guarantee.So in general, just would avoid labeling the quality of other people&#x27;s takes. You never know who is reading yours reply nl 8 hours agorootparentI agree there are currently better ways of doing this (because as you mention the resource&#x2F;protection trade off for this technology on this application is sub-optimal), but the context here is as an example on HN where the data privacy is obvious so I didn&#x27;t have to write a whole paper explaining it. reply refulgentis 6 hours agorootparentIts \"not even wrong\", if you had a million monkeys on a million typewriters with a million trillion millenia, still, none would come up with a paper long enough to explain how that&#x27;d help anything (ex. trivially, microphone) reply jeroenhd 10 hours agoparentprevThis seems like an excellent tool for digital ID cards, banks, government authentication apps, maybe 2FA apps, cryptocurrency wallets, you name it. Anything that&#x27;s more important than a calculator.DRM and remote attestation already use a separate secure environment, so I don&#x27;t see what would change by adding virtualisation. reply heavyset_go 10 hours agorootparentWebsites will require digital ID just to use them, along with remote attestation. They will also be able to ban or block you in an actually effective and comprehensive way.There will be a chilling effect because people won&#x27;t want to upset their Google&#x2F;Microsoft&#x2F;Apple&#x2F;Meta etc overlords by saying or doing the wrong thing, and then get locked out of services they need to exist in society, do their job, spend money, etc. reply jeroenhd 9 hours agorootparentDigital ID exists and is widely used, yet I only need to use my digital ID to authenticate with government services. Remote attestation is the norm for many types of apps already yet I can use my bank app on my rooted phone just fine, or use my phone to authenticate with my government&#x27;s SSO system.I&#x27;m no fan of the modern dependence on Play Services or Google&#x27;s attempts to kill adblockers through remote attestation, but none of these technologies are inherently bad. Business devices authenticating to business websites should allow remote attestation to verify that their hardware has not been tempered with just as an extra security measure.Maybe your government is more evil or incompetent than mine, but bad governments aren&#x27;t going to he limited by technological concepts like these. reply heavyset_go 9 hours agorootparentI&#x27;m not worried about the government, I&#x27;m more worried about inscrutable decisions made by companies like Google, where their automated systems decide that you&#x27;re an anomaly, and thus malicious, and choose to ban you.Instead of just losing your account, you (or at least both your machine and your digital ID) are banned for good. This already happens with phones, where the entire device gets banned by apps for good, adding a layer of digital ID on top of it worsens the consequences of such decisions by platform owners against users.> Remote attestation is the norm for many types of apps already yet I can use my bank app on my rooted phone just fine,Many people can&#x27;t on their rooted phones, and this cat-and-mouse game will eventually be won by the parties with million&#x2F;billions to throw at it. reply usrusr 1 hour agorootparentprevDigital ID is safe from abuse by our ad overlords as long as it only happens in insular implementations for markets smaller than California. Things would look wildly different if digital ID was a thing in the USA (I find it rather amusing how they claim to have no ID at all, yet a decade in Europe seems to involve less presenting of ID than a month in the US involves presenting their driver&#x27;s license ID substitute)But I don&#x27;t disagree, I&#x27;d rather have a rooted phone with a few islands out of my (and the software that I run!) control for sensitive authentication use cases than a phone where I&#x27;m not in control at all. Or than two phones, because only one of them can be rooted. reply wlesieutre 10 hours agoparentprevMaybe banking apps would let you run them on rooted phones if they were in an isolated VM reply heavyset_go 9 hours agorootparentEvery app will want to run in isolated VMs and rooting will mean nothing.It&#x27;s like SafetyNet today, you can&#x27;t run a good amount of apps on unapproved platforms already, even apps that don&#x27;t handle confidential data. reply usrusr 58 minutes agorootparentIn an ideal world, you could opt-out of isolation without giving the code in the container a way to know. You wouldn&#x27;t want to opt-out your banking TAN generator just like you wouldn&#x27;t want to put the password in your email footer, but a Facebook client would likely be a popular target (despite the hypothetical risk of an attacker destroying your reputation by posting in your name). reply treyd 10 hours agorootparentprevThis nonsense means I just use them in the browser. There is no functionality the apps would provide me that makes it worth fighting with their superstitious nonsense. reply heavyset_go 9 hours agorootparentSome banks already require you to install and use apps to approve of transactions made outside of the app.When I traveled, this is how I was able to spend money without having to call my bank every time I tried to use my card in person. reply clnq 6 hours agorootparentWhy do US banks do that? I’ve never had a UK or EU bank call me to verify a transaction.Do you have the IdentityCheck&#x2F;SecureCode&#x2F;3-D Secure stuff (2FA for online transactions and at certain terminals)? Are these calls for transactions without chip + PIN?I’ve had some transactions declined while travelling but maybe about 1&#x2F;1000, and still no call, and nothing the bank support could do to allow them if I called. I’d just have to use a different bank with a vendor. It’s very much a “computer says no” situation then. Otherwise, the payment just goes through in the 99.9% of all cases.But the banks in central EU, the Nordics, and the UK don’t seem to monitor the transactions I make while travelling to the point that there would be an actual person involved (calling me or reaching out in some other way).I’m mostly curious about what problem these bank calls are solving. Is it for credit card fraud? In that case, I wonder why this seems to not be a practice in Europe. Is it because we do chip & PIN in physical payments, and 2FA for online&#x2F;some kiosks? reply usrusr 36 minutes agorootparent> I’ve never had a UK or EU bank call me to verify a transactionThat probably just means that you never made transactions that crossed the banks&#x27; suspicion threshold. Which might be quite high if the bank is confident that it won&#x27;t be on the hook for credential abuse and does not care if their customers lose money to identify theft. That confirmation call would be an indication of good service, not of bad service.I&#x27;m not saying that calls would be preferable to better authentication schemes like chip+pin (in skimming is very much a thing though), calls are just another second factor after all. And not even a particularly safe one. But defense should be layered and that layer stack should absolutely contain a form of confirmation call on some level if you are a bank. reply treyd 8 hours agorootparentprevWhat are you supposed to do if you don&#x27;t have a smartphone? My bank simply texts me if there&#x27;s suspicious purposes and you reply \"YES\". reply candiddevmike 10 hours agoparentprevSee for example the Xbox, where everything runs as a VM. reply kiririn 9 hours agoparentprev [–] Yep, you need only look at the number of server providers offering confidential computing (pretty much only the big 3) and the premium they charge for it (10x, except AWS “trust me bro” Nitro)Confidential computing is cool and useful when you’re the one controlling the VM, but scary when you’re the one blindly running it on your hardwareHopefully this gets (publicly!) backdoored like SEV, SGX, etc reply somanytta 8 hours agorootparent [–] > Confidential computing is cool and useful when you’re the one controlling the VM, but scary when you’re the one blindly running it on your hardwareImportant point.> Hopefully this gets (publicly!) backdoored like SEV, SGX, etcFrom my reading this doesn&#x27;t need to be backdoored, if you have the ability to unlock the bootloader, you are not reliant on googles root of trust to be able to use this feature, you can go ahead and become your own \"vendor\", by signing your own images, or use your choice of vendor, then relock the bootloader and have the same security guarantees.I&#x27;ll admit this only from a cursory glance over the documentation and a vague understanding, happy to be corrected, but seems a lot of the arguments in this thread are about your first point, who has control over the OS.I&#x27;ll also add that the EU is being quite proactive in people having control over their own device, and who is their &#x27;choice of vendor&#x27; so while I understand concerns people bring up, I&#x27;m a bit more optimistic that it can be a more useful tool than not. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Android Virtualization Framework (AVF) will be introduced in Android 14 devices, offering new capabilities for platform developers.",
      "AVF enables virtualization on Android, allowing for isolation of workloads and operating systems.",
      "Developers can opt for one-way isolation, where Android controls the virtual machine, or two-way isolation, with complete isolation between Android and the virtual machine."
    ],
    "commentSummary": [
      "Google is introducing virtual machine technology in Android to improve app security and isolation.",
      "The discussion explores the potential uses, vulnerabilities, and benefits of virtualization in Android.",
      "It also examines alternative virtualization options and the ability to run non-Android virtual machines on Android devices.",
      "Trust and isolation in the face of information leaks or malicious behavior by the OS are highlighted.",
      "The conversation extends to the use of trusted computing technologies in machine learning, data privacy concerns, and the effectiveness of digital ID cards and remote attestation.",
      "Rooted phones and isolated implementations are preferred for sensitive authentication purposes.",
      "The need for bank calls for transaction verification is questioned.",
      "The discussion also touches on smartphone dependency, confidential computing in virtual machines, and the EU's efforts to give individuals more control over their devices.",
      "The writer believes that having control over one's device and vendor choice can enhance the usefulness of a technology tool."
    ],
    "points": 194,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1701816951
  }
]
