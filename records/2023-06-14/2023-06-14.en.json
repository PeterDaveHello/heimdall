[
  {
    "id": 36304143,
    "timestamp": 1686621308,
    "title": "Llama.cpp: Full CUDA GPU Acceleration",
    "url": "https://github.com/ggerganov/llama.cpp/pull/1827",
    "hn_url": "http://news.ycombinator.com/item?id=36304143",
    "content": "Skip to contentProductSolutionsOpen SourcePricingSign inSign upggerganov/llama.cppPublicNotificationsFork 4.4kStar 31.3kCodeIssues298Pull requests70DiscussionsActionsProjects4WikiSecurityInsightsNew issueCUDA full GPU acceleration, KV cache in VRAM #1827OpenJohannesGaessler wants to merge 17 commits into ggerganov:masterfrom JohannesGaessler:cuda-full-gpu-2+798 \u2212119Conversation 63Commits 17Checks 21Files changed 11ConversationCollaboratorJohannesGaessler commented yesterdayJun 12, 2023 \u2022editedThis PR adds GPU acceleration for all remaining ggml tensors that didn't yet have it. Especially for long generations this makes a large difference because the KV cache is still CPU only on master and gets larger as the context fills up. Prompt processing is also significantly faster because the large batch size allows the more effective use of GPUs. For the following performance numbers PP is prompt processing, and TG 128/1024 are the generation of 128/1024 tokens with an empty prompt:GPU Test Model t/s master t/s PR SpeedupRTX 3090 PP 7b q4_0 452 990 2.19RTX 3090 PP 13b q4_0 318 645 2.03RTX 3090 PP 33b q4_0 155 292 1.88RTX 3090 TG 128 7b q4_0 45.66 62.66 1.37RTX 3090 TG 128 13b q4_0 29.94 38.21 1.28RTX 3090 TG 128 33b q4_0 14.89 17.61 1.18RTX 3090 TG 1024 7b q4_0 31.08 56.34 1.81RTX 3090 TG 1024 13b q4_0 20.16 35.20 1.75RTX 3090 TG 1024 33b q4_0 10.37 16.49 1.59Note that I was only using a single thread for the PR since multiple threads have no benefit when all computations are on the GPU but they still add overhead.I added CUDA kernels for scale, cpy, diag_mask_inf, and soft_max. I also added two special kernels for doing matrix vector multiplication with permuted or not contiguous inputs; they are used in conjunction with the KV cache.Changes to ggml.c: I added a utility function ggml_is_permuted.Things that are still to do:Fix VRAM memory leaks.Fix memory usage prints.Check performance for lower-end GPUs and add a --low-vram option if necessary.Check Windows performance and maybe disable features.General code cleanup.\ud83d\udc4d90mirek190, slaren, Glavin001, Extraltodeus, avischiffmann, oobabooga, sequoiar, titoBouzout, franchb, transitive-bullshit, and 80 more reacted with thumbs up emoji\ud83c\udf8925thushan, ggerganov, pabl-o-ce, bxh-io, dakl, theolivenbaum, mdawid, fredsadaghiani, melodysdreamj, DerekChia, and 15 more reacted with hooray emoji\u2764\ufe0f12vihangd, mudler, Josua996, garanews, ShivBhosale, raphaelrk, AttilaBerczik, LiliumSancta, hugoabonizio, Pospelove, and 2 more reacted with heart emoji\ud83d\ude8048Green-Sky, Extraltodeus, avischiffmann, TheSeamau5, tmostak, megupta, sequoiar, andrewschreiber, ijt, DaseinPhaos, and 38 more reacted with rocket emojiJohannesGaessler added performance Speed related topicshardware Hardware relatedlabels yesterdayJun 12, 2023CollaboratorKerfuffleV2 commented yesterdayJun 12, 2023 \u2022editedCheck performance for lower-end GPUs and add a --low-vram option if necessary.Unfortunately, I think that will be necessary. I have a 6GB 1060 - I can't even run main with 2048 context and -ngl 0 on a 65B model. Using -ngl 0 -c 1800 uses about 4.5GB VRAM even though the log output claims total VRAM used: 512 MBSince I upgraded my CPU, offloading to the GPU isn't really worthwhile anymore except for prompt ingestion. It does make a big difference there so it would be unfortunate if just using cuBLAS prompt input was impossible for full context on larger models.edit: By the way, let me know if/when further testing with this old hardware would be helpful. I can probably assist.\ud83d\udc401ruchir29 reacted with eyes emojiContributorTheBloke commented yesterdayJun 12, 2023Really excited to try this! Amazing work!\ud83d\udc4d4oobabooga, lin72h, avideci, and kimdwkimdw reacted with thumbs up emojiContributorTheBloke commented yesterdayJun 12, 2023 \u2022editedDefinitely seems faster and I can see it using 10% more GPU.Unfortunately when I ask it to return 512 tokens I'm getting an abort before the end of generation:CUDA error 1 at ggml-cuda.cu:1920: invalid argumentTesting on H100 80GB. Ubuntu 20.04, gcc 9.4.0. CUDA toolkit 12.0.1.nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2023 NVIDIA CorporationBuilt on Fri_Jan__6_16:45:21_PST_2023Cuda compilation tools, release 12.0, V12.0.140Build cuda_12.0.r12.0/compiler.32267302_0Compiled with:make clean && LLAMA_CUBLAS=1 make -jCommand line arguments: ./main --color -ngl 60 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m /workspace/WizardLM-30B-Uncensored.ggmlv3.q4_0.bin -p \"USER: write a story about llamas\\nASSISTANT:\"Full output:[pytorch2] ubuntu@h100:/workspace/git/cuda_llama git:(cuda-full-gpu-2) $ ./main --color -ngl 60 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m /workspace/WizardLM-30B-Uncensored.ggmlv3.q4_0.bin -p \"USER: write a story about llamas\\nASSISTANT:\"main: build = 678 (0fe5ff2)main: seed = 1686600421ggml_init_cublas: found 1 CUDA devices: Device 0: NVIDIA H100 PCIellama.cpp: loading model from /workspace/WizardLM-30B-Uncensored.ggmlv3.q4_0.binllama_model_load_internal: format   = ggjt v3 (latest)llama_model_load_internal: n_vocab  = 32001llama_model_load_internal: n_ctx   = 512llama_model_load_internal: n_embd   = 6656llama_model_load_internal: n_mult   = 256llama_model_load_internal: n_head   = 52llama_model_load_internal: n_layer  = 60llama_model_load_internal: n_rot   = 128llama_model_load_internal: ftype   = 2 (mostly Q4_0)llama_model_load_internal: n_ff    = 17920llama_model_load_internal: n_parts  = 1llama_model_load_internal: model size = 30Bllama_model_load_internal: ggml ctx size =  0.13 MBllama_model_load_internal: using CUDA for GPU accelerationllama_model_load_internal: mem required = 2532.68 MB (+ 3124.00 MB per state)llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch bufferllama_model_load_internal: offloading 60 layers to GPUllama_model_load_internal: total VRAM used: 17736 MB....................................................................................................llama_init_from_file: kv self size = 780.00 MBsystem_info: n_threads = 13 / 26 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0 USER: write a story about llamas\\nASSISTANT:Once upon a time, in the high mountains of South America, there lived a group of llamas. They were known for their long necks and soft wool coats that kept them warm during the cold winter months. The llamas roamed the rugged terrain, grazing on grasses and plants that grew in the harsh environment.One day, a young llama named Lluvia decided to explore beyond her usual range. She wandered through fields of wildflowers and up steep ravines, until she came to a stream. As she drank from the cool water, she noticed a group of other llamas on the opposite bank. They were dressed in colorful saddles and had bags strapped to their backs.Curious, Lluvia crossed the stream and approached the group. \"Hello,\" she said. \"What are you doing here?\"The leader of the group, a wise old llama named Tupac, replied, \"We are on a journey to deliver supplies to a village at the base of the mountains. The people there need our help.\"Lluvia was impressed by the llamas' bravery and kindness. She asked if she could join them on their journey, and they welcomed her with open hearts.Together, the group set out down the mountain, carrying their heavy loads with ease. They passed through fields of crops and orchards, and Lluvia learned about the different plants and animals that lived in the region.Finally, they arrived at the village, where they were greeted by grateful locals who thanked them for their help. The llamas unloaded their supplies and spent the night in a cozy barn, surrounded by the warmth of the community.In the morning, Lluvia said goodbye to her new friends and returned to her own herd. But she never forgot the experience of helping others and the sense of purpose it had given her. From that day forward, she made a promise to herself to always be kind and compassionate, just like the wise old llama Tupac.And so, Lluvia went on to live a long and fulfilling life, inspiring others with her courage and generosity, and always remembering the lessons she had learned on her journey with the kind-CUDA error 1 at ggml-cuda.cu:1920: invalid argument[pytorch2] ubuntu@h100:/workspace/git/cuda_llama git:(cuda-full-gpu-2) $Reducing to -n 400 resulted in successful completion.CollaboratorAuthorJohannesGaessler commented yesterdayJun 12, 2023@TheBloke I can reproduce the issue. It seems to have to do with -ngl 60; with those parameters some tensors are still on the CPU and the error happens when copying the data to the GPU. Setting -ngl 61 moves the non-repeating layers to the GPU as well and seems to work as intended.\ud83d\udc4d3roy09, lin72h, and wyzhangyuhan reacted with thumbs up emojiContributorTheBloke commented yesterdayJun 12, 2023 \u2022editedAhh OK, thanks. I did -ngl 60 because I thought that'd be the whole model. Yes that fixes it, thank you. Now tested fine up to 2000 tokens.EDIT: these benchmark figures were with --threads 13, which I learned later is very suboptimal. They at not representative of the real speed of this PRMy first benchmark results are, testing with 400 tokens returned, results averaged over 4 runsH100 + Intel(R) Xeon(R) Platinum 8480+:7B q4_0 :main branch: 38.23 ms/tokenPR: 33.49 ms/token (best was 31.38ms)30B q4_0 :main branch: 100.33ms/tokenPR: 85.61 ms/token (best was 79.28ms)CollaboratorAuthorJohannesGaessler commented yesterdayJun 12, 2023Did you set --threads 1? You didn't do it in the command you posted previously and it should give a performance boost due to lower overhead.ContributorTheBloke commented yesterdayJun 12, 2023 \u2022editedOh wow! My apologies, I missed that in your OP.H100 + Intel(R) Xeon(R) Platinum 8480+. 500 tokens returned.7B q4_K_S :main branch (--threads 13): 39.2ms/tok (25.51 tokens/s)PR (--threads 1): 16.4 ms/token (60.97 tokens/s)= 2.39x30B q4_K_S:main branch (--threads 13): 92.69 ms/token (10.79 tokens/s)PR (--threads 1): 53.71 ms/token (18.62 tokens/s)= 1.73xThis is really huge. This is the first time I've ever had GGML outperform GPTQ on any system.The max performance I've ever had from GPTQ 7B is 98 tokens/s, but that requires both a fast GPU (4090) and a top single-core CPU (i9-13900K). This system I'm on now has top GPU (H100), but middling single-core performing CPU, and it does around 45 tokens/s on 7B GPTQ 4bit.So on 7B on this server, GGML is now beating GPTQ 4bit by a healthy margin.I notice that llama.cpp is also pegged at 100% of one core, so I'm assuming a faster single-core CPU would likewise scale llama.cpp's figure. So soon I'll test on that 4090 + i9-13900K system and see if llama.cpp can get close to that 100 t/s mark.I just re-tested 30B GPTQ 4bit on this H100 system and got 20 t//s - still ahead of GGML, but only a tiny bit.This is really game-changing. Well done!\ud83c\udf8913Glavin001, mattkanwisher, thushan, pabl-o-ce, th5, phantom32-0, nightlyworker, lin72h, jacobfriedman, AmineDjeghri, and 3 more reacted with hooray emojiCollaboratorAuthorJohannesGaessler commented yesterdayJun 12, 2023I notice that llama.cpp is also pegged at 100% of one core, so I'm assuming a faster single-core CPU would likewise scale llama.cpp's figure.Currently the llama.cpp main thread always waits for the GPU computation to finish before arranging the next tensor. But it should be possible to change the logic to instead start preparing the next tensor immediately. Then I think CPU performance will be largely irrelevant.\ud83d\udc4d1lin72h reacted with thumbs up emojiContributorTheBloke commented yesterdayJun 12, 2023I notice that llama.cpp is also pegged at 100% of one core, so I'm assuming a faster single-core CPU would likewise scale llama.cpp's figure.Currently the llama.cpp main thread always waits for the GPU computation to finish before arranging the next tensor. But it should be possible to change the logic to instead start preparing the next tensor immediately. Then I think CPU performance will be largely irrelevant.That would be incredible. It's always been both counter-intuitive and frustrating how GPTQ/pytorch inference is held back by CPU. Especially as it means that many high-end CPUs - the kinds you find on servers - actually do really poorly compared to gaming CPUs like the i9-13900K.I mentioned that with GPTQ 4bit my top 7B is 98 tokens/s with 4090 + i9-13900K. My normal figure on most servers I use is around 30 tokens/s, simply because server CPUs like AMD EPYC have really bad single-core performance.If you could completely de-couple from single core performance then GGML is going to leave pytorch/GPTQ in the dust for most home/single-prompt users.CollaboratorAuthorJohannesGaessler commented yesterdayJun 12, 2023I don't think I'll be putting that in this PR though. My top priority is to make sure that the things that are already in it work. Also Python is literally 100x slower than C++ so I'm not sure whether the difference will be as large for llama.cpp.ContributorTheBloke commented yesterdayJun 12, 2023I just tested on the 4090 + i9-13900K system and it's amazing:7B q4_K_S:New llama.cpp performance: 109.29 tokens/sAutoGPTQ CUDA 7B GPTQ 4bit: 98 tokens/s30B q4_K_S:New PR llama.cpp performance: 29.11 tokens/sAutoGPTQ CUDA 30B GPTQ 4bit: 35 tokens/s\ud83d\ude802gotzmann and milyiyo reacted with rocket emojiContributorTheBloke commented yesterdayJun 12, 2023 \u2022editedI don't think I'll be putting that in this PR though. My top priority is to make sure that the things that are already in it work. Also Python is literally 100x slower than C++ so I'm not sure whether the difference will be as large for llama.cpp.Quite understand. Got to have something left to improve next week :)On the 4090 + i9-13900K it's still pegged at 100% CPU and GPU utilisation is at around 69%. So looks like there's still a decent margin there for further optimisations, if you can decouple it from CPU.Could it reach 150 tokens/s on 7B 4bit? \ud83d\ude01\ud83d\udc401lin72h reacted with eyes emojiCollaboratorAuthorJohannesGaessler commented yesterdayJun 12, 2023Could it reach 150 tokens/s on 7B 4bit?You could possibly find that out by compiling with LLAMA_GPROF and digging through the profiling data but I'll tell you right now that the largest remaining use of CPU on my system with an RTX 3090 was the sampling of the repetition penalty which has to be done sequentially and would not be affected at all by what I have in mind. So maybe I was wrong and single core performance will still make a difference.ContributorTheBloke commented yesterdayJun 12, 2023 \u2022editedFair enough. Well it's fantastic either way.LevanKvirkvelia commented yesterdayJun 13, 2023 \u2022editedHi, great PR, does this PR allows you to keep the cache after the generation is done?I think statefulness should speedup chats and libraries like Microsoft Guidance and Jsonformercmp-nct commented yesterdayJun 13, 2023Stunning work, can't wait testing it tomorrow. Given the benchmark results this is a huge step forward..thushan commented yesterdayJun 13, 2023 \u2022editedThis is huge news, it really whips the llama.cpp's ass on CUDA \ud83e\udd73Early attempt this morning we're getting ~2.5-2.8x perf increase on 4090s and about 1.8-2x on 3090Ti.\ud83d\ude048kyushuadamu, geerlingguy, lin72h, jacobfriedman, rlapray, bermanboris, hugows, and houmii reacted with laugh emojiOwnerggerganov commented yesterdayJun 13, 2023Congrats @JohannesGaessler!I'll yet be taking a detailed look in the PR, but I can already tell you have done a really good job based on the feedback.Well deserved for all your hard work \ud83e\udd99I notice that llama.cpp is also pegged at 100% of one core, so I'm assuming a faster single-core CPU would likewise scale llama.cpp's figure.Currently the llama.cpp main thread always waits for the GPU computation to finish before arranging the next tensor. But it should be possible to change the logic to instead start preparing the next tensor immediately. Then I think CPU performance will be largely irrelevant.We will refactor and improve the CPU threading and synchronization logic of ggml soon\ud83d\udc4d1lin72h reacted with thumbs up emoji\u2764\ufe0f17thushan, pabl-o-ce, deadprogram, Rafaelblsilva, nightlyworker, junaid33, Volrath50, PiotrDabkowski, Priestru, ericvolp12, and 7 more reacted with heart emojiggerganov added the high priority Very important issuelabel yesterdayJun 13, 2023Collaboratordeadprogram commented yesterdayJun 13, 2023Fantastic work @JohannesGaessler \ud83d\ude80github-actions bot mentioned this pull request yesterdayJun 13, 2023Hacker News Daily Top 30 @2023-06-13 meixger/hackernews-daily#268OpenPriestru commented yesterdayJun 13, 2023Would it support multi GPU?\ud83d\udc4d3lin72h, avideci, and vid reacted with thumbs up emoji\ud83d\udc403lin72h, foolsh, and curtisgray reacted with eyes emojiCollaboratorAuthorJohannesGaessler commented yesterdayJun 13, 2023Hi, great PR, does this PR allows you to keep the cache after the generation is done? I think statefulness should speedup chats and libraries like Microsoft Guidance and Jsonformer@LevanKvirkvelia I am not changing anything other than the location where the KC cache is stored.We will refactor and improve the CPU threading and synchronization logic of ggml soon@ggerganov What I'm talking about is something different. Currently cudaDeviceSynchronize() is called every time after a calculation is started. But it would be more efficient to instead synchronize at a later point when the results are actually needed which would allow the parallel rather than sequential use of CPU and GPU. Unless the threading PR implements this too.Would it support multi GPU?@Priestru It already does for the computationally most expensive operations. There is no multi GPU support for the KV cache specifically; before I add that I'll need to see if that would even be worthwhile in the first place.CollaboratorAuthorJohannesGaessler commented yesterdayJun 13, 2023@TheBloke Are you setting LLAMA_CUDA_DMMV_X (default 32) and LLAMA_CUDA_DMMV_Y (default 1) at compile time? These values determine how much data the GPU processes at once for the computationally most expensive operations and setting higher values is beneficial on fast GPUs (but make sure they are powers of 2). On my RTX 3090 setting LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2 increases performance by 20%.CollaboratorKerfuffleV2 commented yesterdayJun 13, 2023Just want to make sure you're aware this currently prevents using cuBLAS with large models on low VRAM GPUs like my 6GB 1060, even with no GPU offloading. I can't use full context (-c 2048) with -ngl 0 - it still runs out of VRAM.Dampfinchen commented 20 hours agoJun 13, 2023 \u2022editedSadly with this PR, I get ggml-cuda.cu:1248: out of memory immediately after loading the 13B model.In the regular build, this was my speed (15 GPU layers for my RTX 2060, 6 threads, prompt of 1800 Tokens)Cold run:llama_print_timings: load time = 44221.63 msllama_print_timings: sample time = 144.82 ms / 180 runs ( 0.80 ms per token)llama_print_timings: prompt eval time = 59045.43 ms / 1849 tokens ( 31.93 ms per token)llama_print_timings: eval time = 77643.51 ms / 179 runs ( 433.76 ms per token)llama_print_timings: total time = 142706.14 ms(I've used 13 layers on the new master build and this PR which uses around 3,4 GB VRAM)So yeah, while this is good work and a big step forward for GGML in general, the biggest strength of GGML compared to GPTQ is that you can run large models on GPUs with insufficient VRAM at decent speed is suffering drastically with this approach.So while it is great that a 33B model can now run entirely on a RTX 4090, it's not so great for us that want to run larger models exceeding our GPU's VRAM.\ud83d\ude151ruchir29 reacted with confused emojixueyuanl mentioned this pull request 20 hours agoJun 13, 2023Daily Hacker News 13-06-2023 xueyuanl/daily-hackernews#1009OpenEwoutH commented 19 hours agoJun 13, 2023Awesome work!Here\u2019s the link to the Reddit thread on r/LocalLLaMA for reference, a lot of people share their benchmarks and experiences on this PR.43 hidden itemsLoad more\u2026Fixed incorrect index when going out of contexte8528d4This was referenced 13 hours agoJun 13, 2023Your Issue Title jacky1234/blogPages#28OpenYour Issue Title jacky1234/blogPages#29OpenYour Issue Title jacky1234/blogPages#30OpenContributorTheBloke commented 13 hours agoJun 13, 2023 \u2022editedHere's my results offloading 65B q4_K_M on an A10 + Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHzbranch: master -m guanaco-65B.ggmlv3.q4_K_M.bin -ngl 44 -t 14 -n 200llama_print_timings:    eval time = 61957.43 ms /  199 runs  ( 311.34 ms per token)llama_print_timings:    eval time = 63633.71 ms /  199 runs  ( 319.77 ms per token)llama_print_timings:    eval time = 63416.40 ms /  199 runs  ( 318.68 ms per token)branch: cuda-full-gpu-2 -m guanaco-65B.ggmlv3.q4_K_M.bin -ngl 41 -t 14 -n 200llama_print_timings:    eval time = 63243.73 ms /  199 runs  ( 317.81 ms per token)llama_print_timings:    eval time = 64970.85 ms /  199 runs  ( 326.49 ms per token)llama_print_timings:    eval time = 63186.68 ms /  199 runs  ( 317.52 ms per token)branch: cuda-full-gpu-2-no-k -m guanaco-65B.ggmlv3.q4_K_M.bin -ngl 42 -t 14 -n 200llama_print_timings:    eval time = 63346.43 ms /  199 runs  ( 318.32 ms per token)llama_print_timings:    eval time = 62468.10 ms /  199 runs  ( 313.91 ms per token)llama_print_timings:    eval time = 62256.63 ms /  199 runs  ( 312.85 ms per token)branch: cuda-full-gpu-2-no-v -m guanaco-65B.ggmlv3.q4_K_M.bin -ngl 42 -t 14 -n 200llama_print_timings:    eval time = 61643.87 ms /  199 runs  ( 309.77 ms per token)llama_print_timings:    eval time = 62827.90 ms /  199 runs  ( 315.72 ms per token)llama_print_timings:    eval time = 62729.78 ms /  199 runs  ( 315.23 ms per token)branch: cuda-full-gpu-2-no-kv -m guanaco-65B.ggmlv3.q4_K_M.bin -ngl 44 -t 14 -n 200llama_print_timings:    eval time = 62337.81 ms /  199 runs  ( 313.26 ms per token)llama_print_timings:    eval time = 62527.46 ms /  199 runs  ( 314.21 ms per token)llama_print_timings:    eval time = 62564.66 ms /  199 runs  ( 314.40 ms per token)(I preceded this by benchmarking --threads 8 through --threads 16 on master, and established that --threads 14 was quickest)\u2764\ufe0f3pabl-o-ce, Josh-XT, and lin72h reacted with heart emojiThis was referenced 13 hours agoJun 13, 2023[HackerNews Top 10]-06-14 00-02 jacky1234/blogPages#31OpenHackerNews Top 10 @23yyy-06-14 jacky1234/blogPages#32OpenHackerNews Top 10 @2023-06-14 jacky1234/blogPages#33Openslaren reviewed 12 hours agoJun 13, 2023View reviewed changesggml-cuda.cuOutdated  if (nb0 == ts && nb1 == ts*ne0/bs) {    return cudaMemcpyAsync(dst_char, x, i1_diff*nb1, cudaMemcpyHostToDevice, stream);    return cudaMemcpyAsync(dst_ptr, x, i1_diff*nb1, cudaMemcpyHostToDevice, stream);Collaboratorslaren 12 hours agoJun 13, 2023I think this path is never hit currently, but I believe the parameter should be kind rather than cudaMemcpyHostToDevice here.Collaboratorslaren 12 hours agoJun 13, 2023On minor note, I was really confused by the name of this function at first (sounds like \"tensor 2D\" rather than \"tensor to device\").CollaboratorAuthorJohannesGaessler 12 hours agoJun 13, 2023 \u2022editedI agree that the name is currently bad but I didn't yet have a good idea for another name.slaren reviewed 12 hours agoJun 13, 2023View reviewed changesggml-cuda.cuShow resolvedVRAM KV cache based on -ngl, fixed info printscc60183JohannesGaessler force-pushed the cuda-full-gpu-2 branch from 6967a44 to cc60183Compare12 hours agoJune 13, 2023 17:11gotzmann commented 10 hours agoJun 13, 2023 \u2022editedWow, so now performance is on par with exllama? Got those numbers for 30B model from there turboderp/exllama#16gpu architecture     perf power--------------------------------------------------------------H100 PCIe Hopper  34.1 t/s 300W4090 Ada Lovelace     32.5 t/s 450WA6000 Ada Ada Lovelace     31.6 t/s 300WL40 Ada Lovelace     27.7 t/s 300WA100 SXM4 Ampere  25.8 t/s 400W3090 Ampere  22.7 t/s 350WA6000 Ampere  21.4 t/s 300WA40     Ampere  21.4 t/s 300WA5000 Ampere  17.8 t/s 230WContributorTheBloke commented 10 hours agoJun 13, 2023 \u2022editedHey Johannes, new commit seems to work really well. On the A10 I can now run 65B with the same number of layers (44) as I can in master, with the same performance.Did you see my earlier messages about the assert crash I get when I try to compile with LLAMA_CUDA_DMMV_Y to > 1?CollaboratorAuthorJohannesGaessler commented 10 hours agoJun 13, 2023I did see that the parameter is causing you trouble but I don't understand why it's happening. I find it highly unlikely that the problem was only introduced with this PR though. I can't really fix the bug unless I have a way to reproduce it; if you know how to use GNU Debugger that could give me some useful information but I suspect you don't.ContributorTheBloke commented 9 hours agoJun 13, 2023You are correct on both counts! Yes, master is crashing in the same way.I've noticed that it's not crashing on the A10 system. So in total I have tested:4090 + CUDA 11.8 = crash with assertH100 + CUDA 12.0.1 = crash with assertA10 + CUDA 11.8 = no crash.The 4090 and H100 are both a later compute version than the A10; 8.9 and 9.0 respectively. Don't know if that could be a factor?I'm happy to run through gdb commands if you can describe them, but yes I have no experience of using it myself.CollaboratorAuthorJohannesGaessler commented 9 hours agoJun 13, 2023Can you tell me the line again for the assertion error using the latest commit that I pushed? It's no longer the same line due to changes in the code.ContributorTheBloke commented 9 hours agoJun 13, 2023Sure. It's:.GGML_ASSERT: ggml-cuda.cu:2221: nrows % GGML_CUDA_DMMV_Y == 0In master it's:.GGML_ASSERT: ggml-cuda.cu:1740: nrows % GGML_CUDA_DMMV_Y == 0Added a --low-vram optiondba1452lhl commented 9 hours agoJun 13, 2023 \u2022editedWow, so now performance is on par with exllama? Got those numbers for 30B model from there turboderp/exllama#16Those numbers are a bit out of date. I think that the numbers linked from the exllama README might be a better guideline on performance, but it's going to depend a lot on your individual system where the bottlenecks are.As a point of reference, I just tested the latest pulls from each codebase. My system is a Ryzen 5950X w/ DDR4-3800 RAM, (dedicated no output) RTX 4090 on Arch Linux:On llama-33b q4_0, the llama.cpp branch I get 17.62t/s on full context (run with -ngl 99 -n 2048 --ignore-eos)- the 4090 hits 290W and sustains about 50% utilization, one CPU core is pegged at 100%With exllama, on a GPTQ 4-bit 128g, on 2048t I get 40.40t/s - my 4090 hits 440W and maintains 100% GPU utilization, CPU core hits (but doesn't stay at) 100% during the run.CollaboratorAuthorJohannesGaessler commented 9 hours agoJun 13, 2023@TheBloke The commands should be:make clean && make LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2 LLAMA_DEBUG=1gdb -args <INSERT THE COMMAND THAT CAUSES THE BUG HERE>b exitrupupupp nrowsp *tensorTo get out of GDB, press CTRL+D.Fixed Windows performance51830eeContributorTheBloke commented 8 hours agoJun 13, 2023Thanks very much for the info.In case this is helpful: I learned that that the issue seems to occur only when the non-repeating layers are offloaded. Eg on 7B it happens at -ngl 33, with 13B at 41, and 33B at 61. Offloading fewer layers runs OK.Here's the gdb log:[pytorch2] ubuntu@h100:/workspace/git/llama_cuda_test/cuda-full-gpu-2 git:(cuda-full-gpu-2) $ make clean && make LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2 LLAMA_DEBUG=1 ; gdb -args ./main --threads 1 -ngl 33 -m /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin -p \"once\"I llama.cpp build info:I UNAME_S: LinuxI UNAME_P: x86_64I UNAME_M: x86_64I CFLAGS:  -I.       -O3 -std=c11  -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTSI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTSI LDFLAGS:I CC:    cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0I CXX:   g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server vdot build-info.hremoved 'common.o'removed 'ggml-cuda.o'removed 'ggml.o'removed 'k_quants.o'removed 'llama.o'removed 'main'removed 'quantize'removed 'quantize-stats'removed 'perplexity'removed 'embedding'removed 'vdot'removed 'build-info.h'I llama.cpp build info:I UNAME_S: LinuxI UNAME_P: x86_64I UNAME_M: x86_64I CFLAGS:  -I.       -O3 -std=c11  -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/includeI CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/includeI LDFLAGS:  -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libI CC:    cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0I CXX:   g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0cc -I.       -O3 -std=c11  -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -c ggml.c -o ggml.og++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.og++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.occ -I.       -O3 -std=c11  -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -c -o k_quants.o k_quants.cnvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=64 -DGGML_CUDA_DMMV_Y=2 -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.onvcc warning : incompatible redefinition for option 'optimize', the last value of this option was usedg++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib==== Run ./main -h for help. ====g++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libg++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libg++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libg++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libg++ -I. -I./examples -O3 -std=c++11 -fPIC -O0 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -g -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/libGNU gdb (Ubuntu 9.2-0ubuntu1~20.04.1) 9.2Copyright (C) 2020 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Type \"show copying\" and \"show warranty\" for details.This GDB was configured as \"x86_64-linux-gnu\".Type \"show configuration\" for configuration details.For bug reporting instructions, please see:<http://www.gnu.org/software/gdb/bugs/>.Find the GDB manual and other documentation resources online at:  <http://www.gnu.org/software/gdb/documentation/>.For help, type \"help\".Type \"apropos word\" to search for commands related to \"word\"...Reading symbols from ./main...(gdb) b exitBreakpoint 1 at 0xa7a0(gdb) rStarting program: /workspace/git/llama_cuda_test/cuda-full-gpu-2/main --threads 1 -ngl 33 -m /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin -p once[Thread debugging using libthread_db enabled]Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".main: build = 680 (cc60183)main: seed = 1686688523[New Thread 0x7ffdc6430000 (LWP 1002419)]ggml_init_cublas: found 1 CUDA devices: Device 0: NVIDIA H100 PCIe[New Thread 0x7ffdc5c2f000 (LWP 1002420)]llama.cpp: loading model from /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.binllama_model_load_internal: format   = ggjt v3 (latest)llama_model_load_internal: n_vocab  = 32001llama_model_load_internal: n_ctx   = 512llama_model_load_internal: n_embd   = 4096llama_model_load_internal: n_mult   = 256llama_model_load_internal: n_head   = 32llama_model_load_internal: n_layer  = 32llama_model_load_internal: n_rot   = 128llama_model_load_internal: ftype   = 14 (mostly Q4_K - Small)llama_model_load_internal: n_ff    = 11008llama_model_load_internal: n_parts  = 1llama_model_load_internal: model size = 7Bllama_model_load_internal: ggml ctx size =  0.07 MBllama_model_load_internal: using CUDA for GPU accelerationllama_model_load_internal: mem required = 1862.39 MB (+ 1026.00 MB per state)llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch bufferllama_model_load_internal: offloading 32 repeating layers to GPUllama_model_load_internal: offloading non-repeating layers to GPUllama_model_load_internal: offloaded 33/35 layers to GPUllama_model_load_internal: total VRAM used: 4058 MB.GGML_ASSERT: ggml-cuda.cu:2221: nrows % GGML_CUDA_DMMV_Y == 0Thread 1 \"main\" received signal SIGABRT, Aborted.__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:5050 ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.(gdb) up#1 0x00007ffff0f53859 in __GI_abort () at abort.c:7979 abort.c: No such file or directory.(gdb) up#2 0x000055555561f0f2 in ggml_cuda_transform_tensor (data=0x7ffc426aa380, tensor=0x7ffd31400220) at ggml-cuda.cu:22212221       GGML_ASSERT(nrows % GGML_CUDA_DMMV_Y == 0);(gdb) up#3 0x00005555555b22e9 in llama_model_loader::load_all_data (this=0x5555a3b9c220, progress_callback=0x5555555a529a <<lambda(float, void*)>::_FUN(float, void *)>, progress_callback_user_data=0x7fffffff9670, lmlock=0x0)  at llama.cpp:788788           ggml_cuda_transform_tensor(lt.data, lt.ggml_tensor);(gdb) p nrowsNo symbol \"nrows\" in current context.(gdb) p *tensorNo symbol \"tensor\" in current context.(gdb)PS. If debugging by proxy is too painful, you're welcome to SSH in directly if you send me an SSH public key.CollaboratorAuthorJohannesGaessler commented 7 hours agoJun 13, 2023Access via SSH would be useful. I've sent you a friend request on Discord.Cleaned up code, added commentsa47072bJohannesGaessler marked this pull request as ready for review 7 hours agoJune 13, 2023 22:01CollaboratorAuthorJohannesGaessler commented 7 hours agoJun 13, 2023 \u2022editedAlright, I think this PR is now feature complete. I've added a --low-vram option that disables the CUDA scratch buffer. Performance on Windows is absolutely terrible but with the changes that I did it's now slightly less terrible given enough VRAM and it's at least not more terrible than it was before I added the VRAM scratch buffer.\ud83d\udc4d5KerfuffleV2, Josh-XT, thushan, shouyiwang, and lin72h reacted with thumbs up emojiCollaboratorKerfuffleV2 commented 4 hours agoJun 14, 2023The current version seems fine on my old GTX 1060 with 6GB VRAM. It doesn't seem to change the performance in a noticeable way (perplexity might be a little faster). It's also now possible to use the full 2048 context with or without --low-vramheadllines bot mentioned this pull request 4 hours agoJun 14, 2023Hacker News Daily Top 10 @2023-06-14 headllines/hackernews-daily#1064Opengithub-actions bot mentioned this pull request 4 hours agoJun 14, 20232023-06-13 Hot Posts jiacai2050/hot-posts#313OpenSign up for free to join this conversation on GitHub. Already have an account? Sign in to commentReviewersslarenslaren left review commentsAt least 1 approving review is required to merge this pull request.AssigneesNo one assignedLabelshardwareHardware relatedhigh priorityVery important issueperformanceSpeed related topicsProjectsNone yetMilestoneNo milestoneDevelopmentSuccessfully merging this pull request may close these issues.None yet17 participantsFooter\u00a9 2023 GitHub, Inc.Footer navigationTermsPrivacySecurityStatusDocsContact GitHubPricingAPITrainingBlogAbout",
    "summary": "- This PR adds GPU acceleration for ggml tensors, improving performance for long generations and prompt processing.\n- Performance numbers show significant speedup on RTX 3090 for prompt processing and token generation.\n- The PR includes the addition of CUDA kernels and plans for fixing memory leaks, improving performance for lower-end GPUs, and general code cleanup.",
    "hn_title": "Llama.cpp: Full CUDA GPU Acceleration",
    "original_title": "Llama.cpp: Full CUDA GPU Acceleration",
    "score": 680,
    "hn_content": "- Llama.cpp is a non-Python machine learning software that supports various computation devices.\n- Users find it interesting because it offers an alternative to the Python ML ecosystem and has gained popularity.\n- Python's dependency management is considered to be a hassle and can be time-consuming, unlike the simplicity of running C/C++ programs.\n- The convenience of quickly running C/C++ programs without the need for complex dependency management is appealing to users.\n- Llama.cpp's focus on resource efficiency and optimization for CPU makes it attractive for those working with large models.\n- The ease of installation and usage of Llama.cpp compared to other ML libraries is a significant advantage.\n- Python's dependency management, specifically for ML projects, is seen as a pain point, leading some users to explore alternative solutions.\n- The popularity of Llama.cpp highlights the demand for solutions that offer improved performance and easier deployment.\n- Llama.cpp has gained attention for its ability to run large models efficiently and effectively, contributing to the excitement in the tech-savvy community.- MacWhisper app and Whisper Memos for iOS are recommended for GUI purposes\n- Whisper Memos allows offline and on-device usage, while Aiko provides on-device transcription for macOS and iOS\n- CUDA is the best-supported solution for GPGPU work, offering better performance and a great profiler\n- OpenCL was an alternative to CUDA, but support from AMD and Apple dropped in favor of Metal and ROCm\n- DirectML and various compute shaders are also available but are more focused on gaming\n- AMD dropped support for GPGPU on desktop cards, focusing on gaming for Radeon and compute in MI/CDNA\n- ROCm, AMD's GPGPU platform, has limited hardware support and is Linux only\n- AMD's support for GPU compute has decreased over the years, with the current list of supported GPUs shrinking\n- It can be challenging to implement a good alternative API for matrix multiplication due to the complexity of GPGPU operations\n- The success of CUDA is partially due to its optimized hardware and software integration, as well as support from frameworks like Tensorflow and Pytorch\n- SYCL is a potential cross-platform alternative but has limited adoption\n- Llama.cpp, a popular tool, allows the running of LLM models on various hardware, including low-end GPUs and CPUs\n- Llama.cpp supports hybrid CPU+GPU usage for inference with larger models\n- LLM models can be obtained from huggingface and converted for use with llama.cpp\n- Apple's approach with shared memory architecture and massive RAM/VRAM support is interesting for running large models\n- OpenAI's LLM models have faced criticisms for their high fees and censorship of certain outputs\n- The lack of competition in the GPU market is seen as a pity and a shame, potentially limiting innovation and alternative options\n- AMD is working on competing with Nvidia, and upcoming releases like AMD MI300 are anticipated as game-changers\n- Emulating or translating CUDA to target non-NVIDIA hardware would be a legal challenge\n- Hardware vendors have struggled to create a unified open API for machine learning acceleration, similar to the challenge of creating a unified raster graphics API\n- tinygrad is a project addressing the dominance of CUDA with an alternative solution",
    "hn_summary": "- Llama.cpp is a non-Python machine learning software that offers an alternative to the Python ML ecosystem.\n- Users find it appealing due to the simplicity of running C/C++ programs without complex dependency management.\n- Llama.cpp is popular for its resource efficiency, ease of installation, and usage compared to other ML libraries."
  },
  {
    "id": 36312200,
    "timestamp": 1686672099,
    "title": "Comic Mono",
    "url": "https://dtinth.github.io/comic-mono-font/",
    "hn_url": "http://news.ycombinator.com/item?id=36312200",
    "content": "Comic MonoA legible monospace font\u2026 the very typeface you\u2019ve been trained to recognize since childhood. This font is a fork of Shannon Miwa\u2019s Comic Shanns (version 1).DownloadComicMono.ttfComicMono-Bold.ttfDifferences from Comic ShannsAll glyphs have been adjusted to have exactly the same width (using code based on monospacifier).The glyph metrics have been adjusted to make it display better alongside system font, based on Cousine\u2019s metrics.The name is changed to Comic Mono.A bold version of the font is generated using FontForge\u2019s Embolden operation.I have no font creation skills; I\u2019m just a software developer. This font family is created by patching the original font, Comic Shanns (v1), using a Python script, generate.py.What does it look like?#!/usr/bin/env python2# -*- coding: utf-8 -*-\"\"\"Generates the Comic Mono font files based on Comic Shanns font.Required files:- vendor/comic-shanns.otf- vendor/Cousine-Regular.ttfBased on:- monospacifier: https://github.com/cpitclaudel/monospacifier/blob/master/monospacifier.py- YosemiteAndElCapitanSystemFontPatcher: https://github.com/dtinth/YosemiteAndElCapitanSystemFontPatcher/blob/master/bin/patch\"\"\"import osimport reimport sysreload(sys)sys.setdefaultencoding('UTF8')import fontforgeimport psMatimport unicodedatadef height(font):  return float(font.capHeight)def adjust_height(source, template, scale):  source.selection.all()  source.transform(psMat.scale(height(template) / height(source)))  for attr in ['ascent', 'descent',        'hhea_ascent', 'hhea_ascent_add',        'hhea_linegap',        'hhea_descent', 'hhea_descent_add',        'os2_winascent', 'os2_winascent_add',        'os2_windescent', 'os2_windescent_add',        'os2_typoascent', 'os2_typoascent_add',        'os2_typodescent', 'os2_typodescent_add',        ]:    setattr(source, attr, getattr(template, attr))  source.transform(psMat.scale(scale))font = fontforge.open('vendor/comic-shanns.otf')ref = fontforge.open('vendor/Cousine-Regular.ttf')for g in font.glyphs():  uni = g.unicode  category = unicodedata.category(unichr(uni)) if 0 <= uni <= sys.maxunicode else None  if g.width > 0 and category not in ['Mn', 'Mc', 'Me']:    target_width = 510    if g.width != target_width:      delta = target_width - g.width      g.left_side_bearing += delta / 2      g.right_side_bearing += delta - g.left_side_bearing      g.width = target_widthfont.familyname = 'Comic Mono'font.version = '0.1.1'font.comment = 'https://github.com/dtinth/comic-mono-font'font.copyright = 'https://github.com/dtinth/comic-mono-font/blob/master/LICENSE'adjust_height(font, ref, 0.875)font.sfnt_names = [] # Get rid of 'Prefered Name' etc.font.fontname = 'ComicMono'font.fullname = 'Comic Mono'font.generate('ComicMono.ttf')font.selection.all()font.fontname = 'ComicMono-Bold'font.fullname = 'Comic Mono Bold'font.weight = 'Bold'font.changeWeight(32, \"LCG\", 0, 0, \"squish\")font.generate('ComicMono-Bold.ttf')CDNYou can use this font in your web pages by including the stylesheet. CDN is provided by jsDelivr.<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/comic-mono@0.0.1/index.css\">npm PackageThe contents of this package is also published to npm, although the font files are not optimized. See fontsource package (below) for a better option.Packages published by third partiesFontsource: @fontsource/comic-mono (thanks @DecliningLotus)Arch Linux AUR: ttf-comic-mono-git (maintained by DBourgeoisat)LicenseIt is licensed under the MIT License.This site is open source. Improve this page.",
    "summary": "- 'Comic Mono' is a legible monospace font that is a fork of Shannon Miwa's 'Comic Shanns' font.\n- It has been adjusted to have exactly the same width for all glyphs and the metrics have been adjusted to display better alongside system fonts.\n- A bold version of the font has been created using FontForge's Embolden operation.",
    "hn_title": "Comic Mono",
    "original_title": "Comic Mono",
    "score": 647,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginComic Mono (dtinth.github.io)647 points by lolinder 13 hours ago | hide | past | favorite | 123 commentscrazygringo 12 hours ago | next [\u2013]I was curious why it seems so much nicer than Comic Sans (and many commenters here seem to agree).Many of the letterforms have been dramatically \"straightened\" -- a Comic Sans \"m\" is kind of hideous, with the three vertical strokes intentionally all at different angles... whereas in this one they're all essentially vertical.Comic Sans is ugly because it's terribly proportioned (intentionally) to mimic the way a child would write. But when you straighten everything and the letters necessarily fit into a grid... it's just much, much nicer aesthetically.Just a gentle \"handwriting mono\" font, rather than the original which is (again, intentionally) horribly kerned and where half the letterforms feel like they're about to topple over from imbalance.replypolpo 11 hours ago | parent | next [\u2013]Reminds me of Apple\u2019s Chalkboard font, which basically fixes everything that\u2019s off putting about Comic Sans while still being casual and child-like: http://www.identifont.com/differences?first=Chalkboard&secon...replycubefox 5 hours ago | root | parent | next [\u2013]Comic Sans was made to look as if it was handwritten. The Chalkboard font looks too artificial for that. E.g. k,l,m,n have too straight lines.replybehnamoh 9 hours ago | root | parent | prev | next [\u2013]It's beautiful!replymarkandrewj 11 hours ago | parent | prev | next [\u2013]It seems like this isn't very well known, but the design of Comic Sans is actually intended to help children with dyslexia. The creator of Comic Sans also talks about this in his lectures. Listening to him talk about the design of the typeface changed my perception of it.Ref: Vincent Connare: Comic Sans is the Best Font in the World https://youtu.be/xXdzBTeYZlERef: https://www.nothingcomicaboutdyslexia.com/replytapland 11 hours ago | root | parent | next [\u2013]Was it intended to?I know it's useful for dyslexics but I've not seen anyone claim it was intentionally designed with that intent and I'm pretty sure that has been explained to be a happy coincidence in the past.Ref2 doesn't state that, and I can't do video right now.Edit: And neither does Ref1replyPaul-Craft 10 hours ago | root | parent | next [\u2013]The story I've always heard (which is backed up by Wikipedia) is that Comic Sans was designed for the speech bubbles of characters in Microsoft Bob, to make them seem less \"formal\" and more relatable. The fact that the letterforms are \"terribly proportioned,\" as u/crazygringo says, is what actually makes the letters easier for dyslexic people to distinguish.https://en.wikipedia.org/wiki/Comic_Sans#Historyreplymarkandrewj 11 hours ago | root | parent | prev | next [\u2013]Thank you for the correction. I thought I had read it was part of the initial design consideration at one point, but reading about it further again, I think the discovery that it helped dyslexic readers happened after the typeface was made public.It is still an interesting aspect of the typeface that doesn't get much attention though, although there are other typefaces that also try to help with dyslexia.If you are interested in design, I still recommend watching the talk when you have the time. It has been a while since I watched it, but the designer is well spoken.replyPhrodo_00 10 hours ago | root | parent | next [\u2013]There's a font called OpenDyslexic[1] that was actually designed to do that[1] https://opendyslexic.org/replymikae1 18 minutes ago | root | parent | next [\u2013]Also, there's this open source font: https://www.lexend.com/replymichelb 9 hours ago | root | parent | prev | next [\u2013]Some info about these fonts here https://www.edutopia.org/article/do-dyslexia-fonts-actually-...Fwiw my dyslexic friends feel no benefit from these fonts. Would love to hear other experiences.replymarkandrewj 8 hours ago | root | parent | next [\u2013]It is a good question, I have read mixed reports about the effectiveness as well. Besides specific typefaces, there are also techniques like bionic reading.https://www.indwes.edu/adult-graduate/ng-blog/stories/what-i...https://bionic-reading.com/Sorry again, I thought I remembered the designer talking about this in the video I referenced. I should have watched it again before referencing it.replyTheFlyingFish 4 hours ago | root | parent | prev | next [\u2013]Interesting, my dyslexic sister absolutely swears by Dyslexie (a similar font) and has set her Kindle to use it all the time.I wonder if there are different sub-variants of dyslexia that work on different principles?replyKuraj 6 hours ago | parent | prev | next [\u2013]You're gonna love Comic Neue: https://comicneue.com/replyarchermarks 2 hours ago | root | parent | next [\u2013]I love this? I'm not sure if it's ironic, unironic or what but I'm digging it.replywodenokoto 12 hours ago | parent | prev | next [\u2013]It is not modeled on children\u2019s handwriting but, as the name implies, letters found in speech bubbles in comic books.replycrazygringo 10 hours ago | root | parent | next [\u2013]If you look at the font, it's quite clearly extremely \"irregular\" in a way that children's handwriting is, and comic book speech bubbles very much are not meant to be.Indeed, the creator used some comics as a starting point, but then tried to redraw the strokes with a mouse (!), and intentionally drew them wrong, not keeping them straight, etc.So the goal was not to reproduce comic speech bubbles at all, but to create something instead \"strange and childlike\". You would never want to use it for actual comic book lettering.From an interview [1]:> They wanted all kinds of fonts \u2013 a lot of them strange and childlike. One program was called Microsoft Bob, which was designed to make computers more accessible to children. I booted it up and out walked this cartoon dog, talking with a speech bubble in Times New Roman. Dogs don\u2019t talk in Times New Roman! Conceptually, it made no sense.> So I had an idea to make a comic-style text and started looking at Watchmen and Dark Knight Returns, graphic novels where the hand lettering was like a typeface... Instead, I looked at various letters and tried to mimic them on screen. There were no sketches or studies \u2013 it was just me drawing with a mouse, deleting whatever was wrong.> I didn\u2019t have to make straight lines, I didn\u2019t have to make things look right, and that\u2019s what I found fun. I was breaking the typography rules. My boss Robert Norton, whose mother Mary Norton wrote The Borrowers, said the \u201cp\u201d and \u201cq\u201d should mirror each other perfectly. I said: \u201cNo, it\u2019s supposed to be wrong!\u201d[1] https://www.theguardian.com/artanddesign/2017/mar/28/how-we-...replyCharlesW 11 hours ago | root | parent | prev | next [\u2013]Also, \"real\" comic book fonts do exist: https://www.comicbookfonts.com/replypessimizer 6 hours ago | root | parent | prev | next [\u2013]Except speech bubbles in comic books look a lot better and more regular.replyanikom15 12 hours ago | parent | prev | next [\u2013]Comic Sans is actually quite legible for its class and well-hinted. It works better with antialiasing off. The main criticism is in its overuse.replybityard 12 hours ago | parent | prev | next [\u2013]> to mimic the way a child would writeNo, it was intended to mimic the style of comic book lettering, hence the name.Part of the reason Comic Sans is reviled is because not only is it a terrible font, but it is also a far cry from actual comic book lettering.replycrazygringo 10 hours ago | root | parent | next [\u2013]No it wasn't, see my cousin comment.replynemetroid 12 hours ago | parent | prev | next [\u2013]I think Comic Code stays more true to the aesthetics of Comic Sans.https://www.myfonts.com/collections/comic-code-font-tabular-...replylucideer 12 hours ago | parent | prev | next [\u2013]Another notable deviation that may affect the aesthetic is that it's been slightly seriffed (fIijl) for some reason.replybrazzledazzle 11 hours ago | parent | prev | next [\u2013]I absolutely (irrationally, I admit) despise comic sans from a purely personal aesthetic perspective but this does bug me less which is saying a lot.replynerdponx 9 hours ago | parent | prev | next [\u2013]I thought Comic Sans was meant to mimic the text in a comic strip, not a child's handwriting.replyqbasic_forever 12 hours ago | prev | next [\u2013]Fantasque Sans Mono is similar in spirit and works great for coding too IMHO: https://github.com/belluzj/fantasque-sans/ Don't knock it until you try it, it really looks nice for clean languages without a lot of symbols, operators or noise.replynomel 9 hours ago | parent | next [\u2013]For me, it doesn't seem to matter which font I use, as long as I keep using it for an extended period of time, then it become the \"easy to read\" font. Some with line spacing, indentation, etc. Then, anything else looks terrible.I wonder how much of font design is driven by each authors \"local indentation in the font space\". What's the push to move that indentation?replymoonchrome 8 hours ago | root | parent | next [\u2013]I have the opposite reaction - I switch fonts every now and then because using a different font makes it more fun/feels fresh. Likewise for color schemes.replyotterpro 12 hours ago | parent | prev | next [\u2013]Fantasque Sans Mono is my favorite as well, and is the only font I use for coding (in Vim, VScode, Ryder, etc...). The slight hint of style makes it so easy to read, and not boring (unlike other monospace fonts). It's free and good alternative to Comic Code, which is not free. I don't like the loop on k, so I use the version with non-looped k. Before it, I was on Nanum Gothic, which had the ideal spacing/size for editor.replynerdponx 9 hours ago | parent | prev | next [\u2013]I used this one for a long time, then switched to the Recursive variable font. It also comes with several non-variable alternatives (for editors that don't support variable which is most of them) called \"Rec Mono\". I particularly enjoyed Rec Mono Duotone.replySemaphor 12 hours ago | parent | prev | next [\u2013]That actually looks more normal than Cascadia Code [0] to me. I\u2019d say CC sits between Fantasque and Comic Mono in playfulness[0]: https://github.com/microsoft/cascadia-codereplyksaj 12 hours ago | parent | prev | next [\u2013]It looks cool, but one problem at least on my laptop screen when looking at the small rendering, is that the l and i in the word multi look too similar. I have to lean in to differentiate them. It looks like multl to me.For the word split, it is easier to tell since they are side-b by-each, but still weird to look at without zooming in.It's easy enough when either rendered as a larger font size, or if you lean in. But when small, those two letters are difficult.replyspookie 11 hours ago | parent | prev | next [\u2013]I've been using it ever since I dropped mononoki. So, about 3 years ago. Looks great, and quite readable for me. More so than OP's.I think that it's due to the really tall x-height, and the fact that letters sray 'in-line'. OP's 's' in particular goes terribly below baseline. Not sure if using the proper term here.replylofatdairy 11 hours ago | parent | prev | next [\u2013]Man I unironically love Fantasque Sans Mono (using it for nearly 5 years now) and I'm so glad someone else also brought this one up. Idk, I feel like Comic Sans hate was always a bit much, even ironic hate, and I'm just glad to see another typeface project inspired by it.replyargulane 12 hours ago | parent | prev | next [\u2013]I also quite like Fantasque Sans Mono. Few years ago they even added programming ligatures support. Some like them but I don't, so I keep using the previous 1.7.2 version of the font that does not contain ligatures (not all programs allow disabling ligatures).replykps 10 hours ago | root | parent | next [\u2013]If you're up for building it yourself, delete `update_features(fnt)` from `Scripts/fontbuilder.py`: https://github.com/belluzj/fantasque-sans/blob/996150820b98a...replywhiskeytuesday 6 hours ago | parent | prev | next [\u2013]I installed it months ago as a joke and have been using it ever since. Curly K master race.replyComputerGuru 11 hours ago | parent | prev | next [\u2013]The lowercase symbols are nice but ye god the uppercase Y and E are basically unchanged from Comic Sans MS and give me PTSD.replywrp 4 hours ago | prev | next [\u2013]Several years ago, I experimented with making coding fonts in different styles. One of my creations was Pointfree (https://www.dafont.com/pointfree.font). A few of the others I made are in the Google Code Archive (https://code.google.com/archive/p/i3project/downloads). Since they were just experiments, they only cover the ASCII set. They have no hinting, so look terrible on screen unless you have strong antialiasing enabled.I made some notes on how a font needs to be modified for printing code. To start with, a coding font needs heavier weight than a typical text face, and should be designed for greater word and line spacing.If adapting a proportional font, it needs to have: - minimum space width the same as n - O0, Il1 distinguished - @#$%& roughly equal size and color - punctuation bolder and with more space - ~\\*+=-<> vertically alignedHere are recommendations for specific characters: ABCDEF G -- pronounced lip H I -- must distinguish from l and 1 J -- not below line KLMNOP Q -- slash, not tail RSTUVWXYZ a -- two storey bcdef g -- tail, not double h i -- needs foot jk l -- needs foot mnopqrstuvwxyz 1 -- needs foot 23456789 0 -- needs slash ! -- pronounced taper @ # -- forward vertical slashes $%^& * -- centered, six point, point on top - -- longer _ -- not far below line, not touching =+ ( ), [ ], / \\, { }, < > -- not touching each other, aligned vertically ;:'\",.` -- bigger ? | -- distinguish, maybe gap in middle ~ -- centeredreplytimetraveller26 2 hours ago | parent | next [\u2013]Pointfree was my programming font for many years, thank you so much!Nowadays I use a more classical monospace font, but I really liked to program in \"Comic Sans\" so now I use a font inspired by it for italics in my terminal.replycatears 12 hours ago | prev | next [\u2013]I've been using this as my standard font for maybe 1-2 years now (no, I am not joking). While I don't think that the font is any more legible than other fonts, the quirkiness and the character of the font makes it rather enjoyable to look at.If legibility is an issue then I would seriously recommend increasing the font size, I think that will do much more than choice of \"most optimal\" font. And if increased font size makes your code \"harder to read\", consider that someone else might be unable to use a smaller font and will be forced to read code with a larger font size.replyfilm42 8 hours ago | parent | next [\u2013]Daily driver for me as well, but only for terminal. People laugh sometimes while pair programming, but usually by the end they begin to really like it. Can't use anything else at this point.replyrafadc 12 hours ago | prev | next [\u2013]I've been using Comic Code for a while for coding and it is the best thing it ever happened to me professionally speaking.It teaches me not to take myself too seriouslyreplydavikr 11 hours ago | parent | next [\u2013]Me too! Programming has never been as fun as it is nowadays.replygeon 10 hours ago | parent | prev | next [\u2013]Exactly. Feels like a whiteboard.I\u2019ve been using the normal Comic Sans for 3-4 years now. I don\u2019t mind the non-monospaced-ness.replyveeberz 12 hours ago | parent | prev | next [\u2013]I love Comic Code! First font I actually paid for.replycloin 12 hours ago | prev | next [\u2013]Am I supposed to hate this? Because I really really don't.replythrow2022110401 12 hours ago | parent | next [\u2013]It's cool to not hate Comic Sans and it's brethren right now but as an old school hater I know that our time will come again.https://achewood.com/2007/07/05/title.htmlreplynagonago 30 minutes ago | parent | prev | next [\u2013]I'm the opposite. I want to like it, but it hurts my eyes to read.replymedstrom 12 hours ago | parent | prev | next [\u2013]I like it too. Somehow more suitable for coding than Comic Sans ever was for text.replycoffeebeqn 12 hours ago | root | parent | next [\u2013]It\u2019s just a much more polished font in general. Looks really nice!replymoffkalast 12 hours ago | parent | prev | next [\u2013]I have no strong feelings one way or the other.replydarknavi 12 hours ago | root | parent | next [\u2013]All I know is my gut says \"Maybe\".replymushufasa 12 hours ago | prev | next [\u2013]I also really like Monaco! Classic in the same vein at least IMO https://en.wikipedia.org/wiki/Monaco_(typeface)replyfoxandmouse 12 hours ago | parent | next [\u2013]If you like that, a variant that I enjoy is Monolisa: https://www.monolisa.devI think it sits between the submitted font and Monaco in terms of \"seriousness\".replyriffraff 11 hours ago | parent | prev | next [\u2013]I love monaco and used it for years as my code font. I wish we had Chalkboard[1] mono tho![1] https://en.wikipedia.org/wiki/Chalkboard_(typeface)replyalanh 7 hours ago | parent | prev | next [\u2013]Sorry, but in what sense are they in the same vein?replysedatk 2 hours ago | root | parent | next [\u2013]I believe in the sense that it also has handwriting characteristics in its style.replydisruptiveink 9 hours ago | parent | prev | next [\u2013]My terminal always has to be Monaco 10. Without antialias on non-HiDPI displays.replyjnwatson 12 hours ago | parent | prev | next [\u2013]I first coded using Monaco some 38 years ago. Time flies.replyBugsJustFindMe 12 hours ago | prev | next [\u2013]The i, l, and f serifs feel extremely non-comic to me. It's not right if it's not sans. IMO https://www.dafont.com/pointfree.font is a better monospace comic font.replystOneskull 7 hours ago | parent | next [\u2013]i like pointfree very much. thank you.replydang 11 hours ago | prev | next [\u2013]Related:Comic Mono \u2013 a legible monospace font - https://news.ycombinator.com/item?id=27488524 - June 2021 (51 comments)Comic Mono - https://news.ycombinator.com/item?id=25520510 - Dec 2020 (216 comments)replyslmjkdbtl 12 hours ago | prev | next [\u2013]I use APL386 as my monospace font, has some comic vibe but looks better imo.https://abrudz.github.io/APL386/replywhalesalad 12 hours ago | prev | next [\u2013]not gonna lie this is awesome and i will take it for a spin today before passing judgement.screenshot: https://i.imgur.com/jVeZKgh.pngtheme: https://marketplace.visualstudio.com/items?itemName=Whalesal...replyagilob 12 hours ago | prev | next [\u2013]All uppercase texts looks bad in a way I don't know how to explain. There isn't enough space between letters and look like uppercase letter are flowing above lowercase. Lowercase text I can just scan and without reading and understand what's happening, but uppercase letters in word \"PERFORMANCE ENVIRONMENT\" look like they're two long letters I need to visually investigate text.https://imgur.com/a/1IM2VqnSee the spacing between ME and ams. Lowercase letter look more goofy, asymetrical.replythrowaway106382 11 hours ago | prev | next [\u2013]I think this is a suprisingly pleasant font, but PragmataPro is probably one of the best purchases I've ever made.replyregulation_d 9 hours ago | parent | next [\u2013]Same. At first, I thought PragmataPro was a little narrow, but now that I grown accustomed to it, everything else feels squatty or improperly kerned.replysedatk 3 hours ago | prev | next [\u2013]The \"Issues\" tab is disabled, and there seems to be no support for Turkish characters or ligatures. Good luck, I guess.replysamwillis 12 hours ago | prev | next [\u2013]I'm so confused, I really REALY want to hate it, but think I love it...I'm going to have to set it as my editor font and see how it goes.replybaruchel 12 hours ago | prev | next [\u2013]I have used Apl385 during a long time as my daily monospace font. It shares some features with such font (probably why I don't use it any longer!) https://vfoley.xyz/lesser-known-coding-fonts/replySemaphor 12 hours ago | prev | next [\u2013]217 comments end of 2020: https://news.ycombinator.com/item?id=25520510And I just installed selected this in Rider, I\u2019ll see how I feel about it tomorrow while working. It doesn\u2019t look bad, but I think I prefer Cascadia Code which also has a bit of playfulness.replymhandley 12 hours ago | prev | next [\u2013]Actually looks quite nice, but when I try it for coding I find it a bit too dense, compared to Hack [0] which gets it just about right for me.[0] https://sourcefoundry.org/hack/replyrodolphoarruda 12 hours ago | prev | next [\u2013]Once I turned 45 y.o. I began to have trouble reading anything non serif and enjoy serif fonts. My eBook reader is set up with Courier and it's a real joy for me.I do like this font Comic mono, though. A nice attempt to make a mono type more organic and less machine made.replypetschge 12 hours ago | prev | next [\u2013]Is there a nice screenshot that compares commonly confused glyphs like 1, I, l or O, 0, \u00d8?replynarag 5 hours ago | parent | next [\u2013]I have set the font for this page and there's no confusion. 1 has slope, I double serif, l the top serif only to the left, O clean, 0 with bar and the bar in \u00d8 is wider than the circle.So it's nice, but Fantasque Mono is still more legible for my eyes in general.replyjedberg 11 hours ago | prev | next [\u2013]I've kept my font in my messaging apps Comic Sans for years. It turns out it's super readable when you're trying to read quickly, due to all the letters looking so different.This font is nice, but it sort of \"squares up\" the letters to make them more uniform, which arguably makes it less readable compared to Comic Sans.It should be noted that Comic Sans is also the preferred font for many dyslexics because of how readable it is.People hate it because it's overused, but it is actually a very useful font.replyekam 12 hours ago | prev | next [\u2013]If you like spins on Comic Sans, you may also enjoy:https://comicneue.com/replympsprd 6 hours ago | prev | next [\u2013]I used to hate Comic Sans until a primary school teacher explained to me it was used because the \"a\" is exactly the way they teach how to write it.replyits-summertime 12 hours ago | prev | next [\u2013]Mainly more referring to https://github.com/shannpersand/comic-shanns, but the capital forms seem to very quickly leave a blocky feel, e.g. IT visually combining, NE visually combining. I do wonder if it would be possible to systematically adjust the kerning, la using openCV or similar to detect such things.replyadt2bt 12 hours ago | prev | next [\u2013]I feel like generative AI interfaces should use a font like this for code. It can give a subtle 'hmm maybe I should double check this' vibe.replyarek_nawo 12 hours ago | prev | next [\u2013]It looks nice. In general I have nothing against \"Comic-family\" fonts. They look fun and unique. This certainly changes depending on how much you use them (e.g. an entire page in Comic Sans is... just comical.Applying the same logic to this font, I feel like it would look good in stylized code snippets on a website, but I wouldn't use it in my code editor.replycbsmith 8 hours ago | prev | next [\u2013]\"the very typeface you've been trained to recognize since childhood\"Man, I feel old.replyzzzbra 3 hours ago | prev | next [\u2013]dropped into this in VSCode this morning and just realized I haven't thought about how I'm using it for at least six hours. appears to be good, actually?replyadastra22 12 hours ago | prev | next [\u2013]How does this differ from Comic Code, which I use as my main code editor font?replyfalcor84 12 hours ago | parent | next [\u2013]You mean [0]? If so, the big difference I see is that this one is free.[0] https://tosche.net/fonts/comic-codereplyadastra22 12 hours ago | root | parent | next [\u2013]That\u2019s the one, and fair enough.replylucideer 12 hours ago | parent | prev | next [\u2013]Apart from being free, the differences seem to be:- Comic Code is more heavily serifed - Comic Mono is \"semi-serif\" (a balanced blend between serif mono fonts & the sans-serif Comic Sans). Not sure why they both didn't go full sans-serif: I generally prefer serif code fonts but of these two I prefer the less seriffed variation, and Comic Sans literally has Sans in the name.- Comic Code is more legible (for me) - the widths & kerning seem to leave clearer letter separation with equivalent character spacing.- Comic Mono has nicer kerning (to my eye) - while legibility is a problem at small font-sizes, at larger font-sizes Comic Mono is more pleasing to read.replyWorldMaker 9 hours ago | parent | prev | next [\u2013]Just based on their READMEs it looks like:- Comic Mono was built by a Python script by a non-designer as open source-ish- Comic Code was built by a font designer with commercial intent- Comic Code includes more font variants (italics, ligatures)replypaxys 12 hours ago | parent | prev | next [\u2013]Two tools that do the same thing are allowed to exist simultaneously.replymedstrom 12 hours ago | root | parent | next [\u2013]Sure, but she just asked how it differs. I didn't read the question as value-loaded.replystronglikedan 12 hours ago | parent | prev | next [\u2013]Maybe my eyes are just too old, but the kerning was killing them.replyseydor 12 hours ago | prev | next [\u2013]I like thisI always liked how legible comic sans is. I propose we try it on HN for a weekreplydavidy123 9 hours ago | parent | next [\u2013]And you think the reddit API protests are fierce.replyshrikant 11 hours ago | prev | next [\u2013]I really like, but now I'm wondering how much of this is because of the beautiful colour palette on the blog. Does anyone know what colour scheme is being used?replyskipkey 3 hours ago | prev | next [\u2013]One of these days I am going to make a shirt with the text \u201cFont Nerd\u201d in comic sans, just to see how many heads explode\u2026replySCdF 12 hours ago | prev | next [\u2013]It's beautiful, like I really like it. Damn.replyhk1337 12 hours ago | prev | next [\u2013]No ligatures yet?EDIT:Trying it out now, it's actually a lot nicer than I initially thought. I had to bump the text size up which was already larger than 14.replyxyst 11 hours ago | prev | next [\u2013]Thanks. Going to use this in my YC deckreplyComputerGuru 11 hours ago | prev | next [\u2013]Crazy how much bigger a lowercase l is than a lowercase m in order to fit into a single cell.replyroughly 12 hours ago | prev | next [\u2013]I wonder if I can enable this on a per-project basis.Or per function, depending on test coverage & code quality\u2026replybee_rider 12 hours ago | parent | next [\u2013]I\u2019ve never seen a silly idea that I wanted more.Based on a skim of the write-up, it seems like the author passed a respin of comic sans through a couple scripts, maybe you could start from where they started and try to locate a nice continuous \u201cshittyness\u201d knob to tune.replyduskwuff 11 hours ago | root | parent | next [\u2013]A variable font, but with a \"comedy\" axis that runs from DejaVu Sans to Comic Sans.replythrowkeys 10 hours ago | prev | next [\u2013]No matter what the size is or the editor (Notepad/Sublime) the top of characters - [], {} and () seems to be cut off on Windows 11. Is it same for others?replydsr_ 12 hours ago | prev | next [\u2013]Other than some of the funky capital letters, it's really not bad.replysweeter 10 hours ago | prev | next [\u2013]If you like this font, you might also like Maple Mono NF. Its on github as well, I find it really pleasing to look at and I use it in all of my editors.replylacy_tinpot 12 hours ago | prev | next [\u2013]honestly looks super comfyreplyJohnDeHope 11 hours ago | prev | next [\u2013]I legit use this font for work. It's a nice mono font.replytakeda 10 hours ago | prev | next [\u2013]Feels like Comic Sans is the Javascript of fonts.replyawelxtr 11 hours ago | prev | next [\u2013]Comic Mono crashes my Eclipse on Linux Mint Cinnamonreplypetesergeant 11 hours ago | prev | next [\u2013]I\u2019ve been using the very similar Comic Code for several years, and it consistently sparks joy when I open my editor. Highly recommended. This looks marginally nicer and I\u2019ll give it a goreply29athrowaway 12 hours ago | prev | next [\u2013]I used to make fun of these fonts until I learned they are more readable for people with dyslexia.replyNelsonMinar 12 hours ago | parent | next [\u2013]Sadly this might not be true; most careful research on fonts have shown that font choices does not help people with dyslexia, even including fonts specifically designed for the purpose.https://www.boia.org/blog/do-dyslexia-fonts-improve-accessib...https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5629233/replyroberthahn 12 hours ago | parent | prev | next [\u2013]I found it more readable for coding even though I don\u2019t have dyslexia. Surprised me when I found that out.replyconstantcrying 12 hours ago | parent | prev | next [\u2013]If you have dyslexia use a dyslexia font.Comic Sans is not a bad font, but people make fun of it because it is used where it shouldn't be. (I blame WordArt and its attitudes for most typography atrocities)replyroywiggins 12 hours ago | root | parent | next [\u2013]The evidence for dyslexia fonts is kinda bad.> Use of the OpenDyslexic font did not enhance text readability or reading speed. The study participants strongly preferred Verdana or Helvetica over the OpenDyslexic alternative.https://blog.dyslexia.com/good-fonts-for-dyslexia-an-experim...replyconstantcrying 12 hours ago | root | parent | next [\u2013]Interesting. Is there evidence that Comic Sans works better then dedicated dyslexia fonts?replyjbverschoor 12 hours ago | prev | next [\u2013]I actually like it a lotreplyraydiatian 9 hours ago | prev | next [\u2013]This belongs in a museum dedicated to crazy peoplereplyglobular-toast 10 hours ago | prev | next [\u2013]Why is this forked from the original? The changes seem like things that could have been submitted as pull requests. If the original author rejected them then, sure, make a fork. But at least submit it first.replyking_magic 10 hours ago | prev | next [\u2013]I don't hate it. It's actually really easy on the eyes.replyshrimp_emoji 12 hours ago | prev [\u2013]This is the title bar font I set for my windows on Linux.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Comic Mono is a font that is being discussed on Hacker News.\n- Many commenters agree that it is an improvement over Comic Sans, with straightened letterforms and improved kerning.\n- Some commenters mention that Comic Sans was actually designed to help children with dyslexia, which changed their perception of the font."
  },
  {
    "id": 36303371,
    "timestamp": 1686615794,
    "title": "BBEdit: Where Respect Is Due",
    "url": "https://apps.apple.com/us/story/id1435835881",
    "hn_url": "http://news.ycombinator.com/item?id=36303371",
    "content": "AppleStoreMaciPadiPhoneWatchAirPodsTV & HomeEntertainmentAccessoriesSupport0App Store PreviewMEET THE DEVELOPERWhere Respect Is DueWhy BBEdit creator Rich Siegel has always put the user above all.BBEdit may be one of the most beloved developer tools on any platform, but its creator, Rich Siegel, had relatively modest ambitions. He started writing the first version of the software back in 1989 because he needed an editor that could handle \u201clarge\u201d files\u2014something north of 32 KB.\u201cOne of the limitations of Macintosh Pascal was that source files couldn\u2019t be more than 32 kilobytes,\u201d he explains.Over a quarter century has passed since BBEdit\u2019s commercial debut, and the app has become a favorite among developers, scientists, web designers, and writers alike for its sheer power and speed. (Siegel regularly tests 12 GB files these days.)Siegel remains BBEdit\u2019s principal architect and lead engineer, and the caffeine molecule tattooed on his arm says a lot about his work ethic. We spoke with him from his Bare Bones Software headquarters north of Boston, which Siegel shares with a pair of African gray parrots.Developers, do you have an interesting story to tell? We\u2019d love to hear it!What were the early days of BBEdit like?Back then, the way to put Mac software out in the world was to submit it to the Info-Mac Archive, an FTP repository hosted by Stanford. So that\u2019s what I did, and word started to spread pretty fast online.I had mentioned that if anyone wanted a copy on disk, they could mail a floppy and a self-addressed return envelope to me at my home. I was inundated with floppies.BBEdit has been around for a lifetime by software standards. How have you seen the user base change?Our base started as Mac software developers, scientists, system administrators, and other technical users. The first big change was popularity among HTML authors and web back-end developers. As word spread, we were able to help folks understand that the internet was built with text\u2014that you could treat text as data or you could treat it as a document. So the next wave brought in internet architects, cryptanalysts, and scientists from unexpected disciplines.The third wave has been mostly writers and other content creators\u2014folks who see their text not as data but as words, and who want as little as possible between them and their words.However, as many changes as we\u2019ve seen, there has been a delightful constancy to our customers: They\u2019re people who simply want to get work done. They\u2019re not distracted by shiny things in their software tools.What are some of the more surprising projects you\u2019ve seen people create with BBEdit?There\u2019s been so much! Our customers have used BBEdit to create novels, doctoral theses, and other long written works. One customer has used BBEdit as part of the development workflow to fly UAVs [unmanned aerial vehicles] the size of an F-16; the flight control system sends data to BBEdit in real time, where the engineers examine it, change it, and send it back to the aircraft in just a few seconds. And the Human Genome Project even uses BBEdit to help analyze long sequences of DNA.Version 3.0 of BBEdit, \u201caccelerated for Power Macintosh,\u201d from 1994.How has the software evolved over the years?BBEdit has undergone one huge technical transformation after another. First there was the port to PowerPC. We developed an OpenDoc component, and some of the internal architecture work we did for that is still in use.Then came a nearly complete rewrite of its internal architecture, followed closely by a port to Mac OS X\u2014BBEdit was the first third-party application to run natively on that new OS\u2014and then the port to Intel. Last year we finished rewriting BBEdit again, this time as an AppKit application.In between PowerPC and OpenDoc, there was another development: the emergence of the World Wide Web. BBEdit had a plug-in model at the time, and two of our customers\u2014one in the UK and one in Spain\u2014independently wrote HTML markup tools. This was 1995, and we had no idea what HTML was, but we could tell that it was going somewhere. So we evaluated the tools, chose a set of them to bundle with BBEdit, and off we went. Little did we know...BBEdit is one of the most beloved apps out there. Why do you think that is?We\u2019ve always had the utmost respect for the user. Every internal decision about look and function answers the questions \u201cWhat does the customer need?\u201d and \u201cHow can we help them be more productive?\u201d (Not \u201cHow can we give them what they\u2019re asking for?\u201d because that isn\u2019t the right question to answer.) The Macintosh was introduced to help anybody do great things. It\u2019s something we believe in completely.BBEditLegendary text and code editorVIEWMore ways to shop: Find an Apple Store or other retailer near you. Or call 1-800-MY-APPLE.Choose your country or regionCopyright \u00a9 2023 Apple Inc. All rights reserved.Privacy Policy Terms of Use Sales and Refunds Legal Site Map",
    "summary": "- BBEdit is a text and code editor that has been around since 1989 and is popular among developers, scientists, web designers, and writers.\n- The creator of BBEdit, Rich Siegel, started developing it because he needed an editor that could handle large files over 32 KB.\n- BBEdit has evolved over the years with numerous technical transformations and has been used for various projects such as creating novels, analyzing DNA sequences, and even controlling unmanned aerial vehicles.\n- The success of BBEdit can be attributed to its focus on the user's needs and productivity, rather than just giving them what they ask for.\n- The software has a long-standing reputation for being highly respected and beloved among its users.",
    "hn_title": "BBEdit: Where Respect Is Due",
    "original_title": "BBEdit: Where Respect Is Due",
    "score": 618,
    "hn_content": "- BBEdit is a long-standing and beloved text editor for Mac, with a history of revolutionizing the industry.\n- It was highly regarded for its clean UI and introduction to regular expressions.\n- BBEdit was a go-to editor for Mac developers during the transition from the THINK ecosystem to Metrowerks.\n- The editor had unique features like the ability to open and manage multiple windows and handle large files without performance issues.\n- Users have a huge amount of respect for Rich Siegel, the developer of BBEdit, for his commitment to doing one thing well over the years.\n- Users appreciate BBEdit's focus on productivity and its excellent search-and-replace capabilities, especially with regular expressions.\n- The editor has a dedicated and loyal user base, even though there are alternative text editors available on the market.\n- It is highly scriptable and customizable, making it a powerful tool for developers.\n- BBEdit's recent release, version 14, boasts new features and improvements.\n- Apple's coverage of BBEdit and Rich Siegel is significant, as it recognizes the longevity and impact of BBEdit in the tech industry.- Many people have been switching from Apple to ThinkPad laptops recently, which is interesting because Apple was once seen as the golden age in the early 2010s.\n- The Free version of TextWrangler was a popular text editor for many users, but some found it unnecessary to upgrade to BBEdit, as it was slower and had more features than they needed.\n- Some users liked the inline search pane feature in TextWrangler and wished it was available in other text editors.\n- The shortcut for inline search in BBEdit is Ctrl+S, while the shortcut for the search dialog is Cmd+F.\n- TextMate was another popular text editor, known for its intuitive interface and the ability to extend its functionality easily through snippets and commands.\n- TextMate 2.0 introduced a clunkier interface for extending functionality, causing some users to switch to other editors.\n- Some users still prefer TextMate over other editors, including Sublime Text, because of its support for esoteric language grammars and its overall user experience.\n- In unrelated news, one commenter mentioned taking care of a gray parrot, which was seen as a sweet gesture.\n- One user mentioned using BBEdit in the past but did not specify why they stopped using it after 2004.\n\nThe most important thing people should know about this post is the ongoing trend of people switching from Apple to ThinkPad laptops and the reasons behind it. Additionally, it highlights the preferences and experiences of users with different text editors like TextWrangler, BBEdit, and TextMate. The mention of a gray parrot is unrelated to the main subject matter.",
    "hn_summary": "- BBEdit is a long-standing and beloved text editor for Mac, known for its clean UI and introduction to regular expressions.\n- BBEdit has a dedicated and loyal user base, with users appreciating its productivity focus, search-and-replace capabilities, and customizability.\n- BBEdit's recent release, version 14, introduces new features and improvements."
  },
  {
    "id": 36315300,
    "timestamp": 1686683323,
    "title": "AWS us-east-1 down",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=36315300",
    "content": "",
    "summary": "- The latest AWS outage in the us-east-1 region has caused widespread disruptions for websites and services.\n- Some major websites like Reddit, Twitch, and Slack were affected by the outage.\n- The outage lasted for several hours, impacting the functioning of these sites and causing frustration for users.",
    "hn_title": "AWS us-east-1 down",
    "original_title": "AWS us-east-1 down",
    "score": 612,
    "hn_content": "- AWS us-east-1 is experiencing an outage, with increased error rates and latency.\n- Many users are frustrated with AWS's reporting of outage updates in UTC instead of their local time.\n- Users point out the importance of always displaying the time zone when reporting times to a global audience.\n- Some AWS services, like IAM and Cloudfront ACM certs, are specific to us-east-1, causing dependency issues during outages.\n- Various commenters discuss the benefits and drawbacks of using different AWS regions.\n- Some users mention specific incidents where AWS us-east-1 has experienced failures or issues.\n- Businesses like Toast POS are affected by the outage, potentially causing disruptions in services like food delivery.\n- Users propose workarounds, such as using a different region or deploying in multiple regions, to mitigate the impact of outages in us-east-1.\n- The post highlights the centralized nature of cloud services and the reliance on major providers like AWS, raising concerns about the lack of alternatives.\n- The conversation touches on the financial and operational challenges of running a data center compared to using cloud services.- Toast offline mode allows restaurants to capture credit card information even when the POS system is down.\n- US-East-1 region of AWS is experiencing an outage, affecting various services including Netlify.\n- Some users speculate that AWS's reliance on its own services could cause cascading failures during outages.\n- Multi-region deployments with automatic failover are complex and expensive, so not many organizations use them.\n- There is no estimated time for the resolution of the outage as of now.",
    "hn_summary": "- AWS us-east-1 is experiencing an outage, causing increased error rates and latency.\n- Users are frustrated with AWS reporting outage updates in UTC instead of their local time.\n- Users propose workarounds like using a different region or deploying in multiple regions to mitigate the impact of outages in us-east-1."
  },
  {
    "id": 36316079,
    "timestamp": 1686686406,
    "title": "Cormac McCarthy has died",
    "url": "https://www.nytimes.com/2023/06/13/books/cormac-mccarthy-dead.html",
    "hn_url": "http://news.ycombinator.com/item?id=36316079",
    "content": "Cormac McCarthy (1933-2023)ObituaryA Guide to His Books5 Film Adaptations to StreamFood in His Writing1992 ProfileEarly InterviewsCormac McCarthy, Novelist of a Darker America, Is Dead at 89\u201cAll the Pretty Horses,\u201d \u201cThe Road\u201d and \u201cNo Country for Old Men\u201d were among his acclaimed books that explore a bleak world of violence and outsiders.Give this article448Cormac McCarthy in 2011. In books like \u201cAll the Pretty Horses,\u201d \u201cThe Road\u201d and \u201cNo Country for Old Men\u201d he took a dark view of the human condition.Credit...Dawn Jones/Professor ProductionsBy Dwight GarnerJune 13, 2023Cormac McCarthy, the formidable and reclusive writer of Appalachia and the American Southwest, whose raggedly ornate early novels about misfits and grotesques gave way to the lush taciturnity of \u201cAll the Pretty Horses\u201d and the apocalyptic minimalism of \u201cThe Road,\u201d died on Tuesday at his home in Santa Fe, N.M. He was 89.Knopf, his publisher, said in a statement that his son John had confirmed the death.Mr. McCarthy\u2019s fiction took a dark view of the human condition and was often macabre. He decorated his novels with scalpings, beheadings, arson, rape, incest, necrophilia and cannibalism. \u201cThere\u2019s no such thing as life without bloodshed,\u201d he told The New York Times magazine in 1992 in a rare interview. \u201cI think the notion that the species can be improved in some way, that everyone could live in harmony, is a really dangerous idea.\u201dHis characters were outsiders, like him. He lived quietly and determinately outside the literary mainstream. While not quite as reclusive as Thomas Pynchon, Mr. McCarthy gave no readings and no blurbs for the jackets of other writers\u2019 books. He never committed journalism or taught writing. He granted only a handful of interviews.The mainstream, however, eventually came to him. \u201cAll the Pretty Horses,\u201d a reflective western that cut against the grain of his previous work, won a National Book Award in 1992, and \u201cThe Road\u201d won a Pulitzer Prize in 2007. Both were made into films, as was Mr. McCarthy\u2019s \u201cNo Country for Old Men,\u201d which won the Academy Award for best picture in 2008.ImageImageJavier Bardem as Anton Chigurh in the 2007 film adaptation of \u201cNo Country for Old Men,\u201d which won the Academy Award for best picture.Credit...Richard Foreman/Miramax Films and Paramount VantageThat film, directed by Joel and Ethan Coen, gave the world the indelible image of Javier Bardem as Mr. McCarthy\u2019s nihilistic hit man Anton Chigurh, dispatching his victims with a pneumatic bolt gun meant for cattle.Mr. McCarthy had in recent years been discussed as a potential winner of the Nobel Prize in Literature. The critic Harold Bloom named him one of the four major American novelists of his time, alongside Philip Roth, Don DeLillo and Thomas Pynchon, and called Mr. McCarthy\u2019s novel \u201cBlood Meridian\u201d (1985), a bad dream of a Western, \u201cthe greatest single book since Faulkner\u2019s \u2018As I Lay Dying.\u2019\u201dSaul Bellow noted Mr. McCarthy\u2019s \u201cabsolutely overpowering use of language, his life-giving and death-dealing sentences.\u201dAcclaim for Mr. McCarthy\u2019s work was not universal, however. Some critics found his novels portentous and self-consciously masculine. There are few notable women in his work.Writing in The New Yorker in 2005, James Wood praised Mr. McCarthy as \u201ca colossally gifted writer\u201d and \u201cone of the great hams of American prose, who delights in producing a histrionic rhetoric that brilliantly ventriloquizes the King James Bible, Shakespearean and Jacobean tragedy, Melville, Conrad, and Faulkner.\u201dBut Mr. Wood accused Mr. McCarthy of writing sentences that sometimes veered \u201cclose to nonsense,\u201d of \u201cappearing to relish the violence he so lavishly records,\u201d and of being hostile to intellectual consciousness.The language and tone of Mr. McCarthy\u2019s novels changed markedly over the decades. Among academics and Mr. McCarthy\u2019s legion of obsessive readers, the essential question about his oeuvre has long been: What\u2019s better, early McCarthy or late?ImageMr. McCarthy in 1965 when he published his first novel, \u201cThe Orchard Keeper.\u201d It was a bleak fable set in the Appalachian South.Credit...Joe BlackwellHis first four novels \u2014 \u201cThe Orchard Keeper\u201d (1965), \u201cOuter Dark\u201d (1968), \u201cChild of God\u201d (1973) and \u201cSuttree\u201d (1979) \u2014 are bleak fables, set in the Appalachian South, related in tangled prose that owes an acknowledged debt to William Faulkner. Indeed, the editor of Mr. McCarthy\u2019s first five books, Albert Erskine, of Random House, had been Faulkner\u2019s last editor.These early novels could be carnivalesque in their humor. In \u201cSuttree,\u201d for example, one character has carnal relations with the entirety of a farmer\u2019s watermelon field. The farmer sues, alleging bestiality, but the man later brags, \u201cMy lawyer told em a watermelon wasnt no beast.\u201dCormac McCarthy\u2019s Most Influential WorkCard 1 of 5\u2018Blood Meridian\u2019 (1985). Loosely based on historical events, the novel follows a fictional 14-year-old referred to only as \u201cthe kid\u201d as he drifts through the Southwest. \u201c\u2018Blood Meridian\u2019 makes it clear that all along Mr. McCarthy has asked us to witness evil not in order to understand it but to affirm its inexplicable reality,\u201d Caryn James wrote in her review for The Times.\u2018All the Pretty Horses\u2019 (1992). This best-selling book is an adventure story about a Texan boy who rides off with his buddy to Mexico. \u201cThe magnetic attraction of Mr. McCarthy's fiction comes first from the extraordinary quality of his prose,\u201d Madison Smartt Bell wrote in his review.\u2018The Crossing\u2019 (1994). The novel begins on a small cattle ranch in New Mexico in the last years of the Depression and follows Billy Parham, a teenage cowboy as he repeatedly crosses the border with Mexico. \u201c\u2018The Crossing\u2019 is a miracle in prose, an American original,\u201d Robert Hass wrote in his review.\u2018No Country for Old Men\u2019 (2005). This fast, violent story centers on a stone-cold killer, a small-town sheriff and an average Joe who stumbles across a leather case filled with more than $2 million. \u201c\u2018No Country for Old Men\u2019 is as bracing a variation on these noir orthodoxies as any fan of the genre could expect,\u201d Walter Kirn wrote in his review.\u2018The Road\u2019 (2006). The book is a despairing account of a boy and his father lurching across the cold, wretched, corpse-strewn, ashen landscape of a post-apocalyptic world. \u201cMr. McCarthy has summoned his fiercest visions to invoke the devastation. He gives voice to the unspeakable in a terse cautionary tale that is too potent to be numbing,\u201d Janet Maslin wrote in her review.Mr. McCarthy\u2019s later period began in earnest with \u201cAll the Pretty Horses,\u201d the first volume in his Border Trilogy, which includes the novels \u201cThe Crossing\u201d (1994) and \u201cCities of the Plain\u201d (1998). These novels put on display his powerful and intuitive sense of the American landscape. His prose was now rich but austere, shorn of most punctuation. It owed more to Hemingway than to Faulkner. The location in his fiction had shifted as well, to the desert Southwest.The elegiac quality of \u201cAll the Pretty Horses,\u201d with its existential cowboys, surprised some of his admirers. One of Mr. McCarthy\u2019s friends, the novelist Leslie Garrett, was quoted as remarking about it, \u201cCormac finally has succeeded in writing a book that won\u2019t offend anybody.\u201d\u201cAll the Pretty Horses\u201d attracted a vast audience, and was made into a film in 2000 starring Matt Damon and Pen\u00e9lope Cruz. It was not merely Mr. McCarthy\u2019s first best seller; it was his first novel to sell many copies at all. None of his previous books had by then sold more than 5,000 copies in hardcover.Image\u201cAll the Pretty Horses,\u201d a reflective Western, won a National Book Award in 1992 and was adapted for film in 2000.ImageMatt Damon in a scene from the 2000 film \u201cAll the Pretty Horses.\u201d The book was Mr. McCarthy\u2019s first best seller.Credit...Van Redin/Columbia - TriStar, via Getty ImagesEarly Life in TennesseeHe was born Charles McCarthy on July 20, 1933, in Providence, R.I., the third of six children and the oldest son born to Charles J. and the Gladys (McGrail) McCarthy. Within a few years the family moved to Knoxville, Tenn., where Mr. McCarthy\u2019s father, who had graduated from Yale Law School, worked as a lawyer for the Tennessee Valley Authority.According to one account, Mr. McCarthy adopted the name Cormac, a family nickname, to avoid associations with Charlie McCarthy, the ventriloquist Edgar Bergen\u2019s dummy. By another account, given on a website devoted to Mr. McCarthy, he renamed himself Cormac after an Irish king. Still another has it that Mr. McCarthy\u2019s family had legally changed his name to the Gaelic equivalent of \u201cson of Charles.\u201dThe McCarthy family was affluent for Knoxville, its large white house staffed with maids. The young Mr. McCarthy was drawn, however, to the city\u2019s seedier side. \u201cI felt earlier on I wasn\u2019t going to be a respectable citizen,\u201d he told the Times Magazine. \u201cI hated school from the day I set foot in it.\u201dHe attended Knoxville\u2019s Catholic High School, then the University of Tennessee, where he studied physics and engineering in 1951 and 1952. He joined the Air Force in 1953 and served four years, several of them stationed in Alaska. To quell his boredom, he said, \u201cI read a lot of books very quickly.\u201dMr. McCarthy returned to the University of Tennessee from 1957 to 1959. He learned that he had a knack for language, he once said, after a professor asked him to read a collection of 18th-century essays and repunctuate them for a textbook. He began to publish short stories in the student literary magazine. He never graduated, however, and he moved to Chicago, where he worked in an auto-parts warehouse while writing his first novel.He sent the manuscript of that novel, \u201cThe Orchard Keeper,\u201d to Random House, he said, because \u201cit was the only publisher I\u2019d heard of.\u201dReviewing \u201cThe Orchard Keeper\u201d in The Times in 1965, Orville Prescott called it \u201cimpressive\u201d but noted that Mr. McCarthy deployed \u201cso many of Faulkner\u2019s literary devices and mannerisms that he half-submerges his own talents beneath a flood of imitation.\u201dMr. McCarthy wrote for many years in relative obscurity and privation. After his first marriage, to a fellow University of Tennessee student named Lee Holleman, ended in divorce, he married Anne DeLisle, an English pop singer, in 1966. The couple lived for nearly eight years in a dairy barn outside Knoxville.\u201cWe lived in total poverty,\u201d Ms. DeLisle once said. \u201cWe were bathing in the lake.\u201d She added: \u201cSomeone would call up and offer him $2,000 to come speak at a university about his books. And he would tell them that everything he had to say was there on the page. So we would eat beans for another week.\u201dMr. McCarthy\u2019s second novel, \u201cOuter Dark,\u201d was about a woman who bears her brother\u2019s baby; he leaves it in the woods to die. Guy Davenport, writing in The Times Book Review in 1968, praised its language as \u201ccompounded of Appalachian phrases as plain and as functional as an ax.\u201dHis third novel, \u201cChild of God,\u201d was about a cave-dwelling mass murderer and necrophiliac. Reviewing it at length in The New Yorker, the author and child psychiatrist Robert Coles called Mr. McCarthy a \u201cnovelist of religious feeling\u201d and likened him to the classical Greek dramatists.Mr. McCarthy moved to El Paso in 1976 after separating from Ms. DeLisle. The couple later divorced. The settings of his novels soon changed as well.His last of his early novels to be set in the South, \u201cSuttree\u201d (1979), was his most autobiographical. It is set among the fringe characters who populated Knoxville\u2019s waterfront, a milieu he knew intimately. \u201cI was always attracted to people who enjoyed a perilous lifestyle,\u201d Mr. McCarthy once said.ImageMr. McCarthy in 1979, the year \u201cSuttree\u201d was published. In the book, one character has carnal relations with the entirety of a farmer\u2019s watermelon field.Credit...Dan MooreSome saw the novel as a farewell to his raucous old life. He stopped drinking before the novel was published. \u201cThe friends I do have are simply those who quit drinking,\u201d he said. \u201cIf there is an occupational hazard to writing, it\u2019s drinking.\u201dMr. McCarthy was briefly living in a motel in Knoxville when he learned, in 1981, that he had won a MacArthur fellowship. (In praise of his many mailing addresses, he commented: \u201cThree moves is as good as a fire.\u201d)\u2018A Legion of Horribles\u2019The MacArthur money gave him the time to write \u201cBlood Meridian,\u201d which many critics feel is his finest book. A surreal and blood-drenched anti-western about a gang of scalp hunters and outlaws in Texas and Mexico, the book features among its central characters a crazed, hairless, brilliant, seven-foot tall albino judge who put many readers in mind of Melville\u2019s Captain Ahab.The book delineated what he called \u201ca legion of horribles, hundreds in number, half naked or clad in costumes attic or biblical or wardrobed out of a fevered dream with the skins of animals and silk finery and pieces of uniform still tracked with the blood of prior owners.\u201dAfter the retirement of Mr. Erskine, his longtime editor, Mr. McCarthy moved from Random House to Alfred A. Knopf and acquired a new editor, Gary Fisketjon, who also worked with Raymond Carver, Richard Ford and Tobias Wolff, among other writers. It was before the release of \u201cAll the Pretty Horses\u201d in 1992 that Mr. McCarthy agreed to talk to The Times Magazine for his first major interview.The author of the article, Richard B. Woodward, noted at the time that Mr. McCarthy \u201ccuts his own hair, eats his meals off a hot plate or in cafeterias and does his wash at the Laundromat.\u201dIn that interview, Mr. McCarthy named the \u201cgood writers\u201d as Melville, Dostoyevsky and Faulkner, a list that omitted writers who, as he put it, don\u2019t \u201cdeal with issues of life and death.\u201d About Proust and Henry James, he commented: \u201cI don\u2019t understand them. To me, that\u2019s not literature. A lot of writers who are considered good I consider strange.\u201d\u201cAll the Pretty Horses\u201d is a gritty but often romantic narrative about a young man named John Grady Cole who, evicted in 1950 from the Texas ranch where he grew up, heads for Mexico on horseback along with his best friend. The book sold nearly 200,000 copies within six months.The next two books in the Border Trilogy also sold well, although some critics were not as taken with them. \u201cIt\u2019s axiomatic in publishing,\u201d Mr. Fisketjon said in a 1995 interview, \u201cthat the thrill of discovery is followed by a backlash.\u201dMr. McCarthy for many years maintained an office at the Santa Fe Institute, a nonprofit scientific research center founded in 1984 by the particle physicist Murray Gell-Mann and others. He moved from El Paso to live nearby. He enjoyed the company of scientists and sometimes volunteered to help copy-edit science books, shearing them of things like exclamation points and semicolons, which he found extraneous.\u201cPeople ask me, \u2018Why are you interested in physics?\u2019,\u201d he was quoted as saying in a 2007 Rolling Stone profile. \u201cBut why would you not be? To me, the most curious thing of all is incuriosity.\u201d He would drive to the institute after dropping John, his young son, off at school.Mr. McCarthy published his stripped-down existential thriller \u201cNo Country For Old Men\u201d in 2005. The next year he published \u201cThe Road,\u201d a grueling novel about a father and son\u2019s struggle to survive in a postapocalyptic landscape.Image\u201cThe Road\u201d a grueling novel about a father and son\u2019s struggle to survive in a post-apocalyptic landscape, won a Pulitzer Prize in 2007.The novel is dedicated to his son.\u201cI think about John all the time and what the world\u2019s going to be like,\u201d Mr. McCarthy told Rolling Stone. \u201cIf the family situation was different, I could see taking John and going to New Zealand. It\u2019s a civilized place. \u201dIn the same interview, Mr. McCarthy said he had never voted: \u201cPoets shouldn\u2019t vote.\u201dWriting Till the EndMr. McCarthy sold his archives, 98 boxes of letters, drafts, notes and unpublished work, to Texas State University in 2008 for $2 million. A year later, the Olivetti typewriter on which he\u2019d written each of his novels sold at auction for $254,500. He immediately began working on a new Olivetti, the same model, purchased for less than $20.ImageThe Olivetti manual typewriter on which Mr. McCarthy typed all of his novels from 1958 to 2009, the year it sold at auction for $254,500.Credit...Christie's, via Associated PressIn 2012, Mr. McCarthy wrote a screenplay, \u201cThe Counselor,\u201d about a lawyer in the Southwest who falls into the drug business. Ridley Scott adapted it for a film in 2013 starring Michael Fassbender and Cameron Diaz.Mr. McCarthy was married for a third time, to Jennifer Winkley, in 1998, when he was 64 and she was 32. The marriage ended in divorce in 2006. In addition to his son John, from Mr. McCarthy\u2019s third marriage, he is survived by another son, Chase, from his first marriage; two sisters, Barbara Ann McCooe and Maryellen Jaques; a brother, Dennis; and two grandchildren. His first wife, Ms. Holleman, died in 2009.Late in 2022, Mr. McCarthy released a pair of ambitious linked novels, \u201cThe Passenger\u201d and \u201cStella Maris,\u201d to mostly adulatory reviews. \u201cThe Passenger\u201d is about a racecar driver turned salvage diver named Bobby Western \u2014 he somewhat resembles Mr. McCarthy in his taciturnity, his Knoxville childhood and his fondness for New Orleans and its nightlife \u2014 who sees things he should not see. Before long he is pursued not only by G-men but, it can seem, also by all the ghosts of the 20th century. It\u2019s a novel of ideas \u2014 about mathematics, the nature of knowledge, the importance of fast cars \u2014 that slips into pretentiousness at times but also contains flatulence jokes.The title of the second novel, \u201cStella Maris,\u201d refers to a psychiatric hospital in Black River Falls, Wis. That is where 20-year-old Alicia Western, a doctoral candidate in mathematics at the University of Chicago, has checked herself in because she\u2019s been hallucinating. Central among her visions is the Thalidomide Kid, a shambolic dwarf with flippers and a bent sense of humor. Alicia is carrying a plastic bag stuffed with $40,000, which she tries to give away to the receptionist. Alicia also happens to be Bobby\u2019s sister. Their father was a physicist on the Manhattan Project.Shortly before Mr. McCarthy\u2019s death, it was announced that he had been at work on a screenplay for a film adaptation of \u201cBlood Meridian,\u201d to be directed by John Hillcoat, who directed the film of Mr. McCarthy\u2019s \u201cThe Road.\u201dIn 2007, Mr. McCarthy took part in one of the most unlikely cultural collisions of the new century when he agreed to be interviewed on daytime television by Oprah Winfrey. She had chosen \u201cThe Road\u201d for her book club.He seemed uncomfortable in the spotlight. \u201cI don\u2019t think it\u2019s good for your head,\u201d he told Ms. Winfrey about being interviewed. \u201cYou spend a lot of time thinking about how to write a book, you probably shouldn\u2019t be talking about it. You probably should be doing it.\u201dAlex Traub contributed reporting.A correction was made on June 13, 2023: An earlier version of this obituary referred incorrectly to El Paso, where Mr. McCarthy moved in 1976. It is in Texas, not New Mexico.When we learn of a mistake, we acknowledge it with a correction. If you spot an error, please let us know at nytnews@nytimes.com.Learn moreDwight Garner has been a book critic for The Times since 2008. His new book, \u201cThe Upstairs Delicatessen: On Eating, Reading, Reading About Eating, and Eating While Reading,\u201d is out this fall.A version of this article appears in print on June 14, 2023, Section A, Page 1 of the New York edition with the headline: Cormac McCarthy, 89, Literary Loner in a Dark World, Is Dead. Order Reprints | Today\u2019s Paper | SubscribeREAD 448 COMMENTSGive this article448",
    "summary": "- Cormac McCarthy, a reclusive and acclaimed writer known for his dark and violent novels, has passed away at the age of 89.\n- McCarthy's novels, such as \"All the Pretty Horses,\" \"The Road,\" and \"No Country for Old Men,\" explore grim and violent worlds.\n- His works have received both critical acclaim and criticism, with some praising his use of language and others finding his novels portentous and lacking in female representation.\n- Several of McCarthy's novels have been adapted into successful films, including \"No Country for Old Men,\" which won the Academy Award for Best Picture in 2008.\n- He is considered one of the major American novelists of his time, alongside writers like Philip Roth, Don DeLillo, and Thomas Pynchon.\n- McCarthy's writing style evolved over the years, with his early novels characterized by tangles and influences from William Faulkner, while his later works were more austere and Hemingway-esque.\n- Despite his success, McCarthy maintained a reclusive lifestyle and rarely granted interviews or made public appearances.\n- He leaves behind a body of work that continues to captivate readers with its powerful and sometimes controversial themes.",
    "hn_title": "Cormac McCarthy has died",
    "original_title": "Cormac McCarthy has died",
    "score": 608,
    "hn_content": "- Cormac McCarthy, a renowned American author, has passed away.\n- McCarthy was known for his novels such as The Road, Blood Meridian, and the Border Trilogy.\n- His writing style is characterized by his ability to capture the dark side of the human experience in beautiful language.\n- The Road, set in a post-apocalyptic world, is particularly haunting and emotionally impactful.\n- McCarthy's works often explore themes of violence, morality, and the human condition.\n- He was also interested in math, philosophy, and cognition.\n- McCarthy was a resident copy editor for science papers at the Santa Fe Institute, demonstrating his diverse interests.\n- The announcement of his passing garnered significant attention among literary enthusiasts and fans of his works.- Cormac McCarthy, acclaimed author of novels such as \"The Road\" and \"Blood Meridian,\" has passed away.\n- McCarthy is regarded as one of the giants of American literature and was known for his brutal yet beautiful writing style.\n- His works often explore themes of violence, morality, and the human condition, drawing from a variety of genres such as Western and post-apocalyptic fiction.\n- \"The Road\" is considered one of his most harrowing and influential works, depicting a bleak and hopeless world following an unspecified catastrophe.\n- McCarthy's writing is praised for its vivid descriptions and ability to evoke powerful emotions in readers.\n- Some readers find his books difficult to read due to their dark and violent subject matter, but his writing has also garnered a dedicated following.\n- Many of McCarthy's books, including \"Blood Meridian\" and \"No Country for Old Men,\" have been adapted into successful films.\n- His death is a significant loss to the literary world, as he was considered one of the last great American authors.\n- McCarthy's writing style is often compared to that of other renowned authors such as William Faulkner.\n- The impact of McCarthy's work will likely be felt for generations to come, as his novels continue to resonate with readers.\n- Fans and critics alike consider McCarthy's writing to be an essential part of the American literary canon.\n- His ability to craft powerful and thought-provoking narratives sets him apart as a master storyteller.\n- McCarthy's works embody themes of existentialism, morality, and the nature of humanity.\n- While his writing may not be for everyone, those who appreciate his style find it to be deeply moving and impactful.\n- McCarthy's passing leaves a void in the literary landscape, but his novels will continue to captivate and challenge readers for years to come.",
    "hn_summary": "- Cormac McCarthy, acclaimed author of novels such as \"The Road\" and \"Blood Meridian,\" has passed away.\n- McCarthy is regarded as one of the giants of American literature and was known for his brutal yet beautiful writing style.\n- His works often explore themes of violence, morality, and the human condition, drawing from a variety of genres such as Western and post-apocalyptic fiction."
  },
  {
    "id": 36316134,
    "timestamp": 1686686599,
    "title": "McDonald's just dropped a brand new Game Boy game",
    "url": "https://retrododo.com/mcdonalds-grimaces-birthday/",
    "hn_url": "http://news.ycombinator.com/item?id=36316134",
    "content": "NEWSMcDonald\u2019s Just Dropped A Brand New Game Boy Game In 2023JUNE 13, 2023ANTHONY WALLACEMcDonald\u2019s has just released a brand new Game Boy Color game that plays on actual hardware in 2023. Yes, you read that correctly.Twenty five years after the release of the Game Boy Color, McDonald\u2019s is still attempting to deliver a worthwhile Micky D\u2019s video game.This is quite a surprise to many of us hardcore Game Boy enthusiasts.But with McDonald\u2019s having just released an official Tetris handheld in China, it seems that they are really showing the retro scene some love.We do know that the creation of this game coincides with Grimace\u2019s fictional birthday of June 12th. And McDonald\u2019s has also released a limited edition meal as part of this celebration.Perhaps somebody internally was not happy with the infamous McDonaldland game for the monochrome Game Boy (1992). And they hoped to offer something redeeming before their retirement.Whatever the reason McDonald\u2019s decided to commission an official Game Boy Color game now, we are happy they did!Grimace\u2019s BirthdayThe new Game Boy Color title, Grimace\u2019s Birthday, was produced by Krool Toys.And it was developed using GB Studio, a really wonderful program that makes Game Boy development much easier in 2023.I was surprised to see that the game was developed by somebody in our small niche community: Gumpy Function.Tom Lockwood (aka Gumpy Function) is the developer behind one of the absolute best homebrew Game Boy games, Unearthed.It is very cool to see that they helped bring us this new McDonald\u2019s game.From the moment you start Grimace\u2019s Birthday, you can tell that this is a top notch game for the Game Boy Color.Grimace\u2019s Birthday is a fun little platforming experience with really cool graphics and simple but effective game play.You play as Grimace attempting to locate his buddies so his birthday party can begin. And the game play primarily involves you skateboarding your way through some unique platforming levels.From personal experience developing a Game Boy Color game, I can tell you that this project took quite a bit of time and talent to complete.Gumpy Function did a great job producing the game, and it\u2019s wild to imagine them receiving the call from McDonald\u2019s requesting them to make a brand new Game Boy Color game.ImpressionsIf Grimace\u2019s Birthday were released 20 years ago, it would have been a pretty solid title for the Game Boy. And we would have paid $20 to pick it up on a physical cartridge.But today, twenty five years after the Game Boy Color was released, we can play Grimace\u2019s Birthday for free on our web browser.It is unclear if McDonald\u2019s hoped to keep the game locked on their website or if there\u2019s any particular reason they wouldn\u2019t want us to have a ROM.But needless to say, the internet was very quick to rip the file and share it online.So many of us emulation fans grabbed a copy and loaded it onto our handhelds to give it a spin.Grimace on a skateboard playable on the Game Boy in 2023. I mean, what more do we need to say. It\u2019s pretty much the epitome of coolness.If you\u2019d like to grab a digital ROM of Grimace\u2019s Birthday, it\u2019s currently available here.Just be sure to order some McDonald\u2019s to support them. Who knows\u2026 if we keep eating McDonald\u2019s, they might keep producing these oddball retro gaming related projects.I, for one, will NEVER stop eating McDonald\u2019s. You have my word.This article may contain affiliate links. If you use these links to purchase an item we may earn a commission. Thank you for your support.Anthony WallaceAnthony has been a video game lover ever since he can remember. He became a fulltime nomad in 2018, living throughout most of Asia. He focused his passion in retro gaming and began creating a game for the Game Boy Color while living in Nara, Japan during the 2020 pandemic. He is now in Chiang Mai, Thailand, where he spends most of his time gaming, going on long walks and meeting as many stray dogs as possible.You May Also LikeNEWSCapcom Launches 40th Anniversary Website With Free To Play Retro GamesJUNE 12, 2023ANTHONY WALLACENEWSAnbernic Reveals New Grips For The RG405MJUNE 12, 2023ANTHONY WALLACENEWSNitro Deck Is The Ultimate Handheld Accessory For Your SwitchJUNE 9, 2023ANTHONY WALLACE",
    "summary": "- McDonald's has released a brand new Game Boy Color game called Grimace's Birthday.\n- The game was developed by Gumpy Function, a developer known for their homebrew Game Boy games.\n- Grimace's Birthday is a platforming game where players control Grimace and skateboard through different levels to find his friends for his birthday party.\n- The game has cool graphics and simple but effective gameplay.\n- It can be played for free on web browsers and ROMs of the game are available for download online.\n- McDonald's released the game as part of the celebration of Grimace's fictional birthday.\n- This release is surprising and exciting for retro gaming enthusiasts and fans of McDonald's.\n- It is unclear if McDonald's intended to keep the game exclusive to their website, but it was quickly shared online.\n- Supporting McDonald's by ordering food may encourage them to continue creating retro gaming projects like this.",
    "hn_title": "McDonald's just dropped a brand new Game Boy game",
    "original_title": "McDonald's just dropped a brand new Game Boy game",
    "score": 606,
    "hn_content": "- McDonald's has released a brand new Game Boy game called \"Grimace's Birthday.\"\n- The game was made using GB Studio, a popular game development tool for creating Game Boy games.\n- Many Game Boy Studio projects on Itch.io likely use the same Game Boy emulator called binjgb.\n- The game can be played online on the website grimacesbirthday.com using a Game Boy emulator.\n- The music in the game was created using a music engine called HUGETracker.\n- McDonald's missed an opportunity to compensate the developer of the game, which could have made the project even cooler.\n- The game has sparked interest and nostalgia among readers, especially those familiar with the Game Boy era.\n- The use of retro gaming aesthetics and marketing techniques has captured the attention of tech-savvy individuals.\n- This post showcases the ongoing development of retro games and the active Game Boy homebrew community.- McDonald's released a portable Tetris game console shaped like Chicken McNuggets.\n- The game console is officially playable through an emulator, not as a physical cartridge.\n- The game has a retro feel with pixel art style, which adds authenticity and generates buzz.\n- It is unclear if the game is advertised as a Game Boy Color game due to trademark reasons.\n- The physical tooling to create a Game Boy cartridge and chips exists, but it seems McDonald's did not use it for this release.\n- The Game Boy cartridge is small and needs to run off battery power, presenting a challenge for more complex games.\n- The McDonald's game features the logo appearing on a garbage can, potentially sending a positive message to health-conscious parents.\n- McDonald's menu had healthier options like salads and apple slices compared to other fast food restaurants years ago.\n- Some people have doubts about the actual healthiness of McDonald's \"healthy\" food offerings.\n- The release of the game is seen as a clever marketing move by McDonald's.\n- The game has generated interest and excitement among readers for its unique concept and tie-in with McDonald's branding.\n- The post also mentions the 10-year hunt for a lost McDonald's DS game and makes a joke about the recent AWS meltdown.\n- Some commenters appreciate the lightheartedness and humor of the post and compare McDonald's marketing to MSCHF.\n- A database entry for a mysterious purple shake mentioned in the game is missing.\n- Overall, the post creates a sense of nostalgia and curiosity among readers.",
    "hn_summary": "- McDonald's has released a new Game Boy game called \"Grimace's Birthday\" using GB Studio.\n- The game can be played online using a Game Boy emulator on the grimacesbirthday.com website.\n- The game has sparked nostalgia and interest among readers, showcasing the development of retro games and the active Game Boy homebrew community."
  },
  {
    "id": 36312122,
    "timestamp": 1686671732,
    "title": "Apollo's Christian Selig explains his fight with Reddit \u2013 & why users revolted",
    "url": "https://www.theverge.com/2023/6/13/23759180/reddit-protest-private-apollo-christian-selig-subreddit",
    "hn_url": "http://news.ycombinator.com/item?id=36312122",
    "content": "APPSApollo\u2019s Christian Selig explains his fight with Reddit \u2014 and why users revolted\u2018Reddit has plugged its ears and refuses to listen to anybody but themselves. And I think there\u2019s some very minor concessions that they can make to make people a lot happier.\u2019By David Pierce and Nilay PatelJun 13, 2023, 7:44 AM PDT|CommentsShare this storyChristian Selig is Apollo\u2019s lone developer and at the center of the fight taking over Reddit. Image: Christian SeligChristian Selig did not mean to be the face of a revolution. All the Canadian developer wanted, really, was to be able to keep working on his app. But that app, a Reddit client called Apollo, has become the central figure in an all-out platform war. The short version of a long history goes like this: in April, Reddit announced new terms for its API, the tool through which developers of third-party apps access Reddit\u2019s data. Every time you post a comment, refresh a page, search for something, or take just about any other action in an app like Apollo, the app pings an API to get the data you need. Reddit\u2019s API has been free for many years, leading to a flourishing community of third-party tools. But Reddit finally decided it was time to charge for access, both to recoup the costs of running the API and to help the company become more profitable ahead of its planned IPO.The logic made sense to Selig; the price didn\u2019t. Ultimately, he calculated he would have to pay Reddit $20 million a year just to keep Apollo running, which he couldn\u2019t afford. Other developers building Reddit apps came to the same conclusion and said they would be forced to shut down.Many users decided this wasn\u2019t a fair business deal \u2014 this was a plot to crush third-party Reddit apps. So in response, Reddit users decided to push back. The battle reached its current peak when thousands of subreddits went dark on Monday, protesting Reddit\u2019s new API policies and how they affected everything from app developers to the on-platform tools many users rely on. Reddit\u2019s response? It\u2019s just business. \u201cWe\u2019ll continue to be profit-driven until profits arrive,\u201d CEO Steve Huffman wrote during an AMA session over the weekend. \u201cUnlike some of the [third-party] apps, we are not profitable.\u201dThose third-party apps are clients like Apollo, which Selig has built and run for the last nine years. He\u2019s planning to shut the app down on June 30th, the day before Reddit\u2019s new rules go into effect. But he\u2019s still holding out hope. Hope that Reddit might change its mind or soften its stance, hope that the subreddit blackout might change things, hope that Huffman might pick up the phone and try to smooth things over.In the midst of the platform blackout, we sat down with Selig to talk about what he wants from Reddit, why he still believes in the platform, and whether he sees a chance to keep Apollo alive beyond the end of the month.The following has been edited for length and clarity. For the full conversation, tune into Wednesday\u2019s episode of The Vergecast.Nilay Patel: Do you get the sense that Reddit knows that Apollo users are power users of Reddit? This is something I think about with social networks all the time: their data blinds them to the reality of the platform. So you might say, okay, there\u2019s only 2 million Apollo users; you just don\u2019t know that they happen to be 2 million of the most important users on the platform or the most vocal or the most invested. And you piss them off with a decision like this. Did you ever get the sense they knew that Apollo was important to some users or that you were in the middle of the relationship that Reddit had with that group of users?Honestly \u2014 and I say this with a lot of respect for Reddit \u2014 I feel like this was a decision that kind of got rushed out the door, and they didn\u2019t do their due diligence on understanding that stuff. Because there\u2019s a lot of stuff. Even just making a really big pricing announcement, but not having any pricing. There were a lot of discussions where they were like, \u201cWe\u2019ll have it in two to four weeks.\u201d And I was like, great. And then six weeks passed. And they were like, \u201cOkay, now we have it.\u201dAt the end of January \u2014 I want to say January 26th \u2014 I had another call with Reddit prior to all this where they were saying, \u201cWe have no plans to change the API, at least in 2023, maybe for years to come after that. And if we do, it\u2019ll be improvements.\u201d So then two months, three months later, for them to say, \u201cLook, actually, scratch that, we\u2019re planning to completely charge for the API, and it\u2019s gonna be very expensive,\u201d kind of made me think\u2026 what happened in those three months? This clearly wasn\u2019t something that was cooking for a long time. And I don\u2019t think they understood how much this would affect people and the response that they would get. Because they\u2019re honest, they\u2019re smart people. And I don\u2019t think if they understood everything that they do now, they would have made the same steps. At least I would hope not. David Pierce: So Reddit said, \u201cWe don\u2019t want to kill third-party apps.\u201d But we just went through this with the Twitter API, which pretty explicitly wanted to kill third-party apps, right? Do you buy that either at that moment or now, in hindsight, with two weeks of more conversations, do you think Reddit actually doesn\u2019t want to kill third-party apps? Because it sure looks like it does.I will be charitable and say at the outset, I honestly don\u2019t think they did. Or maybe I\u2019m very naive. Maybe they didn\u2019t care about us at all, but they were like, \u201cWe know you\u2019re important to a subset of users, and we know there\u2019ll be a big blowback if we get rid of you, so we want to make some arrangement where we can keep you but you\u2019re not a pain in the ass.\u201d But I think as time went on, things like only giving us 30 days to make these monstrous changes, I think it started to muddy the waters. It\u2019s like, well, if you don\u2019t want us to die, why are you giving us such aggressive timelines? And why can\u2019t you bump things out? Or listen to us? Why are you acting in this way? I think to a certain extent, after some of the blowback from initial posts from developers being like, \u201cThis is gonna cost us a lot of money,\u201d they almost went on the defensive internally and said, \u201cThese developers are entitled, and they just want a free lunch or something.\u201d And I feel like it got very personal when it didn\u2019t really need to. It was just like, this is gonna kill my business \u2014 can we have a path forward?\u201cI feel like it got very personal when it didn\u2019t really need to. It was just like, this is gonna kill my business \u2014 can we have a path forward?\u201dNP: If they\u2019d offered to buy the app from you, would you have sold it to them?I guess it depends on the stage. I mean, I\u2019m just some guy, so if the number was high enough, sure. Absolutely. At the stage where it was clear that they weren\u2019t interested in having third-party apps around anymore, just because of the pricing and some of the API changes around explicit content or whatnot, if that was the point where they said like, \u201cThat being said, we would like to maybe work with your user base or take your user base and figure out a way to make them happy in the context of the official app and work with you and your app through an acquisition,\u201d I honestly would have listened to that. Prior to that, it would have had to have been a pretty good number, just because I love building Apollo and being so in touch with so many people through the community. It would have to be a big number, losing such a big part of your life and what you do every day. There\u2019s an emotional penalty to losing that is hard to quantify with money, as superficial as that sounds.DP: This brings me to my favorite segment of every Vergecast, \u201cLet\u2019s Do Some Math Together!\u201d Because there\u2019s been a lot of questions and a lot of debate about what it would look like to continue to run Apollo. One of the things Reddit has said is that the way that the API works, it should cost less than $1 per user per month. Your math said it would cost about $2.50 per user per month. On average, yeah. DP: And so there are a bunch of folks out there who are like, \u201cOkay, just charge us $5 a month, give some of that money to Apple, keep the rest, give some to Reddit, everybody wins.\u201d Others are like, \u201cOkay, just make it subscription-only, so only people paying for the app can use the app.\u201d And it does seem like you went to \u201cI can\u2019t afford this; I have to shut down the app\u201d fairly quickly. I\u2019m assuming you went through some of these other scenarios before you got there. Walk me through how you get from \u201cI owe $20 million a year, I have a very popular app\u201d to \u201cI definitely can\u2019t afford this. My only option is to close.\u201dIt\u2019s a two-faceted answer. So say, yeah, just charge $5. Bob\u2019s your uncle, right? The issue there is that your average user uses about 345 requests per day per user. And then, if you extrapolate that over the month, it would cost about $2.50 to support them. The issue is that\u2019s the average user. A free user uses like 200-something requests; an existing paid user is closer to 500. So for that existing paid user who naturally uses more, that\u2019s closer to $3.60 per month in its current state. And if I just charged $5 to them, you take off Apple\u2019s 30 percent or whatever and you\u2019re down to $3.50, you\u2019re already 10 cents in the red per user per month. So the calculus there is already pretty tricky. That being said, if I had more than 30 days, there\u2019s a possibility that I could go in and change some stuff. Like where I check your inbox every so often, where I preload a page for you that I think you might scroll to \u2014 I can kind of cut down on all those and maybe cut that 400 down to, I don\u2019t know, 300 or 200. If I had more than 30 days. But even beyond that, approximately 5 percent of my users used between 1,000 and 2,000 API requests a day. At the low end, those would cost $7.50 a month. And you can imagine the users who use the app the most are kind of the most likely to pay for things. So they\u2019d be the most obvious ones that would want to pay for the app. And when you\u2019re looking at them costing $7.50 a month each, do I have like a $5 tier that hopefully covers most people, and then once you expire that, is it like a phone plan where I call you up and say, like, \u201cDo you want to top off for the month?\u201d That\u2019s not fun. So that\u2019s one facet of it. Say I solve all that. The other issue is that with the very short notice of 30 days from when the pricing was announced to when we start incurring charges, I\u2019ve got about 50,000 yearly subscribers who have already paid for a year of service [at roughly $1 / month]. That price was based on operating costs that I had for design services, server fees, a part-time server engineer. For $1 a month, I can make a profit on that. But [with the API changes], I\u2019ve got like $1 or $2 extra monthly costs per user for those 50,000 people who have already prepaid. I can\u2019t monetize them anymore. They\u2019ve already paid. So starting July 1st, those people will start incurring a bill of $50,000 a month for me that I have no way to monetize further because they\u2019ve already prepaid. And that\u2019s where the calculus got really difficult: Okay, I have a bill for $50,000. And then maybe the next month, some of the people who are close to expiring would expire, and it would go down to 11 months, maybe you\u2019d only be $45,000. And then the next month would be $40,000. But you\u2019re potentially looking at hundreds of thousands of dollars in bills I would get from Reddit for people that I couldn\u2019t make a single more dollar off of because they already paid my old operating costs. And that\u2019s where it got really tricky. \u201cYou\u2019re potentially looking at hundreds of thousands of dollars in bills I would get from Reddit for people that I couldn\u2019t make a single more dollar off of.\u201dEveryone I talked to was kind of just like, \u201cI don\u2019t see how I would make this work.\u201d And then when you add on the extra fact that Reddit\u2019s saying these like bizarre things around threats and blackmail, and they won\u2019t answer your emails anymore, it kind of becomes a thing: I can\u2019t pay for this. How to make a profit out of it is very difficult, and Reddit seems like they have no intention of wanting to work with me or third-party apps anymore. It kind of becomes, like, what\u2019s the future here? And that was kind of where I landed on it. I was staying at an Airbnb with like seven other people for WWDC, and it was just talking with them over. That Wednesday night, I was just like, \u201cI don\u2019t see any other route out of this. It\u2019s just gotten dire.\u201d And that was when I started typing up my post and being like, \u201cYeah, this is kind of it.\u201dNP: Reddit can barely monetize its own users, right? They\u2019re not making money. And the way they\u2019ve chosen to monetize is mostly advertising, which is not exposed in your app. If Reddit had come to you and said, \u201cAlright, you can use our API, we\u2019ll lower the costs, but you\u2019ve got to start serving our ads,\u201d I don\u2019t know how I would have felt about that, as somebody who has used Apollo. That\u2019s pretty icky. Now I\u2019m paying you for software, but I\u2019m getting their ads.Yeah, but that\u2019s one of those things that I felt like if they did it, it would have shown a little bit more effort to include third-party apps insofar as they\u2019d have to build kind of that integration. And yeah, it\u2019s tricky because if that was a path to survive, I wouldn\u2019t like it, either \u2014 I prefer not to have ads, and I don\u2019t mind paying for an ad-free experience \u2014 but if that was the only way to survive going forward, like that\u2019s something I also would have entertained. A revenue share, where you make X amount of money, they want half a bit off the top, that\u2019s something that would have been fine. There were a lot of arrangements I think they could have gone with that just weren\u2019t quite as\u2026 killer, for lack of a better term.DP: What do you want from Reddit? What could have happened in that phone call on May 31st that you would have just hung up and been like, \u201cGreat, cool, perfect. Sounds good. Win-win, everybody. Let\u2019s move forward\u201d?In an absolutely perfect world, Reddit would have said, \u201cOkay, we\u2019re also going to halve the price.\u201d Because that would have taken like my $2.50 a user down to about $1.25 (a month). I charge new users $1.50. So it would have been something that, for my existing users, I can at least afford to keep like them around. They won\u2019t put me in the red. That would have been great.NP: So you\u2019re now the symbol of a much larger thing that\u2019s happening on Reddit, right? There was a post about Apollo shutting down. And that spiraled into an ongoing series of subreddit blackouts. There are other asks in the mix now, mostly around accessibility, which I think even Steve Huffman has said, \u201cYep, Reddit\u2019s got to do a better job of this.\u201d And then there\u2019s a set of asks are Not Safe For Work content, which it feels like they don\u2019t know that Apple has rules \u2014 Reddit can\u2019t just put porn in the app in the way that maybe they\u2019re being asked for. But you\u2019re now one part of a series of asks from a pretty disparate group on Reddit. Do you think that if they fix the API pricing situation and Apollo got back on track, that that would soothe the community at large?I think it would go a long way, honestly, to at least making them feel heard. Because in this whole saga, I don\u2019t think I\u2019ve seen Reddit offer to give an inch on any of the things.NP: I would say Reddit\u2019s political skills at operating its community are\u2026 negative.Well, it\u2019s weird because prior to this, I almost always understood that Reddit as a company understood that they\u2019re very community-focused, and they kind of didn\u2019t do the bullshit corporate speak. And it was weird to kind of see this week, where they engaged in a lot more of that than I have historically ever seen them do. And it just went over\u2026 about as well as I thought it would. NP: I think that we are in an absolute moment of change for what you might call the Web 2.0 era. Have you thought about \u201cI\u2019m just going to take my users and go build a Reddit for ActivityPub\u201d?DP: Even more specifically, one thing a lot of users have been saying is, \u201cWe\u2019re leaving Reddit; we\u2019re gonna go to Lemmy and Kbin!\u201d Those are the two that I keep hearing about. Is there a move that way that you think is real, that you might want to be part of? It\u2019s tricky because, to a certain extent, that does sound really interesting. But with Mastodon, for instance, I love it, but I\u2019ve seen so many people \u2014 even in the tech community, who totally have the means to make that move if they want to \u2014 who have just been too intimidated or just can\u2019t get off Twitter for some reason. In the back of my head, I\u2019m like, if these people who are much smarter than me can\u2019t make that change, is this just like a short-term thing? \u201cIt\u2019s hard for me to build another thing. If it just evaporated again, it would be like a double breakup.\u201dIt\u2019s hard for me to build another thing. If it just evaporated again, it would be like a double breakup. This has been so exhausting for the last few months. The amount of work it would take to port all the API endpoints over to Lemmy or Kbin or something, that would be a gargantuan amount of work that I\u2019m not sure I have the capacity for. And then just the complexity of making it work. Long term, it\u2019s a big question mark for me that, at this stage, I\u2019m not sure I\u2019m totally interested in pursuing. But it\u2019s also one of those things where I completely wish it the best. And if something that was decentralized kind of became the norm, I think that would definitely be a win for everybody.DP: So as it stands right now, you\u2019re set to turn off the API token and basically shut down Apollo on June 30th. What percentage of you believes you will actually shut down on June 30th?Oh, gosh, like 90 percent.DP: So you\u2019re fully prepared for this to happen.I\u2019ve talked to my reps at Apple to get the process started. As much as I would love to say this has been a big bluff\u2026 it was literally a matter of, like, Reddit hasn\u2019t answered my emails in a while. Every public statement I\u2019ve seen seems like the CEO is quite angry over this. They don\u2019t seem to want to budge on the timeline at all. I don\u2019t see how I can make this work. I\u2019ve loved building Apollo \u2014 even at the $3,500 that [Apple\u2019s Vision Pro] costs, I was kind of excited for the opportunity to see what Reddit on that thing could look like. I was really excited for that. So it\u2019s kind of hard for me to say, but at this stage, it\u2019s just hard for me to see a path forward where they are reasonably willing to meet me even a quarter of the way here. It just seems like they\u2019re, they\u2019re so angry, for lack of a better term, that I kind of just feel like it\u2019s better for me to be honest with myself rather than hold onto the hope until the last minute and then just completely fall apart. But that 10 percent of me really hopes that I\u2019ll be able to say, \u201cI hopped on a call with Steve. We talked it out. There were some pleasantries exchanged about misunderstandings. We\u2019re all good now, they\u2019re giving us more time to adopt the API, and we\u2019re sticking around.\u201d I would love that. But it\u2019s totally in Reddit\u2019s court. I\u2019m happy to talk whenever, but I just haven\u2019t been able to reach them.Most PopularReddit CEO tells employees that subreddit blackout \u2018will pass\u2019Reddit crashed because of the growing subreddit blackoutIn the bid to grow at all costs, Instant Pot is cooking itselfRead the new Twitter CEO\u2019s first email to employeesGoogle is getting a lot worse because of the Reddit blackouts",
    "summary": "- Reddit recently announced new terms for its API, which allows developers of third-party apps to access Reddit's data.\n- Christian Selig, the developer of the popular Reddit client app Apollo, calculated that he would have to pay Reddit $20 million a year to keep his app running under the new terms.\n- Many users and developers felt that these new terms were unfair and protested by participating in a subreddit blackout. They believe that Reddit is trying to eliminate third-party Reddit apps.\n- Selig had hoped that Reddit would change its mind or find a compromise, but the company has been resistant to negotiation.\n- Selig is planning to shut down Apollo on June 30th unless there are changes made to the API pricing.\n- The situation has sparked a larger discussion about the future of third-party apps on Reddit and the need for better communication between Reddit and its users.\n- There is a possibility that users may migrate to alternative platforms, such as Lemmy or Kbin, which offer decentralized alternatives to Reddit. However, the transition may be challenging for some users.\n- Selig is prepared to shut down Apollo if a resolution is not reached with Reddit. He believes that it is unlikely that Reddit will meet him halfway on the issue.\n- Despite the challenges, Selig remains hopeful that there could be a positive outcome and that he can continue working on Apollo. However, the ball is in Reddit's court, and they have not been responsive to his attempts to communicate.",
    "hn_title": "Apollo\u2019s Christian Selig explains his fight with Reddit \u2013 and why users revolted",
    "original_title": "Apollo\u2019s Christian Selig explains his fight with Reddit \u2013 and why users revolted",
    "score": 518,
    "hn_content": "- Reddit faced backlash and user revolt after changing their API pricing for third-party app developers.\n- The issue was two-fold: the rate charged to developers was deemed absurdly high, and the changes were implemented with insufficient notice.\n- Many users and developers believe the changes were an attempt to kill off third-party apps in favor of Reddit's own app.\n- Critics argue that Reddit mishandled the situation and should have approached the changes differently, such as by gradually increasing prices over time.\n- There are suspicions that the motivation behind the changes was to control the user experience and ads on the platform.\n- The future of third-party apps and old Reddit platform remains uncertain, raising concerns among users and developers.\n- The incident highlights the challenges of monetizing and maintaining an open platform like Reddit.- Reddit has decided to impose a paid API on third-party apps, causing controversy among developers and users.\n- Apollo, a popular third-party Reddit app, claims that Reddit's pricing is exorbitant and not feasible for developers.\n- The argument revolves around the cost per user and the discrepancy between what Reddit claims it costs to support third-party apps and what developers believe is fair.\n- Reddit justifies the paid API by stating that it accounts for lost revenue opportunities and the burden of supporting third-party apps.\n- There have been allegations of lying and miscommunication from both sides, leading to tensions between Reddit and third-party developers.\n- The situation has sparked discussions about the value of user-generated content and the role of API access in app development.\n- Some people argue that Reddit's decision may lead to the decline of third-party apps and impact the overall user experience of the platform.- Reddit moderators have the ability to hide new posts pending moderator approval, meaning not all posts will be immediately visible\n- Some people question the value of moderators if their duties can be automated by a filter\n- Reddit moderators have been accused of inventing unnecessary work for themselves and making rule changes to justify their existence\n- Engaged users (commenters) and the larger audience (upvoters) have different priorities, leading to tension between moderators and users\n- Rules and moderation decisions in subreddits can vary, with some subreddits becoming more restrictive in what is allowed to be posted\n- Some users believe that the upvoting and downvoting system works well in most subreddits, but not in subreddits that require high-quality content\n- The closure of subreddits and protests by moderators have received media attention, but it is unclear how representative the moderators' views are of the broader user base\n- The role of moderators on Reddit has evolved over time, with some users feeling that they have too much control over what content is allowed\n- The lack of transparency in moderation decisions and removal of posts has been a point of frustration for some users\n- There is a debate over whether Reddit should allow elections for moderators to give users more control over the platform.",
    "hn_summary": "- Reddit faced backlash and user revolt over changes to their API pricing for third-party app developers.\n- Many users and developers believe the changes were an attempt to kill off third-party apps and promote Reddit's own app.\n- The future of third-party apps and the old Reddit platform is uncertain, sparking concerns among users and developers."
  },
  {
    "id": 36310130,
    "timestamp": 1686663785,
    "title": "Rewriting the Ruby parser",
    "url": "https://railsatscale.com//2023-06-12-rewriting-the-ruby-parser/",
    "hn_url": "http://news.ycombinator.com/item?id=36310130",
    "content": "Rewriting the Ruby parser2023-06-12 \u2022 Kevin NewtonAt Shopify, we have spent the last year writing a new Ruby parser, which we\u2019ve called YARP (Yet Another Ruby Parser). As of the date of this post, YARP can parse a semantically equivalent syntax tree to Ruby 3.3 on every Ruby file in Shopify\u2019s main codebase, GitHub\u2019s main codebase, CRuby, and the 100 most popular gems downloaded from rubygems.org. We recently got approval to merge this work into CRuby, and are very excited to share our work with the community. This post will take you through the motivations behind this work, the way it was developed, and the path forward.If you\u2019re unfamiliar with the concept of parsers or how they apply to Ruby, there\u2019s a background section available at the bottom of this post that should get you up to speed.MotivationsThe current CRuby parser has a couple of long-standing issues that we wanted to address. Broadly these fall into four categories: maintainability, error tolerance, portability, and performance. We\u2019ll go into each of these in turn below.MaintainabilityMaintainability is almost entirely subjective, or at least is very difficult to measure. The overall concept can be broken down into many different facets, including but not limited to how easy it is to: read and understand the code, contribute to the code, change the code, document the code, and test the code.The current CRuby parser has no real documentation that we could find. There have been external projects that have attempted to document its design, notably the Ruby Hacking Guide from 2002 and Ruby Under A Microscope from 2013. Other than those decades-old efforts, the best chance you have is reading the 14 thousand-line parse.y file and trying to understand it. This is a daunting task to say the least, and one that we don\u2019t think anyone should have to do.Due to its complexity, the parser is also difficult to change. Consider bug #19392 from two months ago, when it was discovered that def test = puts(\"foo\") and puts(\"bar\") doesn\u2019t work at all like you would expect. Because of the way the parser is structured, it\u2019s not possible to fix this bug without breaking other code. This is a common theme in generated parsers, where seemingly simple changes can have far-reaching consequences.Looking at the contribution list, it\u2019s unsurprising to find that the existing parser can only be maintained by a couple of people. In the 25 years that the parser has existed, only 65 people have contributed to it, and only 13 of those have contributed more than 10 commits. In the last year, only 9 people have contributed to the parser, of which only 2 have contributed more than 10 commits.Maintainability is at the heart of open-source, and unfortunately the situation we find ourselves in is devoid of a maintainable parser.Error toleranceError tolerance is the ability of a parser to continue parsing a program even if it encounters syntax errors. In other words, an error-tolerant parser can still generate a syntax tree even in the presence of syntax errors.Error tolerance is important for a number of reasons. Editors, language servers, and type checkers like Sorbet or steep, rely on parsers to provide accurate metadata \u2014 types, arguments, scope, etc. \u2014 about the code being edited or analyzed. Without an error-tolerant parser, that metadata can get dumped at the first syntax error. This pushes the error-tolerance problem down to the consumers of the parser who have to try to reconcile their lack of metadata to get back to a stable state, which can be difficult and error-prone.As of Ruby 3.2, the CRuby parser has some minor error tolerance, but nothing that you would call a systemic approach. This means even the most trivial syntax errors result in the parser failing to generate a syntax tree. The downstream effects of this are that when you have multiple syntax errors in your file (usually because of copy-pasting) you end up having to fix them one at a time, which is a very slow process. These slower cycles can be frustrating, and stand in contrast to the ideals of \u201cdeveloper happiness\u201d that Ruby is known for.As an example, consider if your editor could only display one error at a time. Each time one is fixed, the next would appear. This can be time-consuming and frustrating for developers. Consider the following snippet:class Foo def initialize(a: 1, b = 2)  true && endThere are 3 syntax errors in the source above (the order of parameters, the missing expression on the right side of the &&, and the missing end keyword). Running this through ruby -c today (which checks for syntax errors) you get:test.rb: test.rb:2: syntax error, unexpected local variable or method (SyntaxError) def initialize(a: 1, b = 2)            ^test.rb:4: syntax error, unexpected `end' end ^~~This mentions the first issue, the second issue is confused for something else, and the third is missing entirely.Error tolerance is therefore something we wanted to bake into YARP from the beginning, accounting for it at every level of the design.PortabilityPortability refers to the ability to use the parser outside of the CRuby codebase. Currently, the parser is tightly tied to CRuby internals, requiring data structures and functions only available in the CRuby codebase. This makes it impossible to use in other tooling.Accordingly, the community fractured and developed multiple solutions, each with their own issues. Over the years there have been many other parsers written, almost all by taking the grammar file and generating a new kind of parser. In our research, we found parsers written in 9 different languages. Some of these made their way into academic papers, otherwise into production systems. As of writing, we know of 12 that are being actively maintained (6 runtimes, 6 tools):CRubymrubyJRubyTruffleRubyrurubynatalieRipperparserruby_parsertree-sitter-rubySorbetlib-ruby-parserEach of these parsers besides the reference implementation have their own issues. This means that each of the tools built on these parsers therefore inherit those same issues. The fracture therefore spreads into tooling. For example, some tools are based on Ripper, including Syntax Tree, rubyfmt, rufo, syntax_suggest, and ruby-lsp. Even more are based on the parser gem, including rubocop, standard, unparser, ruby-next, solargraph, and steep. Even more are based on the ruby_parser gem, such as debride, flay, flog, and fasterer.Clearly this is far from optimal. Every time new syntax is introduced into Ruby, all of the parsers have to update. This means opportunities to introduce bugs, which all get flushed down to their corresponding tools. As an example, Ruby 2.7 was released 4 years ago, and it came along with pattern matching syntax. Of the 10 non-CRuby parsers, only 5 of them support all of pattern matching to this day, and only 2 of them without any caveats.To keep up to date with the CRuby parser, every one of these parsers must carefully watch for any changes to parse.y and attempt to replicate them in their own language/runtime. This is a massive amount of work for a significant number of people who could instead all be helping maintain and improve a single parser instead.Portability also has to do with the usability of your syntax tree. Even if you can extract the syntax tree from the parser, if your syntax tree is too tightly tied to your runtime, it\u2019s not portable. We\u2019ll revisit this topic later when we discuss the design of YARP\u2019s tree.PerformanceOver the years, processors and C compilers have gotten much better using a couple of techniques. These include pipelining, inlining functions, and branch prediction. Unfortunately, the parsers generated by most parser generators make it difficult for any of these techniques to apply. Most generated parsers operate with a combination of jump tables and gotos, rendering some of the more advanced optimization techniques impotent. Because of this, generated parsers have a maximum performance cliff that is extremely difficult to overcome without significant effort.DevelopmentWith those problems and motivations in mind, last May we sat down and started designing solutions. It became clear pretty quickly that while a full-scale rewrite was a daunting task, it would be necessary to address all of the issues we had identified. So, we sat down to design what would become Yet Another Ruby Parser.DesignInitially we created a design document for the project, which you can still find. We shared this document internally before also going to discuss with Matz and the CRuby team, as well as JRuby, TruffleRuby, and maintainers of as many tooling gems as we could find (notably including parser and irb).Some of the more important design decisions that came out of these discussions are included below. Once Matz and the CRuby team were happy with the design, agreed on the approach, and determined that they would merge YARP in when it was ready, the work began in earnest.LanguageThe parser would be written in C. While there was some lively debate about the implementation language, we ended up settling on C. Other options that were considered included C++ and Rust with various interop options (even WASM cross-compilation). There ended up being two compelling reasons that settled the decision. The first is technical: the parser should be able to target any platform that has a C compiler. The second is human: the Ruby parser is going to be maintained by the CRuby team which is a group of C developers. Since one of our main stated goals is maintainability and these are the people that will be maintaining it, it made sense to use the language they were most comfortable with.StructureThe parser would be a hand-written recursive descent parser. This follows the trend of most major programming languages. Of the top 10 languages used by developers, 7/10 of them are hand-written recursive descent. Many tools have undergone the same switch from Bison to hand-written, for example gcc and golang. You can also find reasons why C# decided to go with this approach.The three exceptions of the languages that don\u2019t use hand-written recursive descent are Python, PHP, and Ruby. PHP and Ruby currently use Bison, whereas Python also recently switched to another flavor of recursive descent called PEG parsing. For more on that, see PEP-617. That article is particularly interesting in that it outlines some of the ambiguities in the grammar that you have to work around in the same way we had to historically work around them in Ruby. As an example they cite that in the below snippet:with (  open(\"a_really_long_foo\") as foo,  open(\"a_really_long_baz\") as baz,  open(\"a_really_long_bar\") as bar):it\u2019s actually impossible to express this grammar for context managers using LL(1) parsing (the style of parser they were generating) because the open parenthesis character is ambiguous in this context. To get around it they made their grammar more ambiguous and then enforced that the actual grammar was enforced in their tree builder.It\u2019s not entirely surprising that more established languages would move away from Bison. Bison is a tool meant to generate parsers for context-free grammars. These are classes of languages where each rule in the grammar can be reduced to a deterministic set of tokens. Ruby\u2019s grammar \u2014 as we saw with Python\u2019s \u2014 requires quite a bit of context to parse correctly, making it fall into the set of grammars labeled context-sensitive. To get Bison to generate a parser that can be used by CRuby, a lot of the context, logic, and state has been pushed into the lexer. This means you cannot accurately lex Ruby code without keeping the whole set of parsing state around.Laurence Tratt, a professor at King\u2019s College London has done extensive research into this area. His work was actually cited three times at Ruby Kaigi this year, in The future vision of Ruby Parser, Parsing RBS, and our own talk on YARP. In the first talk in which his work was cited, in the second paragraph he writes:It is possible to hand-craft error recovery algorithms for a specific language. These generally allow better recovery from errors, but are challenging to create.Then, in a blog post specifically about LR versus recursive descent parsing, he states:Existing languages have often evolved in a manner that makes it difficult, or impossible, to specify an LR grammar. There\u2019s no point in trying to fight this: just use recursive descent parsing.andIf you need the best possible performance or error recovery, recursive descent parsing is the best choice.The reality is, Ruby\u2019s grammar cannot be accurately parsed with an LR parser (the kind of parser that Bison generates) without significant state being stored in the lexer. Most of the programming community has come to the same conclusion about their own parsers and have therefore moved toward hand-written recursive descent parsers. It\u2019s time for Ruby to do the same.The last reason to switch to hand-written recursive descent actually comes from Matz himself. In version 0.95 of Ruby \u2014 released in 1995 \u2014 a small ToDo file was included in the repository. One of very few items in that file was:Hand written parser(recursive decent)API/ASTInitially, we had intended on keeping the same syntax tree as CRuby, to cause the least amount of disruption. However, after discussion with the various teams of both runtimes and tools, it was decided to design our own tree from the ground up. This tree would be designed to be easy to work with for both runtimes and tools. It would also be designed to be easy to maintain and extend going forward.The current tree in CRuby sometimes contains information that is irrelevant to consumers and sometimes is missing critical information. As an example, the concept of a vcall is a parser concern: it is an identifier that could be a local variable or a method call. However, this is resolved at parse time. It is still exposed in the Ripper API though, leading to confusion as to its meaning. Contrastingly, the tree is almost entirely missing column information, which is critical for usage in linters and editors.Along with the tree redesign, we worked closely with the JRuby and TruffleRuby teams to develop a serialization API that would allow for these runtimes to make a single FFI call and get back a serialized syntax tree. Once they have the serialized syntax tree, through our structured documentation they can generate Java classes to deserialize it into objects that they can use to build their own trees and intermediate representations.The tree redesign has ended up being one of the most important parts of the project. It has delivered something that Ruby has never had before: a standardized syntax tree. With a standard in place, the community can start to build a collective knowledge and language around how we discuss Ruby structure, and we can start to build tooling that can be used across all Ruby implementations. Going forward this can mean more cross-collaboration between tools (like Rubocop and Syntax Tree), maintainers, and contributors.BuildingWith the design in place, we went about implementing it. During implementation, it quickly became clear that the biggest hurdle was going to be a sufficiently extensive test suite. Since we had our own tree, it meant we couldn\u2019t test against any existing test suites. Fortunately, we implemented parity with the lexer output, so we could test to ensure the tokens that our parser produced matched the existing lexer. Using this approach, we incrementally made progress toward 100% parity in lexer output against the Shopify monolith. Once we hit that, we worked on ruby/ruby, rails/rails, and various other large codebases. Finally, we pulled down the top 100 most downloaded gems from rubygems.org.Along the way, we encountered all kinds of challenges, particularly related to the ambiguities in the grammar. If you\u2019re interested, they are a fun detour through some of the eccentricities of Ruby, detailed at the bottom of this post in the challenges section.MaintainabilityFrom the start we wanted to be focused on the problems we initially noted. The make this parser as maintainable as possible, every node in the tree is documented with examples and explicitly tested. You can find that documentation here. You can also find documentation for as much of the design as we could fit into markdown in here. Finally, there is copious inline comments to make it as maintainable as possible.Fortunately since open-sourcing the repository at the beginning of this year, we\u2019ve had 31 contributors add code to the parser. We\u2019ve been working to improve our contributing guidelines and guidance to make it even easier to contribute going forward.Error toleranceYARP includes a number of error tolerance features out of the box, and we are planning on adding many more in the months/years to come.Whenever source code is being edited, it almost always contains syntax errors until the developers gets to the end of the expression. As such, it\u2019s common for the underlying syntax tree to be missing tokens and nodes that it would otherwise have in a valid program. The first error tolerance feature that we built, therefore, is the ability to insert missing tokens. For example, if the parser encounters a missing end keyword where one was expected, it will automatically insert the missing token and continue parsing the program.YARP can also insert missing nodes in the syntax tree. For example, if the parser encounters an expression like 1 + without a right-hand side, it will insert a missing node for the right-hand side and continue parsing the program.Additionally, when YARP encounters a token in a context that it simply cannot understand, it skips past that token and attempts to continue parsing. This is useful when something gets copy-pasted and there is extra surrounding content that accidentally sneaks in.Finally, YARP includes a technique we\u2019re calling context-based recovery, which allows it to recover from syntax errors by analyzing the context in which the error occurred. This is similar to a method employed by Microsoft when they wrote their own PHP parser. For example, if the parser encounters:foo.bar(baz, qux1 + qux2 + qux3 +)it will insert a missing node into the + call on qux3, then bubble all of the way up to parsing the arguments because it knows that the ) character closes the argument list. At this point it will continue parsing as if there were nothing wrong with the arguments.Putting this all together, if we take our snippet from above again, you can see the red underlines that YARP will add through its language server to indicate the location of every error in the file:Going forward, there are many more techniques we\u2019d like to explore related to error tolerance, but we\u2019re happy with the state of the parser as it is today. If you\u2019d like to see it in action, YARP ships with a language server and VSCode plugin that you can use to try it out. You\u2019ll notice in the document describing how it works, that multiple syntax errors can be displayed in the editor at once, because of the existing error tolerance features.PortabilityYARP has no dependencies on external packages, functions, or structures. In other words it is entirely self-contained. It can be built on its own and used in any tooling that needs it. In languages with good FFI or bindgen support, this can mean directly accessing the parse function and its returned structures directly. Going forward, this means you could build Ruby tooling in languages like Rust or Zig with minimal effort.For languages without this support or for whom calling C functions can be expensive, we provide a separate serialization API. This API first parses the syntax tree into its internal structure, then serializes it to a binary format that can be read by the calling language/tool. This API was designed specifically with JRuby and TruffleRuby in mind, and members of those teams have been actively helping in its development.At this point JRuby has a functional prototype and TruffleRuby has merged YARP in and is actively working on making YARP its main parser. One interesting finding from this process was that YARP deserialization is around 10 times faster than parsing. Going forward, it\u2019s possible that TruffleRuby could to ship serialized versions of the standard library for faster boot speeds.With both the C and serialization APIs in place, we can now build standardized tooling that can be used across all Ruby implementations and as a community start to develop a common language around how we discuss Ruby syntax trees. Going forward this could potentially mean all of the tools mentioned above could be running on the same underlying parser.While we\u2019re very happy about the technical win that this represents, we\u2019re even more excited about the community win. With all of the excellent developers who have had to spend their time maintaining separate parsers now freed up, they can now invest that time in what makes their tools special. If they encounter errors with the parser, this means more eyes on the code, more people to help fix bugs, and more people to help add new features.PerformanceOnce the parser was able to produce semantically equivalent syntax trees, we began looking at performance. We don\u2019t have great comparison numbers yet because as discussed our tree is different and does more things in general (for example we provide unescaped versions of strings on our string nodes to make life easier on the consumers of YARP).What we can share so far is that YARP is able to parse around 50,000 of Shopify\u2019s Ruby files in about 4.49 seconds, with a peak memory footprint of 10.94 Mb. Needless to say, we\u2019re thrilled with these results so far.Going forward performance will be top of mind, and we have many optimizations we\u2019ve been experimenting with. These include reducing memory usage through specialized tree nodes, improved locality through arena allocation, and faster identifier resolution with more performant hash lookups.IntegrationOnce we got to a state where we could parse simple expressions, we wanted to validate our approach and design by integrating with other runtimes and tools.JRuby and TruffleRuby teams began experimenting with the serialization API, and we worked with them to make sure it was sufficient for their needs. With some interesting tweaks (serializing variable width integers, providing a constant pool, and other optimizations) we found a format that suited their needs. Both runtimes now have invested significant energy in integrating YARP into their runtimes, and Oracle has someone working full time on making YARP TruffleRuby\u2019s main parser.We also worked with other tools to validate that our tree contained enough metadata for static analysis and compilation. Syntax Tree is a syntax tree tool suite that can also be used as a formatter, and it has an experimental branch running with YARP as its parser instead of Ripper. Early results show that by replacing Ripper with YARP, in some cases performance increased by nearly two fold. We also built a VSCode plugin that you can find inside the repository to ensure that our error locations and messages were correct, and work continues on that today.Recently, we began experimenting with generating the same syntax tree as the parser and ruby_parser gems in order to seemlessly allow consumers of these libraries to benefit from the new parser. Early results are very promising and show both a reduction in memory and an increase in speed.Finally, in the last week we have begun work on mirroring YARP into the CRuby repository, building it within CRuby, and running it within the same test suite and continuous integration. This is the final step before merging YARP into CRuby, and we\u2019re very excited to see it come to fruition. This work will be done in the next couple of work days.Path forwardThis brings us to today and the path forward. Work continues on integrating YARP into all of the various Ruby runtimes, and we\u2019re excited to try it out on more projects going forward (for example mruby and Sorbet). We\u2019ll continue to work on speed, memory consumption, and accuracy. Matz and the CRuby team have agreed to ship YARP as a library with Ruby 3.3 (to be released this December), so in the next version of Ruby you will be able to require \"yarp\" and play around with your own syntax trees. A couple of things that will happen in the meantime before that exciting release:We will likely release the project as a gem, so that third-parties can begin working with it and integrating it into their own projects.We\u2019ll continue to work with the JRuby and TruffleRuby teams to ensure that the structure of the syntax tree and the serialization API are sufficient for their needs. Hopefully soon we\u2019ll get a release of these language runtimes that includes YARP as their main parser.Syntax Tree is going to adopt YARP as its main parser, which in turn means that ruby-lsp will reap all the benefits.We\u2019ll continue to improve our compatibility with Ripper so that libraries that rely on that (admittedly unstable) API can use our compatibility layer as a means of migrating.A lot more work is planned for the parser itself once it\u2019s merged into CRuby. This includes, but is certainly not limited to:Forward scanning error tolerance - in places where the parser encounters syntax errors that could be interpreted in multiple ways, one approach is to parse with all possible interpretations forward by some number of tokens and then to accept the path that yields the least number of subsequent syntax errorsArena allocation - currently nodes are allocated with individual malloc calls, which can be expensive and lead to fragmentation/a lack of memory localityMemory usage - in general we have kept the tree relatively small in memory, but there is always room to take out any redundant information or generally reduce the size of the tree in memoryPerformance - obviously this is a massive topic, but now that we have reached parity with CRuby, we can start to look at ways to improve performanceWrapping upOverall, we\u2019re very excited about this work and the future of Ruby tooling that it implies. We can\u2019t wait to see what you build with it! If you have any questions this didn\u2019t answer or are interested in contributing, please reach out to us on GitHub or Twitter!ExtrasFor those of you that may want even more background or details, we\u2019ve included some extra information below.BackgroundA parser is the part of a programming language that reads source code and converts it into a format that can be understood by the runtime. At the high level, this involves creating a tree structure that represents the flow of the program. When you\u2019re looking at source code you can often see this tree structure in the indentation of the code. For example, in the following code snippet:def foo barenda def would be the top level node, containing various attributes like foo as a name. That node would have a statements as a child, which is a list of statements inside its body. The first statement would be a call node with bar as the method name.The parser\u2019s responsibility is to create these nodes and build the tree structure before handing it off to other parts of the programming language for execution. In the case of CRuby, the parser is responsible for generating the syntax tree that is then handed off to the YARV (Yet Another Ruby Virtual Machine) virtual machine for compilation. Once compiled, the generated bytecode is what is used for execution.The first step to generating the tree is to break the source code into individual tokens, a process aptly called tokenization. In the case of Ruby, this means finding things like operators (~, +, **, ..., etc.), keywords (do, for, BEGIN, __FILE__, etc.), numbers (1, 0b01, 5.5e-5, etc.), and more. These tokens are evaluated lazily since they need large amounts of context to determine what they are (an identifier like foo can be a bare method call, a local variable, or sometimes even a symbol). You can think of this as a stream of tokens that the parser can pull from as it needs them.The second step to generating the tree is to analyze the tokens by applying a grammar. A grammar is a set of rules that define how the tokens can be combined to form a valid program. For example, the grammar might say that a program can be a list of statements, and a statement can be a method definition, a method call, or a constant definition. The grammar can also specify the order in which the tokens can be combined. For example, a method definition can be a def keyword, followed by an identifier, followed by a list of arguments, followed by a body, followed by an end keyword. This is called a production rule.Once the grammar has been applied and all of the ambiguities resolved, the tree is finally built. This tree is then handed off to the virtual machine for compilation and execution.The parser that CRuby has used is generated by a tool called Bison, a parser generator that generates LR (left-to-right, rightmost derivation) parsers. Bison accepts a grammar file (in the CRuby codebase this is parse.y) and generates a parser in C (parse.c). Importantly, Bison requires the token stream we mentioned earlier. There are tools to generate these token streams, but CRuby has used a hand-written lexer. This lexer is responsible for tokenizing the source code and then providing the tokens to Bison as it needs them (through a function called yylex).ChallengesOperators/keywordsThe * operator can sometimes mean multiply and can sometimes mean splat, and sometimes it comes down to the number of spaces between the operator and the operand. Similarly ... can sometimes mean range and sometimes mean forward arguments. The do keyword can be used in a number of different contexts, including blocks (foo do end), lambdas (-> do end), and loops (while foo do end). Determining which operator or keyword to select depends on a number of different factors, none of which are documented.TerminatorsIn Ruby, expressions can be separated by newlines, comments, or semicolons in almost all contexts, but not all. Lots of state is tracked to determine if a newline should be ignored or not. For example, in{ bar: 1 }the newlines are ignored and the 1 is associated with the bar: label, but indef foo bar: 1endthe newline after bar: is not ignored and the 1 is the only statement in the foo method.Local variablesBecause you can have method calls without parentheses, it can be difficult to determine if a given identifier is a local variable or a method call. For example,a /b#/can be interpreted as a method call to a with a regular expression argument, or as a local variable a divided by b. It depends on if a is a local variable or not (For more eccentricities like this, see a fascinating tric entry from 2022). Because of this ambiguity, a Ruby parser needs to perform local variable resolution as it is parsing.Regular expressionsYou would imagine that regular expressions would be easy to parse, because you can simply skip to the terminator. However, the terminator can be one of many characters. For example, you can write %r{foo}. In this case it\u2019s not hard because you can find the next }, but unfortunately regular expressions (like the other % literals) actually balance their terminators. This means that %r{foo {}} is a valid regular expression because the parser keeps track of the number of { and } characters it has seen.Regular expressions are also complicated by the fact that they can introduce local variables into the current scope. For example, /(?<foo>.*)/ =~ bar introduces a foo local variable into the current scope that contains a string matching the named capture group. This, combined with the local variable complexity above, meant that we additionally had to ship a regular expression parser in order to properly parse Ruby. (CRuby embeds the Onigmo parser which it happily delegates this work to, but again we didn\u2019t want to ship with any external dependencies).EncodingCRuby by default assumes your source file is encoded using UTF-8 encoding, but you can change that by adding a magic comment to the top of the file. The parser is responsible for understanding those magic comments and then switching to using the new encoding for all subsequent identifiers. This is important for determining, for example, if something is a constant or a local which is encoding-dependent.CRuby actually ships with 90 encodings (as of 3.3) that are both not dummy encodings and are \u201cASCII compatible\u201d which means they can be used as an option for encoding source files. YARP ships with the most popular 23 of those encodings, with plans to support more as needed.",
    "summary": "- Shopify has developed a new Ruby parser called YARP that can parse Ruby files in various codebases and gems.\n- The motivation behind developing YARP is to address long-standing issues with the current CRuby parser, such as maintainability, error tolerance, portability, and performance.\n- YARP aims to improve maintainability by providing better documentation, making it easier to read, understand, contribute to, change, and test the code.\n- Error tolerance is important because it allows the parser to continue parsing a program even if it encounters syntax errors, providing more accurate metadata and improving developer productivity.\n- The portability of YARP allows it to be used outside of the CRuby codebase and enables the development of standardized tooling that can be used across different Ruby implementations.\n- Performance improvements are being worked on, and initial results show that YARP can parse a large number of Ruby files in a relatively short time with minimal memory usage.\n- YARP will be integrated into various Ruby runtimes and tools, such as JRuby and TruffleRuby, and will be shipped as a library with Ruby 3.3.\n- The development of YARP has already attracted contributions from many developers, leading to a stronger and more collaborative community.\n- The future path for YARP includes further improving error tolerance, enhancing performance, expanding compatibility with existing tools, and merging it into the CRuby repository.",
    "hn_title": "Rewriting the Ruby parser",
    "original_title": "Rewriting the Ruby parser",
    "score": 471,
    "hn_content": "- The Ruby parser is being rewritten to improve performance and address longstanding issues.\n- The use case of using Ruby as a DSL to generate code for a distributed system is referenced.\n- Some users are skeptical but the progress so far is promising.\n- The parser rewrite is a significant undertaking that involves working with a large codebase and ensuring compatibility with existing tools.\n- The improvements to the parser will benefit development tools like Solargraph and Rubocop.\n- The new parser will generate a more accurate syntax tree, making it easier to analyze Ruby code.\n- The compatibility layer will allow existing tools and libraries to continue working with the new parser.\n- The work on the parser is part of an ongoing effort to maintain and improve the Ruby language and ecosystem.\n- The Ruby parser rewriten will be merged into CRuby soon.\n- The rewrite is expected to improve the performance and maintainability of Ruby applications.- Shopify has heavily invested in GraalVM Ruby (TruffleRuby), which is actively developed.\n- Shopify has been working on migrating to Graal/TruffleRuby for the past 10 years.\n- TruffleRuby was initially a research project and not suitable for production deployment, but it has become a viable option now.\n- Shopify has modified its CI system to support TruffleRuby and is actively running projects against it.\n- YARP (Yet Another Ruby Parser) will make adoption of TruffleRuby easier by eliminating compatibility issues with CRuby.\n- \"Booting\" in the context of a web app (Rails) means getting the app up and running.\n- There have been discussions about whether Shopify should create its own derivative of Ruby, like what Facebook did with PHP.\n- The engineering time spent on parsing Ruby code has been significant.\n- Working on the language implementation of Ruby can be seen as a dream job by some.\n- Reference to parsing error recovery is requested.\n- Shopify's work on improving Ruby is impressive.\n- The choice to stick with Ruby at Shopify may seem dogmatic, but it is a cost-effective option compared to rewriting everything in a different language.\n- Improving Ruby tooling, performance, and error reporting is an effort to avoid stagnation and outdatedness.\n- The collaboration between different Ruby implementations and the shared effort to create a common parser is unique.\n- YARP is expected to replace CRuby as the new \"standard\" in the future.\n- The rebuilding of the core parser and the upgrade of Rails at scale have been challenging processes for Shopify and other companies like Github.\n- The engineering overhead and capacity required to work on language improvements is not common among most tech companies.\n\nThe post showcases the ongoing development and adoption of GraalVM Ruby (TruffleRuby) at Shopify, the challenges and benefits of using Ruby, and the collaborative efforts within the Ruby community to enhance the language. It highlights the progress made in improving Ruby tooling, performance, and error reporting, and emphasizes the commitment of companies like Shopify towards continuously evolving and improving the ecosystem.",
    "hn_summary": "- The Ruby parser is being rewritten to improve performance and address longstanding issues.\n- The improvements to the parser will benefit development tools like Solargraph and Rubocop.\n- The work on the parser is part of an ongoing effort to maintain and improve the Ruby language and ecosystem."
  },
  {
    "id": 36315863,
    "timestamp": 1686685477,
    "title": "Reddit subs with millions of followers plan to extend the blackout indefinitely",
    "url": "https://www.theverge.com/2023/6/13/23759674/reddit-mods-blackout-protest-extended-indefinitely",
    "hn_url": "http://news.ycombinator.com/item?id=36315863",
    "content": "TECH/APPS/INTERNET CULTUREReddit communities with millions of followers plan to extend the blackout indefinitely/ \u2018More is needed for Reddit to act.\u2019By Jay Peters, a news editor who writes about technology, video games, and virtual worlds. He\u2019s submitted several accepted emoji proposals to the Unicode Consortium.Jun 13, 2023, 12:24 PM PDT|CommentsShare this storyIllustration: Alex Castro / The VergeModerators of many Reddit communities are pledging to keep their subreddits private or restricted indefinitely. For the vast majority of subreddits, the blackout to protest Reddit\u2019s expensive API pricing changes was expected to last from Monday until Wednesday. But in response to a Tuesday post on the r/ModCoord subreddit, users are chiming in to say that their subreddits will remain dark past that 48-hour window.\u201cReddit has budged microscopically,\u201d u/SpicyThunder335, a moderator for r/ModCoord, wrote in the post. They say that despite an announcement that access to a popular data-archiving tool for moderators would be restored, \u201cour core concerns still aren\u2019t satisfied, and these concessions came prior to the blackout start date; Reddit has been silent since it began.\u201d SpicyThunder335 also bolded a line from a Monday memo from CEO Steve Huffman obtained by The Verge \u2014 \u201clike all blowups on Reddit, this one will pass as well\u201d \u2014 and said that \u201cmore is needed for Reddit to act.\u201dScreenshot by Jay Peters / The VergeAhead of the Tuesday post, more than 300 subreddits had committed to staying dark indefinitely, SpicyThunder335 said. The list included some hugely popular subreddits, like r/aww (more than 34 million subscribers), r/music (more than 32 million subscribers), and r/videos (more than 26 million subscribers). Even r/nba committed to an indefinite timeframe at arguably the most important time of the NBA season. But SpicyThunder335 invited moderators to share pledges to keep the protests going, and the commitments are rolling in.RelatedReddit CEO tells employees that subreddit blackout \u2018will pass\u2019Apollo\u2019s Christian Selig explains his fight with Reddit \u2014 and why users revoltedSpicyThunder335 notes that not everyone will be able to go dark indefinitely for valid reasons. \u201cFor example, r/stopDrinking represents a valuable resource for a communities in need, and the urgency of getting the news of the ongoing war out to r/Ukraine obviously outweighs any of these concerns,\u201d SpicyThunder335 wrote. As an alternative, SpicyThunder335 recommended implementing a \u201cweekly gesture of support on \u2018Touch-Grass-Tuesdays,\u2019\u201d which would be left up to the discretion of individual communities. SpicyThunder335 also acknowledged that some subreddits would need to poll their users to make sure they\u2019re on board.Reddit spokesperson Tim Rathschmidt declined to comment. As of this writing, more than 8,400 subreddits have gone private or into a restricted mode. The blackouts caused Reddit to briefly crash on Monday.Update June 13th, 4:51PM ET: Reddit declined to comment.Most PopularReddit CEO tells employees that subreddit blackout \u2018will pass\u2019Reddit crashed because of the growing subreddit blackoutIn the bid to grow at all costs, Instant Pot is cooking itselfRead the new Twitter CEO\u2019s first email to employeesGoogle is getting a lot worse because of the Reddit blackouts",
    "summary": "- Moderators of many popular Reddit communities are extending the blackout indefinitely to protest Reddit's expensive API pricing changes. This means that these subreddits will remain private or restricted for an unknown period.\n- Despite some concessions made by Reddit, moderators feel that their core concerns have not been addressed and they are demanding more action from Reddit.\n- Over 300 subreddits, including popular ones like r/aww, r/music, and r/videos, have committed to an indefinite blackout, showing the widespread support for the cause.",
    "hn_title": "Reddit subs with millions of followers plan to extend the blackout indefinitely",
    "original_title": "Reddit subs with millions of followers plan to extend the blackout indefinitely",
    "score": 422,
    "hn_content": "- Several subreddits with millions of followers are planning to extend their blackout indefinitely as a protest against recent changes by Reddit.\n- The blackout is aimed at pressuring Reddit to address issues such as API pricing and third-party app policies.\n- Users and moderators are expressing frustration with Reddit's lack of transparency and communication regarding these changes.\n- The blackout is impacting user activity and revenue for the platform, potentially affecting Reddit's IPO plans.\n- There are concerns about the potential replacement of moderators by Reddit \"super mods\" and the impact on community culture and engagement.\n- Some users are finding alternative platforms to fill the void left by the Reddit blackout.\n- The blackout highlights the important role that moderators and user-generated content play on platforms like Reddit.\n- There is speculation about Reddit's future and the potential for other platforms to emerge as alternatives to Reddit.- Reddit is considering taking over key subreddits and replacing the moderators with their own.\n- The community moderators of subreddits like r/aww and r/movies play a crucial role in maintaining the quality of content and engagement.\n- There are concerns that replacing these moderators with new ones could lead to a decline in content quality and community engagement.\n- Paid moderators may not have the same level of expertise and dedication as the current volunteer moderators, which could negatively impact the subreddits.\n- The potential takeover of subreddits by Reddit could have legal implications and may result in lawsuits.\n- The use of AI for content moderation is being considered, but it may not be able to effectively handle tasks that require human discernment, such as responding to comments and removing spam.\n- There is a need for alternative platforms to Reddit that can provide a similar level of community engagement for niche interests.\n- The future of Reddit and its role as a platform for tailored content could be at risk if the current issues with moderation and community management are not addressed.",
    "hn_summary": "- Several subreddits are extending their blackout indefinitely in protest against recent changes by Reddit.\n- The blackout aims to pressure Reddit to address issues such as API pricing and third-party app policies.\n- Reddit's lack of transparency and communication regarding these changes is frustrating users and moderators."
  },
  {
    "id": 36313967,
    "timestamp": 1686678884,
    "title": "Private equity is buying everything from vet offices to tech conglomerates",
    "url": "https://www.theverge.com/23758492/private-equity-brendan-ballou-plunder-finance-doj",
    "hn_url": "http://news.ycombinator.com/item?id=36313967",
    "content": "DECODERPrivate equity bought out your doctor and bankrupted Toys\u201cR\u201dUs \u2014 here\u2019s why that mattersAuthor and federal prosecutor Brendan Ballou explains why private equity is buying everything from vet offices to tech conglomerates, how this system is broken, and what can be done to fix it.By Nilay Patel, editor-in-chief of the Verge, host of the Decoder podcast, and co-host of The Vergecast.Jun 13, 2023, 7:30 AM PDT|CommentsShare this storyIf you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.Photo illustration by Will Joel / The VergeBrendan Ballou is the author of Plunder: Private Equity\u2019s Plan to Pillage America. Brendan is also a federal prosecutor, and served as special counsel for private equity in the antitrust division at the Department of Justice. (Although, he will be the first to tell you, the book does not reflect the views of the DOJ.)Now, the idea behind private equity or PE is simple: a private equity company gathers up a bunch of cash, raises some investor cash, and takes on a lot of debt to buy various companies, often taking them off the public stock market. Then, they usually install new management and embark on aggressive cost cutting and turnaround programs mostly because they have to pay down all that debt pretty fast. The company can then be sold or taken public again for a hefty profit. But don\u2019t worry \u2014 if it doesn\u2019t work out, the PE firms are extracting fees at every step of the process, so they get paid no matter what happens.In another world, these PE deals are just boring financing strategies or maybe the backbone of the occasional juicy corporate takeover story. In Decoder world, PE is everywhere. We\u2019ve spoken to James Daunt from Barnes & Noble, who was installed as CEO in a PE deal that has revitalized the bookstore chain, Jeremy Andrus, the CEO of Traeger Grills, who bought the company in partnership with a PE firm, and of course, Elon Musk bought Twitter and took it private.Since the modern PE industry kicked off in the 1980s, it\u2019s grown virtually unchecked, and as Brendan explains, that\u2019s had seriously negative consequences for all kinds of markets and consumers because PE firms have reshaped how business works.Private equity affects everything from the modern nursing home industry to the SolarWinds hack, one of the biggest hacks in US history.This is a wonky episode, but it\u2019s essential.This transcript has been lightly edited for clarity.Brendan Ballou, you are a federal prosecutor and you served as special counsel for private equity in the Antitrust Division at the Department of Justice. You\u2019re also the author of a new book called Plunder: Private Equity\u2019s Plan to Pillage America. I feel like those might be related ideas. Welcome to Decoder.Thank you so much for having me.The joke on our show is that this is a show about org charts. Fundamentally, it\u2019s a show about how companies are structured and how they work and how their structure connects to what they make and what their values are. Private equity companies are really opaque in that way. There are a handful of really big ones we\u2019ve all heard of. There are a few smaller ones. How does a typical PE company work? How is it structured?They surround us in a lot of ways. If you consider them together with their portfolio companies, Carlyle, KKR, and Blackstone would be the third-, fourth-, and fifth-largest employers in America. That order might not be exactly right, but they\u2019d be right behind Walmart and Amazon. And yet, most people, I would venture to say, have not heard of those three companies. And part of that is because when Carlyle buys a nursing home chain, they don\u2019t brand it a Carlyle company, or when Blackstone rents you a single-family house, it\u2019s not branded a Blackstone property. Generally, what happens is there is a smallish legal entity that is the private equity firm itself, which advises a series of funds. Carlyle will have Carlyle fund one, two, three, four, and so forth that have a lot of investors, often sovereign wealth funds, pension funds, and so forth.Those funds ultimately buy companies, whether it\u2019s nursing homes, single-family rentals, veterinary clinics, OB-GYN practices. The really interesting thing about that, and I think what drew me to this as a lawyer but also concerns me as a citizen, is that because of the layered ownership structure of private equity firms, oftentimes private equity firms have control of the companies they buy but very little responsibility when those companies do arguably illegal things.During the entire Twitter acquisition drama, a bunch of us on The Verge staff reread Barbarians at the Gate, which is the very famous 1980s book about KKR buying RJR Nabisco, maybe the classic example of a private equity deal that just went completely sideways in a million different ways. I highly encourage everyone to read it, just a classic of the \u201980s corporate raider genre. Those companies, they\u2019ve softened their image. The idea that they\u2019re \u201980s corporate raiders has gone away, and now they\u2019re seen as operators or stewards of capitalism in a way. How has that changed? Are we just not seeing what they really are? Or is there a meaningful difference from what was happening in the \u201980s to now?Listen to Decoder, a show hosted by The Verge\u2019s Nilay Patel about big ideas \u2014 and other problems. Subscribe here!The short answer on leveraged buyouts is no. It\u2019s the same basic idea, which is buy a company with a fair amount of debt, try to make some changes, and sell it for a profit. That basic business model is the same that it\u2019s always been. Private equity firms have very successfully rebranded themselves and pulled themselves up by their bootstraps in terms of reputation. And part of that is that private equity firms have just expanded far beyond private equity. For a company like Carlyle or Blackstone, a lot, perhaps a majority of its business, is now involved in private credit, insurance, real estate. In a lot of ways, private equity firms have replaced the investment banks of the Great Recession in terms of their importance to the financial operations. There\u2019s a quote that stuck with me when I was researching this. An analyst was saying, I\u2019m paraphrasing slightly, \u201cBlackstone reminds me of Goldman Sachs 10 years ago. Wherever something interesting is happening, that\u2019s where they are.\u201dI think when you\u2019re talking about the changing private equity reputation, I think it\u2019s because private equity, to overstate it a little bit, has become everything.I\u2019m really curious about that part because we spend a lot of time talking about venture capital on the show. Once you are actually operating a business, the big private equity firm has become a part of the puzzle in a way that is not only invisible to most people in America, but invisible even to the people who might be thinking about starting a business. You\u2019re going to get to a certain size, and a PE firm is going to show up with a significant interest in your business and change the way you operate or take your business over from you because that\u2019s also an exit that might seem attractive \u2014 even if it\u2019s not attractive for the long-term health of your company.It\u2019s really interesting. Securities lawyers who know more about this stuff than I do will quibble with some of this stuff. I\u2019m a humble antitrust lawyer. But part of the reason that happened is fewer companies started going public and instead, because of a series of deregulation that happened really over the past 20-some years, it\u2019s become a lot easier for companies to stay private \u2014 essentially solicit money on the private markets rather than go through the work of going public and having the disclosure obligations that come along with it. Private equity enters the picture in two ways. One is through a traditional leveraged buyout where they say, \u201cDecoder has really taken off. We want to take it to the next level. We\u2019re going to buy it from you.\u201d That\u2019s a traditional leveraged buyout, and maybe they improve it, maybe they don\u2019t. But also through the private credit market where they say, \u201cOkay, you guys want to grow a little bit. We know that you need a loan. We\u2019ll offer you some money. We\u2019ll treat it just as debt rather than as an equity stake.\u201dThe really interesting part about that is, as I understand it, the private credit market is just significantly less regulated, almost by definition, than the public market. Tthere are voices out there saying that private credit, in large part led by private equity firms, could lead to a bubble simply because of how little transparency there is in it.And that\u2019s when you hear about companies raising debt. This is the problem that\u2019s underneath all of that.Exactly.So the positive case for PE \u2014 and again, this is the case that goes all the way back to Barbarians at the Gate, we\u2019re talking about a 40-year argument we\u2019ve been having about private equity \u2014 is sometimes these companies are too bloated. They\u2019re sitting there in the public markets, they can\u2019t grow, they can\u2019t increase their stock price. They can exit, they can pay a lot of people out because private equity\u2019s going to buy the company. The PE company\u2019s going to come in, they\u2019re going to slash costs. Because they own a bunch of companies, they\u2019ll impose some operational efficiency, because they\u2019ve got great managers. Because they own a bunch of companies, they can simplify your supply chain, and then they can re-exit the company again: leaner, stronger, better, faster. And when it works, it works and when it doesn\u2019t, so it goes. But that\u2019s the positive case. Do you see that playing out? Or are they just basically raiding a bunch of companies, extracting the profits, and moving on?There\u2019s always going to be a role for capital to play in our economy. As long as businesses need to build factories and hire new workers, somebody\u2019s got to be willing to risk the money to help them do that. And to the extent that private equity firms are helping to do that, that\u2019s great. The challenge is that, one: private equity firms tend to invest for the short term. I always joke that if I was trying to maximize the investment on my house, I\u2019d redo the kitchen and add a new inset bookcase. If I was trying to maximize the money over the next week, I would burn it down and try to collect the insurance money. The timeframe that these firms are considering really changes whether they want to invest in research and development, whether they want to invest in employees, whether they want to invest in new output.\u201cWhen things go wrong, private equity firms are very rarely held responsible\u201dThat\u2019s one problem. The other is the debt issue. Private equity firms tend to buy businesses with debt that they are not responsible for but, instead, that the company they buy is responsible for. When KKR buys Toys\u201cR\u201dUs or when Carlyle buys the nursing home chain ManorCare, the debt is held by the company that they bought, and the companies need to spend an enormous amount of money servicing that debt. Toys\u201cR\u201dUs was spending as much money just servicing its debt as it was making in income at the time that it went bankrupt. That\u2019s often a drag on the businesses. The third, you\u2019re talking about the perspective of private equity firms that they get companies to be leaner and meaner. Somebody said that they\u2019re like heat-seeking missiles for profit. Fair enough. The challenge that you\u2019ve got is, when things go wrong, private equity firms are very rarely held responsible.And what that means is it tends to lead to risky short-term strategies that, if they blow up in somebody\u2019s face, it\u2019s not the face of the private equity firm. I think it\u2019s not that private equity leaders are more greedy or anything like that. It\u2019s that the legal structures that we\u2019ve got around it mean that they just have a different set of incentives.And those legal structures have changed. This is another theme that comes up in your book over and over again: that this actually has happened before. Laws were changed in the past to stop what you call money trusts. We relaxed those laws, we\u2019re right back here, and maybe we should tighten those laws up again or make other specific changes.It\u2019s really interesting. I think that there\u2019s a strong historical analogy to private equity firms in the 2020s, which is the trust of the 1920s and 1910s. Legally, the structure of a private equity firm is very similar to things that you might have heard about from the Gilded Age around the sugar trust, the oil trust, and the money trust. Then, there was a long period of dominance in our American economy and politics by the trust and the related interests. But ultimately, the populist and progressive movements constrained the powers of the trust, set out the baseline for our antitrust and securities laws that worked for three generations until the 1970s, and really built the foundation for a working economy in America. We\u2019ve got a lot of problems with the private equity business model today, but I think the past suggests that there might be a way to solve this.This show\u2019s about decisions. I ask every executive how they make decisions. There are a lot of themes in how executives answer that question on the show, but the theme they come back to all the time is that you need to know what you\u2019re doing, you need to think long term, and then, you should figure out what decisions are short term and make those as fast as you can. But you\u2019ve got to have your eye on the ball, and you\u2019ve got to have a long-term vision for what you\u2019re trying to accomplish.If you just go back and listen to every executive who\u2019s ever been on the show answer the \u201chow do you make decisions?\u201d question, it\u2019s some variation of \u201cI know what I\u2019m doing, and then I try to make decisions to get there as fast as I can.\u201d That absolutely cuts against what you say private equity companies do, which is that they\u2019re hyper short term. One of the arguments that I hear from PE companies is, \u201cWe\u2019re going to pull you out of the public market. We\u2019re going to pull you out of the quarter-by-quarter pressure so you can reset the company with a long-term vision and then do a good job.\u201d And when we\u2019ve had companies that are successful in PE deals come on the show, that\u2019s what they talk about. Barnes & Noble does that, Traeger did that: \u201cWe\u2019re going to reset the company, we\u2019re going to make it grow, then we\u2019re going to go back and become public again.\u201d Why do you think a bunch of executives who uniformly, as the conventional wisdom, know that you have to have a long-term vision, end up in a private equity situation where they\u2019re making ultra-short-term choices?Well, the money.I knew that was the answer, but I figured I had to ask the question.No, that\u2019s far too flip of an answer. I should say, there\u2019s a range of private equity firms. Some think short term, some think long term, and the longer term\u2013\u201cNot all private equity firms\u201d is what you\u2019re saying.Yes. Generally, the ones that are thinking longer term, being in the private markets can give executives the space they need to breathe, relieving them of the pressure of quarterly and annual earnings reports, and that can be really important. The challenge that we\u2019ve got is many, if not most, private equity firms have not structured themselves that way and are looking to make an exit in three or five or seven years. But it takes more than that to build a factory. It takes more than that to get a new and innovative product line going, and as a result, they just can\u2019t make those kinds of decisions.Let\u2019s talk about the ManorCare example for one second. You lead off the book with it. Carlyle buys a nursing home chain, they extract profit from it. Walk us through what happened there.Carlyle\u2019s a large private equity firm. It bought HCR ManorCare, which was once the second-largest nursing home chain in America and then executed a number of tactics that are pretty playbook in private equity land. They executed a sale-leaseback, which means they sold the underlying assets of the nursing home chain and had the chain lease it back for a quick hit of money, but now they\u2019ve got a long-term obligation. They executed what\u2019s called a dividend recapitalization, so ManorCare had to borrow money to pay Carlyle and the other investors a profit. Ultimately, staff needed to be laid off, complaints and health code violations spiked. Unsurprisingly, a resident of one of these facilities dies. Without sufficient staff, she has to go to the bathroom by herself. As alleged, she slips, hits her head, and ultimately dies. But when her family sues for wrongful death, Carlyle gets the case against it dismissed.And what it says to the court is, \u201cOh, no, we don\u2019t technically own ManorCare. Instead, we advise a series of funds whose limited partners through a series of shell companies ultimately own ManorCare.\u201d And that was enough to convince the judge to dismiss the case. What I think it illustrates is how a private equity firm can often control the operations of a company without ultimately being responsible for what happens at that company.This is one of the first claims you make in the book that stood out to me: private equity companies, when it comes down to it, are pretty bad operators of businesses. Which strikes me as a pretty huge claim, but also counterintuitive. This is a group of people that buy companies, they do whatever they\u2019re going to do, try to make them better, sharper companies. You\u2019d think they have at least some experience on a broad level of being able to look at a company\u2019s books and say, \u201cHere\u2019s what works here. Here\u2019s where the fat is, we\u2019re cutting it. Here\u2019s where your opportunity is, we\u2019re growing it. We\u2019re at least good at the business part of this. We might not know anything about running nursing homes or steel mills, but the abstract business logic you need to make a company leaner or more efficient, we\u2019re great at that. Bring us in for that.\u201d And your argument is that, actually, they\u2019re pretty bad at that.I don\u2019t want to make sweeping claims that every private equity firm is bad at operations. That\u2019s certainly not the case, and we\u2019ve got some examples of firms that invested for the long term and brought sector-specific knowledge that I think ultimately did a lot of good for the companies they bought. But the basic issue is if you look at the biographies of the people that run private equity firms \u2014 and this is no slight against them \u2014 generally, they\u2019re not folks with experience in engineering, product development, sales or marketing, logistics, or anything like that. They have experience in finance, and so the expertise that they bring to an acquisition is financial expertise. Just as to the extent that I would bring expertise to something, it would be legal expertise. You wouldn\u2019t want me running the company. The challenge that we\u2019ve got is, oftentimes, when private equity firms think that they do have operational experience, it sort of blows up in their face.The New York Times did a really fascinating profile of a series of private equity firms\u2019 acquisition of Payless shoe stores. And they made a number of operational mistakes like buying World Cup soccer sandals for countries that they didn\u2019t actually have stores in. They shut down a factory that did quality control checks on a Chinese production facility and ultimately they had so many bad shoes that it more than made up the savings in inventory that they had to throw away. That is just one example, and there are thousands of acquisitions that are out there. But the basic thing that\u2019s going on here is when private equity firms buy companies, because they have a financial background, generally the changes that they\u2019re trying to make are financial ones. Whether it\u2019s loading a company up with debt, spinning parts of the company off, or trying to combine companies in a roll-up. But that\u2019s different from experience in running a company.If these companies are bad at operating businesses, and they tend to break things apart and extract the value and run away leaving you in bankruptcy as they did with Toys\u201cR\u201dUs or whoever else, why do people make these deals?I think that private equity can be tremendously successful, certainly for the private equity firm, oftentimes for the limited partners that invest with them, and generally, for the executives of the companies that sell to the private equity firms. Whether it\u2019s a big company like Toys\u201cR\u201dUs that sells to KKR or a small one like a veterinary clinic or a dermatology clinic whose doctor wants to retire, selling to a private equity firm can make a lot of sense because you\u2019ll get a large payout because you\u2019ve got a lot of equity. The challenge that you\u2019ve got is for the people that remain. In the medical space, I just mentioned dermatology, the challenge of private equity ownership has become so pervasive that I look at job listings now, and they will literally say, \u201cWe need dermatologists at such and such practice,\u201d and they\u2019ll say in all caps, \u201cNOT PRIVATE EQUITY OWNED.\u201d So it\u2019s become a problem enough in the industry that people see this as a business model that should be avoided.I feel like I need to disclose that my sister\u2019s a dermatologist, and she has sold her practice to a PE roll-up. This explains your great skin.[Laughter]Why is this happening in these markets? Dermatology in particular, just candidly, watching my sister go through it, it seems like this entire market is being taken over by PE companies. They can\u2019t all exit the same way. They can\u2019t all just extract fees from the same client base in the same cities, and yet, they\u2019re all rushing into literally the same markets. Dermatologists and veterinarians and the other practices you mentioned are location-based. There are only so many that you need in a city, and they all seem to be rushing into the same markets. Why is that?I think it\u2019s two things. One is a lot of the acquisitions that we\u2019re just talking about are part of roll-up strategies. The idea here is, \u201cWe\u2019re going to buy all of the dermatology or many of the dermatology practices in a given geography.\u201d Potentially, that has efficiencies, you have the same back-office billing and so forth. But potentially, it\u2019s an opportunity to exert market power. If you control a significant percentage of the dermatologists in a given geography, you can raise prices, you can cut quality care because there\u2019s only so far people are going to be able to drive to get to the next location. Roll-up strategies are really common in the private equity playbook.\u201cPoorer customers don\u2019t have alternatives, so you can raise prices, [and] you can cut quality care\u201dI think the other thing to get to why certain businesses are attractive is that, obviously, private equity is expanding in a lot of different directions. There was $1.2 trillion in acquisitions in 2021 \u2014 so a not insignificant part of the US economy. But one of the surprising things that I saw in my research was that private equity firms often target businesses that service working-class people rather than rich people, which I\u2019ve always found very surprising. You\u2019d think if you\u2019re in the business of making money, you\u2019d go after the rich people. But it seems that the model is that often businesses that service working-class people are attractive because poorer customers don\u2019t have alternatives, so you can raise prices, you can cut quality care, whether it\u2019s in prison services or mobile homes, across a range of practices. You can do all these things essentially without consequence because your customers don\u2019t have an alternative.You were in the Antitrust Division at the Department of Justice. We\u2019ve talked a lot about antitrust law on this show. We\u2019ve talked a lot about the neo-Brandeisians, the hipster antitrust movement, getting away from the consumer welfare standard. This seems squarely outside the \u201cwe need to reform antitrust\u201d problem. You\u2019re going to buy all of the veterinarians in a city and raise prices, you don\u2019t need to reinvent antitrust law. Robert Bork would say, \u201cYou\u2019ve bought up all of the veterinarians in the city and raised prices. You\u2019ve literally harmed the consumer welfare. We should stop it.\u201dI think Robert Bork would find perhaps a reason even to justify that one, but\u2013Probably because he got hired by a private equity company.I can\u2019t get into too much of the details because of my work, but I will say one of the interesting challenges that you\u2019ve got with private equity roll-ups is when you\u2019re looking at antitrust potential cases, when it\u2019s one big company planning to buy another big competitor. I wouldn\u2019t say it\u2019s an easy case, but it\u2019s a straightforward one because it\u2019s a national or international market and you look at, \u201cHow are the market shares of these companies going to change?\u201d And then you hire some economists and try to figure out, \u201cIs that enough to raise prices?\u201d Here, because the acquisitions are much more distributed across geographies, and in antitrust you have to define a geographic and product market, it\u2019s actually a much larger case in some ways to go after these smaller acquisitions because you\u2019re not looking at a national market but many local ones. I think that these sorts of roll-ups are subject to the antitrust laws, but the antitrust cases, interestingly, are much more complicated.I don\u2019t want to push too hard on this because I know you want to keep some distance here, but I just have to say that sounds absolutely backward to me. We\u2019re going to try to block the AT&T-Time Warner merger, and we\u2019re going to do calculus in front of a judge to show that prices will go up or down 15 cents when it\u2019s obvious what the real problem is, but we can\u2019t say that. That was a very complicated, big deal that was a 50 / 50 shot because you had to make a bunch of complicated leaps about what AT&T\u2019s business logic was, which ultimately, made no sense and they sold it anyway.\u201cI\u2019m going to buy all the veterinarians in Chicago and then our plan to make that worth it is raise prices,\u201d is absolutely straightforward. I can explain that to anyone and say, \u201cDo you think you should allow this?\u201d And the answer, regardless of political belief, would be, \u201cThat seems bad.\u201dI can\u2019t get into the details here because of my work, but I will say I think that the leadership of the Department of Justice and the Federal Trade Commission are very attuned to those arguments and thinking about how antitrust laws can be used to help regular people, not these abstract, airy arguments. I\u2019ll also say, and I think that this is a really important point, that under the law, the federal government is not the only enforcer of the antitrust laws. Every state attorney general has the authority to enforce the Clayton Act. Individual litigants have the power to do that, too, so I think as public awareness is growing on these issues and people are understanding how these roll-ups are occurring, there are a lot of different levers people can push on to make action here.It seems like another piece of the puzzle here is just some of the tactics you\u2019ve described. We\u2019re going to go buy ManorCare, we\u2019re going to sell all of their real estate, make money on the sale, and have the company lease it back. They did that with Shopko, which was a big retailer in Wisconsin where I grew up. That kind of strategy seems very obvious. It seems to create the same exact kind of chaos for every company that undergoes those moves. Why is something like a leaseback arrangement so common, and why doesn\u2019t it seem to be as regulated as it should be, given its very obvious negative consequences?Just to set a baseline on the sorts of tactics that we\u2019re talking about, a sale-leaseback is where the private equity firm directs the company to sell its assets \u2014 the Shopko store, the factory, the hardware that it has \u2014 and then leases it back to itself. And that gives you a short burst of cash from the sale, but then you\u2019re shouldered with the ongoing obligation to pay back the things that you once owned. We also mentioned dividend recapitalizations, where a firm will direct a company to borrow money to pay itself a profit.These sorts of things have been on the books for a long time. I think private equity firms have been really smart about identifying the opportunities here and using them. Extracting from those two examples specifically, I think it\u2019s just important to note how effective private equity firms have been in protecting their legal advantages over the years. Private equity firms have donated something like $900 million since 1990 to federal candidates. They have a bench of employees that include former cabinet members, secretaries of state, treasury, defense, chairman of the FCC, SEC. When it comes to protecting preferred legal advantages, like the carried interest loophole, surprise medical billing, and so forth, I think that they\u2019ve just been really, really effective in a way that even other very powerful industries have not been.The healthcare industry and the farm industry lobbies quite a bit in this country, but they\u2019re villains. People can see what they\u2019re doing, they understand the experiences they\u2019re having. They understand that there\u2019s a different kind of experience you might have if you just go to Canada or something. The PE companies are invisible. You can\u2019t quite see how those rules affect regular people. Is that why they\u2019ve built such a political advantage here? Or is it just that they revolving-door every ex-government official onto the board of a PE company?\u201cPrivate equity is everywhere and nowhere\u201dI think it\u2019s a combination of those things. A lot of former government officials went to PE, so they\u2019ve got a friendly face when it comes to lobbying. I think people go to private equity in a way that they might be more reluctant to go to an oil and gas company or even a healthcare company or something like that. But I also think it goes directly to what you just said, which is that private equity is everywhere and nowhere in a lot of ways. And one of the things that I think PE firms have been most successful at is making their issues seem boring, when you talk about the carried interest loophole, your eyes start to glaze over it. It sounds so dull, and yet, it has such profound implications for the equity of our tax code.Or when you talk about surprise medical billing, those laws are the difference between a fifth of Americans carrying significant medical debt and not. It affects literally tens of millions of people\u2019s lives. But I think private equity firms have managed to seem very opaque, very dull, and almost prestigious in a way that I think a lot of other industries have not been able to do.I\u2019m going to just ask you a very silly first-year law student question. I was not a great law student, but\u2013Oh yeah, you were a lawyer.I wasn\u2019t great at that, either, but this thing you\u2019re talking about where the private equity companies actually operate the companies they\u2019re invested in through their funds, they direct their operations, they roll them up, they have preferred supplier agreements, I think you have the example of Talbots in the book where they forced them to change clothing suppliers and Talbots folded. It\u2019s just very obvious that you could go to a court and say, \u201cLook, get through the corporate veil\u201d \u2014 the law school term is \u201cpierce the corporate veil\u201d \u2014 \u201cand get to the people making the decisions.\u201d This whole show is about decisions. \u201cGet to them, we can see them, they\u2019re right there, and they bought this clothing retailer, and they own that T-shirt maker, and they\u2019re forcing this clothing retailer to switch from this vendor to the one they own, and people hate it.\u201d And it doesn\u2019t seem like that is easy or even possible.We\u2019re geeking out like first-year law students here, but it\u2019s really hard to hold the private equity firms responsible for their portfolio company\u2019s actions, and to put a pin in the doctrine that you just said, this is called \u201cpiercing the corporate veil.\u201d The legal doctrines around veil-piercing vary from state to state, but generally, it is extraordinarily hard to do that, in large part thanks to good litigating from defense side law firms. As a result, I don\u2019t want to overstate the case, but it\u2019s almost impossible for a private equity firm to be held legally responsible under common law veil-piercing arguments. And that might make sense for ordinary investors, like if Brendan or Nilay were invested in a given company through our Vanguard account, it wouldn\u2019t really make sense to sue us because we don\u2019t really control their operations. But if you control 51 percent of the stock, if you control 100 percent of the stock, then it seems plausible to me that you probably should be responsible for the actions of the company that you own.Is that argument winning anywhere?No. I\u2019m not sure the argument\u2019s even really being made at this point. I\u2019ve been researching on Westlaw, and I think that the cases that have been made have generally gone in favor of the investors, not the plaintiffs. I think we have an added wrinkle here in that, a lot of times, plaintiffs don\u2019t even know that the company\u2019s owned by a private equity firm. I went to this conference of lawyers that represent nursing home patients, and they were saying, \u201cIt\u2019s impossible for us to figure out who owns the nursing home.\u201d And they\u2019ll play very complicated games to put the assets in related shell companies so that even if you do win the case, you don\u2019t know how many assets there are for you to recover. So partly, it\u2019s a challenge with the law. It\u2019s the challenge of very sophisticated investors and companies structuring their businesses in a way that makes it even harder to figure out who\u2019s responsible.There\u2019s an interplay here with bankruptcy, which is a move that you say a lot of private equity companies use, and it\u2019s part of their playbook. And it just strikes me that your creditors are usually pretty good at getting their money back. If you\u2019re going to go into a company, load it up with debt, load it up with fees on servicing that debt, do a leaseback where now you\u2019re loading it up with even more fees and credit obligations, and then you\u2019re going to tank the company and walk having collected your bag, the creditors are going to say, \u201cWho is responsible for this disaster? We want our money back.\u201d Are they winning?I can\u2019t speak generally, but the examples that I\u2019ve been interested in have been when there have been purposeful bankruptcies by private equity firms. And I can tell one story: Friendly\u2019s, which was a dining chain in the Northeast. Sun Capital, the private equity firm, bought it and then did all the tactics that we were talking about earlier. They executed a sale-leaseback, they did a dividend recapitalization, had layoffs, ultimately pushed the company into bankruptcy. They were the equity owner of Friendly\u2019s, but they were also the largest creditor of Friendly\u2019s. And in the bankruptcy process, normally, the ownership sort of flips. So, as the largest owner and creditor, Sun Capital was able to sell Friendly\u2019s from itself to itself. Reading the court documents around this, there was a fascinating shadow play of describing this as a third party. It was like, \u201cNo, it\u2019s just a different fund of Sun Capital\u2019s.\u201d And it was not, I should be clear, fraud or deception or anything like that. It was just clever lawyering.But the reason that they do that is it allowed them to push off the company\u2019s pension funds from their books and onto the quasi-government agency, the Pension Benefit Guaranty Corporation. It meant that Sun Capital and Friendly\u2019s would no longer be responsible for the pension debts, the PBGC would. And that\u2019s one of the ways that they can use bankruptcy to their advantage. if you\u2019re both the owner and the largest creditor, you can push aside the other unsecured creditors, and that gives you another advantage here. So that\u2019s a long answer to your short question that I think private equity firms have proven extraordinarily adept at using the bankruptcy system as well to their advantage.You\u2019ve got another claim in the book that says, \u201cLook, the problem here is that judges and members of Congress and regulators, they just don\u2019t see how it works. All these moves are hyper-technical, hyper-financialized, and your average district court judge is just flabbergasted, \u201cWhat\u2019s going on?\u201d And saying, \u201cIt\u2019s capitalism at work.\u201d Is that changing? Is it that they\u2019ve just gamed it too hard, and they\u2019ve hired too many people under their boards of directors? How do you fix that kind of problem? We see it in tech all the time, every tech lawsuit is a disaster of misunderstandings. But you\u2019re saying here, it\u2019s happening on the business side as well.So fortunately, we have podcasts like Decoder to educate people on the basics of private equity and help people understand the problems that we\u2019ve got in the business model. Maybe I\u2019m showing my bias here as a bureaucrat, but I think that there are a lot of people in state, local, and federal government that want to do the right thing by ordinary people but don\u2019t understand the consequences of some of these actions. Just to go back to bankruptcy, there are really technical arguments around things like credit bidding and 363 sales that, really, judges just need to be made to understand the consequences of these things. But there\u2019s nobody on the other side helping them do that. I see the same thing happen in other agencies.It would be really helpful if there were ordinary people talking about, \u201cHere are the consequences of understaffing nursing facilities at private equity-owned nursing homes. Here\u2019s the consequence of private equity firms not having a fiduciary duty to their limited partners when pension funds make investments. Here\u2019s the challenge that we\u2019ve got when prison phone services charge exorbitant rates at the direction of their private equity owners.\u201d I think it\u2019s a project of helping people understand how private equity works and then how it impacts their life. If we can do that, there are a lot of different levers that we can move to have some change.I want to ask a few more broad questions. And then you end the book with a long list of very specific policy recommendations that I want to get into.Too long. Yes.We had Cory Doctorow on the show ages ago, and he\u2019s like, \u201cYou don\u2019t want to write the book that\u2019s 12 chapters of problems and one chapter of \u2018Call your representative,\u2019 you want to write the solutions.\u201d And you have a lot of solutions. There\u2019s this idea that, you\u2019ve got full hypercapitalist, laissez-faire, just go for it, the market will figure it out, and then you end up with a quasi-controlled market anyway because you have a handful of giant firms dominating entire sectors of the economy, and instead of having state control of it, you just have two guys.And that seems like a problem that we can see expressed right now. In the private markets, you\u2019ve got a handful of PE companies; in the public markets, you\u2019ve got a handful of index funds, and they might as well just be controlling the economy. That seems like a problem that should cut across party lines. We should have functional markets. Everyone agrees that competition is good for consumers and that having new firms enter lowers prices and makes better stuff. Do you see the political will to actually go and change it, or do you see people saying, \u201cThis is basically hypercapitalism, it\u2019s the thing that we want\u201d on whatever side of the aisle you\u2019re on?\u201cIt is not capitalism; it is a perversion or an aberration of capitalism\u201dIt\u2019s been really fun talking about this book with people because I was doing an interview with a fairly conservative Republican at the beginning of the week, I was doing an interview with some Democratic Socialists of America-adjacent folks in the middle of the week, and I said to them both, \u201cWherever you are on the spectrum, from true free market capitalist to Harringtonian socialist, the basic problem of the private equity business model should concern you\u201d because it is not capitalism; it is a perversion of capitalism or an aberration of capitalism. And so, I think that there should be a bipartisan cross-ideological consensus on this, which is: if you want a healthy economy that works for people and works for the long run, we need to change the incentives of private equity.Now, can that be done? I have to say, I am really encouraged by what\u2019s happening around private equity in specific industries. I just mentioned prison phone services where PE has been very active. There have been a number of activist organizations that have had extraordinary success in limiting the rates that need to be paid by prisoners to make phone calls. At one point, it was as high as $25 for a 15-minute call. This is activism that was happening at the local, state, and now, the federal level. Legislation was just passed in Congress last year to address this. Similar movements are happening around nursing homes. I think there can be real movement on this if you focus on a specific issue or a specific industry. And I think that because we\u2019ve seen it happenThe elephant in the room when you come on a podcast like Decoder is the tech industry, which has upended most of American life. It\u2019s mostly a bunch of companies that were started fairly recently, they have not been colonized by PE, maybe they\u2019ve taken some money. But on a show like Decoder, you show up and say, \u201cLook, Shopko was killed by private equity.\u201d I think most of my listeners are saying, \u201cNo, they were killed by Amazon. Toys\u201cR\u201dUs, they just blew it, they just forgot to make a website that worked and figure out the logistics of delivery. And Amazon did a better job of that because they were the scrappy challenger brand that disrupted that industry.\u201dWhat is the interplay there? Because I really do think most listeners of this show have come along for the ride. Most listeners of this show are pretty interested in being competitors, and they\u2019re going to say, \u201cWell, look, that\u2019s a problem for the dead companies. We\u2019re going to start new companies that do a better job and win.\u201d That just seems like a pretty big tension if you\u2019re describing an economy that\u2019s being taken over by PE.That\u2019s a great point, and I\u2019m always adamant about pushing back slightly on the story of Toys\u201cR\u201dUs. Toys\u201cR\u201dUs was profitable the last year before it declared bankruptcy. The challenge was that it had so much debt that it was servicing that rather than being able to expand its operations, and it had advantages that Amazon didn\u2019t have in terms of physical stores. But that\u2019s a rant perhaps for a future podcast. In terms of, if you\u2019re in the tech industry, why should you care about private equity, two things. One is private equity has expanded into the tech industry in a lot of different ways. The most prominent example, unfortunately, is probably the purchase of SolarWinds, which was the cause of what was called the greatest hack in United States government history.Based solely on the reports filed with the SEC, it appears that SolarWinds moved its engineering work at least partly to Belarus after it was acquired by private equity firms. That\u2019s not necessarily the vector by which the attack happened, but if you care about long-term security, perhaps moving your development operations to a Russian ally is not the best move. So part of it is that private equity is moving into the tech industry, but the other part is what we need to be caring about, which is the overall vibrancy and competitiveness of the American economy. And one of the challenges that we\u2019ve got \u2014 America faces this every 30, 40 years \u2014 is we invent a broken business model. Twenty years ago, it was subprime lenders, 40 years ago, it was savings and loans, 60 years ago, it was conglomerates, 100 years ago, it was trusts. It\u2019s something that we keep doing, and it ultimately has a tremendous drag on economic performance. So if you care about the country growing and prospering, then the challenges that we face with the PE business model should probably concern you.Do you see a future where the Big Tech companies, which now control just massive swaths of the economy themselves and have enormous amounts of money in the bank, start to do things that the PE companies do? If you had Apple\u2019s amount of cash, the number of things that you can do to move the needle is vanishingly small, which is why they keep chasing after cars and healthcare. But it might be easier to buy nursing homes and run the PE playbook and achieve at least fiscal growth that way. I\u2019m not saying that comports with their values, particularly in Apple\u2019s case, but if you\u2019ve got a war chest like that, this is a set of options you might have on the table.Apple seems to have handled its excessive cash by doing stock buybacks. There\u2019s another way that you can use that money. Eventually, you\u2019ve got enough money sitting around and you need to figure out what to do with it. Private equity certainly seems like it would be very appealing. And I think that that should be concerning if you think that this is ultimately going to be a drain on the economy. But, Nilay, I wouldn\u2019t give them any ideas if I were you.In our world, here on Decoder, we mostly get startup CEOs or midsize company CEOs that are saying, \u201cThere\u2019s a kill zone around these companies, where, if we make a product that competes with them, they\u2019ll just bundle it into the operating system and my company\u2019s doomed.\u201d And that seems like one set of competitive problems. But the other set is they\u2019ll just buy everything. And stopping that does not seem to have worked very well for the DOJ or the FTC, at least until recently when there\u2019s been more of an effort to file the cases and lose, just to try to change the law. Do you see an interplay there?Maybe this isn\u2019t quite responsive to your question, but the other interesting possibility is that venture capital firms become more like PE firms. Andreessen Horowitz is just becoming such a massive entity that, eventually, they might be looking at more the leverage buyout model than the traditional \u201cbet and hope you win the lottery.\u201d But I get your point, which is there\u2019s so much money around tech right now that it\u2019s possible that the private equity business model essentially becomes inevitable to at least one of the players there.You have a lot of specific solutions at the end of the book. I want to talk about those. One of them is actually regulating roll-ups, regulating mergers, regulating combinations of firms. That does connect to the antitrust reform movement to say, \u201cWe should stop more of this. We should have more companies, not fewer. We should not allow companies to roll up every veterinarian in Chicago.\u201d Is that possible? That seems, at once, the easiest thing to suggest to just the random person and also, the hardest thing to achieve.I obviously can\u2019t go too far into specifics here. But I will say the basic laws of antitrust apply to everybody, whether it\u2019s a gigantic multibillion-dollar acquisition or if it\u2019s a small purchase of an OB-GYN or veterinary or dermatology clinic. These laws apply no matter what the size if the effect is, to quote the relevant statute, to substantially lessen competition. I think that there is a real possibility here. Obviously, you have the physical constraints of the Department of Justice and FTC, which is that DOJ antitrust staffing levels are the same as they were in the 1970s. Fortunately, there\u2019s been some legislation to change that, but there\u2019s a limited number of cases that can be taken on. I think that the leadership of both the FTC and DOJ antitrust are very attuned to these sorts of issues.I\u2019ll also say, at the risk of being repetitive, that DOJ and FTC are not the only enforcers here. States, and especially private litigants, can take action here. Not to get too in the weeds, but one of the challenges that you\u2019ve got with private litigants is they just don\u2019t see where the money\u2019s going to come from in taking on these cases. Especially because these companies keep targeting poor and working-class people.Exactly. But I think that there\u2019s been really encouraging civic-minded litigation against private equity firms for some of their practices. The American Academy of Emergency Medicine sued a private equity firm and its physician staffing company for violating state corporate practice medicine laws, saying, \u201cHey, you guys are coming into emergency rooms and essentially telling us how to practice medicine. That violates state law.\u201d They ultimately lost that case, but\u2013I was reading the book, and I was struck by how many extremely sensible, well-meaning cases you discuss, and then at the end the caveat is: and then they lost the case. And it seems like a recurrent theme throughout this book: people have a great legal theory and, to the innocent bystander, the case makes a lot of sense, the problem seems obvious, and then they lose on a technicality of standing or corporate structure or funding sources. It\u2019s crazy technical legal defenses, not the substance or the merits of the case itself.Well, two things on that. One is, I don\u2019t want to armchair psychologize, but I do think it\u2019s often very hard for judges to rule against companies when those lawyers for those companies who make five times what the judge makes, who dress extremely well, who have an army of associates sitting on the back benches, say, \u201cHundreds of people are going to lose their jobs, the company\u2019s going to fall apart, and you\u2019re going to be held responsible.\u201d I think it\u2019s very hard for a judge to go against that, just as a human in thinking about their psychology. And the other thing is this has happened before. Before the trusts were ultimately constrained, we had what was called the Lochner era. You had an era of court opinions that was incredibly hostile to social progress, and ultimately that changed through judges retiring and judges changing their mind and also popular pressure.I think a lot of non-lawyers forget, and maybe a lot of lawyers forget that judges are human beings and are influenced as much by public understanding of these issues as anyone else is. So I would not give up on the judiciary quite yet.A recurring theme we have on this show and across The Verge is that, particularly our audience, a tech audience, wants to think of the legal system as a computer, like a deterministic system with inputs and outputs. And that\u2019s how many judges want to think about it. Famously, John Roberts said, \u201cI just call balls and strikes.\u201d That\u2019s a deterministic argument. I think that might be changing. We\u2019re in for an era of Gen Z judges, and I think it\u2019s going to be pretty wild.You have a lot of these specific prescriptions here for agencies. The Department of Health should require minimum staffing in nursing homes. Around prison phones, you say the FCC should regulate the cost of phone calls from prison. Those kinds of very specific regulations within specific businesses and specific industries, here are the rules \u2014 it seems politically easier to get them.It seems politically easier to defend them in a court and say, \u201cLook, here\u2019s all the harm that was caused. Here\u2019s the rule. The rule will stop the harm. Here\u2019s the authority that\u2019s vested in whatever part of the United States code that governs this agent... We\u2019re off to the races.\u201d And it\u2019s weird. It\u2019s easier for the government to make some rule that\u2019s like, \u201cAll the doorbells should be blue\u201d than it is for the government to say, \u201cWe should have a bunch of competition and actually have a market that decides what color the doorbell should be.\u201d Why do you think that? Because that\u2019s the one where, when you talk to the conservative Republican, they should see that protecting the market keeps the government regulation off of them. But it seems like many of these firms would prefer, \u201cHere\u2019s one specific regulation at a time, like the cost of a phone call from prison approach\u201d than the market approach.\u201cIt\u2019s a basic idea of fairness: the people who make decisions should be responsible for those decisions\u201dI think it\u2019s a really good point about activism and organizing. I was getting dinner with an activist, she\u2019s in her 80s, and she was giving me good advice, and she said, \u201cIt\u2019s hard to start a movement if you don\u2019t have something to move.\u201d And I think I\u2019ve seen that happen with a lot of activists, they figure out something to move, and often it\u2019s easiest to do that around a very specific issue because you can personalize it, you can humanize it. It\u2019s harder to do that around abstract issues like \u201chow do we change corporate veil-piercing doctrine?\u201d I don\u2019t want to give up too easily on the idea of some of these bigger structural changes. In one way of talking about it, it\u2019s an obscure legal common law doctrine; on the other, it\u2019s a basic idea of fairness: the people who make decisions should be responsible for those decisions.And if an institution like Congress historically has struggled on some of these policy issues, there are a lot of different avenues here. States could be acting here. If you\u2019re a state legislator, a great thing to say would be, \u201cOkay, I\u2019m going to introduce a bill that says, \u2018If you\u2019re a company headquartered in our jurisdiction and a private equity firm buys you, lays off a bunch of people, the company goes bankrupt, the private equity firm should be liable to the losses of the workers.\u2019\u201d That is a pro-worker, pro-resident reform. It makes intuitive sense. I think that there is a way to have these big structural reforms. It might just not happen through the parts of the government that people are used to.We\u2019re coming up on time here. What\u2019s next in this movement? You\u2019re describing it as a movement. You do have this list of specific things to move. You have this notion that maybe it\u2019ll be more diffuse across states and maybe private litigants. What\u2019s the next turn here?Part of it is the work that you\u2019re doing, which is helping to educate people on private equity and the challenges with the business model. The other thing we need to do is empower the activist community here so that they can keep doing this work of both educating people and pushing elected officials and people in government. There are a number of organizations out there: Private Equity Stakeholder Project, Americans for Financial Reform, American Economic Liberties Project, to name just three. Empowering those groups to really help build a grassroots movement is going to be really important here because, I want to be clear, this is not a thing that we\u2019re going to solve in 18 or 24 months. This is the work of a generation to fix a flawed business model, just as it was the work of a generation to constrain the trusts. But if we did it once, we can potentially do it again.Gen Z judges, man. I\u2019m telling you, this is our future.I\u2019m excited.It\u2019s going to be amazing. The book is Plunder: Private Equity\u2019s Plan to Pillage America. It has one of the best opening sentences of all time, which is you mentioned that the font that the book is set in is licensed by a PE company. I literally just stopped and laughed at that. Thank you so much for coming on Decoder.I really appreciate it. Thank you.Decoder with Nilay Patel / A podcast about big ideas and other problemsSUBSCRIBE NOW!Most PopularReddit CEO tells employees that subreddit blackout \u2018will pass\u2019Reddit crashed because of the growing subreddit blackoutIn the bid to grow at all costs, Instant Pot is cooking itselfRead the new Twitter CEO\u2019s first email to employeesGoogle is getting a lot worse because of the Reddit blackouts",
    "summary": "- Private equity is the process of buying companies, making changes, and selling them for a profit.\n- Private equity firms have expanded into various industries, from nursing homes to tech, reshaping how businesses operate.\n- The private equity business model can have negative consequences, including job cuts, increased debt, and lack of accountability when things go wrong.",
    "hn_title": "Private equity is buying everything from vet offices to tech conglomerates",
    "original_title": "Private equity is buying everything from vet offices to tech conglomerates",
    "score": 403,
    "hn_content": "- Private equity firms are buying a wide range of businesses, from veterinary offices to tech companies.\n- Many young professionals, particularly those who value prestige, are gravitating towards careers in private equity.\n- Private equity firms often acquire companies and rely on administrative assistance and middle management, which allows them to acquire labor at a lower cost.\n- The competition for partnership within the private equity industry is skewed in the firm's favor, with highly motivated professionals competing to prove their worth.\n- Prestige plays a powerful role in motivating individuals to pursue careers in industries like private equity.\n- The acquisition of businesses by private equity firms can have negative consequences, such as cuts in jobs, changes in culture, and decreases in service quality.\n- The private equity industry is a complex and controversial space, with varying experiences and outcomes for companies and employees.\n- There is a need for solutions that support the transition of small businesses, like medical practices, that are being acquired by private equity firms.- Private equity firms are acquiring businesses and using risky financial techniques to generate profits.\n- These firms often saddle acquired companies with high levels of debt, which can lead to financial distress and bankruptcy.\n- Private equity buyouts can result in job losses, reduced quality of goods and services, and higher prices for consumers.\n- The private equity model incentivizes short-term financial gains at the expense of long-term business sustainability.\n- The influence of private equity firms extends beyond the business world, with connections to political figures and large campaign contributions.\n- Critics argue that private equity firms prioritize the interests of their investors over the well-being of the companies and communities they acquire.\n- There is a need for greater transparency and regulation of private equity practices to protect stakeholders and prevent abuses.",
    "hn_summary": "- Private equity firms are acquiring a wide range of businesses, including veterinary offices and tech companies.\n- Many young professionals are attracted to careers in private equity due to the perceived prestige.\n- Private equity firms often rely on administrative assistance and middle management to acquire labor at a lower cost.\n- Competition for partnership in private equity is intense, with professionals striving to prove their worth.\n- The acquisition of businesses by private equity can result in job cuts, changes in culture, and decreased service quality.\n- The private equity industry is complex and controversial, with varying experiences for companies and employees.\n- Private equity firms use risky financial techniques and high levels of debt, which can lead to financial distress and bankruptcy for acquired companies.\n- Private equity buyouts can result in job losses, reduced quality, and higher prices for consumers.\n- The private equity model prioritizes short-term financial gains over long-term sustainability.\n- Private equity firms have connections to political figures and make large campaign contributions.\n- Critics argue for greater transparency and regulation to protect stakeholders and prevent abuses."
  }
]
