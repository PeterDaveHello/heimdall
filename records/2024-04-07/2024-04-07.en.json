[
  {
    "id": 39954422,
    "title": "Generate Cartoon Faces with Faces.js",
    "originLink": "https://zengm.com/facesjs/",
    "originBody": "faces.js A JavaScript library for generating vector-based cartoon faces To load new random faces, click here or press \"r\" on your keyboard. faces.js is a JavaScript library that generates and displays cartoon faces, somewhat reminiscent of how the Nintendo Wii generates random Miis. Faces are drawn as scalable vector graphics (SVG). Each face can also be represented by a small JavaScript object, which allows you to store that object and then draw the same face again later. As you can probably tell, the number of options for each facial feature (eyes, nose, mouth, etc.) is fairly limited, and some of the current options are fairly crude. So fork it on GitHub and add some new options! Usage 1. Install from npm: $ npm install --save facesjs Or yarn: $ yarn add facesjs 2. Display a randomly-generated face (the size of the #my-div-id div determines the size of the displayed face): import { display, generate } from \"facesjs\"; // Generate a random face const face = generate(); // Display in a div with id \"my-div-id\" display(\"my-div-id\", face); More See all the available facial features in the faces.js editor. For more documentation and information (additional options, SVG export, CLI), see the README on GitHub.",
    "commentLink": "https://news.ycombinator.com/item?id=39954422",
    "commentBody": "Faces.js, a JavaScript library for generating vector-based cartoon faces (zengm.com)329 points by starkparker 15 hours agohidepastfavorite56 comments krebby 14 hours agoThis would be fun to use with Chernoff Faces https://en.m.wikipedia.org/wiki/Chernoff_face reply pavel_lishin 10 hours agoparentOoh, ooh, I get to be the one to make the Peter Watts reference today! Chernoff Faces make an appearance in Blindsight, where the ship captain Sarasti uses them. The specific details would be a spoiler, though. reply 082349872349872 53 minutes agorootparentIn this particular case, I think there are at least two nose shapes close enough to π/2 that Sarasti would find them disturbing. reply defrost 42 minutes agorootparentNever saw such a nose in my life 'Tis a nose of parchment It is six times as big but 'tis a nose like my nose https://www.youtube.com/watch?v=bNQKYolhZkg https://en.wikipedia.org/wiki/The_Kiss_and_Other_Movements reply winwang 13 hours agoparentprevThank you for evangelizing -- this is amazing and kinda hilarious. Does it count as biohacking? lol. reply supportengineer 8 hours agoparentprevImagine these on SRE dashboards to determine system state reply rented_mule 13 hours agoprevSomeone I know has done some work to parameterize facial expressions: https://www.redblobgames.com/x/1845-face-generator/ He has a bunch of other interesting things like this, too: https://www.redblobgames.com/ reply sabellito 11 hours agoparentTheir post on Polygonal Map Generation for Games [1], from 2010, got me into procedural generation in general. [1] http://www-cs-students.stanford.edu/~amitp/game-programming/... reply jlturner 12 hours agoparentprevThis is pretty common in 3D work. Blender has a feature called “blend shapes” that implements a similar interface, and is commonly used for complex facial animation and general model parameterization. reply Tijdreiziger 12 hours agorootparentDuolingo did a talk at their Duocon conference about how they use parametrization to animate the characters in their app. https://www.youtube.com/watch?v=fgOqvyPif3g (no affiliation) reply nmstoker 10 hours agoparentprevThat's cool - I had been thinking with the OP site that quite a number looked like they were scowling/angry so a way to vary that is interesting to explore. reply nilslice 3 hours agoprevObviously this also needs to be a library in Zig, Rust, Go, Elixir, Haskell, Java, C#, F#, OCaml, PHP, Python, Ruby, Perl, C, C++ & Lean So, it now lives as an Extism[0] wasm plugin you can call from those languages: https://modsurfer.dylibso.com/module?hash=2050e7f7a129a48df0... [0]: https://github.com/extism/extism reply mnahkies 2 hours agoparentInteresting, I hadn't come across extism before. How hard would it be to package https://github.com/biojppm/rapidyaml in this way? (And do you have a extism for dummies guide?) reply nilslice 1 hour agorootparentExtism can be really useful for packaging up and running cross-language libraries! The most clear information about it is at: https://extism.org, but its a bit focused on the primary use case for Extism, being a universal plugin system. There is a C PDK (https://github.com/extism/c-pdk) which you'd probably want to use in a new wrapper around your library in C++, and compile it to wasm32 freestanding or WASI, but without emscripten. Extism doesn't currently have an interop layer to emscripten. reply throwaway35777 14 hours agoprevimport { display, generate } from \"facesjs\"; // Generate a random face const face = generate(); // Display in a div with id \"my-div-id\" display(\"my-div-id\", face); Beautiful API. reply nox101 49 minutes agoparentNo, that's poor API. If it wants to generate HTML it should return it's root element. Let's say I want to add a random face to every div with class \"face\". With this API have to generate ids. With an API that returns a root element it's trivial and it's not duplicating work (inserting elements into the DOM) that already exists. Taking a selector would not be much better because it would again be duplicating work and make it harder to customize usage. Taking a parent element for it to insert into would also be a poor API because it makes it harder to insert next to a sibling. A good API would be one of const face = generate() someElem.appendChild(face); or const face = generate() someElem.appendChild(face.domElement); or const face = generate() someElem.appendChild(face.html()); // assumes it can render to something else too reply xg15 14 hours agoparentprevAs beautiful as the faces. reply throwaway35777 14 hours agorootparentThe faces are amazing! I hope if he finds an artist they don't get changed too much. I absolutely am looking for an excuse to use faces.js somewhere. reply rkagerer 10 hours agorootparentIt would be neat as a forum icon, if you got to pick one and tailor it. reply cmgriffing 13 hours agoprevReally cool. I would love to see an api for just passing a numeric seed value. Then users of an app can click a \"refresh\" button to get one they like that could persist across page loads and devices without having to store the entire face object in the db. reply two_handfuls 10 hours agoparentThe face object is small (it’s the parameters). You can save that. reply cypressious 12 hours agoparentprevYou could serialize the JS representation to Base64 or Base10 if you want it to be numeric. reply wongarsu 11 hours agorootparentBut that's a lot more data than just storing which of the ~2^50 possible faces was generated. You could serialize the entire face including the colors and scaling factors into a much smaller string, or just take one number that is used as a seed for a random number generator that sets the other parameters reply pests 7 hours agorootparentThe seed version with random parameters doesn't allow you to or your users to design their own. Just random selection until you get one you like, no editing of details. reply ezequiel-garzon 11 hours agoprevApologies for such a basic question. I have node installed on my machine, but clearly no idea how a JS library is included anymore (among other things...). I created an HTML file, included a div ``a div with id \"my-div-id\"`` as instructed, and even added type=\"module\" in the script tag, but I get in Chrome ``Uncaught TypeError: Failed to resolve module specifier \"facesjs\". Relative references must start with either \"/\", \"./\", or \"../\".`` Any pointers on all the steps to running this? Should I expect node to create a js file that could be served together with the HTML file? Or would my hosting server need to have node, and run it every time there is a request? Thanks in advance, I'm sorry for the confusion. reply tylerchilds 10 hours agoparentnot a basic question at all— front end has jumped the shark. you can bypass node, npm install, etc for a prototype entirely in an html file leveraging an import map. this is the importmap i load for all my pages: https://github.com/tylerchilds/plan98/blob/6120e6a80a3d48438... to support faces, i’d add an entry: “facesjs”: “https://esm.sh/facesjs@4.0.0” which should get rid of the import path issue, and to support legacy browsers that still might have that error: https://github.com/guybedford/es-module-shims reply scubbo 8 hours agorootparent> jumped the shark Genuine question - what does that phrase mean to you? I've only ever heard it in the context of television shows (meaning \"has been renewed for so many seasons that the writers have run out of ideas and have started forcing the characters into ever wackier situations to generate novelty\") - and while it's intuitivve to map from that meaning to a more generic \"has gone on for too long and thus become bad\", the translation is fascinating. When you said this, were you intentionally using a term from another domain with a different meaning in the confidence that your audience could translate it appropriately, or does the phrase \"jump the shark\" really just mean \"be old, and thus bad\" to you? (Asking from a purely non-prescriptivist non-judgemental perspective! I'm interested in understanding your thought process, not in judging or correcting you) reply Retr0id 8 hours agorootparentWhile I'm aware of the phrase's origins, I've never heard it used to describe a TV show, only an unrelated field just like in this example. reply tylerchilds 7 hours agorootparentprevhappy days was running out of ideas and wanted to continue to retain audience attention and had The Fonz water ski and jump over a shark. silicon valley was running out of ideas, but steve ballmer famously delivered the Developers, developers, developers speech paving the way for capturing developer market share, for a platform with platform developers is no platform at all. there’s tooling that solves technical problems and tooling that business problems. a system has “jumped the shark” when there are at least five ways to accomplish the same thing and the audience was just trying to have some happy days on the tubes. my core hunch is that the ecosystem is a compile target versus a respected platform and that’s where i believe the shark has been jumped. the web is cool, if only we stopped trying to jump it and befriend the shark already. the long answer is, i’ve got a bespoke markdown like syntax i’m using for both a web publishing and screenplay authoring. i feel no difference between telling stories in interactive web formats or printed pages. maybe i’m jumping the shark. reply tylerchilds 6 hours agorootparenti realize i’m making some claims here, so server side multi page demo: https://sillyz.computer/sagas/sillyz.computer/en-us/000-000.... if you press esc in the top left, you can play with that in the repl. if you visit: https://sillyz.computer, that same text based adventure wizard journey as an embedded widget is the lower post it note. this is the folder i’m using for localizing that journey: https://github.com/tylerchilds/plan98/tree/plan98/client/pub... any of the short hand web components dynamically load from: https://github.com/tylerchilds/plan98/tree/plan98/client/pub... full circle— i’m very much playing across domains with the jumping the shark reference reply philsnow 5 hours agorootparentprevI’ve never heard of importmaps before, thank you. reply Solvency 9 hours agorootparentprevThat is 10x more complicated and unorthodox than simply running \"$ npm install --save facesjs\". reply fzzzy 9 hours agorootparentThat's not enough to get it to load in a browser. Which bundler will you use? Shark jumped. reply Solvency 8 hours agorootparentOne command: npx. reply tylerchilds 8 hours agorootparentprevyeah, i thought about asking if OP was using vite, webpack, snowpack or babel, but based on the context of the question, it seemed like vanilla web, so i answered vanilla web. reply gkoberger 14 hours agoprevVery cool. Here's another style! https://getavataaars.com/ reply jonwinstanley 1 hour agoprevAre there any licensing restrictions to using this library and the images it creates? reply b0bb0 1 hour agoprevI know many around here don't love scarce digital assets but here is my take on procedural faces: https://regular.world/ My goal was to maximize variation across 10,000 faces and add warmth which is hard to do with cgi. reply tvink 1 hour agoparentI thought \"ah maybe they're saying scarce digital assets, because they don't wanna be conflated with NFT pyramid schemes\".. clicked link, and found fullblown ponzinomics with \"assets\" paying out tokens. And people wonder why most of us are tired of this stuff. reply agys 1 hour agoparentprevThose look like a total Fernando Botero rip-off! reply af3d 14 hours agoprevWell, I didn't find the art very appealing. But I do love the idea behind it. Neat project! reply outime 13 hours agoprevI liked it a lot, including the README. The author seems to be an indie dev who creates sport management sim games [1], all of which run entirely in the browser like the linked library (which is used in the games). [1] https://zengm.com/ reply criley2 9 hours agoparentNo wonder I instantly recognized these faces as Basketball GM's art! Dumbmatter has made some very cool things. reply daltonlp 11 hours agoprevIf you like that, you may also like: https://pixelfaces.io/ reply bitwize 24 minutes agoprevSpinnaker's Face Maker (https://www.youtube.com/watch?v=5K4ovkVLp8Y) for the modern era! reply xg15 14 hours agoprevHas a bit of a South Park and/or Futurama vibe to it, but why not? reply rasso 13 hours agoparentExactly, why not! Love it. reply SiempreViernes 12 hours agoprevAww, it's not making Chernoff faces :( (https://en.wikipedia.org/wiki/Chernoff_face) reply ramijames 13 hours agoprevThis is adorable. reply toisanji 13 hours agoprevvery cool. Are there libraries like these that do whole characters with bodies? Would love to play with those! reply Joel_Mckay 12 hours agoprevVery fun little project. Nice =) reply DyslexicAtheist 12 hours agoprevthey should have called it \"4 non blondes\" because there are no blonds reply Minor49er 8 hours agoparentUsing the \"faces.js editor\" link at the bottom of the page, it shows that you can use any color you want for the hair. Also, if it didn't allow blonds, it would probably just be called \"no blonds\" since there are more than 4 possible variants reply thatha7777 15 hours agoprevi love this reply jszymborski 11 hours agoprevWhen are the Faces.js NFTs going for sale? /s reply hacker_88 13 hours agoprev [–] Back in the day out used to be called NFT reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "'faces.js is a JavaScript library designed for creating and showcasing vector-based cartoon faces with a range of facial feature options.",
      "Users can expand the library's feature set by duplicating it on GitHub, integrating new options.",
      "Installation of faces.js is achievable via npm or yarn, enabling users to exhibit randomly-generated faces in their web projects."
    ],
    "commentSummary": [
      "The Hacker News thread focuses on Faces.js, a JavaScript library for cartoon faces, exploring connections to Chernoff Faces and applications like animation and procedural generation.",
      "Discussions cover the Extism plugin for using Faces.js across various programming languages, API enhancements for random face generation, seed values for persistence, HTML file library inclusion troubleshooting, and critiques of the Silicon Valley ecosystem.",
      "References in the conversation span digital face projects, Fernando Botero's art, Basketball GM, and the idea of selling NFTs."
    ],
    "points": 329,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1712428656
  },
  {
    "id": 39956455,
    "title": "Exploring the Depths of xz SSHD Backdoor",
    "originLink": "https://twitter.com/bl4sty/status/1776691497506623562",
    "originBody": "the xz sshd backdoor rabbithole goes quite a bit deeper. I was just able to trigger some harder to reach functionality of the backdoor. there&#39;s still more to explore.. 1/n pic.twitter.com/s1zJ8EBiMl— blasty (@bl4sty) April 6, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39956455",
    "commentBody": "The xz sshd backdoor rabbithole goes quite a bit deeper (twitter.com/bl4sty)303 points by nathell 11 hours agohidepastfavorite173 comments wilkystyle 10 hours agohttps://threadreaderapp.com/thread/1776691497506623562.html ufmace 9 hours agoprevThe weird thing about this one is how it seems super professional in some ways, and rather amateur in others. Professional in the sense of spending a long time building up an identity that seemed trustworthy enough to be made maintainer of an important package, of probably involving multiple people in social manipulation attacks, of not leaking the true identity and source of the attack, and the sophistication and obfuscation techniques used. Yet also a bit amateur-ish in the bugs and performance regressions that slipped out into production versions. I'm not saying it's amateur-ish to have bugs. I'm saying, if this was developed by a highly competent state-sponsored organization, you'd think they would have developed the actual exploit and tested it heavily behind closed doors, fixing all of the bugs and ensuring there were no suspicion-creating performance regressions before any of it was submitted into a public project. If there was no performance regression, much higher chance this never would have been discovered at all. reply orbital-decay 4 hours agoparentYou probably have too high expectations when you hear the \"state-sponsored\" part. Every large organization will inevitably end up like any other. They also have bureaucracy, deadlines, production cycle, poor communication between teams, the recent iOS \"maybe-a-backdoor\" story also shows that they don't always care about burning the vulnerabilities because they amassed a huge pile of them. reply amoss 1 hour agorootparentWelp. Ok, well now my newest worst nightmare is a jira board with tickets for \"Iran\" and \"North Korea\" stuck in the wrong column and late-night meetings with \"product\" about features. reply vinay_ys 47 minutes agorootparentprevstate-sponsored organizations in this space could be usually military-like organizational structure that has people with dedication and motivation unlike corporate workers. command structure can mean less bureaucracy (can also mean more bureaucracy too in some ways) when it is directly aligned with their mission. national patriotism motivations mean they are more dedicated and focused than a typical corporate worker. so yeah, there would be quality differences. reply orbital-decay 21 minutes agorootparentI would say you put too much faith into military-like organization as well. However, from what I can tell it's usually just ordinary security researchers and devs with dubious morals (some are probably even former cybercriminals) that usually don't even have the need-to-know and aren't necessarily aware of every single aspect of their work. The entire thing is likely compartmentalized to hell. You can't conjure quality from nothing (especially if it's pure patriotism/jingoism), large organizations are bound to work with mediocrities and dysfunctional processes, geniuses don't scale. (I feel like stating the obvious) reply varjag 44 minutes agorootparentprevPatriots for the most part suck at coding as much as everyone else. reply Jasper_ 4 hours agorootparentprevEh. Take a look at other state-sponsored attackers. We know they have 0-days for iOS, we know they've been used, but even Apple doesn't know what they are since they are so good at hiding their tracks. I don't think a state-sponsored attack would upload their payload to the git repo and tarball for all to stare at after it's been found out, which only took about a month. NSO Group built a turing-complete VM out of a use-after-free exploit in some JBIG2 decompression code. Uploading a payload to the world wide web and calling it bad-3-corrupt_lzma2.xz is clownshoes by comparison. My best guess as to what this is is an amateur hacker ring, possibly funded by a ransomware group. reply zvmaz 1 hour agorootparent> Uploading a payload to the world wide web and calling it bad-3-corrupt_lzma2.xz is clownshoes by comparison. While the world is trying to understand the backdoor, you sir decided that it's \"clowshoes\". I can only blindly defer to your expertise... \"Clownshoes, amateur, hacker ring, ransomeware group.\" Done. reply orbital-decay 3 hours agorootparentprevThat's why I mentioned the recent story. [0] [1] \"Apple doesn't know\" when the chain uses an internal backdoor in Apple hardware is a... stretch. And the chain gives the strong vibes of corporate-style development, with all its redundancy and mismatch between two parts. It's not alchemy, really. [0] https://securelist.com/operation-triangulation-the-last-hard... [1] https://news.ycombinator.com/item?id=38783112 - HN discussion reply ThePowerOfFuet 1 hour agorootparentRe the secret knock in the Apple silicon, a friend of mine once said \"that's how you lose the NOBUS on a backdoor\", and I think they were absolutely right. The one thing which most leads me to believe this was an intentional backdoor? The S-boxes. reply cesarb 56 minutes agorootparent> The one thing which most leads me to believe this was an intentional backdoor? The S-boxes. If you look at that HN discussion, you'll find a link to a Mastodon post from an Asahi Linux developer explaining that these \"S-boxes\" are actually an ECC calculation, and that the registers are probably cache debug registers, which allow writing directly to the cache bypassing the normal hardware ECC calculation, so you have to do the ECC calculation yourself if you don't want a hardware exception caused by an ECC mismatch on read (of course, when testing the cache, sometimes you do want to cause an ECC mismatch, to test the exception handling). reply orbital-decay 54 minutes agorootparentprevI don't believe it's intentional for the reason you mentioned. Although it could theoretically be like that for plausible deniability, Apple's reputation is definitely more valuable than one patchable backdoor of god knows how many others. But debug backdoor is still a backdoor. reply vinay_ys 44 minutes agorootparentVery large companies are definitely at the mercy of governments. Just look at how they are bending over backwards to comply with DMA etc. So, it is not at all inconceivable that they are forced to put backdoors into their product by the governments. reply dmitrygr 2 hours agorootparentprev> is clownshoes by comparison The quality of work you attract in part depends on how much you pay. Go check out how much is paid for a persistent iOS exploit, compared to a Linux user space exploit. From that, you may draw conclusions about their relative perceived difficulty and desirability. This will explain why iOS exploits are done more professionally. They are rarer, much better paid, and thus attract a better audience and more work on guarding them from discovery. reply cladopa 35 minutes agoparentprevYou probably have seen lots of films deificating state-sponsored organisations. SSO are made by humans under real constrainsts in time, money, personal. It is almost impossible to make something perfect that nobody in the world could detect. In the world there are people with different backgrounds that could use techniques that you never accounted for. Maybe is a technique used in biology to study DNA, or for counting electrons in a microscope or the background radiation in astronomy. For example, I have seen strange people like that reverse engineer encrypted chips that were \"impossible\" to in a very easy way because nobody expected it. They spend 10million dollars protecting something and someone removed the protection using $10. reply tw04 7 hours agoparentprevThe fact that the guys developing the code weren't also simultaneously running valgrind and watching performance isn't hard to believe. They were targeting servers and appliances, how many servers and appliances do you know of that are running valgrind in their default image? Sure, in hindsight that's a \"duh, why didn't we think of that\" - but also it's not very hard at all to see why they didn't think of that. They were likely testing against the system images they were hoping to compromise, not joe-schmoe developer's custom image. reply dralley 4 hours agorootparentIn theory they should probably be testing against the CI pipelines of Debian and Fedora / CentOS, as that's the moat their backdoor has to cross. reply tw04 3 hours agorootparentThey put code in to, in theory, avoid running in a development environment. reply iforgotpassword 6 minutes agoparentprevMy favorite theory is that Jia Tan is a troll. They tried some silly patches and were surprised they got accepted. What started as a little joke on the side because covid made you stay at home slowly spiraled into \"I wonder how far I can push this?\" Two years are enough to make yourself familiar with open ssh, ifuncs etc. Then you do silly things like \"hey um I need to replace the bad test data with newly generated data, this time using a fixed seed so that they are reproducible\", but you don't actually tell anyone the seed for the new data. Then you lol when that gets past the maintainers no questions asked. In the end they maybe just wanted to troll a coworker, like play some fart noises while they listen to music, and since they use Debian well, you better find a way to backdoor something in Debian to get into their machine. Like back in the day when sasser sabotaged half the internet and \"security experts\" said they have a plausible lead to Russia – which as is turned out was because said security experts ran strings on the binary and found Russian text – put there by the German teen who wrote sasser \"for teh lulz\". reply braiamp 9 hours agoparentprevThere are thousands of ways that performance can be impacted. No matter how good you are at developing, there will be a workload that would have a performance hit. Phoronix has been several times reporting issues to the Linux kernel because performance regression with their test suite. Performance tests tend to take more time than correctness tests. reply ufmace 9 hours agorootparentNot seeing that as a point. It's probably not possible to have no performance hit whatsoever when you're checking the exact nanosecond count of every little thing. But usually nobody is doing that. It shouldn't be hard to not cause a substantial enough performance regression in SSHD logins that somebody who wasn't already monitoring that would notice and decide to dig into what's going on. I'm not sure if it's been revealed yet what this thing actually does, but it seems like all it really needs to do is to check for some kind of special token in the auth request or check against another keypair. reply rgmerk 8 hours agoparentprevEvents can happen to anyone, even competent state-sponsored organisations. And intelligence agencies are sometimes rather less ruthlessly competent than imagined (Kremlin assisinations in the UK have been a comedy of errors [1]). Maybe another backdoor, or alternative access mechanism they were using, got closed and they wanted another one in a hurry. [1] https://en.wikipedia.org/wiki/Poisoning_of_Alexander_Litvine... reply cesarb 2 hours agorootparent> Maybe another backdoor, or alternative access mechanism they were using, got closed and they wanted another one in a hurry. Or maybe the opportunity window for the mechanism this backdoor would use was closing. According to the timeline at https://research.swtch.com/xz-timeline there was a github comment from poettering at the end of January which implied that the relevant library would be changed soon to lazy load liblzma (\"[...] Specifically the compression libs, i.e. libbz2, libz4 and suchlike at least. And also libpam. So expect more like this sooner or later.\"), as in fact happened a month later. The attacker had to get the backdoor into the targeted Linux distributions before the systemd change got into them. Of course, the attacker could instead take the loss and abandon the approach, but since they had written that amount of complex code, it probably felt hard to throw it all away. reply readyplayernull 6 hours agorootparentprevAnd they are getting better at it: https://www.bbc.com/news/world-us-canada-68706317 reply pflenker 4 hours agoparentprevWe could be faced with a form of Survivorship Bias here[0]. I find that thought rather chilling. [0] https://en.wikipedia.org/wiki/Survivorship_bias reply gitaarik 2 hours agorootparentWhich would mean that we have all kinds of active backdoors in our systems without us knowing it. But wouldn't they be detected at some point by someone? Or would they be silently removed again after some time so the attackers are not revealed? reply guenthert 33 minutes agorootparentAttacks on a given system or site can be expected to be removed (or auto-removed) after the operation ended, potentially w/o trace. But for supply-chain attacks there's always a history (if someone bothers to investigate). reply heyoni 9 hours agoparentprevI thought performance was actually fine? It only dragged when using valgrind, hence the rhetoric that it took some really unlikely circumstances for it to be detected that quickly. reply jwilk 2 hours agorootparentNo, it wasn't fine. From the advisory (https://news.ycombinator.com/item?id=39865810): > With the backdoored liblzma installed, logins via ssh become a lot slower. reply cykros 15 minutes agoparentprevStates have spent decades if not centuries building up espionage apparatus. All of the testing and development that goes into avoiding bugs has a tendency to run counter to the goals of secrecy or speed. Secret, fast, well tested. Choose 2. reply asveikau 6 hours agoparentprevModifying the sshd behavior without crashes seems by itself pretty difficult. I mean, conceptually it isn't hard, if you are in the same process and can patch various functions, but I think doing so and having it be \"production ready\" to ship in multiple linux distros all the time is a challenge. This thing wasn't around for very long but yet another thing to consider would be to keep it working across multiple versions of OpenSSH, libcrypto etc. reply nick238 6 hours agorootparentI picture some division in [nation-state] where they're constantly creating personas, slowly working all sorts of languishing open source packages with few maintainers (this is the actual hard, very slow part), then once they have a bit of an in, they could recruit more technical expertise. The division is run by some evil genius who knows this could pay off big, but others are skeptical, so their resources are pretty minimal. reply timschmidt 5 hours agorootparentMoxie's reasons for disallowing Signal distribution via F-droid always rang a little flat to me ( https://github.com/signalapp/Signal-Android/issues/127 ). Lots of chatter about the supposedly superior security model of Google Play Store, and as a result fewer eyes independently building and testing the Signal code base. Everyone is entitled to their opinions, but independent and reproducible builds seem like a net positive for everyone. Always struggled to understand releasing code as open source without taking advantage of the community's willingness to build and test. Looking at it in a new light after the XZ backdoor, and Jia Tan's interactions with other FOSS folk. reply jjav 2 hours agorootparent> supposedly superior security model of Google Play Store Let's never forget that the google play store requires giving google the ability to modify your app code in any way they want before making it available for download. Oh sure, that backdoor will never be abused. reply sethherr 4 hours agorootparentprevHe says the decision not to distribute prebuilt APKs is because: > if you aren't able to build TextSecure from source, you probably aren't capable of managing the risks associated with 3rd party sources. Which is a compelling argument from my perspective. I also think that people who can’t compile code should probably not root their phone. reply timschmidt 4 hours agorootparentThat seems like a great way to talk down to your end users, which seems like a security smell all by itself. Many users of F-Droid are technology professionals themselves and are quite aware of the security implications of the choices they make for the devices they own, and F-Droid is often a component of that outlook. Further, I don't think it applies to the F-Droid maintainers, who routinely build hundreds of different Android apps for all our benefit. They even directly addressed his concerns about the signing key(s) and other issues by improving F-Droid and met with continued rejection. reply asveikau 4 hours agorootparentprevI don't think we should assume a state actor. We don't know. It's kind of similar to stuxnet but attacking Linux distros is so broad and has such a huge risk of being exposed, as it was within a few weeks of deployment. A good nation state attack would put more effort into not being caught. But we don't know. So maybe I'm wrong. reply XorNot 1 hour agorootparentAssuming a state-actor is a cope though. It's looking at the problem and saying \"well we were fighting god himself, so really what could we have done?\" Whereas given the number of identities and time involved, the thing we really see is \"it took what, 2-3 or burner email accounts and a few dozen hours over 2 years to almost hack the world?\" The entire exploit was within the scope of capability of one guy. Telling ourselves \"nation-state\" is pretending there isn't a really serious problem. reply richin13 8 hours agoparentprevI read somewhere that some recent changes in systems would've made the backdoor useless so they had to rush out, which caused them to be reckless and get discovered reply sho_hn 7 hours agorootparentThis refers to the fact that systemd was planning to drop the dependency on liblzma (the conpression library installed by xz), and instead dlopen it at runtime when needed. Not for security reasons, but to avoid pulling the libs into initramfs images. The backdoor relies on sshd being patched to depend on libsystemd to call sd_notify(), which several distros had done. OpenSSH has since merged a new patch upstream that implements similar logic to sd_notify() in sshd itself to allow distros to drop that patch. So the attack surface of both sshd and libsystemd has since shrunk a bit. reply rav 5 hours agorootparent> The backdoor relies on sshd being patched to depend on libsystemd to call sd_notify I remember when we added sd_notify support to our services at work, I was wondering why one would pull in libsystemd as a dependency for this. I mean, there's a pure-Python library [1] that basically boils down to: import os, socket def notify(state=b\"READY=1\"): sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) addr = os.getenv('NOTIFY_SOCKET') if addr[0] == '@': addr = '\\0' + addr[1:] sock.connect(addr) sock.sendall(state) With proper error handling, that's about 50 lines of C code. I would vendor that into my application in a heartbeat. [1]: https://raw.githubusercontent.com/bb4242/sdnotify/master/sdn... reply bdd8f1df777b 1 hour agorootparent> With proper error handling, that's about 50 lines of C code. Writing proper error handling in C is a very tedious and error prone task. So it doesn't surprise me that people would rather call another library instead. reply TacticalCoder 45 minutes agorootparent> So it doesn't surprise me that people would rather call another library instead. Which shall be harder to justify now: \"You're calling a gigantic library full of potential security holes just to call one function, to save writing a few lines of code, are you trying to JIA TAN the project?\". reply cesarb 2 hours agorootparentprev> I was wondering why one would pull in libsystemd as a dependency for this. I mean, there's a pure-Python library [...] With proper error handling, that's about 50 lines of C code. There's also a pure C library (libsystemd itself) which already does all that, and you don't need to test all the error handling cases in your 50 lines of C code. It makes sense to use the battle-tested code, instead of writing your own. reply XorNot 1 hour agorootparentThe problem is people keep focusing on the libsystemd element because systemd has it's big hate-on crew and the vector was for what's deemed \"simple\". The better question though is...okay, what if the code involved was not simple? xz is a full compression algorithm, compressors have been exploit vectors for a while, so rolling your own is a terrifically bad idea in almost all cases. There's plenty of other more sophisticated libraries as well where you could've tried to pull the exact same trick - there's nothing about it being a \"simple\" inclusion in this case which implies vendoring or rolling your own is a good mitigation. The saying goes that everyone is always preparing to fight the last war, not the next (particularly relevant because adversaries are likely scouring OSS looking for other projects that might be amenable to this sort of attack - how many applications have network access these days? An RCE doesn't need to be in sshd). reply rlt 3 hours agoparentprevThis doesn’t seem too surprising to me. Any moderately competent developer could gain maintainership of a huge percentage of open source projects if they’re being paid to focus solely on that goal… after all, they’re competing mostly against devs working on it part side or as a hobby. If this is state sponsored, they likely have similar programs in a large number of other projects. reply manquer 7 hours agoparentprevWhy do we assume the person building the trust is the attacker ? Is not possible the attacker simply took over the account of some one genuinely getting involved in the community either hacked or just with $5 wrench and then committed the malicious code ? reply ericpruitt 6 hours agorootparent> Is not possible the attacker simply took over the account of some one genuinely getting involved in the community either hacked or just with $5 wrench and then committed the malicious code ? Given the behavior of the accounts that applied pressure on the original xz maintainer, this seems unlikely to me. reply ilvez 4 hours agorootparentOr they just bought the guy at one point, because I understood the malicious behaviour started quite recently. reply varjag 45 minutes agoparentprevDifferent departments. reply renk 2 hours agoparentprevThat performance regression could also be a way to identify compromised systems. reply Rygian 1 hour agorootparentIf all systems are compromised you don't need to identify anymore. reply renk 1 hour agorootparentWell, for now the payload delivery relies on things like x86, glibc.. still enough. reply bartimus 3 hours agoparentprevWhat's also surprising is how quickly the community seems to be giving someone the benefit of the doubt. A compromised maintainer would probably exactly introduce a fake member joining the project to make certain commits. They might have a contact providing the sophisticated backdoor that they need to (amateurishly) implement. reply est 6 hours agoparentprev> super professional in some ways, and rather amateur in others so this has to be a coordinaed teamwork instead of a single hacker, right? reply bee_rider 5 hours agorootparentIt could be somebody who was just good at some things and not at other things. reply badrabbit 1 hour agoparentprevThis tracks with other nation state sponsored attack patterns. I've had that same reaction before. Most APTs are like this but some Chinese,US and Russian APTs are so well funded, every aspect of their attacks is impressive. Many hackers who work for nation states also have side gigs as crimeware/ransomgang members or actual pentesting jobs. Reminds me of apt3/boyusec: https://www.bleepingcomputer.com/news/security/chinese-gover... It still boggles my mind that americans are against banning companies like huawei and bytedance. The MSS and PLA don't mess around. reply dabbledabble 8 hours agoparentprevIt’s also possible that this could be a change in personnel. Maybe the one who earned trust and took over was no more working for them. And an amateur took over with tight deadlines that lead to this gaffe for them. reply versteegen 7 hours agorootparentThe abrupt change in time-of-day when commits occurred supports the theory that Jai Tan is more than one person: https://twitter.com/birchb0y/status/1773871381890924872 reply swid 6 hours agorootparentThe text near the box makes it sound like these are just the fixes - not adding the test files but updating them. At that point it would have been clear “the race is on” to avoid detection, so it’s not too surprising someone would work late to salvage the operation. reply versteegen 16 minutes agorootparentWhoops, you're right. So this isn't really evidence of anything. Out of interest I looked up the other commit at that time of day visible in that graph, laying on the arrow. It's [1], which changes the git URL from git.tukaani.org to github.com. Of course, moving the project hosting to github was part of the attack. [1] https://git.tukaani.org/?p=xz.git;a=commitdiff;h=e1b1a9d6370... reply hinkley 4 hours agorootparentprevIf the mistakes align with the time of day change, perhaps the author had a distraction that pushed the hours and compromised judgement. reply AndyMcConachie 43 minutes agoparentprevAt this point we don't know who did this. It could have been a single really smart person, it could have been criminal, it could have been a state intelligence agency. We don't know shit. reply k8svet 5 hours agoparentprev(really tinfoil hatty - ) I almost wonder if it's misdirection? reply c6400sc 5 hours agorootparentOr a whitehat who couldn't get attention another way? reply atomicnumber3 10 hours agoprevThe sophistication here is really interesting. And it all got caught because of a fairly obvious perf regression. It reminds of a quote I heard in one of those \"real crime\" shows: \"There's a million ways to get caught for murder, and if you can think of half of them, you're a genius.\" reply rdtsc 9 hours agoparentI can believe it’s because it was a team behind the account. Someone developed the feature and another more careless or less experienced one integrated it. Another one possibly managing sock puppets and interacting in comments and PRs. reply akira2501 5 hours agorootparentI wonder what the web admin control panel for the \"fake human\" looks like, or if it even rises to that level of sophistication yet. reply cookiengineer 3 hours agorootparentIt's called AIMS (Advanced Impact Media Solutions) and is used by several state-level actors these days, both pro- and contra-NATO. Well, at least that one is the most sophisticated one on the market (as of now) and Team Jorge is probably making shitloads of money with it while not giving a damn about who uses their software in the end. reply xorvoid 9 hours agoparentprevMaybe I’m just being naive or too trusting, but this is sort of what I think when folks are getting worried about other backdoors like this in the wild. Is it that they just got unlucky to get caught, or is this type of attack just too hard to pull off in practice? I’d like to think the later. But, we really don’t know. reply breadwinner 8 hours agorootparentThey could have covered tracks better. So says Andres Freund, the person who discovered the backdoor: https://news.ycombinator.com/item?id=39923467 reply devcpp 4 hours agorootparentNote he's not a cybersecurity researcher, he's mostly a database engineer (a great one, making significant PGSQL contributions), so I'm not sure he's familiar with statistics and variety of backdoor attempts. reply ordu 9 hours agorootparentprevI feel the same way. It is too much complexity in one place, it couldn't work without hiccups. reply lyu07282 9 hours agorootparentprevOne measure might be that we never really found that many backdoors. Over time there is quite a large accumulation of hackers looking at the most mundane technical details. This may be confirmed by regular vulnerabilities that are found in sometimes many decades old software, since vulnerabilities are much harder to find than backdoors. For example shellshock was 30 year old code, PwnKit 12 and log4j was ~10 ish. So if backdoors were commonplace, we probably would've found more by now. Perhaps that's changing now, the xz backdoor will for sure attract many copycats. reply hinkley 4 hours agorootparentI’m not convinced that if I found a bug that I’d notice all the security implications of fixing it. Occasionally yes, but I wonder how many people have closed back doors just by fixing robustness issues and not appreciated how big of a bug they found. reply beeboobaa3 3 hours agorootparentSure, but this xz backdoor is far, far more involved than that. reply cjbprime 6 hours agorootparentprevDoesn't your data prove the opposite point? There are so many vulnerabilities and so few people looking for them that even the thirty year old ones have barely been found. A healthy feedback loop would have trended the average age of each vulnerability at the time of detection to be *short\". reply sjs382 8 hours agorootparentprev> Over time there is quite a large accumulation of hackers looking at the most mundane technical details. Are there though? Even if true, there are probably enough places with very few eyes on them. reply almostnormal 1 hour agorootparentMaybe something could be built to put more eyeballs on things. A kind of online-tool that collects the sources to build some relevant distributions, a web front-end to show a random piece of code (filtered by language, probability to show inreasing by less-recently/frequently/qualified viewed) to a volunteering visitor to review. The reviewer leaves a self assesment about their own skills (feed back into selection probability) and any potential findings. Tool-staff double-checks findings (so that the tool does not create too much noise) and forwards to the original authors (bugs) or elsewhere (backdoors). A bit like wikipedias show random page. reply INTPenis 3 hours agoparentprevGiven enough time and local testing they could have gotten away with it. I'm positive their deadline changed due to @teknoraver's patch in libsystemd. reply anamax 9 hours agoparentprev> \"There's a million ways to get caught for murder, and if you can think of half of them, you're a genius.\" Does \"think of half\" apply to the folks trying to solve murders? reply atomicnumber3 7 hours agorootparentNah, it applies to the person trying to get away with the murder. People will do really, really intricate jobs of trying to cover up, then slip up because like, they leave a receipt in their car that accidentally breaks their alibi. reply graemep 59 minutes agorootparentA clever murderer will disguise the murder as an accident, suicide or natural death. It will not even show in the stats as unsolved. I got the idea from fiction (specifically Dorothy Sayers), but the number of murders Harold Shipman committed before anyone even noticed makes it plausible that people with relevant expertise (doctors, pharmacists, cops, etc.) could easily get away with murder. If Shipman had stopped after the first 100 or so he would have. reply ball_of_lint 5 hours agorootparentprevEven if you can think of 10 relatively uncorrelated reasons, that lets you catch the genius murderer 1-(1/2^10) of the time, which is quite good. reply TheDudeMan 9 hours agoparentprevYet most murders go unsolved. reply TheBlight 8 hours agorootparentDepends on locale. In Germany something like 90% of murder cases are solved/cleared. In the U.S., I suspect a majority of the murders technically unsolved by police are cases where the identity of the perpetrators is somewhat of an open secret within communities that don't trust law enforcement (and LE similarly has little interest in working with them either.) reply kybernetyk 8 hours agorootparent>In Germany something like 90% of murder cases are solved You must watch out when reading the German crime statistics. \"Solved\" which is marked as \"aufgeklärt\" in those statistics just means that a suspect has been named. Not that someone actually did it/has been sentenced for the crime. >https://de.wikipedia.org/wiki/Aufkl%C3%A4rungsquote#Deutschl... 2nd sentence reply TheBlight 8 hours agorootparentIs it reasonable to assume a material number of cleared murders in Germany result in no charges and/or no conviction? (Genuinely curious.) reply ssl-3 9 hours agorootparentprevThen most murderers are geniuses. Or most murder investigations are (by definition) incompetent. Or (more likely): The old idiom quoted above is stupid and useless. (That it presumes that murdering and getting away with it is somehow a noble or esteemed deed should be damning enough.) reply graphe 8 hours agorootparentWrong. There’s no money or benefits in solving crimes. It could be done easily in many cases but nobody cares about certain people like gang members. Lots of cases where the murderer tells everyone but nobody cares. reply ssl-3 8 hours agorootparentWrong? Which part is wrong? Only 2/3 of these to choices can be wrong. The remaining one must be correct. reply withinboredom 5 hours agorootparentTechnically, all 3 could be wrong and an unknown 4th option could be correct. That seems to be what they are proposing here. In both cases, the premise is unclear so good luck! reply ssl-3 3 hours agorootparentEh, good call I guess. I didn't see that aspect. The 4th option they may appear to propose suggests that murder investigators don't get paid -- neither in money, nor in benefits. So, to that end: As far as I know, that's not usually the case with government employees, and it is always actionable when it does happen to be the case. reply dboreham 9 hours agoparentprev> And it all got caught because of a fairly obvious perf regression Always possible that was \"parallel construction\" evidence. Someone at a TLA discovered the attack by some other means, had a quiet Signal chat with a former colleague who works at MS... reply GrantMoyer 9 hours agorootparentIt seems like a much more suitable parallel construction story to invent in this instance would be something like \"there were valgrind issues reported, but I couldn't reproduce them, so I sanity checked the tarball was the same as the git source. It wasn't.\" reply rtpg 9 hours agorootparentprevWouldn't it have been easier to just have someone drive-by comment on the changes in the source tree in the comment? Like \"what's up with this?\" Though I guess you end up with some other questions if it's totally anonymous. But I often will do a quick look over commits of things that I upgrade (more for backwards compat questions than anything but) reply paleotrope 9 hours agorootparentprevInteresting possibilty but it seems like the \"discovery\" story is too complex and unbelievable. reply eli 9 hours agorootparentprevThere doesn’t seem to be any evidence to support this whatsoever yet it’s nearly impossible to disprove. Classic conspiracy theory. reply EvanAnderson 9 hours agoprevHas anybody done a writeup of the obfuscation in the backdoor itself (not the build script that installs it)? I threw the binary into Ghidra and looked thru the functions it found, but having no familiarity with the ifunc mechanism it uses to intercept execution I have up and set it aside for others. I'd have to assume since there's anti-debug functionality that the code is also obfuscated. Since it shipped as an opaque binary I assumed at least some of the code would be encrypted with keys we don't have (similar to parts of the STUXNET payload). reply bilekas 9 hours agoparentNo full dissemination of the backdoor itself has been done yet, as for the anti-debug, sure you can avoid things like that with flags. But this was done at compile level so its a bit more tricky. > I'd have to assume since there's anti-debug functionality that the code is also obfuscated. Not really, as above it was done at build time.. So you have already set your home up. It's shown the problems with package managers not taking source from the right place. reply EvanAnderson 8 hours agorootparentSo ifunc is a link-time thing and not a runtime thing, then? (My background, when it comes to linking, is DOS and Windows.) reply asveikau 3 hours agorootparentIt's runtime, but I think dynamic linker. The Windows equivalent would be if a library patched some code at dllmain. Actually the Detours library in the Windows world is similar. But it's for performance; the idea is you would patch some function references based on the CPU revision to get faster code specific to your CPU. reply bilekas 38 minutes agorootparentThis is a really nice windows analogy, however it goes without saying this package wasn't aiming for Windows, ironically, it chose the path (as we seen so far) of least resistance. If you're hooked to an sshd service, your golden. They put 5 checks (maybe comically) in a row to make sure it was linux I this case .. who knows what's next. reply bannable 8 hours agorootparentprevWhat do you mean by \"package managers not taking source from the right place\"? reply sho_hn 7 hours agorootparentI assume they are advocating for package managers to preferably grab signed git tags from repositories rather than download tarballs. The backdoor relied on the source in the tarballs being different from the git tag, adding additional script code. This is common for projects that uses GNU autotools as build system; maintainers traditionally run autoconf so that users don't have to and ship the results in the tarballs. I agree that this should be discouraged, and that distros should, when possible, at least verify that tarbal contents are reproducible / match git tags when importing new versions. reply bilekas 48 minutes agorootparentCorrect. The onus should be now be on the package delivery to provide transperant packages maybe? Maybe add the extra step of pulling instead of trusting the push from maintainers? It's just an extra step the might get more eyes. All said, even in hindsight I wouldn't have called this one out. reply 1vuio0pswjnm7 4 hours agoprevAlternative to thereaderapp.com: https://nitter.poast.org/bl4sty/status/1776691497506623562 reply jpalawaga 8 hours agoprevit's a rather good thing that this was found before it made it out broadly. Not just for obvious reason of not wanting an unknown party to have RCE on your infrastructure. I think as people keep digging they will eventually formulate a payload which will allow the backdoor to be used by anyone. As bad as it is for a single party to have access, it's much worse for any (every?) party to have access. reply justusthane 6 hours agoparentIsn’t that more or less impossible since the payload is a private RSA key? reply timschmidt 5 hours agorootparentSee https://en.wikipedia.org/wiki/Dual_EC_DRBG for another backdoor requiring a private key, in which the key was simply replaced in a subsequent supply chain attack(!) with a key known to the attacker: \"In December 2015, Juniper Networks announced[55] that some revisions of their ScreenOS firmware used Dual_EC_DRBG with the suspect P and Q points, creating a backdoor in their firewall. Originally it was supposed to use a Q point chosen by Juniper which may or may not have been generated in provably safe way. Dual_EC_DRBG was then used to seed ANSI X9.17 PRNG. This would have obfuscated the Dual_EC_DRBG output thus killing the backdoor. However, a \"bug\" in the code exposed the raw output of the Dual_EC_DRBG, hence compromising the security of the system. This backdoor was then backdoored itself by an unknown party which changed the Q point and some test vectors.[56][57][58] Allegations that the NSA had persistent backdoor access through Juniper firewalls had already been published in 2013 by Der Spiegel.[59] The kleptographic backdoor is an example of NSA's NOBUS policy, of having security holes that only they can exploit.\" reply noman-land 8 hours agoprevI haven't been following this story super closely but I find it extremely odd that I've heard zero discussion about the perpetrator of this hack. reply sofrimiento 13 minutes agoparentI would be interested in semantic analysis of the communication from the involved online personas, similar to what was done for Satoshi, to point to a cultural direction. Would also be interesting to see if there were semantic style differences over time pointing to different people acting as the personas. Since it would be quite a lot of code that has been committed as well, would also be interesting to see if code style differences could be found pointing to more people involved than only one. reply alecco 1 hour agoparentprev> However, I believe that he is actually from somewhere in the UTC+02 (winter)/UTC+03 (DST) timezone, which includes Eastern Europe (EET), but also Israel (IST) https://rheaeve.substack.com/p/xz-backdoor-times-damned-time... reply SAI_Peregrinus 7 hours agoparentprevThere's been lots of speculation. It seems likely to be more than one person, likely either a ransomware group or a national intelligence agency. reply db48x 7 hours agoparentprevWhat’s to discuss? Nothing is known about him. reply chrismartin 6 hours agorootparentThere's a lot of metadata about when/how they used git and IRC, and some preliminary analysis on same. Another surname in one of the commits. An apparent LinkedIn account. (See heading \"OSINT\" in https://boehs.org/node/everything-i-know-about-the-xz-backdo... .) A lot of these tracks could be intentionally manipulated by a sophisticated actor to disguise their identity, but it's not \"nothing\". reply db48x 4 hours agorootparentLike I said, we don't know anything worth having a real discussion about. Maybe he was in the +03 time zone, and pretending to be in +08, but that's not enough to base a discussion on. reply qarl 3 hours agorootparentYou're discussing it. reply coginthemachine 7 hours agoprevI'm concerned about the long game nature of things here. 1-Sure they bid their time to setup the \"infrastructure\" to create the backdoors. 2-I'm sure their plan was to play another long game after the exploit got in the wild, in production. It's the right way to spend lottery money. Invest. 3-That makes me wonder if such games are being played today. Scary. reply sureglymop 6 hours agoprevDoes anyone have a good explanation or introduction into the performance testing that was done to find this? And how to get started? Actually measuring performance always seemed to be a very hard task and I'd like to be able to do similar testing as the person which found this backdoor. reply cesarb 1 hour agoparentFrom what I understand, it wasn't even the performance testing itself that caught the backdoor. The developer wanted to have the machine as quiescent as possible (so that nothing else running on it would interfere) before starting the performance tests, but sshd was using much more CPU than expected (and this could be observed with simple tools like \"top\"). My guess is that the usual \"backscatter\" of password guessing ssh login attempts from all over the Internet normally uses very little CPU time in sshd before being rejected, but the backdoor made each login attempt use a significant amount of CPU time to check whether it was an encrypted request from the attacker (and this particular machine had its sshd open to the Internet because it was a cloud machine being accessed via ssh through the open Internet). reply glibg10b 2 hours agoparentprevhttps://www.openwall.com/lists/oss-security/2024/03/29/4 > == Observing Impact on openssh server == > > With the backdoored liblzma installed, logins via ssh become a lot slower. > > time ssh nonexistant@...alhost > > before: > nonexistant@...alhost: Permission denied (publickey). > > before: > real 0m0.299s > user 0m0.202s > sys 0m0.006s > > after: > nonexistant@...alhost: Permission denied (publickey). > > real 0m0.807s > user 0m0.202s > sys 0m0.006s reply intelVISA 2 hours agoparentprev> measuring performance always seemed to be a very hard task It's not hard: it's either easy, or impossible, depending on the culture at your shop. At the basic level it's trivial - you instrument code and interpret the results against the HW and lower level machine counters. Depending on $lang you have many good libraries for this kicking around, the hard part is working with a corp that values performance (99% do not) so none of the required mindset and surrounding infra will be setup to permit this in any useful capacity. reply dvektor 7 hours agoprevSomehow nobody has mentioned this yet but big props to the original author of this post. Super impressive work reply bilekas 9 hours agoprevWithout having a Twitter account I have a really hard time following these threads. Is there some write up? Edit : Check comments. Yes, the backdoor hasn't been decompiled/reverse engineered yet. But it feels like clickbait to say : \"It goes deeper\"... Obviously. Nobody knows what it fully does yet. There was no assumption of knowing what it did. reply aeyes 9 hours agoparentShort summary is that it allows auth bypass, not just RCE. reply mr_mitm 3 hours agorootparentRCE as root is already the worst case though. The auth bypass is basically just a convenience feature of the backdoor. So yeah it's mildly interesting but not really a new development of the story. reply bilekas 9 hours agorootparentprevYes, I understand.. It's just not a surprise. reply ckcheng 4 hours agorootparentIt was surprising because previously we had: 'XZ backdoor: \"It's RCE, not auth bypass, and gated/unreplayable.\"' [0]. [0]: https://news.ycombinator.com/item?id=39877267 (811 comments) reply bilekas 3 hours agorootparentDid you understand the XZ backdoor before the rest of us ? We will figure it out. With all of our stubborn ways, we can document it. Clap reply goalieca 9 hours agoprevThis is fantastic. Great work actually triggering the bug! reply m3kw9 5 hours agoprevEveryone now looking at their test data directory reply sylware 9 hours agoprevah god, another twitter account... reply DustinBrett 9 hours agoparentIt's as if it's not going away... reply MBCook 10 hours agoprevLuckily, thanks to Elon, we’ll never know since you haven’t have a Twitter account to view the thread. reply mkl 10 hours agoparentChange \"twitter\" to \"twiiit\" to get a random nitter instance: https://twiiit.com/bl4sty/status/1776691497506623562 reply opello 9 hours agorootparentAren't all the nitter instances going to die with the anonymous guest account restrictions? I've not closely followed those developments. reply justinclift 9 hours agorootparentprevOh. Nitter didn't stop working a few weeks ago after all? Oops, now that's giving: Instance has been rate limited. Use another instance or try again later. So maybe \"kind of working\" is the better description. :) reply jxyxfinite 9 hours agorootparentprevI thought nitter was officially dead. Nice to see some instances are still working reply ethanwillis 9 hours agorootparentprevNo thanks, I support an open web. reply beepbooptheory 9 hours agorootparentYou support the open web by stealing from the closed one! reply Rodeoclash 9 hours agorootparentI'm glad you get it! reply mkl 9 hours agorootparentprevIgnoring the closed web is not the same as supporting the open web. I support whatever mirrors and tools get closed knowledge into the open. (Edited to remove snark.) reply kibwen 9 hours agorootparent> Choosing ignorance over knowledge If there's some piece of knowledge that's absolutely, positively critical to my life, it will exist somewhere that actually matters, not on Twitter. reply mkl 9 hours agorootparentSure, but almost no knowledge that is interesting, valuable, useful, etc., is absolutely, positively critical to your life. Almost nothing on HN has that level of importance, but you are here learning interesting things, and unfortunately the first place some of those things appear is still Twitter. reply k8svet 8 hours agorootparentWhere does it end, fellow person? What is going to be the excuse/defense/workaround whenever Nitter instances are completely suffocated? Just suck it up and sign up so you can continue to participate on a increasingly hostile, toxic, manipulated platform in service of a narcasists deranged ego? Because Joe Bob Expert is too lazy to post elsewhere? No, I'm sorry, but when is enough, enough? I know I'm missing out on good content, and I don't care. I have _some_ self-respect. reply k8svet 9 hours agorootparentprevI don't even know if that's true, but I don't care. Twitter, at this point, is far more egregious than reddit, and I swore months ago I'd never contribute there. It blows my mind that people still play in Elon's piss-filled sandbox because they love the dopamine hits of bot-inflated engagement metrics. Yes, you casual reader that keeps posting on Twitter due to laziness and momemtum, I'm absolutely talking about you. Your laziness is hurting everyone. And I'm not alone, you're limiting your audience and prioritizing, well, people too lazy to get off Twitter, and ignoring the technical, prescient (observant, at this point?), informed crowd that have left for elsehwhere. /shrug reply ethanwillis 9 hours agorootparentprevAnd yet a short time later: \"Instance has been rate limited. Use another instance or try again later.\" reply publius_0xf3 4 hours agorootparentprevWow, it works. But it's not random, afaict. I keep getting the privacydev instance. How do they keep theirs up and running? reply avalys 9 hours agoparentprevHow much does a Twitter account cost? reply ethanwillis 9 hours agorootparentYour personal information. reply Brian_K_White 8 hours agorootparentprevapproximately twice as many principles as it's worth reply defrost 9 hours agorootparentprevDignity ... reply GaggiX 9 hours agorootparentprevProbably a bit of mental health. reply UncleOxidant 9 hours agoprev [18 more] This. Could people stop posting xitter links and post threadreaderapp links like this instead. Thank you. reply dang 4 hours agoparentWe detached this subthread from https://news.ycombinator.com/item?id=39956782. reply pquki4 8 hours agoparentprevI'd say the original author should just post something as an article instead of tweets. A blog post. Github md file. Github gist. Even pastebin. I don't care its format or where it is hosted, it does not need to be well formatted and could be as casual as it could be -- I don't expect to read a well-written article, and I know that would take a lot of effort. I just want to see something that is not a series of tweets. reply joshmanders 7 hours agorootparent> I don't care its format or where it is hosted Except not formatted as a tweet thread nor hosted on Twitter, right? reply creato 7 hours agorootparentTwitter is literally unusable if not logged in. All I see is the first tweet, and every link I can find that might reveal the rest of the thread takes me to a login page. reply BlueFalconHD 5 hours agorootparentAlso unusable on simpler hardware. The browser on the Kindle Paperwhite I am typing this on is just slightly too old to run Twitter. I get the unsupported browser page, which funnily enough still uses the old colors and logo. reply wrs 7 hours agorootparentprevI agree with you 200%, especially because now it’s not only an idiotic format, reading it lends support to Elon’s X (and I no longer have an X account for that reason so can’t read it). But I’m afraid this sentiment long since reached the point of “yelling at clouds”. reply m3kw9 5 hours agorootparentprevThen how will he gain followers? reply scubbo 8 hours agoparentprevAmusing. I was always irritated by the very concept of threadreaderapp and by people's propensity for posting the links (just read it on the website! There's no need to spend extra compute to join up some divs!) - but Elon's ever-increasing breakage of the site now makes it genuinely useful. reply k8svet 8 hours agorootparent\"ever increasing\"? Twitter is completely, 110% unusable without an account (and dear god, I dare some of you to make a new account and see what the process and default content is. It's gross). I say 110% not to be hyperbolic -- It shows you non-latest tweets on profiles, it doesn't let you see tweet threads or replies, even from the original poster when they post a chain of tweets. I literally can't read any of this content save for the threadreadapp link. ... reply manquer 7 hours agorootparent> ever increasing I don’t think it means currently it is reasonable , just that things are continuing to worsen and we have not yet reached a bottom even if a bottom exists. reply Dalewyn 7 hours agorootparentprevI made an account after Musk took over and Mysterious Twitter X started mandating logging in, in large part since he made the place tolerable and I follow illustrators and official accounts for games I play anyway. Making the account wasn't that annoying. Once upon a time they demanded my phone number and that was obnoxious to the point of noping out, but nowadays (after Musk took over?) they also take email instead. Email for registering accounts is nothing new, so no big deal; been doing that since 2002 when I registered my first forum account. After I made my account I went and followed all the accounts I usually follow, and my recommendations got relevant in very short order: Posts from illustrators, the games, and players who play those games. So, thanks Musk. You've at least convinced one guy to make an account where Dorsey flatly couldn't, and made the guy even happy about it which was pleasantly surprising. reply k8svet 7 hours agorootparent>in large part since he made the place tolerable oh yeah? Interesting you chose not to elaborate on how, given the statements I made about ruining public access stand. But in summary, you're saying you used the platform the same way it was usable before Elon bought it, other than all of the things I mentioned that make it unusable for those not-logged-in? Let's be frank, Elon made it 200% worse, than relinquished to only 150% worse, and that's a win? reply Dalewyn 7 hours agorootparentYou know what happened when Musk took over and quite literally fired all of Twitter Japan literally overnight? The political manipulating stopped. Those Twitter trends? Before it was always about politics nobody gave a stinking fuck about. After it was always about games, anime, manga, music, and other pop cultural things almost everyone cares about. As a side bonus, his mere presence as the new owner made the political asshats exit themselves into a completely separate corner of the internet. So yes, he made the place tolerable and I have absolute appreciation for him for achieving that. reply pvg 5 hours agoparentprevThe site conventions are to post original sources and workarounds in the thread. And to avoid gumming up threads with annoyances-of-everyday-web-life meta. reply ranger_danger 9 hours agoparentprev [–] How do they get around the account/resource limits? reply callalex 8 hours agorootparentWeb scraping is a cat-and-mouse game but all the cats got laid off. reply baobun 9 hours agorootparentprev [–] Nice try, Elon. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Twitter discussions focus on a complex backdoor attack found in the xz sshd, sparking debates on the attack's sophistication and the attackers' expertise.",
      "Users discuss state-sponsored groups, intentional backdoors, Apple hardware, and open-source software security, highlighting challenges in vulnerability detection.",
      "Speculation includes government roles in the attack, murder probes, potential backdoors in package managers and libraries, alongside conversations on Elon Musk's impact on Twitter and platform usability."
    ],
    "points": 304,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1712444365
  },
  {
    "id": 39951649,
    "title": "Asus Refunds Zenfone Buyer in UK for Missing Bootloader Unlock Tools",
    "originLink": "https://www.androidauthority.com/asus-bootloader-unlock-settlement-3431818/",
    "originBody": "Affiliate links on Android Authority may earn us a commission. Learn more. Mobile ASUS refunds Zenfone buyer for failing to provide bootloader unlock tools as promised A money claim case in the UK resulted in a Zenfone buyer getting a full refund after ASUS broke its promise. By C. Scott Brown • Published onApril 5, 2024 Robert Triggs / Android Authority TL;DR In the UK, a Zenfone buyer sued ASUS over the company’s broken promise to provide bootloader unlocking support for its phones. The buyer won, getting a full refund for the phone and their court fees, for a total of £770 (~$973). If you have similar issues, this case proves you have a shot at getting a refund, too. Over the past few months, there’s been a growing controversy among buyers of ASUS phones. Although ASUS makes some of the best Android phones you can buy, the company has fallen short in making bootloader unlocking tools available to buyers. Previously, these tools were easy to operate and readily available, but in May last year, things took a turn. Now, unlocking the bootloader of a recent ASUS phone — including the Zenfone 9, Zenfone 10, and Zenfone 11 Ultra — is not possible. One buyer in particular, who we’ll call Timothy (not their real name), was very upset about this. Timothy told us that they bought an ASUS Zenfone specifically for the ability to unlock its bootloader. Upset by this broken promise, Timothy — who lives in the United Kingdom — sued ASUS in the UK’s version of small claims court. Today, Timothy provided us with documentation of the court case and proof that ASUS has settled. In the settlement, Timothy received a full refund for the phone (£700) and the court filing fee (£70) for a total of £770 (~$973). This has set a basic precedent: if you, like Timothy, are upset that ASUS isn’t providing a way to unlock your phone’s bootloader, taking the company to court might be a viable solution. ASUS bootloader unlocking: What is going on? Unlocking the bootloader of an Android phone is a way of getting full access to the system. It allows you to manipulate the software already installed on the device and even uninstall it and reinstall a new version of Android. While this is often complicated to do, it provides a way for smartphone owners to continue to use their devices after an OEM has terminated support. Given that ASUS has one of the worst software support commitments in the Android world, many tech-savvy ASUS buyers have depended on bootloader unlocking to continue using the company’s phones far beyond the dismal two-year window ASUS provides. In fact, ASUS has essentially acknowledged this phenomenon by promising to continue providing the necessary tools to bootloader unlock its phones. However, last year, it reneged on that promise by disabling the unlocking tools and refusing to give users the codes they needed to perform the procedure. It started removing posts about bootloader unlocking in its ZenTalk forums. Mods of that forum started giving explanations about the unlocking tools being under maintenance or getting overhauled for compliance reasons but giving no information on when they would become active again. In March, Timothy led the charge by taking ASUS to small claims court, which brings us to today’s news of their big win. If you are in the same position as Timothy and would like a refund for your ASUS purchase, small claims court might be the best way to go. However, we have reached out to ASUS to get more information on this situation from the company’s perspective. Unfortunately, due to ASUS being based in Taiwan, we are unlikely to get a response until next week. We will update this article if and when we hear back. Got a tip? Talk to us! Email our staff at news@androidauthority.com. You can stay anonymous or get credit for the info, it's your choice. News AsusASUS Zenfone Comments",
    "commentLink": "https://news.ycombinator.com/item?id=39951649",
    "commentBody": "Asus refunds Zenfone buyer for failing to provide bootloader unlock tools (androidauthority.com)243 points by kirenida 22 hours agohidepastfavorite208 comments xjwm 16 hours agoI just sent feedback to ASUS expressing my concern at the loss of bootloader unlocking. I have 2 perfectly good cell phones that are e-waste now, simply because the vendor stopped issuing patches, and the bootloader can't be unlocked to use LineageOS. I bought a Zenfone recently because I thought I'd be avoiding that issue. If they don't fix this, I won't be buying another one. ASUS CEO contact page: https://www.asus.com/us/support/article/787/ reply blendergeek 16 hours agoparentI just sent feedback as well. I bought an Asus phone in the past and was planning on buying more in the future. I used to recommend them everywhere. I will not be purchasing any more Asus phones until this is fixed. reply brewtide 12 hours agoparentprevI was on the fence about buying a zenphone but waiting to see if the bootloader unlock ever showed back up. Still holding a pixel 3, unlocked, with lineage. There are no small phones with good specs and unlockable. Zenphone seemed the way, until they stopped with their unlocks. What a market to be burying in the mud. reply luuurker 16 hours agoparentprevRewarding their bad behaviour by buying more devices doesn't feel right. reply ijijijjij 16 hours agoparentprevnext [4 more] [flagged] Pannoniae 16 hours agorootparentI think GP meant two other devices (not from asus) where the vendor doesn't patch and the bootloader is locked. reply xjwm 15 hours agorootparentYou're correct. My other phones were from my wireless carrier, and didn't support unlocking the bootloader, so I had to keep upgrading (over a period of years.) I bought the Zenfone to try to break that cycle. The old devices still function, aside from being outside of vendor support (or in the case of one, incompatible with the carrier's network after infrastructure upgrades.) reply ijijijjij 16 hours agorootparentpreveven if it is different companies, not sure if it makes a difference... it just encourage bad practices. AKA.... don't look at the small picture. reply iforgotpassword 17 hours agoprevI have an old Huawei P20 Pro floating around. When it was new, you could contact Huawei and get a code for unlocking the bootloader after providing your phone's serial number. Great I thought, I'll do that later when it stops receiving updates. Stupid me, at some point those arseholes stopped giving out the unlock codes. I wonder if one could sue, but from googling around a bit I can't find a trace of them ever making this a selling point explicitly. reply huppeldepup 13 hours agoparentOn this page: https://consumer.huawei.com/en/community/details/P20-Pro-Boo... there's someone from Huawei who acknowledges the policy change, with a date. Maybe check archive.org for the mentioned time and url? HTH. reply sn0wtrooper 20 hours agoprevLong is gone the time where unlocking bootloaders and installing custom ROMs was the best path to follow. Even if you are able to unlock it (with difficulties such as this one, or others that involve opening the device and soldering a shortcut), you will have a device where apps check for unlocked bootloaders and rooted OS, and forbid you from use the application. reply nimbius 18 hours agoparentthe only app ive seen balk at bootloader status (to date) is google wallet. Using a phone to pay for stuff is an opsec nightmare youd only entertain so long as becoming an integrated and saleable asset in a data brokers portfolio is a life goal. 'pm uninstall' and move on, the custom rom is still far more valuable from a security perspective than bending the knee to some bespoke ecosystem payment app (especially if you have an older device.) the point of oem unlock, and rooting at all, is diametrically opposed to the vendors interest in nearly every facet. The vendor will bark \"hackers\" as a thinly veiled threat for the uninitiated, but we are initiated. what the vendor doesnt need you doing is erasing their telemetry and walled garden spyware. they dont need you developing alternatives to their store and to their apps, and they especially dont need you turning this effort into something as simple as an ubuntu installation for older phones they expect to follow the strict trade-in model of \"buy a new phone every year\" arguably Asus refunded the purchase because this person isn't playing by the rules and being a good consumer. reply lxgr 18 hours agorootparent> Using a phone to pay for stuff is an opsec nightmare Do you mean \"privacy nightmare\"? Security-wise, Google Pay beats using your physical card since it uses a device-specific number that can't be skimmed by terminals and reused online. > the custom rom is still far more valuable from a security perspective than bending the knee to some bespoke ecosystem payment app (especially if you have an older device.) I'd argue that it only makes sense if you have an older device that's otherwise not receiving any more security updates. reply kaszanka 18 hours agorootparentAFAIK it only beats magnetic stripe cards, not EMV chip cards reply lxgr 18 hours agorootparentEMV chip cards still contain your card number and expiry date. Skimmers would need a way to also learn the CVC2 from the back of the card to use it at most (but not all!) online merchants, but that's feasible using a small camera or a waiter/cashier accomplice doing the skimming. With Google Pay and Apple Pay, and similar mobile wallets, that number is never shared during payments (and in fact not even stored on the device). reply jjmarr 17 hours agorootparentThey do, but you can't get the card number from reading the chip. The protocol is a challenge-response one based on a private key stored within the chip. https://en.wikipedia.org/wiki/Chip_Authentication_Program You need to read the entire card number + cvc2 + expiry date with your camera. That's not skimming, that's just taking a photo of the card. reply lxgr 15 hours agorootparentNo, you can most certainly get the card number and expiry via the chip and even over contactless, as it’s a vital part of transaction routing/processing. There are Android apps that can do it. reply jjmarr 13 hours agorootparentIf I could I'd delete my original comment since I did more research and you're right. https://stackoverflow.com/questions/14861908/apdu-command-to... reply worewood 17 hours agorootparentprevYeah, and it's easily solvable with a sticker or a dremel to scrape the number off reply lxgr 15 hours agorootparentYou can't dremel it out of the chip, though. reply justinclift 9 hours agorootparentWell technically you can. The card won't be so usable after that though. ;) reply lxgr 7 hours agorootparentDestroying the chip is easy, actually chiseling away the correct trapped electrons making up the PAN in the EEPROM is the challenge ;) reply M95D 15 hours agorootparentprevAny responsible user will learn the CVC, like any other password, and then erase it from the card. reply didntcheck 14 hours agorootparentI can certainly remember mine from repeated use, but I can't say I've ever heard of someone erasing it reply catlikesshrimp 13 hours agorootparentI have done it since many years ago reply justsid 7 hours agorootparentYou can always tell what part of the HN regularly goes outside and interacts with normal people. I’m sorry but “just memorize the CVV and erase it from the card” isn’t something anyone really does. The comment that Google Wallet is more secure is a generally applicable comment. reply lxgr 14 hours agorootparentprevThat seems like a lot of extra effort for something that's arguably not your opsec problem, but that of the card payment industry. In the end, you'll always have to enter it on payment websites anyway. reply paulryanrogers 17 hours agorootparentprevBank apps, Netflix, and Disney+ also won't work. There are spoofing measures though I've been burned by unlocking and rooting too often to try again, at least not while my devices are still under warranty. reply mmh0000 16 hours agorootparentMy solution * use bank website for the one bank that requires it, otherwise I got a new bank account without silly fake security. * thepiratebay has everything Netflix and Disney does and it works anywhere reply catlikesshrimp 13 hours agorootparentI always use websites when possible instead of installing yet more spyware disguised as a useful app. My bank, however, has the TOTP built in the app. You can't make a transaction without the phone app connected to the internet. reply zappb 7 hours agorootparentprevCommercial copyright interests will always seek to maximize their control over the devices that play back copyrighted stuff. Banks at least have more legitimate security concerns since they involve the end user getting screwed rather than the copyright holder. reply luuurker 16 hours agorootparentprevMagisk + a few modules and most apps should work. The warranty part, this depends a lot in the country, but at least in Europe I don't think they can deny repairs just because you unlocked the bootloader. reply jjmarr 17 hours agorootparentprevI'm in Canada and I can literally just tap the card itself on the reader. Every card has this ability and it can't be skimmed. reply SanDiegoSun 16 hours agorootparentThere are many demonstrations of contactless cards being “skimmed.” Unless you store it in a wallet with a faraday cage, this is a laughable opinion to express. reply cqqxo4zV46cp 18 hours agorootparentprevYour claim that using a smartphone for payments is a privacy(?) nightmare sounds quite baseless. The more pertinent factor is probably the fact that you’re using an operating system built by an advertising company. reply franga2000 19 hours agoparentprevIt sure isn't what it used to be, but if you buy the right phone and make a few moderate compromises, it's still a great option. Installing crDroid on my OnePlus 9 Pro took half an hour, another half to install Magisk Delta with a few modules. The universal dark mode alone (Xposed module \"DarQ\") is worth the effort, but also the ability to clone apps, have proper clipboard sync, make full-system backups and customise the look and functions of my OS to a currently unparalleled degree. The only compromise is I can't seem to be able to do NFC card payments (send or receive), one of my 4 banking apps needs a custom patch every few months to start working and a friend tells me the McDonald's app doesn't work. reply freedomben 19 hours agorootparentDo you keep a factory image for your OnePlus 9 pro in case you want to restore it? If so, how do you go about doing that? After OnePlus decided to stop publishing factory images, I decided to stop buying their phones. It's a real shame, because they really do make some great stuff and prices are quite reasonable generally speaking. I used to buy a new OnePlus phone nearly every year. The OnePlus 6 was one of my favorite phones of all time. reply franga2000 18 hours agorootparentI wasn't aware they stopped publishing them so I didn't back it up, but I can't say I really care for my use case. The only reason I'd need it is to resell the phone, but my plan is to use it until it's either broken beyond repair or backporting new Android versions becomes impossible, at which point nobody would buy it anyways. I agree the OP6 is great (my girlfriend is still using hers), but I was still on my OP 3 like a year ago, until future ROM updates were deemed impossible thanks to Qualcomm binary blobs. It's a real shame it's all over now. The OP 9 Pro was the last OnePlus phone made in their old way (or close to it) - not too expensive, well built, close to stock ROM, easy to reflash, decently repairable. Hopefully it lasts me as long as the 3 did because currently I don't see anything else like that on the market. reply boneitis 17 hours agorootparentprevI might have my hardware/software/firmware components (or your argument) mixed up and conflated. Does Oxygen Updater not source from published images? https://oxygenupdater.com/article/438/ Yes, I am still on my beloved OnePlus 6 running Lineage and had been looking around for a used 7 or 8 for 5G capability (I'm a bit sketched out by the overall throttling hoopla of 9th gen). Perhaps it's time to expand the search beyond OnePlus. reply ktosobcy 17 hours agorootparentprevEh... that's why I'm pondering going back to OnePlus (after short affair with Samsung for the past 2 years) because it's somewhat annoying not being able to tweak stuff... Alas, it's also annoying that some dumb banks (I'm looking at you ING Poland) consider rooted device as \"insecure\" but thay have no problem if I open a bank page using admin/root account on the computer) reply iggldiggl 16 hours agorootparent> I'm looking at you ING Poland Hmm, funnily enough at least a few years ago German Ing-Diba didn't care about rooted phones. I switched banks at some point though, so I have no idea whether that's still true. reply ktosobcy 16 hours agorootparentIt's only a brand, there is almost nothing in common between local branches. As for ING - about 4-5 years ago it was possible to spoof the check but about 3 years ago they went full bonkers and if you didn't get the app from playstore (so for example aurora) it refused to launch... reply ravenstine 17 hours agoparentprevThis is rubbish. I'm running GrapheneOS and have left my bootloader unlocked, and there's no app that has refused to work. The only caveat is some of them need Google Play services. No, I am not rooted, but my last phone was rooted and there might have been one or two apps out of dozens that wouldn't work with root even with Magisk trying to hide the root status. Using a custom ROM is easily one of the beat choices I have made. reply nebulous1 17 hours agorootparentDo you use a banking app? Last I read depending on the type of check used some apps can still be problematic. reply iforgotpassword 17 hours agorootparentSo I guess next thing we need is someone sueing the fucking banks that do that. Mine luckily doesn't because I explicitly use an old phone with LineageOS, the banking app, and nothing else on it for online banking. It's arguably way more secure than using your main phone with a bazillion other Apps installed and online at all times. reply didntcheck 14 hours agorootparentA lot of this actually seems to have come from recent regulatory pressure for 2FA (which I support in principle, don't get me wrong). I don't even think most of them have given much thought to rooted phones, rather they're just cargo culting Industry Standard Best Practices and turning all the device verification options to max. Luckily, most of them realize they still have customers without a compliant smartphone, or one at all, and offer a fallback, which is almost always SMS... Though you get those newer \"app only\" banks. I've never used any since I see that as a major downside, not a selling point, so idk whether they tolerate root. Even with traditional banks, I've come across a few features which can only be accessed via the phone app - in this case likely due to the belief that \"web? Everyone just uses apps!\" rather than security reply eganist 17 hours agorootparentprevHow would that stick? You can just sign into the bank via your web browser in the case of a nonfunctional app. The apps just give you added security assurances beyond using the web. \"The app can't function in a low security environment, but complainant is free to use the web client in such event.\" case dismissed (obviously an oversimplification, but the point stands) reply erinnh 16 hours agorootparentThis is definitely not the case everywhere. Where I live the app is 100% needed because it’s the „second factor“ in the login process. reply The_Colonel 16 hours agorootparentThere has to be a fallback like SMS and/or automated call. reply akvadrako 16 hours agorootparentFor my banks the only fallback is a hardware device that you put your card into. Before the app you had to carry this everywhere when traveling to do online banking. reply Too 16 hours agorootparentprevSMS is magnitudes less secure than the Secure Enclave in my phone. Fallback should never be the weakest link in a security chain. Especially not in something as high stakes as your banking login. I can’t remember how I got my first bank token in my phone. Probably by physically showing up in the bank office with my id. reply The_Colonel 14 hours agorootparentSMS 2FA is not great, but still seems to be more secure than a rooted phone. If your SMS OTP leaks to the attacker, they still need to know the first factor (password, biometrics) to gain access. Meanwhile, if your rooted phone is controlled by an attacker ... that's it, the attacker has everything. reply Too 12 hours agorootparentFair. I still wouldn’t want to have such a fallback available by default. Being stronger than an even worse option doesn’t change that. Because it eliminates the security of the strongest option. reply didntcheck 14 hours agorootparentprevAgreed. Unfortunately almost every bank here forces me to use this less secure option \"for security\" due to my rooted phone. Not one has just offered standard TOTP (perhaps because the pull-only nature of it means they can't present the message explicitly telling the user what they're about to authorize. Which is an understandable qualm I guess) reply eganist 14 hours agorootparentprev> SMS is magnitudes less secure than the Secure Enclave in my phone. The secure enclave on a rooted phone that no longer has execution integrity? reply eganist 16 hours agorootparentprevCurious, can you name this institution that only allows the app to be used as the second factor without fallbacks? reply iforgotpassword 14 hours agorootparentIn Germany: all of them. Well, some offer a hardware device for like 25€ that can do the same thing, but then if you have an account with multiple banks, you need multiple of these devices. reply oarsinsync 16 hours agorootparentprevThere are app-only banks too. Some of them provide a web interface, but it depends on the app to sign you into the web interface (similar to the way whatsapp requires you to use the app to sign into whatsapp web). What happens when you primary bank has been one of these app-only banks for the last 5 years, and you decide to make a technology change to your phone, and can now no longer get into your banking app? reply Too 15 hours agorootparentprevSigning transactions usually take you back to the 2FA app here, where the amount and receiver is repeated. Even if someone hijacks my computers web browser, the worst they can do is see my statements, any attempt to transfer out will pop up a prompt in the phone. reply iforgotpassword 14 hours agorootparentprevThe app is for 2fa. reply realusername 16 hours agorootparentprevWhen you reject GrapheneOS, the most secure mobile OS on the planet but accept a no-name chinese ROM I feel like that you can't invoke security reasons anymore. reply udev4096 15 hours agorootparentprevIt's far from secure. You are using an outdated phone, which hasn't received any kind of firmware or vendor security patches in a while. And as far as I remember, LineageOS doesn't support relocking the bootloader which further reduces the overall security of your phone reply iforgotpassword 2 hours agorootparentWhat's the attack vector? There is nothing else installed on this phone, and I only turn it on when the banking website asks me to confirm the login via their app. So it's connected to my wifi for like 5 minutes. Meanwhile my main phone is always on the mobile network, using a proprietary modem that's running ridiculously complex firmware that does edge, lte, 5g, VoIP, has its own tcp/ip stack and a dozen other super complex protocols, is closed source, gets no security reviews and is exposed to at least my mobile provider at all times. And that's just the modem. Don't let me get started with all the value-add software the phone vendor loaded the device up with. Some of which is running with elevated privileges. You seriously think this is more secure? reply jiminymcmoogley 16 hours agorootparentprevFor UK banks on my Graphened Pixel 6a I can use the apps for HSBC, First Direct, Barclays, NatWest, RBS, Co-Operative Bank and Metro Bank with no issues, and have only had trouble with the Lloyds Bank app as of an update from maybe 2-3 months ago which throws an error saying they've detected I'm using a jailbroken/rooted device reply White_Wolf 16 hours agorootparentI get a message that the device is not secure but I can still make transfers and such from the banking app on a rooted OP9Pro. Never tried to use NFC payments though. reply doublerabbit 15 hours agorootparentprevTry using Monzo or Sterling. Both will nail you to the ground. reply baby_souffle 16 hours agorootparentprev> Do you use a banking app? Last I read depending on the type of check used some apps can still be problematic. It's important to distinguish between banking app and payment app. If you just want to check your account balance or find an ATM, the banking app will probably not mind that you're on a device that can't pass integrity checks. If you want to use your phone's NFC to pay for coffee, though, you're going to have a bad time. reply didntcheck 15 hours agorootparentprevAlso many \"corporate\" things, usually depending on your org's policy. E.g. I can't run OpsGenie (it may actually be the Microsoft SSO step failing, I'm not entirely sure, but the error definitely mentions my device not meeting security policies) reply Xfx7028 15 hours agorootparentprevI use N26, Revolut, ING, and others. No issues, I just add the apps I need to the magisk hide list. I also use NFC payments. Only Google wallet does not work. reply ravenstine 15 hours agorootparentprevYes. Wells Fargo, Discover, Alliant CU, Venmo, Paypal, and M1 Finance all work. reply zerreh50 17 hours agorootparentprevSame with McDonald's, interestingly enough reply vidarh 16 hours agorootparentprevYeah, my bank app both did not work with rooted phones, last I checked, and they also appear to whitelist phone models or something - at one point I had an uncommon mid-range Chinese phone and I had to contact support to have them approve my phone. reply Fire-Dragon-DoL 17 hours agorootparentprevWhat are the downsides with GrapheneOS? I had a few problems with root (Netflix and banking apps) but would love my privacy. My main reason for root is the firewall to block outgoing connections from apps that are not supposed to do it reply 0xcde4c3db 16 hours agorootparentIt's really a downside of the Google app ecosystem and not GrapheneOS per se, but apps requiring higher levels of integrity per Google attestation (Play Integrity/SafetyNet) generally won't work. Intentionally breaking apps on \"untrusted\" configurations is basically the point of that feature, and GrapheneOS does provide the relevant services, but would need to be specifically enabled by the app developer. reply Fire-Dragon-DoL 13 hours agorootparentSo Netflix and such DO NOT work on phone? That's really frustrating reply udev4096 15 hours agorootparentprevFirewall wouldn't be necessary with GrapheneOS. There's a network toggle which you can use to completely cut off internet access for an app. As for the downsides, I would say close to zero. It feels just like a stock OS, without any kind of bloatware and a lot more secure reply DEADMINCE 7 hours agorootparentprevOne big downside is being limited to Pixel phones, without good reason. reply switch007 12 hours agorootparentprevNo NFC payments with Google wallet. You can get unlucky with your bank app but someone maintains a wiki of compatible banking apps Android auto works OK. reply ThePowerOfFuet 15 hours agorootparentprevGrapheneOS is not rooted, so you won't have those issues. GrapheneOS also gives you a Network permission per-app; if you uncheck it, the app has no connectivity, period. Highly recommended. reply Fire-Dragon-DoL 13 hours agorootparentBased on the other comment thread, it seems like Play Integrity and SafetyNet do not succeed and as such, can't really use Netflix, is that correct? reply 63stack 10 hours agorootparentprevNo, parent is 100% correct. Unlocking your bootloader trips SafetyNet. reply ThePowerOfFuet 15 hours agorootparentprevYou should not leave your bootloader unlocked if you care about the security of your device and data. Unfortunately, locking (and unlocking) it wipes user data, so it should be relocked right after installation of GrapheneOS. reply didntcheck 14 hours agorootparentDon't most phones only wipe on unlock? Also can Graphene still update if the bootloader is locked? reply ravenstine 14 hours agorootparentprevI acknowledge that. reply collegeburner 17 hours agorootparentprevwhat? safetynet is absolutely a pain in the ass. i think there are some xposed and magisk modules or whatever that can work around it but that's a cat-and-mouse thing and can break. lot of bank and financial apps, lot of stuff with DRM will break. reply udev4096 15 hours agorootparentprevGrapheneOS is not a ROM. It's an OS. reply jstanley 15 hours agorootparentWhat's the difference? reply Semaphor 19 hours agoparentprevIf you root, you can bypass those issues in most cases. I have 3 apps detecting it, that I can bypass, and only the German health insurance app from TK detects it (according to the internet, it's getting past most solutions somehow). It's not something I'd recommend the average person, but for people who care enough to fiddle, it's still the best way. I think since my first Android (HTC Desire Z/T-Mobile G2) I spent a total of 1 week on stock, never was a fan of any of them. reply arsome 20 hours agoparentprevLargely depends on your priorities and level of effort. You can bypass all current app checks using Magisk and Play Integrity Fix, but it's a bit of work to maintain and can break occasionally. You gain in this case full control of your device like a desktop OS, block ads, modify app behavior, disable unwanted system features, but you have to put in effort to maintain it. However if you don't want to deal with that, you can also just not use those apps, use it like you would a Librem or PinePhone, load primarily open source software to it, optionally don't even bother with play store, etc. Might not be for everyone, but if you don't care that much for Google Wallet or multi-player games on your phone, it's not a bad option. reply BizarreByte 19 hours agorootparent> but it's a bit of work to maintain and can break occasionally. Which is a major problem because my tolerance for my bank's app not working when I open it is so low it might as well be non-existent. I personally gave up this fight. reply rkagerer 17 hours agorootparentOr switch banks and stop fighting. reply mike256 17 hours agorootparentI just switched to another bank. No one should accept apps with such checks. reply zamalek 18 hours agoparentprev> where apps check for unlocked bootloaders and rooted OS Magisk and PINE[1] have solved this for me. Yes, even Google Wallet is all good with my LineageOS ROM. PINE is an auto-updating PIF. [1]: https://github.com/daboynb/PlayIntegrityNEXT reply ac130kz 19 hours agoparentprevStock ROMs are still filled with ads and useless extras, rarely providing meaningful features over an AOSP like LineageOS. reply yooastan 17 hours agoparentprevThis is untrue, I do this now with my Pixel and have to no issues. reply NayamAmarshe 15 hours agoparentprevWith KernelSU, this is no longer the case. It's Magisk that causes most problems. reply myself248 18 hours agoparentprevHuh. I guess I must not run any of those apps? reply jMyles 20 hours agoparentprev> Long is gone the time where unlocking bootloaders and installing custom ROMs was the best path to follow. ...wha? I just installed GrapheneOS on my Pixel 8 Pro and it is, by a decent margin, the best custom ROM experience on a phone I've had to date. reply npteljes 19 hours agorootparentI have it on my Pixel 7a, and it's a great experience, but I also don't need to run apps that check for phone \"security\" or integrity. This is the case OP is talking about. https://grapheneos.org/usage#banking-apps reply encom 20 hours agorootparentprev>GrapheneOS This was not a project I expected to use Discord for support. Sad. reply asdp9iujaspid 19 hours agorootparenthttps://grapheneos.org/contact#community > Our chat rooms are bridged across Discord, Telegram and Matrix so you can choose your preferred platform. > We have an official forum for longer form posts, which is publicly accessible and easier to search. We are using Flarum for our forum. https://discuss.grapheneos.org/ If they mandated discord as a closed support community sure, but you can't be too upset by the mere affiliation with a discord channel when they also offer all the above reply encom 19 hours agorootparentI'm just disappointed that they associate with Discord at all, given that it is the antithesis of privacy and Freedom. reply pulpfictional 18 hours agorootparentThe focus is security. Be disappointed in all the other free platforms that cannot provide adequate moderation or stability. Do you happen to know a suitable alternative? reply cqqxo4zV46cp 18 hours agorootparentprevI’m disappointed that you associate with Hacker News given the (presumably) myriad anti-Freedom anti-privacy startups Y Combinator has funded. reply udev4096 15 hours agorootparentprevIf I remember correctly, their matrix channel was flooding with spam and abuse which was primarily coming from Calyx, which by the way is an terrible OS. Even a stock OS would perform marginally better in terms of security than CalyxOS reply BLKNSLVR 14 hours agoprevWith the increasing difficulty (impossibility) of bootloader unlocking that most manufacturers are building into their Android devices, I wonder whether it's market reasons (the longer the devices are operational, the longer upgrade cycle) or pressure from intelligence agencies due to minimised Google / telemetry data back doors in custom ROMs. Using the \"simplest answer is often the best\" approach, it would historically be the profit motivation at 99% probability. Currently, though, feels like surveillance and intelligence gathering is edging to the higher likelihood. Edited to add: and maybe it's not even intelligence agencies, maybe it's purely profit driven from the personal-data-selling industry. reply alwayslikethis 8 hours agoparentIt would then be quite shocking to know that Google's Pixel phones consistently allowed unlocking without any nonsense like online verification. They also support relocking your bootloaders as well as using your own signing keys for secure boot. reply clementmas 20 hours agoprevThe refund is probably not worth the time spent building the case but it sends a valuable message. Keep your promises. reply avianlyric 19 hours agoparentThere probably wasn’t much time spent building the case. Generally solicitors aren’t involved in these cases because it’s not possible to reclaim legal costs, regardless of who wins. Instead most people just represent themselves, and companies will often just send a local manager to represent them. So super low stakes legal process, where in the worst case scenario your out of pocket for the filing costs (£70) plus reasonable expenses for the other party (travel costs, lost earnings etc) which are all tightly capped, so unlikely to more than another £100-£200. reply xyst 20 hours agoparentprevI would be surprised if Asus even sends a lawyer to defend against small claims. Might be better to form a class action. reply kadoban 19 hours agorootparent> I would be surprised if Asus even sends a lawyer to defend against small claims. Wouldn't that be a good reason to do small claims? I can't imagine why I'd want to wait for years in a class action when I can just do a small claims. reply avianlyric 19 hours agorootparentprevIf this went down the small claims track in the UK then ASUS wouldn’t bother with a lawyer because you’re not allowed to reclaim legal costs in the small claims court. So unless ASUS thinks they’re gonna see a flood of similar claims happening, then the cost of a lawyer would probably be triple the cost of settling, or even winning the case. Also class action cases are very rare in the UK. In the past the courts have generally refused to approve class action cases. It not like in the U.S. where there’s a cottage industry around class action cases. I’ve personally never heard of a class action case happening in the UK, I know they do happen, but they’re so rare that they don’t make it into the news, and most people will never involved in one either directly or indirectly. reply bxparks 17 hours agorootparentprevI would bet that neither small claims or class action is possible in the US because ASUS has a forced binding arbitration clause in their End User Agreement that almost no one read when they activated their phones. reply ktosobcy 17 hours agoprevI do hope that (for example) EU would force makers to provide a way to unlock the device and install any OS/distribution I want... reply hocuspocus 13 hours agoparentThe EU wants to mandate 5 years of security updates, which is a lot more relevant to the immense majority of consumers. reply ktosobcy 12 hours agorootparentOne doesn't (have to) rule out the other? Would it be OK if you were forced to use only the single OS that your computer came preinstalled with? reply Cort3z 18 hours agoprevBesides the point here, but why is it so diabolically hard to decline cookies on this site? reply tim333 12 hours agoparentI'm not sure cookie declining is the way to go these days. You can use \"I still don't care about cookies\" to stop the dumb pop ups and something like \"Firefox Total Cookie Protection\" if you don't want to be tracked? reply alwayslikethis 8 hours agorootparentPersonally I set Firefox to auto clear cookies on window close except some whitelisted sites. I just use accept all most of the times since it will be cleared anyways. reply Cort3z 1 hour agorootparentThis is a false sense of security. Cookies isn’t random data. It is fingerprints and all kinds of dark wizardry. Chances are your cookies will be the same each time they are generated. You have to disable cookies or tell the company responsible that you don’t want it. The latter, I believe (though ianal), is legally binding. reply Cort3z 1 hour agorootparentprevI’m on mobile though, which makes it harder, but thanks for the tip for my desktop! reply mnw21cam 17 hours agoprev> This has set a basic precedent I would note that technically the small claims court in the UK does not set precedents. That would be the function of a higher court. reply justinclift 5 hours agoparentThe article also says that ASUS settled too: Timothy provided us with documentation of the court case and proof that ASUS has settled. Isn't that fairly common when companies don't want a verdict to happen, as they expect to lose? reply solarkraft 17 hours agoparentprevHow so? I thought a precedent was just any case that has been ruled in a certain way, irrespective of the court it has happened in. reply nickff 17 hours agorootparentIt seems like this court does not have the authority to set ‘legal precedent’, though colloquially it has ‘set a precedent’ in the sense that it did something for the first time. reply dragonwriter 11 hours agorootparentprevThis is incorrect, courts have specific rules about what cases may be cited as precedent, and whether that precedent is optional guidance for the court it is presented to (persuasive precedent) or rules that must be followed where the decision conditions in the earlier ruling apply to the current case (binding precedent). For instance, in a US District Court on most questions of federal law, as regards decisions of other federal courts: published decisions of any federal court can be cited as persuasive (most district court decisions are unpublished), and decisions of the Court of Appeals for the circuit in which the District Court is located, or of the US Supreme Court, may be entered as persuasive precedent. reply adw 17 hours agorootparentprevNot all courts have the power to set precedent. Small claims courts in England don’t. reply j45 13 hours agorootparentThe way it's been explained to me is precedent is often referring to rulings that start with the similar courts geographically to other ones. reply mnw21cam 11 hours agorootparentIt's not that simple either. Common law jurisdictions often use rulings at higher courts in completely different countries (as long as they are also common law jurisdictions) as precedent if it helps come to a suitable judgment. reply j45 11 hours agorootparentAbsolutely. Depends on the court, country, and how relevant it might be. A friend recently walked me through the order of precedent for one area of law reply rahimnathwani 16 hours agorootparentprevThere was no ruling. The parties settled. reply FerretFred 16 hours agoparentprevHard to believe that it actually reached the Small Claims Court, let alone succeeded! Well done that person! reply rahimnathwani 16 hours agorootparent> Hard to believe that it actually reached the Small Claims Court It's easy to file a small claim in the UK. More info on the process for England & Wales here: https://www.gov.uk/make-court-claim-for-money > let alone succeeded! The article is light on details, but it sounds like the parties settled before any hearing or ruling. reply bbarnett 15 hours agorootparentIn Ontario, Canada, part of the small claims process is a pre-trial conference, with a retired judge moderating. Further, nothing disclosed may be used as part of the trial. Its goal is to help with an amicable settlement. More info: In small claims, lawyers are not forbidden, but they may only speak for their client, and their client must be there, or present remorely, listening and ready to accept offers or deals. And if lawyers use legalese, the residing judge must explain to you what is being said, and will look unfavorably at the lawyer for not speaking plainly, and wasting everyone's time. Technically lawyers are not allowed as lawyers, but accommodations must be made for a company 1000s of km or more away. Someone must speak for them. reply rahimnathwani 15 hours agorootparentDid you intend to reply to a different comment? reply bbarnett 15 hours agorootparentNo! My comments re: Ontario pre-court conference, were meant to highlight a reaspn why this might have been settled before small claims coirt. I suspect a similar thing happens in the UK, and that forced conference ensures companies must hear reasoning, arguments in full before the case. In Ontario, it's very informal. You just talk. The retired judge only intercedes if it becomes heated, or runs long. It helps solve things. reply rahimnathwani 8 hours agorootparentMediation is offered in the UK, in the hope that it reduces the number of cases that proceed to a hearing. I don't know whether it was used in this case. reply bcraven 14 hours agoprevI think this is the original thread: https://xdaforums.com/t/court-action-against-asus-false-prom... reply ReptileMan 19 hours agoprevOkay - so which devices are left that are easily rootable? I will be in the market for new one soon. It's good if EU after mandating usb-c also mandates unlockable bootloaders for whomever wants it. reply freedomben 19 hours agoparentAll pixel phones are very easy to unlock the bootloader, and Google publishes factory images. So if your root goes wrong or you need to revert to stock, it is very easy. The actual process of obtaining root is as easy as it is on any other device, which is to say, I wish it was a lot easier, but it is very doable. As a bonus, it also opens the door for Graphene OS should you choose to go that direction. reply zamalek 18 hours agorootparentPixels can also be re-locked with a custom ROM present (I think Graphene is the only one that does this, though). For that reason alone I'll be transitioning back to Pixel (once this phone is beyond help). reply onli 17 hours agorootparentCalxyOS is the other one, with less problematic developer history. reply cf100clunk 17 hours agorootparentUp until your comma the comment suited me just fine, but then... let's not get personal about developers' health issues. It isn't helpful, and there has already been an HN discussion on the topic that you've unfortunately exhumed. There has been great progress at solving problems that had come up during a sad time for GrapheneOS and CalyxOS. reply onli 14 hours agorootparentI understand that position, but one can also not simply ignore the situation. It'd be okay if the project had removed the maintainer, but they did not, instead he just sabotaged the Mozilla location service discussion while purporting to speak in the name of the Graphene foundation. There is a responsibility to warn users about that risk factor. reply cf100clunk 11 hours agorootparentGrapheneOS and CalyxOS are great at what they do, and the present situation for both is positive and good. It can be exhausting digging up old interpersonal stuff that does not have a technical bearing on the present, especially as there are/were health and wellbeing issues that are/were at play. Choose to let it go, for once and for all. Peace out. reply onli 11 hours agorootparentNo, that's sadly not true. With the stepping down having been cancelled and the recently repeated paranoid accusations in https://github.com/mozilla/ichnaea/issues/2065#issuecomment-... the present situation of GrapheneOS is far from good, it's an \"absolutely do not use and do not recommend the project\"-situation. This maintainer being active and continuing his bullying is way too risky for the users of the project (and the FOSS android ecosystem as a whole). But let's indeed let it be here, it gets OT for the ASUS topic. The GrapheneOS warning just had to be mentioned as it was related to the bootloader re-locking. reply commoner 16 hours agorootparentprevCalyxOS has been working well for me and I recommend it. I appreciate how the included microG allows me to disable Firebase Cloud Messaging for any app that I don't need push notifications for. Having push notifications without Google Ads or Google Analytics is great. reply j-bos 17 hours agorootparentprevWhat are the history problems with graphene? reply cf100clunk 17 hours agorootparenthttps://news.ycombinator.com/item?id=36089104 In tech as in all of life, wellness of those in leadership can be ephemeral and is never to be taken for granted or assumed. I wish good health to all involved. reply resource_waste 17 hours agorootparentprevNo aux port though. From Electrical Engineering apps to my various current/normal/legacy hardware that uses aux... I don't want to carry around a dongle. Ever. I don't want to attach them to things. I just want my phone to have the $3 peripheral. reply handity 19 hours agoparentprevThe pixel 4a is the last good phone that's small, rootable, with a headphone jack and good rom options. reply resource_waste 16 hours agorootparentI just looked it up... they actually sell Factory 5a... what? No... Maybe I misunderstand the posting. reply yjftsjthsd-h 17 hours agoparentprevLenovo's Moto phones reply ldmosquera 17 hours agorootparentI recently got Lenovo Moto G7 Plus (not recent but recent enough for their purpose), because LineageOS fully supports them [1]. Then I found out to unlock the bootloader I had to: 1. get a string via a `fastboot` command 2. create a motorola.com account 3. paste string in some motorola.com page to get an \"unlock code\" emailed IF Motorola decides your device is \"unlockable\" 4. `fastboot oem unlock UNLOCK_CODE` 5. connect phone to the Internet and wait between 3 and 7 days [2] (turned out to be 3 or 4) Until I did all that shit, the option to unlock the bootloader in system settings was grayed out. Afterwards the device works well, but it was a terrible experience and I DO NOT recommend Motorola devices for rooting based on this. [1]: https://wiki.lineageos.org/devices [2]: https://nerdschalk.com/how-to-fix-oem-unlock-greyed-out-or-o... reply yjftsjthsd-h 15 hours agorootparentOh, ew, they've really regressed then. Step 5 is new:( reply NayamAmarshe 15 hours agoparentprevXiaomi phones are also pretty good for Custom ROMs. reply resource_waste 17 hours agoparentprevSeriously sad. I am such an Asus fan after their insanely good gaming laptops.. $500-900 and you can run AI Art and LLMs. I didn't expect their laptop dominance to exactly extend to Androids. I was hopeful. I should have probably known better, apparently they don't do native linux support. I had to use some Fedora fanboy stuff to get my peripherals to work. It was easy, but still couldn't use most distros. reply n_plus_1_acc 14 hours agoparentprevI'm in love with fairphones reply spuz 21 hours agoprevIt's not clear whether a standard refund is an option for buyers whose phones are still under warranty. Did Timothy try that before going to small claims court? reply throwaway5959 15 hours agoprevCan anyone suggest legitimate reasons one would need to unlock their bootloader? Like I get doing it because you can, but for say 99% of the population, what would be the practical benefit? Edit: downvoted already, lol reply NayamAmarshe 15 hours agoparentI'm on a 5 years old phone with Android 14. The OEM stopped the update on Android 11 but the Custom ROM community is going strong. Not only do I now have the latest software that works smoother, but also better camera with GCam, no ads systemwide, better privacy controls through Warden and other similar apps. I just avoided e-waste. reply throwaway5959 14 hours agorootparent> Not only do I now have the latest software that works smoother, but also better camera with GCam, no ads systemwide, better privacy controls through Warden and other similar apps. Sounds like a lot of that functionality could be achieved by just buying an iPhone instead. The 6s received updates through 7 major versions. reply sexy_seedbox 9 hours agorootparentWith Android, I'm an admin of my phone with full control. With iPhone, you're just a user. reply jstanley 15 hours agoparentprevInstalling an alternative OS. reply throwaway5959 15 hours agorootparentOK, but that’s still a 1% use case. reply hu3 10 hours agorootparentit's 100% for those who want reply Rinzler89 21 hours agoprev>Given that ASUS has one of the worst software support commitments in the Android world [...] It started removing posts about bootloader unlocking in its ZenTalk forums. Which is why I never understood why Asus Zenfone kept being recommended on HN all the time when people asked for good android phones to buy. I thought this community appreciated long SW support. I think the people recommending it were not dogfooding it. Why not go for something that has 5-7 years of SW updates like a Pixel or a S-series? The Zenfone wasn't any cheaper than those either(at least in EU) so you were also getting a poor value for money. reply as1mov 20 hours agoparentAsus allowed bootloader unlock up until a few months ago, which is why I bought the device, i.e even if the company abandons it I can just unlock the bootloader and slap on LineageOS on it. Seems like a mistake now. > Why not go for something that has 5-7 years of SW updates like a Pixel or a S-series? The Zenfone wasn't any cheaper than those either(at least in EU) so you were also getting a poor value for money. SW updates aren't the only criteria when choosing a device, if you're looking for a small phone with a headphone jack and a non-glass/metal back, the number of options are very limited (iirc, Zenfone 9 is literally the only phone which satisfies the constraints). reply jsheard 20 hours agoparentprevThe Zenfones mainly carved out a following because they're the smallest flagship-tier Android phones on the market. Asus has terrible support, but if you want a small-ish device there isn't a great deal of choice nowadays. reply mablopoule 19 hours agorootparentAlso, it's one of the rare remaining flagship-tier phone who still allows headphone jack, which is the main reason why I bought a Zenfone last year. reply praisewhitey 19 hours agorootparentprev>they're the smallest flagship-tier Android phones They're the same size as the Galaxy S series https://phonesized.com/compare/#2261,2398 reply Audiophilip 19 hours agorootparentDoes the Galaxy S series have headphone jacks as well? reply magnio 18 hours agorootparentNo, but can't you use a USB-C earphone? reply punchmesan 18 hours agorootparentFor some people, the ability to use wired headphones/earphones while charging has a lot of value. Additionally, some invest in quality earphones or IEM's and replacing those with the lower-quality limited selection of USB-C earphones is not desirable. Likewise with Bluetooth. The 3.5mm audio connection is nowhere near dead yet. reply jsheard 18 hours agorootparentFWIW for IEMs with detachable cables, which pretty much all of the good ones do, you can get replacement USB-C cables now. That has the advantage of decoupling the sound quality from the highly variable quality of the phones internal DAC since the DAC is instead part of the cable, and some of those cables even have configurable hardware DSP features. It doesn't solve the charging problem though. reply yjftsjthsd-h 17 hours agorootparentprevThat's more expensive and less convenient. reply Uvix 18 hours agorootparentprevNot while charging. reply dangus 18 hours agorootparentUSB C to 3.5mm Headphone and Charger Adapter, 2-in-1 USB Type C to Aux Jack Dongle Cable with PD 60W Fast Charging $8.99 I like headphone jacks too but I’m not going to pretend that this isn’t a solved problem. reply rrix2 17 hours agorootparentYeah I bet that 9$ widget definitely won't induce audio quality issues while you pump 60W through it... reply as1mov 20 hours agorootparentprevIndeed, today if you want a phone that isn't gigantic the options are really limited. Zenfone also had benefit of being quite sturdy since the back wasn't made from glass/metal, so you can go without a protective case. I have dropped mine a couple of times, but it hasn't suffered any damage until now. reply yxhuvud 19 hours agorootparentYou can go without protective shell, but it is so slippery you want one just to avoid airborne phones. reply Rinzler89 18 hours agorootparentprev>The Zenfones mainly carved out a following because they're the smallest flagship-tier Android phones on the market. They're literally the same dimensions as the basic Samsung S23, which was cheaper than the Zenfone as had better SW updates. reply sfmike 18 hours agoparentprevwhat good is SW updates on a pixel that can't even get cellular connectivity reply cbarrick 18 hours agoparentprevHonestly, I switched from Pixel to Zenfone for the form factor. Ever since the visor redesign with the Pixel 6, the Pixel series has been too big for my hands. I do miss the great Pixel software though... reply pohuing 17 hours agorootparentSame thing here. Zenfone android is similar to pixel android while I find Samsung's android insanely ugly. I just miss some of the Google app integrations reply Hendrikto 18 hours agoparentprev> Why not go for something that has 5-7 years of SW updates like a Pixel or a S-series? Or an iPhone. People love to hate on Apple, but they actually support their devices. reply luuurker 16 hours agorootparentOn top of the iOS restrictions vs Android, an iPhone comes with a set of constraints that the type of user that cares about bootloader unlocking doesn't want to deal with. From apps that are not on the store to custom ROMs that have features that the stock OS doesn't have... iPhones are terrible for this. iPhones are good, but in this case it's a bit like recommending a Chromebook to someone that is used to build their own computers and runs linux. reply esalman 21 hours agoparentprevTbh I don't share the sentiment that the community appreciates long SW support. Otherwise we'd see Windows being recommended over Linux more often. reply throwaway11460 21 hours agorootparentThis is more about device support for updated OS, which any PC does. Especially with Linux, unlike Windows 11 that can't be installed on older PCs without some hacks. reply HansHamster 19 hours agorootparentprevYou mean Microsoft Windows which dropped support for Zen 1 with Win11 not even 5 years after Zen 1 was released? Meanwhile, Linux will still run on a 30+ year old CPU... reply skinner927 18 hours agorootparentThey said software support, not hardware support. You can take a win 95 gui app and run it on windows 10 without issue. You can’t do the same on Linux. reply gpm 10 hours agorootparentFor many old windows games (and probably other apps) you'll actually have better luck running them on linux than a modern version of windows, thanks to wine/proton. E.g. see this user report: https://www.reddit.com/r/SteamDeck/comments/1743cec/almost_s... reply Rinzler89 17 hours agorootparentprevPretty much. For the sake of nostalgia, I downloaded an Encarta 2000 ISO form Internet Archive, then spun up a Windows 98 VM to run it on but that VM had a lot of sound issues in Virtual Box, then I realized that Encarta would also run just fine installed on Windows 11 lol. This kind of backwards compatibility is not something I need on a daily basis but it's pretty neat that I can just run very old SW on my main OS without fiddling with VMs. reply yndoendo 15 hours agorootparentprevThis is not 100% true. Some legacy Windows software does not run on current Windows. Never got Slave Zero running on Windows XP or Windows 2000 after upgrading from Windows 98 & ME. https://en.wikipedia.org/wiki/Slave_Zero reply yjftsjthsd-h 17 hours agorootparentprevIn context, it looks like they meant software updates, which is closer to what your calling hardware support. reply shepherdjerred 19 hours agorootparentprevIsn't Windows 10 still supported? reply shepherdjerred 17 hours agorootparentI looked into it. Consumer installs of Windows 10 have updates until 2025. https://en.wikipedia.org/wiki/Windows_10#Support_lifecycle reply dangus 18 hours agorootparentprevWindows 10 support continues until October of 2025. Zen 1 will be 8 years old at that point. It’s pretty much guaranteed that Microsoft will add an extended support period to windows 10. Windows 7 just left extended support last year. reply jethro_tell 19 hours agorootparentprevLol, I guess Linux did just drop 386 and 486 a couple years ago. reply dns_snek 17 hours agorootparentprevIt's really not the same kind of support. We get plenty of active support and development in the Linux world, and open source more broadly. Windows only offers essential life support, trying to ensure that something written 20 years ago still runs today, despite being completely abandoned for 19 years with no reasonable way of fixing it. reply DoneWithAllThat 20 hours agoprevThe article casually refers to Asus breaking “their promise” but nothing in the rest of the article suggests Asus ever promised anything of the sort. That they used to provide the tools wasn’t a promise. Did they ever advertise or up-front communicate that these tools would be made available and maintained? reply Knork-and-Fife 17 hours agoparentA lot of the zenphone series had bootloader unlock as a listed feature which was then removed when they disabled and took down the tool. The Zenfone 10 was advertised as going to have the feature as well before and even after the launch for a bit. Asus claimed that the tool was coming at several points but the date in their promises kept getting pushed back and eventually turned into we don't know and now it appears to be never. This is stuff that was in writing from them, search it up, but you'll need to check the Internet archive for the info that they've removed from their own sites (like the repeated pushing and cancelling of the return of the unlock tool) reply luuurker 16 hours agoparentprevAsus even used to send free phones to developers over at XDA Developers so they could create custom ROMs and stuff like that, so that excuse doesn't work for them. reply dangus 18 hours agoparentprevIt doesn’t matter because my guess is Asus couldn’t be bothered to show up to small claims court over $900. reply solarkraft 17 hours agoprev [–] > ASUS makes some of the best Android phones you can buy > ASUS has one of the worst software support commitments in the Android world How can you possibly say both things in the same article? reply Guzba 17 hours agoparent [–] The type of consumer buying an ASUS device is the type of consumer that thinks the spec sheet tells them how good something is. Eg, faster CPU and more RAM, or higher screen refresh rate or whatever is good regardless of any other variables or the package as a whole. This means they are \"some of the best Android phones you can buy\", as in, they have some of the best specs per $ you can buy. Not that they are actually good phones. It makes total sense someone could think they are great phones while they also have terrible software support since software support is not a simple hardware number on the spec sheet. This is very like PC people that hyper-focus on a few metrics like CPU frequency since it is simple and numerical and easily compared, even if it is not actually sufficient to tell you much about full system performance. Example ASUS phone description from enthusiast: \"It's got good speakers, 2 charge ports, 165 refresh rate, optimal cooling, a set of ultra sonic buttons, ip54, crazy good battery, acceptable camera, storage is crazy high 256GB for 1 grand, 512 for 1.1 grand, 12G ram for 1k, 16G ram for 1.1k, can take 2 sim cards.\" reply akvadrako 16 hours agorootparent [–] I disagree - I don't care about most specs, but ASUS Zenfone is still one of the best small phones, especially for the money. reply Guzba 15 hours agorootparent [–] Fair enough! Tho I am curious what you mean by \"especially for the money\"? reply akvadrako 2 hours agorootparent [–] Well the Samsung S-series and Sony 5-series are also decent small phones, better in several ways but more expensive. There is very little competition. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A UK Zenfone purchaser successfully sued ASUS for not delivering the bootloader unlock tools as promised, securing a full refund of £770.",
      "ASUS disabled the unlocking tools, sparking discontent among customers, potentially prompting others in similar situations to pursue legal recourse.",
      "Despite requests for comments, ASUS has remained silent on the matter."
    ],
    "commentSummary": [
      "Asus customers are expressing disappointment due to the absence of bootloader unlock tools for Zenfone, resulting in bricked devices and a decline in brand trust.",
      "Users are debating the benefits of bootloader unlocking, security risks of custom ROMs, challenges with mobile payments, and concerns about bank security on rooted devices, while considering privacy-focused options like GrapheneOS.",
      "Discussions also involve considerations of legal actions against Asus, implications of cookie tracking, and the advantages of easily rootable devices such as Pixel phones, along with preferences for compact, rootable phones and differences in control between Android and iPhone."
    ],
    "points": 243,
    "commentCount": 208,
    "retryCount": 0,
    "time": 1712402182
  },
  {
    "id": 39955725,
    "title": "Enhancing Language Model Performance through Agent Scalability",
    "originLink": "https://arxiv.org/abs/2402.05120",
    "originBody": "Computer Science > Computation and Language arXiv:2402.05120 (cs) [Submitted on 3 Feb 2024] Title:More Agents Is All You Need Authors:Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye View PDF HTML (experimental) Abstract:We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2402.05120 [cs.CL](or arXiv:2402.05120v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2402.05120 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Deheng Ye [view email] [v1] Sat, 3 Feb 2024 05:55:24 UTC (2,521 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2402 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39955725",
    "commentBody": "More Agents Is All You Need: LLMs performance scales with the number of agents (arxiv.org)210 points by TaurenHunter 12 hours agohidepastfavorite134 comments phire 7 hours agoI'm not sure people in these comments are reading this paper correctly. This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate. Because this paper introduces their alternative method which simply runs the same query multiple times on the same LLM, without any context shared across queries. And then they run a similarity algorithm on the answers and pick the most common answer. (Which makes sense to me. If an LLM is giving you a mixture of \"hallucinations\" and correct answers, the correct answers will similar and the hallucinations will hopefully be chaotic) And this simple algorithm preform just as well (and sometimes better) than all the other multi-agent algorithms. This suggests that the other multi-agent schemes with their clever prompts aren't really doing anything special; Their improve results are coming mostly from the fact that the LLM is run multiple times, that the prompt asks the LLM to pick the best answer. reply zer00eyz 7 hours agoparent>> Because this paper introduces their alternative method which simply runs the same query multiple times on the same LLM, without any context shared across queries. And then they run a similarity algorithm on the answers and pick the most common answer. https://en.wikipedia.org/wiki/Lorenz_system Years ago weather simulations started tweaking input params and running their models over and over. Discarding outliers, taking averages. It works pretty well. Because LLM's mostly have random seeds (aka temperature) feeding them the same input and averaging the output is going to get you a better guess. Lorenz also gives some clues (if not an outright explanation) as to why the \"hallucination\" problem is likely unsolvable. If you buy into this line of thinking then it quickly becomes apparent that LLM's are more or less a dead end when it comes to AGI. Simulating isnt emulating... an LLM is as likely to become intelligent as a forecast is to control the weather. reply Terretta 7 hours agorootparent> it quickly becomes apparent that LLM's are more or less a dead end when it comes to AGI. On the contrary, sit and listen in a college cafeteria, and it quickly becomes apparent most conversation participants are LLMs.* > Simulating isnt emulating... These are not synonyms, true. > an LLM is as likely to become intelligent as a forecast is to control the weather. I don't see uncertainty of intelligence as a property of an LLM as being equivalent to certainty of weather control as a effect of a forecast. Among other things, whether weather was controlled would tend to be agreed by all observers, while it's often unclear if intelligence is being observed in these threads. :-) --- * While my last line was a joke, humans in LLM mode was not. We can drive on autopilot, and get where we need to go while not being able to remember how we got there. We definitely converse on autopilot, indistinguishably from LLMs talking to each other, after an opening line every word of every sentence in the entire exchange perfectly predictable to a stranger. Are the speakers intelligent? What about the stranger who knows what they will say next? To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent. reply hooande 6 hours agorootparentLLMs were specifically trained to emulate human interaction patterns. Of course we sound like them at times. It's the things we can do that they can't that are relevant. If I study Einstein and learn to do a really good impression, the statement \"Einstein often sounds like karmacondon\" will be true. That does not make me Einstein. reply zer00eyz 6 hours agorootparentprev>>> I don't see uncertainty of intelligence as a property of an LLM as being equivalent to certainty of weather control as a effect of a forecast. GTA 5 is a simulation. Do you expect to be arrested out side your front door for the car you stole in game? Weather forecasting is a simulation, it tells you what the weather will look like in the next few days. It gets better as we get more sensors, collect more data and build more accurate models based on those two factors. It will never leap to weather. Language forecasting (because this is what an LLM is) is a simulation. It tells you what the next token (word) will be based on what came before it. It gets better as we collect more data and hone and refine these models. It will never make the leap to intelligence. >> To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent. To say that LLMs are intelligent means that language is a requirement for intelligence. Thats some fairly magical thinking... buy any sufficently advanced technology... reply ToValueFunfetti 4 hours agorootparentIntelligence breaks the pattern here. A simulated intelligence is intelligent, just as simulated math is math and simulated computers are computers. The point of contention shouldn't be whether LLMs are intelligences or simulated intelligences, but whether they're simulating something else. reply qarl 3 hours agorootparentRight. This is Searle's \"a simulated plane won't get you to Japan\" argument. That's true. But a simulated calculator is perfectly effective for doing your taxes. reply ben_w 1 hour agorootparentprev> To say that LLMs are intelligent means that language is a requirement for intelligence. Thats some fairly magical thinking... buy any sufficently advanced technology.. When I was a kid, it was the definition of intelligence that separated humans from animals. And there's a reason \"dumb\" means \"mute\" and independently \"stupid\". It may well be an incorrect requirement. It may be a single form of intelligence out of many which happen to correlate in humans, but not in minds created by artifice. But it does have a history. reply theendisney 5 hours agorootparentprevNot sure if it counts but there is a police chase video online some place with a guy on drugs who claims he thought he was playing gta. The way he throws people out of their vehicle and crashes their car suggests he wasnt lying. reply ben_w 1 hour agorootparentprev> We definitely converse on autopilot, indistinguishably from LLMs talking to each other, after an opening line every word of every sentence in the entire exchange perfectly predictable to a stranger Some people report speaking like this: opening their mouths and not knowing how the sentence will end. I don't experience that, I think. Possibly used to? I have in the past had some autonomous verbal responses, for a bit this included echoing greetings — great when it's \"hello\", embarrassing when it's \"happy birthday\". > To say LLMs are not intelligent is easier if we agree humans spend a good deal of time being unintelligent Kinda; System 1, system 2 — the best LLMs do better than most people's system 1, worse than most people's system 2. Bat and ball, $1.10. reply krainboltgreene 4 hours agorootparentprev> On the contrary, sit and listen in a college cafeteria, and it quickly becomes apparent most conversation participants are LLMs.* Excuse the bluntness, but you're the CTO of a fintech company. Your analysis of people's social life is probably the as valuable as a janitors. reply david-gpu 21 minutes agorootparentLet's address what is being said, rather than who is saying it. The latter doesn't turn into an interesting conversation. reply taberiand 4 hours agorootparentprevI expect an observant janitor would have quite useful insights into people's social lives reply ben_w 1 hour agorootparentprev> If you buy into this line of thinking then it quickly becomes apparent that LLM's are more or less a dead end when it comes to AGI. Simulating isnt emulating... an LLM is as likely to become intelligent as a forecast is to control the weather Up until this point, I agree. This puts humans on too high a pedestal: LLMs aren't magic, and we're not magic either. (There's other reasons for me to think Transformers aren't the answer, but not this kind of reasoning). reply sangnoir 1 hour agorootparent> we're not magic either We pretty much are compared to present-day neural architectures. How many simulated neurons and synapses are in the largest architectures, and how do those numbers compare to humans? reply dartos 32 minutes agorootparentIt’s a non starter to assume that virtual “synapses and neurons” behave like ours do. We barely understand how ours works. Also, modern LLMs built on the transformers architecture no longer use the neuron-inspired perceptron style topology for most of their compute. I’ve heard that spiking NNs are supposed to mimic organic brains more closely, but I haven’t read into them much yet. reply RyEgswuCsn 7 hours agorootparentprevExcept that a weather forecasting model can't experiment on weather, but a LLM system may be designed to be able to perform experiments and take feedbacks? reply ekianjo 7 hours agorootparentprev> LLM's are more or less a dead end when it comes to AGI. I don't think many people believe that LLMs are a way to AGI (whatever that actually means). But LLMs can still have many valid uses even if their prospects are limited in scope. reply buu700 4 hours agorootparentI recently read an interesting thread that laid out the case for LLMs being a path to AGI: https://old.reddit.com/r/singularity/comments/13ox85j/how_do... The argument boils down to the idea that language isn't simply strings of words or bits of factual information, but an actual encoding of logic. By training statistical models on vast amounts of logic, we've given them a generalizable ability to perform logic. A sufficiently advanced LLM could thus potentially fulfill some definition of AGI. To be clear, this doesn't in any way imply that LLMs could ever fit the definition of artificial consciousness, which would be a completely different form of strong AI. They're effectively just mathematical functions (albeit extremely complicated ones), which simply take inputs and return outputs without any intervening subjective experience. Even if they can perform a complicated task, retrieve and effectively summarize complicated information, or say all the right things as a conversational partner, they have no concept of the meaning of their output. Maybe that limitation in itself puts a ceiling on their potential. Maybe the best possible LLM can only ever be 99.99% effective, and that 0.01% of the time it will go completely off the rails and disregard its instructions or hallucinate something ridiculous. Maybe the only way to overcome that is by keeping a human or a true artificial consciousness in the loop, in which case LLMs would still be extremely useful, but a flawed AGI if \"AGI\" at all. Or maybe a sufficiently advanced LLM and/or a sufficiently advanced error correction architecture will actually be enough to mitigate those issues. I don't have a strong opinion on where LLMs are ultimately headed, but I'm looking forward to seeing how it all unfolds. It's amazing how capabilities that were strictly in the realm of sci-fi so quickly became mundane. reply aerhardt 1 hour agorootparentprevThere are plenty of people - technical and non-technical - who seem to be acting like AGI is right around the corner thanks to LLMs, and who are, more broadly, vastly overstating the current capabilities of LLMs. I’m observing this in real life as much as on the internet. There are two very distinct groups of people that stand out to me: (1) High level execs with vested interests around AI and (2) Managers who haven’t even bothered to create an OpenAI account and are asking their subordinates to use ChatGPT for them, in what is an unforeseen usage of LLMs: by human proxy. reply zer00eyz 6 hours agorootparentprev>> I don't think many people believe that LLMs are a way to AGI Please tell Sam Altman ASAP Thanks! reply dartos 31 minutes agorootparentYou think he doesn’t know? Everything he says is marketing for OpenAI. Same as any other CEO with their company. reply DiogenesKynikos 7 hours agorootparentprevLLMs already are intelligent. They're not the same as humans, but they are able to give intelligent answers to highly nontrivial questions. reply hackable_sand 6 hours agorootparentI have yet to see an LLM that is cooperative. The magic of collaborating with someone is that we can both understand the problem and reason about it. The current degree of LLM intelligence is not compelling for a social creature like me. reply ben_w 1 hour agorootparentSurprised to read that. I use them as a cooperative partner by default. Also: quite a few people have had instances work with other instances, sometimes of the same model and sometimes of other models. reply theendisney 5 hours agorootparentprevIs it even allowed to ask questions?? Edit: my sience fiction joke in the 90s was AI though bots chatting in irc channels. They could seemlesly integrate human intelligence that way. reply koonsolo 56 minutes agorootparentprevHave you ever talked to real average people? I would say an LLM is more intelligent than at least some people I know. And in the domain of programming, most people I know. Simply by the fact that most people don't know programming. reply dartos 29 minutes agorootparentProgrammers aren’t any better than someone who doesn’t know how to program. Programming skill isn’t a measure of intelligence. Go outside. Talk to real people. Touch some grass. reply Paradigma11 16 minutes agoparentprevMy impression from github copilot is that hallucinations are the result of certain true facts having a low likelihood and copilot giving you the most likely answer anyway. Typically I have a certain library that does things in a very unorthodox and undocumented way and when I ask copilot for an example it gives me wonderful, totally understandable code of made up functions that I wouldnt need in the first place if the library worked that way. I dont think that running that query multiple times would help. reply 0x008 3 hours agoparentprevThis is a very similar idea to ensemble models, which have been used for a long time in ML and proven to be very good. You average out the results of several predictors (or you let them vote and pick the most common prediction value), thereby reducing the noise in the prediction by choosing the common denominator of multiple predictions. reply sroussey 2 hours agorootparentThis is done in aerospace as well… however, even different teams clean room writing to the same spec have the tendency to make the same errors in their code, which ends up breaking the statistical model when this model was selected. reply ben_w 1 hour agoparentprev> Which makes sense to me. If an LLM is giving you a mixture of \"hallucinations\" and correct answers, the correct answers will similar and the hallucinations will hopefully be chaotic I expect that to give you something close to the confidence of the underlying model to some specific claim, which is good, but I still expect legends (urban and cultural) to be high-ranked. They'd be very human mistakes, but still mistakes. I think the only way past that is to build a world model, look for contradictions, and then look for new evidence to resolve those contradictions. reply dmarchand90 1 hour agorootparentBe interesting to plug this into a bayesian optimization like framework: find out regions of language space where the models maximally disagree and then target those areas for extra training reply m_kos 3 hours agoparentprevI had a very similar idea a few months ago. I wanted to use this approach to have the LLM provide the probability that the generated answer is correct. The probability would simply be what fraction of all generated answers was the one selected. (Each generated answer would be generated with a different seed and the question would be of single choice kind.) The two issues I found were 1) the cost, 2) on some problems, LLMs can be wrong more often than they are not. Hopefully, as inference gets cheaper and of higher quality, someone will come up with a more feasible solution. reply sinuhe69 7 hours agoparentprevBut if I set the temperature to 0, the model will pick the highest probable token and the output will be always the same. But we already know that by no mean it can guarantee a correct answer. So how can multiple runs be better? reply phire 7 hours agorootparentYes, but picking the most similar output from a bunch of queries with a higher temperature is not the same thing as the output from a single low temperature query. reply sinuhe69 6 hours agorootparentPossibly, but it stills doesn’t explain why multiple runs will result in better answer. In the work, the authors also hasn’t compared the multiple runs results with the single run using zero temperature. So, maybe all the overhead is just to achieve the same result already encoded in the networks? I don’t know. Also the result is somewhat counterintuitive. We know that by low level of understanding, if we ask a student a hard question and he tried many times, the most accurate answer is often not the most popular one but a single answer. And that by retaining memory, reasoning capacity and continuous learning , which is not the case with LLM. Btw: HN is for discussion. If some just want to vote for the beauty contest, please leave. reply phire 5 hours agorootparentI found this other paper that tests Temperature: https://arxiv.org/abs/2402.05201 It appears that temperature has no impact on problem solving performance. So this paper isn't getting improved performance because the token for the correct answer is more probable. My theory is that the multiple queries are allowing the whole probability space of possible answers to be sampled. Not just the probabilities of the most likely output token, but the probabilities of all possible internal model states. And sampling that probability space of the whole model state and finding the average is a very different mathematical operation to just picking a single model state at random and then picking the most probable output tokens. reply bt1a 4 hours agorootparentIf I'm reading this correctly, they had to discard Llama 2 answers and only use GPT-3.5 given answers to test the hypothesis. GPT-3.5 answering questions through the OAI API alone is not an acceptable method of testing problem solving ability across a range of temperatures. OpenAI does some blackbox wizardry on their end. There are many complex and clever sampling techniques for which temperature is just one (possibly dynamic) component One example from the llama.cpp codebase is dynamic temperature sampling https://github.com/ggerganov/llama.cpp/pull/4972/files Not sure what you mean by whole model state given that there are tens of thousands of possible tokens and the models have billions of parameters in XX,XXX-dimensional space. How many queries across how many sampling methods might you need? Err..how much time? :) reply mlsu 4 hours agorootparentprevI wonder if there is a clever/more efficient shortcut that could come from before the sample is taken on each token. We have the logits after all. reply numpad0 4 hours agorootparentprevJust from reading comments around, it feels intuitive to me that looking at a heatmap of cascading pendulum would be more “accurate” than looking at just one snapshot, and also that joints on the pendulums don’t necessarily need to be interlinked between iterations of simulations reply wokwokwok 4 hours agorootparentprev> Also the result is somewhat counterintuitive. We know that by low level of understanding, if we ask a student a hard question and he tried many times, the most accurate answer is often not the most popular one but a single answer. This is a bad analogy. Here’s what is actually happening with no “common sense but wrong” understanding of it: - You have a set of probabilities per token. - You randomize them. This is not a “bad student being asked multiple times” it is a system with randomized probabilities, creating a probability distribution. If you want to see what a probability distribution looks like (eg. An electron cloud) then sampling only once is the wrong way to do it. You basically have two distributions; the first one is the LLM, the second one is the shape generated by adding the random factor in the temperature. This allows you to escape the “local maxima” encoded in the LLM distribution to find highly probable solutions that are outside the sample space of the “zero temperature”. If you want a better analogy, look up at the night sky full of stars. Draw circle in the sky; that’s the LLM distribution. The result from a zero temperature will be the brightest point in that circle. When you push the temperature up, you blur the sky randomly. Some points become brighter, some dimmer, but the radius of the circle increases. If there is a very bright point outside the sample circle 10x brighter than the brightest point inside it then repeated random samples will repeatedly find it. It makes perfect sense that an expanded probability distribution sampled repeatedly could find a “good average solution” if that solution is significantly better than the best “zero temp” solution. This is the same reason we have 'temp' at all; by widening the solution space probability distribution, you can find better maxima. Turns out, sampling multiple times lets you have more chances to find better maxima. This is more like \"well that seems obviously like a good idea\" than \"somewhat counterintuitive\"; it's just slow and expensive to do it. You can also adjust the probability distribution by other existing methods, obviously, what's surprising here is not that it works, but that it seem to work so well; probably (and I note they did not try this in their paper), a multi-sample + voting on the output from other methods would also be highly effective. reply smusamashah 6 hours agoparentprevCould multiple agents be used such that tokens emitted from LLM A is passed to B and output of B is passed to A meaning 2 agents will be being used to generate an output in a simple round Robin way? Both will share context in this case. My computer isn't big enough run two large models but this can be tried on tiny models perhaps. I realize that for more than two and very specialised agents this will require some intelligent way to pass the output to specialist agents only. And also this means that their must be some overlap between the agents. reply Sharlin 4 hours agorootparentThat is what’s already been done under the term \"multi-agent\". This paper argues that there’s no need for any such message-passing or context sharing, you just literally run the same query several times on the same model, fully independently, and then pick a \"typical\" reply according to some similarity metric. reply xcv123 3 hours agorootparentThe paper says that it enhances multi-agent methods. It is not a replacement for that. It's an enhancement for existing methods. reply mirekrusin 3 hours agoparentprevGood news is that you can use this setup for self supervised RL (artificial dreaming? increasing contrast?). reply whimsicalism 6 hours agoparentprevhow is chain-of-thought multi-agent? reply Sharlin 4 hours agorootparentHow is what’s described here chain-of-thought? reply xcv123 4 hours agorootparentThey were replying to this: > This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate. reply xcv123 3 hours agoparentprev> I'm not sure people in these comments are reading this paper correctly. > This seems to essentially disprove the whole idea of multi-agent setups like Chain-of-thought and LLM-Debate. I'm not sure you have read the paper at all. Chain of thought prompting is not a multi-agent algorithm. The paper says that it enhances existing methods such as prompt engineering (chain of thought) and multi-agent debate. The sampling method presented in the paper is orthogonal to those methods. reply infogulch 9 hours agoprevThis seems related to an interesting recent ACM ByteCast podcast episode with Edward Chang, an Adjunct Professor in the Department of Computer Science at Stanford University. [1] (Note there is a transcript if you don't want to listen.) The approach he uses is to arrange for multiple LLMs to dialogue between each other about a discussion topic where the human acts as a moderator instead of the question/answer format that LLMs commonly take today. They find that the final answer that multiple LLMs come to in dialogue results in a huge improvement in both precision and accuracy for the same resources. [1]: https://learning.acm.org/bytecast/ep50-edward-y-chang reply rahimnathwani 7 hours agoparentThis paper suggests that you don't need the debating part: just get LLM to work on the problem independently, and choose the most popular answer. reply xcv123 3 hours agorootparentThe paper says that it enhances existing methods such as prompt engineering (chain of thought) and LLM debate. This agent method is orthogonal to LLM debate. reply infogulch 6 hours agorootparentprevInteresting. Somehow it seems odd to add randomness (temperature) and then wash it away by averaging it out. reply naasking 5 hours agorootparentIn optimization problems, randomness can often get you out of local minima/maxima, and so averaging out a bunch of random search paths might get you better results in the worst case. Something similar might be happening here. The training set will be biased in various ways that might create weird local min/max points and so this process could avoid those weird kinks. reply ewild 6 hours agorootparentprevtemp applies to each token so the range of temperature is significantly larger than the average being pulled reply j45 7 hours agoparentprevIs this effectively describing something like crewai? reply kromem 9 hours agoprevFinally. I've been saying that we need to stop focusing on a single agent getting everything right and instead layer agents for about 16 months now, but it's great to have a paper to point to. It's interesting that the diminishing returns for tasks flatten out rapidly around the same size as the ideal human meeting sizes: https://www.researchgate.net/figure/18-Optimal-Meeting-Sizes... If this was done at more granular steps of agent quantity I'm curious just how closely it would match those numbers. I'd also really love to see the eventual follow-up where we see how much more performance can be obtained when the agents are each fine tuned towards slightly different aims. I'd expect there'd even be a performance lift from just having the agents each set at different temperature levels. Very happy to see the research community starting to step in this direction! reply blazingbanana 8 hours agoparentI couldn't agree more. You should check out LLMWare's SLIM agents (https://github.com/llmware-ai/llmware/tree/main/examples/SLI...). It's focusing on pretty much exactly this and chaining multiple local LLMs together. A really good topic that ties in with this is the need for deterministic sampling (I may have the terminology a bit incorrect) depending on what the model is indended for. The LLMWare team did a good 2 part video on this here as well (https://www.youtube.com/watch?v=7oMTGhSKuNY) I think dedicated miniture LLMs are the way forward. Disclaimer - Not affiliated with them in any way, just think it's a really cool project. reply intended 3 hours agorootparentGreat videos. I have one personal niggle: I get annoyed when we end up lying to ourselves. Regarding the 101 section in video 1 - People forgot this the day LLMs came out. I felt this was too generous with the benefit of doubt. This basic point was and remains constantly argued - with “Emergence” and anthropomorphization being the heart of the opposing argument. reply piloto_ciego 9 hours agoparentprevThis is how I think humans work. We have 5 or 8 versions of us running around in our skulls or whatever and one of them is somewhat of a supervisor. reply Trasmatta 7 hours agorootparentI think it's way more than 8 even. And it's common to have many working as supervisors, often at conflict with each other. And some act out the automatic trauma responses, as they're stuck in the past when the trauma occurred. reply champdebloom 3 hours agorootparentThis sounds very much like the internal family systems model: https://en.m.wikipedia.org/wiki/Internal_Family_Systems_Mode... reply piloto_ciego 6 hours agorootparentprevRight it’s like a random forest or something maybe. reply xnx 8 hours agorootparentprevMulti Homunculus Model reply piloto_ciego 6 hours agorootparentI was thinking something like Julian Jayne’s Bicameral mind, but you’re always arguing with yourself lol reply exe34 2 hours agorootparentprevSounds like dennett's multiple drafts hypothesis. reply hackable_sand 6 hours agorootparentprevMaybe. I'm sure one's consciousness corresponds with one's guiding philosophy. I don't think this supervisor model is generally applicable to people with EFD or some forms of Autism, for example. reply nickpsecurity 7 hours agorootparentprevWe have tons of specialized components that work together cooperatively and competitively. There’s multiple ways they connect. There also seems to be global processes that happen, like during sleep. There’s over 3,000 cell types per the BRAIN initiative. Every brain forms on it’s own taking shape like something out of a Transformers movie. God’s design is mostly nothing like man’s neural networks. It’s far superior. Brains are also what’s creating all the artificial, neural nets on top of all math, tech, and economic systems that they run on. AI’s got a lot of catching up to do. reply huffmsa 9 hours agorootparentprevI personally visualize them at a table from time to time. reply infogulch 8 hours agorootparentDo they have to collaborate to decide who gets to use the forks? reply naasking 5 hours agorootparentOf course not, you don't need forks while playing poker. reply Terr_ 8 hours agorootparentprevThat sounds like a culturally bound phenomenon, shouldn't they be using chopsticks? reply infogulch 7 hours agorootparentChopsticks makes sense because there's only 5 of them and they have to share the 5 chopsticks. reply _ink_ 8 hours agorootparentprevHow do they look? Do they look differently? reply suriya-ganesh 8 hours agorootparentprevAnd et voila, you have the script of inside out. \\s But honestly I do think this is how we operate. Depending on our state of metabolism and other psychological factors, the dominant version changes but as a whole we remain the sum total of all these versions. reply jondwillis 8 hours agoparentprevI was working on multi-agent systems for problem solving via https://github.com/agi-merge/waggle-dance for months last year! reply zarathustreal 9 hours agoparentprev“Each fine tuned toward slightly different aims” So…a sort of mixture of experts if you will reply kromem 4 hours agorootparentKind of. More like a mixture of a mixture of experts. The problem is MoE on its own isn't able to use the context as a scratch pad for differentiated CoT trees. So you have a mixture of token suggestions, but a singular chain of thought. A mixture of both is probably going to perform better than just a mixture of the former, especially given everything we know by now regarding in context learning or the degree of transmission synthetic data is carrying. reply j45 7 hours agoparentprevIt seems funny that the researchers are studying what people are building to experiment. crewAI is one example. reply nicklecompte 9 hours agoprevOne frustration I've had with all this mixture-of-experts research: Randomized Algorithms 101 - or basic stochastic reasoning - suggests that if the temperature parameter is > 0, querying an LLM N times and picking the majority result (perhaps with an N+1th query to the LLM) will generally result in better performance than asking it once and choosing that result. It seems plausible to me that the gains can be further improved with a specialized mixture of different LLMs (which could then be run at temp = 0), or by finding better ways to break tasks into subtasks as this paper suggests. But AFAICT nobody has done anything to actually quantify these hypothetical gains versus the dumb randomized algorithm approach! In particular there might be voting strategies or mixtures - even specific models - where MoE/etc is strictly worse than naive repetition. I am a concerned citizen w.r.t LLMs rather than a researcher, so I might be missing something. It just seems odd that LLM researchers forgot the first chapter of Motwani/Raghavan. reply benaubin 7 hours agoparentI'd assume that there's a difference between picking the best _token_ across an assortment of randomly selected tokens, versus picking the best _string_ of randomly-selected tokens. reply skybrian 8 hours agoprevEyeballing the graphs, it seems that most of the gain is with 10 agents, a bit more with 20, and there are diminishing returns after that. Apparently, more agents isn't going to do it. reply yantrams 1 hour agoprevThis is my go to method for pretty much every hard problem that I'm forced to solve where I don't have the domain expertise / interest / time. The trick lies in coming up with a clever similarity metric that incorporates penalties etc. You can even go a level deeper and use multiple similarity algorithms and then poll on top of them. Here's a taxonomy extractor for text that I made using similar principles that is surprisingly as good as anything else that I've seen - https://dash.scooptent.com/text reply nico 11 hours agoprevThey have a public repo: https://anonymous.4open.science/r/more_agent_is_all_you_need... Prompts they used for the benchmarks: https://anonymous.4open.science/r/more_agent_is_all_you_need... Super interesting. It would be cool to see something like this, but benchmarking LLM-based agents using a set of tools reply trash_cat 6 hours agoprevIs this not an incredibly expensive/unsustainable method? I agree with the sentiment that MoE is the way to go as the newer models will probably see diminishing returns. But the compute for a single prompt will suddenly increase 7-15 fold? reply ukuina 5 hours agoparentIf GPT4 is 20x the price of GPT3.5, but it only takes 10x GPT3.5 runs to get similar quality of response (and likely faster), you'll still come out ahead. reply dimask 35 minutes agorootparentI doubt that 10xGPT3.5 > GPT4. There are a lot of tasks that GPT4 can do and GPT3.5 just cannot. Also, in such cases I find that GPT3.5's hallucinations are quite consistent, so such a method is probably not gonna help. reply bearjaws 6 hours agoparentprev\"all you need is a six figure OpenAI bill\" reply imtringued 2 hours agoparentprevSo what? It's not like GPUs are compute starved. reply bigEnotation 6 hours agoparentprevYeah, see GPT 3.5 vs GPT 4 pricing. reply atum47 7 hours agoprevIf we sum all those \"x is all you need\" we're going to realize that we need a lot of things reply hopfenspergerj 9 hours agoprevEnsemble of any number GPT 3.5 agents is less accurate than one call to GPT-4. reply trash_cat 6 hours agoparentIt's funny because GPT-4 is actually a pile of 3.5s. You just need to set it up correctly. reply BinRoo 6 hours agorootparent> GPT-4 is actually a pile of 3.5s I understand the intension and reference you're making. I bet the implementation of GPT-4 is probably something along those lines. However, spreading speculation in definitive language like that when the truth is unknown is dishonest, wouldn't you agree? reply jamala1 6 hours agorootparentprevI guess it's the difference between an ensemble and a mixture of experts, i.e. aggregating outputs from (a) model(s) trained on the same data vs different data (GPT-4). Though GPT-4 presumably does not aggregate, but it routes. reply TheCaptain4815 9 hours agoprevRecommend anyone interested in a agent framework lookup AutoGen by Microsoft reply ukuina 5 hours agoparentThis paper is specifically disproving the efficacy of agentic frameworks like AutoGen. Also, the built-in function-calling in GPT4 is simpler to use than AutoGen2's abstraction. reply dindobre 41 minutes agoprevSmells similar to Sutton's bitter lesson! reply whiteandnerdy 3 hours agoprevI remember hearing that Beam Search doesn't work well for LLMs, because it leads to repetitive, generic output. The majority vote sampling technique in this paper sounds like it'd give similar output to Beam Search, because it's sampling sequences of tokens from a joint distribution. So why doesn't it give repetitive output like Beam Search does? What am I missing? reply sheepscreek 5 hours agoprevHaving given this problem a great deal of thought, I have developed a strong intuition around this. I believe not only is AGI feasible, it is already doable. For example, several hundred GPT-4 based agents specializing in different skill sets should be able to collaboratively solve many problems. Their ability to work on so many facets of the same problem will make them very effective against multidisciplinary problems. What’s the catch? Well, the back and forth has to play out in a serial order, so it cannot be parallelized. At today’s abysmal inference speeds, it may take this AGI many times longer than a trained human. Now imagine the effectiveness of this method when we can speed up inference to several hundred times a minute. Now AGI suddenly becomes way more efficient than a human. reply intended 3 hours agoparentDefinition of AGI at play here? reply 29athrowaway 5 hours agoparentprevThere's a paper about that https://www.microsoft.com/en-us/research/publication/sparks-... reply Buttons840 9 hours agoprevSo this is an ensemble of many LLMs? I wonder how well a bunch of LLMs trained on personal computers, so fairly small, could perform together? Train a LLM on your emails, train an LLM on a text book, download a bunch of arbitrary LLMs from the net you find interesting, throw them all together into a big pile, and use a moderator LLM that knows how to format their output into an assistant format. So, the email LLM would try to autocomplete sentences from your emails, and the text book LLM would try to autocomplete sentences from the text book. People could offer LLMs to download, almost as a way of compressing information, download the LLM of your favorite programming language, and TV series, etc. The important part would be having a moderator algorithm that can shape these LLMs from dumb sentence autocompleters (barely more than a fancy Markov chain) into a coherent assistant format. For example, the text book LLM would just endlessly spew semi-random sentences from the text, but a good moderator algorithm could see that it has sufficiently answered the question and cut it off. In short, it's interesting that separate LLMs can integrate with each other and strengthen each other and it makes me wonder if we could build modular LLMs. reply mofosyne 8 hours agoparentYour idea inspired me to see what such a microstory based on your idea would look like (Of course generated by ChatGPT3.5): > As I delved into my computer, eager to tackle my to-do list, I was met with an unexpected sight: a digital love triangle among the Language Models (LLMs). The Email LLM, with its quick wit, seemed to be engaging in flirtatious banter with the verbose Textbook LLM, while the Programming Language LLM watched on with amusement. I couldn't help but laugh at the absurdity of it all, but as the bickering between the LLMs intensified, I realized their antics were hindering my progress. With a mixture of frustration and amusement, I gently redirected the LLMs back to their intended purpose, finally able to accomplish my task amidst the chaotic comedy within my computer. reply bearjaws 6 hours agoprevI'm sure all the AI companies love the idea of running the same prompt 8 times... reply datascienced 6 hours agoparentNVDA does for sure reply Havoc 8 hours agoprevHere is a link to the main diagram: https://anonymous.4open.science/r/more_agent_is_all_you_need... Seems like a pretty brute force approach of frankly just throwing more compute at the query (via semi-statistical means). I'd be more interested in how to scale this via different agents. i.e. do we have say one type of agent that is specialized to produce ideas, while another is trained to evaluate ideas. Those sort of chains seem like they'd be powerful - if you can find a way to generalize it reply ShamelessC 9 hours agoprevThis usage of the word \"agent\" when they simply mean \"another LLM\" is sort of nonstandard, no? To me an agent implies some degree of RL. reply etamponi 3 hours agoprevIsn't this the same as lowering the temperature of the LLM? reply kshitij_libra 7 hours agoprevThis trend really needs to die. If you can’t come up with an original paper name , maybe the contents aren’t that original either reply phkahler 9 hours agoprevDoes it take less compute to train N agents vs one large model? Seem like a big win. Can the majority of the training be done independently or in distributed fashion? reply parentheses 7 hours agoprevI wonder if this performs even better when this is done tokenwise in the inner loop of the LLM reply bigEnotation 6 hours agoprevI thought this is what GPT 4 was, it uses a boosting algorithm over GPT 3.5? reply al2o3cr 8 hours agoprev35x more reasoning for a 10% increase in accuracy? Scaling with a scale factor of almost zero is still scaling, I guess. reply m3kw9 9 hours agoprevAll these AI researchers treating “..is all you need” a meme or something. reply Intralexical 9 hours agoparentI think they've been overfitted. reply latentsea 9 hours agoparentprevAll you need is all you need. reply spencerchubb 8 hours agoprevThis trend of \"All You Need\" in paper titles needs to die. The original \"Attention is All You Need\" used that title because it is literally true in their case. So many papers just use it as a meme now, and it distracts from the true insight of the paper. reply donovanr 8 hours agoparentit's uncreative/tired but at least to the point. Too many papers are confused / opaque agglomerations of a year's worth of research shoehorned into a paper. At least with these you can fairly easily assess whether the claim is supported or not. reply Terr_ 8 hours agoparentprev\"All You Need Considered Harmful.\" reply naruhodo 6 hours agorootparentWhat if \"Considered Harmful\" is considered harmful? reply benaubin 7 hours agorootparentprev\"'All You Need Considered Harmful.' Is All You Need\" reply nickpsecurity 7 hours agorootparentprev“Make All You Need Great Again Considered Harmful.” reply jordanpg 7 hours agoparentprevI think more or less the same thing about the word \"meme\" as you have used it. reply soulofmischief 8 hours agoparentprevIt's equally annoying to see this comment crop up every single time someone uses the phrase. reply karagenit 7 hours agorootparentAnd it’s equally annoying to see this comment about that comment about the paper every time. Recursion! reply soulofmischief 4 hours agorootparentThis is why I come to Hacker News :) reply EGreg 9 hours agoprevHow about swarms of autonomous agents, such as AutoGPT, maybe thousands per human eventually, amassing karma points on all forums, including this one? I can see in a few years each human being surrounded by a ton of LLM agents, \"shepherding\" their views, downvoting their messages or distracting them with argumentative conversations if they don't conform, and facilitating reputational attacks on scale on all the people whose speech is recognized as being contrary to what's desired. Of course, there wouldn't be just one group deploying these swarms. It would be lots of different groups, akin to slaughterbots video: https://www.youtube.com/watch?v=O-2tpwW0kmU The difference is that there wouldn't be physical violence, it would just gradually turn the entire Internet into a dark forest. reply bhouston 9 hours agoparentThe book a Fire Upon the Deep had everyone surrounded by AI agents which became extensions of themselves. reply williamcotton 9 hours agoparentprevWell then most will be demanding that our governmental institutions issue cryptographically signed ID cards verified by an in-person visit to the DMV. Or, you choose to opt out and swim in a sea of nonsense. reply AnimalMuppet 9 hours agoprevEchoes of Society of Mind by Marvin Minsky. reply rubslopes 8 hours agoparentI don't know why you were downvoted. Although Minsky's work has been highly criticized, it also has been influential in AI research. reply l_l_m_5_0 3 hours agoprev [–] Averaging LLM outputs will ensure the final output will contain a lot of words with no substance. However, it’s essential to recognize that averaging bad data doesn’t always lead to better results. Garbage in, garbage out — averaging cannot magically transform flawed inputs into accurate outputs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Increasing the number of agents through sampling-and-voting enhances the performance of large language models, proven effective across various benchmarks and independent of complex existing methods.",
      "The authors conducted extensive experiments to validate their findings and have openly shared their code for replication.",
      "The study emphasizes the scalability of language models concerning the number of agents and its effect on task complexity."
    ],
    "commentSummary": [
      "The article delves into the utilization of Large Language Models (LLMs) in AI and the ongoing debate about their capability to achieve Artificial General Intelligence (AGI).",
      "It discusses the effectiveness of multi-agent setups, the limitations of LLMs in emulating intelligence, and the obstacles in imitating human intelligence in artificial systems.",
      "The potential of employing multiple runs of LLMs, ensemble models, and specialized agents is examined, alongside concerns regarding the reliability and potential misuse of content generated by LLMs."
    ],
    "points": 210,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1712438750
  },
  {
    "id": 39956008,
    "title": "WinBtrfs v1.9: Bringing Btrfs to Windows with RAID and More",
    "originLink": "https://github.com/maharmstone/btrfs",
    "originBody": "WinBtrfs v1.9 WinBtrfs is a Windows driver for the next-generation Linux filesystem Btrfs. A reimplementation from scratch, it contains no code from the Linux kernel, and should work on any version from Windows XP onwards. It is also included as part of the free operating system ReactOS. If your Btrfs filesystem is on a MD software RAID device created by Linux, you will also need WinMD to get this to appear under Windows. See also Quibble, an experimental bootloader allowing Windows to boot from Btrfs, and Ntfs2btrfs, a tool which allows in-place conversion of NTFS filesystems. First, a disclaimer: You use this software at your own risk. I take no responsibility for any damage it may do to your filesystem. It ought to be suitable for day-to-day use, but make sure you take backups anyway. Everything here is released under the GNU Lesser General Public Licence (LGPL); see the file LICENCE for more info. You are encouraged to play about with the source code as you will, and I'd appreciate a note (mark@harmstone.com) if you come up with anything nifty. See at the end of this document for copyright details of third-party code that's included here. Donations I've been developing this driver for fun, and in the hopes that someone out there will find it useful. But if you want to provide some pecuniary encouragement, it'd be very much appreciated: Paypal Features Reading and writing of Btrfs filesystems Basic RAID: RAID0, RAID1, and RAID10 Advanced RAID: RAID5 and RAID6 Caching Discovery of Btrfs partitions, even if Windows would normally ignore them Getting and setting of Access Control Lists (ACLs), using the xattr security.NTACL Alternate Data Streams (e.g. :Zone.Identifier is stored as the xattr user.Zone.Identifier) Mappings from Linux users to Windows ones (see below) Symlinks and other reparse points Shell extension to identify and create subvolumes, including snapshots Hard links Sparse files Free-space cache Preallocation Asynchronous reading and writing Partition-less Btrfs volumes Per-volume registry mount options (see below) zlib compression LZO compression LXSS (\"Ubuntu on Windows\") support Balancing (including resuming balances started on Linux) Device addition and removal Creation of new filesystems with mkbtrfs.exe and ubtrfs.dll Scrubbing TRIM/DISCARD Reflink copy Subvol send and receive Degraded mounts Free space tree (compat_ro flag free_space_cache) Shrinking and expanding Passthrough of permissions etc. for LXSS Zstd compression Windows 10 case-sensitive directory flag Oplocks Metadata UUID incompat flag (Linux 5.0) Three- and four-disk RAID1 (Linux 5.5) New checksum types (xxhash, sha256, blake2) (Linux 5.5) Block group tree (Linux 6.1) Todo Full fs-verity support (Linux 5.15) Zoned support (Linux 5.11) (HM-SMR not supported on Windows?) Defragmentation Support for Btrfs quotas Full transaction log support Support for Windows transactions (TxF) Installation To install the driver, download and extract the latest release, right-click btrfs.inf, and choose Install. The driver is signed, so should work out of the box on modern versions of Windows. If you using Windows 10 and have Secure Boot enabled, you may have to make a Registry change in order for the driver to be loaded - see below. WinBtrfs is also available on the following package managers: Chocolatey choco install winbtrfs Scoop scoop bucket add nonportable scoop install winbtrfs-np -g Uninstalling If you want to uninstall, from a command prompt run: RUNDLL32.EXE SETUPAPI.DLL,InstallHinfSection DefaultUninstall 132 btrfs.inf You may need to give the full path to btrfs.inf. You can also go to Device Manager, find \"Btrfs controller\" under \"Storage volumes\", right click and choose \"Uninstall\". Tick the checkbox to uninstall the driver as well, and let Windows reboot itself. If you need to uninstall via the registry, open regedit and set the value of HKLM\\SYSTEM\\CurrentControlSet\\services\\btrfs\\Start to 4, to disable the service. After you reboot, you can then delete the btrfs key and remove C:\\Windows\\System32\\drivers\\btrfs.sys. Compilation To compile with Visual C++ 2019, open the directory and let CMake do its thing. If you have the Windows DDK installed correctly, it should just work. To compile with GCC on Linux, you will need a cross-compiler set up, for either i686-w64-mingw32 or x86_64-w64-mingw32. Create a build directory, then use either mingw-x86.cmake or mingw-amd64.cmake as CMake toolchain files to generate your Makefile. Mappings The user mappings are stored in the registry key HKLM\\SYSTEM\\CurrentControlSet\\services\\btrfs\\Mappings. Create a DWORD with the name of your Windows SID (e.g. S-1-5-21-1379886684-2432464051-424789967-1001), and the value of your Linux uid (e.g. 1000). It will take effect next time the driver is loaded. You can find your current SID by running wmic useraccount get name,sid. Similarly, the group mappings are stored in under GroupMappings. The default entry maps Windows' Users group to gid 100, which is usually \"users\" on Linux. You can also specify user SIDs here to force files created by a user to belong to a certain group. The setgid flag also works as on Linux. LXSS (\"Ubuntu on Windows\" / \"Windows Subsystem for Linux\") The driver will passthrough Linux metadata to recent versions of LXSS, but you will have to let Windows know that you wish to do this. From a Bash prompt on Windows, edit /etc/wsl.conf to look like the following: [automount] enabled = true options = \"metadata\" mountFsTab = false It will then take effect next time you reboot. Yes, you should be able to chroot into an actual Linux installation, if you wish. Commands The DLL file shellbtrfs.dll provides the GUI interface, but it can also be used with rundll32.exe to carry out some tasks from the command line, which may be useful if you wish to schedule something to run periodically. Bear in mind that rundll32 provides no mechanism to return any error codes, so any of these commands may fail silently. rundll32.exe shellbtrfs.dll,CreateSubvolrundll32.exe shellbtrfs.dll,CreateSnapshot rundll32.exe shellbtrfs.dll,ReflinkCopy This also accepts wildcards, and any number of source files. The following commands need various privileges, and so must be run as Administrator to work: rundll32.exe shellbtrfs.dll,SendSubvol[-p ] [-c ]The -p and -c flags are as btrfs send on Linux. You can specify any number of clone subvolumes. rundll32.exe shellbtrfs.dll,RecvSubvol rundll32.exe shellbtrfs.dll,StartScrubrundll32.exe shellbtrfs.dll,StopScrubTroubleshooting How do I debug this? On the releases page, there's zip files to download containing the PDBs. Or you can try the symbols server http://symbols.burntcomma.com/ - in windbg, set your symbol path to something like this: symsrv*symsrv.dll*C:\\symbols*http://msdl.microsoft.com/download/symbols;symsrv*symsrv.dll*C:\\symbols*http://symbols.burntcomma.com The filenames are weird! or I get strange errors on certain files or directories! The driver assumes that all filenames are encoded in UTF-8. This should be the default on most setups nowadays - if you're not using UTF-8, it's probably worth looking into converting your files. How do I get this working with Secure Boot turned on? For the very latest versions of Windows 10, Microsoft introduced more onerous requirements for signing, which seemingly aren't available for open-source drivers. To work around this, go to HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\CI\\Policy in Regedit, create a new DWORD value called UpgradedSystem and set to 1, and reboot. Or you could always just turn off Secure Boot in your BIOS settings. The root of the drive isn't case-sensitive in LXSS This is something Microsoft hardcoded into LXSS, presumably to stop people hosing their systems by running mkdir /mnt/c/WiNdOwS. How do I change the drive letter? With the shell extension installed, right-click the drive in Explorer, click Properties, and go to the Btrfs tab. There should be a button which allows you to change the drive letter. I'm still having problems with drive letters In Regedit, try deleting the relevant entries in HKEY_LOCAL_MACHINE\\SYSTEM\\MountedDevices, then rebooting. How do I format a partition as Btrfs? Use the included command line program mkbtrfs.exe. We can't add Btrfs to Windows' own dialog box, unfortunately, as its list of filesystems has been hardcoded. You can also run format /fs:btrfs, if you don't need to set any Btrfs-specific options. I can't reformat a mounted Btrfs filesystem If Windows' Format dialog box refuses to appear, try running format.com with the /fs flag, e.g. format /fs:ntfs D:. I can't mount a Synology NAS Synology seems to use LVM for its block devices. Until somebody writes an LVM driver for Windows, you're out of luck. I can't mount a Thecus NAS Thecus uses Linux's MD raid for its block devices. You will need to install WinMD as well. 64-bit Windows 7 won't load the driver Make sure that you have KB3033929 installed. Or consider installing from an \"escrow\" ISO which includes all updates. The drive doesn't show up and Paragon software has been installed Paragon's filesystem-reading software is known to disable automount. Disable or uninstall Paragon, then re-enable automount by running diskpart and typing automount enable. The drive doesn't show up on very old versions of Windows On very old versions of Windows (XP, Server 2003?), Windows ignores Linux partitions entirely. If this is the case for you, try running fdisk on Linux and changing your partition type from 83 to 7. Changelog v1.9 (2024-03-15): Added support for block group tree (Linux 6.1) Fixed hang when system under heavy load Added /blockgrouptree and /freespacetree options to mkbtrfs Follow Linux in defaulting /noholes to on in mkbtrfs Added support for CRC32C instructions on aarch64 v1.8.2 (2023-01-10): Fixed UAC not working Fixed Smartlocker crash on Windows 11 22H2 Rejigged INF file to work better on Windows 11 Files now signed with SHA256 hash rather than SHA1 v1.8.1 (2022-08-23): Fixed use-after-free when flushing Fixed crash when opening volume when AppLocker installed Compression now disabled for no-COW files, as on Linux Flushing now scales better on very fast drives Fixed small files getting padded to 4,096 bytes by lazy writer Added NoDataCOW registry option v1.8 (2022-03-12): Added minimal support for fs-verity Added test suite Fixed incorrect disk usage statistics Fixed potential crashes when renaming stream to file or file to stream Fixed potential crashes when querying hard links on file Fixed potential hang when opening oplocked file Fixed minor issues also uncovered by test suite v1.7.9 (2021-10-02): Fixed deadlock when mounting on Windows 11 Added support for BitLocker-encrypted volumes Improved filename checks when renaming or creating hard links Miscellaneous bug fixes v1.7.8.1 (2021-06-13): Fixed bug preventing new directories from appearing in listings Fixed Release version of driver still not working on XP v1.7.8 (2021-06-09): Upgraded zstd to version 1.5.0 Fixed regression stopping driver from working under XP Fixed compilation on clang Fixed corruption issue when Linux mount option inode_cache had been used Fixed recursion issue involving virtual directory \\$Root v1.7.7 (2021-04-12): Fixed deadlock on high load Fixed free space issue when installing Genshin Impact Fixed issue when copying files with wildcards in command prompt Increased speed of directory lookups v1.7.6 (2021-01-14): Fixed race condition when booting with Quibble No longer need to restart Windows after initial installation Forced maximum file name to 255 UTF-8 characters, to match Linux driver Fixed issue where directories could be created with trailing backslash Fixed potential deadlock when Windows calls NtCreateSection during flush Miscellaneous bug fixes v1.7.5 (2020-10-31): Fixed text display issue in shell extension Added support for mingw 8 Fixed LXSS permissions not working in new versions of Windows Fixed issue where truncating an inline file wouldn't change its size Fixed crash with Quibble where driver would try to use AVX2 before Windows had enabled it v1.7.4 (2020-08-23): Fixed issue when running compressed EXEs Changed build system to cmake Upgraded zstd to version 1.4.5 Added support for FSCTL_GET_RETRIEVAL_POINTERS Miscellaneous bug fixes v1.7.3 (2020-05-24): Fixed crash when sending file change notifications Improved symlink handling with LXSS Added support for undocumented flag SL_IGNORE_READONLY_ATTRIBUTE Fixed corruption caused by edge case, where address allocated and freed in same flush Improved handling of free space tree Improved handling of very full volumes Fixed spurious warnings raised by GCC 10 static analyser Replaced multiplications and divisions with bit shift operations where appropriate Fixed combobox stylings in shell extension v1.7.2 (2020-04-10): Added more fixes for booting from Btrfs on Windows 10 Fixed occasional deadlock when deleting or closing files on Windows 10 1909 Fixed crash when reading large ADSes Fixed occasional crash when writing files on RAID5/6 Miscellaneous bug fixes v1.7.1 (2020-03-02): Fixed crash when reading beyond end of file Fixed spurious checksum errors when doing unaligned read v1.7 (2020-02-26): Added support for metadata_uuid incompat flag (Linux 5.0) Added support for three- and four-disk RAID1 (Linux 5.5) Added support for new checksum types: xxhash, sha256, blake2 (Linux 5.5) Greatly increased checksumming speed Greatly increased compression and decompression speed Fixed bug causing incorrect free-space reporting when data is DUP Fixed issue creating directories on LXSS when case=dir option set v1.6 (2020-02-04): Added experimental (i.e. untested) ARM support (thanks to DjArt for this) Added fixes for booting from Btrfs on Windows 10 Volumes will now get remounted if changed while Windows is asleep or hibernating Fixed corruption when mounting volume that hasn't been unmounted cleanly by Linux Fixed crash when deleting subvolume v1.5 (2019-11-10): More fixes for booting from Btrfs Added virtual $Root directory (see \"NoRootDir\" below) Added support for Windows XP Added support for renaming alternative data streams Added oplock support Fixed potential deadlock on boot Fixed possible crash on shutdown Fixed a bunch of memory leaks Many other miscellaneous bug fixes v1.4 (2019-08-31): Added fragmentation percentage to property sheet Added support for Windows Server 2003 and Windows Vista Added pagefile support Improved support for file locking Added support for booting from Btrfs on Windows Server 2003 (see https://www.youtube.com/watch?v=-5E2CHmHEUs) Fixed issue where driver could open same inode twice Other miscellaneous bug fixes v1.3 (2019-06-10): Added support for new rename and delete functions introduced to Windows 10 Added support for Windows 10's flag for case-sensitive directories Changed free-space calculation method to be more like that of the Linux driver Added more support for 128-bit file IDs Fixed bug causing outdated root items Fixed bug preventing writing to VHDs v1.2.1 (2019-05-06): Reverted commit affecting the creation of streams v1.2 (2019-05-05): Dramatic speed increase when opening many small files, such as with a Git repository Fixed crash on surprise removals of removable devices Added ability to change drive letters easily No longer creates free-space cache for very small chunks, so as not to confuse the Linux driver Fixed corruption when very large file created and then immediately deleted Minor bug fixes v1.1 (2018-12-15): Support for Zstd compression Passthrough of Linux metadata to LXSS Refactored shell extension Fixed memory leaks Many other bug fixes v1.0.2 (2018-05-19): Minor bug fixes v1.0.1 (2017-10-15): Fixed deadlock Binaries now signed Minor bug fixes v1.0 (2017-09-04): First non-beta release! Degraded mounts New free space cache (compat_ro flag free_space_cache) Shrinking and expanding of volumes Registry options now re-read when changed, rather than just on startup Improved balancing on very full filesystems Fixed problem preventing user profile directory being stored on btrfs on Windows 8 and above Better Plug and Play support Miscellaneous bug fixes v0.10 (2017-05-02): Reflink copy Sending and receiving subvolumes Group mappings (see Mappings section above) Added commands for scripting etc. (see Commands section above) Fixed an issue preventing mounting on non-PNP devices, such as VeraCrypt Fixed an issue preventing new versions of LXSS from working Fixed problem with the ordering of extent refs, which caused problems on Linux but wasn't picked up by btrfs check Added support for reading compressed inline extents Many miscellaneous bug fixes v0.9 (2017-03-05): Scrubbing TRIM/DISCARD Better handling of multi-device volumes Performance increases when reading from RAID filesystems No longer lies about being NTFS, except when it has to Volumes will now go readonly if there is an unrecoverable error, rather than blue-screening Filesystems can now be created with Windows' inbuilt format.com Zlib upgraded to version 1.2.11 Miscellaneous performance increases Miscellaneous bug fixes v0.8 (2016-12-30): Volume property sheet, for: Balances Adding and removing devices Showing disk usage, i.e. the equivalent to btrfs fi usage Checksums now calculated in parallel where appropriate Creation of new filesystems, with mkbtrfs.exe Plug and play support for RAID devices Disk usage now correctly allocated to processes in taskmgr Performance increases Miscellaneous bug fixes v0.7 (2016-10-24): Support for RAID5/6 (incompat flag raid56) Seeding support LXSS (\"Ubuntu on Windows\") support Support for Windows Extended Attributes Improved removable device support Better snapshot support Recovery from RAID checksum errors Fixed issue where creating a lot of new files was taking a long time Miscellaneous speed increases and bug fixes v0.6 (2016-08-21): Compression support (both zlib and lzo) Mixed groups support No-holes support Added inode property sheet to shell extension Many more mount options (see below) Better support for removable devices Page file support Many miscellaneous bug fixes v0.5 (2016-07-24): Massive speed increases (from \"sluggish\" to \"blistering\") Massive stability improvements RAID support: RAID0, RAID1, and RAID10 Asynchronous reading and writing Partition-less Btrfs volumes Windows sparse file support Object ID support Beginnings of per-volume mount options Security improvements Notification improvements Miscellaneous bug fixes v0.4 (2016-05-02): Subvolume creation and deletion Snapshots Preallocation Reparse points Hard links Plug and play Free-space cache Fix problems preventing volume from being shared over the network Miscellaneous bug fixes v0.3 (2016-03-25): Bug fixes: Fixed crashes when metadata blocks were SINGLE, such as on SSDs Fixed crash when splitting an internal tree Fixed tree traversal failing when first item in tree had been deleted Fixed emptying out of whole tree (probably only relevant to checksum tree) Fixed \"incorrect local backref count\" message appearing in btrfs check Miscellaneous other fixes Added beginnings of shell extension, which currently only changes the icon of subvolumes v0.2 (2016-03-13): Bug fix release: Check memory allocations succeed Check tree items are the size we're expecting Added rollbacks, so failed operations are completely undone Fixed driver claiming all unrecognized partitions (thanks Pierre Schweitzer) Fixed deadlock within CcCopyRead Fixed changing properties of a JPEG within Explorer Lie about FS type, so UAC works Many, many miscellaneous bug fixes Rudimentary security support Debug log support (see below) v0.1 (2016-02-21): Initial alpha release. Debug log WinBtrfs has three levels of debug messages: errors and FIXMEs, warnings, and traces. The release version of the driver only displays the errors and FIXMEs, which it logs via DbgPrint. You can view these messages via the Microsoft program DebugView, available at https://technet.microsoft.com/en-gb/sysinternals/debugview. If you want to report a problem, it'd be of great help if you could also attach a full debug log. To do this, you will need to use the debug versions of the drivers; copy the files in Debug\\x64 or Debug\\x86 into x64 or x86. You will also need to set the registry entries in HKLM\\SYSTEM\\CurrentControlSet\\Services\\btrfs: DebugLogLevel (DWORD): 0 for no messages, 1 for errors and FIXMEs, 2 for warnings also, and 3 for absolutely everything, including traces. LogDevice (string, optional): the serial device you want to output to, such as \\Device\\Serial0. This is probably only useful on virtual machines. LogFile (string, optional): the file you wish to output to, if LogDevice isn't set. Bear in mind this is a kernel filename, so you'll have to prefix it with \"\\??\\\" (e.g., \"\\??\\C:\\btrfs.log\"). It probably goes without saying, but don't store this on a volume the driver itself is using, or you'll cause an infinite loop. Mount options The driver will create subkeys in the registry under HKLM\\SYSTEM\\CurrentControlSet\\Services\\btrfs for each mounted filesystem, named after its UUID. If you're unsure which UUID refers to which volume, you can check using btrfs fi show on Linux. You can add per-volume mount options to this subkey, which will take effect on reboot. If a value is set in the key above this, it will use this by default. Ignore (DWORD): set this to 1 to tell the driver not to attempt loading this filesystem. With the Readonly flag, this is probably redundant. Readonly (DWORD): set this to 1 to tell the driver not to allow writing to this volume. This is the equivalent of the ro flag on Linux. Compress (DWORD): set this to 1 to tell the driver to write files as compressed by default. This is the equivalent of the compress flag on Linux. CompressForce (DWORD): set this to 1 to force compression, i.e. to ignore the nocompress inode flag and even attempt compression of incompressible files. This isn't a good idea, but is the equivalent of the compress-force flag on Linux. CompressType (DWORD): set this to 1 to prefer zlib compression, 2 to prefer lzo compression, or 3 to prefer zstd compression. The default is 0, which uses zstd or lzo compression if the incompat flags are set, and zlib otherwise. FlushInterval (DWORD): the interval in seconds between metadata flushes. The default is 30, as on Linux - the parameter is called commit there. ZlibLevel (DWORD): a number between -1 and 9, which determines how much CPU time is spent trying to compress files. You might want to fiddle with this if you have a fast CPU but a slow disk, or vice versa. The default is 3, which is the hard-coded value on Linux. MaxInline (DWORD): the maximum size that will be allowed for \"inline\" files, i.e. those stored in the metadata. The default is 2048, which is also the default on modern versions of Linux - the parameter is called max_inline there. It will be clipped to the maximum value, which unless you've changed your node size will be a shade under 16 KB. SubvolId (QWORD): the ID of the subvolume that we will attempt to mount as the root. If it doesn't exist, this parameter will be silently ignored. The subvolume ID can be found on the inode property sheet; it's in hex there, as opposed to decimal on the Linux tools. The default is whatever has been set via btrfs subvolume set-default; or, failing that, subvolume 5. The equivalent parameter on Linux is called subvolid. SkipBalance (DWORD): set to 1 to tell the driver not to attempt resuming a balance which was running when the system last powered down. The default is 0. The equivalent parameter on Linux is skip_balance. NoPNP (DWORD): useful for debugging only, this forces any volumes to appear rather than exposing them via the usual Plug and Play method. ZstdLevel (DWORD): Zstd compression level, default 3. NoTrim (DWORD): set this to 1 to disable TRIM support. AllowDegraded (DWORD): set this to 1 to allow mounting a degraded volume, i.e. one with a device missing. You are strongly advised not to enable this unless you need to. NoRootDir (DWORD): if you have changed your default subvolume, either natively or by a registry option, there will be a hidden directory called $Root which points to where the root would normally be. Set this value to 1 to prevent this appearing. NoDataCOW (DWORD): set this to 1 to disable copy-on-write for new files. This is the equivalent of the nodatacow flag on Linux. Contact I'd appreciate any feedback you might have, positive or negative: mark@harmstone.com. Copyright This code contains portions of the following software: Zlib Copyright (C) 1995-2017 Jean-loup Gailly and Mark Adler This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software. Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions: The origin of this software must not be misrepresented; you must not claim that you wrote the original software. If you use this software in a product, an acknowledgment in the product documentation would be appreciated but is not required. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software. This notice may not be removed or altered from any source distribution. LZO WinBtrfs contains portions of an early version of lzo, which is copyright 1996 Markus Oberhumer. Modern versions are licensed under the GPL, but this was licensed under the LGPL, so I believe it is okay to use. Zstd Copyright (c) 2016-present, Facebook, Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. BLAKE2 https://github.com/BLAKE2/BLAKE2 (public domain) SHA256 https://github.com/amosnier/sha-2 (public domain)",
    "commentLink": "https://news.ycombinator.com/item?id=39956008",
    "commentBody": "WinBtrfs – an open-source btrfs driver for Windows (github.com/maharmstone)203 points by jiripospisil 12 hours agohidepastfavorite74 comments mgaunard 1 minute agoI feel like btrfs has been in development since forever and getting no adoption at all. When is the year of the btrfs file system coming? reply BirAdam 10 hours agoprevIt's really awesome that this was a complete reimplementation with no Linux code, and it's additionally awesome that this is available for both XP/2k3 and ReactOS. I will have to try it out on one of my older machines :-) reply jauntywundrkind 7 hours agoparentOne of the interesting patterns happening in Rust is io-less libraries. I'm not sure where best to link this phenomenon. It here s a open issue for an io-less quic library, from 2019, https://github.com/aiortc/aioquic/issues/4 It'd be so fracking sweet to see filesystems follow this pattern. If we could re-use the file system logic, but apply it to windows or fuse or Linux or wasm linearly-addressed-storage, that would allow such intensely cool forms of portability/reuse & bending/hacking. reply Arnavion 6 hours agorootparentIt's called sans-io in Python land, which is where I heard it first. https://sans-io.readthedocs.io/ I did it for one of my Rust projects back in 2018 https://github.com/Arnavion/k8s-openapi/commit/9a4fbb718b119... , and it's older than that in Python land. reply dloss 4 hours agorootparentThe above sans-io page links to this PyCon 2016 talk: Cory Benfield - Building Protocol Libraries The Right Way https://youtu.be/7cC3_jGwl_U reply endgame 6 hours agorootparentprevSeems like a rediscovery of \"pure functions\" from the FP world? reply atq2119 55 minutes agorootparentprevIf the property of \"io-lessness\" becomes something statically verifiable as part of dependency handling, it also seems potentially beneficial as a guard against supply-chain attacks. reply GrayShade 9 minutes agorootparentA compromised IO-less file system library can still synthetize malware files on a volume. reply unshavedyak 7 hours agorootparentprevHow is this implemented in practice? Special care to keep io on the outermost layers? Never thought about software in this way. Seems really tough, but interesting Wonder how well it scales to larger applications. Ie is there a codesize where io-less becomes too difficult? Perhaps performance concerns? Hmm reply Arnavion 6 hours agorootparentIt's not really an \"application\" thing. It's meant to be a design for libraries that implement protocols of some sort. All the library API acts on byte buffers and leaves the network socket etc stuff to the library user. So when the library needs to write data to a socket, the API instead returns a byte buffer to the caller, and the caller writes it to the network socket. When the library needs to read data from a socket, it instead expects the caller to do that and then give the populated byte buffer to a library function to ingest it. Also, quite the opposite, it's *easier* to design a library this way because it's strictly less code the library needs to contain. Specifically in Rust it also has the advantage that the library becomes agnostic to sync vs async I/O since that's handled by the library user. Correspondingly, it is slightly harder for the library user to use such a library, but it's usually just a matter of writing a tiny generic wrapper around the network socket type to connect it to the library functions. reply mikepurvis 5 hours agorootparentNice from a testing standpoint too, since you can trivially mock out the hardware. That said, a lot of what’s actually hard about IO is the error/fault handling, imposing timeouts and backoffs and all that jazz. At a certain point I’d wonder if extracting this out to a separate interface might obscure the execution flow in some of these scenarios. reply rav 3 hours agorootparent> That said, a lot of what’s actually hard about IO is the error/fault handling, imposing timeouts and backoffs and all that jazz. Application-level timeout/backoff handling is always scary to me, because I don't know how to make robust tests for it. I wonder if you couldn't use the same I/O-less approach, and split the logic out into pure functions that take the time passed/error state/... as value arguments, instead of measuring the physical time using OS APIs. It's probably not something for reusable libraries, but it could still be a nice benefit to be able to unit test in detail. reply userbinator 8 hours agoparentprevand it's additionally awesome that this is available for both XP/2k3 and ReactOS ReactOS is supposed to be API-compatible with Windows, so that's not too surprising. reply pxc 8 hours agorootparentIt's not surprising, but it is really nice that this gives ReactOS a nice, modern CoW filesystem. reply dang 10 hours agoprevRelated: WinBtrfs – A Windows driver for the next-generation Linux filesystem Btrfs - https://news.ycombinator.com/item?id=15177002 - Sept 2017 (100 comments) WinBtrfs v0.7 - https://news.ycombinator.com/item?id=12794214 - Oct 2016 (1 comment) reply minroot 2 hours agoprevI tried to use it it few weeks ago on a btrfs hard drive. But i couldn't make it work. Then i used wsl to access it. Worked for a few run but then things just started to fail. It wouldn't even get mounted. Then I realized i can just boot a live iso of linux and copy/move files to windows drive and to the btrfs drive. That what i am doing now, using Fedora Workstation live iso on USB drive with ventoy. reply develatio 10 hours agoprevHow come this has \"basic and advanced\" (whatever that means ¿?) RAID 5/6, while BTRFS itself doesn't? (https://btrfs.readthedocs.io/en/latest/btrfs-man5.html#raid5...) reply viraptor 8 hours agoparentThat's not quite right. Linux btrfs supports raid5 in general, but has known edge cases which make it not safe to use. Basically it's \"available, but experimental, for developers only\". Winbtrfs only says the raid5 mode is one of the features, but doesn't really address how well it works. The questions in a related issue (https://github.com/maharmstone/btrfs/issues/293) have been closed without real answers. I wouldn't risk raid 5/6 on it without getting good answers about the status / testing from the developers first. reply cesarb 42 minutes agorootparent> Linux btrfs supports raid5 in general, but has known edge cases which make it not safe to use. Basically it's \"available, but experimental, for developers only\". I recall reading somewhere recently, that the Linux btrfs developer intends to fix these edge cases through a on-disk layout change (IIRC, adding one more btree to the filesystem). So unless this driver already has that on-disk layout change, it's unlikely that these edge cases have been addressed. reply hnlmorg 2 hours agorootparentprevI wouldn’t risk this Windows driver on anything important regardless of whether you use raid 5/6 or not. I’m not taking anything away from the effort that has gone into producing this. Just being realistic about the amount of effort that is required to create a production ready file system driver. reply hsbauauvhabzb 3 hours agorootparentprevI was under the impression that brtfs didn’t support raid, but could be deployed on top of software raid? reply _flux 3 hours agorootparentWell, that's the wrong impression. Here's some Arch documentation about it, basically you just create a btrfs on top of multiple devices and it works (to some extent with raid5/raid6 as well): https://wiki.archlinux.org/title/Btrfs#Multi-device_file_sys... . Raid1 apparently works fine. So if you want raid5/6, deploying on top of md is the better option. reply dark-star 9 hours agoparentprevThey call RAID0/1/10 \"basic\" RAID and RAID5/6 \"advanced\" RAID. I have no idea why. Maybe because the former doesn't require \"advanced\" parity calculations or something. reply gertop 10 hours agoprevI recommend that everybody reads the README. The author answers all the questions I've had - and much more! reply qwerty456127 1 hour agoprevWhy still use hardware RAID nowadays when we have BTRFS and ZFS? reply lm411 46 minutes agoparentPerformance, reliability, and BBU or CacheVault. Hardware RAID is worth the money when uptime and performance are important. I've seen good RAID cards keep a server running where native direct SATA would have brought the server down. reply rkagerer 11 hours agoprevAwesome! Anyone using this long term or in production who can comment on how it's been working? I see TRIM is supported. Is RETRIM also (or whatever is needed during drive optimization to release areas that didn't get TRIMmed the first time due to a full command queue). Could this serve as an effective NTFS replacement with data parity for those who don't like ReFS? How mature is it compared to ZFS on Windows? reply summermusic 8 hours agoparentI have run this casually on my main machine for a few years now. I have a Windows partition, a Linux partition (btrfs on LUKS), and a third btrfs partition where I kept my files. I don’t use it often, but when I do I don’t even notice it. It’s as if Windows could just natively read btrfs all along. This was without any “advanced” usage beyond simply accessing, modifying, or deleting files. reply nyanpasu64 9 hours agoparentprevOne time I accidentally ran a Visual Studio build in a btrfs git clone rather than my main NTFS drive. By the time I noticed and cancelled the build, there were two folders with an identical name but different contents, which I had to delete the folder name twice. I'd say the driver has issues with concurrency. reply Kwpolska 2 hours agorootparentI once ran a `git clone` from WSL1 on the C: drive, and tried to build a C++ project in VS. It complained that \"EXAMPLE.H\" was not found. An \"example.h\" file did exist in the repo, and my code asked for \"example.h\". Turns out WSL1 set some obscure bit not known in Win32 land (but enforced by NTFS) that makes the file names case-sensitive, while VS's path normalisation expects a case-insensitive file system. Perhaps this was related to your issue? reply nyanpasu64 2 hours agorootparentIn a separate occasion, I also got that issue (but worse). I once marked a NTFS folder as case-sensitive to help root out all case mismatch bugs (to get a C++ project eventually building on Linux), but then Visual Studio and CMake started spitting out \"file not found\" errors even for the correct case! I had somehow produced a \"cursed\" folder that could not be used for building code until I copied (not moved) its contents to a regular case-insensitive NTFS folder. reply jiggawatts 10 hours agoparentprevReFS with Storage Spaces already serves this purpose and is integrated and fully supported. From what I’ve heard, BTRFS has a crazy long list of defects where it’ll lock up or corrupt data if you so much as look at it wrong.[1] Using something that is unreliable at best on its native OS shoehorned into Windows is madness. Fun for a lark, sure, but I would never ever entrust this combination with any actual data. [1] “It works for me on my two disk mirror” is an anecdote, not data. reply hsbauauvhabzb 10 hours agorootparent> [1] “It works for me on my two disk mirror” is an anecdote, not data. While that statement might well be correct that the quote is in fact an anecdote, the following is also an anecdote: ‘From what I’ve heard, BTRFS has a crazy long list of defects where it’ll lock up or corrupt data if you so much as look at it wrong.’ reply nisa 10 hours agorootparentMade the mistake using btrfs for a Hadoop cluster at university in kernel 4.x times after reading that SLES uses it and after reading an interview on lwn with someone important, I think the maintainer at that time - that deemed it stable. This must be 10 or 12 years ago or so and it was a wild ride - crashes, manual recovery on 200 machines using clusterssh to get the partitions to mount again. Got out of disk space errors on a 16tb raid 1 (which is not a real raid1) with 5% usage - lot's of sweat I'd rather avoid. Should have just used ext4 in hindsight. For me I decided to not touch it anymore after that experience. I'm sure there is a name for that bias but I don't care. Got burned badly. Lots of people had probably similar experiences and that's were that coming from. Reading the mailing list archives at that time might also be useful to convince yourself that it was more than anecdote. reply hsbauauvhabzb 8 hours agorootparentI’m not disputing any discourse relating to the factual in/correctness of the anecdote, I’m pointing out that gp is providing an anecdote while disputing anecdotes that they don’t agree with. Provide actual data that’s recent. Linux 4.x was what, 10 years ago? Cars are substantially safer now than they were 10/20/50 years ago, so whose to say your experience with a file system would be different? reply Haemm0r 3 hours agorootparentSame could be said about cars: Why ever buy a [insert brand] again after you've been burned by its reliabity or other issues? You probably just don't, as the alternatives are good and plenty. reply jiripospisil 32 minutes agorootparent> You probably just don't, as the alternatives are good and plenty. Which cannot be said about file systems on Linux which support metadata+data checksum and repair though. As far as I'm aware the only file systems which could realistically be used are btrfs and zfs (bcachefs looks promising but not there yet). Zfs is not even a part of the kernel and you have to compile it yourself and hope it does actually compile against your kernel due to API changes. reply grumpyprole 2 hours agorootparentprevIt's unfortunately a very common anecdote over the last 10 years (and a similar experience to my own). And to be honest, it's a red flag with how this critical system component is being developed. reply yarg 8 hours agorootparentprevWitnessing defects means that they exist; witnessing no defects does not mean they don't. reply viraptor 8 hours agorootparentYeah, an interesting scenario is that many people compare the btrfs behaviour to \"never had issues with extfs\". When in practice it's \"extfs couldn't have told me about this issue even if it existed\". reply hsbauauvhabzb 8 hours agorootparentprevIf that’s the case, prove the giant Flying Spaghetti Monster doesn’t exist. reply nailer 7 hours agorootparentI think you’ve misread the parent comment. Witnessing no FSM does not mean there is no FSM. reply hsbauauvhabzb 3 hours agorootparentAssuming FSM is referring to defects, witnessing no defects increases confidence that no defects exist in the observed state. Conversely, witnessing defects does not itself prove defects exist if the test cases were not scientific, it increases the confidence that defects exist but there exists some probability that an unrelated defect (bad ram, kernel error, hardware failure, solar flares) could have caused the issue. But there’s also a lot of evidence to suggest Brtfs has had a lot of defects resolved in recent years, so it’s also important to note that as time moves forward, the amount of existing and likely rate of introducing new defects is likely to decrease. I should add ive had minimal skin in this game until yesterday. I chose brtfs for two systems for snapshot support, but that’s in addition to regular backups on another host, because it’s silly to trust any single compute node regardless of file system. reply unixhero 2 hours agorootparentprevNope. It works perfectly on both my striped arrays raid 0 and mirrored raid 1. reply temac 24 minutes agorootparentprevAdvising ReFS is a little bit insane though. I would certainly not entrust it for my data either. reply jiripospisil 10 hours agorootparentprev> From what I’ve heard, BTRFS has a crazy long list of defects where it’ll lock up or corrupt data if you so much as look at it wrong The list cannot be crazy long if Synology uses it for their NASes. reply tbyehl 2 hours agorootparentNotably, Synology's agent-based backup software requires BTRFS but will not back up BTRFS. https://kb.synology.com/tr-tr/DSM/help/ActiveBackup/activeba... reply yjftsjthsd-h 9 hours agorootparentprev> The list cannot be crazy long if Synology uses it for their NASes. Synology uses a hybrid BTRFS+mdadm arrangement specifically to deal with reliability problems with BTRFS RAID: https://kb.synology.com/en-us/DSM/tutorial/What_was_the_RAID... reply jiripospisil 44 minutes agorootparentI know but that's only one important part of what a file system does. If the file system was otherwise totally broken, they wouldn't use it. reply wolletd 9 hours agorootparentprevWhich is kind of the point. BTRFS only has issues with RAID5/6 configurations. Using it as a filesystem for a single disk or partition should be totally fine. reply Mister_Snuggles 6 hours agorootparentAnecdotally, this is untrue. Personally, BTRFS is the only filesystem that has ever caused me any data loss or downtime. I was using a single disk, so it should have been the perfect path. At some point the filesystem got into a state where the system would hang when mounting it read/write. I was able to boot off of a USB stick and recover my files, but I was unable to get the filesystem back into a state where it could be mounted read/write. At work, we used to run BTRFS on our VMs as that was the default. Without fail, every VM would eventually get into a state where a regular maintenance process would completely hang the system and prevent it from doing whatever task it was supposed to be doing. Systems that wrote more to their BTRFS filesystems experienced this sooner than ones that didn't write very much, but eventually every VM succumbed to this. Eventually the server team had to rebuild every VM using ext4. I know that anecdotes aren't data, but my experience with BTRFS will keep me from using it for anything even remotely important. reply Dalewyn 9 hours agorootparentprevEverything I've read about btrfs's RAID5/6 deficiencies is that it can't tolerate sudden losses of power (aka write hole problem), which I think is fine so long as you are aware of it and implement appropriate safety measures such as a UPS or APU. And besides, if you are doing RAID you are probably concerned with the system's uptime which probably means you will have implemented such measures anyway. Note that, yes, I'm aware most home users either aren't aware (nobody RTFM) or are too lazy/cheap to buy a UPS from Office Depot. So perhaps btrfs is warning people to save them from themselves. reply dark-star 9 hours agorootparentA UPS will not much improve the reliability against sudden power loss. At least here in Europe it is much more likely that a PSU or other component fails than that the power line is suddenly interrupted. And lost writes are a problem that all filesystems have. I recommend reading the paper \"Parity Lost and Parity Regained\" by Krioukov at USENIX 08... reply amaccuish 2 hours agorootparentprevKernel panic too... reply gerdesj 10 hours agorootparentprevOne place where ReFS is rather decent is \"reflinks\" - that's where identical blocks are stored once and in the background the rest are simply links to the one block. That is rather useful in backup systems. XFS also supports reflinks amongst other things and is way older than ReFS and hence considered out of beta (which ReFS isn't, by me) I don't trust data to RefS yet - its a fun project that will no doubt prove itself one day. For now, Windows boxes run NTFS and Linux runs ext4 or XFS. reply Gabrys1 1 hour agorootparentWhat happens if the very important directory you copied 11 times (just to be sure) ends up producing the same block and doesn't indeed get duplicated as you expected? And now, that block gets corrupted... reply defrost 1 hour agorootparentBack in the day if I copied (geophysical air) survey data 11 times and put all the copies in the same walk in fire proof safe (in the hanger), that offered no real additional security in the event of a direct hit by an aircraft and explosion while the door was open. If you're going to make 11 copies, they have to go to different physical locations, different devices at least, geographically different places to be sure, or it's pointless. In this instance, block de-duping on a single device makes sense .. expecting mutiple copies on the same device (with or without duplicat block reuse) to offer any additional safety does not. reply Dylan16807 10 hours agorootparentprevReFS only got put back into normal Windows 11 a few months ago. That's a good sign for the future, but it was looking bad for a long time. Also if you turn on data checksums, my understanding is it will delete any file that gets a corrupted sector. And you can only override this behavior on a per-file basis. Unless this changed very recently? reply MarkSweep 8 hours agorootparentOh, is it no longer exiled to Windows Pro for Workstations? This feature comparison chart still has this there: https://www.microsoft.com/en-us/windows/business/compare-win... For what it’s worth, regular Windows 10 & 11 Pro (and other editions maybe?) have supported reading and writing ReFS this whole time. It’s just the option to create a new volume that’s been disabled. reply marwis 3 hours agorootparentIt still sort of is but you can create Dev Drive which is based on ReFS reply dark-star 9 hours agorootparentprevReFS is terrible. We have seen so many customers lose data on ReFS that I started strongly advising everyone against using it. One example: If you (accidentally or on purpose) attach a ReFS disk or LUN to a newer Windows version, it will be silently upgraded to a new ReFS version without any feedback (or chance to prevent it) for the user. No way of attaching the disk on an older Windows version afterwards. But that is not the real problem. The real problem is that the upgrade runs as a separate (user-space) process. If this process crashes or your PC crashes or reboots while it runs, your data is gone. There is no feedback how long it still has to run (we've seen multiple days on large volumes) So yeah, maybe avoid ReFS for a few more years... reply yjftsjthsd-h 9 hours agorootparentprevWeirdly, it's possible that this version could be more stable/reliable/safe than the Linux version, since it's apparently a wholly independent reimplementation. I suppose it depends on whether BTRFS's problems stem from the underlying data format or the actual code as written for the Linux driver. reply rkagerer 10 hours agorootparentprevThanks. I tried ReFS when it first came out and it was terribly slow (with data parity on), and Storage Spaces was obscure to set up and manage. Has the landscape improved? reply mgerdts 5 hours agorootparentOn WS2022 without patches I noticed that Storage Spaces was only queueing one IO per NVMe device. With current patches queuing is fixed and performance is much better. I think this was fixed sometime in 2023. I’m pretty sure both NTFS and ReFS were affected. reply jiggawatts 1 hour agorootparentAh, that would explain the absurdly bad I/O performance I was seeing in Azure VMs that had the new NVMe virtual disk controllers! I had spoken with some of the teams involved and they were rather cagey about the root cause, but at least one person mentioned that there were some fixes in the pipeline for Windows Server 2022 NVMe support. I guess this must have been it! reply fsiefken 10 hours agoprevWould this make it possible to boot Windows10 and 11 from a btrfs formatted windows usb stick? reply Modified3019 10 hours agoparentYou can use Rufus to install 10/11 on a usb SATA/NVMe drive enclosure as “Windows To Go”. In practice it works out pretty decently in my experience using it with windows 10 daily for a while, with a few caveats: 1. You need a stable usb connection 2. You need a usb drive enclosure with a controller chip that is stable/doesn’t overheat 3. Your drive should be powerless resistant. Unfortunately there’s no resource I know of that evaluates power loss handing. Some drives will have a bad time having power suddenly cut. I’ve had good experience with Intel enterprise sata SSD’s and NVMe drives in a Dockcase with capacitor. If your drive stops showing up, a power cycle might help: https://dfarq.homeip.net/fix-dead-ssd/ 4. Have automatic backups setup. Very useful for performance testing and hardware firmware updates that are windows only. When switching between computers, I’ll often have to boot, windows gets confused and then reboot. After that it works. However, I have no experience trying to make use of WinBTRFS or the separate bootloader project, which is apparently currently broken since a few months ago. Ventoy booting a windows VHD file might also be a decent option reply Cu3PO42 10 hours agoparentprevNot in its own. You also need a different boot loader. The author has an implementation called Quibble [0] that also supports btrfs. [0] https://github.com/maharmstone/quibble reply westurner 9 hours agoprev> See also Quibble, an experimental bootloader allowing Windows to boot from Btrfs, and Ntfs2btrfs, a tool which allows in-place conversion of NTFS filesystems. The chocolatey package for WinBtrfs: https://community.chocolatey.org/packages/winbtrfs reply rustcleaner 9 hours agoprevShould have been ZFS. :*^( reply Fnoord 9 hours agoparentExists! [1] ZFS seems to be the most cross-platform of the modern filesystems. Although there's a Paragon driver for APFS for Windows and a FOSS driver for native Linux APFS as well as one for FUSE. Personally, I keep track of bcachefs which got merged in Linux 6.7. But it won't be cross-platform. [1] https://github.com/openzfsonwindows/openzfs [2] https://bcachefs.org reply graphe 9 hours agoprev [–] What is the purpose of using this in production? I thought people just ssh into Linux if you need it to just work. For my own purposes I used to use an ext3 driver on Win7, never failed on me, just switched to Linux. reply yjftsjthsd-h 7 hours agoparent [–] Some people want to access the same data volume from Linux and Windows (see the person dual-booting upthread). reply Zambyte 5 hours agorootparent [–] You can also just pass the partition to a VM and access the VM storage however you want. I would trust that a lot more than this to be honest. Nothing against this project in particular, I just don't find the idea of using a filesystem driver on Windows to access a filesystem that Windows doesn't normally support. I don't really trust Windows to handle that well :P reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WinBtrfs v1.9 is a Windows driver that brings Btrfs Linux filesystem capabilities to Windows XP and later versions, including RAID support, ACLs, compression, and more.",
      "Included in ReactOS, it necessitates WinMD for MD software RAID tools and offers features like defragmentation, Btrfs quotas, and transaction log support.",
      "Troubleshooting tips, bug fixes, new features, and various configuration options via registry settings and debug log support are part of the provided updates."
    ],
    "commentSummary": [
      "The article covers WinBtrfs, an open-source btrfs driver for Windows, along with the growing interest in io-less file system libraries for better portability and security.",
      "It discusses challenges in I/O operations, concerns about RAID5 in Linux BTRFS, and users' mixed experiences with BTRFS on Windows and its reliability on Linux.",
      "Furthermore, it compares BTRFS and extFS, highlights issues with ReFS, mentions enhancements in Windows Server 2022, and touches on cross-platform filesystem capabilities."
    ],
    "points": 204,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1712440771
  },
  {
    "id": 39952111,
    "title": "Rare Vortex Filmed by NASA Probe in Sun's Atmosphere",
    "originLink": "https://mashable.com/article/nasa-sun-footage-parker-solar-probe",
    "originBody": "The fastest spacecraft in our solar system has captured wild footage inside the sun's atmosphere. NASA's Parker Solar Probe — a craft flying much closer to the sun than any previous mission — witnessed massive \"vortex-like structures\" in our star's outer atmosphere, called the corona. You can see the unprecedented view below, taken by the probe's WISPR camera (short for Wide-field Imager for Parker Solar Probe) and recently posted online. SEE ALSO: NASA scientist viewed first Voyager images. What he saw gave him chills. Researchers suspect that the phenomena was caused by a coronal mass ejection (CME) — when the sun ejects a mass of super hot gas (plasma) into space — interacting with the solar wind. The sun's solar wind is constant, as our medium-sized star emits a steady stream of charged particles. The new research associated with the footage, published in The Astrophysical Journal, concluded that this find in uncharted realms of the sun has created new opportunities to better understand and predict CMEs, outbursts that can severely harm communications and electrical grids on Earth. Technically, these rare vortices are called \"Kelvin-Helmholtz instabilities,\" or KHI. Mashable Light Speed Want more space and science stories in your inbox? Sign up for Mashable's weekly Light Speed newsletter. Sign Me Up By signing up you agree to our Terms of Use and Privacy Policy. \"The direct imaging of extraordinary ephemeral phenomena like KHI with WISPR/PSP is a discovery that opens a new window to better understand CME propagation and their interaction with the ambient solar wind,\" Evangelos Paouris, a space weather researcher and member of the WISPR team at George Mason University, said in a statement. CMEs \"can endanger satellites, disrupt communications and navigation technologies, and even knock out power grids on Earth,\" NASA explains. Infamously, a potent CME in 1989 knocked out power to millions in Québec, Canada. The CME hit Earth's magnetic field on March 12 of that year, and then, wrote NASA astronomer Sten Odenwald, \"Just after 2:44 a.m. on March 13, the currents found a weakness in the electrical power grid of Québec. In less than two minutes, the entire Québec power grid lost power. During the 12-hour blackout that followed, millions of people suddenly found themselves in dark office buildings and underground pedestrian tunnels, and in stalled elevators.\" Related Stories Solar eclipses were once extremely terrifying events, experts say When the solar eclipse hits, you'll see these radiant planets What's the big deal about solar eclipses? It's a 'full-body experience.' NASA astronaut witnessed an eclipse from space. It was 'unnatural.' The most fascinating star in our sky inches closer to exploding An artist's conception of NASA's Parker Solar Probe passing through the sun's outer atmosphere, or corona. Credit: NASA The space agency's Parker Solar Probe will continue its speeding dives into the sun's corona. To withstand the heat, it's fortified with a 4.5-inch-thick carbon heat shield that's pointed at the sun. The shield itself heats up to some 2,500 degrees Fahrenheit, but just a couple of feet behind the shield, the environs are surprisingly mild. Later this year, the spacecraft will reach a whopping 430,000 miles per hour. Topics NASA",
    "commentLink": "https://news.ycombinator.com/item?id=39952111",
    "commentBody": "NASA spacecraft films crazy vortex while flying through sun's atmosphere (mashable.com)195 points by bookofjoe 21 hours agohidepastfavorite72 comments fxj 19 hours agoWhen I did my PhD in the late 80s we were simulating Kelvin-Helmholtz Instabilities on supercomputers with application to solar dynamics. It is very nice to see that they exist and behave like predicted. There are plenty of other plasma instabilities that were predicted at that time and have now been confimed by space probes. reply jakeinspace 19 hours agoprevGiven that video is 7.5 hours start to finish, and Parker is moving at several hundred thousand mph relative to that vortex, is it on the order of the diameter of the sun? reply esaym 17 hours agoparent>Given that video is 7.5 hours start to finish, Link?? reply toufka 17 hours agorootparentTimestamp in the videos: 21:03-4:33 => 7.5hrs reply zmgsabst 14 hours agoparentprevSeems like. 186,000 km/h will cover the sun’s diameter in 7.5 hours. https://www.wolframalpha.com/input?i=%28diameter+of+sun%29%2... Meanwhile, Parker is supposed to be moving substantially faster when close to the sun. reply akira2501 12 hours agorootparentEverything moves substantially faster when close to the sun. reply hackernewds 18 hours agoparentprevno reply throwup238 19 hours agoprevCan anyone find a scale for the photo? How big is this vortex? reply bookofjoe 19 hours agoparent>First Direct Imaging of a Kelvin–Helmholtz Instability by PSP/WISPR https://iopscience.iop.org/article/10.3847/1538-4357/ad2208 >To characterize the spatial scales involved (e.g., radial size, width, and separation of the eddies) we use exclusively observations from WISPR-I, the only instrument where the eddies were discernible. >From the GCS reconstruction, we estimated that the CME propagated radially in a direction with a Carrington longitude of 20° and latitude of 10°. >Since all the features exhibited a rather elliptical shape, to characterize the typical scales involved, we measured the length of the major and minor axes (the major axis is along the propagation direction, while the minor axis is perpendicular to this direction). > From the time-lapse considered, we estimate that the lifetime of the eddies (i.e., the temporal period) is less than 30 minutes. >Table 1. Average Sizes (in Mm) of the Minor (top row) and Major (lower row) Axis of Observed Eddies https://iopscience.iop.org/0004-637X/964/2/139/suppdata/apja... https://content.cld.iop.org/journals/0004-637X/964/2/139/rev... https://content.cld.iop.org/journals/0004-637X/964/2/139/rev... reply Levitating 16 hours agorootparentELI5? reply jeroenhd 12 hours agorootparentAverage sizes seem to range from 36000km to 174000km. For reference: the earth is almost 13000km in diameter. reply JumpCrisscross 9 hours agorootparent3 to 13 Earth diameters. reply bregma 15 hours agoparentprevUnfortunately the banana froze and carbonized at the same time. reply justinclift 8 hours agorootparentIt's probably need a very large number of bananas to even show up as a pixel in the video. :) reply JKCalhoun 19 hours agoprevVery cool. Any point to false-colorizing this footage? A slightly embarrassed normie asking. (Weirdly too, I want sound — maybe those long, low-frequency whistles you hear radio astronomers pick up from the sun.) reply vrighter 14 hours agoparentsound does not travel through space. So that is just sonification of data. It is, in general, bullshit reply westurner 9 hours agorootparentActually, https://twitter.com/NASAExoplanets/status/156144251407831449... : > The misconception that there is no sound in space originates because most space is a ~vacuum, providing no way for sound waves to travel. A galaxy cluster has so much gas that we've picked up actual sound. Here it's amplified, and mixed with other data, to hear a black hole! \"Physicists demonstrate how sound can be transmitted through vacuum\" https://www.sciencedaily.com/releases/2023/08/230809130709.h... : > In a recent publication they show that in some cases a sound wave can jump or \"tunnel\" fully across a vacuum gap between two solids if the materials in question are piezoelectric. \"Complete tunneling of acoustic waves between piezoelectric crystals\" (2023) https://www.nature.com/articles/s42005-023-01293-y Helmholtz resonance https://en.wikipedia.org/wiki/Helmholtz_resonance : > Helmholtz resonance is one of the principles behind the way piezoelectric buzzers work: a piezoelectric disc acts as the excitation source, but it relies on the acoustic cavity resonance to produce an audible sound. reply akira2501 12 hours agorootparentprev> is just sonification of data. Block frequency down conversion. > It is, in general, bullshit It's incredibly useful for a species that has limited sensory capabilities. reply JumpCrisscross 9 hours agorootparentprev> sound does not travel through space They’re not in space. They’re in the Sun. There is presumably some acoustic transmission in that medium. reply echelon 19 hours agoprev> Later this year, the spacecraft will reach a whopping 430,000 miles per hour. This is much faster than Voyager! The speed of light is 670,616,629 miles per hour in a vacuum. We're starting to get into the not insignificant percentage territory here. That's nearly 0.1%! reply kshacker 18 hours agoparentA novice question. 430K miles per hour is obviously a big number. And I have previously heard of this slingshot approach to increase speed so I am familiar with it. However, I believe energy is constant (it can be transformed but unlikely to be created or destroyed). For some object to gain a speed of 430K miles per hour, it must come from elsewhere, obviously it did not burn its own fuel (and I am assuming the slingshot theory). So the Sun transferred it a bunch of energy. I presume that is gravitational energy and to my mind it implies Sun gave away that energy. However, isn't that based on mass? But I do not think the mass would have changed. ELI5 please in terms of energy exchange. Who gained and lost and how? reply testoo 18 hours agorootparenthi kshacker! This is my understanding: That's correct, the energy comes from the body the spacecraft is slingshotting around (the Sun in this case). It's not mass or gravitational energy or anything weird like that, it's actually just a momentum transfer, the same as if the two objects had collided and bounced off each other elastically (i.e. without loss of energy to heat). So a (miniscule amount) of momentum (velocity x mass) is being transferred from the Sun to the spacecraft, and that's where the energy comes from. (source: I studied physics and had a grandparent at NASA who worked on Voyager II and talked about this issue with me; but it's been a while since both of those things, so anyone with more fresh experience feel free to chime in!) reply aio2 17 hours agorootparentI'm studying physics right now, I can say I agree with everything you said. One thing I'd like to expand on to those who don't know how greater energy means greater speed. The kinetic energy equation is 1/2massvelocity^2=KE Since the KE increases from the momentum transfer, and mass of the object stays constant, the only thing that can change is velocity, where it has to go up. ex: KE=2, m=1 2=1/21v^2, v=2 Now if some momentum were transferred, and the kinetic energy increased to KE=8, 8=1/21velocity^2, velocity=4, since the mass can't change reply testoo 6 hours agorootparentprevupdate: sorry kshacker, I may have thrown you off the scent here. My explanation of the slingshot effect is right, but it doesn't look like slingshotting is what the spacecraft is using to increase its speed: the original article doesn't actually mention this at all, but it links to another one which tries (so vaguely it's misleading IMO) to explain the maneuver: https://mashable.com/article/nasa-parker-solar-probe-speed tl;dr: the spacecraft is just falling into the Sun, which is why it speeds up. It isn't gaining speed relative to anything else, and it loses that speed again once it flies away from the Sun. It is using Venus to get closer to the sun each time around by damping its angular momentum, which works but I don't know how to explain that in an ELI5 way. so it's actually a little anticlimactic. BlarfMcFlarf and pfdietz got this right below in their comment thread: \"What the Venus flybys did was not add energy so much as remove angular momentum. The hard part about getting close to the Sun is that conservation of angular momentum prevents it.\" ...and icehawk and vl correctly point out that you can't really use the Sun to increase your within-solar-system speed. Thanks to them for prompting me to look into this further. The cool slingshot maneuvers all involve planets, not the sun. ...but I think the key answer that none of us quite articulated to your question: How is the spacecraft using the slingshot effect to increase its speed each time around? ...is that it isn't! ...the article dramatically describes it as \"picking up speed\" each time it goes around the sun, but that is misleading. It is just getting closer to the sun every time around, so of course it goes faster the closer it gets. the cool part if any is how it uses Venus to get closer to the sun (by sapping angular momentum), but that's hard to explain in a nutshell and doesn't really relate to your energy question. so that is hopefully now a better answer to this mystery that brings together what some of the other commenters have pointed out. reply ShamelessC 13 hours agorootparentprevWhy did this get downvoted so much? Seems accurate enough. reply icehawk 7 hours agorootparentIs it though? A vehicle can't use a gravity-assist slingshot around the Sun to maneuver inside the solar system because the sun is at rest with respect to the rest of the solar system. reply testoo 7 hours agorootparenthey icehawk -- so you can use any celestial object in the solar system to maneuver; all you're doing is basically changing your direction. But you're right, there's something strange going on here. The original article doesn't actually have any information about what this spacecraft is doing, but it links to another one about the speed. I'm looking at that now, gonna add an addendum (or edit the old comment if i can figure out how to do that) reply testoo 12 hours agorootparentprevi was wondering too! Do you think it might be because of citing family as a source? I barely ever post here, so don't have a good muscle memory for norms and best practices. (also both scared and curious of what might result from dropping below 0 karma) reply richrichie 9 hours agorootparentJust ignore it. You will get used to some hyper sensitive HN people that will down vote an apple because it is red. reply BlarfMcFlarf 16 hours agorootparentprevThe slingshots used to deorbit were around Venus, so it slightly gained velocity while the Parker probe lost it to get closer to the sun without expending as much fuel. As for its velocity around the sun, it’s intuitively like pendulum. When far away, it’s like the raised pendulum, and when near the sun, it’s like the pendulum at the bottom of its swing. Its a strained metaphor, but if you look at the orbit, it’s a very deep swing and a very large object it’s swinging towards, so it ends up quite fast at the bottom. reply pfdietz 12 hours agorootparentWhat the Venus flybys did was not add energy so much as remove angular momentum. The hard part about getting close to the Sun is that conservation of angular momentum prevents it. Related to this: a minimum energy transfer between two circular orbits is normally the two-burn Hohmann transfer: an elliptical orbit that is tangent to each circular orbit. But if the radii of the two circular orbits have a sufficiently large ratio, it takes less delta-V to use three burns: go into an elliptical orbit that goes out to very large distance, do a small burn to lower (or raise) the periapsis to be at the other orbit, then circularize with a third burn. This is because doing a burn at large distance adds or removes a very large amount of angular momentum. reply pigpang 5 hours agorootparentprevIt's same as for everything: something is going into lower energy state, while something else going into higher energy state via some energy transfer mechanism. We are 100% sure for one side of energy transfer: both spacecraft and Sun are going to higher energy level first, then to lower energy level. Other part of equation is unknown. It's called \"Gravitational potential energy\", but it's unknown what stores this energy. However, we have few hints: objects creates gravitation waves, gravitational waves are propagated at c (speed of light), there is Higgs field (nature unknown), which is presented everywhere and gives mass, Higgs boson connects objects to Higgs field, gravitational force is proportional to 1/d^2. So, we have a medium (Higgs field), to which objects are connected via Higgs bosons. We can assume that energy can be transferred to/from the medium via bosons, so medium can store the energy and release it. We can speculate that when bosons somewhat connects to medium, it may create a tension in the medium, like bubbles on water, via unknown physical process. As result of that tension, bosons which are closer are spending less energy to create same tension, which makes closer position energetically favorable, so any random motion (because of noise in the medium) in direction of another boson will release some energy, while any movement in opposite direction will require to apply some energy. reply big_paps 17 hours agorootparentprevPotential energy was converted into kinetic energy, not unlike when an apple falls to the ground. So the sun doesn’t really give away this energy, but its the system including these two masses. reply exitb 18 hours agorootparentprevI mostly got the speed from multiple Venus flybys, which slowed down in its orbit a minuscule amount. reply testoo 12 hours agorootparentprevthere's another cool aspect to your question too! \"Who gained and lost [energy] and how?\" >there actually is no objective answer as to which body gained and which lost energy! Energy is always conserved, but which way the transfer happened depends on your reference frame! this isn't too difficult to demonstrate: pick an inertial reference frame A such that the spacecraft is at rest following the \"collision\" (aka the slingshot). In this frame, the spacecraft has 0 kinetic energy post-slingshot; therefore, it lost energy in the slingshot, which was transferred to the Sun. Likewise, pick a frame B such that the Sun is at rest after the slingshot (this would be the more usual frame to pick). In this case, it's the Sun that lost energy, and the spacecraft that gained it. (depending on one's mechanics background this might appear anything from obvious to very weird and unintuitive) reply vl 17 hours agorootparentprevIncreasing speed with slingshot works because you leave vicinity of the planet in the same direction planet travels. Basically this allows you to add planet’s speed to your own. Within solar system you cannot increase speed by slingshotting around the sun. Total energy within system stays the same, some energy is transferred from the planet to the spacecraft. reply colechristensen 17 hours agorootparentprevFlybys to boost speed take a tiny tiny tiny part of a planet’s orbital kinetic energy and exchange it with a spacecraft. The silliest way to describe it is kind of like stepping in front of a bus, but instead of actually getting hit you just get close enough for gravity to pull you along with the planet. Orbital mechanics is really just not intuitive so you can’t get an easy explanation as one doesn’t exist, your life experience with momentum and gravity is just too different for it to make sense easily. reply rkagerer 18 hours agoprevTotally looks like a wormhole. Kelvin-Helmholtz Instability or Einstein-Rosen Bridge; could have fooled me. reply ben_w 13 hours agoparentOutside of sci-fi (as in: in physics based renderings) wormholes look like lenses, and a lens free-floating in space with only point-like stars behind it is hard to notice. ERBs in particular are unstable in a universe containing literally anything else including a single photon, so you can't ever see one. reply nurettin 7 hours agorootparentIt did look like a lens in interstellar. A sci-fi movie. reply ben_w 2 hours agorootparentUntil they went through it. Even though they had Kip Thorne as a scientific advisor and he made a scientifically accurate rendering of various environments, if they'd not made it a swooshy tunnel in the middle then the transit would have been somewhat boring (or sudden) cinema. https://youtu.be/yTpbZ_Psbeo?si=S6RjCd7vFaWrqUQe vs. https://youtu.be/V7e-1bRpweo?si=ab1PjECyuICvl4In reply nurettin 42 minutes agorootparentI love the augmented visualization! Just tried it on my phone. reply ed_mercer 19 hours agoprevWell that was disappointing. No color, no sense of scale and super short. I have no idea what I just saw. reply jcims 19 hours agoparentIf you watch this video it might help provide some orientation and context for the field of view. https://www.youtube.com/watch?v=IQXNqhQzBLM The Parker Solar Probe has a very eccentric orbit around the sun and mostly operates behind this large head shield which always faces the sun. So imagine it like horse blinders and the instruments are facing in the direction of travel and to some extent 'to the right' away from the sun. In the video the sun is always to the left and the probe is going through its closes approach of the orbit (aka perigee) which directly correlates to the velocity telemetry in the bottom left. At the highest speed, it's closest to the sun. So in the video of the vortex the sun is to the left, the axis of the vortex is likely pointing directly at the sun and the probe is flying past it. reply pavlov 19 hours agoparentprevIt’s footage captured inside the Sun’s corona with a scientific instrument, not a Hollywood VFX sequence. reply beeeeerp 19 hours agoparentprevThis probe is flying through an incredibly harsh environment, and it’s likely tuned for certain brightnesses/wavelengths to show features better. It’s also why a lot of space probes use false color; your eyes just wouldn’t be able to see features otherwise. reply echelon 19 hours agoparentprevIt's one of the coolest science images I've seen! I don't understand how one can't be in awe. Look at the time scale. Look at how big that structure is! We're traveling at nearly 0.1% the speed of light, at temperatures of 2,500 degrees Fahrenheit. This is an incredible testament to science and engineering. You know what else might look boring but is actually insanely cool? Emission spectra from exoplanets. Peaks on a graph, but we're sensing atmospheres from worlds our ancestors could never have imagined. Just think what lies ahead for our species. It's incredible to ponder. reply verisimi 16 hours agoprevAt this point in history, with the economies of scale in production from phones etc, surely it's cheaper to put in a colour camera?! reply mulmen 16 hours agoparent“Cheap” and “first in human history” rarely go together. When Apple offers a mass produced solar probe I’m sure it will have a color camera. Until then you have to be satisfied by the actual achievement instead. reply verisimi 12 hours agorootparentWe can send probes to the sun, to Mars, collect samples from asteroids billions of miles away.... but we can't get decent quality colour photos?!? That tech is beyond us? In 2024? .. wtf ... I really don't get why my original post is downvoted and being disputed. It seems such a basic point.. It such an oversight on the part of NASA, it borders on intentional. reply mulmen 11 hours agorootparent> It such an oversight on the part of NASA, it borders on intentional. This is why you are being downvoted. You clearly do not understand the difficulty of these achievements. Yet you claim NASA is incompetent for doing something literally nobody has ever done before. A color camera has no scientific value so they didn’t send one. It’s that simple. Assuming you know more than NASA is rightfully going to earn downvotes. You don’t know more than NASA. Approach this with an open mind and some curiosity and you’ll get a much warmer response. You might even learn something. reply verisimi 11 hours agorootparentnext [2 more] [flagged] mulmen 11 hours agorootparentWe already know what color the sun is. Color cameras take worse quality images and use more data to do so. The black and white images are higher fidelity and thus convey more useful information. > You are telling me that using your visual sense is irrelevant, because of all the data that is being collected. I’m not telling you that. Black and white pictures are still perceived with the visual sense. The picture is data. > Ie scientists like spreadsheets of info, databases, rather than imagery that is faithful to the human eye. Well, no, clearly they value images because they sent a camera. A human eye would be completely obliterated way before reaching the sun’s corona. It’s physically impossible for a person to perceive this environment “faithfully”. > everyone who can use their eyes to judge information will do so Yes but there is less useful information in a color image to judge. > you can do both cos the overhead of a colour camera is so low! The overhead for a useless instrument is extremely high. There are mass constraints, power constraints, and communication bandwidth constraints. A color camera would be a pointless waste of resources. reply kataklasm 16 hours agoparentprevModern day tech often reaches spaceflight circles decades after it becomes ubiquitous in normal use. For spaceflight purposes it is extremely vital that any and all components and tech is matured amd that takes a long time. Add in a ton of certification and paperwork processes and there you go. reply mulmen 11 hours agorootparentI disagree. There’s an enormous amount of bleeding edge tech in spacecraft. This camera was a totally custom thing. The constraints are simply so limiting that we have to make tradeoffs. reply Dylan16807 12 hours agorootparentprev> For spaceflight purposes it is extremely vital that any and all components and tech is matured It's not \"extremely vital\", it's just how NASA has been doing things for a good while. Sometimes ultra perfectionism makes sense. Sometimes you can aim for an 85% mission success rate and launch 5 probes for half the price a decade earlier. reply mulmen 11 hours agorootparent> Sometimes you can aim for an 85% mission success rate and launch 5 probes for half the price a decade earlier. …Which NASA also does. That’s the helicopter on Mars, and the first Starship mission. Which 85% success rate space mission did you pull off? reply Dylan16807 11 hours agorootparent> …Which NASA also does. That’s the helicopter on Mars So you agree with me that it's viable, great. I guess I should have been more clear that the perfectionism is how NASA almost always operates these days. > the first Starship mission. Not NASA. > Which 85% success rate space mission did you pull off? What's with this hostility? All I said is that it's not extremely vital. I didn't even say NASA definitely did anything wrong, just that there are options. reply verisimi 16 hours agorootparentprevYou don't think colour photography is mature? And even if that were the case, do you not think there would be some scientific value to having the photo in colour that it would be worth the risk? Personally - I think its ridiculous that NASA get so many billions but are unable to put in a decent colour camera. I can't see any acceptable reason for this. PS first colour photo was in 1890. PPS I mean 1861! https://www.bbc.com/news/13411083 reply ceejayoz 15 hours agorootparentThe probe's purpose is not to determine the color of the sun; we are readily capable of doing that from here. https://en.wikipedia.org/wiki/Parker_Solar_Probe#Instruments https://en.wikipedia.org/wiki/WISPR reply mcbutterbunz 15 hours agorootparentprevIn addition to all the other replies on this topic, monochrome sensors are capable of higher detail and higher sensitivity than color sensors. There's no scientific benefit to using a color sensor. In fact, a color sensor would be detrimental. reply kryptiskt 12 hours agorootparentYes, no professional telescope uses a color sensor, because the on-chip filters on those are terrible and doesn't go away when you don't want color. All color images are either done by combining images with different filters or are false color images. reply tekla 14 hours agorootparentprevOnly on HN do we find so many people that have so much high regard of their own intellect over a army of Engineers. reply ben_w 13 hours agorootparentI wish. AFAICT, that's the universal human condition. One of the great things about ChatGTP is as a framing point — I can now use it as a standard by which to say: \"this thing we all keep rolling our eyes at for its mistakes? It's knows more about this than ${person} does\" (sometimes I'm that ${person}, helps point me in the direction of intellectual humility). reply verisimi 14 hours agorootparentprevnext [7 more] [flagged] schoen 14 hours agorootparentHere's an article by the scientists who created the camera that took this picture, introducing it and describing its design. https://orbi.uliege.be/bitstream/2268/200751/1/The%20Wide-Fi... It's very much not an off-the-shelf camera; it seems to have involved years of custom engineering work. This article doesn't seem to address the specific question of \"why is this camera monochrome?\" but you can see that it wasn't trivial to make an instrument that would work well in this difficult environment. So it's definitely not like \"and let's throw a commercial digital camera on there too just for fun!\". It's a legitimate question why some kind of color camera wasn't considered worth including, but lots of space missions have sensors that are something other than a simulacrum of human vision. That's why so many astronomical images end up getting published in false color. https://en.wikipedia.org/wiki/False_color In almost all of those cases, the justification for the false color seems to be some form of \"true color wouldn't have been possible or appropriate for the scientific purposes of this imagery\". reply verisimi 12 hours agorootparentThank you for acknowledging this is a legitimate question. I've no issue with all sorts of cameras and scientific instruments being placed on a space craft. I don't expect to get access to all the data that is sent back - though - as it is from the public purse - I think it should be made available. My point is that the only interaction us great unwashed have with these missions is with the imagery that is provided.. How difficult would it be to have a colour camera?!?! I have a crap mobile and it has 3 cameras! And one on the front! When colour cameras have been available to everyone for so long, its simply inconceivable that NASA can never provide decent imagery! Its 2024 ffs, surely we can have colour by now! No? If not now, when? reply schoen 9 hours agorootparent> I don't expect to get access to all the data that is sent back - though - as it is from the public purse - I think it should be made available. You can indeed get the actual data from this instrument! https://wispr.nrl.navy.mil/wisprdata > How difficult would it be to have a colour camera?!?! I have a crap mobile and it has 3 cameras! And one on the front! When colour cameras have been available to everyone for so long, its simply inconceivable that NASA can never provide decent imagery! They do have missions that have great color cameras, like on Mars! The James Webb Space Telescope also produces great color imagery, but it's normally presented in false color because its camera is biased toward the infrared. https://webb.nasa.gov/ Here's one of their popular justifications for that: https://webbtelescope.org/webb-science/the-observatory/infra... In that diagram you see that the Webb sees more colors than the human eye, but most of them are ones that our eyes don't see. They have various arguments that the colors that it does see are more interesting and useful for seeing very faraway stuff. reply mulmen 11 hours agorootparentprev> How difficult would it be to have a colour camera?!?! As has been exhaustively explained to you already “more difficult than it is worth”. At this point you are willfully ignorant on this topic. > I have a crap mobile and it has 3 cameras! And I own a spatula. Both devices are equally capable of taking color photos in this environment. reply ben_w 13 hours agorootparentprevAs someone whose first programming job was processing multispectral satellite data[0][1]: if you put me in charge of a mission, my first question would be \"which specific frequencies provide the most scientific value?\", and then focus on that/those. They won't necessarily have anything in common with what you'd normally use for \"true colour\" (quite a lot of what you see in astronomy falls into this category: even when you see a colourised press-release, it's not what you'd see if you looked at it with your natural eye). As for \"what about a colour camera\": First, look at all the noise in the images, all those slightly curved streaks. That's radiation going through the satellite and hitting the sensor from the side. Normal consumer stuff isn't even trying to cope with that sort of environment. Second, look at how low the frame rate is. That suggests the data rate is really low, and they probably don't have spare capacity for anything merely decorative. [0] https://en.wikipedia.org/wiki/SeaWiFS [1] DOI: 10.3354/meps07437 reply mulmen 11 hours agorootparentprev> Just ask yourself, if you were in charge of the mission, in what world would you decide to use a black and white camera rather than colour to capture the data of what is probably a multi-billion dollar mission? In this world, where I choose the best tool for the job. Color cameras are worse for this task. I don’t make engineering decisions based on gut reactions to things I know nothing about. I ask questions then make a decision based on facts and objectives. reply elorant 16 hours agoparentprevThe camera has to be shielded against various forms of radiation that are emitted from the Sun and that could be a limiting factor for its capabilities. reply hanniabu 16 hours agoparentprevThe conditions it's operating under and level of reliability needed are completely different reply bookofjoe 21 hours agoprev [–] 'Three-Body Problem' https://archive.ph/B1Kui reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA's Parker Solar Probe recorded footage of large \"vortex-like structures\" in the sun's atmosphere resulting from a coronal mass ejection interacting with the solar wind.",
      "These unique vortices, called \"Kelvin-Helmholtz instabilities,\" offer valuable insights into predicting and comprehending CMEs, which pose risks to Earth's communication and power systems.",
      "Equipped with a heat shield, the probe will persist in investigating the sun's corona to advance the understanding of these occurrences."
    ],
    "commentSummary": [
      "NASA spacecraft recorded a vortex in the sun's atmosphere, validating plasma instability forecasts, estimated to be 3 to 13 times the size of Earth.",
      "Discussions centered around spacecraft velocity, energy transmission from the Sun, and navigating Venus flybys.",
      "The dispute regarding color camera adoption in space missions emphasized the challenges of decision-making in space exploration, advocating for monochrome cameras for scientific accuracy and cost-effectiveness, as demonstrated by the James Webb Space Telescope."
    ],
    "points": 195,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1712407815
  },
  {
    "id": 39951571,
    "title": "Loki: Enhancing Fact Verification with Open-Source Tool",
    "originLink": "https://github.com/Libr-AI/OpenFactVerification",
    "originBody": "Loki: An Open-source Tool for Fact Verification Overview Loki is our open-source solution designed to automate the process of verifying factuality. It provides a comprehensive pipeline for dissecting long texts into individual claims, assessing their worthiness for verification, generating queries for evidence search, crawling for evidence, and ultimately verifying the claims. This tool is especially useful for journalists, researchers, and anyone interested in the factuality of information. To stay updated, please subscribe to our newsletter at our website or join us on Discord! Components Decomposer: Breaks down extensive texts into digestible, independent claims, setting the stage for detailed analysis. Checkworthy: Assesses each claim's potential significance, filtering out vague or ambiguous statements to focus on those that truly matter. For example, vague claims like \"MBZUAI has a vast campus\" are considered unworthy because of the ambiguous nature of \"vast.\" Query Generator: Transforms check-worthy claims into precise queries, ready to navigate the vast expanse of the internet in search of truth. Evidence Crawler: Ventures into the digital realm, retrieving relevant evidence that forms the foundation of informed verification. ClaimVerify: Examines the gathered evidence, determining the veracity of each claim to uphold the integrity of information. Quick Start Prerequisites Python 3.9 or newer Required Python packages are listed in requirements.txt Installation Clone the repository: git clone https://github.com/Libr-AI/factcheckservice.git Navigate to the project directory and install the required packages: cd factcheckservice pip install -r requirements.txt Configure api keys cp factcheck/config/secret_dict.template factcheck/config/secret_dict.py You can choose to export essential api key to the environment, or configure it in factcheck/config/secret_dict.py. Example: Export essential api key to the environment export SERPER_API_KEY=... # this is required in evidence retrieval if serper being used export OPENAI_API_KEY=... # this is required in all tasks export ANTHROPIC_API_KEY=... # this is required only if you want to replace openai with anthropic Test To test the project, you can run the factcheck.py script: # String python factcheck.py --modal string --input \"MBZUAI is the first AI university in the world\" # Text python factcheck.py --modal text --input demo_data/text.txt # Speech python factcheck.py --modal speech --input demo_data/speech.mp3 # Image python factcheck.py --modal image --input demo_data/image.webp # Video python factcheck.py --modal video --input demo_data/video.m4v Usage The main interface of the Fact-check Pipeline is located in factcheck/core/FactCheck.py, which contains the check_response method. This method integrates the complete pipeline, where each functionality is encapsulated in its class as described in the Features section. Example usage: from factcheck.core.FactCheck import check_response # Example text text = \"Your text here\" # Run the fact-check pipeline results = check_response(text) print(results) Web app usage: python webapp.py Contributing We welcome contributions from the community! If you'd like to contribute, please follow these steps: Fork the repository. Create a new branch for your feature (git checkout -b feature/AmazingFeature). Commit your changes (git commit -m 'Add some AmazingFeature'). Push to the branch (git push origin feature/AmazingFeature). Open a pull request. Ready for More? 💪 Join Our Journey to Innovation with the Supporter Edition As we continue to evolve and enhance our fact-checking solution, we're excited to invite you to become an integral part of our journey. By registering for our Supporter Edition, you're not just unlocking a suite of advanced features and benefits; you're also fueling the future of trustworthy information. Your support enables us to: 🚀 Innovate continuously: Develop new, cutting-edge features that keep you ahead in the fight against misinformation. 💡 Improve and refine: Enhance the user experience, making our app not just powerful, but also a joy to use. 🌱 Grow our community: Invest in the resources and tools our community needs to thrive and expand. 🎁 And as a token of our gratitude, registering now grants you complimentary token credits—a little thank you from us to you, for believing in our mission and supporting our growth! Feature Open-Source Edition Supporter Edition Trustworthy Verification Results ✅ ✅ Diverse Evidence from the Open Web ✅ ✅ Automated Correction of Misinformation ✅ ✅ Privacy and Data Security ✅ ✅ Multimodal Input ✅ ✅ One-Stop Custom Solution ❌ ✅ Customizable Verification Data Sources ❌ ✅ Enhanced User Experience ❌ ✅ Faster Efficiency and Higher Accuracy ❌ ✅ TRY NOW! Stay Connected and Informed Don’t miss out on the latest updates, feature releases, and community insights! We invite you to subscribe to our newsletter and become a part of our growing community. 💌 Subscribe now at our website! License This project is licensed under the MIT license - see the LICENSE file for details. Acknowledgments Special thanks to all contributors who have helped in shaping this project. Cite as @misc{Loki, author = {Wang, Hao and Wang, Yuxia and Wang, Minghan and Geng, Yilin and Zhao, Zhen and Zhai, Zenan and Nakov, Preslav and Baldwin, Timothy and Han, Xudong and Li, Haonan}, title = {Loki: An Open-source Tool for Fact Verification}, year = {2024}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\\url{https://github.com/Libr-AI/Loki}}, }",
    "commentLink": "https://news.ycombinator.com/item?id=39951571",
    "commentBody": "Loki: An open-source tool for fact verification (github.com/libr-ai)192 points by Xudong 23 hours agohidepastfavorite52 comments axegon_ 17 hours agoOverall great idea though, I'll be definitely checking it back in the future. A few things that hit me out of the box: * The idea behind using Serper is great, however it would be cool if other search engines/data sources can be used instead, ie. Kagi or some private search engine/data. Reason for the latter: there are tons of people who are sourcing all sorts of information which will not immediately show up on google and some might never do. For context: I have roughly 60GB (and growing) of cleaned news article with where I got them from and with a good amount of pre-processing done on the fly(I collect those all the time). * Relying heavily on OpenAI. Yes, OpenAI is great but there's always the thing at the back of our minds that is \"where are all those queries going and do we trust that shit won't hit the fan some day\". It would be nice to have the ability to use a local LLM, given how many and how good there are around. * The installation can be improved massively: setuptools + entry_points + console_scripts to avoid all the hassle behind having to manage dependencies, where your scripts are located and all that. The cp factcheck/config/secret_dict.template factcheck/config/secret_dict.py is a bit.... Uuuugh... pydantic[dotenv] + .env? That would also make the containerizing the application so much easier. reply Xudong 14 hours agoparentThank you for your suggestions, axegon!!! We will definitely consider them and add the features in a future version shortly. Regarding the first version, we are currently working on enabling customized evidence retrieval, including local files. Our plan is to integrate existing tools like LlamaIndex. Any suggestion is greatly appreciated! Regarding the second point, we have found OpenAI's JSON mode to be greatly helpful, and have optimized our prompts to fully utilize these advances. However, we agree that it would be beneficial to enable the use of other models. As promised, we will add this feature soon. Lastly, we appreciate your suggestion and will work on improving the installation process for the next version. reply big_hacker 13 hours agorootparentDead internet. reply antihipocrat 12 hours agorootparentHave to agree with you, every comment from the product creator reads like a chatGPT response. reply swores 0 minutes agorootparentTo me it sounds like someone who speaks English as a second language, writing well and clearly and in a formal style. It's just unlucky for them that that's the style GPT is so good at too. Xudong 11 hours agorootparentprevI will take it as a compliment, lol. But I do hope ChatGPT or some agents could help me with this. Btw, our recent study on machine-generated text detection might be interesting to you. https://arxiv.org/abs/2305.14902 https://arxiv.org/abs/2402.11175 reply xyst 15 hours agoparentprevI fully expect some sort of enshittification of openai at some point. reply lta 14 hours agorootparentThat's assuming it's not done already with their mission of being open completely forgotten reply swores 16 hours agoprevFeedback on the example gif: at the moment it's almost comically useless. First you're bored watching the beginning 90% while commands are slowly being typed, and then the bit that's actually interesting and worth reading scrolls too fast and then resets to the beginning of the gif before there's a chance to read it. reply Xudong 13 hours agoparentThanks for your feedback on the gif figure, swores! We will revise it soon. reply eMPee584 11 hours agoparentprevmpv ftw: playback speed control even for gifs.. reply martinbaun 13 hours agoprevMaybe the name is not so fitting as Loki is a name in Norse Mythology. Known for deceiving and lying which is basically the opposite you're trying to do :) reply smoyer 13 hours agoparentIt's also the name of a well-known open-source log collection system that's part of the LGTM stack (predominantly led by GrafanaCloud Labs.) reply martinbaun 1 hour agorootparentDon't want to be that guy, but I still don't get it :) You want your logs to lie? :D reply croes 13 hours agoparentprevMaybe it's on purpose. Who could better know the patterns of liars than the god of lying. reply Uehreka 9 hours agorootparentWhen coming up with a name for something, the significance of the name should be the first thing that comes to mind, not the opposite thing or a thing that requires explanation. When people say the name of your tool, you won’t always be there to explain it. reply martinbaun 1 hour agorootparentExactly, it needs to be an \"Ah, of course\" moment from the first second reply jimmygrapes 6 hours agorootparentprevDiscord comes to mine although I know it's a play on Eris (Free Net), same as Slack is a nod to Uncle Bob reply croes 1 hour agorootparent>The name Discord was chosen because it \"sounds cool and has to do with talking\", was easy to say, spell, remember, and was available for trademark and website. In addition, \"Discord in the gaming community\" was the problem they wished to solve. reply martinbaun 1 hour agorootparentprevI didn't get this, can you tell me more? reply martinbaun 1 hour agorootparentprevReverse Psychology, I like it but still makes you wonder no? reply vinni2 20 hours agoprevSo only thing they open sourced is the prompts [1] and code to call LLM APIs? There are plenty of such libraries out there. And the prompts seem to be copied from here [2]? [1] https://github.com/Libr-AI/OpenFactVerification/blob/main/fa... [2] https://github.com/yuxiaw/Factcheck-GPT/blob/main/src/utils/... reply raycat7 18 hours agoparentregarding your last concern, I found that yuxiaw is their COO[1], so it can't be considered a copy? [1] https://www.librai.tech/team reply vinni2 18 hours agorootparentOk but bigger issue is there is evidence that the LLMs are not better than specialized models for fact-checking. https://arxiv.org/abs/2402.12147 reply Xudong 14 hours agorootparentHello vinni2, thank you for mentioning the paper. However, I noticed that it hasn't gone through peer review yet. Also, the paper suggests that fine-tuning may work better than in-context learning, but that's not a problem. You can fine-tune any LLMs like GPT-3.5 for this purpose and use them with this framework. Once you have fine-tuned GPT, for example, with specific data, you'll only need to modify the model name (https://github.com/Libr-AI/OpenFactVerification/blob/8fd1da9...). I believe this approach can lead to better results than what the paper suggests. reply vinni2 20 hours agoprevIt’s a bit misleading to call it open source tool when it relies on proprietary LLMs for everything. reply btbuildem 18 hours agoparentPresumably the LLMs are swappable -- today the proprietary ones are very powerful and accessible, but the landscape may yet change. reply vinni2 18 hours agorootparentWell but they don’t mention that it is clickbait to call it open source fact checking tool which needs LLMs to do everything. Also code is not designed to easily swap with a free locally running LLM. reply Xudong 14 hours agorootparentI apologize for any confusion caused earlier. The core components have been defined separately (https://github.com/Libr-AI/OpenFactVerification/tree/main/fa...) to make customization easier. We understand that switching between different LLMs isn't particularly easy in the current version. However, we will be adding these features in future versions. You are most welcome to collaborate with us and contribute to this project! reply rjb7731 19 hours agoprevIsn't this similar to the Deepmind paper on long form factuality posted a few days ago? https://arxiv.org/abs/2403.18802 https://github.com/google-deepmind/long-form-factuality/tree... reply Xudong 15 hours agoparentYes, they are similar. Actually, our initial paper was presented around five months ago (https://arxiv.org/abs/2311.09000). Unfortunately, our paper isn't cited by the DeepMind paper, which you may see this discussion as an example: https://x.com/gregd_nlp/status/1773453723655696431 Compared with our initial version, we have mainly focused on its efficiency, with a 10X faster checking process without decreasing accuracy. reply westurner 12 hours agorootparent> We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document A 2020 Meta paper [1] mentions FEVER [2], which was published in 2018. [1] \"Language models as fact checkers?\" (2020) https://scholar.google.com/scholar?cites=3466959631133385664 [2] https://paperswithcode.com/dataset/fever I've collected various ideas for publishing premises as linked data; \"#StructuredPremises\" \"#nbmeta\" https://www.google.com/search?q=%22structuredpremises%22 From \"GenAI and erroneous medical references\" https://news.ycombinator.com/item?id=39497333 : >> Additional layers of these 'LLMs' could read the responses and determine whether their premises are valid and their logic is sound as necessary to support the presented conclusion(s), and then just suggest a different citation URL for the preceding text > [...] \"Find tests for this code\" > \"Find citations for this bias\" From https://news.ycombinator.com/item?id=38353285 : > \"LLMs cannot find reasoning errors, but can correct them\" https://news.ycombinator.com/item?id=38353285 > \"Misalignment and [...]\" reply dekervin 14 hours agoprevI have a project where I take a different approach [0] . I basically extract statements , explicit or implicit , that should be accompanied by a reference to some data but aren't and I let user find the most relevant data for those statements. [0] https://datum.alwaysdata.net/ reply chamomeal 20 hours agoprevVery cool! I’ve toyed with an idea like this for a while. The scraping is a cool extra feature, but tbh just breaking down text into verifiable claims and setting up the logic tokens is way cooler. I imagine somebody feeding a live presidential debate into this. Could be a great tool for fact checking reply Xudong 14 hours agoparentahah thanks! reply t0bia_s 3 hours agoprevHow does AI observe facts in real world? I find hilarious clarify something as fact checkig based on data on internet. reply siffland 14 hours agoprevWhen I saw Loki as the name, I instantly thought of Grafana Loki for logging. I click on the GitHub and get Libr-AI and OpenFactVerification. I am not commenting on the actual software and I know names are hard and often overlap, but with something as popular as Loki already used for logging I think it might get confusing. reply Xudong 14 hours agoparentHi siffland! Thank you for your feedback. We understand your concern about the potential confusion given the popularity of Grafana Loki in the logging space. When naming our project, we sought a name that encapsulates our goal of combating misinformation. We chose Loki, inspired by the Norse god often associated with stories and trickery, to symbolize our commitment to unveiling the truth hidden within nonfactual information. When we named our project, we were unaware of the overlap with Grafana Loki. We appreciate you bringing this to our attention! I will discuss this issue with my team in the next meeting, and figure out if there is a better way of solving this. If you have any suggestions or thoughts on how we can better differentiate our project, we would love to hear them. Thank you again for your valuable input! reply dscottboggs 20 hours agoprevThat seems like something unlikely to do well at being automated, and not that at least current-gen ai is capable of. Does it...work? reply Xudong 15 hours agoparentHi there, I agree that fact-checking is not something that current generative AI models can directly solve. Therefore, we decompose this complex into five simpler steps, which current techniques can better solve. Please refer to https://github.com/Libr-AI/OpenFactVerification?tab=readme-o... for more details. However, errors can always occur. We try to help users in an interpretable and transparent way by showing all retrieved evidence and the rationale behind each assessment. We hope this could at least help people when dealing with such problems. reply szszrk 15 hours agoparentprevI just tried similar queries as they show on their screenshots with Kagi. Basically asked it the exact same question. While it answered a general \"yes\" when the more precise answer was \"no\", the motivation in the answer was perfectly on point and exactly the same things. As a general LLM for regular user fastGPT (their llm service) is in my opinion \"meh\" (lacks conversations for instance). But it's really impressive that it contains VERY recent data (like news and articles from last few days) and always provides great references. reply tudorw 18 hours agoprevAnyone tried this? https://journaliststudio.google.com/pinpoint/about reply badrunaway 12 hours agoprevI found it very interesting. I had this funny thought that just like CAPTCHA, may be soon we will have to ask humans to give their input on fact verification systems at scale. reply eeue56 15 hours agoprevInteresting. In the Nordics, we have a couple of sites dedicated to fact checking news stories, done by real people. I think these kinds of automated tools can be helpful too, but needs to be tied to reliable sources. This became pretty apparent to me with the tech news coverage of xz, too. Lots of accidental (or sometimes intentional?) misinformation being spread in news articles. I wrote about it a bit[0], it was pretty sad to see big international publishers publishing an article based entirely on the journalist's misunderstandings of the situation. Facts and truth is important, especially as we see gen AI furthering the amount of legitimate looking content online that might not actually be true. [0] - https://open.substack.com/pub/thetechenabler/p/trust-in-brea... reply Xudong 14 hours agoparentI wholeheartedly agree on the necessity of linking fact-checking tools to credible sources. Currently, our team's expertise lies primarily in AI, and we find ourselves at a disadvantage when it comes to pinpointing authoritative sources. Acknowledging the challenges posed by the rapid spread of misinformation, as highlighted by recent studies, we developed this prototype to assist in information verification. We recognize the value of collaboration in enhancing our tool's effectiveness and invite those experienced in evaluating sources to join our effort. If our project interests you and you're willing to contribute, please don't hesitate to reach out. We're eager to collaborate and make a positive impact together. reply pelasaco 15 hours agoparentprev> In the Nordics, we have a couple of sites dedicated to fact checking news stories, done by real people. We have it everywhere. The problem is however well-known: Human bias, political engagement from the fact checkers, etc.. AI (without any kind of lock, political bias built-in etc) could be the real deal, but because it may be not political correct, it will never happen. reply RcouF1uZ4gsC 19 hours agoprev> This tool is especially useful for journalists, researchers, and anyone interested in the factuality of information. Sorry, I think an individual who is not only aware of reliable sources to verify information, and who is not familiar enough with LLMs to come up with appropriate prompts and judge output should be the last person presenting themselves as the judger of factual information. reply Xudong 15 hours agoparentThanks for your response. When discussing fact-checking capabilities, the key question is always: Can we guarantee that it will always offer the correct justification? While it's unfortunate, errors can occur. Nonetheless, we prioritize making the checking process both interpretable and transparent, allowing users to understand and trust the rationale behind each assessment. We present the results at each step to help users understand the decision process, which can be seen from our screenshot at https://raw.githubusercontent.com/Libr-AI/OpenFactVerificati... We will try our best to ensure this tool makes a positive difference reply Der_Einzige 17 hours agoprevYou might want to look into integrating DebateSum or OpenDebateEvidence (OpenCaseList) into this tool as sources of evidence. They are uniquely good for these sorts of tasks: https://huggingface.co/datasets/Hellisotherpeople/DebateSum https://huggingface.co/datasets/Yusuf5/OpenCaselist reply Xudong 15 hours agoparentHi Der_Einzige, thanks for pointing out these two great datasets! We are currently working on including customized evidence sources internally and will definitely consider these two datasets in the future version of this open-source project. reply redder23 11 hours agoprevThe name Loki is such a great fit! WOW! This is some giant BS that is for sure. Some stupid, literally brain-dead AI searching things created by humans to determine what is a \"fact\". This is beyond dystopian crap. We all know all the fact-checker orgs. used by big tech like Facebook and others are filled with hyper biased woke people who do not actually fact-check things but get off on having the power to enforce their beliefs, feelings and biases. I can already tell this is total BS without even looking into it, what kinds of sources will it use? What ranking will they give them? Snopes? ROFL. Probably just uses some woke infested, censored and curated language model to determine a fact based on what has the most matches or THE MOST LIKELY because that how AI works. Has absolutely nothing to do with facts. And it's even worse, we are literally in a time when AI hallucinates things that do not exist. I won't use a stupid AI to find me \"facts\". reply meling 18 hours agoprev [–] My friend’s startup: https://factiverse.ai/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Loki is an open-source tool dedicated to automating fact verification by analyzing text, generating queries for evidence search, and verifying claims, beneficial for journalists, researchers, and information verification enthusiasts.",
      "Its components include Decomposer, Checkworthy, Query Generator, Evidence Crawler, and ClaimVerify, allowing a comprehensive fact-checking process.",
      "The project offers installation guidelines, welcomes contributors, and provides a Supporter Edition with advanced features. It is licensed under MIT, and users can subscribe to the newsletter for updates and engage with the community."
    ],
    "commentSummary": [
      "The post introduces \"Loki,\" an open-source fact verification tool, with suggestions for enhancements like utilizing diverse data sources and improving installation.",
      "Xudong, the creator, intends to incorporate these recommendations based on community feedback.",
      "Discussions on GitHub center around a study on machine-generated text detection, concerns regarding language models for fact-checking, and the significance of accuracy and reliability in AI-based fact-checking tools."
    ],
    "points": 192,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1712401141
  },
  {
    "id": 39955148,
    "title": "VPN surge amid porn restrictions: Legal debate looms",
    "originLink": "https://www.popsci.com/technology/vpn-boom/",
    "originBody": "Technology Internet Online porn restrictions are leading to a VPN boom New internet laws requiring age verification for porn and social media may be contributing to a surge in VPN adoption, experts say. By Mack DeGeurinPublished Apr 3, 2024 3:00 PM EDT Technology VPN technology has, for years, been used by whistleblowers, journalists, and political dissidents worldwide to bolster their anonymity online, especially in authoritarian countries. DepositPhotos SHARE Internet users in a handful of states across the US are finding it more difficult to browse parts of the web anonymously. Over a dozen states, including Texas and Louisiana, have enacted legislation forcing Pornhub and other purveyors of streaming online adult videos to verify the identities of its users to ensure children and teens aren’t accessing “sexual material harmful to minors.” Elsewhere, in states like Florida, lawmakers have introduced so-called online parental consent laws that would limit or ban underage users from accessing social media services over claims they cause psychological harm. In each case, lawmakers want online platforms to collect government-IDs from users or have them submit to third-party age verification methods to ensure they are indeed adults. But determining whether or not kids and teens are actually accessing those sites means platforms have no choice but to verify the ages of all users accessing their sites, minor or otherwise. Adult porn viewers, who could previously dip in and out of websites with a relative degree of anonymity, may now fear having their government name and photograph at arms length away from their last Pornhub search query. At the same time, critics of the new laws worry some far-right, religiously conservative lawmakers could broadly interpret “adult” material to include content from LGBTQ+ creators or other people from marginalized groups who rely on the internet for a sense of community. In that scenario, teens from abusive or difficult family structures could find themselves shut out from support structures online. Experts speaking with PopSci say there are signs internet users in many of these states are turning to Virtual Private Networks (VPNs) to access otherwise blocked materials. Leading VPN provider Top10 VPN claims demand from VPN services jumped 275% on March 15, the same day Pornhub cut off access in Texas. The site says demand for VPNs similarly surged by 210% the day after a similar law took effect in Louisiana last year. ExpressVPN, another popular VPN provider, told PopSci it saw increased web traffic to its site the day anti-porn, online age verification bills took effect in seven out of eight states. “Wherever U.S. lawmakers have imposed age verification on internet users trying to access adult content online over the past 12 months, there has been a clear trend in the corresponding surges in demand for VPNs,” Top10 VPN Head of Research Simon Migliano told PopSci. In the most extreme case, Migliano claims Top10 VPN saw demand for the technology jolt up 847% the day the state’s new laws came into effect. How are VPNs being used? VPNs, which date back to the mid 1990s, create an encrypted tunnel for user’s data and can make it appear as if their computer is based in a different geographical location. Digital streaming viewers often use this VPN masking technology to access shows restricted in certain markets and blacked out sports events. Others view VPNs as useful tools for adding layers of security to private communications. That same technology has, for years, been used by whistleblowers, journalists, and political dissidents worldwide to bolster their anonymity online, especially in authoritarian countries. “A VPN is an effective tool for circumventing any kind of internet censorship, as it allows users to access the restricted content via an IP address from a location under a different jurisdiction from their own,” Migliano said. Though commonly used to bypass content restrictions in other countries, Center for Democracy and Technology Vice President of Policy Samir Jain says their apparent use by Americans to sidestep domestic content restrictions feels “relatively new.” That sudden shift, Jain said, owes itself partly to the language of these new laws which would have previously struggled to stand up to legal scrutiny. Jain, whose organization signed onto an amicus’s brief calling on a court to block the Texas law, said he wasn’t surprised users from affected areas states appeared to be seeking out VPNs. “If you provide a government ID to prove you are in effect no longer anonymous,” Center for Democracy and Technology Vice President of Policy Samir Jain told PopSci. “If people no longer feel like they can do that [access information anonymously] that infringes on their First Amendment expression right.” ExpressVPN Privacy Advocate Lauren Hendry Parsons echoed that sentiment. “We know that when legislators restrict consumer access to services like porn, citizens still find a way to access it,” Hendry Parsons told PopSci. “There is absolutely a middle ground to be found that leans on third-party cooperation instead of limiting consumer rights.” How are platforms responding to the new laws? As of writing, seven mostly Republican-led states have passed some form of legislation relying on age-verification to prevent minors from accessing pornographic material. Nearly all of these so-called “age-gating” bills are copy-cat versions of a pioneering Louisiana legislation, which passed in 2022 and took effect early last year. The Verge estimates the Louisiana bill inspired at least 17 copycat bills, a handful of which are on their way to becoming laws. In Texas, sites found in violation of its law could face penalties of up to $10,000 per day. Some adult content sites like Pornhub have opted to block IP addresses originating from states with these new laws in order to avoid running afoul of the laws. Last month, internet users in Texas attempting to access the world’s largest purveyor of online adult video content were greeted instead with a 10 paragraph note from the company explaining its opposition to the state’s “ineffective, haphazard, and dangerous” law. Pornhub has similarly restricted access to users from half a dozen other states with similar age verification laws. In addition to wanting to steer clear of penalties, experts told PopSci platforms also oppose the laws because they don’t want to be responsible for collecting and maintaining torrents of sensitive users’ data that could pose a ripe target for cybercriminals. “Age verification systems collect a huge amount of data, not only the personal information from each ID but also a record of each and every authentication made—essentially any site you access that features adult content,” Hendry Parsons said. “Combined with the data profiling social media companies create about their users, this treasure trove of personal information is a perfect target for bad actors.” Rising VPN use could attract new lawmaker scrutiny US internet users are reportedly using VPNs to access non-porn related material as well. College students around the country are reportedly already using VPNs to get around efforts from some universities to ban TikTok on campus networks. In Montana, where lawmakers passed a first-of-its kind statewide TikTok ban, creators have been preparing to similarly use the technology to stay connected to their followers. Lawmakers interested in restricting popular online content of various kinds will inevitably find themselves running into a VPN service willing to offer users an escape tunnel. But a continued uptick in VPN to access blocked risks inviting unintended consequences. Internet users appearing to use VPNs to blatantly run afoul of new legislation could incentivize lawmakers to clamp down on the technology. Some of the anti-porn laws, like the one enacted in Utah, already possess language explicitly prohibiting online platforms from letting minors “change or bypass restrictions on access.” Digital rights activists fear other recently proposed legislation aimed at limiting US user access to foreign apps may include provisions in it that would criminalize the use of VPNs. Jain, from the Center for Democracy and Technology, acknowledged those concerns but said new laws banning criminalizing or restricting VPNs could do more harm than good and may face constitutional legal challenges. As for the new wave of laws appearing to fuel the rise in American VPN adoption, Jain said debates over one of those laws could eventually make its way up to the Supreme Court. “There are a lot of legitimate reasons to use VPNs to protect your privacy and anonymity,” Jain said. Internet",
    "commentLink": "https://news.ycombinator.com/item?id=39955148",
    "commentBody": "Porn restrictions are leading to a VPN boom (popsci.com)183 points by geox 13 hours agohidepastfavorite236 comments rrr_oh_man 12 hours agoAnecdote: I stayed in the Airbnb of an (obviously gay) couple in the South-Eastern very conservative Turkish city of Mersin many years ago. The absolute joy they had when I showed them how to set up 1.1.1.1 on their Android TV, phones, and laptops… so they could watch unrestricted gay porn in their heavily censored and state-controlled slice of the World Wide Web… I still get a virtual greeting card from them once in a while. reply godelski 10 hours agoparent> how to set up 1.1.1.1 I just want to mention a few things: Fallback, other DNSs like 1.* and Mullvad's free DNS with more options # Standard 1.1.1.1 1.0.0.1 2606:4700:4700::1111 2606:4700:4700::1001 # Block Malware 1.1.1.2 1.0.0.2 2606:4700:4700::1112 2606:4700:4700::1002 # Block Malware & Adult Content (not useful for this case) 1.1.1.3 1.0.0.3 2606:4700:4700::1113 2606:4700:4700::1003 You can find cloudflare information here[0], and remember to make sure you setup DNS over DoT (TLS) or DoH (HTTPS). Especially for them they will want to have encrypted DNS. Mullvad also offers *free* DNS[1], which also supports encrypted DNS # DoH is port 443 and DoT is port 853 # Standard dns.mullvad.net 194.242.2.2 2a07:e340::2 # Block Trackers adblock.dns.mullvad.net 194.242.2.3 2a07:e340::3 # Block Trackers + Malware base.dns.mullvad.net 194.242.2.4 2a07:e340::4 Mullvad also has block for Adult + gambling and social media (so there are 6 total configurations). You don't need the Mullvad VPN to use these. I should also mention, as this frustrated me a bit, that your browser may implement its own DNS and so just setting these in your router (or pihole) may not completely resolve the issue. In Firefox, go to Settings > Privacy & Security > (scroll all the way down) Enable DNS over HTTPS using > then under either \"Increased Protection\" or \"Max Protection\" you can set a DNS resolver (or turn it off). They have defaults for Cloudflare (default!) and NextDNS. While you're there, also check your settings at the top of that page about \"Enhanced Tracking Protection\" I am *NOT* a network/security person and would greatly appreciate replies to this comment with additional information. Especially about setting up things like piholes, TVs, browsers, encrypted DNS (especially this!), host files, and so on. Technical forum, so let's get technical and learn ^__^ [0] - https://developers.cloudflare.com/1.1.1.1/ip-addresses/ - https://developers.cloudflare.com/1.1.1.1/setup/#dns-over-ht... [1] https://mullvad.net/en/help/dns-over-https-and-dns-over-tls reply Terretta 6 hours agorootparentAnd AdGuard ... which also blocks ads. Plain DNS: # Default servers # AdGuard DNS will block ads and trackers. IPv4: 94.140.14.14 94.140.15.15 IPv6: 2a10:50c0::ad1:ff 2a10:50c0::ad2:ff # Non-filtering servers # AdGuard DNS will not block ads, trackers, or any other DNS requests. IPv4: 94.140.14.140 94.140.14.141 IPv6: 2a10:50c0::1:ff 2a10:50c0::2:ff # Family protection servers # AdGuard DNS will block ads, trackers, adult content, # and enable Safe Search and Safe Mode, where possible. IPv4: 94.140.14.15 94.140.15.16 IPv6: 2a10:50c0::bad1:ff 2a10:50c0::bad2:ff Adguard DNS-over-HTTPS (DoH) # Default server # AdGuard DNS will block ads and trackers. https://dns.adguard-dns.com/dns-query # Non-filtering server # AdGuard DNS will not block ads, trackers, or any other DNS requests. https://unfiltered.adguard-dns.com/dns-query # Family protection server # AdGuard DNS will block ads, trackers, adult content, # and enable Safe Search and Safe Mode, where possible. https://family.adguard-dns.com/dns-query DNS-over-TLS # Default server # AdGuard DNS will block ads and trackers. tls://dns.adguard-dns.com # Non-filtering server # AdGuard DNS will not block ads, trackers, or any other DNS requests. tls://unfiltered.adguard-dns.com # Family protection server # AdGuard DNS will block ads, trackers, adult content, # and enable Safe Search and Safe Mode, where possible. tls://family.adguard-dns.com See more: https://adguard-dns.io/en/public-dns.html reply wolverine876 9 hours agoparentprev> The absolute joy they had when I showed them how to set up 1.1.1.1 on their Android TV, phones, and laptops… so they could watch unrestricted gay porn in their heavily censored and state-controlled slice of the World Wide Web… I'd be very careful giving security advice to someone in that situation - in fact, I probably wouldn't do it, even though I am an experienced professional who knows more than most people in IT. Think about accuracy, which is correctness, completeness, and consistency: Giving security advice to buddies trying to watch out-of-country sporting events has a much different accuracy requirement than advice for someone whose liberty, life, or pursuit of happiness (assets) are at stake. If your advice is incorrect, incomplete (you don't know or account for the entire picture), or inconsistent (different pieces of your advice don't line up) - these people don't miss their favorite team, the might go to jail, get tortured, lose their children, their careers, etc. When I give professional security advice, I spend a lot of time on it. I make sure I know the whole picture, my advice is correct - rechecking old understandings, testing and retesting, etc. - and it's consistent. These people are trusting me. And then if I'm not sure of those things, I find someone who knows better than I do. I don't have time to do that at the Airbnb. In this case, it seems to risk incorrectness and incompleteness: It may be incorrect because setting up DNS lookups so there are no leaks is tricky - I'm not even sure how to do it reliably. For example, some apps and OSes have DNS servers hardcoded; IME some just bypass your configuration (maybe the dev just didn't care to integrate with the system DNS settings); IIRC some have hardcoded fallbacks; sometimes the client overrides your config with the local gateway or the servers it supplies; also, DNS has a complex federated infrastructure. (My first stab at secure DNS is a secure VPN that doesn't leak - that seems much easier). It's incomplete: the authorities might not see the DNS lookups but they will clearly see the IPs that the user accesses, including those of the porn websites. These people are trusting you and you told them they could safely look at porn. Yikes, I would not want that responsibility without some serious work. reply m3kw9 9 hours agorootparentIf they aren’t technical enough to know your setup, they won’t know their current setup either. No big deal, keep it relaxed. reply tawai 2 hours agorootparentprevNo shit they can safely watch gay porn, it isn’t Russia/Iran reply tawai 2 hours agoparentprevSome people are reaching to really wrong conclusions from this post, so a couple of pointers 1. Gay porn is not specifically blocked in Turkey, porn is; so gay porn watchers would be mostly indistinguishable from any porn watchers via their evasion techniques 2. Not that it matters as you are free to be gay in Turkey 3. Not that it matters again, because access to blocked content is never prosecuted. No one ever receives letters from their ISP even for pirating 4. Calling the internet state controlled is a bit far imo, smaller ISPs don’t even apply some blocks and comparatively USDoJ can do domain seizures which is not too different from what happens there (websites being blocked via court orders and “administrative preventative measures”). Although I’d agree that the line for blocking is drawn too close. 5. I also wouldn’t call Mersin very conservative, the left CHP just won the municipal election with ~60% of the votes One last thing, ISPs have moved on to using DPI to block websites so you might wanna suggest something like green-tunnel so they can up their game. reply rrr_oh_man 1 hour agorootparentHave you registered just to say that everything is a-ok in Turkey? Not to be overly suspicious, but it sounds very much like government-sponsored astroturfing. reply heavyset_go 12 hours agoparentprevSet them up with some form of DNS over TLS. reply chpatrick 12 hours agoparentprevThat way the ISP still sees what you're doing though. reply acchow 12 hours agorootparentOnly the DNS lookups. Not the content of what you’re viewing over https. Everything after domain name is encrypted reply slt2021 9 hours agorootparentSNI header from TLS handshake is unencrypted so service providers (and Government's packet inspection engines) absolutely do see what website you are visiting https://en.wikipedia.org/wiki/Server_Name_Indication Server Name Indication payload is not encrypted, thus the hostname of the server the client tries to connect to is visible to a passive eavesdropper. This protocol weakness was exploited by security software for network filtering and monitoring[4][5][6] and governments to implement censorship.[7] Presently, there are multiple technologies attempting to hide Server Name Indication. Also some countries may mandate users to install and trust government's CA certificate which is then used to MITM HTTPS traffic https://en.wikipedia.org/wiki/Kazakhstan_man-in-the-middle_a... reply freedomben 12 hours agorootparentprevThe DNS lookup are often enough. You can try to claim you were shopping for new shoes on pornhub.com, but not many people will buy it reply baobabKoodaa 8 hours agorootparentWell, in some parts of the world the lie may be \"I was watching 100% straight heterosexual porn, no homo\". So a DNS lookup to pornhub.com does not reveal the lie. reply omoikane 9 hours agorootparentprevYou could say you were learning math: https://kotaku.com/on-pornhub-math-teacher-makes-his-mark-te... https://news.ycombinator.com/item?id=28934711 reply akdev1l 8 hours agorootparentYeah. An oppressive government will for sure take your excuse of “I was just learning math on PornHub”. reply toast0 9 hours agorootparentprevI was just shopping for cpu cooling at OnlyFans? reply acchow 10 hours agorootparentprevWatching for the plot… reply nurettin 12 hours agorootparentprevThere is no law against going to pornhub.com, the law is only concerned with dns blocking. reply godelski 10 hours agorootparentSupposing this was 100% accurate, I'm sure that you could understand that going to pornhub.com or definitelynotgayporn.net would arouse suspicion in a government that is banning such behavior and could cause a person to be under higher scrutiny. So even if you're right that the DNS bypass isn't what would technically get them in trouble, I think you can understand the motivation to encrypt your DNS traffic to ensure that you don't get a target painted on your back. And given this, I hope how you can understand that your comment does very little to contribute to the actual conversation and can even potentially mislead others. A lot of communication is implicit (otherwise it'd be incredibly cumbersome) reply nurettin 8 hours agorootparentMisleading and disinformation also comes in the form of FUD, does it not? reply godelski 5 hours agorootparentYou're going to have to clarify what the misleading/disinformation/FUD is. Because I don't see it. reply nurettin 3 hours agorootparentJust read Every Single Suggestion in your reply. > going to pornhub.com or definitelynotgayporn.net would arouse suspicion in a government that is banning such behavior dns blocking doesn't mean adults are banned from visiting porn sites. You start by contradicting your initial concession of assuming this is 100% accurate. > and could cause a person to be under higher scrutiny. So even if you're right that the DNS bypass isn't what would technically get them in trouble, I think you can understand the motivation to encrypt your DNS traffic to ensure that you don't get a target painted on your back. This is a prime example of Fear and uncertainty. > And given this, I hope how you can understand that your comment does very little to contribute to the actual conversation and can even potentially mislead others. A lot of communication is implicit (otherwise it'd be incredibly cumbersome) And this is doubt. Never fails to accompany the other two. reply bawolff 9 hours agorootparentprevI feel like the type of places with heavy handed censorship are also not the type of places you want to go \"well, actually...\" to the government. reply tawai 2 hours agorootparentImagine lecturing people about their own countries, how arrogant can you be? reply nurettin 8 hours agorootparentprevThat approach is way too apprehensive. Those idiots tried to block google docs once. It didn't go well as you might imagine. ERs, research institutes and several government facilities responded with cordial letters about the court's mental state. Then it was unblocked. The reason they get to keep the porn bans is because it isn't enforced upon adults, and it does serve as a thin layer of protection against minors. reply kornhole 11 hours agoparentprevConfiguring 1.1.1.1 is a great option for people outside the US whose countries are unable to pressure Cloudflare for information. A better option for those of us in the US is to run our own DNS resolvers. My DNS resolver runs on a little PFSense box that also has DNS blocking and a ton of other features. reply envy2 9 hours agorootparentLet's be realistic. Porn, the subject of this discussion, is not illegal in the US; this is about age verification laws that apply to the website operators, who then self-block users from particular states. No one is going after Cloudflare for this information as there is no crime committed by the user. Given Cloudflare's logging policy (https://developers.cloudflare.com/1.1.1.1/privacy/public-dns...) and data disclosure practices, the only way US authorities would get anything worthwhile is if they had a specific individual target and a valid legal process. Cloudflare, like most of the big tech companies, is actually very good on this front; they have robust processes and are transparent (https://cf-assets.www.cloudflare.com/slt3lc6tev37/Q1INAiyBub...) about the limited cases in which they do hand over any user data. (I don't work at Cloudflare, but do work on these issues for another big US tech company.) reply danShumway 7 hours agorootparentI don't think this subthread is about the US, but just as a sidenote on the US, Project 2025 directly calls for porn to be made illegal and for attacking Internet providers and companies like Cloudflare that enable its access (https://static.project2025.org/2025_MandateForLeadership_FOR...): > Pornography, manifested today in the omnipresent propagation of transgender ideology and sexualization of children, for instance, is not a political Gordian knot inextricably binding up disparate claims about free speech, property rights, sexual liberation, and child welfare. It has no claim to First Amendment protection. Its purveyors are child predators and misogynistic exploiters of women. Their product is as addictive as any illicit drug and as psychologically destructive as any crime. Pornography should be outlawed. The people who produce and distribute it should be imprisoned. Educators and public librarians who purvey it should be classed as registered sex offenders. And telecommunications and technology firms that facilitate its spread should be shuttered. Even in a Conservative-majority court this kind of ban would likely be ruled unconstitutional very quickly, but we should be clear when talking about this kind of thing where it is that a substantial number of Conservative foundations would like to go (https://www.project2025.org/about/advisory-board/). There is a non-trivial Conservative movement to to ban porn. reply thejazzman 9 hours agorootparentprevThe US is not the subject of this thread... > [a] couple in the South-Eastern very conservative Turkish city of Mersin reply outside1234 9 hours agorootparentprevLet’s be realistic - this is not about age verification but about restricting people from their rights to view a particular type of content because of how one religion feels about it. The age verification is just the smoke screen the Republicans use to take away your freedoms. reply noman-land 12 hours agoparentprevThis story is oddly wholesome. reply samatman 12 hours agoparentprevReminds me of a... peculiarity in Turkey. Men are required to serve a mandatory term in the military, but are exempt if they're homosexual. However, to prevent this being used as a loophole to evade service, the men in question must prove that they're gay. This has resulted in the Turkish military having a very large collection of amateur gay porn. reply archon1410 12 hours agorootparentApparently it doesn't happen anymore. > The system has been undergoing change for the past few years: Lambda Istanbul's lawyer Fırat Söyle stated in 2012 that the rectal examinations, and the photographic evidence of anal intercourse have been dismissed as requirements when they gained worldwide and national media attention. — https://en.m.wikipedia.org/wiki/Pink_certificate But from when it did happen, it sounds... bad. > \"With the picture it was a bit difficult. The face had to be recognizable as well as the penis and the ass. To put all this into a square was a bit complicated. Twice I got cramps because of the position. […] I also got humiliated at the selection, before they announce it [the final diagnosis]. There are about 12 military people, big ones, with uniforms and everything. […] They all looked one by one at the pictures and I had to bring ten pictures, ten different positions there. This is the difficulty of course, but I managed to do it. And at every single picture they looked and compared it with me. I was standing in the middle. And then they asked me questions. For example which positions I like and if I use this position they see regularly. Questions like that.\" — Irlenkauser, Julian. \"Gender Identities and the Turkish Military.\" Thesis. European University Viadrina / Istanbul Bilgi University, 2012. reply superb_dev 12 hours agorootparentprevSo they’re forced to choose between military service or having sex on camera? I’m not sure I’d call that “amateur porn” if it’s not consensual reply Dylan16807 12 hours agorootparentWanting to get out of a pre-existing obligation that every man has, and most don't get out of, should not qualify as lack of consent. reply noman-land 11 hours agorootparentThe obligation itself already lacks consent so anything you feel forced to do to get out of it is similarly non-consentual. reply Dylan16807 11 hours agorootparentIf by \"similarly\" you mean \"equally bad at most\", then I can agree with that. But I think most people would rate non-consensual sex as significantly worse than a non-consensual day job. So if you say the sex on camera is not consensual, you're implying a very different kind of consent violation than is actually happening. reply eastbound 9 hours agorootparent> most people would rate non-consensual sex as significantly worse than a non-consensual day job The argument here is non-consensual. No work is totally consensual, since you need to survive. You accept a life-long inconvenience in exchange for a beautiful house. Can we make sex consensual in exchange for a large enough inconvenience? The response lies in an experiment: Propose a menial job to prostitutes. Even at the same rate, most prefer their occupation. Therefore it’s consensual. A funny quirk about non-consensual work is that the 1930’s ILO convention against slavery and forced work… excluded able-bodied men in age of working (article 11). Which kind of defeats the purpose of the convention. This article was only abolished in 1957. But compulsory military service still exists in probably half of the world countries. reply basil-rash 10 hours agorootparentprevThe Overton window has shifted such that two adults agreeing to engage in sexual activity in front of a camera can be in any way considered “non-consensual”? Wild. Do you think adult actors agreeing to engage in sexual activity in front of a camera, but only because they have chosen to do that for a living when other viable employment alternatives were available, is also “non-consensual”? reply godelski 10 hours agorootparent> in any way considered “non-consensual”? Wild. Consensual means without coercion and being of sound mind. Let's put it this way, do you consider a quid-pro-quo scenario where a boss will promote or give a raise to a female employee if she has sex with him \"consensual?\" Obviously this is arguable in both directions (as we could say that this is a contract between two adults, but then again, prostitution is illegal). What about if we change this to \"Have sex with your boss or be fired?\" The latter case is less ambiguous, but both cases would generally be considered non-consensual in Western legal systems and has been that way for quite some time. This is nothing new... If you understand the above but fail to think \"You *must* enter the military service, for 3 years, and are likely to go to combat and have a high risk of being killed or seriously injured (physically or even more likely mentally), or you can go to jail, OR put your dick in some guy's ass\" I think you can understand how that constitutes significant cohesion. If you are unable to understand this, I am here to inform you that you are overfit and not robust to general problems. reply Dylan16807 9 hours agorootparent> hat about if we change this to \"Have sex with your boss or be fired?\" Definite coercion as stated. But if the boss is already set on firing everyone in the department, but lets one or two people stay on if they have sex with him, I consider that to be a roundabout method of quid-pro-quo prostitution. Plus embezzlement. There's no retaliation if they say no, they go home like everyone else. And the military example is similar, there's no retaliation, most men are doing the service. The offer of an alternate option, one that very few take, is not the problem here. reply godelski 5 hours agorootparent> But if the boss is already set on firing everyone in the department, but lets one or two people stay on if they have sex with him But we'd still call this coercion. The \"dual\" of this is \"have sex with me or be fired.\" Just because not everyone gets to have that opportunity does not mean it is not the same question stated differently. > And the military example is similar, there's no retaliation, most men are doing the service. There is retaliation, it's called jail. Just because others are doing it doesn't make it any less of a coercion. This scales too. reply Dylan16807 3 hours agorootparent> Just because not everyone gets to have that opportunity does not mean it is not the same question stated differently. To me, it matters significantly if refusing is retaliated against or is neutral. If having sex with the boss is the difference between 0 firings and 1 firing, that's retaliation. If it's the difference between 95% fired and 100% fired, that's not retaliation. Also we can say everyone gets the offer if that matters. - The other \"dual\" of this is a version where some employees have already been offered money for sex, but by a completely unrelated party. So if they want they can go take that offer now that they have more free time and only so much severance pay. (Also I am assuming they can get a new job easily enough; there are no destitution-based consent problems in this hypothetical.) Does consent hinge on who is making the offer? > There is retaliation, it's called jail. You misunderstand me. There is no retaliation for not doing the gay sex thing. The default expectation is that you won't do it. Retaliation for refusing to serve in the military is a different matter. And also why I think the mandatory service is where the actual consent issues are. reply valicord 5 hours agorootparentprevAm I understanding your position right? If you have 10 employees and say to 1 of them \"have sex with me or you're fired\" - that's coercion. If you have 10 employees and say to each of them (separately) \"have sex with me or you're fired\" - also coercion. If you have 10 employees and say to 5 \"have sex with me or you're fired\" and to the other 5 just \"you're fired\" - suddenly no longer coercion? reply Dylan16807 2 hours agorootparentNo, that's not it. At its core, it's about whether you're trying to pressure them into sex with you. I should have specifically said everyone gets the offer, to keep it simple. If the expectation is that basically everyone refuses, then it's not very different from walking around town and telling random people you'll hire them if they have sex with you. Which is just prostitution, not coercion. Whether someone feels pressured is a difficult communication problem. It's hard to reduce to a simple thought experiment. reply eastbound 9 hours agorootparentprevAnother aspect is the prevalence of bullying against gays, once pictures are taken. If gays are sometimes killed even in France (Lyon, 2015), it’s worth wondering whether you want photos of yourself at 18, in a country that could evolve both ways for the rest of your life. reply willcipriano 10 hours agorootparentprev\"Hey I made this guy really mad go fight him for me\" \"No he will kill me!\" \"Alternatively you could have sex and let me film it\" Consent. reply Dylan16807 10 hours agorootparentThe problem is in line 1, not line 3. There would be some serious issues with 3 if 1 was a fake threat meant to push you into 3, but it's not. The vast majority go along with 1. reply willcipriano 9 hours agorootparentI see it as rape with a implied third party threat of violence instead of a first party one. You may feel differently if you feel that men's bodies belong to the state to be used in wars. reply Dylan16807 9 hours agorootparentThe violence was already there. It's not a threat to say and honestly mean \"otherwise nothing changes (99% of people pick this option)\". The consent issues are all in the forced service itself, and apply to every man equally, whether or not they take the gay proof exemption. reply godelski 5 hours agorootparentThe violence was not \"already there.\" The violence is generated by the men who are not at risk of going to war. There's an old saying about two people of different nationalities: The difference between you and me is smaller than the difference between us and our respective leaders. Leaders must convince the public that some person in a far away land hates them and is coming to kill them, because otherwise no one gives a fuck. Because frankly, no one is going to come into contact with another. Normal people are not the ones going to invade other countries and claim territory. reply Dylan16807 2 hours agorootparentI meant that the violence is a combination of \"already caused by the leaders\" and \"foregone conclusion\". If you opt not to take the gay sex exemption, your fate is the same as if you had never been asked. reply kilroy123 9 hours agorootparentprevI've never once heard of this, and I've spent years living in Turkey. reply leeoniya 12 hours agorootparentprevthis is basically that family guy episode where Quagmire tries to get out of his marriage to a hooker. reply jgalt212 9 hours agoparentprevisn't 1.1.1.1 of limited utility from this perspective? The ISP can still see the IP address, and I know more than one IP address can map to a single domain, but you're still leaking information that would allow one's adversaries to guess at one's intent. reply not_a_dane 12 hours agoparentprevUK is also censoring porn sites... reply sph 11 hours agorootparentOnly with crappy ISPs like Virgin, which has court-mandated DNS bans for many websites (like ThePirateBay) I use Zen which has no such restriction. Though for safety all my illegal activity is done via VPN because this is still a Five Eyes country. reply PaulDavisThe1st 10 hours agorootparentEE also does this by default (you can turn it off, but you have to contact them) EDIT: or is it Three? I can never remember which company provided the last \"burner\" SIM I got for each trip to the UK. reply sph 4 hours agorootparentIt's Three that asks for credit card numbers before watching the nudes. I actually switched to EE and they have no such restriction. reply gambiting 11 hours agorootparentprevFriendly reminder that all ISPs in the UK are required to store your entire browsing history for a full year, and 17 government agencies, including the Department of Agriculture, can access it without a warrant. Snoopers Act has been passed and lives on for years now, and I'm yet to meet a person who is even slightly bothered by it. reply chgs 11 hours agorootparentExcept that’s not right https://decoded.legal/blog/2021/06/must-all-uk-internet-acce... reply Footkerchief 10 hours agorootparent> Don’t all ISPs have retention notices? Probably not (for the reasons above), but this is not public information, and ISPs and others subject to retention notices face statutory prohibitions on disclosure. > If all Internet access providers were subject to retention notices, wouldn’t it be easier to say that? There would be no need to dance around issues of secrecy, or explain why the list of notice recipients cannot be published. The fact that the Home Office chooses to take this approach undermines the claim that all providers have notices. This source appears to be engaging in wishful thinking. reply 77pt77 11 hours agorootparentprevAt this point Turkey is probably more free than the UK. reply levidos 11 hours agorootparentCan you elaborate please? The UK isn't blocking gay porn sites for example. reply imwillofficial 10 hours agorootparentI think freedom of speech is an obvious example. reply rsynnott 10 hours agorootparent… You think freedom of speech is worse in the UK than in Erdogan’s Turkey? Just what do you think is going on in the UK? reply fullspectrumdev 6 hours agorootparentI’ve noticed Americans get some… extremely weird and biased reporting about free speech in the UK. Extreme outlier cases like that Count Dankula guy get reported as the norm. reply fullspectrumdev 6 hours agorootparentprevI think you are smoking crack if you think Turkey has more freedom of speech than the UK. reply BiteCode_dev 10 hours agoparentprevHopefully their country doesn't outlow watching porn itself, because in that case they are just accumulating evidences against them. reply akdev1l 8 hours agoparentprevI wouldn’t be breaking the law under a repressive government without at least a VPN in the middle. DNS queries still leak the domain unless the client is using DoH. reply bradly 12 hours agoprevAlso it is extremely common in middle and high school to use VPNs to get around school network restrictions. reply lelandfe 12 hours agoparentBack in my day, it was about obscure web proxies. I remember one that was called something like abelincolnfacts.com, which looked like a blog about Lincoln. But clicking an icon of his face in the top right revealed a web proxy interface to browse the web unfettered. reply Shawnj2 7 hours agorootparentA super common one from my time is to use google translate’s web translation feature as a proxy since google translate is almost never blocked reply doubled112 7 hours agorootparentprevBess can't go there. reply jwells89 12 hours agoparentprevThough, circumvention of these things isn’t always even that involved, depending on the competency of IT. At the religious private high school I attended, all it took to make a student a “hacker” was popping open Internet Options on the computer lab’s XP boxes and disabling the proxy they were using. Had to sneakily do this sometimes not to view blocked stuff, but to just have a working internet connection to complete assignments with because the proxy service was provided by some terribly run local company that must’ve been three Staples special Celeron Compaq Presarios and a Linksys router in a shack somewhere, because they were constantly having outages. reply miki123211 12 hours agoparentprevThat must be a US thing, over here in Europe, most people just use hotspots (or cellular data) instead. That was a thing even back in my day, when LTE availability was not a given and cellular packages were much less generous. reply cayde 12 hours agorootparentTethering is idiot expensive in USA. Every provider will DPI and block it or throttle it. It's awful. reply alwayslikethis 9 hours agorootparentHuh. It's interesting that tethering restrictions are implemented via your device, i.e. the device you paid for working against you. Most Android custom ROMs bypass this, and tethering is always enabled regardless of whether your telecom wants you or not. reply vel0city 11 hours agorootparentprevMy $15/mo plan in the US can pull hundreds of megabits at 20ms latency to most sites tethered. Tethering is baked into most phone plans and from my experiences not throttled any more than the regular phone data. Most plans have it, even the cheap prepaid options. reply hodgesrm 9 hours agorootparentprevTethering is free on my US T-Mobile account. I use it instead of crappy hotel Internet connections. Performance very much depends on your location, as the network is often shaky outside of cities. reply ssl-3 8 hours agorootparentprevI've had a very cheap provider in the US for years that has unlimited tethering/hotspot, and unlimited everything else. I've \"only\" ever used it for as much as 1 or 2TB in a month, though -- which is a ton for most individuals. (And only occasionally. It Isn't my primary connection, but sometimes it is a very useful connection.) (They do attempt to throttle it, but that's been solvable for me with just a TTL hack on a client/router or a PDANet fix on my particular devices.) reply beretguy 7 hours agorootparentDid you mean 1 or 2 GB? reply ssl-3 7 hours agorootparentHah, no. I do mean terabytes. There's ways to get this done (at least here in the US) for those who are sufficiently motivated. reply cayde 4 hours agorootparentYes, but its quite unfortunate in that you cannot do what you want with the data you pay for. Even in plan which not unlimited when I buy in visiting USA, I cannot even use the x GB I purchase without restriction. I should not need to resorting TTL „unsupported hacks“ which also say company will ban me for. reply ssl-3 4 hours agorootparentEh? What I pay for with my cheap-shit cellular connectivity is to use as much data as I wish, either on the phone itself and/or for exactly 1 hotspot-connected device, at speeds of up to 5Mbps for that singular hotspot-connected device. I can stay within the operating parameters of the service I signed up for by using up to 5Mbps with hotspot (along with literally-unlimited[1] bandwidth on the phone itself) all of the time, 24/7. I absolutely do get what I pay for, and I absolutely do what I want with it, and let me tell you: I am not paying very much. I have nothing to complain about here. But I am not alone when I occasionally extend that to more than one device (using a router), and/or using TTL mangling to negate the 5Mbps limit. I've never witnessed anyone being banned for this (and I've paid close-enough attention to the noises people make that if banning were a common thing, I'd have seen the screaming at least one time by now). Indeed, some people (in some areas, with some devices) don't seem to have either 5Mbps nor 1-device limits on their phone's built-in hotspot -- without any hacks at all. And while I have used it (and hacks to improve it) rather extensively at various times and for various reasons, I don't typically use it as a primary Internet connection at home: While my (also inexpensive) DOCSIS connectivity at home is often slower than what my phone can provide on a given day, I prefer the stability and consistent latency of DOCSIS compared to using my phone's hotspot, and I like having my home LAN always-connected regardless of whether I am at home or not, and also I enjoy having my phone's battery last for days instead of hours between charges (wifi hotspot is a huge battery suck on a phone). In terms of visiting the States: I don't know what to tell you. I've lived here my whole life so I don't ever travel to the US, but this service is not really intended for visitors. It can probably be made to happen with the help of a friend who does live here, but I don't think anyone but you was ever trying to address any issues of visiting the US here in these threads. Is there a particular aspect about that concept that you'd like to more-thoroughly address? I can answer questions. [1]: Everything has limits, and bandwidth cannot ever be infinite, so \"unlimited\" is with a grain of salt. reply ezfe 10 hours agorootparentprevMy $35/mo Visible plan offers throttled but unlimited hotspot for one device. reply ranger_danger 11 hours agorootparentprevvery easy to bypass simply by increasing TTL by 1 reply jowea 11 hours agorootparentAnd what's the very easy way to do that? reply ssl-3 8 hours agorootparentUnless your phone is rooted, TTL mangling happens on the hotspot-using client devices. Specific details depend on what that client device uses for an operating system, and this makes it impossible to neatly summarize. But it can be done fairly easily with things like OpenWRT or Mikrotik's RouterOS, or a regular stand-alone Linux box, and IIRC it's a simple one-liner on a Windows machine. Because it can't be easily summarized, Google is your friend here. Generally, you want the TTL of all packets leaving the client (router, tablet, laptop, or whatever) to be set to 65 -- which is one more than the Android default of 64. That's all the information you need to know to Google up instructions that work with your particular devices. (I may or may not keep a Mikrotik-based hotspot-abuser in my work truck as a problem solver.) reply jowea 8 hours agorootparentOk that does seems more viable than mangling it on the average Android phone. Unless of course you're tethering another smartphone by sharing the wifi hotspot, which in my personal experience is a common use case. reply ssl-3 7 hours agorootparentYeah, rootless smartphone-to-smartphone (or tablet) is a harder problem to solve without an intermediary device. But! That intermediary device could be something like a Raspberry Pi Zero. It isn't \"low power\" by the strictest definitions, but it is also not particularly expensive power-wise -- it can be powered with USB OTG from a smartphone from the past decade or so. And it is very small, which also counts. It can obviously run a real Linux distro (or just parts of one), but can also run OpenWRT -- which by nature tends to tolerate intermittent power very well. It's theoretical, but (again in theory): A Zero W running OpenWRT might be a relatively simple path for a portable hotspot abuser. First, dump OpenWRT onto an SD card, plug it in an and get in there with a browser -- however that is done. Second: Create an interface or two for getting hotspot data into it: Maybe one of them via wifi, and another via USB tethering from the connected phone. Third: Test. At least the Pi itself should have Internet connectivity by this point. Fourth: Set it up as a wifi access point (I think that chipset can do both at once in OpenWRT), and get NAT going. Fifth: Utter the well-documented incantations for mangling TTL on all of the hotspot interfaces. Sixth: Wrap it in nice 3M Super 33+ electrical tape for posterity and a minimum of protection for those tiny SMD parts. Seventh: ??? Eighth: Profit!!! Or, better: Push the resulting SD card image to the usual places, with correct attribution and license compliance, so that others can benefit more-easily. Seems doable. To use, just power it on/plug it in, and turn tethering/hotspot on with the donor phone. And then connect other phones to the WiFi AP provided by the Pi. It'll eat phone batteries pretty quick, but it might last long enough to get someone else out of a jam. (It'll also work well on a portable power bank, and those are also cheap.) reply phillc73 12 hours agorootparentprevMaybe it's also a generational thing. In the 90s, I remember running my own squid proxy on a remote shared box somewhere, in order to circumvent \"things.\" Granted, today, if it was just getting around general network restrictions via blacklist, then tethering is very convenient. reply chgs 11 hours agorootparentSsh -D is quite handy reply ndriscoll 10 hours agorootparentprevDepending on carrier, the parent can turn off data on the child's line and only allow it to be used for voice/text. reply willcipriano 11 hours agoparentprev\"Start using cgi proxy\" reply donatj 12 hours agoprevTailscale exit node on a cheap vps and you're golden. Well as long as the stuff you're doing is \"legal\". It's still traceable back to you. reply LilBytes 8 hours agoparentWhich VPS provider did you go with? I tried AlphaVPS and another and I had no luck with the traffic routing out of the VPS. Went with a droplet in DigitalOcean in the end that's managed through Portainer. Works a treat and equivalently cheap. reply fragmede 12 hours agoparentprevHow well do subpoenas work across international borders, especially when the charge isn't illegal in the recieving jurisdiction? (asking for a friend, obvs) reply zamalek 12 hours agorootparentThings like torrenting will get you quickly shut down by the provider. My friend found this out while considering setting up their own seedbox. reply simfree 11 hours agorootparentSounds like your friend was VPNing to the wrong jurisdiction. PulsedMedia (Finland), IHostArt (Romania) and many other seedbox server providers exist and will not shut you down. reply reaperman 9 hours agorootparentHighly recommend Whatbox.ca - i run a “pirate netflix” for all my family and friends off one cheap VPS. It also provides me with a more censorship-resistant VPN than most usual VPN providers. Many VPN services were blocked in GCC countries but not my private whatbox server VPN. reply neurostimulant 9 hours agorootparentpreviirc ihostart actually host their servers in their basement, which is bad for reliability but perfectly fine for this use case. The operator also claimed to be an 18 years old, not sure if it's true but if it does I applaud their entrepreneurship spirit at such a young age. reply miohtama 9 hours agorootparentprevNobody cares if it’s not terrorism or national security reply ranger_danger 11 hours agorootparentprevvery well as long as the country has an extradition treaty with your home country (if US, almost all you'd ever travel to does) reply throwaway918274 11 hours agoparentprev1984.hosting payable with Monero reply k8svet 11 hours agorootparentDo you know any similar hosts that attempt to provide shielded VMs? reply reaperman 9 hours agorootparentWould Mullvad count? You can even pay anonymously by mailing cash. reply k8svet 9 hours agorootparentHm, I don't know if Mullvad uses shielded VMs for their infra. I guess it would be neat, but I'm more talking about privacy-oriented generic-hosting providers that provided shielded VMs. Maybe it's just where my head is at, but I look at something like the GP's post and go \"cool!\" and then realize that such a property would be a prime example of a honeypot. Do I think there are individuals interested in offering low-margin \"privacy\" VPS services that accept crypto? Yes. But also, I suspect there's plenty of intelligence outfits that would spin up such a project as a fun side project to spy on guest VMs. :| Unbounded money goes a long way. And while I like to cosplay paranoia, I just have to assume that clients of such services are seeking anonymity for Reasons. Shielded VMs + monero payments would at least move the bar up to \"you need to be a target worth leaning on AMD/Intel for shielded-vm compromise\". Granted, I think you could also create infra architecture that allows you to treat these cannon-fodder, but I digress. reply technion 12 hours agoprevIf vpns become positioned primarily about accessing porn, well I look forward to the day people acknowledge tls exists and we stop seeing misleading advertisements with the various claims everyone on a network can see the contents of your online banking. reply andrewflnr 11 hours agoparentI've already seen a shift in the ads from \"hide your banking details from hackers\" to more \"hide your country from Netflix\". I can't prove causation, but it did seem to date from when Tom Scott did his \"Honest VPN ad\" video. reply theshackleford 8 hours agorootparent\"hide your country from Netflix\" This is the advertising I have seen for what has to be close to a decade at this point. reply andrewflnr 6 hours agorootparentI mean, it's always been there, and the \"security\" claims are still out there, but the focus has shifted. In my experience. reply jimcsharp 13 hours agoprevSurely it's overkill to pay for a whole month of constant vpn use. You could cook something up that provides access for a short session aaand we've circled back to dialers. reply zamadatix 12 hours agoparentIt'sexperts told PopSci platforms also oppose the laws because they don’t want to be responsible for collecting and maintaining torrents of sensitive users’ data that could pose a ripe target for cybercriminals. Good thing these laws specifically ban them from storing that information that they don't want to store. The implication here is so strange. Kids generally don't have a way to buy VPN services (I suppose they could mail cash to mullvad), so mission accomplished? > Some of the anti-porn laws, like the one enacted in Utah, already possess language explicitly prohibiting online platforms from letting minors “change or bypass restrictions on access.” That's a social media law, not a porn law, and it's about parental controls that the parent set up on an account that's been marked as a child account (so only the parent account can modify controls on the child account: duh!). Absolutely garbage article. Didn't even link to the actual laws. And people complain that no one will pay for \"journalism\" like this. reply goalieca 12 hours agoparentWell they can learn to use tor but most likely they’ll get sucked into shady free VPN services and suffer worse. reply tennisflyi 12 hours agorootparentNo way you’re streaming anything via Tor reply InvertedRhodium 12 hours agorootparentI used to leave porn downloading overnight over 56k in the mid 90s. They’ll just have to get a little more organised about it. reply datadrivenangel 8 hours agorootparentprevIt's not uncommon to be able to get a few MBps via Tor. The latency tends to be crap, but once everything is set up it can go pretty fast. reply danShumway 10 hours agoparentprev> Good thing these laws specifically ban them from storing that information that they don't want to store. Well... companies. They don't ban government agencies from retaining that data if they get access to it. But ignore that and assume for a second that they do. The problem is that the laws kind of contradict themselves: \"don't collect information that compromises privacy\" and also \"do this in a way that basically could only work if you collect information that compromises privacy.\" Most of the bills I've read ban companies from saving this data, but require them to collect it presumably in some way that's auditable? They also provide no real mechanisms for guaranteeing that data won't be stored or will be transmitted securely between services. In Louisiana's bill (https://legis.la.gov/legis/ViewDocument.aspx?d=1289498) there is no penalty for retaining data other than that users can sue for \"damages\". But historically, proving damages from retained data tends to be difficult to do. Of course the laws do not clarify how age verification is supposed to work under these restrictions, just that documents will be verified somehow: Texas's \"example\" of age verification explicitly refers to digital identification as information \"stored on a digital network\" (https://capitol.texas.gov/tlodocs/88R/billtext/pdf/HB01181F....). But sure, also make sure you don't store anything! /s Presumably this all means that the information should be stored and transmitted until identification is done and then immediately deleted leaving no record of that identification other than that it happened (which hopefully will be sufficient evidence if the government ever accuses you of using a faulty verification method). But eventual deletion doesn't mean the information isn't still getting temporarily stored or that it's not passing between multiple hands, any of which could have rogue employees or leaks, or which could be storing \"anonymized\" data that turns out to later-on be identifiable. ---- One might argue that since the majority of these laws are proposed and backed by anti-porn organizations, the contradictions might kind of be advantageous to the goals of the backers -- if the law is functionally impossible to comply with and companies are forced to leave the state, well... that's exactly what these groups want anyway. Texas's AG is up-front about being pleased that porn companies are blocking the state. And despite the regular claim that this isn't about restricting porn generally, the vast majority of these bills have ties to religiously conservative groups that have public positions that porn should be banned for everyone. But that's all besides the point: the point is it's just outright false to say that these laws don't require at least the transmission and collection of this data -- they just tack on \"don't store it for too long, delete it afterwards.\" But that doesn't really mean anything: regular identity transmission over the web carries security and privacy risks even if companies could be trusted to reliably delete this data -- which in many cases, they can't. Advocates of these bills ignore that security researchers have an issue with collection and transmission of sensitive data in addition to storage, and so advocates point to narrow, non-specific language about long-term retention as if that solves all of the issues. It doesn't. And to the extent that these laws include any real teeth like actual fines for data leakage, they don't really explain how companies can safely avoid those fines while still proving that their users aren't minors. It's a no-win situation. Note as well, I'm leaving off the accusation I've seen online that this identification would need to be provided per-login/access, because I don't see any language in the bills that would suggest that to me. But of course if that were true, there would be obvious security risks from users providing that information repeatedly as part of regular access. ---- > That's a social media law, not a porn law The law in question explicitly exempts art sharing sites unless they allow specifically porn (https://le.utah.gov/xcode/Title13/Chapter63/C13-63_202305032...): > (H) a professional creative network for showcasing and discovering artistic content, if the content is required to be non-pornographic; Notably, general content harmful to minors like gore, hate-speech, etc is included in that exemption. So your creative gallery site doesn't count as social media and isn't subject to these child-restrictions if you allow nazi emblems or violent imagery, it's only a problem if you allow porn. I will concede on this that conflating the exact language of \"change or bypass restrictions on access\" is a straightforward misreading of the bill, bad on the article for that. But I think it's being a little coy to act like there's no overlap between Chapter 63 and SB 287. Chapter 63 clearly views pornographic content differently than it views other content that is similarly harmful to minors. SB 287 includes language that seems (to me) to explicitly protect VPN and network providers from liability, but it is not clear whether that kind of language will continue to appear in future bills: the majority of these bills so far have been largely copy-paste templates of each other, and in other Internet restriction debates states have expressed interest in going after actors that they deem to be \"enabling\" illegal actions. Notably, the copy-paste template that most states have been using doesn't protect sites that allow VPN access, they just protect the VPNs themselves. It's not clear to me from the text of the bills that states wouldn't view a porn site refusing to block all VPN connections as a violation of the law if they were ever interested in pushing enforcement past state lines (which again, in other Internet speech debates states have expressed interest in pushing enforcement across state lines). The article's example of VPN restrictions is misleading and misrepresents the bills it's talking about -- but the general concern that VPN restrictions might come in the future (most likely through targeting companies that do not block VPNs from accessing their services) is a real concern, just poorly presented in this article. reply ndriscoll 8 hours agorootparentWhy would they need to be auditable? That's not in any of the laws I've read, and in fact as you note, the law makes that impossible. If the law contradicts a requirement that you made up, and does not itself contain that requirement, then why would you presume that it has that requirement? There's literally no reason to have e.g. a knowledge based auth or signed id request hit disk. It doesn't need to be saved for a \"short time\". It doesn't need to be saved on permanent storage at all. \"Let's assume for a moment that the law says the opposite of what it actually says\". But it doesn't. It's easy for the government to investigate whether you check IDs: open the site and see if you request ID information. Present fake info and see if you accept it. Just like they do in person. reply danShumway 8 hours agorootparent> If the law contradicts a requirement that you made up, and does not itself contain that requirement, then why would you presume that it has that requirement? First off, always good to be clear that there are multiple laws here, even if many of them are templates of each other; there's not \"the law\". Secondly, this is hiding behind ambiguity in many of these laws' language; it's easy to claim that a law doesn't specifically require that companies retain information about their efforts, but I guarantee you in any court case about this, requests for that information would come up. It is painfully naive to assume that any company would feel safe implementing a legally required system that does not provide them with any evidence to prove that their system works or has worked in the past. The ambiguity about what many of these bills mean when they call for a \"reasonable method\" of identity verification is exactly the kind of contradicting language that I'm talking about above. \"We didn't ask you to do X, we just put you in a situation where not doing X would be extremely dangerous.\" I would argue that a State going to a company and saying, \"do something 'reasonable'\" with no legal guarantee or precedent about what will and won't be reasonable, and then additionally adding restrictions that make it practically impossible for any existing ID verification system online that I'm aware of to fit that requirement -- I would argue that is tantamount to an attempt to ban porn. It's a system that can't really be safely complied with. Of course companies being able to provide documentation and evidence of their prior verifications is a practical requirement for them operating in that kind of environment. > There's literally no reason to have e.g. a knowledge based auth or signed id request hit disk. It doesn't need to be saved for a \"short time\". It doesn't need to be saved on permanent storage at all. I don't see any indication in the laws I've read that this would be sufficient; where are you getting this idea from? In fact (I'll remind you), Texas's law explicitly refers to digital identification as something that gets stored and accessed as proof of identity. The bill's own language does not support the idea that identification would be completely transient and instantaneous. So it is completely reasonable for critics to question these requirements given that nothing in the law would prevent the government from making a case that completely transient identification is insufficient. And even if it was sufficient, from a purely technical perspective it is not clear to me how this magically transient identification would work. Information transmitted between parties gets stored, that's how this stuff works -- what ID verification system are you imagining that can happen instantaneously without referencing any stored information and without any information leaving RAM? I'm not aware of one. > \"Let's assume for a moment that the law says the opposite of what it actually says\". But it doesn't. What? Every single law I referenced requires the transmission of this data and explicitly suggests sharing it with 3rd-party verification services. That's not me reading into the laws, it's just fact. > It's easy for the government to investigate whether you check IDs: open the site and see if you request ID information. Present fake info and see if you accept it. What system for instant ID verification that does not rely on storing or accessing stored, indexed information about an identity works like this? How do you propose that sites detect fake info without referencing that info against stored identifying information? Because advocates for these laws keep on saying this is easy and then describing systems that as far as I can tell, do not exist. ----- I'm accommodating a little bit of a rabbit hole above, but I do need to loop back around to the more relevant point: > There's literally no reason to have e.g. a knowledge based auth or signed id request hit disk. Regular, consistent transmission and collection of ID information online presents security risks that are unique to remote identity verification and that are not present in physical spaces like shops and stores. Even if there existed a system that allowed this verification to happen entirely in RAM, that would not address the security points that professionals have raised. And even that magical system would necessarily require storing that information in more places -- on user phones and browsers in an easily transmissible format. It would necessarily require users to become more comfortable sharing information online that they should not be comfortable sharing online. I'll repeat the same point I made in my previous comment: > Advocates of these bills ignore that security researchers have an issue with collection and transmission of sensitive data in addition to storage, and so advocates point to narrow, non-specific language about long-term retention as if that solves all of the issues. It doesn't. Pointing to retention as the only security risk in these laws misrepresents the concerns of security professionals. Ambiguous language that is inadequately explained or elaborated on within bills and that (theoretically) addresses one part of security researchers' concerns is not sufficient to dismiss their overall concerns. Regular uploading and transmitting of ID information to 3rd-parties over the Internet is more dangerous than showing your ID in a liquor store; transmission of that data necessarily requires copying that data, putting it in the hands of multiple parties, verifying their trustworthiness, and interacting with extremely complicated systems that have larger attack surfaces than a cashier looking at your face. It's just not accurate to act like they're the same. reply ndriscoll 6 hours agorootparentSure, there are multiple laws. The ones I've read all seem similar enough to me on the points people bring up. Identity verification is not that mysterious. If these sites are afraid to do it themselves, there are turnkey vendors for that, which e.g. banks or docusign use. All the laws I've read say sites can use third party verification services. The Utah law specifically mentions > verification through an independent, third-party age verification service that compares the personal information entered by the individual who is seeking access to the material that is available from a commercially available database, or aggregate of databases, that is regularly used by government agencies and businesses for the purpose of age and identity verification; i.e. KBA, which is already a thing. These companies already know facts about everyone. You claim you're person X. They ask you to tell them a fact they already know. They check your answer against their database. They don't need to store anything you tell them. I'm sure they can tweak their service to only tell the requesting site you are over 18 and not keep any records. These services know how to deal with a highly regulated environment. The Utah law also allows the user to present a \"data file from a state agency or an authorized agent of a state agency that contains all of the data elements visible on the face and back of a license or identification card and displays the current status of the license or identification card.\" No need for the site to save anything. Just check the signature and age. I don't see what makes porn sites unique vs. any other e-commerce business that requires customers to identify themselves wrt. security. Typically those actually store and sell your info. Also many grocery stores do scan IDs when you hand them to the cashier. Who knows what they're doing with that info. Wouldn't surprise me if they retain and sell it. reply danShumway 5 hours agorootparent> Sure, there are multiple laws. The ones I've read all seem similar enough to me on the points people bring up. The laws are template laws, but do occasionally differ in important ways. You've mentioned before that Texas includes a financial penalty for retaining user IDs beyond verification. You didn't mention that Texas is pretty much the only state that does this, and the majority of the other bills only allow for suing for harm and attorney's fees. Harm can be difficult to prove for information retention, and these provisions rely on individual action for enforcement. You mention later in this comment that Utah includes provisions for ID-only verification. You don't mention that Utah is (as far as I can tell) one of the only states that offers this kind of detail, most merely mentioning that \"government identification\" could be used for verification. These things matter. When we treat these bills as a single unit, we run the risk of building a composite bill that theoretically addresses every concern, even though that composite bill doesn't actually exist anywhere. ---- > Identity verification is not that mysterious. Agreed. Do you believe that the security professionals who are intimately familiar with identity verification services and who know how the current services work are just... lying? Like, what do you think is happening here? This is not something complicated where there are a bunch of debates about how ID verification can work, we know how the ID verification services today work. And security professionals are saying there's a security risk. Does the Texas AG know something that they don't? Is there some secret new ID verification system that only lawmakers know about? Like you say, this isn't that mysterious, ID verification online exposes users to privacy and security risks. It's straightforward, this is a known risk. The fact is, there are no identity verification services I'm aware of that I think are secure enough enough to use for this level of transaction -- and every 3rd-party ID service I'm aware of works by retaining and accessing stored information about users. The people talking about the security risks know how existing identity verification services work. They're not that complicated. They work by collecting and transmitting and cross-referencing personally identifying data, and that process is vulnerable to attack and data misuse. ---- > i.e. KBA, which is already a thing. These companies already know facts about everyone. You claim you're person X. They ask you to tell them a fact they already know. They check your answer against their database. They don't need to store anything you tell them. Okay, are you listening to yourself? > They check your answer against their database. So personally identifying information is collected and stored. And that information is linked to requests to access potentially compromising or embarrassing material on a level of granularity where those requests, if intercepted, can be used to link personal identities back to those requests. By your own admission. I don't know, you're agreeing with me and then saying \"see, that means that data doesn't have to be stored.\" No, you just described data getting stored and held by a 3rd-party (notably, a set of 3rd-parties that have had historically awful security and have regularly been irresponsible with those databases) and then cross-referenced with individual access requests in a way that would necessarily require personally identifying to these data brokers which individuals were interacting with which companies. Sure, those services don't need to store your newly uploaded ID -- they already have it! But what comfort is that? They still have the ID either way. You are describing a system that can only exist by hoovering up and retaining huge amounts of data on individuals, and you're advocating that this system should be expanded. And while we're on this subject, none of the laws I've read ban retaining records of this access or selling information about which individuals' identities are verified, even though that could be compromising or personal information. More PII and data is created during this process than just the ID you transmit, and I don't think a single law that I've read addresses that fact. But sure, the data broker that already has your ID won't store the image you sent them. That'll be a huge comfort to Texas users when those sites get hacked and leak access information about which users had their IDs verified for which services. What you're describing is not a privacy-respecting system. ---- > The Utah law also allows the user to present a \"data file from a state agency or an authorized agent of a state agency that contains all of the data elements visible on the face and back of a license or identification card and displays the current status of the license or identification card.\" I avoided pushing this point too hard before, but reminder that there is no requirement in any of the laws I've read for state agents or authorized agents of the state to delete records of that request or to avoid linking those requests to individual services. The laws as written do not block government agencies from using this information to build detailed records of who accesses which services. > No need for the site to save anything. Just check the signature and age. This would not pass a check for fake IDs. Nor would it prevent shared IDs. The laws I've read provide no guarantee that a system that was trivially bypassed would be sufficient to ward off State action. Again with the ambiguity about what \"reasonable\" means, which is a major problem in these bills. \"Don't violate privacy, but it has to work.\" Well, if all you're doing is OCR on a license and you're not cross-referencing that data or storing information about attempts, that is not a system that is hard to bypass. Also as I mentioned above, there isn't just one law. Other laws do not go into this level of detail about what kinds of IDs are accepted or how they could be verified. Great that Utah does (although Utah's example is not sufficient to address concerns) -- that just leaves all of the other bills. > I don't see what makes porn sites unique vs. any other e-commerce business that requires customers to identify themselves wrt. security. Multiple things: A) not all porn sites are e-commerce businesses, and not all platforms affected by these bills are porn sites. These bills are not typically restricted to commercial transactions -- merely accessing commercial sites requires verification, even without a business relationship. B) e-commerce businesses with traditional verification requirements typically do not allow for anonymous usage in the first place. Many of them have extensive \"know your customer\" rules and are not concerned with protecting the privacy of their users -- quite the opposite, many of them are required to retain information about their users. C) Security-wise they're not that different, and the criticism of these bills directly extends from knowledge about the security risks and bad practices of many of those e-commerce sites. Whether or not you understand the security implications, I promise you the organizations and security experts that are pushing back on these bills already understand that Flowroute exists. Note that the theoretical instant, private identification that you seem to be proposing sites will implement doesn't exist for the companies that are relying on this verification today. Once again, I'm left pointing out that you're describing a happy-path scenario that isn't the case for any online identification system I can find. As far as I can tell, these services all store data about their users' individual identities. ---- > Also many grocery stores do scan IDs when you hand them to the cashier. Who knows what they're doing with that info. Wouldn't surprise me if they retain and sell it. Shouldn't you check up on that before advocating that Internet ID verification is fine because it's just like local verification? Me personally, before I compared digital ID verification to local ID verification, I might make sure that local verification isn't retaining and selling all of your data, because otherwise the comparison would look awful. Have you checked to see whether security professionals have also raised alarms about local storage of ID information? Because... they have, for the exact same reasons :) Local ID verification ideally should not involve scanning an ID, and the fact that it sometimes does anyway is worrisome. It doesn't bode well for expanded digital ID verification. If your point is \"local verification doesn't require sending information to multiple parties across the Internet and yet companies still do it anyway, and we still don't know what's happening to your data in that scenario\" then... I mean, you have to understand that's not something that is likely to make anybody feel more charitable to your argument, right? That's not something that makes online ID verification seem like a good idea. ---- Once again, I'll repeat: - Texas's own language refers to these systems as storing user information. - There are no ID verification systems that I'm aware of for online services that work without maintaining and storing information about users. - Addressing long-term retention of submitted information is not sufficient to address the privacy and security concerns that researchers have brought up. - None of the bills I've read are clear that an unverifiable zero-retention policy would be sufficient to avoid liability, this seems to be something you're just reading into the text as an assumption of good will. What you're suggesting above about retention practices and the ability of ID verification services to do this without storing customer data isn't true -- but even if it was true (which it's not) it changes nothing. Regular transmission of this kind of information is dangerous, users should not be trained to submit this kind of information casually, especially not to sites that they don't have business relationships with. The transmission and collection of this information exposes users to risks to both privacy and security. reply sydbarrett74 12 hours agoprevAnyone have experience with Aura's VPN feature? reply ilrwbwrkhv 12 hours agoprevGambling should be banned. Not porn. reply karaterobot 11 hours agoparentI think gambling is a much more destructive force in the world than porn, but I don't think either should be banned. Don't look at it like we have to ban a certain number of things. reply 77pt77 11 hours agoparentprevWhy? No one forces anyone to gamble. reply dijit 11 hours agorootparentEh, Gambling disproportionately impacts poor people. It's as close to Sugar, Nicotine or Alcohol in terms of addiction as you can get without being chemical. Same reason we ban certain ads targeting kids. reply ssl-3 8 hours agorootparentI agree: We should make it illegal for children to gamble. reply game_the0ry 10 hours agoprevWhen it comes to internet vs government censorship, internet wins. Every time. A lesson our baby boomer regulators will never, ever learn. reply apantel 9 hours agoprevWhack-it-a-mole. reply zeroCalories 12 hours agoprevImagine Texas bans private VPNs next. Haha. reply paxys 11 hours agoparentPassing a law and actually enforcing a ban are very different things. In the case of porn the big corporate sites have no choice but to comply. VPN providers don't really care. Unless the state of Texas can manage to find and block every IP address of every VPN server worldwide, people are going to get through. reply btbuildem 12 hours agoparentprevThat's not even haha, that's like 6/10 likely. reply skissane 11 hours agorootparentI doubt Texas will. How would they enforce it? And good chance the federal courts will rule it is pre-empted by federal law (the FCC). For historical (and arguably even political) reasons, federal courts give the states a bit more leeway when to adult content. But regulating general purpose content-neutral VPNs would really be stepping directly into the FCC’s domain, in a way which would directly impact interstate commerce, and I doubt the courts would let them do that reply nunez 8 hours agorootparentGreat Firewall of Texas. I could legit see them investing insane dollars on this project. I don't think this is federally illegal either reply skissane 7 hours agorootparent> I don't think this is federally illegal either If the FCC wanted to overrule it, they could, and the Courts would likely be on the FCC's side. See https://crsreports.congress.gov/product/pdf/R/R46736 But I doubt it would ever get that far. It would be a huge burden on every large corporation operating in Texas, if interstate traffic had to go through some legally mandatory firewall or content filter. The Texas state legislature is generally pro-business, and if almost every major corporation in Texas would be lobbying them not to do something, I doubt they'll do it. And, in the very unlikely event the legislature ignored that lobbying and did it anyway, and the Governor didn't veto it – then those corporations would likely go straight to petitioning the FCC, and I think it is likely they'd succeed in convincing the FCC in overruling it. reply owenmarshall 7 hours agorootparentprev> The Congress shall have power to regulate Commerce with foreign Nations, and among the several States, and with the Indian Tribes. reply tamaharbor 10 hours agoprevI am overnighting in North Carolina this evening. Can’t watch porn without providing a drivers license or passport. (But VPN works good.) reply Nifty3929 12 hours agoprevThe major problem here is that our government is turning us all into criminals. Using a VPN doesn't make it legal for you to consume anonymously, even if doing so makes it harder to enforce, at least for now. Soon VPN use will be illegal in some states. Once again, hard to enforce. But these are things which, if for some reason the gov't decides to target you (in which case they can/will spend the effort to discover), can be used against you. This is not a technology problem, it's a legislative one. It's another example of some of us are using the power of the government to constrain other people's behavior. reply paxys 11 hours agoparent\"Our government\" isn't doing it, some very specific ones are. And the people they represent are 100% behind all such laws. This is the democratic process working exactly as intended. reply samus 10 hours agorootparentConstitutions are supposed to prevent laws like this. Because it is all too easy to make people forget the principles their democratic system is based on and without which it will degenerate into a tyranny of the majority. reply worik 12 hours agoparentprevYes That is the problem with vice laws Tough times reply zarathustreal 12 hours agoparentprevIsn’t “constrain(ing) other people’s behavior” kinda the point of law? If you’re trying to make a point you’re gonna have to be more specific. Preventing rape, theft, and murder are all instances of “behavioral constraint”, without any qualifiers you’re artificially inflating the magnitude of the situation reply dijit 11 hours agorootparentThe issue is that for a large portion of the population, porn consumption is not considered immoral or even if it is immoral; not an offence worthy of calling someone a criminal over. We all know that laws are not a good indicator of morality, but society is better when we try to align laws over time with our moral code. reply ndriscoll 11 hours agorootparentThe laws aren't against porn consumption or even anonymous porn consumption. They're about distributing porn to children, which most people are in alignment that we don't want to allow. These age verification laws all mandate that no identifying information be kept. The Texas law has a $10,000 penalty per instance for record retention. It's still perhaps not the best way to do it, but people are being very disingenuous in their characterization of these laws. It's not that much different from requiring people to show ID at the door of an adult store. reply samus 10 hours agorootparentEven assuming that an age check can be done in a privacy-preserving way*, it is naive to assume that such measures are effective prevent people from getting access to porn. Sex is one if the strongest human drive and we are not in the 80s anymore when magazines and videos were the primary medias for porn. *: are those verification schemes set up such that also the government doesn't get to see who accesses which sites? reply ndriscoll 8 hours agorootparentI don't think it's as intractable as people make it out to be: require porn sites to do some level of due diligence. \"But what about sites outside of US jurisdiction (e.g. Russia)?\" Require ISPs to have a setting for customers to opt into blocking them. The reality is no one in my household ever has any reason to communicate with Russian, Chinese, etc. or even almost any European servers (maybe there could be exceptions for news, government, and university orgs), so it makes sense for us to just block them. There you go. You just eliminated pretty much all access for children to online commercial providers. It's not perfect, but that's a silly reason not to do it. We don't let kids into adult bookstores just because they could (currently) easily get it online. We don't let them buy drugs and alcohol at stores just because they could find an older friend to get it for them. It is already illegal to provide porn to children. Businesses (mostly ad-based ones) have just been getting away with being completely negligent about it for the last 10-20 years. The Utah law for example seems to specify that either the user provides a digital id, which seems to be a sort of signed message from the state that they've saved into a secure element (so not doing realtime checks with the government. It's not really specified how it works, but it says they can save an ID file to their phone), or use a commercial knowledge based auth solution. So yes, it seems that it's been specified such that the government does not get access to that information. reply Dylan16807 10 hours agorootparentprevIt's still dangerous to share records that way. The structure of the law makes it be against porn consumption and anonymous porn consumption. reply thriftwy 12 hours agoprevnext [6 more] [flagged] moonchild 12 hours agoparentRespectfully: 1. I think censorship laws are abhorrent 2. It can be very difficult to tell what effect media has on you, and just because you do not (say) find yourself traumatised or find that your perceptions have been overtly altered does not mean you have not been negatively affected. And you will certainly never be unaffected by anything you experience or do. This is something that I've become more sensitive to and aware of as I've gotten older. And further, it is only through this sensitivity and awareness, and acceptance of the fact that I cannot just decide by fiat how things will affect me, that I have gained some measure of control. reply thriftwy 12 hours agorootparentnext [2 more] [flagged] samus 11 hours agorootparentNor is it provable that you weren't. Which is the point of the previous comment. Anyways, enjoy the nosebleed material... reply refurb 8 hours agoparentprev> ...and I can assure you I was not harmed How do you know? I'm not arguing porn is harmful to minors, I have no idea what the data says. But people tend to be terrible at self-diagnosis. reply thriftwy 53 minutes agorootparentI've never seen any research into the topic and I can't imagine you can have one, since you don't have control group. So I will assume you are conjuring facts out of thin air. reply nanocode 12 hours agoparentprevWhat you wrote sounds a bit like \"As a minor I have lived in asbestos building and I can assure you I was not harmed\". How do you know? As far as I know, there are many scientific studies that disprove what you've said - and that porn has, in fact, a harmful effect on young minds. Of course there are many other harmful things that we nevertheless accept, like alcohol, cars or cigarettes. So maybe we can, as a society, decide that letting kid watch porn is OK and not worth the alternative (privacy intruding regulations). But arguing that it's not harmful at all is not, I believe, scientifically justified. reply lo_zamoyski 10 hours agoprevPornography is one of the most powerful forms of control, because you cooperate in your own enslavement. Because of the deranging and darkening effect it has on the mind, you become increasingly less aware of what's actually happening to you, less conscious of the self-destruction you are employing against yourself. And because it functions this way, it has also been instrumentalized. Sexual \"liberation\" is political control. Wilhelm Reich essentially wrote the playbook here, though we see plenty of precursors, and in the Enlightenment tradition, Marquis de Sade stands out (though he did not have access to mass media like we do). As Chesterton observed, such \"freedom\" is the most transparent of all bribes that slavery can make to rob us of our freedom. Pleasure is far more effective than pain because it conceals the coercive nature of the act. Marcuse saw such things as a conservative force masquerading as \"liberation\", one intended as a distraction (like bread and circuses) and a way of sapping the attention and energies (and I would add intelligence) of those who partake that could otherwise be used toward criticism. The deranging effect is the worst effect of all, as such, but also because its effects can far outlast the period of consumption. That people can use VPNs is not a substantive argument against bans or restrictions on pornography. reply paulpauper 12 hours agoprev [–] VPNs: $12/month if you sign up for 200+ months for blocked IPs and slow servers and service. awesome. reply karaterobot 11 hours agoparentWhich one are you using? It's overpriced and it sounds like a bad experience. Fortunately most VPNs you'll find are cheaper and better than what you're describing! reply aegypti 11 hours agoparentprevReality: €5/month if you sign up for 1 month, blocked IPs, ~10% reduction in Mbps reply 77pt77 11 hours agoparentprev [–] For $12 a year you can get a VPS and just put OpenVPN there. reply paxys 11 hours agorootparent [–] And who is going to pay for the bandwidth? reply luuurker 10 hours agorootparentDigitalOcean, OVH, Hetzner, Vultr, Linode, [...], all include a certain amount of bandwidth (or unlimited with a speed cap) with their VPS. reply 77pt77 11 hours agorootparentprev [–] Those VPS come with like 1 TB a month of bandwidth. It's storage and CPU that are kind of a premium. Bandwidth is cheap. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Laws mandating age verification for porn and social media are driving a notable increase in VPN adoption among users aiming to access restricted content privately.",
      "Platforms are proactively blocking IP addresses from regions with these regulations to prevent penalties and protect user information.",
      "The growing use of VPNs to bypass content restrictions might draw attention from legislators, raising the possibility of future regulations limiting VPN utilization, although legal experts suggest such measures could face constitutional challenges eventually."
    ],
    "commentSummary": [
      "VPN usage is increasing due to porn restrictions in some countries, with services like 1.1.1.1 and Mullvad's free DNS being used to access blocked content.",
      "Encrypted DNS methods are discussed for enhanced privacy, ad-blocking, and setting up devices and browsers.",
      "Debates include internet censorship, mandatory military service coercion, and concerns on online security and ID verification laws, alongside the balance between child protection, individual rights, government control, and VPS service costs."
    ],
    "points": 183,
    "commentCount": 236,
    "retryCount": 0,
    "time": 1712434738
  },
  {
    "id": 39953207,
    "title": "PiVPN v4.6.0: Project Conclusion",
    "originLink": "https://github.com/pivpn/pivpn/releases/tag/v4.6.0",
    "originBody": "pivpn / pivpn Public Notifications Fork 583 Star 6.8k Code Issues Pull requests Discussions Actions Wiki Security Insights Releases v4.6.0 v4.6.0: The end Latest Latest Compare 4s3ti released this · 3 commits to master since this release v4.6.0 d072977 Hi everyone, It's time to say goodbye. This is the final official release of PiVPN. I inherited this project from @0-kaladin and @redfast00, who moved on with their lives. I maintained it as my own with the great help of @orazioedoardo, to whom I'm immensely grateful. He held the boat and kept it floating while I could not be present, and he too gave a lot of himself to this project! But now it's time for me too to move on! I've been giving less and less attention to PiVPN, and the desire to keep up with it is no longer what it once was. When PiVPN was created, it filled a big void and had a clear mission and purpose, which I feel has been fulfilled! We went from OpenVPN being something hard to set up and complicated to manage, to WireGuard being able to run on any toaster and easy to set up. There are so many tools out there that do the job much better than PiVPN does, and I genuinely believe PiVPN's mission in life was accomplished and is no longer relevant. Just as everything in nature has a start, there's also an end, and this is how PiVPN ends its journey. PiVPN has been home for so many of you, starting with Linux, bash, open-source, and everyone was always very welcomed, just like it was for me 7 years ago. I cannot express how grateful I am to all the 84 Contributors for this amazing project. It has been a wild ride, and I've learned so much from PiVPN and from every single one of you! THANK YOU!! PiVPN repositories will be archived and set to read-only, and will no longer be maintained. Unless @0-kaladin rises back again and decides otherwise. The PiVPN Website and its documentation are hosted on GitHub, therefore it will remain accessible under the pivpn.io domain for as long as @0-kaladin keeps paying the bills, just the same way I will keep hosting the redirection for the installation for as long as possible. I will still make a few commits to update the documentation about the project's state, but that will be it. I will maintain ownership of the repository, but I won't pass it down to anyone else. First, because I feel it's not up to me to decide who to pass the project down to, and second, because there is no one else to pass the project to. \"But I want and can maintain it, can I take it over?\" Let me put it plain and simple: No! I don't know you, I don't trust you! Fork it and carry on! About this release, here's what it brings: New Features Add possibility to use Pi-hole in unattended install (#1825) Bugfixes and Refactors Updates to subnet generation and client creation refactor(core): allow any subnet and netmask fix(scripts): prevent adding more clients than the subnet allows fix(scripts): correctly remove leading zeros from ipv6 quartets refactor(core): new probabilistic subnet generation with fallback to other RFC1918 subnets Full Changelog: v4.5.0...v4.6.0 Once again, Thank you all so much for everything! See you around! 4s3ti Contributors 0-kaladin, redfast00, and orazioedoardo Assets 2 208 208 people reacted 23 Join discussion",
    "commentLink": "https://news.ycombinator.com/item?id=39953207",
    "commentBody": "PiVPN v4.6.0: The End (github.com/pivpn)165 points by allanbreyes 18 hours agohidepastfavorite78 comments solarkraft 9 hours ago> \"But I want and can maintain it, can I take it over?\" Let me put it plain and simple: No! I don't know you, I don't trust you! Fork it and carry on! They learned a good lesson from the liblzma situation. reply nickjj 12 hours agoprevThis really goes to show you how valuable a good experience / API is. PiVPN is so easy to use. You run 1 command and pass in the name of the config to generate and you're done. Now you can take that config and use it client side. I've used it on Debian servers (not a Raspberry Pi) and it's been flawless to onboard a bunch of folks into using a VPN (work related). IMO there's no way this project will fail, someone will fork it. reply FerretFred 16 hours agoprevThat's such a shame - I've used PiVPN many times and it's just made life so straightforward. Big, BIG thanks to all involved, and you'll be missed! reply postpawl 14 hours agoparentIt’s probably better to just use the wireguard docker container setup instructions now: https://github.com/linuxserver/docker-wireguard?tab=readme-o... reply jimmyl02 16 hours agoprevcurious to hear does anyone know what the mentioned alternatives are? a super simple to use wireguard control plane is super valuable and PiVPN seemed to fit that gap perfectly unfortunate that it's come to an end but it's nice to hear the maintainer moving on in such a positive way :) reply vundercind 14 hours agoparentAfter meaning for years to spend the 2-3 hours I’d need to set up wire guard and get all my devices on it that I’d want on it (it’s a bit fiddly and time consuming, and inevitably with projects like that, there’s some dumb problem that comes up that wastes a bunch of time) I just did the free tier of Tailscale. Server, two Apple TVs, a couple phones, a tablet, and a laptop all on it in like 15 minutes flat. With one of the Apple TVs configured to act as a gateway, too. Should’ve just done that to begin with. reply Ballas 3 hours agorootparent2-3hours? Just try wg-easy, it should not take more than 10minutes. reply renk 2 hours agoparentprevhttps://github.com/angristan/openvpn-install or if you want to be free from your distro's OpenSSL version: https://github.com/ix-ai/openvpn (recommended, just rebuild the container if it becomes outdated) reply postpawl 14 hours agoparentprevDocker-wireguard: https://github.com/linuxserver/docker-wireguard?tab=readme-o... You set the number of peers and it generates that number of folders with certificates and QR codes for you. reply Saris 14 hours agoparentprevwg-easy is probably the easiest to use simple alternative I can think of. reply pogue 16 hours agoparentprevhttps://news.ycombinator.com/item?id=39953873 reply Havoc 15 hours agoparentprevwg-easy comes to mind reply Handy-Man 6 hours agorootparent+1, it's amazing reply Fnoord 8 hours agoparentprevTailscale. reply xyst 17 hours agoprevSetting reminder to migrate rpi in closet off of pivpn. Might just setup a nixOS arm image with wg instead reply oneplane 17 hours agoprevThis is the best way to conclude a project like this, I wish more clear cut \"this is the end\" choices were made. An ecosystem with zombie projects isn't healthy. reply MuffinFlavored 16 hours agoparent> I've been giving less and less attention to PiVPN, and the desire to keep up with it is no longer what it once was. I wonder if financial/monetary incentive would change this. I don't think it would personally (because putting a value on your free time/mental load/time you can spend with your loved ones doing something else away from the PC is precious) On the flip side... $500/mo? $1k/mo? $5k/mo? I'm sure most projects that go \"defunct\" open-source-free-no-financial-incentive-thanklessly-help-build-something could probably find \"motivated maintainers\" for $3k/mo on average? Internationally? Is the \"capitalist\" answer \"this repo and all of its efforts are not worth $3k/mo to the open market\"? reply powersnail 14 hours agorootparentA lot of these projects are made in people's leisure time, without profitability, for other fellow geeks, and the users also uses them in their hobbies. And as fellow geeks, we are more likely to be financially poised to be on the other side of the equation: getting paid to write code, rather than being able to pay a developer's wage, at least not in the long term, not in any maintainable manner. Can you afford to pay yourself 3k/month to maintain such a project, without any profitability, just for a hobby? reply ianlevesque 5 hours agorootparentAgreed. Also often the gap between what people will pay for a hobby project and what money is being made at a tech company by the people who have the hobby is vast. Sometimes there are contractual restrictions on taking money from other jobs simultaneously that complicate it. reply Nullabillity 13 hours agorootparentprevYou could probably get someone, but would you get someone good (competent, trustworthy, etc)? Perhaps Jia Tan is looking for a new gig. reply ssl-3 12 hours agorootparent>You could probably get someone, but would you get someone good (competent, trustworthy, etc)? The same could be asked of people who work on open projects for free, could it not? Is a financial reward (or lack of such reward), in and of itself, some sort of implicit indicator of the quality of the person putting forth the effort? reply Nullabillity 11 hours agorootparent> Is a financial reward (or lack of such reward), in and of itself, some sort of implicit indicator of the quality of the person putting forth the effort? It is an implicit indicator of how much that person cares about the project. reply ssl-3 10 hours agorootparentPerhaps so. I don't think we need to make a study of it to be sure that GitHub and Sourceforge are rife with free software (\"free\" in terms of beer, and in libre, and also in compensation) in various states of incompletion, haphazard execution, and sheer abandonment. I mean: The open-source community has certainly produced a ton of excellent software for free, but it has also produced (and published) a lot of false starts, loose ends, broken or forgotten code, and unfinished or unpolished work. Open-source volunteerism is awesome, but it isn't all ponies and rainbows. Perhaps the author(s) of some of these things might care more about finishing and maintaining them if their ongoing efforts were producing a meaningful amount of money as a reward. reply Nullabillity 10 hours agorootparent> Perhaps the author(s) of some of these things might care more about finishing and maintaining them if their ongoing efforts were producing a meaningful amount of money as a reward. Have you looked at the average state of commercial software lately? reply ssl-3 9 hours agorootparentYes, it seems to be much worse than it used to be in (pick a timeframe that relates to your own rose-tinted \"back in the day\"), but some of it is excellent. Does any of this somehow mean that a financial incentive must make free (beer and libre) software worse? If so, why and how? It does not to follow, for me, that rewarding software authors with money must make things worse. I've personally put a fair amount of money into various tip jars for free software authors who create stuff that is important to me. There is no part of me that thinks that me doing this somehow disincentivizes them from continuing to do outstanding work. reply animuchan 3 hours agorootparentprevMaybe not quality of the person, but quality of the job done, absolutely. When working for free on my hobby projects, I do my absolute best. Now try to pay me $3.50 per hour for similar work (strictly +Infinity% more than before!), I'll probably flat out refuse / won't focus on it as much. reply DeathArrow 15 hours agorootparentprevWho will pay? For sure there are developers willing to take care of it if they are payed, but who is willing to pay them? reply stavros 17 hours agoparentprevWhy? I wish people would put their projects in something like https://www.codeshelter.co so anyone who's interested can maintain them, instead of just killing them. reply BossingAround 16 hours agorootparentYou can maintain it right now. Make a fork, and continue development. You might even get some shoutout from the original devs. It's all open source after all, making this repo read-only doesn't mean the project's dead if the community is vibrant enough. reply stavros 16 hours agorootparentThe community matters. It's one thing to get control of the official websites, official packages, etc, and another to have to tell every single user \"come use my fork\". reply planb 16 hours agorootparentBut this is dangerous. There are many „Jia Tans“ out there who would love to continue maintenance of those projects with the full community. reply wolverine876 14 hours agorootparentThere are accidents on the highway, planes crash, fires in buildings, etc. Let's reason about Jia Tan - a problem, not a danger to all of FOSS - not, like everything else these days, just embrace ignorant fears. It's cool to destroy social trust, to deny it and abandon it. The counterargument is right in front of your nose - the incredible, infinite, world-changing world of FOSS. Think of all those amazing projects, social trust working over and over and over. You're going to throw all that out over one guy? The only thing we have to fear is fear itself. reply planb 13 hours agorootparentThis is not what I meant. But I prefer a fork of an abandoned project which needs to gain new trust to be installed instead of a new release pushed through an auto update after 3 years that installs malware. The parent comment was not about someone from the community taking over (which to be honest was the case in the xz story) but about posting the project on a „projects without maintenance“ site for any random person to take control. reply wolverine876 9 hours agorootparentThat all makes sense. I agree about the fork. reply stavros 16 hours agorootparentprevYeah, we always knew there were. Open source can't stop existing because there are bad actors. reply sevg 16 hours agorootparentSo you're saying that if projects continue choosing to sunset without handing over the keys to the kingdom, open source will stop existing? This is simply not even close to true. Edit: I can't reply to your reply, so here will do. You've completely ignored my main point. I get that you want projects to pass on the torch, but saying open source will otherwise die is ridiculous. reply stavros 16 hours agorootparent\"Continue choosing to sunset\"? A large amount of projects does not sunset, it gets passed on instead. reply theamk 8 hours agorootparentprevAnd author is pretty explicit about this: > \"But I want and can maintain it, can I take it over?\" Let me put it plain and simple: No! I don't know you, I don't trust you! Fork it and carry on! For something security critical like VPN, ownership change is a big deal. Users trust project's reputation. So if there is not a a trusted successor, shutting it down is way better that giving it up to unknown people. reply bartonfink 16 hours agorootparentprevSo you want someone else to run it so you can just be part of a community? Seems selfish. reply glitchcrab 16 hours agorootparentThat's not what they meant at all, don't be obtuse. The community exists around the project (in this case the repo and associated website etc). If you fork it then you have to hope that the community follows you to your fork and that then everyone coalesces around it. This isn't guaranteed to work though, so passing the existing project onto a new maintainer is a much better way of retaining the existing community. That is what was meant when talking about the community. reply opello 11 hours agorootparentThe earlier comment is concerned for the users being orphaned by the project they used. The project is concerned with protecting the trust the users placed in the project by using it. To trivialize the concern of the project seems worse because it prioritizes convenience in a particularly sticky area (security/privacy) as well as forcing a less informed choice on the user (who they are trusting). There's probably a nice parallel here where we consider the NRL's role in Tor and how FOSS practices, EFF funding, and transparency meant it preserved user trust. reply 8n4vidtmkvmk 16 hours agorootparentprevIsn't xz a prime example of why we don't just hand over the reigns anymore? Like the guy said, they can just fork it. reply Narishma 16 hours agorootparentprevThey can still fork the project and continue maintaining it if they want. Nobody's stopping them. reply logicziller 10 hours agorootparentprevHe did mention in his post that he's not gonna handover the project to someone he doesn't trust. reply bornfreddy 16 hours agorootparentprevDo you, as the project maintainer and possibly even founder, trust these people? reply stavros 16 hours agorootparentThe maintainers are vetted before joining, and are removed if they do something untoward, but when the choice is between killing the project or giving it to some random person, Code Shelter provides a better alternative. reply sthlmb 16 hours agorootparentWhat if they pass the joining process but then later sneak something in that goes undetected until things go boom? There are alternatives, you can fork the original project, and things will go on. As others have said too, you can just update the underlying software and there's a good chance that the wrapper itself will continue functioning, providing there are no giant breaking changes and by that point, a fork or alternative will likely have handled it. reply stavros 16 hours agorootparentWhat if there's no joining process, and they contact a maintainer directly, and peer pressure them to hand over the project, and the maintainer does, and then they sneak a backdoor in some binary test files? reply eropple 15 hours agorootparentThat scenario is exactly what PiVPN is avoiding by refusing to nominate a new maintainer and telling interested parties to fork--so what is your actual and concrete objection? Fork the project. Earn your own trust. reply stavros 15 hours agorootparent> so what is your actual and concrete objection? This: > I wish people would put their projects in something like https://www.codeshelter.co so anyone who's interested can maintain them, instead of just killing them reply eropple 5 hours agorootparentSo to me that says you want it both ways, for while I appreciate what the codeshelter folks are trying to do, it is a task that is going to turn out Sudden But Inevitable Betrayals. Instead of contacting a maintainer directly, they just look sufficiently polished that codeshelter says \"yeah, sure, OK\" and hands it over. Forking the project and earning your own trust really is the safe path forward. reply reachableceo 16 hours agorootparentprevThe project can be forked with a single click. That’s the beauty of GitHub. reply Zambyte 16 hours agorootparentThat's actually the beauty of git, and any other DVCSs. It's one click to \"fork\" with lots of other forges as well. reply eviks 14 hours agorootparentprevWhere do you click second to make all the (dozens of) contributors even be aware of your first single click? reply prmoustache 16 hours agorootparentprevIt is not killed, anyone can pull the repo and work on it. reply Takennickname 11 hours agoprevLiterally installed it yesterday for the first time. Damn. reply Hamuko 13 hours agoprevAnyone got a recommendation for a router with Wireguard support baked in? I've been running PiVPN on a separate box but since I need a new router anyways and it's not going to be supported, that might be a viable replacement. reply t0bia_s 1 hour agoparentMikrotik hAP ax3 or any router with RouterOS that supports wireguard natively. reply Fnoord 8 hours agoparentprevYou can easily install Wireguard on EdgeOS (VyOS fork) 1.x and 2.x and 3.x will have it natively. The OS is kind of RIP otherwise, so I cannot recommend it, but Ubiquiti just released a new UnifiOS-based router with 5 2.5 GHz ports saturating 1.6 GHz with IDS. That, or some random AliExpress x86-64 router with OPNsense. reply logicziller 10 hours agoparentprevRecently we needed a customer in a different country to be able to connect to a wireguard instance and I didn't want to deal with the support headache of walking them through the flashing process. While I was looking for devices that come with OpenWRT preinstalled, I came across FriendlyElec that looked quite decent. Eventually we ended up building a custom Raspberry Pi image. reply spr-alex 13 hours agoparentprevYou can give us a try, https://github.com/spr-networks/super, https://supernetworks.org/. Wireguard is well integrated. We also have a tailscale plugin, and more vpn plugins on the way reply opello 11 hours agoparentprevUbiquiti UDM-Pro has it, but I'm not sure how they're regarded in popular opinion these days. I've had good luck with everything but the PoE on mine, and they gave me a free injector to fix that. reply gamesbrainiac 13 hours agoparentprevGLiNet routers have that. So do the Asus ROG routers, but they don't have NAT acceleration. reply Takennickname 11 hours agoparentprevOpnsense has it. Never got it to work though. reply Scipio_Afri 10 hours agoparentprevPfsense has it reply lazyeye 9 hours agoparentprevProtectli Vault micro appliance running PFSense with the Wireguard module installed. Perhaps overkill but have been running this for many years with zero issues. It does everything though so configuration/setup can take a bit of time. reply poisonborz 16 hours agoprevEh, I just wanted to migrate to this, a lot of threads recommend it as the best way to effortlessly set up Wireguard. WG-easy, Headscale have their own set of problems. I guess there will be forks. reply byteknight 16 hours agoparentShameless plug for an alternative? > WireHole is a combination of WireGuard, Pi-hole, and Unbound in a docker-compose project with the intent of enabling users to quickly and easily create a personally managed full or split-tunnel WireGuard VPN with ad blocking capabilities thanks to Pi-hole, and DNS caching, additional privacy options, and upstream providers via Unbound. https://github.com/IAmStoxe/wirehole reply pogue 16 hours agorootparentSounds very cool, thanks for the recommendation! Lots of videos on YT with setup guides too! reply unethical_ban 16 hours agoprevThanks to the maintainers of the project. It is a handy tool, a good wrapper around setting up simple wireguard quickly. And it pairs with pihole really well. I migrated to OPNSense for my DNS and I haven't needed VPN for a little bit. But I kind of disagree that there is no place for a simple CLI tool for wireguard user management. I was going to make a comment about how unreasonable it is to shut the project down instead of letting someone else take it over. But two things come to mind: First, yes, people can fork it and develop it on their own. Second, right after xz, maybe it would seem unwise to endorse a stranger taking over your security project. PS: PiVPN isn't wireguard itself. Assuming WG's command line doesn't change radically for a while, PiVPN is still completely usable and people don't need to rush to get off it. reply yokoprime 16 hours agoprevCrap, i’ve been running pivpn as a LXC since its so light weight reply lukevp 17 hours agoprev [–] Crazy to abandon a 6.4k star project that presumably many people are actively using… I know maintenance of OSS projects can be burdensome but there’s usually some in the community that are eager to chip in with PR reviews and handling issues. I’m surprised they aren’t interested in pivoting the product in the same general direction but giving it some novel features or something. reply loloquwowndueo 17 hours agoparentWhy is it crazy? If it no longer aligns with the maintainer’s interests or energy, doesn’t provide compensation, he’s within his right to archive it and move on. And people in the community can fork it if they need to. reply jprete 17 hours agoparentprevAfter the sshd debacle, and in the context of GenAI becoming ever better at impersonation at scale, I don't think anyone working on a security-relevant project should simply hand off to an enthusiastic community member they don't know well. reply cocoa19 17 hours agorootparentDo you remember when Raymond Hill ceded control of uBlock to another guy? This new guy started asking for donations (for himself) and then sold the project to AdBlock. That was truly disgusting. That’s what prompted Raymond to create uBlock Origin. reply codetrotter 17 hours agoparentprevIf it were me I would shut it down too after I no longer had energy to maintain it. Just handing responsibility over to someone else for something like a VPN project is definitely high risk. Remember the xz debacle last week? Same kind of people who backdoored xz would love to get maintainership of a VPN project for sure. reply baq 17 hours agoparentprevIt’s crazy to maintain such a project, shutting it down is the only sane option. Chapeau bas for keeping it going for so long. The internet of old was built by irrational hobbyists like these guys. reply ocdtrekkie 17 hours agoparentprev [–] My guess is they think the alternative already meets their needs. If someone else is already doing it better, why not just use that? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "PiVPN has launched its ultimate official version, v4.6.0, marking the end of its maintenance.",
      "The project, managed by multiple contributors, has accomplished its goals and will not continue; repositories will be archived, and the site will stay accessible if expenses are covered.",
      "Following the final release, no further development will take place, including new features and bug fixes."
    ],
    "commentSummary": [
      "PiVPN v4.6.0 will soon be discontinued since the maintainer declines to transfer it, prompting users to explore alternatives like WireGuard and OpenVPN.",
      "The dialogue encompasses the sustainability of open-source initiatives, compensating contributors, engaging the community, entrusting new maintainers, project forking, and suggesting VPN plugins for different routers and devices.",
      "Discussions also involve ending a project due to waning interest and effort, along with apprehensions regarding transitioning control to unfamiliar community participants."
    ],
    "points": 165,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1712418422
  },
  {
    "id": 39951587,
    "title": "Lessons from 503 Days on a FOSS Project",
    "originLink": "https://mathspp.com/blog/503-days-working-full-time-on-foss-lessons-learned",
    "originBody": "Blog All posts Pydon'ts Problems TIL Twitter threads Books Talks Training Workshops Courses About The author Contacts Support Blog 503 days working full-time on FOSS: lessons learned 1st Apr 2024 open source opinion python This article shares some of the lessons I learned from working full-time on a FOSS project for 503 days. 503 days working full-time on FOSS: lessons learned From the 14th of November of 2022 to the 31st of March of 2024 I worked full-time on a FOSS project. This article is an account of some of the things that I learned during those 503 days. I have much more time to devote to you now that I don't have this full-time position. If you're looking for Python training, reach out to me. I'll also be investing my time in this blog and in my books. I joined Textualize, the company behind the popular FOSS Python packages Rich and Textual, on the 14th of November of 2022. When I joined to work remotely from sunny Portugal, I joined Will (the creator of Rich & Textual, and my boss), Darren, and Dave, all of whom worked together in Scotland. For the first time in my professional life, I was the worst Python developer in the room1. This means that my technical knowledge improved substantially over the course of these 503 days, namely in areas such as: asynchronous code execution and Python's asyncio; Python tipe hinting; testing / pytest; and software engineering best practices in general. However, this article will focus on the non-technical lessons I learned during this 503-day experience, that fall under these four broad categories: how an online presence can lead to a job offer; how your ego shouldn't be in the way of your work; how to interact with users & contributors; and how to work on a huge codebase you don't know 100%. Let's dive right in. Everything you do online works as a banner One interesting thing that I learned even before starting my full-time position is that you can definitely find (job) opportunities in unconventional ways. This has always been the case but the Internet makes it even easier to do so. If you're looking for a job, you can submit your CV to a platform or fill out a form. I've gotten job offers by going through that process. But that's not how I got this job nor how I got any of my previous jobs. I'll tell you more or less how I fooled Will into hiring me got this job in the hope that you understand how powerful an online presence can be. I met Will on X/Twitter, where I actively post about Python. Over the course of many months we'd interact sporadically. Sometimes I commented on his posts and sometimes he commented on my posts. I think Will enjoyed the Python tips I posted and the occasional link to an article of mine. After many months of occasional Twitter interactions and a couple of informal video calls, Will messaged me asking if I was interested in working for Textualize. Some months before, I had mentioned that I would see myself working with him further down the line. But I wasn't trying to get a job when I said that. In fact, I remember saying something like “I see myself working with you in 5 years”. Even still, a few months later I had Will's message on my inbox! I agreed to go through the recruitment process and in the end Will made me an offer. I was sad to leave my previous position, where I was enjoying myself working with / teaching APL, but I was also pretty ecstatic about working full-time on a Python FOSS project. Getting this job offer was confirmation that everything you do online can act as a banner for you and your work. In my case, that's my Python posts on social media, my blog articles, my books, etc. For you, it could be something different. But don't forget that possibility! Put your ego aside Before joining Textualize I had always been the best Python developer in my team... Because I was the only Python developer in my team2! Being the best at something is great for your ego but it also means there's no one there to pull you up and to teach you new things... And I wanted that! I wanted to work with people who I looked up to for their technical skills and I realised Textualize would give me that. I joined Textualize and I immediately understood that code reviews would be moments where I could learn a lot. If you think about it, when you're writing code, it's not only about whether the code does what it's supposed to (fix a bug, implement a feature, etc.). The actual code you write is also relevant and code reviews are when your coworkers get to comment on that. Ask ALL the questions One thing I did and that I can recommend is that you use code reviews to ask questions. It happened regularly that I would read someone's code and think “interesting, why didn't they do this with X, Y, and Z?”. Whenever that was the case, I left a comment asking that same question, even if the question sounded a bit basic. Often, they would reply explaining their reasoning, and I would get to learn something new. Some other times, it would turn out that they hadn't thought of the alternative I was considering and they took my suggestion. I think the key points here are that you should ask the question(s) with the assumption that the person whose code you're reviewing has a good reason for having done things differently from what you expected; and you shouldn't refrain from asking questions, regardless of how silly or basic they may seem. Point 1. is important because you don't want the other person to feel they're being attacked and because you should understand the reasoning behind the author's decisions. Point 2. is important because you want to learn as much as possible... And because some times people just forget about simpler alternatives (even senior developers). I'm happy to report that I've fully embraced the mindset of asking questions, even when they may sound silly, so exposing my lack of knowledge in certain areas didn't hurt my ego too much... But having my code reviewed was a whole new story. Embrace (code review) feedback My pull requests were invariably flagged with the red X requesting changes in my work: Will requesting changes on one of my first PRs. Going back to the reason code reviews are so interesting, it wasn't just about whether or not I was fixing the bug or implementing the feature. It was also about the actual code that I wrote. In the beginning it was tough to have all of my work scrutinised. But if I wanted to improve my Python and software engineering skills, I'd have to embrace that scrutiny and the feedback I received whenever someone requested changes on my work. I also got a lot of practice, because I got a lot of changes requested: Montage of “changes requested” on my work. I learned to embrace the feedback and I recommend you do the same! Assuming everyone is well-intentioned, it's one of the best things you can do. Bonus points if you can get people to explain why they recommended a different approach. Everyone makes mistakes Everyone has bad moments, bad days, bad weeks... Some weeks I felt less productive or made more mistakes. But so did everyone else! If you're feeling like you're not at your best, tackle a couple of simpler tasks. That's a reasonable thing to do. It sucks to be in a less productive period, but that's also a good time to figure out if you're working with reasonable human beings or with jerks. Picture this: you make a pull request; it goes through the usual review process and it gets merged; the feature you implemented is released to the public; and some days later users report “obvious” bugs in the feature you implemented. How would that make you feel? To me, it made me feel a bit silly. I felt utterly responsible for the bug that was being reported! After all, I implemented the feature in question and the bug wasn't about an obscure interaction... It was just a blunder of mine... Thankfully, my team reacted differently and they said “we don't play the blame game” when reacting to my feeling responsible. I guess it all comes down to the age-old adage that I already wrote above: “Everyone makes mistakes.” When others make mistakes, be graceful and helpful, instead of judgemental and critical. If you do that, you'll make it more likely that others will be graceful and helpful when if you make mistakes. For comic relief, here is a screenshot of me feeling ashamed after opening a “bug report” that was fueled by a major lapse of judgement regarding the way Python works: Closing the most ridiculous issue I ever opened in my life. Interacting with users and contributors Working on a popular FOSS project brings with it an unexpected blessing and curse: users. Users are obviously a blessing. It doesn't sound fun to pour your heart and soul into a project that no one uses. But handling user interactions is surprisingly time consuming and difficult! Let me share with you some of the things I learned while doing this. Create a contributing guide No one will read it. Or rather, no one will read it before wasting a bit of your time. But having the document there, with clear guidelines and helpful suggestions, will save you a LOT of time. A contributing guide will also have your back in some situations. More on that later! Give clear instructions to bug reporters One of the things I learned is that you have to hold the hand of your users if you want decent bug reports. One thing that really helps is setting up an issue template for bug reports, which is an excellent feature that GitHub has. As of writing this, our bug report template issue looks like this: Have you checked closed issues? https://github.com/Textualize/textual/issues?q=is%3Aissue+is%3Aclosed Please give a brief but clear explanation of the issue. If you can, include a complete working example that demonstrates the bug. **Check it can run without modifications.** It will be helpful if you run the following command and paste the results: textual diagnose Feel free to add screenshots and / or videos. These can be very helpful! Something we should probably change is to ask for a minimum reproducible example instead of asking for a “complete working example”. Sometimes, people report bugs without showing any code to back it up, which is upsetting. But even more upsetting is when users post hundreds of lines of code! I feel like it is reasonable to ask users to simplify the code that produces the bug as much as they can, so adding that to the issue template can be a huge time saver. Depending on the nature of your project, you may need more specific information from the user. One thing that turned out to be invaluable in our project was the textual diagnose command. It prints a lot of useful information to the terminal like OS name and version, Python version and install location, and information about the settings of the terminal that was being used, which heavily influenced the way Textual behaved. You may want to add project-specific instructions in your issue template and/or your contributing guide. Be so kind it's annoying In general, whenever you're interacting with someone else, you should be as kind as possible. Every person is different and every situation is unique. Often, there are nuances we're not even aware of and that would explain a certain behaviour that maybe we think is not reasonable. Did a user sound rude or harsh? Maybe English is not their first language and they can't express themselves well enough. For example, in a certain PR review I ended up saying that the user should be ashamed of what they did because I mixed up an English word with a Portuguese word. Did a user ask something that's already explained in the docs? Assume they actually went through the docs and couldn't find the information or maybe they found it but they didn't understand. Point them to the appropriate link(s) and ask whether something there was unclear or lacking detail. This can be pretty tiring but I find it definitely worth the effort. Interactions that start off seemingly unpleasant can turn out great if you don't escalate the situation. That happened to me more than once! Give a first reply quickly One thing I think is very important is to give a first reply to an external user as quickly as possible. If the user opened an issue, thank them for the report and let them know you'll look at it when you have the time. If it's a bug report, take a look to see if it's missing a reproducible example or more information and ask for it right away. If the user opened a pull request, thank them for their time and say you'll review it when you have the chance. When a user interacts with your project, it's likely that you have something to gain from that interaction. This is obviously true for bug reports and pull requests. So, I believe we shouldn't let the user go for days or weeks without a simple reply. I'm not saying bug reports have to be fixed immediately or pull requests need to be reviewed immediately. I'm saying we should strive to say something to the user soon. I came to this realisation after thinking about how I feel when I open an issue on another project and months go by without a reply! If you're just very busy (which is fine and can definitely be the case) just set up an action that replies automatically with something friendly. “I appreciate your pull request / bug report. I am very busy at the moment and I will get back to you as soon as possible.” How to handle external pull requests Another challenge I faced and that I didn't expect was managing external pull requests. Much like with bug reports, I found invaluable to have a pull request template with a checklist for external users (and often, for myself as well). Mentioning things like runnnig tests and formatting the code with the appropriate formatter(s) can save everyone a lot of time. Another thing I realised is helpful is to make sure that each pull request is associated with an issue. Sometimes users will open pull requests that implement features without those features ever being discussed. Chances are the user is trying to be helpful, but sometimes those features need a bit of discussion. I find that those are the cases where an associated issue will be the most helpful. If a user opens a pull request out of the blue with a non-trivial change, make sure you thank them for their time and ask them to create an issue that discusses the changes they made. They probably wanted to implement a feature that is helpful for them (and possibly for others). Point them to your contributing guide. Your guide has your back here because it makes it less likely that the user will see your request as a personal attack. (Remember that Humans are animals, really...) When it's time to review the code that the external contributors wrote, remember that if someone went out of their way to make a pull request, it's likely that they are well-intentioned. As long as you're not an idiot, you'll likely be able to get them to comply with your requested changes! If you're very peculiar about formatting, naming, design, and/or other things, my suggestion is that you mention those in the contributing guide. If there are tools that automate part of the process, mention them. I personally find it easier to say “as per the contributing guide, we only use variable names that have exactly 5 consonants” – if it's in the guide it obviously applies to everyone and the whole project – versus saying “please use variable names with 5 consonants”, as the user might think you're just being picky with them. The dreadful story of when I rejected my first external PR Most of what I'm writing in this section about interacting with users and contributors came as a realisation after one particular interaction that I had with one specific user. One day, someone opened a pull request trying to improve a specific guide in the documentation. These changes came “out of the blue” in the sense that there was not a previously opened issue asking that we improve that guide. The changes were non-trivial and involved changing the order of some things, duplicating some information, and rewriting sentences. We are sure the user was well-intentioned. In fact, we had already interacted with that particular user. And that well-intentioned pull request had some objectively great suggestions, but it also had other changes that were of subjective quality. In the end, I opened a second pull request where I committed the best parts of the original pull request. I made sure to include the author of the first pull request as a co-author. Then, we merged this second pull request. However, the first pull request was closed. When closing it, I made sure to thank the author for their time and I tried to explain why we were closing their pull request. Thankfully, the user accepted my feedback very graciously! However, if I had instructions about changing the documentation in the contributing guide maybe I could've saved this user some time... Working on a large project Textual was the largest codebase I ever worked on for a sustained period of time. I've made drive-by contributions to larger projects, but a one-time contribution to a large project is much simpler to make than to work consistently on a large project. When a codebase reaches a certain size, it becomes virtually impossible to fit all of it in your head at once. For me, Textual was at this point. With time, I grew more and more familiar with the codebase but there are still things I don't know exactly how they work and there are parts of the codebase that I never touched. To make it easier for me to work on such a huge project, I developed three systems: a set of commandments that I kept in mind while working on larger features or bug fixes; a note-taking system for big tasks, difficult issues, and eventful situations; and a personal pull request checklist I went through before making a pull request. Let me walk you through these three systems. 4 commandments for my work I remember making a couple of bigger pull requests that didn't go very well on the first try. I asked around for suggestions on how to handle these situations (when you're implementing a bigger feature or fixing a very difficult bug) and I ended up with four commandments that I should always keep in mind when writing code. What's best for the developer? – When making design decisions, I should always think about what the people using my code will want and that's what should drive my decisions. What's the spirit of the issue? – Does the code do everything it should, or am I just addressing the things that are explicitly mentioned in the issue I'm solving? Following the spirit of the issue, is there anything else I should do? I take the hit so that the dev doesn't. – I am the one who needs to handle all annoying or difficult edge cases and write the boring code so that the user doesn't have to. Am I preventing the user from doing something that they might want to do? – Do my design decisions prevent the user from doing things that are reasonable and that they may want to do, even if it's not obvious to me why they'd want to do it? These four ideas are related and they're possibly self-evident. In hindsight, they are obvious to me, too. But it definitely helped me writing them down. Take notes Another thing that helped me was writing down notes about bigger issues I was tackling. Whenever I was trying to fix a difficult bug, I'd jot down some notes about things I tried. Conjectures I had about the source of the bug. Attempts at fixing said bug. Personally, the act of taking notes helped commit to memory some of the things I was learning along the way. Strategies that did or did not work when trying to solve certain issues. It's unlikely that note-taking will benefit everyone in the same way, but I'm sure there are variants of this that are likely to benefit you. Personal pull request checklist Taking notes was helpful to me but I reckon my personal pull request checklist is likely to be generally more helpful to others. Whenever I was working on an issue, I followed more or less the following flowchart: Flowchart for making pull requests. If the flowchart above gives off the idea that I ran the tests obsessively, then that's not doing justice to how often I ran the tests. After a couple of mishaps and interesting adventures I realised there was no such thing as running the tests too often. Over the course of these 503 days I managed to break the tests, or see someone else break the tests, in the most unexpected ways. Things like fixing typos in code, removing unused imports, reformatting code, you name it. Whenever I thought “this change is so trivial I don't need to run the tests”, I would break CI. Whenever I made a PR, I always started by making it a draft PR and then I would review my own code. For some reason, reviewing my own code on GitHub helped me catch things that sometimes I would miss when working locally. I would only request someone else's review after reviewing my own code and making sure the tests were passing on CI. One thing that may or may not be obvious from this, and that is a bit of a technical comment, is that tests helped me a lot and gave me the confidence I needed to work on the Textual codebase. There was so much code that it was very easy for me to make a change somewhere, thinking I was doing everything right, and then break the tests in some other (seemingly) unrelated part of the code. Without tests, my work would have been so much harder... Conclusion I learned a lot over the course of these 503 days working on Textual. I'm sad I'm being let go, especially because of what it likely means for Textual, but I don't regret leaving my previous position to join this project. I learned a lot about Python, software engineering, and a lot about working on open-source projects. These are the lessons I take from this 503 experience: Everything you do online works as a banner for you. Ask ALL the questions. Embrace (code review) feedback. Everyone makes mistakes. Create a contributing guide. Give clear instructions to bug reporters. Be so kind it's annoying. Give a first reply quickly. How to handle external pull requests. 4 commandments for my work. Take notes. Personal pull request checklist. Now, I will use my time to help you, your colleagues, and your team. If you're looking for Python training, reach out to me. I will also be devoting more time to writing books and teaching Python online, so stay tuned for that! At the places I worked previously I was always the only Python developer, so I was technically also the worst Python developer there... But you get what I mean. ↩ See the previous footnote. ↩ Become a better Python 🐍 developer 🚀 +35 chapters. +400 pages. Hundreds of examples. Over 30,000 readers! My book “Pydon'ts” teaches you how to write elegant, expressive, and Pythonic code, to help you become a better developer. >>> Download it here 🐍🚀. Previous Post Next Post Blog Comments powered by Disqus. Random Article I'm Feeling Lucky! Stay in the loop Subscribe the newsletter Feeds Atom 1.0 RSS JSON Popular Tags programming247 python233 mathematics115 logic33 combinatorics21 apl16 algorithms16 interpreters15 geometry14 productivity12 machine learning11 visualisation11 pygame10 compilers10 bpci9 dunder methods9 slice of life8 code review8 binary8 number theory8 arithmetic8 recursion7 game7 numpy7 modules6 Remaining Tags Archives 4 April 2024 5 March 2024 2 February 2024 8 January 2024 1 December 2023 22 November 2023 4 October 2023 6 September 2023 7 August 2023 12 July 2023 4 June 2023 4 May 2023 WolframAlpha helped me a lot when studying for Calculus... Blog Books Talks Training About",
    "commentLink": "https://news.ycombinator.com/item?id=39951587",
    "commentBody": "Lessons I learned from working full-time on a FOSS project for 503 days (mathspp.com)133 points by willm 23 hours agohidepastfavorite33 comments righthand 18 hours agoThis is all pretty ideal and I think these outcomes are more likely if the project has a substantial base. Comparatively I was trying to contribute to another FOSS project only to get push back. I started contributing to the docs to build the package which were way out of date. The project switched from qmake to cmake. The project did not have a correctly working cmake setup so I assisted with that. Every time I contributed the maintainers would check my PR then ping another maintainer who was supposedly working on the specific issues. Eventually I managed to get some docs and code merged but after that the maintainers returned harder to blocking my PR’s with their own work which they happily merged in. They would not update my PR that they had already gone in and solved the problem, I had to repull and figure it out myself, which left me with open PRs/tickets just lingering in the space. They would happily clarify my questions with obscure non-revealing conversation about their project. Eventually I gave up. Similarly I have tried to contribute to things like KDE in the past as well and been met with similar walls on contributing. Some developer somewhere is in charge of a FOSS project as a hobby and only really cares about their use cases or providing a simple demo app or rewriting an old app for their needs because the old app was too complicated or probably run by another hobbyist maintainer. This leads to opening tickets and looking for ways to contribute or discuss implementation with little to no response. Or in the case of KDE, having your ticket closed and shifted over to their bug system which requires rediscussion and retriaging. At some point you just lose momentum and interest trying to push a project along due to the sheer uneventfulness and refocusing of it all. I think we need fewer hobbyist project owners and more hobbyist project managers. reply serial_dev 17 hours agoparentNot to be mean, just to provide a different perspective: having an open source repo doesn't automatically mean I want to deal with every pull request in a timely manner, merge docs cleanups that I don't even like, and especially reviewing and merging features that I don't want. It's true for personal and probably for professional projects, too. Open source means what's in the license, everything else is just us setting expectations for others that they don't want to our can't meet. For example, I make my projects open source because I don't care if anyone else uses my code, so why not. reply righthand 17 hours agorootparentOkay well I never indicated any sort of urgency for my contributions, just that they be considered at all, not ignored then reimplemented because the maintainer has trust issues. Projects I have contributed to in the past aren’t personal projects. They are public facing software that is expecting to be used by the public. The first one I mentioned was a UI library that made me sign a contributors agreement, others have been desktop applications and desktop vpn management code. If you have no intent of accepting contributions you should put that in your README.md or CONTRIBUTING.md. Or even indicate the frequency at which you’re able to consider contributions. reply ozim 3 hours agorootparentI’d like to touch the topic of trust issues. When random person from the internet opens PR on my project it is not trust issues to look at it from every angle or not considering it because you don’t have time for that. It is perfectly reasonable not to trust random strangers on the internet. reply serial_dev 14 hours agorootparentprevI still believe that open source is about what's in the license, and everything else is just us putting expectations on others. Understanding this made me less angry with the world. And why do I need to put any of these things to anywhere, why do I need to explicitly opt out? And making promises about the frequency at which I'm considering contributions, promises I'll most likely break at some point, just sounds like a perfect way to introduce anxiety into my hobby project. reply righthand 14 hours agorootparentYou don’t have to do anything, that is perfectly clear. Just don’t expect to be successful or be taken seriously if your project is expected to be used by everyone. I think you might be confusing my intent about a hobby project vs project ownership as a hobby. This article and thread are mainly about the latter. reply m000 15 hours agorootparentprev> Or even indicate the frequency at which you’re able to consider contributions. Since most things are on GitHub these days, implementing some PR-related analytics for each repo would be nice. This would give you an rough estimate on what to expect and make an informed decision on how much time you are willing to spend. reply righthand 15 hours agorootparentI believe the repo commit frequency will tell you this (sort of) combined with reviewing pr closed history. However that doesn’t account for the inner circle issue. The main intent of the information though is to indicate process for good natured contributions, however basic as to minimize time waste for the maintainer. reply CoastalCoder 17 hours agoparentprevI had similar frustrations with a well-known deep-learning framework. I'd get well-vetted PR's for the build system, CI system, etc. committed. Only to have them later undone, without discussion, by people in the inner circle. It doesn't take many experiences like that to lose all motivation. I think some open-source projects deceive themselves (and therefore, outsiders) about their contribution processes. reply ozim 17 hours agorootparentNot wondering people rather start their own thing rather than contribute to someone else projects. Cambrian explosion of JavaScript frameworks explained. For me at least as I never really tried to contribute to any open project. reply righthand 17 hours agorootparentprevNothing stifles progress and contribution more than having a maintainer take your effort and implement it themselves or carelessly merge your work in without review only to have it reverted for not contributing code correctly. For a lot of FOSS the review process is that the maintainer does it themselves when they feel like it. And you just know the discussion by that inner circle that could have happened during code review is happening in some private email or chat app somewhere away from public eyes. reply KiranRao0 21 hours agoprevTo me, these lessons feel very similar to the lessons learned working at a midsized company. Was there any particular takeaways that are exclusive to working at a FOSS? reply magicalhippo 20 hours agoparentWhile I agree that it's not terribly unique to FOSS, it makes it even more useful IMO since relatively few work on FOSS. I think the lessons learned and advice given are great for a large number of developers. reply simonw 20 hours agoparentprevThe lessons about working with external pull requests feel unique to FOSS to me. reply bionhoward 19 hours agoprevI love Rich! Using it for tree_plus and other projects, and it’s beautiful and hackable. Great Python package for terminal visuals. Definitely recommend it if you have a Python CLI, can really improve readability and visual niceties. reply candiddevmike 20 hours agoprevI looked all over Textuals website and I have no idea how they make money. Donations? Consulting? reply SushiHippie 19 hours agoparenthttps://www.willmcgugan.com/blog/tech/post/from-open-source-... > Additionally, I would work on a cloud service using both those projects which at some point would become a business. > Now this was a good plan. The money from GitHub sponsors which I had previously been donating to charity would take the sting out of not having an income for a while. While far from a salary (where I live), at around $1000 a month it would help pay a few bills. > I have far more confidence in that vision now that I have raised seed funding. Predicated on that cloud service, I have enough funding to hire a few developers to work on it with me. Last month I was an unemployed code monkey, today I am a Founder / CEO of Textualize. Go figure. reply candiddevmike 19 hours agorootparentGood find! Getting seed funding to try and monetize a bunch of MIT libraries using a \"cloud service\". Wild. Wonder if OP was always 100% dedicated to FOSS for 503 days or if they worked on the \"cloud service\"... reply CubsFan1060 20 hours agoparentprevI think the eventual plan might be this? https://textual.textualize.io/blog/2023/09/06/what-is-textua... reply smokel 20 hours agoparentprevAds in TUIs are an underdeveloped market, I suppose. reply zeckalpha 20 hours agoparentprevWhy must everything make money? reply lmz 20 hours agorootparentUsually people don't have enough savings to dedicate themselves full time to a non money making project. reply hackernewds 18 hours agorootparentif the project pays them, it works reply weebull 18 hours agorootparentprevIt doesn't need to make anybody rich, but it should support its development in a fashion where the authors aren't worse off. reply tsuica 20 hours agorootparentprevBecause people have to eat. reply Climato 19 hours agorootparentprevIt's called capitalism. If you have money to gift, I can take it if you don't mind and than I can help myself and others to work for free on FOSS. reply 7bit 20 hours agorootparentprevWhy do you not work for free? reply zarathustreal 18 hours agorootparentContributing to OSS is a hobby, not a job. You explicitly work for free on purpose. Whether or not you’re able to pay your bills is unrelated to the OSS contribution, it’s very clear that you’re not going to get paid for writing code that you’re giving away for free. reply wrs 18 hours agorootparentHello, early 1990s! We miss you! https://www.theregister.com/2023/02/24/who_writes_open_sourc... reply willm 18 hours agorootparentprevYou only had to read the title to know that the author of the post was paid for 503 days to work on OSS. It was literally his job. reply zarathustreal 8 hours agorootparentAn example of one person being employed and contributing to open source as part of his job is not a refutation of the statement. You don’t have to read anything to know that, just use logic. To provide a counter-example you’d need to reference an open-source project that makes money selling the source code which, hopefully obviously, doesn’t exist, by definition reply RojerGS 18 hours agoprevThe title of this posting was edited and no longer matches the title of the article linked. I wonder if anybody knows why? reply quantum_state 21 hours agoprev [–] Very valuable … thanks for sharing reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article details the author's 503-day full-time work on a Free and Open-Source Software (FOSS) project, focusing on lessons in establishing an online presence, engaging with users, welcoming feedback, and coordinating external contributors.",
      "Emphasis is placed on effective communication, establishing guidelines, and creating structures to navigate extensive codebases, with reflections stressing the significance of consistent testing and supporting others through education and writing."
    ],
    "commentSummary": [
      "The author reflects on 503 days of working full-time on a FOSS project, discussing challenges with contributing to other projects and unresponsive project maintainers.",
      "Trust issues, open-source contributions, monetization, and managing contributor expectations are emphasized in the discussion.",
      "The author's journey transitions from unemployment to being a Founder/CEO after securing seed funding for a cloud service linked to the FOSS project, outlining motivations and trials in the open-source community."
    ],
    "points": 133,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1712401324
  },
  {
    "id": 39957607,
    "title": "Lago: Open-Source Billing Platform Raises $22M",
    "originLink": "https://techcrunch.com/2024/03/14/lago-a-paris-based-open-source-billing-platform-banks-22m/",
    "originBody": "(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Link Copied Enterprise Lago, a Paris-based open source billing platform, banks $22M Ingrid Lunden@ingridlunden / 9:53 AM UTC•March 14, 2024 Comment Image Credits: Simonkolton (opens in a new window) / Getty Images A startup out of Paris that began life building marketing tools has raised $22 million after making a successful pivot into billing — a space it discovered was even more broken among potential customers. Lago, developer of an open source billing platform, has picked up the funding across two rounds of funding it’s revealing to coincide with its official launch. It’s launching today, but it has been in business in closed beta for some time, during which it’s picked up a number of notable startups such as Mistral.ai, Together.ai and Juni as early customers. The company’s focus on open source is very intentional, said CEO Anh-Tho Chuong: It’s targeting developers who are looking for solutions to tailor their billing to fit whatever cutting-edge, creative new services they are cooking up, a gap in the market that Lago believes is not being addressed well enough by incumbents, and which Lago is betting might best be addressed through an open source approach. “We’re a partner to developers,” she said in an interview. “We honor [their] abstraction, and we use data to meter usage, to help companies handle subscriptions [or other] pricing plans in an easy way.” A strong list of investors have taken note: The latest Series A of $15 million was led by FirstMark; the previous seed of $7 million was led by SignalFire, Chuong said. Other backers include Y Combinator, New Wave, and Script, as well as a number of individuals whose participation underscores the segment of the market Lago is targeting. They include MongoDB monetization head Meghan Gill; Romain Huet (previously of Stripe, now in developer relations at OpenAI); and Hugging Face CEO Clément Delangue. We understand from sources that its valuation is now around $100 million. The Lago doing business today as a billing platform got its start in a very classical startup way: It had no idea that it would be a billing platform. Chuong and her co-founder Raffi Sarkissian were both working at business banking startup Qonto when they decided to strike out on their own and build a new startup. They applied to Y Combinator and got into the Summer 2021 cohort on the strength of their backgrounds. “But we went to YC without a product,” she said. They settled while there on marketing, specifically around the idea of building a “Zapier for marketing teams.” “We honestly thought this was going to be a big one,” she recalled. “It was okay.” Okay was just not going to cut it though. Marketing tech is very crowded, the company was picking up almost no traction for its product. In a moment of trying to growth hack their way into an audience, Sarkissian decided to write a post for Hacker News in which he lamented the problems with billing for developers. It had a catchy title: “Billing systems are a nightmare for engineers,” and it was written with the kind of freedom you might only be able to capture as a creator when you are really writing from the heart. That was because it was something he and Chuong knew well, since their time at Qonto was spent building a product to address that very issue. Chuong said this was really not the point. The impetus for posting on this was to pick something they knew well so that they could monitor engagement and possibly use that to pivot attention to Lago, the “Zapier for marketing teams.” But the post struck a nerve, and to their surprise a lot of people began to speak up about their own billing issues. Lago had its “a-ha” moment: If what they really wanted to do was build something to solve a problem for developers, here was a problem they actually could solve, and they knew they could do it well. Cue the pivot to billing and the startup taking off. Taking off not just with users, but also investors. “We first spotted Lago via HackerNews in early 2023: They had so much traction, for a seemingly solved or trivial topic, that it seemed obvious people had been waiting for an open source solution like theirs. We then reached out to their stargazers on GitHub and the feedback was nothing but stellar,” said Oana Olteanu, a partner at SignalFire, in a statement. If you think marketing is a competitive market, billing is more crowded than Billingsgate Fish Market on a Friday morning. Larger tech companies like Stripe, Adyen, Salesforce, Zoho, Paddle and many more offer billing solutions. There are even a number of providers already pursuing an open source approach, including FOSSBilling, ChargeBee, Kill Bill, AppDirect’s jBilling and the imaginatively named “Open Source Billing.” (Why beat around the bush?) Chuong believes that there is still very much an opportunity, however, for a focus on extensibility and tailored solutions for startups trying to push the boundaries in their own competitive spaces. The AI sector is a strong example of that, in her view. Companies building AI-based products are still working out what the viable business models will be, and in the meantime we are seeing a lot of examples of companies considering hybrid approaches, mixing elements of flat-rate subscriptions with consumption-based pricing. All of this is tricky to manage and relies on tools that can integrate with whatever developers are building, with the ability to discern and apply their usage data. “If you have very simple pricing and billing, there are a lot of those solutions around for that, but for complex billing, there has been no solution,” she said. That leads to many companies (as Qonto did) building their own solutions. “But engineers hate it. And it’s very expensive to hire engineers for that, obviously. So it’s still an unsolved problem.” In Lago’s view, offering open source tools is that best solution to meet a variety of needs and ideas. For some of those users, the open source ethos also lines up with what they are hoping to espouse themselves as businesses. “We chose Lago as our billing provider because we believe in the open-source ecosystem,” Timothée Lacroix, co-founder and CTO at Mistral.ai, said in a statement. “They have been able to follow the pace of our releases and have allowed us to focus on what we do best.” Yes, some will argue that open source might be getting stretched a lot as a concept right now, and may well be exactly the opposite of what many so-called open source companies are building. The company’s aim is to continue building out its existing business but also to start considering what more it might add next. One obvious area is to dip back into Lago’s original thoughts about marketing, and to provide more data analytics to customers about what people are consuming and paying for, and the patterns of their payments. Another is to explore the other side of the billing coin: payments. Lago is unlikely to build a payments stack; however, she added: The focus is almost certainly going to be on payments orchestration, giving users control over what they use but make sure that it integrates well with their billing platform (one that is, ideally, going to be covered by Lago of course). Updated to remove the name of an investor that confirmed it is not involved in the funding. Please login to comment Login / Create Account TechCrunch Early Stage 2024 April 25, BostonFounder Summit LEARN MORE Sign up for Newsletters See all newsletters(opens in a new window) Daily News Week in Review Startups Weekly Event Updates Advertising Updates By subscribing, you are agreeing to Yahoo's Terms and Privacy Policy. Email Subscribe (opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Copy Tags billing Firstmark lago open source Y Combinator",
    "commentLink": "https://news.ycombinator.com/item?id=39957607",
    "commentBody": "Lago, Open-Source Stripe Alternative, banks $22M in funding (techcrunch.com)111 points by Rafsark 7 hours agohidepastfavorite44 comments plantain 2 hours agoI tried to use it for a new SaaS product - but their plans start at 3000$/mo. It seems like they have things backwards. The small fry like me don't want to self-host, we want a managed solution. The big fishes have enough scale to self-host. reply boudin 4 minutes agoparentThey have things backwards only if their strategy is to focus on the low end of the market. It doesn't seem to be the case here. reply plantain 2 minutes agorootparentThe big end of the market starts at the small end with a few large VC-funded exceptions... reply quartesixte 1 hour agoparentprevMedium Trout may be the intended market? reply darylteo 4 hours agoprevSelling to developers is where i know it will struggle... Developers are cheap ass and would rather build it themselves at 10x the opportunity cost. And the moment it attempts to monetise itself in some form, there'll be a massive \"betrayal\" exodus a la Redis. I know, cuz I am one. reply n2d4 2 hours agoparentTypical mind fallacy. I personally love paying for things that save me time, and I'm also a developer. Also, this is kind of their thesis, isn't it? That developers want open-source software to do their billing, which they can modify themselves if they need certain features instead of relying on a proprietary provider like Stripe. Doesn't sound too outlandish to me. reply colesantiago 2 hours agorootparentEveryone uses stripe though which is already 10000x easier than anything out there. The fact that even Amazon has switched to Stripe shows this. And that is not even getting into the PCI, PCI DSS, SOC, I and II, etc compliance soup of self hosting. reply chucke 10 minutes agorootparentFor payments, not for billing, right? I'd be very surprised otherwise, considering how Stripe's offer for billing is quite inflexible and (at least 6 months ago) offering experimental features. reply lucw 2 hours agoparentprevUsage based billing is tough. I took a long hard look at lago and ultimately it wasn't for me (B2C API based business). I couldn't do without the customer portal which is a premium feature, and premium was at least $1,500 USD/month. My revenue couldn't justify this. To Lago's credit, they are purposefully staying away from percentage pricing. So they have to charge a lot on the base price. Stripe Billing charges a percentage and guess what, with a growing business it's only a matter of time before your Stripe bill tops that $1.5k. I also looked at Stripe Billing for usage-based and it didn't meet my requirements either. (though I am using Stripe Billing with flat charges). My exact use case is: - I want to sell API credits upfront including for subscriptions (i.e. user signs up for $10/month of credits, they pay $10 upfront, and can spend $10 worth of credits). Stripe billing doesn't support charging upfront, for usage-based, they only bill after the billing period is finished. Some of my users have been gaming the system, canceling and not paying so that doesn't work for me. I believe Lago did support billing upfront. - I want to freely mix subscription based and pre-paid credits. My users go over their quota one month, they don't want to upgrade their subscription to the highest tier, they prefer a one-off top-up. I need to control over which credit gets consumed first. Stripe billing and Lago both had issues with this, I can't remember exactly what. - I wanted to support as many payment methods as possible, and particularly Chinese wallets for pre-paid credits (Alipay, Wechat). Lago has no plans to implement this, I was half considering implementing it myself inside Lago. I don't think Wechat and Alipay will matter much for B2B businesses. - I'm also a huge fan of massively regression tested code, and Stripe Billing with its test clocks blows Lago out of the water here. Lago has no ability to walk the clock forward for subscription lifecycle testing. Though maybe this matters less if you have faith in the product, you can just expect to get the right callbacks on time. I did notice that the Lago devs on slack took time to answer questions down to the deepest technical level. If I was running a B2B startup, I would probably try really hard to fit Lago in the picture, particularly at at time when stories of Stripe account bans are so prominent. reply baxtr 3 hours agoparentprevThey probably didn’t get the level of funding if they haven’t already solved this. reply that_guy_iain 37 minutes agorootparentDevelopers aren’t that cheap. Plus they’re not the decision makers. reply IshKebab 3 hours agorootparentprevNah there are tons of startups with funding and only vague plans for actually making money. reply pooper 3 hours agorootparentI don't know anything about funding but most of the funding came in the before times, when we had a zero interest rate policy, right? In post zirp, would even Y Combinator darling Dropbox or Airbnb be able to secure funding (without a solid plan)? reply ku1ik 2 hours agoprevMaybe I’m old and my feeling for what open-source means haven’t adapted to the changing reality, but whenever I see “open-source” and “$22 in funding” in one sentence I immediately think “open-source my ass”. reply gigatree 1 hour agoparentIs there a guide somewhere about how someone’s supposed to create open source software without monetizing it? Or is it just the VC money that makes it not real open source? I’ve seen this sentiment a lot, especially from OSS veterans like rich harris (who ironically now gets his paycheck from VC money). On one hand I want to complain about it too and say that people should only build open software for the love of building and sharing, but on the other hand it costs a lot of money to exist in meat space and it seems counterproductive and unfair to expect people to build software I find useful (and most likely even profit off of myself) on nights and weekends and get paid in GitHub stars. reply TheCapeGreek 1 hour agoparentprevI agree with you, but at the same time what's the alternative? Build it anyway on free time, beg for scraps in donations, and have $corporate sell it as a service while giving nothing back? I don't understand the benefit of OSS in Lago's context other than PR/developer goodwill. It's a catch-22 when building in open source nowadays, so if this is the future of it with the benefit of things having better longevity and support, so be it. reply mvkel 5 hours agoprevIf I still have to pay the processing fees, what's the advantage here? Having to maintain my own payments stack (and PCI compliance) sounds like a massive distraction. reply btown 5 hours agoparentThis is an alternative to Stripe Billing, not the core Stripe payment rails themselves. In fact, one would use Stripe or similar with Lago: https://docs.getlago.com/guide/payments/overview . Tracking the edge cases around recurring payments, invoicing, pro-rated tier changes, and metered billing is hard! And Stripe Billing APIs aren’t the most fluent for many cases. It’s good to see new layers in the space. reply lucw 2 hours agorootparentRight Lago could be used as replacement to Stripe Billing. And one attractive feature is that it decouples your tech stack from Stripe (could use another payment processor and still handle the subscription lifecycle the same way). This is going to be more and more interesting to businesses who are getting increasingly wary of Stripe account bans. reply squigglydonut 4 hours agorootparentprevWhat are you talking about Stripe is great and really well made. I just built on it and was able to do whatever I wanted. Lago looks like just one more platform I'd have to keep track of for no reason. reply TheCapeGreek 1 hour agorootparentThen it's evidently not for you, if the use case benefit isn't apparent? Same reason you personally wouldn't use Chargebee but others might. reply hanniabu 3 hours agoparentprevYet people will still claim blockchain has no usecase reply IshKebab 2 hours agorootparentYep. Nothing here seems to contradict that, unless you have found some magical cryptocurrency that has stable value, is easy enough for my mum to use, doesn't have the security properties of stuffing cash in a mattress (and posting a sign outside your house saying \"cash here!\"), is properly regulated, allows chargebacks, has fraud protections, etc. etc. No? Didn't think so. reply EGreg 2 hours agorootparentYes. I have. It’s called USDT. There’s also USDC. And we built blockchain apps that let anyone release their own at https://intercoin.org All those other properties you mentioned are not essential to being used as a medium of exchange. People can make their own conflict resolution procedures without involving the daddy government. Your idea of “security properties” is that a bank holds your money, well around the world people get their assets frozen by these banks. reply IshKebab 2 hours agorootparent> People can make their own conflict resolution procedures without involving the daddy government. Who's going to pay for that? Or enforce it? Can you give me a single example of this? Pure delusion. > around the world people get their assets frozen by these banks Good point, that's another useful feature real banks have that you can't do with cryptocurrency. reply giorgioz 2 hours agoprevSimilarly there is https://hyperswitch.io open source written in Rust. Lago is written in Ruby. I found few other open source billing systems written in Java. Anyone knows anything written in nodeJS? reply abledon 5 hours agoprevTheir github README uses a Drake meme lol https://github.com/getlago/lago https://imgur.com/a/gsrhUXm I'd never thought I'd see Meme-ification of technical docs.... oh god reply ffsm8 3 hours agoparent> I'd never thought I'd see Meme-ification of technical docs.... oh god I guess you entered the industry after 2015? Even the most technical analysis like the Jepsen Reports which evaluated the technical claims of Cassandra DB etc used lots of memes between the technical bits. Presentations always had cat/dog pictures in them etc. reply tamimio 4 hours agoparentprev> I'd never thought I'd see Meme-ification of technical docs They always been, in different forms. I am glad to see them too, harmless and signal IQ/EQ, taking anything too seriously is not a good thing. reply leononame 3 hours agorootparentI also like seeing them. I know there are a lot of people who hate that, similar to cutesy error messages on web pages which I also happen to like. But in no way does this signal IQ/EQ. It's just a personal preference reply triyambakam 3 hours agoparentprevWeird that they have the panels mis ordered though reply pooper 2 hours agorootparentThis is what happens when marketing gets its nasty hands on things, I assume. > Oh, we have to put our brand on top. Except, it ruins the whole meme. reply grzeshru 5 hours agoparentprevMemes existed in technical docs for as long as technical people and docs existed. reply dotancohen 5 hours agoparentprevHave you not looked up recursion in K&R? I believe even Google was in on the joke once. reply leosanchez 3 hours agorootparent> I believe even Google was in on the joke once If you search for recursion. Google asks did you mean recursion. reply vram22 18 minutes agorootparentYup. https://jugad2.blogspot.com/2013/12/google-gets-recursion.ht... reply globalise83 2 hours agoprevParis seems to be absolutely on fire when it comes to producing new fintech startups. reply baxtr 3 hours agoprevThis is the HN post (mentioned in the article) that resulted in their pivot from marketing into billing: https://news.ycombinator.com/item?id=31424450 reply mooreds 5 hours agoprevRaised series A of 15M on a valuation (rumored, not announced) of 100M. 7M was seed, raised in 2023 according to crunch base. Sounds like they aren't trying to be an entire stripe replacement (from the last paragraph of the linked article). Anyway an interesting story of a successful startup pivot starring a passionate HN post. reply azophy_2 2 hours agoprevhave anyone using this encounter any legal issues, as this project use AGPL? reply w-ll 6 hours agoprev [–] whats the motive? reply jpcapdevila 5 hours agoparentFrom the readme this may be interesting: Event-based: if you can track it, you can charge for it; I see this being useful for usage based pricing. reply joe_guy 5 hours agoparentprev [–] This article reads like an ad. reply squigglydonut 4 hours agorootparent [–] My thoughts as well. My first thoughts was \"wow I guess anyone can just bribe techcrunch writers these days reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Enterprise Lago, a Paris-based startup, transitioned from marketing tools to an open source billing platform, securing a $22 million investment, targeting developers' needs in the market.",
      "The company's emphasis on tailored billing solutions for developers has drawn significant investors and early adopters, setting the stage for expansion in a competitive industry.",
      "Future plans include venturing into data analytics and payments orchestration, showcasing Lago's commitment to innovation within the billing sector."
    ],
    "commentSummary": [
      "Lago, a $22M funded open-source Stripe alternative, targets developers favoring open-source billing solutions despite criticism for high pricing for small businesses.",
      "The company sets itself apart from Stripe with a more adaptable billing system, though premium features are deemed expensive by some users.",
      "Discussions cover payment issues, testing, blockchain, stable cryptocurrencies, fraud protection with USDT and USDC, and the unconventional use of memes in technical documentation, along with various tech-related topics like Google's recursion joke and Paris' fintech success."
    ],
    "points": 111,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1712456376
  }
]
