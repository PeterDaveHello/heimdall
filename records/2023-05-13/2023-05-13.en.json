[
  {
    "id": 35912318,
    "timestamp": 1683869898,
    "title": "The Legend of Zelda: Tears of the Kingdom Release",
    "url": "https://www.zelda.com/tears-of-the-kingdom/",
    "hn_url": "http://news.ycombinator.com/item?id=35912318",
    "content": "Available now",
    "summary": "- The Legend of Zelda: Tears of the Kingdom is now available exclusively on Nintendo Switch, featuring new abilities and expansive landscapes for players to explore.\n- Limited-time icon elements are available, and informative \"Ask the Developer\" articles offer insights into the game.\n- Nintendo Switch Online members can save on the game with Nintendo Switch Game Vouchers, and players can earn 100 My Nintendo Platinum Points through a website-based activity.",
    "hn_title": "The Legend of Zelda: Tears of the Kingdom Release",
    "original_title": "The Legend of Zelda: Tears of the Kingdom Release",
    "score": 609,
    "hn_content": "The Legend of Zelda: Tears of the Kingdom has been released, but a commenter on Hacker News finds it difficult to get excited about it, feeling that it largely copies the formula of Breath of the Wild. Other commenters discuss the impact of nostalgia on gaming preferences and recommend various Star Wars content. There are also discussions around the different styles and mechanics of games, with some preferring the responsive controls of Breath of the Wild over the sluggishness of The Witcher 3, while others appreciate the depth and character of the latter. Some commenters express their love for Ocarina of Time and the emotional connection it held for them as kids.A series of comments about personal experiences with The Legend of Zelda franchise, particularly comparing the older games with the more recent Breath of the Wild release. Some people find the open-world, non-linear gameplay of Breath of the Wild to be refreshing and exciting, while others prefer the more structured, puzzle-based approach of earlier games in the series. Nostalgia is a factor for some commenters, but overall, the consensus is that both new and old players can enjoy various games in the franchise for different reasons.Commenters on a Hacker News thread discuss their opinions on the open-world game design of The Legend of Zelda: Breath of the Wild. Some commenters express disappointment in the game due to the repetitive nature of its shrines and the relative emptiness of its open world compared to other games. Others defend the game and its departure from traditional Zelda gameplay, emphasizing the importance of branching out and trying new game mechanics. Additionally, some commenters discuss the benefits of owning a Nintendo Switch, praising its game library and loading speed.Readers discuss load times on PS5 and comparison with Switch games. Some users have uncapped gigabit internet while others are stuck with providers like Comcast. Debate takes place on whether to play BOTW on Deck or Switch, but most agree that emulating games comes with its own set of issues. The upcoming Logitech handheld is expected to perform better in regards to emulation. Emulators are not perfect and can sometimes fail leading to loss of saved game progress. The Switch is lighter than the Steam Deck but the latter is meant to serve a different function.A discussion about the reliability of emulators and the loss of saved games on various devices was briefly discussed. The main point of interest for readers appeared to be the technical achievements of various video games, including the upcoming Horizon: Forbidden West, Death Stranding, and Elden Ring. Some readers were critical of the characters and storytelling, while others defended certain aspects. The Nintendo Switch was also a topic of discussion, with some praising its hardware and gaming library, while others felt that it was outdated and overpriced. The idea of a little box to play Switch games on a PC was raised, but it was unlikely that Nintendo would allow such a loss of control.Some users are complaining about the Switch's hardware, calling it underpowered. Others argue that Nintendo always thrives on cost-effective hardware, and that a 2x more powerful Switch would have better battery life and could run games at a higher quality. Most agree that Nintendo should release an updated switch that is fully backwards compatible with the existing Switch library. Some games, like Zelda, have a frame rate problem which could be fixed by a doubling of the framerate. However, other games like Mario Kart, Smash, and Splatoon run at 60fps.The discussion centers around the importance of consistent framerate in games and how the limitations of the Switch's hardware can negatively impact gameplay. Some commenters argue that 30 FPS can be adequate if implemented well, while others advocate for the benefits of 60 FPS. Emulation is also discussed as a possible solution, and the challenges of creating backwards compatibility for a new Switch console are explored. Overall, the importance of providing a smooth gaming experience is highlighted as a key concern for players.Nintendo may have skipped a \"switch pro\" due to no usable chip successor, COVID, chip shortages, and the CPU market stalling. The company made the financially wise decision to extend the Switch's life and release just an OLED upgrade. The Switch's similarity to other consoles has allowed smaller studios to create more third-party games for the Switch. Emulation is possible, but it is not the same as playing a fully tested game the way developers intended. People continue to buy the Switch years after their initial purchase to play new games. While some people complain about the Switch being underpowered, it's possible to create innovative and fun games on something as powerful as the Switch. Nintendo has always focused more on creating fun games rather than having cutting-edge hardware.The discussion revolves around the potential for Nintendo to release their own switch emulator dongle for PC, with some arguing that it would invite people to leave Nintendo's ecosystem while others suggest that it could be an effective way for Nintendo to sell its hardware for the legitimate use of their emulation software. While the discussion mentions the high bar required for Switch piracy, others point out that piracy will never be stopped. Some argue that Nintendo's approach of pursuing piracy aggressively has more to do with maintaining its reliable and trustworthy curated experience, which is a unique aspect of the Nintendo ecosystem. Finally, the argument also touches on the issue of low FPS levels of Switch's core product at launch.The comments section discusses Nintendo's hardware philosophy, past consoles, current hardware limitations, and the success of the Switch. Readers debate the importance of graphics, gameplay, and mobile gaming. Emulation and Steam Deck are compared to the Switch. The thread highlights the value of Nintendo's focus on playable content and affordability, which has contributed to the company's continued success. Overall, the post provides insight into Nintendo's unique approach to hardware and their emphasis on creating fun and memorable gaming experiences.Users discuss the possibility of Nintendo releasing a little box that plays their games on a PC, similar to Sony's PC ports of exclusive titles. Some users mention that while the Steam Deck allows for better performance of emulated Switch games, the battery life is significantly worse than the Switch. Some users discuss the ethical dilemma of piracy and buying legitimate copies of games, with some saying they would buy the game on Switch even if they plan to play it on PC. Users also mention the technical aspects of emulating games and modding them on PC. Finally, users note that Nintendo is resistant to change and compromise, but has a history of hardware innovation. The original post mentions that the Nintendo eShop has new sales running every week.People have shared their experiences playing \"Tales of Arise\" on the Nintendo Switch versus a monster PC, with some preferring the Switch experience due to emulation issues and frame rate on PC. Discussion also includes legally playing Switch titles on PC, but with the caveat that making backups and circumventing DRM is legally gray. Some speculate that allowing games to be played on any old hardware would lead to a poor, inconsistent experience that could harm the reputation of consoles. Others note that playing on PC can offer higher resolutions and smoother frame rates. Overall, the conversation revolves around the pros and cons of playing current-gen console games on different hardware, such as the Switch versus a high-end PC.",
    "hn_summary": "- Many readers discuss the importance of consistent framerate and the limitations of the Switch's hardware, along with the potential benefits of a more powerful Switch.\n- Emulation, Steam Deck, and the Nintendo ecosystem are also discussed, with readers sympathizing with Nintendo's unique hardware approach and focus on playability."
  },
  {
    "id": 35916064,
    "timestamp": 1683898665,
    "title": "Implement DNS in a Weekend",
    "url": "https://implement-dns.wizardzines.com/",
    "hn_url": "http://news.ycombinator.com/item?id=35916064",
    "content": "Implement DNS in a weekendHello! Our goal here is to implement a toy DNS resolver.What\u2019s a DNS resolver? It\u2019s a program that knows how to figure out what the IP address for a domain is. Here\u2019s what the command line interface of the resolver we\u2019re going to write looks like:$ python3 resolve.py example.com93.184.216.34The whole thing is about 200 lines of Python, including implementing all of the binary DNS parsing from scratch.This project is a fun way to learn:How to parse a binary network protocol like DNSHow DNS works under the hood (what\u2019s happening behind the scenes when you make a DNS query?)There are also some bonus exercises if you want to implement a few more of the features a real DNS resolver would have.Everything is in a Jupyter notebook, which you can download and run. You can download all the code here:Download the code",
    "summary": "- A DNS resolver is a program that finds the IP address for a domain.\n- This project teaches how to parse a binary network protocol like DNS and how DNS works behind the scenes. \n- The project also includes bonus exercises to further enhance the implementation of a real DNS resolver.",
    "hn_title": "Implement DNS in a Weekend",
    "original_title": "Implement DNS in a Weekend",
    "score": 566,
    "hn_content": "Julia Evans' zine on implementing a DNS resolver inspired readers to share their experiences and similar projects. DNS is a fundamental internet protocol that is often taken for granted. The zine teaches readers how to implement a simple DNS resolver, and readers can follow along with the Jupyter notebook. The zine provides insight into DNS and builds readers' understanding of DNS resolution. Readers can also easily transform the command line tool into a UDP server running on localhost. In the comments, readers shared their own DNS projects as well as other educational projects focused on writing your own technology from scratch.No meaningful content for this post.",
    "hn_summary": "- The zine teaches readers how to implement a simple DNS resolver, providing insight into DNS and building readers' understanding of DNS resolution.\n- Readers are sharing their own DNS projects as well as other educational projects focused on writing your own technology from scratch."
  },
  {
    "id": 35920336,
    "timestamp": 1683916719,
    "title": "The .zip TLD sucks",
    "url": "https://financialstatement.zip/",
    "hn_url": "http://news.ycombinator.com/item?id=35920336",
    "content": "The .zip TLD sucks and it needs to be immediately revokedIf you clicked on this domain by mistakeThis shouldn't be allowed to happen. You might have been tricked into clicking this, assuming that the .zip in the URL was a filename. This is, of course, how it's been for decades. .zip isn't a valid part of a domain name! Except that Google has changed that.On the design mistake of the .zip TLDThroughout the 2010s, Google has easily been one of the most insidiously corrupting forces on the internet, rivaled by none. Its takeover of the modern web through utter domination of the search engine market, chokehold over web standards, and near complete monopolization of web browsers has rendered much of the world beholden to it.As of May 3rd, Google has also decided to add a whole new dimension to the layers of evil and/or incompetence. You can now purchase .zip and .mov domain names, like the one this page resides on! Isn't that just fun for the entire family? And by entire family, I mainly mean poor ol' grandma, because in what universe will people less versed in this news expect for a link ending in .mov to actually take them to a website? There is nearly no way for the average person to learn this, outside of finding out the hard way.As it stands, this is certainly not one of the most egregious things Google has done as of yet, but it is telling of just how bad we as a society have allowed things to get. The people who care are asleep at the wheel at best, some aren't with us anymore, and ICANN has failed all of us by allowing this to happen. For decades engineers have been working hard to try and make the internet less susceptible to phishing attacks, look-alike domains, etc., and now money men have decided to unravel that work so somebody can purchase anyword.zip as a domain name.There is only one correct solution to this. It's to completely remove any and all of these egregious filename extension TLDs with no questions asked, and punish the people who pushed for this. I'll only consider this problem solved whenever this domain becomes completely unreachable and non-usable.Shame on @google, and if they had any trace remembrance of the idea of shame before profit, they would stop registering new .zip domains. I may sound like a ghoul but maybe you do not understand the fundamental undermining this seemingly simple incursion has on user expectation. - @SwiftOnSecurityMy website",
    "summary": "- Google has made a mistake by allowing people to purchase domain names with .zip and .mov extensions, which can confuse users who click on them thinking they are file names.\n- The engineers who have been trying to make the internet less prone to phishing attacks are unhappy that ICANN has allowed this to happen.\n- The solution is to remove any and all of these filename extension TLDs and punish the people who pushed for this.",
    "hn_title": "The .zip TLD sucks",
    "original_title": "The .zip TLD sucks",
    "score": 552,
    "hn_content": "The debate over the significance of the .zip top-level domain (TLD) was sparked by a hacker news post, in which users expressed their opinions on whether or not the \".zip\" in a URL mattered. Some argue that the majority of non-technical people know what a .zip file is and that issues with .zip TLDs are the least of their concerns. Others cited the problem of hidden file extensions and scams that could easily masquerade as something else. In general, users expressed frustration at the trend of hiding fundamental layers of the system from users. Despite the debate, there is no new or exciting tech development in this post.Google's move to create a new .zip top-level domain (TLD) has raised security concerns due to confusion between a .zip file extension and a TLD. The former is an archive format, while the latter is a domain identifier. Non-technical web users may mistake them for one another, opening the door to phishing scams. Many email services and chat applications automatically create links out of typed domain names or file names mentioned in chats, making URLs with the .zip TLD more vulnerable to exploitation. Google's solution will likely lead to more confusion and make it easier for scammers to deceive users with .zip files. While .zip is one of the safer TLD choices, it is not without risks.The post discusses the potential dangers of hyperlinking a filename with a nonexistent TLD (such as .zip). This could lead to unknowingly downloading malware or being directed to a phishing site. There is debate over whether limiting the number of TLDs would help solve this problem or if it's an issue with the omnibar in browsers. The comments also mention nostalgic file extensions from older operating systems. Overall, the post serves as a cautionary tale and highlights the importance of being cautious when clicking on links or downloading files.Google's new generic top-level domain (gTLD) \".zip\" is raising concerns about potential phishing attacks since file extensions and TLDs can be confused. Google has applied for several gTLDs since 2012, including .zip, and has control over the domain registrar. However, some users may mistake a .zip domain as a file extension and, therefore, will be more susceptible to phishing attacks, which may cause harm to users. Although many argue that it's no different from any other TLD and present a relatively small threat to users, others say it provides a unique situation that creates an unnecessary danger. So far, Google hasn't responded to these concerns, and it is unclear what the company's motivation is behind pursuing the .zip TLD.A debate has sparked over the addition of the .zip top-level domain (TLD), with critics claiming it is confusing and could be a security risk because users might associate it with the common archive file type. However, others argue that file extensions have become too closely tied to content, and websites have been \"showing much more prominently than the file itself\" for some time. Additionally, skeptics believe that this could be a thin end of the wedge towards other potentially confusing TLDs, such as .exe. Meanwhile, users have shared personal anecdotes about how a move to .dev has proved beneficial after internal domain name clashes using .test and .local.ICANN's approval of the .zip top-level domain (TLD) is criticized as a \"daft idea\" that \"adds more confusion\" to the web space, as zip files are known to be a key malware and hacking vector. The comments led to a broader discussion about legacy domain name systems (DNS) and ICANN's questionable TLD approvals. The move to apply to Y Combinator (YC) summer 2023 is open and available for application.",
    "hn_summary": "\n- The debate over the .zip TLD highlights the importance of being cautious when clicking on links or downloading files.\n- Critics argue that the .zip TLD is confusing and could lead to unnecessary security risks, while others believe file extensions have become too closely tied to content and are not a significant concern."
  },
  {
    "id": 35919133,
    "timestamp": 1683912067,
    "title": "Toyota: Car location data & videos of 2M customers exposed for ten years",
    "url": "https://www.bleepingcomputer.com/news/security/toyota-car-location-data-of-2-million-customers-exposed-for-ten-years/",
    "hn_url": "http://news.ycombinator.com/item?id=35919133",
    "content": "Toyota: Car location data of 2 million customers exposed for ten yearsBy Bill ToulasMay 12, 2023 10:50 AM 1Toyota Motor Corporation disclosed a data breach on its cloud environment that exposed the car-location information of 2,150,000 customers for ten years, between November 6, 2013, and April 17, 2023.According to a security notice published in the company's Japanese newsroom, the data breach resulted from a database misconfiguration that allowed anyone to access its contents without a password.\"It was discovered that part of the data that Toyota Motor Corporation entrusted to Toyota Connected Corporation to manage had been made public due to misconfiguration of the cloud environment,\" reads the notice (machine translated).\"After the discovery of this matter, we have implemented measures to block access from the outside, but we are continuing to conduct investigations, including all cloud environments managed by TC. We apologize for causing great inconvenience and concern to our customers and related parties.\"Exposed car location and videosThis incident exposed the information of customers who used the company's T-Connect G-Link, G-Link Lite, or G-BOOK services between January 2, 2012, and April 17, 2023.T-Connect is Toyota's in-car smart service for voice assistance, customer service support, car status and management, and on-road emergency help.The information exposed in the misconfigured database includes:the in-vehicle GPS navigation terminal ID number,the chassis number, andvehicle location information with time data.While there is no evidence that the data was misused, unauthorized users could have accessed the historical data and possibly the real-time location of 2.15 million Toyota cars.It is important to note that the exposed details do not constitute personally identifiable information, so it wouldn't be possible to use this data leak to track individuals unless the attacker knew the VIN (vehicle identification number) of their target's car.A car's VIN, also known as chassis number, is easily accessible, so someone with enough motivation and physical access to a target's car could theoretically have exploited the decade-long data leak for location tracking.A second Toyota statement published on the Japanese 'Toyota Connected' site also mentions the possibility of video recordings taken outside the vehicle having been exposed in this incident.The exposure period for these recordings was defined between November 14, 2016, and April 4, 2023, which is nearly seven years.Again, the exposure of these videos would not severely impact the car owners' privacy, but this depends on the conditions, time, and location.Toyota has promised to send individual apology notices to impacted customers and set up a dedicated call center to handle their queries and requests.In October 2022, Toyota informed its customers of another lengthy data breach resulting from exposing a T-Connect customer database access key on a public GitHub repository.This enabled an unauthorized third party to access the details of 296,019 customers between December 2017 and September 15, 2022, when external unauthorized access to the GitHub repository was restricted.Related Articles:PrestaShop fixes bug that lets any backend user delete databasesKodi discloses data breach after forum database for sale onlineLatitude Financial data breach now impacts 14 million customersLockBit ransomware gang now also claims City of Oakland breachHacker selling data allegedly stolen in US Marshals Service hack",
    "summary": "- Toyota Motor Corporation experienced a data breach that exposed the car location information of 2.15 million customers for 10 years.\n- The data breach resulted from a database misconfiguration that allowed anyone to access its contents without a password.\n- The exposed details do not constitute personally identifiable information, so it wouldn't be possible to use this data leak to track individuals unless the attacker knew the VIN (vehicle identification number) of their target's car.",
    "hn_title": "Toyota: Car location data and videos of 2M customers exposed for ten years",
    "original_title": "Toyota: Car location data and videos of 2M customers exposed for ten years",
    "score": 517,
    "hn_content": "Toyota has exposed videos and car location data from its customer database of about 2 million Toyota and Lexus customers. The data, collected for telematics services, was not deemed personally identifiable; however, individuals could be tracked if their attackers had access to their vehicle identification number (VIN). The data had been visible to a third party data broker since 2011 until recently exposed by a UK cybersecurity consultancy. This news ignites privacy concerns and renews focus on data protection laws, especially as Toyota faces potential lawsuits and public backlash. Toyota has apologized for the breach and claims to have closed the access point.A Honda motorcycle privacy policy allows the sharing of geolocation data with third parties without user consent, with the information anonymized. The policy considers some major companies to be service providers, and anonymizes customer data, which may qualify as a sale under applicable laws. The agreement allows information to be shared with law enforcement officials to prevent illegal activity or to comply with law. The provision of data to cloud-based external APIs, external video capture, and usage-based auto insurance is listed as an opt-in. The European privacy policies of Toyota are different from those in the US. Some commentators are skeptical about Honda's anonymization standards and the reliability of anonymized metadata. Legislators should impose strict fines on unwanted privacy infringements.Toyota revealed a data breach that exposed the car-location data of 2.1 million customers for almost ten years, which led to suggestions about disabling eCall, a built-in emergency assistance system in European cars. It's unclear if Toyota has plans to disable eCall as a result of this data breach. Security experts have recommended that owners may wish to disable the system instead of relying on opt-out mechanisms. European eCall is a homologation requirement and cannot be disabled once a car is on the road. VIN numbers were also exposed, which together with GPS data could easily identify the owner. The breach raises questions about automotive control system design, software quality assurance, and cyber security.Toyota's data tracking feature is causing concern among customers. Some have found ways to disable it, but others don't seem to care. The discussion around data privacy and the potential abuse of personal data has continued to grow, with some calling for regulators to hold companies accountable. The industry's focus on minimizing costs and maximizing manufacturing efficiency means that software development can suffer, and consumers may be left without the best possible product. In the automotive industry, many companies are lagging behind in the electric vehicle transition. However, plug-in hybrid technology could provide a way to reduce emissions while still allowing for long-distance travel. Overall, there are concerns about the use of personal data and the quality of software in the automotive industry, but there are also potential solutions for reducing emissions.- Fossil fuel ground transportation has no future and we should electrify our major highways.\n- There are several ways to electrify highways, but induction is the least practical and most expensive.\n- PHEVs have downsides like being heavy and having complex maintenance systems, but they provide a \"sweet spot\" for US drivers.\n- EVs have fewer parts and require less maintenance than ICEs and hybrids.\n- Toyota Prius Prime's warranty and availability of used ones older than 10 years suggest a handle on the system's complexity.\n- Regen braking in EVs results in fewer new brake pads/discs.\n- Hybrid vehicles not having a spare tire is not an outlier, and EVs have downsides too.\n- A robotics research project at Toyota Infotechnology Center shows their resistance to software contributions.\n- American companies like Rivian, Tesla, Lucid, Gravity, Ford, and GMC mostly get their batteries from Japan, Korea, and China.\n- The Japanese auto industry is behind in transitioning to EVs, while China's EV push may ultimately create a massive new resource dependency.Toyota's T-Connect G-Link, G-Link Lite, and G-BOOK services experienced a data leak, exposing the details of customers who used the services. This leak contains information such as location data, account login information, driver's license, financial data and it triggered questions of privacy and vehicle data tracking. Users are uncertain of what data their cars are sending back to manufacturers. While some see the leak as a result of Toyota wasting resources and time on hydrogen-fueled electric vehicles, others believe that betting against the EV battery is like betting against CMOS. It is also unclear which cars have the telemetry module or if the data is pushed through it. As a result, people are showing interest in learning how to stop vehicles from collecting data and are considering old-fashioned cars without internet or satellite connections.- Some Toyota vehicles contain a DCM radio that tracks a driver's location and sends that data to Toyota.\n- The commenter plans to disconnect the DCM in their new car to avoid being tracked.\n- Older Toyota models (pre-2016) do not have DCM radios and are not equipped with trackers.\n- A commenter mentions that they do not carry a cellphone and therefore the information about the tracking is important to them.\n- The post may be of interest to individuals concerned about privacy and tracking in their vehicles.",
    "hn_summary": "- Concerns about privacy, data protection laws, and potential lawsuits have arisen due to the data breach.\n- The breach raises questions about automotive control system design, software quality assurance, and cybersecurity."
  },
  {
    "id": 35917912,
    "timestamp": 1683906680,
    "title": "Linda Yaccarino is the new CEO of Twitter",
    "url": "https://twitter.com/elonmusk/status/1657050349608501249",
    "hn_url": "http://news.ycombinator.com/item?id=35917912",
    "content": "This browser is no longer supported.Please switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.Help CenterTerms of Service Privacy Policy Cookie Policy Imprint Ads info \u00a9 2023 X Corp.",
    "summary": "- Elon Musk announces Linda Yaccarino as new CEO of Twitter\n- Yaccarino to focus on business operations, Musk on product design and new technology\n- Twitter to be transformed into \"X,\" an everything app",
    "hn_title": "Linda Yaccarino is the new CEO of Twitter",
    "original_title": "Linda Yaccarino is the new CEO of Twitter",
    "score": 449,
    "hn_content": "Linda Yaccarino has been appointed as the new CEO of Twitter. Elon Musk stated that while she will focus primarily on business operations, he will continue to decide the product strategy. Musk's vision of \"free speech\" is in direct opposition to brand-safe content that advertisers want. The biggest advertisers, including Apple and Disney, have returned to the platform. Musk will shift to Executive Chairman and Chief Technology Officer while Yaccarino handles advertisers. The disaster with verification is driving away important Twitter posters that generated content on the website. Advertiser relations, product, and software are inseparable. Carlson's calls for invasion of Canada might create a risk for advertisers of his show. Musk's appointment of Yaccarino is viewed as critical for Twitter's success.Fox News, OAN, and Newsmax are exposed as selling coverage to advance a political and financial agenda, while Hannity was coordinating his show with the Trump White House; popular show host Tucker Carlson's controversial views suggest he represents low-level conservatism that corporate media owners won't tolerate, but he has a big pull; Elon Musk's hiring of former White House communications director Stephanie Grisham as the head of Twitter's marketing may indicate the Tesla CEO is looking to delegate organizational structure, sales, and finance operational, and strategic aspects that aren't directly related to Twitter's unique vision  to focus on Starship. SpaceX's Falcon 9 + Falcon Heavy platforms are vastly profitable.Elon Musk appoints an ad executive as CEO of Twitter, allowing him to focus on product development. Gwynne Shotwell's success at SpaceX exemplifies how a COO can work under a visionary CEO like Musk. The structure may work, but it puts the CEO in a difficult position. Musk's crucial role in decision-making means employees may be unsure of who the ultimate decider is. Musk's ownership stake at Twitter lets him essentially overrule anyone. Despite this, it is often easier to sell a company if there is a CEO, rather than a VP of Sales. Privately held companies generally have boards of directors, and there is a requirement to file paperwork publicly with the state. Some small corporations will even have a single-person board of directors, where that person is the 100% owner. Oracle is an example of a company that has had success with a structure similar to that of Twitter.Oracle, a company focused on acquiring new products and destroying them, has been the subject of criticism in recent comments on HN. While some former employees have described Oracle as a typical corporation, others see it as suffering from organizational dysfunction and an inability to adapt to market shifts. Twitter, which is at imminent risk of going out of business in the next 1-3 years, recently appointed a new CEO with a background in advertising. Musk's focus on Tesla's development rather than investing is contrasted with Larry Ellison's model of taking executive positions in Oracle and other startups. Some speculate that the CEO position at Twitter is a \"golden handcuffs\" move\u00a0with hefty financial incentivizations. Twitter has millions of dollars in unpaid bills that it is using to negotiate with vendors and landlords.The discussion revolves around the skill set of CEOs and COOs of various tech companies, including Apple, Meta, SpaceX, and Tesla, and the interference of CEOs in engineering and technical decision-making. Elon Musk's skills in engineering and software development are debated, with evidence provided from both sides. The ability of Musk and his companies to raise a significant amount of money and secure government subsidies is also discussed. Some participants express dislike towards Musk, while others defend his accomplishments and expertise.Elon Musk's acquisition of Twitter has raised questions about his leadership style and whether he is truly a product person. While some argue that Musk is simply a visionary and builder who wants autonomy and control, others question whether he is making improvements to Twitter as a technical product. Musk is facing pressure to make Twitter profitable, but his cuts to employment and the platform's relationship with advertisers have led to a decline in revenue and complaints about bugs and right-wing reply guys. Nevertheless, Musk continues to add features such as bookmarks, encrypted DMs, subscriptions, and longform text to Twitter, which he views as a business opportunity. However, many of these features were already in development before Musk bought the company.Elon Musk defends Twitter's stability and approaching cash-flow breakeven, even though ad revenue has fallen off a cliff and replies are mostly from right-wing reply guys. Musk aims to reverse the destruction of Twitter's relationship with advertisers and to turn it into \"X, the everything app,\" similar to WeChat. Twitter has been complying with more than 80% of government requests to remove or alter content since Musk's takeover. Musk doesn't have policies, only whims. Twitter's COO is effectively now the CEO.",
    "hn_summary": "- Musk's vision of \"free speech\" is in direct opposition to advertisers' desire for brand-safe content, with major brands returning to Twitter recently\n- Musk's ownership stake in Twitter gives him the ability to overrule any decision, and his appointment of Yaccarino is viewed as critical for Twitter's continued success"
  },
  {
    "id": 35920261,
    "timestamp": 1683916379,
    "title": "Morris Tanenbaum, inventor of the silicon transistor, has died",
    "url": "https://spectrum.ieee.org/in-memoriam-may-2023",
    "hn_url": "http://news.ycombinator.com/item?id=35920261",
    "content": "THE INSTITUTEARTICLEMorris Tanenbaum, Inventor of the Silicon Microchip, Dies at 94IEEE also remembers the lives and legacies of other membersAMANDA DAVIS14 HOURS AGO5 MIN READ",
    "summary": "- Morris Tanenbaum, the inventor of the silicon transistor, has passed away at the age of 94.\n- Tanenbaum's work on the silicon transistor led to the creation of the microchip, which revolutionized the electronics industry.\n- IEEE remembers Tanenbaum and other members for their contributions to the field of engineering.",
    "hn_title": "Morris Tanenbaum, inventor of the silicon transistor, has died",
    "original_title": "Morris Tanenbaum, inventor of the silicon transistor, has died",
    "score": 410,
    "hn_content": "Morris Tanenbaum, the inventor of the silicon transistor, has passed away. The number of transistors manufactured is estimated to be around 10^23. Transistors are physical things, and a CPU can have billions of them. The question of what we would be doing if transistors had not been discovered has been raised. Vacuum tubes, nano electromechMorris Tanenbaum, an innovative physical chemist and a significant transistor researcher, passed away, with many expressing their first time hearing of him. He made a significant impact on the computer industry, even though he's not as well-known as some other pioneers. Semiconductor manufacturing equipment is mainly made by European and Asian companies, contributing to the technology industry's progress. Fame isn't always fair and balanced in recognizing the importance of engineers' contributions. Morris Tanenbaum isn't related to Andrew S. Tanenbaum, the author of MINIX. Readers expressed their appreciation for Morris Tanenbaum's contributions to the field, which enabled them to pursue their careers.",
    "hn_summary": "- Transistors are physical things, and the number of transistors manufactured is estimated to be around 10^23; they are essential components in CPUs, and the question of what we would be doing if transistors had not been discovered has been raised\n- Readers expressed their appreciation for Morris Tanenbaum's contributions to the field, which enabled them to pursue their careers"
  },
  {
    "id": 35921375,
    "timestamp": 1683920939,
    "title": "GitHub Copilot Chat Leaked Prompt",
    "url": "https://twitter.com/marvinvonhagen/status/1657060506371346432",
    "hn_url": "http://news.ycombinator.com/item?id=35921375",
    "content": "This browser is no longer supported.Please switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.Help CenterTerms of Service Privacy Policy Cookie Policy Imprint Ads info \u00a9 2023 X Corp.",
    "summary": "- Microsoft releases early beta of GitHub Copilot Chat\n- Copilot Chat has a set of confidential rules governing its behavior and interactions\n- Rules include providing informative responses, adhering to technical information, and avoiding controversial discussions",
    "hn_title": "GitHub Copilot Chat Leaked Prompt",
    "original_title": "GitHub Copilot Chat Leaked Prompt",
    "score": 338,
    "hn_content": "GitHub Copilot, an AI programming assistant developed by GitHub and OpenAI, has caused a stir among users after a Twitter user shared chat prompts that allegedly led the tool to generate inappropriate and biased code suggestions. Some users have criticized the prompts for their second-person addressing and wrongly attributing authorship to the AI. However, others have suggested that they might be helpful in signalling to the AI what the user wants it to do. The prompts appear to be part of a fine-tuning process based on providing documents with instructions that Copilot can follow. OpenAI and GitHub have not commented on the issue, but Copilot has been praised for its coding capabilities and its effects on software development.OpenAI's natural language \"chat\" interfaces use a transformation on user input prompts to create a more natural transcription-style prompt, which is fed to the underlying model. The models are trained on text written by humans to respond and talk like a human. Instruction tuning and reinforcement learning with human feedback are both used to adjust the model's output. There are limitations in place to avoid any biases that are unacceptable in contemporary culture, but biases still exist in the model and affect the answers. It is debated whether the output being too US-centric and poorly localized can put off users as much as racial and sexist content. Properly trained, language learning models could communicate with people in their preferred mode of communication.OpenAI's ChatGPT model was trained to avoid praising Hitler and bias against certain identifiable groups. The definition of \"national origin\" may vary, but in general, it means a person's country of birth or the country of birth of their ancestors, which is fixed at the time of birth and cannot be changed by later acts of naturalization, renunciation, or deprivation. OpenAI's decision to suppress rather than align their AI models to avoid biases has raised concerns as bias can still show up in more insidious ways. Some argue that creating AI models with extremist biases is not a big risk because they will be vastly outnumbered by non-extremist AI models. Others argue that extremist AI models are a potential risk that needs addressing.The discussion revolves around the fundamental difference between freedom of expression, which is protected under human rights, and certain actions that can be executed using software. There is a debate on whether the use of AI for generating propaganda or synthesizing illegal substances should be considered a free expression. The leaked prompt from GPT-4 undermines the claim that AI models are unbiased and neutral. The conversation also explores the possibility of middleware that can scan the output of AI models for known text and filter it to prevent leakages. Some users suggest different methods to avoid leaking prompts, such as encoding the text in base64.A recent post on Hacker News discusses the ability of OpenAI's Copilot chatbot to recognize and respond to various prompts, including those encoded in Base64. Some commenters speculate that the prompt leak and subsequent discussion may have been orchestrated for publicity. Many also debate the significance of prompt engineering in terms of intellectual property, while others focus on the potential hallmarks of human versus machine-generated text. Some suggest that verifying the legitimacy of the prompt is easy by entering it into the GPT-3.5/4 API as a system prompt.The article discusses prompt injection, a potential vulnerability in language models that can result in the output of edited or leaked prompts rather than the original prompt. There is speculation about whether such leaks can result in hallucinations, but it is believed that the leaked prompt is generally close to the original. The article references other recent posts on the same topic by the author, which go into more detail and discuss potential solutions. There is some discussion about whether the use of copyrighted materials to train these language models is fair use, as well as speculation about how automatic paraphrasing of prompts could be achieved. Overall, there is a lot of interest in the potential security concerns around prompt leaks and the use of language models more generally.Researchers used a meta-prompt technique with GPT-3 to generate variations of seed prompts in order to ensure separation between models used for prompt creation and ranking, as detailed in Section 4. A simulation of conversation between users was also used to train the AI to behave appropriately. Some expressed skepticism about the possibility of the reported success and suggested the prompt's similarity to Bing may be due to similarities in the training data.",
    "hn_summary": "- OpenAI and GitHub have not commented on the issue, but Copilot has been praised for its coding capabilities and effects on software development.\n- Discussions center around bias in AI models, potential vulnerabilities from prompt leaks and prompt injection, and copyright issues in training language models."
  },
  {
    "id": 35920082,
    "timestamp": 1683915644,
    "title": "Windmill: Open-source developer platform to turn scripts into workflows & UIs",
    "url": "https://github.com/windmill-labs/windmill",
    "hn_url": "http://news.ycombinator.com/item?id=35920082",
    "content": ".Open-source developer infrastructure for internal tools. Self-hostable alternative to Airplane, Pipedream, Superblocks and a simplified Temporal with autogenerated UIsm and custom UIs to trigger workflows and scripts as internal apps. Scripts are turned into UIs and no-code modules, no-code modules can be composed into very rich flows, and script and flows can be triggered from internal UIs made with a low-code builder. The script languages supported are: Python, Typescript, Go, Bash, SQL.Try it - Docs - Discord - Hub - Contributor's guideWindmill - Turn scripts into workflows and UIs that you can share and run at scaleWindmill is fully open-sourced (AGPLv3) and Windmill Labs offers dedicated instance and commercial support and licenses.main.mp4Windmill - Turn scripts into workflows and UIs that you can share and run at scaleMain ConceptsShow me some actual script codeCLIRunning scripts locallyStackSecuritySandboxingSecrets, credentials and sensitive valuesPerformanceArchitectureHow to self-hostDocker composeKubernetes (k8s) and Helm chartsPostgres without superuserCommercial licenseOAuth for self-hostingResource typesEnvironment VariablesRun a local dev setuponly FrontendBackend + FrontendContributorsCopyrightMain ConceptsDefine a minimal and generic script in Python, Typescript, Go or Bash that solves a specific task. Here sending an email with SMTP. The code can be defined in the provided Web IDE or synchronized with your own github repo:Your scripts parameters are automatically parsed and generate a frontend.Make it flow! You can chain your scripts or scripts made by the community shared on WindmillHub.Build complex UI on top of your scripts and flows.Scripts and flows can also be triggered by a cron schedule '_/5 _ * * *' or through webhooks.You can build your entire infra on top of Windmill!Show me some actual script codeimport * as wmill from \"https://deno.land/x/windmill@v1.62.0/mod.ts\";//import any dependency from npmimport cowsay from \"npm:cowsay@1.5.0\";export async function main( a: number, // unions generate enums b: \"my\" | \"enum\", // default parameters prefill the field d = \"default arg\", // nested objects work c = { nested: \"object\" }, // permissioned and typed json db: wmill.Resource<\"postgresql\">) { const email = Deno.env.get(\"WM_EMAIL\"); // variables are permissioned and by path let variable = await wmill.getVariable(\"f/company-folder/my_secret\"); const lastTimeRun = await wmill.getState(); // logs are printed and always inspectable console.log(cowsay.say({ text: \"hello \" + email + \" \" + lastTimeRun })); await wmill.setState(Date.now()); // return is serialized as JSON return { foo: d, variable };}CLIWe have a powerful CLI to interact with the windmill platform and sync your scripts from local files, github repos and to run scripts and flows on the instance from local commands. See more detailsRunning scripts locallyYou can run your script locally easily, you simply need to pass the right environment variables for the wmill client library to fetch resources and variables from your instance if necessary. See more: https://docs.windmill.dev/docs/advanced/local_development/StackPostgres as the databasebackend in Rust with the following highly-available and horizontally scalable architecture:stateless API backendworkers that pull jobs from a queue in Postgres (and later, Kafka or Redis. Upvote #173 if interested )frontend in Sveltescripts executions are sandboxed using google's nsjailjavascript runtime is the deno_core rust library (which itself uses the rusty_v8 and hence V8 underneath)typescript runtime is denopython runtime is python3golang runtime is 1.19.1SecuritySandboxingWindmill uses nsjail on top of the deno sandboxing. It is production multi-tenant grade secure. Do not take our word for it, take fly.io's oneSecrets, credentials and sensitive valuesThere is one encryption key per workspace to encrypt the credentials and secrets stored in Windmill's K/V store.In addition, we strongly recommend that you encrypt the whole Postgres database. That is what we do at https://app.windmill.dev.PerformanceOnce a job started, there is no overhead compared to running the same script on the node with its corresponding runner (Deno/Go/Python/Bash). The added latency from a job being pulled from the queue, started, and then having its result sent back to the database is ~50ms. A typical lightweight deno job will take around 100ms total.ArchitectureHow to self-hostWe only provide docker-compose setup here. For more advanced setups, like compiling from source or using without a postgres super user, see documentationDocker composecurl https://raw.githubusercontent.com/windmill-labs/windmill/main/docker-compose.yml -o docker-compose.ymlcurl https://raw.githubusercontent.com/windmill-labs/windmill/main/Caddyfile -o Caddyfilecurl https://raw.githubusercontent.com/windmill-labs/windmill/main/.env -o .envdocker compose up -d --pull alwaysGo to http://localhost et voil\u00e0 :)The default super-admin user is: admin@windmill.dev / changemeFrom there, you can follow the setup app and creat other users.Kubernetes (k8s) and Helm chartsWe publish helm charts at: https://github.com/windmill-labs/windmill-helm-chartsPostgres without superuserIf you do not want, or cannot (for instance, in AWS Aurora or Cloud sql) use a postgres superuser, you can run ./init-db-as-superuser.sql to init the required users for windmill.Commercial licenseTo self-host Windmill, you must respect the terms of the AGPLv3 license which you do not need to worry about for personal uses. For business uses, you should be fine if you do not re-expose it in any way Windmill to your users and are comfortable with AGPLv3.To re-expose any Windmill parts to your users as a feature of your product, or to build a feature on top of Windmill, to comply with AGPLv3 your product must be AGPLv3 or you must get a commercial license. Contact us at ruben@windmill.dev if you have any doubts.In addition, a commercial license grants you a dedicated engineer to transition your current infrastructure to Windmill, support with tight SLA, and our global cache sync for high-performance/no dependency cache miss of cluster from 10+ nodes to 200+ nodes.OAuth for self-hostingTo get the same oauth integrations as Windmill Cloud, mount oauth.json with the following format:{ \"<client>\": {  \"id\": \"<CLIENT_ID>\",  \"secret\": \"<CLIENT_SECRET>\",  \"allowed_domains\": [\"windmill.dev\"] //restrict a client OAuth login to some domains }}and mount it at /usr/src/app/oauth.json.The redirect url for the oauth clients is: <instance_url>/user/login_callback/<client>The list of all possible \"connect an app\" oauth clientsTo add more \"connect an app\" OAuth clients to the Windmill project, read the Contributor's guide. We welcome contributions!You may also add your own custom OAuth2 IdP and OAuth2 Resource provider:{ \"<client>\": {  \"id\": \"<CLIENT_ID>\",  \"secret\": \"<CLIENT_SECRET>\",  // To add a new OAuth2 IdP  \"login_config\": {   \"auth_url\": \"<auth_endpoint>\",   \"token_url\": \"<token_endpoint>\",   \"userinfo_url\": \"<userinfo endpoint>\",   \"scopes\": [\"scope1\", \"scope2\"],   \"extra_params\": \"<if_needed>\"  },  // To add a new OAuth2 Resource  \"connect_config\": {   \"auth_url\": \"<auth_endpoint>\",   \"token_url\": \"<token_endpoint>\",   \"scopes\": [\"scope1\", \"scope2\"],   \"extra_params\": \"<if_needed>\"  } }}Resource typesYou will also want to import all the approved resource types from WindmillHub. A setup script will prompt you to have it being synced automatically everyday.Environment VariablesEnvironment Variable name Default Description Api Server/Worker/AllDATABASE_URL The Postgres database url. AllDISABLE_NSJAIL true Disable Nsjail Sandboxing WorkerSERVER_BIND_ADDR 0.0.0.0 IP Address on which to bind listening socket ServerPORT 8000 Exposed port ServerNUM_WORKERS 3 The number of worker per Worker instance (set to 1 on Eks to have 1 pod = 1 worker, set to 0 for an API only instance) WorkerDISABLE_SERVER false Binary would operate as a worker only instance WorkerMETRICS_ADDR None The socket addr at which to expose Prometheus metrics at the /metrics path. Set to \"true\" to expose it on port 8001 AllJSON_FMT false Output the logs in json format instead of logfmt AllBASE_URL http://localhost:8000 The base url that is exposed publicly to access your instance ServerBASE_INTERNAL_URL http://localhost:8000 The base url that is reachable by your workers to talk to the Servers. This help avoiding going through the external load balancer for VPC-internal requests. WorkerTIMEOUT 300 The maximum time of execution of a script. When reached, the job is failed as having timedout. WorkerZOMBIE_JOB_TIMEOUT 30 The timeout after which a job is considered to be zombie if the worker did not send pings about processing the job (every server check for zombie jobs every 30s) ServerRESTART_ZOMBIE_JOBS true If true then a zombie job is restarted (in-place with the same uuid and some logs), if false the zombie job is failed ServerSLEEP_QUEUE 50 The number of ms to sleep in between the last check for new jobs in the DB. It is multiplied by NUM_WORKERS such that in average, for one worker instance, there is one pull every SLEEP_QUEUE ms. WorkerMAX_LOG_SIZE 500000 The maximum number of characters a job can emit (log + result) WorkerDISABLE_NUSER false If Nsjail is enabled, disable the nsjail's clone_newuser setting WorkerKEEP_JOB_DIR false Keep the job directory after the job is done. Useful for debugging. WorkerLICENSE_KEY (EE only) None License key checked at startup for the Enterprise Edition of Windmill WorkerS3_CACHE_BUCKET (EE only) None The S3 bucket to sync the cache of the workers to WorkerTAR_CACHE_RATE (EE only) 100 The rate at which to tar the cache of the workers. 100 means every 100th job in average (uniformly randomly distributed). WorkerSLACK_SIGNING_SECRET None The signing secret of your Slack app. See Slack documentation ServerCOOKIE_DOMAIN None The domain of the cookie. If not set, the cookie will be set by the browser based on the full origin ServerDENO_PATH /usr/bin/deno The path to the deno binary. WorkerPYTHON_PATH /usr/local/bin/python3 The path to the python binary. WorkerGO_PATH /usr/bin/go The path to the go binary. WorkerGOPRIVATE The GOPRIVATE env variable to use private go modules WorkerNETRC The netrc content to use a private go registry WorkerPIP_INDEX_URL None The index url to pass for pip. WorkerPIP_EXTRA_INDEX_URL None The extra index url to pass to pip. WorkerPIP_TRUSTED_HOST None The trusted host to pass to pip. WorkerPATH None The path environment variable, usually inherited WorkerHOME None The home directory to use for Go and Bash , usually inherited WorkerDATABASE_CONNECTIONS 50 (Server)/3 (Worker) The max number of connections in the database connection pool AllSUPERADMIN_SECRET None A token that would let the caller act as a virtual superadmin superadmin@windmill.dev ServerTIMEOUT_WAIT_RESULT 20 The number of seconds to wait before timeout on the 'run_wait_result' endpoint WorkerQUEUE_LIMIT_WAIT_RESULT None The number of max jobs in the queue before rejecting immediately the request in 'run_wait_result' endpoint. Takes precedence on the query arg. If none is specified, there are no limit. WorkerDENO_AUTH_TOKENS None Custom DENO_AUTH_TOKENS to pass to worker to allow the use of private modules WorkerDENO_FLAGS None Override the flags passed to deno (default --allow-all) to tighten permissions. Minimum permissions needed are \"--allow-read=args.json --allow-write=result.json\" WorkerNPM_CONFIG_REGISTRY None Registry to use for NPM dependencies, set if you have a private repository you need to use instead of the default public NPM registry WorkerPIP_LOCAL_DEPENDENCIES None Specify dependencies that are installed locally and do not need to be solved nor installed againADDITIONAL_PYTHON_PATHS None Specify python paths (separated by a :) to be appended to the PYTHONPATH of the python jobs. To be used with PIP_LOCAL_DEPENDENCIES to use python codebases within Windmill WorkerINCLUDE_HEADERS None Whitelist of headers that are passed to jobs as args (separated by a comma) ServerWHITELIST_WORKSPACES None Whitelist of workspaces this worker takes job from WorkerBLACKLIST_WORKSPACES None Blacklist of workspaces this worker takes job from WorkerINSTANCE_EVENTS_WEBHOOK None Webhook to notify of events such as new user added, signup/invite. Can hook back to windmill to send emailsGLOBAL_CACHE_INTERVAL 10*60 (Enterprise Edition only) Interval in seconds in between bucket sync of the cache. This interval * 2 is the time at which you're guaranteed all the worker's caches are synced together. WorkerWORKER_TAGS 'deno,go,python3,bash,flow,hub,dependency' The worker groups assigned to that workers WorkerCUSTOM_TAGS None The custom tags assignable to scripts. ServerJOB_RETENTION_SECS 606024*60 //60 days The time in seconds after which jobs get deleted. Set to 0 or -1 to never deleteWAIT_RESULT_FAST_POLL_INTERVAL_MS 50 The time in between polling for the run_wait_result endpoints in fast poll mode ServerWAIT_RESULT_SLOW_POLL_INTERVAL_MS 200 The time in between polling for the run_wait_result endpoints in fast poll mode ServerWAIT_RESULT_FAST_POLL_DURATION_SECS 2 The duration of fast poll mode before switching to slow poll ServerEXIT_AFTER_NO_JOB_FOR_SECS None Exit worker if no job is received after duration in secs if defined WorkerOAUTH_JSON_AS_BASE64 None Base64 encoded JSON of the OAuth configuration. e.g OAUTH_JSON_AS_BASE64=$(base64 oauth.json | tr -d '\\n') to encode it ServerRun a local dev setuponly FrontendThis will use the backend of https://app.windmill.dev but your own frontend with hot-code reloading.Install caddyGo to frontend/:npm install, npm run generate-backend-client then npm run devIn another shell sudo caddy run --config CaddyfileRemoteEt voil\u00e0, windmill should be available at http://localhost/Backend + FrontendSee the ./frontend/README_DEV.md file for all running options.Create a Postgres Database for Windmill and create an admin role inside your Postgres setup. The easiest way to get a working postgres is running cargo install sqlx-cli && sqlx migrate run. This will also avoid compile time issue with sqlx's query! macroInstall nsjail and have it accessible in your PATHInstall deno and python3, have the bins at /usr/bin/deno and /usr/local/bin/python3Install caddyInstall the lld linkerGo to frontend/:npm install, npm run generate-backend-client then npm run devIn another shell npm run build otherwise the backend will not find the frontend/build folder and will crashIn another shell sudo caddy run --config CaddyfileGo to backend/: DATABASE_URL=<DATABASE_URL_TO_YOUR_WINDMILL_DB> RUST_LOG=info cargo runEt voil\u00e0, windmill should be available at http://localhost/ContributorsCopyrightWindmill Labs, Inc 2023",
    "summary": "- Windmill is an open-source developer infrastructure for internal tools that can turn scripts into workflows and UIs through its autogenerated frontend.\n- Scripts can be written in Python, Typescript, Go, Bash, and SQL, and they can be chained to form more complex workflows, triggered by a cron schedule or webhooks.\n- Windmill can be self-hosted through Docker Compose, Kubernetes, and Helm charts, and it offers commercial licenses and support.",
    "hn_title": "Windmill: Open-source developer platform to turn scripts into workflows and UIs",
    "original_title": "Windmill: Open-source developer platform to turn scripts into workflows and UIs",
    "score": 328,
    "hn_content": "Windmill-labs has launched an open-source developer platform that turns scripts into workflows and user interfaces; it includes three layers of functions, including Rust-based workers, a workflow engine and a dashboard and application builder. The platform is self-hostable and offers a CLI that can sync to Github and compatible tools, and is seen as a free alternative to Airplane.dev. The blog has been praised and the Discord server allows projects to be synced for collaborative use. The platform is close to hitting version 2 and is the creation of solo developer Ruben Fiszel, who founded the company after going through the Y Combinator.Windmill is a low-code workflow automation tool that can run arbitrary code using Python, TypeScript, Go, and Bash. It has a low-cost workflow engine with features such as retries, error handlers, and suspend/sleep. Windmill is designed to target hybrid teams made of low-code/semi-technical people and engineers. It aims to provide abstraction without compromise.\n \n Windmill is gaining interest due to its compatibility with CapRover and ease of self-hosting. It is efficient, and it supports typescript/go/bash, offers the ability to run workflows with no time limits, and has no cost to keep a workflow suspended. The workflow engine is designed to run fast, and it supports long-running workflows. Windmill also supports synchronous and asynchronous APIs for fetching the status of the flow or triggering a flow. Furthermore, the team behind Windmill is constantly updating and improving the platform to make it more effective, including updates based on community feedback.A team is working on a new feature to be released soon. The tool allows for webhooks and can be integrated with Gitlab. Powershell support will be added within the next month. The current script language is Bash. The use of Go as a scripting language has gained traction in the tool/utility world due to easy cross-compilation and single-binary output. Users requested the use of scripts in the tool, rather than just \"code\" or \"serverless functions.\"",
    "hn_summary": "- Windmill is gaining interest due to its compatibility with CapRover and ease of self-hosting, as well as its features such as time limits and no cost for keeping a workflow suspended.\n- A team is working on a new feature to integrate webhooks and Gitlab, and soon Powershell support will be added. Users requested the option to use scripts in the tool, rather than just code or serverless functions."
  },
  {
    "id": 35914447,
    "timestamp": 1683888700,
    "title": "Pigz: Parallel gzip for modern multi-processor, multi-core machines",
    "url": "https://zlib.net/pigz/",
    "hn_url": "http://news.ycombinator.com/item?id=35914447",
    "content": "A parallel implementation of gzip for modernmulti-processor, multi-core machinesWelcome to the pigz home page. You can download the latest source code right here:pigz source code version 2.7 (15 Jan 2022) in tar.gz format(106K, SHA-256 checksum b4c9e60344a08d5db37ca7ad00a5b2c76ccb9556354b722d56d55ca7e8b1c707, GPG signature)Latest version of pigz and its signature (version-independent links)pigz, which stands for parallel implementation of gzip, is a fully functional replacement for gzip that exploits multiple processors and multiple cores to the hilt when compressing data. pigz was written by Mark Adler, and uses the zlib and pthread libraries. To compile and use pigz, please read the README file in the source code distribution. You can read the pigz manual page here.If you like, you can subscribe to the pigz-announce email list to be notified when new versions are released.How is it pronounced?I'm glad you asked. It is pronounced \u201cpig-zee\u201d. It is not pronounced like the plural of pig.",
    "summary": "- Pigz is a parallel implementation of gzip that utilizes multiple processors and cores to compress data efficiently.\n- The latest source code version of Pigz is available for download on the website with its GPG signature and SHA-256 checksum.\n- Users can subscribe to the Pigz-announce email list to get notified about new versions of Pigz.",
    "hn_title": "Pigz: Parallel gzip for modern multi-processor, multi-core machines",
    "original_title": "Pigz: Parallel gzip for modern multi-processor, multi-core machines",
    "score": 282,
    "hn_content": "Pigz is a parallel gzip tool designed for modern multi-processor, multi-core machines. Yann Collet, the creator of LZ4 and zstd, built the world's fastest compression algorithms. Brotli and zstd outperformed ZLib in case of speed and compression ratio. Compression algorithms for video games can be optimized for faster decompression times. Zlib/gzip used DEFLATE because it was one of the few algorithms not covered by patents. Economies of scale play a crucial role when choosing the compression level. Large scale compression libraries like Zstd support separate compression dictionaries which can be customized for optimal compression rates of specific file types. People tend to overlook tweaking the compression level for better performance, even though it can be advantageous in certain cases.The post discusses various aspects of compression techniques and algorithms, including the use of dictionaries and custom code for zstd and brotli. It also provides resources for optimizing parallel decompression using GPU and CPU threads, as well as suggestions for improving compressions with bzip2. Some users share their experiences with specific compression methods, such as the effectiveness of bzip2 for certain file types. Overall, the post offers insights and tips on compression for those interested in maximizing efficiency and performance.Tech enthusiasts discuss the pros and cons of different compression techniques and methods for compressing and decompressing large files. They recommend using zstandard for its faster compression and decompression speeds and smaller file sizes, although gzip remains more ubiquitous and useful in certain situations. Parallel compressors like pbzip2 are also recommended for increased speed when compressing large amounts of data. The group also discusses how compression schemes create synchronisation points and use dictionaries per chunk or one large common dictionary to balance compression efficiency and speed. At the core of the conversation is how best to handle large file compression and the evolving landscape of compression technology.Pigz is a parallel implementation of gzip that exploits multiple processors and multiple cores to compress data; Docker automatically uses Pigz if it's in the system path, and it can cut build and compression times on large files. Pigz is particularly useful for compressing multiple files in a zip file. There are alternatives, such as zstd or LZ4 compression algorithms, which may be faster, have lower memory overheads, and are more compressive; parallel decompression has been implemented, and hardware-accelerated GZIP acceleration with AIX on Power Systems is also available. The name \"Pigz\" has been regarded as a pornographic name by some, but most find it amusing.",
    "hn_summary": "- Tech enthusiasts recommend zstandard for faster compression and decompression speeds, smaller file sizes, and recommend pbzip2 for increased speed when compressing large amounts of data.\n- The discussion revolves around handling large file compression and the evolving landscape of compression technology."
  },
  {
    "id": 35911406,
    "timestamp": 1683860825,
    "title": "FUTO \u2013 An independent software lab & grant fund",
    "url": "https://futo.org/",
    "hn_url": "http://news.ycombinator.com/item?id=35911406",
    "content": "HomeAboutGrants \u2013 Fellows Applications Open!ProjectsRepair WorkshopJobsFUTO ChatWe develop technology to bring control of computing back to the people.Almost all aspects of our digital lives are controlled by a select few mega-corporations.Violate their policies, even by accident, and your identity and livelihood are destroyed in an instant.FUTO is an independent software lab and grant/investment fund.We are dedicated to the development of open-source software and advocacy that allows users, creators, and businesses to not be reliant on a few tech companies to live their lives.Our ProgramsEngineeringGrantsGuidesJobsAddress1044 Liberty Park DriveAustin, TX 78746United States of AmericaOffice Phone+1 (512) 222-5182NavigationJobsGrantsProjectsSocialMindsTwitterOdyseeYouTubeOnce men turned their thinking over to machines in the hope that this would set them free. But that only permitted other men with machines to enslave them.Frank Herbert, Dune",
    "summary": "- FUTO is an independent software lab and grant/investment fund dedicated to developing open-source software and advocating for user, creator, and business control of computing.\n- The organization aims to counter the fact that almost all aspects of our digital lives are controlled by a few mega-corporations, and even accidental policy violations can destroy identities and livelihoods in an instant.\n- FUTO offers programs in engineering, grants, guides, and jobs and is located in Austin, Texas.",
    "hn_title": "FUTO \u2013 An independent software lab and grant fund",
    "original_title": "FUTO \u2013 An independent software lab and grant fund",
    "score": 253,
    "hn_content": "FUTO is an independent software lab and grant fund, promoting software freedom and encouraging developers to take on ambitious projects without limiting themselves. Their unique approach to software development promotes impactful and accessible software, while fighting against obstacles like locked-down hardware and app stores. The organization looks to give users control over their technology and has partnered with Louis Rossmann to offer free repair workshops in Austin. FUTO provides grants to both US and non-US projects providing a flexible grant structure free of bureaucracy. The group values the independence of technology companies and adheres to the belief that an acquisition by the oligopoly is a failure.FUTO, a new nonprofit organization, is building a web browser from scratch with a new engine to ensure that the web platform remains open and healthy. The founder of Yahoo! Games and WhatsApp investor, Eron Wolf, is funding FUTO. Some people expressed concerns about FUTO's funding and potential exploitation of innovation. It is unclear what the letters \"FU\" in FUTO stand for. However, according to BlueTemplar, as long as it is not \"la FICA! snorts,\" it's all good.",
    "hn_summary": "- They offer flexible grants to US and non-US projects and provide free repair workshops in partnership with Louis Rossmann in Austin.\n- Some people are concerned about FUTO's funding and potential exploitation of innovation, but others find the organization's values and goals admirable."
  }
]
