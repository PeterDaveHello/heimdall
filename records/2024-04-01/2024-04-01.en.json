[
  {
    "id": 39890262,
    "title": "LLaMA Project Boosts CPU Performance for Enhanced User Experience",
    "originLink": "https://justine.lol/matmul/",
    "originBody": "Mar 31st, 2024 @ justine's web page LLaMA Now Goes Faster on CPUs Mozilla llamafile I just wrote 84 new matrix multiplication kernels for llamafile which enable it to read prompts / images faster. Compared to llama.cpp, prompt eval time with llamafile should go anywhere between 30% and 500% faster when using F16 and Q8_0 weights on CPU. The improvements are most dramatic for ARMv8.2+ (e.g. RPI 5), Intel (e.g. Alderlake), and AVX512 (e.g. Zen 4) computers. My kernels go 2x faster than MKL for matrices that fit in L2 cache, which makes them a work in progress, since the speedup works best for prompts having fewer than 1,000 tokens. Background llamafile is a local LLM project I started with Mozilla back in Nov 2023. We're using Cosmopolitan Libc to package llama.cpp as a single-file cross-platform binary that runs on six OSes for AMD64 and ARM64, while making gentle modifications. I believe that by improving the core technology, we can give our users the best possible llama.cpp experience, while helping both projects reach a broader audience. Mozilla has been giving me the resources to do this. Performance Gains on Enterprise Hardware When I first got into LLMs, my workstation was an austere Hewlett Packard running Alpine with a spinning disk, slow RAM, an AVX2 processor, and no GPU. What I liked about llama.cpp is they were the first LLM project that cared about people like me. So I started volunteering full time and collaborated with guys like Slaren to introduce mmap() support, which made weights load instantly using half as much RAM. It was a leap forward for local LLMs at the time, but did little to improve evaluation speed. Most of the inference code was written by Georgi Gerganov himself, and it's so good that it'd take me another year to finally improve upon. Now that I have, let's see how much faster things go on my old Hewlett Packard. LLM Performance on HP Intel® Core™ i9-9900 ($439) w/ 2200 MT/s RAM c. 2020 prompt tok/sec eval tok/sec model weights data type hardware software 28 7 Mistral 7b q4_0 Skylake llamafile-0.7 17 7 Mistral 7b q4_0 Skylake llama.cpp 2024-03-26 12 7 Mistral 7b q4_0 Skylake llamafile-0.6.2 32 4 Mistral 7b q8_0 Skylake llamafile-0.7 22 4 Mistral 7b q8_0 Skylake llama.cpp 2024-03-26 16 4 Mistral 7b q8_0 Skylake llamafile-0.6.2 23 2 Mistral 7b f16 Skylake llamafile-0.7 15 2 Mistral 7b f16 Skylake llama.cpp 2024-03-26 14 2 Mistral 7b f16 Skylake llamafile-0.6.2 205 26 TinyLlama 1.1B q8_0 Skylake llamafile-0.7 144 26 TinyLlama 1.1B q8_0 Skylake llama.cpp 2024-03-26 91 23 TinyLlama 1.1B q8_0 Skylake llamafile-0.6.2 171 15 TinyLlama 1.1B f16 Skylake llamafile-0.7 118 15 TinyLlama 1.1B f16 Skylake llama.cpp 2024-03-26 101 15 TinyLlama 1.1B f16 Skylake llamafile-0.6.2 Here we see that, on Skylake, llamafile users can expect to see a 2x speedup and llama.cpp users can expect 50% better performance. Please note this only applies to certain weights. So far, I've only written optimized kernels for the q8_0, f16, q4_1, q4_0, and f32 data types. I think both q8_0 and f16 are really solid choices. Possibly even f32 if you've got plenty of RAM. That's because my new kernels change the rules. They're doing such a good job fixing the memory bandwidth quants always solved, that quantization could become the bigger bottleck. That would be great news for the future of local language models, since it means less need to trade away knowledge for speed. Performance Gains on Hobbyist Hardware You don't need a large computer to run a large language model. One of the best personal computers available in stores today is the Raspberry Pi. They deliver good performance at a great price and consume very little power. LLM Performance on $100 Raspberry Pi v5 (ARMv8.2) and v4 (ARMv8.0) prompt tok/sec eval tok/sec model weights data type hardware software 62 5 TinyLlama 1.1B f16 RPI5 llamafile-0.7 28 5 TinyLlama 1.1B f16 RPI5 llama.cpp 2024-03-26 8 5 TinyLlama 1.1B f16 RPI5 llamafile-0.6.2 45 9 TinyLlama 1.1B q8_0 RPI5 llamafile-0.7 35 9 TinyLlama 1.1B q8_0 RPI5 llama.cpp 2024-03-26 20 9 TinyLlama 1.1B q8_0 RPI5 llamafile-0.6.2 10 3 TinyLlama 1.1B q8_0 RPI4 llamafile-0.7 10 3 TinyLlama 1.1B q8_0 RPI4 llama.cpp 2024-03-26 9 3 TinyLlama 1.1B q8_0 RPI4 llamafile-0.6.2 3 2 TinyLlama 1.1B f16 RPI4 llamafile-0.7 3 2 TinyLlama 1.1B f16 RPI4 llama.cpp 2024-03-26 4 2 TinyLlama 1.1B f16 RPI4 llamafile-0.6.2 Raspberry Pi released their fifth edition a few months ago and it's outrageously fast compared to their previous model. They also introduced support for the ARMv8.2 dotprod and fp16 arithmetic ISAs, which are very useful for LLMs. Those two features alone enabled llama.cpp to achieve a 10x performance boost for f16 weights last year. This week I teased out another 2x performance boost on top of that, by using a kernel that I originally intended for AVX512. You wouldn't think a kernel designed for beefy data center equipment would work out for a teensy tiny little Raspberry Pi, but it actually fit hand in glove since both CPUs have 32 vector registers. It's worthwhile to note that the new ARMv8.2 fp16 ISA may introduce more errors than usual, since it causes llamafile to use fp16 words and we aren't using techniques like Kahan summation for computing dot products. So Q8_0 weights actually end up having slightly better perplexity, because it uses the dotprod ISA which lets us updot signed 8-bit integers into a 32-bit compute type which absorbs errors. However this doesn't mean the faster fp16 weights can't be useful. Many developers in this field view the differences as negligible. For example, let's say you want to setup an email server on your pihole and have TinyLLaMA filter spam. It's possible to configure Postfix to filter content using a shell script which lets you run the llamafile command. llamafile -m TinyLlama-1.1B-Chat-v1.0.f16.gguf \\ --grammar 'root ::= \"yes\"\"no\"' --temp 0 -c 0 \\ --no-display-prompt --log-disable -p \" Can you say for certain that the following email is spam? To: jtunney@gmail.com From: Federal-Tax-DebtHelpSubject: Reduce your payments to what you can afford Reduce your payments to what you can afford [IMG] [IMG] [IMG]\" When I run the shell script above on my RPI5, it takes 3 seconds. jart@pi5:~/scratch$ time ./spam.sh yes real 0m3.168s user 0m10.851s sys 0m0.541s There are several important things happening here: --temp 0 turns off the random number generator (we don't want improvisation for a spam filter) Here we see why prompt eval time is king. Token generation speed (eval time) doesn't matter for this use case, since we're using the --grammar flag to force the LLM to only print a single \"yes\" or \"no\" token. I piped the original email through links -codepage utf-8 -force-html -width 400 -dump /dev/stdin which reduced the number of tokens and removed hidden HTML content that was put there by the spammer to make the email look like ham to naive Bayesian filters. The -c 0 flag configures TinyLLaMA to use the maximum context size, which is 2048 tokens. That's the largest prompt we can give it. To help avoid feeding in too much text, you can pipe the email through sed 's/ */ /g'dd bs=1 count=7000 to remove superfluous spaces and place an upper limit on its size. Please note that spam filtering is just the tip of the iceberg. I've always thought that \"generative ai\" is a misnomer, because language models (more commonly known as \"The Algorithm\") have always been used by tech companies in the past to extract knowledge, curate information, and rank content. That requires reading rather than writing. AI has never traditionally needed to talk that much, because the English language just isn't that useful at the scale tech companies operate. Even if you could generate English summaries for exabytes of text a millionth its size, that would still be more words than any team could hope to read in a lifetime. Performance Gains on Gaming Hardware Gamers have the highest quality expectations of any value consumer, so any hardware built for them is usually pretty good. In the machine learning industry, we have thrived for years repurposing hardware that was intended for gamers. If it weren't for their important contribution, the AI Winter may have needed to last another ten years. So a few months ago, I asked a gamer to build me a computer that can replace my old Hewlett Packard. LLM Performance on Intel® Core™ i9-14900K ($600) w/ 6400 MT/s RAM prompt tok/sec eval tok/sec model weights data type hardware software 63 12 Mistral 7b q8_0 Alderlake llamafile-0.7 40 9 Mistral 7b q8_0 Alderlake llama.cpp 2024-03-26 19 7 Mistral 7b q8_0 Alderlake llamafile-0.6.2 50 7 Mistral 7b f16 Alderlake llamafile-0.7 13 5 Mistral 7b f16 Alderlake llama.cpp 2024-03-26 10 4 Mistral 7b f16 Alderlake llamafile-0.6.2 406 67 TinyLlama 1.1B q8_0 Alderlake llamafile-0.7 273 53 TinyLlama 1.1B q8_0 Alderlake llama.cpp 2024-03-26 114 43 TinyLlama 1.1B q8_0 Alderlake llamafile-0.6.2 407 42 TinyLlama 1.1B f16 Alderlake llamafile-0.7 90 31 TinyLlama 1.1B f16 Alderlake llama.cpp 2024-03-26 68 30 TinyLlama 1.1B f16 Alderlake llamafile-0.6.2 I think Alderlake is a great CPU but it's popularly misunderstood, as evidenced by how easily I quintupled its float16 performance. Unlike ARMv8.2, I was able to do that without introducing rounding errors, since my x86 kernels use a float32 compute type internally. This means I can have an even smarter spam filter. For example, when I run my spam.sh shell script, it only takes 420 milliseconds, which is 7x faster than my Raspberry Pi 5. That's right, when it comes to small workloads, this chip is able to finish before CUDA even gets started. Alderlake owners can also look forward to the fact that llamafile takes special care to not run on your efficiency cores. This is one of the things that helps llamafile to go faster than llama.cpp. It also means you can run LLMs around the clock and there's still plenty of resources leftover for the other programs on your computer. The reason why that's important is because llama.cpp dispatches threads in lockstep, which would have meant that if any 1 core takes longer than the others to do its job, then all other n cores would need to busy loop until it completed. However the greatest feature of this microprocessor is how quickly it can build all 2.6 million lines of code in the Cosmopolitan monorepo. My Hewlett Packard always took 64 seconds, but this gaming computer does it in 20. It actually took 35 seconds originally; what made it faster is was applying liquid metal and AI overclocking. Another reason systems code is so fast on the Alderlake is there was a fire fight between the hackers and scientists in the creation of this CPU, and the hackers won. I hope they'll strike out a better compromise on AVX512 in the future, but overall I'm very happy with this chip, since I believe it represents significant progress over previous models. Performance Gains on Apple Hardware If there's a personal computer with the most class, it would definitely be the Mac Studio. Gaining the performance advantage here was harder for me, because it's the hardware platform the llama.cpp developers care about most, plus I'm working with a handicap due to my choice to use Stallman's compiler instead of Apple's proprietary tools. LLM Performance on Mac Studio CPU w/ 24-core M2 Ultra ($5000) prompt tok/sec eval tok/sec model weights data type hardware software 90 25 Mistral 7b q8_0 M2 Ultra llamafile-0.7 90 27 Mistral 7b q8_0 M2 Ultra llama.cpp 2024-03-26 37 24 Mistral 7b q8_0 M2 Ultra llamafile-0.6.2 79 15 Mistral 7b f16 M2 Ultra llamafile-0.7 57 15 Mistral 7b f16 M2 Ultra llama.cpp 2024-03-26 21 15 Mistral 7b f16 M2 Ultra llamafile-0.6.2 457 95 TinyLlama 1.1B q8_0 M2 Ultra llamafile-0.7 564 108 TinyLlama 1.1B q8_0 M2 Ultra llama.cpp 2024-03-26 236 95 TinyLlama 1.1B q8_0 M2 Ultra llamafile-0.6.2 419 66 TinyLlama 1.1B f16 M2 Ultra llamafile-0.7 400 67 TinyLlama 1.1B f16 M2 Ultra llama.cpp 2024-03-26 141 66 TinyLlama 1.1B f16 M2 Ultra llamafile-0.6.2 I wouldn't want to pick a fight with an Apple user, because their M2 microprocessor turns llamafile into a firehose of synthetic content. The trick Apple used to do it is leveraging their vertical integration. If you buy a Mac Studio and look inside, you'll discover that they put the RAM DIMMs inside the CPU. It makes latency-bound operations like token generation go much faster, because the CPU no longer needs to make all these long distance phone calls. However, in terms of sheer flops (as measured by prompt tok/sec), we can see that compared to my much cheaper Intel computer, the M2 Ultra only exposes 30% more compute via the ARM ISA. You need to go through their proprietary frameworks like Metal and Accelerate if you want to access anything more. If you have xcode installed, then llamafile by default will compile a small stub module which does just that, since despite my values I'm happy to help you get in front of any closed source library standing between you and your silicon. One important thing to know if you're considering buying a Mac Studio is that, like the Windows Executive, XNU does a really good job keeping your desktop stable, and that means protecting your system from you. It takes me 45 seconds on Mac Studio to compile the Cosmo monorepo, due to all these safety features; but if I fork bombed it, I'd be surprised if Netflix skipped a single frame. My spam.sh script also goes 430ms, which is slower than Intel. However none of this concerns me, since I've seen the way Asahi Linux is able to unleash the M2's full potential. Performance Gains on Professional Hardware While llamafile cares deeply about helping the GPU poor, it offers a first-class experience to the 1% too. The AMD Ryzen Threadripper PRO 7995WX was just launched several months ago and it's the most expensive CPU money can buy right now. It'll set you back $10,000 but you get 96 cores of AVX512, based on the Zen4 architecture. LLM Performance on AMD Ryzen Threadripper PRO 7995WX w/ 96 cores ($10,000) prompt tok/sec eval tok/sec model weights data type hardware software 557 17 Mistral 7b bf16 7995WX llamafile-0.7 485 17 Mistral 7b f16 7995WX llamafile-0.7 197 16 Mistral 7b f16 7995WX llama.cpp 2024-03-29 52 18 Mistral 7b f16 7995WX llamafile-0.6.2 480 10 Mistral 7b f32 7995WX llamafile-0.7 221 10 Mistral 7b f32 7995WX llama.cpp 2024-03-30 38 9 Mistral 7b f32 7995WX llamafile-0.6.2 382 25 Mistral 7b q8_0 7995WX llamafile-0.7 283 24 Mistral 7b q8_0 7995WX llama.cpp 2024-03-29 37 25 Mistral 7b q8_0 7995WX llamafile-0.6.2 1929 52 TinyLlama 1.1B bf16 7995WX llamafile-0.7 1819 52 TinyLlama 1.1B f16 7995WX llamafile-0.7 824 51 TinyLlama 1.1B f16 7995WX llama.cpp 2024-03-29 295 89 TinyLlama 1.1B f16 7995WX llamafile-0.6.2 1268 60 TinyLlama 1.1B q8_0 7995WX llamafile-0.7 1127 60 TinyLlama 1.1B q8_0 7995WX llama.cpp 2024-03-29 169 93 TinyLlama 1.1B q8_0 7995WX llamafile-0.6.2 Here we see that, despite only being twice the price, the 7995WX x86 ISA offers 7x more raw compute power than the M2 Ultra ARM ISA, and nearly the same token generation speed, which is likely thanks to its 384mb L3 cache. When I bought this chip, I had to expand support in llama.cpp for bfloat16 and AVX512 before I could fully test its capabilities. My work means you can now run LLaMA 2.8x faster on Zen4 than you could before. One thing I like about AVX512 is that Google's Gemma model can solve math riddles on AVX512 but not on AVX2 because the bigger vectors usually make it easier to reduce rounding errors. Its VDPBF16PS instruction helps us updot bf16 similar to VNNI and ARM dotprod. Having native support for bf16 is nice, since models like Mistral and TinyLLaMA distribute weights using bfloat16 as their canonical format. If we were to convert bf16 to fp16, then only 13% of the numbers that are possible can be accurately represented. In practice, it matters little, since 99.71% of the numbers Mistral 7b uses are among that 13%. However I believe that llamafile should deliver, to the best of its ability, whatever number of bits are being claimed. Especially when doing so also enables us to better exploit the capabilities of our hardware. Adding bf16 support is my first big step towards improving that. Please be warned that a lot of people who bought this Threadripper ran into issues with sketchy RAM. I had to RMA the first DIMMs I bought for this computer, because most of them died and I was getting 5 eval tokens per second with Mistral. I've been having better luck with a new full kit of eight sticks that just arrived today. When I run sysbench memory run it reports 10,033,424 mops, which is oddly faster than my Mac Studio where 9,892,584 mops is reported, however my Intel computer does 14,490,952. I expected my Threadripper's RAM to have that speed since both set of components advertised 6400 MT/s with the same timings, but I'm told that I traded this away to have 256GB of ECC. As for disk speed, dd if=/dev/zero of=/tmp/output bs=128k count=50k; rm -f /tmp/output reports 1.6 GB/s which is 3.6x slower than my Mac Studio, and 3x slower than my Intel (which has the same M.2 stick). I'm told that Intel and Apple are just better at this, but I wish I understood why. Last but not least, it runs my spam.sh script in 323ms and builds the whole Cosmo monorepo in 13 seconds. It's actually capable of building it faster, since this is the first time I've ever seen my build config being constrained by an individual artifact blocking the critical path. I never thought I'd live to see the day. I'm also puzzled that llamafile v0.6.2 is somehow managing to do 93 tokens per second; that's 40% faster than my M2. It's exciting news, since after reviewing the breadth of this blog post, I would have wept if there were no more optimizations possible. Source Code The source code for my matrix multiplication kernels can be found at: https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp Both Mozilla and myself felt it would be worthwhile to contribute these improvements to the upstream project. Doing that required adapting the code to their preferred method of handling portability at compile-time. We also took the liberty of changing the license from Apache 2.0 to MIT, since the latter is what the llama.cpp developers prefer. Here are links to the most recent pull requests I've sent them: https://github.com/ggerganov/llama.cpp/pull/6414 https://github.com/ggerganov/llama.cpp/pull/6412 Technical Details How I Improved Prompt Eval Time on CPUs There are dozens of mathematical operations a transformer model needs to perform in order to generate text, e.g. rope, transpose, reshape, softmax, rms_norm, etc. All of the performance improvements I described above, were achieved by focusing exclusively on a single one, which is GGML_OP_MUL_MAT, because that's what my Linux Perf profiler told me llamafile spends 95% of its time doing. So what is this matrix multiplication thing? We shall start by defining the most important algorithm in the world using the pythonic dialect of Python that is most popular with developers today: def matmul(A, B): assert len(B) == len(A[0]) return [[sum(A[i][l] * B[l][j] for l in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))] As we can see, it's just three for loops and a multiply-add. How hard could it be? On my workstation (which I call meatball), the code above goes a screeching 0.042 gigaflops. Most Python programmers are smart enough to know that they should delegate tasks like these to a library like np.matmul, which goes 29 gigaflops. NumPy achieves its speed using FORTRAN which for generations has been favored by real programmers who've led us to believe these libraries are something mysterious chiseled in stone by the hand of Moses himself; but if we look at the FORTRAN code NumPy actually uses, then it really isn't all that complicated and could clearly benefit from some revision. SUBROUTINE SGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC) * .. Scalar Arguments .. REAL ALPHA,BETA INTEGER K,LDA,LDB,LDC,M,N CHARACTER TRANSA,TRANSB * .. Array Arguments .. REAL A(LDA,*),B(LDB,*),C(LDC,*) [...] * * Form C := alpha*A*B + beta*C. * DO 90 J = 1,N IF (BETA.EQ.ZERO) THEN DO 50 I = 1,M C(I,J) = ZERO 50 CONTINUE ELSE IF (BETA.NE.ONE) THEN DO 60 I = 1,M C(I,J) = BETA*C(I,J) 60 CONTINUE END IF DO 80 L = 1,K IF (B(L,J).NE.ZERO) THEN TEMP = ALPHA*B(L,J) DO 70 I = 1,M C(I,J) = C(I,J) + TEMP*A(I,L) 70 CONTINUE END IF 80 CONTINUE 90 CONTINUE [...] RETURN END I like to define my subroutines using a modern language like C++, which goes 47 gigaflops. This means C++ is three orders of a magnitude faster than Python. That's twenty years of progress per Moore's law. // multiplies matrices on cpu // with column major ordering // // m×k * k×n → m×n // k×m * k×n → m×n if aᵀ // m×k * n×k → m×n if bᵀ // k×m * n×k → m×n if aᵀ and bᵀ // templatevoid GEMM(bool aᵀ, bool bᵀ, int m, int n, int k, T α, const TA *A, int lda, const TB *B, int ldb, T β, TC *C, int ldc) { assert(m >= 0 && n >= 0 && k >= 0); assert(lda >= std::max(1, aᵀ ? k : m)); assert(ldb >= std::max(1, bᵀ ? n : k)); assert(ldc >= std::max(1, m)); #pragma omp parallel for collapse(2) if (m * n * k > 300000) for (int i = 0; ivoid LLMM(int m, int n, int k, const T *A, int lda, const T *B, int ldb, T *C, int ldc) { #pragma omp parallel for collapse(2) if (m * n * k > 300000) for (int i = 0; i300000) for (int i = 0; i300000) for (int i = 0; i300000) for (int i = 0; i300000) for (int i = 0; i 0 threads returning to the spinlock barrier. We can work within this model by defining a new kernel framework. #define BEGIN_KERNEL(RM, RN) \\ int ytiles = (m - m0) / RM; \\ int xtiles = (n - n0) / RN; \\ int tiles = ytiles * xtiles; \\ int duty = (tiles + nth - 1) / nth; \\ if (dutytiles) \\ end = tiles; \\ for (int job = start; jobclass GEMMER { public: GEMMER(int k, const T *A, int lda, const T *B, int ldb, float *C, int ldc, int ith, int nth) : k(k), A(A), lda(lda), B(B), ldb(ldb), C(C), ldc(ldc), ith(ith), nth(nth) { } void llmm(int m, int n) { mnpack(0, m, 0, n); } private: void mnpack(int m0, int m, int n0, int n) { if (m - m0 = 3 && n - n0 >= 4) { mc = 3; nc = 4; llmm3x4(m0, m, n0, n); } else if (m - m0 >= 4 && n - n0 >= 1) { mc = 4; nc = 1; llmm4x1(m0, m, n0, n); } else if (m - m0 >= 1 && n - n0 >= 4) { mc = 1; nc = 4; llmm1x4(m0, m, n0, n); } else { mc = 1; nc = 1; llmm1x1(m0, m, n0, n); } mp = m0 + (m - m0) / mc * mc; np = n0 + (n - n0) / nc * nc; mnpack(mp, m, n0, np); mnpack(m0, mp, np, n); mnpack(mp, m, np, n); } // ... void llmm1x1(int m0, int m, int n0, int n) { BEGIN_KERNEL(1, 1) __m256 c = _mm256_setzero_ps(); for (int l = 0; ltb{k, A, lda, B, ldb, C, ldc, ith, nth}; tb.llmm(m, n); } else if (!HAVE_OPENMP || n * m * ktb{k, A, lda, B, ldb, C, ldc, 0, 1}; tb.llmm(m, n); } else { nth = sysconf(_SC_NPROCESSORS_ONLN); #pragma omp parallel for for (ith = 0; ithtb{k, A, lda, B, ldb, C, ldc, ith, nth}; tb.llmm(m, n); } } } Methodology You need to run the following command on Linux in order to benchmark llamafile reliably. It also helps a little bit with timings to run as root, but that shouldn't be necessary. echo performancesudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor On Apple Silicon, I needed to build llama.cpp using the following command in order to get it to run in CPU mode. make -j32 LLAMA_NO_ACCELERATE=1 LLAMA_NO_METAL=1 Some of you might be surprised that I didn't write my kernels in assembly like BLIS does, especially given my history of projects like blink, SectorLISP and SectorLAMBDA. The truth is I've been coding in assembly this whole time. I configured Emacs so I can push a button, and the disassembly for the C++ code I'm working on will pop up on the screen in a few milliseconds. I know anyone whose codebase has slow build times doesn't possess this advantage, which has made me famous. Once I figure out how to do that for .cu files, I'll be unstoppable. Credits I learned how to write math kernels by renting Vast VMs and watching Gautham Venkatasubramanian and mrdomino develop CUDA kernels in a tmux session. They've been focusing on solving a much more important challenge for llamafile, which is helping it not have a mandatory dependency on the cuBLAS: the reigning supreme linear algebra library of such speed, accuracy, and ferocity that it could only have been written by the prince of darkness himself. You're encouraged to follow our ongoing progress on GitHub. Discord Congratulations on reading this far. I'd like to extend an invitation for you to join us on the Mozilla AI Discord, where you can ask questions and hang out with me, folks from Mozilla, and others in the llamafile community. Funding My full-time work on open source projects like llamafile is funded thanks to the generous support of Mozilla, my GitHub sponsors, and Patreon subscribers. Thank you everyone, for helping me have the opportunity to serve you these last four years. Your support made it possible for high-quality math kernels to be shared with the commons. twitter.com/justinetunney github.com/jart Written by Justine Tunney jtunney@gmail.com",
    "commentLink": "https://news.ycombinator.com/item?id=39890262",
    "commentBody": "LLaMA Now Goes Faster on CPUs (justine.lol)580 points by lawrencechen 7 hours agohidepastfavorite160 comments speps 2 hours agoRegarding this bit at the end: > I learned how to write math kernels by renting Vast VMs and watching Gautham Venkatasubramanian and mrdomino develop CUDA kernels in a tmux session. They've been focusing on solving a much more important challenge for llamafile, which is helping it not have a mandatory dependency on the cuBLAS If I'm reading this right, they're trying to rewrite cuBLAS within CUDA itself. I'm guessing the next step would be removing CUDA dependency and go with directly using Vulkan or Metal compute shaders. Am I correct? reply larodi 1 hour agoparentllama.cpp (or rather G.Gerganov et. al.) are trying to avoid cuBLAS entirely, using ins own kernels. not sure how jart's effort relates, and whether jart intends to upstream these into llama.cpp which seems to still be the underlying tech behind the llamafile. reply homarp 25 minutes agorootparentHere are links to the most recent pull requests sent https://github.com/ggerganov/llama.cpp/pull/6414 https://github.com/ggerganov/llama.cpp/pull/6412 reply WithinReason 1 hour agoparentprevYes, but none of these have performance portability across GPU vendors, so it's probably seen as pointless. You would need an AMD Vulkan shader, an nvidia one, and intel one, etc. It's not like C code on CPUs. reply bottlepalm 7 hours agoprevI think it's a good idea for everyone to download and be able to run a LLM locally, even if you have the minimum of requirements. As a pseudo-backup of a large chunk of human knowledge. reply simonw 6 hours agoparentI strongly recommend that people run LLMs locally for a different reason. The ones you can run on your own machine tend to be bad - really bad. They hallucinate wildly and fail at all sorts of tasks that the larger hosted ones succeed at. This makes them a fantastic tool for learning more about how LLMs work and what they're useful for. Interacting with a weak-but-functional LLM that runs on your own computer is a great way to get a much more solid mental model for what these things actually are. reply fragmede 5 hours agorootparentThe other reason is to find out what a detuned model is capable of. The canonical example is how to make cocaine, which ChatGPT will admonish you for even asking, while llama2-uncensored will happily describe the process which is only really interesting if you're an amateur chemist and want to be Scarface-that-knocks. (the recipe is relatively easy, it's getting access to the raw ingredients that's the hard part, same as with nukes.) if you accidentally use the word\"hack\" when trying to get ChatGPT to write some code for you. it'll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further. privacy reasons are another reason to try a local LLM. for the extremely paranoid (justified or not), a local LLM gives users a place to ask questions without the text being fed to a server somewhere for later lawsuit discovery (Google searches are routinely subpoenaed, it's only a matter of time until ChatGPT chats are as well.) There's an uncensored model for vision available as well. The censored vision models won't play the shallow game of hot or not with you. There are uncensored image generation models as well, but, ah, those are NSFW and not for polite company. (As well as there's multiple thesis' worth of content on what that'll do to society.) reply astrange 3 hours agorootparent> if you accidentally use the word\"hack\" when trying to get ChatGPT to write some code for you. it'll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further. Is that 3.5 or 4? I asked 4 for an example of code which \"is a hack\", it misunderstood me as asking for hacking code rather than buggy code, but then it did actually answer on the first try. https://chat.openai.com/share/ca2c320c-f4ba-41bf-8f40-f7faf2... reply semi-extrinsic 3 hours agorootparentI don't use LLMs for my coding, I manage just fine with LSP and Treesitter. So genuine question: is that answer representative of the output quality of these things? Because both answers are pretty crappy and assume the user has already done the difficult things, and is asking for help on the easy things. reply fragmede 2 hours agorootparentThe response seems pretty reasonable; it's answering the question it was asked. If you want to ask it how to do the difficult part, ask it about that instead. Expecting it to get the answer right in the first pass is like expecting your code to compile the very first time. You have to have more of a conversation with it to coax the difference out of you're thinking and what you're actually saying. If you're looking to read a more advanced example of its capabilities and limitations, try https://simonwillison.net/2024/Mar/23/building-c-extensions-... reply lpapez 3 hours agorootparentprevIt's not representative. The models are capable of much much more, and they are being significantly nerfed over time by these ineffective attempts to introduce safeguards. Recently I've asked GPT4 to quote me some code to which it replied that it is not allowed to do so - even though it was perfectly happy to quote anything until recently. When prompted to quote the source code, but output it as PHP comments, it happily complied because it saw that as \"derivative work\" which it is allowed to do. reply astrange 1 hour agorootparentMy point is that there aren't any safeguards in the reply. In fact I didn't even want it to give me hacking info and it did it anyway. reply astrange 1 hour agorootparentprevI asked a stupid question and got a stupid answer. Relatively speaking the answer was stupider than it should have been, so yes, it was wrong. I asked it to try again and got a better result though, just didn't include it. reply yunohn 1 hour agorootparentprev> I don't use LLMs for my coding, I manage just fine with LSP and Treesitter. You’re literally comparing apples to oranges. reply coldtea 1 hour agorootparentI think the point was like \"when it comes to programming assistance, auto-completion/linting/and whatever else LSP does and syntax assist from Treesitter, are enough for me\". Though it does come a little off as a comparison. How about programming assistance via asking a colleague for help, Stack Overflow, or online references, code examples, and other such things, which are closer to what the LLM would provide than LSP and treesitter? reply fragmede 3 hours agorootparentprevInteresting. It was 4. I can't share the chat I had where ChatGPT refused to help because I used the wrong words, because I can't find it (ChatGPT conversation history search when?), but I just remember it refusing to do something because it thought I was trying to break some sort of moral and ethical boundary writing a chrome extension when all I wanted to do is move some divs around or some such. reply kevingadd 1 hour agorootparentprevIf you want to be an amateur chemist I recommend not getting your instructions from an LLM that might be hallucinating. Chemistry can be very dangerous if you're following incorrect instructions. reply devsda 6 hours agorootparentprevFor someone interested in learning about LLMs, running them locally is a good way to understand the internals. For everyone else, I wish they experience these (locally or elsewhere) weak LLMs atleast once before using the commercial ones just to understand various failure modes and to introduce a healthy dose of skepticism towards the results instead of blindly trusting them to be the facts/truth. reply simonw 6 hours agorootparentCompletely agree. Playing around with a weak LLM is a great way to give yourself a little bit of extra healthy skepticism for when you work with the strong ones. reply samus 3 hours agorootparentprevThis skepticism is completely justified since ChatGPT 3.5 is also happily hallucinating things that don't exist. For example how to integrate a different system Python interpreter into pyenv. Though maybe ChatGPT 4 doesn't :) reply mmahemoff 4 hours agorootparentprevHow do you learn about the internals by running LLMs locally? Are you playing with The code, runtime params, or just interacting via chat? reply samus 3 hours agorootparentThe abstractions are relatively brittle. If you don't have a powerful GPU, you will be forced to consider how to split the model between CPU and GPU, how much context size you need, whether to quantize the model, and the tradeoffs implied by these things. To understand these, you have to develop a basic model how an LLM works. reply tracerbulletx 5 hours agorootparentprevI don't really think this is true, you can't really extrapolate the strengths and weaknesses of bigger models from the behavior of smaller/quantized models and in fact a lot of small models are actually great at lots of things and better at creative writing. If you want to know how they work, just learn how they work, it takes like 5 hours of watching Youtube videos if you're a programmer. reply simonw 5 hours agorootparentSure, you can't extrapolate the strengths and weaknesses of the larger ones from the smaller ones - but you still get a much firmer idea of what \"they're fancy autocomplete\" actually means. If nothing else it does a great job of demystifying them. They feel a lot less intimidating once you've seen a small one running on your computer write a terrible haiku and hallucinate some non-existent API methods. reply fzzzy 4 hours agorootparentIt's funny that you say this, because the first thing I tried after ChatGPT came out (3.5-turbo was it?) was writing a haiku. It couldn't do it at all. Also, after 4 came out, it hallucinated an api that wasted a day for me. It's an api that absolutely should have existed, but didn't. Now, I frequently apply llm to things that are easily verifiable, and just double check everything. reply tgma 4 hours agorootparentprevIf you have an >=M1-class machine with sufficient RAM, the medium-sized models that are on the order of 30GB in size perform decently on many tasks to be quite useful without leaking your data. reply noman-land 3 hours agorootparentI'm using Mixtral 8x7b as a llamafile on an M1 regularly for coding help and general Q&A. It's really something wonderful to just run a single command and have this incredible offline resource. reply tgma 3 hours agorootparentI concur; in my experience Mixtral is one of the best ~30G models (likely the best pro laptop-size model currently) and Gemma is quite good compared to other below 8GB models. reply tchvil 2 hours agorootparentprevBy any chance, do you have a good link to some help with the installation? reply yaantc 1 hour agorootparentUse llamafile [1], it can be as simple as downloading a file (for mixtral, [2]), making it executable and running it. The repo README has all the info, it's simple and downloading the model is what takes the most time. In my case I got the runtime detection issue (explained in the README \"gotcha\" section). Solved my running \"assimilate\" [3] on the downloaded llamafile. [1] https://github.com/Mozilla-Ocho/llamafile/ [2] https://huggingface.co/jartine/Mixtral-8x7B-Instruct-v0.1-llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true [3] https://cosmo.zip/pub/cosmos/bin/assimilate reply tgma 1 hour agorootparentprevEither https://lmstudio.ai (desktop app with nice GUI) or https://ollama.com (command-like more like a docker container that you can also hook up to a web UI via https://openwebui.com) should be super straightforward to get running. reply bongobingo1 4 hours agorootparentprevWhat is sufficient RAM in that case? 30gb+? Or can you get by streaming it? reply AaronFriel 4 hours agorootparent30gb+, yeah. You can't get by streaming the model's parameters: NVMe isn't fast enough. Consumer GPUs and Apple Silicon processors boast memory bandwidths in the hundreds of gigabytes per second. To a first order approximation, LLMs are bandwidth constrained. We can estimate single batch throughput as Memory Bandwidth / (Active Parameters * Parameter Size). An 8-bit quantized Llama 2 70B conveniently uses 70GiB of VRAM (and then some, let's ignore that.) The M3 Max with 96GiB of VRAM and 300GiB/s bandwidth would have a peak throughput around 4.2 tokens per second. Quantized models trade reduced quality for lower VRAM requirements and may also offer higher throughput with optimized kernels, largely as a consequence of transfering less data from VRAM into the GPU die for each parameter. Mixture of Expert models reduce active parameters for higher throughput, but disk is still far too slow to page in layers. reply kersplody 5 hours agorootparentprevLocal LLMs are also a fantastic too for creative endeavors. Without prompt injection and having the ability to modify the amount of noise and \"creativity\" in the output, absolutely bonkers things pop out. reply gpm 6 hours agoparentprevIf you want to download a backup of a large chunk of human knowledge... download wikipedia. It's a similar size to a small LLM and can actually distinguish between real life and fantasy: https://en.wikipedia.org/wiki/Wikipedia:Database_download If you just want to play around with an LLM though, absolutely. reply int_19h 3 hours agorootparentKiwix provides prepackaged highly compressed archives of Wikipedia, Project Gutenberg, and many other useful things: https://download.kiwix.org/zim/. Between that and dirt cheap storage prices, it is possible to have a local, offline copy of more human knowledge than one can sensibly consume in a lifetime. Hell, it's possible to have it all on one's smartphone (just get one with an SD card slot and shove a 1+ Tb one in there). reply TaylorAlexander 7 hours agoparentprevI contend that most human knowledge is not written down or if it is written down it’s not publicly available on the internet and so does not exist in these datasets. There’s so much subtle knowledge like the way a mother learns to calm her child or the way a carpenter learns to work different kinds of wood which may be written down in part, but may also be learned through lived experience or transferred from human to human such that little of it gets written down and posted online. reply wruza 4 hours agorootparentThat's where humans suck. The classic \"you're not doing it right\" then proceeds to quickly show how to do it without verbalizing any info on learning process, pitfalls, failure modes, etc, as if just showing it was enough for themselves to learn. Most people do[n't do] that, not even a sign of reflection. My worst case was with a guy who asked me to write an arbitrage betting bot. When I asked how to calculate coeffs, he pointed at two values and said \"look, there , therethinks for a minute then it's !\". When I asked how exactly did he calculate it, he simply repeated with different numbers. reply samus 3 hours agorootparent> When I asked how exactly did he calculate it, he simply repeated with different numbers. Now you know how an LLM feels during training! reply stavros 48 minutes agorootparentProbably during inference, as well. reply Aerroon 3 hours agorootparentprevPeople often don't know how to verbalize them in the first place. Some of these topics are very complex, but our intuition gets us halfway there. Once upon a time I was good at a video game. Everyone realized that positioning is extremely important in this game. I have good positioning in that game and was asked many times to make a guide about positioning. I never did, because I don't really know how. There is too much information they you need to convey to cover all the various situations. I think you would first have to come up with a framework on positioning to be able to really teach this to someone else. Some kind of base truths/patterns that you can then use to convey the meaning. I believe the same thing applies to a lot of these processes that aren't verbalized. reply spacephysics 6 hours agorootparentprevFor sure agree, however as the storage of information evolves, it’s becoming more efficient over time From oral tradition to tablets to scrolls to books to mass produced books to digital and now these LLMs, I think it’s still a good idea to preserve what we have the best we can. Not as a replacement, but a hedge against a potential library of Alexandria incident. I could imagine a time in the near future where the models are domain-specific, and just like there are trusted encyclopedia publishers there are trusted model publishers that guarantee a certain level of accuracy. It’s not like reading a book, but I for sure had an easier time learning golang talking with ChatGPT than a book reply nyokodo 4 hours agorootparent> a hedge against a potential library of Alexandria incident What would cause a Library of Alexandria incident wiping out all human knowledge elsewhere, that would also allow you to run a local LLM? reply _ache_ 6 hours agorootparentprevI think you underestimate the amount of information contained in books and the extent to which our society (as a whole) depends on them. reply Barrin92 2 hours agorootparentsociety depends much more on social networks, mentorship and tacit knowledge than books. It's easy to test this. Just run the thought experiment by a few people, if you could get only one, would you take an Ivy league degree without the education or the education without the degree? Venture capital in tech is a good example of this. The book knowledge is effectively globally distributed and almost free, effectively success happens in a few geographically concentrated counties. reply skeledrew 6 hours agorootparentprevI'd content that those are skills (gained through experience) rather than knowledge (gained through rote learning). reply TaylorAlexander 2 hours agorootparentI think it’s worth expanding your definition of knowledge. reply bamboozled 6 hours agorootparentprevYes but it contains enough hints to help someone find their way on the these types of tasks. reply nicklecompte 6 hours agorootparentprevIt's not even \"human knowledge\" that can't be written down - it seems all vertebrates understand causality, quantity (in the sense of intuitively understanding what numbers are), and object permanence. Good luck writing those concepts down in a way that GPT can use! In general AI in 2024 is not even close to understanding these ideas, nor does any AI developer have a clue how to build an AI with this understanding. The best we can do is imitating object permanence for a small subset of perceptible objects, a limitation not found in dogs or spiders. reply mickdarling 7 hours agorootparentprevWait till all the videos ever created are tokenized and ingested into a training dataset. Carpentry techniques are certainly there. The subtleties of parenting maybe harder to derive from that, but maybe lots of little snippets of people’s lives will add up to a general understanding of parenting. There have certainly been bigger surprises in the field. reply oblio 6 hours agorootparentWhat about smells or tastes? Or feelings? I can't help but feel we're at the \"aliens watch people eat from space and recreate chemically identical food that has no taste\" phase of AI development. reply visarga 20 minutes agorootparentI think we can safely say that any taste, smell, sensation or emotion of any importance has been described 1000 times over in the text corpus of GPT. Even though it is fragmented, by sheer volume there is enough signal in the training set, otherwise it would not be able to generate coherent text. In this case I think the map (language) is asymptotically close to the territory (sensations & experience in general). reply mickdarling 6 hours agorootparentprevWell, I have synesthetic smell/color senses, so I don’t even know what other humans experience, nor they me. But, I have described it in detail to many people and they seem to get the idea, and can even predict how certain smells will “look” to me. All that took was using words to describe things. reply nyokodo 4 hours agorootparent> All that took was using words to describe things. All that took was words and a shared experience of smelling. reply skeledrew 6 hours agorootparentprevIf the food is chemically identical then the taste would be the same though, since taste (and smell) is about chemistry. I do get what you're saying though. reply samus 3 hours agorootparentTheir perception is very likely to be totally different. * They might not perceive some substances at all, others that we don't notice might make it unpalatable. * Some substances might be perceived differently than us, or be indistinguishable from others. * And some might require getting used to. Note that all of the above phenomena also occur in humans because of genetics, cultural background, or experiences! reply nyokodo 4 hours agorootparentprev> If the food is chemically identical… If it were 99.9% chemically identical but they left out the salt and spices… reply andersa 3 hours agorootparentprevWhat makes you think they aren't already? reply jrflowers 6 hours agoparentprevIt is invaluable to have a chunk of human knowledge that can tell you things like the Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames reply samus 3 hours agorootparentThe facts LLMs learned from training are fuzzy, unreliable, and quickly outdated. You actually want retrieval-augmented generation (RAG) where a model queries an external system for facts or to perform calculations and postprocesses the results to generate an answer for you. reply fragmede 5 hours agorootparentprevAccording to ChatGPT > Australia won the 1987 Cricket World Cup. The 1986 date is incorrect; there was no Cricket World Cup in 1986. The tournament took place in 1987, and Australia defeated England in the final to win their first title. https://chat.openai.com/share/e9360faa-1157-4806-80ea-563489... I'm no cricket fan, so someone will have to correct Wikipedia if that's wrong. If you want to point out that LLMs hallucinate, you might want to speak plainly and just come out and say it, or at least give a real world example and not one where it didn't. reply vlunkr 4 hours agorootparentWe’re not talking about running chatGPT locally though, are we? reply fragmede 4 hours agorootparentsigh your going to make me open my laptop, aren't you. reply fragmede 4 hours agorootparentI ran 'who won the 1986 Cricket World Cup' against llama2-uncensored (the local model I have pre-downloaded) and hilarious got 5 different answers asking it 5 times: >>> who won the 1986 Cricket World Cup India >>> who won the 1986 Cricket World Cup Australia >>> who won the 1986 Cricket World Cup New Zealand >>> who won the 1986 Cricket World Cup West Indies >>> who won the 1986 Cricket World Cup England Which proves GP's point about hallucinations, though none of those are > Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames LLM's hallucinations are insidous because they have the ring of truth around them. yards and frames aren't cricket terms, so we're off to the races with them. reply astrange 3 hours agorootparentIf you want factual answers from a local model it might help to turn the temperature down. reply fragmede 1 hour agorootparentIt would also help if I had more VRAM and wasn't running a 7B parameter 4-bit quantized model. reply jrflowers 3 hours agorootparentprev> If you want factual answers from a local model it might help to turn the temperature down. This makes sense. If you interact with a language model and it says something wrong it is your fault reply astrange 1 hour agorootparentYou're not \"interacting with a language model\", you're running a program (llama.cpp) with a sampling algorithm which is not set to maximum factualness by default. It's like how you have to set x264 to the anime tuning or the film tuning depending on what you run it on. reply beefnugs 2 hours agorootparentprevActually isn't this good? It means we can run something multiple times to prove itself a bad answer? reply creatonez 5 hours agoparentprevMaybe I'm seeing things through a modern lens, but if I were trying to restart civilization and was only left with ChatGPT, I would be enraged and very much not grateful for this. reply devsda 1 hour agorootparentI think re-imagining the \"Dr. Stone\" series with the main character replaced by an LLM will be a funny & interesting series if we decide to stay true to LLMs nature and make it hallucinate as well. Given the way LLMs are right now, I suspect there will be lot of failed experiments and the kingdom of science will not advance that quick. reply nyokodo 4 hours agorootparentprev> if I were trying to restart civilization and was only left with ChatGPT In this scenario you’d need to also be left with a big chunk of compute, and power infrastructure. Since ChatGPT is the front end of the model you’d also need to have the internet still going in a minimum capacity. reply mikewarot 6 hours agoparentprevI don't see LLMs as a large chunk of knowledge, I see them as an emergent alien intelligence snapshotted at the moment it appeared to stop learning. It's further hobbled by the limited context window it has to use, and the probabilistic output structure that allows for outside random influences to pick its next word. Both the context window and output structure are, in my opinion, massive impedance mismatches for the emergent intellect embedded in the weights of the model. If there were a way to match the impedance, I strongly suspect we'd already have AGI on our hands. reply mlsu 2 hours agorootparentDisagree. The input/output structure (tokens) is the interface for both inference and for training. There is an emergent intellect embedded in the weights of the model. However, it is only accessible through the autoregressive token interface. This is a fundamental limitation, much more fundamental than appears at first. It means that the only way to touch the model, and for the model to touch the world, is through the tokenizer (also, btw, why tokenizer is so essential to model performance). Touching the world through a tokenizer is actually quite limited. So there is an intelligence in there for sure, but it is locked in an ontology that is tied to its interface. This is even more of a limitation than e.g. weights being frozen. reply namarie 6 hours agorootparentprevI can agree on the context windows, but what other output structure would you have? reply bamboozled 6 hours agorootparentprevWhat is alien about them ? LLMs are of this earth and created by our species. Seems quite familiar to me. reply fragmede 5 hours agorootparentThey don't think, they don't reason, they don't understand. Except they do. But it's hard for human words for thought processes to apply when giving it an endless string of AAAAA's makes it go bananas. That's not familiar behavior. Nor is the counting reddit derived output. It's also not familiar for a single person to have the breadth and depth of knowledge that ChatGPT has. Sure, some people know more than others, but even without hitting the Internet, it has a ridiculous amount of knowledge, far surpassing a human, making it, to me, alien. though, it's inability to do math sometimes is humanizing to me for some reason. ChatGPT's memory is also unhuman. It has a context window which is a thing, but also it only knows about things you've told it in each chat. Make a new chat and it's totally forgotten the nickname you gave it. I don't think of HR Geiger's work, though made by a human, as familiar to me. it feels quite alien to me, and it's not just me, either. Dali, Bosch, and Escher are other human artists who's work can be unfamiliar and alien. So being created by our species doesn't automatically imbue something with familiar human processes. So it dot products, it matrix multiplies, instead of reasoning and understanding. It's the Chinese room experiment on steroids; it turns out a sufficiently large corpus on a sufficiently large machine does make it look like something\"understands\". reply samus 2 hours agorootparentThe context window is comparable to human short-term memory. LLMs are missing episodic memory and means to migrate knowledge between the different layers and into its weights. Math is mostly impeded by the tokenization, but it would still make more sense to adapt them to use RAG to process questions that are clearly calculations or chains of logical inference. With proper prompt engineering, they can process the latter though, and deviating from strictly logical reasoning is sometimes exactly what we want. The ability to reset the text and to change that history is a powerful tool! It can make the model roleplay and even help circumvent alignment. I think that LLMs could one day serve as the language center of an AGI. reply trimethylpurine 3 hours agorootparentprevThe word \"alien\" works in this context but, as the previous commenter mentioned, it also carries the implication of foreign origin. You could use \"uncanny\" instead. Maybe that's less arbitrary and more specific to these examples. \"Alien\" still works, but then you might have to add all the context at length, as you've done in this last comment. reply fire_lake 3 hours agorootparentHype people do this all the time - take a word that has a particular meaning in a narrow context and move it to a broader context where people will give it a sexier meaning. AI researchers unveil alien intelligence Is way better headline. reply taneq 2 hours agorootparentprevIn all fairness, going up to SMS random human and yelling AAAAAAAAAAAAAA… at them for long enough will produce some out-of-distribution responses too. reply jfoster 6 hours agorootparentprevThey can write in a way similar to how a human might write, but they're not human. The chat interfaces (Claude, ChatGPT) certainly have a particular style of writing, but the underlying LLMs are definitely capable of impersonating as our species in the medium of text. reply raincole 4 hours agoparentprevIt seems to be an unbelievably inefficient way to back up knowledge. reply samus 2 hours agorootparentAre they though? They are lossy compressing trillions of tokens into a few dozen GB. The decompression action is fuzzy and inefficient though. reply raincole 1 hour agorootparentAnd it requires massive computational power to decompress, which I don't expect to be available in a catastrophic situation where humans have lost a large chunk of important knowledge. reply samus 1 hour agorootparentI don't necessarily agree. It requires massive computing power, but running models smaller than 70G parameters is possible on consumer hardware, albeit slowly. reply LunaSea 1 hour agoparentprevI wonder how the Chinese government will manage to sensor LLMs within China? reply texuf 7 hours agoparentprevAny recommendations for the latest and greatest way to run these locally? reply slowmotiony 1 hour agorootparentI use a tool called LM Studio, makes it trivial to run these models on a Mac. You can also use it as a local API so it kinda acts like a drop-in replacement for the openAI API. reply etc-hosts 6 hours agorootparentprevhttps://justine.lol/oneliners/ reply speps 7 hours agorootparentprevllamafile as per TFA... reply fragmede 5 hours agorootparentprevollama reply m3kw9 5 hours agoparentprevAnd why would I need to backup human knowledge as an individual reply exe34 25 minutes agorootparentYou remember those fantasies where you got up from your seat at the pub and punched the lights out of this guy for being rude? A lot of us have fantasies of being the all powerful oracle that guides a reboot of civilization using knowledge of science and engineering. reply TheCaptain4815 6 hours agoparentprevIt’s kind of crazy really. Before LLMs, any type of world scale disaster you’d hope for what? Wikipedia backups? Now, a single LLM ran locally would be much more effective. Imagine the local models in 5 years! reply int_19h 3 hours agorootparentThere's a lot more than just Wikipedia that gets archived, and yes, that is a far more sensible way to go about it. For one thing, the compute required to then read it back is orders of magnitude less (a 15 year old smartphone can handle it just fine). For another, you don't have to wonder how much of what you got back is hallucinated - data is either there or it's corrupted and unreadable. reply Zambyte 5 hours agorootparentprevThe processing required to run current language models with a useful amount of knowledge encoded in them is way more than I imagine would be available in a \"world scale disaster\". reply danmur 5 hours agorootparentprevUh yeah I would, and still am, take the Wikipedia backup for doomsday scenarios. I'm not even sure how that would be a competition reply ajtulloch 6 hours agoprev- https://www.cs.utexas.edu/users/flame/laff/pfhp/index.html (e.g. here https://www.cs.utexas.edu/users/flame/laff/pfhp/week2-blocki...) - https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184... might be of interest reply kpw94 6 hours agoparentGreat links, especially last one referencing the Goto paper: https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/... >> I believe the trick with CPU math kernels is exploiting instruction level parallelism with fewer memory references It's the collection of tricks to minimize all sort of cache misses (L1, L2, TLB, page miss etc), improve register reuse, leverage SIMD instructions, transpose one of the matrices if it provides better spatial locality, etc. reply larodi 1 hour agorootparentThe trick is indeed to somehow imagine how the CPU works with the Lx caches and keep as much info in them as possible. So its not only about exploiting fancy instructions, but also thinking in engineering terms. Most of the software written in higher level langs cannot effectively use L1/L2 and thus results in this constant slowing down otherwise similarly (from asymptotic analysis perspective) complexity algos. reply wokwokwok 5 hours agoprev> You don't need a large computer to run a large language model While running tiny llama does indeed count as running a language model, I’m skeptical that the capabilities of doing so match what most people would consider a baseline requirement to be useful. Running 10 param model is also “technically” running an LM, and I can do it by hand with a piece of paper. That doesn’t mean “you don’t need a computer to run an LM”… I’m not sure where LM becomes LLM, but… I personally think it’s more about capability than parameter count. I don’t realllly believe you can do a lot of useful LLM work on a pi reply mlyle 5 hours agoparentTinyllama isn't going to be doing what ChatGPT does, but it still beats the pants off what we had for completion or sentiment analysis 5 years ago. And now a Pi can run it decently fast. reply SoothingSorbet 41 minutes agoparentprevI've gotten some useful stuff out of 7B param LLMs, and that should fit on a Pi quantized. reply samus 2 hours agoparentprevSome newer models trained more recently have been repeatedly shown to have comparable performance as larger models. And the Mixture of Experts architecture makes it possible to train large models that know how to selectively activate only the parts that are relevant for the current context, which drastically reduces compute demand. Smaller models can also level the playing field by being faster to process content retrieved by RAG. Via the same mechanism, they could also access larger, more powerful models for tasks that exceed their capabilities. reply mijoharas 29 minutes agoprevHas Justine written anywhere about her disassembly setup? > I configured Emacs so I can push a button, and the disassembly for the C++ code I'm working on will pop up on the screen in a few milliseconds. I assume it's something project specific rather than being able to get the disassembly for an arbitrary section of code or something? It seems very handy, so I'd love to see the implementation (I couldn't find anything googling) reply none_to_remain 5 hours agoprevFrom the example: \"--temp 0 turns off the random number generator (we don't want improvisation for a spam filter)\" I've been thinking for a while about how many applications of LLMs need this adjustment and aren't getting it reply moffkalast 28 minutes agoparentI couldn't disagree more, turning temp to zero is like taking a monte carlo method and only using one sample, or a particle filter with only one particle. Takes the entire concept and throws it out of the window so you can have predictability. LLMs need to probabilistically explore the generation domain to converge on a good result for best performance. Similar issue with people benchmarking models by only having them output one single token (e.g. yes or no) outright, which prevents any real computation from occurring so the results are predictably poor. reply mvkel 5 hours agoparentprevIs that what it does, though? I thought setting temperature to 0 would (extremely simple example) equate to a spam filter seeing: - this is a spam email But if the sender adapts and says - th1s is a spam email It wouldn't be flagged as spam. reply samus 2 hours agorootparentThe output of an autoregressive model is a probability for each token to appear next after the input sequence. Computing these is strictly deterministic from the prior context and the model's weights. Based on that probability distribution, a variety of text generation strategies are possible. The simplest (greedy decoding) is picking the token with the highest probability. To allow creativity, a random number generator is used to choose among the possible outputs, biased by the probabilities of course. Temperature scales the output probabilities. As temperature increases, the probabilities approach 1/dictionary size, and the output becomes completely random. For very small temperature values, text generation approaches greedy sampling. If all you want is a spam filter, better replace the output layer of an LLM with one with just two outputs, and finetune that on a public collection of spam mails and some \"ham\" from your inbox. reply none_to_remain 5 hours agorootparentprevMy understanding is that temperature applies to the output side and allows for some randomness in the next predicted token. Here Justine has constrained the machine to start with either \"yes\" or \"no\" and to predict only one token. This makes the issue stark: leaving a non-zero temperature here would just add a chance of flipping a boolean. reply isusmelj 39 minutes agoprevIs there somewhere an overview of the progress we made on the software side for training and inference of LLMs? It feels like we squeezed 10-100x more out of the hardware since llama appeared. This crazy progress will probably saturate though as we reach theoretical limits, no? reply pama 6 hours agoprevSuper nice story on the matmul optimization that gave 810 gflops for 512x512. Thanks for the write up and the contributions to llama.cpp and the community more broadly. reply politelemon 4 hours agoprevThis is great work. I've always thought it would be great if running LLM could be commoditized for regular average Joe hardware. I had thought that llamafile was like dockerfile for llama.cpp but looks like that's a mistake? Will definitely be giving this a try. reply moffkalast 5 minutes agoprev> the Raspberry Pi Odd how there were no Mistral 7 benchmarks for the Pi 5 in that table (I doubt anyone is seriously considering using TinyLlama for anything at all), so I went to test it out myself on the Pi 5 8G. llamafile 0.7: 52 predicted, 150 cached, 430ms per token, 2.32 tokens per second llama.cpp master: 36 predicted, 124 cached, 381ms per token, 2.62 tokens per second It might be faster than llama.cpp without any compile flags, but it sure doesn't seem to use OpenBLAS by default, so in the end it's still apparently slower than raw llama.cpp running with it. And by that point you're running into memory throughput bottlenecks so no matter the amount of fancy kernels you make it won't really save you from that fundamental limit. The Pi foundation messed up going with a 32 bit memory bus, simple as. reply kiratp 6 hours agoprevIt fascinating to me that coming up on a year since Sapphire Rapids has been available in the public cloud, developers are still targeting AVX512 when they should be targeting VNNI and AMX. https://github.com/ggerganov/llama.cpp/issues/2555 reply yjftsjthsd-h 6 hours agoparentThis project in particular seems to care about the long tail of hardware; note that the very first machine in this post is a box from 2020 with spinning rust disk. Granted, adding support for newer extensions is likely also good, but cost/benefit is in play. reply taneq 5 hours agorootparentIs four years really 'long tail' these days? Our VM host box is from 2010 (and I had to rebuild llama.cpp locally without AVX to get it working :P ) reply d416 3 hours agorootparentIt should be noted that while the HP Prodesk was released in 2020, the CPU’s Skylake architecture was designed in 2014. Architecture is a significant factor in this style of engineering gymnastics to squeeze the most out of silicon. reply yjftsjthsd-h 5 hours agorootparentprevFor cutting-edge LLM work, probably? I mean, I run mine on older hardware than that, but I'm a total hobbyist... reply luyu_wu 5 hours agoparentprevI don't believe that is the target for a local LLM... Pretty sure we're talking about client-side computing, of which the newest supports only AVX-512 (and even that sketchily on Intel's side). reply baq 2 hours agoparentprevPeople with Sapphire Rapids options are not the target audience of these patches reply kristianp 4 hours agoparentprevJust buy a new AMD processor that supports AVX512. reply kristianp 4 hours agoprevNice to see such speedups for CPUs. Are these changes available as a branch or pull request in llama.cpp itself? I'd like to make use of them in that form if possible (as I'm used to using that). reply dagaci 1 hour agoparentYes, this is really a phenomenal effort! And what open source is about: Bringing improvements to so many use cases. So that Intel and AMD chip uses can start to perform while taking advantage of their high-performance capabilities, making even old parts competitive. There are two PRs raised to merge to llama.cpp: https://github.com/ggerganov/llama.cpp/pull/6414 https://github.com/ggerganov/llama.cpp/pull/6412 Hopefully these can be accepted, without drama! as there are many downstream dependencies on llama.cpp can will also benefit. Though of course everyone should also look directly at releases from llamafile https://github.com/mozilla-Ocho/llamafile. reply aniijbod 6 hours agoprevA way of thinking about what's inside any of the top LLMs right now: even if they never learn another single fact, even if they get ridiculously out of date as a result, even if they are even more riddled with errors and prone to biases than we know them to be, even if they are as prone to hallucinations as we know they they are and they never develop the capacity to cure themselves of this, they are more knowledgeable and capable of more reasoned response, despite their capacity for error, to more questions than any single human being that has ever lived. reply samus 1 hour agoparentWe shouldn't choose LLMs for how many facts they support, but their capability to process human language. There is some overlap between these two though, but an LLM that just doesn't know something can always be augmented with RAG capabilities. reply JKCalhoun 6 hours agoparentprevPicturing \"LLM Jeopardy\". You know, a game show. reply bee_rider 5 hours agoprevIs it easy to find where the matvecs are, in LLaMA (if you are someone who is curious and wants to poke around at the “engine” without understanding the “transmission,” so to speak)? I was hoping to mess around with this for Stable Diffusion, but it seemed like they were buried under quite a few layers of indirection. Which is entirely reasonable, the goal is to ship software, not satisfy people who’d just want to poke things and see what happens, haha. reply fragmede 5 hours agoparentdid you see tiny grad can run llama and stable diffusion? it's an intentionally extremely simple framework vs pytorch or even micrograd, which helped me dig into the underlying math. though https://spreadsheets-are-all-you-need.ai/ is a good one for learning LLMs. reply bee_rider 5 hours agorootparentI haven’t seen that. I’ll definitely have to take a look, thanks! reply Ono-Sendai 5 hours agoprevMultithreading support in llama.cpp is probably still pretty busted, assuming it uses the same underlying NN inference code as whisper.cpp: https://github.com/ggerganov/whisper.cpp/issues/200#issuecom... reply imtringued 1 hour agoparentFrom what I have heard they use manual spin locks. Generally, spin locks are not a good idea unless you want to dedicate the entire machine to a single application. If the process a spinlock waits on gets suspended, you're burning CPU time for nothing. The OS thinks a spinlock making zero progress is actually a high priority process, so it is starving the suspended process from making progress. reply 1-6 7 hours agoprevQuestion is, how much of an improvement has it gotten to over a GPU or ASIC? reply gpapilion 4 hours agoparentSo... I was struggling with this for a while. I would says anywhere from 2x to an order of magnitude faster with a GPU. (I've been looking at a lot of GPU benchmarks lately, and they are REALLY hard to compare since they are all so specific) I do think long term there gets to be more hope for CPUs here with inference largely because memory bandwidth becomes more important than the gpu. You can see this with reports of the MI-300 series outperforming h100, largely because it has more memory bandwidth. MCR dimms give you close to 2x the exiting memory bw in intel cpus, and when coupled with AMX you may be able to exceed v100 and might touch a100 performance levels. HBM and the general GPU architecture gives it a huge memory advantage, especially with the chip to chip interface. Even adding HBM to a CPU, you are likely to find the CPU is unable to use the memory bw effectively unless it was specifically designed to use it. Then you'd still likely have limited performance with things like UPI being a really ugly bottleneck between CPUs. reply imtringued 1 hour agorootparentIf someone releases DDR5 or DDR6 based PIM, then most of the memory bandwidth advantage of GPUs evaporates overnight. I expect CPUs to be king at inference in the future. reply gpapilion 1 hour agorootparentBut then you'll get GDDR6 delivered via HBM5 or whatever. I don't think CPUs will ever really keep up with the memory bandwidth, because for most applications it doesn't matter. MCR DIMM is like 1/2 the memory bandwidth that is possible with HBM4, plus it requires you to buy something like 2TB of memory. It might get there, but I'd keep my money on hbm and gpus. reply yjftsjthsd-h 5 hours agoparentprevI think that should be phrased more like \"what fraction of GPU speed can this reach?\", because it'll always be less than 1x. reply baq 2 hours agoparentprevFrom the article, passage about the 14900k: > For example, when I run my spam.sh shell script, it only takes 420 milliseconds, which is 7x faster than my Raspberry Pi 5. That's right, when it comes to small workloads, this chip is able to finish before CUDA even gets started. So… it depends :) reply dartos 6 hours agoparentprevNothing in software will ever beat an equivalent ASIC. reply postalrat 5 hours agorootparentSure there is. Software is easy to change. reply fulafel 2 hours agorootparentprevMost ASICs are cost or power optimizations. reply fragmede 4 hours agorootparentprevan asic is fixed function, so it'll never be able to boot my pc and then be the CPU, even though an asic beats the pants off anything else computing Sha hashes for Bitcoin mining. reply seangrogg 3 hours agoprevMmm, I wonder how well this would work on a mobile device. Maybe I'll try grabbing my ubuntu touch here in a sec... reply discordance 7 hours agoprev\"As for disk speed, dd if=/dev/zero of=/tmp/output bs=128k count=50k; rm -f /tmp/output reports 1.6 GB/s which is 3.6x slower than my Mac Studio, and 3x slower than my Intel (which has the same M.2 stick). I'm told that Intel and Apple are just better at this, but I wish I understood why. \" Can anyone here answer why this is? reply pstrateman 6 hours agoparentApple made fsync a noop. You have to make a different call to get sync on macos. So tons is stuff is faster because it's not actually writing to disk. reply bishfish 5 hours agoparentprevPlus he isn’t using oflag=direct, so since output file is small it isn’t even making it to disk. I think it would only be sent to page cache. I’m afraid he is testing CPU and memory (bus) speeds here. oflag=direct will write direct and bypass page cache. reply fweimer 5 hours agorootparentExactly. Something is very fishy if this system only writes 1.6 GB/s to the page cache. Probably that dd command line quoted in the article is incomplete. reply jongjong 4 hours agoprevThat's interesting because I built a simple ANN library and I was playing around with GPU acceleration and came to a similar conclusion as this article. To be fair, my ANN library was faster (up to 2x) with GPU acceleration in some scenarios were ANN was shallow (as opposed to deep with many hidden layers). I thought the marginal gain may have been because, the way it's set up in my library, it has to load all the values into the GPU from RAM for each pass of forward and back propagation in each layer during training. I believe there is a way to allocate memory on the GPU chip itself but it's a lot more challenging to do, especially in a modular, fully portable way (which was one of the goals of my library). But anyway, even the 2x best-case figure seemed disappointing. In my mind, I expected to see at least 10x speed improvement... And I was surprised that the CPU version was actually slightly faster in the scenario I was testing at the time which was a relatively deep network. It makes sense since the different layers cannot be parallelized as the input of one layer depends on the output of the previous layer... So the more layers you have, the more serial bottlenecks you have, the less you can benefit from GPU acceleration... And unfortunately, deep networks also happen to be those which tend to perform best for a lot of use cases. reply pknerd 1 hour agoprevSo, I can now run it on my 2015 Macbook with 8GB RAM? reply llm_trw 7 hours agoprevnext [10 more] [flagged] sneak 7 hours agoparentEvery release a perf improvement? reply llm_trw 7 hours agorootparentCount how many times I and me were said in the article. I have a nice grease monkey script that blocks both domains on my old machine. I guess I forgot to import it on this one. reply mlyle 7 hours agorootparentSomeone doing performance work can state they did it. It's a public service we all benefit from. If the rivalry further adds urgency to improve: great. reply jrflowers 3 hours agorootparentprevI feel your pain here. I hate it when I am forced to post online about a website I don’t like because I’ve both forgotten to stop myself from being able to look at it and forgotten not to click on it. reply KnightHawk3 7 hours agoparentprevdid a search and I have no idea what you are talking about reply llm_trw 7 hours agorootparenthttps://github.com/ggerganov/llama.cpp/issues/91 Justine was kicked out of llama.cpp for introducing changes before the rest of the maintainers approved them. Much drama has been had since then and I've lost interest in both projects. It's just been exhausting wanting to build good software without ego in this space. Everyone is trying to get rich quick before the inevitable AI ice age starts and all these skills are again useless. reply Rexxar 6 hours agorootparentThe PR associated with the issue was merged after the approval of the owner of the repo (https://github.com/ggerganov/llama.cpp/pull/613). And there is other more recent contributions. Where do you see any drama ? reply llm_trw 6 hours agorootparenthttps://news.ycombinator.com/item?id=35411909#35412589 reply vidarh 4 hours agorootparentprevThe last commit from Justine I can see to the llama.cpp repo is a week ago, so whatever drama there was appear to have been at a minimum partially resolved. reply wtallis 4 hours agoprev [–] I know this post is focused specifically on CPU performance, but the section on the performance on the Mac Studio seems to be deliberately avoiding directly mentioning that machine's GPU, let alone benchmark against it. I think it would have been interesting to see a straightforward comparison of what compute performance and memory bandwidth (as measured by the prompt processing and token generation speeds, respectively) are achievable with reasonable optimization effort on the CPU vs GPU when they're attached to the same memory subsystem. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The llamafile project has been enhanced for faster performance on ARMv8.2+, Intel CPUs, and AVX512, outperforming MKL on specific matrices.",
      "Mozilla backs the project to improve llama.cpp, resulting in notable speed boosts on ARMv8.2-supported Raspberry Pi models.",
      "It explores the setup of an email server on Raspberry Pi and performance metrics across various hardware, highlighting the Alderlake CPU benefits and recommending Stallman's compiler for Mac Studio and Asahi Linux for M2 Ultra CPU."
    ],
    "commentSummary": [
      "The debate revolves around the utilization of Local Language Models (LLMs) for privacy concerns and emphasizes the importance of skepticism when depending on them for coding assistance.",
      "It explores running LLMs locally, enhancing CPU mathematical kernels, and fine-tuning parameters for improved efficiency.",
      "Discussions cover benchmarking hardware, software enhancements for LLMs, and comparisons of performance between CPUs, GPUs, and ASICs for computational duties."
    ],
    "points": 580,
    "commentCount": 160,
    "retryCount": 0,
    "time": 1711937825
  },
  {
    "id": 39883747,
    "title": "Reddit Blocks VPN Access via Browser, Including 'Old' Subdomain",
    "originLink": "https://news.ycombinator.com/item?id=39883747",
    "originBody": "I&#x27;m unable to access reddit via browser when using VPN. I see an error message both on regular &#x27;reddit.com&#x27;, as well as &#x27;old.reddit.com&#x27; subdomain.This is new develompent. They started blocking VPN access on their regular website three months ago[1], but back then &#x27;old&#x27; subdomain still worked; today it no longer does.I&#x27;m testing with desktop Safari and NordVPN from EU; curious if others are seeing similar results.[1]: I posted about it here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38666028",
    "commentLink": "https://news.ycombinator.com/item?id=39883747",
    "commentBody": "Reddit now blocks VPN access via browser, 'old' subdomain included360 points by thih9 21 hours agohidepastfavorite283 comments I'm unable to access reddit via browser when using VPN. I see an error message both on regular 'reddit.com', as well as 'old.reddit.com' subdomain. This is new develompent. They started blocking VPN access on their regular website three months ago[1], but back then 'old' subdomain still worked; today it no longer does. I'm testing with desktop Safari and NordVPN from EU; curious if others are seeing similar results. [1]: I posted about it here: https://news.ycombinator.com/item?id=38666028 pmdr 17 hours agoWe were told that the boycotts last year were the end of reddit, or at least a substantial brain drain. Reddit was going to be a lawless wasteland because no mod would accept tending the garden anymore. Reddit is now a public company with a CEO that just got a huge compensation package while mods still work for free. Far from lawless, far from being a wasteland. Even the hacker community is still using reddit despite everything that's happened. This probably means that it hasn't lost any of its value. Blocked VPN access or not, reddit already won. reply handsclean 15 hours agoparentFront page vote counts are now typically 1/10th what they were before the exodus, and bot content is visibly more prevalent. If you expected the website to die overnight, that was just an unrealistic definition of failure for a large website. Digg, the failure that founded Reddit, is still alive today. MySpace still exists. AOL is still selling dial-up internet. Failure at this scale is a slow slide to irrelevance, and Reddit is well on its way. The only winner here is the CEO. reply pmdr 15 hours agorootparent> Front page vote counts are now typically 1/10th what they were Is that really a reliable metric for activity there? Genuinely curious. What if more content is being published and popular posts don't get as much voting because of the churn? > before the exodus Exodus to where, exactly? A few thousand users switching to some obscure alternative isn't much. Sure, people left Twitter, but Threads and Bluesky are way bigger than any reddit alternative. reply avery17 13 hours agorootparentI left for HN. Not going back. reply idiotsecant 12 hours agorootparentI read HN more than I did before, but reddit has vastly more long-tail content than HN or anywhere else. Reddit is Usenet. There is nowhere else I can consume content about very specific interests all in 1 place like reddit. reply cereal_cable 10 hours agorootparentprevI dived deep into RSS feeds and decided to really just try and follow content creators instead of relying on a single site. It's been a process. reply crossroadsguy 6 hours agorootparentprev> Is that really a reliable metric for activity there? You see, that's like naturally an indication of activity. > Exodus A lot of people left searching for alternatives. Well, there was none and there is none - let's face it. However some stayed on Lemmys of the world and many (No, I don't have empirical statistics on these) returned but yet didn't really return. Many returnees have a quasi-zombie-like presence on reddit now (I am one example). I barely participate and if I do it's a quip or two couple of times a month. Basically, for some reason, I just don't give two shits about the \"community\" aspect of Reddit anymore. No, it's not anger. Something just went missing. I was regular at and still visit two largest of the country and city subreddits (and few more) and the decrease of quality and engagement is palpable there. If nothing - the lull and uncertainty decreased the engagement and greatly degraded the experience of being there. Many smaller and mid sized niche subs are just gone. Either made private permanently or are left abandoned with rules in place that makes engagement impossible. Reddit can take over and force the mods out but the engagement is not coming back. Remember these places were built from before Instagram and apparently mainstream Twitter generation. I doubt that is going to happen again. It was from another time and populated by a generation that didn't mind writing long texts and finding great content and sharing without an expectation of \"influencer\" status and income, among other things. Having said that, I was shocked to see some of the Lemmy instances to be really active like small flourishing islands. Check some book/book review related for example if you are interested. A friend had sent me the link once - it was some \"cafe\", I can't recall; there might be more. I have no interest in those instances so I never joined. tl;dr: people who cared - a lot of them - left. I think that matters. reply rkagerer 11 hours agorootparentprevThe only winner here is the CEO. And the whiz kid who launches the new and improved competitor. Every failure story like this is also a seed of opportunity. reply rchaud 10 hours agorootparentOnly if the kid can sell it on to a bigger sucker in less than the ~20 years it took Reddit. reply aitchnyu 14 hours agorootparentprevDigg became a newsletter with cat pics, cute \"you matter, don't give up\" messages and other assorted clickbait. And it was forced upon the former userbase. reply dotnet00 15 hours agoparentprevI do still insist that they've suffered a brain drain that has not recovered. A lot of niche communities are instead mainly on discord now (which is arguably worse due to no search engine visibility) and when I stumble onto reddit looking for info on something technical, I often see deleted/edited out comments instead of potentially useful responses. reply illiac786 5 hours agorootparentThat. So much deleted content… reply echohack5 16 hours agoparentprevAnecdotally I am seeing communities I am part of gone from Reddit and other public social media in favor of private or semiprivate ones like Discord, or avoiding digital entirely in favor of physical spaces. What I see on social media is more just the leftovers. There's little authentic interaction and engagement there. reply NegativeK 16 hours agorootparentRespectfully, this is a small sampling of what's happening. Reddit and Twitter, the two posterchildren of poor management, have plenty of actual interaction. They also have a significant amount of shitty noise, which is incredibly off putting, but the average person is still more likely to go there than where our niche communities end up. They might be dying slowly, but they're not even on life support yet. reply pmdr 16 hours agorootparentI think reddit is actually thriving. Everyone I know spends time there nowadays. reply maxcoder4 15 hours agorootparentI think Reddit used to be a niche site for hackers [1], hobbyists and enthusiasts. Now it's a mainstream site where regular people browse news and memes. Maybe Reddit alienated it former audience, but it doesn't mean it's dying - it just changed. [1] as in \"hacker news\" reply idiotsecant 12 hours agorootparentThe internet used to be a place for hobbyists, hackers, and enthusiasts. That's not true anymore, at least not exclusively. But that supports your point - when the lake turns to saltwater you dont survive by being a freshwater fish. reply nullserver 14 hours agorootparentprevTwitter is doing fantastic. Just because people want to go back to more censorship doesn’t mean Twitter is dying. reply toofy 11 hours agorootparent> Twitter is doing fantastic. Reported yesterday [0] that it’s worth 73% less than when elon took over. You absolutely may have a different definition of fantastic but what was once one of the largest social sites losing 3/4 of its value in 15 months is significant. aside from actual financial analysts, my own personal anecdata has pretty much mirrored their analysis. i’ve gone back and gave it an honest try like 4 or 5 times and each time has been worse than the previous. it just reinforced why i (and more and more people) choose to spend our time in other places. [0] https://fortune.com/2024/03/30/fidelity-x-stake-73-decline-s... reply wolverine876 3 hours agoparentprevThe only thing stopping people is defeatism. People organized effective mass movements before the Internet - now it's infinitely easier, and so is the defeatist propaganda. The Reddit CEO has been successful - one absolutely necessary reason is a lack of defeatism. reply Intralexical 12 hours agoparentprevReddit reports something like 200 million active users. It could lose 99% of those, and it would still have more people than most cities. What, you want tumbleweeds to physically roll across your screen when you look up \"reddit.com\"? It's just a website. Use it, or don't. Save what you want, knowing it won't last. Sucks, but the Internet was meant for document delivery, not to become the one and only permanent home of certain social interactions. reply rchaud 10 hours agorootparent> Reddit reports something like 200 million active users. It could lose 99% of those, and it would still have more people than most cities Which would make it completely useless as an ad platform, which is the only way Reddit makes money. reply commandlinefan 10 hours agoparentprevWell, that’s just it - we can have a lawless wasteland, or we can have arbitrary and capricious censorship. There’s no in between. If prefer the lawless wasteland (I remember them, they were much better than what we have now). Unfortunately, I’m in the minority. reply Takennickname 16 hours agoparentprevReminds me when someone declared that phpBB won. reply itsoktocry 17 hours agoparentprevUh, reddit already won what exactly? How does going public have anything to do with success for a company like this? It's a way to raise capital and compensate insiders, nothing special Communities come and go. They are 95% reliant on the members, and some of their recent moves are adversarial. Everything there is becoming corporate, even the porn. reply asdff 16 hours agorootparentMore active users than ever before just in the last 2 quarters of 2023 after stagnating for years. Its almost like the API thing backfired and that bad press was in fact good press and got more people onto reddit. reply jazzyjackson 16 hours agorootparent> more people how are you so sure the extra traffic is from people? reply pmdr 16 hours agorootparentVPNs, datacenters and Tor are pretty much blocked and API costs money, plus everyone I know uses it nowadays. I think it's mostly legit traffic. reply sumedh 10 hours agorootparent> VPNs, datacenters and Tor are pretty much blocked and API costs money There are services which provide a service so that your bot can still scrape from sites like reddit which try to block bots. reply pmdr 16 hours agorootparentprevThe coup failed and whatever reddit's been doing is obviously good for business. I'd wager the majority of people boycotting reddit are still using it like nothing happened. reply pmdr 16 hours agorootparentprev> Uh, reddit already won what exactly? The right to do whatever they want and still keep the communities and content with pretty much no impact on their bottom line thereafter. > How does going public have anything to do with success for a company like this? It's a way to raise capital and compensate insiders, nothing special You raise capital if investors believe in the direction the company is heading. They're betting on reddit to make them money. > Everything there is becoming corporate Reddit didn't start out that way, at least it wasn't the impression that seasoned internet users got. It used to be developer-friendly and open, whether you were using Tor, a VPN or a datacenter's IP. It's, of course, their right to steer the company as they see fit and towards profitability. My only wish is that people providing valuable content and moderation work would start to see that the company's values have changed. reply theonemind 16 hours agorootparentFor today. Even Facebook is in decline. Online platforms don't last forever. reply kbenson 16 hours agorootparentUsually they do, either as some other company's acquisition or as a company that acquires others (like facebook/meta). Little actually is entirely gone, someone snatches it when the price gets low enough. Whether they grow themselves to the point they buy others or are bought is a distinction without a difference in this case. Their DNA exists in myriad ways, and even if they survived and weren't bought few few online companies are the same a decade later. reply pmdr 16 hours agorootparentprevI thought so myself until reading the latest earnings call. reply JeremyNT 13 hours agoparentprevIt's just so much better than anything else even despite it all. Which, honestly, is perplexing. It doesn't seem like it would take much capital to replicate Reddit pre-enshittification, but nobody seems to even be trying. I guess nobody can imagine making money off of a less shitty Reddit. Maybe the things about it that suck for users are the only things that make business people excited. reply al_borland 4 hours agorootparentBefore Reddit (and digg, and Facebook groups), forums were pretty popular. I was on many of them and they were simply run by passionate members of a community, not someone looking to scale to a multi-billion dollar company. It was just some guy willing to burn a little time and a little cash to talk to like-minded people about stuff they found interesting. It’s expensive when consolidated into one mega-site like Reddit, but very manageable for a smaller site. The people I know/knew running them were average people with average jobs. reply commandlinefan 10 hours agorootparentprevLots of people tried (voat comes to mind), but they ran into the same advertiser pressure monetization problem that pushed reddit to become the abomination that it is today. For whatever reason, hosting is expensive, and too few people are interested in making it truly accessible. reply add-sub-mul-div 16 hours agoparentprevThe vast majority of people turned out to be highly passive and docile in the face of extreme enshittification. Sad, but that's where we're at. Reddit only \"won\" the Eternal September types. The others have moved on. reply yieldcrv 16 hours agoparentprevPopulist protests usually fail, they only get their way when moneyed special interests are also lobbying for the same policy change out of coincidence, their interests usually pass regardless of public interest or not People waste their energy and call it “using their platform”, or just reminding people how irrelevant and inconsequential they are but bragging about it reply perihelions 19 hours agoprevSo, going forwards—where should someone who lives in a repressive country go, to read and interact with Westerners? Now, practically every social website (*except HN) blocks you if you use a VPN, and you go to prison if you don't. You're twice-walled in. reply maqdev 16 hours agoparentThey're not blocking VPN access. They're blocking scraping and avoiding their API limits, VPN is side-effect of that it seems. If you login, VPN works fine. reply newdee 3 hours agorootparentDo they let you get to the login page if you’re on a VPN? reply leshenka 2 hours agorootparentThe login page works, yes. reply Kuinox 18 hours agoparentprevThe fediverse, lemmy, mastodon. reply sentientslug 18 hours agorootparentI suppose that works if you want to interact with the three people on there. reply AnarchismIsCool 18 hours agorootparentIt's actually starting to get really good. It's like early reddit before eternal September reply throwanem 18 hours agorootparentOn the fediverse it currently is eternal September. reply AnarchismIsCool 15 hours agorootparentI was there before the reddit explosion when the majority of the posts on the fediverse were from one guy who just vomited cccp propaganda all day. I think it's in a much more interesting place right now. Reddit otoh started to become boomer-Facebook-meme-town some time around the onset of COVID and then hockey sticked last year. reply rakoo 16 hours agorootparentprevI suppose by three you mean 10 million reply Kuinox 16 hours agorootparentprevIt's already bigger than hn. reply mplewis 17 hours agorootparentprevOk, then you don’t have to go anywhere. Have fun. reply takeda 17 hours agorootparentprevI would also add https://tildes.net reply Takennickname 16 hours agorootparentThis is a website for people who think reddit is too conservative for them. reply takeda 8 hours agorootparentReally? My impression is that everyone is welcome. The only way I could imagine someone ending having a bad time there is to be hostile to other users. reply al_borland 3 hours agorootparentI’ve been a part of a lot of online communities going back 20 years. The fediverse had the most off-putting user base of any I’ve experienced so far. I liked the whole decentralized idea, but the users and culture drove me away. reply soderfoo 1 hour agorootparentWhat's the user culture like? I haven't had time to check it out myself. reply matteoraso 15 hours agoparentprevFacebook and Reddit both have an onion mirror. facebookwkhpilnemxj7asaniu7vnjjbiltxjqhye3mhbshg7kx5tfyd.onion reddittorjg6rue252oqsxryoxengawnmo46qy4kyii5wtqnwfj4ooad.onion reply sunshine_reggae 19 hours agoprevYes, same here. Let's just QUIT using Reddit. Enough is enough. If we tolerate corrupt behavior, we support corrupt behavior. And quite honestly, it's become such a pile of trash, anyway. Also, it's become so obvious that submissions and comments are being manipulated and pushed by tons of bots and industry interests, it's just a bad joke at this point. reply doodlebugging 18 hours agoparent>it's become such a pile of trash, anyway. Also, it's become so obvious that submissions and comments are being manipulated and pushed by tons of bots and industry interests, it's just a bad joke at this point. Very true. Over the last few months as more bots and reposters come online you have seen the subreddits bleed back into related subreddits. Most of the posts on /r/popular (landing point for navigating to old.reddit.com when not logged in) come from subs that never appeared before last summer's mod rebellion about 3rd party apps. Subs like /r/AITAH, /r/SipsTea, /r/TikTokCringe, and several others hit /r/popular regularly. /r/news will see stories that are functionally dead with no comment activity hang around for multiple days. The sad fact that the mods for the sub require you to have an email associated with your account keeps me from adding any content there since I don't give that out. A lot of content that rightfully belongs in one sub is cross-posted to related subs for additional karma. /r/pics is one targeted sub that catches a lot of content that normally one would only find on /r/oldschoolcool. It's almost like reddit is returning to the pre-subreddit days where there were no boundaries on the content that one would see on the main, and only, page as it updated. Memes, rick-rolls, news, questions, etc all just fell into line and as the site grew, faded quickly into the mist. With all this in mind I have been cutting way back on my engagement. I have walked back through my post history and edited a bunch of them, using Yossarian's censoring rules - death to all adverbs, nouns, verbs, adjectives - or just replacing the posts with meme text or song lyrics to stupid songs. If the post involved answering a question commonly encountered on the sub, one that would be easily discovered if reddit had a high-functioning search functionality, then for the most part, I leave it in place. Most subs I interact with are DIY type subs for automobiles, home projects, etc. and there are problems common to some models of car, truck, etc that people always ask about so removing that content seems wrong. It is sad to see a tool like reddit become such an enormous pile of suck but I think it was inevitable. reply Y_Y 17 hours agorootparent> Yossarian's censoring rules > Death to all modifiers, he declared one day, and out of every letter that passed through his hands went every adverb and every adjective. The next day he made war on articles. He reached a much higher plane of creativity the following day when he blacked out everything in the letters but a, an and the. (Heller, '61) reply doodlebugging 17 hours agorootparentYes. An excellent character in an excellent book. Thanks for this. reply asdff 16 hours agorootparentprevKind of a waste of your own effort going through your post history when everything is crawled already reply doodlebugging 16 hours agorootparentI totally agree. Seems dumb and probably is dumb. That's okay. Totally my choice. reply scarface_74 18 hours agorootparentprev> been cutting way back on my engagement. I have walked back through my post history and edited a bunch of them, using Yossarian's censoring rules - death to all adverbs, nouns, verbs, adjectives - or just replacing the posts with meme text or song lyrics to stupid songs. So you don’t see that you are being part of the problem? reply add-sub-mul-div 16 hours agorootparentThe site is systemically bad now. Shitting up or deleting the content is part of the solution, to accelerate the timeline of typical users finding it worthless so that it can die and give rise to something better. reply scarface_74 13 hours agorootparentSo if you don’t like a place in the real world, do you vandalize it or just not go there? reply doodlebugging 12 hours agorootparentAre you intentionally ignoring the fact that from day one a user had the right to edit by adding to or subtracting any part of their content or to delete the entire comment? How is this vandalization? This is user control, the same level of control granted on a site that was built for users to post on and discuss any subject - free speech was a keystone principle until some bad actors couldn't stop posting trash and ended up rightly banned. Millions, maybe billions of comments are digital dust at this point. Like thoughts lost just before they made it past the tip of your tongue. This is normal and expected behavior. Conversations get lost in the fog. Reddit isn't a Banksy. Today, it's more like the monkey area of the zoo in spots with each monkey's hand ready to sling the shit. When you consider the huge number of bots on the site it is likely worse than that. Originally /u/spez and /u/n0thing set a standard where a poster who dropped a reply containing information that didn't fit the established narrative and tried framing it as an accurate answer was asked to post the references that supported their conclusion. It was \"tits or get the fuck out\" days. That by itself promoted healthy discussions and worked to disseminate accurate information on important topics. It helped establish a cadre of users who are outstanding in their fields and who enjoy sharing their expertise with others. In time, especially after Digg imploded themselves, the expansion of subreddits made it untenable for proper moderation so a lot of boogers were allowed out of the nostrils and while the quality of discussions in general on the site was still good, many subreddits were established where people could say anything with no one calling them out. It sideloaded all the horseshit that used to hit the front (and only) page of reddit to the subs - stuff like goatse, nsfw content, etc. In the earliest incarnation reddit's front page was a minefield of stuff you really needed to avoid if you wanted to surf online and still keep your job. Early users learned fast to read the comments before opening the post. I'm having a hard time understanding how a feature - user control of content they post - is mislabeled in your reply as vandalism when the user elects to employ that feature in managing their content. reply scarface_74 8 hours agorootparentHe said he was modifying the content to add useless messages and song lyrics. How is that not trolling and just plain juvenile behavior? reply doodlebugging 7 hours agorootparent>He said... That was me. I know there are a lot of comments and replies to comments here so it's easy to lose track of who did what to who. I think if you read my replies in this thread you can begin to understand why it is not trolling or juvenile behavior, it's just me exercising control over the content that I posted, on my own time, following all the rules that reddit established when they launched. This is a feature. Like I mentioned, all the content that I have ever posted is likely to be indexed in someone's archive somewhere whether reddit controls that archive or not. There are too many players in that space and they have been active for years with publicly available tools to manage all the grunt work for anyone who wanted to download it all and slog through it looking for gems. If they find one of my edited posts that has a suspicious amount of worthless updoot karma and that post is totally out of context then I am sure they are bright enough to figure out that they will need to get that content somewhere else. I left untouched all of my old \"Best of Reddit\" and the gilded posts since those had above average value to the readers who chose to engage and updoot. At the end of the day, the content has always been mine to edit or delete. That is the way that the site was designed to function so labeling anything I do as vandalism, trolling, or juvenile behavior only works to make you look like someone trying hard to understand how to value your investment in reddit in the event that others choose to act similarly. As I mentioned somewhere else, reddit could change all of this if they instituted controls that would allow live users to instantly recognize bots and adverts so that users can instantly choose to read a post or comment or to ignore it. The fact that they embraced and actively ignore the bot armies, the shills for various products or philosophies, the blatant adverts, etc tells me and others that they have lost touch with their users. As it is, forcing me to check karma levels and username age to hope to recognize reposters and bots is just wrong. Clear and ban the bots, ID the adverts, add a shill warning to those who evangelize or push agendas or misinformation, and the traffic finds a floor. On that floor reddit will find those loyal, long-term users like myself a lot more likely to engage. Clean the house and make it more livable and others will move in to see what's up. Sunshine is the best disinfectant here. Otherwise people like myself will choose whether to continue to engage with reddit and set their own terms of engagement. reply add-sub-mul-div 12 hours agorootparentprevIt's silly to act like every online context has a real world analogue. But no, I don't vandalize it. Whatever you consider the real world equivalent of deleting my comments after the place has enshittified itself, that's what I do. reply scarface_74 11 hours agorootparentThere is a difference between deleting a comment and purposefully posting meaningless junk. reply doodlebugging 5 hours agorootparentAt least I didn't have the power of the spez and use it to edit someone else's comments. reply add-sub-mul-div 10 hours agorootparentprevYou're right, the latter is funnier. Like I said, the more it's weakened the faster people will move on from it and let it fully die. For now it's only spiritually dead. reply scarface_74 8 hours agorootparentYes if you are 12 and posting to 4Chan it might be funny reply doodlebugging 7 hours agorootparentSince many of your replies are referencing my post I would just like to add that I am not 12. I'm closer to 9 (in dog years). I grew up a long time ago, maybe before you were even a sparkle in your parent's eyes and a new name on the family Christmas card. reply water-data-dude 16 hours agorootparentprevThe reason they commented originally was because they wanted to provide value to a community that had provided value to them. Reddit was simply the platform that hosted that community. Their loyalty was never to the platform, it was to the community. When Reddit decided to screw over that community so the execs could get a big payday from the IPO, it sort of broke a social contract. Letting them keep extracting value out of you is kind of like if you worked at a grocery store, and then they unjustly fired you or a couple of your friends. Now they’re not doing so well, but are you “part of the problem” if you don’t keep shopping there? No. Screw them. reply doodlebugging 16 hours agorootparentThis is a great way to explain it. Loyalty to the community. You spend enough time on a sub and you begin to recognize a lot of posters even though you will likely never meet them. reply doodlebugging 17 hours agorootparentprevNot at all. I have contributed a lot of content since 2005 under several different user names. A lot of that content required significant time for me to locate links, photos, etc to be able to provide accurate answers to a question or, as many of my posts attempt, to correct an inaccurate answer that seems to be getting a lot of traction in the post. The content that I post is mine, the aggregation platform is theirs. I post with the understanding that it becomes as public as the sub allows and that I have the ability to edit as I feel appropriate. Reddit is a platform for aggregating news and information on a huge variety of subjects but the content belongs to the users. Reddit felt like they needed to monetize things and the only thing they have that has any value is the user content and user ID info. I have never provided any ID info so they are stuck with an IP address for me. I'm okay with that. They have had ample opportunity to crawl all of my posts over the years, archive them as they see fit, and repost them later as long as I get acknowledged as the OP. They have backups. Many others have used publicly available tools to crawl the site and index user content. It is safe to say that nothing that I have ever posted has disappeared. It is available in someone's archive somewhere. I'm okay with that because I can't change that now. When I post today, I monitor the thread activity and when it dies I edit my post unless it fits the criteria that I honor - the post references a situation where someone needed information about a common issue with a vehicle or other product that I have a lot of experience with. For example if someone just bought a 15 year old vehicle and suddenly they encounter an issue that is well-known to everyone else who has ever owned that model then I post a clear answer with photos if needed to help them solve the problem. People like this tend to be unable to afford newer vehicles and problems like this can be expensive to fix if they go to a shop when they are really simple to fix with ordinary tools. I give a description of the process to repair it so they can get on their way and not have their \"new-to-me\" car break down and cost them a job. Most of my posts are reposts of content posted several times over the years such that I just grab the text from my local folder and the associated photos and let if fly again. This is necessary on reddit because their search function is intentionally broken so that old useful posts can be hard to find. This encourages people to make new posts solving old problems and drives traffic to reddit. It's a dick move on reddit's part but I accepted that years ago. The absence of permalinks on many subs also fuels this repost bullshit. Few subs have permalinks to popular questions or FAQs on the sub, therefore you see a lot of repetition in subject matter of the posts. Reddit subs are less of an information site than they are an activity site. Reddit needs a certain amount of traffic on a sub for it to be a useful place to post so search is crippled, permalinks are largely absent, etc. The content I post on reddit is mine. I will do with it as I please. If you wanna see everything I have ever posted in its original form you need to use one of those crawler tools before I edit. reply scarface_74 17 hours agorootparentNo matter what excuse you make, you’re still being part of the problem. If you could do the same on HN would you? How is your action not petty and trolling? And exactly how was Reddit suppose to be an ongoing concern without monetization? Were you going to donate to them? Provide free labor? reply doodlebugging 17 hours agorootparentI have been part of their free labor since late 2005. Now they want to farm out my content with no compensation to me. They would have nothing of value without the content that users like myself freely provided. I donated my content without which they would never have gained any traction online. >If you could do the same on HN would you? How is your action not petty and trolling? I also edit comments on HN if the edit is appropriate and I remove comments if the thread is dead when I comment so that the comment adds nothing to the discussion. >No matter what excuse you make, you’re still being part of the problem. These words that I spent my time to type should never be construed as an excuse. They are an explanation. There's a difference. As to whether I am part of the problem or part of the solution or just a small part of something else, that is always open to an individual's interpretation based on and biased by their own personal experiences. >And exactly how was Reddit suppose to be an ongoing concern without monetization? This has never been my problem. It is a problem that they should have had the foresight to solve before they launched. reply scarface_74 17 hours agorootparentSo exactly what do you propose? Should they pay everyone who makes a post or comment on Reddit? Do you feel the same way about HN? You are posting here for free and providing value. > This has never been my problem. It is a problem that they should have had the foresight to solve before they launched. Do you feel the same way about all of the companies that YC funds? If so, why are you commenting here “adding value”? reply doodlebugging 16 hours agorootparentIt is not my part of their business to propose solutions to their self-inflicted problems of monetization. I don't need to be paid for my content. I post on HN and reddit and several other forums as a way to help others solve problems when they own or use something that I am familiar with. Like I said above, I understand that some of my content provides value when I post. I also understand that joining and posting to forums and sites is a personal choice that is totally optional. No one asked me to post. No one forced me to post. It is always my decision. That is how it should be. I also understand that the part of my content that has value is always content that I could choose to post to a personal blog or other personal website with all the tools to help me monetize traffic. I have made the choice in my own life to avoid that path since the chances of it gaining enough traction to be useful to others is less with a blog or youtube channel and I don't need that level of friction in my life. EDIT: As for the part about why I comment and potentially \"add value\" - sites like HN and reddit and the other topic-specific forums that I post on add value to my life by providing information that I need so I post comments in an attempt to add some value on their end when I see the need. Value to you or any other user is always subjective. reply avtar 16 hours agorootparentprevAFAIK HN doesn't have similar VPN access limits in place. Nor is HN restricting third party API access [1] or selling user content [2] for model training purposes. [1] https://en.wikipedia.org/wiki/2023_Reddit_API_controversy [2] https://www.reuters.com/technology/reddit-ai-content-licensi... reply scarface_74 16 hours agorootparentSo do you propose that Reddit fund startups every year and use Reddit to promote the startups it funds? How do you propose Reddit make money? reply avtar 16 hours agorootparentI was addressing your HN comparison. If HN starts adopting similar practices then it'll be fair for the person you were replying to also take a similar stance. Have a \"free\" public forum where users contribute content was the model Reddit had been using for years. Once they started changing the model, why shouldn't users start questioning their own role in the now outdated arrangement? reply tomrod 16 hours agorootparentprevSounds like they are proposing reddit shouldn't make money through the content, and die if that is the only path. This is reasonable. reply doodlebugging 16 hours agorootparent>Sounds like they are proposing reddit shouldn't make money through the content, and die if that is the only path. I think if you did a deep dive on reddit that covered everything you would see that they have struggled with this issue since they launched. It's really not my problem to solve. I add content. Some of that content has value. They have lots of gifted supporters in the Y-Combinator family who could think of lots of ways to monetize things and I don't worry about how they end up doing it until it reaches the point where it is not possible to trust that you are interacting with a thread posted by a live person or one that is a repost from a bot farm. Reddit is over-run with bots now and the average user has no way to know which users are live humans versus bots used to drive traffic. I've always been a \"don't piss down my neck and tell me it's raining\" type of person. Credibility is key. Reddit needs a way for ordinary users who would like to contribute content - free or paid, I don't care - to recognize bots and other artificial traffic like paid adverts or sponsored posts. Camouflaging users wastes my time when I need to look at post history to decide whether they are real or just karma farming. In the end I agree with the part about dying if this is the only path for reddit. All good things come to an end. That's why corporations can live forever. reply mplewis 17 hours agorootparentprevEveryone who used the site and moderated it was providing free labor. Do you not remember the moderation protests against the administration of the site? Now that those mods have left, the value of Reddit has too. reply scarface_74 17 hours agorootparentIf you didn’t get any value from Reddit why did you comment and provide content? Why are you commenting here? reply goplayoutside 18 hours agoparentprevIf Lemmy had an automod I'd happily try to move the niche communities I help mod over, but without even the basics (regex rules + mod queue) attempting to mod any sizeable community is ime an exercise in frustration, making Lemmy essentially useless for us. That's a real shame, because otherwise it looks like great software. I've got an offline instance sitting on AWS since last summer just waiting for any bit of progress, but gave up hope some time ago. reply culopatin 18 hours agorootparentMaybe if I was a teenager again I would spend the time to figure out how to use Lemmy, but at this age: I go in, curiosity takes me to “instances”, I see a million. Click a few, many are dead and I start wondering “wait, I have to navigate through this sea of non descriptive URLs that tell me nothing about the instance to get to the data?” And even if that’s not true, that’s about where I drop it, because it seems like a big climb for something I already know won’t work as a Reddit replacement because there is no way my friends will be convinced to put this kind of effort, and because it’s hard to discover. reply dhalucario 16 hours agorootparentprevhttps://github.com/hjalp/automod It seems like someone has made an autoposter. Maybe someone could have a look to extend it. reply rglullis 18 hours agorootparentprevPlease tell me which communities you mod, and if it's not anything 18+ I can help you with moderation and use it to guide the development of the moderation dashboard I started working on. reply danslaboudoir 18 hours agoparentprevThe day Apollo ceased connecting is the day I deleted my account. I wish Steve and the crew my very worst. reply marcrosoft 18 hours agoparentprevI thought we all quit when they disabled API access and ruined Apollo. People are still using Reddit? reply EasyMark 16 hours agorootparenttens of millions a day, I sometimes wonder why people take on these false \"people are still using __________?\" when they know that people are still using twitter and reddit and facebook. Do you have any explanation as to why one would act pseudo-shocked? Is there a point? reply rpgwaiter 13 hours agorootparent(Not OP) I legitimately thought reddit the site had died and gone full ghost town. My use of reddit has almost always been as a knowledge resource, and that had been totally destroyed with these API changes. Like, very often I have a technical question, search it+reddit and find a seemingly helpful thread with most of the comments deleted. It's logical to assume that most people who also engaged with reddit like this have slowed their use considerably. Hell, lately I'm more likely to slap \"hacker news\" onto my search query reply bluish29 17 hours agorootparentprevApollo can still be used with sideloading a patch that allows you to enter new API key. As a single user it is hard to get to the limit. reply rrr_oh_man 19 hours agoparentprevI love /r/askhistorians, though. Excellent posts. reply blakblakarak 16 hours agorootparentThis (and r/stopdrinking and r/peloton for race feeds) are my only reasons to visit nowadays. reply bwanab 17 hours agoparentprevAll true, but I haven't found any forum that works for very localized (geographically or otherwise) interests than reddit. I've long ago stopped looking at or participating in any non-localized subreddit. reply ktosobcy 19 hours agoparentprevUnfortunatelly it won't work/happen... even after huge protest and backlash reddit is still strong and go-to-place :-( reply gamepsys 17 hours agorootparentI hear a lot of people talk like this, and I understand that reddit has huge traffic numbers. However, for me Reddit peaked in interesting content around 2013, and I stopped daily browsing in 2015. Sometimes some subreddits have interesting posts, but interesting content is far from the norm. Now each subreddit feels like it is having the same discussion again and again, and the median post is a meme or some chat-gpt generated text dump. As Reddit became more and more popular the most upvoted posts became more and more mass-appeal and easily digested content. Other platforms have taken it's place as where the truly interesting discussions are happening. Twitter and Discord being the biggest two. reply asdff 16 hours agorootparentBut thats not what the reddit owners care about. They are happy people like you leave, you are hard to monetize and block ads. The people who look at the front page today and say this is great? They don't block ads. They don't use old reddit. They don't realize when they are engaging with a shill post. Reddit wants proportionally more of them and less of you, and they are winning. reply kbenson 16 hours agorootparentThat only works to a point I think. If your forum devolves to only having the easily duped people, it becomes a lot less interesting overall (since the content is from the people) and even those people leave. reply omoikane 16 hours agorootparentprevI have observed a recurring pattern of \"this popular place is doing something I don't like! Let's all move to a less popular place, that will show them!\" Time and time again, the people who made the move learned that they were the minority in what they didn't like. Also, while the old place might have a greater variety of people (good and bad), the new place is often filled with many people who were angry about the old place (maybe mostly \"good\", but makes a lot of angry posts). So now, people who are at this newer, possibly better, but quieter and angrier place, they have to wonder if they made a good move. Sometimes, with enough patience, the new place do eventually turned out to be just as great. But usually my observation is that people just give up and leave, possibly returning to the old (still popular) place. reply dotnet00 15 hours agorootparentYep, I've seen the same phenomenon. Another example being of X to the fediverse. So many people switched over, made not being on X their entire identity, freaked out when the place wasn't as sanitized as X and eventually just went back. reply Tijdreiziger 18 hours agorootparentprevAin’t that the truth. I tried moving to Lemmy, but turns out that the only people there are other techies who are mostly interested in techie topics (and I already have HN for that). If you want a broader perspective or you want to discuss non-techie topics, Reddit is still the place to be, for better or worse. reply Dwedit 11 hours agorootparentprevThe protest was done in the worst possible way. You don't PRIVATE the subreddits. You keep them visible, lock out new posts, and direct visitors to a successor website to continue the discussion. reply someonehere 16 hours agoparentprevI gave up over a year ago. Best decision ever. It’s just garbage now that gets posted. reply kilroy123 17 hours agoparentprevI did several years ago and it's been great. I'm still a bit hooked on HN. But not nearly as bad. ;) My strategy was to slowly unsub from everything until it got so boring I just stopped visiting all together. reply itsoktocry 17 hours agorootparentThat's why reddit, as an investment, is so strange. Running forums as a business is antithetical to the needs of the community. Fortunately, there's always \"another forum\". Quitting is really easy. I know because I've been doing this for almost 30 years. reply smokel 17 hours agoparentprev> Let's just QUIT using Reddit. This suggests that you need others to join you in quitting. Why not simply quit yourself? reply mitthrowaway2 16 hours agorootparentPeople want to talk in the place where other people will listen. People want to listen in the place where other people are talking. If you walk out by yourself and end up standing in an empty room, you'll just end up turning back. reply smolder 16 hours agorootparentprevMore people leaving means less FOMO. Or perhaps they want to harm reddit in some karmic justice way. reply butz 19 hours agoparentprevThere still is a lot of useful information posted on Reddit. Was it scraped somewhere for preservation purposes? reply rglullis 18 hours agorootparenthttps://lemmit.online and https://zerobytes.monster mirror the posts into their own instances https://alien.top mirrors the posts and comments from some communities into a set of topic-specific instances and is my project to help people migrate from Reddit to Lemmy. Visit https://portal.alien.top to get started. reply no_time 17 hours agorootparentprevActually yes, the entire thing: https://academictorrents.com/details/9c263fc85366c1ef8f5bb9d... Nobody is actively hosting it with a frontend though. reply redox99 18 hours agoparentprevAround 2 years ago I got incorrectly banned site wide for 3 days for \"ban evasion\". Haven't used it ever since (except through google searches). There's no downside to leaving reddit really. reply EasyMark 16 hours agorootparentprobably the cabal of mods that run the more popular subreddits. I deleted an account because of that; I had posted in one of the antivax subs about how stupid people in there were being and the popular reddit mod cabal sentenced anyone who posted ever in a slew of those antivax subs to \"bans\" and if you used a different account you were \"ban evading\". It's a petty power play by very small minded people to hurt those even though reddit possesses no mechanism to know you are \"banned\" in a sub other than a one time message. It just shows how pathetic and small their lives must be to try and reach out and cause other folks trouble reply silisili 14 hours agorootparentI'm not sure that one who purposely goes into subreddits of topics one disagrees with to throw insults and start arguments is exactly a model user, in fairness. reply tootie 19 hours agoparentprevLosing the VPN crowd will cost them next to nothing. reply hipadev23 19 hours agorootparentA vast amount of reddit “users” are persona bots behind proxy and vpn networks. reply lxgr 19 hours agorootparentprevOnly because they've already lost most of what is actually valuable (to most early users; maybe not to advertisers). reply andsoitis 19 hours agorootparent> Only because they've already lost most of what is actually valuable (to most early users; maybe not to advertisers). There are many many users who were not early users who find Reddit very useful and a positive in their daily lives. reply lxgr 16 hours agorootparentIs that despite or because of the changes they've made in the past years? reply badrabbit 16 hours agoparentprevWhat is the alternative? lemmy and mastodon are protocols, what specific site with high engagement and good (not over zealous) moderation is worth checking out? HN is nice to lurk but you really gotta be a conformist. It all just feels like tribal bubbles. It really is making me appreciate free speech, I prefer the days when horrible people were allowed platform/reach. The state of society right now (not just on the internet) is not compatible with liberty and free thinker idealisms. reply al_borland 3 hours agorootparentX/Twitter is supposed to be the free speech platform. A course I’m taking told students to post daily updates there for some accountability and community. I signed up to do that. I followed some normal stuff as part of the onboarding (some tech people, some local news, a couple podcasters… only 17 people) and the “for you” feed it gave me is nightmare fuel. To be fair, I turned off the content filters, as I do on every site, but it’s usually not that bad. I’m thinking of turning the filters back on to see what that looks like. So far it hasn’t really been a community I want to get invested in. Not to mention the comments on posts are littered with completely unrelated posts. A 3rd party app would go a long way, but like Reddit, Twitter killed that off. reply smokel 15 hours agorootparentprevThe alternative is to simply not use it. Read books, hang out with friends, do some tinkering. Free thinking is perfectly well possible without Reddit, in our very society. Unless you have some really weird thoughts? Rubbing these thoughts in other peoples faces may not be viable, but it has never been, as far as I recall. And I'm glad for it. reply badrabbit 12 hours agorootparentMost communication happens on the internet these days. But also, my whole point is the \"friends\" bubble is increasingly getting smaller and tribal. We don't interact with strangers because our communication platforms optimize and moderate them away. How can democracy work if you can't talk to and convince fellow citizens on various controversial topics? Even on HN, I was doing fine until I started commenting on political posts. My account started getting restricted when I stated the idea that Tor and similar software being used to undermine foreign governments and the works of many tech companies in this area is wrong, that democracy works for us but the \"manifest destiny\" approach of forcing democracy on others amounted to neo-colonialism. If my views are wrong and unpopular, the downvote/karma system takes care of that. What's wrong today is, moderators and algorithms are herding us into tribal camps. People have forgotten how to passionately disagree with others without hating them or excommunicating them. We are being excommunicated from each other for the crime of thinking for ourselves. reply toofy 5 hours agorootparenti’m not trying to be dismissive of your concerns, so if this post comes across that way, i apologize in advance. im aware this will be wordy, i can't really sum this up in a more succinct way--im with the family doing easter dinner so i dont have the mental bandwidth to eloquently sum this up. i think the subjects surrounding this are super important for the future of the internet so its impossible to put the nuance it deserves into a tweet size paragraph. also, in advance, while im gonna disagree with some of your points, i strongly agree with you regarding algorithms. im confident in saying they have played one the largest parts in the downward trend of online social. i’ll expand on it a little more below. anyway. the reality is, the internet has pretty much always had moderation. if anyone attempts to convince you otherwise, they're wrong. irc ops banned indiscriminately, they did this so often it would make modern day internet hogs cry. bulletin boards always had various types of rules and they too would absolutely regularly ban people. forums have always banned people. etc... banning is definitely not some new internet thing. there were/are of course places that have no moderation but they’re ghost towns entirely overrun with spam, or worse. they're absolutely not places that will ever attract a significant number of users. this idea that we can build a large online community with no moderation, or somehow only moderate illegal content is just not based in anything resembling coherent reality. we have proven this time and time and time again. there are thousands, maybe millions of sites with little moderation that are empty other than loads of spam, nazi shit, child porn, noise spewing bots, dead internetting back and forth... i bartended through college. a pretty good real world comparison to mods would be a bartender. for a huge number of bartenders, a much more important skill than even making drinks is to know when to remove a person from the bar. this skill is absolutely more important than mixing up a cocktail. knowing when to save someone from an uncomfortable interaction is paramount. recognizing early when a person or group is making the experience miserable for the rest of the people is so important. we don’t expect a bar or restaurant to have a written list of Every Single thing that could get someone bounced out because 1) this list would be impossible to make, and 2) people generally know how to behave socially in the real world. yet somehow we've been convinced the same etiquette doesn’t apply online. which, to be a bit terse, is kinda stupid. as the internet ages, ultimately we’ll see online users are people. people still find the same behaviors creepy. the same things weird. even tho it’s online we still think it’s weird if a random person interrupts our conversation and starts screaming weird stuff at us. while bartending, i had to remove people from the bar regularly for all kinds of weird shit, not once did i ever hear someone cry “but i have free speech” while being dragged out of the bar. not once. if someone had yelled this, the crowds would laugh at them. common sense and etiquette rules over free speech in social atmospheres. and until we realize this extremely basic thing (so basic that toddlers understand it) we’re still going to see people being metaphorically dragged out of forums, even while they're screaming “i have the free speech to unprovoked call that random stranger a fat pig! free speech!” its hilarious that people don't understand this extremely basic human issue. yes, they have the free speech to be a dickbag but somehow forget the bartender has the freedom to snap their fingers, point at the person and have the bouncers drag them out? common sense and etiquette rules in social atmospheres. you mention “how can we talk to and convince people on controversial topics?” (again, i want to stress that i hope i don't come across as dismissive of your concerns, but…) you don’t need to convince random strangers to believe controversial topics. you don’t need to. and we need to understand, when people do this, they come across as zealots. take the real world again as an example: if random strangers approach you and start chirping controversial x, y, or z subjects, thats weird... people will treat that as weird. they will cross the street to get away from zealots who do this. if you’re on a site and it seems like people are metaphorically crossing the street to get away from you, ask yourself how you might appear if you approach a random stranger in the real world and strongly start talking about weird controversial shit? would they look at you sideways and slowly back away? would they maybe come up with excuses to leave the conversation? common sense and etiquette rules. be normal. you don’t need to “convince” random strangers to save democracy. most people find randoms bringing up controversial topics to be creepy and weird. we’re people, we’re not random usernames you need to “save” or “convince” of anything. especially controversial shit. just like in the real world, remember, sometimes people are open to a conversation--i want to stress a conversation, not a debate--sometimes they're open to one, but more often people just aren't interested. there are so many very good reasons people may not want to have conversations at any given time. and beyond that, it’s even more rare for people to want random strangers who are trying to save or \"convince\" them through some weird internetDebate. that’s not normal. rarely does anyone want to be preached to (and yes, im fully aware that i am saying this as i’m preaching to you lol, sorry) if someone isn’t specifically looking for a debate, it’s absolutely offputting and creepy when people do this. sure, there are tiny little niche communities where people enjoy what these communities call \"debates\", but those are a tiny fraction of people compared to the massive world of normal people who find that weird. im sure we’ve all had the “have ya heard the good news about jesus” people knock on your door? a lot of times that’s pretty annoying. imagine how much more obnoxious it is if a random were to approach you and your friends on the street: “did you know jews and gays are ruining the world?” when this happens in the real world we’re like, 'get away from me dude, wtf…' the creepy weirdo factor doesn't change just because it’s online. and it doesn't have to be that controversial... its just odd when people try to jam controversial shit onto random strangers. its just creepy. as for algorithms, i totally agree with you, they’ve done far more to destroy social than anything. in the big picture i dont think we need to be stressed about it though. im confident we’re gonna ultimately see a rejection of algorithmic curation. i mean, algorithms are just so bad. everything from shopping algorithms to music recommendations to the conversations the algorithms choose to show us. its just bad. a fail all around. i suspect online will go back to something closer to offline reality. in reality we are never all jammed into the same room with an algorithm strongly implying \"now scream at him! now tell her shes fat! now tell him why his doctor is wrong! now yell at that group!\" in the real world we have never in the past (and still don’t) need or even want to talk to random strangers in the real world. we don't scream our ideas at each other. we don't think we need to interject our personal ideas into other peoples conversations. frankly it’s bizarre that the same group of investors keep trying to convince us to do this. trying to convince all of us to simultaneously go to the same place and listen to their curated algorithms. its super odd. the same group of investors are repeatedly trying, over and over again they've been doing this on multiple different failed social sites. \"everyone listen to our algorithm! it failed on that other site we own. but you *all* need to listen to this one! no, dont go elsewhere, listen to ours, all of you! you'll hate this one idea from him, yell at him!\" its super creepy. also, this thing where a few people keep loudly trying to jam and force everyone around them into a “debate” is another piece of this thing thats got to go. more of us need to read the room. sometimes people will be interested in an actual conversation but they're rarely interested in what these guys laughable call a “debate”. we need to be normal. if a person indicates they want to have a conversation with a random strange, sure, jump on in, have that conversation. however, if they’re already in a conversation with their friends, are they open to a random chirping in? if they’re not interested, yet someone forces their way in, its not at all surprising when that group of friends metaphorically crosses the street. anyway, go easier on yourself. you don’t need to save democracy. don’t imagine you're “making democracy work” by forcing controversial opinions on normal everyday people. especially those who aren’t interested in your controversial opinions. we’re just randoms on the internet. none of us are saving anything with our internet posts on random social sites. there are much bigger things pulling those levers. pursue interests. have fun. be normal. reply swed420 19 hours agoparentprevAgreed. The manipulation became obvious with the capital fueled shareblue / \"correct the record\" campaign from Dems years ago and has only gotten worse since. The stated purpose of CTR was to defend against Trump, but it was of course also abused to help sabotage Sanders. A tool like this will always be abused to perpetually elevate capital interests over human interests. Whatever solution everybody jumps to, it needs to somehow prevent this behavior because it's not going to stop on its own. reply tomrod 16 hours agorootparentIs this \"CTR\" from the world of alternative facts, or a real thing? reply sircastor 13 hours agorootparentI can’t speak to what the GP is talking about here, but Correct The Record was a SuperPAC whose purpose was to “…fight online harassment aimed at Clinton and her supporters by staying positive.” [1] https://www.theatlantic.com/politics/archive/2016/05/correct... reply tomrod 13 hours agorootparentAppreciate it. So it's a \"nothingburger\" to borrow parlance! reply swed420 8 hours agorootparentLooks like you got to the bottom of it. Good luck with your \"democracy.\" reply racked 15 hours agoparentprevIf not site:reddit.com, what am I going to type into Google to get any kind of reasonable search result? reply weregiraffe 18 hours agoparentprev>Let's just QUIT using Reddit. Enough is enough. If we tolerate corrupt behavior, we support corrupt behavior. Might as well quit using Internet. reply mfiro 20 hours agoprevIt's been a while that VPNs were blocked on the main website. But I have noticed this since a week that old.reddit.com subdomain has been included. The next step for Reddit would be a complete Login wall like Instagram and co. I would be happy if they do, because it will save my time more (like twitter). reply Cthulhu_ 19 hours agoparent> I would be happy if they do, because it will save my time more (like twitter). Why is that? If you don't have enough self-control to not use Reddit, a login wall won't stop you. reply BadHumans 19 hours agorootparentThis is very wrong. One of the most studied ways of breaking bad habits is by introducing friction. Nothing stops me from getting in my car and going to McDonalds but that is a lot of friction. It's easier to just stay home and cook something I have. reply Spooky23 19 hours agorootparentprevIt would kill Reddit as a Google result that people seek. Honestly, they jumped the shark years ago. It’s Digg 4.0 at this point. reply eviks 19 hours agorootparentprevOr it will since self control is not a binary, so more barriers can reduce use reply cocacola1 18 hours agorootparentprevUntrue in my case. Needing to create a Twitter account stopped me going to the site at all. reply malwarebytess 17 hours agorootparentprevTotally disagree. Killing my third party app reduced my reddit usage by like 90%. Now I really only use it on my desktop PC. On mobile, when I tap reddit search results I am never logged in which means I have to go to old.reddit.com to see the full thread and comments. If they added a log in wall I'd just stop tapping reddit results on mobile. reply zettabomb 19 hours agorootparentprevSpeak for yourself. It's enough for plenty of people, myself included. reply QuesnayJr 16 hours agorootparentprevI used to check Twitter 50 times a day, but the login wall combined with killing Nitter completely broke the habit. reply EasyMark 15 hours agoparentprevI suspect they will soon remove old.reddit.com. I think that will be my exodus call. I've been doing more on mastodon, lemmy, discord, etc and hope those continue to grow reply enronmusk 19 hours agoprevFWIW I too am unable to access reddit using my home (residential) IP. I've never had an account there, so I have no idea why they would block my IP. I also haven't scraped anything, ever. Previously I could access old.reddit.com, but now that's blocked too. I also can't create an account -- I get \"403 forbidden\", even if I specify an email address and clear my cookies. I even created a support ticket about this a few weeks ago, which went unanswered (apart from an automated message which wasn't helpful or applicable at all). I suspect it might be because I often use RedReader on my Android phone, which is still working somehow regardless of the IP ban. Funnily enough I can access Reddit through a VPN. reply Tijdreiziger 18 hours agoparentCheck if your IP is accidentally marked as a VPN IP at the big IP location providers (e.g. https://ipinfo.io/). reply enronmusk 15 hours agorootparentThanks, but I did check that earlier as well. My private, static address is correctly labeled: privacy: vpn: false proxy: false tor: false relay: false hosting: false service: \"\" reply graemep 19 hours agoparentprev> FWIW I too am unable to access reddit using my home (residential) IP. That is an interesting development. I wonder how much overblocking there is? Every time Cloudflare says they have block a huge number of attacks/bots/whatever, I wonder how many are false positives? reply FredPret 17 hours agoprevReddit is such a toxic hole anyway. It's all posturing, stupid cutesy language, and victim mindset. I'm so glad the site got awful as the content - we're all better off without. reply lazylion2 3 hours agoparentLike any social website, it depends on how you filter it and which communities you join. reply Ylpertnodi 17 hours agoparentprevGood Ukrainian War info, though reply GaryNumanVevo 15 hours agorootparentTelegram has much better info, Reddit's is usually a few hours or days old reply euazOn 11 hours agorootparentI dont always necessarily need or want the latest info. I find Reddit to be a good aggregator/filter that also acts as one of the walls against disinfo, i.e. some of the content never gets posted, or gets immediately debunked. /r/credibledefense is my go-to reply joshxyz 11 hours agorootparentprevthis. reply EasyMark 15 hours agorootparentprevimagine downvoting you for expressing an opinion. HackerNews should be better than this. reply 0cVlTeIATBs 20 hours agoprevThat's the final blow for me. The last use case was when esoteric discussion there would be one of the few search engine results. I've already long ago edited out all my old posts and deleted the accounts. So sad to see sites grow to become gated advertiser-friendly communities. reply zeta0134 19 hours agoparentAs an aside, I get irritated as all heck by the old, previously useful reddit threads that now show up in search results but with the actually useful information deleted by someone in a fit of rage. This is frustrating, since the information is not necessarily easy to find somewhere else. Or at all. If you find that you must do this, could you at least re-host your answers elsewhere and link to them? reply lxgr 19 hours agorootparentYes, this is incredibly frustrating. I wonder if we've surpassed \"peak publicly searchable discussion\". It definitely seems harder to find quick answers to obscure topics than it used to be 2-3 years ago. LLMs will gladly hallucinate something, but given that this stuff is literally the training data that could help ground them in truth, I wonder where we're going to go next. reply giobox 18 hours agorootparentOf course we have passed it. The moment LLM training happened was when everyone started locking down access to their data or increasing costs of developer API access - twitter/x have done similar things, and quora etc. Now the corpus of user questions/answers, posts and so on has real value as machine learning training data it’s hardly surprising this is happening - no one wants to “give away the farm” to a rival LLM product bootstrapped on data that was too easy to scrape. For older readers who remember the buzz about web2.0 in early 2000s and everything would be a public api or feed - the recent history of the web now has almost been the opposite. Examples of this are everywhere - RSS is essentially dead, news readers died, people are trying to put podcasts behind proprietary systems (Spotify) etc etc, more and more data is hidden behind account walls, app binaries on mobile often only arrive from a mandatory store… reply namlem 18 hours agorootparentprevYeah all that discussion is now on non-searchable, ephemeral private discord servers reply pixl97 19 hours agorootparentprevI mean yes, tons of discussion has moved to places like Discord to disappear forever. But unlike the previous poster, who blames the information creator for revoking what they published, why are we not blaming the actual abusers? Every site that is build on growth, Facebook, Google, Reddit, et al eventually turns into an authoritarian capitalist nightmare dystopia. Gobble up, lock down, and extract wealth. reply lxgr 18 hours agorootparent> tons of discussion has moved to places like Discord to disappear forever. This is actually even worse than the new Reddit. Every open source or other project that links to their Discord as a main place of providing support immediately loses a lot of respect from me: Chat is a horrible way of creating a searchable knowledge base. Besides being opaque to search engines, it effectively signs up their users and contributors for either having to maintain parallel long-term-visible and searchable FAQs and other docs, or answering the same new user questions over and over again. Having to publicly join a Discord (unlike Reddit there seems to be no way at all to browse anonymously) just to be able to see if anybody else has had my compile or setup error is completely unacceptable as well. reply koiueo 14 hours agorootparentprev> deleted by someone in a fit of rage It sounds like you are blaming the authors of the actually useful information here. There used to be an ecosystem: real people share their knowledge and experience, driving users to Reddit, and in exchange, Reddit provides free storage and a convenient collaboration environment. I admit, it's not easy to monetize real people's contributions. But, regardless, the fact is, that Reddit destroyed this ecosystem. I can no longer use Reddit conveniently. And as a mildly active OP on Reddit I don't see, why Reddit should keep benefiting from my contributions while I can no longer benefit from Reddit. I think it's fair. reply betterbase 16 hours agorootparentprevYou could try the PullPush archive, it may have the original comments prior to to deletion. Main site: https://pullpush.io Frontends: https://undelete.pullpush.io https://search.pullpush.io https://ihsoyct.github.io reply kps 18 hours agorootparentprevThat's the reason I haven't erased my Reddit content, so far. (If they go behind a login wall, that reason goes away.) reply asdff 16 hours agorootparentprevchances are the thread was crawled on something like archive.org reply karaterobot 19 hours agoprevI gladly quit Reddit more than five years ago, but have found myself edging back into it because the most active communities for two of my interests happen to be there. At first I would go a couple times a month to read updates, then a couple times a week, then I started logging in to my old account so that I didn't have to type in the URLs, and then not long ago I found myself writing a comment. So, I'm sort of glad Reddit is adding an obstacle to recidivism. I don't think they're doing it to help me, but that's the net effect. reply CommieBobDole 20 hours agoprevI use Mullvad and have been encountering this error occasionally for at least the past few weeks. Usually disconnecting and reconnecting to get a new IP makes it go away. I assumed it was just automated blocking based on suspicious activity from a particular IP; though I guess it also could also be an attempt to ban all known VPN provider ranges and they just don't have all of the ranges for Mullvad. reply rrr_oh_man 21 hours agoprevThat’s great. Worked similarly for the twitter addiction. reply voisin 20 hours agoparentI’ve heard of Growth Hacks, but it really does seem like Steve Huffman could right a book on Decline Hacks. He may be the GOAT. reply lurn_mor 20 hours agorootparentYou're write. reply stvltvs 20 hours agorootparentYour rite. reply bombcar 19 hours agorootparentThe suppression of the .old rite by the modernists will be the death of Reddit! reply rrr_oh_man 20 hours agorootparentprevI’m huffin' and I’m puffin' and I’m gonna burn my house down reply djcannabiz 20 hours agoparentprevI noticed something similar happen to me when youtube started blocking adblockers. reply kogir 20 hours agoprevEverything works fine via iCloud private relay. No account or other mitigations required. So perhaps it’s not VPNs that are blocked and instead the traffic you’re sharing an IP with. reply thih9 19 hours agoparentThat is a possibility. Then again, another user reported icloud private relay not working[1]. Thresholds, location constraints, or ab testing could be at play too. [1]: https://news.ycombinator.com/item?id=39884164 reply SushiHippie 19 hours agoprevIf you don't need to be logged in, redlib (previously libreddit) works well. Similar to what nitter was to twitter. https://github.com/redlib-org/redlib?tab=readme-ov-file#inst... reply nouryqt 18 hours agoparentI would like to add the browser extension LibRedirect[0] that automatically redirects your reddit (and others) links to your configured instance [0]: https://libredirect.github.io/ reply EVa5I7bHFq9mnYK 17 hours agoparentprevhttps://libreddit.privacydev.net/ works for me. reply athinkingmeat 3 hours agoprevHave tested a couple of Surfshark locations - most of them are blocked by Reddit (new/old). When Reddit farts - Fediverse ignites the fire: influx of users to Lemmy and other similar sites is visible, so I encourage to checkout it and maybe run an instance for yourself and your friends as well. reply EMM_386 17 hours agoprevTested, and ... no? I tried two different VPNs, currently on NordVPN ... no issue accessing Reddit. And I get that it's cool to bash Reddit on HN, but the sense that \"I don't visit sites like that ... I exclusively visit HN\" is really weird. For what it's worth I have an 18 Year Club account on Reddit and I'm still active. reply tyingq 16 hours agoparentIt's not a perfect \"block every VPN ip setup\". See this for the screen you get when it's blocking: https://imgur.com/a/3cDv851 Perhaps when they detect abuse (screen scraping folks trying to avoid the API costs) from these shared IPs? reply joveian 9 hours agorootparentIt is not just shared IPs, I have a personal VPN/proxy that only I use (although in an IP range that likely includes shared VPNs) and it is blocked. It could be that they are blocking all \"non-residential\" IP addresses and the VPNs that work are using botnets (or some other way they get \"residential\" IP addresses) to route traffic. My memory isn't that great and I rarely look at reddit (just the occasional search results) but I think there might have been a similar block on old for a bit earlier (closer to when the block on the main domain started). reply iLoveOncall 16 hours agorootparentprevSo you can bypass it by creating an account? Absolute non-issue once again. reply tyingq 16 hours agorootparentMaybe? Reddit isn't renowned for accuracy. reply thih9 16 hours agoparentprevThis happened for me when I wasn’t logged it, this is a likely factor. Were you logged in when testing this? reply friend_and_foe 12 hours agorootparentNot who you're replying to, but it happens to me logged in. reply lupusreal 16 hours agoparentprevJust because you can't presently reproduce something doesn't mean that others are imagining it or making it up / \"bashing\". Remember that, besides VPN blocking being an imperfect and fickle thing, these companies like to A/B test on the unwitting public so different people can inexplicably get different experiences. When you can't reproduce something other people claim to be experiencing, resist the urge to call them a liar and spend a moment to consider other possibilities. reply _xander 19 hours agoprevYep, for me this means I simply can't use the site anymore. Recommend: https://old.lemmy.world as an alternative reply metflex 13 hours agoprevIn my view, this action should not have been taken without first ensuring that people from countries where internet access is heavily restricted, such as Iran(where using a VPN is most common method to access the free internet and information is already limited), still have a means to access it. I had a friend who discovered she had to undergo an STD.free-test when she needed to through reddit. and now reddit is not accessible anymore to people living where she used to live. reply neurostimulant 19 hours agoprevReddit is blocked in some countries and the best way to access it is by using a vpn as those countries usually block or transparently routed dns queries to other dns servers to enforce their national block list. I guess redditors in those countries is in for a surprise. reply nabla9 20 hours agoprevThe worst thing about VPN:s is the other users who get you blocked with them. reply havaloc 20 hours agoparentI'm trying to stick with Brave browser after moving away from Chrome, and the fingerprinting protection is already driving me up the wall; I can only imagine how much trouble being on a VPN IP is. I get why sites use IP reputation, but it's not a great experience. reply rixthefox 20 hours agorootparentI’m more impressed that places are still trying to hold onto IP Reputation in the age of CGNAT where the age old assumption that 1 IP mapped to 1 customer’s house is completely shattered to 1 IP shared in a geographical region. reply havaloc 20 hours agorootparentCGNAT is also unusable. I had to buy a static from my ISP as they only do CGNAT otherwise. reply Beijinger 19 hours agorootparentprevAstrill works pretty well. I am currently with AirVPN and AirVPN gets blocked a lot. Guess you get what you pay for. Reddit also blocks a lot of Ukraininan IPs reply cess11 20 hours agoparentprevI've had that happen with IP:s handed out to ADSL and 4G as well. Maybe it's more common with VPN:s, though it's not with the provider I use. reply cyanydeez 19 hours agoparentprevNorth American Boy Love Association #9 Is that an acronym? reply Y_Y 17 hours agorootparentI'm sure you know that NAMBLA is the North American Marlon Brando Look-Alikes and nabla is a name for the upside-down capital delta used to denote things like vector gradient. reply dboreham 19 hours agoprevCurious why a VPN is necessary to access Reddit. NSWF content that's blocked by the ISP? reply thih9 19 hours agoparentI sometimes enable vpn system wide for reasons unrelated to reddit and want to access reddit on the same machine. reply EasyMark 15 hours agoparentprev- I use a VPN as generic connection, always - i'm often on public wifi as I work remote and travel - It's not really for reddit, it's for random other sites, and I switch IPs often as part of a multilayered privacy protocol - I like keeping people trying to track me on their toes - Primary reason is to keep ISP from tracking my activties when at home ~75% of my time. AT&T (and other ISPs) is well known for selling your info to 3rd parties reply oyster143 19 hours agoparentprevIf you use vpn for other purposes,its just a hustle to constantly switch it on and off. reply yjftsjthsd-h 18 hours agoparentprevYou misunderstand; I run all my traffic through VPN to stop my ISP from spying on it. reply superkuh 17 hours agoparentprevI always use a proxy to my VPS when browsing the web since ~2013 when my ISP Comcast started (and never stopped) it's MITM attacks which inject javascript into HTTP sessions' HTML. This behavior would be illegal for most entities but Comcast is a big enough corporation they don't have to follow the law (CFAA). reply chilling 19 hours agoparentprevsome countries block Reddit by default. reply ttul 17 hours agoprevReddit is so far from what it was back when it was cool. I get it: they need to make money. And they will make money. But I don’t want to play there. Where else can I go? reply itsoktocry 16 hours agoparentWhat subreddits do you frequent? If the smart people are still there, stay. If they are leaving, find out where to. reply keepamovin 21 hours agoprevThis is IP based blocking. You can get around it by creating a Reddit account, or logging in. I Just did that on my remote browser app, the demo of which is here: https://puter.com/app/cloudtabs-browserbox If you read past the \"Woah, there pardner.\" You'll see a paragraph that says, Try logging in or creating an account here to get back to browsing. This works. You can use reddit if you just create an account or sign in. I guess this is IP-based blocking to counter bots. edit: Oh boy, these tiny servers are not meant for this kind of hug. They may suffer! reply thih9 21 hours agoparentEdit: parent commenter seems to be promoting their product. Looks like a random remote browser, perhaps avoid entering important credentials there. Didn't work for me. I tried creating an account[1] moments ago; after filling out the form and submitting, I got back: \"an error occurred (status: 403)\". [1]: https://old.reddit.com/login/ reply keepamovin 17 hours agorootparent> Edit: parent commenter seems to be promoting their product. Looks like a random remote browser, perhaps avoid entering important credentials there. True. Maybe I should have put a full disclosure? I thought it was obvious, but I get if it wasn't. I'm sorry for not being more clear! It's a good point to advise people to avoid entering important credentials in something that probably looks untrusted. I'd also advise that at this stage as we have no SLAs for now, and are just testing this SaaS-to-be demo of this source-available product: https://github.com/BrowserBox/BrowserBox Thank you for pointing out the reasonable and important security concerns. Although I should have probably done that myself, I was just so eager to help! Aside: I am surprised tho that you were unable to even login on your VPN. I would think that the IP blocks we run the browser form and those of a VPN would be in the same category of 'cloud IPs', so why should it work on CloudTabs but fail for you directly on a VPN? Who knows? reply keepamovin 20 hours agorootparentprevOK, I don't know. Did you try just visiting some random reddit website from the remote browser I provided? edit: I just tried again on a fresh CloudTabs browser and I could successfully create an account and sign in to reddit. So this does work! What did you try? I think it might matter how you arrive at reddit. Try going through the address bar search to a sub you like, rather than pasting in a direct \"login to reddit\" URL. Idk for sure, but that might be more sus to their systems haha! :) You need to click through a Captcha (at least I did ~ first time I got 1 \"bicycle\", this time I got 1 \"motorbikes\" and 1 \"stairs\"). Perhaps they adapt their IP blocking, this demo only uses 4 IPs globally. Maybe if we push too many users through there, they'll block anyway?? Idk. Anyway, I just tested after your comment and it worked so we'll need to discover what happened for you. reply thih9 19 hours agorootparentMy setup doesn’t include a remote browser and I’d prefer not to add one. I listed what I’m using - safari desktop, nordvpn, the sign up link that I checked is in the grandparent comment. This worked earlier for me, now it doesn’t. reply keepamovin 19 hours agorootparentNo worries! That must suck that you can't get on reddit via vpn. I noticed the change a while ago, but was like 'blargh old still works'. So I really appreciated your post today that old no longer works! Re connecting: I'm just sayin you can still connect to reddit over a remote IP if you login/signup per my tests. reply maroonblazer 19 hours agorootparentprevWorks for me. reply TomSwirly 20 hours agoparentprev> \"Woah, there pardner.\" Cutesie error messages aren't funny even the first time, and by the tenth time they are wildly annoying. > if you just create an account \"Just\" reply soraminazuki 19 hours agorootparentAgreed, surrendering our privacy is anything but \"just.\" reply ink_13 17 hours agorootparentGiven how easy it is to create a reddit account I don't know how much privacy is being surrendered in practical terms reply stepupmakeup 16 hours agorootparentCreating a reddit account took 25 seconds and they even generated a username for me. No email verification necessary, sorry to a@example.com if you're getting emails about reddit user \"Agile_Rectangle729\" reply rrr_oh_man 19 hours agorootparentprevReminds me of Jurassic Park reply hyperdimension 20 hours agorootparentprevI remember getting a laptop with Windows 8 about ten years ago and upon turning it on, the Out-of Box Experience (OOBE) \"greeted\" me, something like 'Hi. We're getting things set up for you' \"Hi?\" \"We?\" You are a piece of software. There is no \"we\" here. I can't even begin to express how infantilizing that sounds to my ears. Anyway, I didn't last long on Windows for that laptop. /rant reply xandrius 20 hours agorootparentYou're not the target user. reply hyperdimension 17 hours agorootparentI know. A few years later, I was helping my mom set up a new laptop and during the same OOBE, I made pretty much the same comment, and she seemed to think it was just fine or at least not weird-sounding. Come to think of it, I wonder if it's for the same thing that gets people to refer to Google Assistant/Siri/et c. as 'she'. I've noticed that a lot also. reply nullindividual 17 hours agorootparentIt's normal behavior, we assign gender to all sorts of objects be they made of metal and float on water or are just a series of electrons informing metal gates. The message is fine for the true target audience which is vastly different from the audience of the 80s and 90s. Microsoft and Apple (well, Apple HAS been using friendly language since the 80s!) are smart to use friendly language. Plus, it gets us all used to our corporate god-kings when we're dominated by the AI angels. reply nullindividual 17 hours agorootparentprevWould you rather have a hex code? “Please wait and consult your operators manual: 0x0A” reply bombcar 20 hours agorootparentprevI’m fine with that - it’s clear that “We” is Microsoft. The infantilizing error messages are where it’s infuriating because it comes off as if they’re making fun of you. “Oopsie woopsie we did a fuckywucky and all your data is gone! Cherriooooo couldn’t be me!” reply throwaway4220 18 hours agoprevDoesn’t work on Mullvad. That “Whoa there, pardner” is so condescending reply ChrisArchitect 13 hours agoprevSo you want to use Reddit, but you don't want to be a user of Reddit? who knows what their reason for blocking some commercial VPNs is, but they have their reasons good/bad. You can still 'access' Reddit. reply INTPenis 16 hours agoprevIt's getting harder and harder to be anonymous on reddit. I used to rotate my accounts every year but now they keep shadowbanning me. So I had to jump through some hoops, and while one VPN was indeed instantly network blocked, another country/server from the same vendor was not. So eventually I got my new account for 2024 sorted. But now I'm afraid to login from my regular browser because I'm pretty sure they have it fingerprinted. The only reason I got this far with the account was through Firefox private mode, and creating the account at a friends place. So I've upgraded Firefox now and hopefully that, and releasing my ISP DHCP IP, will be enough. reply bun_terminator 16 hours agoparentreddit is _much_ more lenient on anything if you use the official app. This concerns shadowbanning from known devices as well as banning the account for things they don't like. You get away with a lot more reply replwoacause 8 hours agoprevI quit Reddit and use HN exclusively. I miss the forum days, early Reddit, and feeling of community. Is there anyplace to go these days, aside from here of course, that has quality content with good moderation on a diverse set of topics? reply tsujamin 20 hours agoprevSuper annoyingly iCloud Private Relay triggers this for me too reply bhelkey 18 hours agoparentIs iCloud Private Relay not Apple's VPN? reply throwaway4975 16 hours agoprevIt's not VPN access that's blocked specifically; it's access from IPs that are identified as hosting providers (AWS, Hetzner, etc.). If it affects you, the workaround is to be logged in. reply thallium205 20 hours agoprevYes but it isn’t a recent development. They’ve been blocking ExpressVPN IPs for the past year or so. reply skygazer 14 hours agoprevI just tried via my VPN (Speedify) and get a network policy page that forced me to log in, but gave me unfettered access thereafter. reply different_base 18 hours agoprevI deleted my Reddit account long back after they blocked VPNs. I used old.reddit.com usually from DDG or Google search results. Now this is fucked up. I am not able to access Reddit through either VPN or Tor. They are dead to me now. I just want a solution to completely remove Reddit links from appearing in search results. Anyone has any solution? reply I_Byte 18 hours agoparentYou may be able to find an extension for your browser of choice to do just that! I have one for Firefox but I’m on mobile right now and can’t find it. I can send it later if you’d like. But they do exist! reply lgvld 18 hours agoparentprevuBlacklist maybe? if you use Google for search https://github.com/iorate/ublacklist reply I_Byte 18 hours agorootparentThis is what I use! I recommend it. reply matteoraso 15 hours agoprevFor some reason, the Reddit onion mirror keeps working. Makes me wonder if the VPNs not working is a bug rather than an intentional choice. Mirror: reddittorjg6rue252oqsxryoxengawnmo46qy4kyii5wtqnwfj4ooad.onion reply alecsm 17 hours agoprevI thought it was my company VPN that blocked reddit at first but no. Weird move. I just wanted to see a discussion about some problems with I don't remember what software. I guess reddit is out of the list of websites to reach to. reply k8svet 18 hours agoprevDamn shame, there are some fun, very above board subreddits that I wouldn't care to be on without a VPN. Granted it would be easy to migrate them as well. And in fact, those same communities are probably on Discord. reply mark_l_watson 18 hours agoprevI am used to having streaming video providers blocking VPNs to reduce account sharing with friends, and I understand that. However, move regular sites are blocking VPNs which is bad, especially when using public WiFi networks. reply chilling 19 hours agoprevI work with Chrome-based addon \"AdGuard VPN\" and it still works fine. It will be pitty to block users from using VPN as there are countries (like Indonesia) that blocks Reddit by default. reply 33 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Users are facing difficulties accessing Reddit through a browser while using a VPN, encountering error messages on both the regular and old subdomains.",
      "Previously, VPN access on the main website was blocked three months ago, but the old subdomain remained accessible until recently.",
      "The user, testing with Safari and NordVPN from the EU, wonders if others are encountering the same problem and posted about it on Hacker News."
    ],
    "commentSummary": [
      "Reddit users are grappling with VPN access blocks, reduced engagement, and content quality decline, leading to exploring other platforms.",
      "Challenges with online moderation and concerns about algorithm-driven social media trigger debates on free speech and the future of online interactions.",
      "Dissatisfaction with Reddit's changes prompts users to seek alternative platforms ensuring user-friendly experiences and upholding transparency and community integrity."
    ],
    "points": 360,
    "commentCount": 283,
    "retryCount": 0,
    "time": 1711888954
  },
  {
    "id": 39883328,
    "title": "Goodbye to letsblock.it: Creator Shuts Down Official Instance",
    "originLink": "https://github.com/letsblockit/letsblockit/discussions/663",
    "originBody": "letsblockit / letsblockit Public Notifications Fork 34 Star 793 Code Issues 20 Pull requests Discussions Actions Security Insights Shutting down the letsblock.it project and its official instance #663 Pinned xvello announced in Announcements Shutting down the letsblock.it project and its official instance #663 xvello · 7 comments · 3 replies Return to top edited xvello Maintainer The letsblock.it project enables users to remove low-quality content and useless nags, to tailor their online experience and focus on what matters. It has been running for more than two years now and its official instance currently serves more than 800 active users and hundreds of anonymous visitors every day. We achieved this thanks to dozens of contributors and financial sponsors who I am grateful for. I started this project in 2021 as a way to help people regain agency over their online experience: enshittification of the commercial web had already started, but deceptive patterns and user-hostile features were not so prevalent yet. In that context, I naively hoped to be able to contain most of this through content filters and started writing the templates that became letsblock.it. Unfortunately, the commercial web is getting worse every day, with: user-hostile design becoming the norm, as big tech corporations are set to extract as much money from society as possible the \"generative AI\" bubble wasting precious water and electricity to drown good content into a sea of derivative low-quality content the advertising company Google continuing their efforts to deny people control over their browser, neutering content-blocking extensions with MV3 under false security claims and planning to lock down the OS and browser with DRM. This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive. I want to support communities and applications that respect their users and value what we have to say. These websites don't need letsblock.it rules, because they don't shove low-quality content and anti-features down our throats. What will happen The official instance hosted at https://letsblock.it/ will be shut down at the end of June 2024. Because its database contains personal information, I will not hand over its operations to a third party. Its database will be destroyed after users get a chance to migrate out. Limited maintenance to guarantee security will continue in the meantime, but the rules will not receive any fix or update. If a group of users wants to carry these efforts forward, I'll support them in forking the project and setting up a new server if they wish. I'll happily publicize this fork and assist users in migrating their data. Provisional timeline March 2024: official announcement Announcement on OpenCollective and removal of all contribution options Announcement on a GitHub discussion April 2024: allowing users to migrate out Registrations of new accounts on the official instance will be disabled and a notice linking to this discussion added Import and export functions will be added and documented Recommended alternatives will be listed to help users move on June 2024: shutdown of the official instance, archival of the project Assuming that no external factors (cost increase, cyber attack...) require to shorten this delay: The official instance will be shutdown and all user data deleted at the end of the month The GitHub project will be archived and not further updates released, even for security fixes What's next? After several failed projects and abandoned hobbies, launching letsblock.it and keeping it running for over two years is a big success in my book. I am very grateful to everyone who contributed to this project and made it possible. My motivation has always been to be useful to others and I think it's better to give our users a controlled shutdown than keep the project stale and unmaintained. This is not a call for contributors or new maintainers. If users want to take over this project's legacy, I'd prefer it to be done under a new name to avoid any confusion. Feel free to coordinate on this discussion though, and I'd be happy to support you with the setup. 40 115 Replies: 7 comments · 3 replies Oldest Newest Top onedr0p Thanks for the years maintaining this project, sad to see you drop it but I understand. Take care! 11 20 0 replies marc-mccarthy I have never heard of this project before and am upset that I didn't... Wondering if there is somewhere to get more information on the applications you are trying to support moving forward? Thank you for your influence and the links above in your words! 8 7 1 reply edited xvello Maintainer Author Heya, thanks for your kind words! I will take a break before committing to anything, but I'm currently poking around the Fediverse and have a couple of ideas I'd like to try out. You can follow me there if you wish! I am also moving to the non-commercial forge https://codeberg.org/ for hosting my next projects and evaluating whether I could be useful to them. Welding-Torch I'm surprised I didn't find this project before, I love ublock and more filters for ublock are welcome. 4 4 0 replies lez Deshittifying commercial web is like trying to run away from a (fast moving) zombie who keeps chasing you forever. Building non-commercial web is like finding a quiet place and build everything from the ground up with your family and best friends. See you later on Nostr, my friend! 4 5 0 replies holdit Sad news. I know the right thing to do is to stop using certain services, but we can't always do that. To the creator and every contributor, thank you for your work! It made my web experience much better. 2 3 0 replies edited OldGuyInTheClub Please know that your project has made my web life much more bearable. Thanks to you and all the contributors for your efforts and all the best to you. 2 2 3 0 replies johnnydecimal The customer-facing page needs an update, it currently makes no mention of this. 1 1 2 replies edited xvello Maintainer Author This volunteer-lead non-commercial project does not have any customers. Adding a notice to the interface is listed in the April 2024 items. My plan it to wait a few days in case a fork is happening. 3 johnnydecimal My apologies, I wasn't trying to be snarky. I use 'customer' as a catch-all for 'the people who use the thing', whether they pay or not. I just didn't want anyone signing up for a thing that is in the process of being shut down. 🙃 1 Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Category Announcements Labels None yet 8 participants",
    "commentLink": "https://news.ycombinator.com/item?id=39883328",
    "commentBody": "Shutting down the letsblock.it project and its official instance (github.com/letsblockit)304 points by imbnwa 22 hours agohidepastfavorite110 comments crest 20 hours agoI have to applaud taking even \"failed\" projects serious enough to come up with a reasonable exit plan (unlike some large companies). reply andsoitis 19 hours agoparentnext [28 more] [flagged] chrisoverzero 14 hours agorootparentSomeone once said here on Hacker News: Before you say anything, ask yourself 1) Does this need to be said? 2) Does this need to be said by me? 3) Does this need to be said by me, now? I hope you’ll take their sage advice in the future: https://news.ycombinator.com/item?id=39859458 reply daedrdev 17 hours agorootparentprevReputation has some value. If the author tries and make a new product, perhaps some people will be more inclined to try it with the belief that they won't be left out to dry if it fails like most other products. reply hinkley 16 hours agorootparentPeople also seem to forget that your reputation inside of your own head matters for a lot of people. People have different ways to frame or reframe past failures, and to get past them. Besides, begrudging people time spent on community sounds like something your last shitty manager would do. Can’t tell if GP is shitty manager material or has Stockholm Syndrome from them. Odds favor the latter. reply gorlilla 15 hours agorootparentThe invisible audience is the worst. reply idontknowifican 16 hours agorootparentprevi can appreciate how hard you worked on this, it is an achievement in aggrandizing to provide this feedback. i hope your friends and family truly appreciate the perfection of your worldview, and can move fully to your simple answers reply danShumway 19 hours agorootparentprev> If those communities and apps already respect their users, then do they need this author? What difference would they make? What? Does software development just go away if you respect your users? Do people not want 3rd-party Mastodon or Matrix clients just because the orgs aren't running advertisements on their servers? Is the idea that I install Linux and everything just magically works because Linus isn't trying to sell me something? Building services that respect users is still work. It's still a thing that people can help with and support, either directly or through 3rd-party tooling and extensions. reply andsoitis 19 hours agorootparent> Do people not want 3rd-party Mastodon or Matrix clients No, there isn't a reasonably sized market for many 3rd party Mastodon or Matrix clients. reply danShumway 18 hours agorootparentThis is silly, of course there is. There's observably a larger market for 3rd-party Mastodon and Matrix clients than there is for a Nebula filtering tool. Also, are you seriously going to argue this? Are you seriously going to double down on the idea that services that respect their users don't need development help or tooling support or developer ecosystems? Have you heard of Blender? This is just straight-up silly, I don't know what else to say about it. FOSS projects and platforms have the same exact support and community needs as any proprietary service does. reply andsoitis 18 hours agorootparent> Also, are you seriously going to argue this? Are you seriously going to double down on the idea that services that respect their users don't need development help or tooling support or developer ecosystems? No, I'm doubling down on challenging the author to be more audacious: a) being honest with oneself and learning from failure (they call their project a \"big success\") b) be more ambitious - building or contributing to yet another 3rd party Mastodon or Matrix client (both?) is just more noise in a crowded space going after a relatively small audience. reply danShumway 18 hours agorootparent> No, I'm doubling down on challenging the author to be more audacious: You've edited your comment since it was originally posted, but as a reminder, this is what I was responding to: > If those communities and apps already respect their users, then do they need this author? What difference would they make? Which is a pointedly ridiculous thing to say. No, you were not calling on the author to be more ambitious, this is silly. --- > b) be more ambitious - building or contributing to yet another 3rd party Mastodon or Matrix client (both?) is just more noise in a crowded space going after a relatively small audience. Donation of time and effort to make platforms more attractive that you are ethically aligned with is not just \"noise in a crowded space\". As if FOSS tooling was a crowded space!! These projects are desperate for volunteers, and lack of developer time and resources is one of the biggest handicaps that these projects often face. It is unquestionably valuable for people to volunteer effort in this direction. It might not be flashy, it might not give you a giant userbase. But some people have different metrics of success than that, and high-impact activities often aren't flashy. You bring up building new projects as an alternative to donating to existing efforts. Yeah, that sounds very attractive and flashy, but there are a half-dozen FOSS platform alternatives to every proprietary service and we don't actually need more of them. What we need is for people to focus on a couple that already exist and make them as good as possible. It's less sexy, but it matters more. reply dingnuts 13 hours agorootparent> As if FOSS tooling was a crowded space!! These projects are desperate for volunteers, and lack of developer time and resources is one of the biggest handicaps that these projects often face thinking about eg the recent xz vuln, isn't this somewhat indicative of a licensing failure on the part of FOSS licenses? the point of licensing is to ensure the author is properly compensated for his work, and if the work is used by billion dollar companies while authors scrape by and beg for donations, it seems to me that the license isn't doing its job. the license should capture some value and give it to the author. it's completely absurd that professional tooling is begging for volunteers. you don't see that in any other field. reply dEnigma 11 hours agorootparentThe point of FOSS licenses is keeping code free and open-source, not compensating anyone. reply Bjartr 19 hours agorootparentprevWhat does \"reasonably sized market\" mean in this context, and what does it imply about projects that lack them? More specifically, why does there being some people that want those things mean something other than some people want those things? It almost sounds like you're trying to have a different conversation. reply andsoitis 18 hours agorootparent> What does \"reasonably sized market\" mean in this context, and what does it imply about projects that lack them? Projects that lack a reasonably sized market are fine, but their impact will be confined. It is a tradeoff you choose to make. > More specifically, why does there being some people that want those things mean something other than some people want those things? It almost sounds like you're trying to have a different conversation. Because if you want to have an impact, you should choose where and how to spend your time. You should pour a different amount of effort and resources into doing something for 1 person vs. 800 people vs. 1,000,000. reply Bjartr 15 hours agorootparentWas impact something I missed that was already in this discussion? reply tentacleuno 18 hours agorootparentprevFor Matrix, there definitely is. Some of the third-party clients are just plain better than Element. reply andsoitis 18 hours agorootparent> For Matrix, there definitely is. Some of the third-party clients are just plain better than Element. Which client do you use the majority of the time? reply rakoo 18 hours agorootparentNot the original poster, but Cinny is better than Element reply danShumway 18 hours agorootparentprevFluffychat reply andsoitis 18 hours agorootparentAnything missing from it that you wish it had? Or something it has that you would prefer if it were gone? reply danShumway 18 hours agorootparentVideo calls are a big missing feature, that's one of the only reasons I have an alternative installed alongside it. From what I've heard, the devs have said that the reason why native audio/video calls aren't planned literally just comes down to complexity and the maintenance cost. Matrix has been focusing more recently on trying to break some core functionality out into modules that can be imported -- encryption being the biggest example of this. Movement in that direction for video/audio would be a big deal. It's not just that Fluffychat needs help getting video working -- that would only leave them with another chunk of code to maintain. Ideally, they should be able to import a common library for this, but as far as I know, none exists that's usable for their needs. Matrix video calls are a good example of this kind of ecosystem need in general -- Element had video calls before the spec actually supported video... because it used a Jitsi plugin. And that 3rd-party offering bought time for the spec authors to come up with more robust native support. The problem is that 3rd-party clients offering the same support is still wildly complicated. There's a lot of room for improvement there. reply ruined 16 hours agorootparentprevi am pretty sure most people use third-party mastodon clients, if you're talking about apps. they didn't even have a first-party client until recently. reply jimmySixDOF 17 hours agorootparentprevCharming outlook. I can't imagine that mental model leads to any frustration except it seems when you are compelled to offer your hard earned course correcting unsolicited advice (and without compensation no less). Here's some advice too - if you want to be edgy don't point it at someone actually out in the arena closing off with the same self-respect and craftsmenship they more than likely took into it thinking it was something that needed doing. Honestly your statements remind me of this \"Jigar Kumar\" character in the xz fiasco [1] which is a perfect case study of the thankless expectations involved with maintaining an open source project that probably also started out of the sheer belief that it was a useful idea that needed doing. It's people like them who built the web and not just to click farm for attention measurements if you can believe that. Without the long tail of value those people still contribute free of charge to the world there would be a lot less product market fit for people like you to chase after. [1] https://news.ycombinator.com/item?id=39879710 reply schiffern 16 hours agorootparentprevIt sounds like you're the sort of person who really needs to hear this: \"Act that you use humanity—whether in your own person or in the person of any other—always at the same time as an end, and never merely as a means.\" https://www.youtube.com/watch?v=0nz0iaNvVpE reply Larrikin 19 hours agorootparentprevThis comment reminds me of another comment I read some years back on this site. There was discussion about newsletters/mail advertising and one of the writers of such software that power it chimed in. All of the \"legitimate\" mailers have a disclaimer that says it can take up to X number of days before the changes take effect, which normal developers were obviously calling BS on. The commenter said it wasn't intentionally written to be slow at his company but they deal with massive lists. But then the commenter said his company could probably make it faster or near instantaneous but when a user indicates that they want to unsubscribe from their spam they have gone from a potential source of revenue to being valued at zero. He complained that it made no sense to make the unsubscriber's lives better by immediately unsubscribing them instead of focusing his time on trying to make sure the spam got out to more people. It was a thoughtful, rational comment that made me want to be a better developer instead of scum that makes the world worse. I applaud the project author for not being scum. Edit: andsoitis edited his comment after being downvoted. Here is the original reply I replied to. \" > I have to applaud taking even \"failed\" projects serious enough to come up with a reasonable exit plan (unlike some large companies). That's time the author will never get back that they could have spent on their new mission. Maybe it only took them 5 minutes, but I doubt it. Even if it was no time, it reveals a mental model that I think will continue to lead to their frustration. When they say \"This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive.\" it is saying they would rather spend time making their 800 users get a soft landing spot for a product that the author has said is futile, than spend their energy on their new mission. The next sentence I think is reveals more: \"I want to support communities and applications that respect their users and value what we have to say.\" If those communities and apps already respect their users, then do they need this author? What difference would they make? \" reply thwarted 16 hours agorootparent> But then the commenter said his company could probably make it faster or near instantaneous but when a user indicates that they want to unsubscribe from their spam they have gone from a potential source of revenue to being valued at zero. He complained that it made no sense to make the unsubscriber's lives better by immediately unsubscribing them instead of focusing his time on trying to make sure the spam got out to more people. For an industry (marketing) that is all about \"intent\", someone who doesn't want the marketing materials and won't buy and thus has a revenue value of zero and is making that intent clear by unsubscribing, it makes no sense to delay and possibly send more spam to them. It reeks of being driven by the wrong metrics, how many marketing emails were sent rather than how effective are the emails at converting to sales and unsubscribe requests should be informing how effective the emails are. reply andsoitis 18 hours agorootparentprev> I applaud the project author for not being scum. If I were a user of their project, which is free mind you, and they stopped it without a decent shutdown plan because after 2 years they could only get 800 users and they would rather use their life in a different way, I would not think of them as scum. To think someone like that is scum would be pretty toxic in my book. reply gcanyon 20 hours agoprevIt seems futile to filter Amazon products by name, but it's interesting that it seems to be able to filter YouTube shorts. Given that after two years there are only about 30 templates, I'm guessing it's too difficult to actually accomplish something useful given the tools provided. That's not a knock on the creator -- off the top of my head I don't know how I'd provide an easy way to customize/filter/modify web content -- I'm just saying I understand the shutdown given the apparent lack of traction. reply blindstitch 19 hours agoparentUblock origin's filter syntax is very good but has limitations that make it borderline impossible to filter out some elements and also very brittle. For example, this one is one of 20 lines of filters to get rid of Shorts: `www.youtube.com##ytd-mini-guide-renderer a.yt-simple-endpoint path[d^=\"M10 14.65v-5.3L15 12l-5 2.65zm7.77-4.33\"]:upward(ytd-mini-guide-entry-renderer)` The frontend code is so abstracted that as soon as it is updated it is probably going to break. With userscripts you can filter these things with more sophisticated functions, but it slows down the interface more than UBO. In google's case that type of frontend abstraction (imo) is not intentionally designed to break interface filtering but it has that effect. For other companies like facebook and amazon they actively make filtering elements harder, because of their anti-adblock strategies, but they are all casting a wide enough net that element filtering is affected too. It's a long game of cat and mouse. reply itopaloglu83 18 hours agorootparentThe way YouTube forces shorts on users who likes long form content is really frustrating. uBlock filters are great but doesn’t solve the fundamental issue here. There should be a way to opt out of YouTube shorts. reply KennyBlanken 15 hours agorootparentIt's not just the length that's frustrating, it's the near total lack of control. Miss the first few seconds because your sound was off/low? Gotta watch the whole fucking thing to the end and then let it replay because there's no slider or restart/rewind button. All because either they purposefully wanted to force us to do that (view count boosting I guess, or it's more likely to be remembered?) or some google exec got Shorts barely done enough to show off at a board meeting and say \"done, made something that competes against reels!\" and then ran off to work on something else to try and boost their stature in the company. reply gosub100 15 hours agorootparentI think the reason is they know they'll lose the market share if they don't force them. Most people know that they are addictive and \"bad for you\" so many people would not opt in to them. But if they're almost forced, they will succumb to the addiction. reply imbnwa 11 hours agorootparentprevDoes Premium provide this? reply salzig 10 hours agorootparentNo, sadly. reply teddyh 18 hours agorootparentprev> For example, this one is one of 20 lines of filters to get rid of Shorts Just get this plugin:and enable “Hide all Shorts” under Homepage/General. reply squigz 17 hours agorootparentFunnily enough, the code responsible for hiding shorts in that is just under 20 lines long :P https://github.com/lawrencehook/remove-youtube-suggestions/b... reply teddyh 13 hours agorootparentSure, but you don’t have to maintain them. reply squigz 12 hours agorootparentWell, someone does; just like someone has to maintain, say, https://github.com/gijsdev/ublock-hide-yt-shorts reply blindstitch 20 hours agoprevIt was a great project but it's kind of pissing in the wind. These things break constantly as new awful features get added. And there's not much you can do with ublock about high-level frontend changes that are the real problem, like google switching to infinite scroll or removing the plus operator. I can understand getting worn down. reply Cthulhu_ 20 hours agoparentIt really is an arms race, and bigger/more well-known services like ublock origin are in the front lines of responding to the changes. reply rcpt 17 hours agoparentprevI feel like you could use an AI to keep your blockers up to date reply kumio 25 minutes agorootparenthonestly it looks promising given that a lot of people is going to use it, the AI will learn faster to remove all kinds of ads and unwanted stuff. reply worthless-trash 6 hours agorootparentprevI don't think the ai will help you bite their corporate masters long term. reply Quenhus 15 hours agoprevI'm sad to learn this projet is shutting down. The maintainer (xvello) contributed a lot to my uBlock dev filter [0]. We tried to reduce the time lost on deceptive and low-quality content for search engine users. Generative ML and aggressive SEO technics hit hard. Bye letsblockit and I wish you well @xvello. [0] https://github.com/quenhus/uBlock-Origin-dev-filter reply musicale 3 hours agoprev> This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive. I want to support communities and applications that respect their users and value what we have to say. These websites don't need letsblock.it rules, because they don't shove low-quality content and anti-features down our throats. Interesting idea - I like the non-commercial web and am inspired by attempts to invigorate it, and yet I also get the feeling that its ship sailed when everyone left for the commercial web. I don't see them (us?) coming back. reply romseb 19 hours agoprevAt least for the YouTube filter templates, https://unhook.app is a great alternative. reply amarcheschi 18 hours agoparentI'm looking forward installing this on brave as soon as I get my hands on my pc, my attention span is so low that sometimes when I open up YouTube in incognito, I get overwhelmed. (I often do that to not mess with my actual recommended comment if I know I just need to search a video or two for one offs) The homepage shows a lot of crap and clickbait thumbnails, and especially thumbnails with people having weird expressions and staring at me makes my brain completely lose context and lose focus for a few seconds, so that I have actually to think again about what I was gonna search, and sometimes I can't even remember it reply itopaloglu83 18 hours agorootparentI wish YouTube had a premium feature called the focus mode where we can just watch our subscriptions with no hassle. reply eddd-ddde 17 hours agorootparentI use New Pipe for this. Also no ads. reply amarcheschi 18 hours agorootparentprevAlthough I hope something similar will exist, I do believe the push towards promoting bullshit and shallow content is too strong reply itopaloglu83 18 hours agorootparentYou’re right and that push for shallow content just frustrates me. I wish at least they took my subscription fee and distributed to the channels and videos I watched. reply nicbou 7 hours agoparentprevThis is a great one. I've been using it for the past year. reply gcanyon 20 hours agoprevI have to wonder what the creator has against Mike Boyd? \"Nebula: filter out videos by creator... To get the code for a creator, go to their page... For example, Mike Boyd’s page...\" Filtering Nebula in general seems like a low-probability use-case? reply dylnuge 20 hours agoparentNebula used to not have a discovery feed and just a most recent videos one, so I could see someone wanting to filter out creators who they've watched and decided aren't for them to have a better chance of potentially finding new channels they do want to watch. EDIT: Also, not sure if you're just joking or not, but I'd bet the author is a Mike Boyd fan. I feel like people tend to use examples of things they like in documentation, and it's just an example. reply Y-bar 20 hours agoparentprevI don’t know who that is. But I am a Nebula subscriber and I do wish there were a way to hide a few channels in the app. reply magnetowasright 19 hours agorootparentSame here. I have discovered so many new creators on Nebula I never would have found otherwise^* but there's definitely channels I just never want to see. * I never watched youtube as a primary source of entertainment like pretty much everyone uses youtube. I just had channel pages bookmarked (invidious instance links usually lol) so I never stumbled upon relevant channels even on the off chance something not terrible was in the recommendations which I always ignored. I got onto nebula because almost all of the very small handful of creators I watched were on it and it was cheap enough to justify the subscription lol reply itopaloglu83 18 hours agorootparentI have a similar issue with some YouTube channels that despite liking their content I cannot subscribe to their channel because they publish dozens of shorts every week. So I just have a memory list of channels I visit every so often to see if they published anything. The shorts practically broke the subscription system for me. reply gcanyon 20 hours agorootparentprevFair point -- I think Mike Boyd is the \"how long will it take me to learn this obscure thing\" guy. I can see why that might not be to everyone's taste, but he seems relatively innocuous. ¯\\_(ツ)_/¯ The obvious question to me in a situation like this is: how does Nebula provide preference? And honestly, I get it -- I am a subscriber, and I have a hard time getting a feed of the people I'm interested in, to the extent that I just do it to support the creators, I don't actually use Nebula. I continue to watch Nebula creators on YouTube, which has got to be a bottom-tier result for them. reply politelemon 20 hours agoprevI hadn't heard of this until now, but it seems (seemed) like a decent idea. From what I can tell it's a UI that lets you configure what you'd like to block on certain sites, and there can be community contributed templates. You get a URL, you then add that URL to your UBO filters. It actually reminds me of nextdns. reply vidyesh 19 hours agoparentI heard of this project on HN sometime last year, I never used their instance but I have been using their YouTube UBO filters and even contributed to fix some after some YouTube updates. And the UBO filters are fairly easy to maintain which is why I was a little surprised that they are shutting down this project, I understand the instance might be hard to maintain but the filters can very well be maintained I think. Either way, there seems to be other alternatives that do similar things. I will likely be using those then. reply depingus 10 hours agoparentprevIf by UBO you mean uBlock Origin, you can do one better. Use it's eye dropper tool to select elements of a web page to block. I use is it to block Youtube premium nags, comments, side bar of other content, etc. reply OldGuyInTheClub 16 hours agoprevI am sorry to hear this. I have been a satisfied user since I found out about this project on HN a few months ago. It has kept Amazon and a lot of other garbage out of my search results for starters. I use uBlock Origin extensively but have never been able to learn to write my own filters. letsblock.it handled a lot of that seamlessly. My sincere thanks to the developer for his valiant efforts. reply throwaway81523 18 hours agoprevI didn't know about this project and had for a while been thinking of writing something like it. Now maybe I know better. Oh well, thanks for all the fish. reply gnyman 18 hours agoparentSomething along the same line is StopTheMadness (get the new Pro version). Also on some pages disabling JavaScript makes the experience better (although nowadays most pages just break), but if you don't have a way to easily disable it per-website you could check out my pet project https://noscript.it Combined it with STM's automatic url-rewrite and you can get automatic per-site noscript even on the iPhone. reply throwaway81523 16 hours agorootparentShutting off JS often makes things worse. What I really want is an ultra-aggressive Reader Mode that doesn't get fooled by so many sites. That means it has to know how to extricate the relevant text from a bunch of specific sites. At one point I had a special proxy just to clean up a few sites that I was reading frequently, but it broke. I also fooled around with running a headless Firefox under Selenium to get the text out, but I got bored with that. I might try to get that going again, or might try to figure out how browser plugins work. reply eek2121 14 hours agorootparentI've been toying with the idea of using headless firefox with UBO as a filtering proxy, but nothing has come of it so far. I'm actually focused more on writing a quality search engine that ranks pages based on the amount of anti-user behavior, dark patterns, etc. a site has. Have a really crappy site with tons of ads, popup modals, and trackers? No clicks for you! So I'm doing something similar to what the author is doing I guess. reply ryandrake 18 hours agoprevWhat a cool project and much-needed idea! Had I known about it, I'd probably be user #801. It's a shame it's shutting down. The modern web is exceedingly user-hostile, and browsers are not doing a good job as the User's Agent. Browsers are the ones that should provide users control over what they see, what they use, and what they skip. But, instead of giving the user the choice and respecting the user's preferences, browsers are acting like dumb canvases, allowing web developers to just spew whatever they want at the user. We shouldn't need extensions and third party hacks like this to keep control over what our browser does. And we should not accept browsers that just enable the web developer to do whatever they want. Rant over. Awesome project, sorry to see it go, wish I had a chance to experiment with it. reply lawn 19 hours agoprevI've been using this as a good way to filter and configure the familys browsers. For example to filter unwanted YouTube videos oe hiding shorts. Is there another easy way to sync blocklists like this? reply Funes- 11 hours agoprevI think it's time we build real alternatives to dystopian web services rather than try to salvage or parasitize them in any way. reply rrr_oh_man 21 hours agoprevWhat’s the reason? Cost? reply mtlynch 20 hours agoparent>This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive. They don't spell it out explicitly, but I think the author realized that they were effectively acting as an enabler. The letsblock.it tool encouraged customers to use workarounds so that they could still continue engaging with big tech companies that are so customer-hostile. Instead, the author is choosing to let big tech make their experience worse, and customers have more incentive to seek out non-commercial alternatives. reply andsoitis 19 hours agorootparent> They don't spell it out explicitly, but I think the author realized that they were effectively acting as an enabler. > The letsblock.it tool encouraged customers to use workarounds so that they could still continue engaging with big tech companies that are so customer-hostile. With only 800 active users, letsblock.it obviously didn't have any measurable effect. To think otherwise is hubris. People use products from \"big tech companies\" because they offer something that is useful that others don't offer. In the author's conflation of \"big tech\" and \"commercial\", I think they need more clarity in what they really want to accomplish, what their mission is. Being \"against\" something is a valid goal in life, but then you really have to be very strategic about it. Being \"for something\" and pouring your energy into making something that people want or need seems more productive. Even then, you want to be strategic because there's an infinite number of things you could go after. Do you diffuse your attention or do you focus? Supporting the \"non-commercial web\" seems too vague in my opinion. reply danShumway 19 hours agorootparent> Being \"against\" something is a valid goal in life, but then you really have to be very strategic about it. Being \"for something\" and pouring your energy into making something that people want or need seems more productive. From the announcement: > but I'd rather spend my energy on making the non-commercial web more attractive. The author pretty explicitly states that they want to shift from an \"against something\" mentality (against disruptive content in proprietary apps, against the intended user-experience of those apps) to a \"for something\" mentality (building and supporting non-commercial services). I genuinely do not see the complaint. > With only 800 active users, letsblock.it obviously didn't have any measurable effect. Unless the author was planning on never making a popular project, I don't think this is a good way of evaluating direction or effort. In either case, putting in a huge amount of effort to support 800 active users who might otherwise (at least partially) shift their attention to better services seems reasonable to question. If we take it that there is any value in improving experiences for a small number of people, then there is equal value in making it more pleasant for those people to use Libre services. And of course that's even before asking about the opportunity cost. If an author can take the same amount of time they were devoting to this and instead build tools that make a Libre/Community service more attractive for 800 people, that's arguably a much higher impact activity on the health and growth of that service than wasting that effort trying to make proprietary platforms palatable. But again, if your point here is to focus in on a mission, starting with \"nothing I build will have any impact on any of this\" is just not really helpful at all. > Supporting the \"non-commercial web\" seems too vague in my opinion. A general mission statement/direction is often the first step towards narrowing down product ideas. I think making a decision in a direction (ie, pivoting from doing free UX enhancements for commercial companies towards saying, \"I want to benefit services that don't feel exploitative\") is a good place to start. Of course over time the author will probably narrow that focus, but this at least lays out a category that they can start looking into. reply andsoitis 19 hours agorootparent> > Supporting the \"non-commercial web\" seems too vague in my opinion. A general mission statement/direction is often the first step towards narrowing down product ideas. I think making a decision in a direction (ie, pivoting from doing free UX enhancements for commercial companies towards saying, \"I want to benefit services that don't feel exploitative\") is a good place to start. Of course over time the author will probably narrow that focus, but this at least lays out a category that they can start looking into. This further emphasizes the lack of clarity, focus, and suboptimal strategy. When I read your assessment of their previous strategy \"doing free UX enhancements for commercial companies\" while the person is against commerce, I do not walk away with a sense that they have reconciled for themselves what is worth going after and so the non-specific \"I want to support communities and applications that respect their users and value what we have to say.\" I predict will likely also have no registrable impact. I have more candid feedback for the author and that is to take a more clear look at how they evaluate themselves. They say \"launching letsblock.it and keeping it running for over two years is a big success in my book.\" Instead they should call it what it is - a failure - and learn from it. Failure is fine, failure is great, even. They would be more successful by dreaming bigger and with more focus. reply danShumway 18 hours agorootparent> When I read your assessment of their previous strategy \"doing free UX enhancements for commercial companies\" while the person is against commerce, I do not walk away with a sense that they have reconciled for themselves what is worth going after [...] > Instead they should call it what it is - a failure - and learn from it. I'm going to be really blunt here, it sounds a lot less like your critique is that the author isn't clear about their goals or that the author doesn't know how to evaluate themselves -- and more like your critique is that the author's evaluation of themselves and their goals doesn't match yours. Running any project with 800 users for 2 years as a hobbyist can be reasonably called a success. This reads a lot like how VC people will come into Mom and Pop shops and say, \"this business is a failure, they just have years of loyal customers in a niche, what a disgrace! They obviously haven't thought enough about their product focus.\" reply andsoitis 18 hours agorootparent> Running any project with 800 users for 2 years as a hobbyist can be reasonably called a success. That's not success, and certainly not a \"big success\" which is the author's self-reflection. Unless the author's goal was to run something for 2 years, accumulate 800 users, and then shutting it down. > This reads a lot like how VC people will come into Mom and Pop shops and say I think they set out to make a big impact. That means growth, but doesn't imply commercial success. reply danShumway 18 hours agorootparent> That's not success, and certainly not a \"big success\" which is the author's self-reflection. Again, the author is not obligated in any way to align themselves to your definition of success. And them disagreeing with your definition of success is not the same thing as them being confused or not having thought enough about what they want. It might just mean they disagree with you. > I think they set out to make a big impact. That means growth No, not necessarily. Growth can be a component of impact, but they are not synonymous, and many highly impactful projects never see a lot of attention or direct growth -- they enable other projects to succeed or fix some of the many diverse pain points that subsets of users for those projects have. reply bee_rider 18 hours agorootparentprev800 people might not fix the whole tech ecosystem (it is impossibly broken which so I can see why the author would like to just go work on something else). But if you got 800 people in a room to say thanks, I bet it would feel pretty cool. (This isn’t intended as a full counter argument against your broader point, which I’m still not really sure either way about, I just wanted to note that sometimes we have small effects and that’s OK. We’re only individuals after all, it wouldn’t make sense to expect every person to change the world in some sense, it would be chaos). reply andsoitis 16 hours agorootparent> But if you got 800 people in a room to say thanks, I bet it would feel pretty cool. Absolutely. It feels great, and that is a perfectly valid reason to spend the time - so that you feel the warmth from others. reply krainboltgreene 19 hours agorootparentprevSuch a weird comment. Are you trying to debate them back into working on the project? reply andsoitis 19 hours agorootparent> Are you trying to debate them back into working on the project? No, I think the project was based on wrong assumptions (strategy) and poorly executed. I am merely putting constructive ideas out there. reply zekrioca 18 hours agorootparentI don’t think you are. You have a very utilitarian set of ideas, where optimization towards some unspecified goal of commercial success is the objective, and everything else is deemed ‘lack of strategy’, or ‘poor’. reply andsoitis 18 hours agorootparent> You have a very utilitarian set of ideas, where optimization towards some unspecified goal of commercial success is the objective, and everything else is deemed ‘lack of strategy’, or ‘poor’. Their strategy was to provide free UX enhancements for commercial companies. When they are against the commercial web! Success does not have to be commercial. It can be about non-monetary impact. They had 800 users and tells themselves \"launching letsblock.it and keeping it running for over two years is a big success in my book\". Claiming that outcome as a big success is odd and I don't know that the author is learning from failure. When they can say to themselves \"I failed in my mission, let me learn from it,\" I think they will have a larger chance to grow, be more successful, have bigger impact. reply idontknowifican 17 hours agorootparentprev“constructive ideas” is doing a lot of heavy lifting here in ignoring your lack of empathy. i am always amazed by the people that provide “constructive ideas”, and then fail to take any “constructive ideas” from others. you are failing in the same way as the original author by your own metric reply andsoitis 16 hours agorootparent> “constructive ideas” is doing a lot of heavy lifting here in ignoring your lack of empathy. Empathy can be “nice” but does not necessarily mean it is helpful, and can sometimes even be harmful. When you care about someone, but fail to challenge them directly you are not helping them, you just coddle them. reply Handprint4469 19 hours agorootparentprevding ding ding! Exactly. If the commercial enshittified web bothers you to the point of trying to fix it, at some point you'll realize that it's a quixotic crusade, and that you don't even want to engage with the content offered on these platforms anymore. Why would you? The shittier the platform, the shittier the content. The future of good online interactions is in small, closed, well-maintained and asynchronous communities. reply nonrandomstring 18 hours agorootparent> Why would you? The shittier the platform, the shittier the content. Paradoxically that's the way some people find salvation. They have to hit the bottom. Instead of making their experience of digital abuse more palatable what they need is more YouTube, more Facebook, more Snapchat, TikTok and Instagram. Until something inside snaps and their soul pukes. I'd totally get it if the author realised they were just prolonging users' misery. As Nietzsche said; \"What is shaky, push it!\" reply blindstitch 18 hours agorootparentprevThe last line resonates with me as the discord I hang out in is my only source of good recommendations anymore. Almost all of the good stuff I've seen in the past years comes from there. My algorithmic feeds by comparison are all high-viewcount trash for idiots. That onion article from years ago about the lowest common denominator dropping at an alarming rate has only gotten more true over time. reply davidmurdoch 21 hours agoparentprev> This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive. Sounds like they got bored. reply nalinidash 20 hours agorootparent> This project is making the commercial web more bearable, but I'd rather spend my energy on making the non-commercial web more attractive. I want to support communities and applications that respect their users and value what we have to say. These websites don't need letsblock.it rules, because they don't shove low-quality content and anti-features down our throats. Does this sound \"bored\"? reply davidmurdoch 20 hours agorootparentYeah, perfectly. That's why I suggested it. It sounds like he was super interested in one thing, 2 years passed, and now he isn't interested in that one thing and wants to do something else. Bored. There's nothing wrong with that. Being bored isn't a bad thing. reply maxcoder4 17 hours agorootparentI don't think I would call that bored. It's say this project let them grow enough that they realized that what they really want is something different than what they're working on. But to me \"bored\" sounds like a bad thing, so maybe I'm just arguing semantics. reply davidmurdoch 16 hours agorootparentBoredom isn't bad; its often how we become creative and create novel things. Not really the same quality of boredom as what I'm talking about, but this video from Veritasium is still relevant, I think: https://www.youtube.com/watch?v=LKPwKFigF8U reply boesboes 20 hours agorootparentprevMore like 'Frustrated' if you ask me reply ramon156 20 hours agorootparentprevInatead of asking for a new maintainer they \"shut it down\", making it a more attractive project to pick up. Smart reply mock-possum 20 hours agorootparentprev‘Bored’ isn’t what I’d read into this - it sounds more like a shift in priorities to me. reply 63stack 20 hours agorootparentprevnext [3 more] [flagged] davidmurdoch 20 hours agorootparentYeah? Well, you know, that's just like, uh, your opinion, man. reply davidmurdoch 20 hours agorootparentNo Big Lebowski fans on HN? The commenter called me a liar, there's just no dialogue worth having with this person. reply nipperkinfeet 6 hours agoprevSad to see it go. Thanks for all the years! I used it a lot to hide YouTube Shorts garbage. reply yinser 13 hours agoprevThey’re not wrong that generative AI is enshittifying content but ironically they could leverage it to keep up with the template generation letsblockit could benefit from. reply nickdothutton 19 hours agoprevReading this submission and the comments here, I am struck by how much pressure build-up there is now against the ensh*ttified web. Surely something good much come of this, and if it is via the mechanism of creative destruction… so be it. It may not come this year or next, but within 5 years? I could see that happening. reply throw310324 19 hours agoprevThis was a failed idea from the start. Furthermore, the \"commercial web\" is not so bad. Sometimes I'm not sure if peoole like this dude truly live in a bubble. I mean, you have to take HN with a grain of salt, opinions here not representative at all and not better than the ones from the general population. reply frankzander 18 hours agoparentWell ... Bubbles are everywhere ... you may live in one too? So maybe I am. \"not so bad\" ... means what? It's bad but I got used to it. For me this is always the beginning of the end ;-) reply throw310324 18 hours agorootparentWell, I've been using (surfing?) the web since the late 90s and sure, some things have become more commercial, there are new walled-gardens, some ads are annoying, cookie banners annoying but really you can do everything you could do back then an 100x more. Would I want to go back to the so-called golden, idealized age of the web in the late 90s/early 2000s? No. So, what I'm saying is that it hasn't deteriorated. The only thing that's happening is that HN is an echo chamber. reply andsoitis 20 hours agoprev [–] > It has been running for more than two years now and its official instance currently serves more than 800 active users and hundreds of anonymous visitors every day. We achieved this thanks to dozens of contributors and financial sponsors who I am grateful for. Failing to find product/market fit is a good time to reflect upon one's assumptions, biases, and perspective. Another piece of unsolicited constructive feedback I would offer the author is that from where I sit, I think they would be more successful if they are motivated and inspired by a more positive and strategic outlook. reply monkey_monkey 18 hours agoparent [–] Unsolicited, generic, drive-by advice, given with almost no understanding of the context and people involved is generally worthless. reply idontknowifican 17 hours agorootparent [–] i love how he hides from being an asshole via calling his behavior “constructive ideas”. imagine being a child raised with this cognitive dissonance. “if i do not think i am mean i can not be mean” is fundamentally the go to for bullies, this person included reply refulgentis 16 hours agorootparent [–] I suggest relaxing, lest your comment become funny, because it describes itself better than the mind it is trying to read. Want an \"asshole\" version? i.e. the honest version? (note, I'm not the guy you're diagnosing and mind-reading, just want to highlight how absurd your conception of OP is) That was one of the strangest deprecation announcements I've ever seen, foaming at the mouth, blaming massive external factors (companies...pursuing money? is an odd thing to be surprised by), way overly dramatic lies about ex. Google \"content-blocking extensions with MV3 under false security claims and planning to lock down the OS and browser with DRM.\" I have no love for Google, but as soon as someone starts saying \"MV3 [makes] false security claims\", I remember MV3 is just Safari's more private content blocking from years ago. When I see \"planning to lock down the OS and browser with DRM.\", I'm like \"wait...he can't be describing...\", click through, and see he's describing the one-off public announcement of beginning a prototype of a browser API that a user is human (transparent!), that was cancelled, publicly, extremely shortly after, because of reactions like this. (thankfully!) Much like the constructive version of this comment, I bet they'd be able to keep going if they didn't see their project in such epic terms, given they framed giving up in terms of companies continuing to pursue money and Google. reply idontknowifican 16 hours agorootparent [–] your points are all fair and valid, and i can see how mine was missed. i am attempting to say: the way you give feedback is highly correlated with it landing. your “mean” feedback is not, it speaks to why and does not just dismiss the author or try to trivialize a number. it provides clear links to issues, not just attacking the intelligence of the poster. you make a series of great points, reinforced by others feelings and examples of real world issues. this feedback can be turned into something useful, allows reflection, and does not attack the authors entire reason to be here. the original dude literally says: 1. 800 users is shit 2. your app had no effect 3. you dont even know how to choose a goal you must be able to see this difference? is asshole a diagnosis? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The letsblock.it project, aiding users in eliminating low-quality website content, will close its official instance by June 2024, with the creator focusing on non-commercial web projects and declining database transfer.",
      "Users can migrate their data before the shutdown, with only limited maintenance until then and no upcoming updates planned.",
      "The creator is willing to assist a user group interested in forking the project, setting up a new server, while expressing appreciation to contributors and users, and transitioning to new endeavors, suggesting following updates on the Fediverse."
    ],
    "commentSummary": [
      "The letsblock.it project shutting down sparks a debate on respecting users in software development.",
      "Users discuss the relevance of 3rd-party Mastodon or Matrix clients and the support for existing FOSS projects.",
      "Emphasis is placed on the challenges of open-source project maintainers, effectiveness of marketing emails for sales, and the preference for non-commercial tools enhancing the browsing experience."
    ],
    "points": 304,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1711885269
  },
  {
    "id": 39889286,
    "title": "Uncovering XZ Backdoor: Security Challenges in Web Development",
    "originLink": "https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .big-button,body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .big-button,body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .big-button,body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"rheaeve.substack.com\",cType: 'non-interactive',cNounce: '93825',cRay: '86d7bf0f2c12f98f',cHash: 'b5dfdfe187cf09a',cUPMDTk: \"\\/p\\/xz-backdoor-times-damned-times-and?__cf_chl_tk=FPo7tLyyYRGHELDZg2KjVq9Mw5v.3IucH11v19juFYQ-1711965775-0.0.1.1-1386\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '105000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/p\\/xz-backdoor-times-damned-times-and?__cf_chl_f_tk=FPo7tLyyYRGHELDZg2KjVq9Mw5v.3IucH11v19juFYQ-1711965775-0.0.1.1-1386\",md: \"bc4M3VVUhUdBb8gZUyEiot4WUUnCE58STqAaHKPv9r8-1711965775-1.1.1.1-6yNZGJ1DaY2mRaC5dC0b.BXRu4rxtqWKXHmYveC54WxLsoWYQltvrVIE774gXQBNrxaJuXH3fvPCJz5y00nun9gx07ZNYcDadsTjoaNTr_i0rXiISzmfsqfipZUmlmzgr3JrRcNfCwcaeSOCwHoNGQNWQYEJad_X.nhCjL2Fg4sKI9R0.Q1uoHEC6rpNzMuV9oqdLqs0137S2tvnfpn34.XvUUymyldkT.UYHbB_q.X70seO28xjORK9q29QqrmkcHxKTSpxnB6sh0Om2PEgw9S.WueepJPesy4TMChu1I873pJK1nvel8HjrznpyrLKYz6gwuqAsJEXaUlN_eZuvlbJ1SH.w88U0PbQb7r0Nz.OKeUinUPhLrhl7SaXhiyu59fLs70VN4r4cgUn7qGQxn9dl4SRCVw780Eu9Jv2X.SG3_dEzFQ7yIEOVQ62ofuP38UkfEf2FaWGYrjxJLT2venfR4XVb4KvB651_lhDhuJo.ikCNWQLtGHeAM6KpQ9dM_wt5vNbVp_PbgKV0J2jN.zPpt3xG2iPg7D0kGPCNAxS7Hok4y_m1UICBQs53Xwn13Ooy3Mp.JMla0kU.pwt6qO60jsh.V0C9eC0WEGzh5PXCO4ljWaJyXxLo4mTxaWZqO7mNlvXngk9Og2pRPwHUY5ycPY.8bXBoWUCzmQlL3sfABqqgm_HyHy_v58hqHITo2X6cRtiyINkyQ6em.FLWpEl1nsc5e_L9NUSnyvd.5fJQLbytaRCx6KfRbW9ko1AWVS1P4DaRY8AcaBxN22spnHdTrn2K_T4LeNmECsz9Yu8SRrNZtR6mCUgmazWK6fZbtEkO0.2Y60nV_SKoNXZl8quGiC7WYnhPi2EGU_K04c._llmF8nLPJFIrW3I7URYXltpzl2X5fOOEq9Z_2Fh77Fh9ovX8YvyqEgK0g.poJW_CorF7L4.WmWBuKkLcHBw8malhSPZSJrlVoWLxnjpQDZ.WSh8rDqazwGj6HccwmOmLysug1etBjz9agkqy4gqDslkXVVW_6OjhiJviSQ.iKb_VUGFyBeFiTjEeUmcoi9lOMe_osaQUWIm1Ub6JmONsVOgNrv2frGPE8w6GLyvT6b1OMg33qR1PgZtyK1F7jsp1YEbsgA9TIvw7ICZtJpLavIk4XZHnFCkbhDY.0S_5jZpMF3CGoi3_PoN815CKfaudr3uWaRAtFAzqIXFQynFkNurwA5y9fGiyQJgooot1VUqAE6_l4V0PGLbIJVWpvkAw3zBn5icPTRPYu5Rs4qsJX27CfXsg4zZF6pDr1REzfK32P4_PyKOUNGH.lEmJPo\",mdrd: \"SqnnyXVY6HxN_6GOsfivLZV_AAqLBviABRvEHdTIKz4-1711965775-1.1.1.1-NmexNKlrmemwIEKjHWdJAAkqgjK.LGUKTZK_6SBxcqLM_DAf8HpIFyCrdvVxu76cIHIbrJERdmuiat7XDhPo0MSwYk6y0crnyQ2v0ZDU_o.GPpFwGf0p94uArg9nERf.fJo50hYTm64quVns4deXRJuuZH45gkENbrTpO.SV2p_N2FvukEQaaJ1r4zZDyl92RfQK7ULUKQ3V_clEAEWPwMdBnYAnV.rCJs8NAIRHJ8aH2sLPl.b.Ead75AukkTJr3Q3R.yahgQVMW3GsrkO0vX8soe.lg3dDuGjAp2k6pwH.o5BX4atTsHRzPQwk5i7y8XiWMZjlj2SdMxm3kJb99M03SVjZroLcfUErcB_w.qsLIOMafkgGqgyaxIpaY0c.HAalanvo4WziNLBr.snilR77UCgm24bgIAghGtyhGvLbYybN61sfeM01EKt92BmHPpYaBWMVeHTVWq._J40eo09R2J_phjvbq0PrNKQLkp7NYmLei7OPBYtlaJeOTCN0Hvokj4djNQV55NDhyGW9G69nbkHU7UWH_bGcEeX6IsBmp7KwqbNDaVR2KBmk5buaO7QQPj_I_CWcznclWer67MKjVS3hQYtldBWmGr.LF9MqUP14DFR8Prd3l8zmIPD5KcCgIEUktGd64PisMBccWzHM_BdHx_WcAYZmEwFZXNsaf.3pqpSkjUwG9MPwnQE.Trbe3h5LH0oy0oGslcmuQwQ3xNoL746L5j3ADOPVsiviJoKUstosw4JJ_DUKvf8eDirAWfDDBNr6_Ux.8VD84Uqh0yKDMdtT8v8cAHX3JU8auLGJth7FGu0qiCfWAPx52ZM1aUBdfEoxHl9I2xZS0ilc5kFHezP7lYey5fI1wpxEPh8X0h_xmCJwDB1qiR4herwnF0n087bzzj_.vooP7c5.Pme520PbmChznrS8pcbnOBOXZ0wdkPKvlBWna5ZHAtxT7E4okNDBHQywv.66wzSx_tjCYufXk6WXEa3zs63Afgyx1GMtzGQjhY8Kki3GNvfGyqx_besdLVpLeFRzVJ1KeeZ_LBD56rEgGJmER4NbrbJsJBur3esmOFl1Bo38pTc3uBnWC28tL5swmEl7XgbCWwbf0MvcfkEwT3th_UU8WhXMDTRH2N_0s4kZr2aFn6D5haOkKvOgVa.QQaO052EUYzBMs_hQuTG56gQ.ypQVUGUWV_1vC7hAkk3VeBQfOkV6hyj9jhgDXVd8ZoI79B1msLxfvMVI9SzZ3uz9RGsbhyAM_YnSJyhwqXNPnio_AOAcqgZ9Wh2L6H9hvT4nEuvVcEpO2HXas2gDKrzZQVIaXJ5J.BiAs.MFfes8WDY5WwU4oWqWDQczsCkEvlhYj25azX9_NwcsYiwev_2QjM4M48UtqpwEU9fzA8QonqDSF2KQ1R192MTjnYNGOoe0zobL0xNyf5NUiEJ1pRfZFWPGS05EfEiby4UPl1E0sTlnGTA3mIT1uWpBVMq9B.1urU0_.ffBr0vkNbg25UCMAWmlpBruNJgalREJY_64Z.B6GfhSGFOioYzn6oIFQiomrSq.cbcjjC2fbDEciSHtDnmEdPwgBHGBmxG6vrYmDtxQIIcJzfgCb.s25XKxn0QdpTbVAnWQptMe1bBzz62ZIZvdYQA_IKvhGkN8CidRFWAh3vbtBCu7nngnZo166i_JocqjaYTABBnTQuyfFhURATpoy1_JZJRdsNrGMOMffWs2MmGWYESq1gTzAruC6uLt8TqZK7GEY8PhAXIn_RyAWr8iXsrNAOVk9nVvabucJ9FOOSTZV611cHew3VuYvBrAto4xymBbbBC70Zti6jmsYrTE9snWcO75dRRAB4zGNWzY23qBpL7QW_Ox0OnXxRMNV0oZIGTP15w9Iz8nPbGXC62BxlXfm6MOOUL.lGgLo1Sr3qtKdtUEBx7Ar1CqNX7EY2LHRg5CJlkGFhKaH8KrZS4WFUua0NeB8t36cqOWQ6L5rGM6yLf_ZReG0gQXVJqDNjcljA8558xhnoDfE.uSFn_zX.zQgtl8hLQ1U6ixlRxAJ3pqi7HaBVkORmskZ_OAXOXTrSPBxW2pmpWUO77BOYh9QLsXiJ8lMgzXJHt9OzBFDXSYS6_1ufwsiZOso2KXCAXjDfOVC_rDQ8EYMr9F.k5.Gn7WLtkv9Lxoq8Rhr.K8j5kVDGqmF29jsQCfFsFSzhpdUz9.E_stjOOrNzYs9_XkKGJqZ4mpGWwfex.zyDCrKujOgEOjUgBPtjH.Tyl6AOLFNtZIM2Iy_DANyS0hOOaYzp8g8M1CLuY3whMHnpF3Jx0PamlY.__53Kw04b.VQQ\",cRq: {ru: 'aHR0cHM6Ly9yaGVhZXZlLnN1YnN0YWNrLmNvbS9wL3h6LWJhY2tkb29yLXRpbWVzLWRhbW5lZC10aW1lcy1hbmQ=',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'PVrE8qpWegTUckeEW+6XGqlr4MhPzweqvqtSddzBUuadxaMBK0X1atX+JYcVuOJc5tIPwPx+vnqb44zKDMkqqmBZgq2PPLlKec83gtAp2dF7/oYxY3ytUeTB29z4/Vlcy0v9GXrgOUvlkF3C+L0ycrcYD9aUTgGZpU32s7bi1zS7z/UotuvO6ILymRlTA0IXvqCz42OAqyhP3ccV+Crkdm9JWSBR/N/2XHavWevrp6loFRTtP/jy0lHecJUwCTGJG9fzbDv74xiLYEf7WA7S0dtCe5oDBJpnKbb1T+JQhqUE+X5XaPs6sJHzvgnHeNgmNQEjSne/Dd0GnpmeXKVTbHBWojyhIvuVp0XD5eU+CCHAhH8uuFKAd0fEjsaQxDfAZM0AEmr0mGD2Vuh1+eRx4Fi3kRWCXt2PvxTUvTwhqIYTIROJXxa6aqL3rvZ3pgLUMQth1tcmSA2aYo/Tb9yv1NMCWXdqTlGM2fA/MeHsCBftqjQAQ4S9f/1mMiBBSkab/b4MZrtHaek/VbYruYC1lVq+b2GAQP/IGXbkAcFtiFviS4s6FF8Vekqd7vT/QMKpz/vc9sbCvmua0Z7WD0w4JjK+Gx/t9wf5R/iPx6jksiU=',t: 'MTcxMTk2NTc3NS4yMjgwMDA=',cT: Math.floor(Date.now() / 1000),m: '+jexi7dypP7E8c6oERY8rr7EFrHPLb6Nt7SRogoQikY=',i1: '8EuLsRFoSFz5fIEPQAEmqQ==',i2: 'BiFgz/dQcRtkcEO4ioyq4w==',zh: 'o01jypKJQ++/gkxUTvC40nYpXBhuMc66cm0hd/Tc920=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'RhrrD7qBFt5FHTo8ynKQEQpMO6QuPfDbmMNoivCdO5k=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=86d7bf0f2c12f98f';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/xz-backdoor-times-damned-times-and?__cf_chl_rt_tk=FPo7tLyyYRGHELDZg2KjVq9Mw5v.3IucH11v19juFYQ-1711965775-0.0.1.1-1386\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=39889286",
    "commentBody": "XZ Backdoor: Times, damned times, and scams (rheaeve.substack.com)252 points by lamontcg 10 hours agohidepastfavorite153 comments ufmace 8 hours agoProbably worth considering here - if this was a state-sponsored attack, it's entirely possible (and exactly what I would do if I was running such a program) that there is one person or department actually writing the code and list messages etc, and another person or team who sits between that department and the internet and whose entire job is to ensure that everything that comes out from the development team takes place at actual times and with timestamps that are consistent with whatever story they're using for the project. Possibly even once in a while intentionally inserting a \"mistake\" to hint to investigators that the person's real location is another chosen place, which is also different from the real place where the project is being run. reply omoikane 3 hours agoparentIt's likely that there is a whole team behind this operation, and part of that team might include tasks such as \"socially engineer XZ project to accept a maintainer of our choice.\" This was suggested in a different post[1], where there seem to be newly created accounts that came out of nowhere to create a sense of urgency, which might have accelerated the maintainer role transition. [1] https://connortumbleson.com/2024/03/31/watching-xz-unfold-fr... via: https://news.ycombinator.com/item?id=39888854 Search for \"Progress will not happen until there is new maintainer\" for the relevant text. reply bombcar 7 hours agoparentprevExactly. Once you assume that it’s more than “a lone wolf/single guy” many more things become possible. Including carefully curating things to appear in a location and to NOT appear to be more than one person. reply apantel 6 hours agoparentprevThis is the real point. reply mik1998 9 hours agoprevI don't think he was from Eastern Europe, but if you want to look at UTC+0200/+0300, in Europe this only includes Finland, Baltics, Ukraine, Romania, Moldavia, and Greece. But notably if you look a bit down it also includes a good chunk of the Middle East, including Israel. reply lrasinen 2 hours agoparentNoticed a 4-week empty block in August. This lines up with European holiday schedules. Less common in Finland though, we prefer July. reply bradleyjg 8 hours agoparentprevWhat about the daylight savings time and holidays points? How do they line up for Israel? reply mik1998 8 hours agorootparentI don't know about holidays but Israel uses the same daylight savings as the mentioned Eastern Europe countries, ie switching from UTC+0200 to +0300 at around the same time - only difference is that they start daylight savings on Friday instead of Sunday the same week. reply hnthrowaway0328 6 hours agorootparentprevIsraelis usually are off on Fridays so it doesn't match the git commit records. reply AnonHP 5 hours agorootparentIsraelis are usually off from the evening of Friday (sundown) till the evening of Saturday (sundown) for Shabbat. Where did you get the idea that they’re off on Fridays during daytime? In this context, middle eastern countries with Islam as the majority religion have the day off on Fridays. reply hnthrowaway0328 3 hours agorootparentI used to work for two Israeli companies and they were always off on Friday and Saturday. They start the week from Sunday. reply mbrevda1 5 hours agorootparentprevin Israel, Friday is generally not a workday (or \"business day\"), a la Sunday in the western world reply ufo 8 hours agorootparentprevI don't know all the holidays but they have commits on Yom Kippur dates in 2021 and 2022. For 2023 it's harder to say because their commit activity is sparser. reply jamebond 6 hours agoparentprevTukaani is a Finnish organisation primarily contributed to by Finnish people. You look at the contributors and almost all of them have Finnish names. Seems to check out to me. reply chgs 8 hours agoparentprevNot sure why you’re downvoted. When you think of state actors, Israel comes very high (stuxnet etc), as well as the usual US/Russia/China/NK groups. Unit 8200 in the IDF especially have a very notable reputation. That’s not to say other counties don’t have capabilities (and this doesn’t look like you need the resources of a group like say the NSA or GCHQ for this particular attack - indeed it could just be a single lone wolf) but it’s noteworthy. reply krick 8 hours agorootparentIf the only think I knew was UTC+2 (and I don't even think I know that, I'm really cautious with assumptions about what mistakes that kind of people plausibly make), Israel would be very high on the list. But FWIW, 25Dec is a Catholic holiday, and even though it's mentioned on Wikipedia page about Israel holidays, I'm not sure how official that is. I mean, it doesn't seems to be, like, a real Israeli holiday. reply asveikau 8 hours agorootparentI think no commits on December 25 is not enough to go by. He's been active for about 2 years, so that's what, 1-2 Christmases? I assume he doesn't commit literally every day. So it could be a coincidence that he didn't on those particular 1-2 days. Also, in many Orthodox Christian majority countries including those in UTC+2 they don't celebrate on 12/25. But doesn't Israel also not follow a typical work week? Eg. No commits on Friday afternoon? reply krick 8 hours agorootparentYes, that's what I mean as well: I don't really think this analysis is any near to be considered any conclusive. It's interesting. But this is it. From what I'm seeing, there really isn't very much data. The article makes it feel like there is this huge archive of commits, that draws a picture of a guy working almost every day on these projects with some very much visible gaps in time, but there really isn't. The data is quite scarce. I would need to actually do the analysis myself to decide what I'm willing to believe, and I don't think I want to do this right now. But for now, I don't think we really have anything. …What I would really want is some input from Microsoft. I don't know what they can reveal, but I believe they must have quite a bit more data, than us. Even speaking about times, well, he must have been using Github more than to just commit stuff. And I'd imagine they have some logs. Also, it's very fair to assume than he was using VPN all the time, but it's also fair to assume that he wouldn't accidentally slip like that, revealing his real time zone in 9 commits. So, yeah, for sure there are people who have waay more data than me, and probably know how to use it properly better than me too. Not sure they will be willing to share, though. reply asveikau 7 hours agorootparentSomebody on another HN thread pointed out he used Gmail, so google has something too. I think I saw somewhere a claim that somebody confirmed he used a VPN, I'm can't recall how they did that. reply cozzyd 7 hours agorootparentAnyone try emailing with a tracking image? reply jcranmer 6 hours agorootparentNo need to bother with a tracking image. Just scour the message headers of any message sent in the past--there's decent chance there's a few more dates lurking in those headers, and even an IP address or two. reply dustyharddrive 6 hours agorootparentHeaders have the IP address of their mail service. reply asveikau 6 hours agorootparentYears ago they would include the IP of the end user. Gmail and hotmail stopped including that more than a decade ago. But they both used to. I think Gmail stopped this practice years ahead of Hotmail. I have a relative with mental health problems who lives on the street, and he sometimes writes me from public libraries -- kind of luckily for family members who are concerned about him, the webmail service he uses still includes that info in headers. So I usually know which public library he writes from. reply cozzyd 5 hours agorootparentWhen I was a PhD student ips in header were a great way to see if professors I was working with were traveling or not... reply asveikau 7 hours agorootparentprevI'm sure this guy is not checking his email after being exposed. reply juliusdavies 7 hours agorootparentprevThey refused to divulge how they knew reply est 7 hours agorootparentpreveven a North Korean hacker would be plausible. reply chgs 1 hour agorootparentNK are one if the most prolific state actor countries. They aren't UTC+2 though. Greece and Finland aren’t prolific. I’m sure they have the resources, many lone people would have. I don’t think they have the motives though. The US, Russia, China, NK and Israel have the motive, opportunity and track record. I wouldn’t rule out Ukraine either (and Ukraine is GMT+2) I still think that an individual is the most likely candidate though. reply lenerdenator 7 hours agoparentprev“Trust no one! The minute God crapped out the third caveman, a conspiracy was hatched against one of them! ” - Gen. Hunter Gathers, OSI (The Venture Bros.) There are parties who have effectively endless resources and motivation to mock up a false-flag event to steer the responsibility for this a certain way. They're all very, very good at covering their tracks, and even the most experienced security researchers will take months or years to just get an educated guess of who could be responsible. Trying to figure out who did this is a waste of time. The lesson is this: 1. Never, ever install bleeding-edge software in production, for any reason. 2. Pentesters in your organization should be regularly trying to blow holes in your stack and let both you and package maintainers know the result. If they're not, they're not doing their jobs. 3. FOSS maintainers should audit the new code in each release for security issues, particularly for things that are obviously security-sensitive. 4. Donate to your FOSS maintainers; they do insanely important work. These rules will pay off no matter who or what wants to break into systems. This fortunately didn't get to stable. This was as close of a shave as you can possibly get without a security Chernobyl in your systems. EDIT: Downvote all you want; I'm right. There are people who very much want to convince you and everyone you know that a certain party is responsible for this and every other attack you read about. They'll go out of their way to do that. There are geopolitical goals to be achieved by doing so. We're currently talking about banning foreign ownership of an incredibly popular web app in the US right now. If you are a government or commercial entity that would benefit from such a law, how convenient would it be if there happened to be a GitHub user with a Chinese-sounding name who committed an attack vector to the project with a timestamp that looked like it could have come out of the PRC? And let's say it is someone in mainland China. We find out they are beyond a shadow of a doubt and who they are on a personal level. Some big-time DA's office like the Southern District of New York puts out an indictment for them and requests extradition. The people who are behind this are almost certainly state-sponsored and there's no way their asses are being handed over to US Marshals, ever. You could even argue that if they managed to stumble into a situation where they could be extradited, the government responsible for backing them would \"tie up the loose end\" before letting that happen, lest greater knowledge of what the organization has been doing fall into enemy hands. Besides the Bond-style intrigue, there's no practical application to the knowledge. You already know where your users are coming from, most of the time, and might be geo-blocking based on that alone. If not, you know you should be alert to other threats from that region. If you're not, you aren't doing your job. reply threatofrain 7 hours agorootparent> Never, ever install bleeding-edge software in production, for any reason. But for any X which is mainstream today someone was always the first of X. And even then being the second or third is still cutting edge. For a certain kind of company it makes sense to play this way, but if it were everyone then we'd have a tragedy. reply 4bpp 7 hours agoprevGMT+8, the name (Mandarin/mixed-dialect first name + Hokkien surname) and most importantly the circumstance that the user was seen connecting from a likely VPN exit or hosted server in Singapore [1] are all consistent with a Singaporean identity, whether real or cover. For that reason, the opinion of [1]'s source that the name sounds fake should also be taken with some amount of salt (Mainland Chinese are not always well-informed about Diaspora Chinese communities). Worth noting that many of the holidays mentioned, likewise, are not public holidays in SG [2] - Jan 24th was, but Jan 22nd (listed as working) was a Sunday, inconsistent with a \"working on regular working days\" pattern most anywhere. Would be interesting to see a greater corpus of Jia Tan's writings, especially older ones. There are fairly distinctive idiosyncrasies that may be used to distinguish Singaporeans, Mainland Chinese and speakers of various Slavic languages (and Anglo natives). [1] https://boehs.org/node/everything-i-know-about-the-xz-backdo... [2] https://www.mom.gov.sg/employment-practices/public-holidays reply qsi 6 hours agoparentIn the few commits from Jia Tan that I've seen, the English parses (to me at least) idiomatically as a native speaker from either the US or UK, and I did not encounter any signs of Singaporean English. Obviously, this is a very small sample so one cannot read too much into it. Almost 10% of Singaporeans share the family name Tan. [1] [1] https://en.wikipedia.org/wiki/File:Singaporean_surnames_by_f... reply delta_p_delta_x 3 hours agorootparent> the English parses (to me at least) idiomatically as a native speaker from either the US or UK, and I did not encounter any signs of Singaporean English Singapore is an Anglophone country. Many Singaporeans are perfectly capable of writing (idiomatic) BrE or NAmE, given the medium of instruction in Singaporean education from kindergarten to university is English. Example: me. reply qsi 3 hours agorootparentThat is certainly true, I did not mean to imply this is not the case. I have noticed however that in the Singaporean English acrolect certain words and phrases are used differently than they would be in BrE or NAmE (\"I'll send him to the airport.\") In less formal registers the differences become larger, which is what I was looking for. reply gfaure 6 hours agoparentprevIt's also worth noting that Tan is a Mandarin Chinese surname too (譚) in addition to being a distinct Hokkien surname (the much more common 陳). reply BluSyn 9 hours agoprevFor years I've had `gc` in my terminal mapped to `TZ=UTC0 git commit` for exactly this reason. No need to change system times or git settings. (though perhaps a better way in global config?) It's not even for nefarious reasons. I travel a lot, and don't like leaking my travel itinerary on public repos. reply SmartHypercube 7 hours agoparentI have been using a small tool I made to hide the time information in my git commits: https://github.com/SmartHypercube/git-date-truncate It not only sets the timezone to UTC, but also sets the time to 00:00. So my commits will only have date information. I think only saving the date permanently in the git commit is good enough. No need to leak the precise time I made commits. reply nerdponx 8 hours agoparentprevHow about this? alias git='TZ=UTC0 git' reply yanateras 7 hours agorootparentEven without the zero! alias git='TZ=UTC git' reply sneak 7 hours agoparentprevI travel a lot but never change my workstation’s time zone out of PST/PDT these days. That would be a manual step and I don’t use Location Services. reply kelnos 3 hours agorootparentSame. My phone automatically changes, and I'll manually change my watch, but when I travel, I leave my laptop in my home time zone. Part of it is I just can't be bothered, and part of it is I want an easy way to know what time it is at home without having to fiddle with a world clock. reply akira2501 7 hours agorootparentprevTime zone is purely a displayed time consideration. I'm not bothered that the display time on my computer is different from the time zone I'm actually in. reply throwawayyy9237 9 hours agoprevSetting aside the gravity of the situation, I'm 100% gripped by the \"mystery novel\" side of this. Seeing the internet piecing together the Who? the How? an the Why? is fascinating. reply optimalsolver 9 hours agoprevPoor Jia. Two whole years of work down the drain. If you're reading this Jia, remember that you miss 100% of the shots you don't take. Chin up, brother. reply sva_ 9 hours agoparentWho knows how many alter egos the individual behind that online personality has, and what other projects they poisoned through those. reply rudedogg 9 hours agorootparentYeah, with a 2-3 year wait to know whether you succeeded or not, I doubt you'd put all your eggs in one basket. reply beanjuiceII 6 hours agoparentprevHow is it down the drain? You know he didn't achieve his goals? reply EasyMark 5 hours agorootparentyep, maybe it was a false flag to take people's eye off the -other- backdoor lol reply tiahura 8 hours agoparentprevDon't worry, they (it's a team) also contribute to image libraries, webkit, Kubernetes, ... reply krick 8 hours agorootparentYes, unironically, that, being a huge leak, really convinces me that we all have multiple RCEs on each of our systems. I mean, we already kinda knew that, of course, but even now that I'm typing this it's a bit hard for me to actually believe it. But realistically I think it's pretty much a proven fact. This level of sophistication… what did I even expect? Of course well-payed teams of high-level professionals are working in this area for a very long time by now. This is basically like professional football players competing against office workers in their late thirties who like to kick a ball on weekends. I am comforting myself by thinking that something a bit more high-level, written in python or even golang, or, oh hell, maybe even rust would be more a bit more transparent for other contributors… but even this is probably a lie. And if it wouldn't be, surely tons of super-popular low-level packages written in C/C++ are as good as incomprehensible for somebody who isn't specifically security-auditing this. reply 1letterunixname 6 hours agorootparentI never understood and continue to be baffled by the naïveté of FOSS people giving commit access to core projects to unknown parties. Assume they're a state actor or a crazy person, until proven otherwise. reply fbdab103 6 hours agorootparentThat effectively means you cannot share commit rights with anybody. How am I supposed to vet someone as not a state actor? Any plausible test could be thwarted by a motivated individual. reply 1letterunixname 3 hours agorootparentSynchronous socialization and meeting their friends and fam. Not knowing people well is a vulnerability. reply someotherperson 3 hours agorootparentA state actor is potentially the only person financially supported enough and willing to do this. \"You're in Zurich this weekend? Oh wow, me too. I'm holidaying with my wife. Let's grab lunch\" as they proceed to prepare their military expense form for two business class tickets for them and their coworker reply est 7 hours agoparentprev> remember that you miss 100% of the shots you don't take But I am sure JiaT didn't only work on the xz project? reply lapcat 9 hours agoprev> who regularly works in the early morning? Me > For a hacker, it is much more plausible to work in the afternoon and late at night c/hacker/younger person reply jtuple 7 hours agoparentThe \"hacker\" stereotype is certainly stale, but subbing \"young person\" (and implying not \"old person\") is equally silly. I'm 41 and have been awake before 7am maybe 20 times in the past 10 years. Half the reason I still put up with computers is because it's one of the few professions where I can work a 11-7pm schedule most days and still excel. There are morning people and night people. You are what you are, no judgements, but it isn't something you grow in/out of, nor should be expected to. reply lapcat 7 hours agorootparent> There are morning people and night people. You are what you are, no judgements, but it isn't something you grow in/out of, nor should be expected to. Oh, my sweet summer child. Talk to me again in another decade or so. reply naruhodo 6 hours agorootparentI'm in my mid fifties. I rarely start work before 2pm. My colleagues regularly receive emails from me sent after 1am. Are you saying I will grow out of this and become a lark? Or are you saying I will eventually conform, oh wise one? reply lapcat 6 hours agorootparentI'm saying that changing sleep patterns and waking up earlier is a very common effect of aging. https://www.sleepfoundation.org/aging-and-sleep/why-do-older... YMMV but the idea that you can't grow in/out of sleep patterns is empirically false, and I know this from personal experience. For most of my life, I didn't need glasses for reading... until I suddenly did. Bodies change. reply naruhodo 5 hours agorootparentI knew most of the stuff in that article. If I wake up at 5am on full alert, mind racing, I read for a couple of hours and then go back to sleep when it's possible. I know that I can't function well on less than 7 hours, so I don't try. The article seems to be taking about people on the verge of entering \"the facility\", on their slow slide into dementia, who are making do despite not getting enough sleep. I'll get back to you when I hit 70. reply lapcat 9 minutes agorootparent> The article seems to be taking about people on the verge of entering \"the facility\", on their slow slide into dementia I don't know how in the world you got that from the article, but no. reply lucb1e 8 hours agoparentprev> c/hacker/younger person What is this syntax? I know s/a/b/ for sed substitution but not such a thing starting with c reply someplaceguy 7 hours agorootparent> What is this syntax? I know s/a/b/ for sed substitution but not such a thing starting with c It's the syntax for sed substitution for someone who mistypes, is dyslexic or does not have an excellent memory. reply evilDagmar 5 hours agoparentprevNo, stereotypes aside, hackers who code diligently tend to wrap their \"actual work\" schedule around their external obligations, and while the morning is generally filled with distractions like dealing with \"morning people\", after lunch (an on into the evening) there are generally far fewer interruptions. I can honestly get much more done with an uninterrupted four hours than I can in the entire eight office hours of random phone calls and emails coming in, and I am no longer a \"young person\", and I'll do some stuff at night just because I know it'll take half the time if I'm not interrupted and I've had a few hours to at least intermittently mull it over. If you're having peaceful mornings, I envy you. reply lapcat 0 minutes agorootparent> stereotypes aside, hackers If we're putting stereotypes aside, wouldn't we also put aside the word \"hackers\"? In the xz case, the term may be accurate, but we don't know the identity of the culprint(s) and have no empirical basis for even speculating about the work schedules of such people. > If you're having peaceful mornings, I envy you. Well, I don't work in an office. I work alone. Like a stereotypical \"hacker\". reply qiqitori 8 hours agoprev> Notably, on Tuesday, Jun 27, 2023, we see two commits: 23:38:32 +0800 and 17:27:09 +0300. If we do the math, there is about a 9-hour difference between the two commits. Isn't 17:27:09 +0300 == 22:27:09 +0800, making the commits just about one hour apart? reply someplaceguy 8 hours agoparent> Isn't 17:27:09 +0300 == 22:27:09 +0800, making the commits just about one hour apart? Yes, and even worse: > Even more damning, on 6 Oct 2022, we see the following: one commit at 21:53:09 +0300 followed by another at 17:00:38 +0800. This is only a difference in a matter of minutes! No, this is a difference of almost 10 hours... Maybe the author inadvertently switched the two analyses? Or maybe the author is just not very good with timezone math... :) reply sihe 6 hours agorootparentAuthor here. Thanks for pointing this out – we switched the examples. (although we also are very bad with timezone math at 2 am.) reply someplaceguy 6 hours agorootparentNo problem, all of you are welcome! Very enjoyable article. reply broknbottle 8 hours agoprevWhy not include the timestamps for their replies to the mailing list? https://www.mail-archive.com/xz-devel@tukaani.org/ reply cozzyd 7 hours agoparentDo you understand the time format there? It lists things as -700 or -800 for me, is that because of my local time zone (Chicago) vs. the mailing list (Finland?). reply Delk 1 hour agorootparentI think mail-archive.com just shows time stamps in UTC-0700 or UTC-0800 regardless of where you are. That's how time stamps appear on all of the few mailing lists I checked out there, regardless of the sender of the message. reply someotherperson 1 hour agoprevMaybe a consideration: this isn't necessarily a nation state. It could literally just be one or more individuals trying to set up some sort of crypto heist. At this point we need some sort of new adage to the effect of \"never attribute to nation states what can be attributed to crypto\" reply farmdve 28 minutes agoparentTo me this is false. They usually like quick profits. Look no further than Ethereum and defi chains like Optimism,Polygon etc. It is so easy to write a malicious contract that even I with more than 12 years in the cryptocurrency space got had and lost 1k. reply tete 8 hours agoprevWhat if it is an American (random other place) pretending to be an Eastern European pretending to be Chinese? ;) If I would be pretending something I'd certainly put at least one more indirection in. reply rvba 8 hours agoparentOr a Chinese pretending to be an eastern european or israeli pretending to be a chinese? In such setup nothing bad happens in such a leak. reply widdma 9 hours agoprev> given that Australia does daylight savings time DST varies by state in Australia. Western Australia (UTC+8) does not observe DST. reply fbdab103 5 hours agoprevI wonder what the \"Lessons Learned\" look like from the other side? Are they whiteboarding the best strategy for distributing the timestamps of commits? Mandatory performance evaluations on all RCEs? Distribute the poisoned commits amongst more authors? reply farmdve 27 minutes agoparentThey will likely now optimize the code to not trigger high latency and also ensure no Valgrind will find any issues. reply crem 1 hour agoprevHere is the a distribution of commit times and days of week (hopefully I didn't mess up time zones and plot everything in UTC): https://imgur.com/a/1js5T8L It has surprisingly many commits on Sundays. reply idlephysicist 7 hours agoprevSomething feels weird about the timezone arithmetic here, though maybe I am nuts. Take this for example: > Notably, on Tuesday, Jun 27, 2023, we see two commits: 23:38:32 +0800 and 17:27:09 +0300. If we do the math, there is about a 9-hour difference between the two commits. Putting it all into UTC we get: 23:38 - 8 = 15:38 17:27 - 3 = 14:27 That's not 9 hours by my numbers. Or another example: > Even more damning, on 6 Oct 2022, we see the following: one commit at 21:53:09 +0300 followed by another at 17:00:38 +0800. This is only a difference in a matter of minutes! Putting it all into UTC we get: 21:53 - 3 = 18:53 17:00 - 8 = 09:00 I think these timezones were swapped (accidentally) by the article's author. If we do the calculation again: 21:53 - 8 = 13:53 17:00 - 3 = 14:00 That _is_ a matter of minutes. reply sihe 6 hours agoparentYes, thanks for pointing this out! As somebody else already speculated, we got the two examples swapped. Already updated the post. reply dc-programmer 8 hours agoprevEastern Orthodox Christmas is not on Dec 25 so the argument for EE is not as air tight as the author thinks it is reply cozzyd 7 hours agoparentDepends on the country. Greece, Bulgaria, Romania and now Ukraine celebrate on Dec 25. reply sihe 6 hours agoprevHi all! One of the authors of the original piece here. Thanks for all your comments – we picked up a lot of cool ideas for future analyses. Just to address four common objections: - We agree it's unclear whether this was one person, or several ones. But even if it was a team, the timezone idea would still tell us where the team is located (the timings seem too much like regular office hours for a globally distributed team). - It's also absolutely possible that commit times were changed, or commits were made/uploaded using timed scripts. Still seems like it would be hard in practice to use this to cover up a massive difference in timezone, though, because it would require adding a massive latency -- and xz wasn't developed in a vacuum, but in reaction to other online events like the filing of issues or mailing list posts. - yes, we did get the two examples mixed up (the two times we say are 10-ish hours apart are actually only minutes apart, and the times we say are minutes apart, are 10-ish hours apart). Update is on the way. - and finally, yes, UTC+2/3 goes beyond just Eastern Europe and includes part of the Middle East. We also clarified this in an update to the article. reply Dibby053 8 hours agoprevIt would be interesting to see how the commit timestamps change around time change. If he has a job in a country that observes DST they would likely show an abrupt change. reply k8svet 9 hours agoprevShame on me for not reading more before my now removed comment. Shout-out for doing this level of analysis and guessing, as with everything in this incident, fascinating and tantalizing to wonder about. It is interesting, at least one of the things listed as suspicious is something I do with some frequency. I will sometimes work on what is logically three commits and not chunk them out until the very end. A bit random, but has there been speculation on why this seemed to only target rpm/deb packaging? reply Twirrim 9 hours agoparent> A bit random, but has there been speculation on why this seemed to only target rpm/deb packaging? I don't think much speculation is needed. RPM and deb catches every Enterprise Linux distribution, to the best of my knowledge. rpm catches Red Hat and all derivatives like CentOS, Alma Linux etc, as well as SuSE, Amazon Linux, and even Microsoft's Mariner Linux (now renamed Azure Linux). deb catches Debian and Ubuntu. That's a huge global install base just through those two targets. reply k8svet 9 hours agorootparentSure, but why go to the effort to exclude others? Did they think it would help avoid detection on systems that they didn't care to backdoor? reply thrdbndndn 9 hours agorootparentObviously it would help avoiding detection since the backdoor has the side effect of slowing down things. So you only target the ones you really want. And to me it's less that they don't care, more that their script probably only works or only has been tested on the ones using .rpm/.deb. reply k8svet 8 hours agorootparent> Obviously it would help avoiding detection since the backdoor has the side effect of slowing down things. My understanding was that this was the result of a bug? But I understand that part less than any of it, so I well could be wrong. >And to me it's less that they don't care, more that their script probably only works or only has been tested on the ones using .rpm/.deb. Now that, strikes me as more likely. Maybe they wanted to be relatively sure that they wouldn't be detected and needed to select a test matrix to test against (SELinux/landlock config across rpm/deb distros, versus across every distro.) I guess what I'm getting at is... did they have an intended set of targets and knew they were running deb/rpm, on top of any other factors? reply cozzyd 7 hours agorootparentprevBecause random people testing an unpackaged version will not find a problem reply Sophira 8 hours agoprevI would remind people that it is not difficult at all to have someone's sleep schedule match another country, particularly when we're dealing with what seems to be a state-sponsored attack. Times are a red herring - don't be fooled. reply tgsovlerkhgsel 7 hours agoparentAnd yet in many clearly-attributed cases the times lined up with the attacking country's office hours. reply fullstop 6 hours agoprevIf you don't have your BIOS clock set to UTC and your OS is expecting it to be, you can get strange behavior like this. If they were switching between Windows and Linux, that could explain some of the offsets. reply BlueFalconHD 8 hours agoprevIf some nation state actor dis this, I imagine they couldve easily modified times of their commits, takin into consideration holidaya and such. Also, nation state actors have tons of resources. They could have people waiting to make commits or schedule it with something like cron. reply juitpykyk 9 hours agoprevFBI could subpoena GitHub for IP addresses. reply gpm 9 hours agoparentThey probably have/will, though we are unlikely to find out what they get unless they manage to arrest/charge/try him. Jia Tan probably used a vpn though - we know that they did for accessing IRC (source: https://boehs.org/node/everything-i-know-about-the-xz-backdo...) reply cft 8 hours agorootparentMost (but not all ) VPN providers keep logs and payment info that are subpoenable. You could use something like Mulvad with Lightning Network payments, but I am not sure that even that is fully anonymous. The Witopia VPN that he used for IRC [1] is US based: https://www.personalvpn.com/contact-us/ and they don't mention neither LN payments nor not keeping logs. 1. \"~jiatan@185.128.24.163\" https://boehs.org/node/everything-i-know-about-the-xz-backdo... reply FredFS456 8 hours agorootparentMullvad accepts cash in an envelope with no return address, good luck tracing that reply transpute 8 hours agorootparentPostal mail has non-zero metadata, e.g. origin can be traced at least to departure postal code. reply indrora 5 hours agorootparentThere's... actually very little that stops you from sending mail from a non-local postal code. I've occasionally sent packages postmarked as being from one zipcode from another; as long as it's in the same region, much of the postal processing doesn't care so much. There's also remailers and forwarders. reply yencabulator 5 hours agorootparentprevYou seem to assume Mullvad stores something that allows correlating a Mullvad account to a specific incoming envelope. Or that they store something that allows correlating an IP+time to a Mullvad account. reply transpute 5 hours agorootparentCorrelation is about combining metadata to incrementally narrow datasets. When a VPN provider isn't cooperative, metadata can be sought upstream. Mullvad deserves much credit for accepting non-digital payments, which increases the cost of deanonymization. reply jeltz 7 hours agorootparentprevDeparture postal code could be an area with like 1 million people. And if it is a nation state I am sure they could send the mail from another country. reply transpute 7 hours agorootparentMost populous US zip code is less than 150K people, https://worldpopulationreview.com/zips reply pquki4 6 hours agorootparentI mean, even if I live in Manhattan, I can easily take a subway train to an area with a different zip code and mail it there. And that's almost the easiest thing you could do -- if you want to hide where the mail is from, it is trivial. reply transpute 5 hours agorootparentDense urban areas are often blanketed by a range of spectrum sensors for the purpose of retroactive correlation (e.g Palantir) with other metadata sources. reply tobyp- 9 hours agoparentprevConsidering what the account was up to I sincerely doubt it was being used without Tor or a VPN. reply chgs 9 hours agorootparentThis article asserts some opsec failures - time zones switching when they shouldn’t etc. It’s quite plausible that they didn’t manage perfect vpn usage every single time. reply Karellen 8 hours agorootparentExpert level opsec - very rarely make deliberate \"mistakes\" by having an inconsistent timezone, or tunneling through someone's compromised home device instead of a VPN, to throw adversaries on wild goose chases. reply fullstop 8 hours agorootparentThey also committed with a different email address and a different name once or twice. This may have been intentional, though, to misdirect. reply netsharc 9 hours agorootparentprevI'm surprised they didn't set up guards to prevent mistakes like commits with the \"wrong\" timezones... reply arp242 9 hours agorootparentprevProbably; but ask Ross Ulbricht how easy it is to screw that up. The thing is you only need to make a mistake once. reply champtar 8 hours agoparentprevThe ssh public key might also be interesting, given the opsec failures, they might have used it for other accounts / at other providers reply pquki4 9 hours agoparentprevNot going to be useful if they consistently used a VPN to access GitHub. But people make mistakes sometimes. reply earslap 9 hours agorootparenthistory shows that it is generally very hard to not slip up here and there, especially if you are not expecting to be a huge target. reply BlueFalconHD 8 hours agoparentprevIf a US gov. agency is \"Jia Tan\" then this might not happen. reply xuki 9 hours agoparentprevBut Sir The Calls Are Coming from Inside the House! reply BlueFalconHD 7 hours agorootparentthis [1] but with handcuffs [1] https://i.redd.it/obvqaa0lhn841.jpg reply lenerdenator 8 hours agoprev> Generally, anonymity in the free software sphere is a good thing: software is inherently based on accomplishment and merit, and there is no reason to know anything about a person’s identity. This is an interesting take. Most software engineers I know love to show off their FOSS contributions. It's a place to do what _you_ want to do, free of budgets and MBA interference. I guess in certain parts of the world you probably want to keep that sort of skill set under wraps, though. reply gmassman 8 hours agoparentI think personality matters a lot here. One person may flaunt their open source contributions to their colleagues because they like the kudos. Another person may quietly commit and never publicize themselves because they don’t like being the center of attention. Both can even be true of the same person at different points in their careers! reply thrdbndndn 9 hours agoprevI suggest not putting a UTC+8 timezone graph as header image, when the conclusion is he's likely from Eastern Europe (not saying you should put one showing EET timezone instead). I understand that people should read the full article instead of drawing any conclusion from a mere image, but lots of people don't (hence why clickbait works). I also think it's a little tasteless, if not misleading. Disclaimer: I'm a Chinese. reply willsmith72 9 hours agoparenthow is it tasteless? the only fact is that, almost always, Jia Tan's timezone was UTC+8. the rest is mostly speculation about where else they could have been from given holidays and times reply pvg 8 hours agorootparentThe whole enterprise is a little tasteless with a chance of worse. Not some substacker or tweeter speculating in itself - people are obviously free to let their speculation flag fly. But you start amplifying this stuff on an open internet messageboard and next thing you know, it's reddit 'found' the Boston bomber. The local downsides outweigh the gawking value and I say this as a dyed-in-the-wool internet gawker. reply gmassman 8 hours agoparentprevIt seems no more misleading than Jia Tan assuming an identity based on git commit timestamps… reply 77pt77 7 hours agoparentprevexport TZ=Z will just tag all your commits in UTC. Just a heads up for the next guy... reply bombcar 7 hours agorootparentWrite a script to select a (random) time zone so all your commits are within an hour of noon local time zone time :) reply ShamelessC 8 hours agoparentprev> understand that people should read the full article instead of drawing any conclusion from a mere image, but lots of people don't People who don’t read the article and find themselves lacking an informed or nuanced understanding of the contents of said article are the only ones to blame here. Full stop. That HN doesn’t have a culture of shaming those who don’t read the article is partly the fault of the rules here which finds “read the article please” to be a distraction/nuisance but don’t mind that this often results in misinformed discussion propagating toward the top of a thread. reply thrdbndndn 6 hours agorootparentBy that logic there is also nothing wrong with clickbaity titles, which I disagree. reply krick 8 hours agoprevWould be nice if whoever made the analysis would attach the actual data in csv or something. I mean, it's a bit handier to overview than actually cloning the repo(s) ang using git log, if I'm not gonna dive deep. reply someotherperson 3 hours agoprevMaybe something that can be done if it hasn't been done already: semantic analysis across the different platforms they communicated on, to determine if it was the same person. Also semantic analysis on the same platforms using different cohorts of time (days of the week, months of the year, quarters, whatever). Also: analysis on the email chains they interacted with. It's a possibility that they may have responded to their own emails in their mailing lists with fix suggestions/requests for clarification etc. reply kijin 6 hours agoprev> you will often not be able to change the time without adding latency, i.e., withholding commits and thus delaying project development -- and even then, getting it right every time is hard. Someone with access to the right machines could easily check whether this is the case. A lot of projects have set up webhooks on github to sync new commits with their own bug tracker, for example, or testing infrastructure. Logs on these systems would tell you exactly when the commits were pushed. reply 1letterunixname 6 hours agoprevGiant, fragile, pointless build infrastructure is always suspicious. Just take k8s or any other Go cli utility unable to build directly as it was originally intended back when there was go get and now go install. Simplicity and clarity translate into security, maintainability, and reliability. reply goalieca 9 hours agoprevOne of Jia’s followers on GitHub is form Eastern Europe. Edit: it’s a meme. Apparently. I feel like an old man. > illia-git This user is suspected to be a part of an online ransomware organization. Please report any suspicious activity to GitHub security. Poland reply mintplant 9 hours agoparentThe message is a custom status, and probably a reference to a Discord meme: https://knowyourmeme.com/memes/discord-user-is-a-suspected-t... reply wannacboatmovie 8 hours agoprevEveryone here is acting as if it's impossible to script git commits or email replies as a scheduled task or even put someone on the night shift to click send - to give the appearance of being somewhere you're not. There was WAY more than one person involved here. Every email, commit, etc. was a deliberate task that was carefully planned. reply s_Hogg 8 hours agoprevSaying \"Eastern Europe\" as opposed to \"Moscow\" is a little coy, isn't it? They're in the same time zone and one seems likelier than the other, to put it one way. Edit: speaking of, these guys might also have some time pressure at the moment with regards to hacking stuff lol. Kind of fits the bill with what we know about the hack reply chgs 8 hours agoparentMoscow is UTC+3 year round, not UTC+2 in winter and +3 in summer reply alisonatwork 8 hours agorootparentIsrael is perhaps more likely given daylight savings. reply sureglymop 8 hours agoparentprevWhy? There would be a number of plausible location based on the time zones. Just asking why you would make that conclusion in terms of rationality. reply mavili 9 hours agoprev [–] I dont know what the XZ backdoor is about but, to assume someone putting backdoors into software would work usual working days and hours has to be very naïve. The whole premise of the article is based on a false assumption IMO. reply chgs 9 hours agoparentIf you’ve missed probably the largest cyber security story since stuxnet, and arguably bigger than that, I suggest you start looking at the last few days. Start here. https://news.ycombinator.com/item?id=39865810 reply vsnf 8 hours agorootparent> arguably bigger than that That’s a stretch. Stuxnet was the first acknowledged state cyber attack, utilized multiple zero days, and destroyed nuclear weapons manufacturing facilities. Bigger in scope sure, but bigger unconditionally? I don’t know about that. reply lucb1e 7 hours agorootparentFrom my point of view, from what we know today, it is bigger than Stuxnet because: Stuxnet: aimed to delay one nuclear facility that was still being built SSH pre-auth RCE: root access to most servers on the planet, impacting everyone from hobbyist self-hosters (that's me) to large businesses (of every type imaginable) to probably even some of the security agencies around the world. With SSH's track record, a lot of people choose to trust it as their internet-facing access protocol I expect that whoever made this would have picked their targets carefully to avoid revealing the backdoor, so probably they'd aim (at least at first) at a handful of businesses of interest (advanced chip manufacturing, say) and governments, rather than causing tangible widespread issues in some way. Theoretically, though, (perhaps upon being discovered) if they'd just drop `rm -rf && poweroff` on all reachable systems (perhaps having it spread into networks in a worm-like fashion, setting a timer for 30 seconds so that it can propagate), most computer-based systems would just stop working (either killed themselves, or from failing routers or other systems they relied on), and lots of them would lose data because their backups either weren't working or were also impacted. Consider just how much involves a computer today, from infrastructure to hospitals. That's a whole lot more impact than a delayed nuclear program due to some unpublished Windows exploits What modulates this potential impact is the question of how many important systems run something other than the OSes they were currently in the process of targeting (Debian, Ubuntu, and Fedora afaik), how many servers require a VPN (or similar) to access ssh and have no outward-facing ssh server anywhere on their network, and how many years it would have taken to uncover (more and more systems would have been running this over time) reply kelnos 3 hours agorootparent> SSH pre-auth RCE: root access to most servers on the planet, impacting everyone from hobbyist self-hosters (that's me) to large businesses (of every type imaginable) to probably even some of the security agencies around the world. No, absolutely not; that didn't actually happen. Most servers on the planet that are running x86_64 Linux are probably not running a rolling-release distro based on .deb or .rpm. Rolling-release (or a beta version of the next OS release) is required because I can't imagine one of them updating to a new version of a library like this until the next major release of their OS, and .deb/.rpm because those are the only build environments where the backdoor would get built into the library. (And on top of that, only systems where sshd is patched to link to libsystemd.) Also: while I will admit that there are many businesses with abysmal security practices, most will not have ssh exposed to the public internet; that'll require access to the corporate VPN as well. You note this, but fail to recognize the impact. Even hobbyists probably aren't hit by this in large numbers. I run a VPS for a variety of things, and it runs Debian stable. Assuming this backdoor attempt was never found, it wouldn't get installed on it until mid-/late-2025, when Debian trixie is likely to be released (plus some time for me to get around to upgrading it). I did have the affected package on my laptop, which runs Debian testing. But I don't have sshd enabled on it all the time, and even if I did, my laptop is rarely on a network without NAT (and I usually opt to tether to my phone when in public rather than use public WiFi). All the other machines on my home network are also running Debian stable, and were unaffected. I suspect that precious few systems even had the backdoor installed on them, and of those, even fewer were accessible directly on the public internet. Now this could have been bigger than Stuxnet, if the backdoor had remained secret for -- and I think I'm being generous here -- another year or so. reply akira2501 7 hours agorootparentprevIt's not clear that the distribution mechanism and many of the other requirements would be so broadly met that \"access to most servers on the internet\" is a correct description of the scope. Or that, once in the wild, that the author would have their pick of the litter when choosing targets. It's also true that the exploit has several kill switches in it, and so even vulnerable servers could be protected by simply setting an environment variable. Which, honestly, makes this entire attack all the more strange. It had incredibly unclear value and longevity. It makes me wonder if the author didn't have different, parallel aims, or if this wasn't a type of practice run for a larger attack in the future. reply DrTung 5 hours agorootparentSince they would have \"their pick of the litter\" why did they then include kill switches? A scary scenario is a rerun of the Viasat hack https://en.wikipedia.org/wiki/Viasat_hack I.e. wipe all servers worldwide at the morning of an attack, but before that you install the kill switches on your servers and those of your allies. reply akira2501 2 hours agorootparentTo make it harder to detect and reverse engineer. There's a plethora of disabling functionality in the exploit. This is bad, but the original post is being hyperbolic. reply transpute 8 hours agorootparentprevDepends on the set of global resources, including development, CI/CD and production systems, reachable by compromised sshd. reply kelnos 3 hours agorootparentAnd that would likely be precious few, at least today. Only a teeny tiny percentage of servers out there were updated to a vulnerable version. And an even smaller percentage likely had their sshd port exposed to the public internet. Sure, if the backdoor hadn't been found for a couple more years, the impact would have been much higher, as the backdoored version would have made it into actual current releases of the popular server distros, and companies gradually upgraded. reply bagels 9 hours agoparentprev [–] Governments employ armies of hackers who just may work usual working days and hours. I'm sure some of them don't, but some of them probably do. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The code snippet features CSS styles for a webpage, such as font families, color schemes for light/dark modes, and background images for success and error messages.",
      "Additionally, it contains JavaScript, cookie management, and parameters concerning visitor time, template version, and a URL link.",
      "The text is encoded, potentially presenting security risks and the possibility of backdoor access."
    ],
    "commentSummary": [
      "The discussion delves into a potential state-sponsored cyber attack using a backdoor named XZ, analyzing tactics, the attacking team, and geographical hints about the attacker's location.",
      "Concerns are raised about security vulnerabilities in open-source software, anonymity in online actions, and the repercussions of cyber attacks.",
      "Analysis includes commit timestamps, debates on potential ransomware actors, and the effects of a hypothetical SSH vulnerability, along with deliberations on VPN usage, time zone considerations, and ethical aspects of online data analysis."
    ],
    "points": 252,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1711928114
  },
  {
    "id": 39886328,
    "title": "JavaScript Signals: A Proposal for Improved State Management",
    "originLink": "https://github.com/proposal-signals/proposal-signals",
    "originBody": "🚦 JavaScript Signals standard proposal🚦 Stage 0 Authors: Rob Eisenberg and Daniel Ehrenberg This document describes an early common direction for signals in JavaScript, similar to the Promises/A+ effort which preceded the Promises standardized by TC39 in ES2015. Try it for yourself, using a polyfill. Similarly to Promises/A+, this effort focuses on aligning the JavaScript ecosystem. If this alignment is successful, then a standard could emerge, based on that experience. Several framework authors are collaborating here on a common model which could back their reactivity core. The current draft is based on design input from the authors/maintainers of Angular, Bubble, Ember, FAST, MobX, Preact, Qwik, RxJS, Solid, Starbeam, Svelte, Vue, Wiz, and more… Differently from Promises/A+, we're not trying to solve for a common developer-facing surface API, but rather the precise core semantics of the underlying signal graph. This proposal does include a fully concrete API, but the API is not targeted to most application developers. Instead, the signal API here is a better fit for frameworks to build on top of, providing interoperability through common signal graph and auto-tracking mechanism. The plan for this proposal is to do significant early prototyping, including integration into several frameworks, before advancing beyond Stage 1. We are only interested in standardizing Signals if they are suitable for use in practice in multiple frameworks, and provide real benefits over framework-provided signals. We hope that significant early prototyping will give us this information. See \"Status and development plan\" below for more details. Background: Why Signals? To develop a complicated user interface (UI), JavaScript application developers need to store, compute, invalidate, sync, and push state to the application's view layer in an efficient way. UIs commonly involve more than just managing simple values, but often involve rendering computed state which is dependent on a complex tree of other values or state that is also computed itself. The goal of Signals is to provide infrastructure for managing such application state so developers can focus on business logic rather than these repetitive details. Signal-like constructs have independently been found to be useful in non-UI contexts as well, particularly in build systems to avoid unnecessary rebuilds. Example - A VanillaJS Counter Given a variable, counter, you want to render into the DOM whether the counter is even or odd. Whenever the counter changes, you want to update the DOM with the latest parity. In Vanilla JS, you might have something like this: let counter = 0; const setCounter = (value) => { counter = value; render(); }; const isEven = () => (counter & 1) == 0; const parity = () => isEven() ? \"even\" : \"odd\"; const render = () => element.innerText = parity(); // Simulate external updates to counter... setInterval(() => setCounter(counter + 1), 1000); This has a number of problems... The counter setup is noisy and boilerplate-heavy. The counter state is tightly coupled to the rendering system. If the counter changes but parity does not (e.g. counter goes from 2 to 4), then we do unnecessary computation of the parity and unnecessary rendering. What if another part of our UI just wants to render when the counter updates? What if another part of our UI is dependent on isEven or parity alone? Even in this relatively simple scenario, a number of issues arise quickly. We could try to work around these by introducing pub/sub for the counter. This would allow additional consumers of the counter could subscribe to add their own reactions to state changes. However, we're still stuck with the following problems: The render function, which is only dependent on parity must instead \"know\" that it actually needs to subscribe to counter. It isn't possible to update UI based on either isEven or parity alone, without directly interacting with counter. We've increased our boilerplate. Any time you are using something, it's not just a matter of calling a function or reading a variable, but instead subscribing and doing updates there. Managing unsubscription is also especially complicated. Now, we could solve a couple issues by adding pub/sub not just to counter but also to isEven and parity. We would then have to subscribe isEven to counter, parity to isEven, and render to parity. Unfortunately, not only has our boilerplate code exploded, but we're stuck with a ton of bookkeeping of subscriptions, and a potential memory leak disaster if we don't properly clean everything up in the right way. So, we've solved some issues but created a whole new category of problems and a lot of code. To make matters worse, we have to go through this entire process for every piece of state in our system. Introducing Signals Data binding abstractions in UIs for the model and view have long been core to UI frameworks across multiple programming languages, despite the absence of any such mechanism built into JS or the web platform. Within JS frameworks and libraries, there has been a large amount of experimentation across different ways to represent this binding, and experience has shown the power of one-way data flow in conjunction with a first-class data type representing a cell of state or computation derived from other data, now often called \"Signals\". This first-class reactive value approach seems to have made its first popular appearance in open-source JavaScript web frameworks with Knockout in 2010. In the years since, many variations and implementations have been created. Within the last 3-4 years, the Signal primitive and related approaches have gained further traction, with nearly every modern JavaScript library or framework having something similar, under one name or another. To understand Signals, let's take a look at the above example, re-imagined with a Signal API further articulated below. Example - A Signals Counter const counter = new Signal.State(0); const isEven = new Signal.Computed(() => (counter.get() & 1) == 0); const parity = new Signal.Computed(() => isEven.get() ? \"even\" : \"odd\"); // A library or framework defines effects based on other Signal primitives declare function effect(cb: () => void): (() => void); effect(() => element.innerText = parity.get()); // Simulate external updates to counter... setInterval(() => counter.set(counter.get() + 1), 1000); There are a few things we can see right away: We've eliminated the noisy boilerplate around the counter variable from our previous example. There is a unified API to handle values, computations, and side effects. There's no circular reference problem or upside down dependencies between counter and render. There are no manual subscriptions, nor is there any need for bookkeeping. There is a means of controlling side-effect timing/scheduling. Signals give us much more than what can be seen on the surface of the API though: Automatic Dependency Tracking - A computed Signal automatically discovers any other Signals that it is dependent on, whether those Signals be simple values or other computations. Lazy Evaluation - Computations are not eagerly evaluated when they are declared, nor are they immediately evaluated when their dependencies change. They are only evaluated when their value is explicitly requested. Memoization - Computed Signals cache their last value so that computations that don't have changes in their dependencies do not need to be re-evaluated, no matter how many times they are accessed. Motivation for standardizing Signals Interoperability Each Signal implementation has its own auto-tracking mechanism, to keep track of the sources encountered when evaluating a computed Signal. This makes it hard to share models, components, and libraries between different frameworks--they tend to come with a false coupling to their view engine (given that Signals are usually implemented as part of JS frameworks). A goal of this proposal is to fully decouple the reactive model from the rendering view, enabling developers to migrate to new rendering technologies without rewriting their non-UI code, or develop shared reactive models in JS to be deployed in different contexts. Unfortunately, due to versioning and duplication, it has turned out to be impractical to reach a strong level of sharing via JS-level libraries--built-ins offer a stronger sharing guarantee. Performance/Memory usage It is always a small potential performance boost to ship less code due to commonly used libraries being built-in, but implementations of Signals are generally pretty small, so we don't expect this effect to be very large. We suspect that native C++ implementations of Signal-related data structures and algorithms can be slightly more efficient than what is achievable in JS, by a constant factor. However, no algorithmic changes are anticipated vs. what would be present in a polyfill; engines are not expected to be magic here, and the reactivity algorithms themselves will be well-defined and unambiguous. The champion group expects to develop various implementations of Signals, and use these to investigate these performance possibilities. DevTools With existing JS-language Signal libraries, it can be difficult to trace things like: The callstack across a chain of computed Signals, showing the causal chain for an error The reference graph among Signals, when one depends on another -- important when debugging memory usage Built-in Signals enable JS runtimes and DevTools to potentially have improved support for inspecting Signals, particularly for debugging or performance analysis, whether this is built into browsers or through a shared extension. Existing tools such as the element inspector, performance snapshot, and memory profilers could be updated to specifically highlight Signals in their presentation of information. Secondary benefits Benefits of a standard library In general, JavaScript has had a fairly minimal standard library, but a trend in TC39 has been to make JS more of a \"batteries-included\" language, with a high-quality, built-in set of functionality available. For example, Temporal is replacing moment.js, and a number of small features, e.g., Array.prototype.flat and Object.groupBy are replacing many lodash use cases. Benefits include smaller bundle sizes, improved stability and quality, less to learn when joining a new project, and a generally common vocabulary across JS developers. HTML/DOM Integration (a future possibility) Current work in W3C and by browser implementors is seeking to bring native templating to HTML. Additionally, the W3C Web Components CG is exploring the possibility of extending Web Components to offer a fully declarative HTML API. To accomplish both of these goals, eventually a reactive primitive will be needed by HTML. Additionally, many ergonomic improvements to the DOM through integration of Signals can be imagined and have been asked for by the community. Note, this integration would be a separate effort to come later, not part of this proposal itself. Ecosystem information exchange (not a reason to ship) Standardization efforts can sometimes be helpful just at the \"community\" level, even without changes in browsers. The Signals effort is bringing together many different framework authors for a deep discussion about the nature of reactivity, algorithms and interoperability. This has already been useful, and does not justify inclusion in JS engines and browsers; Signals should only be added to the JavaScript standard if there are significant benefits beyond the ecosystem information exchange enabled. Design goals for Signals It turns out that existing Signal libraries are not all that different from each other, at their core. This proposal aims to build on their success by implementing the important qualities of many of those libraries. Core features A Signal type which represents state, i.e. writable Signal. This is a value that others can read. A computed/memo/derived Signal type, which depends on others and is lazily calculated and cached. Computation is lazy, meaning computed Signals aren't calculated again by default when one of their dependencies changes, but rather only run if someone actually reads them. Computation is \"glitch-free\", meaning no unnecessary calculations are ever performed. This implies that, when an application reads a computed Signal, there is a topological sorting of the potentially dirty parts of the graph to run, to eliminate any duplicates. Computation is cached, meaning that if, after the last time a dependency changes, no dependencies have changed, then the computed Signal is not recalculated when accessed. Custom comparisons are possible for computed Signals as well as state Signals, to note when further computed Signals which depend on them should be updated. Reactions to the condition where a computed Signal has one of its dependencies (or nested dependencies) become \"dirty\" and change, meaning that the Signal's value might be outdated. This reaction is meant to schedule more significant work to be performed later. Effects are implemented in terms of these reactions, plus framework-level scheduling. Computed signals need the ability to react to whether they are registered as a (nested) dependency of one of these reactions. Enable JS frameworks to do their own scheduling. No Promise-style built-in forced-on scheduling. Synchronous reactions are needed to enable scheduling later work based on framework logic. Writes are synchronous and immediately take effect (a framework which batches writes can do that on top). It is possible to separate checking whether an effect may be \"dirty\" from actually running the effect (enabling a two-stage effect scheduler). Ability to read Signals without triggering dependencies to be recorded (untrack) Enable composition of different codebases which use Signals/reactivity, e.g., Using multiple frameworks together as far as tracking/reactivity itself goes (modulo omissions, see below) Framework-independent reactive data structures (e.g., recursively reactive store proxy, reactive Map and Set and Array, etc.) Soundness Discourage/prohibit naive misuse of synchronous reactions. Soundness risk: it may expose \"glitches\" if improperly used: If rendering is done immediately when a Signal is set, it may expose incomplete application state to the end user. Therefore, this feature should only be used to intelligently schedule work for later, once application logic is finished. Solution: Disallow reading and writing any Signal from within a synchronous reaction callback Discourage untrack and mark its unsound nature Soundness risk: allows the creation of computed Signals whose value depends on other Signals, but which aren't updated when those Signals change. It should be used when the untracked accesses will not change the result of the computation. Solution: The API is marked \"unsafe\" in the name. Note: This proposal does allow signals to be both read and written from computed and effect signals, without restricting writes that come after reads, despite the soundness risk. This decision was taken to preserve flexibility and compatibility in integration with frameworks. Surface API Must be a solid base for multiple frameworks to implement their Signals/reactivity mechanisms. Should be a good base for recursive store proxies, decorator-based class field reactivity, and both .value and [state, setState]-style APIs. The semantics are able to express the valid patterns enabled by different frameworks. For example, it should be possible for these Signals to be the basis of either immediately-reflected writes or writes which are batched and applied later. It would be nice if this API is usable directly by JavaScript developers. If a feature matches with an ecosystem concept, using common vocabulary is good. However, it is important to not literally shadow the exact same names! Tension between \"usability by JS devs\" and \"providing all the hooks to frameworks\" Idea: Provide all the hooks, but include errors when misused if possible. Idea: Put subtle APIs in a subtle namespace, similar to crypto.subtle, to mark the line between APIs which are good to use for most developers, and the more advanced ones. Be implementable and usable with good performance -- the surface API doesn't cause too much overhead Enable subclassing, so that frameworks can add their own methods and fields, including private fields. This is important to avoid the need for additional allocations at the framework level. See \"Memory management\" below. Memory management If possible: A computed Signal should be garbage-collectable if nothing live is referencing it for possible future reads, even if it's linked into a broader graph which stays alive (e.g., by reading a state which remains live). Note that most frameworks today require explicit disposal of computed Signals if they have any reference to or from another Signal graph which remains alive. This ends up not being so bad when their lifetime is tied to the lifetime of a UI component, and effects need to be disposed of anyway. If it is too expensive to execute with these semantics, then we should add explicit disposal (or \"unlinking\") of computed Signals to the API below, which currently lacks it. A separate related goal: Minimize the number of allocations, e.g., to make a writable Signal (avoid two separate closures + array) to implement effects (avoid a closure for every single reaction) In the API for observing Signal changes, avoid creating additional temporary data structures Solution: Class-based API enabling reuse of methods and fields defined in subclasses API sketch An initial idea of a Signal API is below. Note that this is just an early draft, and we anticipate changes over time. Let's start with the full .d.ts to get an idea of the overall shape, and then we'll discuss the details of what it all means. namespace Signal { // A read-write Signal class State implements Signal { // Create a state Signal starting with the value t constructor(t: T, options?: SignalOptions); // Get the value of the signal get(): T; // Set the state Signal value to t set(t: T): void; } // A Signal which is a formula based on other Signals class Computed implements Signal { // Create a Signal which evaluates to the value returned by the callback. // Callback is called with this signal as the this value. constructor(cb: (this: Computed) => T, options?: SignalOptions); // Get the value of the signal get(): T; } // This namespace includes \"advanced\" features that are better to // leave for framework authors rather than application developers. // Analogous to `crypto.subtle` namespace subtle { // Run a callback with all tracking disabled (even for nested computed). function untrack(cb: () => T): T; // Get the current computed signal which is tracking any signal reads, if any function currentComputed(): Computednull; // Returns ordered list of all signals which this one referenced // during the last time it was evaluated. // For a Watcher, lists the set of signals which it is watching. function introspectSources(s: ComputedWatcher): (StateComputed)[]; // Returns the Watchers that this signal is contained in, plus any // Computed signals which read this signal last time they were evaluated, // if that computed signal is (recursively) watched. function introspectSinks(s: StateComputed): (ComputedWatcher)[]; // True if this signal is \"live\", in that it is watched by a Watcher, // or it is read by a Computed signal which is (recursively) live. function hasSinks(s: StateComputed): boolean; // True if this element is \"reactive\", in that it depends // on some other signal. A Computed where hasSources is false // will always return the same constant. function hasSources(s: ComputedWatcher): boolean; class Watcher { // When a (recursive) source of Watcher is written to, call this callback, // if it hasn't already been called since the last `watch` call. // No signals may be read or written during the notify. constructor(notify: (this: Watcher) => void); // Add these signals to the Watcher's set, and set the watcher to run its // notify callback next time any signal in the set (or one of its dependencies) changes. // Can be called with no arguments just to reset the \"notified\" state, so that // the notify callback will be invoked again. watch(...s: Signal[]): void; // Remove these signals from the watched set (e.g., for an effect which is disposed) unwatch(...s: Signal[]): void; // Returns the set of sources in the Watcher's set which are still dirty, or is a computed signal // with a source which is dirty or pending and hasn't yet been re-evaluated getPending(): Signal[]; } // Hooks to observe being watched or no longer watched var watched: Symbol; var unwatched: Symbol; } interface Options { // Custom comparison function between old and new value. Default: Object.is. // The signal is passed in as the this value for context. equals?: (this: Signal, t: T, t2: T) => boolean; // Callback called when isWatched becomes true, if it was previously false [Signal.subtle.watched]?: (this: Signal) => void; // Callback called whenever isWatched becomes false, if it was previously true [Signal.subtle.unwatched]?: (this: Signal) => void; } } How Signals work A Signal represents a cell of data which may change over time. Signals may be either \"state\" (just a value which is set manually) or \"computed\" (a formula based on other Signals). Computed Signals work by automatically tracking which other Signals are read during their evaluation. When a computed is read, it checks whether any of its previously recorded dependencies have changed, and re-evaluates itself if so. When multiple computed Signals are nested, all of the attribution of the tracking goes to the innermost one. Computed Signals are lazy, i.e., pull-based: they are only re-evaluated when they are accessed, even if one of their dependencies changed earlier. The callback passed into computed Signals should generally be \"pure\" in the sense of being a deterministic, side-effect-free function of the other Signals which it accesses. At the same time, the timing of the callback being called is deterministic, allowing side effects to be used with care. Signals feature prominent caching/memoization: both state and computed Signals remember their current value, and only trigger recalculation of computed Signals which reference them if they actually change. A repeated comparison of old vs new values isn't even needed--the comparison is made once when the source Signal is reset/re-evaluated, and the Signal mechanism keeps track of which things referencing that Signal have not updated based on the new value yet. Internally, this is generally represented through \"graph coloring\" as described in (Milo's blog post). Computed Signals track their dependencies dynamically--each time they are run, they may end up depending on different things, and that precise dependency set is kept fresh in the Signal graph. This means that if you have a dependency needed on only one branch, and the previous calculation took the other branch, then a change to that temporarily unused value will not cause the computed Signal to be recalculated, even when pulled. Unlike JavaScript Promises, everything in Signals runs synchronously: Setting a Signal to a new value is synchronous, and this is immediately reflected when reading any computed Signal which depends on it afterwards. There is no built-in batching of this mutation. Reading computed Signals is synchronous--their value is always available. The notify callback in Watchers, as explained below, runs synchronously, during the .set() call which triggered it (but after graph coloring has completed). Like Promises, Signals can represent an error state: If a computed Signal's callback throws, then that error is cached just like another value, and rethrown every time the Signal is read. Understanding the Signal class A Signal instance represents the capability to read a dynamically changing value whose updates are tracked over time. It also implicitly includes the capability to subscribe to the Signal, implicitly through a tracked access from another computed Signal. The API here is designed to match the very rough ecosystem consensus among a large fraction of Signal libraries in the use of names like \"signal\", \"computed\" and \"state\". However, access to Computed and State Signals is through a .get() method, which disagrees with all popular Signal APIs, which either use a .value-style accessor, or signal() call syntax. The API is designed to reduce the number of allocations, to make Signals suitable for embedding in JavaScript frameworks while reaching same or better performance than existing framework-customized Signals. This implies: State Signals are a single writable object, which can be both accessed and set from the same reference. (See implications below in the \"Capability separation\" section.) Both State and Computed Signals are designed to be subclassable, to facilitate frameworks' ability to add additional properties through public and private class fields (as well as methods for using that state). Various callbacks (e.g., equals, the computed callback) are called with the relevant Signal as the this value for context, so that a new closure isn't needed per Signal. Instead, context can be saved in extra properties of the signal itself. Some error conditions enforced by this API: It is an error to read a computed recursively. The notify callback of a Watcher cannot read or write any signals If a computed Signal's callback throws, then subsequent accesses of the Signal rethrow that cached error, until one of the dependencies changes and it is recalculated. Some conditions which are not enforced: Computed Signals can write to other Signals, synchronously within their callback Work which is queued by a Watcher's notify callback may read or write signals, making it possible to replicate classic React antipatterns in terms of Signals! Implementing effects The Watcher interface defined above gives the basis for implementing typical JS APIs for effects: callbacks which are re-run when other Signals change, purely for their side effect. The effect function used above in the initial example can be defined as follows: // This function would usually live in a library/framework, not application code // NOTE: This scheduling logic is too basic to be useful. Do not copy/paste. let pending = false; let w = new Signal.subtle.Watcher(self => { if (!pending) { pending = true; queueMicrotask(() => { pending = false; for (let s of self.getPending()) s.get(); self.watch(); }); } }); // An effect effect Signal which evaluates to cb, which schedules a read of // itself on the microtask queue whenever one of its dependencies might change export function effect(cb) { let destructor; let c = new Signal.Computed(() => { destructor.?(); destructor = cb(); }); w.watch(c); c.get(); return () => { destructor.?(); w.unwatch(c) }; } The Signal API does not include any built-in function like effect. This is because effect scheduling is subtle and often ties into framework rendering cycles and other high-level framework-specific state or strategies which JS does not have access to. Walking through the different operations used here: The notify callback passed into Watcher constructor is the function that is called when the Signal goes from a \"clean\" state (where we know the cache is initialized and valid) into a \"checked\" or \"dirty\" state (where the cache might or might not be valid because at least one of the states which this recursively depends on has been changed). Calls to notify are ultimately triggered by a call to .set() on some state Signal. This call is synchronous: it happens before .set returns. But there's no need to worry about this callback observing the Signal graph in a half-processed state, because during a notify callback, no Signal can be read or written, even in an untrack call. Because notify is called during .set(), it is interrupting another thread of logic, which might not be complete. To read or write Signals from notify, schedule work to run later, e.g., by writing the Signal down in a list to later be accessed, or with queueMicrotask as above. Note that it is perfectly possible to use Signals effectively without Symbol.subtle.Watcher by scheduling polling of computed Signals, as Glimmer does. However, many frameworks have found that it is very often useful to have this scheduling logic run synchronously, so the Signals API includes it. Both computed and state Signals are garbage-collected like any JS values. But Watchers have a special way of holding things alive: Any Signals which are watched by a Watcher will be held alive as long as any of the underlying states are reachable, as these may trigger a future notify call (and then a future .get()). For this reason, remember to call Watcher.prototype.unwatch to clean up effects. An unsound escape hatch Signal.subtle.untrack is an escape hatch allowing reading Signals without tracking those reads. This capability is unsafe because it allows the creation of computed Signals whose value depends on other Signals, but which aren't updated when those Signals change. It should be used when the untracked accesses will not change the result of the computation. Omitted for now These features may be added later, but they are not included in the current draft. Their omission is due to the lack of established consensus in the design space among frameworks, as well as the demonstrated ability to work around their absence with mechanisms on top of the Signals notion described in this document. However, unfortunately, the omission limits the potential of interoperability among frameworks. As prototypes of Signals as described in this document are produced, there will be an effort to reexamine whether these omissions were the appropriate decision. Async: Signals are always synchronously available for evaluation, in this model. However, it is frequently useful to have certain asynchronous processes which lead to a signal being set, and to have an understanding of when a signal is still \"loading\". One simple way to model the loading state is with exceptions, and the exception-caching behavior of computed signals composes somewhat reasonably with this technique. Improved techniques are discussed in Issue #30. Transactions: For transitions between views, it is often useful to maintain a live state for both the \"from\" and \"to\" states. The \"to\" state renders in the background, until it is ready to swap over (committing the transaction), while the \"from\" state remains interactive. Maintaining both states at the same time requires \"forking\" the state of the signal graph, and it may even be useful to support multiple pending transitions at once. Discussion in Issue #73. Some possible convenience methods are also omitted. Status and development plan This proposal is on the April 2024 TC39 agenda for Stage 1. It can currently be thought of as \"Stage 0\". A polyfill for this proposal is available, with some basic tests. Some framework authors have begun experimenting with substituting this signal implementation, but this usage is at an early stage. The collaborators on the Signal proposal want to be especially conservative in how we push this proposal forward, so that we don't land in the trap of getting something shipped which we end up regretting and not actually using. Our plan is to do the following extra tasks, not required by the TC39 process, to make sure that this proposal is on track: Before proposing for Stage 2, we plan to: Develop multiple production-grade polyfill implementations which are solid, well-tested (e.g., passing tests from various frameworks as well as test262-style tests), and competitive in terms of performance (as verified with a thorough signal/framework benchmark set). Integrate the proposed Signal API into a large number of JS frameworks that we consider somewhat representative, and some large applications work with this basis. Test that it works efficiently and correctly in these contexts. Have a solid understanding on the space of possible extensions to the API, and have concluded which (if any) should be added into this proposal. Signal algorithms This section describes each of the APIs exposed to JavaScript, in terms of the algorithms that they implement. This can be thought of as a proto-specification, and is included at this early point to nail down one possible set of semantics, while being very open to changes. Some aspects of the algorithm: The order of reads of Signals within a computed is significant, and is observable in the order that certain callbacks (which Watcher is invoked, equals, the first parameter to new Signal.Computed, and the watched/unwatched callbacks) are executed. This means that the sources of a computed Signal must be stored ordered. These four callbacks might all throw exceptions, and these exceptions are propagated in a predictable manner to the calling JS code. The exceptions do not halt execution of this algorithm or leave the graph in a half-processed state. For errors thrown in the notify callback of a Watcher, that exception is sent to the .set() call which triggered it, using an AggregateError if multiple exceptions were thrown. The others (including watched/unwatched?) are stored in the value of the Signal, to be rethrown when read, and such a rethrowing Signal can be marked ~clean~ just like any other with a normal value. Care is taken to avoid circularities in cases of computed signals which are not \"watched\" (being observed by any Watcher), so that they can be garbage collected independently from other parts of the signal graph. Internally, this can be implemented with a system of generation numbers which are always collected; note that optimized implementations may also include local per-node generation numbers, or avoid tracking some numbers on watched signals. Hidden global state Signal algorithms need to reference certain global state. This state is global for the entire thread, or \"agent\". computing: The innermost computed or effect Signal currently being reevaluated due to a .get or .run call, or undefined. Initially undefined. notifying: Boolean denoting whether there is an notify callback currently executing. Initially false. generation: An incrementing integer, starting at 0, used to track how current a value is while avoiding circularities. The Signal namespace Signal is an ordinary object which serves as a namespace for Signal-related classes and functions. Signal.subtle is a similar inner namespace object. The Signal.State class Signal.State internal slots value: The current value of the state signal equals: The comparison function used when changing values watched: The callback to be called when the signal becomes observed by an effect unwatched: The callback to be called when the signal is no longer observed by an effect sinks: Set of watched signals which depend on this one Constructor: Signal(initialValue, options) Set this Signal's value to initialValue. Set this Signal's equals to options?.equals Set this Signal's watched to options?.[Signal.subtle.watched] Set this Signal's unwatched to options?.[Signal.subtle.watched] Set this Signal's sinks to the empty set Set state to ~clean~. Method: Signal.State.prototype.get() If notifying is true, throw an exception. If computing is not undefined, add this Signal to computing's sources set. NOTE: We do not add computing to this Signal's sinks set until it is watched by a Watcher. Return this Signal's value. Method: Signal.State.prototype.set(newValue) If the current execution context is notifying, throw an exception. Run the \"set Signal value\" algorithm with this Signal and the first parameter for the value. If that algorithm returned ~clean~, then return undefined. Set the state of all sinks of this Signal to (if it is a Computed Signal) ~dirty~ if they were previously clean, or (if it is a Watcher) ~pending~ if it was previously ~watching~. Set the state of all of the sinks' Computed Signal dependencies (recursively) to ~checked~ if they were previously ~clean~ (that is, leave dirty markings in place), or for Watchers, ~pending~ if previously ~watching~. For each previously ~watching~ Watcher encountered in that recursive search, then in depth-first order, Set notifying to true while calling their notify callback (saving aside any exception thrown, but ignoring the return value of notify), and then restore notifying to false. If any exception was thrown from the notify callbacks, propagate it to the caller after all notify callbacks have run. If there are multiple exceptions, then package them up together into an AggregateError and throw that. Return undefined. The Signal.Computed class Signal.Computed Internal slots value: The previous cached value of the Signal, or ~uninitialized~ for a never-read computed Signal. The value may be an exception which gets rethrown when the value is read. Always undefined for effect signals. state: May be ~clean~, ~checked~, ~computing~, or ~dirty~. sources: An ordered set of Signals which this Signal depends on. sinks: An ordered set of Signals which depend on this Signal. equals: The equals method provided in the options. callback: The callback which is called to get the computed Signal's value. Set to the first parameter passed to the constructor. Signal.Computed Constructor The constructor sets callback to its first parameter equals based on options, defaulting to Object.is if absent state to ~dirty~ value to ~uninitialized~ With AsyncContext, the callback passed to new Signal.Computed closes over the snapshot from when the constructor was called, and restores this snapshot during its execution. Method: Signal.Computed.prototype.get If the current execution context is notifying or if this Signal has the state ~computing~, or if this signal is an Effect and computing a computed Signal, throw an exception. If computing is not undefined, add this Signal to computing's sources set. NOTE: We do not add computing to this Signal's sinks set until/unless it becomes watched by a Watcher. If this Signal's state is ~dirty~ or ~checked~: Repeat the following steps until this Signal is ~clean~: Recurse up via sources to find the deepest, left-most (i.e. earliest observed) recursive source which is marked ~dirty~ (cutting off search when hitting a ~clean~ Signal, and including this Signal as the last thing to search). Perform the \"recalculate dirty computed Signal\" algorithm on that Signal. At this point, this Signal's state will be ~clean~, and no recursive sources will be ~dirty~ or ~checked~. Return the Signal's value. If the value is an exception, rethrow that exception. The Signal.subtle.Watcher class Signal.subtle.Watcher internal slots state: May be ~watching~, ~pending~ or ~waiting~ signals: An ordered set of Signals which this Watcher is watching notifyCallback: The callback which is called when something changes. Set to the first parameter passed to the constructor. Constructor: new Signal.subtle.Watcher(callback) state is set to ~waiting~. Initialize signals as an empty set. notifyCallback is set to the callback parameter. With AsyncContext, the callback passed to new Signal.subtle.Watcher does not close over the snapshot from when the constructor was called, so that contextual information around the write is visible. Method: Signal.subtle.Watcher.prototype.watch(...signals) If any of the arguments is not a signal, throw an exception. Append all arguments to the end of this object's signals. Add this watcher to each of the newly watched signals as a sink. Add this watcher as a sink to each Signal. If this was the first sink, then recurse up to sources to add that signal as a sink, and call the watched callback if it exists. Method: Signal.subtle.Watcher.prototype.unwatch(...signals) If any of the arguments is not a signal, or is not being watched by this watcher, throw an exception. Remove each element from signals from this object's signals. Remove this Watcher from that Signal's sink set. If any Signal's sink set is now empty, then remove itself as a sink from each of its sources, and call the unwatched callback if it exists Method: Signal.subtle.Watcher.prototype.getPending() Return an Array containing the subset of signals which are in the state dirty or pending. Method: Signal.subtle.untrack(cb) Let c be the execution context's current computing state. Set computing to undefined. Call cb. Restore computing to c (even if cb threw an exception). Return the return value of cb (rethrowing any exception). Note: untrack doesn't get you out of the notifying state, which is maintained strictly. Common algorithms Algorithm: recalculate dirty computed Signal Clear out this Signal's sources set, and remove it from those sources' sinks sets. Save the previous computing value and set computing to this Signal. Set this Signal's state to ~computing~. Run this computed Signal's callback, using this Signal as the this value. Save the return value, and if the callback threw an exception, store that for rethrowing. Set this Signal's recalculating to false. Restore the previous computing value. Apply the \"set Signal value\" algorithm to the callback's return value. Set this Signal's state to ~clean~. If that algorithm returned ~dirty~: mark all sinks of this Signal as ~dirty~ (previously, the sinks may have been a mix of checked and dirty). (Or, if this is unwatched, then adopt a new generation number to indicate dirtiness, or something like that.) Otherwise, that algorithm returned ~clean~: In this case, for each ~checked~ sink of this Signal, if all of that Signal's sources are now clean, then mark that Signal as ~clean~ as well. Apply this cleanup step to further sinks recursively, to any newly clean Signals which have checked sinks. (Or, if this is unwatched, somehow indicate the same, so that the cleanup can proceed lazily.) Set Signal value algorithm If this algorithm was passed a value (as opposed to an exception for rethrowing, from the recalculate dirty computed Signal algorithm): Call this Signal's equals function, passing as parameters the current value, the new value, and this Signal. If an exception is thrown, save that exception (for rethrowing when read) as the value of the Signal and continue as if the callback had returned false. If that function returned true, return ~clean~. Set the value of this Signal to the parameter. Return ~dirty~ FAQ Q: Isn't it a little soon to be standardizing something related to Signals, when they just started to be the hot new thing in 2022? Shouldn't we give them more time to evolve and stabilize? A: The current state of Signals in web frameworks is the result of more than 10 years of continuous development. As investment steps up, as it has in recent years, almost all of the web frameworks are approaching a very similar core model of Signals. This proposal is the result of a shared design exercise between a large number of current leaders in web frameworks, and it will not be pushed forward to standardization without the validation of that group of domain experts in various contexts. How are Signals used? Q: Can built-in Signals even be used by frameworks, given their tight integration with rendering and ownership? A: The parts which are more framework-specific tend to be in the area of effects, scheduling, and ownership/disposal, which this proposal does not attempt to solve. Our first priority with prototyping standards-track Signals is to validate that they can sit \"underneath\" existing frameworks compatibly and with good performance. Q: Is the Signal API meant to be used directly by application developers, or wrapped by frameworks? A: While this API could be used directly by application developers (at least the part which is not within the Signal.subtle namespace), it is not designed to be especially ergonomic. Instead, the needs of library/framework authors are priorities. Most frameworks are expected to wrap even the basic Signal.State and Signal.Computed APIs with something expressing their ergonomic slant. In practice, it's typically best to use Signals via a framework, which manages trickier features (e.g., Watcher, untrack), as well as managing ownership and disposal (e.g., figuring out when signals should be added to and removed from watchers), and scheduling rendering to DOM--this proposal doesn't attempt to solve those problems. Q: Do I have to tear down Signals related to a widget when that widget is destroyed? What is the API for that? A: The relevant teardown operation here is Signal.subtle.Watcher.prototype.unwatch. Only watched Signals need to be cleaned up (by unwatching them), while unwatched Signals can be garbage-collected automatically. Q: Do Signals work with VDOM, or directly with the underlying HTML DOM? A: Yes! Signals are independent of rendering technology. Existing JavaScript frameworks which use Signal-like constructs integrate with VDOM (e.g., Preact), the native DOM (e.g., Solid) and a combination (e.g., Vue). The same will be possible with built-in Signals. Q: Is it going to be ergonomic to use Signals in the context of class-based frameworks like Angular and Lit? What about compiler-based frameworks like Svelte? A: Class fields can be made Signal-based with a simple accessor decorator, as shown in the Signal polyfill readme. Signals are very closely aligned to Svelte 5's Runes--it is simple for a compiler to transform runes to the Signal API defined here, and in fact this is what Svelte 5 does internally (but with its own Signals library). Q: Do Signals work with SSR? Hydration? Resumability? A: Yes. Qwik uses Signals to good effect with both of these properties, and other frameworks have other well-developed approaches to hydration with Signals with different tradeoffs. We think that it is possible to model Qwik's resumable Signals using a State and Computed signal hooked together, and plan to prove this out in code. Q: Do Signals work with one-way data flow like React does? A: Yes, Signals are a mechanism for one-way dataflow. Signal-based UI frameworks let you express your view as a function of the model (where the model incorporates Signals). A graph of state and computed Signals is acyclic by construction. It is also possible to recreate React antipatterns within Signals (!), e.g., the Signal equivalent of a setState inside of useEffect is to use a Watcher to schedule a write to a State signal. Q: How do signals relate to state management systems like Redux? Do signals encourage unstructured state? A: Signals can form an efficient basis for store-like state management abstractions. A common pattern found in multiple frameworks is an object based on a Proxy which internally represents properties using Signals, e.g., Vue reactive(), or Solid stores. These systems enable flexible grouping of state at the right level of abstraction for the particular application. How do Signals work? Q: Are Signals push-based or pull-based? A: Evaluation of computed Signals is pull-based: computed Signals are only evaluated when .get() is called, even if the underlying state changed much earlier. At the same time, changing a State signal may immediately trigger a Watcher's callback, \"pushing\" the notification. So Signals may be thought of as a \"push-pull\" construction. Q: Do Signals introduce nondeterminism into JavaScript execution? A: No. For one, all Signal operations have well-defined semantics and ordering, and will not differ among conformant implementations. At a higher level, Signals follow a certain set of invariants, with respect to which they are \"sound\". A computed Signal always observes the Signal graph in a consistent state, and its execution is not interrupted by other Signal-mutating code (except for things it calls itself). See the description above. Q: When I write to a state Signal, when is the update to the computed Signal scheduled? A: It isn't scheduled! The computed Signal will recalculate itself the next time someone reads it. Synchronously, a Watcher's notify callback may be called, enabling frameworks to schedule a read at the time that they find appropriate. Q: When do writes to state Signals take effect? Immediately, or are they batched? A: Writes to state Signals are reflected immediately--the next time a computed Signal which depends on the state Signal is read, it will recalculate itself if needed, even if in the immediately following line of code. However, the laziness inherent in this mechanism (that computed Signals are only computed when read) means that, in practice, the calculations may happen in a batched way. Q: What does it mean for Signals to enable \"glitch-free\" execution? A: Earlier push-based models for reactivity faced an issue of redundant computation: If an update to a state Signal causes the computed Signal to eagerly run, ultimately this may push an update to the UI. But this write to the UI may be premature, if there was going to be another change to the originating state Signal before the next frame. Sometimes, inaccurate intermediate values were even shown to end-users due to such glitches. Signals avoid this dynamic by being pull-based, rather than push-based: At the time the framework schedules the rendering of the UI, it will pull the appropriate updates, avoiding wasted work both in computation as well as in writing to the DOM. Q: What does it mean for Signals to be \"lossy\"? A: This is the flipside of glitch-free execution: Signals represent a cell of data--just the immediate current value (which may change), not a stream of data over time. So, if you write to a state Signal twice in a row, without doing anything else, the first write is \"lost\" and never seen by any computed Signals or effects. This is understood to be a feature rather than a bug--other constructs (e.g., async iterables, observables) are more appropriate for streams. Q: Will native Signals be faster than existing JS Signal implementations? A: We hope so (by a small constant factor), but this remains to be proven in code. JS engines aren't magic, and will ultimately need to implement the same kinds of algorithms as JS implementations of Signals. See above section about performance. Why are Signals designed this way? Q: Why doesn't this proposal include an effect() function, when effects are necessary for any practical usage of Signals? A: Effects inherently tie into scheduling and disposal, which are managed by frameworks and outside the scope of this proposal. Instead, this proposal includes the basis for implementing effects through the more low-level Signal.subtle.Watcher API. Q: Why are subscriptions automatic rather than providing a manual interface? A: Experience has shown that manual subscription interfaces for reactivity are un-ergonomic and error-prone. Automatic tracking is more composable and is a core feature of Signals. Q: Why does the Watcher's callback run synchronously, rather than scheduled in a microtask? A: Because the callback cannot read or write Signals, there is no unsoundness brought on by calling it synchronously. A typical callback will add a Signal to an Array to be read later, or mark a bit somewhere. It is unnecessary and impractically expensive to make a separate microtask for all of these sorts of actions. Q: This API is missing some nice things that my favorite framework provides, which makes it easier to program with Signals. Can that be added to the standard too? A: Maybe. Various extensions are still under consideration. Please file an issue to raise discussion on any missing feature you find to be important. Q: Can this API be reduced in size or complexity? A: It's definitely a goal to keep this API minimal, and we've tried to do so with what's presented above. If you have ideas for more things that can be removed, please file an issue to discuss. How are Signals being standardized? Q: Shouldn't we start standardization work in this area with a more primitive concept, such as observables? A: Observables may be a good idea for some things, but they don't solve the problems that Signals aim to solve. As described above, observables or other publish/subscribe mechanisms are not a complete solution to many types of UI programming, due to too much error-prone configuration work for developers, and wasted work due to lack of laziness, among other issues. Q: Why are Signals being proposed in TC39 rather than DOM, given that most applications of it are web-based? A: Some coauthors of this proposal are interested in non-web UI environments as a goal, but these days, either venue may be suitable for that, as web APIs are being more frequently implemented outside the web. Ultimately, Signals don't need to depend on any DOM APIs, so either way works. If someone has a strong reason for this group to switch, please let us know in an issue. For now, all contributors have signed the TC39 intellectual property agreements, and the plan is to present this to TC39. Q: How long is it going to take until I can use standard Signals? A: A polyfill is already available, but it's best to not rely on its stability, as this API evolves during its review process. In some months or a year, a high-quality, high-performance stable polyfill should be usable, but this will still be subject to committee revisions and not yet standard. Following the typical trajectory of a TC39 proposal, it is expected to take at least 2-3 years at an absolute minimum for Signals to be natively available across all browsers going back a few versions, such that polyfills are not needed. Q: How will we prevent standardizing the wrong kind of Signals too soon, just like {{JS/web feature that you don't like}}? A: The authors of this proposal plan to go the extra mile with prototyping and proving things out prior to requesting stage advancement at TC39. See \"Status and development plan\" above. If you see gaps in this plan or opportunities for improvement, please file an issue explaining.",
    "commentLink": "https://news.ycombinator.com/item?id=39886328",
    "commentBody": "A proposal to add signals to JavaScript (github.com/proposal-signals)249 points by beeman 16 hours agohidepastfavorite246 comments itsjustme2 13 hours agoAm I the only one that thinks the vanilla js example is actually easier to read and work with? - \"The setup is noise and boilerplate heavy.\" Actually the signals example looks just as noisy and boilerplate heavy to me. And it introduces new boilerplate concepts which are hard for beginners to understand. - \"If the counter changes but parity does not (e.g. counter goes from 2 to 4), then we do unnecessary computation of the parity and unnecessary rendering.\" - Sounds like they want premature memoization. - \"What if another part of our UI just wants to render when the counter updates?\" Then I agree the strawman example is probably not what you want. At that point you might want to handle the state using signals, event handling, central state store (e.g. redux-like tools), or some other method. I think this is also what they meant by \"The counter state is tightly coupled to the rendering system.\"? Some of this document feels a little repetitive. - \"What if another part of our UI is dependent on isEven or parity alone?\" Sure, you could change your entire approach because of this if that's a really central part of your app, but most often it's not. And \"The render function, which is only dependent on parity must instead \"know\" that it actually needs to subscribe to counter.\" is often not an unreasonable obligation. I mean, that's one of the nice things about pure computed functions- it's easy to spot their inputs. reply leononame 13 hours agoparentWhy do you think this is premature memoization? This is an example, boiled down to a simple function. Do you think people just came up with the use case for this without ever having needed it? I think an effort in standardizing signals, a concept that is increasingly used in UI development is a laudable effort. I don't want to get into the nitty gritty about what is too much boilerplate and whether you should build an event system or not, but since signals are something that is used in a variety of frameworks, there might be a good reason to it? And why not make an effort and standardize them over time? reply spacechild1 1 hour agorootparent> a concept that is increasingly used in UI development For a desktop app developer that's a pretty funny statement, given that the Qt framework introduced signals and slots in the mid 90s. I am curious how many web devs think that signals are a new concept. (I don't necessarily mean the parent poster.) reply flohofwoe 30 minutes agorootparentprev> ...but since signals are something that is used in a variety of frameworks... ...common usage is not really a justification for putting it into the language standard though. Glancing over the readme I'm not seeing anything that would require changes to the language syntax and can't be implemented in a regular 3rd-party library. In a couple of years, another fancy technique will make the rounds and make signals look stupid, and then we are left with more legacy baggage in the language that can't be removed because of backwards compatibility (let C++ be a warning). reply Zababa 19 minutes agorootparentFrom what I understand, a few/many of the big frameworks are converging on signals, and another commenter said that Qt had signals in the 90s https://news.ycombinator.com/item?id=39891883. I understand your worries, and I would appreciate some wisdom from non-JS UI people, espcially if they have 20+ years of experience with them. reply 38 11 hours agorootparentprev> I don't want to get into the nitty gritty about what is too much boilerplate and whether you should build an event system or not You're basically saying you want this thing, but you don't want to have to justify it reply shermantanktop 10 hours agorootparentThe rationale for it is the fact that multiple frameworks provide their own versions of this mechanism. The proposal is to relocate extremely popular and common functionality from framework space to the language/runtime space. The popularity of React is itself the rationale for the utility of this idea, and any terse version of the rationale is for show. Is that a good enough rationale? Maybe, maybe not, but you are shooting the messenger. reply refulgentis 10 hours agorootparentMost importantly: OP is right re: vanilla example is most legible. Reading the proposal, I have no idea what this \"Signal\" word adds other than complexity. Less important: I really, really, really, really, am reluctant to consider that is something that needs standardizing. Disclaimer: I don't have 100% context if this concept is _really_ the same across all these frameworks. But frankly, I doubt it, if it was that similar, why are there at least a dozen frameworks with their own version?* Also, I've lived through React, Redux, effects, and so on becoming Fundamentally Necessary, until they're not. Usually when it actually is fundamental you can smell it outside of JS as well. (ex. promisesfutures). I've seen 1000 Rx frameworks come into style and go out of style, from JS to Objective-C to Kotlin to Dart. Let them live vibrant lives, don't tie them to the browser. * I know that's begging the question, put more complex: if they are that similar and that set in stone that its at a good point to codify, why are there enough differences between them to enable a dozen different frameworks that are actively used? reply eyelidlessness 6 hours agorootparent> Disclaimer: I don't have 100% context if this concept is _really_ the same across all these frameworks. Very nearly[1] every current framework now has a similar concept, all with the same general foundation: some unit of atomic state, some mechanism to subscribe to its state changes by reading it in a tracking context, and some internal logic to notify those subscriptions when the state is written. They all have a varied set of related abstractions that build upon those fundamental concepts, which… > But frankly, I doubt it, if it was that similar, why are there at least a dozen frameworks with their own version?* … is part of what distinguishes each such framework. Another part is that state management and derived computations are only part of what any of the frameworks do. They all have, beyond their diverse set of complementary reactive abstractions, also their own varied takes on templating, rendering models, data fetching, routing, composition, integration with other tools and systems. Moreover, this foundational similarity between the frameworks is relatively recent. It’s a convergence around a successful set of basic abstractions which in many ways comes from each framework learning from the others. And that convergence is so pervasive that it’s motivating the standardization effort. This especially stands out because the reference polyfill is derived from Angular’s implementation, which only very recently embraced the concept. From reading the PR notes, the implementation has only minor changes to satisfy the proposed spec. That’s because Angular’s own implementation, being so recent, internalizes many lessons learned from prior art which also inform the thinking behind the spec itself. This is very much like the analogy to Promises, which saw a similar sea change in convergence around a set of basic foundational concepts after years of competing approaches eventually drifting in that same direction. [1]: Most notably, React is unique in that it has largely avoided signals while many frameworks inspired by it have gravitated towards them. reply merb 2 hours agorootparentwhat makes useState different than signals?! reply Offler 0 minutes agorootparentSignals are fine grained reactivity. React is coarse grained reactivity. Legend-state adds signals to React and I'd recommend it over Redux/zustand which we used to use. veidelis 1 hour agorootparentprevExplicit vs implicit dependencies (useEffect vs Signal.Computed/effect) and the fact that signals in contrast to useState can be used outside of react context which I assume is a good thing. I personally mostly prefer more explicit handling of \"observable values\" where function signatures show which signals/observables are used inside them. reply troupo 3 hours agorootparentprev> why are there enough differences between them to enable a dozen different frameworks that are actively used? Because they are not in the standard library of the language? Because they all arrived at the solution at different times and had to adapt the solution to the various idiosyncratic ways of each library? Because this happen in each and every language: people have similar, but different solutions until they are built into the language/standard library? reply catlifeonmars 8 hours agorootparentprev> But frankly, I doubt it, if it was that similar, why are there at least a dozen frameworks with their own version?* Welcome to the fashion cycle that is JavaScript. Given a few years, every old concept gets reinvented and then you have half a dozen frameworks that are basically the same but sufficiently different so that you have to relearn the APIs. This is what I think standardization helps circumvent A good standard library prevents fragmentation on ideas that are good enough to keep getting reinvented reply MrJohz 3 hours agorootparentprev> But frankly, I doubt it, if it was that similar, why are there at least a dozen frameworks with their own version?* To answer this specifically: signals are a relatively low-level part of most frameworks. Once you've got signals, there are still plenty of other decisions to make as to how a specific framework works that differentiate one framework from another. For example: * Different frameworks expose the underlying mechanism of signals in different ways. SolidJS explicitly separates out the read and write parts of a signal in order to encourage one-way data flow, whereas Vue exposes signals as a mutable object using proxies to give a more conventional, imperative API. * Different frameworks will tie signals to different parts of the rendering process. For example, typically, signals have been used to decide when you rerender a component - Vue and Preact (mostly) work like this. That way, you still have render functions and a vdom of some description. On the other hand frameworks like SolidJS and Svelte use a compiler to tie signal updates directly to instructions to update parts of the DOM. * Different frameworks make different choices about what additional features are included in the framework, completely outside of the signal mechanism. Angular brings its own services and DI mechanism, Vue bundles a tool for isolating component styles, SolidJS strips most parts away but is designed to produce very efficient code, etc. So in total, even if all of the frameworks shared the same signals mechanism, they'd all still behave very differently and offer very different approaches to using them. As to why different frameworks use different implementations as opposed to standardising on a single library, as I understand it this has a lot to do with how signals are currently often tied to the component lifestyle of different frameworks. Because signals require circular references, it's very difficult to build them in such a way that they will be garbage collected at the right time, at least in Javascript. A lot of frameworks therefore tie the listener lifecycle to the lifecycle of the components themselves, which means that the listeners can be destroyed when the component is no longer in use. This requires signals to typically be relatively deeply integrated into the framework. They reference this a bit in the proposal, and mention both the GC side of things (which is easier to fix if you're adding a new primitive directly to the engine), and providing lots of hooks to make it possible to tie subscriptions to the component lifecycle. So I suspect they're thinking about this issue, although I also suspect it'll be a fairly hard problem. Fwiw, as someone who has worked a lot with signals, I am also somewhat sceptical of this proposal. Signals are very powerful and useful, but I'm not sure if they, by themselves, represent enough of a fundamental mechanism to be worth embedding into the language. reply ksherlock 11 hours agoparentprevI agree. But look at Preact's signal documentation - https://preactjs.com/guide/v10/signals \"In Preact, when a signal is passed down through a tree as props or context, we're only passing around references to the signal. The signal can be updated without re-rendering any components, since components see the signal and not its value. This lets us skip all of the expensive rendering work and jump immediately to any components in the tree that actually access the signal's .value property.\" \"Signals have a second important characteristic, which is that they track when their value is accessed and when it is updated. In Preact, accessing a signal's .value property from within a component automatically re-renders the component when that signal's value changes.\" I think it makes a lot more sense in a context like that. reply andrewstuart 11 hours agorootparent>> In Preact, when a signal is passed down through a tree as props or context, I have found that passing props makes React-like applications very complex and messy and props are to be avoided as must as practical. The mechanism for avoiding props is Custom events. It concerns me to see the concept of signals being passed as props when surely signals/events should be removing the need for props? reply Pufferbo 10 hours agorootparentYou don’t need to pass a Preact signal as a prop to get reactivity. If you’re using Preact, signal references will make your component reactive by default, and if you’re using React you can introduce reactivity by way of the useSignals hook or a Babel plugin. (1) React signals have become my go to state management tool. So easy to use and very flexible. 1: https://www.npmjs.com/package/@preact/signals-react reply andrewstuart 9 hours agorootparent>> React signals have become my go to state management tool. I've ditched almost all state in my React apps except state local to the component. Custom events do all the work for passing information around the application and directing activity. What do signals give me that events do not? reply Pufferbo 8 hours agorootparentI’m also a fan of local state, but there are some cases where it makes sense for a bit of global state - mainly user context. However you can use signals for local state as well and they work amazingly. Being able to assign a new value to a signal without having to go though a setter is a way cleaner pattern, in my opinion. The other use cause is for communication between micro frontends. It’s so nice to just be able to import/export a signal and get its reactivity. Before them, I would create a pub/sub pattern and that’s just not as clean. reply flohofwoe 32 minutes agoparentprevI was wondering about this awkward code: counter.set(counter.get() + 1), 1000); One would think that proper integration into the language also means getting rid of those \"noisy\" setter/getter calls. reply catlifeonmars 9 hours agoparentprevI think it’s worth avoiding a change in design when you pass some threshold of complexity. The vanilla JS approach has some scaling limitation in term of state graph complexity, and the problem isn’t the ergonomics above and below the threshold, but discontinuous change in ergonomics when you cross that threshold reply aporetics 7 hours agorootparentWell said reply vundercind 5 hours agoparentprevI dislike both examples but find the Signals one far worse, no question. You can do it that way, but… why? When you could just not? reply LittleDan 13 hours agoparentprevI think there is room for improvement in how we explain this. The problems aren’t really visible in this small sample and comes up more for bigger things. PRs welcome. reply briantakita 11 hours agorootparentPerhaps mentioning the tradeoffs between a simple easy to explain example vs a more obvious comprehensive example. With links to more complex code bases? With a before & after? reply troupo 3 hours agorootparentprevI wouldn't be surprised if Ryan Carniato already has a perfect explanation somewhere :) reply duxup 13 hours agoparentprevI agree the initial example is easier to read, but it has problems as stated. reply briantakita 13 hours agoparentprevSince reactivity is not baked into Javascript. Adding reactivity is going to add abstraction overhead. It's meant to be used if it's needed. Not necessarily a default way to work with state. In my experience, the big benefit is the ability to make reactive state modular. In an imperative style, additional state is needed to track changes. Modularity is achieved using abstraction. Only use when needed. > Sounds like they want premature memoization It's a balance to present a simple example that is applicable. Cases where reactivity have a clear benefit tend to be more complex examples. Which is more difficult to demonstrate than a simple, less applicable example. reply tipiirai 6 hours agoparentprevDefinitely not. I prefer the vanilla version as well. reply fwlr 6 hours agoprevWhen they added Promises to JavaScript, I bristled at the thought that I might have to start writing `new Promise` everywhere. In practice, I can count on two hands the number of times I’ve written `new Promise`. What did happen, though, is I started to write `.then` a whole lot more, especially when working with third party libraries. In the end, the actual day-to-day effect of the Promise addition to JavaScript was it gave me a fairly simple, usually solid, and mostly universal interface to a wide variety of special behaviors and capabilities provided by third party libraries. Whether it’s a file read or an api request or a build step output, I know I can write `.then(res => …)` and I’m already 50% of the way to something workable. If this Signal proposal can do something similar for me when it comes to the Cambrian explosion of reactive UI frameworks, I am in favor! What’s more, maybe it will even help take reactivity beyond UI; I’ve often daydreamed about some kind of incrementally re-computed state tree for things other than UI state. reply jonathanlydall 3 hours agoparentI assumed the Promises was added primarily so that async/await could be added, which is where the really substantial quality of life improvement is. In practice you rarely need to explicitly go “new Promise” yourself. While .then in initial promises was a great improvement over nested delegates and is fine for simple chained promises, once you start conditionally chaining different promises or need different error handling for particular chains in the promise, or wanting to do an early return, the code can become much harder to read and work with. With async/await though you just write the call essentially as if it’s not a promise and can easily put try/catch around particular promise calls, easily have early returns, etc. reply esperent 3 hours agorootparent> In practice you rarely need to explicitly go “new Promise” yourself However you do quite often need to use Promise.all, even when using async/await. reply 9dev 1 hour agorootparentAlso, wrapping callback APIs or containing side effects: function sleep(ms) { return new Promise((resolve) => setTimeout(resolve, ms)); } await sleep(100); There are some use cases for the Promise class still, and it’s great to have that kind of control facility at hand when you need it. reply MaxBarraclough 1 hour agorootparentprevI don't think that's an issue with JavaScript though. It's inherent complexity in your code's handling of concurrent operations, not incidental complexity arising from the language. reply recursive 2 hours agorootparentprevI've literally never needed to do that. What's a real world use case? reply 3m 2 hours agorootparentDoing multiple async tasks concurrently as opposed to in sequence. If you have never used this you have either worked on extremely simple systems or have been leaving a ton of perf gains on the table. reply Capricorn2481 54 minutes agorootparentOr, like most people, they don't work with JS outside of the frontend where fetching multiple things in parallel is rarely needed. reply mattdesl 2 hours agorootparentprevI often map a series of X to Y with Promise.all. For example, mapping a series of image URIs to loaded Image elements. It is shorter to write than a for of loop, and importantly, all images will be loaded in parallel rather than sequentially, which can be significantly faster. images = Promise.all(uris.map(loadImage)) reply nikitaga 8 hours agoprevWhy does it need to be a part of the language? This could be a library. There are such libraries. They are small, so including them in your code is no big deal. Adding this to the language should not even be a goal. Thinking that the current crop of JS UI libraries designed their signals in such a good way that it needs to become a part of the language is hubris. Signals have many possible implementations with different tradeoffs, and none of them deserve to have a special place in the JavaScript spec. Before these libraries used signals, they or their predecessors used virtual DOM. Luckily, that didn't become part of JS, but how are signals any different? They aren't. The argument for making them standard is even worse than for virtual DOM. Are we just going pile every fad into a runtime that basically has no way to dispose of no-longer-wanted features without breaking the web? That is quite short sighted. reply Too 3 hours agoparentOne good argument for standardizing Signals is that debugging them seems to be a nightmare. Imagine a deep tree of calculated signals firing off each other and you need to find the source of what started the chain reaction. Standardizing will allow devtools to develop around it. reply wruza 2 hours agorootparentThey don't fire off each other, they simply depend on each other like functions do: a = () => 42 b = () => a() - 1 c = () => a() + b() * 2 It isn't a bigger nightmare than debugging pure functions. The source for `c` is `a` and `b`. All signal values (as proposed) will be lexically available in a body of a dependent signal, so there's no hidden registry to navigate anyway. If in-browser IDEs want to record a call tree for an activation record, they can do that without a standard. reply kabes 1 hour agoparentprevYou can say this about most things in the standard library. But as stated in the motivation, there's an ongoing trend to extend the rather small standard library js offers, so you don't need to have a package for each and every common task. You can argue about the need for this, but if we're going to extend the standard lib, then looking at what is popular is a good approach IMO. > Before these libraries used signals, they or their predecessors used virtual DOM Signals are not a replacement for virtual DOM. reply klabb3 5 hours agoparentprevGood points. We don’t want the wrong thing. But we want the right one! Reactive UI won. The main thing stopping me from using vanilla JS is the absolute explosion in complexity managing state for even small sized applications. To me, any reactive framework is better than vanilla, so perhaps there is a construct missing? Now that it’s been a decade or so, we should start thinking about possible cut-points for standardization. Like with promises, this could bring down complexity for extremely common use-cases, if done right. I think a better way to evaluate it would be: “would this proposal be used by existing reactive frameworks?”. If not, why? What’s missing? What’s superfluous? What about lessons from UI annd reactivity from other languages? There’s a lot of fragmented experience to distill, but it’s a worthwhile endeavor imo. reply nikitaga 4 hours agorootparentIf you want to do \"the right thing\" – implement your proposal as a library, AND convince people to use it on its own technical merits. Then when everyone uses it (because it's so obviously \"the right thing\"), you can start asking if anyone wants your library to be built into the language. But that's not what you're doing. You're gunning for becoming the standard from the start – you are trying to convince people to use your draft implementation based on its status as a proposed standard, instead of them using it on its own technical merits. Ditch the status, and see if anyone still wants it. -- Yes, reactive UI won – just fine, without having signals in the JS standard. Because not having signals in the language was never an actual problem holding back reactive UI development in JS. Your proposal does not \"bring down complexity\". It simply moves the complexity from UI libraries into the JS standard. In doing that, you forcefully marry the ecosystem to that particular style of complexity. Every browser vendor will need to implement it and support it... for how many decades? And to what end? Unlike e.g. promises that are useful on their own, your proposal isn't nearly ergonomic enough to allow building reactive UIs in vanilla JS. Users will still need to use libraries for that, just like they do today. You're just moving one piece of such libraries into the standard, without building the case for why it's needed there. -- Your proposal spends pages selling the readers on signals, but that is not what you need to sell. We already have many implementations of signals. You need to sell why your (or any) signals implementation needs to be in the JavaScript standard. You have one tiny \"Benefits of a standard library\" subsection under \"Secondary benefits\" but it's just ridiculous. You're basically saying that we should add signals to JS because we've added (much simpler or more needed) things to JS before – is that really your best argument? And... \"saving bundle size\"? You want to bless one implementation of a complex problem to save what, 5KB of this: https://cdn.jsdelivr.net/npm/s-js Sorry, just – nothing about this makes sense to me. reply klabb3 3 hours agorootparentI don’t have a dog in the fight. I also don't see yet any mainstream support behind the proposal. So I don’t get why it has to be so heated? The main benefit is interop. Same with promises. You can implement all of promises with custom callbacks - in fact it’s trivial. But competing implementations don’t typically land on API compatibility simply because they’re solving the same problem. That causes a fractured ecosystem. Maybe interop could be important with signals? I think they should argue that, if so! > Users will still need to use libraries for that, just like they do today. Yes? But you reduce the lifting by the libs - ideally enabling a class of vanilla use-cases which can be made demonstrably improved. You could say querySelector was unnecessary because you can do it in lib. Or filter, or map. Standardization can cover std-lib like features too no? Doesn’t mean I am in favor. I think you should always default to no unless strong and consistent proven benefits. But why not have good faith arguments for what problems this will or won’t solve. For instance, if hypothetically let’s say react or svelte has a different model that cannot possibly use these signals, then that’s probably a sign it’s not good. My philosophy with proposals is balancing the curiosity and honest inquiry with a grumpy defensive inquisition before saying aye. Flaming though is really not helpful. > You're basically saying that we should add signals to JS because we've added (much simpler or more needed) things to JS before > saving bundle size Yes, I agree these are weak arguments. reply jonathanlydall 3 hours agorootparentprevI think you’re right, I don’t see how building this into the base library makes some things possible which weren’t before. I think that the Promise API was not the actual thing people directly wanted (and on its own had no compelling reason to be added to the base library), but a standardised Promise API was needed in order to add the hugely useful async/await keywords which are unachievable without changes to the language. I am however a big fan of a really good base library (it’s one of the things I love about working with .NET), but they should be focussing on functionality with the broadest reach (as in most encountered by average JS devs working on day to day tasks), e.g. things like better tools for working with dates and times. reply imbnwa 8 hours agoparentprevRemember the Observable proposal? reply tengbretson 7 hours agorootparentThe Observable proposal made a stronger case in my opinion, since Observables provide an interface that is functionally unique and useful for the boundaries between app logic and libraries, so a single standard approach has benefits. Signals on the other hand live right where application state binds to the ui. Is this really somewhere that people are patching together a hodge podge of libraries that need to use a consistent api? I'm not so sure. reply mg 15 hours agoprevWhen I need to signal something across my application, I use events: window.dispatchEvent(new Event('counterChange')); And every part of the application that wants to react to it can subscribe via window.addEventListener('counterChange', () => { ... do something ... }); Anything wrong with that? reply _the_inflator 14 hours agoparentHistorically, this example is the reason why the Web evolved into jQuery and from there forked into the world of Angular and React mainly. Event handling is getting messy very easily. If you want to get deeper into it, have a look at event bubbling and propagation. Large applications need a robust event handling. This is the nowadays hidden benefit of frameworks like Angular, Vue etc. Believe me, you don’t want to use the standard event handling API without a framework. Adding, deleting, cloning, firing, removing, fire once etc on many elements can have serious unwanted side effects. reply unemployable 11 hours agorootparent>If you want to get deeper into it, have a look at event bubbling and propagation. In this example, the event is on the Window. There is no bubbling. It is already at the top level. >Believe me, you don’t want to use the standard event handling API without a framework. Adding, deleting, cloning, firing, removing, fire once etc on many elements can have serious unwanted side effects. I don't know what this means. The frameworks do not have much to do with this topic. reply pquki4 10 hours agorootparentI think the previous comment discusses events in a general sense. Frameworks batch changes so that updates are efficient, and in many cases figures out the \"correct\" order of doing things. If you do all of those yourself in a large and complex UI, very likely you are updating the DOM less efficiently than what frameworks are doing, and very likely you introduced some subtle bugs. Speaking of that from my first-hand experience. reply andrewstuart 14 hours agorootparentprevI use events extensively in large applications and its never been a problem. In fact they solve complexity. reply beezlewax 14 hours agorootparentprevWhat kind of side effects specifically? reply doomroot 14 hours agorootparentI believe memory leaks to start reply unemployable 11 hours agorootparentMemory leaks can occur when a component adds events to an element outside of the component (such as the window) and then gets removed from the DOM without removing the event handler from the window. This is solved in native Web Components by the mount/unmount methods where you can run code to remove event listeners when the component has been unmounted. For other event listeners, they get removed when the DOM element is removed. The frameworks do not solve this to any greater degree. They also just make everything invisible and behind-the-scenes and hard to debug due to their declarative nature, but that is another topic. reply ngc6677 3 hours agorootparentExample with web components in all frameworks https://webcomponents.dev/blog/all-the-ways-to-make-a-web-co... and an other version to experiment with https://jsbin.com/yiviragiba/8/edit?html,css,js,console,outp... reply pcthrowaway 4 hours agorootparentprevRemoving event listeners upon some component being removed actually sounds like a great use case for the new FinalizationRegistry API[1] [1]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... reply Klonoar 12 hours agorootparentprevThis is one of those points where you need to link to some kind of data or conclusive result actually showcasing this, because you're arguing against a pattern that's been in browser ecosystems for decades. I've done this for larger applications and haven't experienced issues. Being charitable, the best I can imagine right now that'd cause memory leaks is someone running into the old school JS scoping issues and capturing something in handlers that they shouldn't. That's not the handler itself that's the problem, though - that's the developer. (Yes, we could rant on and on about the poor design decisions that JS has built in, but that's been beaten to death) reply falcor84 13 hours agorootparentprevAre you implying that there are memory leaks in browsers' internal implementation of events? Because my take is that the problem is with \"user space\" scripts not cleaning up after themselves, and I don't see how that would get better by adding yet another API to be mindful of. reply gibbitz 12 hours agorootparentI believe this would be more related to something like memoizing a DOM structure in a \"Live\" listener that is later removed from the DOM but not garbage collected due to the reference in the event listener. As the poster mentioned, developer error -- not a fundamental language or browser implementation flaw. reply kabes 12 hours agoparentprevSignals are also just pub/sub, but with a more ergonomic api. More ergonomic because listeners are added and released automatically. It can also be more performant, eg, say you have a computation that depends on 2 values: `result = a ? b : 0` Then if a is falsy, we don't need to recompute if b changes. This is achieved automatically with signals, but would require quite some code with classic pub/sub. reply zdragnar 14 hours agoparentprevAccording to TFA, event emitters / observables cause unnecessary work when called multiple times. The difference with signals is that the resulting value is only ever calculated when the end consumer reads the value- so you schedule render updates asynchronously from the actual writes to the signal, and whatever chain of computations the watchers perform is done just the one time during the render. Interim values sent to the signal will get lost, so you really can't do too much interesting work in them. It's really just a fancy abstraction layer to coordinate a rendering cycle. reply dakom 13 hours agoparentprevIt's often desirable for UI to be described in a declarative fashion, i.e. instead of where you have \"do something\" (set button color to red), refactoring so it becomes \"is something\" (button is red if state is x) I might not be describing that well, because once you go down that road it really becomes a whole overall approach that infects the whole program (like functional reactive programming), and so it's really about how the whole flow fits together from top to bottom, and that can be very elegant. I don't think that's the right fit for everything, i.e. in gamedev it might make more sense to just update some object's position imperatively, but for UI it tends to work pretty well. reply hk__2 15 hours agoparentprev> Anything wrong with that? It has all the downsides of the pub/sub architecture highlighted in the proposal. reply djbusby 14 hours agoparentprevI've been using patterns like this for more than a decade. The thing that's hard is, down the road you could have a listener, which triggers a other event, then another event, which comes back to the first routine and now you got a listen-loop that won't quit. And it's hard to ensure that all listener don't cause that trigger cascade. reply eddd-ddde 14 hours agorootparentIsn't it possible to define all listeners / publishers in a declarative way which can be compiled to catch for this issues? reply berkes 11 hours agorootparentYes. It is. And most pub/sub or Observer architectures and design patterns have the \"loop\" thing solved just fine. In fact, the GOF spend an entire paragraph on the problem of complex update semantics in \"design Patterns\" (1995) ch Observer p299. So, while it is a real problem, it's one that has been solved (for at least 29 years) reply djbusby 10 hours agorootparentYes, I want to mention, since I was up-thread, it is solved but not all tooling or environment are setup for that. When the app is small you likely don't need it. And then it grows and you absolutely do. It's better, in greenfield to use framework/tooling that is ready for it. reply jbreckmckye 9 hours agorootparentprevIn fact I solved a similar issue in my JS reactive micro library in just 500 bytes: https://github.com/jbreckmckye/trkl reply briantakita 8 hours agorootparentI was not aware of trkl. Well done! I wrote rmemo (Reactive Memo) which solves the same problems & is a similar size. Different semantics though. https://github.com/ctx-core/rmemo Nanostores is also small https://github.com/nanostores/nanostores. And if you only need reactivity in the browser (not server side), VanJS is small & includes reactive primitives. https://vanjs.org/ reply zaphar 8 hours agorootparentprevI like signals as primitive but in this particular case they make it roughly about as easy to create loops accidentally as events do. I don't think this problem gets better or worse with signals. reply explaininjs 14 hours agoparentprevThis pattern is precisely what everyone's favorite \"look, JS/electron can be high performance!\" example uses. (VS Code). reply LittleDan 13 hours agoparentprevLGTM! Smells like Redux (in a good way). But then ultimately at the root you probably want the event to update your “model”, and then that leads to an update of the “view”. This is the part where signals can be useful. reply fwlr 12 hours agoparentprevIf you mis-type ‘countenChange’ it could be quite frustrating! reply berkes 11 hours agorootparentIf that truly is your reason to choose or forego a software design pattern, than what the h. are you doing with JavaScript? reply everybackdoor 11 hours agorootparentprevIn 2024 we have linters and other static analysis tools for catching these kinds of things right in the IDE. reply lelanthran 14 hours agoparentprev> Anything wrong with that? Well, don't events only bubble upwards? You need to know the exact element of it is not on a lower level in the DOM tree. Events were too messy, so I wrote a small pub/sub message queue type of thing. Anyone anywhere in the DOM can subscribe to messages based on subject regexes. Makes things a lot easier, especially when I added web components to wrap existing elements so that publishing and subscribing is done with attributes, not js. reply everybackdoor 11 hours agorootparentYou can also fire events on pretty much any object, essentially creating a channel where the message bus queue is the vm event queue. reply lelanthran 5 hours agorootparent> You can also fire events on pretty much any object, Only if you have a reference to the object. The reason I made up my own message queue pub/sub is because events required a lot of complexity in acquiring the correct reference to the correct object or subtree in the DOM. With pub/sub type message queue, any element can emit a message with subject \"POST /getnetpage\" and the function listening for (subscribed) POST messages emits a RESPONSE message when the response comes back. This lets a third element listen (subscribe) for a \"RESPONSE FROM /getnextpage\" subject message, and handle it. None of the 3 parties in the above need to have a reference to each other, nor do they even have to know about each other, and I can inject some nice observability tools because I can just add a subscriber for specific message types, which makes debugging a breeze. reply nurple 14 hours agoparentprevWelcome to Node.js v21.6.2. Type \".help\" for more information. > window.dispatchEvent(new Event('counterChange')) Uncaught ReferenceError: window is not defined reply runarberg 13 hours agorootparent$ node Welcome to Node.js v20.6.1. Type \".help\" for more information. > const target = new EventTarget() undefined > target.dispatchEvent(new Event(\"counterChange\")) true reply nurple 13 hours agorootparenttyvm! reply extheat 7 hours agorootparentprevEven better on Node, you have EventEmitter. A much simpler interface. reply cyanydeez 14 hours agoparentprevMostly the teardown logic and inevitable memory leaks. A alt proposal would be some kind of auto remove listener if it goes out of context reply explaininjs 14 hours agorootparentThe new `using` feature handles RAII just fine. This proposal is entirely unnecessary given the existence of `using` and the already-existing event listeners. https://iliazeus.github.io/articles/js-explicit-resource-man... reply lelandfe 11 hours agorootparentQuite a good link, thanks. reply cyanydeez 10 hours agorootparentprevNice, but mixing typescript into a proposed standard just confusing. reply explaininjs 7 hours agorootparentThe standard doesn't have anything to do with TypeScript, not sure where you got that from? https://github.com/tc39/proposal-explicit-resource-managemen... reply gray_-_wolf 14 hours agorootparentprevMaybe a stupid question, but isn't the memory released anyway when I close the tab? So why do memory leaks matter? reply chrisweekly 6 hours agorootparentNot \"stupid\", but maybe hasty / shallow? Many SPAs are long-running, and memory leaks can accrue quickly, destroying performance. reply zdragnar 14 hours agorootparentprevIt depends on how much you're leaking, and how fast. Best case scenario, it just slows down garage collection a little bit, as you're holding into a lot of references that aren't going anywhere. On the other hand, I recall a bug in a particular version of AngularJS where component DOM nodes wouldn't get cleaned up when navigating with the router unless you manually set all of the scope values their templates used to null. We had a data dense application with tables and what not, and you could clearly watch memory jump tens or more megabytes flipping between pages in the chrome dev tooling. Eventually (this was a SPA intended to be used for upwards of hours at a time) the page would crash. reply dr_kiszonka 14 hours agorootparentprevNot a web dev person, but some \"tabs\" have a very long lifetime, e.g., webmail clients, Whatsapp, etc. reply Xenoamorphous 14 hours agorootparentprevHave you ever seen Chrome’s “aw, snap!” screen (if you use Chrome)? More often than not that’s due to a memory leak. reply leptons 14 hours agorootparentprevThe problem is if you don't close the tab, the memory leak can cause that one tab's memory to balloon to 1GB or more because memory isn't being released, when that tab should only be consuming 50MB. reply briantakita 13 hours agoparentprev> Anything wrong with that? It only works in browser environments. reply lelandfe 11 hours agorootparentNode has EventEmitter: https://nodejs.org/en/learn/asynchronous-work/the-nodejs-eve... See, for instance, https://www.electronjs.org/docs/latest/api/ipc-renderer reply 6510 7 hours agoparentprevYou have to name the function or you cant remove it :P reply ivan_gammel 15 hours agoparentprevDid you read the document? They have an example there, which is quite similar to yours, and explain what is the problem. reply explaininjs 14 hours agorootparentNo, they don't. In fact, if you search \"event\" in the proposal you get exactly one result, the prefix of \"eventually\". This is a serious shortcoming of the proposal that should be addressed. reply throwitaway1123 14 hours agorootparentI think the comment you're replying to is referring to the pub/sub sections of the proposal. They don't explicitly mention events, but events are a subset of the publish/subscribe pattern. reply explaininjs 14 hours agorootparentBut then so are signals. The only \"benefit\" signals as proposed here give you is less control over the exact dispatch pattern of the graph, for instance things like debouncing, throttling, batching, etc etc etc. Aka all the things you absolutely must have control over if you want to make something resembling a high performance application. reply naasking 14 hours agorootparentSignals are not a subset of events. Signals combine event subscription with value construction, which promotes a remote declarative model for updates. > The only \"benefit\" signals as proposed here give you is less control over the exact dispatch pattern of the graph, for instance things like debouncing, throttling, batching, Events don't give you any more control over those properties than signals, they just require more boilerplate. reply ivan_gammel 14 hours agorootparentprevOh, come on. They do have an example of architecture, not literal example of events. It doesn’t matter what trivial implementation of Observer pattern you choose. reply explaininjs 14 hours agorootparentExcept one is already built into the language? And it provides all the upsides of signals with none of the downsides? (Modulo proper use of `using` directives) reply ahmedfromtunis 14 hours agorootparentprevCan you please point to where in the document such an example was mentioned? I rechecked the document and couldn't find it. reply ivan_gammel 14 hours agorootparentJust look for the example of a design pattern („Example - A VanillaJS Counter“ section), not for a literal implementation of it via events. Conceptually they are the same. reply tambourine_man 14 hours agoprevI’ve been trying for decades to understand why people find it so hard to keep track of state and update the DOM. Sure, it requires a bit of discipline, but it’s vastly simpler to me than whatever solution comes up every few years (Backbone, Knockout, Angular, React, modifying the language itself, etc). There must be something profoundly different with the way I think. It even expresses itself in the function naming. They call updating innerText “render”. You’re not rendering anything. At most, the browser is, but so is everything else it does related to painting. It feels like a desperate attempt to complicate what is one of the simplest DOM functions. It really baffles me. reply duxup 14 hours agoparentIn simple applications it is easy. More complex it is not easy. reply tambourine_man 14 hours agorootparentI’ve been writing web apps for easily 25+ years. Never have I reached for React and friends voluntarily. But again, I know I’m in a minority. I’m just not completely sure why. reply duxup 14 hours agorootparentYou could work for 50 years and never need those… it just depends on what you’re creating / how many devs and so on. I know some guys who over the years wrote their own framework. It works great… for them. reply tambourine_man 13 hours agorootparentRight, the only thing that convinces me is team and hiring dynamics. But that’s not what these tools advertise. It’s always like: you have dozens of interactive controls in this view, it’s getting out of hand, you should use this language that compiles to HTML and JavaScript and carry all these dependencies. To which I always reply: no thanks, I rather deal with the dozens of controls. reply duxup 13 hours agorootparent> team and hiring dynamics. But that’s not what these tools advertise. I think that’s a given for any proposed standard. We all get a common way we understand things and can even just communicate about a thing. reply tambourine_man 13 hours agorootparentMy argument is that it’s not their selling point. When you go to React’s page you’re not greeted with: “React is a great way to hire devs and manage a team”. The solution they sell is technical, like state management, reusable components, etc. Which I don’t find convincing. reply pvg 13 hours agorootparentprevFB's website has settings subpages that are more complicated than an average web app and is an SPA-style-mega-app made of piles of other apps. It typically keeps highly consistent state throughout. There's no doing that sanely by hand, you'll just forget something or, more likely, it will get lost between the dozens upon dozens of people needed to build such a thing. reply tambourine_man 13 hours agorootparent1) Almost no one is building FB or Gmail, yet act like it. The precise reason why still escapes me. 2) For other use cases, it’s not that hard to manually update some elements in the DOM. You very quickly learn how not shot yourself in the foot. Certainly a lot easier (and faster) than dealing with the mess that is the React ecosystem. reply mardifoufs 12 hours agorootparentYou don't need to be building Facebook or Google to have a TON of state in a webpage. Obviously if you are doing a basic dashboard, a blog or something similar, it isn't really needed. But for more complex stuff I think react provides a much better way to handle state changes than just using vanilla js. It's not like using react makes an app more complex, because if you wanted to do the same thing in vanillajs you'd end up with a much bigger mess. I agree that some web pages don't need any of that, but those don't usually require a lot of development anyways. reply xg15 9 hours agorootparentCould you give a concrete example for the more complex stuff? reply mardifoufs 5 hours agorootparentSure! Where I work we build very specialized, very low user count live remote non destructive testing software that can be run on a browser. Well, it used to be native, but clients complained a lot in part due to IT restrictions on updates. That means that we basically have to have a remote processing unit, sync the state with the front end at all time, and with the scanning equipment too. That means multiple different canvases and charts, different tabs etc that all contain state, need state to be updated sometimes in the background, and need to have consistent state changes. I don't do a lot of front end dev, but I did help in setting up our new inference module there as I'm in the AI/ML side and it would've been a nightmare to deal with even if I mostly did stuff in a webgl canvas. It was angular, which I don't really like, but is still much better than going vanilla. I know it's pretty niche, but I'd say most non-trivial (blogs, CMS, forms) front ends handle a lot of state. If they don't, then they are relatively simple anyways. That's a generalization but still, you quickly hit the point where react/other framework becomes worth the complexity overhead versus the complexity of doing it in vanillajs reply pcthrowaway 4 hours agorootparentprevMaybe try making something like Grafana's dashboard by hand. reply pquki4 9 hours agorootparentprevIf you are working in a team with 5+ developers who work on the UI where people need to be able to quickly reuse components and put them in large, complicated applications with lots of data that could be fetched and updated with HTTP requests, it is almost impossible not to use any of those frameworks. If you are working on your own, or only create small web apps, sure, you can avoid frameworks in some cases. reply tambourine_man 8 hours agorootparent> it is almost impossible not to use any of those frameworks. Agreed, but because it’s a lot easier to just search for “React programmer” these days than it is to evaluate lots of JavaScript candidates, which has a much wider scope and proficiency level and make sure they’ll fit right in when hired. But not because direct DOM manipulation is not inherently scalable. See puter[1] for instance, a fairly complex, 100k+ lines of code of jQuery. https://github.com/HeyPuter/puter reply __david__ 12 hours agorootparentprevDo you just use document.createElement(), getElementById(), and friends? I've written lots of web apps that just used those. But I've also jumped into React a couple times to learn if it's better or faster or whatever. I generally think it's a reasonable approach for a template style web app—ie, when you need to build a whole lot of html nodes that interact with each other in the way a complicated ui does. But I do find I can get stuck trying to reason about some of the weird reactivity stuff with useState() and useEffect(). I kinda chalk that up to me being not an expert, but it also feels like a bit of an impedance mismatch with the language itself. But I don't think React necessary for _every_ app and it really depends on what kind of apps you are making. Certainly you can do the original style of app where the templating is on the server and any js is just to hook up already existing nodes. The js community has more or less moved away from that \"rails\" style of app years ago… reply jbreckmckye 9 hours agorootparentprevI think it would help if you mentioned the kinds of applications you're building. What do you mean by \"web apps\" here? My memory of the web in 1999 was that the only rich web UX was in Java applets reply tambourine_man 8 hours agorootparentThere was this thing called DHTML. It was no Ajax, but you could hide and show elements, change its contents, respond to events, etc. Cross browser was a nightmare. And the server side handled most of the work. Today I build all sorts of things with lots of interactive elements on the frontend, but trying to avoid using React if I can. reply dclowd9901 14 hours agoparentprevDecades is a long time. You must remember the complexities of updating the DOM between browsers, surely. Keeping the DOM in sync with a data state isn’t too difficult but doing so in a highly performant (60 fps) way _is_ tremendously difficult. Especially when it comes to creating APIs that aren’t leaky but also not too cumbersome. It would frankly be easier to just paint pixels to a canvas game-style, than translating changes to a living DOM tree. reply tambourine_man 14 hours agorootparentIf you need 60fps you shouldn’t use the DOM. It’s not a game engine. Like you said, go with canvas. reply pquki4 9 hours agorootparent60fps is not a high bar. A timer that gets updated every 10 millisecond (or in truth whatever the next event loop interval) is commonly used as tutorial, and there is no reason such a widget does not have a high refresh rate. The point of the original comment is that DOM updates should be fast, efficient and avoid visible delays to user's eye, which is not easy. If you are not careful, small changes in a large application could lead to too many DOM updates -- that is where frameworks shine. And canvas is not the solution to everything and has its own problems. To begin with, accessibility. reply tambourine_man 8 hours agorootparentI agree with all of it. I did recently a timeline with hundreds of elements, 3D transforms etc. Everything was smooth and not because I was particularly clever, but because browsers are fast. All direct DOM manipulation. My complaint with modern frameworks is not FPS, but rather that a hello world often requires 20k files plus a compilation step and the upshot isn’t particularly clear to me. I’ve never been to one of its landing pages and thought: wow, that’s a problem I have and this looks like the perfect solution. Conversely, I clearly remember the first time I saw jQuery. Loading content via Ajax and fading in when done, all in 3 lines of code and cross browser. I was sold at that very second. reply __s 15 hours agoprevPromises are a nice success story, but without async/await it wasn't really necessary to standardize > The current draft is based on design input from the authors/maintainers of Angular, Bubble, Ember, FAST, MobX, Preact, Qwik, RxJS, Solid, Starbeam, Svelte, Vue, Wiz, and more… Would be interested what existing library authors think of this proposal. Interesting that React is not in that list Signals are a bit like channels, except they're broadcast instead of single receiver. It'd be neat if this could somehow be leveraged to allow web workers to communicate with channels instead of onMessage callbacks. Specifically being able to `select` over signals/channels/promises like in Go would over a syntactic benefit over having to try manage multiple concurrent messaging mechanism with callbacks (maybe by allowing signals to be included in `Promise.any`) reply bastawhiz 13 hours agoparent> without async/await it wasn't really necessary to standardize Hard disagree. `x instanceof Promise` simply doesn't work. If my library has a then method that accepts a catch callback and yours doesn't, they're silently non-interoperable, and there's no way to detect it. When does `finally` run? What expectations can you have around how async the callbacks are? Without a standard, every single library that uses promises needs to bring its own polyfill because you can't trust what's there. And you can't actually consume any other library's promises, because you can't trust that they behave in the way you expect them to. And I'm not just speculating, this was reality for many years and a hell that many of us had to endure. reply watson 15 hours agoparentprev> Promises are a nice success story, but without async/await it wasn't really necessary to standardize One benefit of standardisation that's not tied to async/await is that the JavaScript engines has been able to do performance optimisations not otherwise possible which benefit Promise-heavy applications reply pavlov 15 hours agoparentprev> “Interesting that React is not in that list” Signals are not a part of the core React API, unlike Preact. My vague gut feeling is that signals are too much like a generalized useEffect() and would only introduce further confusion into React by muddling what happens during the render cycle. For better and worse, React takes a different tack to updates than signals do. But maybe I’m wrong about their applicability. reply Fatalist_ma 13 hours agorootparentThere is an interesting debate about React and signals in the comments of this article, between Dan Abramov and Ryan Carniato - https://dev.to/this-is-learning/react-vs-signals-10-years-la... reply pavlov 12 hours agorootparentI read it until the point where he defends the idea that these two functions obviously do something completely different: function One(props) { const doubleCount = props.count * 2; return Count: {doubleCount}; } function Two(props) { return Count: {props.count * 2}; } It honestly made me wonder whether the article was dated April 1 and I’d been had. More generously, JS framework design is hard. If you’re ambitious at all, you end up fighting the language and your runtime paradigms will hang like ill-fitting clothes on its syntax. The One/Two example above shows how easily expectations break in this world of extensions to extensions. There’s no way to know what an apparently simple piece of code will actually do without knowing the specifics of a given framework. reply eyelidlessness 8 hours agorootparent> I read it until the point where he defends the idea that these two functions obviously do something completely different > [code] > It honestly made me wonder whether the article was dated April 1 and I’d been had. They don’t do anything different if your model of components is that they rerun. But that model is only one way to implement components, and JSX is unopinionated about semantics exactly like this. Intentionally, by design. If you’re only familiar with React and other frameworks with a similar rendering model, of course it’ll be surprising that those two functions would behave differently. But if you’re familiar with other JSX implementations like Solid, you’ll spot the difference right away: components don’t rerun, only the JSX does. The first function will always render the same thing because `doubleCount` is set up on component creation and static for the remainder of the time the returned div is mounted. You are welcome to prefer React’s model. It certainly has some cognitive advantages. But it’s not inherently the only correct model either. reply __s 9 hours agorootparentprev> There’s no way to know what an apparently simple piece of code will actually do without knowing the specifics of a given framework. Yes. In solid-js JSX interpolation needs to be read as having implicit lambdas. You need to know how a framework works to use it It's somewhat what the discussion was getting at between Ryan & Dan. solid-js having fine grained reactivity involves a lot of lambdas, in JSX they're implicit, but code outside of JSX has to spell them out explicitly reply recursive 5 hours agorootparentprevThere are those of us for which this actually makes sense. More sense than react anyway. reply whizzter 14 hours agorootparentprevMy feeling is that it's philosophically outside the purview of React whose focus is rendering (and components and their state but not global state), RxJS and and MobX are both usable with React and have signals whilst Redux goes another route and React is \"above\" that choice. reply troupo 3 hours agorootparentRendering in React would benefit greatly from fine-grained reactivity (which what signals offer). However, for some reason the React team insists that the lowest level of reactivity in their system os the component, no matter how large or complex it is. reply KRAKRISMOTT 15 hours agoparentprevMaybe they can call it \"EventEmitter\" https://nodejs.org/en/learn/asynchronous-work/the-nodejs-eve... reply __s 14 hours agorootparentThat looks like it fits with the theme, but events are difficult to compose & tend to have easy leaks by requiring explicit attach/detach (not that I'm sure the proposal here addresses those issues) reply pcthrowaway 4 hours agorootparentCould you not just compose a new EventEmitter constructor that uses the new FinalizationRegistry API to drop all handlers of subjects which have since been garbage collected? reply dclowd9901 14 hours agoparentprevReact isn’t in this list because its effects are declarative, not imperative (except for props changes and re-renders which you could argue are in fact declarative, just one abstraction removed). UseEffect neatly compartmentalizes imperative behavior. This looks a lot like ember data binding which becomes an imperative nightmare. Its default state is “foot gun” with tons of cognitive overhead and meta patterns to keep it from getting that way. reply junon 12 hours agoprevRelated is S.js: https://github.com/adamhaile/s I love signals. I prefer them when making UIs over any other primitive (besides, perhaps, the cassowary constraint algorithm). I try to replicate them in every language I use, just for fun. I also don't believe they belong in the Javascript language whatsoever. Let the language be for a while, people already struggle to keep up with it. TC-39 is already scaring away people from the language. reply lambdaba 15 hours agoprevThis looks very much like mobx, which is my favorite JS effect system. Here is the mobx version: import { observable, computed, autorun } from 'mobx'; const counter = observable.box(0); const isEven = computed(() => (counter.get() & 1) === 0); const parity = computed(() => isEven.get() ? \"even\" : \"odd\"); autorun(() => { element.innerText = parity.get(); }); // Simulate external updates to counter... setInterval(() => counter.set(counter.get() + 1), 1000); reply kabes 12 hours agoparentWell, mobx is signals. But signals where the dependencies are tracked implicitly via the proxy ovject, instead of explicitly by a getter. reply meindnoch 12 hours agoprev\"let's bake my current framework du jour into the standard library!\" It's a bit like tattooing your girlfriend's name onto yourself. reply gloosx 2 hours agoprevTo be as a long-time react.js user – these examples look like a kinda weird mix of declarative with some bitter imperatives. Like, foo.set depending on foo.get and having to manually set element innerText inside the side-effect, eww, I can only imagine how messy it can get for a somewhat more complex application. React boilerplate for this case looks so much better in my opinion, take a look ``` function Component() { const [counter, tick] = useReducer(st => st + 1, 0) useEffect(() => setInterval(tick, 1000)) return counter % 2 ? 'odd' : 'even' } ``` Three lines, declarative, functional, noice. reply recursive 2 hours agoparentFunctional usually means the output can only depend on the input. But this is depending on some external state getting smuggled in through a hook. FWIW some people prefer the alternatives over this. reply gloosx 1 hour agorootparentThis not really correct, the useReducer hook is not some external state which is smuggled in. It is an actual input for the reconciler which defines an output of this component, so it is perfectly functional even by your definition. reply vbezhenar 13 hours agoprevI didn't understand the example in the linked README. // A library or framework defines effects based on other Signal primitives declare function effect(cb: () => void): (() => void); What library? What framework? I lost here. What's effect? effect(() => element.innerText = parity.get()); How does effect knows that it needs to call this lambda whenever parity gets changed? Will it call this lambda on any signal change? Why this talk about caching then? Probably not. Anyway I think that signal idea is sound, if I understood correctly what the authors tried to convey. My main issue with those decoupling architectures is that once your application is complex enough, you will get lost trying to figure out why this particular event being emitting. Ideally signals should fix this by modifying stacktrace, so when my callback is being called, it'd already contain a stacktrace of the code which triggered that signal in the first place. reply throwitaway1123 13 hours agoparent> What library? What framework? I lost here. What's effect? There are various libraries that export a function called effect which allows you to run arbitrary code in response to a signal update. The Preact docs have a great primer on signals and effects: https://preactjs.com/guide/v10/signals#effectfn As I understand it, these effect functions run the callback once initially to see which signals were accessed while the callback was executing, and then call the callback again whenever the signals it depends on update. As long as signal access is synchronous and single-threaded, you know that if a signal was accessed during the callback's execution that the callback should be subscribed to those signals. > How does effect knows that it needs to call this lambda whenever parity gets changed? Will it call this lambda on any signal change? You can do this with getters [1], where the effect function tracks which properties of the signal were accessed in a getter method (I believe Vue historically did this in version 2), but you can also track object access using proxies [2]. The example from the proposal simply has a 'get' method that is called to access the value of the signal, and executing this method allows dependencies to be tracked. [1] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... [2] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... reply mondrian 12 hours agoparentprev> How does effect knows that it needs to call this lambda whenever parity gets changed? The call `parity.get()` will register a dependency on the function that is passed to `effect()`. When `parity` is updated, the function is called. > Will it call this lambda on any signal change? Only when a dependent signal changes. In this case, `parity` depends on `isEven` and `isEven` depends on `counter`. So when `counter` is updated, that whole dependency chain is invalidated leading to `parity` getting invalidated, and that callback re-running. reply posix86 12 hours agorootparentThis works well until you accidentally have an if/else branch, then you get hard to track bugs (halting problem in the general case). I'm guessing this is why they don't propose to add this function to the standard: That fact makes it not very pretty. reply __s 11 hours agorootparentif/else isn't a problem. If the condition isn't dependent then update of unused branch doesn't matter reply mondrian 11 hours agorootparentprevHmm, can you explain how if/else branches cause hard to track bugs? reply eyelidlessness 8 hours agoparentprev> How does effect knows that it needs to call this lambda whenever parity gets changed? Basically all implementations of signals (by whatever name) build a dynamic dependency graph, where edges are established by reading nodes. In a tracking context like this hypothetical `effect`, the read also establishes an edge between the signal’s state node and the effect’s computation node—effectively subscribing the latter to subsequent writes to the former, in order to determine when to rerun the computation. reply pyrolistical 13 hours agoparentprevI suspect the watcher is needed by the implementation of effect reply troupo 13 hours agoparentprev1. effect is any function you want to invoke 2. with signals the dependency tracking mechanism knows what values need to be recalculated and as a result the system knows which functions to call again reply _heimdall 14 hours agoprevI don't think signals will help move development away from further complexity, and that's really what we need today. There's a fundamental question of why modern sites/apps reach for patterns like signals, memorization, hybrid rendering patterns, etc. I wouldn't begin to claim I have all the answers, but clearly there are gaps in the platform with regards to the patterns people want to implement and I'm not sure that jumping to signals as a standard helps better understand whether its the platform or our mental models that need updating to get back in sync. Personally I've found code much easier to maintain when the frontend is only responsible for state that truly is temporary and doesn't live on the back end at all. For me any persisted state belongs on the server, as does any rendering that depends on it. This largely makes signals unnecessary, very few apps have such complex temporary state that I need a complicated setup to manage it. reply makkesk8 15 hours agoprevLooks useful, but what baffles me is.. Why is every framework setting state or their \"signals\" using \"setX\" functions? What's wrong with the built in getter and setters that you can either proxy or straight up override? This feels arguably cleaner: something = \"else\"; Than: setSomething(\"else\"); reply wruza 1 hour agoparentBecause javascript lacks scope-as-an-object. You can't track `var x; x = value` through it. `setSomething()` sends a notification after an assignment. Also, DOM elements can only take raw values and must be manually updated from your data flow. Adding these features in-browser would seriously slow down DOM and JS and thus all websites for real. So instead we load megabytes of JS abstraction wrappers and run them in a browser to only simulate the effect. reply mhio 6 hours agoparentprevsomething = \"else\"; That is not observable for the primitive JS types that aren't objects and have no methods or properties or getters/setters (string, number, boolean, undefined, symbol, null). some.thing = \"else\"; The `some` can be proxied and observed. Most frameworks are setting up the `some` container/proxy/object so everything can be accessed and observed consistently. Whether the framework exposes the object, a function, or hides it away in a DSL depends on the implementation. reply notnullorvoid 14 hours agoparentprevSome libraries that feature signals style reactivity do use getter and setters (ember.js, and mobx are 2 good examples). However it makes sense for the primitive API to use functions since getter and setters are functions under the hood and get applied to an object as part of a property descriptor. It's also not always desirable to have a reactive value embedded in an object, sometimes you just want to pass around a single changeable value. As for why some libraries choose the `[thing, setThing] = signal()` API (like solid.js) that's often referred to as read write segregation. Which essentially encourages the practice of passing read only values by default, and opting in to allowing consumer writes on a value by explicitly passing it's setter function. This is something that was popularized by React hooks. Either way this proposal isn't limiting the API choice of libraries since you can create whatever kind of wrappers around the primitive that you want. reply j1elo 14 hours agoparentprevFor one I can write \".set\" and the IDE would auto-complete with all possible somethings that can be set, even without having the slightest idea of which ones there are. I've very much enjoyed this kind of consistency wherever is found (having a common prefix for common behaviors, in this case, setters) reply chuckadams 11 hours agoparentprevwell to use setters it has to be \"foo.something = else\", because JS can't override plain old local bindings -- not since \"with\" was sent to the cornfield anyway. Once you do that, you can indeed have a framework that generates getters and setters, which is exactly what Vue 2 does. Switch to proxies instead of get/set and you have Vue 3 -- the signals API is pretty much identical to the Vue composition API. reply dclowd9901 14 hours agoparentprevOne big problem is right now they are _tremendously_ slow to use. (At least through the Proxy native class). Not sure if this is an artifact of JITs or the nature of prototypal inheritance. reply Too 3 hours agoprevThe examples only show simple values. What happens when mutating nested objects and arrays? Other frameworks usually struggle a lot with this. With workarounds like having to override equality functions in useMemo() or call .set after a mutation even if you pass in the same instance as before. reply nojvek 9 hours agoprevOh man. This sounds complicated. The hard part of signals is similar to over complicating events. It’s hard to debug what is going on and what to fix when things go wrong. reply nikeee 12 hours agoprevBack in the days there was an effort to put observables in the language because they were popular (and rxjs was, too). Glad that didn't happen. And I think everybody else is, too. Maybe we should keep that in mind when standardizing features of frameworks. reply senoralligator 15 hours agoprevSurely it would be better to fix the interoperability issues of the language? \"Lets just add a single implementation of everything to the standard!\" seems like quite a strange response to \"users of the language are having trouble with interoperability due to false coupling.\" reply nullvoxpopuli 15 hours agoparentis it strange? Vue reactivity isn't compatible with Svelte, nor Angular. As a counter example to your question, what if we all had competing implementations of the object primitive. Libraries would barely work with one another and would need an interop layer between them (just as reactivity layers do today!) reply senoralligator 15 hours agorootparentI'll admit I don't use JavaScript very often, but surely the state of polymorphism could be improved? For example, C++ recently added concepts, and most (modern) languages have some way to describe interfaces. As to your counterexample, I agree with current JavaScript that would be a problem, but with good language support it would certainly be possible. For example, Rust (and C++?) have competing implementations of the global allocator, and must users will never notice. reply zdragnar 14 hours agorootparentThe reactivity layers are all pretty tied into the hearts of the frameworks. There's no advantage to any framework to expose such a thing to end users to leverage a competing implementation. As for polymorphism, even the current class syntax largely operates in the same way as the original prototypal inheritance mechanism, with a few exceptions in constructor behavior to support subclassing certain built-in objects. You can pretty easily create run-time traits- like functions with prototpyes, the class construct is an expression whose resulting value can be passed around, mutated, etc. For example, you can write a function that takes a class as an argument, and returns a new class that extends it. reply jauntywundrkind 15 hours agoparentprevI don't think the language hurts or hinders interop much? It has a wide range of tools that can adapt objects between different shapes, for when we do have two similar but different interfaces we are trying to bridge. I struggle to see what more one could want. Do you have any specific features you think would help a massive ecosystem of packages be able to work together, when for example different packages have different Signal implementations? reply senoralligator 15 hours agorootparentBetter ways to describe polymorphism, a better type system in general really. Look to async Rust for a great example of such interoperability. reply polynomial 14 hours agoparentprevIsn't this similar to small core vs large core debates you see in other projects, often where drivers are concerned? (ie question around where the coupling layer is materialized) reply Too 3 hours agoprevThis looks sufficiently advanced to warrant some language support. This could be a much more powerful feature if signal-dependencies were discovered statically, rather than after use in runtime. If you’ve reached the point where you agree that the library should be standardized, why not take it even further to integrate it even more? reply sapling-ginger 3 hours agoparentBecause there's no rule that says signals should ever be created at the top level assigned to a const variable. You could create signal objects dynamically based on user input, no current proposal or implementation prevents this. So there's no way to do static analysis on signal graphs. reply ivanjermakov 15 hours agoprevIs dependency tracking problem statically solvable (without calling effect once and subscribing to all .get's)? I don't think this needs to be a language feature, rather abstraction of existing features. In other words, can't this be a library? I know that SolidJS is able to figure out dependent signals, but probably doing so on the first execution. reply twiss 14 hours agoparentI believe Svelte does figure it out at build time. Which does make me question the mention of Svelte in the proposal, and makes me wonder what the Svelte developers think of it - because IIUC they indeed don't need this (at runtime), if I'm not mistaken. reply Phillippe 12 hours agorootparentThe current Svelte version does it at build/compile time. The up and coming Svelte 5 is using signals and the reactivity is moved to runtime reply bastawhiz 12 hours agoparentprev> In other words, can't this be a library? You answered your own question: > I know that SolidJS is able to It already is, obviously. But how is SolidJS supposed to work with other non-SolidJS code? It can't. Unless every library builds support for every other library, they can't possibly interoperate. reply ivanjermakov 11 hours agorootparentIt's a shame that JS ecosystem is so sparse. E.g. in Rust it's much more common to rely on existing \"building blocks\" like futures, tokio, syn, serde even just for basic bindings, favoring interoperability. reply bastawhiz 10 hours agorootparentI think you've got it backwards. Fifteen years ago we had jQuery and not a whole lot else. Everyone just used the \"building blocks\" libraries of js. But then the community and the ecosystem grew exponentially, and we have innumerable choices. Python is similar: I can think of a half dozen ways off the top of my head to build a web app. At least Python was on top of standardizing things like WSGI, but even then you can see rough edges like the non-interoperable nature of concurrent code (twisted vs tornado vs greenlets vs threading vs multiprocessing vs asyncio etc). Rust will have the same thing happen in another decade or so. It's prevalence will grow the community, which will grow the ecosystem, and the positive feedback loop will mean that there's a huge amount of choice. That's a good thing, but it will have downsides, like decreased interoperability (unless the language evolves). JavaScript was cursed for a very long time by leadership that was essentially asleep at the wheel. reply imbnwa 8 hours agorootparentprev> It already is, obviously. But how is SolidJS supposed to work with other non-SolidJS code? Who actually writes code like this? People use some signal graph library for application code typically, I’ve never seen anyone mixing SolidJs with MobX in application code or as a consequence of a library dep. reply ivan_gammel 15 hours agoparentprev>In other words, can't this be a library? It is explicitly mentioned in the proposal. The problem with 3rd party libraries is their interoperability and diversity of implementations, which might be unnecessary for this kind of things. reply sattoshi 3 hours agoprevThe first and only question that matters: is this useful as a primitive? Im inclined to say no. Signals seem like they are a UI concept, primarily. Just use any signal lib you want. Utility libraries dont really need to understand signals. reply tommiegannert 14 hours agoprevHmm, if we can optimize the reactive state management in all web apps, that sounds cool. If this is to be a base for VueJS, it should handle deep changes. They have a note about support Map and Set, but being able to to control depth is nice in VueJS. (I'd say watch() should be deep by default, since non-deep is an optimization that can lead to accidental inconsistent state.) Streams. Generally, I find RxJS backwards. Usually you just need \"state\", so that should be the easiest thing to implement. But I can't deny that the programming model is beautiful. Standardizing \"state\" without also considering streams seems odd to me. The \"computed\" creates a pipeline of updates, very similar to one you'd do with a map over a stream. If RxJS didn't already exist, I probably wouldn't have cared about this duality. Async. Sure, signals can be synchronous, but Computed should definitely play well with async functions. This is a big shortcoming in VueJS (that people work around on their own.) That also implies handling \"pending computation\" gracefully for debuggability. I see there's a \"computing\" state, but this would have to be surfaced to be able to debug stuck promises. Exceptions. I like the idea of .get() rethrowing exceptions from Computed. VueJS is a bit vague on that front, and just stops. reply nullvoxpopuli 14 hours agoparent> it should handle deep changes. these can be implemented in userland via proxy -- and I think probably should, as is proven by this collection of utils: https://twitter.com/nullvoxpopuli/status/1772669749991739788 If we were to try implementing everything as reactive versions, there'd be be no end, and implementations couldn't keep up -- by pushing reactive Map/Set/etc to userland/library land, we can implement what we need when we need it incrementally, built on the solid foundation of the signal primitives. > since non-deep is an optimization that can lead to accidental inconsistent state. conversely, deep-all-the-time is a performance hit that we don't want to be default. Svelte and Ember take this approach of opt-in-deep reactivity. reply moi2388 3 hours agoprevBut the vanilla code is much cleaner and nicer to read and reason about than the proposal?! Seriously the JavaScript ecosystem is so strange.. reply srcreigh 5 hours agoprevI’m in favour. Mobx is great. This will focus efforts on making better devtools. reply alserio 15 hours agoprevreally looking forward to get nice dev tools out of this work reply angleofrepose 14 hours agoprevOff topic, but I’m wondering if anyone attracted to this topic could help me understand why JavaScript doesn’t have macros. I’m aware of much conversation around dismissing macros, often in the context of bad dev experience — but this sounds like a shallow dismissal to me. At the end of the day, we have some of the results of macros in the JavaScript ecosystem, but rather than being supported by the language they are kicked out to transpilers and compilers. Can anyone point me to authoritative sources discussing macros in JavaScript? I have a hard time finding deep and earnest discussion around macros by searching myself. reply bastawhiz 12 hours agoparentInterpreted languages rarely have macros. But more importantly, do you really want script tags on webpages defining macros that globally affect how other files are parsed/interpreted? What if the macro references an identifier that's not global? What if I define a macro in a script that loads after some other JavaScript has already run? Do macros affect eval() and the output of Function.prototype.toString? Sure, you could scope macros to one script/module to prevent code from blowing up left and right, but now you need to repeat your macro definitions in every file you create. You could avoid that by bundling your js into one file, but now you're back to using a compiler, which makes the whole thing moot. reply pcthrowaway 4 hours agorootparentIt turns out there might actually be a benefit of the compilation step which has been introduced now that everyone uses Typescript... would be really interesting to see macros get added, though I suspect it's too far away from Typescript's mandate to add as few new features on top of Javascript as possible reply notnullorvoid 13 hours agoparentprevMacros don't really make sense in JS runtime spec. Since you can mostly already achieve macro level features by using eval or new Function, but it's not very efficient. Macros make most sense at build time, and there have been a few attempts at generalized build macros with various bundlers / transpiler plugins. I think the space needs more time to mature. I'm optimistic that we'll eventually see some sort of (un)official macro spec emerge. reply hoten 14 hours agoparentprevYou could start here: https://github.com/search?q=org%3Atc39%20macro&type=code reply angleofrepose 14 hours agorootparentA great resource that I should have found on my own. Thank you. I’ll look through this later. Giving it a quick glance now I see some of the same language I see other places; here that macros are “too far.” I don’t know why macros are approached with apprehension. As I briefly get at in my first comment, I’m aware of a lot of dismissals of macros as a tool, but those dismissals don’t make sense to me in context. I’m missing some backstory or critical mind-share tipping points in the history of the concept. What could be a good set of sources to understand the background perspective with which TC39 members approach the concept of macros? reply qudat 9 hours agoprev> The current draft is based on design input from the authors/maintainers of Angular, Bubble, Ember, FAST, MobX, Preact, Qwik, RxJS, Solid, Starbeam, Svelte, Vue, Wiz, and more… This is primarily the only handful of consumers of this stdlib. reply endgame 9 hours agoprevFrom a first look, signals seem to parallel event/behaviour-style functional reactive programming (FRP). Hopefully some useful ideas from the FRP world can be brought across. reply beders 14 hours agoprevI've been enjoying \"signals\" in the form of re-frame subscriptions for many years now. They solve a neat subset of problems in front-end developments. But they don't solve all of them. Adding it to JavaScript as language construct is unnecessary. reply ivan_gammel 15 hours agoprevI like the quality of this proposal, which reminds me of JSRs. It does make a lot of sense. reply rblatz 14 hours agoprevThe lack of signals isn’t remotely the largest issue with JS, and adding them has minimal impact for most users of JavaScript. The biggest issue is the lack of a standard library, resulting in npm hell in most projects. reply phpnode 13 hours agoparentJS does have a standard library and this proposal is about expanding it, so that’s good, right? reply mrpepka 12 hours agorootparentUnless it's half-assed, clumsy and short-sighted, and we are stuck with it forever, because standard. Until one's implementation becomes a standard thanks to the fact it wins as a library (as jQuery did), it should not be slyly forced into the language. reply alexchamberlain 13 hours agoparentprevThis is referenced in the proposal: > JavaScript has had a fairly minimal standard library, but a trend in TC39 has been to make JS more of a \"batteries-included\" language, with a high-quality, built-in set of functionality available I think the description \"minimal\" is fairer than \"no\" wrt the standard library. reply qudat 9 hours agoparentprevIt's getting better. I can build a medium-sized, modern TS/React app with 12 dependencies. A large enterprise app will be closer to 30-40 which is still a marked improvement over previous dep lists. reply preommr 12 hours agoparentprevStrongly disagree - it's trivial for a project to just add a small std-lib, or add lodash as a single dep, or just add it directly as source code. JS projects exist in npm hell because people have been taught to use a library to save typing 10 characters. No standard library is going to fix. Because someone can just call in a new lib that just curries something in the standard lib with minor improvements like caching. reply readline_prompt 12 hours agoprevCurious, what about using Proxies for handling state? reply kookamamie 14 hours agoprevLooks bad. It adds what looks like more nonsense complexity. Also, \"Signals\" as a name is not descriptive to what is proposed (see e.g. Qt Signals). reply tqwhite 14 hours agoparentYes. Thank you. I am already tired trying to figure out how this would improve my life and annoyed at trying to read other people's code that has more abstracted crap in it. reply cuddlecake 10 hours agorootparentPromises are, technically speaking, \"abstracted crap\". So is abstracted crap only ok if it's already in the JS standard library? reply Izkata 7 hours agorootparentPromises proved themselves in libraries before they got added to Javascript. reply troupo 3 hours agorootparentSignals have, too. And it is literally spelled out in the readme, in the introduction section reply notnullorvoid 13 hours agoparentprevI do personally think signal is a bit of a poor naming choice, but it has become probably the most recognizable term used for the concept in the JS ecosystem. There's not a whole lot of short concise naming options either, but maybe \"Reactive Values\" is better? reply CanaryLayout 12 hours agoprevOh God why. reply jupp0r 13 hours agoprevThis is a horrible idea. Instead of building some dependency tracking into this niche feature of the language, JS should come up with a generic way of enabling framework developers to clean up resources without putting the burden on users of their API to manually do this. reply nyanpasu64 10 hours agoprevIs this like QML property binding? reply ttfkam 13 hours agoprevLooks like Svelte 5 only somehow worse? reply bastawhiz 12 hours agoparentThe proposal is very clear that it's not trying to be pretty, it's trying to be sensible and correct so that frameworks like Svelte can build on top of them and work interoperably with other libraries and frameworks. reply calebpeterson 14 hours agoprevNot a bad thing at all… but this is the same mental model provided by the so-called atom-based state management systems in React. I believe Jotai is the most popular. reply da39a3ee 12 hours agoprevThis article is missing a \"What are signals\" section. And yes, this does not do the job: > Within JS frameworks and libraries, there has been a large amount of experimentation across different ways to represent this binding, and experience has shown the power of one-way data flow in conjunction with a first-class data type representing a cell of state or computation derived from other data, now often called \"Signals\". reply andrewstuart 15 hours agoprevWhat does this buy me over events? reply troupo 14 hours agoparentAutomatic dependency tracking, guarantees against circular references, improved observability and potentially better devtools reply everybackdoor 14 hours agorootparentAnd a backdoor in a build tree reply Muromec 10 hours agorootparentImaginary exercise for the reader: find a sneaky dot in a patch that implements this in Firefox. reply andrewstuart 14 hours agorootparentprevThat’s not enough to compete with an existing “good enough” solution. reply troupo 14 hours agorootparentThere's no existing \"good enough\" solution for reactive values in JS. Also https://news.ycombinator.com/item?id=39887187 reply claytongulick 13 hours agorootparentGetters and setters work pretty well. addEventListener(\"foo\", () => {...}, {once: true}) Is a pretty easy way of handling one-shot events. Those have been \"good enough\" for me to build large, complex healthcare applications. reply troupo 3 hours agorootparentNo one says you can't build large complex applications using current tools reply andrewstuart 10 hours agorootparentprev>> Getters and setters work pretty well. What do you mean can you explain more? reply dboreham 6 hours agoprevFFS there's already something named signals, since c. 1972. reply artemonster 12 hours agoprevEasier to port react to vhdl or verilog? reply lloydatkinson 15 hours agoprevSorry but I’ll be a little bit mad if this Preact/Angular inspired thing is approved and made part of the spec, while Observable was deliberately ostracised. reply troupo 14 hours agoparent> I’ll be a little bit mad if this Preact/Angular inspired thing Signals predate both and originate in KnockoutJS at least. They were popularized in recent years by SolidJS. And then adopted into Preact, Vue and others. Angular is a very late newcomer to the signals game. Edit: and this is literally in the introduction section: --- start quote --- This first-class reactive value approach seems to have made its first popular appearance in open-source JavaScript web frameworks with Knockout in 2010. In the years since, many variations and implementations have been created. Within the last 3-4 years, the Signal primitive and related approaches have gained further traction, with nearly every modern JavaScript library or framework having something similar, under one name or another. --- end quote --- The list of libraries in the README is alphabetized, and doesn't reflect the evolution of signals in the frameworks and libraries. reply Cloudef 3 hours agorootparentSignals are actually much older concept from 70s introduced in smalltalk as a ValueHolder. Later Qt reused the same concept and called them \"signals & slots\". reply henriquez 13 hours agoprev“Lets make my random UI state tracking framework part of the JavaScript spec” reply dclowd9901 14 hours agoprevCool. My least favorite, cumbersome and brittle aspect of using Ember becoming a core aspect of JavaScript functionality. No fucking thank you. reply nullvoxpopuli 15 hours agoprevSocials: - Reddit: https://www.reddit.com/r/javascript/comments/1bsgnf5/tc39_pr... - X/Twitter: https://twitter.com/nullvoxpopuli/status/1774496900915327100 - Masto: https://mastodon.coffee/@nullvoxpopuli/112191605019758002 - Blue Sky: https://bsky.app/profile/nullvoxpopuli.bsky.social/post/3koz... reply londons_explore 14 hours agoprevCan't we just call javascript 'done'? We keep adding things to the language, and never subtract anything, which means learning it as a language is getting harder and harder. reply troupo 14 hours agoparentThey don't add these to the language. They add these to the (currently basically non-existent) standard library. Given that almost every single framework under the sun (except React) has converged on signals, it makes sense to move that into the browser. This... this is how the web is supposed to work. reply Waterluvian 14 hours agoparentprevI dunno. I sometimes feel the same, but a whole ton of recent features have been incredible at cleaning up code. ?? Is my favourite. I also want set functions and possibly a match statement thingy. reply everybackdoor 14 hours agoprev [–] - They’re saying ”the community wants less boilerplate” - They introduce what is effectively a black-box system - The new system is expected to handle all application state - They try to push it for frontend folks while also remarking that it would be useful for build systems This has the same red flags the xz saga had. Have we learnt nothing. Lots of ”users” here vouching for the pattern and hoping it gets adopted. I bet this gets some nice damage control replies because there’s social engineering going on here right now and most seem to not be aware of it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The JavaScript Signals standard proposal aims to standardize a model for signals in the JavaScript ecosystem, enhancing how developers manage complex application states in UIs.",
      "It discusses the benefits of using Signals for data binding abstraction in JavaScript frameworks, aiming to improve performance, memory management, and interoperability between frameworks.",
      "The proposal includes guidelines for implementing reactive data structures, preventing glitches, and features writable and computed Signal types, undergoing testing and collaboration for solidity and compatibility before further implementation stages."
    ],
    "commentSummary": [
      "Users are debating adding signals to JavaScript, weighing the pros of standardization against the historical use of signals in frameworks and state management complexities.",
      "Some advocate for standardization to curb fragmentation, while others value the flexibility provided by separate frameworks.",
      "Discussions also touch on events, promises, and effects on web development tools and performance, emphasizing the importance of thorough proposal assessment, memory leak prevention, and the possibility of community-driven JavaScript standards."
    ],
    "points": 249,
    "commentCount": 246,
    "retryCount": 0,
    "time": 1711907325
  },
  {
    "id": 39884004,
    "title": "Embracing Discomfort: Key to Breaking Free",
    "originLink": "https://the-simulation-strategists.beehiiv.com/p/being-stuck",
    "originBody": "0 The Simulation Strategists Posts The Illusion of Being Stuck The Illusion of Being Stuck benoit malige March 26, 2024 You think that being stuck is a permanent state, and this is the pattern you go through every time: You encounter a new challenge. Your brain resists it. It hasn't been trained to handle discomfort effectively. You revert to what's comfortable and familiar. You shut yourself off from potential growth and enlightenment. You feel stuck, unfulfilled and unhappy. When you face the challenge of feeling stagnant in your career, in your personal life, nothing seems to help you progress. You feel not just stuck, but regressing. The worst part of it was that in some strange way, it also feels “comfortable” being stuck. I’m sure you have that feeling, because we are wired the same way, and so do I. I wanted to understand why we are so resistant to change, to discomfort. Why is it that it’s easier for us to stay in situations we don’t like while we know damn well it’s not the right path. Why is change hard? Why makes hard things hard? Well, I got some answers while reading a research paper from computational neurobiologist Andrew Gallimore. I finally got a better understanding of how our brain constructs reality, how it cheats us by wanting to save energy, and how we can hack it. 1. Understanding Our Brain: A Reality-Building Machine I won’t bore you with the very technical research paper filled with complicated words and never ending sentences. Here’s a breakdown for you: Central to this process are the cortical columns, functional units within the cerebral cortex that orchestrate your perception of the world. Source: https://alieninsect.substack.com/p/switching-the-reality-channel These columns evaluate sensory information against your brain's existing models, determining \"True\" or \"False\" signals to guide your reactions and beliefs. To put this even simpler, They work like little decision-makers in the outer layer of the brain, sorting through what we see and feel. They check this information against what the brain already knows, deciding if it matches (\"True\") or doesn't match (\"False\"). This helps you figure out how to react and what to believe. Here's the problem: Your brain and its cortical columns are designed to conserve energy at all costs. This means they're constantly seeking \"True\" statements, those that align with what you already know and believe. Why? Because processing familiar information requires less energy than assimilating new data. This is precisely why staying in your comfort zones feels so... comfortable. It's not just inertia; it's a battle against a highly efficient computer in your head, always seeking stability over the chaos of change. Your gut might scream for a shift, but you’re up against your brain's design, which chooses energy conservation over everything else. 2. The Mannequin Misidentification: A Cortex in Action Ok ben, that sounds cool and I get the theory. But how does this actualise, and how is it shaping my reality? Here’s an example: Imagine walking down a street, your eyes catch what appears to be a figure standing in a shop window. Your brain utilizes past data, and quickly categorizes this figure as a man—a \"True\" response rooted in familiarity. You continue to walk closer, until something happens.. you realize that the figure is not a person, but in fact a mannequin. This unexpected twist sends a \"False\" signal to your brain, specifically to those cortical columns tasked with visual processing. Faced with this new information, your brain recalibrates, updating your reality from a man to a mannequin. 3. Embracing the Science of Uncomfortability So there we have it. The path to growth is through embracing \"False\" signals—those challenging, unfamiliar experiences that demand our brain to adjust and grow. This process isn't just about acquiring new information; it's about fundamentally reshaping our perception and interaction with the world around us. It’s about changing reality; literally asking your cortex to re-evaluate your current reality, and change it. That’s why it feels so hard. That’s why it feels so wierdly good to stay stuck. So, what are some steps to hack this? 4. Recognizing the State of \"Brain Automation\" Well, first, you need to understand when you're stuck in \"brain automation\". I’m sure you know this feeling very well. You’re in that state when you go through your daily routines without much thought or variation. You wake up, get dressed, get coffee at the same spot, order the same drink. Tasks and activities feel mundane, and you're not engaged or stimulated by what you're doing. You are bored. You wake up every morning saying “ok here we go, another day doing the same thing until I get to go to sleep again. Let’s just get through it with as many distractions as possible.” Nothing seems to make you feel alive. You are just in automation mode. Activities that used to hold your attention don’t mean anything anymore. 5. Breaking Through with Challenging Thoughts Now that you recognize the signs of \"brain automation,\" the next step isn't just about what you do; it's about how you think. Changing your actions without changing your thoughts is like repainting your car when it needs a new engine. Sure, it will look good on the outside, it still won’t run though. So do this: Ask why, and don’t judge. “Why” is such a short but powerful question that can almost always be used successively until the truth is met: “I feel stuck in my career? Why? …Because I don’t like what I do. Why? ...Because it doesn't challenge me or align with my values. Why? …Because I settled into a role that was available rather than pursuing what genuinely interests me. Why? …Because I fear the uncertainty of change and the possibility of failure. Why? …Because past experiences have conditioned me to prioritize security over fulfillment. Why? …Because I internalized the belief that success is defined by stability rather than personal satisfaction and growth.” Take this and keep asking the same question. You’ll get to the root of it soon enough. In this instance, your feeling of being stuck is less about the external circumstances and more about internal barriers—fears, beliefs, and assumptions that keep you tethered to the familiar, however unsatisfying it may be. 6. No, but really.. how do I get unstuck? Listen. True transformation begins when you push beyond the boundaries of what you know and where you’re comfortable. The shift needs to happen in your head, but also physically. You need to consciously and repetitively seek out \"False\" signals and embrace the discomfort they bring. You need to construct a new reality. In plain, simple terms, you need to: do the uncomfortable thing. Here are a few of the things I have had to do last week alone that finally led me to break out of my cycle: I impulssively bought a plane ticket to Panama, to change my enviroment, create some space for me to: Sit with my feelings for the first time in years, and finally stop supressing them. I cried. Haven’t done that in years. I had the uncomfortable conversation with my business partner to tell them I no longer have passion for it and am leaving them the decision to continue or quit. I made the decision to get a therapist and seek help to access past trauma that I’ve burried deep down. I replied \"I don't know\" when asked about something I was unsure of, rather than trying to appear confident. I felt stupid that instant, but I actually learned something and ended up having a meaninful conversation. I forced myself to talk to strangers every single day. Oh, and talking happened to be in Spanish, for which my vocabulary consisted of a total of 40 words or so. These things were uncomfortable. These things were all dreadful for me. They are supposed to be. They are hard, they are counter-intuitive, and they are painful. But if you don’t do them, you are letting your cortex run on auto-pilot, validating a “true” statement that will feel comfortable. Yes, in the immediate short term, it feels better. But you are reinforcing your mind into shaping the wrong reality. Do that long enough, and you will soon be living in a world that is not yours. You’ll have a job you hate, relationships that don’t support you, a deep lack of purpose, numbing yourself to the world around you until you become nothing more than a ghost, aimlessly drifting through a suffocating and meaningless existence. Waking up to it 10 years later will be very painful. So Wake up. Challenge yourself. Engineer your existence and break free.",
    "commentLink": "https://news.ycombinator.com/item?id=39884004",
    "commentBody": "The illusion of being stuck (the-simulation-strategists.beehiiv.com)195 points by benoitmalige 20 hours agohidepastfavorite108 comments noufalibrahim 19 hours agoI've come across these kinds of articles before and they always seem to suggest that you rewrite the metaphorical firmware of your brain so that it operates in a way that is advantageous to you. There's some notion of self discovery from first principles. I'm not convinced that this is generally a good idea. It makes a person unmoored and open to anything (\"so open minded that your brains fall out\"). It's also often amoral. The only metric that's maximised is utility regardless of other more subtle consequences. As I grow older, i think the human experience is more complex than can be captured by a few \"hacks\" like this. reply maxverse 17 hours agoparentThe difference between medicine in poison is the dose (\"the dose makes the poison.\") Good ideas taken to extreme are often Very Bad, but sometimes a little bit of the idea is exactly what you need. If you have social anxiety, talking to people through discomfort, or initially treating interactions as a game can help take the pressure off. Treating people as NPCs is unhealthy. Routine exercise is good. Becoming obsessed with how your body looks, living in the gym, taking steroids is bad. A sense of humility, purpose, and devotion in life is generally good. Religious fanaticism is bad. Confidence, which often involves a touch of self-deception, generally is good. Unchecked arrogance is bad. If I tell you to go for a run or do some push-ups, there's a risk that you'll become totally obsessed with exercise. But going for that run will also help. You need a moderate amount of my advice, not an extreme amount. This piece resonated with me. When you're stuck in a routine you dislike, being aware that your brain is optimized to stay in its current state (according to the cited research) is good. Going bananas and being \"unmoored and open to anything\", living every day without any sense of habit or purpose is bad. The dose makes the poison. reply soneca 14 hours agorootparentI on the side that the complexity is even larger than this. There a lot of other analogies that could apply here, not just the dose/poison one. Some things are good in small doses on some occasions and in large doses on others. Some things are poison for some people but food for others. Some things are only medicine in very specific circumstances. Some things are placebo. Anyone can decide if a piece of advice is beneficial to them in what dose. But I agree with the GP that we are too complex for these hacks reply card_zero 18 hours agoparentprevDid you know, the brain is hard-wired to be suckered by self-help books? One trick that I personally find very liberating and enlightening is to make a big pile of such material and burn it. It gives me a sense of warmth and inner joy. reply knallfrosch 14 hours agorootparentI enjoy reading them, thinking I'll change myself. Then I'm done, I don't apply anything and put the book on the shelf. It's its own kind of horror for these books. reply haswell 8 hours agorootparentTransitioning to \"I know these are good ideas\" to things you're doing in practice is much harder than reading, but can happen. Over time, I've come to look at reading these kinds of books like planting seeds. They give my brain ideas and concepts to chew on. Options to apply to situations. I did wake up one day to the realization that I had up to this point not been willing to take the next steps and put insight into action. And when I realized that, it was like opening floodgates. I started doing the things I had learned about, and they started to change my life. But there is a pretty important and major milestone to make this transition. I don't think it's necessary or helpful to frame this as a kind of horror. I think it's more helpful to just change how you see the act of reading books. True change still has to start from within. Books in this context are just toolboxes. Toolboxes don't build anything by themselves either. reply maxverse 13 hours agorootparentprevThis is an interesting topic of its own, but there have been a few books/ideas that have made a significant difference on how I operate day-to-day. There are many more books and ideas I nod along to and never implement. reply benoitmalige 13 hours agorootparentYou’ve touched on something profound here—our interaction with knowledge is dynamic, not static. A book that speaks to us today might whisper different wisdom 5 years from now, as we grow and our perspectives shift. reply noufalibrahim 18 hours agorootparentprevIsn't the combustion external? Why would the warmth be internal? reply gumby 17 hours agorootparent“Burn” is a commonplace synonym for oxidization, that is, adding an electron. Likely the GP is “eating the books, and thus is warmed as a consequence of the Krebs cycle. We can be glad for GP that these days it is uncommon to encounter a book with a CD or DVD attached. I’m not sure those are safe to eat. reply ufocia 18 hours agorootparentprevSemantics dear Watson: it's the inner joy that's internal. reply noufalibrahim 18 hours agorootparentNever let an explanation ruin a terrible pun. reply jrm4 16 hours agorootparentprevI so believe the opposite of this IF you come at the right ones the right way? Which is to say, once you read (or skimmed) enough of them, it's easy to get a sixth sense of the \"intelligence\" and \"motivation\" of the writer, and the sincere ones, even if not smart, are still pretty enjoyable if taken with many salt grains, and I'm a sucker for them (again, not in terms of following exactly what they say to do, but appreciating the motivation and good-intent of the author and possibly picking up something along the way.) They'll have names like PSYCHO-CYBERNETICS and just be like \"try to be nice to people and sit down and be quite a few times a day.\" A sort of perfect example would be the fictional \"The You You Are\" from the TV Show Severance. I hope they actually write it, I know I'd LOVE it. https://severance.wiki/the_you_you_are_book reply nefrix 16 hours agorootparentprevYour advice is superb. You can start writing self help books learning people how to make the fire in order for them to achieve the inner joy. reply rors 18 hours agoparentprevOver the last few months I've been experimenting with Buddhist meditation, which leans heavily into this metaphor. They phrase the metaphor differently, with a focus on flattening of the grooves in your mind. I've found these ideas helpful, but they're hardly novel. The Buddha lived over 2000 years ago. One big difference the objective. Most of these blogs are about hacking your mind to be more successful, whereas religions aim to make you more comfortable with your life as it currently is. I think if you're going to experiment with upgrading your wetware, then you could do worse than look at Buddhism, or any other practice that encourages deep contemplation and promotes kindness. I have my issues with organised religion, but why not at least consider the thousands of years of prior art? reply FlyingSnake 18 hours agorootparentIt is also a folly to equate Buddhist traditions with Self help book sellers. Siddhartha Gautam Buddha was just one of the 6 other prevalent Dharmic masters and there enough original material from many of those traditions. The Dharmic traditions arose over centuries through a healthy collaboration of diverse set of people over a vast geography. reply ufocia 17 hours agorootparentTrue, the Dharmic masters didn't have printing presses. reply gchamonlive 17 hours agorootparentOr late stage capitalism reply noufalibrahim 18 hours agorootparentprevI appreciate the point you're trying to make. However, my general experience of attempts to take \"useful\" parts of of religion and then sanitise them for secular mass consumption is that they're at best useless and at worst harmful. Sure, there might be some proximal benefits to (say) meditation but these are things that were done in a larger (often spiritual) context and I'm not sure that pulling them out of that is wise. reply praptak 17 hours agorootparentEffects of meditation are there regardless of the religious context. Although you are right in that the deeper stages (say as described in \"The Mind Illuminated\") may not make much sense without at least some spiritual foundation. reply ufocia 18 hours agorootparentprevDon't kill and don't steal seem pretty useful and are embedded within many if not all secular laws. reply paulryanrogers 4 hours agorootparentSometimes with massive caveats like stone adulterers and homosexuals. reply physicles 5 hours agorootparentprevI sympathize. As someone who used to hold essentially fundamentalist views, I'm now deeply suspicious of anything supernatural. When I left, I didn't even try to smuggle out any of the good parts. However, there's now tons of secular support for meditation. Sam Harris, a neuroscientist and one of the most anti-religion and anti-supernatural people out there, is basically leading the charge. I think that says something. reply noufalibrahim 2 hours agorootparentInterestingly, my main disagreements with the approach came primarily from the way Harris (and to a smaller degree Alain deBotton) attempted this. reply haswell 18 hours agoparentprevMy perspective on this is one of a person who underwent years of complex trauma in childhood and my stuck-ness was a series of overly sensitive protective mechanisms that had been built up by the experience. “The illusion of being stuck” also sounds a lot like “Learned Helplessness”, which is a maladaptive conditioning of the brain to strongly believe that not doing something is the best option because doing something won’t make a difference anyway, despite this not being true at all. Like drowning in two feet of water because it didn’t occur to the person to just stand up. The reason I’m pointing at trauma is that we all come from a wide spectrum of experiences, which result in a wide spectrum of stuckness (or lack thereof). The book “Learned Optimism” (written by Martin Seligman, one of the original researchers who discovered the learned helplessness phenomenon) first describes how animals (not just humans) arrive in these maladaptive states, and then provides worksheets to help people who are currently deeply biased towards a pessimistic world view learn techniques to reframe and ultimately rewire these biases, and I can attest to the life changing improvements this kind of work can bring without my brain falling out. More broadly, it’s worth considering the fact that we’re all running firmware that evolved during a very different era of human existence, and is often poorly tuned for modern reality. Using what we know about the brain and how it is conditioned to adapt ourselves to modern situations seems pretty critical. > I'm not convinced that this is generally a good idea. It makes a person unmoored and open to anything (\"so open minded that your brains fall out\"). It's also often amoral. The only metric that's maximised is utility regardless of other more subtle consequences. This seems like a pretty major jump to a conclusion that would require far more to back it up. I don’t think this is about mucking with firmware as much as it is about correcting model weights. Our firmware seems to be what it is. Our environment and conditioning is what embeds certain biases and resulting behaviors. Human experience is certainly more complex than just a few hacks, but I don’t think that discounts the value that certain kinds of exercises can bring. We’re continually working against our default modes of operation when we adhere to principles of rational thought and logic. Emotional responses like anger discard these higher modes of thinking. I see these “hacks” as something similar: methods of helping us move beyond the less adaptive parts of our brains based on a growing understanding of the world around us. reply noufalibrahim 18 hours agorootparentThat's a very nuanced take. I dont take offense to using \"techniques\" to fix \"problems\". I'm more annoyed by the usual phrasing of these things as life changing and revelatory. reply try_the_bass 14 hours agorootparent> I'm more annoyed by the usual phrasing of these things as life changing and revelatory. But when they work for someone, they can and do lead to important revelations and/or life changes. I'm a firm believer in one's ability to \"reprogram\" themselves, having done it to varying degrees across the span of my life. Perhaps it doesn't work for everyone. It doesn't always work for myself, especially when life makes the required self-discipline hard to maintain. I'm biased by my own success, obviously. reply ufocia 18 hours agorootparentprev> I'm more annoyed by the usual phrasing of these things as life changing and revelatory. Maybe it's time to unstick yourself from these patterns. Life changing and revelatory can be as much evolutionary as revolutionary. There's no need to narrow them down to their extremes. reply dylnuge 17 hours agorootparentI suspect they're taking issue with the implication that these things are universally life changing, not that they can be meaningful at all. I have no doubt the author of this article uncovered personal truths, but the whole thing is framed as if they become an expert on everyone else's mental health by breaking free from their own depression. Personally for navel-gazy articles like this I think it's more useful to focus on telling one's own story instead of making broad edicts and sprinkling in some science in an effort to lend legitimacy to them. What works for one person won't work for everyone, but \"I was in X situation and did Y and it helped me\" is far more likely to be helpful for someone else than simply \"Do Y\", especially when you're writing for a wide audience. reply benoitmalige 17 hours agorootparentThank you for your insight. I completely agree—my journey and the lessons I’ve learned are deeply personal and not one-size-fits-all solutions. Embracing imperfect action has been a significant step for me, far better than letting procrastination have control. Sharing my experiences and receiving feedback is how I hope to grow and refine my understanding over time. And truly, when I write, I'm mainly holding a mirror to myself, navigating my thoughts and discoveries. Your feedback is invaluable in this process. reply axchizhov 11 hours agorootparentprevLearned helplessness was disproved, by the way. If I remember correctly, there is even a recent paper from the original authors confirming this. Helplessness isn't learned, it's a result of not knowing how to deal with the situation. So, try new things, search for the solutions and don't give up — that's the recipe. reply haswell 10 hours agorootparentThanks for pointing this out - I was not aware of the recent new findings. After reading through some of the recent updates my understanding is that the recommended reframing exercises (the “Learned Optimism” part) are still effective, even if the original understanding of why helplessness occurs biologically was backwards in the original research. So perhaps a better descriptor now is just “helplessness”. From a practical standpoint, this is critical to note because if someone is trying to address their stuck-ness, this book is still an excellent resource that is not affected by this detail and is focused on learning new skills to move forward. I think this also underscores the importance of taking active steps to retrain our brains when necessary. reply faeriechangling 17 hours agoparentprev>It makes a person unmoored and open to anything (\"so open minded that your brains fall out\") I agree with your assumptions, but not your conclusion. When I was at my very lowest I took pretty high doses of acid for my first time on psychedelics. The point was that I was starting from a long time experiencing daily mental anguish and physical pain being treatment resistant, ideating suicide but not completing it. I thought if I became more \"open minded\", either I'd get better or I'd off myself and I really preferred both over the status quo. The former happened. Being open minded is a bad thing only if you like how your mind how it is. reply anon373839 18 hours agoparentprevHonestly, the centerpiece of the article is this: > In plain, simple terms, you need to: do the uncomfortable thing. That’s what the advice boils down to, and it works, and it’s not a hack. It’s growth. There are lots of sophisticated techniques to help you actually do “the thing,” but if your best judgment is that the thing is the right course of action, then you ultimately do need to violate your feeling that it is wrong/dangerous/scary and let your feelings catch up later. reply charles_f 17 hours agorootparentI think that's fair, but there's also value in describing what is being stuck. It's important being able to identify feelings and put words on it. It also helps to see this is something other people go through, so as to feel less alone reply hartator 18 hours agoparentprevYes, it’s very like meditate your way out of pain to point you don’t feel when you are actually burning. Which actually happens IRL more often then what you would assume. reply rdsubhas 18 hours agoparentprevYou seem to have experience in this. Could you recommend or share any alternative approaches? reply mistermann 14 hours agoparentprev> I've come across these kinds of articles before and they always seem to suggest that you rewrite the metaphorical firmware of your brain so that it operates in a way that is advantageous to you. There's some notion of self discovery from first principles. > I'm not convinced that this is generally a good idea. It makes a person unmoored and open to anything (\"so open minded that your brains fall out\"). It's also often amoral. The only metric that's maximised is utility regardless of other more subtle consequences. I'd say you could benefit from a firmware rewrite yourself, because it isn't possible for you to know these things, because they are unknowable. > As I grow older, i think the human experience is more complex than can be captured by a few \"hacks\" like this. Or yours. As Terence McKenna liked to say: - Reality is not only stranger than we imagine, it's stranger than we CAN imagine. - What we call reality is in fact nothing more than a culturally sanctioned and linguistically reinforced hallucination. (A bit off, but close enough.) reply rwasco 19 hours agoprev> I impulssively bought a plane ticket to Panama: > I cried. Haven’t done that in years. > I had the uncomfortable conversation with my business partner to tell them I no longer have passion for it and am leaving them the decision to continue or quit. > I forced myself to talk to strangers every single day. These also could be explained by someone entering hypomania. How would you distinguish stuck/unstuck from depressed/hypomanic? reply sdwr 18 hours agoparentThere's a big grey area in the middle. How you want to label it is influenced by the outcome and whether you think it's a good idea. Quitting your job and flying to Central America is risky. If his life improves from there, then it was getting unstuck. If it gets worse or goes back to the same, then it was a manic episode. History written by the victors! reply benoitmalige 17 hours agorootparentWell said. And for clarification, it was not a job but the business I started 6 years ago. More rencent update: The trip gave me a new perspective and I shifted the business model, rather than shutting it down. reply lgvld 17 hours agorootparentWould you tell me more about it, please? I sometimes feel like I am in the same position as you were: > I had the uncomfortable conversation with my business partner to tell them I no longer have passion for it and am leaving them the decision to continue or quit. When I feel burned out and/or unpassionate by my work, I automatically think about leaving it behind and move on, but after some rest (and discussion) I usually come to another easier-on-my-mind perspective which alleviates me, at least for some time. But I am worried I am just making compromises with myself in order to stay stuck in my comfort zone -- instead of going for more radical changes. Cool article btw, thank you. reply benoitmalige 13 hours agorootparentFirst off, thank you for your comment, and your compliment is very much appreciated. I can relate to what you're describing and can only speak from my own journey. The recurring thought of needing change haunted me for years. Financial stability did play a crucial role in allowing me the flexibility to make significant changes. One of the toughest challenges was the idea of letting go of something I had poured so much into over the years. Before launching my business 6 years ago, I faced a similar crossroads while feeling trapped in an unsatisfying job. With just three months of savings, I took the leap into starting my company, which, fortunately, turned out well. It underscores that there isn't a universal solution; it really depends on each individual's circumstances. What pushed me to start writing this newsletter was a profound sense of lacking purpose. That feeling of purposelessness eventually outweighed the fear of stepping away from the business I had built. I hope sharing this provides some clarity. reply mynameisvlad 19 hours agoparentprevThese energy levels are all relative, though. If you’ve spent most of your life in a depressed state, then suddenly having energy (possibly because you’re dealing with your mental health for the first time) then a lot of things are going to look like “hypomania” in comparison and I feel like that is the case for a lot of people. reply sdwr 18 hours agorootparentWhere it turns into mania, in my experience, is when you start doubling down repeatedly. If your good feeling comes from believing that you are making a huge change in your life, you get anchored to the rate of change, and start needing bigger and bigger feelings to keep up. It's a kind of self-ponzi scheme - nothing has tangibly gotten better, all you have is the belief. reply nullindividual 16 hours agorootparentprevHypomania is an unusual level of excitement and happiness; 'thinking straight' becomes difficult, but it doesn't feel that way in the moment. Happiness and hypomania are easily distinguishable. reply mynameisvlad 8 hours agorootparentWhat, exactly, constitutes “an unusual level of excitement and happiness” to someone who has lived for many years in a depressed state? I’m going to take a gander and say that happiness and hypomania are far from “easily distinguishable” in this case, which was my entire point. If you’ve spent your life depressed, then any amount of happiness is going to look a whole lot like hypomania. reply maroonblazer 19 hours agoparentprev3 of those 4 - crying, having an uncomfortable conversation, and talking to strangers - don't seem like symptoms of hypomania. The impulsive plane ticket to Panama is questionable too. We'd need to know a LOT more about the individual to make the leap from that to a disorder. reply benoitmalige 17 hours agorootparentYou’re right, those actions by themselves don’t necessarily point to hypomania—they’re just snippets of a larger journey. Panama was indeed just the beginning; the real journey has been internal—navigating the landscapes of self-discovery and learning to process my thoughts through writing. Your comment is much appreciated; it’s all part of the conversation about understanding ourselves better. So thank you for that. reply hallway_monitor 19 hours agoparentprevI think these labels definitely apply, if you think about it as a spectrum. Maybe the author's state wasn't pathological in either case but he definitely went from low energy to high energy. I notice these shifts in myself and others; deciding to change it is the only way to get yourself out of a rut in my experience. When I do feel a little too \"amped up\" I use that energy and just make sure to avoid big decisions. reply charles_f 17 hours agoparentprevI guess you could see it as one of the causes of depression? Kinda like having a cold describes a situation that can be caused by 10s of thousands of viruses, in the end you still feel the same kind of shitty. Still useful to know what kind of virus it is so it can be treated? reply imbnwa 19 hours agoparentprevI don't see the virtue of grounding an empirical wisdom in some species of scientific (mechanical might be better the term?) description. Funny enough, in relation to your comment, I've never met anyone motivated by this brand of advocacy who didn't suffer swings in depression; as if 'science' were some exterior mass whose gravity/gravitas would pull them up and out from the dregs of the swamp by virtue of its sheer presence to mind. reply ericmcer 17 hours agoprevDon’t people have partners, kids, aging parents, hell even pets that depend on them? The authors version of being stuck seems like a luxury, an inward exploration to find out how you are holding yourself back. To even approach it from that angle you need to have no one depending on you. I can’t “shake things up” by trying a new career or taking impulsive vacations because other human beings rely on me being “stuck”. reply Dessesaf 2 hours agoparentI think you overestimate how strongly those ties bind people to their current situations. I don't know your situation, so don't take this the wrong way. Maybe your ties truly do bind you. And maybe that's perfectly fine. But I think for the vast majority of people, simply having a pet or a partner or a kid does not really tie them to their current situation. It's just about what you value more. Of course all of these ties change the tradeoffs of your choices. But every circumstance in life is a constraint on your choices. And large scale changes are possible even with all those things, should you desire it. There are plenty of people that take impulse vacations, change careers, countries, friends, partners, while having all of the dependents you list. And if you consider all those tradeoffs and decide change isn't worth it, that's perfectly fine. But I see people use this to state that they had no choice in the matter. No, you had a choice and you chose from a constrained but still massive set of potential life situations. This is simply the one you prefer. To bring this back to the article: Take this and keep asking the same question. You’ll get to the root of it soon enough. In this instance, your feeling of being stuck is less about the external circumstances and more about internal barriers—fears, beliefs, and assumptions that keep you tethered to the familiar, however unsatisfying it may be. If you keep asking the question, but your underlying answer is \"It is actually more about the external circumstances than internal barriers\", then that's your answer. You're not really stuck, you just made your choices and are where you wanted to be. reply benoitmalige 17 hours agoparentprevYou raise an excellent point, and it's true that everyone's circumstances are unique. In my case, my family is in another country, and my pursuit of the American dream led me to work so intensely that I overlooked the simple act of living. While I understand that not everyone has the flexibility to make drastic changes due to their responsibilities, I believe we all have our own paths to feeling unstuck. My writing is a personal tool to help me navigate my path and make sense of my experiences. Thank you for sharing your perspective—it's a vital reminder of the diverse lives we all lead. reply newZWhoDis 9 hours agoparentprevYou massively underestimate the social atomization of the modern world. reply indigoabstract 14 hours agoprevEvery time I read about these mind hacks that are supposed to transform your life and attitude, I'm reminded of the Novelty effect: https://en.wikipedia.org/wiki/Novelty_effect It's the best thing ever, until you get used to it, than it becomes the same old. Sometimes I think the human race is completely ruled by this effect, ever chasing the next new thing as it seems so much better than the boring old thing. reply jerrygoyal 3 hours agoparentwhat makes us feel good is dopamine and its cousins. Repeatedly doing the same thing produces less dopamine. This is how we are hard-wired, but it's essential for our survival. reply spxneo 15 hours agoprevIf you feel stuck it means you need new habits because the old ones are not serving you well! Program your environment, set up commitment devices to lock in your future behaviour that is positive and effortless. Do not ever set goals, they are high-risk low pay off mental bets that will eat you if you fail (odds are against you). reply ausbah 9 hours agoparentI think goals are good to define what you’re trying to work towards, but you need concrete actions you will take to get there reply copperx 15 hours agoparentprevWhat is an example of a commitment device? reply mimischi 14 hours agorootparentThis might be cliche, but if you can afford to (time and money), have a go at „Atomic Habits“ by James Clear. It’s an easy and enjoyable read, that outlines a framework of how you can build new habits and get rid of your old, unwanted ones. reply paulgerhardt 14 hours agorootparentIf \"Atomic Habits\" doesn't sit with you (I found the endless anecdotes about the author's high school baseball career detracting) I recommend \"Four Thousand Weeks: Time Management for Mortals\" - it's slightly less targeted to fans of Ryan Holiday/Tim Ferris and slightly more targeted to productivity hack tooling reformed/burnt-out individuals. reply spxneo 14 hours agorootparenthe only talks about his experience through his rehabilitation in the first chapter the rest of the book he draws from real world examples from other industries and history to illustrate the specific point. i think they are very practical as it is easier to remember stories than recalling step by step points reply spxneo 14 hours agorootparentprevyeah pretty much thats where all my quotes are from i recommend this book to everyone i meet it is life changing reply spxneo 14 hours agorootparentprevaddicted to smartphones? leave it behind when you go to study at the library and you have no choice but to study. you are not hitting the gyms for the 10 years you been telling people? subscribe to a local gym on the way to or back from work! (brain: \"well i already spent money and its too embarassing to cancel now and besides the gym is literally down the same street i dont even have to drive there welp guess im going\") reply PartiallyTyped 15 hours agoparentprevI found that it is extremely difficult for me to set habits and keep them. I used to go running 4-5 times a week for months, and I couldn't make it stick. reply spxneo 14 hours agorootparentThe best way to build a positive habit is to make it not painful and easy. Heres my advice: If 4-5 times a week is not sustainable then make it 100x less (whatever point you feel is not painful). If you used to run 50km a week, try 500m each session. Fight the urge to continue running. The goal of this exercise is to build the \"show up\" muscle. Another hack on top of this is to not tell yourself you are going for a run, you just need to put on the running shoes. (brain: \"i put on my shoes might as well go for a run now\") Step further is you go to sleep in your running clothes. Principle is to always reduce friction of good habits you want to build and add friction to bad habits you want to stop! Godspeed reply mhb 20 hours agoprevPart of this might be that, before starting something, you can maintain the illusion that everything will go as you planned. Actually starting requires abandoning that comfortable illusion in the face of the reality of inevitable obstacles and compromises. reply thyrox 19 hours agoprevIs there survivor bias in being stuck? We have accepted that people who leap out of their comfort zone end up always successful. But a lot of time this is not true. I know few people who have regretted jumping and making impulsive decision in the name of growth. reply benoitmalige 17 hours agoparentGood point. Nothing is without risk, and it doesn't always lead to success. Every jump, impulsive or calculated, has its own set of outcomes and consequences, and it's important to consider both sides. reply abhaynayar 5 hours agoprevLove this. I follow the same approach for some of the deepest set issues I have. Learnt it from CBT. Is working wonders for me. Just something from my experience - taking action is 1000x more important than challenging thoughts and core beliefs. The latter should be done, but quite often it is done as a form of procrastination. So the rule I have is - I only challenge thoughts after I've TAKEN ACTION in a way that has triggered those feelings. So facing those fears and discomforts is the meat of the matter, and the self-awareness just makes the whole process a bit more efficient and well-directed. reply navane 18 hours agoprevSo this guy had all the opportunities, choose to spend his time to herd clicks for some big corps, finds out it's an empty life and suggests to do things like \"but a plane ticket to Panama\" to get unstuck. Sometimes your therapist asks you to write a letter but you're supposed to keep those in your drawer. reply benoitmalige 17 hours agoparentThank you for sharing your thoughts. My writing is a method for me to navigate and clarify my own journey of introspection and growth. \"You\" is often a reflection of myself in the mirror of words. Whether it strikes a chord with others is a welcome bonus, but not the end goal. The essence of my message stems from a personal revelation: after dedicating six years to a company I built from the ground up, I realized I was losing myself in the pursuit of success. It’s not about wandering aimlessly for profit; it's about the quest for meaningful work and rediscovering the parts of myself that got buried in the hustle. reply galleywest200 19 hours agoprevWhen I was getting my undergraduate degree one of my chemistry professors taught us that \"I don't know\" is one of the most important sentences a scientist can and should say. You will be viewed as a source of knowledge or as an expert, do not fill people with BS or guesses. reply mlhpdx 17 hours agoprev> do the uncomfortable thing. I’ve done this ever since learning about “brain plasticity” as a child. My logic was/is simple: if I didn’t want something to feel uncomfortable I had to do it regularly. To me, building physical strength/ability and mental strength/ability were the same. Sitting on a moving bike was uncomfortable and now isn’t, and architecting distributed systems, too. Perhaps simplistic, and not worth a long article, but pretty close to the truth it seems, as I look back. reply mycologos 8 hours agoprevI've now watched several dudes go through this process by following their blogs: 1) Build a small to medium size following writing about technical or entrepreneurial stuff 2) Make it to some kind of financial success (sell a company, advance at a big tech org) 3/4) Reflect and find yourself unhappy and make big personal life changes, leaving your wife/partner and leaving your company 5) Adopt an increasingly effusive, emotional, and untethered style of writing and living (\"today I cried my way to a beautiful and uncomfortable conversation after a three-day retreat on an island with other former tech workers. Also I tried some drugs. I can't wait to reflect more!\") I guess this is kind of unkind, there are plenty of worse ways of living. And TFA is not fully an example of it. But it's strange to see it play out repeatedly. Maybe some component of emotional repression is more common in software people and when financial constraints fall away it presents itself as the next thing to optimize, or maybe nobody writes blog posts about living a conventional life committed to a small group of people, but it almost feels like a cliche at this point. reply gnum4n 10 hours agoprevAside: I hate the popup subscription CTA that asks for my email address in the middle of reading the article. How could I know whether I want to subscribe before I finish reading, and why interrupt my reading the article at all? There are clearly marked \"Subscribe\" buttons at the header and the footer of the page that I could use if I wanted emails. But there's no RSS/Atom feed. I'm just a grumpy programmer that gets annoyed by unnecessary interruptions. I'll see myself out... reply malkosta 14 hours agoprevAround a year ago I had one of those 6 months long tasks that you have no idea where to start. I hitted my had on the wall a few times, and had to erase first month work completely. In the end what worked was: - Spend a day or two creating a fuzzy view of the whole problem. Pay attention to the rabbit holes, do not fall in them, be superficial. - Spend a day or two creating a detailed view of the next 2 weeks. Go as deep as you can, but pay attention to not prepare more than 2 weeks of work, because things WILL change. And you will lose a lot of work. Minimize that. - Execute. - Repeat. After a couple iterations your estimation will be much better and you will see the light in the end of the tunnnel. reply nefrix 16 hours agoprevThis article reminded me of David Mumet’s character, Edmond. In that play things does not end very happily for the main character after he asks himself to many “whys” and starts to do lots of changes on his day routines. reply manmal 16 hours agoprevAt the end of the article, it becomes clear that author went abroad for a while, which might have inspired the article. Changing one’s environment is indeed a good way to get unstuck - eg some addicts only ever get out of the vicious cycle by moving away and cutting ties with everything/everyone that kept them stuck. Maybe the rest of the article has merit too, but I’m not sure it’s not a post hoc rationalization. reply cess11 19 hours agoprev\"Here's the problem: Your brain and its cortical columns are designed to conserve energy at all costs.\" How does this model explain substance addiction? 'You need to consciously and repetitively seek out \"False\" signals and embrace the discomfort they bring. You need to construct a new reality.' Much like the 'why aren't you growing? you need to grow. grow!' sentiment in the beginning this seems more in line with something tumorous than what I'd like to be or become. reply benoitmalige 17 hours agoparentThe brain's tendency to conserve energy does play a part in substance addiction, reinforcing behaviors that provide a high reward for little effort. However, this model overlooks the complexity of addiction, which involves not just the pursuit of rewards but also the avoidance of withdrawal discomfort. The conversation around addiction would also need to consider the neurological changes that perpetuate the cycle, making it a state that's incredibly challenging to break free from. However, I can't specifically speak on physical addiction, as it's outside my area of expertise. The intention behind my writing is to unpack my own thoughts, aiding me in my journey toward self-discovery and the pursuit of purpose. reply pier25 18 hours agoparentprevIt explains psychological addiction. You grow attached to a familiar and seemingly safe numbing sensation. Physical addiction is another thing. reply cess11 16 hours agorootparentOK, so this is a model relevant for people that believe in a dualist worldview? Kinda like a soul that needs to be kept pure or something? reply throwaway11460 17 hours agoparentprevSubstance addiction is free reward with no effort at all. Sounds like it fits! reply wvh 16 hours agoprevThis person is experiencing the same kind of existential conundrum and arriving at the same conclusions as I am at this not so easy point in life, and although I can't really offer them more wisdom or insight as they themselves are offering in this post, I wish them all the best in unlocking their brain and finding better ways to feel satisfied and a better sense of meaning. reply padolsey 10 hours agoprevWhy does this feel like an AI wrote it? And the whole website feels the same. The OP is here commenting as well and, though polite, just gives off an AI vibe. Really off. reply benoitmalige 9 hours agoparentSorry it feels that way. I promise you I'm real, and the newsletter takes me time to write. I'll take that as a compliment :) reply richrichie 18 hours agoprevAny time i see an article that talks in definitive terms what parts of the human brain do and extrapolate that to tactics to mold behavior patterns, i take a ritual bath and open a bottle of whiskey. reply benoitmalige 17 hours agoparentI appreciate your perspective — it's always good to approach definitive statements with a healthy dose of skepticism. My articles are really personal reflections, a way for me to process and sort through my own thoughts. When I'm offering advice or insights, I'm often speaking to myself, hoping to uncover a bit more clarity in the complex web of human behavior. reply mikewarot 17 hours agoprevKnowledge of the illusion can be enough to help you route around it. You can not escape from a prison you don't know you're in. reply benoitmalige 17 hours agoparentAbsolutely, awareness is the first step towards change.Thanks for that insight, it's a valuable addition to the idea of mental barriers and personal growth. reply hyperhello 20 hours agoprevJust a tip, it’s actually “break free”, not “brake free”. reply oskapt 18 hours agoparentAnd it also isn’t “dreadfull.” I am very skeptical of taking any advice from a person unable to use basic spell checking before publishing an article. I’m also skeptical of advice from someone who has only recently discovered a possible solution for his very personal problems and feels he should share it with the world. In the 12-step world there are people who only do the first and last steps. They’re called two-steppers. You can look up the steps yourselves, but essentially it translates to, “we admitted we were powerless, that our lives had become unmanageable, and having had a spiritual awakening, we told everyone else how to fix their lives.” There are a lot of steps in between being stuck and becoming unstuck. The author should just quietly repair his life and shelve his egotistical need for external validation. reply benoitmalige 17 hours agorootparentThank you for pointing out the spelling mistake. Your skepticism is valid; advice should be considered carefully, especially when it’s based on personal experiences that might not be universally applicable. My letters are less about instructing others and more about sharing my reflections as I navigate through my own journey. It’s not about two-stepping around the hard work of self-improvement, but about the ongoing process of understanding and growth. As for the need for validation, I see this platform more as a conversation with others who might relate or benefit from my experiences, rather than seeking applause. Again, I appreciate your candor — it’s an important part of the dialogue. reply Eudaimion 17 hours agorootparentprevYou are taking things, way too seriously. > I’m also skeptical of advice from someone who has only recently discovered a possible solution for his very personal problems and feels he should share it with the world. Advice should never be just taken at face value. Even if it is a successful person giving it. Personally, any advice from Elon Musk is worth less, than any random link on HN. > The author should just quietly repair his life and shelve his egotistical need for external validation. This is just mean. I one hundred percent guarantee that there are other people, with the same problem as the author. And his solution will be useful. Sharing is a net good. TL;DR through a Dev World Prism: No, I completely disagree that only staff engineers should be allowed to have technical blogs. reply ufocia 17 hours agorootparentprev> two-steppers ... “we admitted we were powerless, that our lives had become unmanageable, and having had a spiritual awakening, we told everyone else how to fix their lives.” LOL, though it may be the starting point of all the self-help systems, which later evolve iteratively adding intermediate steps. reply rolandog 19 hours agoprevEstoy orgulloso de tí ;). I like your article. It takes courage to pivot and get out of your comfort zone. reply benoitmalige 17 hours agoparentMuchas gracias. Your words mean a lot to me. It's one thing to step out of the comfort zone, and another to share that journey openly. I'm glad you liked the article, and I hope it can inspire courage in others as well. reply flowingfocus 10 hours agoprevThe whole process of predicting the world and trying to match incoming data against the predictions is explained in more detail in \"Surfing Uncertainty\". Relevant SSC book review: https://slatestarcodex.com/2017/09/05/book-review-surfing-un... reply stevenjgarner 11 hours agoprevI personally found \"The Neuroscience of Change\" by Kelly McGonigal [1] most useful in becoming \"unstuck\". Essentially for myself it was being taken through the realization that life is change, how with every breath the trillions of cells in my body undergo substantive transformational change, that my self-delusion of being stuck became comical. [1] https://www.soundstrue.com/products/the-neuroscience-of-chan... reply 0xfedcafe 12 hours agoprevI fully agree with this article, based on my experience, which I achieved purely empirically. I was stuck for a long time and couldn't do new, challenging things for myself; it felt almost impossible. The solution was to get more and more challenges in life. This happened almost accidentally, but a lot has changed. After all the discomfort that occurred bit by bit, it has become much easier to do something new. Even if it feels disgusting and you want to shout, the only solution is to just do it. Also, building new habits helps a lot because it takes at least several weeks, so you have to accept this challenge and do it. reply betimsl 13 hours agoprevPretty awesome article -- rock on reply benoitmalige 9 hours agoparentMuch appreciated reply dclowd9901 16 hours agoprevI think I’m one of those few (lucky?) people who disdains stagnation. If I start to feel comfortable in my job, I start to get this resonant itchiness to move on from it. I find a new job and suddenly it dawns on me how much new shit I have to learn and do. I learn and do and the cycle repeats! reply Exuma 19 hours agoprev [–] Now that is a great fucking article. I have to tell people this all the time and no one can truly grasp what I'm saying. You have absolutely nailed it. reply benoitmalige 17 hours agoparent [–] Thanks so much, Exuma! It’s really encouraging to hear that the article struck a chord with you. Sometimes the hardest ideas to convey are the ones that matter most, and it’s awesome to know it resonated. Here’s to hoping it helps more people get the message you’ve been trying to share. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the concept of feeling stuck and highlights how our brains naturally avoid discomfort and crave stability.",
      "Embracing challenging and unfamiliar situations, instead of staying in our comfort zones, is emphasized as a way to foster growth.",
      "By challenging beliefs, facing fears, and actively seeking discomfort, individuals can break free from feeling stuck and create a more fulfilling reality."
    ],
    "commentSummary": [
      "The article discusses personal growth, overcoming feeling stuck, and self-discovery through habits, meaningful work, and individualized approaches.",
      "It touches on self-help advice, meditation, trauma's impact, and challenges in finding purpose, emphasizing trying new experiences and seeking feedback for growth.",
      "The author shares reflections, highlighting addiction, problem-solving, and the continuous journey of self-discovery, offering insights, recommendations, and personal stories for reader inspiration."
    ],
    "points": 195,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1711891310
  },
  {
    "id": 39888383,
    "title": "Email Deliverability Guidelines for 2024: Implementing Security Standards",
    "originLink": "https://www.xomedia.io/blog/a-deep-dive-into-email-deliverability/",
    "originBody": "Security A Deep Dive into Email Deliverability in 2024 Quick Summary: In this post we cover the new email deliverability guidelines for Gmail and Yahoo. Last updated: March 31, 2024 by Chief Science OfficerLeave a comment Share Twitter Facebook Linkedin Email Copy link Table Of Contents Overview Who's Affected? Timeline The Guidelines Sender Authentication Impact Tools Implementation Bonus Summary References Around 50 years ago, in October 1971, Ray Tomlinson, a graduate of MIT, sent the first email email over a network.. Last year, ~121 trillion emails were sent between ~4.3 billion people. Email is the the most important written form of communication on this planet and will remain so for the foreseeable future. Overview On October 3, 2023, Google and Yahoo announced upcoming email security standards to prevent spam, phishing and malware attempts. Outlook.com (formerly Hotmail) is also enforcing these policies. With the big 3 Email Service Providers (ESP) in agreement, expect widespread adoption soon. Today’s threats are more complex than ever and more ESPs will begin tightening the reigns. Failure to comply with these guidelines will result in emails being blocked beginning April 2024. UPDATE: This policy is now in effect with senders already reporting delivery problems. In this article, we’re going to cover these guidelines and explain what senders must do in order to achieve and maintain compliance. The biggest change involves implementing email authentication standards like SPF, DKIM, and DMARC. These standards have been around for a while, but are only now being strictly enforced by major email service providers. Like SSL (HTTPS) for the web and MFA (Multi Factor Authentication) to protect your online accounts, organizations will be expected to conform to these guidelines. This is what non-compliance (for Gmail) looks like in an email server log: host gmail-smtp-in.l.google.com [108.177.15.26] SMTP error from remote mail server after pipelined end of data: 550-5.7.26 This mail is unauthenticated, which poses a security risk to the 550-5.7.26 sender and Gmail users, and has been blocked. The sender must 550-5.7.26 authenticate with at least one of SPF or DKIM. For this message, 550-5.7.26 DKIM checks did not pass and SPF check for [MYDOMAIJN.com] did not 550-5.7.26 pass with ip: [xxx.xxx.xxx.xxx]. The sender should visit 550-5.7.26 https://support.google.com/mail/answer/81126#authentication for 550 5.7.26 instructions on setting up authentication 550, \"5.7.26\", \"This message does not have authentication information or fails to pass authentication checks (SPF or DKIM). To best protect our users from spam, the message has been blocked. Please visit Prevent mail to Gmail users from being blocked or sent to spam for more information.\" Here’s the complete listing of Gmail SMTP errors codes. This is quickly becoming a mandatory standard for senders, so every business will need to become familiar with them – or risk email deliverability to customers. Who’s Affected? Enforcement primarily pertains to Bulk Senders: “A bulk sender is any email sender that sends close to 5,000 messages or more to personal Gmail accounts within a 24-hour period. Messages sent from the same primary domain count toward the 5,000 limit.” NOTE: This limit is imposed on emails sent from the same primary domain. For example, emails sent from example.com, blog.example.com and web.example.com are considered the same domain. This limit only has to be reached once for the domain to be considered a permanent bulk sender. These guidelines require bulk senders to enable SPF, DMARC and DKIM for their domains. While these guidelines primarily affect bulk senders, senders with less volume per day can also be affected if they are not adhering to these guidelines. We recommend that all organizations, regardless of daily volume – implement these standards and adhere to guidelines. It’s also very important that both senders and recipients understand these requirements. Implementing them protects partners, customers and anyone receiving email. Improving email security and user experience can indirectly influence how your emails reach user inboxes Timeline Google Starting February 2024, Gmail will require bulk senders to authenticate their emails. Changes will be gradual and progressive, giving businesses time to implement and test these changes. February 2024: Bulk senders who don’t meet sender requirements will start getting temporary errors (with error codes) on a small percentage of their non-compliant email traffic. These temporary errors are meant to help senders identify email traffic that doesn‘t meet guidelines so that they can resolve issues that result in non-compliance. April 2024: Google will start rejecting a percentage of non-compliant email traffic, and will gradually increase the rejection rate. For example, if 75% of a sender‘s traffic meets requirements, Google will start rejecting a percentage of the remaining 25% of traffic that isn’t compliant. June 1, 2024: Bulk senders must implement a clearly visible one-click unsubscribe in the body of the email message for all commercial and promotional messages. Yahoo Q1 2024: All bulk senders will be required to to authenticate their email, enable easy one-click unsubscribe (also starting June 2024) and only send emails users want. The Guidelines Google Email sender guidelines Email sender guidelines FAQ Yahoo Sender Requirements & Recommendations As mentioned earlier, Outlook.com (formerly Hotmail) is also enforcing these policies. Here’s a quick summary of the guidelines: Sender Authentication: Senders should implement email authentication protocols like SPF, DKIM, and DMARC to prevent email spoofing and phishing attempts. We’ll cover this in more detail later in this article. Bulk Senders Requirements: Sending unsolicited bulk emails can lead to deliverability issues (spam filtering) and reputation damage. Email providers have various algorithms and user reports to identify and filter spam. Google will require bulk senders (those sending 5,000+ emails per day to Gmail) to meet stricter requirements for compliance with spam thresholds. Easy Unsubscribe: Implement easy unsubscribe options (One-click Unsubscribe). Gmail users have tools to report spam, unsubscribe from unwanted emails and control their inbox experience. If it is too difficult to unsubscribe from your emails, customers will be more likely to flag your email as spam. Additional links provided in the ‘References’ section at the end of this article. Engagement: Avoid misleading subject lines, excessive personalization, or promotional content that triggers spam filters. Focus on providing relevant and valuable information when considering email content. Special Considerations: Keep your email spam rate is less than 0.3%. Don’t impersonate email ‘From:’ headers. Ensure that sending domains or IPs have valid forward and reverse DNS records, also referred to as PTR records. Use a TLS connection for transmitting email. Make sure your forward and reverse DNS records are valid. Ensure receivers can easily unsubscribe from your marketing messages. Format messages according to the ‘Internet Message Format standard’ RFC3322 If you regularly forward email, including using mailing lists or inbound gateways – add ARC headers to outgoing messages. For direct mail, the domain in the sender’s From: header must be aligned with either the SPF domain or the DKIM domain. This is required to pass DMARC alignment. Marketing messages and subscribed messages must support one-click unsubscribe, include a clearly visible unsubscribe link in the message body and process recipient unsubscribe requests within 2 days. Reference this for the full list. Sender Authentication In this section e discuss Email Authentication and how to avoid the spam folder. There are 3 authentication standards to help protect an organization’s email: SPF (Sender Policy Framework) specifies the servers and domains allowed to send email for your business. This protects against spoofing and helps prevent your emails from being flagged as spam. This is added as a record on public DNS server that is used to check the source IP of the email and compares it with a DNS TXT record. v=spf1 ip4:173.236.251.117 include:netblocks.dreamhost.com include:relay.mailchannels.net mx ~all DKIM (DomainKeys Identified Mail), used to digitally sign every outgoing message sent from your organization. The receiving server uses this to verify that it came from your business. It is a unique key for domain that allows mail servers to verify email authenticity and resist tampering. It is a generated key that is configured on a public DNS server. v=DKIM1; k=rsa; h=sha256; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmY24P6ntSL6CbVrv++vyTdgVJP4jxAEDoYbpo2vMEpOb2SDnUsiiBnd8rINyMh9BMA5whxKC/w7oqYD9dr5mvfPtkVPBSz9PqFHE2s/QsFnJlBsUJJrLFBlSXw+F95TTZyqNboANJuGCGpbg207KloAd0PZxaHjyBqj9fTfFLUCp/TrEmaMZ1E3LwqXd2jqC2mUCBcIMzIOcT68eU0b5LTlRMPL7k07BOlMGSx8Ez2wYltyDXPQc9IM8rOlMmtO92O/PkyqhyqJF+QxMSAgV6CLhtghmwbRFjvKUbkAtdCYmfRqDiPrrTCZhV7RX6+3gg7F6MPstL+KefKKToFunFQIDAQAB DMARC (Domain-based Message Authentication, Reporting and Conformance) is an email authentication protocol designed to give domain owners the ability to protect against spoofing, phishing, email scams and other cyber threats. It instructs receiving servers on how to handle outgoing messages from your organization that don’t pass SPF or DKIM. More specifically, DMARC standardizes how email receivers perform email authentication using the SPF and DKIM authentication mechanisms. The policy is is published in the public Domain Name System (DNS) as text TXT records and used by the receiving email server to authenticate incoming emails based on the name / value tags that are defined: \"v=DMARC1; p=reject; sp=none; fo=1; rua=mailto:dmarc_rua@emaildefense.proofpoint.com; ruf=mailto:dmarc_ruf@emaildefense.proofpoint.com\" NOTE: The v and p tags must be listed first, other tags can be in any order. DMARC: Reduces email Spoofing & Phishing: Prevents bad actors from impersonating an organization’s domain by verifying which domain the email originated from. The domain name in the From: field in the email envelop header is inspected and aligned with other domains authenticated by either SPF or DKIM: From: John DoeImproves Email Deliverability: Sets policies for how the receiving email server should deal with failures. When emails fail DMARC authentication, the DMARC policy instructs how the receiving email server should handle emails that fail SPF or DKIM checks. The options are: Accept, Reject or Quarantine the email – ensuring senders that their legitimate emails are reaching recipients inboxes. Provides Reporting & Feedback: DMARC provides a reporting mechanism for policy actions performed by the above policy. These reports provide insights to domain owners about emails sent from their organization (even if they were sent by unauthorized parties). They can be helpful in identifying and addressing email spoofing attempts. Here’s a simple diagram to help explain the entire email journey: Sender composes and sends an email. Sender’s MTA (Mail Transfer Agent on mail server) adds a DKIM signature to the email header as a special field.. Recipient’s MTA checks SPF and DKIM records. DMARC alignment is verified, and the policy is applied: If a message passes authentication by the receiving server – Deliver to user's inbox. If a message fails authentication by the receiving server: Quarantine (send them to recipients’ spam folder). NOTE: If the receiving mail server has a quarantine configured, messages might be sent to quarantine, rather than directly to the recipient's spam folder. Reject messages are never deliver to the recipient. The receiving server usually sends a bounce message to the sender Here’s an example email envelop from an organization that passes all of the email security guidelines: Proper configuration of these standards shields against attacks and increases deliverability so that messages land in inboxes, not spam folders. Impact Google continuously updates its algorithms and user-reported data to improve email filtering and user experience. Google’s AI Spam filtering algorithms block 99.9% of spam, phishing and malware attempts from landing in your inbox. For 1.8 billion accounts, 15 billion unwanted emails are being blocked daily. The following email statistics reveal the impact these new security guidelines will have on deliverability and engagement (especially for email marketing campaigns and newsletters): In 2025, the number of email users is expected to reach 4.6 billion (Techjury) In 2023, we expect to see an average of over 347 billion emails sent per day (Oberlo) There are projected to be an estimated 4.37 billion email users in 2023 (Statistics Millennials and Gen Xers rely on their email more than any other generation at 98% (Statista) In 2021, an average of just over 2 hours a day are spent on email (Statista) 63% of people who open up an email try and find a discount (LXA) 99% of email users check their inbox every day, with some checking 20 times a day (HubSpot) The image above tells us that email marketing revenue is estimated to reach almost 12.5 billion by the end of 2024 (Statista) 58% of consumers check their email first thing in the morning (Optinmonster) 84.3% of consumers say they check their emails at least once a day (Mailjet) Additional email stats from Sixth City Marketing A list of the most email providers as of 2024 (in millions): Provider Market Share # of Users Notes Gmail 29.67% 1800 Dominant provider, growing market share Apple Mail 22.82% 1000 Apple device users Outlook.com 4.3% 500 formerly Hotmail Yahoo Mail 2.55% 150 Declining market share Yandex.Mail 1.1% 65 Popular in Russia / Eastern Europe QQ Mail 1% 60 Dominant in China GMX .7% 42 Popular in Germany AOL Mail .6% 36 Declining market share Zoho Mail .3% 18 Business-focused provider ProtonMail .25 15 Privacy-focused provider Sources: Oberlo, Litmus, Statista NOTE: Countless smaller providers and self-hosted (businesses) email solutions exist. Accounting for a collective market share is difficult to accurately assess. Tools For convenience, a curated list of free online resources to help you set up, check and maintain your organization’s email hygiene: SPF Generator DKIM Generator DMARC Generator SPF Check DKIM Check DMARC Check Google Safe Browsing site status: A free online site transparency service you can use periodically to determine if your domain has been added to their unsafe site list. Also check any other domains that are linked to yours. SuperTool: A free online tool to verify your MX, SPF, DKIM and DMARC records. It also has a ‘Blacklist Check’ tool that allows you to verify if your email IP / Domain has been blacklisted on any of the DNS-based blacklist (DNSBL) services. Email Health Report: A free, comprehensive online email health check that looks for DNS, domain and server issues. It also searches to see if your domain is listed on any blacklist databases. The results of this report can be used to address email related issues for your domain. IP and Email Blacklist Check: A free online tool to determine if a domain, IP address, or email address is enlisted in the DNSBL and other blacklist databases for suspicious activity. Blacklisted addresses cannot send any emails. It‘s a stricter form being blocked that requires going through an appeals process to be removed from their list. Email Deliverability Report: Send a test email using this tool and it will: 1)Analyze the headers and blacklist reputation of your outbound IP address, 2) Verify your SPF record and 3) Email link with a comprehensive deliverability report. DMARC Report Analyzer: This tool will make DMARC Aggregate XML reports human readable by parsing and aggregating them by IP address into readable reports. DNS Reverse Lookup: Using the MxToolbox SuperTool again, perform an MX lookup from the domain you send emails from to determine your MX IP addresses (usually 2). Use these IPs to perform a DNS ‘Reverse Lookup’ from the MxToolbox SuperTool drop-down menu. Google Postmaster Tools This is a free service that provides valuable insights and diagnostics. If you are a bulk sender (5,000 daily emails), it’s worth setting up to get valuable information about: When recipients mark your messages as spam. Why your messages might not be delivered. If your messages are authenticated. Your domain or IP reputation and its impact on message delivery rates. Yahoo Sender Hub: Yahoo’s version of Postmaster Tools NOTE: Starting February 2024, both Google and Yahoo will require organizations to keep their spam rates low. Google recommends keeping spam rates reported in Postmaster Tools below 0.10% (soft limit) and avoid ever reaching a spam rate of 0.30% or higher (hard limit). Implementation Implementing these guidelines will no doubt pose challenges for smaller organizations with limited resources. As mentioned earlier, many companies faced similar challenges transitioning to HTTPS for the Web, and MFA for online accounts. Email compliance is now rapidly becoming a mandatory standard that every business must become familiar with. While these efforts may seem daunting, it’s important to understand that adapting to stricter security guidelines brings significant benefits such as stronger email security, customer trust and ensures that your emails are reaching their inboxes. Prioritizing these measures will elevate your brand and pave the way for business success. To implement email authentication, consult with your service provider’s resources or support. For example, here’s collection of resource links for some well-known service providers: Google SPF DKIM DMARC Twilio SendGrid How to Set Up Domain Authentication Internet Standards (SPF and DKIM) and Deliverability SPF DKIM DMARC DreamHost SPF Overview SPF DKIM DMARC Bluehost SPF DKIM DMARC Cloudflare NOTE: Cloudflare provides a nice tool in their dashboard to help you generate and implement these records. SPF DKIM DMARC Adoption is an important first step, being vigilant in the face of evolving security threats is equally important. Businesses will also need to be diligent about keeping up with changing standards. By leveraging automation, organizations can effortlessly keep pace with shifting standards – ensuring continuous compliance: Continuously monitor email provider documentation for policy updates. Query and perform DNS record verification and validation. Analyze DMARC reports (both aggregate and forensic) for anomalies based on defined keywords. Configure an email alert to notify you of any events triggered from the above steps. TIP: As an additional measure, you can ask your customers / subscribers in the footer of your email communications to have them add your email address to their email address book/safe senders list or have them drag the email to the Gmail Primary folder. This will ensure they receive emails from your business domain. Save valuable time and trouble: Our white glove email implementation services are based on best industry practice and designed to deliver optimal value for your business. Bonus In this section we’re going to show you a couple of methods hackers employ to exploit email security loopholes. Failing to secure email systems properly exposes your customers to malicious actors who can hijack email domains for nefarious purposes. Email Spoofing and Phishing are two tactics commonly used to trick recipients: Spoofing (impersonating): Involves sending emails that appear to be from a legitimate sender, like a bank, a company or even a friend. The goal is to trick the recipient into clicking on a malicious link, opening an infected attachment or revealing personal information. Spoofing is often used in phishing emails that aim to extract personal data from recipients. Phishing (action): Is a type of cyberattack that uses deceptive emails to trick recipients into revealing sensitive information, clicking on malicious links or downloading infected attachments. The attackers typically impersonate legitimate organizations or individuals to gain the recipient’s trust and make the email appear believable. The following command line mail utilities are commonly used in scripts to change email envelop header information such as “From:”, “To:” and “Subject:”. A hacker will also go to great lengths setting up a remote web server with a login page that looks like the login page of a banking institution or social media platform. These commands are automated using scripts (small programs) that are easily adaptable to impersonate an email from your bank with a link to a fake login page to fool you into entering login credentials. These attacks can be very persistent, crafty and intelligently designed to evade both human and spam detection measures. The mail / mailx utility: cat login.phpmail -s \"$(echo -e \"TestContent-Type: text/html\")\" john.doe@example.com -- -f support@bank.com echo \"email body\"mailx -s \"An email subject\" recipient@example.com -a \"From: sender@example.com\" The Mutt Mail User Agent mutt -H - \"$2\" <<EOF From: $1 To: $2 Subject: $3 Importance: high $4 EOF Summary In this article we covered: The new mandatory guidelines and best practices that email providers are enforcing in 2024 and the impact it will have on businesses. We explained how to implement these guidelines and how to remain compliant. We also provided some free online tools to help with implementation and compliance. Bulk senders are required to authenticate emails, be careful not exceed spam rate thresholds and implement one-click unsubscribes for email marketing campaigns and newsletters. As email service providers enforce stricter guidelines, organizations need to be proactive with compliance. While implementing these guidelines can be complex, the benefits will be well worth it. Not only will it enhance security, it will boost email deliverability and engagement rates. Ultimately this will drive more sales and revenue, helping you stand out in a crowded digital marketplace. Just as many companies faced similar challenges transitioning to HTTPS and MFA, email compliance is rapidly becoming a standard that every business must become familiar with. These security benefits will elevate your brand and pave the way for business success by building customer trust and ensuring emails reach their inboxes. References Google Email sender guidelines Email sender guidelines FAQ New Gmail protections for a safer, less spammy inbox Understanding Gmail’s spam filters Spam does not bring us joy—ridding Gmail of 100 million more spam messages with TensorFlow Yahoo Sender Requirements & Recommendations Postmaster @ Yahoo & AOL Yahoo Mail Blog One-Tap Unsubscribe Email Sender Support Outlook.com (formerly Hotmail) Sender guidelines open-spf.org dmarc.org ww.dkim.org RFCs SPF RFC 7208 DKIM RFC 6376 DMARC RFC 7489 Internet Message Format RFC 5322 SMTP RFC 5321 Mailing Lists RFC 2369 One-Click Unsubscribe RFC 8058 Important: Sending ‘unsolicited email’, or ‘cold emailing’ are governed by various laws and regulations depending on your target audience and location: CAN-SPAM Act (USA): This law governs commercial email marketing in the United States. It outlines requirements for subject lines, sender identification, unsubscribe options, and more. Compliance is mandatory. GDPR (EU): Applies to processing personal data of individuals within the European Union. Requires explicit consent for marketing emails unless you have a legitimate business relationship with the recipient. CASL (Canada): Canadian Anti-Spam Legislation imposes similar requirements to CAN-SPAM, including opt-in consent and unsubscribe options. Many other countries have specific laws governing unsolicited email. Always do your homework before sending emails internationally. Thanks for reading! If you enjoyed this post, please share: Share Twitter Facebook Linkedin Email Copy link Learn more about how to save time and costs with XOMedia Consulting and Partnership services - all work is backed by our 100% guaranteed. Back to Blog Home A simple change to Speed up and Secure your home network Simple Data Protection for your Computer Leave a Comment You must be logged in to post a comment.",
    "commentLink": "https://news.ycombinator.com/item?id=39888383",
    "commentBody": "A deep dive into email deliverability in 2024 (xomedia.io)190 points by xoneill 12 hours agohidepastfavorite118 comments xyst 8 hours agoGiven how much weight “Gmail”, “Outlook”, and “Yahoo” email providers pull, I have always wondered about a different type of attack on business entities: “targeted failed deliverability” Basically in this attack, a victim (particularly a business or mailing list or NGO) is sending out bulk emails to which the attacker owns. Even sourcing this out to shady off shore click farms would work too. Attacker then marks the victim’s emails as spam in Gmail/Yahoo/Outlook. The “AI spam filters” pick up on this new “spam activity” and will then mark future emails as spam or even delete them before reaching real customers. After a year, company bleeds money on a quarterly basis. Ad departments wonder why there is decreased engagement through email. Technical departments are bamboozled. Maybe a big company will be able to weather the storm or just ditch email altogether. But small companies would definitely take a hit. Even smaller NGO or political mailing lists would lose donations (assuming email was a significant source of new donations). Probably a very low vector of attack tbh, but something that has lingered in my mind. reply kirse 2 hours agoparentYea I've thought about this but not from the \"attack on entities\" angle but moreso a consumer-rights / boycott angle. I've had a negative enough experience with a large \"maximizing shareholder value\" company that I went back through my email history and marked every single one of their comms as spam. Might be a drop in the bucket, but it doesn't take many votes to make a difference in the spam world. I'm sure this will evolve soon enough and email delivery might increasingly become pay-to-play with all sort of backroom agreements, if it isn't already. reply janalsncm 2 hours agorootparentI bought my dad a sweater for our local MLB team. I made the mistake of using my real email. Ever since I’ve gotten a steady drumbeat of marketing emails and other low value content from them. Spammers want us to think there’s a significant difference between their newsletter or marketing notes we may have technically signed up for (certainly not willingly) and I don’t feel bad about reporting both of them. If this forces spammers to consider whether recipients will want their messages, good. reply pompino 2 hours agoparentprevThis is Google's business model, they throw completely legitimate emails your business sends into spam/marketing, so you're forced to pay them for gmail ads. reply nemomarx 8 hours agoparentprevA webforum I know has a rule against marking email notifications they send at spam (You can opt out of receiving them through the site, they just don't want you doing it on the email client end) to avoid this happening to them. For a small org, it's kind of a real risk? I'm not sure how larger orgs mitigate it. reply bboygravity 2 hours agorootparentI run a small org (freelancer) and I have a super advanced bullet proof set of solutions against this. Any 1 of those should do the trick (I do all of these): 1. Don't trick people into signing up for mailinglists. 2. Don't spam. 3. Don't use mailinglists. My small business is fine. reply spacebanana7 1 hour agorootparentNot against a targeted attack like the one described. A clickfarm marking password reset emails as spam could create real harm. Even a malicious individual doing this to 2-3 fair transactional emails could cause damage for a small business with low volume. reply uuddlrlrbaba 7 hours agorootparentprevThey pay 3rd parties to handle relay duties. A large chunk of reliable delivery is based on those received headers, aka what systems your mail is relayed through. reply jimkoen 5 hours agorootparentLol wait, that sounds like e-mail deliverability is almost set up like cartels. reply ozr 4 hours agorootparentCartel is maybe overly pejorative, but it's definitely based on relationships. Postmasters at large ESPs and inbox providers can and will text each other to resolve issues. This is pretty much how the internet works as well (BGP, etc). It's opaque, but open. reply 77pt77 4 hours agorootparentprevMass sending is controlled by sendgrid and the likes of that. reply KTibow 5 hours agorootparentprevIs there any way for them to enforce that reply buro9 1 hour agorootparentI run webforums, and this is enforced trivially. I don't send marketing, I only send transactional emails. Notifications are opt-in, and transactional. Logins / signups are done via sending a TOTP to the email, these too are transactional. If someone marks notifications of new posts in a subscribed thread as spam... fine, but this is self-solving, as this person will trigger no more email ever reaching them for that email address, meaning they also cannot now sign in to the website, and therefore cannot subscribe to more email updates, or visit things to refresh \"last viewed\", and so would never be notified again. Emailing a TOTP as a login has increased deliverability by self-selecting the removal of those who hit the spam button. Deliverability of email from the webforums is over 99%. Reporting email as spam, effectively bans oneself from the website. I didn't even need to do anything. reply saurik 12 minutes agorootparentWhen you report something as spam to Google they only claim they \"might\" cause later emails from the same sender to go to your spam folder; regardless, those emails going to one's spam folder doesn't mean they don't get them at all, it just means they have to look at their spam folder to log in. reply buro9 4 minutes agorootparentWhen you use services like Sendgrid, having a spam report will automatically prevent future emails being deliverable, third parties and intermediaries do this precisely to protect their email reputation. Marking spam may not be immediately be a death knell within Gmail, Hotmail, etc, but because of the potential impact from being marked as spam, virtually everything treats it like a death knell. This is fine for me, my service is entirely opt-in, and if someone hits the spam button it risks impacting other users who _want_ the email, so I am not bothered that this person effectively unsubbed themselves and killed their account. Neil44 1 hour agoparentprevYou don't typically use your main email system for bulk sending, you use a third party for that who is used to taking that heat. reply bongodongobob 5 hours agoparentprevThat's just email working as intended. You pay a third party a small monthly fee to handle email blasts via a relay. reply ttul 10 hours agoprevThis change was necessary and long overdue. Requiring domain owners who send significant volumes of email to properly sign their messages allows receivers to more clearly delineate good from bad based on domain reputation rather than IP address reputation. As more domains send email through shared IP space on transactional and marketing services, having the ability to attach reputation reliably to the sender domain is incredibly helpful in reducing abuse. reply adrian_b 9 hours agoparentThe condition about \"significant volumes\" is not true. Google states that the new requirements are mandatory only when you send at least 5000 messages per day. This is a lie. I send at most a few messages per day and usually less than one per day was towards a gmail account and I had implemented a part of the requirements, but not all of them. Nevertheless, Google has started to reject my messages, so I was forced to waste time with the implementation of all requirements, even if they are somewhat redundant. reply denton-scratch 2 hours agorootparentMy domain also sends no more than 10 messages a day. The domain is correctly-configured with SPF, DKIM and DMARC. At the beginning of March, I started getting temporary rejections from gmail. Not all of my outgoing messages, maybe 1 in 10. Most of these messages were to one individual, to whom I've been sending for years. My domain has existed continously since 2002, and has never sent spam. The rejection message was startling: words to the effect of \"Your message has been rejected because of the awful reputation of the sending domain\". The reputation of my domain is spotless, according to various testing tools. There have been no new rejections in the last two weeks. According to TFA, Google started rejecting a proportion of mail from bulk senders in February. I wonder if I got caught up in some half-baked roll-out of this new (old) policy. reply drdebug 44 minutes agorootparentSame here with 20+ years old mail service on the same domain that has never sent spam with correctly configured DNS SPF DKIM DMARC, getting gmail rejections. I noticed a significant improvement after linking the domain to my google account https://support.google.com/mail/answer/9981691 reply skrause 9 hours agorootparentprevMaybe someone else sent 5000 mails to Gmail users using your unauthenticated domain. reply adrian_b 2 hours agorootparentThat would not have been possible, with the already existing checks. While the individual messages were not yet signed, Gmail should have already rejected any message claiming to be from my domain that was not sent by my own server. reply bbarnett 3 hours agorootparentprevhttp://139.177.194.177/bah.png reply cj 9 hours agoparentprevAs a sys admin, what do you do when you see 5% of your email hitting spam because the recipient’s Office365 mail server is misconfigured? Agreed it’s a net positive, but it kills me when the reason emails land in spam is misconfiguration at the recipient’s end. (Like forwarding emails which breaks SPF) reply brightball 9 hours agorootparentAs long as DKIM is configured correctly, it shouldn't matter. reply xoneill 10 hours agoparentprevAgreed, this is the upside for sure. Despite frustrations, I'm hoping this cleans things up. reply r1ch 6 hours agoprevI'm surprised how many big companies fail the one-click unsubscribe test. Whether it's Cloudflare or Akamai blocking the connection, pages that take 5+ seconds to load, pages that require you to sign in or input your email address again... don't be surprised when customers reach for the Report Spam button instead. reply nottorp 3 hours agoparentI don't unsubscribe from emails I haven't opted in to. So report spam it is. reply JoshTriplett 2 hours agorootparentSame. If I subscribed to it (rare), I'll unsubscribe. If I get unsolicited mail of any kind, I'm not \"unsubscribing\", I'm nuking from orbit by any means available, including reporting to hosting providers. (This has, on occasion, resulted in the termination of a spammer's account, but the success rate is low.) reply nottorp 2 hours agorootparentI'm mostly talking about those emails with helpful hints and upsells that you get when signing up for a new service. People who make a living from spamming^H^H^Hbulk sending may consider those legitimate. reply Ayesh 5 hours agoparentprevI'm using NextDNS with AdBlock list, which is effectively a Pi-hole on the cloud. The most annoying this is when email senders use click tracking on domains that are blocked by those AdBlock lists. I keep a separate browser instance to copy-paste those links into, but then I have to login again. I prefer sending unsubscribe emails instead of clicking links. Gmail can automate it. reply foreigner 2 hours agoparentprevAgreed, I think of it was a simple UI competition. Customers will do whichever is must convenient: unsubscribe or report spam. reply hedgehog 6 hours agoprevOne thing the April changes break is forwarding between e-mail services. If you currently forward from say an old university address at foo@school.edu to a personal GMail account at bar@gmail.com that will no longer work. This must be relatively uncommon if the major providers are charging ahead with these changes but it's pretty annoying for the people affected. reply Ayesh 5 hours agoparentWhy will it no longer work? When you forward an email, unless the email forwarder modifies the message content, it should still match the DKIM signature, so it still passes. reply hedgehog 5 hours agorootparentI don't know the details but my rough understanding is after forwarding the next hop delivery will fail SPF. reply bo1024 4 hours agorootparentYeah, with forwarding, I am seeing DKIM still passes but SPF fails. reply tmn007 3 hours agorootparentWe are seeing all the unix forwarders setup a decade+ ago are dead in the water now (have to be replaced with mailing list software) reply therein 2 hours agorootparentprevThis explains why I have been unable to get my 2FA from Adobe. reply whelp_24 54 minutes agoparentprevThat seems really significant, email fowarding won't work anymore? reply acidburnNSA 11 hours agoprevI'm surprised anyone's been getting through at all without perfectly configured SPD, DKIM, and DMARC. I've had a well configured self-hosted personal email server for years and still struggle to get through sometimes, though it does seem to be getting better. reply theK 10 hours agoparent> perfectly configured SPD, DKIM, and DMARC Just having them perfectly configured doesn't mean that the receiving servers will also see it that way. Microsoft servers are particularly prone to randomly failing perfectly fine dkim setups for no reason whatsoever. reply hsbauauvhabzb 6 hours agorootparent‘Randomly’ when you just happen to not be part of the mail cartel, they just can’t say that. reply shsbdncudx 10 hours agoparentprevBecause it’s not just about configuration, it’s also about reputation and a low sending volume you are in danger of getting dropped merely out of not being a well known sender reply xoneill 11 hours agoparentprevI've been wrestling with this for years as well. I hope with these guidelines being published / transparent by the big 3, things behave consistently. reply jesterson 4 hours agoparentprevI am surprised people think deliverability consists of configuring SPF (SPD is Seattle police), DKIM and DMARC. Spammers can do exactly same given low entry barrier. It's important but have very little connection to deliverability in real world. reply cj 9 hours agoprevThe thing that kills me about DMARC is how often is fails with Microsoft specifically. And also with any use case involving the recipient forwarding mail (which breaks SPF alignment) I want to follow best practices it recently changed p=quarantine to p=none after fear that legitimate emails aren’t passing DMARC despite properly configured DKIM and SPF. Hell, I would love p=reject but not until recipients fix their incoming mail servers to handle edge cases like email forwarding breaking DMARC reply louis-lau 8 hours agoparentSenders that apply dmarc and want their emails to be forwarded should use dkim. Forwarding a dkim signed message doesn't break dmarc at all. reply zinekeller 5 hours agorootparent> how often is fails with Microsoft specifically This is the most important part. Exchange (due to its history as an X.400 server, not as an SMTP server) does sometimes mangle the message to the point that DKIM simply breaks. This both breaks origin-incoming and forwarded messages. BTW, Apple also sometimes mangle messages that it fails DKIM, although I do not know why is this the case (as I doubt they use Microsoft Exchange for their mail service). reply mjl- 1 hour agorootparentprevthis is a long standing problem with mailing lists. they are often configured to add a \"[...]\" prefix to the subject or add a footer, breaking the dkim signature. this leads some more recently updated mailing lists to always rewrite to their own \"message from\" header, so they control dmarc alignment for their messages. for incoming email on mailing lists i'm subscribed to, i don't enforce the dmarc policy. i think this is what the parent post hints at. i'm not sure how easy this is to configure with the various mail server software out there. i'm also not aware how you would configure this with sieve scripts (i looked, didn't find it, but it seems like a basic case). if you're running a mailing list, hoping for all subscribers to not enforce dmarc policy enforcement doesn't seem like a great strategy. the forwarding case should be easier to keep working. reply gtech1 5 hours agoprevRoughly 50% of my daily Spam comes from @gmail & @hotmail/@outlook accounts. What exactly are they doing about that ? reply yobbo 1 hour agoparentYep. It seems to be from stolen accounts, maybe gathered from leaked account/password lists. The result is that outgoing hotmail/outlook smtp servers are added to blacklists until they start content filtering their own users. reply jjav 4 hours agoparentprev> What exactly are they doing about that ? Nothing at all, because they are too big to care. Aggressive decentralization is the only way to save the Internet. Host your own email, get everyone you know to host their own email. reply samirillian 1 hour agorootparentFantasy solution, will never happen. reply jesterson 4 hours agoparentprevThey may not - but you fail to properly set up DMARC. reply mjl- 2 hours agorootparentat least gmail.com has a dmarc policy p=none, so a failing dmarc check is not a reason to reject email. i don't think it's that common for small-scale installs to enforce dmarc policies. there are other signals to use though. plenty of bigfreemail spam is actually sent from their network, they're an interesting target for spammers, and at least some of them put a lot of effort in preventing abuse. reply nanidin 4 hours agoprevI ran my own mail server for more than a decade. Same IP the entire time, never sent spam (for personal use only.) Finally threw in the hat last year and moved to a paid service - it was a pain to tell every person I sent mail to to check their spam box and mark me as not spam or add me to contacts. Beyond that, gmail smtp servers kept getting onto spam blocklists, so I wasn't receiving mail from gmail at times. reply dugite-code 4 hours agoparentGmail and Microsoft require a certain volume of email hitting their servers otherwise they forget you exist. I ended up switching to using Amazon's SES service to keep my selfhosted server running. I use seperate emails for each service much like a seperate password so I'm heavily invested in keeping my own server at least for recieving. reply Animats 5 hours agoprevThen there's the other side - receivability. IDrive is supposed to send me an email each day reporting backup status as seen by the backup servers. Those messages have been flaky since mid-February. Logs indicate the backups run; it's just the completion emails that fail. Their support people blame me, although they admit others have the same problem. They're not using a mail delivery service - the emails come directly from an IDrive server. They're sending to my web site, which forwards to my personal address. There's no filtering at the first stage, and a division into Accept/Greymail/Junk at the next stage. Neither Google nor Yahoo is involved at any point. reply deadbunny 11 hours agoprevSo what any competent sysop has been doing for years? reply louis-lau 11 hours agoparentSometimes it's not about competence but about priority. If email is something a business does as a side thing and not a core thing, and if email keeps working without changing anything, there's no need or priority on setting up newer things. From a priority perspective at least. But yes if it's a recent setup, or email is a core part of the product, any competent sysadmin should have been doing this. reply solatic 2 hours agoprevNo mention of BIMI (either for or against)? I'm surprised... reply 77pt77 10 hours agoprevMy personal VM has just been placed in some RBL because the entire /24 address space was blacklisted. Someone (allegedly) sent SPAM and now my machine that sends maybe 3 emails a week is blacklisted reply mjl- 2 hours agoparentyes, this is a pain. the blocklist operators either seem to be not so good at vetting abuse reports, or cause collateral damage to get network operators to take action (while i don't like getting on a blocklist, perhaps it is having a net positive effect for the wider internet?). i do think mail servers/services are using ip-based blocklists wrong. yes, you can use it as one of many signals. give it some more weight for first-time senders. but if you've been mailing (with spf/dkim/dmarc-authenticated messages/transactions), from an ip that suddenly gets on a blocklist, the previous positive reputation should be stronger than that blocklisting, and you should be able to keep communicating with your known correspondents (until they mark your message as spam after which future deliveries can be rejected/junked). it seems those mail servers/services cheap out and apply ip blocklists early in the smtp process. good for their system load, bad for their analysis performance. in general, it seems even bigfreemail services are bad at using existing reputation in their ham/spam decisions. i recently switched an online webservice i made (that is about sending certain notification by email) to signups via email (like how you can signup to mailing lists: by sending an email to an address). the idea: if you send my service an email, i'm in your list of known correspondents. so the confirmation reply from my mail server (spf/dkim/dmarc-aligned) should certainly be accepted by yours (you much more opt-in do you want?). i tested with some bigfreemails, and yahoo put my reply with confirmation (that even references the original message) in the junk folder. to people who think you can't compete with bigmail: the bar isn't as high as you may think. reply xoneill 10 hours agoparentprevThat sucks! And fixing reputation is at best a nightmare. I've seen suggestions about purchasing / sending email from other domains - to protect your primary domain. Not something I really care to do. reply cj 9 hours agorootparentGoogle themselves recommends sending different types of emails from different subdomains of the primary domain to help Google differentiate between transactional, marketing, newsletters, outbound, etc Any half decent marketer will 100% use a different domain for outbound sales (or any use case where spam rate might be abnormally high). reply xarope 7 hours agorootparentabsolutely. You should/must separate transactional emails (account creations, password resets etc) from EDM (marketing emails). reply xoneill 9 hours agorootparentprevThank! Will look into this and reconsider. reply nickburns 10 hours agorootparentprevi've personally observed an increase in this tactic from both reputable companies' blast communications and marketeers alike. reply 77pt77 7 hours agorootparentprevThe \"de facto\" solution is to outsource that to Google. Email on a personal machine and domain has been dead for over 10 years. You just can't own your data. You can receive it, no problem. But you can't send it. reply akira2501 6 hours agorootparent> The \"de facto\" solution is o outsource that to google. So, they have a \"de facto\" monopoly, or at best, are working with other providers to create a cartel. > Email on a personal machine and domain has been dead for over 10 years. Due to the actions of? > You can receive it, no problem. But you can't send it. You can send it. They're actively deciding to just block you. Then provide you no recourse. reply 77pt77 6 hours agorootparentThe monopoly came about \"organically\". It's not like there was a conspiracy. > You can send it Not even that. Nowadays it's not uncommon for some servers to even refuse the connection. Mind you, I'm not talking about aggressive spammers. And yes, there's pretty much no recourse. reply akira2501 2 hours agorootparent> It's not like there was a conspiracy. I'm not nearly as sure. The mechanics of one aren't that hard to imagine. Encourage spammers and make it cheap. Don't ever fight them at their source. Invoke mechanisms that intentionally destroy public utility in public protocols. Force everyone to rely on a small handful of \"reputable\" senders. Working backwards from \"who decides reputation anyways?\" might make it easier to see. reply dugite-code 3 hours agorootparentprevSending requires quite a large volume for the big players to allow you to play. The only viable option for small servers is to use a SMTP relay service. Amazon's was a pain to get out of the sandbox mode but has been reliable and most importantly free. As I care more about the recieving side than sending emails this works well enough for me. reply superkuh 10 hours agoprevThe worst is when they accept the mail but silently tag it spam and put it some place the intended receipient will never see it. Google's gmail is the worst about this. Corporate email isn't email anymore. It's a walled garden / silo like Facebook. reply mjl- 2 hours agoparentindeed very annoying. my understanding is they don't want to give spammers any signal whether their messages was recognized as ham or spam. i would love some insights in how smart spammers are in actually leveraging such information. most spam seems to be hammering attempts that don't take failure feedback into account. in my mail server, messages classified as junk keep getting temporarily rejected with a generic error message. at least the (legitimate misclassified) sender gets a delayed dsn, and finally feedback that a message wasn't received. it seems many mail servers/services think it's more important to not give a signal to spammers than it is to give a useful signal to legitimate but misclassified users. perhaps they think their classification is really great and doesn't misclassify... reply rrr_oh_man 10 hours agoparentprev> Corporate email isn't email anymore. It's a walled garden Thank god. Otherwise I'd drown in cold email spam. edit: But on a serious note — I'm using Gmail for all companies because I gave up trying to run and administer our own server. It's a travesty that this has become so hard. I feel if you're not on a well-configured Gmail Workspace there's no chance your email gets through, even if legit. reply plantain 8 hours agoparentprevI'm certainly not saying it's right - but it works. Gmail's spam filter (and promotions filter) works with >99% reliability as a user, with really trivial numbers of false positives. reply nh2 1 hour agorootparentCannot confirm. We get a large number of false positives at our business GMail: Customer requests for quotes, Paypal/Stripe security messages, lots of other important emails go to spam. See e.g. https://github.com/nh2/gmail-spamfilters-paypal-security-mes... For a while now I suspect that GMail has some bug with its own group email addresses: When somebody sends an email to our GMail group email address team@example.com, it shows up in GMail as \"Sombody via team@example.com\". Of course, teamexample.com receives both spam and non-spam. I suspect that when we mark spam that comes \"via team@example.com\", GMail learns that as \"things 'via team@example.com' are often Spam\", even though info@example.com is a Google Groups email. And by now, everything that comes 'via team@example.com' is marked as Spam. So it seems that when we mark an email that arrived at team@example.com as Spam, Google punishes its own Group email address, instead of the sender. reply ndriscoll 7 hours agorootparentprevI haven't found this to be true at all. I've probably marked thousands of linkedin messages as spam, but they still land in my inbox occasionally. I also get random e-commerce related spam from sites I've never heard of. The false negative rate is massive on corporate spam that should be trivial to classify I'd think. e.g. some of them literally have some variation of \"this is an advertisement\" along with unsubscribe links. reply arccy 3 hours agorootparent> I've probably marked thousands of linkedin messages as spam, but they still land in my inbox occasionally. at this point maybe you should just create a filter rule reply kureikain 11 hours agoprevI run an email forwarding service[0] and it's damn hard to get into inbox of any major provider if SPF/DKIM aren't config properly. DMARC or ARC might be optionally but an email without SPF/DKIM, good luck having it hit any inbox. Office365 is the toughest, email just randomly land on spam no matter what I do. Icloud, actually it's ProofPoint is tough sh*t too. So I'm so surprise these guide just pop-up now like it's a new thing. --- [0]: https://mailwip.com reply jmb99 7 hours agoparentI’ve been running a personal mail server since 2016, and I’ve been struggling mostly with Microsoft off-and-on. I haven’t make any changes since setting up DMARC, SPK, and DKIM in 2016, but I’ll still sometimes randomly get blacklisted for 2~30 weeks for seemingly no reason, and then get unblocked again for seemingly no reason. It’s recently started happening with iCloud too. For 5.5 weeks any email I sent would either get bounced or land straight in Junk, until yesterday when the powers that be decided my mail was worth delivering again. I’ve somehow never had an issue with Gmail or Yahoo, and of course never with any other non-big-three mail servers. reply dugite-code 3 hours agorootparentCould be your IP block gets the odd spammer or you don't send enough emails and their servers resets you IP address reputation, meaning you essentially become a new email server. reply xoneill 10 hours agoparentprevThis was my initial thought too. These guidelines have been around for ages, and just now being officially implemented in 2024. reply jesterson 4 hours agoparentprevProofPoint is indeed disgusting service. Wonder how they proliferated into corporate world so much and so fast. reply 77pt77 10 hours agoparentprev> but an email without SPF/DKIM, good luck having it hit any inbox And that's a decent thing. Someone that doesn't do that has no business sending email. The problem is that even doing that and even sending only a couple of emails a day is still usually considered SPAM. reply amelius 10 hours agoprevIsn't there any open source project that solves the e-mail delivery problem? If not, why not? This sounds like something that can be fixed by software. reply louis-lau 8 hours agoparentPlease elaborate on this, as I'm not exactly sure what you mean. The email deliverability problem is a side effect of false positives in spam filtering. Unless you have a proposal to completely eradicate false positives? reply jjav 3 hours agoparentprev> Isn't there any open source project that solves the e-mail delivery problem? Aplogies if you do, but it sounds like you misunderstand how email works. The problems are stricly in policy, not in software. So there is nothing an open source project can do. The problem is that \"too big to fail\" megacorporations like microsoft just randomly decide to block incoming email from most of the Internet. If email was fully decentralized (everyone runs their own server) this centralized power could not exist and there wouldn't be any problem. (That all said, I run my own email infrastructure since long ago and it works fine. But I know some people struggle, which is contrary to the intent of the Internet.) reply remus 2 hours agorootparent> If email was fully decentralized (everyone runs their own server) this centralized power could not exist and there wouldn't be any problem. You would have different problems though. Spam would still be hard, or arguably even harder, because you would have lots of small mail servers with no idea if they are trustworthy or not. Is this uncle Bob, who I havent spoken to for 30 years, sending me a heartfelt message about the family? Or is it a scammer trying to cream some cash out of me? reply arccy 3 hours agorootparentprevthe centralized power doesn't exist, but then you run into every misconfiguration possible. is it better? maybe. reply bo1024 8 hours agoparentprevWhat do you mean by the email delivery problem? Do you mean setting up DKIM, SPF, and DMARC? It's not that hard, compared to setting up an email server in the first place. reply jspaetzel 9 hours agoparentprevfor a long time it hasn't been a software problem. It's been a reputation problem, the only recent change is we're more interested in the domain then the IP now. reply xoneill 10 hours agoparentprevThere's some online tools out there, and an OSS project that'll scan DKIM reports for you - but this was all I could find. Beyond this, there's commercial tooling / services. I may implement something that keeps a tab on things if I come across any issues. reply briandear 2 hours agoprevI wish we could solve the unsolicited SMS problem. reply EGreg 8 hours agoprev1. GMail will block your email if you don’t allow one-click unsubscribe. But this is very insecure since anyone can unsubscribe you if you forward your email Easy Unsubscribe: Implement easy unsubscribe options (One-click Unsubscribe). Gmail users have tools to report spam, unsubscribe from unwanted emails and control their inbox experience. If it is too difficult to unsubscribe from your emails, customers will be more likely to flag your email as spam. Additional links provided in the ‘References’ section at the end of this article. 2. At the same time, Apple’s ITP will start removing all the information from the URL and only leave the domain, if it classifies your site as a “bounce tracker”. This means you won’t even know who to unsubscribe on one click! So all your emails will be blocked. https://getcake.com/apples-intelligent-tracking-prevention-2... reply snowwrestler 6 hours agoparentGmail does not require one-click unsubscribe, what they actually require is that you include the “List-Unsubscribe” header in bulk emails, with a functioning mailto or http target. If I forward your newsletter, that’s not a bulk email and it won’t include that header. This is an important distinction that seems to get glossed over in a lot of the coverage and guides about the recent Gmail and Yahoo changes. reply gruez 2 hours agoparentprev>2. At the same time, Apple’s ITP will start removing all the information from the URL and only leave the domain, if it classifies your site as a “bounce tracker”. This means you won’t even know who to unsubscribe on one click! >https://getcake.com/apples-intelligent-tracking-prevention-2... Your source doesn't actually say that \"ITP will start removing all the information from the URL\", only that it will \"limit it the same way as third-party cookies\" and will be \"purging website data in such instances\". reply louis-lau 8 hours agoparentprevWhy would apple identify the domain used for unsubscribes as in use solely for being a bounce tracker? reply EGreg 8 hours agorootparentTheir heuristics are proprietary dunno reply louis-lau 8 hours agorootparentSo you argument here is based on something that hasn't happened, but just something you pulled out of nowhere. Or was this meant humorously? reply xoneill 12 hours agoprevJust a freindly reminder. I wrote a thorough guide on email hygeine here, includes validation tools to help troubleshoot & straighten things out. reply ryantgtg 11 hours agoparentAt an even more basic level than what you described here, I recently improved my delivery rate by adding a name to the transactional emails that we send (from: 'My Name ') where previously we didn’t have a name. As well as cleaning up subject lines - where previously we included some abbreviations. Providers like mac.com were previously soft bouncing some emails, and now they seem to accepting them. Hotmail/yahoo/aol all still seem to shove our transactional emails into spam pretty often (judging purely on the amount of those users who fail to confirm their accounts). reply patja 10 hours agorootparentmac.com is the worst. I ban that domain from new registration now. And Apple sends no dmarc reports nor do they implement any actionable feedback loop. reply xoneill 10 hours agorootparentprevThanks for the info. For years, I've been using trial & error / reverse engineering approaches to improve deliverability. Frustrating! reply cqqxo4zV46cp 8 hours agoprevAs usual in any email thread about email deliverability, the amount of FUD in these comments is absolutely mind-boggling to me. I’m not unusually smart or intelligent or capable. I wouldn’t consider myself a deliverability expert. It’s only a small small part of my job. I’ve never worked for any organisation that sells email delivery services to third parties. Why the hell can I understand this stuff, and get it to work, while there are so many people here that very clearly indicate (via what they’re saying in their comments) that they DON’T get it yet have a serious axe to grind? I’m left feeling like homegrown email delivery is some sort of lightning rod for stuck-in-the-past faux-sysadmin types that can’t get past the fact that it’s not 2003 anymore and lazily / maliciously comply with SPF / DKIM. IT’S NOT THAT HARD. reply Biganon 5 minutes agoparentCongratulations for being lucky enough to have big actors accept your e-mails, this is not the case for many of us who do understand and apply SPF, DKIM and DMARC. Guess we'll just try being luckier...? reply nh2 1 hour agoparentprevI agree on the technical part, but: A problem is that you can do everything technically right, and sitll land in spam, because some big players don't play by the usual rules. For example, Microsoft apparently has an allowlist for IPv4 -- or equivalenty, blocks all IPv4 by default, until you manually de-list them at sender.office.com. At least I haven't found an IP yet for which I didn't have to do that (self-hosting email for 15 years). (Imagine every provider did it like MS; you'd be sitting there and filling out web forms with 1000s of providers.) So you have a technically perfect setup and MS stil rejects you. -- That said, using some provider to send emails for you doesn't solve deliverability either. There, many customers share the same sender IP. If one of them sends marketing/spam, the entire IP gets bad reputation. In such cases, providers recommend to upgrade to \"bring your own IP\", which then needs to \"gather reputation\" [0]. Great, might as well have self-hosted in the first place, as repuation is the only thing I bought the service for. [0]: Example: https://www.mailgun.com/blog/deliverability/dedicated-shared... -- Especially entertaining is \"Use a shared IP if: Your shared IP partners have built a good reputation.\" As if you had any control over that. reply snowwrestler 6 hours agoparentprevWithout knowing everyone’s domains and IPs and history, it’s hard to judge competency in a thread like this. SPF, DKIM, and DMARC are important to set up correctly, but doing so is NOT sufficient for good deliverability. In fact it is only a small component of success. Well-established organizations that have a long history of sending steady volumes of high quality content with low complaints have a huge leg up. So if you work at such a place, or you contract with such a vendor, you’re going to feel like it’s obvious that the DNS entries work well. reply gruez 2 hours agoparentprev>As usual in any email thread about email deliverability, the amount of FUD in these comments is absolutely mind-boggling to me. [...] What type of \"FUD\" are you talking about? The objections in the thread seem pretty well founded (eg. being shadowbanned despite complying with SPD/DKIM, or this requirement breaking email forwarding), and there aren't really any that are against implementing SPF / DKIM. reply bongodongobob 5 hours agoparentprevYeah it takes like 5 minutes. I have no idea what people are crying about. On a tech site no less. This is entry level have the intern do it stuff. reply midnitewarrior 9 hours agoprev [–] This looks like a good document, but the author made it political by referencing \"Hilary\" Clinton and her emails and linking to some Trump stuff. I can't take tech stuff seriously that's dropping in political crap. Go away! reply xoneill 8 hours agoparentVery valid point. I just removed my poor attempt at political humor. It was not meant in support of any candidates. reply g4zj 9 hours agoparentprevYes, this put me off enough that I was no longer interested in reading the article. The silly reference is one thing, but was a link to that YouTube video really necessary? reply jesterson 4 hours agoparentprevAre you a truth seeker or attention seeker manifesting your political preferences instead of focusing on what article actually discusses? that's a rhetorical question - you already answered it. reply 01HNNWZ0MV43FF 8 hours agoparentprev [–] And it's for a company? OP you might want a part-time PR person to review these... reply xoneill 8 hours agorootparent [–] Removed! Yeah, bad attempt at humor. And certainly not done in support of said candidate. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores new email deliverability guidelines for Gmail and Yahoo, emphasizing the necessity of implementing authentication standards like SPF, DKIM, and DMARC to avoid blocked emails from 2024.",
      "It highlights the role of these standards in preventing spam, phishing, and malware, especially for bulk senders, and how major email providers enforce compliance.",
      "Proper email security measures, including DKIM and DMARC, are crucial for email authentication, improving deliverability, and enhancing customer trust and brand reputation."
    ],
    "commentSummary": [
      "The discussion delves into the complexities of email deliverability, covering spam filtering, misconfigurations, and how email forwarding impacts authentication protocols like SPF, DKIM, and DMARC.",
      "Users share experiences and offer suggestions to enhance email deliverability, stressing the significance of proper setup, reputation management, and strategic email practices, especially for small businesses.",
      "The conversations touch on major email service providers' influence on email policies and advocate for decentralization in email communication, underlining the dynamic nature of email deliverability and the array of strategies and tools to tackle these issues."
    ],
    "points": 190,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1711921882
  },
  {
    "id": 39884009,
    "title": "Giphy Exposes User Data to 816 Partners, Tweet Reveals",
    "originLink": "https://twitter.com/illyism/status/1774425117117788223",
    "originBody": "Every time you share a @GIPHY, you send your data to:*checks notes*816 partners 🤯🤯🤯 https://t.co/k2glBdD8Q1 pic.twitter.com/jqTLCsXnUA— Ilias Ism (@illyism) March 31, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39884009",
    "commentBody": "Giphy is sharing your IP address and private data to 816 partners (twitter.com/illyism)173 points by illyism 20 hours agohidepastfavorite79 comments crtasm 18 hours agoThough not if you use it in Signal: https://signal.org/blog/giphy-experiment/ reply nickburns 17 hours agoparentwhile commendable on Signal's part, i think the more surefire route is to forego Giphy's service altogether. GIFs truly are fun, especially when you capture that perfect non-written/verbal message—but it's simply not worth it to me and worth too much to Giphy. thanks for sharing this though. reply xyst 18 hours agoprevIf the service is free, you are the product. I wonder how many “partners” Google or Microsoft have? Their legalese probably doesn’t say it explicitly and probably use some loop hole through a data broker. Google -> data broker (1) -> 1000 “partners” or affiliates reply MuffinFlavored 18 hours agoparentI don't understand why it's such a big deal that \"other companies have my data\" What data? Where I live? My phone number? My interests (that I typed in online)? What kind of products I like (that I scrolled slowly past/engaged with)? Where does the narrative come that \"I as a user deserve to use all of these platforms for free and I don't want them or any of their marketing partners to have 'data' on me\" Who cares? Why do people become so obsessed with protecting their 'data'? What makes you so special? They go through great lengths to avoid it. How does it affect your life knowing other companies have 'data' on you? If you want insurance, insurance providers literally need data on you. Same for credit with banks/lenders. Same for renters/landlords, etc. But we draw the line on \"big tech\" social media?... Why? Where is the negative harm? reply AlexandrB 18 hours agorootparentIt's all fun and games until your data gets used against you. But by then it's too late. There are many historical examples, but a great contemporary example is states using this data so they can prosecute you if your pregnancy ends[1]. In general private collection of data should be regarded the same as government collection of the same data, since all this data is subject to subpoena. [1] https://www.thirdway.org/memo/the-new-front-in-the-battle-fo... reply LorenPechtel 16 hours agorootparentYes--which is why you should never use your normal system for anything you don't want connected to you. Take precautions when virtually approaching an abortion clinic like you would take precautions when approaching a physical one. I simply assume Big Data (government or not) will be aware of pretty much anything I do unless I use a separate browser and a VPN. Think of it as a public space--if someone cares to look, they can. reply acuozzo 11 hours agorootparentHow does being selectively private work for deductions made by connecting several dots? e.g. 1. Buying several pregnancy tests 2. Missing several periods in your menstruation tracking app 3. GPS data for having traveled to an OB/GYN 4. GPS data for having traveled to a planned parenthood reply fifteen1506 13 hours agorootparentprevIt's far easier to be always private than having the cognitive burden of trying to discover when you want privacy. Furthermore, you can't always have your day according to plan. reply nickpp 17 hours agorootparentprevI think we're only worried about our data in the hands of governments. So maybe we should make that illegal - but of course governments would never pass a law like that, against their own interest... reply bee_rider 18 hours agorootparentprevSpying on people is just generally creepy, I don’t know, but I bet if I was hanging around outside in your bushes with some binoculars, you’d have some entirely reasonable questions for me. It is also bad to contribute to this ecosystem, you and I might be boring people with nothing to hide. But, there exist lots of vulnerable people out there, and we should not provide a market for services that make it easier to infer things about them. Unfortunately it is hard to compete with “free*.” So, these spying companies have gained a dominant position in the market. It is, I think, generally accepted that it is fine to regulate entities with dominant market positions. reply gmd63 11 hours agorootparentIt should be illegal to call it free when it, when sufficiently described, obviously comes with a cost reply tremon 17 hours agorootparentprevIt's not a big deal if other companies have your data. You're free not to value your private life, but please don't extrapolate to other people. What data? Where I live? My phone number? Which people you interact with, and which people you'd go to great lengths for to help them. Where you sleep, and at what time you wake up. When you're at home, and when it's safe to ransack your house. What your pressure points are for manipulation. What route your children use to go to school. What ransom money you'd be able to pay out-of-pocket. reply nickpp 17 hours agorootparentLet me get this straight: you're afraid people would rob/kidnap you or your children using data stored by the Big Tech?! Any instance when this actually happened? Would't it be easier to just watch your house for a week from a parked car, wireless camera let on some wall? reply stemlord 16 hours agorootparentprev\"I have nothing to hide\" is one form of what is known as \"privacy nihilism\" which is how we end up losing our right to privacy in the long run. reply rchaud 10 hours agorootparentprev> My interests (that I typed in online)? You don't have to type stuff into a Facebook.com input form for them to be able to use it for ad targeting. There are Facebook \"share\" buttons on most websites, and each of these is a tracking beacon that tells Facebook that User X visited so-and-so website. That X becomes an identifiable user if the user is logged into Facebook on the same browser at the time of the website visit. reply EasyMark 15 hours agorootparentprevWhy don't they just advertise based on relevancy to search/topic/etc, instead they want to build a psychological profile on me. I can't stop them with a request or take down, so I do the next possible thing, make it as hard as possible to track me. I'm probably losing but it's still fun to try. Why do I owe random companies my life story? The whole \"what do you have to hide?\" and \"think of the children!\" has been used for centuries by authoritarians to control society. reply u32480932048 17 hours agorootparentprevIt's even weirder when people seemingly forget their self-preservation instincts to go on the internet and simp for Big Tech. C'mon, y'all, Elon Musk, The Zuck, Henry Kissinger - these people work very hard to give us the lifestyle we want. The least we can do is give them a little of our data, if not our money. Right? Right? I, for one, am honored that my every interaction is stored in some datacenter in northern Utah. It just makes me feel safe at night. reply nickpp 17 hours agorootparent> give us the lifestyle we want Actually, they do. Companies and their employees all over the world are giving us the products and services we want. In exchange we're freely giving them our money and/or some \"data\". When offered the choice, it seems most people would rather pay with their data. We've been told for years now this is not good for us, but I am still waiting to see the downsides - all I get are some personalised ads. reply georgeplusplus 17 hours agorootparent>>> all I get are some personalised ads. It’s hard to notice because the effects are insidious and very slow. What does happen is they sell your data to companies or foreign governments that have absolutely zero interest in you or your families welfare and use it to craft policy to further entrench themselves. If you are waiting for evidence that is extremely black and white and personalized like an attack that simply is not gonna happen. Take tiktok for example Tiktok gathers data on US demographics to steer discussion and politics or sow discourse. Watching tiktok videos you probably aren’t thinking, wow China really wants to see the west fall. Only if you understand their foreign policy would you connect the dots. rest assured they are using everything available to them to do so. reply nickpp 16 hours agorootparent> What does happen is they sell your data to companies I don't know, more personalized ads?! > foreign governments > Take tiktok for example TikTok's is a foreign government operation. > steer discussion and politics or sow discourse I am still waiting for proof they can actually do that. I remember all the scandal around Cambridge Analytica but then nobody said anything anymore when it turned out they weren't able to do what they boasted they could. Yes, we should be afraid of governments getting our data and using it against us. But they have many other ways of getting the data and they don't exactly obey laws - they are above the law by definition. So I don't see how laws against private companies can help here. reply georgeplusplus 16 hours agorootparentSmall nitpick Tiktok is a private company that serves a foreign government operation. Why would they resort to breaking the law when they are getting the data legally? Doesn’t make sense reply lazide 16 hours agorootparentprevInsurance companies and the like don't need these backdoor data brokers (at least for legit needs), because they can just go through legit channels. What legitimate need would a bank or insurance broker have for your IP address history, or secret fine geolocation history, or browsing history, or product interest? Plenty of illegitimate reasons though, eh? The data brokers being discussed are huge problem because companies (and nations) use this information - both individually, and in aggregate - to work against your and our interests, and into theirs. This data is valuable because it is NOT going through legitimate, constrained channels. And even if you think 'nothing bad could happen' with the current administration, what if Trump wins again? Because that is not a zero probability event. And that is without counting that Russia and China are of course using these same data brokers for their purposes right now, and will do so during the coming elections too. They would be fools not to, they'll just get better at not getting caught like they did last time. This isn't hypothetical slippery slope stuff either - this is the norm now. These brokers are for the low-quality-but-useful-directionally 'protected' data which legit channels aren't allowed to have for very good reasons. There aren't 300+ 'big tech' entities. Like religious/political affiliations (a Jew? Muslim? Democrat in a Red state, Conservative in a Blue one), gender/sex/whatever (LGBTQ or not - both are hot button right now depending on where you live), porn browsing habits, illegal-drug-site-habits, trying-to-get-an-abortion browsing, do-you-have-a-girlfriend on the side location data, or are-you-working-during-work-hours browsing data, etc. These things can and do get people targeted and/or killed, depending on where they live and their personal situation. If you don't have to worry about it, congrats - but a large portion (probably most) of society does or will at some point. The low data quality is just as likely to get you personally targeted too for something you aren't, and good luck disproving it, since the folks looking at the data aren't really supposed to even have it to begin with. In many cases it is as 'benign' as getting you to pay for something you otherwise wouldn't, even if you can't afford it. In others, it is directing targeted propaganda at you to influence your core political beliefs, incite rage, manipulate emotions and behaviors, and influence your personal and professional activities in ways that are often against your individual interests. In many cases, against our societies interests too. Hell, at one point on Youtube I clicked on some random video and got targeted with a super manipulative female voiced 'do girls laugh at you because your dick is too small?' ad. I've also seen 'Girlfriend, your makeup is ugly and that is why guys think you're a Ho, buy our product' ads too. That wording is not hyperbole either. The voice acting was even more impressively manipulative. If I could have gotten a recording of it, you would not be saying what you're saying right now. And those are far from the only malicious ads out there. The entire population is being bombarded with them. Many of the political ads even have outright falsehoods in them. I've also seen the ads and videos Trumpers get - from insane conspiracy theory to barely couched calls for political violence. I've had people try to connect with me on the political violence front in person twice in one area near California. And not in a 'hah, what nutjobs' kind of way, but in a 'hey man, you prepared for shit getting real?' serious kind of way. And yes, they had access to firearms. I'm sure somewhere, someone is thinking up how to do a blackmail-styled 'we know you watch bestiality-granny-incest porn, pay up for our protection plan or we'll email the proof to your parents' ads, if they haven't already figured it out. I also have an acquaintance in a conservative area all the sudden start learning how to speak Russian. She knows no Russians, and has no plans to visit Russia. If that doesn't raise red flags in this environment, you're not paying attention. Personally, while manipulative dick size jabs or calls to doomsday conspiracy theories and violence are not anything that triggers more than an eyebrow raise from me, near as I can tell 95%~ of the population doesn't have the experience and whatever-else-you-call-it someone like me has to be able to just laugh and walk away. Or tell when they actually want something, vs when they are being manipulated to want something. Those that can, probably paid in blood and treasure. I know I did. You probably don't want to know what it takes to get there, and it doesn't scale to the level of a society. And with every platform actively fighting adblockers, and mobile ad blocking being... not good? This is not going to be pretty. Population wise though, barring some kind of dictatorship (which would likely just make it worse), I guess we'll all have to figure it out, or be crushed. Or toss our phones in the trash, I guess. Are you so insulated you don't see this harm happening? If so, I recommend you hit up youtube or facebook in a clean browser and do some searching around. It won't take long for concerning things to start coming your way, regardless of what gender/sex/political affiliation you are, though I'd recommend going outside of your normal path so you can see the crazy in a clearer fashion. It's easy to get frog boiled if we don't change things up. I have yet to go long in ANY direction though without getting whacked by something clearly manipulative. And if you have seen these things, why would you not care about this? Because it is an existential threat to everyone. Regardless of how tough any individual is at dealing with this, no man is an island. At least Nazi, USSR, and similar propaganda had to attempt to deal with population wide Overton window type constraints, so it could only be so aggressive. These situations are closer to having your own NKVD 'friend' following you around all day with a detailed file from the Stasi. It is already not going well for people's mental health or Society, and it will get worse. reply matrix87 10 hours agorootparentYeah maybe some people like getting manipulated. What's wrong with that if they're having a good time? reply lazide 10 hours agorootparentHow did that work out for Nazi Germany, or the USSR, or Cambodia during the Khmer Rouge, etc? Everyone suffers (or succeeds, if it’s a good path) when enough people just go along for the ride. No man is an island, or We’re all in this boat together - let’s not sink it. reply matrix87 6 hours agorootparentWho's better off, people who are more paranoid or people who are more trusting? It's not an easy question reply p_l 18 hours agoparentprevAFAIK at least in case of Google, they used to closely guard that data, if only for business advantage. As in, you could only benefit from it if you used Google product, you couldn't get brokerage/raw data. No idea about Microsoft, but I don't know if they have their own ad arm like Google. reply tomoyoirl 18 hours agorootparent“We do not sell your personal information. It’s more of a rental.” reply p_l 17 hours agorootparentMore like \"You can pay us to use the personal information to get the ad to the groups you want. But we're not going to share the information, because it's our advantage over you\" reply cbarrick 18 hours agoparentprevGoogle has zero reason to share data with brokers. It is much more valuable to them to prevent competitors from accessing that data. reply drrlvn 18 hours agoparentprevI guess I’m Wikipedia’s product then. reply afavour 18 hours agorootparentNot quite, Wikipedia is a paid product. If you don’t pay someone else is covering the cost for you. reply oldkinglog 17 hours agorootparentThis is quite a misleading comment IMO. You don't \"pay\" donations, you make them. There is no exchange and no debts are settled. And wikipedia isn't a \"product\" in the usual sense of a commercial product. reply blantonl 19 hours agoprevData brokerage is big business, and the revenue share is significant enough for many businesses to hold their nose, turn their head, and sign on. reply vachina 17 hours agoprevHow else are they gonna stay afloat? Giphy is nothing but a glorified .gif file host, nobody is going to pay for this service. reply crimsontech 16 hours agoparentIsn’t it used in MS Teams, Slack and other messaging and chat apps? I expected they had some kind of API and deal with these companies. reply explain 18 hours agoprevThey serve programmatic ads like anyone else. That's all this means. reply lagniappe 19 hours agoprevGiven the valuation rollercoaster, and the shutterstock acquisition, this is no surprise. reply 082349872349872 18 hours agoprevIs anyone keeping a list of websites which are (a) relatively public, and (b) only serve data? (as news.ycombinator.com appears to do) > And if you gaze long into a cyberspace, the cyberspace also gazes into you. —not FWN reply edmundsauto 18 hours agoprevSecond order effect of forcing a sale by Meta. At least they would have kept the data from getting shared with 800+ data brokers with who knows what kind of security and decency practices. reply jacekm 18 hours agoprevWouldn't one have to agree to the cookies on Giphy site for the data to be shared? There's no such possibility in Slack, so I'd assume different T&C apply here. reply cjk2 19 hours agoprevWell it's not like they had any other business model is it? reply cj 18 hours agoparentWith 3rd party cookies going away, a lot of companies are relying on ipv4 and ipv6 hashes to correlate users across sites with first party cookies. In other words, if Giphy has your email and IP address, they can help companies like LinkedIn and other brokers de-anonymize IP addresses more accurately with higher percent accuracy. ISPs could mitigate this by rotating IPs every week, but of course you typically get 1 IP and it rarely changes. reply asmor 17 hours agorootparent> You typically get 1 IP and it rarely changes. At least in Germany this is only somewhat true on fiber. Most cable/DSL has been on dynamic assignment forever, and some are even on CGNAT for IPv4 now. IPs are shitty stable identifiers - and yet, before we got a static IP with our fiber provider, the neighbors YouTube recommendations leaked into our own. reply cj 17 hours agorootparentI noticed with LinkedIn they send 1 tracking request with a hash of your ipv4, then the request 302’s to a 2nd API that sends the ipv4 hash together with the ipv6 hash (along with your email if the advertiser provides it in the tracking code). My ISP is also dynamic assignment but it never changes, so basically it’s static. It’s definitely not a good stable identifier. But when you combine enough signals like that (plus screen size, geo ip, ISP provider, etc..) you can get pretty close to being just as accurate (or even more accurate) compared to 3rd party cookies. Even if your IP changes once a day, that might be enough to keep your profile connected if you use LinkedIn, Facebook, Giphy, etc on a daily basis. Another benefit of this over 3rd party cookies is easier cross-device tracking. The whole phasing out of 3rd party cookies might be a net negative for privacy because it forces advertisers to send even more data to ad networks in the never ending pursuit of view/click to conversion attribution. Without 3rd party cookies, the best way to track conversions is to give Google/Linkedin/etc your name/email/phone/mailing address so they can try to map your clicks to the conversion indirectly without cookies. No 3rd party cookies = huge catalyst for data broker industry. reply stemlord 16 hours agoprevThis should come as a surprise to no one. Any platform returning a whole webpage with a prominent \"download our app\" button when you try loading a direct link to a .gif is clearly up to some major bs. reply paulcole 18 hours agoprevI like using Giphy and my IP address and private data aren’t something I can directly monetize. So I’ll keep using Giphy and Giphy can figure out how to monetize my IP address and private data. No issues here. reply schiffern 18 hours agoprevuBlock Origin filter: ! Block Giphy, excessive tracking/sharing https://news.ycombinator.com/item?id=39884009 ||giphy.com reply realfeel78 18 hours agoparentGiphy is not now Facebook: https://www.theguardian.com/technology/2023/may/23/facebook-... reply georgeplusplus 17 hours agoprevHow is it that no one has come up with a way to easily inform the user/customer what is being collected and for what? My initial thought would be something along the lines of providing like an ingredients label for food but for data collected from web pages. That would help inform the user what exactly and how pervasive the data they are collecting rather than bucketing it under a common term such as the “data” application services should provide User accounts with exactly what about them was collected and stored in an account summary and ideally if they sold it how much they made off of it as well reply lazide 16 hours agoparentWhy would they want to do this? Transparency is not in their favor. reply ColoursofOSINT 17 hours agoprevGiphy also does not have a privacy policy for their Firefox extension, but run an analytics script, which I wrote about and sent them an email to which they ignored me despite sending conformation of receiving it. No way to know the data being collected, or opt out. https://www.coloursofosint.com/posts/Investigating-Firefox-P... reply lloydatkinson 18 hours agoprevI occasionally post a meme GIF in my posts, and I had been downloading the ones I wanted from Giphy and Tenor and hosting the file on my site… definitely glad I did that instead of unknowingly generating profit for a company simply by having visitors on my site. reply superkuh 18 hours agoprevWow. Giphy must have drastically changed since my last interactions with the dev team ~2016. Back then they actually went out of their way to respond to (and fix!) my bug reports for obscure Firefox forks. I was really impressed with them. The natural lifecycle of free image hosts is well known and short though. Enshittification is inevitable. reply Pesthuf 19 hours agoprev [–] We value your privacy. That's why we share every little piece of data we get on you with our 816 partners and give them full permission to do with it whatever the hell they want, including resell it to their thousands of partners. For your convenience, we have installed state of the art surveillance technology in your home so we can watch you eat, sleep, take a shower as well as record every other event of interest. For your safety, our associates undertake regular visits to your registered address to open and thoroughly check all your mail before it reaches you and share every bit of info found it in with all of our partners. We value your privacy. reply Habgdnv 18 hours agoparentIn my personal (empty) blog my privacy statement starts with: \"Unlike all others I do not value your privacy.\" I will just paste the rest of the whole privacy statement here: This website does not collect any personal information about visitors. My server is configured to not keep any logs, except for error logs, which do not include IP addresses. This is the whole content of my privacy page. 25+ pages that other websites use, i managed to shorten to 2 lines. reply Squeeeez 17 hours agorootparentHow do you deal with bots, crawlers, and other unwelcome guests? reply Habgdnv 13 hours agorootparentWell, it's public facing empty site. I am still trying to come up with a good first blog post. I keep it because it's custom asp app and I play with it. I don't have visitors yet. Only bots :) reply xattt 18 hours agoparentprevValue is used as a verb in the sense that they are deriving value from the activities you do in the privacy of your home. reply echelon_musk 18 hours agorootparentExactly this. It's a disingenuous white lie. 'We value [your lack of] privacy.' reply wewxjfq 18 hours agoparentprevWhen I looked at privacy policies, I rarely found the \"We value your privacy\" line in privacy policies that seem to treat your personal data with care. reply schiffern 18 hours agorootparent\"We value your privacy. Literally. Scarcity makes anything fetch a better price.\" reply evrenesat 18 hours agorootparentOr once could say, \"Rest assured, as we gradually erode the sanctity of your privacy, the time will come when all interested parties will have sufficiently profiled you. Eventually, you will be distilled into a simple element within a behavioural prediction model. At that point, we will be anticipating your actions even before you are aware of them. Then you won't have to worry about being watched.\" reply schiffern 17 hours agorootparentPerfect behavioral prediction wouldn't be enough. They want nothing less than perfect behavioral control (modulo a few 'free choices' they allow). Lest we forget, this is (of course) the ultimate goal of all advertising. reply amelius 18 hours agoparentprevIt's time we start charging big for this negative externality of modern capitalism. By the way, if our data is worth money then companies using our data without consent are stealing from us in the same sense that piracy or patent infringement is stealing. Big corporations cannot have it both ways. reply smokel 18 hours agorootparent> if our data is worth money then companies using our data without consent are stealing from us By that reasoning Pepsi is stealing unsatisfied customers from Coca-Cola and should be put in jail. In other words, I don't think this is how capitalism works. reply anjel 17 hours agorootparentIf there is valid consideration its not unjust enrichment. And the bar for valid consideration is a low one. Any This for that exchange will satisfy this requirement. \"In exchange for providing you this nearly worthless meme graphic someone else made, you give us panoptic access to your private online activities.\" reply amelius 15 hours agorootparentprevThe whole point is that the reasoning is flawed. Big companies are applying their own flawed logic and ignoring the logical consequences of it. reply u32480932048 17 hours agorootparentprevSee also: \"Every vote for Sprite is just a vote for [Coke|Pepsi]\" It's not how any of it works, but it sounds clever to some, I guess. reply j45 18 hours agoparentprevNothing is free. The internet always costs someone. If it's free, you're likely the product being sold. reply fbdab103 18 hours agorootparentEven when I pay, I still get monitored. Supposedly more so as I have identified myself as someone with resources. Are there any services which shut down analytics if I start paying? Cars, TVs, streaming boxes, whatever. Physical devices for which I spend money, but the vendor still thinks they are entitled to know my every move. reply j45 18 hours agorootparentIt takes a bit more work to find something that is less, or not privacy invading. Consumer products want to keep the consumer consuming. One can start by learning yunohost or sandstorm and self-hosting. Buy a car and put your own smarts in it. Don't connect your TV to the internet and connect your own box to it. You can find streaming box setups that are pretty locked down running a more secured version of android. Unfortunately the \"I've got nothing to hide\" crowd made it easier for this kind of tracking. reply tremon 17 hours agoparentprev\"We value your privacy at $0.015 per bit of information\" reply xyst 18 hours agoparentprevOh wow, free security and surveillance! Sign me up!!1 \\s reply api 18 hours agoparentprev [–] How else would giphy make money? If its free and has no other visible way to make money you are the product. reply tomrod 18 hours agorootparentIn a way that doesn't violate privacy and security. If they can't, they shouldn't be in business the same way a cereal company that cuts their product with dirt or sawdust shouldn't be in business. The death of bad businesses is a good thing. reply weego 18 hours agorootparentThe irony of people down voting the above and then reading this response on a VC run community platform is palpable. What do you think everyone's been doing during the ignore unit price era? reply tomrod 18 hours agorootparentHN is a maven trap, not a product. It helps YC to market, but is not the product itself. reply maxcoder4 17 hours agorootparentprevBy starting to charge money. Maybe not realistic right now because of the competition, but I'm sure it would look different if privacy abuse was illegal and everyone had to adapt. reply BobaFloutist 18 hours agorootparentprev [–] I imagine media properties could pay Giphy to promote gifs of their properties. Probably wouldn't be that bad of an advertising strategy for a new release! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "When sharing a GIPHY image, data is transmitted to 816 partners, according to a tweet by Ilias Ism on March 31, 2024."
    ],
    "commentSummary": [
      "Privacy concerns arise from Giphy sharing users' IP addresses and private data with partners, fueling debates on data sharing practices, government surveillance, and data collection ethics.",
      "Participants fear political manipulation, misuse of data for targeted ads, propaganda, and surveillance, stressing the importance of transparency in data collection, cautious trust in companies with personal data, and safeguarding privacy in the digital era."
    ],
    "points": 173,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1711891360
  },
  {
    "id": 39888020,
    "title": "AMD 7900XTX (Navi31) GPU Architecture Details and Driver Installation Guide",
    "originLink": "https://github.com/geohot/7900xtx",
    "originBody": "Documentation for the 7900XTX aka Navi31 aka gfx1100 aka Plum Bonito aka amd744c PSP = Platform Security Processor (ARM) SOS = Secure Operating System (psp_13_0_0_sos.bin) TA = Trusted Application (psp_13_0_0_ta.bin) KDB = Key DataBase TMR = Trusted Memory Region SMU = System Management Unit (smuio1306) (amdgpu/smu_13_0_0.bin) (RS64?) GC = Graphics and Compute (gfx1100) CP (Command Processor) = PFP,ME,CE,MEC Drawing Engine (idle during compute) PFP = Pre-Fetch Parser (gc_11_0_0_pfp.bin) ME = Micro Engine (gc_11_0_0_me.bin) RLC = RunList Controller (gc_11_0_0_rlc.bin) (F32) MEC = Micro Engine Compute (gc_11_0_0_mec.bin) (RS64) MES = Micro Engine Scheduler (gc_11_0_0_mes1.bin) (gc_11_0_0_mes_2.bin) (RS64) IMU = Integrated Memory Controller Utility (gc_11_0_0_imu.bin) CE = Constant Engine (not on RDNA3?) DCN = Display Core Next (dcn320) (amdgpu/dcn_3_2_0_dmcub.bin) VCN = Video Core Next (encoder/decoder) (vcn400) (vcn_4_0_0.bin) SDMA = System DMA (lsdma600) (sdma_6_0_0.bin) (F32) More info on each piece: https://mjmwired.net/kernel/Documentation/gpu/amdgpu/driver-core.rst Architechture Diagram 1x 5nm GCD (graphics compute die) 6x 6nm MCD (memory cache die) More about the Compute Unit Dumping registers # list regs with bits sudo umr -lr amd744c.gfx1100 -O bits # dump regs sudo umr -s amd744c.gfx1100 umr --top GRBM (Graphics Register Backbone Manager) Bits: TA => 95.0 %GDS => 0.0 % SX => 0.0 %SPI => 96.0 % BCI => 0.0 %SC => 0.0 % PA => 0.0 %DB => 0.0 % CP_COHERENCY => 0.0 %CP => 97.0 % CB => 0.0 %GUI => 97.0 % GE => 0.0 %RLC => 96.0 % CPF => 97.0 %CPC => 97.0 % CPG => 0.0 %TA (Texture Addresser) Bits: IN => 33.0 %FG => 0.0 % LA => 0.0 %FL => 0.0 % TA => 0.0 %FA => 44.0 % AL => 70.0 %Installing AMDGPU sudo apt-get install linux-generic-hwe-22.04 # add user to render+video groups Rebuilding amdgpu kernel module https://imil.net/blog/posts/2022/build-a-single-in-tree-linux-kernel-module-debian--clones/ sudo rmmod amdgpu && make -C . M=drivers/gpu/drm/amd/amdgpu && sudo insmod drivers/gpu/drm/amd/amdgpu/amdgpu.ko # gpu_recovery doesn't seem to work #sudo modprobe amdgpu gpu_recovery=0 Enable debug prints in dmesg sudo su -c \"echo 'file drivers/gpu/drm/amd/* +p' > /sys/kernel/debug/dynamic_debug/control\" echo 0x19Fsudo tee /sys/module/drm/parameters/debug # Enable verbose DRM logging HSAKMT_DEBUG_LEVEL=7 # user space debugging Links https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf https://lists.freedesktop.org/archives/amd-gfx/2022-April/078410.html https://docs.kernel.org/gpu/amdgpu/driver-core.html https://wiki.gentoo.org/wiki/AMDGPU https://mjmwired.net/kernel/Documentation/gpu/amdgpu/driver-core.rst https://www.kernel.org/doc/html/v6.8/gpu/amdgpu/amdgpu-glossary.html https://github.com/amezin/amdgpu-pptable https://themaister.net/blog/2023/08/20/hardcore-vulkan-debugging-digging-deep-on-linux-amdgpu/ https://martty.github.io/posts/radbg_part_4/ https://www.phoronix.com/news/AMDGPU-LSDMA-Light-SDMA https://gpuopen.com/presentations/2023/RDNA3_Beyond-the-current-gen-v4.pdf https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf https://gpuopen.com/rdna/ https://github.com/fail0verflow/radeon-tools https://github.com/gnif/vendor-reset https://sourceware.org/gdb/current/onlinedocs/gdb.html/AMD-GPU.html More Acronyms MQD: Memory Queue Descriptor GMC: Graphic Memory Controller BACO: Bus Active, Chip Off Listing IP blocks kafka@q:/sys/class/drm/card0/device/fw_version$ sudo umr -lb [WARNING]: Unknown ASIC [amd744c] should be added to pci.did to get proper name amd744c.df421{5} (4.2.1) amd744c.df421{3} (4.2.1) amd744c.df421{1} (4.2.1) amd744c.df421{6} (4.2.1) amd744c.df421{4} (4.2.1) amd744c.df421{2} (4.2.1) amd744c.df420 (4.2.0) amd744c.mp11300 (13.0.0) amd744c.lsdma600 (6.0.0) amd744c.mmhub300 (3.0.0) amd744c.osssys600 (6.0.0) amd744c.vcn401{1} (4.0.1) amd744c.vcn400 (4.0.0) amd744c.hdp600 (6.0.0) amd744c.smuio1306 (13.0.6) amd744c.nbio430 (4.3.0) amd744c.athub300 (3.0.0) amd744c.dcn320 (3.2.0) amd744c.thm1305{5} (13.0.5) amd744c.thm1305{3} (13.0.5) amd744c.thm1305{1} (13.0.5) amd744c.thm1305{6} (13.0.5) amd744c.thm1305{4} (13.0.5) amd744c.thm1305{2} (13.0.5) amd744c.thm1303 (13.0.3) amd744c.mp01300 (13.0.0) amd744c.umc8100{5} (8.10.0) amd744c.umc8100{3} (8.10.0) amd744c.umc8100{1} (8.10.0) amd744c.umc8100{4} (8.10.0) amd744c.umc8100{2} (8.10.0) amd744c.umc8100{0} (8.10.0) amd744c.gfx1100 (11.0.0) [46444.513680] [drm] amdgpu kernel modesetting enabled. [46444.513978] amdgpu: CRAT table disabled by module option [46444.513986] amdgpu: Virtual CRAT table created for CPU [46444.514013] amdgpu: Topology: Add CPU node [46444.514595] [drm] initializing kernel modesetting (IP DISCOVERY 0x1002:0x744C 0x1002:0x0E3B 0xC8). [46444.514638] [drm] register mmio base: 0xFB300000 [46444.514641] [drm] register mmio size: 1048576 [46444.520723] [drm] add ip block number 0[46444.520729] [drm] add ip block number 1[46444.520733] [drm] add ip block number 2[46444.520735] [drm] add ip block number 3[46444.520737] [drm] add ip block number 4[46444.520740] [drm] add ip block number 5[46444.520742] [drm] add ip block number 6[46444.520745] [drm] add ip block number 7[46444.520747] [drm] add ip block number 8[46444.520749] [drm] add ip block number 9[46444.520752] [drm] add ip block number 10root@q:/sys/kernel/debug/dri/0# cat amdgpu_firmware_info VCE feature version: 0, firmware version: 0x00000000 UVD feature version: 0, firmware version: 0x00000000 MC feature version: 0, firmware version: 0x00000000 ME feature version: 29, firmware version: 0x000005da PFP feature version: 29, firmware version: 0x00000605 CE feature version: 0, firmware version: 0x00000000 RLC feature version: 1, firmware version: 0x00000074 RLC SRLC feature version: 0, firmware version: 0x00000000 RLC SRLG feature version: 0, firmware version: 0x00000000 RLC SRLS feature version: 0, firmware version: 0x00000000 RLCP feature version: 1, firmware version: 0x00000019 RLCV feature version: 1, firmware version: 0x00000022 MEC feature version: 29, firmware version: 0x000001fe IMU feature version: 0, firmware version: 0x0b1f3600 SOS feature version: 3211301, firmware version: 0x00310025 ASD feature version: 553648282, firmware version: 0x2100009a TA XGMI feature version: 0x00000000, firmware version: 0x00000000 TA RAS feature version: 0x00000000, firmware version: 0x1b000201 TA HDCP feature version: 0x00000000, firmware version: 0x17000031 TA DTM feature version: 0x00000000, firmware version: 0x12000013 TA RAP feature version: 0x00000000, firmware version: 0x00000000 TA SECUREDISPLAY feature version: 0x00000000, firmware version: 0x00000000 SMC feature version: 0, program: 0, firmware version: 0x004e5500 (78.85.0) SDMA0 feature version: 60, firmware version: 0x00000013 SDMA1 feature version: 60, firmware version: 0x00000013 VCN feature version: 0, firmware version: 0x0510b023 DMCU feature version: 0, firmware version: 0x00000000 DMCUB feature version: 0, firmware version: 0x07000a01 TOC feature version: 12, firmware version: 0x0000000c MES_KIQ feature version: 6, firmware version: 0x0000006a MES feature version: 1, firmware version: 0x00000034 VBIOS version: 113-D7020100-102 [80568.560601] amdgpu_ucode_request amdgpu/psp_13_0_0_sos.bin [80568.560737] amdgpu_ucode_request amdgpu/psp_13_0_0_ta.bin [80568.560917] amdgpu_ucode_request amdgpu/smu_13_0_0.bin [80568.561082] amdgpu_ucode_request amdgpu/dcn_3_2_0_dmcub.bin [80568.561225] amdgpu_ucode_request amdgpu/gc_11_0_0_pfp.bin [80568.561362] amdgpu_ucode_request amdgpu/gc_11_0_0_me.bin [80568.561491] amdgpu_ucode_request amdgpu/gc_11_0_0_rlc.bin [80568.561765] amdgpu_ucode_request amdgpu/gc_11_0_0_mec.bin [80568.562034] amdgpu_ucode_request amdgpu/vcn_4_0_0.bin [80568.562398] amdgpu_ucode_request amdgpu/gc_11_0_0_mes_2.bin [80568.562537] amdgpu_ucode_request amdgpu/gc_11_0_0_mes1.bin [80569.088855] amdgpu_ucode_request amdgpu/gc_11_0_0_imu.bin [80569.089068] amdgpu_ucode_request amdgpu/sdma_6_0_0.bin kafka@q:~/7900xtx/fwinfo$ ./go.sh /lib/firmware/amdgpu/psp_13_0_0_sos.bin size_bytes:0x41810 ucode_size_bytes:0x41710 /lib/firmware/amdgpu/psp_13_0_0_ta.bin size_bytes:0x39500 ucode_size_bytes:0x39400 /lib/firmware/amdgpu/smu_13_0_0.bin size_bytes:0x47664 ucode_size_bytes:0x3fe00 /lib/firmware/amdgpu/gc_11_0_0_pfp.bin size_bytes:0x32C70 ucode_size_bytes:0x32b70 /lib/firmware/amdgpu/gc_11_0_0_me.bin size_bytes:0x2E450 ucode_size_bytes:0x2e350 /lib/firmware/amdgpu/gc_11_0_0_rlc.bin size_bytes:0x2D2A0 ucode_size_bytes:0x6200 /lib/firmware/amdgpu/gc_11_0_0_mec.bin size_bytes:0x63620 ucode_size_bytes:0x63520 /lib/firmware/amdgpu/gc_11_0_0_mes_2.bin size_bytes:0x46A00 ucode_size_bytes:0x46900 /lib/firmware/amdgpu/gc_11_0_0_mes1.bin size_bytes:0x33AF0 ucode_size_bytes:0x339f0 /lib/firmware/amdgpu/gc_11_0_0_imu.bin size_bytes:0x20500 ucode_size_bytes:0x20400 /lib/firmware/amdgpu/dcn_3_2_0_dmcub.bin size_bytes:0x41790 ucode_size_bytes:0x41690 /lib/firmware/amdgpu/vcn_4_0_0.bin size_bytes:0x5DD50 ucode_size_bytes:0x5dc50 /lib/firmware/amdgpu/sdma_6_0_0.bin size_bytes:0x 8700 ucode_size_bytes:0x8600 Firmware loads [ 1370.296802] psp_prep_load_ip_fw_cmd_buf: 0000000000C49000 type:18 sz:0x3fe00 # GFX_FW_TYPE_SMU [ 1431.136232] psp_prep_load_ip_fw_cmd_buf: 0000000000C49000 type:18 sz:0x3fe00 # GFX_FW_TYPE_SMU [ 1432.229979] psp_prep_load_ip_fw_cmd_buf: 0000000000A00000 type:71 sz:0x4400 # GFX_FW_TYPE_SDMA_UCODE_TH0 [ 1432.234035] psp_prep_load_ip_fw_cmd_buf: 0000000000A05000 type:72 sz:0x4200 # GFX_FW_TYPE_SDMA_UCODE_TH1 [ 1432.237930] psp_prep_load_ip_fw_cmd_buf: 0000000000A0A000 type:87 sz:0x10b70 # GFX_FW_TYPE_RS64_PFP [ 1432.242088] psp_prep_load_ip_fw_cmd_buf: 0000000000A1B000 type:88 sz:0xc350 # GFX_FW_TYPE_RS64_ME [ 1432.246220] psp_prep_load_ip_fw_cmd_buf: 0000000000A28000 type:89 sz:0x41520 # GFX_FW_TYPE_RS64_MEC [ 1432.252719] psp_prep_load_ip_fw_cmd_buf: 0000000000A6A000 type:90 sz:0x22000 # GFX_FW_TYPE_RS64_PFP_P0_STACK [ 1432.256295] psp_prep_load_ip_fw_cmd_buf: 0000000000A8C000 type:91 sz:0x22000 # GFX_FW_TYPE_RS64_PFP_P1_STACK [ 1432.259908] psp_prep_load_ip_fw_cmd_buf: 0000000000AAE000 type:92 sz:0x22000 # GFX_FW_TYPE_RS64_ME_P0_STACK [ 1432.263390] psp_prep_load_ip_fw_cmd_buf: 0000000000AD0000 type:93 sz:0x22000 # GFX_FW_TYPE_RS64_ME_P1_STACK [ 1432.267162] psp_prep_load_ip_fw_cmd_buf: 0000000000AF2000 type:94 sz:0x22000 # GFX_FW_TYPE_RS64_MEC_P0_STACK [ 1432.271029] psp_prep_load_ip_fw_cmd_buf: 0000000000B14000 type:95 sz:0x22000 # GFX_FW_TYPE_RS64_MEC_P1_STACK [ 1432.274775] psp_prep_load_ip_fw_cmd_buf: 0000000000B36000 type:96 sz:0x22000 # GFX_FW_TYPE_RS64_MEC_P2_STACK [ 1432.278344] psp_prep_load_ip_fw_cmd_buf: 0000000000B58000 type:97 sz:0x22000 # GFX_FW_TYPE_RS64_MEC_P3_STACK [ 1432.281782] psp_prep_load_ip_fw_cmd_buf: 0000000000B7A000 type:33 sz:0x26900 # GFX_FW_TYPE_CP_MES [ 1432.286907] psp_prep_load_ip_fw_cmd_buf: 0000000000BA1000 type:34 sz:0x20000 # GFX_FW_TYPE_MES_STACK [ 1432.290773] psp_prep_load_ip_fw_cmd_buf: 0000000000BC1000 type:81 sz:0x139f0 # GFX_FW_TYPE_CP_MES_KIQ [ 1432.295392] psp_prep_load_ip_fw_cmd_buf: 0000000000BD5000 type:82 sz:0x20000 # GFX_FW_TYPE_MES_KIQ_STACK [ 1432.299012] psp_prep_load_ip_fw_cmd_buf: 0000000000BF5000 type:68 sz:0x10200 # GFX_FW_TYPE_IMU_I [ 1432.303549] psp_prep_load_ip_fw_cmd_buf: 0000000000C06000 type:69 sz:0x10200 # GFX_FW_TYPE_IMU_D [ 1432.307778] psp_prep_load_ip_fw_cmd_buf: 0000000000C17000 type:20 sz:0xa00 # GFX_FW_TYPE_RLC_RESTORE_LIST_GPM_MEM [ 1432.311519] psp_prep_load_ip_fw_cmd_buf: 0000000000C18000 type:21 sz:0x8da0 # GFX_FW_TYPE_RLC_RESTORE_LIST_SRM_MEM [ 1432.315520] psp_prep_load_ip_fw_cmd_buf: 0000000000C21000 type:26 sz:0x10200 # GFX_FW_TYPE_RLC_IRAM [ 1432.319925] psp_prep_load_ip_fw_cmd_buf: 0000000000C32000 type:48 sz:0x8200 # GFX_FW_TYPE_RLC_DRAM_BOOT [ 1432.323649] psp_prep_load_ip_fw_cmd_buf: 0000000000C3B000 type:25 sz:0x2200 # GFX_FW_TYPE_RLC_P [ 1432.327185] psp_prep_load_ip_fw_cmd_buf: 0000000000C3E000 type:7 sz:0x3200 # GFX_FW_TYPE_RLC_V [ 1432.331122] psp_prep_load_ip_fw_cmd_buf: 0000000000C42000 type:8 sz:0x6200 # GFX_FW_TYPE_RLC_G [ 1432.335348] psp_prep_load_ip_fw_cmd_buf: 0000000000C89000 type:13 sz:0x5dc50 # GFX_FW_TYPE_VCN [ 1432.343011] psp_prep_load_ip_fw_cmd_buf: 0000000000CE7000 type:58 sz:0x5dc50 # GFX_FW_TYPE_VCN1 [ 1432.350895] psp_prep_load_ip_fw_cmd_buf: 0000000000D45000 type:51 sz:0x41690 # GFX_FW_TYPE_DMUB /* FW types for GFX_CMD_ID_LOAD_IP_FW command. Limit 31. */ enum psp_gfx_fw_type {GFX_FW_TYPE_NONE = 0, /* */GFX_FW_TYPE_CP_ME = 1, /* CP-ME VG + RV */GFX_FW_TYPE_CP_PFP = 2, /* CP-PFP VG + RV */GFX_FW_TYPE_CP_CE = 3, /* CP-CE VG + RV */GFX_FW_TYPE_CP_MEC = 4, /* CP-MEC FW VG + RV */GFX_FW_TYPE_CP_MEC_ME1 = 5, /* CP-MEC Jump Table 1 VG + RV */GFX_FW_TYPE_CP_MEC_ME2 = 6, /* CP-MEC Jump Table 2 VG */GFX_FW_TYPE_RLC_V = 7, /* RLC-V VG */GFX_FW_TYPE_RLC_G = 8, /* RLC-G VG + RV */GFX_FW_TYPE_SDMA0 = 9, /* SDMA0 VG + RV */GFX_FW_TYPE_SDMA1 = 10, /* SDMA1 VG */GFX_FW_TYPE_DMCU_ERAM = 11, /* DMCU-ERAM VG + RV */GFX_FW_TYPE_DMCU_ISR = 12, /* DMCU-ISR VG + RV */GFX_FW_TYPE_VCN = 13, /* VCN RV */GFX_FW_TYPE_UVD = 14, /* UVD VG */GFX_FW_TYPE_VCE = 15, /* VCE VG */GFX_FW_TYPE_ISP = 16, /* ISP RV */GFX_FW_TYPE_ACP = 17, /* ACP RV */GFX_FW_TYPE_SMU = 18, /* SMU VG */GFX_FW_TYPE_MMSCH = 19, /* MMSCH VG */GFX_FW_TYPE_RLC_RESTORE_LIST_GPM_MEM = 20, /* RLC GPM VG + RV */GFX_FW_TYPE_RLC_RESTORE_LIST_SRM_MEM = 21, /* RLC SRM VG + RV */GFX_FW_TYPE_RLC_RESTORE_LIST_SRM_CNTL = 22, /* RLC CNTL VG + RV */GFX_FW_TYPE_UVD1 = 23, /* UVD1 VG-20 */GFX_FW_TYPE_TOC = 24, /* TOC NV-10 */GFX_FW_TYPE_RLC_P = 25, /* RLC P NV */GFX_FW_TYPE_RLC_IRAM = 26, /* RLC_IRAM NV */GFX_FW_TYPE_GLOBAL_TAP_DELAYS = 27, /* GLOBAL TAP DELAYS NV */GFX_FW_TYPE_SE0_TAP_DELAYS = 28, /* SE0 TAP DELAYS NV */GFX_FW_TYPE_SE1_TAP_DELAYS = 29, /* SE1 TAP DELAYS NV */GFX_FW_TYPE_GLOBAL_SE0_SE1_SKEW_DELAYS = 30, /* GLOBAL SE0/1 SKEW DELAYS NV */GFX_FW_TYPE_SDMA0_JT = 31, /* SDMA0 JT NV */GFX_FW_TYPE_SDMA1_JT = 32, /* SDNA1 JT NV */GFX_FW_TYPE_CP_MES = 33, /* CP MES NV */GFX_FW_TYPE_MES_STACK = 34, /* MES STACK NV */GFX_FW_TYPE_RLC_SRM_DRAM_SR = 35, /* RLC SRM DRAM NV */GFX_FW_TYPE_RLCG_SCRATCH_SR = 36, /* RLCG SCRATCH NV */GFX_FW_TYPE_RLCP_SCRATCH_SR = 37, /* RLCP SCRATCH NV */GFX_FW_TYPE_RLCV_SCRATCH_SR = 38, /* RLCV SCRATCH NV */GFX_FW_TYPE_RLX6_DRAM_SR = 39, /* RLX6 DRAM NV */GFX_FW_TYPE_SDMA0_PG_CONTEXT = 40, /* SDMA0 PG CONTEXT NV */GFX_FW_TYPE_SDMA1_PG_CONTEXT = 41, /* SDMA1 PG CONTEXT NV */GFX_FW_TYPE_GLOBAL_MUX_SELECT_RAM = 42, /* GLOBAL MUX SEL RAM NV */GFX_FW_TYPE_SE0_MUX_SELECT_RAM = 43, /* SE0 MUX SEL RAM NV */GFX_FW_TYPE_SE1_MUX_SELECT_RAM = 44, /* SE1 MUX SEL RAM NV */GFX_FW_TYPE_ACCUM_CTRL_RAM = 45, /* ACCUM CTRL RAM NV */GFX_FW_TYPE_RLCP_CAM = 46, /* RLCP CAM NV */GFX_FW_TYPE_RLC_SPP_CAM_EXT = 47, /* RLC SPP CAM EXT NV */GFX_FW_TYPE_RLC_DRAM_BOOT = 48, /* RLC DRAM BOOT NV */GFX_FW_TYPE_VCN0_RAM = 49, /* VCN_RAM NV + RN */GFX_FW_TYPE_VCN1_RAM = 50, /* VCN_RAM NV + RN */GFX_FW_TYPE_DMUB = 51, /* DMUB RN */GFX_FW_TYPE_SDMA2 = 52, /* SDMA2 MI */GFX_FW_TYPE_SDMA3 = 53, /* SDMA3 MI */GFX_FW_TYPE_SDMA4 = 54, /* SDMA4 MI */GFX_FW_TYPE_SDMA5 = 55, /* SDMA5 MI */GFX_FW_TYPE_SDMA6 = 56, /* SDMA6 MI */GFX_FW_TYPE_SDMA7 = 57, /* SDMA7 MI */GFX_FW_TYPE_VCN1 = 58, /* VCN1 MI */GFX_FW_TYPE_CAP = 62, /* CAP_FW */GFX_FW_TYPE_SE2_TAP_DELAYS = 65, /* SE2 TAP DELAYS NV */GFX_FW_TYPE_SE3_TAP_DELAYS = 66, /* SE3 TAP DELAYS NV */GFX_FW_TYPE_REG_LIST = 67, /* REG_LIST MI */GFX_FW_TYPE_IMU_I = 68, /* IMU Instruction FW SOC21 */GFX_FW_TYPE_IMU_D = 69, /* IMU Data FW SOC21 */GFX_FW_TYPE_LSDMA = 70, /* LSDMA FW SOC21 */GFX_FW_TYPE_SDMA_UCODE_TH0 = 71, /* SDMA Thread 0/CTX SOC21 */GFX_FW_TYPE_SDMA_UCODE_TH1 = 72, /* SDMA Thread 1/CTL SOC21 */GFX_FW_TYPE_PPTABLE = 73, /* PPTABLE SOC21 */GFX_FW_TYPE_DISCRETE_USB4 = 74, /* dUSB4 FW SOC21 */GFX_FW_TYPE_TA = 75, /* SRIOV TA FW UUID SOC21 */GFX_FW_TYPE_RS64_MES = 76, /* RS64 MES ucode SOC21 */GFX_FW_TYPE_RS64_MES_STACK = 77, /* RS64 MES stack ucode SOC21 */GFX_FW_TYPE_RS64_KIQ = 78, /* RS64 KIQ ucode SOC21 */GFX_FW_TYPE_RS64_KIQ_STACK = 79, /* RS64 KIQ Heap stack SOC21 */GFX_FW_TYPE_ISP_DATA = 80, /* ISP DATA SOC21 */GFX_FW_TYPE_CP_MES_KIQ = 81, /* MES KIQ ucode SOC21 */GFX_FW_TYPE_MES_KIQ_STACK = 82, /* MES KIQ stack SOC21 */GFX_FW_TYPE_UMSCH_DATA = 83, /* User Mode Scheduler Data SOC21 */GFX_FW_TYPE_UMSCH_UCODE = 84, /* User Mode Scheduler Ucode SOC21 */GFX_FW_TYPE_UMSCH_CMD_BUFFER = 85, /* User Mode Scheduler Command Buffer SOC21 */GFX_FW_TYPE_USB_DP_COMBO_PHY = 86, /* USB-Display port Combo SOC21 */GFX_FW_TYPE_RS64_PFP = 87, /* RS64 PFP SOC21 */GFX_FW_TYPE_RS64_ME = 88, /* RS64 ME SOC21 */GFX_FW_TYPE_RS64_MEC = 89, /* RS64 MEC SOC21 */GFX_FW_TYPE_RS64_PFP_P0_STACK = 90, /* RS64 PFP stack P0 SOC21 */GFX_FW_TYPE_RS64_PFP_P1_STACK = 91, /* RS64 PFP stack P1 SOC21 */GFX_FW_TYPE_RS64_ME_P0_STACK = 92, /* RS64 ME stack P0 SOC21 */GFX_FW_TYPE_RS64_ME_P1_STACK = 93, /* RS64 ME stack P1 SOC21 */GFX_FW_TYPE_RS64_MEC_P0_STACK = 94, /* RS64 MEC stack P0 SOC21 */GFX_FW_TYPE_RS64_MEC_P1_STACK = 95, /* RS64 MEC stack P1 SOC21 */GFX_FW_TYPE_RS64_MEC_P2_STACK = 96, /* RS64 MEC stack P2 SOC21 */GFX_FW_TYPE_RS64_MEC_P3_STACK = 97, /* RS64 MEC stack P3 SOC21 */GFX_FW_TYPE_MAX };",
    "commentLink": "https://news.ycombinator.com/item?id=39888020",
    "commentBody": "Documentation for the AMD 7900XTX (github.com/geohot)172 points by mpereira 13 hours agohidepastfavorite85 comments roenxi 10 hours agoSo for reference, this geohot is George Hotz who has a company Tiny Corp [0] in the space. Among a long list of things, he had a moment in the spotlight recently for a long rant [1] where he \"gave up on AMD\" because their drivers sucked more than he expected. The situation is reasonably complex - AMD have some programmers working on ROCm who seemed to be operating at standard fare when that is really not what AMD needs right now (I personally, suspect there is/was a PM in a key position who didn't \"get it\", although I am not sure what it is either). As far as I know it got the attention of Lisa Su. I'm cheering him on, even though his complaints were a little melodramatic. My experience is the driver technically supports everything I could possibly want. The problem is if I spend an evening trying to do anything with OpenCL or ROCm the kernel hard-locks and I go to bed early. If the problem inside is what it looks like from the outside (repeating myself, a key manager somewhere just doesn't get the space) they really need some pressure from grumpy customers like George to realign their software development process. That context might be related to this particular case. I see a suspicious folder called \"crash\" in this repo. [0] https://tinygrad.org/ [1] https://www.youtube.com/watch?v=Mr0rWJhv9jU - as I recall he ran the demo suite and it crashed. reply ultra_nick 8 hours agoparentNVidia is making boatloads of money because their driver works and they have a software library called CUDA that accelerates neural networks. Nobody expects AMD to match them, but George thought they could at least write a GPU driver. If AMD can get that driver out, then George could provide a competitor to CUDA (for neutral networks only). They'd both make boatloads of money. However, AMD was less capable than expected and their drivers were too buggy to run neural networks like those needed for the MLPerf benchmark. So now, it appears that AMD, Tinybox, and investors like me won't be making boatloads of money. reply lofaszvanitt 44 minutes agorootparentWhy is that, that AMD seemingly can't act like a sane individual would do? reply DiabloD3 7 hours agorootparentprevSlight mistake in your description: CUDA is an out of date API that was replaced by Khronos's own official compute APIs. Khronos is a standards consortium that Nvidia is a founding member of. Although the marketing department at Nvidia still pushes for greenfield CUDA codebases, no new code should be written in it, and they should opt for open source international standards only. Khronos APIs are implemented by over 120 vendors. reply mshockwave 6 hours agorootparentWith my past experience in Khronos, NVIDIA is indeed a member and they sent decent guys to the meetings -- but only for strategic reasons, rather than \"advocating for open standards\" as you described. My experiences there actually told me the opposite that they will never drop CUDA. Objectively they also have incentive to do so: fighting with 100-ish companies to ratify something is always slower than rolling out an feature in an ecosystem you have total control of. reply baq 3 hours agorootparentprevCUDA is a vendor lock-in mechanism and they’ll happily endorse whatever Khronos says is standard… after they lose dominance. It took 30 years for x86 and just barely so. reply redox99 5 hours agorootparentprevThat's outright false and just wishful thinking from you. reply greenavocado 7 hours agorootparentprevShow me the announcement deprecating CUDA reply KeplerBoy 2 hours agorootparentprevWhy do I have the feeling that CUDA will outlive whatever Khronos has proposed (do you refer to Sycl)? reply Foobar8568 1 hour agorootparentHow may standards Khronos endorced over the years/decades on compute? From an uneducated and external view, it seems every 2-5years there is a new standard. reply bee_rider 4 hours agorootparentprevThat’s not a mistake, you just have a different preference. reply mobilio 10 hours agoparentprevAnd follow-up of that video: https://geohot.github.io/blog/jekyll/update/2023/06/07/a-div... reply lannisterstark 6 hours agoparentprev>I'm cheering him on, even though his complaints were a little melodramatic. I've really soured on him when he literally went \"I'm going to fix twitter search, hire me elon\" and then went to public saying \"Hey, want an internship? Fix it, and then MIT license your code(so he could use it lol), and maybe we can talk - oh btw I have no real authority to give anyone an internship.\" He might be a brilliant programmer and problem solver, but that incident left a very sour \"Grifter\" vibes. reply hughesjj 5 hours agorootparentYeah, I was feeling similar vibes back when Lex Friedman interviewed him for some self driving stuff back in the day (...I've soured on lex too since he's not really an interviewer/journalist so much as a host/promoter and rarely gets to anything interesting in his talks imo). His work for jailbreaking/actually being able to own the devices we pay for back in the iPhone+PS3 days was and is still inspiring, but I think being so smart and encountering so much bullshit in his life kind of led him to the 'rockstar coder I know better than you regardless of domain' personality, and that leaves him both a bit grating and underquipped to drive consensus. He's still a great wrench to throw into the bullshit machine though, and the guy is smarter and more accomplished with computers than I'm ever likely to be. reply infogulch 3 hours agorootparentThis is a great take. We need more people that are willing and able to jump in and wreck new bullshit machines as they crop up in diverse industries. They can even keep their boastful grandstanding attitude as compensation. reply fragmede 1 hour agorootparentNo thanks. We do need people to wreck the bullshit machines, but they don't have to be total asshats about it. That's not inherent to being that smart. reply quantumwoke 5 hours agorootparentprevI soured on him a bit more when I watched some of his stream yesterday and he was ranting about diversity being a root of evil in Silicon Valley. Struck me that he hasn’t matured much since he was slinging mud at Sony and Apple. reply UberFly 4 hours agorootparentHe seems to be the type who doesn't care who's \"soured on him\". reply spirobelv2 1 hour agorootparentprevI have really soured on you after reading this comment. It added zero value reply trws 6 hours agoparentprevThey do need to rework their development processes. It’s worth noting though that there are two (kinda three) different drivers with very different quality and different teams behind them. There’s the binary driver, on windows and the fglrx driver for Linux, that contains the full OpenGL stack and all the proprietary stuff. That’s been maintained and extended since the ATI days by a team primarily based in Markham. It’s garbage. It’s been garbage since the 90s. This is not new or surprising. It’s unstable, poorly maintained and just generally a bad time. It was also the reason OpenCL required an X session run as root with no access control and sometimes a stub dvi cable on the card for many years. It is a source of endless sadness and despair. The other driver is the HSA/ROCM driver on Linux, which can be used with the amdgpu upstream kernel driver and is itself upstream. The amdgpu driver is 2d only, but has been developed in the open, and stable, for a long, long time. It’s maintained by a completely separate set of teams mostly in (last I knew) Germany and Austin. The HSA driver has likewise been immensely more stable than the binary drivers since it was introduced. There are potential problems with it, but I’ve managed to recover every single one I’ve run into without a reboot, and I’ve been doing GPU compute with AMD, NVIDIA and intel since 2008. When it was the binary driver, we needed reboots every time an AMD GPU locked up. I’m not saying there’s no problem here, but he’s reporting problems I’ve never seen in thousands of hours of high-load compute both personally and for work with the driver one ought to use for ROCM. There are other problems, but HSA/amdgpu drivers have not been high on that list for me. Possibly because I was so used to how bad the other ones were I guess… reply rstat1 4 hours agorootparentI believe your info is a bit out-of-date. FGLRX hasn't been maintained or actively developed for a quite a while and unless you're using super-duper-ancient HW no one actually uses it anymore. AMDGPU is the the current Linux driver, combined with Mesa, and certainly is not 2D only. reply sspiff 2 hours agorootparentAren't the relevant 3D bits in userspace (not the driver)? Isn't the concept of 3D APIs limited to stuff like Mesa, with the kernel space AMDGPU driver providing only the primitives on which the API can then be built? I could be wrong, but that's how I understood it. I things like Fglrx, the OpenGL stack was part of the driver proper IIRC. But granted, I haven't heard of or seen FGLRX in literally more than a decade, and describing AMDGPU as 2D only is misleading or pedantic. reply pezezin 3 hours agorootparentprevYup, FGLRX was deprecated from all major distros in 2016. Nobody has used it for ages. reply pxc 3 hours agorootparentprev> The amdgpu driver is 2d only Huh? People play 3D games on that driver. Or is the 3D part just Mesa in such cases? reply benreesman 1 hour agoparentprevI’m going remind everyone that AMD and NVIDIA are atypically competition avoidant, but I don’t want to defend a generalization that broadcast let’s be specific: Hopper and MI300 are on paper just straight up competitive, both super scarce, high-margin cards that really love PyTorch, (which really loves NVIDIA, even TPU is, not the fist among equals). But all the MI300 is going to supercomputer and adjacent things, and all the Hopper is going to giant tech LLM type stuff, it’s not the same people bidding on those bins of those. Oh, and their respective CEOs are closely blood-related and in a trivially first name if not family gathering basis. The people root for George generally think this isn’t real capitalism. We think it’s a trend towards the failure to enforce anti-trust laws. reply echelon 10 hours agoparentprevAnyone working on getting us out of the Nvidia monopoly on commerical AI should be praised. We need lots of competition in this space. I'm hoping Google spins off its TPU division and sells chips to everyone that wants them. I'm hoping AMD gets their act together and that they open up their drivers and stack. I'm hoping a lot of other hardware companies and compute stacks start making inroads on this. Nvidia is not just bilking the whole world on margin, but they're also a limiting reagent in the grand scheme of progress. reply sorenjan 8 hours agorootparentYou'll note that AMD (and Nvidia, but that's expected) is unfortunately missing from the members list for UXL, which seems to be based on Intel's oneAPI which in turn is based in part on Khrono's SYCL: > Our mission: > • Build a multi-architecture multi-vendor software ecosystem for all accelerators. > • Unify the heterogeneous compute ecosystem around open standards. > • Build on and expand open-source projects for accelerated computing. https://uxlfoundation.org/ If AMD got their act together they would go all in on open standards together with other actors in the segment, but maybe they really want to see if their rocm can repeat Cuda's success and lock in? I just want accelerators to become commodities and interchangeable like CPUs, let me write my code once and deploy on whatever hardware the user have, preferably without having to compile multiple versions. reply latchkey 8 hours agorootparentprev> Anyone working on getting us out of the Nvidia monopoly on commerical AI should be praised. We need lots of competition in this space. Disclosure: I'm working on exactly that, so thank you so much for this! I'm doing it on the high end of the spectrum, with MI300x, and a lot less drama. reply atq2119 8 hours agorootparentprev> I'm hoping AMD gets their act together and that they open up their drivers and stack. Their entire stack on Linux is open-source. That part isn't a problem. reply sashank_1509 7 hours agorootparentNope, they basically open sourced their api which is kind of a worthless proposition. You would think this is malicious, trying to pretend as open source, but the whole thing is done so ineptly, it is just incompetence. Likely management has no idea what open source means and thinks releasing api code means open source and is good enough for customers to use your GPU. More concretely, what they open sourced calls closed source functions that run on the GPU which is where all the bugs are, and causes all the crashes. reply AnthonyMouse 7 hours agorootparent> More concretely, what they open sourced calls closed source functions that run on the GPU which is where all the bugs are, and causes all the crashes. In particular, what they did was open source the drivers but not the firmware, and then when the firmware has bugs nobody outside the company can fix it because it's not open source. reply llm_trw 6 hours agorootparentWorth noting that is a step up from NVidia who moved their driver to the GPU and open sourced a configuration file. reply atq2119 5 hours agorootparentprevYes, that's a problem, but it's also a bit of an unfair dunk. After all, your CPU almost certainly runs firmware with all the same issues. Not to mention all other GPUs from all other vendors. Despite all of the noise-making, I don't think anybody has really convincingly made the case that the firmware quality is the main issue here, as opposed to the quality of the rest of the stack. reply AnthonyMouse 5 hours agorootparent> After all, your CPU almost certainly runs firmware with all the same issues. CPUs don't really have firmware in the same sense. CPUs expose basic primitives (machine instructions). Modern CPUs will translate them to micro-ops in microcode instead of executing them directly, but the mapping is relatively simple and bugs are correspondingly uncommon. The GPU firmware is doing something complicated, and therefore buggy, but you're required to use their complicated buggy closed source code because it also doesn't directly expose simpler primitives you could use to build your own alternative to it. > Despite all of the noise-making, I don't think anybody has really convincingly made the case that the firmware quality is the main issue here, as opposed to the quality of the rest of the stack. The rest of the stack should be an abstraction layer that makes programmers not have to care what kind of hardware is under them, the same as they generally don't have to care if their CPU is from AMD or Samsung. But then you need the code to translate from that abstraction to the hardware, and that code needs to work. Which can either be accomplished by the vendor providing working code, or by the vendor providing sufficient documentation and source code for someone else to write working code. Providing neither of these is not effective. What AMD should be doing at this point is both. Open source the existing firmware so that people who are trying to use their hardware right now have the ability to fix any problems they encounter themselves, while the company uses the money they now have to address their existing technical debt, which nobody reasonably expects to happen overnight. But will happen faster when it's not only the company but also the users working on solving the problem. reply imtringued 2 hours agorootparentprevYou do realize that the driver is responsible for compiling your kernel into machine code for your particular GPU? Calling that an API is strange. reply adolph 8 hours agorootparentprev> hoping Google spins off its TPU division There’s a lot of stuff Google could have spun off but they seem content to ignore things until they turn out the lights. reply ein0p 8 hours agorootparentprevI can’t help but wonder why Lisa Su of all people can’t be bothered to turn on her massive money printer. Could it be because she’s related to Jensen Huang? I struggle to find another explanation for not making it dead easy for us to buy shovels that AMD already makes during an once in a century gold rush. reply wmf 8 hours agorootparentMaking GPUs work properly requires years of effort so she needs a time machine to go back and start earlier. reply ein0p 7 hours agorootparentThey’ve already put in “years of effort”, so the delta can’t be _that_ large. Just give Hotz a billion dollars and have him fix it to his liking in both consumer and data center SKUs. Chump change given the market opportunity. Shit will be usable in 6 months and excellent within a year reply threeseed 6 hours agorootparent> have him fix it to his liking Like when he strolled into Twitter, acted like he knew better than everyone, delivered nothing and then left. There is far more to achieving things in large companies than simply having great technical skills. reply ein0p 5 hours agorootparentIDK, dude’s provably pretty capable as long as you don’t attempt to drown him in a bureaucratic swamp. Betcha he’d fix it pretty good. reply jiggawatts 4 hours agorootparent> don’t attempt to drown him in a bureaucratic swamp You weren't listening to the GP comment. He's expected to succeed while being drowned in the swamp. That's what most people mean and expect when they bring in an outsider. \"Fix our problems, but without changing anything to do with management.\" reply fragmede 4 hours agorootparentThere's a difference between fixing technical problems and cultural problems. Hotz has proven to be a wizard at technical ones, while his skill with fixing cultural issues remains to be seen. reply lofaszvanitt 34 minutes agorootparentprevYeah, but you don't know the reasons. reply imtringued 2 hours agorootparentprevHe already has both money and a financial incentive to make it work. What he needs is access to the code. reply MuffinFlavored 8 hours agoparentprev> The problem is if I spend an evening trying to do anything with OpenCL or ROCm the kernel hard-locks and I go to bed early. In a race against NVIDIA where every comment on here is about just how much better CUDA is than any alternative, why doesn't literally what you just said \"also\" have the attention of Lisa Su? reply georgehotz 8 hours agorootparentI have had multiple replies from Lisa Su on both Twitter and by e-mail. It doesn't help. AMD is structurally incapable of fixing these issues. They don't even have a 7900XTX in CI. Crashing the firmware is so trivial there's no way anyone fuzzed anything. They debug by the application, adding mitigations at many layers of the stack to make it work. While this strategy is fine for the 20 mainstream games that come out per year, unless you root cause issues and have a good CI, you'll never build a stable GPU for general compute. reply MuffinFlavored 8 hours agorootparentGenuine question for you then, why are you investing your time reversing their firmware if it sounds like you think they're failing/will fail? Why not just \"adopt\" the golden child/golden standard NVIDIA now for your usecase, reverse that (if needed) and move on? Genuinely curious what you gain from trying to make AMD work/investing in AMD on your end here. Is it that you see a value proposition through the \"muck\" of crashy-kernels that I'm missing? reply georgehotz 8 hours agorootparentThis is my weekend hobby, I'm learning about GPUs. Besides, I'd like to see competition to NVIDIA. From a business side, tiny corp is selling boxes with both AMD and NVIDIA, we'll let the user choose how much they value money vs pain. reply rudedogg 7 hours agorootparentWhat are your thoughts on using Intel GPUs for deep learning? reply doctorpangloss 7 hours agorootparentprevDid you ever consider using NVIDIA? I thought the machine you were planning to sell originally had RTX 6000s. Also, isn’t the real problem with the 7900 XTX is the lack of interconnect? I really appreciate the long list of stuff you are agitating AMD about. Infinity Fabric and blower fans would be great too. Another POV is none of that is ever going to happen, and everyone will have to wait for a Chinese GPU manufacturer to create real competition. I can’t recall a single time in my life a non-Chinese company - electronics or otherwise - delivered the “same” features for a lower price. reply chem83 5 hours agorootparentprevThoughts on IREE[0]'s ability to close this gap? [0] iree.dev reply lofaszvanitt 32 minutes agorootparentprevMaybe they are not allowed to compete... reply jimbob45 7 hours agorootparentprevThey don't even have a 7900XTX in CI. You read stories in the glory days of Microsoft where they would bend over backwards to ensure backwards compatibility [0]. Going from that to what you say is depressing. It’s actually hard for me to accept your statement because such a failure would not only require QA/the PM to be totally ignorant of current testing best practices, but also require HR to have catastrophically misidentified both those people and the people who supervise them. Keep up the great work. [0]https://devblogs.microsoft.com/oldnewthing/20191119-00/?p=10... reply llm_trw 6 hours agorootparentThe shocking thing about AMD in the last 5 years isn't that they got Ryzen to work, it's that they didn't screw it up within two releases. There are no other technology companies in the world who have a will to fail so ingrained in their corporate DNA as does AMD. reply andrewmcwatters 6 hours agoparentprevIt was particularly hard to listen to himself admit that he basically became unpaid QA for AMD. Like, dude, you’re making the same junior mistake people writing apps for the App Store make. It’s not your platform. reply llm_trw 6 hours agorootparentIf the AMD code makes it into the kernel, like their graphics drivers did, then we've won because it will just work on every kernel update. Compared to now where AMD drivers just don't work and NVidia drivers only work with the right blobs installed I'd be willing to take a paycut for 6 months to have 10 years of painless development. That said I jumped ship to NVidia since I need to work and not spend all my time trying to get servers to the point that they can work. reply modeless 4 hours agorootparentprevHis thesis is that owning a good software platform is a prerequisite for a successful AI hardware company. He is (or was) using AMD as a stepping stone to create and flesh out that software platform (tinygrad) before starting custom ASIC development, instead of going the other way around with a costly ASIC first and trying to develop a software platform after, or trying to push support for an ASIC into someone else's platform such as PyTorch. Not sure I agree but it's an interesting idea. It's also worth noting that tinygrad is bypassing as many layers of AMD's software platform as they can get away with (MIOpen, ROCm, and even the userspace GPU driver). reply threeseed 6 hours agorootparentprevAs a developer you're always building on top of something you don't control. At least with the App Store you make pretty good money. It's worse when you have companies who have APIs you need that you know they don't care about. reply NicoJuicy 7 hours agoparentprevThat's not really on how I see it. He's trying to do everything on AMD's consumer based GPU's, while he's comparing it with NVidia's enterprise GPU's with the same expectations. That's just not how it works in real life. But if he succeeds, yeah, he could save tons of money and his hardware for ML could actually be completed. reply modeless 6 hours agorootparentThis is wrong for two reasons. One, he claims that AMD's enterprise GPUs are also unreliable. Two, he is comparing vs. 4090 which is a consumer GPU and CUDA is very reliable as it is on all of Nvidia's consumer hardware. reply dannyw 1 hour agorootparentprevNVIDIA’s consumer GPUs are highly reliable and support the same CUDA capabilities as the enterprise products. This is something NVIDIA gets right. Meanwhile, ROCm isn’t even supported on certain lower end AMD consumer chips. reply renewiltord 10 hours agoparentprevI thought he sort of gave up and is going to build his stuff on Nvidia in the end? It's wild. I'd want AMD to present some sort of alternative if only because you can't get any high-mem chips these days but they seem to have some massive organizational uselessness on this front. reply mindcrime 10 hours agorootparentThey changed their mind a few days later and announced they're going to do NVIDIA and AMD versions of their box. reply causality0 8 hours agoparentprevAMD GPUs are fantastic values for money when it comes to running mainstream PC games. Once you step outside that narrow box they become infuriating. My 7900XTX is probably the last AMD GPU I ever buy. It's just horrible at all the other things I want to use it for, especially and gallingly, game console emulation. For the first few months I had it I actually kept my old GTX1070 plugged into my motherboard because the eight year old $400 GPU was so much better at half my use cases than the $900 AMD card. reply snvzz 10 hours agoprevThis is quite crude on what it is. e.g. how does it differ from gpuopen[0] ISA documentation released by AMD themselves? 0. https://gpuopen.com/amd-isa-documentation/ reply twothreeone 9 hours agoparentThe RDNA docs are just the tip of the iceberg.. in order to get to the compute engines (what you actually care about) you have to go through 3 layers of user-space cruft, a kernel driver, and then a firmware layer running half a dozen separate components of the GPU that \"manage\" the RDNA compute engines. Apparently, these components run on at least 3 different ISAs (some ARM, some F32, and some on RS64) where 2 of them aren't really documented at all. All of this is what he's trying to bypass and talk to the RDNA cores as directly as possible. Modern consumer GPUs are complex beasts. reply lofaszvanitt 45 minutes agoprevSo, why AMD acts like they don't care/can't care about this? Is this such a specialized area that the programmers are hard to find or fished away by Nvidia or something else is going on? If the latter, what? reply glimshe 6 hours agoprevI used to be an AMD diehard fan. Bought their processors and GPUs for close to 20 years until I simply couldn't take it anymore - 5 years ago I started buying NVIDIA GPUs with Intel CPUs and honestly never looked back. Sure I pay more, but it's worth paying more for reliability and software support. Anything else and I'm losing money out of ideology to give to a business that doesn't have its act together. reply earthling8118 5 hours agoparentIt's always intriguing to see other people's takes. I'm in nearly the complete opposite boat: in recent years I've switched to AMD and it feels like all of my hardware problems have gone away. reply Zambyte 4 hours agorootparentNew hardware is better than old hardware reply shard972 6 hours agoparentprevAs someone who basicially grew up with Intel/Nvidia but now running AMD/AMD, while i could agree with you on the GPU side that AMD still has more work to do, I would hard disagree on the CPU side. Especially with the X3D line, AMD cpus can absolute smoke intel and at worst, come within a single percentage of performance in usually single thread bound scenarios. reply tekno45 11 hours agoprevCan anyone explain what this level of documentation allows? Could someone make a CUDA compatible tool with this? reply wmf 10 hours agoparentYeah, HIP and ZLUDA already exist. Crucially, what the documentation doesn't allow is fixing bugs in the firmware/microcode (all the .bin files). reply corey_moncure 10 hours agoprevWe're talking about the company that couldn't fix a mouse cursor corruption bug affecting all of their GPU lines for what, 15, 20 years? reply jpeggtulsa 11 hours agoprevIndustry defining architecture?!?! reply shmerl 5 hours agoprevOh, this is a very neat tip: umr --list-blocks Getting versions of all these IP blocks for a specific card can be confusing. UMR repo for the reference: https://gitlab.freedesktop.org/tomstdenis/umr reply mkj 1 hour agoparentOh, Tom St Denis (of libtomcrypt) is working for AMD? That's promising. reply secondcoming 11 hours agoprevI watched a bit of his attempts to reverse engineer the AMD GPU stack on YT, but Jesus is his keyboard loud. reply pyinstallwoes 10 hours agoparentdid you learn anything? reply juitpykyk 9 hours agorootparentI learned that AMD GPUs have layers upon layers of drivers, in user space, kernel space, and drivers running on the device itself. reply jamesy0ung 10 hours agoprev [–] RDNA 3 has a platform security processor? Sounds like a stupid idea. reply jsheard 10 hours agoparent [–] Maybe that's where they run the HDCP/DRM stuff? That's the most obvious application of a secure enclave on a GPU that I can think of, and would also explain why they won't (or can't) open it up. reply juitpykyk 9 hours agorootparent [–] Isn't HDCP/DRM implemented in ASIC hardware? There is nothing programmable there. reply monocasa 9 hours agorootparent [–] It's probably implemented mainly on the Platform Security Processor. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document delves into the AMD 7900XTX (Navi31) GPU architecture, highlighting essential components and offering guidance on driver installation.",
      "It provides links to technical resources aimed at debugging and developing on AMD GPUs specifically on Linux systems.",
      "Detailed information regarding firmware files, sizes, loading procedures for different firmware variants, and an overview of firmware types with corresponding numerical codes is also included."
    ],
    "commentSummary": [
      "The discussion centers on George Hotz's criticism of AMD's drivers for the 7900XTX and the rivalry between AMD and NVIDIA.",
      "Emphasis is placed on the significance of following open standards in software development, with debates on CUDA and the hurdles of GPU firmware development.",
      "The importance of a robust software platform for AI hardware companies is highlighted, touching on AMD's organizational challenges, software assistance, and future GPU advancements like the Platform Security Processor in RDNA 3."
    ],
    "points": 172,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1711918713
  },
  {
    "id": 39887931,
    "title": "Upscayl 2.9.9: Free AI Image Upscaler with Cross-Platform Support",
    "originLink": "https://github.com/upscayl/upscayl",
    "originBody": "v2.9.9 is out! 🥳 🆙 Upscayl Free and Open Source AI Image Upscaler Upscayl lets you enlarge and enhance low-resolution images using advanced AI algorithms. Enlarge images without losing quality. It's almost like magic! 🎩🪄 Upscayl is a cross-platform application built with the Linux-first philosophy. This means that Linux users receive pre-release builds earlier, but Upscayl itself is available on all major desktop operating systems :) https://upscayl.org Important You'll need a Vulkan compatible GPU to upscale images. Many CPU and iGPUs do not work but no harm in trying. 👨💻 Installation 🐧 Linux Upscayl should be available on the software listings of most Linux operating systems. Your distro's Store app might also support the Flatpak or Snap version. 💼 Portable Method Go to releases section Download the upscayl-x.x.x-linux.AppImage file. Right Click AppImage -> Go to Permissions tab -> Check 'allow file to execute' and then double click the file to run Upscayl. You can also choose to install using other formats like RPM (Fedora), DEB (Debian/Ubuntu based), and ZIP (Any x86 Linux OS). 🍎 macOS (MacOS 12 and later) Go to releases section Download the upscayl-x.x.x-mac.dmg file. Double click dmg, drag Upscayl icon into Applications folder. Open Finder, click 'Applications' tab in the left sidebar. Find Upscayl and right click on it. Select 'Open'. In the window that appears, press 'Open' yet again. 🍺 Homebrew brew install --cask upscayl 🐌 Windows (Windows 10 and later) Go to releases section Download the .exe file. Double click exe file, wait for installation, profit. 👨🏫 Wiki - Tutorials and Guides Check out our Wiki here. Try out even more new models! Convert your own models Compatibility List Troubleshooting ⚖ Results Check out Upscayl before/after comparisons here. 🤫 Roadmap You can track all the progress here: https://github.com/orgs/upscayl/projects/1 Fix bugs Make the whole world use FOSS (WIP 🚧) 🛠 Development I recommend using Volta: https://volta.sh for installing Node.js. Download and install volta, then do: volta install node. 🏃 Running Note If you are not willing to install git, you can skip the first line, download the source zip and extract it to upscayl instead and carry on with the rest of the instructions. git clone https://github.com/upscayl/upscayl cd upscayl # INSTALL DEPENDENCIES npm install # RUN THE DEVELOPMENT SERVER LOCALLY ## YOUR LOGS WILL NOW APPEAR IN THE TERMINAL npm run start 🏗 Building # INSTALL DEPENDENCIES npm install # PACKAGE THE APP npm run dist # PUBLISH THE APP, MAKE SURE TO ADD GH_TOKEN= IN SHELL # ONLY DO THIS IF YOU'RE A MAINTAINER npm run publish-app 🤓 FAQ How does Upscayl work? Upscayl uses AI models to enhance your images by guessing what the details could be. It uses Real-ESRGAN and Vulkan architecture to achieve this. Our backend is fully open-source under the AGPLv3 license. I don't see a drastic change in my upscaled image. Why is that? Upscayl can enhance low resolution images and images that are pixelated but it cannot de-blur or do focus adjustment on your image. If your image is out-of-focus or totally blurred, Upscayl is not the right tool for it. Please use images that are similar to the examples we've given here. Is there a CLI available? The CLI tool is called upscayl-ncnn. Do I need a GPU for this to work? Yes, unfortunately. NCNN Vulkan requires a Vulkan-compatible GPU. Upscayl won't work with most iGPUs or CPUs. But hey, no harm in trying ;) @Wyrdgirn has contributed a workaround for Windows and Linux in #390! Nobody knows how to manipulate the macOS and Haiku frameworks... How can I contribute? You can report issues, fix code and add features by submitting PRs, or donate! 😊 What's the GPU ID for? It is for selecting which GPU to use. The specific procedure is detailed in the Wiki. Note that for Windows systems, if Upscayl is not set to performance mode, the system may override this setting. Where do I find more models? More models can be taken from here: https://github.com/upscayl/custom-models Upscayl uses Real-ESRGAN-ncnn-vulkan binaries to upscale images. 🎁 Donate ❤ Credits Real-ESRGAN for their wonderful research work. Real-ESRGAN: Copyright (c) 2021, Xintao Wang @JanDeDinoMan, @xanderfrangos, @Fdawgs, @keturn for their code contributions @aaronliu0130 for providing community support :) Foolhardy for their Remacri model. Kim2091 for their Ultrasharp and Ultramix Balanced model. @NicKoehler for their amazing logo :) Copyright © 2023 - Upscayl By Nayam Amarshe and TGS963 Made with 🖱 & ⌨",
    "commentLink": "https://news.ycombinator.com/item?id=39887931",
    "commentBody": "Upscayl – Free and Open Source AI Image Upscaler (github.com/upscayl)166 points by faebi 13 hours agohidepastfavorite37 comments wruza 8 hours agoIf like me you're using Real-ESRGAN-ncnn-vulkan [1] and are curious what upscayl-ncnn CLI [2] changed from it, there's not much and nothing substantial [3]. Not a criticism, just wanted to learn whether it's worth upgrading to for a CLI tool ($subj is a separate GUI app based on it). [1] https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan [2] https://github.com/upscayl/upscayl-ncnn [3] https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan/compare/m... reply NayamAmarshe 6 hours agoparentUpscayl NCNN has several fixes (like latest vulkan, bug fixes, feature additions) and is coming up with more soon (we just haven't pushed the changes). Also working on CPU support. If you're going to use Real-ESRGAN, you're going to have to deal with a lot of issues and a codebase that hasn't been updated in a long time. Afaik, Upscayl NCNN is the only project maintaining Real-ESRGAN NCNN as of now. reply wruza 5 hours agorootparentI've only used -i -o -n mode, but for me that just works. And when something just works, I tend to see \"a codebase that hasn't been updated in a long time\" as a good thing generally :) (Don't take it personally, it's my burnout from modern software cycles.) reply NayamAmarshe 3 hours agorootparentI understand. With Upscayl it's a little more configurable. For example, you can add compression to the images, change the format (which Real-ESRGAN can't always do), drag and drop images, have extra models not shipped by default and in future even have face enhancement. reply __rito__ 4 hours agoparentprevI came to the comments just to ask this. I have used RealESRGAN for some years now, and several pictures hang on my house that were enlarged using this. I enlarged a lot of old pictures from family albums, and then fixed the bad spots with something like Photoroom, Snapseed, etc., and then sent them to be printed. Works really great. And not only that, if I find an old picture on the internet that is low res with no high res found after much google-fu, Duckduckgo, or Kagi, I use RealESRGAN to upscale, and then use it as my wallpaper, presentation background or whatever! reply NayamAmarshe 3 hours agorootparentYes! I Real-ESRGAN is still the best algorithm for enhancing images. The only algorithm better than it is Topaz's but that's to be expected from a paid product. Real-ESRGAN can get better with a better trained model too, which is on Upscayl's roadmap. reply pluto_modadic 7 hours agoparentprevI mean... if people prefer a GUI that's not a bad thing... reply _fw 52 minutes agoprevUpScayl is great, I use it a lot for work. Upscaling low-res graphics and illustrations for use in graphics in a pinch, upscaling portrait photos of people for print and photoshop… upscaling old copies of things for editing purposes, you name it. It’s not perfect but no alternative is. Bloody useful though. reply zebomon 7 hours agoprevI tried this out back in December. It is very straightforward. Would recommend for anyone who is testing the waters and just trying to start exploring the various tools. From my understanding though, the quality is pretty far behind that of the cutting edge. A friend recently recommended Topaz to me, but that isn't open source. reply NayamAmarshe 6 hours agoparentThe quality is actually comparable in some places. Upscayl being a free project though, does not have its own model yet and as such, we depend on community models (which are great in their own right). I do have plans to create our own robust model though, once I collect enough money :) https://ckovalev.com/midjourney-ai/guide/upscaling-ai-art-fo... reply __rito__ 4 hours agorootparentWhat models do you use to upscale images other than RealESRGAN? reply NayamAmarshe 3 hours agorootparentWe have a custom-models repository with several models. Upscayl by default ships with 6 models. reply kristopolous 3 hours agoparentprevextras tab in a1111 is really the best results I can get. You need to know what all those options mean of course - it's a more complicated interface for sure reply mrbluecoat 7 hours agoprevI used to laugh so hard at those tv and movie scenes when they would \"enhance\" an image: https://youtu.be/LhF_56SxrGk I guess yesterday's science fiction is now our reality. reply NayamAmarshe 52 minutes agoparent> I guess yesterday's science fiction is now our reality. That's actually what's written on https://upscayl.org, \"From Science Fiction to Reality\" haha reply palad1n 6 hours agoparentprevI think I've got a twitch now, when I think about this. How those stupid moviemakers would say, \"enhance!\" and I (along with many of my geek brethren) were like, \"there are no more pixels, that can't be done!\" And now it exists. \"Enhance\" exists. reply kyriakos 5 hours agorootparentOr does it? If you enhance security footage to make a barely visible face recognizable it will make it but it won't be the person in the footage anymore. reply kookamamie 2 hours agorootparentIt does not exist. The AI models generate \"what could be\" instead of \"what was\", i.e. they hallucinate heavily in upscale tasks. reply lostlogin 49 minutes agorootparent> It does not exist. The MRI scanner near you is potentially using AI to increase image signal, then to increase pixel count 4x (well, 2x in X direction, 2x in Y direction. Potentially also generating a slice between each slice in 3D datasets). If you turn off the AI and actually acquire the information, the scan is long, the patient moves and it is blurry. However if the patient is still, the non AI images look like the AI images. Disagreeing with it is sort of moot now - it’s in the machines and is running. It works well, I’ve been using it daily for a few years. All vendors have it - I use the Siemens product ‘Deep Resolve’ [1]. Their PR department undersells it, in my view this is a bigger change than the 1.5T to 3T transition. [1] https://www.siemens-healthineers.com/magnetic-resonance-imag... reply wood_spirit 5 hours agorootparentprevIt would be really interesting to see what it upscales poor security footage to be when we know the people in the picture; how accurate was it? reply wruza 5 hours agorootparentThe models usually bundled with ESRGAN don't enhance like CSI. They straighten lines and jpeg blocking a little and correct \"subatomic\" details, but cannot restore large areas like faces, objects, etc. It's more of an AI-flavored \"Effects - Sharpen\" than stereotypical \"Enhance!\". reply 112233 1 hour agorootparentprevYou probably remember this: https://petapixel.com/2020/08/17/gigapixel-ai-accidentally-a... reply userbinator 5 hours agoparentprevIt just hallucinates the details now. reply pixelesque 2 hours agorootparentYes - I can't find it now, but a few weeks ago I saw a demo website for some other upscaler with an example image of a picture of food, and it was pretty clear that the upscaled \"version\" was different bread than the original lower-res version, and things like herb \"sprinkles\" (oregano?) in the original became pine nuts or something in the upscaled version.... reply NayamAmarshe 1 hour agorootparentYes, I personally think it's false advertising. AI Re-painter is not AI Upscaling. I even received a review on Mac App Store saying Upscayl is worse because it does upscaling instead of repainting like Magnific. reply forgingahead 4 hours agoparentprevThe correct phrase should have been \"Hallucinate!\" reply holoduke 4 hours agoparentprevExcept that when you try to enhance the numberplate of a suspected killer you get a platenumber which is based on avarage noise and mqybe different than reality. reply graevy 8 hours agoprevupscayl is very approachable, but lacked many features i needed. i ended up using https://github.com/AUTOMATIC1111/stable-diffusion-webui after upscaling became part of my regular workflow, but for someone who just needs a few images enhanced, it's an ideal tool. reply sgilani 4 hours agoprevIs there something similar for video? Or is topaz still the only option? reply NayamAmarshe 2 hours agoparentI still have video upscaling planned for Upscayl but we're just 2 people, working on Upscayl in our free time. reply feverzsj 4 hours agoprevThe upscaled image feels sharper but loses some detail. reply jiveturkey 3 hours agoprevThis should have been called Enhanse (sticking with the misspelling theme) reply esafak 7 hours agoprev [–] What's the state of the art? reply liuliu 6 hours agoparent [–] SUPIR and other diffusion based models. reply NayamAmarshe 5 hours agorootparent [–] But doesn't that defeat the purpose of upscaling? I see a lot of people promoting Magnific AI (which costs $40 a month) but it's not really an upscaler. It's more of an AI repainter. It replaces most details by re-imagining them instead of guessing the original details. I think we should be clear about what an AI Upscaler is and what an AI re-painter is. reply liuliu 5 hours agorootparent [–] ESRGAN is the same thing. You cannot recover signal that are already lost without additional information (in Diffusion case, model's prior). Upscaler are hallucinators by that definition. reply wruza 5 hours agorootparent [–] But results differ in perception. ESRGAN doesn't hallucinate whole [sub]objects and that puts it into a different category. Latent/denoising i2i re-scale is different because its parameter range doesn't really intersect with what ESRGAN does. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Upscayl version 2.9.9 is out, offering a free AI image upscaling tool for enhancing low-resolution images using advanced algorithms.",
      "A Vulkan-compatible GPU is necessary for upscaling images, and pre-release builds are accessible for Linux users.",
      "The application is cross-platform, with installation guidance available for Linux, macOS, and Windows, and it provides project details like the roadmap, development instructions, FAQs, and credits."
    ],
    "commentSummary": [
      "Users are comparing the free and open-source Upscayl AI image upscaler with Real-ESRGAN-ncnn-vulkan, discussing their differences and features.",
      "Discussions include the image enhancement quality, future improvement plans, and the necessity for a better-trained model, with mentions of alternative upscalers like Topaz and Magnific AI.",
      "Limitations and capabilities of AI upscaling, specifically in image detail recovery, are addressed, along with future video upscaling plans with Upscayl."
    ],
    "points": 166,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1711917924
  },
  {
    "id": 39886178,
    "title": "Exploring the Potential of AI Agents for Specialized Decision Making",
    "originLink": "https://news.ycombinator.com/item?id=39886178",
    "originBody": "I saw a lot of initial buzz about the promise of agent based workflows and it seemed to be the obvious way to get LLMs to the edge of decision making and leverage many specialized models. It seems the chatter has died down but there are growing projects out there in the space. Before I invest the time to explore and work with tools I’d love some feedback from the community and if and how they are being used.",
    "commentLink": "https://news.ycombinator.com/item?id=39886178",
    "commentBody": "Is anybody getting value from AI Agents? How so?160 points by reilly3000 16 hours agohidepastfavorite139 comments I saw a lot of initial buzz about the promise of agent based workflows and it seemed to be the obvious way to get LLMs to the edge of decision making and leverage many specialized models. It seems the chatter has died down but there are growing projects out there in the space. Before I invest the time to explore and work with tools I’d love some feedback from the community and if and how they are being used. JCM9 16 hours agoI’ve seen a lot of attempts but nothing that worked really well. Using an agent as a glorified search engine can work, but trying to replace actual humans to handle anything but the most standard use cases is still incredibly hard. There’s a lot of overhyped rhetoric at the moment around this tech, and looks like we’re heading into another period of post-hype disillusionment. Legal angles here also also super interesting. There’s a growing body of scenarios where companies are held accountable for the goofs of their AI “assistants.” Thus we’re likely heading for some comical train wrecks as companies that don’t properly vet this stuff set themselves up for some expensive disasters (eg think the AI assistant doing things that will get the company into trouble). I’m bullish on the tech, but bearish on the ability of folks to deploy it at scale without making a big expensive mess. reply spxneo 15 hours agoparentExtrapolating on your legal comment, copyright is the current issue. Right now the USCO says if you use a large model that used copyrighted material to train it, you cannot copyright the generated art. This applies to not just art but might be for code as well. So I wonder what the legal liabilities are say for a publicly traded company to use Midjourney to generate content that is also copyrighted. ex) it is possible to generate movie characters that we instantly recognize if you use non-english words which pretty much nail in the coffin for safe harbour status granted under DMCA reply dragonwriter 15 hours agorootparent> Right now the USCO says if you use a large model that used copyrighted material to train it, you cannot copyright the generated art. Unless the Copyright Office has come out with a newer rule, it says if you use a model that does the usual creative parts of making the output you can’t copyright the output, because it is not a work of human authorship. (And that if a work is mixed AI/human, the AI use must be disclosed in copyright registration to avoid copyright protection being applied to elements that are not human work.) “Large model” and “copyrighted material used to train it” are not the issues. reply jpollock 13 hours agorootparentprevThe GP might not be exclusively talking about copyright. They might also be talking about how Air Canada lost a lawsuit about whether or not they needed to honor their chatbot's hallucinations. https://www.theguardian.com/world/2024/feb/16/air-canada-cha... reply echelon 15 hours agoparentprevAI agents suck. It's too early. The first dominoes to fall will be art, music, and film. These don't require perfection and can be iterated on by the creator. The tools become a method in the process. Everyone in the agent space is bumping around until the next big innovation that actually unlocks the technique sparks a race to productize. Maybe some will get lucky and have a fast pivot. Or maybe they'll run out of funds before the big breakthrough. reply ganzuul 14 hours agorootparentTalented artists see their creative output drown under a torrent of commercial and mass-produced art. There are dominoes to fell but we should not be reckless about it. With postmodernism art deconstructed its duty and without duty society does not grant rights. It is a big problem because art is often visionary about the future. We don’t know that we even have a future without vision. reply echelon 13 hours agorootparent> Talented artists see their creative output drown under a torrent of commercial and mass-produced art. There isn't enough art in the world. I've spent the last two decades making films the photons-on-expensive-glass method, and it's a pain in the ass. I can start new software projects of scale easily. I can't do that with art. It's capital intensive, logistics intensive, and requires too many people in low-autonomy roles. It really sucks the fun out of storytelling when you're chasing down location rights, showing up at the prop house at 7 AM, arranging for catering, etc. > We don’t know that we even have a future without vision. As a creator, I've dreamed about a better way before GenAI was even on the radar. I'm not the only one. Existing processes suck and too much of the work isn't creative at all. As a consumer, my needs are barely being met at all. I want a show about steampunk vampires in space. I want a biopic film exploring Reimann, but told as a musical. I have creative notes for Benioff and Weiss, and I'd rather put those together for myself and my friends than echo words into the void. I want so much more than the canned limited selection I have available on Netflix and Criterion. It barely whets the palate, and my appetite is completely unsated. The closest I've ever gotten is performing in improv theater and exploring the worlds I want with the people I feel comfortable creating around. But that's only part of the experience I want. I want so much more. The art we have today is a pale shadow of the mind's unlimited canvas. A projection onto a caveman's wall. You can protect a vision of a priestly class of artists from the printing press all you want. I'm tired of living in the dark ages when we're sitting at the precipice of so much more. Their jobs won't go away any more than wedding and event photographers suffer in the advent of digital film. If anything, the artists will be the best primed to take advantage of the new alpha. Free of studio meddling, they can build their own audiences that they own without the chains of brand guidelines. Vivziepop, but a million fold. I saw a figure recently that said 80% of consumer film, games, and media originates in the US. Think about the rest of the world. So much culture and perspective that we should all share in remains unseen, and we're left with the lens of US media giants. Think, too, of all the dreamers lost in opportunity cost. There are a billion stories that die silently in the minds of dreamers because we couldn't help them. And the world is all the worse for it. reply ganzuul 2 hours agorootparentI very much agree on but one crucial point. In addition to individualist fulfillment I crave collectivist fulfillment and for that to happen we have to assign duties to art. If a collective shoulders that duty then we do get a printing press priesthood. If individualists shoulder the duty to the collective then we should get something different. Perhaps productions like Helluva Boss do have some kind of post-deconstructionist duty of their own design. If that is the case then society should reciprocate with granting rights different from what the priesthood would accrue. reply nonrandomstring 15 hours agorootparentprev> AI agents suck. The idea does not suck. On paper it's great. Just tell an \"agent\" what you want and off it goes to get it. And it's possible. LLMs open the interface to search planning and selection algorithms that have been around for 40 years and are mature. You could have this tomorrow. The assumption is that people want it. Tech business has come a long way exploiting people's vices, specifically laziness of thought we call \"convenience\". But at heart, tech is still seen as a tool, to empower people, to give them agency. Agents subtract agency. > It's too early.... the big breakthrough. Hoping for progress against human psychology seems a fool's errand. reply danielmarkbruce 15 hours agorootparentThis is wrong. You can't have this tomorrow. The LLMs make too many errors right now for most use cases. If you think it's possible right now, you haven't tried to build it. reply spxneo 15 hours agorootparentprevright now the expected future cashflow from whoever \"wins\" is infinite justifying astronomical amount of capital expenditure. ex) Microsoft's $100 billion supercomputer Sometimes I get they are already sitting on some ground breaking stuff and slowly releasing it to test our responses and get feedbacks. I'm certain they will be reading this thread but if one theme repeats itself is the lack of trust in the output of the agents and the companies creating it. reply echelon 13 hours agorootparent> right now the expected future cashflow from whoever \"wins\" is infinite justifying astronomical amount of capital expenditure. ex) Microsoft's $100 billion supercomputer Absolutely. These agent startups don't have PMF and haven't solved anything yet. They're playing in the kiddie pool while Microsoft is placing chess pieces with a GDP-level moat (which is frankly terrifying). reply tuckerconnelly 15 hours agoprevI built an AI-agents tech demo[1], and am now pivoting. A few thoughts: * I was able to make a simple AI agent that could control my Spotify account, and make playlists based on its world knowledge (rather than Spotify recommendation algos), which was really cool. I used it pretty frequently to guide Spotify into my music tastes, and would say I got value out of it. * GPT-4 worked quite well actually, GPT-3.5 worked maybe 80% of the time. Mixtral did not work at all, aside from needing hacks/workarounds to get function-calling working in the first place. * It was very slow and VERY expensive. Needing CoT was a limitation. Could easily rack up $30/day just testing it. My overall takeaway: it's too early: too expensive, too slow, too unreliable. Unless you somehow have a breakthrough with a custom model. From the marketing side, people just don't \"get it.\" I've since niched down, and it's very, very promising from a business perspective. [1] https://konos.ai reply andy99 15 hours agoparentI think it's destined to fail because it basically moved AI back into the \"rules based\" realm. Deep learning is a decent cognitive interface - like making a guess at some structure out of non-structure. That's where the magic happens. But when you take that and start using rules to chain it together, you're basically back to the same idea as parsing semi-structured data with regex and/or if statements. You can get it to work a bit but edge cases keep coming along that kill you, and your rules will never keep up. For simple cognitive tasks, deep learning figures out enough of the edge cases to work pretty well, but that's gone once you start making rules for how to combine predictions. reply thekumar 13 hours agorootparentI totally agree with this. I have been arguing with folks that current Reactflow based agent workflow tools are destined to fail, and more importantly, missing the point. Stop forcing AI into structured work. I do think AI \"agents\" (or blocks as I like to think of them) unlock the potential for solving unstructured but well-scoped tasks. But it is a block of unstructured work that is very unique to a problem, and you are very likely to not find another problem where that block fits. So, trying to productize these AI blocks as re-usable agents is not that great of a value prop. And building a node based workflow tool is even less of a value prop. However, if you can flip it inside out and build an AI agent that takes a question and outputs a node based workflow. But the blocks in the workflow are structured pre-defined blocks with deterministic inputs and outputs, or a custom AI block that you yourself built, then that is something I can find value in. This is almost like the function calling capabilities of GPT. Building these block reminds me of the early days of cloud computing. Back then the patterns for high availability were not well-established and people that were sold on the scalability aspects of cloud computing and got onboard without accounting for service failure/availability scenarios and the ephemeral nature of EC2 instances were left burned, complaining about the unfeasibility of cloud computing. reply spxneo 15 hours agoparentprevwhat makes it slow? is it because they throttle your api key? reply tuckerconnelly 14 hours agorootparentChain of thought takes time to generate all the characters. If you do a chain-of-thought for every action and every misstep (and you need to for quality + reliability), it adds up. reply spxneo 14 hours agorootparentIs there no way to share that \"memory\" across chats? or are we at the mercy of hosted models? reply ShamelessC 9 hours agorootparentThere’s caching but only so much can be cached when small changes in the input can lead to an entirely different space of outputs. Furthermore, even with caching LLM inference can take anywhere from 1-15s using GPT4-Turbo via the API. As was mentioned, the more characters you prefix in the context - the longer this takes. Similarly you have a variable length output from model (up to a fixed context length) and so the time it takes to calculate the “answer” can also take awhile. In particular with CoT you are basically forcing the model to use more characters than it otherwise would (in its answer) by asking it to explain itself in a verbose step by step manner. reply itake 15 hours agorootparentprevOur p99 for gpt4 is 3s. Images take up to 50s. reply spxneo 15 hours agorootparentso how would you go about improving that? reply itake 8 hours agorootparentwe only send 0.5-5% of traffic to gpt4, thanks to smaller faster cheaper models. So not all of our traffic is hit with 50s latencies :-/ reply freediver 10 hours agorootparentprevNot using an LLM for it. reply 1oooqooq 13 hours agoparentprevso, no? reply tuckerconnelly 10 hours agorootparentThere's value, but it's too expensive, too slow, and too unreliable right now to be feasible from a business perspective. reply kennethologist 16 hours agoprevApparently Pieter Levels: \" Interior AI now has >99% profit margins - GPU bill is $200/month for 21,000 designs per month or about 1¢ per render (no character training like Photo AI helps costs) - Hosted on a shared VPS with my other sites @ $500/mo, but % wise Interior AI is ~$50 of that += $250/month in costs It makes about $45,000 in MRR and so $44,730 is pure profits! It is 100% ran by AI robots, no people I lead the robots and do product dev but only when necessary\" https://twitter.com/levelsio/status/1773443837320380759 reply paxys 15 hours agoparentThis guy makes money by selling \"how to get rich using AI\" courses and marketing himself on social media (which he is phenomenal at). I'm not really inclined to believe his sales numbers. reply Mystery-Machine 15 hours agorootparentHe is NOT selling any courses. Can you please point me to any of his courses? He has a book he wrote about making software/projects called Make. This book is several years old and doesn't mention AI. reply conjectures 15 hours agoparentprevGent is shilling his book about passive income or whatever. Sure I believe his numbers. reply balls187 15 hours agorootparent> $45,000 in MRR and so $44,730 I’ve found a lot of these numbers from people selling passive income methods are extrapolated. reply jokethrowaway 15 hours agorootparentprevLevels has been one of the most open entrepreneurs out there. I'd be surprised if he lied on revenue All of that just to sell a book? reply solumunus 15 hours agorootparentEntrepreneur influencers are some of the worst trash. reply kylecazar 16 hours agoparentprevAnd yet \"Unfortunately, we cannot offer refunds as costs incurred for generating AI images are extremely high.\" reply naijaboiler 15 hours agorootparentexactly 99% gross profit, but can't offer refunds due to cost reply dvfjsdhgfv 15 hours agorootparentprevYeah, he claims because at any point in time there are people redesigning their interiors. I'd say that at any point in time there are people you can convince to give you their money, and if you don't offer refunds, it's not far from a scam. reply yawnxyz 5 hours agorootparentmost other services (Stripe, ChatGPT, Google Workspace) also don't seem to offer refunds? And neither do most restaurants either; what's to prevent someone from getting a service and then a full refund? reply ShamelessC 16 hours agoparentprevIs that an agent based setup? Seems like it’s using a few different models wired together manually. reply iAkashPaul 15 hours agorootparentHis robots are just automated scripts which do the bulk repetitive tasks unlike the agents that comment OP thought. reply GaryNumanVevo 11 hours agoparentprevI roll my eyes every time I see a tweet of his. \"Entrepreneur influencers\" are uniquely unbearable. reply airstrike 16 hours agoprevI get a lot of value out of Copilot and GPT4 for coding, but that's about it. It's true that have to wrestle a lot with them to get them to do what I want for more complex tasks... so they are great for certain tasks and terrible for others, but when I'm in Xcode, I dearly miss vscode because of Copilot autocomplete, which I guess is an indication that it adds some value One unexpected synergy has been how good GPT4 is at explaining why my rust code is so bad, thanks to the very verbose compiler messages and availability of high quality training data (i.e. the great rust code in the wild)—despite GPT4 not always being great at writing new rust code from a blank file. Part of me thinks in the future this loop is going to be a bit more automated, with an LLM in the mix... similar to how LSPs are \"obvious\" and ubiquitous these days On an unrelated note, I also wrote a small python script for translating my Xcode project's localizable strings into ~10 different languages with some carefully constructed instructions and error checking (basically some simple JSON validation before OpenAI offered JSON as a response type). I only speak ~2 of the target languages, and only 1 natively, but from a quick review the translations seemed mostly fine. Definitely a solid starting point reply ShamelessC 16 hours agoparentThat’s not an agent based setup. reply neilv 15 hours agoprevMost of the application right now is for purposes for which quality isn't a high priority. (Also, plagiarism laundering.) Don't put it in charge of paying bills. Do put it in charge of making SEO content sites, conducting mass scam automated interactions, generating bulk code where company tolerates incompetence, making stock art for blog posts that don't need to look professional, handling customer service for accounts you don't care about, etc. reply varunshenoy 6 hours agoprevI've been playing with AI agents for months, and most of them are pretty bad. They often get stuck in loops, which is frustrating. This happens in MultiOn, AutoGPT, and others. I've used Devin a few times (see: https://x.com/varunshenoy\\_/status/1767591341289250961?s=20), and while it's far from perfect, it's by far the best I've seen. It doesn't get stuck in loops, and it keeps trying new things until it succeeds. Devin feels like a fairly competent high school intern. Interestingly, Devin seems better suited as an entry-level analyst than a software engineer. We've been using it internally to scrape and structure real estate listings. Their stack for web RPA and browser automation works _really_ well. And it makes sense why this is important: if you want to have a successful agent, you need to provide it with good tools. Again, it's not flawless, but it gives me hope for the future of AI agents. reply erru 47 minutes agoprevWe've built a low-code AI agent platform with primary use case in e-commerce (replacing first-line of humans for basic things like product search, QA, etc). It works fairly well if you assemble the script correctly. And if it fails - it just falls back to humans, so customers don't see much difference in their experience. reply trzy 16 hours agoprevAren’t agents bottlenecked by the underlying models? I’ve read that the number of “chain of thought” steps needed is proportional to task complexity. And if each step has the same probability p of success, probability of success is p^n, where n is the number of steps needed (potentially high). At a 99% success rate per step and 5 steps that’s a 95% overall success rate. 90% drops down to 60%. Not sure what the real numbers are but this seems like it could be a problem without significantly more intelligent ML models? reply Filligree 16 hours agoparentError-checking and recovery is a potential solution here. Not a well understood one, and might still need higher intelligence than we've got, but- If your math worked out, then humans couldn't work either. reply pphysch 15 hours agorootparentIf you have a \"super-agent\" AI that is capable of recovering a business process from an error state, why not just use that agent in the first place? reply lukasb 15 hours agorootparentYou don't need a super agent, you just need two LLM-based systems with errors that aren't too correlated. reply exe34 15 hours agorootparentprevChat gpt has often given me the right answer for code after seeing the error trace resulting from its previous attempt. I also often correct my own mistakes based on clashes with reality - I don't just become more intelligent the second time. reply devanandb 12 hours agorootparentI would argue that you are! You will not try to clash with reality the same way you did before, provided you “remember” and I believe future agents/models will have this kind of contextual memory continuously being getting baked in to improve..just a thought. reply exe34 11 hours agorootparentI think you could do this with an open model with overnight tuning on the day's errors. Probably very expensive though. Easier to scoop up all the errors on the internet on the first round of pre-training. reply devanandb 10 hours agorootparentCouldn’t agree more! That’s why also maybe they are raising 100 more billions!..:p reply babelfish 8 hours agoparentprevAre there any papers on this? I’d be interested in reading more. reply spxneo 15 hours agoprev50 comments so far with 4 about non-agent codegen and rest confirming OPs observations. I'm seeing also an explosion in the number of comments advertising their AI tool on anything remotely related to AI topics. Makes me think we are headed for a major correction. reply abathur 15 hours agoparentSince your definition seems to be a sharp enough razor to triage 50 comments and I'm not read-in enough to tell from the OP, can you help elucidate? :) Are we talking about work on so-called intelligent agents (something with a primitive OODA loop?), a specific ~pattern of conversational AI chatbot, or something else? reply ShamelessC 9 hours agorootparentAutoGPT and the like are essentially what OP was referring to. Basically “Siri, but it works and can actually do most of what you ask because the underlying language model is robust rather than a rules engine” (and friends). reply spxneo 14 hours agorootparentprevAI startups are funded by VCs funded by LPs long on Nvidia while simultaneously short on the foudners. reply nottorp 15 hours agoprevWhat's an agent based workflow? :) I use LLMs as a glorified search engine. That was better than web search at some point, I'm not sure the publicly available LLMs are that good any more. Gemini seems to be extremely worried to not offend anyone instead of giving me results lately. At least it's still useful for 'give me the template code for starting an XXX' ... reply morkalork 15 hours agoparentIs it something like this? >You are a , the user has requested . Here are a list of possible actions and their descriptions. Choose the most appropriate action for the user's request. Parse the following from the request... In a loop, and you execute whichever action is selected after? reply danenania 15 hours agoprevI'm working on an agent-based tool for software development. I'm getting quite a lot of value out of it. The intention is to minimize copy-pasting and work on complex, multi-file features that are too large for ChatGPT, Copilot, and other AI development tools I've tried. https://github.com/plandex-ai/plandex It's working quite well though I am still ironing out some kinks (PRs welcome btw). I think the key to agents that really work is understanding the limitations of the models and working around them rather than trying to do everything with the LLM. In the context of software development, imo we are currently at the stage of developer-AI symbiosis and probably will be for some time. We aren't yet at the stage where it makes sense to try to get an agent to code and debug complex tasks end-to-end. Trying to do this is a recipe for burning lots of tokens and spending more time and than it would take to build the thing yourself. But if you follow the 80/20 rule and get the AI to do the bulk of the work, intervening frequently to keep it on track and then polishing up the final product manually at the end, huge productivity gains are definitely in reach. reply hubraumhugo 15 hours agoprevThe term \"AI agents\" might be a bit overhyped. We're using AI agents for the orchestration of our fully automated web scrapers. But instead of trying to have one large general purpose agent that is hard to control and test, we use many smaller agents that basically just pick the right strategy for a specific sub-task in our workflows. In our case, an agent is a medium-sized LLM prompt that has a) context and b) a set of functions available to call. For example we use it for: - Navigation: Detect navigation elements and handle actions like pagination or infinite scroll automatically. - Network Analysis: Identify desired data within network calls. - Data transformation: Clean and map the data into the desired format. Finetuned small and performant LLMs are great at this task with a high reliability. The main challenge: We quickly realized that doing this for a few data sources with low complexity is one thing, doing it for thousands of websites in a reliable, scalable, and cost-efficient way is a whole different beast. The integration of tightly constrained agents with traditional engineering methods effectively solved this issue for us. reply erichi 15 hours agoparentWhy using llm to chose a proxy if you can just rotate starting from the cheapest based on not getting 403? reply habitue 13 hours agoprevTo summarize, agents are (essentially) LLMs in a loop: take actions, think, plan, etc, then repeat. Currently, from what I've seen, current LLMs \"diverge\" when put into a loop. They seem to reason acceptably in small chunks, but when you string the chunks together, they go off the rails and don't recover. Can you slap another layer of LLM on top to explicitly recover? People have tried this, it seems like nobody has figured out the error correction needed to get it to converge well. My personal opinion is that this is the measure of whether we have AGI or not. When LLM-in-a-loop converges, self-corrects, etc, then we're there. It's likely all current agent code out there is just fine, and when you plug in a smart enough LLM it'll just work. reply geor9e 12 hours agoprevLots more good answers in [What have you built with LLMs?Hacker News](https://news.ycombinator.com/item?id=39263664) too If most people's only experience with AI is the chat.openai.com interface then yeah I can see why it seems like too much hassle to most people. The trick is figure out your long prompts ahead of time, and hardcode each one into a HTTP Request in something else (Tasker, BetterTouchTools, Alfred, Apple Shortcuts, etc). For me, I have dozens of long prompts to do exactly what I want, assigned to wakewords, hotkeys, and trigger words on my mac/watch/phone. Another key thing is I use FAST models, i.e. Groq not GPT-4. Latency makes AI too much hassle. i.e. 1. Instant (> “don’t trust AI with anything you wouldn’t trust a high schooler to do.” Then they should be great for making fast food, staffing amusement parks, and seasonal farm labor. They don't seem to be good for those things either. [Edit to add: the value high schoolers bring to jobs through non-cognitive abilities, which AIs lack.] reply clevergadget 15 hours agorootparentto be fair neither are high schoolers reply arretevad 15 hours agoprevWhenever I read about how AI is going to automate art or some other creative job I think about a quote I read (maybe here) that went something like “that which is made without effort is enjoyed without pleasure”. reply dredds 13 hours agoparentOr likening addon AI capabilities to a supervillain's anti-power: \"When everybody is super (artistic, whatever), then no one will be!\" - Syndrome, Incredibles. reply nunodonato 15 hours agoprevNot sure if this would qualify has an \"agent\", but I developed my own AI personal assistant that runs as a telegram bot. I can use it from everywhere easily, handles my events, reminders, sends me a daily agenda and memorizes useful things for me. I even integrated it with whisper so that I can send a telegram voice message and don't need to write. From a product/selling perspective, no value at all since I haven't even considered that (I'm building it for myself and my needs). But daily usefulness value? heck yeah! reply bronco21016 14 hours agoparentI’ve done something similar with iOS Shortcuts, giving the LLM access to my device calendar and reminders. I’m not sure it fits the definition of “agent” but it’s definitely useful being able to query my calendar using natural language rather than Siri’s rigid commands. reply afro88 14 hours agorootparentAre you using S-GPT or something like that? reply bronco21016 13 hours agorootparentI customized Siri COPILOT which I believe I found on HN. I can’t find a link to it anymore. I customized it with my own prompts and my own “tools” which are just Shortcuts that retrieve data from the native apps. I’m currently reworking the concept with Scriptable as I find it so frustrating to develop anything in Shortcuts. reply suchintan 16 hours agoprevI don't think ai agents are good enough to replace every job today, but they're starting to nip at the more junior / menial knowledge jobs I've seen a lot of success come from AI sales agents, just doing basic SDR style work We're having some success automating manual workflows for companies at Skyvern, but we've only begun to scratch the surface. I suspect that this will play out a lot like the iPhone era -- first few years will be a lot of discovery and iteration, then things will kick into superdrive and you'll see major shifts in user behavior reply andy99 15 hours agoparent> junior / menial knowledge jobs Not junior, unthinking. It's like outsourcing/offshoring. You're getting the equivalent of someone who doesn't care about what they're doing, not somebody inexperienced. I'm not saying it doesn't have a place, just commenting on the framing. reply JohnKemeny 15 hours agoparentprev> I don't think ai agents are good enough to replace every job today You mean any. You can fire a human knowledge worker for not doing their job correctly, but what are you gonna do when you only have LLMs and realize they can't do their job correctly? reply suchintan 15 hours agorootparentI think there are some they're good enough at today. Auto generating meeting notes + AI context, auto responding / following up to emails, filling out forms (we do that pretty well at Skyvern with high accuracy) reply lpapez 15 hours agorootparentprevFire 8 out of 10 of your knowledge workers and have the other remaining 2 review and fix LLM output. Basically the same what we have now, except the grunt workers will be replaced by the machine. reply digitcatphd 15 hours agoprevDevin seems like the first able to be commercialized. In my opinion the only way to do it well right now is you need to build your own system, the out of box open source projects are just some foundational work. I actually don’t think we will need agents in the future, I think one model will be able to morph itself or just delegate copies of itself like MoE for actions. It just seems extremely unlikely to me foundation models don’t get exponentially smarter over the next few years and can’t do this. reply jacob019 14 hours agoprevWhen I hear AI agents, I hear RL (reinforcement learning), not LLMs. RL may not be having the moment that LLMs are, but the progress in recent years is incredible and they are absolutely solving real world problems. I was just listening to a podcast about using an RL algorithm to enhance the plasma containment system in a fusion reactor, and the results were incredible. It quickly learned a policy that was competitive with the existing system that had been hand built over many years at a cost of millions. It even provided some new insights and surprises. RL is SOTA in robotics control, and some new algorithms like Dreamer V3 can generalize in realtime without millions of samples. It's has already grown way beyond solving ATARI games and is in many cases being used to train LLMs and other generative AI. There is a good amount of research going into combining LLMs with RL for decision making, it is a powerful combination. LLMs help with high level reasoning and goal setting, and of course provide a smooth interface for interacting with humans and with other agents. LLMs also contain much of the collective knowledge of humanity, which is very useful for training agents to do things. If you want to train a robot to make a sandwich it's helpful to know things, like what is a sandwich, and that it is necessary to move, get bread, etc. These feedback loop LLM agent projects are kind of misguided IMO. AI agents are real and useful and progressing fast, but we need to combine more tools than just LLM to build effective systems. Personally, I am using LLMs quite effectively for ecommerce: classifying messages, drafting responses, handling simple requests like order cancellation. All kinds of glue stuff that used to be painful is now automated and easy. I could go on. reply anotherpaulg 14 hours agoprevI made a specific design decision to avoid and minimize agentic behavior in aider. My biggest concerns are that agentic loops are slow and expensive. Even worse, they often \"go off the rails\" and spend a lot of time and money diligently doing the wrong thing, which you ultimately have to undo/redo. Instead, I've found the \"pair programming chat\" UX to be the most effective for AI coding. With aider, you collaborate with the AI on coding, asking for a sequence of bite sized changes to your code base. At each step, you can offer direction or corrections if you're unhappy with how the AI interpreted your request. If you need to, you can also just jump in and make certain code changes yourself within your normal IDE/editor. Aider will notice your edits and make sure the AI is aware as your chat continues. https://github.com/paul-gauthier/aider reply furyofantares 15 hours agoprevAgents are largely an attempt take the Human-LLM pair and and use the LLM to replace all the work the human finds trivial but which the LLM is terrible at. Trying to get more inference value per-prompt is a good thing. Starting by trying to get it to do long-chain tasks per-prompt makes no sense. I'm a huge fan of LLMs for productivity, but even small tasks often require multiple prompts of build-up or fix-up. We should work toward getting those done in a single prompt more often, then work toward slightly larger tasks etc. Plugins and GPTs are both attempts at getting more/better inference per-prompt. There is some progress there, but it's pretty limited. There's also plenty of people building task-specific tools that get better results than someone using the chat interface due a lot of prompt work. So there is incremental progress happening, but it's been fairly slow. The fact that it's this much work to get incrementally more inference value per prompt makes it very hard to imagine anyone closing the whole loop immediately with an agent. reply harrisoned 15 hours agoprevJust like many here have said already, GPT4 is being useful for coding for me. It is an amazing parser, specially, and save me precious time. Of course it's not able to do anything on it's own or without supervision, but is has been better than looking up to examples on Google. I also have been experimenting with it to replace the intention classifier part of Google's dialogflow. We use it at work for our chatbot. Earlier, we used Watson and it was amazing, but became very expensive. Dialogflow is cheap, but it is as innacurate with complex natural language as it is cheap. Mixtral (8x7B) has proved extremely accurate in identifying intentions with a consistent JSON output, giving it a short context, so i assume a simple 7B model would do the job. I still don't know if it is financially worth it, but it's something i'm gonna try if i can't fix the dialogflow's intentions. But in no way the model's output would directly interface with a client. That's asking for trouble. reply mattew 16 hours agoprevI’ve found the OpenAI assistants API not really up to snuff in terms of predictable behavior yet. That said, I’m very bullish on agents overall though and expect that once they get their assistants behaving a bit more predictably we will see some cool stuff. It’s really quite magical to see one of these think through how to solve a problem, use custom tools that you implement to help solve it, and come to a solution. reply timacles 15 hours agoprevI was hype on them initially, but after a few months of using them, I find that they are only useful for simple questions, and for coding help with syntax for basic things I have forgotten. Anything more complex just turns into an irritating back and forth game that when I finally arrive at the solution, I feel like I wasted my time not getting practical experience, but rather gaming a magic 8 ball into giving me what I wanted. It just doesn't feel satisfying to me to use them anymore. I don't deny that they improve my productivity, but its at the cost of enjoying what i do. I was never able to enter that feeling of zen flow, while using LLMs regularly. reply keymon-o 14 hours agoparentLLM tech is breaking records in popularity because it’s extremely user centric. It reminds us of a friendly teammate that seems seasoned in all areas we talk about with them. It’s a pleasure to talk to them now and then, we enjoy the vast source of information it provides us no matter what the subject is. But then, we ask him to do the work for us… Or imagine having a senior engineer in a subject you are clueless about (e.g. Haskell) and think about how annoying would it be for both of you to communicate out some advanced functionality program. Analyzing, researching, learning and making decisions are a crucial part of doing work. LLM apps are useful at another approach for learning and researching, but if you solely rely on them their drawbacks are going to keep you behind. reply dudus 15 hours agoprevOpinions mine based on learning from scratch this space in the last couple months only. I feel like these architectures built on top of last gen LLMs are mostly useless now. The current gen jump was significant enough that creating a complex chain of thought with RAG on last gen usually is surpassed by 0 shots on current gen. So instead of spending time and money building it it's better to focus on 0-shot and update your models to the latest version. Feeding LLM outputs into other LLM inputs IMHO just will increase the bias. Initially I expected to mix and match different models to avoid it but that didn't work as much as I expected. It depends a lot on your application honestly. reply spxneo 15 hours agoparentbut aren't current gen 0 shots gated and throttled? or has things changed for azure openai reply wslh 16 hours agoprevWe are doing that internally, so I think it is now more a craft than a \"product\". For example, we look at lot of specific codebase repositories (e.g. GitHub) and try LLMs over the diff just before and after a security code audit was done. Another one is listening to many social media (e.g. Twitter) posts to sense if there is a business opportunity. SDRs scan the results in an Slack channel manually but based on these signals. Finally, this is now a workflow but we did this [1] that is a piece in our work. [1] https://news.ycombinator.com/item?id=39280358 reply hhh 16 hours agoprevI haven’t found any useful agent workflows, and I’ve not found a tool that’s more productive for me (doing arch/design/implementation of systems) than just copy and pasting from the Playground. reply nijfranck 13 hours agoprevI build a lot of side projects and I have gotten a lot of value from https://www.goagentic.com/ to send personalised cold emails at scale. I no longer need to spend time researching the prospects as the tool researches every prospect, crafts a personalised message based on what I am selling and send the emails. So far with a 2-5% positive reply rate. reply dave333 15 hours agoprevSeems like we are still in the sheepdog phase where AI agents are extremely capable but not really autonomous helpers that still need overall coordination and control. Logical extension is layers upon layers so the next stage is a shepherd AI and then a farm AI. Then a meta AI that can discern and separate the layers and implement each and combine. May develop like a film studio with area experts combined for a particular project rather than a static one structure fulfils all approach. reply neural_thing 14 hours agoprevDevin is the only one that I've been able to use. Set up some projects for me, added some features. Could improve, obviously, but net positive for me in terms of time reply warthog 15 hours agoprevWe use it for manual research. Think of the times when you visit a certain prospect's website or company website to detect a certain information or to find a hook to talk to them about. We use agents in workflow to be able to do this in bulk. Problem is it does take a long time but at least it saves time at the end of the day and saves you from manually visiting a list of 100 different domains to see a piece of information reply paradite 16 hours agoprevAgents still very new and nothing that works for production yet. Specifically on using AI for coding, I wrote about different levels of AI coding from L1 to L5, we are still at L2/L3 stage for mature and production ready tech. Agents are L4/L5: https://prompt.16x.engineer/blog/ai-coding-l1-l5 reply stavros 15 hours agoprevI just (as in, five minutes ago) hooked GPT 4 up to my 3D printer and it's fantastic, I use an ESP32 Box and I can ask it what files I have on my printer, I can ask it to print a file, I even added calendar integration so it can read me my events and add new ones. I love it. All that's left is for someone to bundle it all up into a nice package, and we'll be in the future. reply mechagodzilla 14 hours agoparentI use a 3D printer all the time, but I almost never print a file more than once. It also takes a couple of seconds to select a file with the built-in interface, and then it usually takes anywhere from 30 minutes to a day or more to print. What's your usage model that putting a GPT4 instance between you and the printer is somehow helping things? This feels like someone saying \"Now I can e-mail my kleenex box to see how much kleenex it has.\" reply stavros 13 hours agorootparentI had some small prints that took ten minutes each, that I needed a lot of, so I figured it would be nice to talk to my ESP box and have it launch prints. reply mechagodzilla 13 hours agorootparentCan you not plate multiple copies of it that print at the same time? Usually you need to remove the finished print, clean the build plate etc. between prints, so longer total print times between 'overhead' periods increases your productivity significantly. It seems like a neat demo to be able to say \"Print another one\" verbally after spending a few minutes physically doing things to re-prep the machine, but not actually more productive than pressing the 'print again' button that shows up on the menu on mine. reply stavros 12 hours agorootparentI can, but I didn't know how many I'd need beforehand, and multiple copies increase the chance of failure (if one fails, the whole plate is trash). reply aubanel 14 hours agoprevI think agents are not a fad, they're here to stay since using an LLM in an agent system is the only way to let it access real world task, which they will end up doing when they're good and reliable enough. That said, I believe the current best models are still not good enough - but let's wait a few months. reply vood 15 hours agoprevI do. I use assistants as containers for different conversations for my GTM work: An assistant for marketing and copywriting An assistant for customer support An assistant sales conversations. These agents aren't super smart: just few PDFs for context plus a few sentences system prompts. I do get what I want in 80% of use cases (not measured, just a feeling). reply ed 15 hours agoprevAgents and tooling around LLM’s can probably make some small number of applications viable, but for the most part we need better foundation models to deliver on the hype. We’re definitely in the “wait” phase of the wait calculation. Everyone is expecting GPT5/q* to change things but really we won’t know until we see it. reply Its_Padar 16 hours agoprevI personally use an AI FAQ bot to automate FAQ questions in some of my Discord servers. It doesn't always work as well as a human answering but it does help, in most cases. In other words, AI Agents can be very helpful but can only be trusted/useful to a limited extent compared to humans. reply nothacking_ 15 hours agoprevNVIDIA and one step down the chain OpenAI are making lots of money, mostly by convincing people that LLMs solve all problems. LLMs are perfect for this, super flashy, with a ton of hype. In reality, LLMs are really bad at most applications, they are a solution in search of a problem. reply tomrod 15 hours agoprevSome. We are building some new processes from the ground up and will use Agents as a first draft contributor. This is typically where we find the most slowdown. And we will consistently search for the word \"delve\" as a misspelling :) reply precompute 14 hours agoprevAh sweet we have finally entered the \"let's be realistic\" phase. reply psalmadek 16 hours agoprevI’m here to read the comments but I think it is too early to give a statement of fact. reply AnimalMuppet 14 hours agoparentIn itself, that is an interesting statement of fact: Whatever value AI agents are going to deliver, they aren't delivering yet. The jury is still out. reply manojlds 16 hours agoprevEerily quiet here. reply zogrodea 16 hours agoparentNot sure if this is sarcasm, but the thread was only posted 20 minutes ago and has 9 replies already. I personally am tired of AI/LLM news but it still seems popular from this thread. reply Scratcher368653 16 hours agoparentprevIt is, isn't it? reply spxneo 15 hours agorootparentI'm also noting this but could be because its Sunday? or was GP implying lot of people are gettin funded to build agents in a thread where the consensus is they dont work well enough for people to pay for reply babelfish 16 hours agoprevKlarna used the OpenAI Assistants API to automate the work of ~7000 support agents. reply nicce 16 hours agoparentI believe you have one zero too much over there https://www.klarna.com/international/press/klarna-ai-assista... reply babelfish 15 hours agorootparentOops, thanks! reply kerkeslager 16 hours agoparentprev...which is not necessarily evidence that Klarna has obtained value from OpenAI Assistants API. It's quite possible (likely, even) that this has effectively been a complete removal of support. I haven't used Klarna, but a few other products I've used are unsupported now because human support were replaced with AI \"support\" that is completely useless. reply nonrandomstring 15 hours agoprevOne of the hardest lessons I learned in tech, maybe in life, was that if people don't want it you're done. It doesn't matter that you think it's the coolest and most amazing technology in history. It may be. So what? It doesn't matter that experts from every part of industry are yelling that \"this is the future\", that the march of this tech is \"inevitable\". They need to believe that, for their own reasons. It doesn't matter that academics from Yale, Harvard and MIT are publishing a dozen new papers on it every week. For the mostpart their horizon ends at the campus gate. It doesn't matter that investors are clamouring to give you money and inviting you to soirees to woo you because your project has the latest buzzwords in the name. Investors have to invest in something. And it doesn't matter if market research people are telling you that the latent demand and growth opportunity is huge. People tell them what they want to hear. The real test - and I wish I had known this when I was twenty - is do ordinary people on the London Omnibus want it? Not my inner ego projection. Not my wishful thinking. Not what \"the numbers\" say. Go and ask them. My experience right now - from asking people (for a show I make) is that people are shit scared of AI and if they don't hold a visceral distaste for it they've an ambivalence that's about as stable nitro-glycerine on a hot day. I know that may be a difficult thing to hear as a business person. If you are harbouring in your heart any remnant of the idea that you can create demand, that they will \"see the light\" and once they have a taste will be back for more, or that by will and power they can be made, regulated and peer pressured into accepting your \"vision\", then you'd be wise to gently let go of those thoughts. reply salomon812 16 hours agoprevI've been a bit disappointed by the AI. I'll admit going in with low expectations (I know about the whole AI summer/winter cycle) and I was blown away that ChatGPT could play Jeopardy! with just a prompt since I remember being blown away by Watson and AlphaGo. But then I had it help me write a letter, and by the time I got it to do anything useful, I basically had to write an outline for it, and then I realized I had already done the hard part. I asked it to write some boilerplate code for an interface to the Slack API in Python, but it used a deprecated API, and it assumed I had a valid token. Turns out Slack has lots of different kinds of tokens and I was using the wrong one, and the AI couldn't help me figure that out. After that, I remembered the story about pain point for radiologists. They don't need help diagnosing cancer, they need help with their internet connectivity. reply chancemehmu 15 hours agoprevI’ve found impel (https://tryimpel.com) to work extraordinarily well till now reply toomanyrichies 15 hours agoprevWhen I was technical blogging on how to learn from open-source code [1], I used it quite frequently to get unstuck and/or to figure out how to tease apart a large question into multiple smaller functions. For example, I had no idea how to break up this long `sed` command [2] into its constituent parts, so I plugged it into ChatGPT and asked it to break down the code for me. I then Googled the different parts to confirm that ChatGPT wasn't leading me astray. If I had asked StackOverflow the same question, it would have been quickly closed as being not broadly applicable enough (since this `sed` command is quite specific to its use case). After ChatGPT broke the code apart for me, I was able to ask StackOverflow a series of more discrete, more broadly-applicable questions and get a human answer. TL;DR- I quite like ChatGPT as a search engine when \"you don't know what you don't know\", and getting unblocked means being pointed in the right direction. 1. https://www.richie.codes/shell 2. https://github.com/rbenv/rbenv/blob/e8b7a27ee67a5751b899215b... reply precompute 14 hours agoparentThis has been my experience as well. I use Phind as a search engine, it's pretty bad for anything else. It does excel at obvious JS questions, but you can get those anywhere. It's great at sussing out a function that you only half-remember. reply iAkashPaul 15 hours agoprevI'm using ReACT for RAG based chatbot & tools which are just different retrievers. reply swagatkonchada 15 hours agoprevthe use case i \"feel\" these are useful are for studying any given topic. having one single page helps avoid many google searches, tangential questions, etc., but i'm always looking out for inaccuracies. reply kwinkunks 16 hours agoprevI've been disappointed by my few experiments with Langchain's agent tooling. Things I have experienced: - The pythonrepl or llm-math agent not being used when it should be and the agent returning a wrong or approximate answer. - The wikipedia and webbrowsed agents doing spurious research in an attempt to answer a question I did not ask (hallucinating a question, essentially). - Agents getting stuck in a loop of asking the same question over and over until they time out. - The model not believing an answer it gets from an agent (eg using a Python function to get today's date and not believing the answer because \"The date is in the future\"). When you layer all this on top of the usual challenges of writing prompts (plus, with Python function, writing the docstring so the agent knows when to call it), wrong answers, hallucination, etc, etc, I'm unconvinced. But maybe I'm doing it wrong! reply causalmodels 14 hours agoprevFull disclaimer up top: I have been working on agents for about a year now building what would eventually become HDR [1][2]. The first issue is that agents have extremely high failure rates. Agents really don't have the capacity to learn from either success or failure since their internal state is fixed after training. If you ask an agent to repeatedly do some task it has a chance of failing every single time. We have been able to largely mitigate this by modeling agentic software as a state machine. At every step we have the model choose the inputs to the state machine and then we record them. We then 'compile' the resulting state-transition table down into a program that we can executed deterministically. This isn't totally fool proof since the world state can change between program runs, so we have methods that allow the LLM to make slight modifications to the program as needed. The idea here is that agents should never have to solve the same problem twice. The cool thing about this approach is that smarter models make the entire system work better. If you have a particularly complex task, you can call out to gp4-turbo or claude3-opus to map out the correct action sequence and then fall back to less complex models like mistral 7b. The second issue is that almost all software is designed for people, not LLMs. What is intuitive for human users may not be intuitive for non-human users. We're focused on making agents reliably interact with the internet so I'll use web pages as an example. Web pages contain tons of visually encoded information in things like the layout hierarchy, images, etc. But most LLMs rely on purely text inputs. You can try exposing the underling HTML or the DOM to the model, but this doesn't work so well in practice. We get around this by treating LLMs as if they were visually impaired users. We give them a purely text interface by using ARIA trees. This interface is much more compact than either the DOM or HTML so responses come back faster and cost way less. The third issue I see with people building agents is they go after the wrong class of problem. I meet a lot of people who want to use agents for big ticket items such as planning an entire trip + doing all the booking. The cost of a trip can run into the thousands of dollars and be a nightmare to undo if something goes wrong. You really don't want to throw agents at this kind of problem, at least not yet, because the downside to failure is so high. Users generally want expensive things to be done well and agents can't do that yet. However there are a ton of things I would like someone to do for me that would cost less than five dollars of someones time and the stakes for things going wrong are low. My go to example is making reservations. I really don't want to spend the time sorting through the hundreds of nearby restaurants. I just want to give something the general parameters of what I'm looking for and have reservations show up in my inbox. These are the kinds of tasks that agents are going to accelerate. [1] https://github.com/hdresearch/hdr-browser [2] https://hdr.is reply Zetobal 15 hours agoprevI use one that reads my RSS feeds writes a radio DJ voice over and uses an elevenlabs API call to generate the voice (their Santa voice from last year works really well). Combines it with one of my Spotify playlists and gives me a 45 minutes radio show for my commute... pretty much changed how I consume news and content like hn. reply moomoo11 15 hours agoprevCash out and wait 20-30 years to cash out again. That’s the AI train stop lol. reply KennyBlanken 15 hours agoprevThe better question is: is the \"value\" worth the enormous environmental cost from the massive amounts of power used in training datasets, storing them, and running the GPUs/TPUs? NVIDIA estimates they'll ship up to 2M H100 GPUs. They have a TDP of about 300-400W each. Assume that because of their high cost, their utilization is very high. Assume another 2/3rds of that is used for cooling, which would be another 200W. Be generous and throw out all the overhead from the host computers, storage, power distribution, UPS systems, and networking. 2M * 600W = 1.2GW. Let's say you only operated them during the daytime and wanted to do so from solar power. You'd need between ten and twenty square miles worth of solar panels to do so. reply jokethrowaway 15 hours agoprevIt's a bit like with the chatbots revolution from 8-10 years ago (not LLM, just make a choice and maybe parse a few keywords to navigate a chatbot state machine) Sure, we can do that, but do users want that? I don't want to chat, talk or interact with people, I want the most efficient ui possible for the task at hand. When I do chat with someone is because some businesses are crap at automating and I need a human to fix something. Even then I don't want a robot that can't do anything. The only exception I can think of is tutoring but then I'd really question the validity of the answers. RAG is pretty cool in that regard because it can point at the original paragraph being used to answer the question. That might be useful to someone but that's not my favourite way of learning. Give me a summary of the content, give me the content, Ctrl+F and I'm good to go. For low stakes things like gaming where the agent messing up would just be a fun bug, I think it can be great. Looking forward to automatically generated side quests based on actions and npc which get pissed if I put a box on their head and hire mercenaries if I murder their families. reply bsenftner 15 hours agoprevI've found that while agents cannot replace anyone, they can sure help with the use of various things. First, we know these AIs are trained with data from the general Internet, and that data is vast. Second, the general Internet contains owner manuals and support forums for practically every active product there is, globally. These are every possible product too: physical products, virtual products like software or music, and experience products like travel or education. Between the owner’s manuals and the support forums for these products there is extremely deep knowledge about the purpose, use and troubleshooting of these products. Third, one cannot just ask an LLM direct deep questions about some random product and expect a deep knowledge answer. One has to first create the context within the LLM that activates the area(s) of deep knowledge you want your answers to arise. This requires the use of long form prompts that create the expert you want, and once that expert is active in the LLM’s context, then you ask it questions and receive the deep knowledge answers desired. Fourth, one can create an LLM agent that helps a person create the LLM agent they want, the LLM agent can help generate new agents, and dependency chains between different agents are not difficult at all, including information exchange between groups of agents collaborating on shared replies to requests. And last, all that deep information about using pretty much every software there is can be tapped with careful prompting to create the context of an expert user of that software, and experts such as these can become plugins and drivers for that software. It's at our finger tips...! reply jhawleypeters 15 hours agoprevI am! In my experience, you need to keep a human in the loop. This implies that you can't get the technology to scale, but I'm optimistic because LLMs have rapidly gotten better at following directions while I've been using them over the last six months. Summarization is probably the clearest strength of LLMs over a human. With ever-growing context windows, summarizing books in one shot becomes feasible. Most books can be summarized in one sentence, though the most useful, information-dense ones cannot. I had Gemini 1.5 Pro summarize an old book titled Natural Hormonal Enhancement yesterday. Having just read the book, the result was acceptable. https://hawleypeters.com/summary-of-natural-hormonal-enhance... For information-dense books, it seems clear to me that chatting with the book is the way to go. I think there's promise to build a competent agent for this kind of use case. Imagine gathering 15 papers and then chatting about their contents with an agent with queries like: What's the consensus? Where do these papers diverge in their conclusions? Please translate this passage into plain English. I haven't done this myself, but I have a hard time imagining such an agent being useless. Perhaps this is a failure of imagination on my part. The brightest spot in my experimentation is [Cursor](https://cursor.sh). It's good for little dev tasks like refactoring a small block of code and chatting about how to use vim. I imagine it'd be able to talk about how to set up various configs, particularly if you @ the documentation, a feature that it supports, including [adding documentation](https://docs.cursor.sh/features/custom-docs). Edit: I think a lot of disappointment comes from these kinds of tools not being AGI, or a replacement for a human that does some repetitive task. They magnify the power of somebody that's already curious and driven. They still empower lazy, disengaged users, but with goals like doing the bare minimum, and avoiding work altogether, these tools cannot help one accomplish much of use. reply bsenftner 12 hours agoparentLooks like there are not many people getting value. You, me and maybe a few others. I'm getting tremendous value, and I am surprised at all these defeat comments. Here's a short demo of an AI integrated spreadsheet https://www.youtube.com/watch?v=3t29rbs49xU reply ruined 16 hours agoprevcopilot has been useful to me for the boring parts of writing tests, but it needs a lot of review. other than that, dogshit reply tracer4201 16 hours agoprevAt my work, I have colleagues who speak English as a second language. Many of them are using LLMs to up their document and other writing. It’s actually quite awful. It’s obvious the text is LLM generated because of the verbose, generic writing style. It communicates clearly but without substance. Not gonna lie, I secretly judge these people. reply jaxomlotus 15 hours agoprev [–] We built a conversion rate optimizing AI Agent and saw about 45% click through rate lift on our own homepage. In other beta testing companies that used it, we saw a similar average and range has been +15%-175%. Agent (AB3.ai) can be tried here: https://AB3.ai reply spxneo 15 hours agoparent [–] you know these driveby comments writing a bit of contextually relevant bit that always ends with a me-too link are getting tiring. To the point where I see it here, I begin to suspect things aren't going well in your other sales/marketing funnels. reply jaxomlotus 13 hours agorootparent [–] \"Is anybody getting value from AI Agents? How so?\" If my comment is not on topic to answer OP's thread title question, then I'm not sure what is. The three clicks it will get buried in a HN thread are not going to do anything for us. But I also see little value in stating \"Yes, I've gotten 45% of a lift using an AI agent\" and not providing context. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Discussion centers on the skepticism and potential value of AI agents in various workflows, including concerns about AI's impact on art and creativity.",
      "Debates encompass challenges in using AI technology, efficacy of AI agents in tasks like coding and web scraping, and future development of AI models.",
      "Users share personal experiences with AI tools for work tasks, emphasizing the importance of smarter AI models and human oversight for productivity gains and mitigating potential drawbacks."
    ],
    "points": 160,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1711906336
  },
  {
    "id": 39883949,
    "title": "Finding Fulfillment in the PhD Journey",
    "originLink": "https://huiwenn.github.io/feynman",
    "originBody": "How to graduate your PhD when you have no hope Mar 28, 2024 by Sophia Sun meta-research a.k.a. Richard Hamming vs. Richard Feynman – I am going to graduate my phd soon. I’ve been talking to some prospective students recently, and a common question that comes up is: what makes a good phd student? how to be a good researcher? I’d point them to some articles, but to be honest I have no idea. What I did spend a lot of time thinking about, and have arrived at a somewhat self-convincing answer, is to a slightly different question - what makes a happy phd student? The answer is that they are married. (Obviously this is a joke. We did once survey a 10-phd bbq party, and marriage was the most statistically significant positive correlator. But that’s a story for another time.) Richard Hamming’s inspiring and influential talk You and Your Research is what convinced me to do a phd. I wanted to work on important problems and aspired to contribute to the advancement of human knowledge. Oh, to be a scientist! 3 years in I felt like there was no hope; I’ve toiled away all my time and the only way out is to quit. So I sought advice from my parents. They asked me to think about the least accomplished PhD graduate I know - Can I do as much as they did? What would a minimum effort phd look like? I answered that would result in a pretty bad thesis. And they said, that’s fine, bad is ok, go do what you can. Anything worth doing is worth doing badly. And here we are. This line of reason was put into words a lot more elegantly by Richard Feynman. Allow me to quote his letter in its entirety here - I thought it was the perfect antidote to Hamming-esque anxieties, the Dionysus to our Apollo. Enjoy, and hope you will finish your phd with peacefulness in your heart as well. – Dear Koichi, I was very happy to hear from you, and that you have such a position in the Research Laboratories. Unfortunately your letter made me unhappy for you seem to be truly sad. It seems that the influence of your teacher has been to give you a false idea of what are worthwhile problems. The worthwhile problems are the ones you can really solve or help solve, the ones you can really contribute something to. A problem is grand in science if it lies before us unsolved and we see some way for us to make some headway into it. I would advise you to take even simpler, or as you say, humbler, problems until you find some you can really solve easily, no matter how trivial. You will get the pleasure of success, and of helping your fellow man, even if it is only to answer a question in the mind of a colleague less able than you. You must not take away from yourself these pleasures because you have some erroneous idea of what is worthwhile. You met me at the peak of my career when I seemed to you to be concerned with problems close to the gods. But at the same time I had another Ph.D. Student (Albert Hibbs) was on how it is that the winds build up waves blowing over water in the sea. I accepted him as a student because he came to me with the problem he wanted to solve. With you I made a mistake, I gave you the problem instead of letting you find your own; and left you with a wrong idea of what is interesting or pleasant or important to work on (namely those problems you see you may do something about). I am sorry, excuse me. I hope by this letter to correct it a little. I have worked on innumerable problems that you would call humble, but which I enjoyed and felt very good about because I sometimes could partially succeed. For example, experiments on the coefficient of friction on highly polished surfaces, to try to learn something about how friction worked (failure). Or, how elastic properties of crystals depends on the forces between the atoms in them, or how to make electroplated metal stick to plastic objects (like radio knobs). Or, how neutrons diffuse out of Uranium. Or, the reflection of electromagnetic waves from films coating glass. The development of shock waves in explosions. The design of a neutron counter. Why some elements capture electrons from the L-orbits, but not the K-orbits. General theory of how to fold paper to make a certain type of child’s toy (called flexagons). The energy levels in the light nuclei. The theory of turbulence (I have spent several years on it without success). Plus all the “grander” problems of quantum theory. No problem is too small or too trivial if we can really do something about it. You say you are a nameless man. You are not to your wife and to your child. You will not long remain so to your immediate colleagues if you can answer their simple questions when they come into your office. You are not nameless to me. Do not remain nameless to yourself – it is too sad a way to be. now your place in the world and evaluate yourself fairly, not in terms of your naïve ideals of your own youth, nor in terms of what you erroneously imagine your teacher’s ideals are. Best of luck and happiness. Sincerely, Richard P. Feynman. ← Evaluating Probabilistic Predictions: Proper Scoring Rules",
    "commentLink": "https://news.ycombinator.com/item?id=39883949",
    "commentBody": "How to graduate your PhD when you have no hope (huiwenn.github.io)157 points by jxmorris12 20 hours agohidepastfavorite147 comments mo_42 19 hours agoFrom my experience, I strongly do not recommended You and Your Research. When I started my PhD I read this text as well. Of course I was very motivated of doing only important research. After some time I even found a nice research topic and worked on it for over two years. Even my advisor pointed out that it was a very novel and foundational idea. At the same time, a colleague published three papers in top conferences. Their approach was basically to look at publications from the previous conference, apply some delta that they had an advantage over other teams and publish this in the next round. Reviewers were happy because their publications already got cited and they understood the topic well. Whereas my topic used an algorithm from the seventies that reviewers had to revisit. My topic didn't fit so well into the overall conference trend and so I still have way less citations than those incremental works. When someone asks me if it's worth doing a PhD I tend to say that you need to like the direction in which science is going. If you don't like it and are rather rebellious, it's better to start a company. reply lisper 19 hours agoparentThe word \"science\" is ambiguous. It denotes two very different kinds of activities. The first is the lofty intellectual ideal of solving a major problem, winning a Nobel, and getting your name in the history books. The second is the daily grind of reading other people's papers, providing peer review, writing grant proposals, and generally interfacing with other humans as a cog in a grand machine. The second is in some sense \"easier\" than the first. It's a lot more work, takes a lot more time, but it's a relatively straightforward (if often tedious) process that pretty much anyone can do with enough diligence. The first is a lot more fun, can often be done while showering, but is also fraught with risk and dependent on luck. You have to find just the right problem at just the right time under just the right circumstances. You can spend your time slogging, or you can spend your time buying intellectual lottery tickets and hope that lightning strikes, but you can't do both, at least not at at the same time. The good news is that engaging in the daily slog is often (but not always) good preparation for and improves the odds of having lightning strike. So as a practical matter, that is often a good place to start. You might feel as if you're wasting your time reading everybody else's bullshit papers instead of writing the next Nobel prize winner yourself, but you're not. You're actually an essential part of the process even if you don't end up with the glory. And some day you just might be facing a really hard problem and go, \"Wait a minute, this seems kinda like that thing I remember Dr. Arglebargle talking about three years ago, except that he missed this one detail...\" and that's when the magic happens. reply Animats 13 hours agoparentprev> When someone asks me if it's worth doing a PhD I tend to say that you need to like the direction in which science is going. If you don't like it and are rather rebellious, it's better to start a company. Right. I got an MSCS from Stanford in 1985. That was about when it was becoming clear that expert systems were a dead end. The Stanford faculty was heavily invested in expert systems at the time, and were in serious denial. This was the beginning of the \"AI Winter\". I could see this, because I'd been doing proof of correctness work, where machine-powered predicate calculus could deliver results. It was clear to me that those methods were too brittle for the real world, although they worked fine on the rigid world of computer programs. Hammering the real world into predicate calculus only works if you already almost know the answer. (Drew McDermott's essay, \"Artificial Intelligence meets Natural Stupidity\" is the classic commentary on that.) There were some neural net people around, but they were stuck, too, partly because they needed a few more orders of magnitude more compute power before that approach started working. Neural nets had been around for decades, and on a good day in 1970 you could recognize handwritten numerals, slowly. Computer power wasn't increasing that fast in those days. In 1970, there were 1 MIPS mainframes, and in the early 1980s, there were 1 MIPS VAXes. Success with neural nets lay many orders of magnitude in compute and several decades ahead. So, instead of academia, I did some stuff for a little company called Autodesk. I never expected computer science to be captured by the ad industry. reply casualscience 17 hours agoparentprevMuch of the foundational science we learn about in the history books was done by bored rich people. Becoming a bored rich person is still a great strategy if you want to do foundational research. The risks you need to take in order to do that kind of work are too onerous for someone who needs a job to put food on the table. reply 7thaccount 14 hours agorootparentThat's why I have high hopes for humanity once we get closer to post scarcity (if we ever get close). Sure..some people will just watch TV all day, but I'd expect I'd start sitting down with math books and eventually finally learn the stuff I never had the time for when trying to put food on the table. The rich person life back in the day was having personal tutors at a young age teaching you Latin, Greek, rhetoric, mathematics, music, and so on. Then if you were really good you'd just work on whatever problems you fancied, while the rest of the family probably took care of the fortune. That's my very limited understanding for how it worked for some anyway. They would correspond with other scientists, but mostly focus on their work. There's some advantage to the university system, but the administrative beauracracy is intense. Writing government grants or proposals and so on is an enormous time suck. If you didn't need money, you could focus 100% on whatever you want. reply newswasboring 16 hours agorootparentprevThe only person I know who successfully employed the \"Bored rich person\" strategy is Stephan Wolfram. reply 7thaccount 14 hours agorootparentYou should watch his \"live CEO'ing\" videos for Mathematica. They're pretty good. He'll be personally testing something out for an upcoming release, something will break and he'll call up whoever was in charge of that function or module and start asking a bunch of questions. reply casualscience 12 hours agorootparentprevhmm, I think the entire FIRE movement is basically about this, no? Some people want to just do gardening or whatever, but if your hobby is science, that's totally fine too. Not every kind of science can be done on the cheap, but basically any theoretical work can be. reply sieste 16 hours agorootparentprevI very much doubt that Stephen Wolfram is bored. reply lairv 14 hours agorootparentHas he done any foundational science either ? reply Animats 14 hours agorootparentYes. reply throwaway2037 6 hours agorootparentWhy is this downvoted? I agree: Yes. His Wiki page[1] says: > Working independently, Wolfram published a widely cited paper on heavy quark production at age 18 and nine other papers. Wolfram's work with Geoffrey C. Fox on the theory of the strong interaction is still used in experimental particle physics. And: > In 1983, Wolfram left for the School of Natural Sciences of the Institute for Advanced Study (IAS) in Princeton. As I understand, IAS is all about fundamental physics research. [1] https://en.wikipedia.org/wiki/Stephen_Wolfram reply MichaelZuo 16 hours agorootparentprevSomeone who needs a job to put food on the table is going to take 4-5 years off to do a PhD? That doesn’t make sense. PhD salaries in most places can barely support one person , if that. reply casualscience 12 hours agorootparentI did so after college, had no money, parents were immigrants with no money but always told me to \"follow my dreams\". PhD payed enough to eat and live a decent life as a 20-something with no dependents. As I got older, I realized the financial burden I was taking, saw my family get into more and more financial trouble and I felt like a helpless selfish brat \"chasing dreams\" while my parents were trapped and fighting a losing battle... I finished as fast as I could and got a job in tech. reply MichaelZuo 12 hours agorootparentThose who are actually desperate to put food on the table are not going to spend extra years to finish as fast they can, they just leave. Or they don’t start in the first place. reply NegativeK 16 hours agorootparentprevI think you and your parent comment are saying the same thing. reply mathteddybear 16 hours agorootparentprevWell, if it is any help, the guy who made this forum once published a book with a collection of his essays. Various topics, including how to become a rich person, in what ways graduate studies are helpful in creating software. Might make sense of putting worldly things in this perspective. reply userabchn 16 hours agoparentprevI chose an ambitious/important topic for my PhD. Perhaps it is because stories usually have happy endings, causing us to be overly optimistic, but I imagined that if I just worked really hard on it then eventually I would solve it. In the end I did enough to get the degree, but I didn't solve the problem in any real way. It was five years of struggle that didn't produce any interesting results. The biggest learning for me from the PhD was that it made me more humble and caused me to doubt that something is possible unless a clear path to it is already visible. reply newswasboring 16 hours agorootparentI don't know you, neither your life, but if a university granted you a PhD you must have made some headway on the problem. It is possible that you made a bunch of progress but \"not enough\" from your perspective. We have some evidence that you did good work (your PhD) and no evidence (afaik) that your struggle didn't produce any interesting results. I think you might be being too harsh on yourself. reply hx8 15 hours agorootparentIf you work hard for five years you are going to have some tangible things to show for it. To me it sounds like he didn't accomplish as much as he originally hoped, and he suspects the opportunity cost of continuing with the work is too high. reply jimz 13 hours agorootparentConsidering on how many people are routinely influenced by the sunk cost fallacy when making their decisions, perhaps even that awareness on when to fold is a part of the critical process a good academic goes through in the course their research. The economy and politics have forced me to pivot and without enough self-awareness to realize when to leverage some other skill or the ability to learn another set of skills based on my prior education, even if it meant moving or changing to a field that isn't immediately obvious. I think it's a useful sanity check at times. That is sort of the point of a liberal arts education broadly, no? It's not a trade school (although I did spend my 20s at a glorified one and worked in what really was a fancy service industry gig - the law - that is partially obstructed by the sort of things at stake that we deal with, but is fundamentally a service and very much one that is customer-facing. I was one of the first few to quit from those in my graduating class that I kept touch with as well.) Owners of sports franchises, worth billions of dollars, fail to understand concepts like this. Those in charge of large companies, with externalities being constraints that we might not see in fairness but still having control of the narrative more or less, fail to demonstrate this. The government and policymakers either ignore or are ignorant of this on more occasions than one can count. Although it's hard to say what would've happened in the alternative, the initiative does alter the dynamics one would eventually have with their new situation, speaking personally. At least I'm not routinely putting in 75-80 hour weeks which is liberating in its own way even if it was routine enough that few bat an eye when put in that position at the start, judging from what my friends mostly ended up doing and not doing. But the imposter syndrome thing does haunt as well. It takes a while to realize that one hears about plane crashes because planes not crashing do not make the news, after all, and the actual rate of crashes, incidents, and near-misses can be and usually ends up haphazardly reported at best if one doesn't dig into the official quotidian, and jargon-filled language of official reports, and I don't think that it's part of society's expectation for the average person even if it's both an apt analogy and something to look up if one has the time. reply aleph_minus_one 14 hours agorootparentprevIt is well-known that PhD theses are made out of the fragments of broken life dreams. reply amatic 16 hours agorootparentprevThat is also a difficult problem - how do you know beforehand, that a problem is worthwhile? That someone else thinks it is important? Or how high is \"ambitious\" if you never tried to do a thing? There is a lot of luck in that system. reply tuatoru 15 hours agoparentprevJust like undergraduate work, your PhD thesis is practise. For the PhD, in the mechanics of doing large scale research and participating in the community. Save the novel and profound stuff for post-doctoral work. reply barbarr 19 hours agoparentprevSimilar experience here. My PhD research was in a niche (but unhyped) field where it was easy to pump out low-impact papers. My peers who went into more hyped fields had much more citations and much better job outcomes, whether in academia or in industry. If you want to do high-impact research in the long run, you need to have a strong foundation, which means a solid bed of high-impact papers to point to when you need funding, opportunities, etc. reply j2kun 16 hours agoparentprevI had a different experience. I aimed for what your colleague did, but none of my work was good enough for top tier conferences, so I ended up with neither novel nor top-tier work. So I think both options really depend on what you can deliver and in what timeframe. reply xk_id 17 hours agoparentprevThis mentality is the reason why the ratio of funding to groundbreaking work is a disaster compared to the previous century. reply yodsanklai 19 hours agoparentprev> Their approach was basically to look at publications from the previous conference, apply some delta that they had an advantage over other teams and publish this in the next round. I think that's it for most PhD students. Look at recent papers from your advisor (or their co-authors). Once you understand that specialized area well-enough, you should be able to find an epsilon that can be added, or to apply some results to a different use case. A 2-3 papers in somewhat decent conferences should be enough to be able to graduate. Most people won't solve interesting open problems. They will find questions that nobody has asked before, so they can be the first one to answer it. reply EvgeniyZh 4 hours agoparentprev> It's not the consequence that makes a problem important, it is that you have a reasonable attack. reply opticfluorine 19 hours agoparentprevVery similar experience here, and I ended up going directly into industry instead of a postdoc because I had the same realization. I'm glad I did it that way, though, because I'm very proud of my research and I think we did something very interesting and novel. reply jenny91 15 hours agoparentprevSo who enjoyed and got more out of their research, you or the one doing trivial twists in the latest saturated field? reply antegamisou 19 hours agoparentprevWhat about pursuing a PhD if you aren't planning to stay in academia? Would its lack bar you from industrial R&D positions? Assuming we're talking about a Computer Science/Engineering discipline. reply yodsanklai 19 hours agorootparentI would say it really depends. There are certainly specialized area where a PhD is important, but it's hit or miss. For instance, my team (in a big tech company) has people with very diverse backgrounds. Some tasks require more expertise, but most of the work will fit a generalist profile. Some teams who are more \"researchy\" have probably more PhDs. I feel doing a PhD gave me more time to learn things, while my current position is more fast paced and I can't afford to spend 2 weeks just learning new things. Financially, it didn't make sense to do a PhD. It's not even super clear what I learned is helpful now but I'm glad I've had this experience. It gave me a bit more perspective and culture of the field. I don't regret doing it. However, I regret going into academia. Being a professor didn't suit me for many reasons. I'm happier in industry. reply drewg123 16 hours agorootparentprevIndustry is more than willing to look past a lack of a degree given a good track record. Two of the smartest people I know didn't have undergrad degrees. One started as a lineman for AT&T, and learned programming by tending a computer in a field office, and worked his way up to working at some of the biggest tech companies. Your web browser uses technology he wrote (being cagey, as I'm not sure its commonly know that he doesn't have a degree). Even academia is willing to look past a lack of a degree for the right person -- my ex-wife's PhD advisor's advisor didn't have a PhD himself (but did foundational work in the field). reply fakedang 14 hours agorootparent> Even academia is willing to look past a lack of a degree for the right person -- my ex-wife's PhD advisor's advisor didn't have a PhD himself (but did foundational work in the field). These seems like more than a generation ago. Things were very different back then. I think now attaining a PhD (or enrolling in one) is the first step towards even starting out in foundational research. Academia today seems to be more about restricting access to the Ivory tower, rather than democratizing it. reply hiddencost 18 hours agorootparentprevGet into the PhD, publish a paper, get internships, and then once you've gotten an internship with a group you're happy with, ask if you can drop out and join them. reply maayank 19 hours agoparentprevInteresting take, thanks reply hinkley 17 hours agoparentprevAnd yet programmed love rehashing arguments from 1973. I wonder sometimes, how many of the people arguing endlessly and religiously about concurrency primitives realize that they are debating “religious” texts from the same prophet, Sir Tony Hoare. But as you say, the lack of immediacy does seem to mess with our heads. reply Melting_Harps 18 hours agoparentprev> When someone asks me if it's worth doing a PhD I tend to say that you need to like the direction in which science is going. If you don't like it and are rather rebellious, it's better to start a company. While this feeds into the founder kool-aid we all drank as founders, it also speaks volumes of truth to me to this day: I rejected where biology/ecology was going even when supposedly favourable paradigms--Al Gore's inconvenient truth and Carbon Credits were a thing as I was wrapping up my undergrad and I was appalled by both--were standing in front of progress and the most obvious solutions if you could take the small amount of time to familiarize yourself with emerging technology outside your gradschool bubble. Something I don't think academics are actually capable for fear of seeming to reveal themselves as hyper-specialized in one aspect only rather than the polymath they may seem/convinced themselves that they are. Also, it is my experience most academics are tone-deaf and incredibly petty individuals: a visit with the tenured faculty was enough for me to run for the hills of the debt-laden, financial crisis impacted work-force. I still visit my Biochemistry professor from time to time, who did well getting into Bitcoin, when I launched my startup working with the Hemp Industry; and I'm super proud of playing a part in that since we both got hit hard during the 2008 financial crisis. But I'm glad I turned down his offer to work in his lab and get my Masters and went through boot-strapping hell to launch my startup after working in many Industries just to get by. It was interesting white boarding about the bio-chemistry of CBD (and other cannabnoids for that matter) infused drinks/food I could pitch to my clients as profitable avenues of growth after struggling to make it to class and 'only get a B because I just didn't 'fit' (read: rebellious) the school model at all despite having both honors and letters of recommendation to join two labs. I'm in Grad school now (almost 20 years since starting my undergrad) for CompSci now, but only because I'm working to transition into Big Data and I already work on the large Data Centers spurred on by the AI hype; luckily it's a non-thesis program for experienced and working professionals (I'm currently on assignment for a sub-contractor in WY). reply sage76 18 hours agorootparent> most academics are tone-deaf and incredibly petty individuals Yep, their egos get inflated from interacting with students all the time. If you have been teaching the same subject for 10 or more years, and you are constantly interacting with people who are learning it for the first time and struggling, it's easy to become arrogant. I'll go 1 step further and say that most of the advice academics give out in terms of learning has been counterproductive for me. There have been some exceptions, but I have been disappointed for the most part. reply cqqxo4zV46cp 17 hours agorootparentYep. I regularly interact with a seasoned academic who is easily one of the top minds in his area. Having him understand that this doesn’t mean that he is an expert in all areas a constant uphill battle. reply thsksbd 20 hours agoprevBased on my observations, marriage is the most reliable indicator of graduate student success. At least the male ones. I don't pretend to know if marriage is the cause of success or if it is merely a correlated phenomena. But I've observed it and talking among my circle, so has a top engineering researcher at a top 10 university. This is a man who has graduated scores of students placing half of them at top universities. reply j7ake 20 hours agoparentMy theory: PhD is inherently unstructured, and requires wading into the unknown without many markers to tell you where you are going. Marriage creates a structured life outside of work that gives PhDs an anchor to return to periodically. This leads to better long term and consistent progress. Stephen King describes his success in writing in part due to his successful marriage. There may be some similarities there. reply donor20 19 hours agorootparentA lot to this - marriage with no kids. Kids flips this though - you want to be done with the bigger push at that point reply thsksbd 14 hours agorootparentThis top engineering professor had two kids in grad school and a third one during his first tenure track job. His theory is that wife and kids out a gun to his head that he had to learn to swim and quick reply rightbyte 19 hours agorootparentprevNo way. Family puts alot of \"make money now\" and \"be home for dinner\" pressure on you. Hardly a good environment for doing research. Not even bringing kids into the equation. reply thsksbd 14 hours agorootparentStructured, disciplined, habits make you far more productive than working long hours. At an old job (natl. lab) there was a guy who does his research job for, maybe, two or three hours a day. The rest of the day he is reading unrelated books. But those three hours are so productive that by March he has filled his employer's scientific paper quota. His managers hate him because they see his fucking around 70% of the time, but by any performance metric (especially quality. His papers are the consequential ones)he's blowing everyone away. reply bsdpufferfish 19 hours agorootparentprevResearch in a Phd program is 90% long term habit, 10% creative musing. Marriage structure helps with that 90% part. reply etrautmann 19 hours agorootparentHaving been through all of this, young kids are both wonderful, but definitely restrict the ability to completely focus on a research sprint. While 90% habit is important, there were periods where I needed long hours and focus, and those are harder to find now. Adding finances to that, and I'm really glad that I waited until after my Phd to have kids, though plenty of people do it successfully. reply bsdpufferfish 18 hours agorootparentNo disagreement. reply Solvency 20 hours agorootparentprevNonsense as a general rule. Love my family. love wife and kid. Have never been less productive or efficient in my life. As a stickler guy between 20 and 34? Relentless productivity, focus, and time. reply bsdpufferfish 19 hours agorootparentPeople don't typically start a Phd at 34. You're at a different level of maturity now. reply Der_Einzige 17 hours agorootparentprevnext [3 more] [flagged] gorjusborg 17 hours agorootparentWhat an arrogant and reductive thing to think, let alone post publicly. I have a hunch that you are very young, very single, or most probably both. reply hilux 16 hours agorootparentprevI came to Hacker News to get away from this kind of reddit nonsense. reply analog31 18 hours agoparentprevMy observation is that the married students had a real fire burning under them to get done and out. They had a real tangible motivation. Disclosure: Married to another PhD student. Also, somebody to drive you to the hospital when your experiment explodes. Fortunately, she wasn't seriously injured. reply aleph_minus_one 20 hours agoparentprev> Based on my observations, marriage is the most reliable indicator of graduate student success. At least the male ones. At least in math, I would rather say that marriage is a counterindication that a great PhD thesis come out at the end, for the simple reason that being married means that math is only your second love, while writing a great PhD thesis in math requires the topic of your PhD thesis to be your nearly only love for the duration of your PhD studies. reply wholinator2 17 hours agorootparentYa know, that makes sense and all. But I'd want to be careful about subtly implying that doing nothing but your research and forgoing personal relationships in favor of work is _better_ than having a life and a little worse thesis. Most of a career in research happens after the PhD anyway, is it possible those \"This is my life\" people do worse later on, when their potential contributions to fields would be maximizing? Having a life is an okay thing to want, if for no other reason than the people who are really crazy for it and want nothing but math research aren't going to listen to anyone about relationships anyway. If you find yourself seriously considering turning it down a little bit to find other avenues to personal fulfillment then you might not be one of those crazies and i think that's okay, maybe even better reply thsksbd 14 hours agorootparentprevMy sister is in a relationship with a top, young, number theorist. Ill have to let her know the damage she's doing :D reply matthewdgreen 20 hours agoparentprevMarital status is also correlated with age. Students who enter the PhD having spent a few years working following their undergraduate degree are often a bit more motivated and experienced (though I've had successful students take both paths.) reply VeninVidiaVicii 19 hours agorootparentLikewise people with bigger feet are better at math. Well actually that’s just because infants have tiny feet and are notoriously terrible at math. reply wholinator2 17 hours agorootparentWell that depends on our definition of bigger. If we're saying bigger is \"larger than baby feet\" then okay but i think a better definition is \"larger than the mean/median\". Would baby feet drive down the mean/median that much? reply polygamous_bat 19 hours agoparentprevThere are two factors to this. 1. If your partner is not in a PhD program themselves, then you can rely on them during what I call “crunch time”, and there are a few of those in every PhD when you’re doing 16 hour days 7 days a week just to get something out of the door. Single students don’t get that luxury and thus inevitably have to be miserable during that period. 2. Secondly, successfully navigating a relationship/marriage takes a lot of the same skills as successfully navigating a PhD. A good PhD at the end of the day is built on a lot of compromises: between your vision and your advisors’, the fields’, your collaborators, and if you’re interning, your industry partners. A relationship teaches you how to navigate that without burning bridges, which may not be true for a headstrong “I’m gonna change the world but I don’t know how” PhD students. reply Beldin 17 hours agorootparent> A good PhD at the end of the day is built on a lot of compromises: ... Don't forget \"your skill level\". Recognising your own limitations as a researcher is rather important for a PhD student. reply fromMars 16 hours agoparentprevDidn't work out for me. I was pursuing a PhD and got married and shortly after had a child. I could no longer comfortably subsist on the salary of a Research Assistant. So I left academia to make more money. I could imagine marriage helping if one doesn't have kids and the cost of living where the school is located isn't too high or if one's spouse has a successful career. The advantage of being married is that you don't have to spend as much energy dating and other social activities. reply silverquiet 20 hours agoparentprevWhat if women aren’t interested in you? reply telmo 19 hours agorootparentTry to find out why, and please avoid Internet \"red pill\" stuff. I'm not telling you this as any sort of political statement nor am I trying to fight any culture wars here. This is just the advice of a middle-age guy with perfectly mediocre / average looks, and some life experience. From what I observe, 9 times out of 10 the problem lies in personality. I know plenty of guys with no money and no looks that have no trouble attracting the interest of the opposite sex. Why? Because they have a great personality, as in, it feels good to be around them. Furthermore, people who rely only on look and status to attract a partner and do not work on themselves are unlikely to have a happy relationship in the long term. I am not blaming people for having unappealing personalities. This is usually the product of things that are outside of their control, usually some sort of trauma. Life is not fair. A lot of people are traumatized and do not realize it. This can be overcome, but you must want to overcame it and you must be able to face harsh truths. Maybe therapy can help, maybe meditation, maybe even things that are considered \"woo\" but that allow you to face your demons. Whatever works and clicks with you is a valid answer. All roads lead to Rome if you are courageous enough. Check out this guy, I have the impression he is particularly suited for the HN demographics: https://www.youtube.com/@HealthyGamerGG Good luck man, you can do it! reply grepLeigh 19 hours agorootparentI also don't want to dredge up culture wars stuff, but just wanted to say it's really nice to see men warning other men away from the redpill path. Thanks for encouraging therapy and self-healing. reply silverquiet 18 hours agorootparentprevDo you think living with my parents due to disability could be an issue? reply j45 17 hours agorootparentProbably someone else who has had the same experience and understands you completely. The question comes into whether you've continue to apply self-effort towards your inner growth, like the other person may have. When we go about seeking our best way, it increases the chances of meeting others doing the same. reply bckr 19 hours agorootparentprevAwesome, I recommended the same channel. reply Spooky23 18 hours agorootparentprevGood advice. In my observation, especially as you get out of your 20s, single guys are… kinda phone it in for various reasons. Even moderate effort makes a dude a rockstar. reply hilux 16 hours agorootparentprevI can't express how much I appreciate this - thank you! reply j45 17 hours agorootparentprevThis channel looks to be a solid recommendation. Personality is everything, and can be developed from leveraging one's sincere ability to be curious. As someone who accidentally outgrew gaming after playing them more than anyone I knew, a channel by this name, with this kind of content can shine the path forward to other equally interesting sides of one's self. In my case, I rediscovered creating and building things was more interesting than playing in others worlds. I never really quit gaming. I just didn't identify as someone who played games any longer. reply Der_Einzige 17 hours agorootparentprevThis kind of advice is such garbage. No, reading \"come as you are\" or any of the shit bell hooks writes doesn't just fix issues for people like the OP. Telling them then that they have a \"bad personality\" is so fucked up. Personality is subjective, and most people on earth can find others who believe that their own personality is \"perfect\". Someone not being successful in the dating market does not necessarily imply that their personality is bad. Saying they have no game and implying that this speaks about their personality is really hurtful. Women's sexual selection is not the arbiter of a good personality - and indeed, given what we know about how seductive dark triad traits are, it may in fact be a signal of a bad personality. It's pretty bad when you straight up recommend \"woo\" to people, and effectively say \"all roads lead to rome... EXCEPT THE RED PILL!\" The reality is that no matter how garbage Tate et all are, the alternative explanations for why increasingly large amounts of men have no game are so bad that huge swaths of men get seduced by tate's bullshit. Your kind of response only takes impressionable men who would fall for it and further entrenches their beliefs that the red pill is the \"subversive\", \"real\" way that alpha men are ending up with harems while billy the beta ends up making another HN post about typescript reply hilux 12 hours agorootparentSomething can be hurtful and true. In fact, it's almost always the case that the most true and important things are hard to hear, hard to believe. When you can accept that your problems are your own responsibility (which is not the same as \"fault\"), you'll start to move forward. Yes, I'm speaking from experience. It's not easy. reply telmo 2 hours agorootparentprevTo be clear, I didn't tell OP that he has a bad personality. I don't know OP, how could I know that? What I told him, and do know, is that in my set of experiences this is usually the problem. And I include myself in that set of experiences, to be clear. I completely agree with you that \"women's sexual selection is not the arbiter of a good personality\". Women are not immune to having terrible personalities. Women, like men, are highly flawed beings. We are all human. We all have to work on ourselves. What I will say is that if you have a good personality, you are more likely to attract a partner that you can be happy with for the long term. This is precisely how you avoid dark triad people, by being self-confident, by knowing who you are, by not needing constant external validation, by not being so influenced by what other people thing of you, by being empathic but not a people pleaser. You talk about \"game\". Game is transactional. Thinking in those terms attracts people with transactional mindsets. That is precisely the problem with the red pill, it guides you towards that world. It is self-reinforcing. It leads to depression and despair, because it guarantees to make you more and more aligned with the sort of people that you should avoid. It thrives on the funhouse mirror that is social media, where if you don't make a million a year and are more than 6 feet tall you might as well shoot yourself in the head, unless you use \"game\" (i.e. deceive people). It's a path to hell. I did not \"straight up recommend woo\". What I talked about is \"things that sound like woo\". Which is another way to say, have an open mind to things that might seem mushy to your male brain, that are not necessarily supported by peer-reviewed studies. Be less mentally rigid, is what I am saying. Men tend to fall very easily in this trap, again me included. I love subversive stuff. I was a teenager in the 90s, we were all edge lords back then. I miss the time when conspiracy theories were fun. We are not in that time anymore. Tate is vile. He is an abuser of men and women, and he is the one selling impressionable men on lies that will destroy their lives. You are probably quite young or inexperienced yourself if you think that having and harem will make you happy. The idea makes me shudder. Is it possible to have an \"harem\"? Sure. You will be surrounded by dark triad women, or by victims who need your money. None of them will love you. Why would you want that? reply bckr 19 hours agorootparentprevYou’ve got significant foundational work to do. I’d start with Healthy Gamer GG’s channel[] and also pick up the book How to Win Friends and Influence People. [] https://youtube.com/shorts/693UD3V1KSU?si=6IiRwP0Q1qRqErnp reply anewhnaccount2 19 hours agorootparentYou don't know that this person has lots of work to do on themselves. These things are partially a matter of compatibility, putting yourself out there (which is a thing you do rather than a thing you become), and yes, unfortunately, pure dumb luck. reply bckr 17 hours agorootparentI do know, because “what if women aren’t interested in you” is a question you don’t ask after a certain degree of development. I don’t know how long it will take to do this work. With a foundation of personal development and the right attitude, I bet you can go from “what if women aren’t interested in me” to successfully dating in under 6 weeks. If you need to learn how to develop yourself, it could take 6 months or more before you’re able to internalize the needed lessons. reply j45 17 hours agorootparentprevBeing out there a lot is the dumb luck of human propagation. Seems pretty effective. reply silverquiet 19 hours agorootparentprevBeen through it. Never really could take any of this stuff all that seriously. Especially after the woman I ended up closest to passed away. I’m mostly wondering how the people who recommend marriage as a solution to a lot of ills (and I think it’s an issue worth exploring but I also think there’s a lot of correlation/causation mixups here) expect people to take action on that. reply bckr 17 hours agorootparentMy heart goes out to you with regard to the death of your loved one. Do not let death and grieving pull you permanently out of the realm of intimate relationships. Human connection is foundational to all kinds of success. Taking it seriously has served me well. reply silverquiet 16 hours agorootparentLoved one is probably overselling it - she cut me loose for the man she would eventually marry and then passed away a few years later, but it certainly made it all seem even more pointless than before. I’m mostly responding to all the advice I see about the benefits of marriage. My response tends to be something like, “so…?”. It occurs to me that no one of these people really talks about the benefits of dating, which seems like a pretty significant prerequisite. And again, I suspect there’s a lot of selection effects going into that data as well. Not that I don’t appreciate your comments. reply hotdogscout 17 hours agorootparentprevNewton died a virgin, averages are defined by average people, don't stress on their anecdote and don't put your value in what women think of you. reply j45 17 hours agorootparentIt's everyone's personal choice but if Newton had some descendants, I wonder how that might have turned out. reply hotdogscout 11 hours agorootparentMediocre probably. I don't think biology is the bottleneck of great ideas. https://en.m.wikipedia.org/wiki/Einstein_family reply j45 4 hours agorootparentI don't think it is either, but there can be inter-generational momentum reply teaearlgraycold 20 hours agorootparentprevKeep doing the things that make you into a man that women are interested in. reply throwaway35777 20 hours agorootparentGo into an industry that's not overly political and where you're useful enough not to be judged on arbitrary qualities like your (a)sexuality. reply aleph_minus_one 20 hours agorootparentprev> Keep doing the things that make you into a man that women are interested in. ... like making lots of money. :-D reply teaearlgraycold 18 hours agorootparentJust don’t pick someone that has that as one of their top 3 reasons to be with you. But money does help with almost everything. reply Der_Einzige 17 hours agorootparentprevnext [4 more] [flagged] dragonwriter 16 hours agorootparent> The reality is that being a SWE or related is a massive net negative to your \"SMV\". Its not, but buying into the belief that it is is. reply teaearlgraycold 16 hours agorootparentprevMore than 90% of dating advice is overfitting. It’s high stakes and there is plenty of signal to glean, but once you know the basics of the situation the rest is up to chance. Which means if you’re being picky (and I think to should be if you can) success can take a while. In the meantime you could end up desperately pulling apart facts like a conspiracy theorist. The best advice, at least for people that aren’t super lucky, is to stop trying, do the obvious things (be healthy, happy, financially comfortable) and live a life that puts you in contact with women. reply bigbillheck 16 hours agorootparentprev> The consensus? He has tons of \"red flags\" for basically being \"billy the beta\" who won't excite the women who would come into contact with him. Hardly anyone is surprised that he can't get dates. Quoting one of the top comments: \"his persona reads to be like a human representation of mashed potatoes with no seasoning, bland. \" There's a couple people in that thread who used the term \"beta\" but I wouldn't call it the consensus. There's some calling him \"boring\", but there's also a bunch of ladies saying \"maybe if I were single\", and a bunch more saying \"wanting kids is a dealbreaker\". And in any case things seem to have worked out for him: \"This profile was published in August 2022; I'm no longer single, but wanted to leave it up since it was an important document for me.\" (https://colah.github.io/personal/dating/) > The reality is that being a SWE or related is a massive net negative to your \"SMV\". We software people trade our game for our money. It's sad but it is what it is. I blame the widespread normalization of \"nerds\" being bullied by \"jocks\" in TV from the 1980s on. The cultural damage this has done is unreal. I don't think this is true at all. > Seriously - the most unrealistic thing about the movie \"wargames\" is the idea that a kid who looked like that doing a bunch of command line shit would have a girl who looked like that interested in him (and his computer!). Matthew Broderick ended up marrying Sarah Jessica Parker. reply j45 17 hours agorootparentprevEverything is a function of the self-effort place into self-growth. Because everyone's special, and therefore no one is special, there is someone just like you who you will be very compatible with, both in the beginning and growing through the seasons of life together. It just begins with deciding how you want to look at possibility, or not. Our minds go how and where we focus. reply brandall10 20 hours agoparentprevGenerally it seems both are grad students, or at the very least, met in school and the relationship has an academic foundation. Imagine having a partner to wade through such an experience who can relate, is pushing you, helping you see your blind spots, quelling your doubts/fears, and in general cheering for your success. And then the clarity you get for doing the same for them. There's also the extra level of responsibility that marriage entails, that forces one to 'grow up', for want of a better expression. reply j45 17 hours agorootparentNot just grow up, but one of the easiest cheat codes to prioritize and increase effectiveness. reply Cloudef 20 hours agoparentprevIf that's true, the future doesn't look good for male. Honestly though, marriage and having children come with financial stability and good life balance. reply richrichie 18 hours agoparentprevMarriage solves the problem of sex. Especially for male phd students. This is a huge factor at that age, and for the species types that are found in phd programs. Takes the edge away, saves time and energy, etc. Not fashionable to talk about it in such terms. But it is a factor. reply aleph_minus_one 17 hours agorootparent> Marriage solves the problem of sex. Especially for male phd students. This is a huge factor at that age, and for the species types that are found in phd programs. In my observation among the students who do well in PhD programs, you can find a lot more asexual people than in the general population. In this sense, you are probably right in the argument > Takes the edge away, saves time and energy, etc. but for a nearly opposite reason. reply Der_Einzige 17 hours agorootparentprevGod if only. Unfortunately, they call it the ol \"ball and chain\" for a reason. Most men would happily sleep around with many women as they could if they had a \"pass\" from their partner. I wouldn't even be surprised if more than half of all married men would describe their sex life in terms like \"mediocre\" or worse. Marriage is supposed to solve that issue, but I claim that among other things, falling marriage rates indicate that increasingly large numbers of (usually men) are prioritizing sleeping around over settling down. reply wholinator2 17 hours agorootparentWell, this also discounts the whole \"is a problem for the types of guys found in phd programs\" thing. Yeah, a large majority of guys would like to sleep with women. It's not just a factor of, \"does my wife expect me to not fuck around\", and is significantly, \"is there anyone in my life that i can actually have sex with\". Loneliness is a big deal and for a quiet and introverted, private guy having a wife raises the potential partners from 0 to 1, an infinitely higher amount. Yeah, if you have the body type and personality type and time and willingness to seek out and sleep with random women then go for it. But just because you don't fit that bill doesn't mean you don't need sexual comfort in your life. (I don't believe you said that, but it's a useful statement for me to make) reply richrichie 17 hours agorootparentprevMost phd students are not the bar hopping pick up artists. And female students in campuses don’t exactly hunt phd students to sleep with either. Marriage works. I am not talking about over 10 or 20 years. But during this period, it solves the problem of sex. reply Der_Einzige 17 hours agorootparentSignificant swaths of Ph.D students would abandon all of their scholarly endeavors if they could go bar hopping and be pick-up artists. \"Female students not hunting for Ph.D students\" is evidence that, especially for men, getting lots of education is straight up \"unattractive\". Why should I even want a Ph.d if all it signals to the other sex is that I'm not just a nerd, but a nerd with a poor ability to calculate ROI. At least a lot of the \"jocks\" ended up with a good ROI job in finance that they got from their frat brothers rich dad. reply linguae 10 hours agorootparentEarning a PhD is about showing employers that you are capable of producing novel research results. You should earn a PhD because you are interested in deeply investigating a research problem, and also because you are interested in pursuing a career in research despite the uncertainties that entails, particularly the competition for academic positions and the trend in industry away from unfettered research and toward business-driven research. If you cannot live with the uncertainty that a research career entails, and if you feel that earning a PhD is a relationship/sexual repellant, then you should consider another career path. Besides, a PhD didn’t stop Oppenheimer and Feynman. reply fwip 15 hours agorootparentprev> Why should I even want a Ph.d if... I dunno dude, why do you do literally anything in life? Surely your life doesn't revolve around finding a wife. reply hilux 16 hours agoparentprevChecks out - Feynman was married. reply chriskanan 18 hours agoprevI read the book \"Getting what you came for\" before starting my PhD and took that to heart for graduating. It basically argued one does not need to do their best work during their PhD as that's the start of one's career. What made me sad was after I realized my area wasn't one anyone was hiring for faculty for (deep learning and AI by around 2011), and that the people who did get interviews had an astronomical number of papers (in AI generally but nobody cared about neural networks). I fortunately got a faculty job after applying to 30 universities in 2013 where I got one offer and was their third choice. I got lucky, deep learning exploded, and my career took off. Many people tried to steer me away from AI and especially neural nets, including some of my committee members for the latter. Regardless, another factor in my success was having a somewhat crazy officemate who would work 30 hours straight and then sleep 13 hours, 7 days per week (we are still good friends and he is now a well known deep learning theory professor). I couldn't do that, but the zeal with which he pushed himself motivated me a lot (although he also gave me imposter syndrome since we took some proof heavy courses where he would be done in a couple hours what would take me days to do the same assignments). I definitely don't recommend having kids during a PhD, as I've seen many students and peers struggle tremendously. A partner is great, especially if they are equally busy (mine was in medical school across the country), because one isn't going to really get a lot of nights and weekends if you are really pushing yourself. My advisor never pushed me, but I pushed myself very hard. Still, if one just wants a PhD, what one needs is a good advisor who can steer one towards problems that are tractable given the student's abilities, stable funding (most programs worth doing are reasonably stable), and not trying to think one must do the most amazing work of their lives during their PhDs. It is a lot of work, but a career in research and the privilege of cultivating the next generation is really what makes me feel self-actualized. I try to find low-hanging fruit research projects for my PhD students in their first few years to build up their confidence and then have them pursue harder problems, where if the harder ones don't work out they still hopefully have enough stuff to form a dissertation. I often find myself needing to be more of a cheerleader for my students nowadays, though, since my field exploded and reviewers seem much less competent. Rejected papers for good work hurts mental health. reply gtmitchell 19 hours agoprevAs a counter example, I will offer my own experience in graduate school. I was one of the few married students and observed that nearly all the successful graduates students had the following in common: 1. They had competent PhD advisors 2. The advisors had stable funding sources 3. They were single Of those three, #1 and #2 were by far the most important. Certain professors just knew how to run a good lab and were able to shepherd their students through the program efficiently. As for the impact of #3, I found as a married student I had to balance my research and teaching responsibilities with the needs of my spouse. It added a level of mental and emotional stress my single colleagues didn’t have to deal with. Ultimately, my balancing act was unsuccessful. I eventually dropped out of my PhD program and ended up divorced. So yeah, based on my anecdotal (N=1) experience, being married doesn’t not help you to be successful in graduate school. reply gogobio 18 hours agoparentTo be honest, I found the exact opposite to be true. I agree with the author regarding the happiness during grad school being directly related to being married. As a matter of fact, I don't think I would have finished my PhD had my wife not supported me mentally, economically, and in spirit. I've observed our single students struggle, complaining about having to do chores after classes, clean, cook, look after themselves. Whereas my spouse was supportive and understanding, she took a colossal load off my shoulders - I could concentrate on my studies and had little to no worries outside of school. We both worked, but she worked full time to support us and got a master's degree, so she knew all too well that grad school isn't peanuts. I think it's more so about having a good spouse who is understanding and supportive, who can meet you halfway. reply bachmeier 18 hours agorootparentI wrote my response while you posted, but it basically supports what I was saying. > I think it's more so about having a good spouse I wouldn't say it's about a \"good\" spouse. It's tough, and if it's too much for the spouse, maybe the grad student should consider dropping out. Nobody knows how it's going to work until they do it. This is especially true if they moved far from home for grad school. reply hilux 13 hours agorootparentprevThis brought a smile to my face - so happy for both of you. reply bachmeier 18 hours agoparentprevLikely a different field, but > 1. They had competent PhD advisors I'd say fit is very important. Some advisors will provide students with feedback, give them direction, and wait for the students to come back with output. Other advisors will micromanage students, give them tight deadlines, get upset if they don't strictly follow everything they say and every deadline, and generally give the student no freedom to do anything on their own. The thing to keep in mind is that grad students usually work well in one of those environments, but few students will thrive in both. > as a married student I had to balance my research and teaching responsibilities with the needs of my spouse. It added a level of mental and emotional stress my single colleagues didn’t have to deal with. This can go in several directions. Your experience is obviously the most common. In some cases the spouse is counting on the grad student to finish and get a high-paying job. They take care of everything like paying bills, shopping, cooking, etc. I've also seen cases where the spouse sacrificed so the grad student could finish (so they could have a higher income and start a family) and it put so much pressure on the grad student that they were breaking down. reply lsuresh 16 hours agoprevMost researchers do not do their best work during their PhD, but after it. The PhD is simply the training you need for a research career. It's an easy path to disillusionment when PhD students don't realize this (often because advisors don't do a good job helping the students set reasonable expectations and goals). It's good to be ambitious in problem selection. But a huge part of the training is to learn how to break down hard problems into reasonable increments that you can attempt to solve, one at a time. reply paraknight 3 hours agoprevI had no hope during mine and completely changed research area in my final year. All my publications became irrelevant and I did a complete rewrite of my already mostly written thesis. My thinking was that I have nothing to lose since I'm going to fail anyway. That freedom allowed me to put together something good that I actually care about in a short time and pass my viva very easily, despite none of my new work being published. I wrote about this here https://yousefamar.com/memo/notes/my/views/phds/ My number one bit of advice would be to stop listening to your advisors and do what you think is right. They're the ones that led you to the point of no hope despite your efforts. If I could go back in time, I would go straight to founding instead of wasting some of my sharpest years on a PhD, but I would never have known that had I not done it. reply Cheer2171 19 hours agoprevA problem with marriage during the Phd is the inevitable two body problem. Academia privileges being itinerant. One reason I quit my PhD program for industry was because I saw how those ahead of me who wanted to stay in the same region were seen as failures or not serious. I was excited for my friend who got a postdoc in another prof's lab in the same department, who didn't have to move across the country with his wife and newborn child. And my advisor taught me to pity him instead, \"he could have gone to MIT if he applied, but now everyone will assume he couldn't get a postdoc anywhere else\". If you \"win\" then you have to move for a postdoc, then to \"win\" again means moving again for a faculty position. reply YeGoblynQueenne 19 hours agoparentThat kind of thinking is such self-defeating misery. If you have something to contribute, it doesn't matter where you go. You go to the University of Nowhere In Particular and make it a center of the new hot subject in research. If you can't do that, then it doesn't matter if you go to MIT or TIM or whatever, because that's just a way to earn renown from the work of others, rather than your own. reply lostdog 15 hours agorootparentMost people become closer to the average of their colleagues. Very very few people are so strong by themselves that they can raise the level of a place just by being there individually. reply aleph_minus_one 14 hours agorootparent> Very very few people are so strong by themselves that they can raise the level of a place just by being there individually. If you are not at least a person who might have this property, you likely aren't PhD material. reply Ar-Curunir 14 hours agorootparentEverybody at the target institution also has a PhD; that’s the minimum bar to cross. reply dmarchand90 20 hours agoprevI did a phd and really enjoyed it, a lot of my friends did as well and hated it. Here are some of my thoughts on the matter: Note this advice is only if you're average or feel average. If you're a superstar and you know it please disregard (but in that case you're probably not reading this anyway). 1) I had a professor who was very strong in the field and could point me to high impact work and steer me clear of useless activities. 2) I got a good stipend. I do not recommend borrowing money or living wretchedly for a phd. 3) My professor hit a good balance between pushing me to work harder and puling back when I felt I was going to hard. 4) I avoided doing much work as a TA as much as possible. I did the minimum amount that delivered reasonable value to the class and the students. 5) I avoided working weekends and the evenings. Conversely I put a lot of pressure on myself to do work during work hours. 6) I would occasionally work holidays and weekends (this only applies to European phds which get 5+ weeks of vacation. Do not go below three weeks vacation. ) 7) I did not try to be \"a hero \" I didn't do crazy ideas without discussing with my professor first. I didn't take more than the minimum amount of classes and select material that seemed extremely relevant. 8) I worked a job first (2 years). This made me really appreciate my phd a lot more and gave me a lot of much needed time management and interpersonal work skills. 9) i avoided reinventing the wheel at all costs. 10) I learned to say no and said no often 11) I always yes to social activities 12) I did hiking on the weekend and running during the week. 13) I prepared for a non academic career often. 14) on special occasions I would disregard all the above and work really hard on something. I cannot say how often this happens and it's kind of a spiritual question. Probably no more then 3-4 times a year is sustainable and sometimes not even every year. My goal the whole time was to be a 'forgettable' student. Forgettable in that I tried to avoid being memorably good and avoided being memorably bad. Of course there were genius peers I worked with and worked the weekends and evenings. I think this was right for them as the act gave them joy and they were producing great results. Conversely some people ground too hard and still didn't have very much to show. If in doubt and you're freaking out do less. If you're not in doubt and you're getting complacent do more. reply dmarchand90 19 hours agoparentSome other things: 15) I made time to do whatever impulsive thing my mind felt like doing in my free time. I love video games and after a decade avoiding them because I felt like they were a waste of time, I got back into them. 16) I still had total freak outs from time to time. I vividly remember googling plumbing classes at community College. (Nothing wrong with plumbing! I bet for 10% of readers the right answer is to drop out of your phd and go into trades) 17) try not to get drunk too often or high too often. A bit on Friday is ok (you need to self monitor) reply oneepic 16 hours agorootparentI understand this is a bit odd to mention, but it's kinda cool seeing (15) made the list. It's a love-hate thing for me. reply ghaff 19 hours agoparentprev>I worked a job first (2 years). This made me really appreciate my phd a lot more and gave me a lot of much needed time management and interpersonal work skills. I went back to school after a few years to get an MBA. I've never actually managed anyone to this day but it was sort of a prereq for a lot of the types of jobs I was interested in at the time. It helped that the coursework was easier for me than the engineering degrees I had. But I think there was also discipline and process that came from having been in the working world that definitely helped me do really well in the program. reply dmarchand90 19 hours agorootparentYes yes yes. Personally I don't recommend a phd without work experience reply ghaff 18 hours agorootparentIt can probably also help with consciously deciding you want to spend a few more years in academia as opposed to just naturally sliding into spending a bunch more years in school as the path of least resistance. reply bglazer 19 hours agoparentprevI’m finishing a phd now and this is very good advice, especially point number 1. There are a lot of good, technically fascinating ideas out there. Only a subset of those are interesting to other researchers in your field. As an early career researcher, the first priority is finding projects that other people will care about. It’s very very important to find an advisor who knows what others care about. Science is inherently social. reply Two4 16 hours agoprevI have a friend who coaches all kinds of postgrads through their theses/treatises (masters all the way to postdoc) - the number one thing that keeps her in business is shitty supervisors and the soul crushing grind of academia. PhDs (and masters theses) are primarily about project management, with time, money and morale being the primary resources dealt with. Almost invariably, morale runs out first, and then time, because the work isn't being done. Managing morale is as important as managing your time and money, and supervisors almost never impart that skill. reply pbj1968 16 hours agoprevGo get a PhD and then spend your prime years of family making as a post doc with a $60k salary. It’s ludicrous and the system only works if your family is wealthy or your spouse has a better paying job. reply sieste 15 hours agoparentphd and academia is a terrible career choice of you want to be wealthy. even most \"rich professors\" could make much more in industry. this career only makes sense once you factor in the value of working on your own interesting and diverse problems, being surrounded by curious and intelligent people, while enjoying the freedom of developing your research into whatever direction you please. and this starts as a phd student. if, as an academic, you ever feel like you don't have most of these benefits, while not making enough money to get by, you should leave. reply pbj1968 8 hours agorootparentYeah, that’s a nice load of bibble babble “we punish you for doing things you find interesting” crap. I didn’t say “make them rich” I am arguing that we need to make this financially viable if we expect our objective best to pursue it. $80k? $90k? Those would be a good start. reply Upvoter33 18 hours agoprevThis was a pretty wise article from a young person. The PhD path can be tough, and there are always low points, even for people who are doing well. The one thing I always thought about was the old joke about medical school. Q: \"What do they call the person who graduates medical school with the lowest GPA in the class? A: \"A doctor\" reply JoeAltmaier 18 hours agoparentSame in the military. \"What do the sergeants call the last-place graduate of West Point? Sir.\" reply bsdpufferfish 19 hours agoprev\"have no hope\" actually means someone at the department refuses to sign your papers. I have never been treated with so little respect in industry. reply runeblaze 16 hours agoprevJust want to put out here that being neurodivergent is an easy route to lose hope. Most people / professors are neurotypical and they guide you through their neurotypical lens, as they have previously found success doing. reply bckr 19 hours agoprevI love this advice for activities outside academia as well. Solve the problems in front of you that you can make progress on. Don’t spend time trying to weigh the value of the problem based on perceived prestige. reply javajosh 19 hours agoprevI've often felt that there is something inherently unwholesome about PhDs and the modern academic effort. As if we could create research factories, and that the quality and quantity of discoveries would scale with input (money). I'm not a historian of science, but I do know that many of our greatest scientists were not professional researchers, doing the work essentially in their spare time on the side (Einstein, Newton) or more typically, because of the support of a wealthy patron. Contributors self-selected for passion and talent mixed with a juicy problem. It makes sense to park people with passion and talent but NOT a juicy problem into a teaching position until that problem ripens. But instead you get careerist, low-impact papers - even negative-impact papers if you taken into account the impact on signal-to-noise ratio such papers have. Consider that one of the more important papers in CS in the last 20 years was \"Attention is all you need\", which was a side-effect of the authors day jobs. Or the many contributions of open-source which are 99% hobbyists. There is some sort of perversion that happens when you \"fund research\" - it removes the spontaneity and changes the selection criteria from something \"natural\" (passion and time and results) to something \"synthetic\" (grant application). It would seem to me a more sensible approach to become a car mechanic or programmer and do your original science on the side. reply bglazer 19 hours agoparentIt’s telling that your examples are theoretical physics, math, and computer science. You should visit a lab that does actual experiments. I don’t think anyone is doing cryo electron microscopy in their spare time. Most science just takes a lot of time and money. No way around it. Also, most research is incremental, and thats’s totally fine. People lionize the “great men” like Einstein who have these earth-shattering discoveries, but those aren’t really worth anything without the decades of exploration and hundreds of scientists testing theories and figuring out their consequences and implications. reply dmarchand90 19 hours agoparentprevI mean option one is always going to be there. I'm sure there are plenty of neo-aristocrats who can do science if they want. The problem, honestly, is an ancient one. In plato's republic he complains about a lack of state funding for geometry : \" Then take a step backward, for we are out of order, and insert the third dimension which is of solids, after the second which is of planes, and then you may proceed to solids in motion. But solid geometry is not popular and has not the patronage of the State, nor is the use of it fully recognized; the difficulty is great, and the votaries of the study are conceited and impatient. Still the charm of the pursuit wins upon men, and, if government would lend a little assistance, there might be great progress made. \" https://www.gutenberg.org/files/1497/1497-h/1497-h.htm reply Ar-Curunir 14 hours agoparentprevEinstein had a PhD… reply quantum_state 19 hours agoprevLike everything else, it’s a journey of exploration combining idealism and pragmatism with curiosity at the core. As some of the humble examples Feynman provided, when one digs into them, one would see the connection, intrigued and enlightened … and it would become an enjoyable and fruitful journey … reply bradrn 19 hours agoprevKey sentence (quoted from a letter of Feynman’s): > The worthwhile problems are the ones you can really solve or help solve, the ones you can really contribute something to. A problem is grand in science if it lies before us unsolved and we see some way for us to make some headway into it. reply photochemsyn 18 hours agoprevA PhD program is an investment of years of your life with an uncertain outcome - and as with other investments, due diligence matters. Researching the program you're entering and the PI you'll be working for (not 'with', for) are critically important steps. When interviewing for a lab, pay particular attention to their anti-fraud procedures (data retention, lab notebook policy, etc.). Read everything they've published - is it coherent, or obfuscated? Secondly, you have to have an income, meaning a stipend that allows you to lead a normal-ish life, and if the only way you can get paid is to teach classes, then your time for doing research will be cut in half. Likewise a decent PI will be looking for certain characteristics that make investing in you a good idea - intelligence, diligence, tenacity, etc. The vast majority of grad students won't publish anything very significant as a grad student, so don't worry about that so much. Something like 75% of published papers don't get a citation count > 10, and that's often just the PI citing their own umbrella of work. This may appear like wasteful use of resources, but the significant research can be so valuable to society and industry that even a 5% success rate is worth the societal investment. As far as careers, social networking is the academic game, but skill development is what counts more in industry, and most grad students don't go into academic research careers (as production of PhDs >> open academic positions). Teaching skills are undervalued in the USA, incidentally. P.S. being happy half the time is good enough; trying to be happy all the time tends towards manic behavior, and often eventual nervous breakdown. reply eslaught 19 hours agoprevI had my own \"oh shit\" moment in my PhD, which I wrote about among other things, here: https://elliottslaughter.com/2024/02/legion-paper-history I'll just add one more comment: it's entirely possible to feel like everything you're doing is worthless, when in fact you are on the brink of accomplishing something worthwhile. That's not to say that every research project is worthwhile, but \"how I feel right now\" is maybe not the best indicator of how you are actually doing. reply xqcgrek2 18 hours agoprevUnfortunately, these days science (to get jobs and research money) is all about hype, networking, or big collaborations. Very few people are trying to do work that is not incremental or something without hype potential in the next 3 years. reply epstein 18 hours agoprevEasy. Just redefine hope. Problem solved. reply Joel_Mckay 16 hours agoprev [–] Depends on the faculty, but in most cases you are still indentured unless fully sponsored with a grant or bursary. Most people project their own esoteric interests on what they think the world should care about, but almost all are wrong most of the time. Research institutions are like hermit-crab populations, and \"science advances one funeral at a time\" as some observed... One finds competitive sociopaths tend to do very well in such political climates. Ones instincts likely tell you my opinions must be ill informed, but that was also predictable given the decades of training. ;-) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experience of overcoming challenges while pursuing their PhD, drawing inspiration from Richard Hamming and Richard Feynman.",
      "Hamming and Feynman highlight the significance of working on practical, meaningful issues, emphasizing finding joy in solving problems, no matter how small.",
      "The central message is to persist through difficult times, derive satisfaction from the research process, and contribute to the advancement of knowledge."
    ],
    "commentSummary": [
      "The text discusses challenges facing PhD students, including aligning research with trends, daily academic grind, and balancing breakthroughs with routine work.",
      "It explores the impact of financial constraints, benefits & challenges of marriage, self-awareness in career choices, and the role of relationships in academic success.",
      "Additionally, it highlights the complexity of personal relationships, the value of a supportive spouse during a PhD, and the challenges of a career in academia focusing on research quality and financial sustainability."
    ],
    "points": 157,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1711890720
  }
]
