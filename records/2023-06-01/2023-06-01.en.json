[
  {
    "id": 36141083,
    "timestamp": 1685554239,
    "title": "Had a call with Reddit to discuss pricing",
    "url": "https://old.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/",
    "hn_url": "http://news.ycombinator.com/item?id=36141083",
    "content": "Reddit's proposed pricing for its API threatens the future of third-party apps like Apollo, which will be forced to pay $20 million per year to keep running. In response, Apollo users, including software engineers, call for Reddit to revise the pricing to a \"reasonable price.\" Some users theorize that the expensive API pricing is an attempt to kill third-party apps to force users to use",
    "summary": "- Reddit is proposing a high pricing structure for its API, which would hinder the future of third-party apps like Apollo. \n- Third-party users are requesting Reddit to revise its pricing plan to a reasonable amount. \n- Some Apollo users speculate that Reddit is attempting to eliminate third-party apps and have users solely use their platform instead.",
    "hn_title": "Had a call with Reddit to discuss pricing",
    "original_title": "Had a call with Reddit to discuss pricing",
    "score": 1752,
    "hn_content": "Reddit users are considering developing their social media platform to avoid censorship, with many citing the high cost of awards, moderation inaction, and potential censorship as reasons to leave the platform. While creating a new platform seems daunting, Reddit users are motivated by the example of an older internet forum that fell out of favour after a third-party mobile application was blocked. A new forum was developed with similar features, and it proved popular with users, drawing traffic because of its convenience and the popularity of the original application. While some Reddit users push for development of a new platform, others disagree and argue that the platform should be reformed instead of being abandoned.Users discuss potential alternatives to Reddit, including Lemmy.ml and Tildes, due to concerns about Reddit's recent pricing changes and moderation issues. Some suggest co-opting third-party clients for a new platform, while others express doubts about the feasibility of moving an existing community. Many emphasize the importance of good moderation, with some suggesting the use of AI for content filtering. Several users reminisce about the Digg to Reddit exodus and express frustration with online platforms that prioritize monetization over community building.A conversation on HN discussed alternatives to Reddit and how to create a text storage and delivery platform. The discussion highlighted the excessive complexity in tech and how simpler solutions can work best. The conversation discussed how many popular platforms have a \"life cycle\" where they start with new and cheaper prices before the prices get dramatically increased as the platform expands. The participants warned that Reddit may eventually become too expensive for small users, and alternative options like Apollo or LemmyNet may become more attractive. Finally, the discussion mentioned how Reddit's value is in its user-generated content, which should be viewed as a \"commons\" that deserves protections.Reddit's value lies in controlling a cultural meeting point rather than owning a server farm, making aggressive control of the meeting point rent-seeking. Network effects make it difficult to coordinate migration to an alternative platform, and Reddit's users might move to an alternative. Charging for valuable services is not rent-seeking, and third-party clients like Apollo are viable options on Reddit. The move to ban apps could be an attempt to price out third-party clients. People may move on to better platforms, such as Tildes.net. Toxicity and greed are present in the community and fan base surrounding Apollo, and Google Maps may have been the first to implement the idea of \"API EEE.\"Reddit and other platforms have restricted their APIs and increased pricing structures due to high volume APIs used by LLM platforms and third-party client restrictions. This is part of a corporate consolidation and monetization step that aims to generate big revenue. Discord presents a possible alternative to Reddit, but has discoverability problems and lacks a hierarchical reply/conversation tree. Apollo is seen as an alternative backend option for Reddit, but its limitations include being developed by a single developer. The supply and demand graphs that can be used to calculate price elasticity of demand are not a reliable indication of real-world scenarios.- There is a discussion on a potential new backend that would need to support tens of thousands of users.\n- Some users suggest that Apollo dev could secure enough VC to take down Reddit.\n- Little to no innovation and competition exist in social media that are text-based.\n- Text-based social media may not be as profitable as other media.\n- Reddit's subreddits are a key feature, but custom feeds like BlueSky could be implemented.\n- Third-party apps can be more valuable than ad-watching users.\n- Reddit's UI and native application are considered bad by users.\n- Sustainable online communities are needed.Popular social media platform Reddit is being criticized for its intrusive ads.\nSome users suggest Reddit should switch to a distributed system like Aether or a community-run centralized system like Archive of Our Own.\nHowever, these suggestions face hostility and hate from Reddit users who want everything they find offensive to be banned.\nIn other news, applications are now open for YC Summer 2023.",
    "hn_summary": "- Some Reddit users want to develop a new social media platform due to concerns about high costs, moderation, and censorship.\n- Alternatives to Reddit, such as Lemmy.ml and Tildes, are discussed, and the importance of good moderation is emphasized.\n- Monetization over community building and the pricing structure of APIs are criticized, and third-party clients like Apollo are seen as viable options."
  },
  {
    "id": 36136819,
    "timestamp": 1685530574,
    "title": "Rarbg Is No More",
    "url": "https://web.archive.org/web/20230531105653/https://rarbg.to/index80.php",
    "hn_url": "http://news.ycombinator.com/item?id=36136819",
    "content": "Hello guys,We would like to inform you that we have decided to shut down our site.The past 2 years have been very difficult for us - some of the people in our team died due to covid complications,others still suffer the side effects of it - not being able to work at all.Some are also fighting the war in Europe - ON BOTH SIDES.Also, the power price increase in data centers in Europe hit us pretty hard.Inflation makes our daily expenses impossible to bare.Therefore we can no longer run this site without massive expenses that we can no longer cover out of pocket.After an unanimous vote we've decided that we can no longer do it.We are sorry :(Bye",
    "summary": "- Rarbg, a popular torrent site, has announced its decision to shut down due to various reasons including COVID-19 complications, power price increase in data centers, inflation, and team members fighting in Europe.\n- The announcement states that the decision was unanimous, and the team can no longer bear the expenses of running the site.\n- Rarbg expressed regret and apologized for the decision to shut down.",
    "hn_title": "Rarbg Is No More",
    "original_title": "Rarbg Is No More",
    "score": 1226,
    "hn_content": "The popular torrent site Rarbg has been shut down. The comments section on Hacker News is filled with speculations about running illegal operations online and how to avoid getting caught. A lawyer points out that tracking down illegal activities via the court system is a time-consuming and expensive process. The discussion also covers topics such as the weak links in ad companies and payment processors, the possibility of using legal insurance, and Google's relationship to pirate content. Many users note that remaining anonymous online is feasible as long as individuals keep a low profile and avoid drawing attention to themselves.Google has removed Pirate Bay torrents from its search results following a \"legal request submitted to Google.\" In response, some users on the Hacker News comment section argue that Google is likely to be unaffected by future legal cases, while others argue that Google can only remove specific links rather than entire sites. Others posit that the issue of piracy is a civil law issue rather than a criminal one, meaning that citizens are only at risk of lawsuits if copyright owners initiate them. While some users suggest that this could change in the future, others contend that intellectual property is treated differently in different jurisdictions. Finally, some users discuss the technical details of piracy, including magnet links and the difficulties involved in authenticating users who download content illegally.No meaningful content to summarize.This thread discusses the use of cryptocurrency to circumvent local laws and its potential use in decentralized curation for torrents. The use of blockchain technology is debated, with some suggesting it is experimental engineering and others proposing it as an interesting application. The importance of curation in torrent websites is highlighted, with concerns raised about the presence of fake, illegal, and low-quality content. Suggestions for decentralized curation include a straw-man proposal for trust networks and the use of seeding as a measure of quality. However, concerns are raised about the difficulty of supporting anonymous and distributed systems and the need for some centralization in curation. Overall, the thread highlights the potential of cryptocurrencies and blockchain technology in facilitating complex financial infrastructure and improving security in casual economic activity.The comments section discusses various aspects of decentralized search engines and torrent sharing platforms. The possible methods of curating content and ensuring its legitimacy are touched upon, but the most emphasized aspect is the importance of the number of seeders for the viability of content. Different decentralized approaches to search exist, but there seems to be a lack of public search engines for this. Some commenters give out alternative torrent sites, public and private, for movies and TV shows. Overall, the comments section serves as an informative discussion on the intricacies of torrent sharing and decentralized search engines.A discussion on whether torrent site owners should release their sites as a torrent file itself. The suggestion is that it would bring costs down to zero while still allowing self-updating. A debate ensues in the comments discussing the ethics of site attribution, removal of files added to torrent listings, and the motives behind releasing sites as torrents. The most common reasons given for not releasing sites in this manner are legal battles, loss of revenue from ads, and fear of prosecution. Some examples are provided where certain sites have added value-added work to their releases, such as HEVC/H.265 encoding, and more.",
    "hn_summary": "- Rarbg has been shut down, and the discussion on Hacker News speculates about illegal activities and how to avoid getting caught.\n- The thread highlights the potential of cryptocurrencies and blockchain technology in facilitating complex financial infrastructure and improving security in casual economic activity for torrent sharing.\n- Decentralized search engines and torrent sharing platforms are discussed, mainly the importance of the number of seeders for the viability of content. Different decentralized approaches to search exist, but there seems to be a lack of public search engines for this."
  },
  {
    "id": 36138304,
    "timestamp": 1685540515,
    "title": "Slide to Unlock",
    "url": "https://cs.uwaterloo.ca/~csk/slide/",
    "hn_url": "http://news.ycombinator.com/item?id=36138304",
    "content": "Reset",
    "summary": "- This website is best experienced by visiting the website directly.\n- Please visit the website to read the article.",
    "hn_title": "Slide to Unlock",
    "original_title": "Slide to Unlock",
    "score": 1007,
    "hn_content": "The article discusses a feature on Huawei phones called \"AI Touch,\" which allows users to search and purchase products by touching an image with two fingers, and how it poses a risk for accidental purchases. The conversation then shifts towards the use of analytics to design user interfaces, and how toddlers unintentionally contribute to this by influencing their parents' usage. The article also touches upon the power of women's purchasing decisions and the popularity of features like image recognition shopping, especially in East Asia. Some comments mention the potential for data collection and the creation of social credit profiles through these features. Lastly, there is discussion of other apps with similar image recognition shopping capabilities.A game called \"Slide\" is causing frustration among players due to its dexterity challenge and unusual design, resembling an inverse slider puzzle with multi-touch. The game progresses to four fingers and players must move fingers in a designated path to progress. The game resembling Twister or Operation is also drawing attention to the designer, Craig Kaplan. Some of the frustration is due to the game significantly challenging players' fine motor control, while others feel it highlights the mobile UX frustrations they already experience. There are 31 levels, and many commenters suggest their phone are too small or lacking tactile precision for this game.A multi-touch digitizer test game has been developed that challenges users to keep multiple fingers within lines. Players must maneuver their device to complete each level's puzzle, making the game both a physical and digital challenge. The game can be frustrating but intriguing, with some finding it enjoyable while others found it anxiety-inducing. Users recommend creative initial positioning for managing four fingers and suggest using two hands to complete easier levels. Many expressed their enjoyment of the innovative and unique game. There are also comparisons to other games, such as \"The Witness,\" and some suggest potential for using the game as a CAPTCHA system.",
    "hn_summary": "- Huawei phones have a feature called \"AI Touch\" that allows users to search and purchase products by touching an image with two fingers.\n- Analytics are used to design user interfaces, sometimes influenced by toddlers, with potential for data collection and social credit profiles.\n- A game called \"Slide\" challenges fine motor control with multi-touch features and frustratingly small phone sizes."
  },
  {
    "id": 36134249,
    "timestamp": 1685504818,
    "title": "Ask HN: Is it just me or GPT-4's quality has significantly deteriorated lately?",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=36134249",
    "content": "",
    "summary": "- This website is best experienced by visiting the website directly.\n- Please visit the website to read the article.",
    "hn_title": "Ask HN: Is it just me or GPT-4's quality has significantly deteriorated lately?",
    "original_title": "Ask HN: Is it just me or GPT-4's quality has significantly deteriorated lately?",
    "score": 886,
    "hn_content": "Users on Hacker News have raised concerns about the deteriorating quality of OpenAI's GPT-4 natural language processing model, which appears to be generating lower quality responses with more buggy code. There is speculation that a corporate layer has been added to the model to prevent it from performing certain tasks. Critics are calling for an open-source \"unaligned\" version of GPT-4 to be created to prevent corporate biases from impacting the AI. However, there are concerns raised around scalability, as the model is estimated to have almost one trillion parameters, requiring vast amounts of computing power and storage. Additionally, some suggest that the model's problems could be due to scaling issues rather than conspiracy theories. There are also discussions around the potential for crowd-sourcing the model's training.\nOpenAI's GPT-4 has been criticized for its instability and nerfing of capabilities. Some users are calling for OpenAI to provide a way to pin specific model versions. The concern arises about GPT-4's stability and reliability, as the API only offers two default models with little transparency into changes made. Users suggest that the API's instability could hinder those who rely on it for business or scientific purposes. Additionally, there is criticism for OpenAI chairs, who have intelligence agency backgrounds, and some believe they may have submitted their \"recommendations\" to OpenAI.Developments regarding OpenAI's latest GPT-3.5 and its upcoming GPT-4 models have raised concerns about nuanced limitations on certain tasks due to a token limit. Some users have reported a decrease in the quality of output with certain plugins and browsing enabled, while others suggest that OpenAI may be 'nerfing' models to reduce costs. There are also concerns about the lack of dark content and restrictions on certain complex tasks. PDF parsing, for instance, is a complex task due to the ambiguity of the format specification, and the issue of relying on Adobe's proprietary extensions. While the models show promise, it remains to be seen how OpenAI will address these challenges moving forward.The comments section discusses the limitations and biases of LLMs, particularly GPT-4. Some argue that the biases are a result of the data fed into the model, while others suggest that the model's objective is to prioritize the most likely response. There is also speculation about Microsoft's role in influencing the development and capabilities of ChatGPT and Copilot. Overall, the comments highlight the importance of addressing biases in AI and the challenges of creating unbiased models.The discussion revolves around model bias in AI training materials and feedback. It highlights that AI reflects implicit bias and that there are prompt engineering tricks that can help reduce it. There are examples of how prompts can lead to undesirable patterns, showing the need for variation in training data. The post also mentions the debate on whether AI models should be \"neutered\" to the point where their output is on par with open-source. The discussion is focused on the importance of fairness and bias mitigation, and how certain metrics might not always be accurate in determining them. A broad range of demographic and cultural contexts could affect the unknown unknowns, and producing a model that is detached from reality.The article is a discussion thread about the recent nerfing of OpenAI's language models for safety concerns. Participants debate the implications of this censorship and speculate as to potential reasons, such as cost or concerns about unwanted skills being taught. Some commentators express disappointment in the quality of the models post-nerfing, while others praise alternative models like Bard. However, there is no new or exciting development or release mentioned in the article.No meaningful content to provide a summary for.",
    "hn_summary": "- Users are concerned about a decline in quality and increased bugs with OpenAI's GPT-4 language processing model.\n- Critics are calling for an open-source \"unaligned\" version of GPT-4 to prevent corporate biases.\n- Discussions around scalability, potential for crowd-sourcing training, and limitations of LLMs."
  },
  {
    "id": 36139852,
    "timestamp": 1685548718,
    "title": "The greatest risk of AI is from the people who control it, not the tech itself",
    "url": "https://aisnakeoil.substack.com/p/is-avoiding-extinction-from-ai-really",
    "hn_url": "http://news.ycombinator.com/item?id=36139852",
    "content": "Discover more from AI Snake OilWhat makes AI click, what makes it fail, and how to tell the differenceOver 9,000 subscribersSubscribeContinue readingSign inIs Avoiding Extinction from AI Really an Urgent Priority?The history of technology suggests that the greatest risks come not from the tech, but from the people who control itARVIND NARAYANANMAY 31, 2023549ShareBy Seth Lazar, Jeremy Howard, and Arvind Narayanan.This is the year extinction risk from AI went mainstream. It has featured in leading publications, been invoked by 10 Downing Street, and mentioned in a White House AI Strategy document. But a powerful group of AI technologists thinks it still isn\u2019t being taken seriously enough. They have signed a statement that claims: \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\u201d\u201cGlobal priorities\u201d should be the most important, and urgent, problems that humanity faces. 2023 has seen a leap forward in AI capabilities, which undoubtedly brings new risks, including perhaps increasing the probability that some future AI system will go rogue and wipe out humanity. But we are not convinced that mitigating this risk is a global priority. Other AI risks are as important, and are much more urgent.Start with the focus on risks from AI. This is an ambiguous phrase, but it implies an autonomous rogue agent. What about risks posed by people who negligently, recklessly, or maliciously use AI systems? Whatever harms we are concerned might be possible from a rogue AI will be far more likely at a much earlier stage as a result of a \u201crogue human\u201d with AI\u2019s assistance.Indeed, focusing on this particular threat might exacerbate the more likely risks. The history of technology to date suggests that the greatest risks come not from technology itself, but from the people who control the technology using it to accumulate power and wealth. The AI industry leaders who have signed this statement are precisely the people best positioned to do just that. And in calling for regulations to address the risks of future rogue AI systems, they have proposed interventions that would further cement their power. We should be wary of Prometheans who want to both profit from bringing the people fire, and be trusted as the firefighters.And why focus on extinction in particular? Bad as it would be, as the preamble to the statement notes AI poses other serious societal-scale risks. And global priorities should be not only important, but urgent. We\u2019re still in the middle of a global pandemic, and Russian aggression in Ukraine has made nuclear war an imminent threat. Catastrophic climate change, not mentioned in the statement, has very likely already begun. Is the threat of extinction from AI equally pressing? Do the signatories believe that existing AI systems or their immediate successors might wipe us all out? If they do, then the industry leaders signing this statement should immediately shut down their data centres and hand everything over to national governments. The researchers should stop trying to make existing AI systems safe, and instead call for their elimination.We think that, in fact, most signatories to the statement believe that runaway AI is a way off yet, and that it will take a significant scientific advance to get there\u2014one that we cannot anticipate, even if we are confident that it will someday occur. If this is so, then at least two things follow.First, we should give more weight to serious risks from AI that are more urgent. Even if existing AI systems and their plausible extensions won\u2019t wipe us out, they are already causing much more concentrated harm, they are sure to exacerbate inequality and, in the hands of power-hungry governments and unscrupulous corporations, will undermine individual and collective freedom. We can mitigate these risks now\u2014we don\u2019t have to wait for some unpredictable scientific advance to make progress. They should be our priority. After all, why would we have any confidence in our ability to address risks from future AI, if we won\u2019t do the hard work of addressing those that are already with us?Second, instead of alarming the public with ambiguous projections about the future of AI, we should focus less on what we should worry about, and more on what we should do. The possibly extreme risks from future AI systems should be part of that conversation, but they should not dominate it. We should start by acknowledging that the future of AI\u2014perhaps more so than of pandemics, nuclear war, and climate change\u2014is fundamentally within our collective control. We need to ask, now, what kind of future we want that to be. This doesn\u2019t just mean soliciting input on what rules god-like AI should be governed by. It means asking whether there is, anywhere, a democratic majority for creating such systems at all.And we should focus on building institutions that both reduce existing AI risks and put us in a robust position to address new ones as we learn more about them. This definitely means applying the precautionary principle, and taking concrete steps where we can to anticipate as yet unrealised risks. But it also means empowering voices and groups underrepresented on this AI power list\u2014many of whom have long been drawing attention to societal-scale risks of AI without receiving so much attention. Building on their work, let\u2019s focus on the things we can study, understand and control\u2014the design and real-world use of existing AI systems, their immediate successors, and the social and political systems of which they are part.You\u2019re reading AI Snake Oil, a blog about our upcoming book. Subscribe to get new posts.Subscribe54 Likes\u00b78 Restacks549Share",
    "summary": "- A group of AI technologists signed a statement that mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n- While risks from AI exist, the focus should be on the people who control it rather than the technology itself, as the greatest risks historically come from those who control technology using it to accumulate power and wealth.\n- The urgent priority of AI risks should be mitigating the more likely risks posed by people negligently, recklessly or maliciously using AI systems, exacerbating inequality and undermining individual and collective freedom.",
    "hn_title": "The greatest risk of AI is from the people who control it, not the tech itself",
    "original_title": "The greatest risk of AI is from the people who control it, not the tech itself",
    "score": 579,
    "hn_content": "The greatest risk of AI is from the people who control it, not the tech itself, according to a post on aisnakeoil.substack.com. There is concern that AI is being used to power and make impactful decisions in high-risk infrastructure that people rely on, without significant controls in place, explainability/audibility, and the ability quickly reverse any bad decision. There is a worry that people are integrating this tech too quickly with everyday things, causing a real human safety problem. While AI has been identified as a safety issue, there is also a problem of those acting as doomsday cult leaders that encourage violence. However, many consider these individuals to be punchbags and uninteresting individuals.The debate around AI has sparked concerns about the potential dangers of artificial general intelligence (AGI) and its impact on human life. Some experts argue that the creation of AGI could lead to a malevolent AI that poses an existential threat to humanity. The counterargument dismisses this as sci-fi nonsense and a baseless assumption. The discussion focuses on the lack of understanding of the technology and its implications for the safety of human life. Despite the hype around AGI, some argue that the arguments presented lack clarity and technicality. The core argument is the possibility of creating ML models smarter than humans, the unknown danger of AI, and the ethical responsibility of controlling these machines. The AI-doomer worldview argues algorithms are being used to make decisions about what to show people, and these algorithms have been changed subtly over the years to maximize engagement. The people responsible for developing the algorithms are the same people whose irresponsible actions could maximize engagement until it blows up our entire culture.Debate on the potential danger of AI and future AGI. Some argue that current AI's quick progression indicates a short timeline towards dangerous AGI. Others disagree and claim that there is zero evidence for such claims. Many smart people have become increasingly convinced, but skeptics are dismissive. There is a risk of humans losing control if AI becomes a centralized entity with supreme power, which is more possible with the potential for scalable and effective technologies for monitoring and controlling humans. The danger is that this consolidation may result in world domination, a risk that was previously considered impossible. AI is also useful in making decisions more transparent. The risk of AI causing an existential threat is difficult to predict, but experts disagree on the possibility and timeline. Despite disagreements and varying perspectives, there is a general concern over the incredible advancement of AI that could potentially be dangerous.Unknown factors in AI alignment and intelligence raise concerns about potential hypothetical scenarios where hyperintelligent AI may create goals that necessitate human extinction. The unknowns involved make it impossible to say if and how this could happen or how to stop it. Premature regulation may not be ideal, but a measured response is needed to mitigate the potential existential threat that autonomous superhuman AGI poses. While we currently lack evidence for the creation of a new life form with godlike abilities, it is reasonable to believe in the possibility of superhuman AGI with the potential to threaten humanity. The concern is not limited to current technology but the potential for advancements to accelerate. The unknowns involved mean alarm is warranted, and we should try to prevent or control the development of dangerous AI before it's too late.The discussion in this post revolves around the potential threat of AI. Some argue that AI could pose an existential threat, while others believe it poses more of an economic one. While there is no concrete evidence to support the idea of a \"new life form\" with godlike abilities, the unpredictability of the future development of AI is a cause for concern. The discussion also touches upon the potential replacement of human labor with AI and the resulting socioeconomic disruption. It is suggested that the hype cycle surrounding AI may be affecting predictions, and more concrete reasoning is needed. Overall, the post raises questions about the future implications of AI and its potential impact on society.There is a debate on whether superhuman Artificial General Intelligence (AGI) is possible or not. While we have no scientific evidence that it is, most believe we are on the path to developing it. The main concern is the potential dangerous outcomes when we reach an arbitrary threshold where we lose control over AGI. There is a risk of AI being misused by businesses to make decisions that harm people, such as planning coercive disconnections to maximize profit from customers based on credit history. Regulations are necessary to prevent such abuse of power and ensure fairness and accountability. We should focus on identifying bad use cases for AI and regulating those specific actions and methods. Bias analysis tools could also be used to ensure that AI outputs are not biased and do not harm people.AI is not introducing more concern but allowing companies to do more intrusive background checks using machine learning algorithms like LLMs. The practice is already incorporated, and companies have been doing it long before the AI era. There has been an increasing potential for massive data abuse by big companies, which profit from it. Governments and large utilities may use AI to track every action of their citizens and customers, hence infringing on their privacy. A movement to regulate AI is gaining traction. A bipartisan congressional committee seems to agree with Sam Altman's recommendation to license GPUs beyond hobbyist capabilities. Large established companies like OpenAI may be monopolizing AI, reducing competition among developers.The comments discuss the potential risks of artificial intelligence (AI) and its potential to cause human extinction. Some argue that the risks are overstated, while others point out that there are unknown factors that could result in AI posing an existential threat. The discussion includes concerns about the regulation of AI and the difficulty in preventing its development. The potential for smaller groups of hackers to create something that could defeat regulatory regimes is highlighted. The possibility of using GPUs to train AI is also discussed, as is the idea that the hardware could be supplied by other chipmakers if necessary. Overall, the discussion focuses on the potential risks and challenges of creating AI without proper consideration of its potential consequences.Experts in AI have signed an open letter calling for regulation and awareness around the technology's risks, including doomsday scenarios of the \"extinction\" of humanity. Critics have derided the letter as misdirected, with the more pressing risks presented by AI being\u00a0the displacement of jobs and shifts in society. Others also claim that the calls for regulation would be shaped by corporations in their favor, adding that such risks are impossible to prevent.\u00a0\n- Recent calls for regulation in the tech industry are suspected by some to be anti-competitive attempts at regulatory capture.\n- There is a difference between shaping regulation to benefit oneself and engaging in bad faith.\n- OpenAI has signed a letter calling for a pause on training new big models, which some see as a concrete proposal for regulation, but others see as unrealistic.\n- A registration system for people training models has been proposed, but some see it as a potential avenue for regulatory capture.\n- It is unclear whether open source models pose a threat to companies like OpenAI, and no specific and realistic regulations have been proposed.\n- The development of AGI is likely to involve a decentralized code base that could be developed by individuals in their basements, making regulation and control difficult.",
    "hn_summary": "- The greatest risk of AI is from the people who control it, not the tech itself.\n- There is a concern that AI decisions are being made without significant controls in place, causing a real human safety problem.\n- The potential for superhuman AGI poses an existential threat to humanity, and concerns over its development and regulation are warranted."
  },
  {
    "id": 36133226,
    "timestamp": 1685494693,
    "title": "Nvidia DGX GH200: 100 Terabyte GPU Memory System",
    "url": "https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/",
    "hn_url": "http://news.ycombinator.com/item?id=36133226",
    "content": "DEVELOPERHomeBlogForumsDocsDownloadsTrainingJoinTechnical BlogFilterSubscribeData Center / CloudEnglish\u4e2d\u6587Announcing NVIDIA DGX GH200: The First 100 Terabyte GPU Memory SystemMay 28, 2023By Pradyumna Desale+60LikeDiscuss (0)At COMPUTEX 2023, NVIDIA announced NVIDIA DGX GH200, which marks another breakthrough in GPU-accelerated computing to power the most demanding giant AI workloads. In addition to describing critical aspects of the NVIDIA DGX GH200 architecture, this post discusses how NVIDIA Base Command enables rapid deployment, accelerates the onboarding of users, and simplifies system management.The unified memory programming model of GPUs has been the cornerstone of various breakthroughs in complex accelerated computing applications over the last 7 years. In 2016, NVIDIA introduced NVLink technology and the Unified Memory Programming model with CUDA-6, designed to increase the memory available to GPU-accelerated workloads. Since then, the core of every DGX system is a GPU complex on a baseboard interconnected with NVLink in which each GPU can access the other\u2019s memory at NVLink speed. Many such DGX with GPU complexes are interconnected with high-speed networking to form larger supercomputers such as the NVIDIA Selene supercomputer. Yet an emerging class of giant, trillion-parameter AI models will require either several months to train or cannot be solved even on today\u2019s best supercomputers. To empower the scientists in need of an advanced platform that can solve these extraordinary challenges, NVIDIA paired NVIDIA Grace Hopper Superchip with the NVLink Switch System, uniting up to 256 GPUs in an NVIDIA DGX GH200 system. In the DGX GH200 system, 144 terabytes of memory will be accessible to the GPU shared memory programming model at high speed over NVLink. Compared to a single NVIDIA DGX A100 320 GB system, NVIDIA DGX GH200 provides nearly 500x more memory to the GPU shared memory programming model over NVLink, forming a giant data center-sized GPU. NVIDIA DGX GH200 is the first supercomputer to break the 100-terabyte barrier for memory accessible to GPUs over NVLink.Figure 1. GPU memory gains as a result of NVLink progression NVIDIA DGX GH200 system architectureNVIDIA Grace Hopper Superchip and NVLink Switch System are the building blocks of NVIDIA DGX GH200 architecture. NVIDIA Grace Hopper Superchip combines the Grace and Hopper architectures using NVIDIA NVLink-C2C to deliver a CPU + GPU coherent memory model. The NVLink Switch System, powered by the fourth generation of NVLink technology, extends NVLink connection across superchips to create a seamless, high-bandwidth, multi-GPU system.Each NVIDIA Grace Hopper Superchip in NVIDIA DGX GH200 has 480 GB LPDDR5 CPU memory, at eighth of the power per GB, compared with DDR5 and 96 GB of fast HBM3. NVIDIA Grace CPU and Hopper GPU are interconnected with NVLink-C2C, providing 7x more bandwidth than PCIe Gen5 at one-fifth the power. NVLink Switch System forms a two-level, non-blocking, fat-tree NVLink fabric to fully connect 256 Grace Hopper Superchips in a DGX GH200 system. Every GPU in DGX GH200 can access the memory of other GPUs and extended GPU memory of all NVIDIA Grace CPUs at 900 GBps. Compute baseboards hosting Grace Hopper Superchips are connected to the NVLink Switch System using a custom cable harness for the first layer of NVLink fabric. LinkX cables extend the connectivity in the second layer of NVLink fabric. Figure 2. Topology of a fully connected NVIDIA NVLink Switch System across NVIDIA DGX GH200 consisting of 256 GPUsIn the DGX GH200 system, GPU threads can address peer HBM3 and LPDDR5X memory from other Grace Hopper Superchips in the NVLink network using an NVLink page table. NVIDIA Magnum IO acceleration libraries optimize GPU communications for efficiency, enhancing application scaling with all 256 GPUs. Every Grace Hopper Superchip in DGX GH200 is paired with one NVIDIA ConnectX-7 network adapter and one NVIDIA BlueField-3 NIC. The DGX GH200 has 128 TBps bi-section bandwidth and 230.4 TFLOPS of NVIDIA SHARP in-network computing to accelerate collective operations commonly used in AI and doubles the effective bandwidth of the NVLink Network System by reducing the communication overheads of collective operations.For scaling beyond 256 GPUs, ConnectX-7 adapters can interconnect multiple DGX GH200 systems to scale into an even larger solution. The power of BlueField-3 DPUs transforms any enterprise computing environment into a secure and accelerated virtual private cloud, enabling organizations to run application workloads in secure, multi-tenant environments.Target use cases and performance benefitsThe generational leap in GPU memory significantly improves the performance of AI and HPC applications bottlenecked by GPU memory size. Many mainstream AI and HPC workloads can reside entirely in the aggregate GPU memory of a single NVIDIA DGX H100. For such workloads, the DGX H100 is the most performance-efficient training solution.Other workloads\u2014such as a deep learning recommendation model (DLRM) with terabytes of embedded tables, a terabyte-scale graph neural network training model, or large data analytics workloads\u2014see speedups of 4x to 7x with DGX GH200. This shows that DGX GH200 is a better solution for the more advanced AI and HPC models requiring massive memory for GPU shared memory programming.The mechanics of speedup are described in detail in the NVIDIA Grace Hopper Superchip Architecture whitepaper.Figure 3. Performance comparisons for giant memory AI workloadsPurpose-designed for the most demanding workloadsEvery component throughout DGX GH200 is selected to minimize bottlenecks while maximizing network performance for key workloads and fully utilizing all scale-up hardware capabilities. The result is linear scalability and high utilization of the massive, shared memory space. To get the most out of this advanced system, NVIDIA also architected an extremely high-speed storage fabric to run at peak capacity and to handle a variety of data types (text, tabular data, audio, and video)\u2014in parallel and with unwavering performance. Full-stack NVIDIA solutionDGX GH200 comes with NVIDIA Base Command, which includes an OS optimized for AI workloads, cluster manager, libraries that accelerate compute, storage, and network infrastructure are optimized for DGX GH200 system architecture. DGX GH200 also includes NVIDIA AI Enterprise, providing a suite of software and frameworks optimized to streamline AI development and deployment. This full-stack solution enables customers to focus on innovation and worry less about managing their IT infrastructure.Figure 4. The NVIDIA DGX GH200 AI supercomputer full stack includes NVIDIA Base Command and NVIDIA AI EnterpriseSupercharge giant AI and HPC workloadsNVIDIA is working to make DGX GH200 available at the end of this year. NVIDIA is eager to provide this incredible first-of-its-kind supercomputer and empower you to innovate and pursue your passions in solving today\u2019s biggest AI and HPC challenges. Learn more.Related resourcesGTC session: Developer Breakout: Everything NVIDIA DGX Cloud (Spring 2023)GTC session: NVIDIA RTX 4000 SFF Ada Generation: Next-Level Performance for Compact Workstations (Spring 2023)GTC session: Unleash Lightning-Fast Storage for Unprecedented AI Efficiency and Performance (Presented by DDN) (Spring 2023)Webinar: NVIDIA A100 - Giant Leap in AI Computing TechnologyWebinar: Accelerating COVID-19 Research with NVIDIA DGX A100Webinar: The Expanding Universe of NVIDIA-Certified SystemsDiscuss (0)+60LikeTagsData Center / Cloud | Generative AI | Networking | Recommenders / Personalization | Simulation / Modeling / Design | Cloud Services | Financial Services | HPC / Scientific Computing | Public Sector | A100 | BlueField DPU | DGX | Grace CPU | NVLink | SmartNIC | 101 / Getting Started | News | AI Enterprise | Hopper | Supercomputing / ClusterAbout the AuthorsAbout Pradyumna DesaleAs a product lead in Enterprise Product Group at NVIDIA, Pradyumna Desale spearheads the scale up NVLink systems that leverage NVIDIA cutting-edge technology to address biggest customer challenges. He started his career at NVIDIA as a design engineer and later led a global engineering team that optimized the performance and power of high-speed IOs in NVIDIA GPUs and SoCs. He holds an MS degree in Electrical Engineering from Pennsylvania State University, State College and an MBA from University of California, Berkeley.View all posts by Pradyumna DesaleCommentsStart the discussion at forums.developer.nvidia.comRelated postsNVIDIA Hopper Architecture In-DepthDefining AI Innovation with NVIDIA DGX A100Introducing NVIDIA HGX A100: The Most Powerful Accelerated Server Platform for AI and High Performance ComputingScalable GPU-Accelerated Supercomputer in the Microsoft Azure CloudHGX-2 Fuses HPC and AI Computing ArchitecturesSign Up for NVIDIA Developer NewsSubscribeFollow NVIDIA DeveloperFacebookTwitterLinkedInInstagramYouTubeCopyright \u00a9 2023 NVIDIA CorporationLegal InformationTerms of UsePrivacy PolicyCookie PolicyContactNVIDIA uses cookies to deliver and improve the website experience. See ourCookie Policy to learn more.Cookies Settings Accept All Cookies",
    "summary": "- NVIDIA announced the DGX GH200, the first supercomputer with 100 terabytes of memory accessible to GPUs over NVLink, pairing NVIDIA Grace Hopper Superchip with the NVLink Switch System.\n- The DGX GH200 features a GPU complex baseboard interconnected with NVLink, with each GPU accessing other GPUs' memory at NVLink speed, and can form larger supercomputers.\n- The DGX GH200 is designed for the most demanding giant AI workloads, with potential speedups of 4x to 7x for deep learning recommendation models, terabyte-scale graph neural networks, or large data analytics workloads compared to the DGX A100.",
    "hn_title": "Nvidia DGX GH200: 100 Terabyte GPU Memory System",
    "original_title": "Nvidia DGX GH200: 100 Terabyte GPU Memory System",
    "score": 526,
    "hn_content": "NVIDIA has announced the DGX SuperPOD, a modular system that can scale up to 192 NVIDIA A100 Tensor Core GPUs with 15.36 petabytes of high-bandwidth memory for artificial intelligence. It connects individual compute nodes using NVIDIA\u2019s Mellanox-acquired HDR InfiniBand networking fabric at speeds of 640GB/s. The aim of NVIDIA's latest AI tool is to enable companies to get into deep learning rather than the increasingly difficult task of developing deep learning, helping to set up systems with clearly defined goals into which clusters of A100 GPUs can be plugged. The system is expected to be a more affordable solution to allow companies to explore AI solutions as well.The discussion centers around the idea of advertising products while mentioning similar products from other brands, either collaboratively or through random selection. The proposal is flawed and likely violates free speech laws in the US. There is skepticism around Google's TPU outcompeting Nvidia's AI hardware due to limited software availability and cost. The conversation also touches on regulating the ad industry to reduce persuasive messaging while maintaining informative content. There is doubt around Google's future success, but the innovation engine that created the company will likely continue driving future tech developments.A discussion on HN forum explores the future of tech giants like Alphabet, Microsoft, and their competitors, as well as their likelihood to fail. The comments highlight factors such as market share and sentiments, as well as current product offerings and diversification efforts. The conversation also touches on topics such as GPU architecture, workload parallelism, and wafer-scale integration computing. As a result, the thread gives insights into the current state of different tech industries and their possible directions.There is speculation about training an artificial intelligence model with 100T parameters, but there are challenges, such as the lack of sufficient data and the required computational power to train it. A rule of thumb is that 20 tokens per parameter are needed, and with a 100T model, it would require 10 petabytes of text data to train. The compute requirement for optimal model growth increases quadratically, and the training time for a 100T model would take 30k years with 1e18 FLOPs per second computational power. Despite the challenges, there is ongoing research to develop larger models with multi-modal input training, including audio and visual data. Nvidia's new Grace CPU has NVLink that connects GPUs vertically rather than using PCIe, providing higher bandwidth and more energy-efficient processing, contributing to more powerful systems for training such models.Nvidia's new supercomputer system, called Grace, features new GPU architecture designed for AI and high-performance computing. Grace is aimed to address the needs of the world's largest, most complex AI and HPC workloads. It comes with the new Arm-based Grace CPU/mezzanine board. The system is capable of transferring data over direct links that are faster than PCIe, via a network fabric from Mellanox -- the Israeli networking company that Nvidia recently acquired for $7 billion. Furthermore, Grace's high-speed interconnect technology will outperform CPU-based systems and greatly benefit AI researchers. The system offers sixteen A100 GPUs connected to one another and to CPUs through NVIDIA\u2019s third-generation NVLink interface, and through a high-performance network fabric based on InfiniBand. The system delivers a 10x improvement in performance over previous-generation systems.Summary:\n- Supercomputers differ from ordinary servers in terms of low latency network, high bandwidth network, parallel file system, few node types, and a software stack aware of topology and efficiency. \n- They have contributed to meaningful research in academic fields, astrophysics, molecular dynamics simulations, weather forecast, energy system simulations, and weapons research. \n- Modeling the human brain is still very far away from reality, and even modeling a single biological neuron has its difficulties. \n- The brain is different from the technology we use, as it is analog and chemical while AI is digital and silicon based. \n- The possibility of quantum effects in human consciousness is unlikely due to the brain's hot and dense environment.There is a discussion in the comments section about the meaning of \"Moore's Law\" and how it applies to density and performance in hardware. The conversation covers topics such as the potential end of Moore's Law, the limitations of vertical scaling, the benefits of smaller transistors, and the future of computing in relation to power consumption and computation per joule. One person suggests that modeling the human brain could result in more power-efficient computations. The commenters also discuss the limitations of analog-to-digital and digital-to-analog converters and how it pertains to large scale problems. There is mention of a recent release of GPUs with up to 480GB of memory, potentially offering improved performance. However, there is skepticism about the continued adherence to Huang's law, which predicts an increase in GPU performance. Overall, the conversation offers insight into technical and theoretical considerations related to the future of hardware and computing.- Limitations in memory and bandwidth have prevented servers with the latest graphics cards from competing in many machine learning (ML) tasks.\n- Despite this, developments in ML are pushing consumer GPUs to get closer to supercomputer GPUs in terms of tensor cores and higher memory.\n- The \"next gen\" of gaming could involve intelligent NPCs with realistic behavior, requiring more powerful GPUs.\n- Google's Project Starline offers an interactive display that feels like an open window, bringing us closer to the holodeck.\n- The post does not contain any news or releases, but rather a discussion around the current state and potential future of GPUs and their applications in ML and gaming.",
    "hn_summary": "- Nvidia has announced the DGX SuperPOD, a scalable system for AI with 192 A100 GPUs and 15.36 petabytes of memory, improving AI development ease and affordability.\n- There is discussion around ad regulations, Google's TPU being better than Nvidia's AI hardware, and the future of tech giants and their diversification efforts.\n- The limitations of training an AI model with 100T parameters have been explored, along with ongoing research for larger multi-modal input training models and Nvidia's new Grace CPU."
  },
  {
    "id": 36136179,
    "timestamp": 1685524297,
    "title": "Notes apps are where ideas go to die (2022)",
    "url": "https://www.reproof.app/blog/notes-apps-help-us-forget",
    "hn_url": "http://news.ycombinator.com/item?id=36136179",
    "content": "What\u2019s Reproof?BlogJoin our waitlistNotes apps are where ideas go to die. And that\u2019s good.Insurance for your mind.by Matthew Guay \u00b7 February 15, 2022We don\u2019t write things down to remember them. We write them down to forget.Like a hunter/gatherer stashing their prey, the ideas and the links we stumble upon feel valuable, rare, something worth saving. We ascribe value to the time we spend discovering things online. Surely that time wasn\u2019t in vain.Then we\u2019re burdened with our findings. It\u2019s tough to focus on something new when you\u2019re still holding the old in your mind.So we write things down. Bookmark them. Add them to our reading list. Highlight our findings. Make long lists and check them twice. We need a cave, a storehouse, somewhere to stash our findings.Sherlock Holmes, in BBC\u2019s rendition, builds a fabled mind palace, an imaginary castle in which to stash his clues and concepts for later recall. Mere mortals with our average powers of recollection turn instead to notes and bookmarking apps, with their promises to be our \u201csecond brain\u201d and help us \u201cremember everything.\u201dAnd they do, for a time. You think of something, write it down, and feel free. Find something else, bookmark it, and close the tab without worry. If you need that discovery again, it\u2019s only a few taps away. The placebo effect\u2014or, at least, the new app effect\u2014is real.By letting go, you\u2019ve cleared up space for new quests. No more dozens of tabs open forever; you saved them, then let them go back into the ether. No perpetual thinking on an idea; you wrote it down, let your second brain remember for you.Then we\u2019re free. We\u2019ve stalked the prey, secured it for later nourishment. We can safely forget. We\u2019ve insured against faulty memories. Now on to the next quest, finding something new to stash.That's the true value of notebooks, notes apps, bookmarking tools, and everything else built to help us remember. They\u2019re insurance for ideas. They let us forget.Getting Things Done author David Allen preached the freedom of forgetting as the core way GTD would help you be more productive. \u201cThere\u2019s no real way to achieve the kind of relaxed control I\u2019m promising if you keep things only in your head,\u201d he advised.So GTD recommends an inbox to file every task and idea that flits through your mind. You write things down to forget them, trusting they\u2019ll be there when you come back later and need them. Then, you\u2019re to organize and prioritize the tasks, delegate and do them, flip back through the archives and see how you actually got things done.That first step of emptying your brain was what actually mattered, though. Most of our thought and the random things we discover aren\u2019t actually valuable. We\u2019ll write them down then never give them a second thought. You could get the same value by writing them down, then setting fire to the paper and scattering the ashes to the wind.Almost.The problem is we ascribe value to our thoughts and findings. They took time to think up and find; they\u2019ve got to be worth something. We\u2019re scared to lose them. As Daniel Kahneman explains the concept of \u201cLoss aversion\u201d in Thinking, Fast and Slow, \u201cThe response to losses is stronger than the response to gains.\u201d We\u2019d fear losing $100 from our bank account more than we\u2019d value gaining $150 out of the blue. We fear losing our ideas the same. It\u2019s biological, naturally selected into our DNA: \u201cOrganisms that treat threats as more urgent than opportunities have a better chance to survive and reproduce.\u201dTime and thoughts have value, to us anyway, so we\u2019re averse to losing them, too. Enough that we fear losing the things we\u2019ve already found out more than we favor gaining new ideas.\u201cI don\u2019t want to throw anything out. At least not yet,\u201d wrote William Germano in On Rewriting of his early drafts. \u201cI might change my mind, I tell myself.\u201d\u201cMurder your darlings,\u201d advised Sir Arthur Quiller-Couch. But it hurts to throw out perfectly good prose. We hesitate, finger hovering over the delete button, reluctant to expunge the words we love. It\u2019ll \u201cbreak your egocentric little scribbler\u2019s heart [to] kill your darlings,\u201d warned Stephen King in On Writing.So we hoard. Try to remember it all with misplaced loss aversion, only to strain under the weight of a million open mental tabs that erode our ability to remember the important things.We need to forget, but we first must feel safe forgetting.That\u2019s why notes and bookmark apps are so valuable to us. Their promise of a storehouse for all our fleeting whims looks like the salvation we so desperately need. Absolution from procrastination at the altar of getting things done.Notes let us forget and remember, simultaneously. No more loss aversion; we can have our ideas and forget them, too. We can cut and trim and still keep our darlings.We need to feel safe that our memories were not in vain, that they\u2019ll be there if we want them again. Only then can we let go.Then the cracks appear. You read something new, think new thoughts. Then you go to save it and feel a tinge of d\u00e9j\u00e0 vu, think you\u2019ve seen this thing before, yet you couldn\u2019t find the memory. And, come to think of it, you never did use all those murdered darlings, either. Your faith in the second brain falters.Flipping through your old notes suddenly \u201cfeels like sifting through stale garbage,\u201d as Dan Shipper found, disillusioned after building a galaxy of notes in Roam Research. It turns out most of our ideas and discoveries aren\u2019t actually worth that much, not on their own anyhow.But some of the stuff\u2019s really good; we\u2019ll use that, at least, get value from the 1% of what we save. Then you try to relocate a note, only to find that your favorite app\u2019s search doesn\u2019t seem to be as good as you thought it was at first.Now we don\u2019t feel safe forgetting anymore. The spell is broken; back to trying to remember everything again, now that our second brain turned to dust.So we try again. This next app will be the one true way. We had the philosophy all wrong before. Arrows, perhaps, are better than checklists. Folders and hierarchies versus wikis and backlinks. The sages saw technological enlightenment at the end of the revolution; we simply haven\u2019t attained perfection yet.Evernote to OneNote, Moleskins to Field Notes, Roam to Obsidian. We blame the tools, the techniques. Surely they\u2019re to blame. A new app will be better.Then we dump our newest thoughts into it, try the latest features to organize notes, until we\u2019re back to safely forgetting things. Then the illusion gets shattered again, and we\u2019re on to the next new thing.Yet maybe the apps worked all along by letting us forget. We didn\u2019t need bookmarks and notes as much as we needed the safety of letting go. Anywhere we could save our thoughts was enough.We did the most important work when we wrote the ideas down. \u201cI\u2019m not writing it down to remember it later,\u201d declares every Fields Notes notebook, \u201cI\u2019m writing it down to remember it now.\u201d The action of writing is what counts, what imprints important ideas in our brain. The note itself is a permission slip to let things go.Note and bookmarks apps need to make us feel safe. Safe that we can save everything and forget it, that it\u2019ll be there when we come back. Anything could be that safe place, even a plain document, a scratchpad, that gets longer the more things you add to it. Search, linking, organizing, filing\u2014all good for your most important notes, but then again, the most important stuff will show up again on its own (something else the best notes apps could do, resurfacing older notes like Apple Pictures does with your photo \u201cmemories\u201d). You\u2019ll come across those best ideas again and again; your notes end up merely being a record of when you first encountered the idea.Then rely on it. Dump everything there. Cut mercilessly from your writing, knowing you can save your darlings for later. Sweep open tabs and snippets away, trusting they\u2019ll be there if you really need them.You could almost delete your notes every so often, trusting instead in the process.But hey, storage is cheap. Might as well keep the illusion of value going, as long as it gives you the mental safety to forget.Cover photo from Kind and Curious on Unsplash.More from the Reproof blog:People are curious. That's why you should write.A case for content marketing.\u2192What you shouldn't writeJust because you can doesn\u2019t mean you should.\u2192 The writing platform forcreativityJoin the waitlist. AboutFeaturesTerms\u00a9 2023 Reproof, Inc.",
    "summary": "- Notes apps and bookmarking tools are marketed as a \"second brain\" that helps us remember everything.\n- The psychological concept of loss aversion causes people to hoard ideas, leading to mental overwhelm and an inability to remember important things.\n- Notes apps can be valuable in allowing us to forget and clear space for new ideas, but relying too heavily on them can lead to disappointment and the illusion of value.",
    "hn_title": "Notes apps are where ideas go to die (2022)",
    "original_title": "Notes apps are where ideas go to die (2022)",
    "score": 500,
    "hn_content": "The article argues that note-taking apps are where ideas go to die, as blindly making notes is not an effective way to memorize. However, note-taking can be useful for brainstorming and solidifying ideas into a more easily accessible format. Some people prefer plain text files for note-taking, while others use more complex tools such as Obsidian or Logseq. The article concludes by discouraging spending time optimizing note-taking tools and encouraging finding a system that works best for the individual. Several commenters note the importance of notes for their personal productivity, and some mention specific note-taking tools that they find helpful. A few commenters discuss the potential utility of AI-powered note search engines.People discuss various methods for taking and organizing notes, including using platforms like Obsidian and Google Keep, as well as the benefits of spaced repetition and handwritten notes. Some commenters argue that the most important aspect is building a system that works for the individual and promotes the flow of ideation, rather than simply storing and retrieving information. One commenter suggests that expertise is built by learning the ways to fail, while others debate the value in organized notes versus a messy pile. Finally, a few commenters argue that the key to success is in following certain methods, such as taking handwritten notes in a bound volume.The post is about note-taking habits, discussing methods such as time boxing and Markdown files, and tools like Leuchtturm1917 A5 or the app Obsidian. Some argue that note-taking is necessary for memory, organization, or managing complexity, with the GTD method emphasizing the importance of an external trusted system. Others warn of digital hoarding and advocate for a \"read it now or read it never\" approach to avoid FOMO and information overload. The post also includes personal anecdotes and opinions about note-taking, as well as tangents about ADHD and Silicon Valley community colleges.Using note-taking tools can relieve the stress on your brain and help you perform better tasks, but there is a learning curve to the process. Using tools like hashtags can help in managing large streams of information and identifying connections. It's important to review and reorganize notes periodically to identify planned actions and create a category of notes for personal documentation of tasks. Choosing a single search-optimized note app is better for recalling/retrieving notes. It's important to stop overthinking elaborate systems and workflows unless for deep, long-term research and study and focus on using the tools to solve meaningful problems. Note-taking is helpful for studying, research, designing, or other focused mental activities that require pulling information together, discovering obscure connections, and seeing the overall picture.Notes apps may not be as valuable as once thought, as people rarely go back to review them despite taking copious notes. Some people still find value in organizing notes, utilizing old thoughts to resurface ideas and track progress in learning. However, some disagree with the premise that forgetting is the most useful aspect of note-taking; notes apps can be valuable in helping people reflect on things and flesh out ideas for future exploration, and audio recordings can be even more valuable than written notes. Additionally, the never-ending debate about note-taking apps seems unnecessary; the key is finding a balance between too much detail and too little. Finally, some are exploring new features, such as spaced repetition and AI, to help organize notes in a more effective way.- The article discusses the topic of notes and their usefulness over time.\n- Some people keep notes to remember ideas for future use or inspiration.\n- Various methods and applications are mentioned, including handwritten notes, Google Docs, file directories on a Mac, and apps like Obsidian and Anki.\n- Some users prioritize organization and syncing, while others emphasize simplicity and quantity over quality.\n- The article's premise is criticized by some commenters who find value in revisiting old notes or keeping ideas for future use.",
    "hn_summary": "- Note-taking apps can be useful for brainstorming and solidifying ideas.\n- Various tools and methods for note-taking are discussed, including Obsidian and spaced repetition.\n- Finding a system that works for the individual is important, rather than spending time optimizing note-taking tools."
  },
  {
    "id": 36144241,
    "timestamp": 1685566693,
    "title": "Japan's government will not enforce copyrights on data used in AI training",
    "url": "https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/",
    "hn_url": "http://news.ycombinator.com/item?id=36144241",
    "content": "Latest NewsDelos PrimeMay 30, 2023Japan Goes All In: Copyright Doesn\u2019t Apply To AI TrainingIn a surprising move, Japan\u2019s government recently reaffirmed that it will not enforce copyrights on data used in AI training. The policy allows AI to use any data \u201cregardless of whether it is for non-profit or commercial purposes, whether it is an act other than reproduction, or whether it is content obtained from illegal sites or otherwise.\u201d Keiko Nagaoka, Japanese Minister of Education, Culture, Sports, Science, and Technology, confirmed the bold stance to local meeting, saying that Japan\u2019s laws won\u2019t protect copyrighted materials used in AI datasets.Japan, AI, and CopyrightEnglish language coverage of the situation is sparse. It seems the Japanese government believes copyright worries, particularly those linked to anime and other visual media, have held back the nation\u2019s progress in AI technology. In response, Japan is going all-in, opting for a no-copyright approach to remain competitive.This news is part of Japan\u2019s ambitious plan to become a leader in AI technology. Rapidus, a local tech firm known for its advanced 2nm chip technology, is stepping into the spotlight as a serious contender in the world of AI chips. With Taiwan\u2019s political situation looking unstable, Japanese chip manufacturing could be a safer bet. Japan is also stepping up to help shape the global rules for AI systems within the G-7.Artists vs. Business (Artists Lost)Not everyone in Japan is on board with this decision. Many anime and graphic art creators are concerned that AI could lower the value of their work. But in contrast, the academic and business sectors are pressing the government to use the nation\u2019s relaxed data laws to propel Japan to global AI dominance.Despite having the world\u2019s third-largest economy, Japan\u2019s economic growth has been sluggish since the 1990s. Japan has the lowest per-capita income in the G-7. With the effective implementation of AI, it could potentially boost the nation\u2019s GDP by 50% or more in a short time. For Japan, which has been experiencing years of low growth, this is an exciting prospect.It\u2019s All About The DataWestern data access is also key to Japan\u2019s AI ambitions. The more high-quality training data available, the better the AI model. While Japan boasts a long-standing literary tradition, the amount of Japanese language training data is significantly less than the English language resources available in the West. However, Japan is home to a wealth of anime content, which is popular globally. It seems Japan\u2019s stance is clear \u2013 if the West uses Japanese culture for AI training, Western literary resources should also be available for Japanese AI.What This Means For The WorldOn a global scale, Japan\u2019s move adds a twist to the regulation debate. Current discussions have focused on a \u201crogue nation\u201d scenario where a less developed country might disregard a global framework to gain an advantage. But with Japan, we see a different dynamic. The world\u2019s third-largest economy is saying it won\u2019t hinder AI research and development. Plus, it\u2019s prepared to leverage this new technology to compete directly with the West.Just a friendly reminder, countries are going to do what's best for their citizens. US Law, theoretically, is the same on AI training data. If the West is going to appropriate Japanese culture for training data, we really shouldn't be surprised if Japan decides to return the favor.AI Regulation Is Dead, Nvidia Killed itFalcon LLM: Understanding The Commercial License",
    "summary": "- The Japanese government will not enforce copyrights on data used in AI training, allowing AI to use any data for non-profit or commercial purposes, whether it is obtained legally or illegally.\n- Japan's government believes this no-copyright approach is necessary for the nation to remain competitive in the AI technology industry and plans to become a leader in AI technology with the help of ambitious plans and collaborations with local tech firms.\n- While some Japanese creators are concerned about the impact of AI on their work value, the academic and business sectors are pressing forward, and the effective implementation of AI could potentially boost Japan's GDP by 50% or more. Japan hopes to leverage Western literary resources for Japanese AI in exchange for the use of Japanese culture for Western AI training data.",
    "hn_title": "Japan\u2019s government will not enforce copyrights on data used in AI training",
    "original_title": "Japan\u2019s government will not enforce copyrights on data used in AI training",
    "score": 403,
    "hn_content": "Japan's government will not enforce copyrights on data used for AI training, promoting open data and science from research institutions. The validity of using generative AI and copyrighted material remains under debate, with some arguing that the training data provides value that deserves compensation. Copyright law only protects reproduction, not any provided value. The distribution of a model trained on copyrighted material may be considered copyright infringement, but it is unclear how a court would rule. The post also touches on the persuasion tactics used in politics and journalism, as well as the similarities between the emerging AI and crypto industries. However, there is no significant new release or development mentioned.This discussion centers on the distinction between \"learning from\" and \"copying\" in regards to neural networks' training process and copyright violations. It emphasizes that the neural network itself is just a tool, and the liability of copyright infringement lies with the company using generative AI products. The commenters discuss the differences between copyright and trademark infringement and argue that neural networks can produce sufficiently distinct new works of art from what they have learned. They question how companies can ensure that the content generated by LLM products is non-infringing and if it's possible for tech to respect the licenses of used works. Finally, they mention that the question of whether training a model violates copyright is highly dependent on the local law, the medium, and the media.The potential for copyright infringement by generative AI trained on copyrighted content may lead to arguments both for and against the legality of such models. There is debate regarding whether training a model involves copying copyrighted material, and whether the model's output can compete with or substitute for the original. There is precedent for copying digital content being found to be copyright infringement, and the fact that many AI models are marketed as for-profit products that compete with the original works is a strike against them being deemed fair use. However, some jurisdictions allow for the making of a single copy of copyrighted material for personal use, and there is no universal copyright law. Training an AI model is fundamentally different from human learning and is a statistical function that produces optimized outputs for given inputs and not a human thought process.Researchers warn that large language models (LLMs) can easily recreate training data, raising concerns for privacy as well as intellectual property (IP) violations. The extraction of private information was demonstrated in GPT-2, which retrieved physical and email addresses, phone numbers, and more by prompt alone. Meanwhile, claims of IP violations stem from the potential for works generated by LLMs to be too similar to the copyrighted work. Lossy compression in audio can help to prevent IP violations, and a shortage of computational freedom means that it's not possible for AI models to reproduce copyrighted works in exact detail. However, legal limits on IP rights may need to be revised if AI-generated content continues to mimic copyrighted works.The discussion is centered around whether the use of AI-generated content constitutes copyright infringement. Some argue that LLMs create copies of their training data, which includes copyrighted materials, and this should be considered infringement, while others argue that it's not the copying that's the issue, but rather the distribution of copyrighted materials without permission. The discussion delves into the nuances of copyright law and fair use, with some arguing that AI-generated content is akin to memorization and should be treated as such. However, others note that as AI systems become more advanced, they may be able to create larger works that could constitute infringement. The need for better tools to check for copyright infringement in AI-generated content is also discussed.- There is a debate about whether models trained on copyrighted works can be considered collective copyright infringement\n- The output generated by the models is not the issue, but rather the use of copyrighted materials during training\n- Japan has proposed a law to restrict the use of copyrighted works in machine learning training\n- The music industry is exploring licensing arrangements with the AI community for access to copyrighted works for AI training, following a recent Supreme Court decision\n- Some concerns have been raised around potential job loss due to AI, but in Japan, this seems to be less of a focus due to current labor shortages and difficulty in laid off full-time employees\n- Japan's adoption of AI technology has been surprisingly quick, despite being perceived as behind in other areas of technology\n- Examples of Japan's outdated technology practices are disputed, with some citing quicker processes for obtaining ID cards and driver's licenses.The discussion revolves around the use of AI and copyright laws in Japan. An opposition figure advocates for stronger copyright protection, while a minister confirms that existing laws do not prohibit the use of copyrighted content by AI for commercial or non-commercial purposes, including content obtained from piracy sites. The article claiming the relaxation of copyright laws is a lie. Meanwhile, several commenters also discuss the efficiency of various Japanese processes, such as residence card and driving license issuance, data security issues with government-issued Mynumber cards, and the use of paper and fax. Some debate the extent of AI's potential to boost Japan's GDP.The post discusses various aspects of AI-generated content, such as the potential legal and ethical issues surrounding copyright infringement and issues with accuracy and hallucination. It also touches on the impact of AI-generated content on the cultural and artistic expressions of the future. Several examples of AI-generated content, such as AI music covers and ghost AI artists that replicate the style and work of more prolific artists, are also mentioned. The post further illustrates the limitations and success rates of AI-generated content, including reproducing copyrighted materials. Finally, several discussions, arguments, and personal opinions on the topic from the forum are highlighted.The Japanese government has confirmed that it is legal to train machine learning models on illegally obtained copyrighted materials, but there are concerns around balancing development of AI technology with copyright protection. The current laws are considered too loose and allow for controversial exploitation of works. Although it is permissible to exploit a work if it is not intended for personal enjoyment or causing enjoyment of others, it cannot unreasonably prejudice the interests of the copyright owner. The Ministry of Education, Culture, Sports, Science and Technology is monitoring the situation and compiling information about case law on copyright and AI but has no current plans to amend the law in either direction. Some individuals feel the only solution is for generated content to be uncopyrightable.",
    "hn_summary": "- Japan's government will not enforce copyrights on data used for AI training\n- There is debate about whether AI-generated content constitutes copyright infringement\n- The extraction of private information and concern for IP violations are potential issues with large language models (LLMs)"
  },
  {
    "id": 36133263,
    "timestamp": 1685495057,
    "title": "I try to answer \"how to become a systems engineer\"",
    "url": "https://rachelbythebay.com/w/2023/05/30/eng/",
    "hn_url": "http://news.ycombinator.com/item?id=36133263",
    "content": "WritingSoftware, technology, sysadmin war stories, and more.Tuesday, May 30, 2023Feedback: I try to answer \"how to become a systems engineer\"I got some anonymous feedback a while back asking if I could do an article on how to become a systems engineer. I'm not entirely sure that I can, and part of that is the ambiguity in the request. To me, a \"systems engineer\" is a Real Engineer with actual certification and responsibilities to generally not be a clown. That's so far from the industry I work in that it's not even funny any more.Seriously though, if you look up \"systems engineering\" on Wikipedia, it talks about \"how to design, integrate and manage complex systems over their life cycles\". That's definitely not my personal slice of the world. I don't think I've ever taken anything through a whole \"life cycle\", whatever that even means for software.In the best case scenario, I suppose some of my software has gotten to where it's \"feature complete\" and has nothing obviously wrong with it. Then it just sits there and runs, and runs, and runs. Then, some day, I move on to some other gig, and maybe it keeps running. I've never had something go from \"run for a long time\" to \"be shut down\" while I was still around.This is not to say that I haven't had long-lived stuff of mine get shut down. I certainly have. It's just that it's all tended to happen long enough after I left that it wasn't me managing that part of the \"life cycle\", so I heard about it second- or third-hand and much much later.If anything, some things have lived far too long. My workstation at the web hosting support gig started its life with me in 2004 as a pile of parts that had formerly been a dedicated server. It had a bunch of dumb tools that I wrote and other people found useful. It should have been used to inspire the \"real\" programmers at that company to code up replacements, but seemingly did not. That abomination lived until *at least* 2011, or five years after I moved on from that company. None of that stuff was intended to run long-term, but someone kept tending it for years and years. It was awful.But, okay, let's be charitable here. Maybe the feedback isn't asking for that exact definition, but rather something more like \"how to get a job sort-of like the things I've done over the years\". That's the kind of thing I definitely could take a whack at answering, assuming you like caveats.I think it goes something like this: you start from the assumption that when you see something, you wonder why it is the way it is. Then maybe you observe it and maybe do a little research to figure out how it came to be the thing you see in front of you. This could go for just about anything: a telephone, a scale, a crusty old road surface, a forgotten grove of fruit trees, you name it. By research, I mean maybe you go poking around: try to open that scale with a screwdriver, get out of the car and walk down the old road, or turn over some of the dirt in the field to see if you can find any identifying marks.I should also point out that this goes for trying to understand how people and groups of people came to be the way they are, too, but most tend to not respond well to being opened with screwdrivers, walked on, or turned over in the dirt. (And if they do, well, don't yuck their yum.)Anyway, if you start from this spot, then maybe you start coming up with some hypotheses for how something happened, and then sort of mentally file that away for later. Or, maybe you even write it down. Then as more data comes down the pipe over the years, you revisit those thoughts and notes and refine them. Some notions are discarded (and noted as to why), but others are reinforced and evolved.Do this for a while, and sooner or later you might have some working models. They might not necessarily be the actual explanation for why something is the way it is, but it gives you a starting point.Then, one day, something breaks, and you end up getting involved. It might be a high-level system that's new to you, but it has some low-level stuff deep inside, and you recognize some of that. One of those low-level things had a history of doing a certain thing, and that never changed. They might've built a whole obscure system over top of it, but the fundamentals are still there, and they still break the same way. You go and look, and sure enough, some obscure thing has happened. Nobody else saw something like this before, and so when you point it out and flip it back to sanity to restore the rest of the system, they look at you like you just pulled off some deep magic.The question is: did you, really? It's all relative. If you've been poking and prodding at things and have remembered the results of these experiments from over the years, it's not really new to you. It's just one of many events and might not be anything particularly special by itself. It just happened to be important on this occasion.Some people will accept this explanation. Others will refuse it and will insist that you are a magician for fixing \"the unfixable\". A few others will know exactly what you did because they did it themselves once upon a time.Then there are the one or two in every sufficiently large crowd who will see that you are being celebrated for knowing and utilizing some obscure factoid, and they will make it their mission to wreck your world. Basically, they have to make your random happenstance about them somehow, and so they make it about how it hurt them and how they need to get back at you. If this sounds pathological, it's because it is, and unfortunately you will encounter this at any company which doesn't have the ability to screen out the psychos.This also goes for the web as a whole. Having something you've done be (temporarily!) elevated to a point of visibility somewhere public will just set these people off. This, too, is enabled by having forums which don't notice this and deal with their pests.Now, for some examples of obscure knowledge that paid off, somehow.pid = fork(); ... kill(pid, SIGKILL); ... but they didn't check for -1. \"kill -9 -1\" as root nukes everything on the box. This takes down the cat pictures for a couple of hours one morning because it turns out you need web servers to run a web site. Somehow, the bit in the kill(1) man page about \"it indicates all processes except the kill process itself and init\" stuck in my head. Also, the bit in the fork(2) man page that says \"on failure, -1 is returned in the parent\".malloc(1213486160) is really malloc(0x48545450) is really malloc(\"HTTP\"). I think this came from years of digging around in hex dumps and noticing that the letters in ASCII tend to bunch together (this is entirely deliberate). Seeing four of them in a row in the same range with nothing going over 0x7f suggested SOME WORD IN ALL CAPS. It was.The fact I had seen some of this stuff before is just linked to some chance events in my life, combined with doing this kind of ridiculous work for a rather long time now. There are plenty of other times when something broke (or was generally flaky) and I had no idea what it could possibly be, and had to work up from first principles.For someone who's just getting started, it's a given that you haven't seen many of these events yet. Don't feel too badly about it. If you keep doing it, you'll build up your own library of wacky things that could only be earned by slogging away at the job for years and years.Also, if you think this is nuts and choose another path, I don't blame you. This *is* nuts, and it's entirely reasonable to seek something that doesn't require years of arcane experiences to somehow become effective.More writing | Contact / send feedback",
    "summary": "- An anonymous feedback requested an article on how to become a systems engineer; it is an ambiguous term and involves complex management tasks beyond software development's scope.\n- The author proposes that becoming a systems engineer is about understanding the history and design of complex systems, developing hypotheses, refining models, and building up a library of experiences over time, which could be earned by slogging away at the job for years and years.\n- The author also acknowledges the existence of psychopathic individuals in the industry who make it their mission to wreck other people's world.",
    "hn_title": "I try to answer \u201chow to become a systems engineer\u201d",
    "original_title": "I try to answer \u201chow to become a systems engineer\u201d",
    "score": 343,
    "hn_content": "A post on how to become a systems engineer received over 300 upvotes on Hacker News. Comments outlined the differences between regular developers and systems engineers, including understanding networks, memory usage, and operating systems. The post also discussed the confusion around job titles and how \"software engineer\" has become meaningless. Readers shared their own experiences and the challenges of staying up to date with skills in an industry where role names are constantly changing. The post offers insights into the skills needed for systems engineering and provides a forum for engineers to discuss the challenges of remaining current in their profession.People in tech should have a basic knowledge of mechanical engineering, such as thermal management and vibrations, and of how the underlying systems work. Being curious and observing things around you can lead to problem-solving. Systems engineering is important in creating a big-picture understanding of how everything functions. It involves being able to test theories of operation, understanding the physical and software components of a product, and having a leg up on solving problems when things go wrong. Strong fundamentals in understanding operations of a system are essential to be able to build robust and reliable systems. Some examples of not having a strong foundational understanding include not accounting for parameters like latency and network maintenance or wrongly assuming flawless network connectivity.The post discusses the importance of curiosity and scientific attitude while programming. It also highlights the common occurrence of weird programming behavior and emphasizes the need to investigate and understand the underlying cause. The definition of systems engineering is discussed, and the ambiguity of job titles in the corporate world is pointed out. The importance of understanding how systems work, how they are designed, implemented, and maintained is emphasized. The post also includes personal anecdotes where the author deduced the underlying cause of a programming error and sped up a script by disabling logging. Overall, the post emphasizes the need for a curious scientific attitude, understanding of systems, and engineering basics.The article discusses the difference between systems engineering and systems software engineering. Some readers disagree with the use of the term \"systems engineer\" to describe software engineers who work on system-level issues. The discussion includes anecdotes about programming errors and the importance of understanding the entire system when troubleshooting. The article suggests that becoming a sysadmin is a good way to learn about system-level issues, and underscores the complexity of systems engineering. Readers debate the technical details of a comment referencing malloc().This post contains comments on the topic of systems engineering, including confusion around programming concepts like pointers and string literals. One reader points out that the size of pointers and size_t may not always be equal, with one system having a different size. The discussion also includes debates on the benefits of running Linux as a desktop operating system, and the distinction between systems engineers and technicians in terms of their understanding of information systems. Additionally, the importance of understanding TCp sockets in making a process listen indefinitely is emphasized. Overall, the post provides insights into technical discussions and considerations related to systems engineering.",
    "hn_summary": "- The post discusses the skills and challenges of becoming a systems engineer, including understanding networks, memory usage, and operating systems.\n- Curiosity and understanding of how underlying systems work are emphasized as important qualities for engineers.\n- There is debate around the definition of systems engineering and confusion around job titles in the industry."
  },
  {
    "id": 36142285,
    "timestamp": 1685558879,
    "title": "Reddit API Pricing Would Cost Apollo Developer $20M per Year",
    "url": "https://daringfireball.net/linked/2023/05/31/reddit-apollo-api-pricing",
    "hn_url": "http://news.ycombinator.com/item?id=36142285",
    "content": "By JOHN GRUBERARCHIVETHE TALK SHOWDITHERINGPROJECTSCONTACTCOLOPHONRSS FEEDTWITTERSPONSORSHIPSky Guide brings the beauty of the stars down to Earth.REDDIT API PRICING WOULD COST APOLLO DEVELOPER $20 MILLION PER YEARChristian Selig, developer of the splendid Reddit client Apollo:I\u2019ll cut to the chase: 50 million requests costs $12,000, a figure far more than I ever could have imagined.Apollo made 7 billion requests last month, which would put it at about 1.7 million dollars per month, or 20 million US dollars per year. Even if I only kept subscription users, the average Apollo user uses 344 requests per day, which would cost $2.50 per month, which is over double what the subscription currently costs, so I\u2019d be in the red every month.I\u2019m deeply disappointed in this price. Reddit iterated that the price would be A) reasonable and based in reality, and B) they would not operate like Twitter. Twitter\u2019s pricing was publicly ridiculed for its obscene price of $42,000 for 50 million tweets. Reddit\u2019s is still $12,000. For reference, I pay Imgur, a site similar to Reddit in userbase and media, $166 for the same 50 million API calls.Selig does some ballpark math and estimates that Reddit currently generates about $0.12 in revenue per month per active user. The average Apollo user would cost $2.50 per month in API fees\u2009\u2014\u200920\u00d7 higher.Right now Apollo is free to use, but offers a Pro tier with a slew of additional features and fun stuff for a one-time payment of $5, and an Ultra tier with even more for a $13/year subscription. If Reddit goes through with this API pricing, Apollo\u2019s free and Pro tiers would be unsustainable, and the Ultra subscription would have to cost at least $50 or $60 per year.\u2605 Wednesday, 31 May 2023Display PreferencesCopyright \u00a9 2002\u20132023 The Daring Fireball Company LLC.",
    "summary": "- Reddit's API pricing would cost Apollo developer, Christian Selig, $20 million per year, after estimating that the 7 billion requests made for Apollo last month would cost $1.7 million per month.\n- Selig expressed disappointment at the high cost, given Reddit's earlier claims that pricing would be reasonable and based on reality.\n- If the API pricing is implemented, Apollo's free and Pro tiers would be unsustainable, and the Ultra subscription would have to cost at least $50 or $60 per year.",
    "hn_title": "Reddit API Pricing Would Cost Apollo Developer $20M per Year",
    "original_title": "Reddit API Pricing Would Cost Apollo Developer $20M per Year",
    "score": 329,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit login[dupe] Reddit API Pricing Would Cost Apollo Developer $20M per Year (daringfireball.net)329 points by aboveandbeyond 15 hours ago | hide | past | favorite | 18 commentsdang 13 hours ago | next [\u2013]Comments moved to https://news.ycombinator.com/item?id=36141083, which was posted earlier and has the original source.Submitters: \"Please submit the original source. If a post reports on something found on another site, submit the latter.\" - https://news.ycombinator.com/newsguidelines.htmlreplymetadat 15 hours ago | prev [\u2013]Existing active discussion unfolding:Had a call with Reddit to discuss pricing. Bad news for third-party appshttps://news.ycombinator.com/item?id=36141083 (152 comments)replyadamors 14 hours ago | parent [\u2013]Somehow this is no longer on the frontpage ..replymustacheemperor 14 hours ago | root | parent | next [\u2013]@dang I refresh HN way too often and didn't even see this - 240 points, 152 comments, and it's on page 3 in position 81. What's up/can we merge all these threads?Edit: in particular asking because it's not visibly flaggedreplydang 13 hours ago | root | parent | next [\u2013]I'll merge the threads but in the future if you want to get this information to us please use hn@ycombinator.com. \"@dang\" is a no-op and I only saw your comment by accident.replygary_0 13 hours ago | root | parent | next [\u2013]TIL. I see so much \"@dang\" in HN threads I assumed it actually did something.replymustacheemperor 13 hours ago | root | parent | prev | next [\u2013]Thanks, will do in the future!Just curious, how\u2019d that submission wind up so far downranked?replydang 13 hours ago | root | parent | next [\u2013]It set off the flamewar detector a.k.a. the overheated discussion detector. Surprising as that may be for a reddit thread.I get to repeat my joke since if the earlier explanation wasn't seen, the joke wasn't seen either.replymustacheemperor 13 hours ago | root | parent | next [\u2013]Haha, and I appreciate you did.replydang 13 hours ago | root | parent | prev | next [\u2013]It set off the flamewar detector. Surprising as that may be for a reddit thread!I'll turn that off and merge the threads momentarily.replyadamors 12 hours ago | root | parent | next [\u2013]Thank you!replytheturtletalks 14 hours ago | root | parent | prev | next [\u2013]Literally saw that post and it was gone seconds later. Kind of suspicious considering Reddit was a YC company.replydang 13 hours ago | root | parent | next [\u2013]Such suspicions will be evergreen (everblack?) for as long as HN exists but no, we don't moderate HN that way. Lots of past explanation here: https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu....I know it's no fun, but please try to remember that the overwhelming majority of the time, the explanations for these things are really boring and standard. In this case the thread set off the flamewar detector.replykillingtime74 13 hours ago | root | parent | prev | next [\u2013]I see your point but that was 18 years ago. I wonder if they still have shares leftreplytheturtletalks 13 hours ago | root | parent | next [\u2013]Well, they are about to IPO soon so I wouldn't rule out conflict of interests. I saw the link, clicked to read the Reddit post, when I came back, the link is gone. I had to search to find it and it had nearly 300 points. I see no reason to have flagged it since Christain and Apollo have done no wrong. He was telling his users that Reddit is effectively killing Apollo and he was allowed to disclose it.replysamwillis 14 hours ago | root | parent | prev | next [\u2013]I believe there are a number of domains that have a higher \"gravity\" as they tend to lead to unconstructive \"flamey\" threads. I wouldn't be surprised if Reddit is one of them.replysymlinkk 13 hours ago | root | parent | prev [\u2013]The \u201cactive\u201d page has all the juicy stuff - https://news.ycombinator.com/activereplydang 13 hours ago | root | parent [\u2013] Juan was taught from out the best edition,    Expurgated by learn\u00e9d men, who place Judiciously, from out the schoolboy's vision,    The grosser parts; but, fearful to deface Too much their modest bard by this omission,    And pitying sore his mutilated case, They only add them all in an appendix, Which saves, in fact, the trouble of an index.replyApplications are open for YC Summer 2023Guidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Reddit's API pricing would cost Apollo developer $20M per year.\n- A previous thread about Reddit API pricing has 152 comments.\n- The thread about the article may have been flagged by a flamewar detector."
  }
]
