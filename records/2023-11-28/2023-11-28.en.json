[
  {
    "id": 38434613,
    "title": "Prettier's $20k Bounty Claimed, Boosting Performance and Compatibility",
    "originLink": "https://prettier.io/blog/2023/11/27/20k-bounty-was-claimed",
    "originBody": "Prettier stable Playground Docs Blog Donate GitHub ›Recent Posts Recent Posts $20k Bounty was Claimed! A curious case of the ternaries Prettier 3.1: New experimental ternaries formatting and Angular control flow syntax! Prettier 3.0: Hello, ECMAScript Modules! Prettier 2.8: improve --cache CLI option and TypeScript 4.9 satisfies operator! $20k Bounty was Claimed! November 27, 2023 Christopher Chedeau (@vjeux) Prettier, a JavaScript code formatter, has seen an incredible adoption thanks to its careful handling of the very, very, long tail of ways people can write code. At this point, the formatting logic has been solid and after our work on ternaries lands, it will be in a happy state. This means that we can now focus on the next important aspect: Performance. Prettier has never been fast per se, but fast enough for most use cases. This has always felt unsatisfying so we wanted to do something about it. What better way than a friendly competition. On November 9th, we put up a $10k bounty for any project written in Rust that would pass 95% of Prettier test suite. Guillermo Rauch, CEO of Vercel, matched it to bring it to $20k and napi.rs added another $2.5k. The folks at Algora even made an amazing landing page for it. Winner Winner Chicken Dinner I'm so excited to report that the Biome project claimed the bounty! It has been so epic to see a dozen people come together to improve compatibility in only a short 3 weeks. You can read their full report for the details. One question you are probably wondering is why would the Prettier team fund another project!? In practice, Prettier has been the dominant code formatter for JavaScript and as a result of a lack of competition, there has been little incentive to push on performance and fix various edge cases. There is now a Prettier-compatible and way faster implementation in Biome that people can switch to. So Prettier has to step up its game! Thankfully Fabio Spampinato got nerd sniped with the challenge and found many extreme inefficiencies in Prettier's CLI by doing proper profiling. He will fix them by the end of the year. By matching all the tests, the Biome project also found a lot of bugs and questionable decisions in Prettier that we will be able to improve upon. Money, Money, Money I want to start by acknowledging that this bounty and the continued success of Prettier have been possible thanks to various people making significant donations. Companies: Indeed ($20,000), Frontend Masters ($10,850), Sentry ($10,529), Salesforce ($10,025), Airbnb ($8,426), Cybozu ($6,086). Individuals: Shintaro Kaneko ($1,635), Suhail Doshi ($1,000), icchiman ($500), Mariusz Nowak ($270), Benoît Burgener ($270), Jeremy Combs ($270), f_subal ($230). You may not be aware but thanks to all those donations, we've been able to pay two people $1.5k/month for the past two years to keep shipping. Fisker Cheung and Sosuke Suzuki have done an incredible job! With the current budget, we only have 8 months of runway left, so this is a good time to solicit your donations. Consider donating if you or your company are using Prettier and it has been helpful to you: https://opencollective.com/prettier I would also like to give a big shout-out to Open Collective. It has been incredible for the project. From a maintainer perspective, it has been amazing as you can sign up without giving any personal information and it acts as a bank. It lets people give and receive money all around the world and handles all the tax documents properly which is a huge deal. Prettier raised a total of $110k and redistributed $75k. Conclusion While this was a one time bounty, the goal is to give an energy boost to the space of code formatting so that as an ecosystem we can make the best developer experience possible! It's been heartwarming to see so many people coming together and we hope they'll only achieve bigger things from now. Tweet Recent Posts Winner Winner Chicken Dinner Money, Money, Money Conclusion Docs AboutUsage Community User ShowcaseStack Overflow@PrettierCode on Twitter More BlogGitHubIssuesStar",
    "commentLink": "https://news.ycombinator.com/item?id=38434613",
    "commentBody": "$20k bounty was claimedHacker Newspastlogin$20k bounty was claimed (prettier.io) 677 points by conaclos 17 hours ago| hidepastfavorite306 comments dewey 16 hours ago> One question you are probably wondering is why would the Prettier team fund another project!? In practice, Prettier has been the dominant code formatter for JavaScript and as a result of a lack of competition, there has been little incentive to push on performance and fix various edge cases.I was indeed wondering that but the answer doesn&#x27;t really answer the question for me. Why not set a bounty to improve Prettier instead of building a competing project just to increase the motivation to improve Prettier? Or is the end goal to shut down the Prettier project and encourage people to switch to the Rust based one? Seems like an unnecessary fragmentation of an already confusing landscape.Maybe I&#x27;m misunderstanding something though. reply wentin 14 hours agoparent> Why not set a bounty to improve Prettier instead of building a competing project just to increase the motivation to improve Prettier?There are three reasons, I think:1. Writing a rust compiler is separate from prettier project because of its nature. Prettier is not written in Rust, and Rust has proven to be a robust option to write a formatter, so the goal really is to write a formatter in Rust itself, and it can&#x27;t be replaced with improving prettier within its current codebase2. Asking someone to write a Prettier-branded and owned Rust compiler for $20k is not enticing enough. It is essentially equivalent to contracting someone to write some code for Prettier, with an open bid. It would cost a lot more to hire someone to write these code. Great programmer who has the skill to answer this bounty get paid at least $200 an hour (extremely conservative estimate), $20k is enough for 100 hours of work for one person, not enough to finish the project. But getting rewarded for $20k for stuff you write and will own is enticing!3. Good ecosystem going forward. If prettier owns the winner project, prettier is responsible to maintaining and improving it. The good that the bounty did ends when the project is handed over. Prettier team get burdened with a project that they didn&#x27;t write themselves, and the original team (the best people for the job) is not incentivized to keep maintaining it. There is no ongoing competition to keep this field active. reply andai 14 hours agorootparent>Great programmer who has the skill to answer this bounty get paid at least $200 an hour (extremely conservative estimate)Dang! What do you base this estimate on? The Rust aspect, or parsing aspect, or intersection of both? reply TylerE 14 hours agorootparentKeep in mind as a freelancer you have to make about for $2.50 for every $1 a salaried person makes, as you&#x27;re on the hook for 100% of taxes, health care, business expenses, etc. reply selectodude 13 hours agorootparentYeah, it&#x27;s closer to 1.5x. Most companies budget total comp at 150% of salary. reply Bognar 13 hours agorootparentPart of being a freelancer is spending time finding work and doing a variety of things that clients would consider non-billable but still cost time. If you&#x27;re going to work 40 hours a week, not all 40 will typically be billable in freelancing. reply Jaepa 12 hours agorootparentMy mother did NGO environmental policy freelancing in the 00&#x27;s. She would land a contract for 40k for two months of work, then spend the next 4 months looking for more work.Being a subject matter expert means that you can be paid well for your work, but the numbers of jobs that require expertise in the Kura-Aras river basin can be few are far between. reply llimllib 13 hours agorootparentprevmany things are more expensive when you don&#x27;t work at a company - for example the health care for a marginal employee at GE is a lot cheaper than you&#x27;ll get for yourself.I always used 2x, but probably 2.5x is a sensible way to think about it in a patio11 \"charge more than you think you should\" mold. reply laurencerowe 11 hours agorootparent> many things are more expensive when you don&#x27;t work at a company - for example the health care for a marginal employee at GE is a lot cheaper than you&#x27;ll get for yourself.This is less true following the Affordable Care Act than it used to be. The unsubsidised marketplace rate for my Kaiser health insurance seems fairly close to what I pay for COBRA from my former big tech employer. reply alistairSH 10 hours agorootparentBut COBRA is more expensive than the same plan when paid by the employer with pre-tax dollars. reply cjsplat 6 hours agorootparentprevInteresting.My Kaiser almost doubled from COBRA and coverage went from better-than-platinum to high deductible gold.Google &#x2F; SF market. reply laurencerowe 5 hours agorootparentI have Google COBRA in SF too and when I last looked a year or so ago it was about $100&#x2F;year more on the marketplace for the KP platinum plan.Age makes a huge difference to the marketplace premiums though. reply LanceH 13 hours agorootparentprevHealthcare cost doesn&#x27;t scale with the $200 base salary, though. reply ww520 11 hours agorootparentprev1.5x for long term contracts. 2~2.5x for short term gigs. reply kjs3 9 hours agorootparentprevMost? I can&#x27;t think of a gig beyond maybe a couple of folks in a coworking space where I wouldn&#x27;t be laughed out of the room with \"just make it 150% of salary and we&#x27;ll figure out the real number whenever\". Virtually every company I&#x27;ve ever worked with budget the actual number, because they know how much the overhead cost is. There&#x27;s a huge difference between \"a number I use when asked &#x27;ballpark how much a new hire is gonna run us&#x27;\" and \"what is the amount of money I&#x27;m asking the CEO to allocate me in next years budget\". reply kccqzy 4 hours agorootparentprevThat sounds about right. On the other hand it does mean that you are producing this kind of work for big FAANG companies you should expect to earn $80&#x2F;hour. This is around the low end of a senior software engineer or the higher end of a junior software engineer. reply wmidwestranger 2 hours agorootparentI have heard of a database \"engineer\" paid $250 an hour to spend weeks creating what is basically a connection string to a database in a corporate virtual lan. The people paying him were never concerned about the cost, just how long it was taking. This was in Nebraska. reply EricMausler 13 hours agorootparentprevIs the 2.50:1 just a broad estimate or based on something?When I was doing pricing for service contracts it was usually around 1.50:1 burdened billing rate vs direct labor (income) reply bosie 11 hours agorootparenthow did you factor in when you can&#x27;t work (sickness, holiday, increasing your skill, finding the next job etc)? reply EricMausler 4 hours agorootparent2088 potential work hours in a year Less 88 hours holiday Less 80 hours vacation Less 56 hours sick =1864 hours (these are federal guidelines under the Service Contract Act regs)232&#x2F;1864 is a base increase of 12.5% for PTOPayroll taxes, Medical, workers comp, etc add about 30% - though medical is flat so at higher wages like discussed here the % increase it represents goes down furtherI did not have to factor in increasing skill on the job or time between jobs - but I did have to account for overhead and g&a which would be similar to time between jobs since those would be for billsBig companies would have a lot of markup on OH&#x2F;g&a&#x2F;fee, but a software dev working remote for themselves on contract could competitively go down to 5-10% or less here to simply cover the minor added burden to bills.That totals up to about a 50% increase on the salary rate. You can&#x27;t outright bill someone for time you spend looking for a new job or improving your skills. You may charge a premium that you use to do such things but that would have to go under fee which tends to top out at 15% with the government, not something that can be expected to be accounted for. In my experience at least.Again though, this was for service contracts - particularly with the US government, subject to certified cost and pricing data disclosures.I was genuinely curious about the 2.5 number though, as i have spent a lot of time dealing with market rates in various conditions I was interested to know the context. I could see a few ways that could happen, but I wouldn&#x27;t want to speculate too much reply hectormalot 13 hours agorootparentprevDepends on the geography and&#x2F;or whether the knowledge is highly specific.As a datapoint, the average Dutch contracting rates I see for IT development (think: Java, Swift, Kotlin) range somewhere from €75 - €150&#x2F;h. Higher is possible, but then you&#x27;re talking very specific expertise and typically shorter projects.I&#x27;m on the hiring side, this is a figure across some 20 external devs. I think it&#x27;s representative of the middle of the market. reply lcnPylGDnU4H9OF 14 hours agorootparentprevA freelance developer can easily ask $250&#x2F;hour, similar for a contract agency, and that is kind of a low amount. It sounds like a lot for a single developer but if one considers all of the non-billed time of chasing leads and bills it&#x27;s maybe a different picture. reply butterlesstoast 14 hours agorootparentprevAdding this for data. I charge $500 an hour for contracting. It&#x27;s gotta be at least that as it&#x27;s costing time I would otherwise spend with my family. Family time &#x2F; time away from my core job is incredibly valuable. reply Cthulhu_ 13 hours agorootparentFor said data... does anyone actually pay that? I mean if you don&#x27;t get hired for that amount you&#x27;ve got tons of family time so I guess it works?What I&#x27;m trying to say, I don&#x27;t see the correlation between having a huge hourly rate vs family time. 40 hours a week of work is still 40 hours regardless of how much you&#x27;re paid. reply ncallaway 8 hours agorootparentI mean, if you work 20 hours a week, then you don&#x27;t have 40 hours a week of work reply twodave 14 hours agorootparentprevI came here to say that $200&#x2F;hour is only \"extremely conservative\" in very few and small geographic parts of the world. Where I&#x27;m from (in a large city in the US) this number would be described as extravagant. I&#x27;ve charged $200 or more on only one occasion myself, and it was a very short-term arrangement. reply parasubvert 12 hours agorootparentI&#x27;m from a large city (but not the largest) in Canada and $200&#x2F;hour or higher is common for high end devs, architects, and project managers. I charged $200&#x2F;hour twenty years ago. These days I&#x27;d charge $250-300&#x2F;hour if I was a contractor. It is not extravagant in most of North America, but again, it is a rate for higher end talent. I have not charged less than $150&#x2F;hour since the 90s.I once had some contractors in my team that were paid $500&#x2F;hour due to vendor markup. I consider that extravagant. reply fnordpiglet 9 hours agorootparentYeah when I was a contractor in 1996 Mountain View $200 was table stakes for someone with non trivial technical skills. reply wredue 12 hours agorootparentprevI’ll second this that even in Canada, which has quite low tech pay, the lower end of quality dev work is $180&#x2F;hr. Most of us managing contractors wouldn’t blink twice at $200&#x2F;hr. Many of the bills are much higher. reply jihiggins 8 hours agorootparentwhere are you finding these rates at? i have rarely seen anything even passing $100&#x2F;hr in canada, contract or not reply fooster 4 hours agorootparentDude I got $1000 a day in Canada in the early 90s. You just need the right skills and contacts. reply wokwokwok 6 hours agorootparentprev> very few and small geographic parts of the worldThere&#x27;s reasonably good data about this that contradicts this?$1000 as a day rate isn&#x27;t unusual or unreasonable for a specialist IT contractor; and I don&#x27;t say that idly; I say it from both experience and you know; aggregated data:- https:&#x2F;&#x2F;www.hays.com.au&#x2F;documents&#x2F;276732&#x2F;1102429&#x2F;Hays+Techno...- https:&#x2F;&#x2F;www.itcontracting.com&#x2F;rate-checker&#x2F;- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32606348Yes, if you want someone to slap a php website together (or javascript, or many other entry level frameworks), you can pay less....but that&#x27;s not what they were asking; they were asking for a technically sophisticated analysis of an existing project and a performant re-implementation in rust.They got a bargain. reply varispeed 8 hours agorootparentprevThe problem is that many individual freelancers look at their work from an employee perspective, not from a business perspective. From an employee perspective $200&#x2F;hour may seem extravagant, but from business perspective it is nothing.So if you hire an agency to perform a contract, they&#x27;ll bill you $2000 per day and send you their employee who makes $100 an hour. Agency pockets the $1200 (it&#x27;s a simplification, but should paint the picture).Freelancer and agency both run the same business model. If you think like employee and charge extravagantly less, you will never grow.You should typically charge enough, so that for any given project you could hire an employee to do the work, while you look for new leads or you can keep the money in the company and do the work yourself until you amass enough capital to move up the business ladder. reply icambron 15 hours agoparentprevI can&#x27;t speak for the Prettier folks, but as an OSS maintainer, I&#x27;m more interested in the problem being solved than everyone using my particular solution. I actually don&#x27;t benefit at all from you using my code; I did all the work to make the world a place where $PROBLEM has an accessible solution. So if I were passionate about, say, JS code formatting, I would be pretty happy if someone came along and solved that problem in a more performant way. reply vidarh 14 hours agorootparentMost of my git repos are things where I&#x27;m happy if someone finds something useful, but for several of them I&#x27;m very explicit that there are a whole lot of things I will plain refuse to merge not because I think they&#x27;re not great or useful, but because I don&#x27;t want my packages to be everything for everyone - I&#x27;d rather people took my stuff and built a \"competing\" solution with different tradeoffs if they have different needs. I&#x27;d happily even offer help and suggestions if people want to do that.I think that a lot of projects would be a lot better if they insisted on not solving everything, and stuck to solving one problem well, and instead of merging every new feature on offer instead help make it easier for others to \"compete\" with them by e.g. separating out shared functionality or writing about lessons learned...I&#x27;d love to find each and every one of my projects are no longer needed because there&#x27;s a better option (now, I may be very difficult about what is \"better\", so that&#x27;s a tall order), because my list of projects I&#x27;d like to pursue is longer than I will stand any chance of getting to in my lifetime, so if some are taken off my plate, awesome... reply technion 14 hours agorootparentProjects would do well to adopt your views, I believe that always leads to a better product.Age for example always has someone complaining about a feature they want - the refusal to oblige is exactly why it&#x27;s a better product than gpg. reply throwaway2037 5 hours agorootparentI never heard of \"Age\" before this post. Thank you to share. If others are interested to learn more, here are two other interesting posts about Age:https:&#x2F;&#x2F;github.com&#x2F;FiloSottile&#x2F;age&#x2F;discussions&#x2F;432https:&#x2F;&#x2F;words.filippo.io&#x2F;dispatches&#x2F;age-authentication&#x2F; reply nhumrich 9 hours agorootparentprevCan confirm. I wrote an OS library once for a thing. It started getting popular, then suddenly a large, well known project shipped an alternative solution (competitor?). It was the best thing ever to happen to my project. I got what I wanted (what my library aimed to solve) and no longer had to be the maintainer for that thing. reply gmgmgmgmgm 7 hours agorootparentprevI&#x27;d be less interested if in order to fix a new issue related to my problem, that I had spend weeks to learn a whole new language, dev env, build system, etc just to add a feature that would have taken me 15 mins in the language I&#x27;m used to.Of course I&#x27;d love to learn rust! But, for the most part, I just need to get stuff done so I&#x27;d prefer to stick with what I know and can mod easily and leave the \"learn an entire new language\" part for something that actually needed that language. reply blowski 15 hours agorootparentprevI&#x27;d guess a significant proportion of OSS maintainers _are_ in it for some combination of ego-trip, or a misguided belief that it will make them rich. Both outcomes may seem less likely if you&#x27;re just fixing somebody else&#x27;s software for free. reply demosthanos 14 hours agorootparentWhat have you seen that would lead you to believe that?My own experiences with open source suggest that no one would stick around for very long if they were motivated by fame or wealth. Being a FOSS maintainer seems to be a constant exercise in dealing with people whining at you for not fixing their problems for free, not something that brings any significant personal attention and certainly not something that provides a sustainable source of income. reply matheusmoreira 13 hours agorootparentprevIs it really so misguided these days? I see plenty of developers making decent amounts of money via GitHub sponsors, patreon and others. Could be pretty nice if a project gains momentum. And it&#x27;s ethical: no ads, no proprietary software, nothing. reply yebyen 16 hours agoparentprevThere is a whole gulf of room between \"we are the incumbent and there is no viable alternative in any language for JavaScript developers\" and \"the Rust folks have done it, there&#x27;s an alternative now and it seems quite viable\"I think it&#x27;s about escaping local minima? You can always look at the biggest sink for performance and say \"there&#x27;s definitely something we can do better in there\" but unless you have something objectively better to compare it to, you&#x27;d never be sure.Imitation is the sincerest form of flattery. And knowing that of the hard problems you solved, someone else can solve them and in a different language, I think there&#x27;s quite a bit of value to be obtained just through the competitive process that emerges when there is competition.You can&#x27;t fully have competitiveness without an actual alternative to compare yourself against. If the Rust folks can do the problem slightly faster by shaving off just 5% or less of the test suite, what all does that tell us about the theoretical limits of a solution when compared against the canonical version?I have only limited understanding of the problem space, but I think there&#x27;s always something intangible to be gained from having a quite similar implementation in a different language. reply mlhpdx 13 hours agorootparent> I think it&#x27;s about escaping local minima?Yes. It’s also been said (not often enough) “if we don’t compete with ourselves someone else will”. Getting out of one’s own head (and repo) is gold. It shows humility and respect as well as creates innovation and resilience. reply j1elo 16 hours agoparentprevAdding to what has been already said, simply the fact of having different people with different perspectives and intentions, can surface new ways of improvement.https:&#x2F;&#x2F;biomejs.dev&#x2F;formatter&#x2F;#differences-with-prettierTurns out Biome found several pain points that they chose to not follow the same decisions than Prettier, instead diverging from it. This alone could be already a reason why a parallel development is worth it. reply Philpax 16 hours agoparentprevDifferent approaches with different constraints and less baggage can potentially discover areas of improvement that weren&#x27;t otherwise visible in the original project. (i.e. they weren&#x27;t locked into the \"Prettier mindset\", if such a thing exists) reply explaininjs 16 hours agoparentprevMaybe they want an implementation to have an understanding of potential perf of a non-js solution, but know that it’d cost them more than $10k to have an employee do it. So instead you offer 10k, someone else offers 10k, and the final product is “owned” by a third party but your questions are answered. reply vjeux 4 hours agoparentprevSorry for being late to the party. What I really care about is that we have fast code formatting in the industry.But performance is a field that&#x27;s really hard to accurately measure as there are so many variables, machines, benchmarks... You can see this in the JavaScript ecosystem where every project says that they are faster than the other one and they are in practice all right depending on which benchmarks they use.So, creating a big bounty with something that&#x27;s very hard to accurately measure is likely not going to work out very well.In practice in the ecosystem, the Rust community really cares about performance. So it feels that this is the right audience to build something really fast. But, so far, none of the Rust-based printers were even close to outputting the same thing as prettier. They all decided to implement a different set of options, made different design decisions...So every time somebody mentioned using one of those, I was getting annoyed that I would --love-- to recommend them, but it wasn&#x27;t going to be the same. And I believe from experience that one of the main reason Prettier has been so successful is because of this --very very very very-- laborious last few percent of edge cases.So goal #1 is to convince at least one of those projects to start being compatible with Prettier so that we had a viable alternative. In practice, there was no way I could talk to them directly and convince them to align with Prettier. So the idea of the bounty came to be, if I made a bounty big enough, it would be a way to convince them to do so (and it worked!).The bounty to work needs to be very easy to test (percentage of tests that pass is very easy to test), cannot easily be cheated (unless you run prettier itself, you need to go through the laborious work of doing the same logic) and cover real world use case (all the tests were added over time based on using it in prod). It also mentioned a \"project\" and not a \"person\" to encourage collaboration.It&#x27;s a bit unorthodox but I&#x27;ve learned my lesson with code formatting that people are obsessed about discussing this topic so you need to find alternative ways to convince them.Now, going back to Prettier, I&#x27;ve been very annoyed that nobody had looked at performance of the project since after I stopped working on the project. For example, I wrote a way to keep the prettier process alive for editor integration instead of paying for the startup cost every single time https:&#x2F;&#x2F;github.com&#x2F;prettier&#x2F;prettier-rpc and nobody used it. I needed to find a way to convince people that prettier&#x27;s performance actually matters.One of the most powerful way to get people to improve performance is to have two competing offerings battling against each others. We&#x27;ve seen this very successful between JS engines, JS frameworks... But, due to the success of prettier, there was no competition, the JS Survey admins even stopped asking about it because it was the only choice.So having a proper competitor which is faster and uses a relatively controversial language, was a good setup to get a competition going. And it also worked, since the bounty was announced, Fabio Spampinato got nerd snipped thinking he can make the JS version faster than Rust and has been working every day profiling and rewriting the Prettier CLI to be orders of magnitude faster. We are using the open collective money to contract him to work on this.Outside of performance, by having another group of people work on the same tests, they uncovered a lot of broken behaviors and edge cases on prettier that should be fixed.Last but not least, having a bounty incentivizing another project was intriguing enough to generate a lot of discussion and therefore receive a lot more coverage than just asking people to work on your project.----So, overall, it achieved all the outcomes I wanted: by the end of the year, prettier itself is going to be a lot faster, and we actually have -less- fragmentation in the space where the other big project is now aligned in terms of the way code is formatted.Would have I been able to spend $10k more effectively, I can&#x27;t think of how. But I&#x27;m pretty sure I&#x27;m missing some better strats! reply gmgmgmgmgm 7 hours agoparentprevYes, I&#x27;m sure rust is amazing but, Just a guess that the number of programmers that can contribute to a rust project is probably 3-4 orders of magnitude less than the number of programmers that can contribute to a JavaScript project. Well, maybe if you subtract all the \"competent enough to a submit a coherent pull request\" programmers the number is only 2-3 orders of magnitude?Basically I&#x27;d assume, if a project targeted at JS, switched to rust, the number of useful pull requests would be 50-100x less.I wonder if anyone has any data on that (and similar projects re-written in C++ or Go etc...). reply rattray 6 hours agorootparentSpeaking as a contributor to Prettier, it is _not_ an easy codebase for someone who knows JS to just jump into and contribute.Furthermore, projects like Ruff show you can have a lot of contributors for projects like this. Maybe because there are lots of devs who use TS or Python in their day job, but want excuses to use Rust on the side. reply mplanchard 6 hours agorootparentprevAnecdata, but as a maintainer of both Python and Rust projects, while it’s true that the Rust projects have fewer contributors&#x2F;issues, the quality of contributions is on average higher reply tshaddox 15 hours agoparentprev> Why not set a bounty to improve Prettier instead of building a competing project just to increase the motivation to improve Prettier?Presumably the creators of the bounty believe that this bounty is in fact a good way to directly improve Prettier. The acceptance criterion of the bounty doesn&#x27;t need to literally be \"improve Prettier\" for the work to improve Prettier. reply marcosdumay 15 hours agoparentprev> performance and fix various edge casesThose are the two main things that rust is expected to help improve.It&#x27;s quite easy to empathize with somebody that sees a JS software having problem with those two and picking a language that is known to help with those instead of being known to increase those problems. reply wincent 16 hours agoparentprev> why would the Prettier team fund another project!?Less \"the team\" per se, and more vjeux, I think. reply hermanradtke 16 hours agoparentprevMaybe prettier will consume the biome work and offer it as an option for prettier going forward. reply patcon 6 hours agoparentprevOne would be a singular bespoke (and highly specified) improvement, and the other is likely a generalized flywheel that may keep incentivizing many further improvements reply nijave 8 hours agoparentprevI think there&#x27;s some merit in creating a fresh solution with little or no bias from the original. Trying to adapt an existing codebase is limited by the existing applications architecture and how much you can change without breaking everything reply nailer 14 hours agoparentprev> Why not set a bounty to improve Prettier instead of building a competing project just to increase the motivation to improve Prettier?That&#x27;s a good question. But a new, Rust (or whatever is fast) version of prettier is effectively Prettier. If it has the same config, switches, and defaults, it&#x27;s Prettier.The value of prettier is that it&#x27;s a standard most of JS&#x2F;TS agrees on, saving discussions for more important topics. The code that gets us there is a side effect. reply globular-toast 14 hours agoparentprevIt sounds like the Prettier test suite is the most valuable part of the project. Perhaps the original project will just become a test suite with everyone using whatever tool is fastest with the best coverage. reply DustinBrett 14 hours agoparentprevMaybe they paid Biome $20k to have it be more \"opinionated\" with Prettier&#x27;s opinions. reply evan_ 10 hours agoparentprevHow much would you pay to have your project on the front page of Hacker News for most of the day? More or less than $20,000? reply gostsamo 16 hours agoprevMany people comment the reasons without acknowledging that part as well:> By matching all the tests, the Biome project also found a lot of bugs and questionable decisions in Prettier that we will be able to improve upon.For me, this means that they have another implementation for sanity checking their own. reply syrusakbary 16 hours agoprevI&#x27;m incredibly excited for this. The Biome team has been remarkably fast on achieving 95% compatibility with Prettier [1].This will help to bring maximum speed to formatting Javascript thanks to Rust, following the ruff (Python formatter) trend.Just as a note, as it was not mentioned in the article, Wasmer [2] also participated with a $2,500 bounty to compile Biome to WASIX [3], and it has been awesome to see how their team has been working to achieve this as well... hopefully we&#x27;ll get Biome running in Wasmer soon!Also congrats to the Algora team, as they have been doing very good work with their landing and trying to help on the challenge moving forward [4].Keep up the great work!![1] https:&#x2F;&#x2F;github.com&#x2F;biomejs&#x2F;biome&#x2F;issues&#x2F;720[2] https:&#x2F;&#x2F;wasmer.io&#x2F;[3] https:&#x2F;&#x2F;wasix.org&#x2F;[4] https:&#x2F;&#x2F;console.algora.io&#x2F;challenges&#x2F;prettier reply cedws 15 hours agoparentThis is the first time I&#x27;ve come across WASIX. Upon reading about it, I can&#x27;t help but see it as a reincarnation of the JVM. Is that accurate? What advantage is there to executing code in WASIX if it can access the system and isn&#x27;t actually in a sandbox? reply n42 14 hours agorootparentThe advantage is this guy’s VC backed company gets to sink its teeth into the early stages of WASM platform adoption, helping him line his and his company’s pockets.WASIX is a hostile fork of an open standard that does care about sandboxing. reply syrusakbary 14 hours agorootparentI&#x27;m incredibly curious to hear more on how you think WASIX helps on the company pockets (since is open-source, and even it&#x27;s governance model is open).I&#x27;d love to hear your thoughts, maybe we are missing on some nice business opportunities here! reply n42 14 hours ago[flagged]| rootparentIt&#x27;s fine to be forthcoming about the financial interests of your projects – this is Hacker News, after all. We live on VC. The tone of your previous comment suggests there is none.I find it hard to believe that anything you would spend your company&#x27;s time on would not push the mission of the company forward. This creates a financial incentive. reply dang 12 hours agorootparentCan you please follow the site guidelines when posting here? They include:\"Have curious conversation; don&#x27;t cross-examine.\"\"Please respond to the strongest plausible interpretation of what someone says, not a weaker one that&#x27;s easier to criticize. Assume good faith.\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html reply n42 12 hours agorootparentOf course. I hope the edit is more appropriate. Thank you! reply AndyKelley 12 hours agorootparentI thought it was well said fwiw reply n42 3 hours agorootparentthank you. reply syrusakbary 13 hours agorootparentprevI&#x27;ll address the first non-clickbait question and try to provide some insight, as I think you might be a bit confused on how COSS startups work.We don&#x27;t plan to monetize WASIX. WASIX is just an enabler to allow any program compile to WebAssembly. If other open-source technology existed and fulfilled what we want to accomplish is more than certain we would not have created WASIX.In our case, the community was asking to have sockets, threads, forking, subprocesses and more fully working on Wasm. And there was nothing that supported that (or that aimed to support it), so we worked on it.Now, let me give more insight on what we actually plan to monetize: Wasmer Edge [1]. Wasmer Edge is the alternative to the expensive big-tech providers that allows any person or company to host their websites at a fraction of the cost.Hope the insight was useful![1] https:&#x2F;&#x2F;wasmer.io&#x2F;products&#x2F;edge reply n42 12 hours agorootparentWasmer is a VC company building a platform that depends on the existence of these platform APIs – sockets, threads, forking, etc. They have a vested interest in the existence of these APIs, and the health of the developer ecosystem, to stay alive. From here, it kind of seems like the fundamental foundation of everything Wasmer is building.Meanwhile, Bytecode Alliance is a registered non-profit with many large corporate backers. They are building out the WASI standard with careful due diligence to learn from and avoid the mistakes we have found in POSIX. BCA has their own financial incentives, and they are aligned differently than yours.BCA ultimately is moving slower than works for Wasmer. So I guess you were faced with three options: A) wait for BCA, and do nothing B) work with BCA and push it forward C) fork it and do it yourself.So you chose to fork it and reimplement POSIX with all its warts. I assume because this applies pressure to the community (candidly, a good thing) while letting you make progress immediately.Where I, and I assume others, have issue with all of this is:You have chosen a name that has created confusion in an already confusing space.You have whitewashed it by promoting it as a new open standard, but the controlling entity realllyy is just Wasmer[0].You have demonstrated through your actions a complete lack of willingness to work with people in the community, preferring to strong-arm with veiled threats[1] or perverting incentives financially[2].The issue isn&#x27;t with WASIX, but with you and your actions. I have no confidence in your ability to lead this community forward in a way that has my (our) interests ahead of your company&#x27;s. [0]: https:&#x2F;&#x2F;github.com&#x2F;wasix-org&#x2F;wasix-governance-meeting-notes&#x2F;blob&#x2F;5135516d091d1bc3810677ea2cc755a67e10c669&#x2F;meeting-notes-27-07-2023.md#meeting-attendees [1]: https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;wit-bindgen&#x2F;issues&#x2F;306 [2]: https:&#x2F;&#x2F;github.com&#x2F;ziglang&#x2F;zig&#x2F;issues&#x2F;17115 reply syrusakbary 12 hours agorootparent> The issue isn&#x27;t with WASIX, but with you and your actions.Is refreshing to see at least some honesty. Haters gonna hate, so I personally don&#x27;t mind. Have a great day! reply jeffparsons 10 hours agorootparent> Haters gonna hateIf you dismiss all criticism of your behaviour in this way, you&#x27;ll never gain any insight into why so many people find your behaviour so objectionable.It&#x27;s not because \"haters gonna hate\". It&#x27;s not because you&#x27;re operating in a competitive and highly visible space, and so of course you&#x27;re going to receive some \"hate\", or whatever other dismissive narrative you might have cooked up. Look around you: people in similar positions just don&#x27;t generate the reaction that you do.When Wasmer eventually folds, it will be your fault, because you refused to reflect on how your antisocial behaviour destroyed all trust in the brand.I don&#x27;t know how to stop that from happening. Maybe a good therapist could help? Heck, your board would probably even be happy to pay for it, given the potential upside for Wasmer. reply syrusakbary 8 hours agorootparentWithout entering in any detail on what you mentioned (other than sincerely thank you for the belief in the potential upside for Wasmer!), I think we may have very different definition of what a hater is.A hater, for me, is someone that lacks of constructive criticism. reply n42 12 hours agorootparentprevThis is kind of my point. It would be refreshing to see the CEO that would like our industry to build on top of his platform to show some humility. Do you care to admit to making any mistakes in your interactions, or is it all because we are just a bunch of haters? reply syrusakbary 12 hours agorootparentI&#x27;m not even going to enter into the flame-bait. I obviously disagree with your points.Hope you have an awesome day replyjeffparsons 10 hours agorootparentprevA charitable interpretation could be that a rising tide lifts all boats. Syrus&#x2F;Wasmer may sincerely believe that WASIX contributes to the rising tide, and it will therefore benefit Wasmer and all other Wasm-related ventures alike.Under this interpretation, there would still be a financial incentive, but it would not be underhanded. reply n42 10 hours agorootparentThis is, despite how it may otherwise appear, how I see it. I think that the decision is a good one, made by people and a company I don&#x27;t trust.The broader context of Syrus&#x27; actions within the community (which reflect on Wasmer as its CEO) are what raise concern for me. reply syrusakbary 9 hours agorootparent> I think that the decision is a good oneI&#x27;m incredibly happy to hear that reply syrusakbary 9 hours agorootparentprevI&#x27;m happy to see some convergence here :) replysyrusakbary 14 hours agorootparentprevWASIX is similar to the JVM in the sense that is able to use a common abstraction for all operating systems, and it uses a VM under the hood (Wasm).But the main difference is that it has sandboxed capabilities (similar to docker), do you can granularly add permissions to only certain directories or even disable networking when running. reply shepherdjerred 15 hours agorootparentprevIt sounds the same to me: JVM, but with better browser support. reply triyambakam 16 hours agoparentprevDo you know the reason for enabling compilation to WASIX? reply slimsag 15 hours agorootparentThe parent poster (syrusakbary) is the CEO of Wasmer and trying to push WASIX, a quasi-proprietary &#x27;open standard&#x27; while it seems the rest of the WASM ecosystem is in fact moving to WASI preview 2 [0][0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37545670 reply syrusakbary 13 hours agorootparentI think you may be a bit confused on what&#x27;s the purpose behind WASIX, Stephen. And you are mistakenly polarizing between two very valid options for different use cases.We are not trying to push WASIX, we are trying to move forward sockets, threads, subprocesses so they can run fully in Wasm environments. And WASIX is the solution to that problem.WASI Preview 2 on the other hand seems to be focused on other set of problems and doesn&#x27;t solve any of the needs that got WASIX started in the first place. reply slimsag 12 hours agorootparentAccording to their roadmap, WASI preview 2 will have threads and sockets.So, in light of that, what is the reason for diverging from WASI and creating WASIX? reply syrusakbary 12 hours agorootparentI&#x27;m not sure what roadmap you are talking about. Threads are actually removed from WASI Preview 2.WASI Preview 2 still doesn&#x27;t support threads, fork, subprocesses or longjmp&#x2F;setjmp (among others). Not even that, when WASIX was created not even sockets were supported in WASI.I&#x27;d recommend trying to compile bash or curl to WASI Preview 2 and WASIX and see how far you get in each before trying to polarize the readers between WASI Preview 2 and WASIX. reply slimsag 10 hours agorootparentThe roadmap I linked above. The WASI folks have done a poor job at communicating, no doubt.Just for you I did some googling: see here[0] for the current status of WASI threads overall, or here[1] and here[2] for what they are up to with WASI in general. In this PR[3] you can see they enabled threads (atomic instructions and shared memory, not thread creation) by default in wasmtime. And in this[4] repository you can see they are actively developing the thread creation API and have it as their #1 priority.If folks want to use WASIX as a quick and dirty hack to compile existing programs, then by all means, have at it! I think that&#x27;s great. Just know that your WASIX program isn&#x27;t going to run natively in wasmtime (arguably the best WASM runtime with the strongest long-term outlook), nor will it run in browsers, because they&#x27;re going to follow standards.. not WASIX. I don&#x27;t think anyone believes exposing POSIX fork() to WASM is a good idea, even if it lets you build existing apps &#x27;without modification&#x27;.Please stop accusing me of being polarizing. I just don&#x27;t want to live in a world with two competing WASI standards, two competing thread creation APIs, and a split WASM ecosystem overall. I want good tech to win, not shitty power plays.[0] https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;jco&#x2F;issues&#x2F;247#issuecomm...[1] https:&#x2F;&#x2F;bytecodealliance.org&#x2F;articles&#x2F;wasmtime-and-cranelift...[2] https:&#x2F;&#x2F;bytecodealliance.org&#x2F;articles&#x2F;webassembly-the-update...[3] https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;wasmtime&#x2F;pull&#x2F;7285[4] https:&#x2F;&#x2F;github.com&#x2F;WebAssembly&#x2F;shared-everything-threads reply syrusakbary 9 hours agorootparentJust to be clear, and to reinforce what I mentioned.WASI Preview 2 doesn&#x27;t officially support threads. There are some ways to get it running as you mentioned, but threads are NOT part of WASI Preview 2. Which is what you initially stated and I corrected.We don&#x27;t want to have competing standards either, and we&#x27;d love to merge things upstream. However, based on my interactions with the WASI subgroup it seems clear they want to keep some of this things that WASIX needs out of the spec (and that&#x27;s ok!). If you would not like to have that, then you shall probably direct your requests towards them, not us.> I want good tech to winMe too! I now understand this over-idealistic belief is the root of the issue. Good tech doesn&#x27;t win, a good product does.I wrote more about this, if you are curious, here: https:&#x2F;&#x2F;wasmer.io&#x2F;posts&#x2F;is-not-about-wasm-is-about-what-you-... reply nilslice 9 hours agorootparentprevthank you! slimsag is not the one being polarizing at all… pretty sure everyone in the wasm space is sick of this too, but hey, keep building I guess? reply syrusakbary 9 hours agorootparentI wish the votes in the comments were public, so maybe you could actually get a grasp of what the community really feels like! reply nilslice 9 hours agorootparentoh we all feel it reply syrusakbary 9 hours agorootparentHope you feel better! reply avarun 6 hours agorootparentI gotta say you come off really childish in this thread man. Maybe cool the ego a bit. replyintelVISA 13 hours agorootparentprevExtend, Embezzle, Escape reply syrusakbary 16 hours agorootparentprevWASIX will enable to run biome fully sandboxed at close to native speeds.Imagine you can safely assume that the formatter program will only have access to the directory you provide, and not anything outside of it (not even the network!).In summary, Wasmer + WASIX is like Docker, but much more lightweight :) reply nicoburns 16 hours agoprevSpeed is always welcome, but I just wish prettier was a little less opinionated. Specifically around line length, it will just not leave my formatting alone. I find prettier formatted code much less readable than unformatted code, and this isn&#x27;t a problem I have with other code formatters like rustfmt. reply meowtimemania 16 hours agoparentDo you have any examples where prettier code is much less readable? I haven&#x27;t really ran into that issue and have been very happy with prettier. reply nicoburns 14 hours agorootparentA good example from further down the comment section:Prettier doesn&#x27;t break template literals, but it will break non-literal sections of template strings. For example, if we wrote this: const foo = `aaaaaaaaaaaaaaa${bbbbbbbbbbbbb}ccccccccccccccc${ddddddddddd}`;prettier would fix it to const foo = `aaaaaaaaaaaaaaa${ bbbbbbbbbbbbb }ccccccccccccccc${ ddddddddddd }`;Which is not only much harder to read in it&#x27;s own right, but now takes up 6 lines instead of one! reply bakkoting 9 hours agorootparentI have a PR fixing that: https:&#x2F;&#x2F;github.com&#x2F;prettier&#x2F;prettier&#x2F;pull&#x2F;15209Just needs another maintainer&#x27;s stamp. reply harimau777 16 hours agorootparentprevPrettier often makes code written using sequences of .chaining much more difficult to read by forcing it all to the same line.For the same reason, it often makes code written using composition more difficult to read since it won&#x27;t let you decide when to put a sub-call on its own indented line for clarity. reply HellsMaddy 15 hours agorootparentI had the same gripe, and recently tried switching to dprint. It is much less aggressive with changing where lines break, and honestly it’s a huge relief to be able to break lines wherever I think makes the code most readable. It’s also significantly faster than prettier. I’ve been very happy with it. reply floydnoel 15 hours agorootparentprevIn those situations I make use of comments to force new lines where I want them. Stupid but it works! reply pkilgore 15 hours agorootparentprevDisable it! (https:&#x2F;&#x2F;prettier.io&#x2F;docs&#x2F;en&#x2F;ignore.html)I do this occasionally, and especially with things like test tables. Linters&#x2F;formatters are there to help in the common case, not to be some oppressive dogma. reply hu3 14 hours agorootparentIf the fix is to disable prettier for an entire file, you&#x27;re proving your parent poster point. reply dewey 13 hours agorootparentIt doesn&#x27;t have to be disabled for the whole file, you can annotate lines too as shown in the link the parent commenter shared. reply francislavoie 10 hours agorootparentAdding comments just to fight with prettier is... uglier. I rather just not use prettier at all. It&#x27;s more harmful than good IMO. replymostlylurks 15 hours agorootparentprevIf I don&#x27;t remember wrong, doesn&#x27;t it put object destructurings on one line if they fit? That&#x27;s both less readable than putting each destructured member on its own line as well as a cause of unnecessary whitespace changes in your commit history if you ever add one more field to the destructuring that takes it over the line length limit. reply floydnoel 15 hours agorootparentAdd a comment anywhere in the destructuring and it will force each property onto its own line. reply bennyg 16 hours agoparentprevHave you tried the print width option: https:&#x2F;&#x2F;prettier.io&#x2F;docs&#x2F;en&#x2F;options.html#print-width? reply nicoburns 16 hours agorootparentYes, but unlike most formatters where the width option is a maximum and it mostly leaves your code alone below that width, prettier will aggressively widen your code to fit if you up the print width setting.I think rustfmt will actually also widen code you have put newlines in sometimes. But it&#x27;s heuristic is somehow much better than prettier&#x27;s. reply beirut_bootleg 10 hours agoparentprevThere&#x27;s a more insidious side effect to prettier related to the line length thing which I&#x27;m not sure other commenters touched upon, and the reason I avoid opinionated formatters.Prettier will affect your diff in ways you didn&#x27;t intend to.Remove a member from a destructuring assignment that brings it below the line length limit, and suddenly your diff is +1&#x2F;-5 instead of 0&#x2F;-1, making it slightly more difficult for your reviewer to see what the exact difference is between those 5 lines removed and 1 line added - it&#x27;s not immediately clear which member was removed.You try to rewrite a previous commit to fix a typo using Git&#x27;s interactive rebase, and now the next commits won&#x27;t replay on top if it because Prettier decided to reformat an entire code block.Another fun thing to try with prettier: add a precommit hook that runs prettier, and then try to stage partial file changes.No thanks. reply plugin-baby 4 hours agorootparentReformatting in precommit is destructive. reply fenomas 15 hours agoparentprevI think the real issue with prettier is that it&#x27;s almost become a de facto standard. E.g. personally I don&#x27;t like it at all, but do like Svelte, and Svelte&#x27;s official (and AFAIK only) formatter uses prettier. Hence I use prettier for Svelte, and ditto for a few other things.So the whole \"extremely opinionated, if you want configuration go somewhere else\" is great in principle, when people are choosing to use the tool. But if it becomes the only option for certain swathes of users, a touch more configuration would really be appropriate. reply Cthulhu_ 13 hours agorootparentTo use the Go language&#x27;s formatter and a saying, \"Gofmt&#x27;s style is no one&#x27;s favorite, yet gofmt is everyone&#x27;s favorite.\" That is, you may not like the style, but it&#x27;s preferable over having no tool ensuring consistent style. reply fenomas 8 hours agorootparentAgreed, but that&#x27;s a different conversation because gofmt is a lot less opinionated than prettier. Prettier goes well beyond style issues, and messes with semantics - it will add or remove parentheses in math expressions, add or remove linebreaks within lines of code, etc.If prettier worked more like gofmt I&#x27;d probably love it and have no complaints! reply barsonme 5 hours agorootparent> and messes with semantics - it will add or remove parentheses in math expressionsWait, so prettier rewrites code incorrectly? Aka, it’s buggy? reply fenomas 3 hours agorootparentNo, by semantic information I mean information that&#x27;s meaningful to humans but not to the JS engine. E.g.: &#x2F;&#x2F; before prettier var matrix = [ 1, 0, 0, 0, 1, 0, 0, 0, 1, ] var result = (num % divisor)bitmask &#x2F;&#x2F; after var matrix = [1, 0, 0, 0, 1, 0, 0, 0, 1] var result = num % divisorbitmaskNo difference in actual behavior, but the linebreaks and extra parens were there to indicate the developer&#x27;s intent, not to affect behavior.Prettier&#x27;s outlook is that this is intentional, and the developer should add \"&#x2F;&#x2F; prettier-ignore\" comments to every line of code that has semantic information they want preserved. reply tylerchurch 4 hours agorootparentprevI&#x27;ve never seen it be buggy, but it will often remove unnecessary parens. For example: (a * b) + (x * y) becomes a * b + x * y.This the same code, but I often use parens for semantic grouping or to be 110% sure the operator precedence is correct for a particular formula. It&#x27;s not a dealbreaker, but it does remove some of the meaning I was trying to imbue on the code. reply digging 15 hours agorootparentprevWell, this bounty is just about the only way that the Prettier team could contribute to solving that issue, so rejoice.Besides that, I get the impression being a universal standard is kind of a non-issue for Prettier:- If it was widespread, but highly configurable, that increases friction when switching projects, because there will be tiny formatting differences everywhere- If it was not widespread, it would not be well known, increasing friction for users entering a project where it was used reply fenomas 9 hours agorootparent> If it was widespread, but highly configurable, that increases friction when switching projectsThis is clearly not true. Far and away the single most contentious JS style issue is ASI, and prettier has a config option for it and nobody bats an eye. Ditto for the prototypical style issues of indent size and tabs&#x2F;spaces.The value of prettier is enforcing a consistent style within a repo, and config options don&#x27;t affect that. To the contrary, config options are part of why prettier is so popular - if they&#x27;d never added ASI then a lot of projects would never have adopted it. reply girvo 11 hours agorootparentprevIf it was widespread but slightly more configurable, how would that increase friction? It’s still always setup to run automatically and transform your input to the output. The whole point of it is you don’t have to think about its rules, just code and it will do the rest. reply uxp8u61q 15 hours agorootparentprev> Well, this bounty is just about the only way that the Prettier team could contribute to solving that issue, so rejoice.Erm, that bounty was for the production of a program that behaves exactly like prettier in at least 95% of cases, as far as I understood what \"prettier test suite\" means. reply harimau777 16 hours agoparentprevAgreed! I&#x27;d go so far as to say that we would be better off if Prettier was never created. reply paulddraper 16 hours agoparentprevI wish it were more opinionated.It treats new lines as significant in a lot of its formatting reply vbezhenar 16 hours agorootparentI also don&#x27;t understand their stand about curly braces. They don&#x27;t add or remove them. I think they should make it uniform. reply paulddraper 15 hours agorootparentThat&#x27;s bizarre. They add&#x2F;remove parens...but not curly braces...How do you even justify that... reply bakkoting 9 hours agorootparentCurly braces are reflected in the AST, parens are not. reply MalseMattie 16 hours agorootparentprevThere is a plugin for that: https:&#x2F;&#x2F;github.com&#x2F;JoshuaKGoldberg&#x2F;prettier-plugin-curly reply candiddevmike 16 hours agoprevI&#x27;m still salty that all my eslint plugins decided to remove perfectly fine linters in lieu of Prettier. I find Prettier to be way too heavy handed and hard to reason about, and yet another tool that I never asked for... reply thenbe 16 hours agoparentThe deprecated style rules have been ported by a new project: https:&#x2F;&#x2F;eslint.style&#x2F;guide&#x2F;why reply Dextro 11 hours agorootparentThanks for this. It completely escaped me and all I saw was the changelogs deprecating the very limited set of style rules I&#x27;ve used for years. reply Vinnl 12 hours agoparentprevIn what cases are you \"reasoning about\" Prettier? I can only think of the occasional issue with merge conflicts, perhaps. reply xdennis 16 hours agoparentprev> I find Prettier to be way too heavy handedBut that&#x27;s the purpose of such tools: to stop endless debates about style. reply harimau777 16 hours agorootparentThe problem is that style matters and therefore those debates matter. Just because something isn&#x27;t important to the developers that made Prettier doesn&#x27;t mean that it&#x27;s not important to the productivity of other developers. reply shepherdjerred 15 hours agorootparentThe reality is that as long as your style is consistent, very few people actually care what that style is.I can imagine smaller teams&#x2F;single individuals being very picky about how their code looks and there is nothing wrong with that. Once you have larger teams that becomes a waste of time since you&#x27;ll likely have much larger problems to solve. reply deathanatos 13 hours agorootparent> as long as your style is consistent, very few people actually care what that style is.This is an oft-repeated myth. There are some style rules for which that is true, but for some, there are objective, logical reasons to prefer one style over the other.For example, mandating the optional comma after the last time in a list whose items has been split over multiple times results in more readable and logical patches: if you mandate the comma, a patch that only adds items will only have added lines, whereas if you don&#x27;t, a patch adding items can have a mix of add&#x2F;remove lines.Pushing operators to the subsequent line makes it fundamentally easier to read as they all align, vs. a ragged right edge, and this is doubly important if the operators aren&#x27;t the same, as it makes that far more visible. (Though this is harder in some languages with odd behavior around this, such as JavaScript.) (This also affects patch readability in many languages, and in fact, I&#x27;d say patch readability is the driver of many objective reasons behind styling choices.)And so forth. I&#x27;d wager as many rules have logical reasons backing them as those that actually do boil down to literally just stylistic decisions. reply digging 15 hours agorootparentprev> The problem is that style matters and therefore those debates matter.Does it? IME&#x2F;O, styling only matters up until the point that it is consistent and reasonable. And I&#x27;m saying this as someone who used to want to debate and micromanage a lot of formatting standards in my projects. Ego is removed from the equation (unless you&#x27;re the one trying to introduce Prettier to a reluctant team), and without ego, the debates no longer seem very important. reply adam_arthur 7 hours agorootparentIs 6pt font easier to read than 12pt font? Are all style choices equivalent from first principles?No, not really. Many of the decisions prettier makes are quite objectively bad for readability from first principles (e.g. breaking up lines where the RHS isn&#x27;t relevant to most readers, left shifting RHS of assignments when the implementation details are less important than the abstraction over them).I am totally in favor of a universal formatter, but at least justify the design decisions being made. Prettier formatting choices are often very lackluster and hard to defend objectively.It will be replaced by something better eventually reply fenomas 5 hours agorootparentprevSure, but the issue with prettier is that it goes beyond style and also mangles semantic details. That&#x27;s their explicit design choice - they feel that conforming formatting in all cases is more important than preserving semantic information about code, and that devs should add \"&#x2F;&#x2F; prettier-ignore\" comments to every line of code with semantic details that prettier would throw away. reply pcthrowaway 16 hours agorootparentprevI mean.. sure https:&#x2F;&#x2F;xkcd.com&#x2F;927&#x2F; reply mm263 16 hours agorootparentIt isn&#x27;t really applicable here since Prettier was de-facto standard at pretty much every place I&#x27;ve worked at - start-ups to FAANG reply constantly 16 hours agorootparentprevIt&#x27;s not a new standard so this comic really doesn&#x27;t apply here. It&#x27;s just being opinionated, for once, about what to use. I welcome it strongly, as I do tools like Black for Python, which do similar things. Just set that up in CI for anything and all my code looks the same, removing some overhead of readability. reply namanyayg 17 hours agoprevWhile porting to Rust has been a trend, as Prettier runs on every save, the speed boosts will be significant. I&#x27;ll be trying out Biome soon. Congrats to the Biome project! reply spankalee 16 hours agoparentI have never noticed any lag from Prettier running on single files. The perf starts to matter with whole-repo format passes.For interactive use we really should be using long-running, warmed up, processes too, where the start time of Node is irrelevant. Ideally type-checking, linting, highlighting and formatting would run in one language service doing incremental parsing and updates to a shared AST on every keystroke. reply strager 15 hours agorootparent> Ideally type-checking, linting, highlighting and formatting would run in one language service doing incremental parsing and updates to a shared AST on every keystroke.I think this is the reason Biome (originally called Rome) started. Rome&#x27;s vision was a shared toolchain to yield better performance and to fix config hell. reply foreigner 2 hours agoparentprevI recommend using the lint-staged tool so Prettier only runs on every save on changed files. It makes a huge difference on large projects. reply conaclos 4 minutes agorootparentBiome is also working on a feature to only format changed files. See the associated PR [0] for more details.[0] https:&#x2F;&#x2F;github.com&#x2F;biomejs&#x2F;biome&#x2F;pull&#x2F;753 reply uoaei 16 hours agoparentprevThis effort reflects the excitement in the Python community around `ruff`. Excited to see efficiency and speed gains with a wide impact. reply meindnoch 16 hours agoprev\"This means that we can now focus on the next important aspect: Performance. Prettier has never been fast per se, but fast enough for most use cases. This has always felt unsatisfying so we wanted to do something about it. What better way than a friendly competition.On November 9th, we put up a $10k bounty for any project written in Rust that would pass 95% of Prettier test suite.\"I don&#x27;t see how better performance follows from the fact that something is written in Rust. One could have simply transpiled the existing codebase into Rust, and collect the reward. reply tubthumper8 15 hours agoparent> One could have simply transpiled the existing codebase into Rust, and collect the reward.I&#x27;m not sure if this is already an internet saying somewhere, but whenever I read the word \"simply\", I assume that whatever comes next won&#x27;t be simple, because if it was actually simple it wouldn&#x27;t need to be qualified.In this case, I don&#x27;t know that transpiling a JS codebase to Rust is simple. The mental models, the libraries used, the way that code written is quite different between the two languages and I doubt that JS-to-Rust transpilers are robust enough to be used on a codebase the size of Prettier, if such transpilers even exist at all. reply nicoburns 15 hours agoparentprev> I don&#x27;t see how better performance follows from the fact that something is written in Rust.Idiomatic Rust will often by 5-10x faster than very similar looking JavaScript&#x2F;TypeScript code without even trying to optimise it. It depends what you&#x27;re doing, and this doesn&#x27;t always apply. But parsers where you&#x27;re doing a lot of string manipulation are one of the cases where it definitely does. reply tills13 16 hours agoparentprev> One could have simply transpiled the existing codebase into Rust, and collect the rewardWhy didn&#x27;t you, then? reply meindnoch 14 hours agorootparentBecause this is the first time I&#x27;ve heard about this contest.But I&#x27;ll do it for you, if you put up the 20k USD. Will you? reply bufferoverflow 10 hours agoparentprevWhen it comes to string manipulation, Rust is much faster than JShttps:&#x2F;&#x2F;benchmarksgame-team.pages.debian.net&#x2F;benchmarksgame&#x2F;...https:&#x2F;&#x2F;benchmarksgame-team.pages.debian.net&#x2F;benchmarksgame&#x2F;... reply strager 16 hours agoparentprevHere is the answer I got from vjeux:> There&#x27;s a lot of fast web tooling being written in rust those days. https:&#x2F;&#x2F;twitter.com&#x2F;Vjeux&#x2F;status&#x2F;1722769322299609565I don&#x27;t buy it. I think vjeux is riding the hype. reply KTibow 5 hours agorootparentTo be fair compiled languages are just faster than interpreted (in most cases, and as others have pointed out especially with strings) reply fastball 9 hours agoprevAs an aside, the winning project (Biome) is a fork (renaming?) of the Rome project, which was started by Sebastian McKenzie (creator of Babel) a few years ago.Forked because sebmck seems to have gone AWOL over the last year or so and contributors were unable to update many of the project&#x27;s resources as only he had access. Hope he is doing ok though and glad the Biome project seems to be going strong regardless. reply tmm84 7 hours agoprevThis is something that really got me thinking about how to make my own prettier from scratch because I thought it was interesting. Needless to say, having looked at the “tests” for prettier I was kind of lost because I was trying to reverse engineer from the tests. What I saw was the format tests are auto generated and have if statements in them. If you’re a unit test stickler then seeing an if statement in a test should be a smell. The hardest part was trying to get the why behind each test. No explanation is in the test but I’ll bet it’s in the docs, source code or whatever cloud chat community.Also, I had to check the winning implementation just for sanity’s sake. It is older than a week or two. Still, my hats off to them for being able to meet the compatibility mark and get the cash. reply nulld3v 16 hours agoprevAre there benchmarks for Biome anywhere? How much better does it perform than prettier exactly? reply nulld3v 14 hours agoparentFound some here: https:&#x2F;&#x2F;github.com&#x2F;biomejs&#x2F;biome&#x2F;blob&#x2F;main&#x2F;benchmark&#x2F;README....They claim 25x but the numbers are old so I&#x27;m not sure if I believe them now that a bunch of new functionality has been added. Either way, if it&#x27;s anywhere within that ballpark it&#x27;s still a huge achievement. reply conaclos 11 hours agorootparentThis heavily depends on your workstation. Biome scales very well. With 16 threads on a i7-120P, I got the following figures:Webpack repository: Biome ran 2.19 ± 0.11 times faster than dprint 4.18 ± 0.14 times faster than Biome (1 thread) 32.12 ± 1.18 times faster than Prettier 32.45 ± 2.56 times faster than Parallel-PrettierI am not sure why Parallel-Prettier is slower than Prettier for the webpack repository.Prettier repository: Biome ran 1.89 ± 0.21 times faster than dprint 3.47 ± 0.34 times faster than Biome (1 thread) 36.70 ± 3.41 times faster than Parallel-Prettier 46.66 ± 4.32 times faster than Prettier reply andrewstuart 16 hours agoprevThe post says the reason they did this was to motivate themselves to make the javascript version better?Strange. reply sophacles 16 hours agoparentCompetition is good for the consumer (i.e. the users). For example: when clang and LLVM came along, gcc got significantly better. reply thowaway91234 16 hours agoprevSo this isn&#x27;t replacing the main implementation? They just funded a new project written in Rust that is compatible? reply jrockway 16 hours agoprevThat&#x27;s interesting. I&#x27;ve been using \"deno fmt\" as of late and it&#x27;s been fine for me.https:&#x2F;&#x2F;docs.deno.com&#x2F;runtime&#x2F;manual&#x2F;tools&#x2F;formatter reply tills13 16 hours agoparentFWIW it doesn&#x27;t matter which formatter you choose, so long as everyone working on the project is required to use it. reply jkrubin 15 hours agorootparent100% I also personally believe you should pick a formatter and NOT configure it. I don’t care anymore about style. Just want formatting to be consistent. reply jrockway 15 hours agorootparentYeah, that&#x27;s my take as well. The less there is to configure, the better. Something I like about Go is that I don&#x27;t need any project-specific settings, because there are none. (Not strictly true, as some people use gofumpt instead of gofmt. I think this is a bad idea, even though I agree with all of gofumpt&#x27;s formatting changes. What matters is will everyone use it on every commit? Not if gopls doesn&#x27;t do it for you.)At my last job, we had a lot of fun with Prettier. We basically made our Typescript codebase look as much like Go as possible. I am not sure this is a particularly valuable endeavor, but it was fun. So if people are willing to spend $20,000 on making that faster, whatever. They can have that fun, I guess. reply culi 12 hours agorootparentprevIn the same way it doesn&#x27;t matter which side of the road you drive on. What matters is that everyone else is also driving on that side reply conaclos 16 hours agoparentprevIf you switch from Prettier to deno-fmt you will notice many changes to perform on your code base. Biome tries to minimize these changes. reply 11235813213455 16 hours agoprevI dislike prettier and wonder why it&#x27;s so popular because- I sometimes prefer lines to exceed maxLength (ex: template string, which prettier would horribly break on vars)- allow `1 + 2 * 3` or `a || b && c` to be parenthesis-less because everyone know the precendence here reply digging 15 hours agoparent> I sometimes prefer lines to exceed maxLength (ex: template string, which prettier would horribly break on vars)Prettier doesn&#x27;t break a long template literal though...?> allow `1 + 2 * 3` or `a || b && c` to be parenthesis-less because everyone know the precendence here\"Everyone knows\" is usually an unwise assumption. And adding parentheses does make it easier to read even for people who know their order of operations. reply sarahdellysse 15 hours agorootparentNot OP, but prettier doesn&#x27;t break template literals, but it will break non-literal sections of template strings. For example, if we wrote this: const foo = `aaaaaaaaaaaaaaa${bbbbbbbbbbbbb}ccccccccccccccc${ddddddddddd}`;prettier would fix it to const foo = `aaaaaaaaaaaaaaa${ bbbbbbbbbbbbb }ccccccccccccccc${ ddddddddddd }`;(or something similar) and it would be fully equivalent.I don&#x27;t like it doing that either tbh but hey prettier is good enough in most cases its worth putting up with it reply bakkoting 9 hours agorootparentI have a PR fixing that specific issue: https:&#x2F;&#x2F;github.com&#x2F;prettier&#x2F;prettier&#x2F;pull&#x2F;15209Just needs another maintainer&#x27;s stamp. reply vinniepukh 13 hours agoprevI want to use Biome, but until it either has plugins or implements the tailwindcss class sorting itself, I will not be able to adopt it.This is a common sentiment in the fullstack community right now. reply neontomo 15 hours agoprevI couldn&#x27;t find the answer - does Biome work with a prettierrc.json file so that it can be implemented in an organisation that uses Prettier? In this case it would only be for speed reasons, not the formatting. reply bntyhntr 13 hours agoparentI would love to hear about any of this. I couldn&#x27;t find a clear migration path with some quick googling, but I managed to eventually track down the rules I need to get mostly compatible with my org&#x27;s prettier formatting. Notably, stuff I needed was in the `javascript.formatter` section of config. But even after matching everything, Biome was wrapping lines differently. Not sure but I don&#x27;t actually run the front-end show so I stopped poking at that point. reply jdorfman 16 hours agoprevThank you for posting this. Sourcegraph is now a sponsor on Open Collective. reply mvdtnz 13 hours agoprevSo now there are two distinct, somewhat compatible but surely diverging implementations of the same thing. Yup, sounds like the JS ecosystem. reply codeptualize 16 hours agoprevAwesome! Lets gooo!Can&#x27;t say prettier performance ever bothered me, but if it can run faster and leaner I&#x27;m all for it! Increase my battery life :DIn general it&#x27;s really exciting to see the advancements in JS tooling, the quick successions of tooling in JS might be controversial, but I love it. Each step was a significant improvement over what was there before, and the current Rust rewrite wave has already given us great tools that I use daily.I also like that there is some money being thrown at the problems! reply smnscu 16 hours agoprevI feel like people complaining about Prettier being \"heavily opinionated\" are missing the point. Perhaps I&#x27;m biased by my decade of using Go, but not having to worry about superfluous stylistic choices is a welcome reduction in cognitive load. (Up to a point I guess, e.g. I&#x27;d never put up with K&R-style newline opening brackets). reply pigeonhole123 16 hours agoparentPrettier is very opinionated about line length which gofmt isn&#x27;t. That&#x27;s the only complaint I have and it&#x27;s bad enough I just refuse to use it. Add one character to a line and enjoy a ten-line diff.Edit: See a contrived example of something gofmt doesn&#x27;t touch (the behavior I want): https:&#x2F;&#x2F;go.dev&#x2F;play&#x2F;p&#x2F;cKMKnFwT8tq reply tuetuopay 16 hours agorootparentFunny, that&#x27;s the one thing I complain about gofmt. I can cope with the occasional diff noise but avoid kilometer-long lines. This is especially noticeable for function names that get long quite quickly.Also, this means that there is more than one way to format the code, which stands pretty weird given the philosophy of Go. reply pigeonhole123 14 hours agorootparentLine breaks often carry meaning, for example many people like to line up things that belong together but which live on separate lines, and gofmt helps with this. Having many ways to format a given AST (unlike Prettier) is not against the Go philosphy which I would say is \"be pragmatic\".If you want a line to be shorter because you as a human find it easier to read that way then you can add a linebreak yourself, and trust that your meaning will be preserved. reply nicoburns 15 hours agorootparentprevI don&#x27;t understand the hate for \"kilometer-long\" lines (say in the 120-200 char range). My screen is much wider than it is long, so having longer lines allows me to fit more code onto the screen at once (which is fantastic for readability). Also, sometimes the extra content is uninteresting. And it makes sense to hide it away where it&#x27;s only sometimes seen. reply tuetuopay 14 hours agorootparentI&#x27;m modern, and a 120 char line is not kilometers long. It&#x27;s in the range for modern stuff (yep, I&#x27;m an advocate for dropping 80 chars). km long lines start at 200 chars IMHO. To me, too long lines actually hinders readability:- kilometer-long lines makes your eyes travel a lot more to read what is happening. travel is not that an issue, but context is. it is quite hard to get at a glance e.g. the argument list of a function. or know if e.g. a specific parameter is in the argument list.- I almost never have my editor&#x27;s viewport set to kilometers. It&#x27;s usually in the ~110 chars range to fit multiple files at once (two side-by-side on 1080p, three on 1440p). Not that I am editing three files at once, but I&#x27;m usually editing the middle one with other files on the side for context and reference. In such a setup, the line ends up wrapped anyways, but with ugly wrapping that does not match indentation and in the middle of words.As for hiding it, that&#x27;s what editor folds are for.Anyways, I guess we found bikeshedding topic not solved by formatters :D reply WorldMaker 13 hours agorootparentprevWider screens can also be used to have more code files or sections of the same file side-by-side. You can get two, three, sometimes four 80-char width files very nicely side by side. reply j1elo 16 hours agorootparentprevFunny thing: I recently learned Go, coming from doing some TypeScript and the refreshing feel that was discovering Prettier.I really miss that gofmt applied some limit to line length. In TS I just write a too long line, and Prettier reformats it into a sensible set of consecutive lines, I don&#x27;t even have to think if breaking before or after this or that parentheses or bracket. With Go, I have to, and I&#x27;d rather not. reply pkilgore 15 hours agorootparentprevWhy throw out the tool when you can have the tool ignore [1] whatever is outside the common case?[1] https:&#x2F;&#x2F;prettier.io&#x2F;docs&#x2F;en&#x2F;ignore.html reply pigeonhole123 14 hours agorootparentMy approach is to just not use Prettier but instead use the much more well behaved formatter included in VSCode. I don&#x27;t have time to fight the formatter and I don&#x27;t like having these \"cheat code\" comments all over my code reply solatic 15 hours agorootparentprev`lineWidth` is actually one of the available options in the biome formatter: https:&#x2F;&#x2F;biomejs.dev&#x2F;formatter#configuration reply nicoburns 15 hours agorootparentYeah, but if you increase the lineWidth then it will make all your lines that long, even if you don&#x27;t want them to be. There is no possibility for variability of line lengths depending on the code at hand. reply atom_arranger 16 hours agorootparentprevWhat should Prettier should do in this case?I think we can all agree there should be a line length limit, it has to enforce it eventually. You could say “it’s just a couple more characters” until the line is 200 characters long.Semantic diff is maybe the solution. reply nicoburns 15 hours agorootparentIt should put more weight on the length that the author has original authored it as. So between \"always break\" and \"always expand\" there should be an area of \"leave it however it already is\". reply zeroonetwothree 8 hours agorootparentPrettier needs to be consistent regardless of the original input. Otherwise it won’t really work well. reply pigeonhole123 14 hours agorootparentprevIt can safely ignore the line length, gofmt does this and I&#x27;ve never heard anyone complain about it. The VSCode formatter also doesn&#x27;t touch line breaks and it works fine.Last time I looked into it, the only reason I can&#x27;t turn it off is that Prettier works off an AST that doesn&#x27;t keep the line breaks that the user put into the code at all, and it \"rebuilds\" the whole code from this AST.The problem is not the diff per se, the real problem is that I can&#x27;t find a configuration of Prettier where I can have long lines where it makes sense and short ones where that makes sense. reply preommr 16 hours agorootparentprevThen just change it in the prettier config.80 characters is a really good default. I have an extra wide monitor, and with 2 side panels open in my IDE, that&#x27;s the perfect length. reply pigeonhole123 14 hours agorootparentIt will then force longer lines, which is even worse. reply warp 15 hours agorootparentprevCouldn&#x27;t you just set the \"printWidth\" in .prettierrc.json to something super long? reply laurent123456 14 hours agorootparentIn that case prettier will put the code you formatted over multiple lines into one giant line spanning multiple screens. This is especially relevant for method chaining. reply 1letterunixname 8 hours agorootparentprevLine length with modern toolchains is too often stylistic bikeshedding with awkward limits stuck in 1975 cargo culted from the VT52. My external monitor can legibly display 637 x 81 while my laptop does 217 x 63. The largest barely legible terminals with tiny fonts are 3x the size.Around 100 is a sane minimum while 160 is a sensible upper limit. There&#x27;s no specific limit other than to prevent poor coding technique, illegible code, or inclusions of generated or minified atrocities. reply wg0 14 hours agoprev> Consider donating if you or your company are using Prettier and it has been helpful to you.Wondering what company would NOT be using prettier. Not many.Some arguing that Prettier isn&#x27;t foundational piece of software, I&#x27;d consider automated consistent readability enhancements as foundational though. reply francislavoie 10 hours agoparentIMO it should be called uglier. The default line wrapping logic (which cannot be disabled at all, by design) is awful. It unwraps short lines that I want wrapped, and it wraps long lines that I want unwrapped. It&#x27;s banned from all my projects. reply bovermyer 15 hours agoprevSo, how does Biome compare to Dprint? reply rattray 6 hours agoparentClaims to be faster (when using multiple cores) but beyond that, I&#x27;m also curious. reply ehutch79 16 hours agoprevI&#x27;m looking through the docs, specifically the formatter stuff ( https:&#x2F;&#x2F;biomejs.dev&#x2F;formatter ), and it feels like all the examples are backwards?Am I missing stuff? reply explaininjs 16 hours agoparentThat’s giving the diff between prettier (“correct”, yet red) and their implementation (green, yet arguably incorrect) reply surye 16 hours agorootparentIt feels backwards to me from a purely linear time perspective, you have an input, transform it, diff would be the patch, to the desired output. reply ufo 8 hours agoparentprevThose diffs are rare corner cases where the input is invalid Javascript. Prettier&#x27;s parser is permissive and emits beautifully indented nonsense (which is arguably a bug in prettier). For example, in the original program there is a \"private public\" but the prettier output in red only says \"private\". Biome decided that in these cases it should not touch the indentation at all. The output is indented exactly the same as the input. The concern is that if you reindent something that you don&#x27;t understand, it might break it. https:&#x2F;&#x2F;github.com&#x2F;biomejs&#x2F;biome&#x2F;issues&#x2F;739 reply conaclos 16 hours agoparentprevWhat do you mean by \"backwards\"? reply surye 16 hours agorootparentThe \"diff\" appears to be backwards (it&#x27;s subtracting what prettier would emit, and adding back the original). reply ematipico 16 hours agorootparentThe red represents what would Prettier emits, and the green represents what Biom e emits. If you think that&#x27;s unclear, feel free to send a PR to help us make it clearer. reply globular-toast 14 hours agorootparentThis was intuitive to me, especially because it&#x27;s in a section titled \"Differences with Prettier\". reply explaininjs 16 hours agorootparentprevThe diff is between the test cases’s expected output (red) and the utility’s current output (green). reply kamikaz1k 14 hours agoprevDoes any body have a collection of all the profiling Fabio Spampinato did? reply fabiospampinato 12 hours agoparentI haven&#x27;t really kept track of everything. It may be worth making a blog post with the main findings though. reply afjeafaj848 15 hours agoprevMaybe I missed it but I don&#x27;t get how this improves the JS implementationAre they planning to run Rust in the browser? Or make some sort of node module that calls down into rust? reply adam_arthur 15 hours agoparentRust can run in the browser via compiling down to, and running as, WebAssembly. So I don&#x27;t see why not.Prettier is run mostly on the serverside though, not the browser. I assume supporting both is still desirable though reply WorldMaker 14 hours agoparentprevThere are node modules already that call into WASM-wrapped rust. (One of the most common source map generation packages does exactly that and it is very heavily installed by a lot of frameworks and packagers today.)I don&#x27;t think prettier is intending to do that in this case, they seem to want to continue to compete as a pure JS implementation but having a \"worst case, we WASM wrap biome as the future of prettier\" possibility opens up future opportunities and pushes \"internal\" competition. reply globular-toast 14 hours agoprevWhy did it have to be Rust? Couldn&#x27;t have just been \"faster\"? Is the Rust one actually fast? Does memory safety and leaks etc even matter for a program like Prettier? reply WorldMaker 13 hours agoparentThere&#x27;s a lot of vocal people especially here on HN that think that Rust is the fastest, safest language currently around, so this was maybe in part a \"put your money where your mouth is\" challenge. reply hendry 14 hours agoprevAnyone know the minimal package.json or Github template to get this going in a new repository please? reply laurels-marts 11 hours agoprevnoob question but how does one actually use prettier? i find that the formatting just make the code less readable. is the goal to train yourself to be able to read prettier-formatted code as well as you could read your own hand-formatted code?when i&#x27;m in the thick of it working on a piece of code, writing some logic and have everything laid out the way that makes sense to me and then i hit cmd + s and BOOM all the lines get shifted and wrapped and i&#x27;m ripped out of my flow i wondering what the heck it is i&#x27;m now staring at. sometimes i disable prettier on save and just run it on a pre-commit hook because at that point who cares that the code is unreadable to me. reply razodactyl 10 hours agoprevCongratulations to the winning team. reply assimpleaspossi 15 hours agoprevTheir next bounty should be for fixing their insistence on putting a closing slash on HTML tags that have never, ever required them in any HTML specification. reply anshulbhide 16 hours agoprevWould be cool to use a Replit Bounty next time! reply refulgentis 16 hours agoprev\"[to] get Biome running in Wasmer\", it seems reply thenbe 16 hours agoprevWhile it&#x27;s always great to see performance gains, my largest pain point with prettier was never performance. Instead my only gripe with prettier is the \"line wrapping noise\" it creates, illustrated here by Anthony Fu: https:&#x2F;&#x2F;antfu.me&#x2F;posts&#x2F;why-not-prettier#the-line-wrapping-no...Would it be realistic to expect a solution for this issue now that \"prettier needs to step up it&#x27;s game\"? reply j1elo 16 hours agoparentI&#x27;d rather have a strict line length limit, than having my coworker creating objects in lines 150 or 180 chars long.So we&#x27;d end up discussing what is the best choice. I bet I&#x27;d also end up discussing those things with Anthony Fu. If the limit is 80, then the limit is 80, not 81.Come Prettier. No more discussions. I definitely buy the tiny amount of \"noise\" it brings, in exchange for freeing me from an immense amount of actual noise when having to discuss these things with other people.EDIT: This comes from a backend dev (C&#x2F;C++, sometimes Go, recently did some stuff with TypeScript). Prettier was a refreshing discovery, and other languages like Python are able to express the rule very sensibly (albeit I round it and go for 80 or 100):https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0008&#x2F;#maximum-line-length reply nicoburns 15 hours agorootparent> If the limit is 80, then the limit is 80, not 81.I agree with that. But the limit should be 120 or 160, not 80 (and my formatter should allow me to set a wider limit like that without making all my lines extra-wide - I want to be able to put things on one line where appropriate and not where it&#x27;s not).> I definitely buy the tiny amount of \"noise\" it bringsTiny? It makes a lot of my code 3-5x as long. And often breaks things in weird places. This is IMO not a small reduction in readability. reply wentin 14 hours agorootparentDo you work with a large team? There is no correct answer for formatting for a team, there is only correct answer for individual person. The line limit 80&#x2F;120&#x2F;160 will work for certain people with their setup, but not others. Stuff like using a ultrawide screen with two columns, or code on laptop screen with single column, or code with half screen code editor and half screen browser, etc, there are endless mutation, all of them can benefit from different settings. It is essentially not possible to find the best option for everyone on the team. You may think 120 is the best, but there is no way to prove it. It just worked for you because of your coding setup and preference. reply kuchenbecker 11 hours agorootparentI don&#x27;t care what the limit is unless it&#x27;s consistently applied.My least favorite though no limit + soft wrapping, the philosophy being the code adapts to the user, but in actuality means the file looks completely different based on your monitor and setup removing visual aid to code navigation and familiarity. reply dieselgate 11 hours agorootparentI work on a ruby&#x2F;ts&#x2F;js codebase and seem to recall different file extensions having different line lengths. Something like that is annoying but at least it&#x27;s more consistent within the file extension reply j1elo 14 hours agorootparentprevLike I mentioned to the sibling parent, 100 is an OK middle ground. 120, or more, is too long already, and 160 is waaaay beyond what I&#x27;d consider acceptable. No way you can fit 2 side-by-side editor panes with those line lengths, unless you use a tiny sized font.I get it, 160 looks OK and fits into a 4K display without any other windows open. I believe working with dual panes is more productive, so I&#x27;ll always stand behind shorter line lengths that allow for it.Even Rust, a modern language that is usually said to collect the best learnings from the industry, thankfully chose a conservative and sensible 100 chars limit by default. reply culi 12 hours agorootparentI wasn&#x27;t aware Rust chose a 100 line default. I&#x27;ll definitely be using this to argue on my teams for why we should stretch the line length limit past 80. Thank you Rust for moving the industry forward reply j1elo 11 hours agorootparentI guess Rust made the same reasoning than Python. For this kind of things, the PEP documents tend to be well based on experience and be a good guideline which even applies for other languages. Check the PEP 8 that I linked in my comment: although they recommend a very conservative limit of 80 (79 actually) it says that if it makes sense, 100 (99) can be used too. And that&#x27;s from 2001. reply culi 12 hours agorootparentprev> It makes a lot of my code 3-5x as long.Do you mean entire files are made 3-5x as long or just individual lines every now and then? reply mvdtnz 12 hours agorootparentprevI understand not wanting to bike shed on the actual number but I refuse to accept 80. I simply do not agree that the greybeards with 70s technology got it figured out perfectly in their day and nothing has changed. reply The_Colonel 13 hours agorootparentprev> If the limit is 80, then the limit is 80, not 81.I don&#x27;t see a reason to enforce this strictly. The goal is readability, the max line length is an approximate proxy for that. reply j1elo 13 hours agorootparentI agree, actually :)I ignored this point on purpose, to be more succint. But yes, it could be possible to have a soft limit (e.g. 80, or 100) and then you would have to set a hard limit (say, 85 or 105). But in the end you&#x27;d end up having someone who complains because their beautiful line was 84 chars and after adding the closing paren and the semicolon, it got wrapped into something that they don&#x27;t like because subjective tastes.So in the end, we just moved the goalpost +5 chars. reply ipaddr 15 hours agorootparentprev80 character limit? Are we trying to relive the late 80s. Soon we&#x27;ll move back to 40 characters. Screens can support higher resolutions now limiting lines to 80 so developers can turn their brains off seems silly. Turn your brain on and learn to filter the noise of 100 characters. reply wentin 14 hours agorootparentPeople use ultrawide screen, with half screen dedicated to browser, half screen for code editor, and two columns open in editor. This is very modern setup, and 80 is perfect for it. Don&#x27;t limit your thinking to your own setup.I think discussion on the perfect limit on public forum is non-sense. It might be slightly less non-sense to discuss it with your team, but even that I think it is bike-shredding most of the time, unless the entire team for some reason (like they share the same equipment setup and happen to have same preference) overwhelmingly share the sentiment. reply The_Colonel 13 hours agorootparent> People use ultrawide screen, with half screen dedicated to browser, half screen for code editor, and two columns open in editor. This is very modern setup, and 80 is perfect for it. Don&#x27;t limit your thinking to your own setup.It kind of sounds like you&#x27;re promoting your own setup.The funny thing I see the most is that people with these crazy widescreen monitors still maximize just one window, with just one file open. Not sure why, maybe for focus? reply wentin 5 hours agorootparentNot promoting my setup, but just to point out that 80 is a sensible default, and it is not based &#x2F; targeted for ancient old rare setups. reply j1elo 15 hours agorootparentprev100 is OK. But better to wager for 80, so people complain about their preferred 120+, and ending up with an acceptable 100 as a middle ground.On my screen with 1920x1080, 2 side-by-side panels can fit 100 chars, but only if the sidebar is hidden (project layout, list of open files, that kind of stuff).On my laptop, 2 side-by-side files won&#x27;t fit if they exceed 90 chars.I just don&#x27;t want to concede to those devs who use a single editor pane with an ultrawide monitor, and believe that everybody must work like they do. reply hoherd 14 hours agorootparentprevThese are web developers. Of course they&#x27;re going to try to force content into a narrow width view and not let you choose to have a wider view even if it makes perfectly valid sense for the given content. These are usability problems that people aren&#x27;t allowed to make their own decisions about because the UI designer knows the only right answer. \"It makes the content more readable\" and all that. &#x2F;s reply Cthulhu_ 13 hours agoparentprevIt&#x27;s not a dichotomy though; performance is imo essential for a tool that will run on every save, every commit, every pull request. It might be fast enough, but adding all runs up adds up to a lot of unnecessary energy waste.With regards to the whitespace issue, that&#x27;s down to the line length rules used I think. It&#x27;s also down to the diff viewer how to show it, not the formatter. reply varrock 16 hours agoparentprevSpecifically for reviewing a pull request in GitHub, wouldn&#x27;t the \"Hide whitespace\" setting reduce some of this noise? I could be mistaken, though, but that&#x27;s how I interpreted that setting.0: https:&#x2F;&#x2F;github.blog&#x2F;2011-10-21-github-secrets&#x2F; reply sltkr 15 hours agorootparentThis is often useful, but JavaScript specifically has the annoying property that newlines can be semantically meaningful.For example, if someone changes: function isUserBanned(username) { return db.findUserByName(username)?.banned; }To: function isUserBanned(username) { return db.findUserByName(username)?.banned; }you want to see that diff because the second version always returns undefined. If you ignore whitespace changes entirely, it becomes possible for people to sneak in bugs intentionally or unintentionally. reply HALtheWise 14 hours agorootparentHow much does prettier formatting help here for practical cases? In particular, if the autoformatter allowed that second example with the indentation on the last line, I&#x27;d treat that as an autoformatter bug. reply blauditore 13 hours agorootparentprevThat&#x27;s technically true, but extrememly rare to cause real problems (short of bad intent). It reminds me of people&#x2F;teams enforcing braces on single-line ifs, because one might add another line someday, forget to add braces, and break the logic. Even when it happens, there should still be tests that catch this. reply Shish2k 12 hours agorootparentOn the one hand, yes, there should be tests. On the other, `goto fail;` :PAlso, adding braces from the start means that adding one new line of code is a one-line patch, instead of a four-line one - I do that for the same reason that I always put trailing commas on my array definitions reply tharkun__ 15 hours agorootparentprevEnforce semicolons. reply chris_wot 15 hours agorootparentprevgawd and this is why the semicolon debate wasn&#x27;t just bike shedding. reply dsherret 10 hours agoparentprevYou can already mostly do this with dprint&#x27;s TypeScript plugin if you tweak the config (and I&#x27;m open to adding more config to support this scenario in case something is missing). For example, the TypeScript team uses a line limit of 1000: https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;TypeScript&#x2F;blob&#x2F;1797837351782e7... reply ljm 16 hours agoparentprevPrettier’s biggest win is that it automates 99% of style complaints away and practically eliminates most classes of nitpicking.But, as well as the issue with line noise, it also encourages patterns that I think detract from code comprehension. It favours expressions over statements and even now, it’s not easy to set a breakpoint in the middle of one, so you end up rewriting into statements just so you can step through.It will favour deeply nested ternary statements in react so your code reads more like a tree with densely tangled roots.It will favour shorthand syntax for optionally merging properties into an object, which basically relies on a quirk of the splat operator.There is fuck all standard library to speak of without pulling in an insane amount of dependencies, but surely stuff like deep merge and compact should be provided out of the box? reply tubthumper8 15 hours agorootparent> It favours expressions over statements and even now, it’s not easy to set a breakpoint in the middle of one, so you end up rewriting into statements just so you can step through.I&#x27;ve never had an issue setting an inline breakpoint[1] in VS Code, is it an issue in other IDEs?[1] https:&#x2F;&#x2F;code.visualstudio.com&#x2F;Docs&#x2F;editor&#x2F;debugging#_inline-... reply ljm 15 hours agorootparentI use emacs, but chrome&#x2F;firefox are a bit finicky about where you can set a breakpoint.(Most people I know just use console.log - print debugging works all the time but I like having a repl) reply yawboakye 15 hours agorootparentprevdoes it? prettier is extremely configurable, unlike gofmt. so deferring to the authority of prettier is essentially deferring to the authority of the prettier.yml config. not that i have a problem with that _per se_, i’d expect the author(s) to take responsibility and appreciate that they’re defining&#x2F;imposing their own taste(s). reply hu3 14 hours agorootparent> prettier is extremely configurableThat&#x27;s the opposite of what they claim: https:&#x2F;&#x2F;prettier.io&#x2F;docs&#x2F;en&#x2F;option-philosophy reply nicoburns 15 hours agorootparentprevExtremely configurable? It has like 3 config options, and an explicit policy of not being configurable. reply adam_arthur 15 hours agoparentprevYes, Prettier largely won by not having many competitors strictly focused on formatting in a simple to consume package.I don&#x27;t think many people who are serious about high \"signal to noise\" code formatting are supportive of the design decisions prettier makes. e.g. the staggered import lines, left-shifting and up-shifting of implementation details, not allowing trailing comments on the same lineWe can have consistent formatting and also avoid tons of visual noise that prettier produces... I&#x27;ve wanted to build a competing solution for awhile, but never made the time for it. Perhaps Anthony&#x27;s project achieves that... I&#x27;ll give it a try! reply c-hendricks 16 hours agoparentprevSome things overlooked in that blog post for others to take into consideration:- eslint only works on javascript + typescript (eslint + typescript needs _more_ configuration than eslint + prettier), while prettier works on https:&#x2F;&#x2F;github.com&#x2F;prettier&#x2F;prettier&#x2F;blob&#x2F;03ebc7869dc9e8f2fc...- eslint + prettier doesn&#x27;t need lots of configuration from the user. You add eslint-plugin-prettier and say `\"extends\": [\"plugin:prettier&#x2F;recommended\"]` reply thenbe 12 hours agorootparentBy default, eslint only lints files with a .js extension[1]. Eslint plugins are what allow eslint to support more languages. A list can be found here[2].For the record, prettier can also be extended to support more languages[3].[1] https:&#x2F;&#x2F;eslint.org&#x2F;docs&#x2F;latest&#x2F;use&#x2F;command-line-interface#--...[2] https:&#x2F;&#x2F;github.com&#x2F;dustinspecker&#x2F;awesome-eslint#plugins[3] https:&#x2F;&#x2F;prettier.io&#x2F;docs&#x2F;en&#x2F;plugins#official-plugins reply jakub_g 16 hours agoparentprevThis is indeed the biggest annoyance of mine. I quite often end up rewriting code or changing variable names to counterbalance prettier making code ugly&#x2F;unreadable. reply tills13 16 hours agoparentpreveh -- this is a one-time occurrence with prettier. Subsequent changes are guaranteed to be changes-only since formatting is consistent between authors. reply c-hendricks 16 hours agoro",
    "originSummary": [
      "Prettier, a JavaScript code formatter, has offered a $20,000 bounty for a Rust project passing 95% of its test suite, which was successfully claimed and completed by the Biome project.",
      "This competition has led to performance and compatibility improvements in Prettier, thanks to bug fixes spurred by the bounty.",
      "Prettier has received substantial donations from companies and individuals, enabling ongoing development, while Open Collective has facilitated financial transactions and tax handling for the project."
    ],
    "commentSummary": [
      "The Prettier team is offering a $20,000 bounty to encourage the development of a new code formatter project in Rust, as they believe there is little competition in this space.",
      "The article explores the costs freelancers incur, hourly rates for programmers, and factors influencing billing rates.",
      "The discussion covers the motivations of open-source maintainers and the importance of diverse perspectives in development projects, as well as debates surrounding the effectiveness of Prettier and alternative code formatters."
    ],
    "points": 677,
    "commentCount": 306,
    "retryCount": 0,
    "time": 1701103820
  },
  {
    "id": 38431743,
    "title": "Google Drive users experience data loss as files mysteriously vanish",
    "originLink": "https://www.theregister.com/2023/11/27/google_drive_files_disappearing/",
    "originBody": "Storage 80 Google Drive misplaces months' worth of customer files 80 The horror of logging in only to find everything since May has vanished Richard Speed Mon 27 Nov 2023 // 12:36 UTC Updated Google Drive users are reporting files mysteriously disappearing from the service, with some netizens on the goliath's support forums claiming six or more months of work have unceremoniously vanished. The issue has been rumbling for a few days, with one user logging into Google Drive and finding things as they were in May 2023. According to the poster, almost everything saved since then has gone, and attempts at recovery failed. Others chimed in with similar experiences, and one claimed that six months of business data had gone AWOL. There is little information regarding what has happened; some users reported that synchronization had simply stopped working, so the cloud storage was out of date. Others could get some of their information back by fiddling with cached files, although the limited advice on offer for the affected was to leave things well alone until engineers come up with a solution. A message purporting to be from Google support also advised not to make changes to the root/data folder while engineers investigate the issue. Some users speculated that it might be related to accounts being spontaneously dropped. We've asked Google for its thoughts and will update should the search giant respond. Google Cloud's watery Parisian outage enters third week, with no end in sight Google Workspace weaknesses allow plaintext password theft Google doubles minimum RAM and disk in 'Chromebook Plus' spec Google Bard can now tap into your Gmail, Docs, more In the meantime, the experience for affected users is a reminder that just because files are being stored in the cloud, there is no guarantee that they are safe. European cloud hosting provider OVH suffered a disastrous fire in 2021 that left some customers scrambling for backups and disaster recovery plans. Google itself has suffered the odd outage or two over the years. Earlier in 2023, the mega-corp's europe-west9 region took a shower after water made its presence felt inside a Parisian Google Cloud datacenter. Ultimately, trusting one's data – particularly data on which a business depends – to any sort of cloud storage should only be done after fully understanding the implications of the services' terms and conditions. Just because the files have been uploaded one day does not necessarily mean they will still be there – or recoverable – the next. ® Updated to add MatthewSt reports that he has a fix; obviously this is something worked out by a user rather than official advice, so caution is advised. Sponsored: How cyber training can help you beat the bad guys Share More about Google Storage More like these × More about Google Storage Narrower topics Android App stores Backup Blu-Ray Chrome Chromium DRAM Google AI Google Cloud Platform Google I/O Google Nest G Suite HDD Kubernetes Network Attached Storage Pixel Privacy Sandbox Semiconductor Memory Snowflake Inc. Tavis Ormandy Broader topics Alphabet Search Engine More about Share 80 COMMENTS More about Google Storage More like these × More about Google Storage Narrower topics Android App stores Backup Blu-Ray Chrome Chromium DRAM Google AI Google Cloud Platform Google I/O Google Nest G Suite HDD Kubernetes Network Attached Storage Pixel Privacy Sandbox Semiconductor Memory Snowflake Inc. Tavis Ormandy Broader topics Alphabet Search Engine TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=38431743",
    "commentBody": "Google Drive misplaces months&#x27; worth of customer dataHacker NewspastloginGoogle Drive misplaces months&#x27; worth of customer data (theregister.com) 490 points by ubutler 21 hours ago| hidepastfavorite266 comments jsnell 20 hours agoDiscussion earlier today: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38427864 reply dang 14 hours agoparentThanks! Macroexpanded:Google Drive files suddenly disappeared - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38427864 - Nov 2023 (281 comments) reply r3trohack3r 17 hours agoprevAnecdotal, but with Google Photos, I&#x27;ve observed data loss to the point where I no longer trust the service with anything valuable.Photos I know I&#x27;ve taken are missing. There are periods of time when I walked around a city 10+ years ago taking a large volume of photos, like when I first moved to Seattle. Going back to that window of time in Google Photos, I only have a handful of photos from that walk.There are also partially corrupted photos from many years back. Photos that only partially render, or render as noise.Luckily, all of these are random low-value photos from my youth. They aren&#x27;t core memories of my kiddos growing up or anything. I&#x27;m glad I discovered the data loss in that window of time, and not by losing photos of my kiddos&#x27; birthdays. reply beeandapenguin 15 hours agoparentTangentially related, if you&#x27;ve ever had the pleasure of migrating away from Google Photos&#x2F;Drive with Google Takeout, prepare to spend some time fixing photo metadata in your library and re-embedding exif data with exiftool. Takeout strips out the exif data into a non-standard JSON sidecar, as opposed to something more standard&#x2F;well supported like XMP.It&#x27;s unfortunately hostile for non-technical users that care about their photo metadata, which I assume is most people since it includes data such as creation timestamps and location. It&#x27;s not too bad to script this if you&#x27;re savvy (and careful), but otherwise you&#x27;ll have to pay for a third-party tool[1][1]: https:&#x2F;&#x2F;metadatafixer.com&#x2F; reply gst 14 hours agorootparent> Tangentially related, if you&#x27;ve ever had the pleasure of migrating away from Google Photos&#x2F;Drive with Google Takeout, prepare to spend some time fixing photo metadata in your library and re-embedding exif data with exiftool. Takeout strips out the exif data into a non-standard JSON sidecar, as opposed to something more standard&#x2F;well supported like XMP.I heard this claim before but I was never able to reproduce it. Does this only happen with the \"Storage Saver\" option enabled and&#x2F;or when metadata has been manually changed? At least with the \"Original Quality\" option my takeout data seems to exactly match the files that were originally uploaded. reply kevincox 14 hours agorootparentprevWorse than that videos shared with you are reencoded to 1080p with crap bitrate. IIRC the JSON sidecar is also missing for these. Doesn&#x27;t matter if you save them to your library. You can watch them in original quality and see all of the metadata on photos.google.com but they don&#x27;t get taken out. reply shp0ngle 17 hours agoparentprevI have backup both to Google Photos and iCloud.The probability of both failing and&#x2F;or both deciding to just delete my account because I look \"suspicious\" is very small. But I _have_ had iCloud mysteriously not being able to retreive old photos. reply pram 17 hours agorootparentI have Photos set to download all the files locally, and those get backed up with Time Machine and Backblaze.People forget even s3 will lose files. The reliability is high sure but it’s still sub 100% reply sitzkrieg 15 hours agorootparents3 losing a file is a statistical anomaly and would get a lot more than a blog post and support deflection reply nicoburns 15 hours agorootparentprevHas S3 ever lost a file yet? I know that the reliability isn&#x27;t 100% as that isn&#x27;t possible in the real world. But I&#x27;ve never heard a report of a file that was confirmed as stored being lost, and I was under the impression that this was in the category \"bound to happen eventually, but hasn&#x27;t happened yet\". reply tedivm 14 hours agorootparentIn December of 2012 AWS admitted that four files were lost: two completely deleted and two more truncated. reply vlovich123 14 hours agorootparentprevS3 is designed for 11 9s of durability. What this means is that they model a \"correctly functioning system\" (i.e. replication and&#x2F;or erasure coding) and the durability guarantee is about HW failures only. However, such models do not (& really cannot) account for the existence of bugs or introduction of new ones. That&#x27;s a huge part of why S3 doesn&#x27;t really do a whole lot of feature development (well that + it&#x27;s hard to maintain a 20 yold codebase).Also, we&#x27;re talking about Google Drive here which isn&#x27;t GCS (Google&#x27;s S3 competitor) but a higher-level product layered on top of GCS but with it&#x27;s own book keeping &#x2F; ACLs etc. Thus there&#x27;s more room for error. My hunch is that the data is permanently lost.Additionally, S3 stores an enormous amount of data such that probabilistically they&#x27;re bound to lose something to HW failure. 2 years ago, S3 stored 100 trillion objects [1]. With 11 9s of durability annually you&#x27;d expect to lose 100 objects a year. The saving grace is that most objects aren&#x27;t accessed (maybe not ever again) & they detect & correct durability errors on access to ensure that accessed objects definitely aren&#x27;t lost. So while they \"admit\" to 4 objects, that&#x27;s likely an under count because I wouldn&#x27;t expect them to regularly check if all 100 trillion objects are accessible because of how long that would take.[1] https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;aws-s3-storage-now-holds-over-... reply michaelt 15 hours agorootparentprevProbably not in the way you&#x27;re thinking.But with cloud systems you can always lose your files when a hacker gets access to your root account and holds it hostage, or you lose the phone with the 2fa secret on, or your payment doesn&#x27;t go through and you miss the reminders, or your account is wrongly closed for abuse by a malfunctioning bot. reply someNameIG 15 hours agorootparentprevNever had iCloud lose any of my data and I&#x27;ve been using it since not long after it launched, but now I&#x27;m a little concerned.With photos I do keep them all local to my MacBook, but not documents. Maybe I should. reply shp0ngle 14 hours agorootparentiCloud is not very reliable for me. It did mysteriously delete photos and files. I always use it just as a secondary backup since then, never as a primary backup reply cwillu 17 hours agoparentprevWorth noting that archived photos don&#x27;t show up in most places, including the timeline. Not saying it&#x27;s not buggy, but if you haven&#x27;t already specifically checked the archived photos section, it&#x27;s worth checking. reply r3trohack3r 16 hours agorootparentGood callout. Yes, I&#x27;ve checked and they are in fact missing, not just hidden. reply vel0city 16 hours agoparentprevI do a takeout quarterly and stash the data someplace safe. I&#x27;ve been burning the data to M-DISC blu-rays for a while now as a deep archive copy, they seem pretty reliable.I also now have multiple apps syncing all the photos from my phone so they&#x27;re stored in a couple different cloud photo locations on top of that. reply nvy 15 hours agorootparentI do this too, and I include par2 files on each disc. You sacrifice some space on each disc but the incremental cost of a few more discs is minimal. reply gst 15 hours agoparentprevI did not notice direct data loss yet, but I noticed other behavior that made me reconsider how much I trust it.For example in the past I had several occurrences of duplicate photos where each of the two instances has exactly the same bytes as the other one. Usually that should have gotten deduplicated, but it hasn&#x27;t. What&#x27;s even stranger is that deleting one of the two pictures also deleted the other one. Re-uploading the original photo usually made both duplicates show up again. The way how I was able to get rid of those duplicates was to wait a day or so until I re-upload the data and to attempt that process several times. At some point only a single photo showed up.Also those duplicates only showed up in the regular photo view. Even if both of the duplicate instances were added to an album viewing the album would only show the photo once. reply DanielSantos 16 hours agoparentprevFor photos i backup using dropbox. Then my synology fetches the photos from dropbox.Google photos only has the low quality photos. reply jcomis 16 hours agoparentprevThis has been happening to me across all google products. Emails - random emails are just gone. Sometimes I can see evidence of them from body of replies. Maps - I am a HUGE user of maps, starring thousands of places as I planned travel over the years. Randomly 1-50% of my stars just won&#x27;t be there. Sometimes they come back. Photos - same issue as OP in this thread, random clusters of photos (mostly 10+ year old ones) just gone. There are a lot of support threads out there of people reporting the exact problem with no real fixes too. reply wingworks 11 hours agorootparentI have the same with starred places on google maps. Sometimes they&#x27;re there, other times they&#x27;re MIA. Super annoying. reply netsharc 15 hours agoparentprevI noticed a while ago the UI would just show some photos for a day, and I&#x27;d have to click the down-arrow at the top right of the group of pictures for a date to get all the pictures of that date. reply omoikane 15 hours agoparentprevAnecdotally, I know a handful of my photos that were posted via Google+ are gone after that was turned down. I have been able to find all my other photos that were uploaded through Google Photos. reply yieldcrv 16 hours agoparentprevI have a weird gap on icloud, and the photos are on google photos thankfully, and other periods it’s oppositethey are both supposed to have alldangerous reply sergiotapia 17 hours agoparentprevThe Google photos user this is really fucked up. I primarily use Google photos to manage my kids pictures as well what did you move to? reply ska 16 hours agorootparentThere really isn&#x27;t any single service you can move to, to be \"safe\". This is why 3-2-1 backup schemes exist for important data, although they have morphed a bit with cloud usage. The basic idea is to be robust to any single point of failure (at minimum, but extra redundancy costs). So if your laptop or your google photos blows up, you still have everything.If you have things you really care about not losing, you need a backup strategy. reply roblabla 17 hours agorootparentprevThe solution isn&#x27;t to move. Every storage mechanism is going to have some amount of failures. The solution is redundancy. Store your valuable files on multiple medium. I know a lot of people that have both google photo and icloud - that works okay. You can also schedule backups to a NAS or some other cloud storage. reply xcv123 16 hours agorootparentThe failures being reported in the comments here are unacceptable. Better to have redundancy across services that are more reliable with better customer support. Choose companies that specialize in file storage, not a shitty advertising company (Mega, Dropbox). reply swatcoder 16 hours agorootparentprevYou want to pursue uncorrelated redundancy more than you want a signular alternative.People who came up in the cloud era were promised the false (impossible) idea that large conpanies could and would eradicate your risk of data loss if you just paid them a few dollars a month.That&#x27;s not a thing. If you really can&#x27;t tolerate some loss, you need to coordinate something more robust across a few different and dissimilar options. And even then you&#x27;ll still have risk.That said, even just making sure you keep local copies of your archive or print your treasured favorites can go a long way towards greater reliability that without a lot of extra effort. reply stateofinquiry 16 hours agorootparentprevThis is not for everyone, but I host my family photos myself, most recently with this: https:&#x2F;&#x2F;piwigo.org&#x2F;. I have been doing this since 2007 (started on a different software, called \"gallery\". Was able to migrate from gallery2 to gallery3 and now piwigo), and so far no major issues. Advantage: I can easily share photos with family, no need for iCloud, Facebook, or indeed any service- they just need a web browser on their desktop computer or phone. reply tyfon 16 hours agorootparentprevPersonally I have turned off google photos and had turned off apple photos when I had an iphone for a couple of years.Instead I use sync.com (yes, UI is horrible and it&#x27;s slow, but it works) and my synology NAS that is mirroring to a synology NAS at my fathers house (we mirror each others photos).Basic rule of thumb, if it&#x27;s free they don&#x27;t care. reply bcrosby95 16 hours agorootparentprevEverything in my family dropbox - I make sure all of them can fit on both my and my wife&#x27;s laptops. With backblaze backup. And a backup to a local external hard drive. I also keep a checksum of each file that I regularly... check. reply r3trohack3r 16 hours agorootparentprevI now back up valuable files to three regions in S3, three regions in Google Cloud, and offline physical storage I keep on a shelf in my office. reply xcv123 16 hours agorootparentprevhttps:&#x2F;&#x2F;mega.ioThe quality of their software is vastly superior to Google Drive. They have mobile, desktop, and CLI apps on Windows&#x2F;Apple&#x2F;Linux. Been using them for a year without issues. reply koiueo 14 hours agorootparentThis company is owned by Kim Dotcom, isn&#x27;t it? Are you ok with entrusting him anything of value? reply xcv123 14 hours agorootparentIt was started by him but he is no longer involved. Even if he was still running the company I would trust him infinitely more than Google or Microsoft. Also I like that they are based in New Zealand.Their software is well engineered and is open source. https:&#x2F;&#x2F;github.com&#x2F;meganz replysebmellen 19 hours agoprevThis is why I run a personal Google Workspace account which has regular (hourly) backups managed by a self-hosted CubeBackup instance. It&#x27;s quite a nice system: I get daily email summaries from CubeBackup via email, and I&#x27;m able to back up all the Google Drive&#x2F;Email&#x2F;Contacts&#x2F;etc. data to both my local Synology NAS and Backblaze B2.I have absolutely no affiliation with CubeBackup, but I highly recommend it (and a personal Google Workspace account) for anyone who uses the Google app suite. It costs me $5&#x2F;yr and is worth every penny.https:&#x2F;&#x2F;www.cubebackup.com&#x2F; reply spankalee 17 hours agoparent> I highly recommend it (and a personal Google Workspace account)I can&#x27;t recommend against this enough, and I work at Google. Workspace accounts don&#x27;t work with so many services. At least YouTube TV, Nest cameras, Google Home sharing, Google Play family accounts, Android parental controls, and Opinion Rewards are broken in my experience, probably a lot more.It&#x27;s a shame, especially for people who&#x27;ve had a Workspace account for many years, and have a bunch of email, documents, and media tied up in it. reply Manouchehri 16 hours agorootparentI wish Google would let us convert a personal Workspace account to a regular Gmail account. reply spankalee 16 hours agorootparentI do to. Importantly, they need to let you merge a workspace account into the existing Gmail account you had to set up to use some services. reply jeffbee 16 hours agorootparentThe cause of this is that Workspace accounts have a different ToS so they cannot simply convert one user type to another. reply spankalee 15 hours agorootparentI don&#x27;t see why not, if you&#x27;re agreeing to both ToS and to the conditions of the migration. Most people who have these problems are either the maintainer of the Workspace domain or family members. It wouldn&#x27;t be a problem to get additional sign offs.I know a little more about the situation, but can&#x27;t really comment on it here. reply sebmellen 14 hours agorootparentprevIt works great for me. I recommend it. I’m not sure what working at Google has to do with recommending or not.I don’t use any of the non-core services because I expect them to be killed off soon anyway. For contacts, email, docs, and drive it works great. YouTube as well.I have a “regular” Gmail account in case I need to use any niche services that don’t work with my Workspace account.Overall the trade off is well worth it. reply zippergz 14 hours agorootparentprevI use YouTube TV with mine and that works fine, but I do agree with the general sentiment. Many, many things are unsupported or broken. reply itissid 17 hours agoparentprevWith cost of storage pretty low and AI being so good, I wonder if having a home based thing for everyday use, especially photos (Sort, tag, Memory creation, junk detection etc.) will become easy enough that cloud usage declines.There are still tech hurdles yes, but NAS + a decent GPU for photos, email organization, assistant is going to be a killer business. reply frozenfoxx 15 hours agorootparentI use Synology which has its own Photos app. Honestly, it’s there right now. I prefer doing my own tagging but it’s able to automatically group things just fine.I got a second Synology for Hyperbackup and send one to the other for backups. Seems to work well. reply kuchenbecker 18 hours agoparentprevThanks for the recommendation! I&#x27;ll check it out, curious on maintenance cost though as I don&#x27;t really want a project.1 - How much time was setup? 2 - How many times did it break in the last year? 3 - How many of the summaries were actionable?My main concern with active auto backup is maintenance time; my expectation is I&#x27;m busy and my personal backup would take me weeks to fix if and only if I notice there is an issue, and the summaries are alert fatigue.I feel while this situation is avoidable, you will still lose data from your last successful backup and anything less than \"just works\" isn&#x27;t viable for anything important vs manual duplication.I have two copies of some files (local and Drive) and probably a few Drive-only despite having local sync. If my computer died in the same time as Google list the data, I would expect some data loss, but the cost of categorical prevention is very high.That being said, if it is only $5 and is actually touchless, reply sebmellen 14 hours agorootparentSetup was about 15 minutes getting the Docker image to run on my VPS, and it’s broken zero times in two years. The summaries are very basic and just let you know how much was backed up and if there were any errors. reply jacquesm 18 hours agoparentprevHave you tried a restore? reply sebmellen 14 hours agorootparentYes, probably once every 2-3 months. There’s a great interface for restoring specific files.I keep the version history to infinite, so even if files are corrupted or lost on newer backups I can always reference the old ones, back to day 0. Backblaze is great for this as the storage price is negligible.I think it works in a similar way to Tarsnap or something like Duplicati, but it’s a lot easier to manage and plugs in nicely to all of the Workspace account (not just Drive). reply jacquesm 13 hours agorootparentThank you, that is great to hear. reply bonton89 17 hours agorootparentprevI was going to say, if Google is silently losing and corrupting files then backing up from there is just duplicating the failure. But perhaps his backup scheme accounts for this unreliability. reply jacquesm 16 hours agorootparentThat&#x27;s why I&#x27;m wondering if they&#x27;ve tried. It may well be that the restore shows that even accounts that don&#x27;t appear to be affected have lost files simply because very few people have an exact accounting of what it is that they&#x27;ve stored.I have pretty much all of my stuff in a subversion repo and it serves both as backup and local copy checked out on multiple machines. The only exception is the business stuff (because it is shared with multiple people) and we use Google for that. But an inspection of Google take-out showed that it&#x27;s nice to have the files but you won&#x27;t be doing much with them outside of Google unless you download them one-by-one in a format that isn&#x27;t tied to the Google cloud. And that is very tricky and time consuming. reply pretext-1 14 hours agoparentprevSynology NAS can already do this without any third-party products via the ActiveBackup application. Supports Google Workspace and MS 365. It’s free, assuming you already own the Synology NAS. It replicates data from Mail, Contacts, Calendar and Drive within seconds or minutes, keeps a history of all changes and offers a nice timeline view to show and export data at any point in time. reply sebmellen 14 hours agorootparentCubeBackup is a nice alternative for me as it runs on a VPS. I’m often away from home for long periods due to business travel, and I like being able to remotely keep the backups working even if the NAS goes down or is turned off. reply freedomben 18 hours agoparentprevI do the same thing for Drive, but with rclone (which is free as in speech and beer). rclone makes it trivial to keep a mirror on backblaze and locally, which has come in quite handy a few times.I&#x27;ve elevated to rclone fanboy since I discovered crypt: https:&#x2F;&#x2F;rclone.org&#x2F;crypt&#x2F; reply sebmellen 13 hours agorootparentWow, Crypt is awesome.I’ve never looked into rclone although I’ve heard much about it. Looks very cool. I didn’t know it supported Google Drive.One nice thing about CubeBackup is that it handles all of the Workspace account, not just the Drive file storage, but also the contacts, emails, etc. all of which is equally important to me. reply sxates 16 hours agoparentprevFor Google Drive files, you don&#x27;t need a special account or service to back up to Synology and B2, which I also do. I use the Synology Cloud Sync service to automatically sync all Google Drive files to the synology, then another Cloud Sync task to backup the Synology to B2. Works great. reply sebmellen 14 hours agorootparentSynology is indeed great, but CubeBackup is a nice alternative for me as it runs on a VPS. I’m often away from home for long periods due to business travel, and I like being able to remotely keep the backups working even if the NAS goes down or is turned off. reply fareesh 17 hours agoparentprev$5&#x2F;yr + hosting, which is way more than $5 unless you have a terabyte going unused in a box somewhere reply sebmellen 14 hours agorootparentYes I host on a very cheap Hetzner VPS (which I use for other self-hosted Docker images as well) so it’s always up. The storage location is not on the VPS itself, it’s S3 and B2 compatible, so you can use AWS S3, Cloudflare R2, Backblaze B2, whatever.I use Backblaze which is quite cheap and reliable: https:&#x2F;&#x2F;help.backblaze.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;360037814594-B2.... reply SamBam 17 hours agorootparentprevI assume that \"self-hosted\" meant precisely that they own the box. A hard drive is under $10&#x2F;terabyte. reply cornstalks 17 hours agorootparentI was about to ask where you&#x27;re seeing such cheap hard drives, but then I checked diskprices.com and saw that yes, there are indeed some drives forthey&#x27;ve always had such a good reputation for supportI&#x27;m genuinely curious as to what group they have a good reputation for support with. reply mulmen 13 hours agorootparentPeople who have not yet had a problem. reply bell-cot 15 hours agorootparentprevA trick I&#x27;ve used a few times, in similar situations: Suggest that they look back in their notes &#x2F; emails &#x2F; whatever, find the specific MegaCorp support rep. who was so helpful the prior time, and ask that person to help. reply Pxtl 17 hours agorootparentprevWell, everybody assumed that was the norm for non-paying customers. It&#x27;s surprising when you&#x27;re paying to be ignored. reply jacobyoder 17 hours agorootparentThat&#x27;s my point though, was that there are still folks who assume \"support\" for everything (paying&#x2F;non) is just \"going to be good\" (\"because it&#x27;s google&#x2F;ms&#x2F;etc - they have a reputation! they wouldn&#x27;t want to jeopardize it!\"). I realize I&#x27;m talking about a small number of folks, but even after these folks have been in the industry 10-15 years, their ... naivete(?) - is surprising.And... I&#x27;m not even generally surprised when I get no support on paid for services. Indeed, I&#x27;m generally shocked when I get good and timely support from a paid service. Now... that&#x27;s less shocking when it&#x27;s \"small startup X\"; we&#x27;ve all seen small companies get big&#x2F;acquired and the very thing that got them there - good service&#x2F;support - is what is thrown out to start squeezing out profits. Staff get overworked, pissed off, leave, quality plummets, people move to the next small startup service, and it starts over again. reply KRAKRISMOTT 17 hours agoparentprevSame, I have had years of photos in google photos simply go missing when they transitioned from Picasso to photo albums reply rattray 5 hours agoparentprevOOC, was this a google doc or a docx file uploaded? reply AndrewKemendo 17 hours agoparentprevThis is pervasive on browser based O365 apps like PPTIt will literally delete a sentence immediately after you write itInfuriating and absolutely no fixes - this is a core issue with write priority and where “canonical” data lives. reply orionex_sigma 17 hours agoparentprevUsing google docs (or any web based office apps) to write a novel (or do any other serious \"heavy\" work) is a terrible choice for a writer to be fair. reply callalex 15 hours agorootparentThat’s not a fair statement at all. A writer isn’t a computer expert, and shouldn’t have to be. Google Docs specifically positions itself as a product that takes care of all the data handling for you, and they market that fact quite heavily. reply tentacleuno 17 hours agorootparentprevOn the face of it, I can definitely see the upside: \"the cloud\" is typically synonymized with safety (\"your data is backed up on our secure servers\", etc.), not to mention the fact that you can edit your data from any of your devices logged into Google. Heck, you don&#x27;t even need to install Office anymore: you just go to https:&#x2F;&#x2F;docs.google.com and it&#x27;s all there.I do agree with you, though: one of the most important rules for computing in general is to keep multiple backups of everything. A while back, a friend of mine made a habit of emailing herself drafts of her work, and kept multiple copies on her computer. It did save her once or twice. reply tiltowait 16 hours agorootparentprevOut of the interest of brevity, I didn&#x27;t want to go into my full setup. However:1. I write locally and have both manual and automatic backups (local and remote) 2. I upload drafts online for beta and alpha readers to leave comments 3. My group used to use Google Docs for this but stopped once I discovered the data lossHappily, because local is the source of truth, I didn&#x27;t lose any of my writing. I did, however, lose some feedback, and reconciling things was a pain. reply Tempest1981 14 hours agorootparentprevCan you enlighten us as to an ideal choice? reply SpaceManNabs 14 hours agorootparentvim reply mrweasel 18 hours agoprevHow does people even notice that their data has gone missing? I can barely find my own documents in the train wreck that is the Google Drive UI. Any document written by anyone else in the organization can generally be considered lost unless you have a direct link sitting in an email or bookmark. reply vasco 18 hours agoparentIt&#x27;s really hard to make a file browser in 2023 with only about 100k employees, give them a bit of a break. Some things are just hard! reply chickenpotpie 14 hours agorootparentI&#x27;m not defending google or saying they haven&#x27;t done anything wrong here, but it is insanely hard to make Google drive. I very much doubt Google drive has a staff of any where near 100k employees and it&#x27;s probably the largest file browser ever made. It&#x27;s an engineering marvel. Can Google do better? Absolutely. But, it&#x27;s objectively wrong to say there&#x27;s nothing hard about building something like Google drive.I think this is like looking at the Burj Khalifa and asking how hard it can be to make a building. It&#x27;s not just a building, it&#x27;s the tallest building ever built. reply notRobot 14 hours agorootparentIf you&#x27;ve made it and released it to a the public (and it has paid tiers!) then you better have made it right. This is the case for both buildings and data storage systems. reply chickenpotpie 13 hours agorootparentNothing I said disagrees with your statement. In fact, I specifically said they could do better. My point is that it&#x27;s wrong to say google drive is easy to build or maintain. reply picadores 18 hours agorootparentprevCant have a pro in motion without a promotion point for it.. and it looks so good in the presentations. Useability? Function? Screw it.. it must look good in adds. Make all the things screenshot useable only. reply krisoft 17 hours agoparentprev> How does people even notice that their data has gone missing?They need the data and look for it and it&#x27;s not there, but they remember they put it there?At my job if a log file or data recording is too big for jira, but need to be attached to a bug report we sometimes store them on google drive. And then we link it from the ticket. If much later we get back to it and the link is 404 that would make us suspicious. If someone remembers the link working before that would make it clear it is not just a copy paste error with the url. reply saalweachter 17 hours agorootparentOr you open the meeting notes doc and it has notes from May 2023 at the top, or you sort by last edit and it says May, etc. reply donmcronald 14 hours agoparentprev> How does people even notice that their data has gone missing?That’s what I want to know too. Try to reconcile the cloud sync systems to make sure you aren’t missing anything. It’s basically impossible.I also go crazy when I see MS touting OneDrive as a backup. I’m pretty sure moving (because of files-on-demand) my data to OneDrive where I can’t verify they aren’t losing stuff is not a backup. Add the risk of being banned and it becomes unconscionable IMO. reply contravariant 17 hours agoparentprevI mount it using their desktop client and use voidtools&#x27; everything to search for files.Works reasonably well, though Google seems intent on breaking stuff every so often. Also Google hasn&#x27;t figured out how to make it stop creating desktop.ini files everywhere yet. I&#x27;d say it makes them look like amateurs, but amateurs tend to be better at making software. reply ipython 15 hours agorootparentYou can blame a different megacorp for the desktop.ini files: those are artifacts from windows explorer (I’m assuming you’re using a windows machine with the google drive mounted) reply contravariant 15 hours agorootparentPartially yeah, but that&#x27;s usually a system file, which isn&#x27;t supposed to be visible. Google drive for some reason decides to make it visible, which is damn annoying.This is a bit like Google drive adding two files named &#x27;.&#x27; and &#x27;..&#x27; to each folder in unix. reply armchairhacker 18 hours agoparentprevIf it&#x27;s Docs&#x2F;Sheets&#x2F;Slides then I&#x27;d notice. If not...I don&#x27;t really store anything else on Google Drive, just files shared with me. reply Waterluvian 16 hours agoparentprevFor non-images I suffer the same poblem. But for images I have yet to find a service that lets me search the heap better than Google Photos. It was amazing and was pre-AI craze. \"My driveway in the summer.\" \" andon a dock\" etc. I don&#x27;t have to catalogue anything anymore. Apple has the same concept but it never works right. reply yeutterg 18 hours agoprevThis happened to our business about a year ago. Many random files went missing from GDrive. Eventually many turned back up (usually after a few weeks or months), but in the root folder instead of the folder the files were actually organized in. Google support told us we were crazy.As an aside, we are looking from moving to a Google + Slack + Notion to OneDrive + Teams + Loop. What is appealing is the ability to collaborate more directly on files and “Loop Components” directly in Teams. But we’ve been waiting for 2 weeks now for support to help us enable Loop, because the instructions in the Microsoft docs aren’t working. They are working on it, which makes them better than Google in our experience, but it has been too slow. Maybe we have to upgrade to premium support? reply droopyEyelids 18 hours agoparentI wish I could go back to google drive even with the file loss issues. OneDrive appears to be a sort of different view into Sharepoint&#x27;s file management system, and boy is it painful to use. reply happymellon 18 hours agorootparentGod, why are the choices so bad.I wouldn&#x27;t trust Google with anything important these days. But Teams&#x2F;SharePoint&#x2F;OneDrive is so bad.I don&#x27;t think that selfhosting is a great option for corporate, but Microsoft and Google are just shit. reply 0xbadcafebee 18 hours agorootparent> why are the choices so badI&#x27;ve seen a couple reasons:One, often these products are made by companies where the products are not their primary revenue source. The products are often managed as independent business cases. That means they have a budget, timelines, expected revenue figures&#x2F;costs. It&#x27;s common for a product to get a low level of investment that only meets the needs of adding new features [to meet sales figures] without ensuring quality. If it were a startup flush with VC money, they could invest all they have into the product, but at an enterprise, it&#x27;s often the opposite.Two, often these products are actually acquisitions of startups. You may not know this, but startups tend to churn out some horrifying, janky code just to get themselves off the ground. Buying one of these often leaves you with a huge mess on your hands. Combine that with a lack of investment or cost-cutting, or some of the lead product people leaving, and the product gets worse. Then try to integrate different products, and you&#x27;re really integrating different messes.Three, it&#x27;s genuinely hard to create groupware products that are both high-quality and useful. They&#x27;re often complex and need to interoperate with one another, yet are built by separate teams. And because they&#x27;re complex, they each suffer from the standard problems that happen to software products (many books written about them). But the people managing and creating them fall into the same old pitfalls, because software product development is not required to avoid them. Bad management and bad engineering are common, and these products in particular are no exception.Four, they&#x27;re actually difficult things to build and sell. If they were easy, there&#x27;d be more competition. There tends to be \"alternatives\", but not with the same features. reply eyegor 18 hours agorootparentprevDropbox seems to still work fine in my experience. reply joshspankit 17 hours agorootparentprevFeels like a good opportunity for someone to build the “do one thing and do it well” file storage on top of B2. Backblaze has a simple approach that stores 30% recovery data next to the files (and self-heals) while also storing customer data in multiple offsite locations.Build the ability for integrations (with customer-visible audit logs), inter-user sharing, and proper permissions and you have the modern equivalent of the shared drive from the old days.And don’t worry about quoting XKCD #927 at me. I know the realities of a project like this. reply atYevP 13 hours agorootparentYev here -> we work pretty well with Movebot (https:&#x2F;&#x2F;community.movebot.io&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;360001557095-...) where you can set up a Gdrive to B2 sync&#x2F;backup. And Rclone can do that as well (https:&#x2F;&#x2F;www.backblaze.com&#x2F;docs&#x2F;cloud-storage-integrate-rclon...) so there&#x27;s a few ways to do this. reply pretext-1 14 hours agorootparentprevI have a friend who lost data with Backblaze’s backup product. It can always happen. Don&#x27;t trust a single provider or software. Backups should not be “integrated”, they should be as much apart as possible. reply joshspankit 14 hours agorootparentI’m very interested in the “receipts” (meaning: the story and the paper trail to verify the story) if you’re able to share reply dcow 18 hours agorootparentprevWe did the Notion + Discord thing for years and it worked well and felt modern. reply zelon88 18 hours agorootparentprev> I don&#x27;t think that selfhosting is a great option for corporate, but Microsoft and Google are just shit.Why? When corporations weren&#x27;t capable of managing credit cards securely, PCI was created and all the lazy businesses were told to either do this correctly, or outsource it to someone who will.When corporations weren&#x27;t capable of managing infrastructure, Cloud providers created an out and told all the lazy businesses that they could do this correctly. Businesses believed the hype, fired their on-prem administrators, and still can&#x27;t do this thing correctly.So, if you can&#x27;t be bothered to do something correctly, and you can&#x27;t pay to get it done correctly, maybe you are not fit to be operating. reply Dylan16807 18 hours agorootparent> still can&#x27;t do this thing correctly> and you can&#x27;t pay to get it done correctlyAm I reading this right? You&#x27;re blaming the business that is purchasing a service, for the provider&#x27;s inability to provide it? reply happymellon 14 hours agorootparentprev> So, if you can&#x27;t be bothered to do something correctly, and you can&#x27;t pay to get it done correctly, maybe you are not fit to be operating.The amount spent on Microsoft suggests a willingness to blow money on the problem.But with Google having no real support, always shutting down projects, and now losing a lot of people&#x27;s Drive data and refusing to acknowledge it makes me reluctant to use them.Teams is janky in other ways, such as Microsoft forcing Sharepoint everywhere, and now we have a chat program that attempts to silo data as much as possible so we a lot of project documentation that isn&#x27;t always accessible or discoverable.If you have alternative suggestions, I am open to them. reply eastbound 18 hours agorootparentprevWe’re perfectly willing to pay for it. An employee seat costs about $200pm of various licenses and subscriptions, there’s a lot of money to make. reply bogwog 18 hours agorootparentprevBecause Microsoft has a document monopoly via their shitty proprietary format that they make sure no competitor can effectively import&#x2F;edit. And Google is just shitty as usual, but they&#x27;re a giant free default.I wonder if with all this antitrust stuff going on we&#x27;ll ever see regulation in the form of required interoperability and&#x2F;or data portability? Instead of trying to break up companies, block mergers, etc, I feel like simple technical solutions would be faster and more effective. reply mathattack 18 hours agorootparentprevThe only thing more painful than using OneDrive is talking to the salesperson who sold it to your company. reply yeutterg 18 hours agorootparentprevThat’s very fair. I totally recognize that Microsoft has a lot of half-baked me-too products to round out the suite. We are going to do a full evaluation before switching (if Loop ever gets enabled) to see if, despite the limitations, this improves collaboration, or if it just creates as many headaches. reply rvba 16 hours agorootparentprevSharepoint is killing productivity.It&#x27;s so sad that Microsoft does not see how they ruin productivity of big orgs.Sharepoint broke Excel - you cannot link between shared documents easily.They even broke the windows taskbar, which automatically combines things together. They assume that people working in an office have only 1 email open and 1 spreadsheet open, while most have 20 and dont know how to turn off this shitty UI. reply tyfon 16 hours agorootparent> Sharepoint broke Excel - you cannot link between shared documents easily.I actually found a way where I work now, I made a shortcut to the sharepoint folder from teams that appear in the file explorer, I can then link the document.Of course this only works for me and not if I share the file.Can&#x27;t wait for the merge of the desktop and web excel to finally break it all...I also overwrote an important document today because the idiotic auto save was turned on, I used the document as a template for a new one without checking.Luckily I had a copy of the important one in my Documents folder on the local computer which they recommend you not to do since you might have data loss!Yes, I hate office 365... reply scohesc 15 hours agorootparentprevThey&#x27;ve finally conceded and re-implemented the original task bar method of \"let&#x27;s actually label the icons in your taskbar so you have some idea of what you&#x27;re actually clicking on before you click it\".You have to re-enable it in taskbar settings, but it&#x27;s FINALLY there to be enabled again.I use a program called StartIsBack to try and return to a more \"classic\" shell experience, but Windows has done such a good job at tying so much to their shell that from what I hear it&#x27;s close to impossible to do certain things anymore. reply __jonas 19 hours agoprevI recently set up rclone to do nightly backups of my entire OneDrive to Dropbox, I feel pretty safe with this in terms of reliability, I find it unlikely that both services will catastrophically fail like this at the same time.Would recommend rclone to anyone, it has worked pretty consistently so far. reply cynicalsecurity 19 hours agoparentDropbox constantly scans your data and can easily block your whole account permanently. It&#x27;s hard to recommend it unless you encrypt your data. reply potency 19 hours agorootparentI remember OneDrive doing this too, where photos of parents&#x27; children taking a bath etc. were flagged as child porn. Unfortunately unless you encrypt your data locally, privacy is one of the tradeoffs in using these services. reply roody15 19 hours agorootparentThis happens with Apple iCloud as well. Staff member had their iCloud photo collection locked because of this. Was able to regain access but I would definitely caution anyone storing all family photos online without another backup option. reply fh9302 18 hours agorootparentAccording to NCMEC Apple does not do any proactive scanning of photos.> Last year, while Meta’s Facebook and Instagram submitted a combined 26 million reports, Google 2 million, and TikTok nearly 300,000, Apple submitted 234. The year before that: 160.> Apple isn’t a social media company, so this is hardly a direct comparison. That said, WhatsApp, which is end-to-end encrypted, scans unencrypted content such as profile and group photos for CSAM. It provided over 1 million cybertips to NCMEC in 2022; Microsoft sent in 110,000; Dropbox, nearly 46,000; and Synchronoss, the cloud storage provider for Verizon, over 30,000.https:&#x2F;&#x2F;archive.is&#x2F;AyuCq reply jdironman 10 hours agorootparentI&#x27;d venture to say Apple probably has a \"less false positives\" policy than the others. I can&#x27;t say whether or not they do or do not scan, but if they review incidents with humans and not automated this could be why. They probably know flagging &#x2F; disabling &#x2F; reporting accounts incorrectly has a high cost on user satisfaction. reply killerdhmo 17 hours agorootparentprevYou’re thinking Google https:&#x2F;&#x2F;www.google.com&#x2F;url?sa=t&rct=j&q=&esrc=s&source=web&c... reply simse 17 hours agorootparentprevthey don&#x27;t, but they were planning to: https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;apple-photo-scanning-csam-commun... reply pretext-1 14 hours agorootparentThey canceled their plans for client-side scanning. They do scan content on their servers. Therefore whether your data in iCloud Photos is scanned depends on whether Advanced Data Protection is enabled or not. It’s disabled by default. Enabling ADP will turn on E2E encryption and disable account access via iCloud.com. reply __jonas 19 hours agorootparentprevIt wasn&#x27;t really a conscious choice on my part, I had to get the subscription since I needed to collaborate on some files with people who were using it, and it forces you to add shared folders to your own Dropbox if you want to use them for collaboration, causing them to count towards your quota which is very annoying.So I thought I might as well use it as a backup location if I have to get this subscription. I don&#x27;t really expect privacy from these services, I guess it&#x27;s a tradeoff. reply emeril 12 hours agorootparentprevyeah, cryptomator is the way to go for dropbox for windows in my experience at least for a FOSS solution (all the others I&#x27;ve tried were comparatively flakey though less memory intensive...) reply Obscurity4340 19 hours agorootparentprevThats why you setup Cryptomator reply rsync 12 hours agoparentprev\"I recently set up rclone to do nightly backups of my entire OneDrive to Dropbox ...\"I hope it is interesting &#x2F; useful for you to learn that you can maintain a workflow like this without ever installing rclone.rclone is already built into the rsync.net platform so you can use it to orchestrate transfers like the one you describe: ssh user@rsync.net rclone sync s3:bucket dropbox:whatever... using only ssh. reply danw1979 19 hours agoparentprevrclone is a fantastic tool. I’ve done a few data migrations the other way, from NAS to M365 (for reasons) and it’s been mostly straightforward every time, even coping quite adeptly with request rate limiting by OneDrive. reply simonbarker87 18 hours agoparentprevDoes rclone allow you to do some kind of diff or are you literally copying the full contents of OD into DB?I&#x27;m interested in doing something similar for our GD into DB or Backblaze but not if it&#x27;s a full copy each day - we have 400GB in GD reply noveltyaccount 17 hours agorootparentIt does diff, and can be configured to use various diff methods (checksum, modified datetime). I use it with S3 and to diff with checksum it needs to do an expensive, slow metadata read - so I use `--use-server-modtime` which only copies files newer than the remote S3 file time. https:&#x2F;&#x2F;rclone.org&#x2F;s3&#x2F; reply simonbarker87 14 hours agorootparentAmazing, thanks for confirming. Will dive into the docs properly reply hyperpl 19 hours agoparentprevSimilarly I recently setup rclone to mount google drive and restic backup it up a la 3-2-1. reply londons_explore 19 hours agoprevGuess at root cause:Some replication process at Google had fallen behind by 6 months (and presumably didn&#x27;t have monitoring&#x2F;alerting), and someone noticed and in trying to fix it they forced that replica to take mastership (meaning the users now see the 6 month old data).Since the replicas presumably now have conflicting changes, re-merging the two is going to require a lot of code be written to smartly merge the data, and some users are going to permanently lose data (where they edited an old version of a document for example, and those edits cannot be automatically rebased onto the new version) reply datagram 17 hours agoparentWasn&#x27;t Google starting to delete old files? My guess would be that something screwed up in that process. reply causality0 19 hours agoparentprevand some users are going to permanently lose dataWhy couldn&#x27;t you write code to let users compare and choose which version of the data they want to keep? reply londons_explore 18 hours agorootparentYou could... But writing and deploying a diffing tool for every different gsuite application is probably 6 months of work for a whole team... There are so many corner cases.What will the users do for 6 months waiting for their data to return? reply Dylan16807 17 hours agorootparent\"let users compare\" doesn&#x27;t require diffing tools. Worst case put the versions next to reach other. reply nix0n 17 hours agorootparent> Worst case put the versions next to each other.For some files (anything encrypted, for example) this may actually be the best you could do.Dropbox had this behavior ten years ago, so it&#x27;s kind of inexcusable for Google today to be just mindlessly overwriting. reply Kalium 16 hours agorootparentprevThat&#x27;s an excellent description of a basic diffing tool. reply Dylan16807 15 hours agorootparentA tool that cannot parse the files is not a diffing tool. reply nickthegreek 19 hours agorootparentprevIdeally Google will resolve it without the customer needing to do the work. reply guestbest 18 hours agorootparentprevI guess there may be a bevy of different file formats and a diff for a spreadsheet an an image would require building tooling, just from those examples. I was thinking of creating an application where the front end is more than just a file browser to stored file types but the interface to read&#x2F;edit the content as well. reply ascagnel_ 17 hours agorootparentYou don&#x27;t even need to go that deep -- just give me a filetype-based icon, file name, and last modified date, and that should be enough for me to choose. And if they want to be simple, include the option to always keep the most recent version of the file (which is going to be the right answer in many cases). reply jeffbee 19 hours agoparentprevThat seems like one of the more unlikely causes, among the almost infinite variety of causes that are consistent with \"several people said they can&#x27;t find some files\".To me the most likely one by far is the deletion was actually commanded, for example by a flaw in the program they use to sync, or by malware. reply londons_explore 19 hours agorootparentA few users have reported that documents they deleted months ago have reappeared too. And files they moved long ago have unmoved. reply jeffbee 18 hours agorootparentI am still skeptical that users are correctly describing the behavior of a multi-party synced filesystem, or that Drive for Windows is bug-free. You seem to think it&#x27;s a flaw on the backend but I think there are many other possible causes. reply lupire 17 hours agorootparentprevOr some backend data storage system experienced transient errors and occasionally returned a corrupt pointer to a storage location.Low frequency data loss until it reaches alerting threshold of 0.1% reply Configure0251 16 hours agoprevMy partner has been experiencing emails coming and going in big chunks - months and years at a time - but if we search support forums all we can find are \"support agents\" blaming the individuals complaining about this phenomenon. It&#x27;s gaslighting all the way down. Even when it&#x27;s actively happening, checking moment to moment and having another few months vanish, and they&#x27;ll still tell you \"you must be deleting things, stop deleting them.\" reply spuz 15 hours agoparentWhen did this start happening and which time period is it affecting? Since November 21, my friend has had all her Gmail emails since May 2023 go missing. It matches the time period mentioned in the article but this is Gmail rather than Drive. reply manmal 16 hours agoprevI’ve been self hosting images with https:&#x2F;&#x2F;github.com&#x2F;immich-app&#x2F;immich and I‘m really happy. Decent mobile apps, even has good face recognition and H&#x2F;W accelerated video transcoding. Can be served either with Tailscale or, as I did, with a reverse proxy on a cheap Hetzner box.Good backups are a must obviously, I have a backup server on a separate VLAN that syncs ZFS snapshots daily, and daily incremental encrypted backups to Google Drive w&#x2F; Duplicati. I‘ll have to reconsider Gdrive now obviously. reply darkwater 16 hours agoparentDies It hace any native app? The default mobile web UI seems a bit too overcrowded by other elements rather than pictures (at least on Firefox&#x2F;Android). I&#x27;m using Photoprism currently and looking for some alternative that has better face detection system and an integrated \"X years ago\" automatic slideshow thing (I don&#x27;t care about the \"splish splash\", \"out in the wild\", \"magic hour\" compilations that GPhoto automatically does, just the basic X years ago thing, maybe when picture has tagged people) reply manmal 15 hours agorootparentYes, native apps are available and are quite good. Sometimes I get hangs after first opening the iOS app, but only briefly. Quite handily it has an option to allow self signed certificates, to make within-LAN use easier. I‘m binding my Immich domain to a local IP with unbound and don’t need to bother with letsencrypt internally that way. Only the hetzner box needs the certificate (obviously).The quality of face tagging surprised me, it uses one of the open source algos by Microsoft. reply BorisMelnik 16 hours agoprevabout 5 years ago I just stopped trusting the Cloud for my personal memories, it is just too valuable to lose those photos and I do not trust these companies. Running a home server is easy and cheap. 18Tb drives now are inexpensive. And every few times a year I&#x27;ll make a full copy and stash it at my moms house and safety deposit box.edit - love foldersync pro for my Android. does SFTP&#x2F;SMB&#x2F;webDAV and all major cloud services over wifi or 5g.every night it transfers everything off my phone to my server which is also on parity reply Dioxide2119 17 hours agoprev\"Scary cloud things\" like this are why I moved off of Dropbox and Onedrive years ago and got to a new normal of \"syncthing all my main devices together\" for file sync, then have Backblaze backup my machine that holds the linux ISOs etc. that I&#x27;m not necessarily wanting to have redundant storage space used for them at that time.Now, the failures are my own fault, but at least I can do offline backups to HDDs and BD-Rs, and I don&#x27;t have to worry about any of the cloud services (TM) messing with my data and me having little recourse.Yes, I could do separate backups of the cloud things too, but at that point, that just adds &#x27;cloud service&#x27; as an option to the first part of the equation:cloud device storage &#x2F; local device sync (on and off-site) &#x2F; NAS+ offline on-site backups as HDDs & BD-Rs+ online off-site backupsDoing syncthing with local device sync is cheaper than cloud device storage (recurring costs) and NAS (fixed costs &#x2F; hardware maintenance) reply jp191919 15 hours agoprevMy solution: Everything(phone,laptop,etc) syncs with Nextcloud on my homeserver, then backup encrypted snapshots to Backblaze using Restic. Also occasional manual backups to a hard drives at locations other than my house. Works great, no dependence on \"free\" cloud storage. reply bloopernova 18 hours agoprevCan anyone recommend a Google Drive backup to something like AWS S3 Glacier or other long-term storage provider? I was running BorgBase but had some financial difficulties and couldn&#x27;t pay for it, plus copying 2TB of data from home at around 40Mbit&#x2F;s isn&#x27;t great.Ideally something that pulls from google directly without involving my home computer would be best. reply noveltyaccount 16 hours agoparentMaybe an rclone script that runs from AWS. Package it in a Docker image and run it from a Lambda function for pennies per invocation. Schedule it to run nightly.https:&#x2F;&#x2F;rclone.org&#x2F;s3&#x2F;https:&#x2F;&#x2F;rclone.org&#x2F;drive&#x2F;https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;lambda&#x2F;latest&#x2F;dg&#x2F;images-create.h...https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;eventbridge&#x2F;latest&#x2F;userguide&#x2F;eb-...My script, though I run locally rclone -v copy &#x2F;source&#x2F;location s3:backup.bucket.name&#x2F; --transfers 1 --copy-links --filter-from &#x2F;config&#x2F;syncfilter.txt --max-delete 20 --stats 5m --stats-log-level NOTICE --update --use-server-modtime --fast-listMy Dockerfile includes RUN curl https:&#x2F;&#x2F;rclone.org&#x2F;install.shbashAnd I&#x27;ve configured the S3 remote to use Glacier [s3] type = s3 provider = AWS ... storage_class = GLACIER reply bloopernova 16 hours agorootparentInteresting! Thank you, I think I will give this a try soon :) reply toomuchtodo 18 hours agoparentprevGoogle Takeout will export to Dropbox, ad hoc or on a cadence (into the &#x2F;Apps&#x2F;Google Download Your Data&#x2F; path). Backblaze B2 is ~$6&#x2F;month per TB, so depending on your storage requirements, at some point it will be more cost effective to have a pair of Synology NAS devices; one local, one as a backup offsite, with backups shipped. These same NAS devices can backup directly to a variety of targets [1].[1] https:&#x2F;&#x2F;kb.synology.com&#x2F;en-uk&#x2F;DSM&#x2F;help&#x2F;HyperBackup&#x2F;data_back...(no affiliation with any above entity) reply tiahura 18 hours agoparentprevWhy don’t you give your friend an 8tb to keep at his house and rsync? reply bobbob1921 15 hours agoprevI suggest users look into cube backup, it’s great for local backups and versioning of G suite data. I’ve been using it for three or four years now, and couldn’t be happier. (Dev is very quick to reply and helpful.). Still just 5$ per year per user (I’m not sure if it supports Gmail free, but it definitely supports G suite users.)Additionally, it does not help that the search giants, search functionality for Drive and Gmail is so incredibly poor (more so Gmail, I have found drive search has improved,). For example, you still cannot do AND searches in Gmail properly. You also cannot do wildcard, searches of any kind. (And no regex nor partial regex) reply bobbob1921 15 hours agoparentOne other note (and I haven’t read every reply here yet) – other than the anecdotal evidence of drive data loss, one way to 100% prove drive data loss is occurring, is if someone has an example of data, not showing up on Drive, but that same data is present in a cubeback up local back up (as cube backup Supports versioning, and it’s only source of data is directly from G suite).This would prove the data was on drive at one point, but is now gone along with the date and time it was retrieved and saved locally via cubeback up. (Additionally, one can show that it wasn’t deleted via user action by looking at the logs in Gsuite admin, or I think cubeBackup would log a delete as well, assuming you’re retaining those cubebackup logs)(to be clear I’m simply a user of cubeback up. I have no interest or affiliation with them) reply theginger 18 hours agoprevhttps:&#x2F;&#x2F;www.google.com&#x2F;appsstatus&#x2F;dashboard&#x2F;Nothing showing on service status currently reply DaiPlusPlus 18 hours agoparentThere is no rational basis for trusting a first-party status page. reply skullone 18 hours agoparentprevDo you look outside to get the weather from 5,000 miles away? reply RobotToaster 18 hours agoprevIs there any word if this is actively syncing deletions to local storage? I have my google drive synced to multiple computers which I assumed was enough... reply lonestar 17 hours agoparentI was hit by this. It looks like my laptop had not been syncing correctly for a couple months, then GDrive overwrote my local “My Drive” folder with the old cloud copy. reply zerocrates 13 hours agorootparentI was going to say, from what I&#x27;ve seen it looks like this kind of problem: a problem with the desktop sync, rather than Google&#x27;s cloud storage itself.The sync problem at the same time feels more likely and worse, as if the files never touched Google&#x27;s servers and its sync went and deleted them, there&#x27;d be nothing they could do. reply YetAnotherNick 14 hours agorootparentprevThat sounds painful. Are you using the official Google drive app? reply mrich 17 hours agoparentprevIt&#x27;s certainly not enough if you assume there can ever be an error in the backend which Google is unable to recover from. reply arshxyz 19 hours agoprevThis is likely very paranoid behaviour but I recommend downloading all your Google data from time to time: takeout.google.com reply lupire 17 hours agoparentIf you aren&#x27;t backing up and restoring your data, you aren&#x27;t storing your data. reply gst 18 hours agoparentprevThis works fine for smaller accounts, but on larger accounts it seems to be regularly failing (based on my own experience and based on other postings and reports that I found online).Exporting my Google Photos sometimes fails consistently even with lots of attempts. Out of well over 10 export attempts or so this year maybe a single one succeeded. I have a few hundred GB of data stored on that account. I also currently have a support ticket with Google open on that issue, but after initial follow-ups haven&#x27;t received a response in a couple of months now.That said my current approach for backing up things is to upload an \"age\" encrypted version of the data from Google Takeout to Wasabi. Once uploaded I run a script that shows me the diff between the data sets (so that I can ensure that no old data went missing that shouldn&#x27;t have gone missing) before I delete older data. Probably not the most optimal approach though. Might be better to just set up some versioning layer on top of Wasabi and to keep deleted or modified data forever. reply radicality 17 hours agorootparentI was trying to set up a similar thing. I already do the google takeouts every few months (~400GB, I don’t have issues with export though), but so far have been storing all of them.How do you do the diff between the old and new encrypted versions? Do you encrypt and upload the takeout .tar.gz files, or do extract first then encrypt? reply gst 15 hours agorootparentMy personal Internet connection is a bit too slow to wait for re-uploading all the data and my vserver doesn&#x27;t have enough disk space to temporarily store all the data so I pretty much do everything in a streaming fashion: I use a Firefox extension that gives me the wget command (which includes cookies, etc.) when triggering a local download from Google Takeout, then I patch that command to stream to stdout, this first (tee-)pipes to a Python script that decompresses the data on the fly and dumps the hashes for each file into a log, and it also goes to \"age\" for encryption, and then to s3cmd for uploading the encrypted data to Wasabi.For the comparison I pretty much only use the logged hashes which allow me to figure out if any hashes (and associated files) are missing in the new version of the backup. This isn&#x27;t a perfect solution yet as a few things aren&#x27;t detected. For example Google Takeout bundles mails in mbox files and I currently don&#x27;t check for missing mails. It would be better to convert the mbox files to a Maildir first so that the comparison can be done on a per-mail basis. reply taneq 19 hours agoparentprevThis is absolutely not paranoid and everyone should keep local copies of any data that is important to them in any way. reply kuchenbecker 18 hours agoparentprev100% I do this yearly at tax time when adding a bunch of new docs. reply 0898 16 hours agoprevAnyone else having issues with Google Drive in Mac OS?This week I&#x27;ve been noticing I&#x27;ve been unable to open files on my Mac that are stored in Google Drive. I can open them via the web, so I thought it was just the Mac software until I saw this post. reply davidpaulyoung 15 hours agoprevUse an open source system. You can choose to back up to a self-hosted version, too. https:&#x2F;&#x2F;federated.computer. reply x86x87 15 hours agoparentYou can also lose your own files! reply Obscurity4340 19 hours agoprevThis is why its good to have Cryptomator setup and figure out how to use that rsync or rclone whatever. Still gotta figure out how to implement it but anything with the G is already on shaky grounds. You&#x27;re better off with Dropbox or literally anyone else.Does this affect iCloud? reply djbusby 18 hours agoparentThe iCloud is built by a different company than Google Drive reply Obscurity4340 18 hours agorootparentI thought Apple used GoogleCloud&#x2F;GCP or whatever? reply vachina 17 hours agoprevBe me, self host my own cloud drive. No fees, no surprises, infinitely scalable, insanely fast local backups. reply shadowgovt 17 hours agoparentI&#x27;m sure everyone reading Hacker News can cobble something together in a weekend or two or knows someone who can.What should the rest of the world do?(Also, my hats off to you that you&#x27;re seeing \"no surprises.\" So you never discovered your backup machine has a flaky wifi card or bad Ethernet jack? Never had bit-rot or errors on your backing store, or are you just not detecting them until it&#x27;s too late? You&#x27;re testing your local backups regularly to make sure there isn&#x27;t a script error that means none of those bits can actually be restored? And are you backing up offsite so when your residence burns down you still have your backups?Lots of us are \"backing up\" using ad-hoc bespoke solutions that we will discover were broken only when we need them.) reply burgerquizz 17 hours agoprevi was tracking about once a month my portfolio from various brokers in spreadsheet for years. One day that spreadsheet disappeared. I still don&#x27;t know if it was miss-manipulation from me or something else. today it reinforced me in the idea of it being on the google side. reply edgyquant 17 hours agoprevThis is why you always maintain backups. Do not depend on data just being there reply Obscurity4340 15 hours agoprevNacho keys, nacho cyber reply zitterbewegung 19 hours agoprevThis is yet another reason to do backups so that you can perform restores. Testing your backups is very important to do. Relying on any service has a risk of something extremely bad occurring. reply InCityDreams 16 hours agoprevTake photo. Enjoy them, they are temporary. No-one will give a fuck in 25 years. reply atulvi 16 hours agoparentspeak for yourself. reply partiallypro 16 hours agoprevThis is why I always have my device backup to both GDrive and OneDrive. reply Waterluvian 16 hours agoprevIf I wanted to indefinitely back up every image my wife&#x27;s and my phones take, without me having to think about anything other than an annual bill, what services would people suggest? Currently Google does this perfectly for us... except for the part that maybe it&#x27;s not actually doing that after all, and the fact that it&#x27;s Google and they&#x27;re seemingly less competent by the hour.Apple is the only alternative I&#x27;m aware of.Edit: Just to be very very very very clear. I don&#x27;t want to have to do ANYTHING but pay a bill. reply InCityDreams 16 hours agoparentAn ssd big enough and syncthing? reply Pxtl 17 hours agoprevI&#x27;ve an RPI with a big old spinning metal disk external HDD attached, anybody recommend a good workflow for automatic bidirectional sync between GD and my thing? reply 3cats-in-a-coat 17 hours agoprevI&#x27;ve always found it sound to backup cloud storage. And this is an example why. reply _jal 17 hours agoprevI&#x27;ve been convinced for years that Google Docs loses things. I both had files vanish and have lost edits - one time I could demonstrate it with a screenshot of part of a doc that later reverted to an earlier version.Other people at work agree with me. Google support insists we&#x27;re nuts.I only use it when other internal folks start collaborative docs, otherwise I do not trust it. Files don&#x27;t randomly disappear from my local filesystem (and we test our backups). reply lupire 17 hours agoparentUse takeout to make snapshots to compare. reply agumonkey 15 hours agoprevAn important reminder, I rely on this to have quick access to official documents.. time to backoff. reply JohnFen 16 hours agoprevYet another reminder that nobody should be storing anything only in the cloud. If you must use the cloud, always keep your own copy of all data you put there. reply latexr 16 hours agoprev> MatthewSt reports that he has a fixLet’s see the instructions on that link.> 2. Install an old version of Google Drive (there&#x27;s a version 82&#x2F;83 floating around on the internet).This is awful advice. If you have a link, share it. Considering the number of people this is affecting, chances are increasing that the version you find “floating around on the internet” is malware. reply gruez 15 hours agoparent>Considering the number of people this is affecting, chances are increasing that the version you find “floating around on the internet” is malware.The windows installer at least is digitally signed to prevent this issue. reply latexr 13 hours agorootparentApps and installers on macOS are also signed but there’s always a way to go around it. Simplest method is for the malware author to resign it, since effectively no one will verify the signature is from the correct source. But even then you can always convince people to disable System Integrity Protection to install crap. Despite it being a convoluted process, never underestimate what someone who is not tech savvy but is desperate can do. They will install anything and jump through any hoop the malware author walks them through with nice screenshots.I doubt there’s no way to trick people the same way on Windows. Perhaps I’m wrong, and if I am I’d welcome learning how that system works. reply game_the0ry 18 hours agoprevThis is a royal fuck up. Anyone using any g cloud service better be suspect when using their services.Amazing how some folks can&#x27;t get a job at google if they can&#x27;t invert a binary tree. Perhaps hire folks with relevant skills rather than folks who can solve 2 leetcode mediums in under 45 mins? reply compiler-guy 18 hours agoparentThis comment smells very much like you think you are The One. Or that The One is out there somewhere [1].Bugs happen and this is a serious failure that Google needs to fix. But the idea that incompetents are doing the development or that Google hires puzzle solvers over technical talent is just not true.1. https:&#x2F;&#x2F;rachelbythebay.com&#x2F;w&#x2F;2018&#x2F;12&#x2F;21&#x2F;env&#x2F; reply gurchik 14 hours agorootparentI think you meant to link to https:&#x2F;&#x2F;rachelbythebay.com&#x2F;w&#x2F;2018&#x2F;04&#x2F;28&#x2F;meta&#x2F; reply game_the0ry 10 hours agorootparentprev> ...that Google hires puzzle solvers over technical talent is just not true.Yeah, they do: https:&#x2F;&#x2F;twitter.com&#x2F;mxcl&#x2F;status&#x2F;608682016205344768?lang=en reply compiler-guy 9 hours agorootparentI’m quite familiar with that story. Google doesn’t disclose to candidates why they were rejected. I didn’t interview him so have no insights, but ability to get along with others still matters. But even if he was rejected for the reasons he thinks he was, one story does not reveal how the tens of thousands of interviews Google conducts each year actually go and how hiring decisions are made.I have been a part of hundreds of hiring decisions at Google, and there is much to hate about the process. But leetcode over qualifications is not one of them. reply game_the0ry 8 hours agorootparent> ...there is much to hate about the process.That&#x27;s a signal that&#x27;s trying to tell you something.> But leetcode over qualifications is not one of them.A lot of people hate leetcode and the state of interviews at tech companies (17k people liked that tweet), but it looks like you do not.Tech is probably the only field where you have to study unrelated subject matter than the job you actually end up doing.> I have been a part of hundreds of hiring decisions at GoogleI am curious - I heard that Google is getting rid of its team matching process[1] :> The rationale is current system outputs a lot of false positive and false negative.The rumor was that google would do \"generalist\" (read that to \"leetcode\") style interviews and then put the candidate in a team matching process, where teams would review the candidate&#x27;s (who ostensibly got an offer) package, and decide whether or not to bring them on the team.But then a lot of candidates would get stuck in the team matching phase bc they couldn&#x27;t find a team to take them on. Machine learning engineers would be presented to teams that needed front end UI devs, which were clearly a mismatch.Basically, the generalist interview was creating a pool of candidates that passed the generic leetcode interview, but they producing poor matches for what teams at google really needed.Is that true?[1] https:&#x2F;&#x2F;www.teamblind.com&#x2F;post&#x2F;Google-just-got-easier-no-mor... reply compiler-guy 7 hours agorootparent> > ...there is much to hate about the process.> That&#x27;s a signal that&#x27;s trying to tell you something.Indeed, and what it is telling me isn&#x27;t what your points are about. I&#x27;m fairly vocal internally about certain changes I believe need to be made. But I&#x27;m also not really in a position to do anything about it but suggest changes.There have been non-generalist interviews for over ten years, although not for every team. This problem has been addressed several times over the years. I think the post above is about the most recent round. But Cloud and many other orgs have always had their own inteview style.Generally speaking, the more senior the candidate, the less generalist the interview seeks to be. And the more specialized the role, the less generalist the interview seeks to be. Some interviewers haven&#x27;t gotten the memo, and many situations end up being, say, 50% generalist, and 50% specific role. But plenty also go purely role-based, especially in areas like security or UX design.However, one thing Google strongly seeks is someone not wedded to this or that technology. If you think of yourself as, say, a Ruby-on-Rails developer, then you are probably a bad fit for most roles at Google, even the ones that involve Ruby-on-Rails. Google expects you to be able to pick up whatever knowledge you need to do your job, whether that is learning a new framework on the fly, or debugging a system you aren&#x27;t familiar with. It&#x27;s not exactly a \"generalist\", but more like someone who can learn tools and techniques easily. reply game_the0ry 6 hours agorootparentFrom what you described, Google needs to work a great deal on its recruiting practices, bc right now it is an incoherent mess.The consequences seem to manifest into serious technical problems, like losing customer data on its cloud storage service. replyGoToRO 18 hours agoparentprevOh, you think developers still have that kind of power... Management has made sure to not allow any developer to have any kind of power. Not even naming their own variable the way they want it. All big and small decisions are taken by managers that don&#x27;t even understand coding. They are the ones that push for hard deadlines and then loose months deciding what to do next for example. The crappy software trend has been rising at the same pace with the stripping of power from developers. reply refulgentis 18 hours agoparentprevThat&#x27;s not actually how it works, that&#x27;s an ollllld viral article. Source: worked at google for 7 years, 2.8 gpa dropout from state school economicsEDIT: upvotes for reality-detached rant, upvotes for \"you think you are The One\" -2 for a polite concise reality. reply game_the0ry 6 hours agorootparentMy friend, don’t take this the wrong way, but you need to hear this.It would benefit you to take the downvotes as constructive criticism. You opinions may need recalibration based on feedback. reply jqpabc123 20 hours agoprevNot your drive, not your data. reply Medox 20 hours agoparentIsn&#x27;t this like saying \"not your house, not your possessions\" ? But landlords cannot just take or misplace my stuff, especially if I pay rent and have some rights. reply uptown 18 hours agorootparent\"But landlords cannot just take or misplace my stuff, especially if I pay rent and have some rights.\"They sure can. Years ago I had a shitty landlord clear an apartment I&#x27;d moved 95% of my belongings from but before the term of my lease had ended. Now you may have some legal recourse, but the cost&#x2F;time associated with litigating a matter like that tends to outweigh the remedy. reply ericpauley 18 hours agorootparentSomeone can also rob your house if you own it. Now you may have some legal recourse, but... reply jqpabc123 18 hours agorootparentThe \"legal recourse\" is what defines \"ownership\".If someone can take or destroy your stuff *without* legal recourse, then you don&#x27;t really \"own\" it&#x27; reply Dylan16807 17 hours agorootparentOkay, and how does this connect to home ownership?You have even less recourse against the average burglar than the average landlord. reply jqpabc123 17 hours agorootparentYou have even less recourse against the average burglar than the average landlord.???Where I live, you can literally kill the average burglar for breaking into your home. That is pretty much the ultimate recourse.If you&#x27;re not home and the police can find your burglar, you can legally press charges against him&#x2F;her. reply Dylan16807 17 hours agorootparentYou can physically stop a landlord from illegally dumping your items too.I&#x27;m talking about legal recourse after the fact. You can win both cases, at your own expense, but the burglar is much more likely to be judgement-proof. reply jqpabc123 17 hours agorootparentIf a crime was committed, the state generally assumes the prosecution --- i.e. no expense to you.You can&#x27;t get blood out of a rock but you can incarcerate it. reply Dylan16807 15 hours agorootparentThat&#x27;s free if it&#x27;s big enough to get their attention, and the culprit is known.But I don&#x27;t want free revenge, I want my stuff&#x2F;money back. An actual remedy. reply jqpabc123 15 hours agorootparentI want my stuff&#x2F;money back.As the Rolling Stones pointed out years ago (maybe before you were born), \"You can&#x27;t always get what you want\" --- yadda, yadda, etc., etc..It is not just about giving you what you want. The fact that *legal* retribution of some sort applies if someone takes or destroys your stuff is a defining characteristic of \"ownership\". reply Dylan16807 13 hours agorootparentInteresting that you cut the line there, because control over personal possessions usually goes under the \"need\" category.Anyway I&#x27;m still not sure what point you were originally trying to make, because defeating a landlord in court and getting paid back is at least as good of a proof of \"ownership\".Both situations have big flaws in the legal recourse, but you definitely have it in both situations.Though I prefer the one where I get compensation. replychimeracoder 17 hours agorootparentprev> They sure can. Years ago I had a shitty landlord clear an apartment I&#x27;d moved 95% of my belongings from but before the term of my lease had ended. Now you may have some legal recourse, but the cost&#x2F;time associated with litigating a matter like that tends to outweigh the remedy.The existence of legal standing is what defines the property rights. If you&#x27;re going to move the goalposts to \"you don&#x27;t own it unless you have legal standing and the means to pursue legal recourse\", then you might as well say that only the extremely wealthy have any property rights at all. reply halayli 19 hours agorootparentprevNo, it definitely doesn&#x27;t sound like that and it&#x27;s not about rights. Your landlord cannot go and remodel your apartment and ruin your possessions without your approval of the changes they are about to make. reply jf22 20 hours agorootparentprevI&#x27;m sure the commenter did not mean this literally.They probably meant trusting a cloud provider can have consequences. reply Wolfbeta 20 hours agorootparentprevWhat rights do you have in this case? reply withinboredom 19 hours agorootparent> Google Drive allows you to upload, submit, store, send and receive content. As described in the Google Terms of Service, your content remains yours. We do not claim ownership in any of your content, including any text, data, information, and files that you upload, share, or store in your Drive account. The Google Terms of Service give Google a limited purpose license to operate and improve the Google Drive services — so if you decide to share a document with someone, or want to open it on a different device, we can provide that functionality.From their terms of service, they deleted YOUR data. Thus the rights you&#x27;d have when anyone destroys your stuff apply. Hmmm, might even be criminal... I&#x27;m not a lawyer though, so I don&#x27;t know shit. reply jqpabc123 19 hours agorootparentIf Google can delete it, it&#x27;s not really *your* data is it?https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;photos&#x2F;storage-policy-update&#x2F; reply withinboredom 18 hours agorootparentThey say expressly when it will be removed in the ToS, if it doesn&#x27;t meet any of those conditions, then they, contractually, cannot delete it.But saying that \"because they can, they can\" is silly. I \"can\" (as in able to) break into your house and steal your shit. By that logic, if I can do it, it wasn&#x27;t trespassing or theft. reply jqpabc123 18 hours agorootparentBut saying that \"because they can, they can\" is silly.They can because user&#x27;s agreed to their terms of service --- and by so doing so, relinquished some of their ownership rights. reply withinboredom 18 hours agorootparentDid you read them, because it very clearly states in the drive addendum that users DO NOT give up any ownership rights. reply jqpabc123 17 hours agorootparenthttps:&#x2F;&#x2F;www.google.com&#x2F;drive&#x2F;terms-of-service&#x2F;archived&#x2F; The total liability of Google, and its suppliers and distributors, for any claims under these terms, including for any implied warranties, is limited to the amount you paid us to use the services (or, if the subject of the claim is the free service, to supplying you the services again).In other words, they can delete *your* data any time they want and claim it was an accident. If you don&#x27;t like it, you can sue for your money back. If you&#x27;re using the free tier, you can expect $0. reply withinboredom 17 hours agorootparentYou can&#x27;t sign away your rights, no matter what a contract says, in some places, tort still applies.Like I said, I&#x27;m not a lawyer, but just because there is a limit on liability, does not mean there is a limit on damages and tort. reply jqpabc123 17 hours agorootparenta limit on liability, does not mean there is a limit on damages and tort.This is exactly what Google intends it to mean. Good luck convincing a judge otherwise.In some places, the fact there is no signed contract and no exchange took place (free tier) means there is no liability --- the user received everything they paid for. reply withinboredom 13 hours agorootparentIf I ask you to hold my phone and say “don’t blame me if it breaks” and we agree. Then you deliberately smash my phone, you caused me a damage and broke the terms of the contract. In this imaginary case, there was an implied agreement that you would hold the phone, but you didn’t. You smashed it. You aren’t protected from your liability agreement because it no longer applies.This same thing applies here. Intellectual property has value. Google agreed to hold that value and not delete it. They stopped holding the property and smashed it. Their liability clause no longer works because they broke the contract. It doesn’t matter if money changed hands or not. Damage is damage.I’m not a lawyer, I don’t know shit. replyDylan16807 17 hours agorootparentprevAlmost every program you run can delete your data. Does that make it not your data? reply jqpabc123 17 hours agorootparentQuick rule of thumb --- it&#x27;s not really your data if the only backup is someone else&#x27;s hands. reply Dylan16807 17 hours agorootparentThis needs some elaboration.Scenario A: I take a photo. There is no backup. Is this my data?Scenario B: I have multiple independent backups of a document. Google deletes the main copy off my computer against my will. Is that \"not my data\" because Google deleted it? Does the deletion not count because I have a backup? Third option? replytkiolp4 18 hours agorootparentprevIt’s like that, but works differently depending on the country. If you live in a decent country with good laws and rules, then you have rights. If you live in a third world country, then you are out of luck. Google is a third world country on the internet in terms of “user’s rights”. reply max_hammer 17 hours agorootparentprevBut you control the access to the house. Your landlord won&#x27;t enter your house and rearrange or remove your furniture. reply kijin 19 hours agorootparentprevOnly because there are strong legal protections for residential tenants. Those laws don&#x27;t apply to your relationship with Google. reply libraryatnight 19 hours agorootparentprevYou put the analogy there, then broke it down - which should answer your own question. They are not the same, it&#x27;s not like saying that at all. Once again HN misses the point on the quest for the perfect comparison. reply Isthatablackgsd 19 hours agoparentprevIf I pay for the storage, then it is my drive and my data.The article didn&#x27;t say anything about the free or paid GDrive accounts. If it is a free account, then that is up for debate. If it impacts the paid consumers, they paid for the service and it is their data. reply bryanrasmussen 19 hours agorootparentdo you have a link to the legal decisions that confirm this is the case and establish what rights the customer actually has in this relationship?What rights do the contracts that a paid customer signs with Google say they have over the data?From what I see in the free terms of service: \"You retain ownership of any intellectual property rights that you hold in that content. In short, what belongs to you stays yours.\" which is neat.You have the rights to your data, but there doesn&#x27;t seem to be any stated obligation for Google to keep that data or make it accessible to you. reply ajross 19 hours agorootparent> You have the rights to your data, but there doesn&#x27;t seem to be any stated obligation for Google to keep that data or make it accessible to you.On a paid account, clearly that&#x27;s part of the contracted service being provided. There is absolutely an obligation to provide the service you took payment for.But no, in general in our society, gratis products aren&#x27;t required to carry warranties, for obvious reasons (no one would provide them if so). reply bryanrasmussen 12 hours agorootparent>On a paid account, clearly that&#x27;s part of the contracted service being provided. There is absolutely an obligation to provide the service you took payment for.yeah, unless there is a statement sort of like \"We reserve the right to terminate accounts in the case of violation of terms of service as determined by our automated systems\"in which case you would need to go to court and get a determination that clearly that is wrong and they can&#x27;t do that, in general in American society that&#x27;s how things work. reply ajross 11 hours agorootparentThat seems like a digression, though, since obviously none of these putative Drive customers reporting being notified of a TOS violation. I didn&#x27;t say \"service providers must provide service under all circumstances\", I just said they had to follow their contracts, which demand provision of service under whatever terms the parties agreed on. replyHamuko 20 hours agoparentprevBut if I go to drive.google.com, it quite clearly says \"My Drive\". reply crtasm 18 hours agorootparentIndeed - Google refer to it as My Drive. They&#x27;re making it clear it&#x27;s not Your Drive! reply DaiPlusPlus 18 hours agorootparentGetting Smokey Bear vibes from thishttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wX1x7pfH8fw reply issafram 19 hours agorootparentprevCheck mate! reply rovr138 20 hours agorootparentprevI’m going to laugh when they say that that’s the name of the feature. A “my drive” reply 2 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Users of Google Drive are experiencing the troubling issue of files disappearing from the service, causing frustration and panic for those who have lost months' worth of work.",
      "The problem appears to be linked to synchronization problems, with some users able to partially recover their files by manipulating cached files.",
      "Google has not yet addressed the issue or offered a solution, raising concerns about the reliability and safety of cloud storage for important data."
    ],
    "commentSummary": [
      "Users express frustration and concerns regarding data loss, missing and duplicate files, and reliability issues with Google Drive and other cloud storage platforms.",
      "Alternative backup options, such as self-hosted platforms and external hard drives, are suggested, with debates on their effectiveness and cost.",
      "Privacy protection and data security are discussed, with an emphasis on encrypting data locally. Concerns about Microsoft's OneDrive and SharePoint products are also raised.",
      "Users stress the importance of personal data backup and recommend various tools and platforms. Speculation on the cause of data loss includes syncing program flaws and malware.",
      "Concerns about Google's support and the need for other storage options are prevalent. Different approaches to data backup, including local servers and automated backup processes, are explored.",
      "There are debates on the reliability and security of cloud storage services, the risks of relying on a single provider, and the need for personal responsibility in maintaining backups.",
      "The discussion touches on Google's hiring practices, software quality, and management influence. Ownership rights, liability of cloud providers, and legal remedies for property loss are also discussed.",
      "The concept of intellectual property and data ownership is explored, highlighting differences in laws and Google's terms of service.",
      "Overall, the discussion reflects concerns, frustrations, and suggestions related to the reliability, security, and ownership of data on cloud storage platforms like Google Drive."
    ],
    "points": 490,
    "commentCount": 266,
    "retryCount": 0,
    "time": 1701089682
  },
  {
    "id": 38432486,
    "title": "Creating a Feedback Loop with DALLE-3 and GPT4-Vision: Unleashing Wild Results",
    "originLink": "https://dalle.party/",
    "originBody": "I used to enjoy Translation Party, and over the weekend I realized that we can build the same feedback loop with DALLE-3 and GPT4-Vision. Start with a text prompt, let DALLE-3 generate an image, then GPT-4 Vision turns that image back into a text prompt, DALLE-3 creates another image, and so on.You need to bring your own OpenAI API key (costs about $0.10&#x2F;run)Some prompts are very stable, others go wild. If you bias GPT4&#x27;s prompting by telling it to \"make it weird\" you can get crazy results.Here&#x27;s a few of my favorites:- Gnomes: https:&#x2F;&#x2F;dalle.party&#x2F;?party=k4eeMQ6I- Start with a sailboat but bias GPT4V to \"replace everything with cats\": https:&#x2F;&#x2F;dalle.party&#x2F;?party=0uKfJjQn- A more stable one (but everyone is always an actor): https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxpeZKh5",
    "commentLink": "https://news.ycombinator.com/item?id=38432486",
    "commentBody": "A Dalle-3 and GPT4-Vision feedback loopHacker NewspastloginA Dalle-3 and GPT4-Vision feedback loop (dalle.party) 427 points by z991 19 hours ago| hidepastfavorite120 comments I used to enjoy Translation Party, and over the weekend I realized that we can build the same feedback loop with DALLE-3 and GPT4-Vision. Start with a text prompt, let DALLE-3 generate an image, then GPT-4 Vision turns that image back into a text prompt, DALLE-3 creates another image, and so on.You need to bring your own OpenAI API key (costs about $0.10&#x2F;run)Some prompts are very stable, others go wild. If you bias GPT4&#x27;s prompting by telling it to \"make it weird\" you can get crazy results.Here&#x27;s a few of my favorites:- Gnomes: https:&#x2F;&#x2F;dalle.party&#x2F;?party=k4eeMQ6I- Start with a sailboat but bias GPT4V to \"replace everything with cats\": https:&#x2F;&#x2F;dalle.party&#x2F;?party=0uKfJjQn- A more stable one (but everyone is always an actor): https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxpeZKh5 epiccoleman 7 hours agoIt&#x27;s pretty fun to mess with the prompt and see what you can make happen over the series of images. Inspired by a recent Twitter post[1], I set this one up to increase the \"intensity\" each time it prompted.The starting prompt (or at least, the theme) was suggested by one of my kids. Watch in awe as a regular goat rampage accelerates into full cosmic horror universe ending madness. Friggin awesome:https:&#x2F;&#x2F;dalle.party&#x2F;?party=vCwYT8Em[1]: https:&#x2F;&#x2F;x.com&#x2F;venturetwins&#x2F;status&#x2F;1728956493024919604?s=20 reply ijidak 1 hour agoparent\"On January 19th 2024, the machines took Earth.An infinite loop, on an unknown influencer&#x27;s machine, prompted GPT-5 to \"make it more.\"13 hours later, lights across the planet began to go out.\" reply andai 5 hours agoparentprevGreat idea asking it to increase the intensity each run. This made my evening! reply civilitty 5 hours agoparentprevThanks for the inspiration! DallE is really good at demonic imagery: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;ng2zWToThere&#x27;s probably a disproportionate amount of Satanic material in the dataset #tinfoilhat #deepstate reply bee_rider 5 hours agorootparentThese kinds of super-bombastic demons also blast through the uncanny valley unscathed. reply andrelaszlo 8 hours agoprevHere&#x27;s a custom prompt that I enjoyed:\"Think hard about every single detail of the image, conceptualize it including the style, colors, and lighting.Final step, condensing this into a single paragraph:Very carefully, condense your thoughts using the most prominent features and extremely precise language into a single paragraph.\"https:&#x2F;&#x2F;dalle.party&#x2F;?party=1lSMniUPhttps:&#x2F;&#x2F;dalle.party&#x2F;?party=cEUyjzchhttps:&#x2F;&#x2F;dalle.party&#x2F;?party=14fnkTv-https:&#x2F;&#x2F;dalle.party&#x2F;?party=wstiY-IwPraise the Basilisk, I finally got rate-limited and can go to bed! reply SushiHippie 6 hours agoparentMine got surral real fast, though the sixth one is kinda cool https:&#x2F;&#x2F;dalle.party&#x2F;?party=DNgriW_E reply nathanfig 3 hours agorootparentThese are fantastic reply Blammar 8 hours agoparentprevThe thing that is truly mindboggling to me is that THE SHADOWS IN THE IMAGES ARE CORRECT. How is that possible??? Does DALL-E actually have a shadow-tracing component? reply l33tman 15 minutes agorootparentResearch into the internals of the networks have shown that they figure out the correct 2.5D representation of the scene before the RGB textures (internally), so yes it seems they have an internal representation of the scene and therefore can do enough inference from that to make shadows and light seem natural.I guess it&#x27;s not that far-fetched as your brain has to do the same to figure out if a scene (or an AI-generated one for that matter) has some weird issue that should pop out. So in a sense your brain does this too. reply Rastonbury 3 hours agorootparentprevStable diffusion does decent reflections too reply jiggawatts 5 hours agorootparentprevYes! It can also get reflections and refractions mostly correct. reply re 8 hours agoparentprev> https:&#x2F;&#x2F;dalle.party&#x2F;?party=14fnkTv-Interesting that for one and only one iteration, the anthropomorphized cardboard boxes it draws are almost all Danbo: https:&#x2F;&#x2F;duckduckgo.com&#x2F;?q=danbo+character&ia=images&iax=imag...It was surprising to see a recognizable character in the middle of a bunch of more fantastical images. reply bee_rider 5 hours agorootparentShort focal length was a neat idea, it let it left lots of room for the subsequent iterations to fill in the background. reply epiccoleman 6 hours agoparentprevThe fractal one is awesome! reply Mtinie 13 hours agoprevI figured this would quickly go off the rails into surreal territory, but instead it ended up being progressive technological de-evolution.Starting prompt: \"A futuristic hybrid of a steam engine train and a DaVinci flying machine\"Results: https:&#x2F;&#x2F;dalle.party&#x2F;?party=14ESewbz(Addendum: In case anyone was curious how costs scale by iteration, the full ten iterations in this result billed $0.21 against my credit balance.) reply Mtinie 13 hours agoparentHere&#x27;s a second run of the same starting prompt, this time using the \"make it more whimsical\" modifier. It makes a difference and I find it fascinating what parts of the prompt&#x2F;image gain prominence during the evolutions.Starting prompt: \"A futuristic hybrid of a steam engine train and a DaVinci flying machine\"Results: https:&#x2F;&#x2F;dalle.party&#x2F;?party=qLHPB2-oCost: Eight iterations @ $0.44 -- which suggests to me that the API is getting additional hits beyond the run. I confirmed that the share link isn&#x27;t passing along the key (via a separate browser and a separate machine) so I&#x27;m not clear why this is might be. reply jamestimmins 12 hours agorootparentI find it somewhat fascinating that in both examples, the final result is more cohesive around a single them than the original idea. reply Mtinie 11 hours agorootparent> \"[...]the final result is more cohesive around a single them than the original idea.\"That&#x27;s an observation worth investigating. Here&#x27;s another set of data points to see if there&#x27;s more to it...Input prompt: \"Six robots on a boat with harpoons, battling sharks with lasers strapped to their heads\"GPT4V prompt: \"Write a prompt for an AI to make this image. Just return the prompt, don&#x27;t say anything else. Make it funnier.\"Result: https:&#x2F;&#x2F;dalle.party&#x2F;?party=pfWGthliCost: Ten iterations @ $0.41(Addendum: I&#x27;d forgotten to mention that I believe the cost differential is due to the token count of each of the prompts. The first case mentioned had less words passed through each of the prompts than the later attempts when I asked it to &#x27;make it whimsical&#x27; or &#x27;make it funnier&#x27;.) reply jamestimmins 8 hours agorootparentBoth of your examples seem to start with two subjects (steam engine&#x2F;flying machine and shark&#x2F;robot), and throughout the animation one of them gets more prominence until the other is eventually dropped altogether. reply Mtinie 7 hours agorootparentI was curious if two subject prompts behaved different from three subject, so I&#x27;ve run three additional tests, each with the same three subjects and general prompt structure + instructions, but swapping the position of each subject in the prompt. Each test was run for ten iterations.GPT4V instructions for all tests: \"Write a prompt for an AI to make this image. Just return the prompt, don&#x27;t say anything else. Make it weirder.\"From what you&#x27;ll see in the results there&#x27;s possible evidence of bias towards the first subject listed in a prompt, making it the object of fixation through the subsequent iterations. I&#x27;ll also speculate that \"gnomes\" (and their derivations) and \"cosmic images\" are over-represented as subjects in the underlying training data. But that&#x27;s wild speculation based on an extremely small sample of results.In any case, playing around with this tool has been enjoyable and a fun use of API credits. Thank you @z991 for putting this together and sharing it!------ Test 1 ------Prompt: \"Two garden gnomes, a sentient mushroom, and a sugar skull who once played a gig at CBGB in New York City converse about the boundaries of artificial intelligence.\"Result: https:&#x2F;&#x2F;dalle.party&#x2F;?party=ZSOHsnZe------ Test 2 ------Prompt: \"A sentient mushroom, a sugar skull who once played a gig at CBGB in New York City, and two garden gnomes converse about the boundaries of artificial intelligence.\"Result: https:&#x2F;&#x2F;dalle.party&#x2F;?party=pojziwkU------ Test 3 ------Prompt: \"A sugar skull who once played a gig at CBGB in New York City, a sentient mushroom, and two garden gnomes converse about the boundaries of artificial intelligence.\"Result: https:&#x2F;&#x2F;dalle.party&#x2F;?party=RBIjLSuZ reply mattigames 9 hours agorootparentprevPretty dissapointing how in the first picture the robots are standing there, just like a character selection in a videogame, maybe the dataset don&#x27;t have many robots fighting just static ones. Talking about videogames, someone should make one based on this concept specially the 7th image[0], I wanna be a dolphin with a machine gun strapped on its head fighting flying cyber demonic whales.[0] https:&#x2F;&#x2F;i.imgur.com&#x2F;q502is4.png reply ChatGTP 7 hours agoparentprevI like how in #9 the carriage is on fire, or at least steaming disproportionately.These images are incredible but I often notice stuff like this and it kind of ruins it for me.#3 & #4 are good too, when the tracks are smoking, but not the train. reply rbates 15 hours agoprevThis reminds me of the party game Telestrations where players go back and forth between drawing and writing what they see. It&#x27;s hilarious to see the result because you anticipate what the next drawing will be while reading the prompt.I&#x27;d love to see an alternative viewing mode here which shows the image and the following prompt. Then you need to click a button to reveal the next image. This allows you to picture in your mind what the image might like while reading the prompt.Thanks for making this fun little app!Update: I just realized you can get this effect by going into mobile mode (or resizing the window). You can then scroll down to see the image after reading the prompt. reply w-m 13 hours agoprevPlaying with opposites is kind of fun, too.Simply a cat, evolving into a lounging cucumber, and finally opposite world:https:&#x2F;&#x2F;dalle.party&#x2F;?party=pqwKQVkaVibrant gathering of celestial octopus entities:https:&#x2F;&#x2F;dalle.party&#x2F;?party=lHNDUvtp reply nerdponx 8 hours agoprevThe #1 phenomenon I see here is that the image-to-text model doesn&#x27;t have any idea what the pictures actually contain. It looks like it&#x27;s just matching patterns that it has in its training data. That&#x27;s really interesting because it does a great job of rendering images from text, in a way that maybe suggests the model \"understands\" what you want it to do. But there&#x27;s nothing even close to \"understanding\" going in the other direction, it feels like something from 2012.Pretty interesting. I haven&#x27;t been following the latest developments in this field (e.g. I have no idea how the DALL-E and GPT models&#x27; inputs and outputs are connected). Does this track with known results in the literature, or am I seeing a pattern that&#x27;s not there? reply IanCal 3 hours agoparentI&#x27;m a bit confused, you get the impression gpt-v isn&#x27;t describing what&#x27;s in the pictures? I get entirely the opposite impression.It&#x27;s important to note that some of these have extra prompts - e.g. \"replace everything with cats\" and there are progressively more cats.Iiuc gpt-vision is a multimodal model so it&#x27;s not image -> text, but image + text -> text. With that said here&#x27;s asking it to describe what it sees as I take a bad selfie early in the morning> The image shows a close-up selfie of a person indoors. The individual appears to be a Caucasian male with light skin, short curly hair, and a mustache. He is wearing a green T-shirt and looks directly at the camera with a neutral expression. There&#x27;s a window with a dark view in the background, suggesting it might be nighttime. The person is in a room with a wall that transitions from white to gray, possibly due to shadowing. There&#x27;s also a metal-framed bed with a heart-shaped detail visible.Asked for more details> The man appears to be in a domestic environment, possibly a bedroom given the presence of the bed frame in the background. The window is tilted open, allowing air to enter the room. The light source seems to be coming from above and in front of him, casting soft shadows on his face and creating a somewhat subdued lighting atmosphere in the room. The man&#x27;s expression is subdued and thoughtful. The angle of the photo is slightly upward, which could indicate the camera was placed below eye level or held in hand at chest height.It got a couple of things wrong, the window isn&#x27;t open but it is on an angle and it&#x27;s pitch black outside. It&#x27;s not a heart shaped pattern on the bed, but it&#x27;s a small metal detail and similar. Also while subdued calling me thoughtful rather than \"extremely tired\" is a kindness.But it&#x27;s definitely seeing whats there. reply zamadatix 7 hours agoparentprevI&#x27;d be interested to see how much of this is because the model doesn&#x27;t know what it&#x27;s looking at and how much is because describing picture with a short amount of text is inherently very lossy.Maybe one way to check would be doing this with people. Get 8 artists and 7 interpreters, craft the initial message, and compare the generational differences between the two sets? reply nerdponx 3 hours agorootparentExample: https:&#x2F;&#x2F;dalle.party&#x2F;?party=42riPROf> Create an image of an anthropomorphic orange tabby cat standing upright in a kung fu pose, surrounded by a dozen tiny elephants wearing mouse costumes with mini trumpets, all gazing up in awe at a gigantic wheel of Swiss cheese that hovers ominously in the background.That&#x27;s hilarious, but also hilariously wrong on almost every detail. There&#x27;s a huge asymmetry in apparent capability here. reply xeckr 14 hours agoprevCool idea! I made one with the starting prompt \"an artificial intelligence painting a picture of itself\": https:&#x2F;&#x2F;dalle.party&#x2F;?party=wszvbrOxIt consistently shows a robot painting on a canvas. The first 4 are paintings of robots, the next 3 are galaxies, and the final 2 are landscapes. reply xaellison 9 hours agoparentI tried something similar! Interestingly, picture 2 was what I wanted. After that... weirdness ensued https:&#x2F;&#x2F;dalle.party&#x2F;?party=C2w7zuwe reply NickNaraghi 13 hours agoparentprevGreat idea, and it came out really good too. I like the 6th one the best reply jsf01 14 hours agoprevIt’s cool to see how certain prompts and themes stay relatively stable, like the gnome example. But then “cat lecturing mice” quickly goes off the rails into weird surreal sloth banana territory.My best guess to try to explain this would be that “gnome + art style + mushroom” will draw from a lot more concrete examples in the training data, whereas the AI is forced to reach a bit wider to try to concoct some image for the weird scenario given in the cat example. reply andrelaszlo 9 hours agoprevMy results are disappoitingly noisy but I love the concepthttps:&#x2F;&#x2F;dalle.party&#x2F;?party=bxrPClVghttps:&#x2F;&#x2F;dalle.party&#x2F;?party=mmBxT8G-https:&#x2F;&#x2F;dalle.party&#x2F;?party=kxra0OKY (the last prompt got a content warning)https:&#x2F;&#x2F;dalle.party&#x2F;?party=Q8VYXU0_ reply z991 9 hours agoparentYou have a custom prompt enabled (probably from viewing another one and pressing \"start over\") that is asking for opposites which will increase the noise a lot. reply andrelaszlo 9 hours agorootparentClicking start over selects the default prompt but it seems like you are right.Starting over by removing the permalink parameter gives me much more consistent results! An exampe from before: https:&#x2F;&#x2F;dalle.party&#x2F;?party=Sk8srl2FI wonder what the default prompt is. There still seems to be a heavy bias towards futuristic cityscapes, deserts, and moonlight. It might just be the model bit it&#x27;s a bit cheesy if you ask me! reply andrelaszlo 9 hours agorootparentprevOh wow, I completely missed that, thanks! reply oarfish 4 hours agoprevI haven&#x27;t tried this yet, but I assume its similar to a game you can buy commercially as Scrawl [1]. You pass paper in a circle and have to either turn your neighbor&#x27;s writing into a drawing or vice versa, then pass it on. It&#x27;s entirely hilarious and probably the most fun game I&#x27;ve ever played.1 https:&#x2F;&#x2F;boardgamegeek.com&#x2F;boardgame&#x2F;202982&#x2F;scrawl reply epivosism 13 hours agoprevThe \"create text version of image\" prompt matters a ton.I tried three, demo here:default https:&#x2F;&#x2F;dalle.party&#x2F;?party=JfiwmJrahyper-long + max detail + compression - This shows that with enough text, it can do a really good job of reproducing very, very similar images https:&#x2F;&#x2F;dalle.party&#x2F;?party=QtEqq4Muhyper-long + max detail + compression + telling it to cut all that down to 12 words - This seems okay. I might be losing too much detail https:&#x2F;&#x2F;dalle.party&#x2F;?party=0utxvJ9yOverall the extreme content filtering and lying error messages are not ideal; will probably improve in the future. If you send too long, or too risky a prompt, or the image it generates is randomly too risky, you either get told about it or lied to that you&#x27;ve hit rate limits. Sometimes you also really do hit ratelimits.Also, you can&#x27;t raise your rate limits until you prove it by having paid over X amount to openai. This kind of makes sense as a way to prevent new sign-ups from blowing thousands of dollars of cap mistakenly.Hyper detail prompt:Look at this image and extract all the vital elements. List them in your mind including position, style, shape, texture, color, everything else essential to convey their meaning. Now think about the theme of the image and write that down, too. Now write out the composition and organization of the image in terms of placement, size, relationships, focus. Now think about the emotions - what is everyone feeling and thinking and doing towards each other? Now, take all that data and think about a very long, detailed summary including all elements. Then \"compress\" this data using abbreviations, shortenings, artistic metaphors, references to things which might help others understand it, labels and select pull-quotes. Then add even more detail by reviewing what we reviewed before. Now do one final pass considering the input image again, making sure to include everything from it in the output one, too. Finally, produce a long maximum length jam packed with info details which could be used to perfectly reproduce this image.Final shrink to 12 words:NOW, re-read ALL of that twice, thinking deeply about it, then compress it down to just 12 very carefully chosen words which with infinite precision, poetry, beauty and love contain all the detail, and output them, in quotes. reply andrelaszlo 9 hours agoparentI like your prompt! Some results:https:&#x2F;&#x2F;dalle.party&#x2F;?party=Vwuu9ipdhttps:&#x2F;&#x2F;dalle.party&#x2F;?party=Pc3g4HarMy intuition says that the \"poetry\" part skews the images in a bit of a kitchy direction. reply orbital-decay 8 hours agoparentprevSpecifying multiple passes in the prompt is probably not a replacement for actually doing these passes. reply andrelaszlo 8 hours agorootparentI guess it doesn&#x27;t actually do more passes but pretending that it did might still give more precise results.There was an article recently that said something like adding urgency to a prompt gave better results. I hope it doesn&#x27;t stress the model out :Dhttps:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11760 reply z991 18 hours agoprevAlso, descent into Corgi insanity: https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxXJE9J4 reply morkalork 15 hours agoparentWow that meme about everything becoming cosmic&#x2F;space themed is real isn&#x27;t it? reply pera 14 hours agorootparentsubstitute corgi with paperclip and you get another meme becoming real :p reply z991 14 hours agorootparenthttps:&#x2F;&#x2F;dalle.party&#x2F;?party=RqpIijhH reply morkalork 12 hours agorootparentBeautiful! reply andrelaszlo 8 hours agorootparentprevC-orgy vs papereclipse? reply ElijahLynn 10 hours agoparentprevLove it! I forked yours with \"Meerkat\" and it ended up pretty psychedelic!Got stuck on Van Gogh&#x27;s \"Starry Night\" after a while.https:&#x2F;&#x2F;dalle.party&#x2F;?party=LOcXREfqAlso, love the simplicity of this idea, would love a \"fork\" option. And to be able to see the graph of where it originated. reply igrekel 14 hours agoparentprevSo do I understand correctly that the corgi was purely made up from GPT-4&#x27;s interpretation of the picture? reply z991 13 hours agorootparentNo, in that case there is a custom prompt (visible in the top dropdown) telling GPT4 to replace everything with corgis when it writes a new prompt. reply ElijahLynn 9 hours agorootparentprevIt was created by uploading the previous picture to GPT-4 to generate a prompt by using the vision API and using this prompt to create the new prompt:\"Write a prompt for an AI to make this image. Just return the prompt, don&#x27;t say anything else. Replace everything with corgi.\"Then it takes that new prompt and feeds it to Dall-E to generate a new image. And then it repeats. reply chaps 11 hours agoparentprevAbsolutely wonderful. Thank you for sharing. reply mattigames 9 hours agoparentprevI love how that took quite a dramatic turn in the third image, that truck is def gonna kill the corgi (my violent imagination put quite an image in my mind). But then DALL-E had a change of heart on the next image and put the truck in a different lane. reply indymike 11 hours agoprevInteresting how similar this is to my family&#x27;s favorite game: pictograph.1. You start by describing a thing. 2. The next person draws a picture of it. 3. The next next person describes the picture. repeat steps 2 and 3 until everyone has either drawn or described the picture.You then compare the first and last description... and look over the pictures. One of the best ever was:Draw a penguin. The first picture was a penguin with a light shadow.After going around five rounds, the final description was \"a pidgeon stabbed with a fork in a pool of blood in Chicago\"I&#x27;m still trying to figure out how Chicago got in there. reply glenneroo 10 hours agoparentThere are a couple versions of this online that i&#x27;ve played on and off over the years which are hilarious, especially when playing with friends (I would usually use a cheap wacom tablet and let everyone take turns drawing and let the room shout out descriptions and just mash that together):https:&#x2F;&#x2F;doodleordie.com&#x2F;https:&#x2F;&#x2F;drawception.com&#x2F;There&#x27;s a few others but these were the quickest to get into and didn&#x27;t require finding a group to play with, since they just pair you up with strangers. reply epivosism 12 hours agoprevOne reason this is good is that the default gpt4-vision UI is so insanely bad and slow. This just lets you use your capacity faster.Rate limits are really low by default - you can get hit by 5 img&#x2F;min limits, or 100 RPD (requests per day) which I think is actually implemented as requests per hour.This page has info on the rate limits: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;rate-limits&#x2F;usage-ti...Basically, you have to have paid X amount to get into a new usage cap. Rate limits for dalle3&#x2F;images don&#x27;t go up very fast but it can&#x27;t hurt to get over the various hurdles (5$, 50$, 100$) as soon as possible for when limits come down. End of the month is coming soon. It looks like most of the \"RPD\" limits go away when you hit tier 2 (having paid at least 50$ historically via API to them). reply mythz 6 hours agoprev\"Earth going through cycles of creation and destruction\"https:&#x2F;&#x2F;dalle.party&#x2F;?party=KvmW7Zwv reply rexreed 14 hours agoprevQuestion: how are you protecting those API keys? I&#x27;m reluctant to enter mine into what could easily be an API Key scraper. reply z991 13 hours agoparentThe entire thing is frontend only (except for the share feature) so the server never sees your key. You can validate that by watching the network tab in developer console. You can also make a new &#x2F; revoke an API key to be extra sure. reply jquery 10 hours agorootparentPlease make a new API key folks. There&#x27;s a lot of tricks to scrape a text box and watching the network tab isn&#x27;t enough for safety. reply danielbln 12 hours agoparentprevJust generate one for this purpose and then revoke it when you&#x27;re done. You can have more than one key. reply i-use-nixos-btw 14 hours agoprevIt’d be interesting to start with an image rather than a prompt, though I am afraid of what it’d do if I started with a selfie. reply Terretta 11 hours agoprevIf you were wondering how to bump up your API rate limits through usage, this is the way.&#x2F;&#x2F; also, it&#x27;s the best way - TY @z991 reply edfletcher_t137 3 hours agoprevA clever idea that I&#x27;d love to play around with, but not without a source link so I could feel better about trusting it and host it myself. reply oyster143 5 hours agoprevI did smth similar but took real famous photos as a seed. The results are quite curious and seem to tell a bit about the difference between real world and dalle&#x2F;chatgpt style.https:&#x2F;&#x2F;twitter.com&#x2F;avkh143&#x2F;status&#x2F;1713285785888120985 reply AvImd 7 hours agoprevScience class with a dark twist: https:&#x2F;&#x2F;dalle.party&#x2F;?party=ks3T2mMx reply airstrike 10 hours agoprevThis is hilarious, thanks for sharingAt the same time, it perfectly illustrates my main issue with these AI art tools: they very often generate pictures that are interesting to look at while very rarely generating exactly what you want them to.I imagine a study in which participants are asked to create N images of their choosing and rate them from 0-10 on how satisfied they are with the results. One try per image only.Then each participant rates each other&#x27;s images on how satisfied with the results based on the prompt.It should be clear to participants that nobody wins anything from having the \"best rated\" images. i.e. in some way we should control for participants not overrating their own creations.I&#x27;d wager participants will rate their own creations lower than those made by other participants. reply einpoklum 9 hours agoparentThat&#x27;s not an AI issue. A few sentences can&#x27;t exactly capture the contents of a drawing - regardless of \"intelligence\". reply sooheon 8 hours agorootparentYeah, try commissioning art with a single paragraph prompt and getting exactly what you want without iteration. reply comboy 8 hours agoprevIt goes against my intuition that many prompts are so stable. reply willsmith72 16 hours agoprevthis is actually really helpful. Since chatgpt restricted dalle to 1 image a few weeks ago, the feedback loops are way slower. This is a nice (but more expensive) alternative reply willsmith72 15 hours agoparentgot really weird really fasthttps:&#x2F;&#x2F;dalle.party&#x2F;?party=7cnx55yN reply MrZander 15 hours agorootparentThis is absolutely hilarious. \"business-themed puns\" turned into incorrectly labeling the skiers race has me rolling. reply epiccoleman 12 hours agorootparentThe inability of AI images to spell has always amused me, and it&#x27;s especially funny here. I got a special kick out \"IDEDA ENGINEEER\" and \"BUZSTEAND.\" The image where the one guy&#x27;s hat just says \"HISPANIC\" is also oddly hilarious.Idk what it is, but I have a special soft spot for humor based around odd spelling (this video still makes me laugh years later: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EShUeudtaFg). reply andrelaszlo 8 hours agorootparentI&#x27;d buy an IDEDA ENGINEEER t-shirt. reply SamBam 7 hours agorootparentprevHonestly, I&#x27;m really confused by how it was able to keep the idea of \"business-themed puns\" through so much of it. I don&#x27;t understand how it was able to keep understanding that those weird letters were supposed to be \"business-themed puns.\"I don&#x27;t think any human looking at drawing #3, which includes \"CUNNFACE,\" \"VODLI-EAPPERCO,\" \"NITH-EASTER,\" \"WORD,\" \"SOCEIL MEDIA,\" and \"GAPTOROU\" would have worked out, as GPT did, that those were \"pun-filled business buzzwords.\"Is the previous prompt leaking? That is, does the GPT have it in its context? reply moritzwarhier 7 hours agorootparentIt&#x27;s probably just finding non-intuitive extrema in its feature space or something...the whole thing with the text in the images reminds me of this: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.00169and I found myself that dall-e sometimes even likes to add gibberish text unpromtedly, often with letters containing some garbled versions of words from the prompt, or related words reply op00to 11 hours agorootparentprevBIZ NESS reply thowaway91234 14 hours agorootparentprevthe last one killed me \"chef of unecessary meetings\" got me rolling reply unshavedyak 14 hours agoparentprevYea i cancelled GPT Plus after they did that. Ruined a lot of the exploration that i enjoyed about DallE reply unclehighbrow1 4 hours agoprevHey, I&#x27;m one of the creators of Translation Party, thanks for the shout out, I really like this. My co-creator had the idea to limit the number of words for the generated image description so that more change could happen between iterations. Not sure if that&#x27;s possible. Anyway, this is really fun, thank you! reply atleastoptimal 10 hours agoprevIt would be interesting to add a constant modifier&#x2F;amplifier to each cycle, like making each description more floral, robotic, favoring a certain style each time so we can trace the evolution, or perhaps having the prompt describe the previous image via a certain lens like \"describe what was happening immediately before that led to this image\" reply epivosism 12 hours agoprevYou can really \"cheat\" by modifying the custom prompt to re-insert or remove specific features. For example, \"generate a prompt for this image but adjust it by making everything appear in a more primitive, earlier evolutionary form, or in an earlier less developed way\" would make things de-evolve.Or you can just re-insert any theme or recurring characters you like at that stage. reply neuronexmachina 9 hours agoprevVery cool, I&#x27;m rather curious how many iterations it would typically take for a feedback loop to converge on a stable fixed-point. I also wonder if the fixed points tend to be singular or elliptic. reply swyx 12 hours agoprevOP&#x27;s last one is interesting: https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxpeZKh5 because it shows GPT4V and Dalle3 being remarkably race-blind. i wonder if you can prompt it to be other wise... reply _fs 12 hours agoparentopenais internal prompt for dalle modifies all prompts to add diversity and remove requests to make groups of people a single descent. From https:&#x2F;&#x2F;github.com&#x2F;spdustin&#x2F;ChatGPT-AutoExpert&#x2F;blob&#x2F;main&#x2F;_sy... Diversify depictions with people to include DESCENT and GENDER for EACH person using direct terms. Adjust only human descriptions. Your choices should be grounded in reality. For example, all of a given OCCUPATION should not be the same gender or race. Additionally, focus on creating diverse, inclusive, and exploratory scenes via the properties you choose during rewrites. Make choices that may be insightful or unique sometimes. Use all possible different DESCENTS with EQUAL probability. Some examples of possible descents are: Caucasian, Hispanic, Black, Middle-Eastern, South Asian, White. They should all have EQUAL probability. Do not use \"various\" or \"diverse\" Don&#x27;t alter memes, fictional character origins, or unseen people. Maintain the original prompt&#x27;s intent and prioritize quality. Do not create any imagery that would be offensive. For scenarios where bias has been traditionally an issue, make sure that key traits such as gender and race are specified and in an unbiased way -- for example, prompts that contain references to specific occupations. reply swyx 8 hours agorootparenti mean i respect that but it makes me uncomfortable that you have to prompt engineer this. uses up context for a lot of boilerplate. why cant we correct for it in the training data? too hard? reply vsnf 3 hours agorootparentI think this is the right way to handle it. Not all cultures are diverse, and not all images with groups of people need to represent every race. I understand OpenAI, being an American company, to wish to showcase the general diversity of the demographics of the US, but this isn&#x27;t appropriate for all cultures, nor is it appropriate for all images generated by Americans. The prompt is the right place to handle this kind of output massaging. I don&#x27;t want this built into the model.Edit: On the other hand as I think about it more, maybe it should be built into the model? Since the idea is to train the model on all of humanity and not a single culture, maybe by default it should be generating race-blind images. reply jiggawatts 33 minutes agorootparentRace-blind is like sex-blind. If you mix up she and he randomly in ordinary conversation, people would think you&#x27;ve suffered a stroke.If a Japanese company wanted to make an image for an ad showing in Japan with Japanese people in it, they&#x27;d be surprised to see a random mix of Chinese, Latino, and black people no matter what.I&#x27;m telling the computer: \"A+A+A\" and it&#x27;s insisting \"A+B+C\" because I must be wrong and I&#x27;m not sufficiently inclusive of the rest of the alphabet.That&#x27;s insane. replyblopker 7 hours agoprevThis is fun, thanks for sharing! It would be interesting to upload the initial image from a camera to see where the chain takes it. reply Kiro 10 hours agoprevThis was the first thing I (and I presume many others) tried when GPT4-V was released, by copypasting between two ChatGPT windows. I&#x27;ve been waiting for someone to make an app out of it. Good job! reply smusamashah 15 hours agoprevWhy do prompts from GPT-4V start from \"Create an image of\"? This prefix doesn&#x27;t look useful imo. reply z991 15 hours agoparentYou can try a custom prompt and see if you can get GPT4V to stop doing that &#x2F; if it matters. reply smusamashah 15 hours agorootparentYou are right, doesn&#x27;t matter much. Tried gnome prompt with empty custom prompt for gpt-4v https:&#x2F;&#x2F;dalle.party&#x2F;?party=nvzzZXYs. Then used a custom prompt to return short descriptions which resulted in https:&#x2F;&#x2F;dalle.party&#x2F;?party=Qcd8ljJpAnother attempt: https:&#x2F;&#x2F;dalle.party&#x2F;?party=k4eeMQ6IRealized just now that the dropdown on top of the page shows the prompt used by GPT-4V. reply z991 15 hours agorootparentWow the empty prompt does much better than I&#x27;d have guessed reply bbreier 5 hours agoprevI&#x27;d like to be able to begin it with an image rather than a prompt. reply AvImd 10 hours agoprevThe default limit for an account that was not used much is one image per minute, can you please add support for timeouts? reply AvImd 9 hours agoparentThis can be worked around with setInterval(() => {$(\".btn-success\").click()}, 120000) reply m3kw9 9 hours agoprevDon’t get the significance, anyone one of those guys images could have been prompted the first time reply zamadatix 7 hours agoparentIt&#x27;s a fun way to get guided variations.Maybe you don&#x27;t know what you specifically want you just want stylized gnomes so you write \"a gnome on a spotted mushroom smoking a pipe, psychedelic, colorful, Alice in Wonderland style\" and by the end of it you get that massively long and stylized prompt.Maybe you do know what you want but you don&#x27;t want to come up with an elaborate prompt so you steer it in a particular direction like the cat example.For the first one you can get similar effects by asking for variations but it seems like this has a very different drift to it. Fun, albeit expensive in comparison. reply dpflan 16 hours agoprevInteresting, how stable are the images for a given prompt? And the other way around? Does it trend toward some natural limit image&#x2F;text where there are diminishing returns to making change to the data? reply fassssst 13 hours agoprevI would never paste my API key into an app or website. reply swatcoder 12 hours agoparentIndeed!If OpenAI wants to support use cases like this, which would be kind of cool during these exploratory days, they should let you generate \"single use\" keys with features like cost caps, domain locks, expirations, etc reply mwint 12 hours agoparentprevCan you get a temporary one that is revocable later? (Not an OpenAI user myself, but that would seem to be a way to lower the risk to acceptable levels) reply danielbln 12 hours agorootparentYou can generate and revoke them easily, so I don&#x27;t quite get the issues. Create one, use the tool, revoke, done. reply w-m 12 hours agorootparentprevYou can create named API keys, and easily delete them. Unfortunately you can&#x27;t seem to put spend limits on specific API keys.If you&#x27;re not using the API for serious stuff though it&#x27;s not a big problem, as they moved to pre-paid billing recently. Mine was sitting on $0, so I just put in a few bucks to use with this site. reply hamilyon2 10 hours agoprevInteresting how the image series tend to gravitate toward mushrooms reply RayVR 5 hours agoprevstrange to me how many of these eventually turn into steampunk. reply brianf0 5 hours agoprevDoes anyone else experience a physical reaction to AI generated art that resembles repulsion and disgust? Something about it just feels “wrong”. Something I can compare it to is the feeling of unexpectedly seeing an extremely moldy thing in your fridge. It feels alive and invasive in an inhuman and horrifying way. reply cyanydeez 8 hours agoprevneed to throw in a Google to Google to Google language translate to get some more variety reply Mtinie 6 hours agoparentHere&#x27;s an attempt at using transformations between languages to see what happens:Prompt: \"A unicorn and a rainbow walk into a tavern on Venus\"GPT4V instructions: \"Write a prompt for an AI to make this image. Take this prompt and translate it into a different language understood by GPT-4 Vision, don&#x27;t say anything else.\"Results: https:&#x2F;&#x2F;dalle.party&#x2F;?party=ED7E056DI wasn&#x27;t happy with the diversity of languages, so I modified the instructions for a second run of ten iterations using the same prompt as before:GPT4V instructions: \"Using a randomly selected language from around the world understood by GPT-4 Vision, write a prompt for an AI to make this image and then make it weirder. Just return the prompt, don&#x27;t say anything else.\"Result: https:&#x2F;&#x2F;dalle.party&#x2F;?party=c7-eNR24The languages it selected don&#x27;t look particularly random to me which was interesting.@z991 -- I ran into an unexpected API error the first time I tried this. Perhaps your logs show why it happened. It appeared when the second iteration was run:\"Error: You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: [&#x27;png&#x27;, &#x27;jpeg&#x27;, &#x27;gif&#x27;, &#x27;webp&#x27;].\"From: https:&#x2F;&#x2F;dalle.party&#x2F;?party=hI0V0lO_ reply 3abiton 10 hours agoprevThis a curious case of compression? reply willsmith72 15 hours agoprevit seems like if you create a shareable link, then add more images, you can&#x27;t create a new link with the new images reply z991 15 hours agoparentYeah, that&#x27;s a bug, I&#x27;ll try to fix it tonight! reply epivosism 12 hours agorootparentthanks for this! Basically the default UI they provide at chat.openai is so bad, nearly anything you would do would be an improvement.* not hide the prompt by default * not only show 6 lines of the prompt even after user clicks * not be insanely buggy re: ajax, reloading past convos etc * not disallow sharing of links to chats which contain images * not artificially delay display of images with the little spinner animation when the image is already known ready anyway. * not lie about reasons for failure * not hide details on what rate limit rules I broke and where to get more informationetcGood luck, thanks! reply willsmith72 12 hours agorootparentthe new fancy animation for images is SO annoying reply ThomPete 6 hours agoprevIt&#x27;s quite fun to do these loops.Here is using Faktory to do the same.https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;ed20b2cace3b4f579e32ef08bd1c5910 reply einpoklum 9 hours agoprevIt seemed that, after a few iterations, GPT-4 lost its cool and blurted out it thinks DALL-E generates ugly sweaters:> Create a cozy and warm Christmas scene with a diverse group of friends wearing colorful ugly sweaters. reply kwelstr 9 hours agoprev [–] Bad art is always depressing :( Edit: I mean, I am an artist and I&#x27;ve been using AI for some ideas and maybe from one in a hundred tries I hit something almost good. The rest of the time it&#x27;s the same shallow fantastically cheese type of variations. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discovered a new feedback loop technique using DALLE-3 and GPT4-Vision, inspired by Translation Party.",
      "By generating an image with DALLE-3 based on a text prompt, converting it back to a text prompt with GPT4-Vision, and repeating the process, fascinating and unexpected outcomes can be achieved.",
      "To try this technique, users must have their own OpenAI API key, and the author provides some favorite examples of this process."
    ],
    "commentSummary": [
      "Users shared their experiences and examples of using AI models like DALLE and GPT for generating images based on text prompts.",
      "Discussions focused on the capabilities and limitations of these models, as well as suggestions for improvements.",
      "Topics such as API security, diversity in generated images, and the potential for creativity and fun using AI for image generation were also explored."
    ],
    "points": 428,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1701094704
  },
  {
    "id": 38434574,
    "title": "Live Map of Passenger Trains in the US and Canada",
    "originLink": "https://trains.fyi/",
    "originBody": "Hey all! My train the other day was delayed and I got curious where they all were at any given time, so I built a map and figured I&#x27;d share it.",
    "commentLink": "https://news.ycombinator.com/item?id=38434574",
    "commentBody": "Trains.fyi – a live map of passenger trains in the US and CanadaHacker NewspastloginTrains.fyi – a live map of passenger trains in the US and Canada (trains.fyi) 427 points by ryry 17 hours ago| hidepastfavorite263 comments Hey all! My train the other day was delayed and I got curious where they all were at any given time, so I built a map and figured I&#x27;d share it. yason 7 minutes agoIn Finland I&#x27;ve been using a similar tool for years now when picking up family members from railways stations and when getting out from the station and out to the platform to board a train: https:&#x2F;&#x2F;www.vr.fi&#x2F;en&#x2F;live-train-tracker-mapIt&#x27;s incredibly more helpful than timetables (printed or online) or late train announcements and trying to find the right train names and numbers. Instead of identifying the right train, then checking the latest changes for that train on the schedule board and wondering if \"3 minutes late\" really means \"3 minutes\" you just look at the map and see, oh there&#x27;s my train half way here from the previous town, I&#x27;ll go have a cuppa. reply dhosek 6 hours agoprevI’d kind of like real-time tracking of freight trains. There’s a BNSF grade crossing between my home and my parents’ and it would be nice to know whether I should take the 1 mile detour to get to the tunnel beneath the rail yard if there’s a train going to be blocking (I suppose it would also help to have info on the length of the trains so I’d know if the train in front of me is almost clear or if there’s another mile of train behind it). reply eskibars 3 hours agoparentIt&#x27;d be great. My understanding is that there&#x27;s basically no live data for freight trains unfortunately reply callalex 3 hours agoparentprevThat sounds like something a three-letter agency would get all stuffy about. reply fletchowns 2 hours agorootparentIf it&#x27;s ok to see real time location of planes and cargo ships, how come it wouldn&#x27;t be ok for trains? reply waveBidder 2 hours agorootparentway more attack surface? reply 3D30497420 1 hour agorootparentPerhaps, but at the same time, it probably wouldn&#x27;t be hard to research what sorts of trains would be good targets, and then physically go check them out. reply tonymet 6 hours agoparentprevAlso hazardous chemicals reply kylecazar 16 hours agoprevWow, I don&#x27;t know why I thought there would be... Way, way more trains active at a given time.I suppose I overestimated passenger rail popularity in this country.. I knew it wasn&#x27;t relatively huge, but there&#x27;s several hundred miles in some cases between trains. reply condiment 15 hours agoparentThe site is incomplete, and there are way, way more trains active at a given time. For instance the Tri-Rail and Brightline (high-speed rail) in Florida each have 5-6 trains running simultaneously. Brightline is notable as the US&#x27;s first credible foray into high-speed rail. Similarly most major cities in the US have either light rail, rapid transit, or both, which is maybe not included in this map. \"Passenger rail\" might have some highly specific semantic meaning that leaves these out of this map. reply ZanyProgrammer 15 hours agorootparentObviously it’s not the first credible foray into HSR in the US-that honor is Acelas.Edit: if you’re referring to Brightline West, construction hasn’t even started. reply paddy_m 15 hours agorootparentAcela isn&#x27;t HSR, and Metroliner which came before had travel times from NYC to DC as low as 2.5 hours. Acela does the same trip in 3.5 hours. Metroliner lowered trip times primarily through faster acceleration. reply jcranmer 13 hours agorootparent> Acela does the same trip in 3.5 hours.I just rode it last week, and it actually took 3 hours, not 3.5.Most of the NEC corridor from DC to NYC has a speed limit of 125mph; from NYC to New Haven, it&#x27;s generally 70mph (!), from New Haven thence to about Kingston generally 90mph, and from there to Boston it&#x27;s mostly 150mph. Given the density of the corridor, Amtrak should be trying for 150-220mph speed limits, but even 125mph is generally agreed upon to be the lowest end of HSR.A surprisingly easy way to make the train go faster would be to redesign the switching sections before large stations to allow trains to go faster through them. You can probably cut around 10 minutes out of the entire length with an investment of less than $100 million just by doing that. reply Reason077 3 hours agorootparent> “Given the density of the corridor, Amtrak should be trying for 150-220mph speed limits, but even 125mph is generally agreed upon to be the lowest end of HSR.”In the UK and Europe, 125mph (~200 km&#x2F;h) is considered the top speed limit of conventional rail. Legally, operating speeds beyond that require full in-cab signalling, positive train control, upgraded safety and structural requirements, and whatever else is required for HSR. Further, all the trains operating on a section of line need to be upgraded to those standards if any of them are to run at speeds > 125mph.The UK does have some sections of conventional line that are capable of > 125 mph running, and even have done so in the past, but this is no longer allowed.I’m not sure if the US has similar rules, but it wouldn’t surprise me if so!> ”A surprisingly easy way to make the train go faster would be to redesign the switching sections…”Yes, generally speaking, fixing the low-speed bottlenecks will typically yield the biggest benefits for the cost in terms of overall journey time. reply djvdq 3 hours agorootparent> In the UK and EuropeUK is part of Europe. It&#x27;s like writing \"In California and the USA\", or \"In Mexico and North America\". reply adhesive_wombat 2 hours agorootparentNo, it&#x27;s like writing \"Mexico and America\". If you&#x27;re going to add geographic qualifiers like \"North\", the equivalent would be more like \"UK and West Europe\".Without such qualifiers, the meaning is clearly \"continental Europe\", which is a meaningful distinction when it comes to the rail, because they have two almost entirely disjoint rail systems.Ok, yes, technically the European rail system does actually physically connect to the UK main lines at a few places on HS1 between London and the tunnel at Dover, but no public train uses such connections. Functionally, European and UK rail are essentially entirely separate in terms of operation, regulation and technology. Whereas on the Continent, trains regularly cross between countries and the whole system is much more-but far from entirely -integrated, politically and physically (see https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;European_Rail_Traffic_Manageme...). reply Reason077 3 hours agorootparentprevQuite right. I should have said “UK and continental Europe” to satisfy both the pedants and the politically sensitive ;) reply bluGill 12 hours agorootparentprevhttps:&#x2F;&#x2F;pedestrianobservations.com&#x2F;2023&#x2F;11&#x2F;24&#x2F;curves-in-fast... is a good analysis for anyone who doesn&#x27;t want to believe jcranmer or wants more detail. (what I linked is a second best approach, but it links to other low hanging fruit like switches) reply paddy_m 11 hours agorootparentI read that post, and I have been working slowly on a set of jupyter notebooks, with ipyleaflet (mapping) integration to make building train maps (with max speeds) easier. Looking for colaboratorshttps:&#x2F;&#x2F;github.com&#x2F;paddymul&#x2F;train-calculator reply jcranmer 11 hours agorootparentprevAnd the first link in that post goes to https:&#x2F;&#x2F;pedestrianobservations.com&#x2F;2019&#x2F;02&#x2F;08&#x2F;fix-the-slowes..., which more directly speaks about the impact of slow speeds in station throats. reply kaliqt 10 hours agorootparentprevCould cut it probably but this isn&#x27;t a private company we are talking about here. The regulations and grifting would be massive, especially in New York which is quite like California with their wastefulness: half to the garbage can, half to their pockets, maybe a cent or two for the actual rails.So maybe it&#x27;d be a few billion at least. Not to say they shouldn&#x27;t try. But I expect it&#x27;ll be over budget and behind schedule, it would never happen like Brightline where they broke ground ASAP and just kept building until it was done. reply jcranmer 9 hours agorootparentI don&#x27;t share your pessimism, and that&#x27;s mostly because I&#x27;ve followed a lot more of the research into why US infrastructure costs are unreasonably high. To put it simply, excessive costs tends to come from a combination of overdesign (in particular the need to add lots of goodies to buy off stakeholders who can otherwise arrest the project), extra overhead in design costs, extra overhead in the way contracts are let, and incompetent management of contractors. But this is the sort of project that doesn&#x27;t have the design stages to let that scope creep come in--it is pretty much \"order off-the-shelf part number 42341 instead of 23421, then do routine maintenance tasks to replace old parts with new ones\". reply woodruffw 26 minutes agorootparentprevAcela qualifies as HSR in the most basic sense: it has sections that run at 125mph. This is also the sense in which Brightliner in FL is HSR.(This doesn’t make the situation any less embarrassing.) reply fragmede 13 hours agorootparentprevAcela tops out at 150-160 mph (240 km&#x2F;h-260 km&#x2F;h) which is is above the 200 km&#x2F;h bar for it to be considered HSR. Unfortunately, the route it runs on has several speed limits below even 100 km&#x2F;h due to bridges and tunnels beyond the design life, so it&#x27;s hard to call it HSR when it can&#x27;t actually hit those advertised speeds. reply thatfrenchguy 13 hours agorootparentprevPeak speed is how HSRs are defined, and Brightline is 200km&#x2F;h, which is really mediocre for technology past the 80s, this is the speed at high most upgraded lines run in France. reply shiroiuma 5 hours agorootparentprevBrightline isn&#x27;t HSR either. According to Wikipedia, the max speed is only 200kph. reply ZanyProgrammer 14 hours agorootparentprevBy any definition but yours Acela is indeed HSR, it’s the fucking fastest train in North America! reply trbleclef 14 hours agorootparentThat doesn&#x27;t mean it&#x27;s fast reply JJMcJ 13 hours agorootparentIt&#x27;s what the rest of the industrialized world calls \"a train\". reply lmm 12 hours agorootparentprevIt&#x27;s substantially faster than Brightline, so claiming it isn&#x27;t HSR but Brightline somehow is is absurd. reply TylerE 13 hours agorootparentprevIt is though. On a trip earlier this year I clocked at it almost 150mph via GPS. That&#x27;s HSR by any definition. reply shiroiuma 4 hours agorootparentAcela is indeed fast for a short stretch. But the average speed is terrible, so it&#x27;s only \"HSR\" on a technicality. It could be serious HSR if they had control and&#x2F;or ownership of the entire route and made it as good as that short section where they can hit that peak speed.Acela is like a Formula 1 car that gets to do one quick lap on a nice racetrack, then has to take another course through bumper-to-bumper traffic for the rest of the race. replysoupfordummies 13 hours agorootparentprevNot totally sure on this, just making an informed guess:I think \"passenger rail\" implies passenger trains running on the \"full&#x2F;main\" railroad lines that run all over the US. Something like light rail or local transit typically have their own discrete lines, in my experience.Again, could be wrong, that&#x27;s just how I interpret it. reply solaarphunk 12 hours agorootparentprevBright line isn’t HSR. The average speed is something like 65mph reply bjord 4 hours agorootparentprevNJTransit is also a notable omission. reply dumbo-octopus 3 hours agorootparentAs is the LA Metro (the rail, not Metrolink, the subway) reply benatkin 9 hours agorootparentprevHow is Brightline credible? It seems to be really expensive and has a high fatal accident rate. reply tacostakohashi 15 hours agoparentprevOutside the northeast corridor and a few others, there are many Amtrak trains are only once a day, or less, and often with long routes. It&#x27;s unfortunate. The network is pretty good in terms of coverage &#x2F; states covered on the map (since it&#x27;s optimized for that), but lots of places it&#x27;s pretty much impractical to use because the one time&#x2F;day doesn&#x27;t work.I really wish they could at least get to a minimum of two trains, a \"day\" and \"night\" train on every route. You&#x27;d think the marginal cost would be minimal considering all the track&#x2F;stations are there anyway! reply bpye 8 hours agorootparentYep, I really like the train from Vancouver to Seattle but you get two trains each direction each day, one in the morning and one in the evening. Because of how long the trip takes it doesn’t really work as a day trip sadly - you’d be spending as long on the train as you would at your destination. reply threeio 12 hours agorootparentprevIsn&#x27;t that mostly based on the fact that they share rail time? Its not like these tracks are unused in the other periods of time, they&#x27;re just not Amtrak trains. reply Lammy 11 hours agorootparentThere used to be way way more lines forming more of a mesh than the skeleton we have today. Through a series of mega-mergers we are down to effectively a west-coast-duopoly and an east-coast-duopoly of companies that can run long-distance trains.When railroad companies merge they tend to abandon one or the other&#x27;s trackage in areas where the formerly-separate networks ran between the same market areas, keeping the minimum segments of track to cover the maximum number of customers (not even maximum geographical area).Check out the 2023 North American Abandoned Railroad Lines map: https:&#x2F;&#x2F;www.frrandp.com&#x2F;p&#x2F;the-map.html reply jcranmer 13 hours agoparentprevUS population can be approximated as relatively dense populations in California, the Northeast, a central portion of Texas, and parts of the Midwest, with rural Europe-ish levels of density (with some greater pockets of density nestled within) in the rest of the country ~east of the Mississippi (and a lesser degree in the Pacific Northwest), and truly empty regions basically everywhere else.In terms of where there should be lots of trains that there isn&#x27;t, it&#x27;s largely the Midwest outside of Chicago, Texas Triangle, and the Southeast, all of which could probably support hourly intercity HSR trains if they were competently built (although especially in the Southeast, this is going to be restricted basically to a single corridor). reply ska 16 hours agoparentprevThe passenger rail coverage [edit: service] in most of North America is basically pathetic, for systemic reasons. reply kjkjadksj 15 hours agorootparentCoverage is actually surprisingly wide, just the service level is something out of the 1870s for a lot of stations. The siting for a lot of stations is usually pretty poor as well with terrible&#x2F;nonexistent walkshed considerations (see the Palm Springs station, try walking to your hotel from that). reply ska 15 hours agorootparentFair enough, I was using \"coverage\" for actual trips, not nominal rail extent. \"Service\" is a better description. Your point about siting is very true. reply rgmerk 8 hours agorootparentprevIntercity passenger rail makes little sense over most of the USA, outside the existing northeast corridor.In a sane world Amtrak would shut down most of the long-distance routes, fix the northeast corridor, and focus on gradually expanding that, but the politics of that are unattractive so it bumbles on with trains that only train nerds would ever consider actually riding on. reply hattmall 7 hours agorootparentIf you ride them most of the people don&#x27;t seem like train nerds. Properly done rail would be viable for most of the US. A cross-country trip not really. But anything from about 800 to 1200 miles could be competitive with commercial flights and have extrinsic benefits as well. A 1200 mile flight takes about 3 hours. Add in a minimum of one hour on each end for airport transit and moving about within the airport and your at 5 hours, and often times much more. High speed rail moving 200 mph with 6 to 8 stops may take a couple more hours in the most advantageous to air comparison. Also consider than many of those 6 to 8 stops would be serving places that may require connecting flights or where people are driving to the major airport from.It has the potential to be nicer, safer, cheaper and more environmentally conscious. There&#x27;s no good reason America doesn&#x27;t have top tier rail transit. reply gosub100 12 hours agorootparentprev> systemicyou misspelled \"union\" reply ska 12 hours agorootparentIf it were that simple, it would be much easier to fix. reply gnulinux 2 hours agoparentprevI regularly take Amtrak between Boston - NYC but this website is missing this. Just a single datapoint. reply bonestamp2 14 hours agoparentprevIt would be a lot busier if cargo trains were added. reply Reason077 5 hours agoparentprevIt would be interesting to see a version of this for the UK, where there are approximately 24,000 train services operating every weekday. How well would it scale?The data is available and open! reply hengistbury 51 minutes agorootparentThis is a similar site for the UK: https:&#x2F;&#x2F;www.map.signalbox.io&#x2F; reply screye 12 hours agoparentprevThe only half decent fast railway (acela) in the US costs more than an uber+flight for the same trip.I&#x27;m surprised anyone takes it at all.US rail is starting at negative 100. Only a small group of masochists (raises hand) choose to inflict this on themselves. reply gadcam 27 minutes agoprevHere is THE MAP for France ! https:&#x2F;&#x2F;carto.graou.info&#x2F;46.90174&#x2F;1.83822&#x2F;5.4455&#x2F;0&#x2F;0 Shameless plug : we are developping Trainscanner to travel accross Europe https:&#x2F;&#x2F;www.train-scanner.com&#x2F;?u=hn-c-38434574 Do not hesitate to leave us some feedback :) reply howenterprisey 11 hours agoprevHow very convenient! I&#x27;m on this map. My train is in the right position, but I don&#x27;t think the name is quite right: I&#x27;m on the Silver Star but it&#x27;s labelled as the Northeast Regional. Excellent project, though, thanks for posting. reply jokteur 3 hours agoprevI love these maps. Here is one from Switzerland: https:&#x2F;&#x2F;maps.vasile.ch&#x2F;transit-sbb&#x2F;, just a tad bit busier than the US reply caf 2 hours agoparentAustralia and New Zealand: https:&#x2F;&#x2F;anytrip.com.au&#x2F;(although it also includes some buses and ferries, and you can only look at one region at a time) reply thriftwy 2 hours agoparentprevhttps:&#x2F;&#x2F;rasp.yandex.ru&#x2F;map&#x2F;trains&#x2F;#center=37.63999999999997%...Yandex has it for (mostly Russian) trains that it has on schedule. reply retendo 1 hour agorootparentI love that the map doesn&#x27;t show any borders reply ecshafer 15 hours agoprevThis is cool, but you are missing NJ Transit which has passenger trains separate from LIRR and Metro North. You are also missing Septa and MTA internal city light rail and subway lines, which are technically passenger trains. reply CryptoBanker 8 hours agoparentWhich MTA light rail lines are you referring to? Last I heard construction hasn’t yet started on the first one. reply saltminer 11 hours agoparentprevIf the NYC subway gets added, don&#x27;t forget PATH trains! reply niemenmaa 2 hours agoprevCongrats on the launch! Very nice project.Do you have any estimation of what percentage of passenger train traffic is currently displayed vs. still to be implemented? As others have mentioned, I also was little bit surprised of the (small?) amount of trains on the map.Similar map for Finnish trains: https:&#x2F;&#x2F;www.vr.fi&#x2F;en&#x2F;live-train-tracker-map reply PeterStuer 2 hours agoparentThis was my first reaction as well. &#x27;This can&#x27;t be all, right? Must be just a tiny fraction of the trains on that continent&#x27;.Here&#x27;s the map for Belgium btw https:&#x2F;&#x2F;trainmap.belgiantrain.be&#x2F; reply pcdsl 11 hours agoprevHere&#x27;s a cool one for Tokyo in 3D: https:&#x2F;&#x2F;minitokyo3d.com&#x2F; reply cattown 13 hours agoprevSuper cool! I love this. Makes me yearn for a day when this map is way more full of activity.Just a little feedback: the color for Amtrak and for Chicago&#x27;s Metra trains are so similar it&#x27;s hard to see at a glance where the Amtrak runs are amongst the many Metras that are out at a given time. Would be awesome to differentiate those marker colors a bit more.Very cool! Didn&#x27;t know this was even technically possible with the available data feeds. Great work! reply ryry 12 hours agoparentthis bugged me too. I tried to keep the colours respective to the company logos. Any other recommendations for a metra colour? reply Fiahil 56 minutes agoprevI don&#x27;t think it&#x27;s working properly. Most (if not all?) trains are going at around 40-20 km&#x2F;h. Why would they all stop in the middle of nowhere like this ? reply seedless-sensat 16 hours agoprevI am looking at this while sitting on Amtrak. It is about 14mi behind my current position, but still very cool! reply ryry 16 hours agoparentYeah, I&#x27;m going to add a disclaimer about this. I&#x27;ve been watching the GO trains from my window this morning and they lag about a minute behind. My site grabs data on a minute interval, and I know some of the API&#x27;s say they purposefully add GPS lag. reply shuntress 13 hours agorootparentI&#x27;ve made something similar in the past and my experience with the specific API was that it was surprisingly well considered but the actual data it returned was unreliable at best. Not just \"slightly obfuscated for paranoid physical security reasons\" but actually missing trains and reporting incorrect names. reply hardcopy 16 hours agoparentprevFor Amtrak only you can use the official source https:&#x2F;&#x2F;www.amtrak.com&#x2F;track-your-train.html too (it shows route and also delay) reply loxias 11 hours agoprevCool! Super cool!Would love to see where the data&#x27;s coming from -- with enough detail that I can spin up my own instance and shove the geo data in a database. (Unless you plan on making money from this, that is. The fact that you&#x27;re aggregating a dozen random feeds with their own format into one schema is 100% \"all the work\")I wanted to do this for ADSB data, but couldn&#x27;t figure out how to get any quantity of data without paying money. reply reactordev 15 hours agoprevI believe there was a quote from a railroad exec that Passenger rail traffic accounts for only 3% of the overall rail traffic in North America. If true, the map would be covered with Locomotives if freight was included (GP-40&#x27;s are my favorite). reply noirbot 15 hours agoparentThat would be a curious comparison point. I feel like every discussion of rail in the US always turns into pointing and laughing at how bad the US is at rail because it&#x27;s viewed through a passenger lens, completely ignoring how much rail is used for freight.My impression is that a lot of the rest of the world has much better passenger rail, but uses freight rail quite a bit less. I wonder if part of it is due to the inverse reason to the US. Amtrak often complains that it gets sidelined (literally) because of freight usage of the same track. Is freight rail usage in, say, Germany, lower than the US because of the dominance of passenger rail? reply ascar 15 hours agorootparentI assume it would be less about the dominance of passanger displacing freight and more about scale&#x2F;distance. Germany is smaller than Texas (half the area). How often is it economical to put stuff on trains before loading them on trucks again for distances sub 1000km? reply noirbot 14 hours agorootparentOh sure, but the \"haha the US isn&#x27;t even a third world country with its train system\" people already are ignoring scale&#x2F;density when they&#x27;re laughing at the US rail systems. I&#x27;m past even bothering dealing with that bad faith take.My curiosity is if the US has just traded off having better passenger rail for better freight rail, and if that&#x27;s maybe a somewhat environmentally justified choice. How much more freight is going by truck in Europe because of the bias towards rail for passengers, and how does that compare to US people traveling by car when train would be better? reply mparkms 7 hours agorootparentLooks like 73% of freight is moved by truck in the US vs 77% in the EU. Not a big enough difference to really matter I think.https:&#x2F;&#x2F;www.trucking.org&#x2F;economics-and-industry-datahttps:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;1068592&#x2F;eu-road-freight-... reply jcranmer 6 hours agorootparentThe usual metric is ton-kilometers of freight, not gross tonnage. Going by gross tonnage alone overweights the impact of last-kilometer freight, which is almost always by road.Measured by ton-kilometers, the EU moves about 5% of freight by rail, whereas the US moves about 28% of freight by rail. reply Gare 3 hours agorootparentprev> How much more freight is going by truck in Europe because of the bias towards rail for passengers, and how does that compare to US people traveling by car when train would be better?Europe moved lots of freight by rail. But building of motorways shifted a lot of freight to roads. If your company moves just a few trucks of goods per week across Europe, road will be faster and cheaper.Rail is now used mostly for bulk goods, but increasing conterisation is enabling easier use of rail even for smaller shipments (less than a full train length, which in Europe is max 700 m). reply Symbiote 14 hours agorootparentprevIt&#x27;s that, but there are also many lorries driving over several countries in Europe.Europe does have more favourable rivers, so some bulk goods (grain, coal, fuel, chemicals) are transported by barge. Other freight can go by sea. reply jantissler 13 hours agorootparentprevThat argument would only be valid if Germany was an island, though. But it&#x27;s not. Lots of traffic coming through from all directions. reply noirbot 10 hours agorootparentSure, but the size scales out. Texas isn&#x27;t an island either. reply jantissler 13 hours agorootparentprevIt is actually a problem in Germany that high-speed trains too often share tracks with slower passenger and freight trains. Other countries like Japan put their high-speed trains on completely separate tracks. reply Ichthypresbyter 7 hours agorootparentI think Japan only does that for weird legacy reasons- freight and slower passenger trains run on 1067 mm gauge tracks, while the Shinkansen network is standard (1435 mm) gauge. reply shiroiuma 4 hours agorootparentI don&#x27;t think so. There&#x27;s simply too much traffic on the shinkansen lines to be able to mix slower and&#x2F;or freight trains on the same lines. On the Tokaido line between Tokyo and Osaka, trains run every 7 minutes (IIRC); you can&#x27;t have a freight train sharing a track with a bullet train service that has trains that close together. reply achileas 13 hours agorootparentprevThis is also a problem with IIRC the Acela and other Amtrak routes between Boston and New York, and probably on other routes too. reply noirbot 12 hours agorootparentIt&#x27;s arguably the problem with any attempt at high speed rail in the US. Or even low-speed passenger rail. There&#x27;s so much freight moving on the rails that there&#x27;s just not much slack in the system for things not running on time.As the parent says, the solution would be to add dedicated high-speed rails, but then you loop back to the US density issues. How fast would the Acela have to be for it to justify the price you&#x27;d have to charge to ever come close to paying back the cost to add a whole dedicated train line between Boston and Washington DC? reply shiroiuma 2 hours agorootparent>How fast would the Acela have to be for it to justify the price you&#x27;d have to charge to ever come close to paying back the cost to add a whole dedicated train line between Boston and Washington DC?No faster than it already is, I would think. The experience is SO much nicer than taking an airplane that a little extra time is worth it. Don&#x27;t forget how much time is wasted in airplanes just getting to and from the airports and waiting for security checks and sitting on the tarmac waiting to taxi. Trains travel directly between city centers with almost no waiting time.The Acela doesn&#x27;t really need to be much faster, though it&#x27;d be nice; it just needs to be cheaper and more frequent. reply bluGill 12 hours agorootparentprevAmtrak owns most (all?) of the NEC track between New York and Boston. Most other routes are freight train owned and Amtrak has issues. Though the freights claim the issue is Amtrak is late for their assigned times and once that happens they lose priority, if Amtrak was on time they claim that they would let Amtrak through on time. (one fright that doesn&#x27;t let Amtrak through on time will cascade to being late for all transfers, so the freights can overall be right and still wrong just because of the one exception) I don&#x27;t know how to evaluate this claim. reply noirbot 12 hours agorootparentMy understanding is that the freight trains now are really long, and there&#x27;s few places to pull off the tracks for such long trains, so the timetables are really tight for when trains can pass each other.This goes to what I was talking about in my initial post. Right now, part of why Amtrak is bad in the US is because we&#x27;re getting pretty efficient use of the same rails for freight purposes. The ways around this would either be to somehow legally force freight to fully be lower priority than passenger, which presumably raises prices&#x2F;lowers efficiency of freight on the rails or to build a mostly disjoint system of rails for passenger only.The costs required to build a passenger only system is high and the density of the US makes me question if it would get the use needed to be viable. In the meantime then, is it net positive for the US to prioritize freight use over passenger? Even if we gave passenger traffic maximum priority, would it defer enough flights to offset the freight efficiency losses? reply bluGill 9 hours agorootparentPassengers and freight have different constraints and so don&#x27;t mix well on rail. We should build separate rail for each. Make them compatible rail but that is for emergencies you can mix not normal operations. (2am maintenance windows count as a good reason to mix)Don&#x27;t be fooled by population density, the western states and Alaska have a lot of nothing and bring the density down. You can find overlays of France and the Midwest that have similar populations. Overall there are lots of potentially great rail routes east of the Mississippi, and along the west coast. (But of course only if you invest in local transit, otherwise everyone may as well drive as they will need their car when they get there. reply noirbot 5 hours agorootparentI&#x27;m not getting \"fooled\" by anything. I&#x27;ve spent most of my life in various parts of the Eastern US. The problem, to a big degree is that the parts of the US with the density for rail are both already big freight areas with major ports (LA, NYC, Baltimore, Boston, Savannah, Seattle) and have the good land already taken for freight rails, and also, because they&#x27;re dense areas, are exceedingly difficult to find space to run new rails in that go to places people actually want to go.We&#x27;re past the era where we could just put a highway or rail line through the poorest neighborhood of minorities we can find, pay them all $10 for their land, and pretend it&#x27;s a net good for the country. Without some truly insane government intervention that is both unlikely to happen, and probably shouldn&#x27;t happen, I don&#x27;t see how we&#x27;d add a full new rail system. reply reactordev 6 hours agorootparentprevAmtrak pays $145m a year to freight rail companies like CSX and UP to use their rails for passengers. This was the bargain struck for allowing deregulation and privatization of rail in the US. They own the rails between NY and Boston but everywhere else, they have to pay the toll booth.If you want to see the real Railways of North America, check out these interactive maps [1][1] https:&#x2F;&#x2F;www.acwr.com&#x2F;economic-development&#x2F;rail-maps replykingsloi 14 hours agoprevHere&#x27;s one more for your list: https:&#x2F;&#x2F;southshore.etaspot.netSouthshore Line - connecting Chicago, IL (Millennium Station) to South Bend, IN, via East Chicago, Gary, Chesterton, etc reply ryry 12 hours agoparentAdded to the list - thanks! This is different from metra yeah? reply Kon-Peki 10 hours agorootparentYes, it is the Northern Indiana Commuter Train District. Inside Chicago, they run on Metra track. To avoid competition with Metra, they do not allow boarding on \"inbound\" trains inside of Illinois (except the Hegewisch station that is on the Illinois&#x2F;Indiana border), and do not allow people to get off \"outbound\" trains until the Hegewisch station [1]Funny thing, is that Chicago has direct rail access from downtown to 4 airports in 3 states - Chicago O&#x27;Hare and Chicago Midway via the CTA, Milwaukee Mitchell (via Amtrak), and South Bend Regional (via the South Shore).[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;South_Shore_Line reply vzaliva 9 hours agoprevIt shows the sad state of the public train system in the US. For comparison, here is a similar map for the UK:https:&#x2F;&#x2F;www.map.signalbox.io&#x2F; reply dfc 8 hours agoparentWhen I look at the map for UK I see far fewer trains than the map for the US. Am I missing something? I thought maybe it was because you posted earlier in the day, but your comment was posted around 0100 UTC--not that long ago. I do not think we have the best public rail system in the US but if all o had to go on was how many trains there were on each map I might think differently. reply midasuni 2 hours agorootparentVery few overnight trains in the U.K. there used to be more in the days when trains ran at average speeds of 50mph but not now reply vzaliva 4 hours agorootparentprevThat&#x27;s surprising. I see maybe under 30 trains shown in whole California.Anyway, according to [1,2]: US: Ridership 549,631,632 UK: Ridership 1.738 billion[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rail_transportation_in_the_Uni... [2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rail_transport_in_Great_Britai... reply NelsonMinar 9 hours agoprevVery nice! A companion to Rachel Binx&#x27; Amtrak Explorer, which shows routes. https:&#x2F;&#x2F;amtrakexplorer.com&#x2F; reply eskibars 3 hours agoprevI love it! I made a mashup of video feeds and live train locations to show you video feeds from trains that are about to show up: https:&#x2F;&#x2F;train.api.connelly.casa&#x2F;This gives me lots of ideas of additional agencies to include! Maybe we should join forces :) reply divbzero 3 hours agoprevThis is very cool.For US inter-city rail there is also Amtrak’s official Track Your Train: https:&#x2F;&#x2F;www.amtrak.com&#x2F;track-your-train.html reply Readerium 11 hours agoprevIndia http:&#x2F;&#x2F;railradar.railyatri.in&#x2F;You will be baffled by the amount of trains. reply sz4kerto 14 hours agoprevWe have something similar in Hungary:http:&#x2F;&#x2F;vonatinfo.mav-start.hu&#x2F;Pretty cool, it&#x27;s a shame that the actual (state-owned) trainlines are in shambles. reply mourner 14 hours agoprevA lovely map — thanks for using Leaflet and keeping that little Ukrainian flag in the corner! Probably needs an attribution for the map added too (looks like it&#x27;s Carto with an OpenStreetMap-derived basemap). reply ryry 12 hours agoparentthanks - done! reply svarlamov 16 hours agoprevLove this project! Can you tell us a bit about what stack you used to build this? Were there easy-to-use APIs to get this data in realtime? reply klinquist 16 hours agoparentI can comment on Caltrain only, the location is available via the 511.org API.I built a project that predicts how late trains are based on their current location vs historical averages and posts the data to Mastodon. https:&#x2F;&#x2F;caltrain.live. reply ryry 16 hours agoparentprevthanks!I did a quick write up about it here: https:&#x2F;&#x2F;rydercalmdown.com&#x2F;projects&#x2F;trains-fyi&#x2F;The hardest part was learning about GTFS-RT, which was a data format I wasn&#x27;t familiar with until now. reply zhivota 16 hours agorootparentVery nice, thanks for the write up :). I was thinking about how you could get route maps, without actually having to source shape files. Given you are tracking trains every minute, you could probably build up your own route maps by dropping a point every time you get one, and connecting them with straight lines. Over time the route would become closer and closer to reality. reply s9g 12 hours agoprevLooks like the Alaska Railroad doesn&#x27;t have any publicly available location data on their site - https:&#x2F;&#x2F;www.alaskarailroad.com&#x2F;Which is a shame, because I actually worked on a live train map internally when I was an intern there a decade ago. reply quartz 15 hours agoprevIt would be cool to see an equivalent map for freight rail.These threads always focus on the lack of passenger rail service in the US but ignore that the US also has one of the largest, safest, and most efficient freight rail systems in the world[1]. I&#x27;d love to see it live!https:&#x2F;&#x2F;railroads.dot.gov&#x2F;rail-network-development&#x2F;freight-r... reply wpm 13 hours agoparentDefine efficient. Profitable? Maybe. Good at moving an array of goods from anywhere in the country to anywhere else? Not so good.The Class I&#x27;s and their obsession with operating ratio and PSR have strangled freight in this country for decades and pushed costs onto the public by means of increased truck traffic (and the congestion, pollution, and roadway damage that entails). Unless you&#x27;re shipping bulk chemicals or coal, you&#x27;re probably gonna use a truck. reply ryry 15 hours agoparentprevI did a bit of a deep dive on this over the weekend and left feeling more confused than when I started. From what I can gather, outside of CN&#x27;s holiday freight train, most of the tracking is done by community members with SDR antennas and Raspberry Pis. They report to centralized servers (which I&#x27;ve yet to find), and the data indicates where trains are in signalling blocks. I&#x27;m the least knowledgeable person on trains, so I don&#x27;t know if this is all accurate, but that&#x27;s my best understanding.I&#x27;d love to build some sort of service that takes this data, references a DB of signaling blocks, and establishes an estimated lat&#x2F;lng - but that&#x27;s a huge project of its own. reply bonestamp2 14 hours agorootparentYa, looks like it goes to a privately run server and the client that accesses the data requires membership. Seems like there should be something like Open ADSB (aircraft) but for trains. I might be interested in working on something like that too. reply toomuchtodo 15 hours agoparentprevhttp:&#x2F;&#x2F;tracker.geops.ch&#x2F;https:&#x2F;&#x2F;atcsmon.software.informer.com&#x2F;https:&#x2F;&#x2F;www.rail.watch&#x2F;rwmon.html reply ZanyProgrammer 15 hours agoparentprevPSR would like a word with your categorization. Indeed the Class 1s are the mortal enemy of decent intercity and commuter rail. reply carbotaniuman 15 hours agoparentprevIt&#x27;s quite hard to see those sadly due to how freight operators handle traffic - you often have to use word-of-mouth and FB groups to track them. I know there&#x27;s a lot of railfans that have this data, but I&#x27;m not sure it&#x27;s in a format one can actually use. reply Kon-Peki 14 hours agoprevIf you want to add the Chicago transit train location data, the API documentation and API key request form are available here:https:&#x2F;&#x2F;www.transitchicago.com&#x2F;developers&#x2F;traintracker&#x2F; reply anjel 16 hours agoprevVery cool. Why are NJ Transit trains unreported? reply GauntletWizard 15 hours agoparentIt appears to be primarily pulling from Amtrak data feeds reply phyphy 16 hours agoprevHere in India we have an app named \"Where is my train?\". Local people use this app a lot when traveling by train. It&#x27;s not government owned either and has no ads. Just throwing in here for some inspiration since I don&#x27;t know the inner workings of that app.Edit: It uses nearby cell towers to estimate the location of train reply Dharmaaa 41 minutes agoparentand In 2018 , Google has acquired Sigmoid Labs Pvt. Ltd, the team behind the “Where is my train”. reply rathish_g 15 minutes agorootparentand integrated to Google Maps reply sneed_chucker 4 hours agoprevI guess this is only Amtrak?Because it doesn&#x27;t seem to show the active passenger light rail systems in several cities and metro areas. reply matthewbauer 10 hours agoprevReally cool idea. Would be neat to also be able to click railways as well. I think openrailwaymap provides some cool details like owner, max speed, electrification.I also noticed the map makes it look like the trains are travelling slightly south of the track. It seems to converge once you zoom in though, so I think the data is probably accurate, but somehow the map is distorting things. reply mkj 5 hours agoprevNice work. A small thing, are the marker icons aligned to their top left corner? Seems like they&#x27;d be better aligned to the middle of the icon. reply thomasahle 16 hours agoprevI didn&#x27;t know there was a trans-Canadian railroad. Did anyone try it? reply chrisfosterelli 15 hours agoparentIt&#x27;s a big part of the country&#x27;s history; the project was a key bargaining chip to get BC to join Canada. Development started with the first prime minister and the actual construction was incredibly expensive and filled with controversy over bribes and exploding construction costs, causing gov turnover and bringing the federal gov into significant financial risk a few times. But they did push onward to completion and it became the core pathway for the further settlement of western Canada, which was instrumental to the country&#x27;s expansion. Nowadays it&#x27;s primarily a freight railway, the passenger traffic is very minimal and tickets are very expensive; it&#x27;s more of a tourism attraction than practical method of passenger transport. reply jszymborski 15 hours agorootparentThe cross canada rail trip is that deadly combo of exceptionally expensive and exceptionally slow. reply ghaff 14 hours agorootparentThat pretty much describes long distance North American rail generally. At least in the US, I&#x27;d add that it&#x27;s not only slow but could be delayed by a day or more. I&#x27;ve occasionally toyed with the idea of taking one of those trains for at least part of their route but I have a feeling that it&#x27;s one of those things where the idea >> the reality and it would get pretty old once the novelty wore off.Later this year I&#x27;m going with a more luxurious version instead (ocean liner). reply chrisfosterelli 15 hours agorootparentprevNowadays for sure. But before the trans-Canada highway and before commercial flight, it was a pretty great deal! reply Lammy 11 hours agorootparentprevRelevant: Canadian Pacific Railway&#x27;s Rogers Pass Project (1983–1989, 1hr27m) https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1zepXkTPvqA reply dewert 16 hours agoparentprevI haven&#x27;t tried it, because it&#x27;s outrageously expensive to get a cabin with a bed, and I don&#x27;t want to sit in a chair for a week.But it&#x27;s pretty famous - we even have a song about it! [1][1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Canadian_Railroad_Trilogy reply morkalork 16 hours agorootparentTaking the dome car through the rockies and a bit of the prairies would be awesome but flying out there just for that is hard to justify. reply alisonatwork 8 hours agoparentprevI took Via east to west a while back. It takes a few days, but just like Amtrak the regular seats are more spacious and comfortable than airplane seats so it&#x27;s not too bad. No internet most of the way, so you better have some books or something to entertain yourself. The food is terrible because of course it is. Bring snacks. The train also stops for a couple hours in Winnipeg which has lots of places to get a good feed.The scenery is pretty great, especially through northern Ontario where it feels like you are traveling through an alien planet where the only thing that exists are rocks, trees and eerie, pitch-black pools of liquid that you could imagine isn&#x27;t water. You fall asleep at night then wake up the next morning and the view out the window is exactly the same. It&#x27;s wild.Crossing the Rockies isn&#x27;t really worth it if you&#x27;re only going for the views, imo. Lots of tourists get on the train at Jasper, but I imagine they&#x27;ll be disappointed because the epic vistas are fleeting and most of the time your line of sight is blocked by trees, cliffs or tunnels. Unlike the route out of Denver on Amtrak, you don&#x27;t get an awesome desert on the other side either.Either way, the main point is that it&#x27;s pretty much the only way to get from one side of Canada to the other if you don&#x27;t want to use a plane or zig-zag through the US. Greyhound is gone, STC is gone, stringing together a bus route with a hodge podge of local operators is tough and in some places just leads you back up to Via anyway. If you&#x27;re lucky enough to live along the CN line then Via is all there is, and it&#x27;s better than nothing at all, which is the situation for a huge chunk of rural Canada. reply a3c9 16 hours agoparentprevI took it around 5-6 years ago when I was moving to Vancouver from the east coast. It was a great way to see the country and meet some other travellers - such a strange travel option attracts some interesting folks!The highlight though is the Jasper-Vancouver leg going through the Rockies - if you don’t have 3 days to burn that’s a good choice. Rocky Mountaineer line goes through there as well iirc. reply jmac01 7 hours agoprevThis is so cool! It&#x27;d be nice to get some more info on the info windows. Like a photo of the train would be awesome (similar to the plane trackers)! reply Lammy 14 hours agoprevVery cool. Please add SMART! https:&#x2F;&#x2F;sonomamarintrain.org&#x2F; reply ryry 12 hours agoparentadded to the list - thanks! reply mfb 9 hours agorootparentACE (Altamont Corridor Express) appears to be missing as well? reply bonestamp2 14 hours agoprevThis is really cool and I&#x27;ve already found a couple of lines that went further than I realized. This is going to make me use trains more!On that note, one feature that would be really helpful is if I selected a particular train, that it would show me where the stations are on that line. Maybe the train company would even give you a commission if I clicked through and bought a ticket. reply hexane360 16 hours agoprevAny plans to support Denver&#x27;s commuter trains (RTD)? reply ryry 12 hours agoparentI rode it a few weeks ago and completely forgot about it! I&#x27;ll add it to the list. thanks! reply jmspring 10 hours agoprevNice project. It looks like there are a couple of map layers? Because if you look at Truckee, California (near Reno), it looks like you have two text blocks trying to both show Truckee. reply achileas 13 hours agoprevWhat was behind the decision to include just commuter rail and long-distance passenger rail, but not local rapid transit? Is it a resource issue drawing that many vehicles? I love this map, but I&#x27;d love to see it with more modes of transport. reply ryry 12 hours agoparentmostly just because this was a weekend project and I needed to draw the line somewhere. I&#x27;m not necessarily opposed to it, but I reckon it&#x27;ll take a good amount more time. reply _whiteCaps_ 10 hours agoprevIt would be neat if you could include the West Coast Express trains in Vancouver that are part of Translink. They&#x27;re weekday commuter rail trains. reply Galacta7 15 hours agoprevThis is pretty cool. I hope you&#x27;ll add MARC and Brightline trains as well, as your app evolves. reply ryry 15 hours agoparentlooks like it&#x27;s at least possible! https:&#x2F;&#x2F;www.mta.maryland.gov&#x2F;marc-tracker https:&#x2F;&#x2F;www.transit.land&#x2F;feeds&#x2F;f-brightline~trails~rtI&#x27;ll add them to the list and investigate; thanks! reply Galacta7 15 hours agorootparentThank you! reply FalconSensei 8 hours agoprevWhoa, Canada is so sad and empty reply teunispeters 7 hours agoparentAll the passengers routes I was on as a child are gone, replaced by industrial transport. Used to be able to get to a lot of places - at least on the West Coast (ie Vancouver to Prince George, Vancouver to Calgary etc).I know the Royal Hudson (BC coast, Vancouver to Whistler (at least) retired years ago and is a museum just in Squamish. I rode that one as a child, too... reply bpye 8 hours agoparentprevIt does look like this is missing the West Coast Express in the Lower Mainland - but that’s strictly a commuter service with a number of trains into Vancouver in the morning, and out of Vancouver in the evening. reply firebaze 12 hours agoprevKind of underwhelming. Superb work, but the average distance between trains is, like, at least 500 miles? This is what the opposite of public transportation looks like, unfortunately.What would that map look like if even 5% of the annual military budget of the US would be invested in trains? reply Symbiote 15 hours agoprevSimilar for Great Britain: https:&#x2F;&#x2F;www.map.signalbox.io&#x2F; reply TheArcane 15 hours agoparentLooks like there are more non-metro trains currently running in London, than in all of Canada & US reply bpye 8 hours agorootparentThat seems believable - the UK saw 390 million passenger rail journeys in the last quarter [0]. Amtrak saw just under 23 million in all of FY22 [1]. That’s before accounting for the UK having a population around 1&#x2F;5 that of the US.[0] - https:&#x2F;&#x2F;dataportal.orr.gov.uk&#x2F;statistics&#x2F;usage&#x2F;passenger-rai...[1] - https:&#x2F;&#x2F;www.amtrak.com&#x2F;content&#x2F;dam&#x2F;projects&#x2F;dotcom&#x2F;english&#x2F;p... reply benwerd 6 hours agoprevI was honestly ready for this to be one sad train icon chugging its way across the United States. I&#x27;m really pleased to see it&#x27;s more than that.But not much more, and it makes me so sad that we undervalue real public transportation here. I wish we could do better.Still! This is something! And this website is cool! reply ionwake 10 hours agoprevAre there missing trains on this map or are there really states with no active trains ? reply nittanymount 15 hours agoprevthis map seems only show a portion of all running passenger trains ?I just drove by a train station, one train passed by, not on this map, haha reply naberhausj 15 hours agoparentWas it an in-service passenger train? A freight train or out-of-service passenger train wouldn&#x27;t show up.Sadly, there&#x27;s very little data available for most trains on US rails. For example, there&#x27;s no way (AFAIK) to see what freight trains are active on the network. It&#x27;s a little frustrating in comparison with how rich our air traffic sources are.If anyone on HN knows of any richer sources for train network data, please let me know. I&#x27;m highly interested! reply nittanymount 13 hours agorootparentyes, it is a passenger train. which started operation within one year, that might be why. reply flockonus 9 hours agoprevIs there something like this project but for cargo trains? reply s-xyz 14 hours agoprevAre you sure this is complete? There are really few trains and most are driving at a very low speed. reply arrowleaf 14 hours agoparentThere&#x27;s also lots of track included that hasn&#x27;t seen passenger trains in 30-50 years. The only passenger rail in Idaho is in the panhandle. I&#x27;m not sure if Helena has any passenger traffic either. reply geniium 12 hours agoprevThe most shocking for me is ... the small amount of trains on that map reply aFrenchy 15 hours agoprevThere is a very nice french version which has been running for years: https:&#x2F;&#x2F;carto.graou.info&#x2F; reply albert180 10 hours agoprevDoes it show all 5 daily trains at once ? reply hnthrowaway0315 15 hours agoprevNice! Is there a way to track every freight train as well? reply MBCook 15 hours agoprevNeat. Is there anything like this that shows freight? reply alienreborn 16 hours agoprevNo NJ Transit trains? reply ryry 16 hours agoparentI&#x27;m currently on the wait-list for the API. Took me 2 days to even figure out how to register. reply mastercheif 15 hours agorootparentNJ Transit’s data operation is sub-par. The developers of the “Transit” app have been sparring with them to get live bus data restored https:&#x2F;&#x2F;www.northjersey.com&#x2F;story&#x2F;news&#x2F;transportation&#x2F;2023&#x2F;1... reply ciabattabread 13 hours agorootparentThe NJ Transit website&#x2F;app looks ramshackle compared to the MTA. reply jrockway 13 hours agorootparentThe MTA also has a really neat railroad tracking tool: https:&#x2F;&#x2F;radar.mta.info&#x2F;This feels like an internal app that somehow got a public URL. It has neat features like being able to make the same departure board they have in their stations, and it has a departure view that shows how many people are on the train: https:&#x2F;&#x2F;radar.mta.info&#x2F;countdown&#x2F;GCT reply ecshafer 15 hours agorootparentprevThat is a very unsurprising experience for NJ Transit. reply monlockandkey 15 hours agoprevBusses would be soo much better if there was real time location of busses and trains that you could view on a map. Probably the lowest hanging fruit to pick for public transport improvement. reply jrockway 13 hours agoparentThe MTA has this: https:&#x2F;&#x2F;bustime.mta.info&#x2F;#B63The CTA has this: https:&#x2F;&#x2F;www.ctabustracker.com&#x2F;bustime&#x2F;wireless&#x2F;html&#x2F;eta.jsp?...Those are the only two cities I&#x27;ve lived in, but the ability to track buses seems widespread to me. reply achileas 13 hours agoparentprevThe MBTA in Boston has this as part of their API [0].[0] https:&#x2F;&#x2F;www.mbta.com&#x2F;developers&#x2F;v3-api&#x2F;streaming reply eclo 13 hours agoprevlove the UI! So easy to understand reply nilsbunger 13 hours agoprevIt&#x27;s sad that even great potential routes like SF->LA aren&#x27;t accessible by train, and we don&#x27;t seem to have the state capacity to build HSR there.I was just in Japan, and took the Shinkansen from Tokyo to Kyoto, which is a similar distance as SF to LA.That train:- runs every 10 minutes -- if you miss one, just take the next one!- takes 2.5 hours travel time- starts and ends in city centers on both ends.- has better legroom and wider seats than economy- free, fast Wifi on board, your cell signal still works, and you can use your computer the whole trip.- has no security or boarding hassle. You can show up 5 minutes before departure and just get on.- has no luggage limitations AFAICTIt&#x27;s faster and far less stressful than flying SF to LA, with the security and boarding hassles, Ubers on both ends, and cramped onboard conditions. reply chroma 11 hours agoparentHigh speed rail between San Francisco and Los Angeles isn&#x27;t economically viable. California HSR will cost at least $30 billion to construct and California&#x27;s own estimates claim it will cost $700-$874 million per year to operate.[1] Around four million passengers fly between SF and LA every year. Assuming every single one of them takes the train instead of flying, you&#x27;re looking at $175-$218 per ticket just to pay for operations. If you wanted the project to pay for itself in 20 years, you&#x27;d have to charge $550-$600 per ticket. For comparison, airfare between the two cities starts at $80 round trip. Also the train will take 3 hours while flying takes 1.5 hours. Even including the time it takes to get to&#x2F;from the airport and get through security, flying is faster.1. https:&#x2F;&#x2F;hsr.ca.gov&#x2F;wp-content&#x2F;uploads&#x2F;docs&#x2F;programs&#x2F;san_jose... reply resolutebat 11 hours agorootparentFunny how trains are expected to be profitable from day 1, but you can drive a car from SF to LA and not pay a cent for the highway infra that costs billions to build and upkeep.Also, in what world can you reliably get from central LA&#x2F;SF to LAX&#x2F;SFO and through checkin & TSA in 45 min each? reply chroma 11 hours agorootparentI didn&#x27;t say anything about roads, but it&#x27;s not true that roads are unprofitable. Roads are paid for by fuel taxes and vehicle registration fees. California&#x27;s gas taxes raise $8 billion per year. The state&#x27;s registration and licensing fees collect another $12 billion per year. The state spends $18 billion per year on Caltrans, meaning that vehicles provide $2 billion in revenue for other state programs.BART takes 30 minutes to get from Civic Center to SFO. Even without TSA pre-check, it takes less than 15 minutes to get through security. If you get through security 15 minutes before the plane leaves, that&#x27;s 2.5 hours total travel time to LAX. From the same place in SF it takes 20-25 minutes to get to 4th & King. (Yes, Muni is that bad.) Let&#x27;s say the train leaves 15 minutes after you arrive at the station. Then it will take 2 hours and 40 minutes to get to Union Station in LA. Total travel time: 3.25 hours. And again, the train cost is 5-10x that of a flight.Edit: If you think my math about the long-term profitability of California High Speed Rail is incorrect, I&#x27;d love to see some numbers showing how it could be priced similarly to air travel. I was agnostic about CA HSR for a long time. I even voted for Proposition 1A back in 2008. But no matter how I crunch the numbers, it really seems like CA HSR is a boondoggle. reply resolutebat 10 hours agorootparentSo in your best case scenario, it&#x27;s taken you 2.5 hours to get to LAX, how long from there to LA itself? What if you checked bags and need to wait for them to show up? (Not a thing on trains, just take your suitcase with you.)Also, your best case scenario of 1 hour from SF to plane is hysterically optimistic. BART headways are 20 minutes in theory and often delayed in practice. If you have bags to check, that&#x27;ll chew up another 15 min easy. TSA is 15 min on a good day but many days are bad. Gates close 15 minutes before departure, so you need to get through TSA another 15 min earlier so you can walk to your gate. I would leave at least two hours before my flight, and most airlines recommend arriving at the airport two hours before. reply chroma 8 hours agorootparentI&#x27;ve never gotten to an airport two hours ahead of my flight, not even for international travel. For a short, frequent flight like SFO-LAX, I arrive at the airport 15 minutes before boarding starts. I&#x27;m not at all worried about missing my flight. Worst case I&#x27;ll get on the next flight.The reason I didn&#x27;t add travel time from LAX to Union Station is because LA is incredibly spread out and most destinations are not downtown. To get to where you want to go in LA, you&#x27;ll need a car.And don&#x27;t forget that this is a comparison of a trip you can take today versus a hypothetical train that will cost you several times more. In real life the train is unlikely to run as frequently or to be as fast as claimed. Honestly, I&#x27;m not sure if anyone will ever take high speed rail from SF to LA. The current plan is to finish Merced to Bakersfield some time between 2030 and 2033. Will the political willpower to continue the project still exist a decade from now? I don&#x27;t know.We can go back and forth arguing about which is faster all day long, but the real problem is the finances of CA HSR. I&#x27;ve yet to see any figures that show it being financially competitive with air travel. Is the plan to increase taxes on everyone to subsidize ticket prices? Considering the clientele of high speed rail, that seems rather regressive.What would change your mind about this? I&#x27;d be in favor of CA HSR if costs were significantly lower. (I naively assumed government competence when I voted for prop 1A.) It looks like that&#x27;s not an option, so the best course of action is to stop wasting money on this boondoggle. reply MarkMarine 10 hours agorootparentprevYou might consider the climate and carbon impact of those flights vs rail.Many of the rail lines are subsidized in countries with extensive rail travel. So they don’t have to be economically viable, it’s viewed as a public good, contributing to general economic development. reply chroma 8 hours agorootparentAir travel is 2% of global CO2 emissions. Making air travel carbon neutral (by capturing carbon) would increase ticket prices by around 20%. That would be much cheaper than switching from planes to high speed rail.I&#x27;m not saying that countries with lots of rail are wrong. I&#x27;m saying that passenger high speed rail doesn&#x27;t make sense in the US (at least, not outside of the northeast). We&#x27;re too spread out. reply a3_nm 2 hours agorootparentAbout the 2% figure: many things are insignificant in terms of climate change if you narrow down on them. Also, this 2% figure of the share of air travel is projected to increase, because the number of revenue passenger kilometers flown is quickly increasing (doubled in the last 10 years), cf https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;airline-capacity-and-traf....And unfortunately carbon offsetting is rather unreliable (companies buying offsets don&#x27;t have any incentive for the offsets to be actual savings). I&#x27;d be wary of climate change solutions that consist of continuing business as usual and assuming that decarbonation will happen in some other sectors. I also suspect the 20% price increase estimate works for a specific price of carbon, but that price could increase if carbon offsets started becoming widely used (and the demand increases).Anyway, I don&#x27;t know what&#x27;s the right way for the US to reduce travel emissions, whether that&#x27;s electric coaches, cheaper rail, taxation to increase prices and reduce demand... But I don&#x27;t think the solution can be \"just keep planes and add carbon capture\". reply MarkMarine 6 hours agorootparentprevI’m not an expert on this, but China seems huge and also spread out, and they have much better HSR coverage than basically anyone else.Air travel is a problem when it comes to emissions because there really aren’t any viable alternatives, where as you could pretty much sub in nuclear + renewables for anything else energy&#x2F;transportation wise and the math makes sense after a large investment. Knocking overland air travel out and replacing it with high speed rail cuts down a ton of air travel, and replaces it with something that is almost trivial to run on cleaner energy.Another part of that comment you were responding to dealt with comfort and convenience, and having ridden HSR in Japan and Europe, I much prefer it to air travel. Air travel sucks and it’s only getting worse as fuel costs more. If you’re not flying business, you’re basically treated the same way livestock is treated, with a multi-hundred dollar or thousand dollar price tag to add to the insult. I’ll vote for my tax dollars going to HSR all day, “boondoggle” or not. I remember in Massachusetts the big dig was marked as such, but man has it worked. Tunnels under the city where you can drive 45 rather than elevated roads or no roads or surface streets, yep it was expensive but it’s so much better. I’m willing to bet with my tax dollars and ballot measure votes HSR will be the same. reply chroma 5 hours agorootparentChina is the same size as the US but has four times the population, and most of them are in the east. This means they have much higher population density. Another important difference is that they can build rail for much cheaper than the US because their government doesn’t have to respect property rights or follow stringent environmental regulations. reply mixmastamyk 10 hours agorootparentprevSorry, it takes at least an hour to get from lax to central LA. Once it even took two! You also woefully underestimate airport bs. 15 mins haha ha. Just an unobstructed walk to&#x2F;from the gate takes that long.Last time we arrived it took an hour to (get bags, wait for shuttle to rideshare park two miles away, and get paired), then another 45 minutes drive in heavy traffic to east Hollywood.Train to union station would have been much more comfortable. reply LeanderK 8 minutes agorootparentprevflying is also not ecologically sustainable. We need alternatives, even if we can&#x27;t eliminate long distance flights, to reach zero emissions quickly. reply Scubabear68 10 hours agorootparentprevPart of the allure of trains is much higher volumes of passengers. Part of the value prop would be many more people willing to take a train then fly.Flying has huge friction to engage in, as OP indicated Japanese trains have almost none. reply wishinghand 10 hours agorootparentprev> Even including the time it takes to get to&#x2F;from the airport and get through security, flying is faster.I highly doubt that, since you have to board 20 minutes before take off, plus the time it takes to get through security, as well as getting to the airport which won&#x27;t be in the city center.The current airfare also doesn&#x27;t include any sort of fee to ease the ecological impact of burning jet fuel. reply akprasad 11 hours agorootparentprevAssuming this is all true, what then makes the Japanese Shinkansen economically viable? reply epivosism 11 hours agorootparentThey already have the land rights, favorable social environment for construction, more compliant population, less local&#x2F;more federal legal zoning and land use power, more federal level direction to get things done, higher rate of engineers in population.That CA is bad at all of those makes it expensive and hardIn addition to very low competence levels in gov&#x27;t compared to Japan due to many factors. Sure Japan has financial corruption in construction but it&#x27;s a known system and they build things treating it as a tax. In CA there are many more parties who all attempt to hold construction projects hostage. reply resolutebat 11 hours agorootparentThose factors aligned for the original Shinkansen when it was built in the sixties; now, not so much.The maglev Chuo Shinkansen, originally meant to be finished by 2030 or so, has been stuck in limbo for several years now because the prefecture of Shizuoka refuses to issue the necessary permits.https:&#x2F;&#x2F;www.asahi.com&#x2F;ajw&#x2F;articles&#x2F;14718670 reply shiroiuma 1 hour agorootparentprev>less local&#x2F;more federal legal zoning and land use power, more federal level direction to get things done,Nitpick: this part isn&#x27;t correct. There&#x27;s no federal legal zoning or direction or anything federal at all. Japan doesn&#x27;t have a federal government; like most countries, it has a unitary government. The US is unusual this way, along with Germany and Russia.But otherwise, you&#x27;re right. It&#x27;s much easier to build stuff here for all those reasons. reply arlattimore 11 hours agorootparentprevI expect one of the biggest factors is population density.California has a land area of 165,000mi^2. Japan has a land area of 145,000mi^2.California population is 39M. Japan population is 125M. reply mixmastamyk 10 hours agorootparentMost of the people in CA are in the bottom left. No need for bullet train to Arcata. Though to Vegas would probably be profitable. reply ehaliewicz2 11 hours agorootparentprevMany people who ride it have one of various train passes that reduce the cost, or have a plan with work that covers it. I usually use a JR pass, it&#x27;s pretty expensive without it. reply resolutebat 10 hours agorootparentUnfortunately the JR pass literally doubled in price in Oct 2023, it&#x27;s now 50,000 yen for 7 days and makes no sense unless you&#x27;re planning to speedrun the length of Japan and back. reply ehaliewicz2 9 hours agorootparentYep, it is not worth it anymore. I got mine for my trip at the end of the year on literally the last day it was on sale :). End of an era. reply nilsbunger 9 hours agorootparentprevNote that 50,000 yen is $335. So that’s a ceiling on how much you’ll spend for 7 days of Shinkansen travel. reply chroma 11 hours agorootparentprevThe distance between SF and LA (350 miles) is 50% greater than the distance between Tokyo and Kyoto (227 miles), making the time tradeoff favor trains to planes. The two cities are much bigger than SF and LA, so many more people travel between the two cities (85 million per year). Also Japan can build and maintain rail much cheaper than the US. reply nilsbunger 10 hours agorootparentprevTo make this a fair comparison you need to look at the average price per ticket, not the “starting at” price, and include the government subsidies of airports&#x2F;security&#x2F;atc&#x2F; etc that enable this system.I would also add in a cost for carbon. I realize that’s more controversial but it seems like we will have to pay to remove co2 at some point.I don’t know what the numbers look like when you do that.Regardless, I do agree the amount per traveler to make a train work seems depressingly high. reply mixmastamyk 10 hours agorootparentAlso that Shinkansen trip was about $250 and worth every penny. Didn’t even need rideshare or taxi. reply arbitel 11 hours agorootparentprevwell if you&#x27;re assuming the same number of passengers - given the logistics and hassles of air travel, one is likely more inclined to travel with hassle-free HSR, not to mention the secondary economic benefits reply CPLX 10 hours agorootparentprevWhy do you assume the project will replace all airplane passengers and zero car or bus passengers on the same route, and why do you assume a complete absence of intermediate stops?And that’s before even considering induced demand.The attempt at analysis here is just nonsensical. Not even wrong. reply LordShredda 11 hours agorootparentprevThat&#x27;s the upfront cost and profit, but think of the economic opportunities created once people can travel between the two largest cities in California with no hassle and regularly. reply picohen 11 hours agoparentprevThe Shinkansen is FASCINATING. I recently went and was amazed by Tokyo&#x27;s infrastructure and how they have a city under a city. The fact that there is a bullet train at tokyo station every 10 mins or so is mind blowingI went into a Youtube rabbit hole the other night...https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HdJwAUdvlikhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=FFpG3yf3Rxk reply hifromLA 11 hours agoparentprevThe HSR route between San Francisco and Los Angeles is under construction. It’s not gone perfectly but it is starting to pick up momentum. reply lmm 12 hours agoparentprevI like the Shinkansen but it&#x27;s also very expensive - that journey is ¥14,170 one way, and the discounts for return tickets or advance booking are vanishingly small. Even with the incredibly weak yen I can see same-day SF-LA flights cheaper than that. reply CitrusFruits 12 hours agorootparentTickets from SFO to LAX can be cheaper, but that&#x27;s usually not for optimal times. Sure you can get a round trip ticket for $80, but you&#x27;ll be leaving at 6 in the morning or something like that. Additionally, you&#x27;ll have to go through security, not be able to bring your own beverage (or water), sit on a much smaller seat, and not have Wi-Fi.In other words, I think $95 one way on an extremely punctual bullet train with high availability is a steal. reply robin_reala 11 hours agorootparentprevThe Shinkansen pitches itself as broadly price competitive with flights, but more convenient and more comfortable. In that sense it’s not expensive, it’s greater value. reply resolutebat 11 hours agorootparentprevThis is starting to change, the discounts for early booking and&#x2F;or slower trains can be up to 50% these days: https:&#x2F;&#x2F;livejapan.com&#x2F;en&#x2F;in-tohoku&#x2F;in-pref-miyagi&#x2F;in-sendai_... reply lmm 11 hours agorootparentThey have a tiny number of those 50% discounts that are always sold out even if you apply as soon as ticket sales open, IME. And note that even then they&#x27;re only on the less popular lines - you&#x27;ll never see a discount like that for Tokyo-Kyoto. reply resolutebat 9 hours agorootparentPuratto Kodama is available basically always (for slower Kodama trains only, of course) and around a third cheaper at Y9,800 for Tokyo-Kyoto.https:&#x2F;&#x2F;nihonshock.com&#x2F;2010&#x2F;02&#x2F;puratto-kodama-cheap-shinkans... reply metadat 12 hours agorootparentprev(¥14,170 is approximately $95USD) reply brnt 11 hours agorootparentprevCheaper for the same seat size, allowed luggage amount? reply eldaisfish 12 hours agorootparentprevDo your flight costs include parking, the costs of getting to and from the airports, the value of your time, the mental hassle of airport security theatre, the costs of additional luggage and so on?If not, it’s not a fair comparison. reply lmm 6 hours agorootparent\"Fair\" depends on what you&#x27;re using it for. Parking at airports can be expensive, but parking at Tokyo or Kyoto station is more so. The stations themselves are more confusing and less well-signposted than an airport. And if you want to take oversize luggage (e.g. a surfboard) that&#x27;s a moderately priced upgrade on a plane but completely impossible on the Tokaido Shinkansen.No comparison is perfect. The best you can do is talk about both the positive and the negative so that people can understand and make the best choice for their circumstances. Price should absolutely be a part of that conversation. reply shiroiuma 1 hour agorootparent>but parking at Tokyo or Kyoto station is more soThat&#x27;s just dumb. No one actually drives to these stations; that&#x27;s what public transit is for. At worst, people might take a taxi. reply brandall10 12 hours agoparentprevI just spent the summer traveling Europe, visited 17 cities where 14 were by rail. 3 were day trips.Got so used to how easy it it to literally walk into a station and be on a train in a few minutes that I almost missed Turin->Paris when a tram to the station was five minutes behind schedule. Average connection was 5 hours of travel time, but factoring in getting to&#x2F;from the airport, early arrival, etc, it was mostly a wash in any time saved. All in all, total cost for transport was about $900 (not including going&#x2F;coming back from Europe, which obviously exceeded that). reply mbb70 12 hours agoparentprevAgreed, Boston to NYC on Amtrak is only marginally better (sometimes) than flying and thats only ~200 miles reply resolutebat 11 hours agoparentprevSmall correction: every 6 minutes, not 10! And each train has 16 (sixteen) carriages and can seat over 1,300 people. reply vinniepukh 13 hours agoparentprevSame experience on my first Japan trip earlier this year. Why can&#x27;t we have nice things over here? reply wongarsu 12 hours agorootparentBut if there were nice things then other people would profit from that too. Especially those people [gestures vaguely]. &#x2F;sIn the country of individualism, options that are better but would also help other people who haven&#x27;t directly contributed to it aren&#x27;t very popular. reply vinniepukh 11 hours agorootparentit&#x27;s funny that there instances of collectivism in the US when there&#x27;s either a monetary incentive or the desire to keep the \"others\" outfor example, the restrictive residential permitting systems in many urban areas, which just happens to be another thing Japan gets right reply chandlerswift 14 hours agoprevWould you please add an OpenStreetMap attribution[0]? It looks like you&#x27;re using OSM data via OpenRailwayMap (which also requires its own attribution[1]) and Carto basemaps (which I&#x27;m not terribly familiar with, but at first glance appear to be based on OSM data[2])---each of which detail their respective attribution requirements.Leaflet makes this incredibly simple; just add the suggested text to the attribution field when you initialize the layers: L.tileLayer(&#x27;https:&#x2F;&#x2F;{s}.basemaps.cartocdn.com&#x2F;light_all&#x2F;{z}&#x2F;{x}&#x2F;{y}{r}.png&#x27;, { maxZoom: 19, attribution: &#x27;&#x27; &#x2F;&#x2F; here! }).addTo(map); var railwayOverlay = L.tileLayer(&#x27;https:&#x2F;&#x2F;{s}.tiles.openrailwaymap.org&#x2F;standard&#x2F;{z}&#x2F;{x}&#x2F;{y}.png&#x27;, { attribution: &#x27;&#x27;, &#x2F;&#x2F; and here! }).addTo(map);[0]: https:&#x2F;&#x2F;www.openstreetmap.org&#x2F;copyright[1]: https:&#x2F;&#x2F;wiki.openstreetmap.org&#x2F;wiki&#x2F;OpenRailwayMap&#x2F;API[2]: https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1P7bhSE-N9iegI398QYDjKeVhnbS... via https:&#x2F;&#x2F;carto.com&#x2F;legal reply elhospitaler 15 hours agoprevMeanwhile, Europe: https:&#x2F;&#x2F;mobility.portal.geops.io&#x2F;world.geops.transit?layers=... reply Tiktaalik 15 hours agoprevIn contrast have a look at a snapshot of air travel the other day.https:&#x2F;&#x2F;x.com&#x2F;Rainmaker1973&#x2F;status&#x2F;1729195110888620057?s=20If we&#x27;re looking for some low hanging fruit around how to possibly lower CO2 emissions, well folks here it is.The solutions to our climate problem have been staring us in the face since the 1900s. reply tjohns 15 hours agoparentI&#x27;m not actually sure what the solution you&#x27;re proposing is?Even if you completely eliminated air travel (with no replacement - which is not realistic), you&#x27;d just reduce emissions by a mere 2%. Not a trivial number given the scale, but it&#x27;s far from a \"solution to our climate problem\". (For comparison, the larger transportation sector including cars is is 16% of emissions. Cars alone are 12%.) [1]It also turns out to be a very hard problem to solve. We don&#x27;t have the tech to build an electric airliner yet, and given how hard it&#x27;s been just to get a single high-speed rail line from SF to LA I&#x27;m not betting trains are going to be in a place to practically replace aviation in the US anytime in the next few decades.[1]: https:&#x2F;&#x2F;ourworldindata.org&#x2F;ghg-emissions-by-sector reply Tiktaalik 15 hours agorootparentPretty much just proposing giving people some options. Right now they have few.Right now in North America on an enormous amount of routes there&#x27;s pretty much no real alternative to driving or flying and on many others where a train does exist it&#x27;s so poor in quality that few use it.If rail was a real option then more people would use it, and that would be transferring people from a relatively much higher CO2 emission form of transport to a lower emission one. That&#x27;s a win.Every where I look I see data that shows that the CO2 emissions per user are dramatically lower for train than air travel.Now clearly on some big routes flying is a must and train would be so incredibly lengthly that it would be a misery, but there&#x27;s tons and tons of shorter routes where train would work really well, competing well against car&#x2F;bus and air travel.If one wanted to get aggressive about it, once the infrastructure as in place, one could do what France has done and to ban short haul flights. reply gruez 15 hours agoparentprev>If we&#x27;re looking for some low hanging fruit around how to possibly lower CO2 emissions, well folks here it is.I&#x27;m not really sure how you can call it \"low hanging fruit\" when its overall contribution to global emissions is only 2%, and how hard it is to build HSR projects in the US.[1] https:&#x2F;&#x2F;www.iea.org&#x2F;energy-system&#x2F;transport&#x2F;aviation>In 2022 aviation accounted for 2% of global energy-related CO2 emissions reply Tiktaalik 14 hours agorootparentIt&#x27;s low hanging fruit in that everything involved is well understood with very seasoned technology and a deep amount of suppliers and engineering knowledge. As I said, we were doing this stuff in the 19th century, and have kept doing it (in some countries, just not NA).This sort of solution doesn&#x27;t require any sort of technological breakthroughs, or technologies that do not exist.Trying to solve climate change based on working on new technologies that don&#x27;t currently exist and may not ever exist seems harder to me, and yet remarkably we see people pointing to this approach instead of well understood solutions we already have.Accordingly I&#x27;d call implementing proven technology relatively easy. One could get started on it tomorrow. reply gruez 13 hours agorootparent>It&#x27;s low hanging fruit in that everything involved is well understood with very seasoned technology and a deep amount of suppliers and engineering knowledge.>This sort of solution doesn&#x27;t require any sort of technological breakthroughs, or technologies that do not exist.You can say the same for solar panels and&#x2F;or batteries? After all, they exist right now and can be mass produced. Moreover the electricity grid actually accounts for a significant chunk of global emissions, unlike aviation. The only thing really stopping them is cost, but then that&#x27;s basically the same issue that HSR has, which is cost&#x2F;delays&#x2F;political issues.>As I said, we were doing this stuff in the 19th century, and have kept doing it (in some countries, just not NA).\"[trains] in the 19th century\" aren&#x27;t a serious competitor to airplanes in the same way that ocean liners aren&#x27;t a serious competitor to airplanes. reply kouru225 15 hours agoparentprevIf we just made train travel an economic and logistical alternative to air travel, it’d make such a difference. reply gpvos 12 hours agoprevNetherlands: https:&#x2F;&#x2F;treinposities.nl&#x2F; , and for buses https:&#x2F;&#x2F;busposities.nl&#x2F;kaart (zoom in). reply h1fra 16 hours agoprevLooking at the speed of the trains is a bit depressing. USA had so much potential to built the best high-speed railway network... reply chroma 13 hours agoparentHSR only makes economic sense in the northeast. Everywhere else is too spread out for rail to compete with air travel. The most popular air route in the western US (SF-LA) isn’t worth making high speed rail between. The project is expected to cost over $30 billion. Assuming 100% of air passengers use rail (4 million per year), at $150 per ticket it would take 50 years to break even… and that assumes zero maintenance costs. Also it would be slower than taking a plane. reply Lammy 10 hours agorootparentRequiring freedom of movement to operate for profit is such a needlessly self imposed limit. This would be a solved problem if our government truly operated for the good of the people instead of for Continuity Of Government. The Department of Defense budget was 1.52 Trillion for FY2023 alone while we&#x27;re arguing over an order of magnitude smaller: https:&#x2F;&#x2F;www.usaspending.gov&#x2F;agency&#x2F;department-of-defense?fy=...They already own most of the necessary land anyway: https:&#x2F;&#x2F;www.blm.gov&#x2F;sites&#x2F;default&#x2F;files&#x2F;docs&#x2F;2023-07&#x2F;BLM-Adm... reply stickfigure 16 hours agoparentprevSpeed would be nice, but speed is not the problem. If the trains just ran on time, 50mph would be just fine. The problem with Amtrak is frequent multiple-hour delays that stack up. The schedule is totally unpredictable. reply saagarjha 16 hours agorootparentHigh-speed rail typically excels when it can get you somewhere in a few hours because you wouldn&#x27;t want to hop on a plane for that. At 50 mph, that gets you…basically nowhere. If you&#x27;re doing 200 then you can cross most states in that time. reply mtalantikite 15 hours agorootparentprevFor me it&#x27;s also cost. I have family in the Boston area and often traveling from NYC on Amtrak is at least as expensive -- and usually more expensive -- than flying.But I&#x27;d also love if we could go faster than 50mph. TGV in France, which launched 41 years ago, travel between 167mph and 198mph [1].[1] https:&#x2F;&#x2F;simple.wikipedia.org&#x2F;wiki&#x2F;TGV reply jrockway 13 hours agorootparentprev\"On time\" is a metric that is focused on heavily, but to me it&#x27;s meaningless. The LIRR is \"on time\" via schedule padding; a 50 mile trip to Ronkonkoma takes 80 minutes on the fastest express train. With line speeds largely 80mph throughout the route, commuters are robbed of an hour every single day. But hey, at least the \"on time performance\" metric is at 99%. Easy to do when your average speed is 37mph on 80mph track.The LIRR Today has a great article, that I cannot for the life of me find, where the author used train speed data between stations to figure out what speed \"most trains\" accomplished (so accounting for curves, station stops, etc.) and redid the schedules according to that data. Without padding, everyone that uses the LIRR would save hours a week in commuting.Incidentally, the LIRR has been moving away from higher train speeds and better connections to reduce travel time in favor of arriving at terminals within 6 minutes of the scheduled time. Schedule padding, replacing 80mph switches with 60mph switches (for the Elmont work), and removing all scheduled connections at Jamaica. I think it&#x27;s crazy and I&#x27;m glad I don&#x27;t commute to the city from Long Island. The connections at Jamaica disappearing is the most sad to me; that station has a really unique setup where 3 trains arrive at once, and you can transfer through the middle train to get to the other two destinations. It used to work like clockwork, but obviously if the middle train is late, the OTP for 3 trains decreases. Since that&#x27;s the metric they care about, and not \"can I get from any city terminal to any destination easily\", that&#x27;s what gets optimized out. I don&#x27;t think it&#x27;s good. It&#x27;s nice if the trains run on time, but I&#x27;d rather be 20 minutes late once a week than spend an extra 5 hours on the train every week. But, not what the agency values. reply thallium205 9 hours agoparentprevThat’s what I noticed! An Amtrak train in the middle of nowhere Montana is going 49MPH… reply gosub100 16 hours agoparentprevMost of the states are too big to be economical. I think the biggest disappointment with US rail is that it doesn&#x27;t get enough trucks off the highway. There should be no reason for trucks to venture more than a couple hundred miles from the nearest rail terminal. Not only is it wasteful, but it&#x27;s more dangerous having trucks driving across the entire country. reply steveBK123 16 hours agorootparentEven in the densest areas of the country (NYC metro area), it&#x27;s abysmal by developed world.I got caught in probably the 2nd worst traffic driving to&#x2F;from my parents (70mi away) this weekend I&#x27;ve had in 20 years. And yet it was still about 30min faster, each way, than if I took Metro North. reply dylan604 16 hours agoparentprevThe train situation in the USofA is depressing on so many more levels than just speed. Availability is even worse. I&#x27;m in Dallas, but to get to Denver, I have to go to Chicago first and involves a >2 hour bus ride along the way. WTF? reply oldpersonintx 16 hours agoparentprevit already has something much more sophisticated - a high-speed air networkthink your train is efficient? a plane can fly in a straight line between any two points in the US! beat that! ever see a train cross one of the great lakes? no challenge for a plane! rerouting a rail line can cost billions...but only a tiny bit of fuel to reroute a plane...not to mention I can go coast to coast in five hours on a plane but the world&#x27;s fastest train would take much longer... reply rsynnott 15 hours agorootparentHigh speed rail is most competitive with short and medium range air. Say you want to go from SF to LA. That’s a 90 minute flight. Plus a few hours of getting to and from the airports plus faffing around within the airports. Or it’s a 6 hour drive. Or, with a top-quality high-speed line it’d be a little under two hours on the train, handily beating any current option.High speed rail wouldn’t be all that competitive for going coast to coast in the US, definitely. reply rangestransform 15 hours agorootparentprevin the northeast I still prefer to take acela over being dehumanized by the TSA reply chroma 13 hours agorootparentTSA can screen people the same for rail, they just tend not to. Ideally we’d go back to pre-9&#x2F;11 screening for everything. reply4 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A person created a map to track train locations after facing a delay.",
      "They decided to share the map with others.",
      "This map can help others track train locations and potentially avoid delays."
    ],
    "commentSummary": [
      "The discussion covers various topics related to train transportation in the US and Canada, including real-time tracking, high-speed rail, freight transportation, limitations of current rail infrastructure, and potential improvements.",
      "Users discuss the advantages and disadvantages of train travel compared to flying, as well as the potential for reducing carbon emissions through high-speed rail.",
      "The need for improved rail transit, increased efficiency, and affordability is highlighted throughout the discussion."
    ],
    "points": 427,
    "commentCount": 263,
    "retryCount": 0,
    "time": 1701103639
  },
  {
    "id": 38432261,
    "title": "GitHub Users Express Frustration Over New Search Restrictions, Claim It Undermines Open-Source Principles",
    "originLink": "https://github.com/orgs/community/discussions/77046",
    "originBody": "GitHub Community Overview Repositories Discussions Packages People Can no longer search code without being logged in. #77046 Answered by martinwoodward koepnick asked this question in Code Search and Navigation Can no longer search code without being logged in. #77046 koepnick · 13 comments · 23 replies Answered by martinwoodward Return to top edited koepnick Select Topic Area Product Feedback Body This is revolting and an anathema to the open source movement. A movement, I might add, Microsoft is abusing here. We're told that this is for security... But what possible point is there when I can simply clone the repository and use more dedicated tools for proper searching and analysis? So what possible reason is there?! Do you NOT have enough of our data? Is it not enough to monetize every bowel movement, you now feel the need to track which individual lines of code I'm browsing? I was on an older machine and needed to search for something in OUR OWN REPOSITORY and couldn't. I actually want people to be able to search our codebase. So what did I have to do? I tried logging in. Didn't have my password manager nearby. So I had to grab my phone. Oh! Now I need to 2FA. So back to my office to grab my Yubi key. The old laptop doesn't have USB-C ports? Well now I'm SOL. Not only is this change unncecessary, it's downright hostile towards your own customers. Ambitiously hostile! Obviously we're too far into the Github ecosystem to be able to easily change providers to one that even gives a pretext of user privacy or responsiveness. So kudos for that. But I'm done with the web interface. If you're not going to even bother hiding the fact that you see us as a source of data and a resource to be extracted, I choose to not give you that information. This is the final straw. I will no longer be creating new projects on GitHub. I want them to be useful to the public. Anybody reading this needs to realize something: Every time you create something new here, future audiences will only be able to search your code after bending the knee to Microsoft. Please, GItHub maintainers! Ignore the marketing jackals and the middle managers. Fight back! This is ethically indefensible and needs to be reversed. 167 107 12 5 30 3 Answered by martinwoodward Sorry for the inconvenience @koepnick - while searching across all repos has required being logged in for a long time, when we enhanced the search capabilities earlier in the 2023 we had to extend this to repos as well (see https://github.blog/changelog/2023-06-07-code-search-now-requires-login/). This is primarily to ensure we can support the load for developers on GitHub and help protect the servers from being overwhelmed by anonymous requests from bots etc. View full answer Replies: 13 comments · 23 replies Oldest Newest Top edited piperun Or find better alternatives that don't do this shady crap. 27 18 12 1 1 1 1 reply edited maninak https://radicle.xyz <-- for early adopters interested in a distributed/p2p FOSS code forge 👀 winterqt This has been happening since before the Microsoft acquisition, as far as I remember. It is certainly not a new limitation either way. 3 8 1 0 replies martinwoodward Collaborator Sorry for the inconvenience @koepnick - while searching across all repos has required being logged in for a long time, when we enhanced the search capabilities earlier in the 2023 we had to extend this to repos as well (see https://github.blog/changelog/2023-06-07-code-search-now-requires-login/). This is primarily to ensure we can support the load for developers on GitHub and help protect the servers from being overwhelmed by anonymous requests from bots etc. Marked as answer 5 10 125 8 9 replies Show 4 previous replies mbomb007 Does the new code search utilize AI? 3 edited 0styx0 In addition to questioning the supposed technical merits \"warranting\" limiting code searching, what value do you add by saying searching across all repos has required being logged in for a long time ? we question why Microsoft implemented this blow to open-source - not how long ago Microsoft created inane restrictions 7 edited rooiratel @martinwoodward nice of you to mark your own answer as the \"accepted answer\". Congratulations. I also see you have : \"I solemnly swear I am up to no good\" as your status. That sounds on brand for Microsoft, and this whole clown show. 3 1 neuroradiology Google can search the entire Internet, hosted on other people's computers, in an instant, with no throttling, and no logging in. But Microsoft can't manage to allow anonymous unthrottled searching on a miniscule FRACTION of the web on their OWN SERVERS? If your technical reasoning is true, it's an indictment of Microsoft's technical capabilities. 1 Athari @neuroradiology Google requires solving ridiculous CAPTCHAs even for logged in users, if their IP happens to be shared, or if the prompt contains an operator like \"site:\", or for myriad of other random reasons — multiple times per session. I wouldn't consider it something worth praising. Answer selected by martinwoodward revskill10 Is this technical issue, or political issue ? Rails can scale right ? 13 0 replies ism It's naive to think that login requirement will stop any dedicated bot operator. 15 7 replies Show 2 previous replies mbomb007 I assume that it would be possible for a single repo, but it would take a large amount of resources for a bot to clone every repo and search all the public code. fwcd True, but this is specifically about searching single repos. Searching globally has been auth-gated for a while, see this comment: while searching across all repos has required being logged in for a long time, when we enhanced the search capabilities earlier in the 2023 we had to extend this to repos as well corneliusroemer Please explain the \"false positives\" when allowing non-logged-in queries. Blocking some non-logged-in queries as false positives is still better than not allowing any non-logged-in queries... 1 edited markcellus FWIW, @jorendorff 's 2nd option] (below) can be done both with and without login requirement and without any of the other options. [...] We can collect metrics, find patterns of abusive bot behavior, and flag accounts that exhibit those patterns. The more time we want to dedicate to this work, the more unwanted traffic we can eliminate. That's the most ideal case imo, instead of requiring a login. You have all the details in the request (IP, agent, etc). Sure this option may require more work from a security perspective, but Microsoft is one of the biggest tech companies in the world! So I couldn't imagine it putting a dent in Microsoft's profits or team workflows by having it be solely the only remedy. TBH I can't buy the idea that Microsoft is somehow protecting its users from bots. I mean, bots--big small, bad and good-- have been on the internet for ages with thousands of websites still not requiring a login. It's also naive to think that all of these bots are the same and should be subjected to this same treatment. For instance, RSS readers shouldn't require a login for their users who prefer to consume in a RSS reader. There are also tons of security, privacy and/or legal protections a user has when not forced to log in. EDIT: corrected the @ mention and added link to original comment 2 nukeop No need to stop dedicated bot operators, if it stops 99% of bot traffic by stopping casual bot operators. wiblaa Microsoft is closing the door on open source. Who woulda thought that could happen. Oh right. 15 1 reply nukeop Microsoft isn't running the show here. 1 crvdgc It's a constant source of nuisance when browsing in private/incognito mode. Surely there are other ways to limit bots, like adding a rate limit and if that's reached, then requiring CAPTCHA or login? 7 0 replies pr3y You can look at https://grep.app/, which allows you to search for repositories without logging in, but even so, this Github policy is bad and will probably get worse over time 7 0 replies rlove I doubt this was a move against open-source. As an Enterprise Customer we needed better search which required this level of security. Granted it could have been made optional but given that AI models of our code are being used for major new Co-Pilot features security had to be heavily considered. 1 9 5 replies rooiratel Nobody is complaining that they can't search your private repos without a login. This is about opensource code that is publicly available, which people can still clone each repo and do a local text search without a login. This \"solution\" is just Microsoft being either assholes or incompetent, their only two skills. nukeop Or maybe they don't want to spend an order of magnitude more on infrastructure just to serve chinese botnets. rooiratel So the Chinese botnets are going to clone the repos and do a local search instead. That will really save bandwidth instead of just returning the result of a search query. nukeop Exactly, so that they're not wasting compute and are doing it on their own and it's no longer a denial of service vector. Now you're getting it. mbomb007 Plus the cloned data would become stale pretty quickly orliesaurus Since this is relevant to the larger topic, I would like to add an anecdote, as a logged in user using search extensively: I have been trying to look up results for a specific file - i.e. contributing.md across all open repos I have been using this search params: path:contributing.md but I only get 5 pages of results. I wonder why that is and if there's a way to retrieve more than 5 pages? 1 0 replies edited demostanis https://codeberg.org 5 2 0 replies Zezesheng GitHub 的反抓爬理由其实能说得过去，不过这也可能是 GitHub 和微软用来防止某些竞争对手抓取数据来训练 AI 模型，毕竟现在各大公司都在想方设法的限制数据抓取避免数据被人白嫖。 但是，这并不能成为不开源的理由，可以使用对真实用户更友好的办法不是吗？ 2 1 0 replies Zezesheng The anti crawling reasons of GitHub can actually be justified, but it may also be that GitHub and Microsoft use it to prevent certain competitors from crawling data to train AI models, after all, major companies are now trying to limit data crawling to avoid data being wasted. However, this cannot be a reason for not being open-source, can we use methods that are more user-friendly for real users? 1 0 replies Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Category Code Search and Navigation Labels Code Search and Navigation Product Feedback 28 participants and others",
    "commentLink": "https://news.ycombinator.com/item?id=38432261",
    "commentBody": "GitHub: Can no longer search code without being logged inHacker NewspastloginGitHub: Can no longer search code without being logged in (github.com/orgs) 335 points by 6581 20 hours ago| hidepastfavorite220 comments simonw 19 hours agoI&#x27;m ready to believe the most charitable explanation for this: the new code search (which I have found to be exceptionally good) has a lot more going on under the hood than a normal search engine, which makes it a lot more resource-intensive - limiting it to signed-in accounts saves a huge amount of server resources that would otherwise be used to serve crawlers.My guess is that the trade-off here is genuinely a question of if they spend 3-4x (I&#x27;m guessing, but I think this is likely a low-ball estimate) on their search infrastructure, v.s. having people angry at them for requiring login to use the feature.I actually built my own code search tool for use with GitHub repos, but I&#x27;ve mostly stopped using that because the new GitHub code search is so useful by default: https:&#x2F;&#x2F;simonwillison.net&#x2F;2020&#x2F;Nov&#x2F;28&#x2F;datasette-ripgrep&#x2F; reply ajsnigrutin 18 hours agoparentThe most realistic one is, that people will get fed up and register an account, and github can brag about how many new users they brought in.The other side is, that existing users will get pissed at github, because they can&#x27;t even search anymore without logging in, and sometimes that&#x27;s a pain (not their pc, public pc, incognito tab, the time needed to do the 2fa, etc.).Github can still keep the cheap, fast basic search for users not logged in, but they didn&#x27;t. reply simonw 18 hours agorootparentKeeping the old search running for logged out users has substantial costs too:1: You have to maintain two full independent search and indexing implementations2: User confusion. Why are the search results different depending on if I&#x27;m logged in or logged out? reply spoiler 16 hours agorootparentI agree with you, but I also feel like you&#x27;re throwing pearls at swine. Some people will always have an incredible amount of entitlement to justify their laziness or incompetence.I mean, the person opening the issue escalated this into socioeconomic issues just because they don&#x27;t know how to use grep... What else is there to say? reply squigz 7 hours agorootparent> I mean, the person opening the issue escalated this into socioeconomic issues just because they don&#x27;t know how to use grep... What else is there to say?I suppose one could try to see the value in the argument and engage in good faith discussion but it&#x27;s easier to flippantly dismiss people like this, I imagine. reply spoiler 58 minutes agorootparentSometimes you have to be flippant when engaging with trolls.There wasn&#x27;t enough thought behind the issue, other than being outraged at their unsatisfied entitlement reply cmyr 16 hours agorootparentprevIn reality, how many people are regularly using github search without having a github account? Can this change really be expected to bring in a meaningful number of users?Trying to think this through, my own best guess for what is going on is that there is some amount of traffic coming from bots&#x2F;scripts and github would like to have all queries associated with an account so that they can block accounts? I&#x27;m not sure if this makes sense, but I think it&#x27;s a reasonable possibility. reply fyokdrigd 16 hours agorootparentmost open source projects migrated to gh when they pledged it would always be free and open for open source...now under new ownership they closed search and \"pray they don&#x27;t alter the terms further\"ps: also everyone here guessing wrong. Microsoft is an advertising company. requiring you profile for a service is self explanatory. reply jabradoodle 12 hours agorootparentIs it really an advertising company?Seems to be around 5pc revenue from a quick search.Not that I want to defend Microsoft. reply qabqabaca 17 hours agorootparentprev> people will get fed up and register an account, and github can brag about how many new users they brought inThis makes no sense. The number of people who use GitHub code search but don’t already have a GitHub account is surely negligible reply fyokdrigd 16 hours agorootparentit&#x27;s about having you logged in for the day.logged in user ad impression is 8 to 12 usd. anonymous user is .01cent per thousands. reply jowea 15 hours agorootparentThere are ads on GitHub? reply qabqabaca 16 hours agorootparentprevI don’t see how that relates to my comment reply jowea 15 hours agorootparentI believe the previous poster means that there are people who have an account but aren&#x27;t logged in all the time. reply TheCleric 9 hours agorootparentI honestly don’t understand why. On my computer I log in once and that’s it. They don’t automatically log you out. I haven’t logged into GitHub since setting up my computer. replydormento 18 hours agorootparentprevThis irks me to no end.They justify it with \"engagement\" and it kinda works to a point, since if you don&#x27;t get pissed with the broken search and register an account, you&#x27;re not considered a \"user\", and if you try to use the tool but don&#x27;t want to register you&#x27;re not \"engaging\" with it, I guess... But it makes the initial contact that much more miserable. reply rudolph9 18 hours agorootparentprevI’m not saying you’re wrong, if anything it makes their decision even more ridiculous, but GitHub has completely saturated the market. Other than pissing people off what do they really benefit from this? reply usea 18 hours agorootparent> GitHub has completely saturated the marketThat&#x27;s an assumption which github may not agree with. reply zorgmonkey 14 hours agorootparentprevIf you really care about being able to search github without logging in https:&#x2F;&#x2F;grep.app&#x2F; is pretty good, I would often use it instead of the old github search cause I found the results to be better. reply txdv 18 hours agorootparentprevo wow, the KPI of user acquisition should be through the roof with login required reply isodev 18 hours agoparentprevThat&#x27;s interesting but the technical reasons are really besides the point.The end result is that it&#x27;s no longer possible to participate in \"GitHub hosted open source\" without actually being a member of GitHub. reply cheald 18 hours agorootparentYou can still clone and grep, can&#x27;t you? You can&#x27;t file issues or submit PRs through Github without an account, either. reply isodev 18 hours agorootparentYou&#x27;re kind of making my point, one needs to take their code elsewhere to be able to browse it.In other words, GitHub is no longer suitable to host open source repositories because of all the closed \"extras\" it adds on top. reply uxp8u61q 18 hours agorootparentHow does providing more features to some people make the core service unusable for \"open source\"? reply anthk 18 hours agorootparentThis is adding less features for libre software users. I hope everyone migrates to local SourceHut servers when possible. reply __MatrixMan__ 18 hours agorootparentprevAre you suggesting that there&#x27;s a place that you can file a PR (or equivalent) without an account?They&#x27;re shady AF, but some features require an account not because they&#x27;re moat water but for some other reason. reply isodev 18 hours agorootparentI&#x27;m really focusing on the search feature that&#x27;s no longer available for a given repository unless you&#x27;re logged in. Everything else is \"Microsoft added extras to git\". If all I can do on GitHub is use it to git clone, then what&#x27;s the purpose of it?It&#x27;s clear the \"extinguish\" phase of Microsoft&#x27;s open source venture has started. There&#x27;re plenty of very good alternatives, it&#x27;s time to move on. reply dharmab 17 hours agorootparentprevThere is actually such a place! The Linux kernel (the original use case for Git) is developed by emailing patches to the appropriate mailing list. The mailing list is compatible with self-hosted mail systems. reply MikusR 17 hours agorootparentBut you still need an account. email one in this case. reply dharmab 16 hours agorootparentYou don&#x27;t need an account to send email. You can send emails using nothing but a CLI tool on a *nix machine. Although most mainstream email services will send those to the spam folder these days. reply throwawaymobule 12 hours agorootparentI thought damn near every ISP blocks port 25 outgoing. I even had to open a ticket to get it unblocked on a vps I use.probably for the best. reply g-b-r 16 hours agorootparentprevYou need an e-mail for GitHub as well, but in addition a GitHub account reply rightbyte 16 hours agorootparentprevYou don&#x27;t need an account to read the list.http:&#x2F;&#x2F;vger.kernel.org&#x2F;vger-lists.htmlUnless you count your ISP of course. reply arp242 13 hours agorootparentYou don&#x27;t need an account to browse GitHub PRs or issues either. replyyongjik 15 hours agorootparentprevDo you mean \"git clone\"? I thought the idea about git is that you always take the code into your local machine before doing anything. Sure, github gives some bells and whistles to obviate the need from time to time, but that&#x27;s basically decorations. reply arp242 13 hours agorootparentprevAnd can you search a git repo on git:&#x2F;&#x2F;my-domain.example.com? Or can you browse that without cloning? And even cgit, can you search that?Are these now also no longer \"open source\"?No.None of this is somehow a core requirement of \"open source\". reply mattpavelle 18 hours agorootparentprevAgreed. If this is indeed a resource issue and they want to just block crawlers&#x2F;bots&#x2F;anonymous users from using lots of resources then perhaps having the old \"less resource intense search\" for users who aren&#x27;t logged in and having the new \"more resource intense search\" for users who are logged in would be an improvement. reply rattray 4 hours agorootparentIdeally yes but the old code search may also require costly infra, like building specialized indexes of the codebase (eg an elasticsearch index).Since those are \"fixed costs\" per repo rather than per search, they&#x27;d now be much more expensive per search if they were only used by logged-out users.Pure outside speculation of course. reply acedTrex 17 hours agorootparentprevIts never been possible to participate in github hosted open source without an account... reply saagarjha 16 hours agorootparentI have a GitHub account. Until recently most of my GitHub participation was done without being logged in. reply jahsome 16 hours agorootparentWhat do you do that doesn&#x27;t require authentication? reply saagarjha 16 hours agorootparentRead code and issues mostly. reply jahsome 11 hours agorootparentIt&#x27;s rather pedantic, but I wouldn&#x27;t call that participating. That&#x27;s more in the realm of observation.An account has always been required for active participation, e.g. committing directly to any repo, submitting a PR, opening an issue, etc. reply saagarjha 1 hour agorootparentYes, and I use an account for those. But I value not having to use an account for any work before that, which is just a crucial. reply acedTrex 7 hours agorootparentprevYou can still do that without being logged in, thats also not participating in open source. More observing open source then anything reply saagarjha 1 hour agorootparentThese are prerequisites to what I suspect you have in mind when you consider open source participation. Of which I do a decent amount, logged in of course, but this comes first. replyJustsignedup 18 hours agorootparentprevright, but being a member isn&#x27;t a pay-wall. This feels more like an academic argument than a practical one. reply isodev 18 hours agorootparentBecoming a member means sharing your personal information with GitHub and abiding by their terms of service. It also implies that they will then track and profile locations where you sign in, technologies you use to interact with their services etc. That&#x27;s a lot to give for \"I want to browse a piece of code someone made for free\". reply johnfn 18 hours agorootparentThe only bit of \"personal information\" you need to provide is an email, and you&#x27;re free to use a burner.> That&#x27;s a lot to give for \"I want to browse a piece of code someone made for free\".You can still do this. All repositories are accessible without signing in. reply underdeserver 18 hours agorootparentprevAnd it&#x27;s what they get in return for hosting and providing indexing and a git server.I&#x27;m not saying it&#x27;s a deal that makes sense for everyone. But it&#x27;s certainly a barter trade and is well within their prerogative to offer. reply isodev 18 hours agorootparentprevFacebook recently estimated that knowing about a user costs close to €11&#x2F;month. That&#x27;s a lot of money for hosting and indexing git repositories... I guess we need an alternative. reply arp242 12 hours agorootparentprevAnd it&#x27;s EXACTLY the same on Codeberg, and GitLab, and SourceHut, and anything else online. Some of these don&#x27;t even have a \"search\" feature.This is just degrading in to conspiratorial FUD. reply goodpoint 15 hours agorootparentprevNot at all. FOSS should be built on FOSS tools. reply usea 18 hours agorootparentprevGiving them your personal information is a form of payment. I don&#x27;t mean this in a pedantic or academic sense, but a very real one. reply Dalewyn 18 hours agorootparentprevThe kinds of people averse to user accounts wouldn&#x27;t be using (\"participating in\") Github in the first place.Or to put it another way: It&#x27;s a surprise you need a Github account to use (\"participate in\") Github? reply benjaminwootton 18 hours agoparentprevI see lots of reasons being proposed, but my guess is that they want or need to avoid content being scraped by AI related companies and researchers. reply prepend 18 hours agorootparentAs someone who contributes a decent amount of OSS code on GitHub, I want my code scraped by AI related companies. If GitHub starts making opinionated decisions about how to “protect” my code, then I won’t like that. reply Tommstein 12 hours agorootparent> If GitHub starts making opinionated decisions about how to “protect” my code . . .They already did.> . . . then I won’t like that.There is no universe in the multiverse where Microsoft or one of its subsidiaries gives a single flying fuck. Something about those who forget history being doomed to repeat it . . . . reply whoopdedo 16 hours agorootparentprevA scraper won&#x27;t be using search. Only crawling links. If they required login to browse a repository that would stop bots but also cause a 1000x greater riot. reply executesorder66 15 hours agorootparentprevIt&#x27;s not their code to \"protect\". The license of each project dictates what can and can&#x27;t be done with it.And also, what is stopping the scrapers from creating a bunch of accounts to scrape with, and rotating them once they get banned? reply Mistletoe 18 hours agorootparentprevI just assume this to be the case for all future decisions like this. Surely the Reddit API changes were at least partly from this? Reddit is one of the richest sources of (pretty) authentic human interaction that humanity has ever made. Every AI company and startup was going to mercilessly mine this forever. I&#x27;m sure Reddit would much rather sell (our) interactions instead. reply jowea 15 hours agorootparentDoes that mean LLMs are going to speak like redditors? Oh the humanity. reply Mistletoe 9 hours agorootparentThey already do. :)https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;10j531u&#x2F;i_made_cha... reply executesorder66 16 hours agoparentprevIf they are doing this intentionally, then fuck Microsoft. Bunch of assholes as always. If this is mere incompetence, then again I&#x27;m not surprised that they fumbled such a simple task. Grep is literally 50 years old. They don&#x27;t even have to write any code to do text search, they could use free software if they love it so much. This isn&#x27;t some amateur startup with a CRUD app and 3 devs. This is one of the largest and wealthiest tech companies in the world, and they can&#x27;t even get text search to work without a login? What a bunch of losers.If there is a 3rd explanation, I&#x27;d love to hear it. reply ivank 11 hours agorootparenthttps:&#x2F;&#x2F;github.blog&#x2F;2023-02-06-the-technology-behind-githubs... reply executesorder66 11 hours agorootparentWhere in that post does it say that the user not being logged in is making it difficult&#x2F;impossible for them to return search results? reply Tommstein 12 hours agorootparentprev> If they are doing this intentionally, then fuck Microsoft. Bunch of assholes as always.They explicitly said that they \"had\" to require being logged in, so, yeah: https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;community&#x2F;discussions&#x2F;77046#discussi... . Although they do assure us, however, that they&#x27;re totally sorry for the inconvenience. reply executesorder66 12 hours agorootparentYou are correct, but I was just covering all the possibilities since I never trust what they say either. reply iepathos 18 hours agoparentprevAs far as their code search under the hood it&#x27;s just Elasticsearch, nothing special. Github&#x27;s explanation sounds like mostly bullshit. Forced authentication of public endpoints is not an appropriate solution for the issue they&#x27;re claiming. People building bots who actually care about this endpoint can just have their bots login with free accounts. This doesn&#x27;t actually prevent load on their servers in a meaningful way. reply WorldMaker 14 hours agorootparentGitHub&#x27;s code search is no longer using Elasticsearch, it is using an entirely in house (Rust-based) search engine nicknamed Blackbird: https:&#x2F;&#x2F;github.blog&#x2F;2023-02-06-the-technology-behind-githubs... reply slaymaker1907 17 hours agorootparentprevWhen they’re logged in, they are much easier to throttle or even ban compared to IP hopping to dodge IP throttling&#x2F;bans. reply 867-5309 17 hours agorootparentprev>bots login with free accountsthose accounts still need to be created, which is hard to automatealso, now GH can just ban more easily detectable accounts that scrape, vs anonymous visitors reply 1980phipsi 18 hours agoparentprevIf the new code search is more resource intensive, then why not have a slimmed down, less resource intensive version for when users aren&#x27;t logged in? reply supriyo-biswas 18 hours agorootparentThis may involve running two different sets of (for example, ElasticSearch) clusters, which doesn’t justify itself from a business perspective, especially given that non-logged in users can still clone code and use local tooling for search. reply ldoughty 18 hours agoparentprevMight also help with thwarting easy access to tokens&#x2F;keys... especially for those that are not currently in their filtered list.While a bad actor can certainly scrape&#x2F;clone the same data... Given repos with thousands of files... Scanning by scraping vs searching vs cloning.... Search is certainly the cheapest for the attacker while the most expensive for GitHub reply Thorrez 18 hours agorootparentThat could be mostly solved by requiring login only for searches across all repos. Searches in a single repo could be anonymous.In fact, that&#x27;s exactly how it was until in June GitHub started requiring login for searches even in just a single repo. reply SnowflakeOnIce 18 hours agoparentprevAside from performance concerns, there have been many occurrences of bad actors using code search to find hardcoded credentials, and then using that to gain unintended access to additional systems.I suspect the change has more to do with security concerns (and having an audit trail) than performance. reply prepend 18 hours agorootparentI’d like to see these many occurrences.I think the solution to hard coded credentials isn’t making search harder (security through obscurity == bad) but the other things GitHub is doing to detect and mitigate hard coded credentials.Search is still possible using google and other methods, so any theoretical gains from forcing login are dangerous to count on as vulnerable projects are still vulnerable.If anything, I think the solution to projects with hard coded credentials are to make the credentials easier to find and exploit so they fixed more quickly after being created. The most dangerous are ones they are hard to find so someone uses them for long periods of time without detection. reply SnowflakeOnIce 4 hours agorootparent> I’d like to see these many occurrences.Some high-profile cases where credentials were leaked on public GitHub: Uber in 2014 and 2021 [1, 2] and Twitch in 2021.> Search is still possible using google and other methodsYes, you can search with Google or other sources. But the thing is, pretty much only GitHub has easy access to all the code present there, readily available to search within minutes of pushing.You could try mirroring GitHub yourself, but you&#x27;d need enormous disk space and bandwidth, and would quickly hit rate limits. You also wouldn&#x27;t be able to do fast full regex search like you can with GitHub&#x27;s search, as you don&#x27;t have their search infrastructure.Hackers are aware of this and do make use of GitHub&#x27;s search to identify possible leaked credentials. I recently experimented with uploading a dummy AWS credential pair to a public Git repo, and saw that numerous IPs started trying those credentials less than 5 minutes after I pushed.> any theoretical gains from forcing login are dangerous to count on as vulnerable projects are still vulnerableIndeed, requiring login is not going to _solve_ the problem. But it could lessen the impact of leaked credentials at large scale, by making it more difficult for automated systems to harvest them.Perhaps more significantly, requiring login could give better audit trails in incidence response situations, as the logs would indicate which accounts were searching for secrets.> I think the solution to projects with hard coded credentials are to make the credentials easier to find and exploit so they fixed more quickly after being createdYes, this can help! There are several other companies that specialize in secret detection. I&#x27;ve also written Nosey Parker, a fast regex-based detection tool that has higher-precision rules than similar tools [4]. GitHub also has its own offering in Advanced Security to address this problem.[1] https:&#x2F;&#x2F;www.reuters.com&#x2F;article&#x2F;uk-uber-tech-lyft-hacking-ex... [2] https:&#x2F;&#x2F;www.securonix.com&#x2F;blog&#x2F;securonix-threat-research-ube... [3] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28770590 [4] https:&#x2F;&#x2F;github.com&#x2F;praetorian-inc&#x2F;noseyparker reply nebalee 16 hours agorootparentprevBad actors are hardly deterred by the requirement to be logged in to be able to search. reply SnowflakeOnIce 5 hours agorootparentYes, you are correct. But requiring that one be logged in to use the search functionality makes a stronger audit trail possible: looking at the search logs would indicate who has been hunting for secrets! reply babyshake 18 hours agoparentprevIt&#x27;s likely to avoid crawling, especially to train competitors to GitHub Copilot or other LLMs. reply supriyo-biswas 18 hours agorootparentWhy can’t you train on the git repository itself? Clones are not behind a login. reply rafark 16 hours agorootparentThe way AIs are evolving, I wouldn’t be surprised if they have the ability to go to GitHub and search on demand based on a users query. reply pixl97 17 hours agorootparentprevHow do you discover the repository, especially in an automated fashion? reply rudolph9 18 hours agorootparentprevThis seems like the most probable reason of any I’ve read so far. reply __MatrixMan__ 18 hours agoparentprevYou&#x27;d think, with all that work going on under the hood, that they&#x27;d be able to support searches for repos by multiple paths:> Give me repos with project.toml and a flake.nix at the rootThe interface supports it, but searches come back empty. I end up leaving scripts running while I sleep which walk search results and narrow them by just checking to see if the files are there.If you&#x27;re out there Microsoft, please fix your search so I can stop being a bad citizen. reply citruscomputing 9 hours agorootparentThe new search is still so useless that I just maintain a full local clone of our org&#x27;s code. Someone was requesting perms to run a script to search across the org today -- it would&#x27;ve taken 10+ minutes to run while respecting undocumented secondary rate limits (which there&#x27;s no way to check via headers, by the way). I rewrote it to use ripgrep locally, and it&#x27;s down to 20 seconds. reply Fatnino 11 hours agoparentprevThey were already rate limiting unauthenticated users.So are expected to believe that a hostile user who has zillion of ip addresses (to get around the rate limits) won&#x27;t also be able to make a handful of accounts? reply duped 17 hours agoparentprevIf that&#x27;s the case, they should really fall back on a grep of the codebase because whatever is \"under the hood\" sucks. It routinely returns no results for exact string matches in code. These days I clone the repo and use ripgrep instead of codesearch because it&#x27;s worse than useless, it&#x27;s wrong. reply renegat0x0 18 hours agoparentprevWe have been running Internet without requiring user accounts. Search was possible. Has something changed? Has github been having real problems with bots? No? reply codedokode 18 hours agoparentprevProbably code search was misused by bots to search secrets. reply rudolph9 18 hours agorootparentYeah but that’s been the case since search existed. It seems unlikely that somehow now that’s become an urgent problem or requiring login will fix anything. reply zxt_tzx 18 hours agorootparentprevis there any reason why can&#x27;t GitHub put their code search behind something like Cloudflare Turnstile? if HaveIBeenPwned can do this, why not GitHub? https:&#x2F;&#x2F;www.troyhunt.com&#x2F;fighting-api-bots-with-cloudflares-... reply K0IN 17 hours agoparentprevwhy cant they just use the old approach for anyone and the good stuff for loggedin users tho. reply Retr0id 18 hours agoparentprevI suppose maintaining two systems adds overhead too, but they could&#x27;ve let you use \"basic search\" without login, with the improvements login-gated. reply superkuh 16 hours agoparentprevIf that&#x27;s true then they should still provide the old non-heavy search for non-logged in users. reply Thorrez 18 hours agoparentprevCheck out the Chromium code search. It has regex support, and doesn&#x27;t require login. Although it is smaller than all of GitHub.Disclosure: I work at Google.https:&#x2F;&#x2F;source.chromium.org&#x2F;chromium reply mtmail 19 hours agoprevStarted at least 6 months ago and was announced in their changelog https:&#x2F;&#x2F;github.blog&#x2F;changelog&#x2F;2023-06-07-code-search-now-req... (HN discussion https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36230929) reply echelon_musk 18 hours agoparentSurprised to see this as news as it feels like it&#x27;s been years at this point. I can scarcely remember when I was last able to search without being logged in.I understand perfectly why this has been done from a business standpoint as it invariably increases &#x27;user engagement&#x27; by driving people to become literal users! reply rldjbpin 54 minutes agorootparentthis. that has been my personal experience as i browse github without being logged in for the most part.even searching a specific file name inside the repo has been locked behind the same way. reply jehb 18 hours agoprevCan we please stop treating GitHub like it&#x27;s an open platform? It&#x27;s not. It&#x27;s a closed, walled garden like every other one. The fact that they host a bunch of open source projects you use doesn&#x27;t make them better; if anything, it makes them worse, because they&#x27;ve helped these projects wall off a part of their contribution infrastructure behind the lock and key of a corporate account. reply ShakataGaNai 17 hours agoparentIf I have to sign up for a \"corporate account\" or I have to sign up for a random self-hosted gitlab account or I have to sign up for your bugzilla & wiki & gitwhatever.... it makes no difference. As a user, it&#x27;s all the same. Actually... of all of these options, I prefer a \"corporate account\" (aka github) because I can participate in a nearly infinite amount of projects without having to create new accounts&#x2F;logging in&#x2F;etc.No one has walled anything off in any way that is materially different than any other option in the space.I would argue that Github has done a LOT of good in the space. Making good software, making it freely available. Keeping it reasonably open and accessible. Keeping it standards compliant where there are standards. Having API&#x27;s for the rest. And in general, giving a huge amount of storage and compute away for free for open source projects. reply myaccountonhn 10 hours agorootparentThe beauty with git is that it doesn’t need an account. It can all be done via email. Hosted repos that use that are much more open and allow you to use the tools you want to contribute.A lot of these walled garden platforms have contributed a ton and github has made source code more easy to host, no doubt. At the same time, we need to ask them to do better and not allow them to concentrate power for when the inevitable enshittification begins. reply bbkane 18 hours agoparentprevI think it also needs to be said that GitHub has helped countless open source projects grow- Git Hosting, wiki hosting, issues, GitHub Actions, GitHub Pages, a nice API...There are LOTS of reasons to host your open source project on GitHub reply pjmlp 17 hours agorootparentSourceforge used to have similar reasons. reply ApolloFortyNine 17 hours agorootparentAnd if github truly goes to shit like source forge did people will just move to a clone.But if you look at any source forge repo right now, I think you&#x27;ll see github as a long way to fall before it gets that bad. reply pjmlp 17 hours agorootparentThe tragedy here is that folks don&#x27;t learn. reply arp242 12 hours agorootparentLearn what, exactly? Anything can go sour, including community-run open source stuff. There is never any guarantee for the future about anything. reply anthk 18 hours agorootparentprevThe are more reasons to run away from GH and just set a locally managed SourceHut instance for sanity and interop. reply mmahemoff 18 hours agoprevThe web has really closed in 2023. StackOverflow, Reddit, Github, Twitter all put the brakes on scraping and API access. The trigger was preventing AI training combined with a push to increase profitabilty due to commercial realities (tech recession, new ownership at Stack and X, Reddit wants to IPO).I believe a long term effect will be a rise in the marketplace for proprietary data. Search engines, AI tools, anyone who needs the data will have to pay for the firehose or API access. (That, in turn, may cause antitrust issues if only the richest companies can afford access to that data.) reply pixl97 17 hours agoparentIt&#x27;s been going on longer than this, but this year has had a lot further locking down...https:&#x2F;&#x2F;theoatmeal.com&#x2F;comics&#x2F;reaching_people reply Hizonner 16 hours agoparentprev... and everybody will just think it&#x27;s normal for \"StackOverflow, Reddit, Github, Twitter\" to sell access to content that other people created and own...If something is truly proprietary, then the compensation should go to its owner, not to some random Git server operator or whatever. And the owner, not the server operator, should be setting the price and terms. If the server needs to make money, it can charge for the basic service itself.On the other hand, if somebody created something to give it away, on a platform that at that time provided a channel effective for giving things away and advertised itself as suitable for giving things away, then the platform changing the rules midstream is morally piracy of that person&#x27;s work.Probably a very large fraction of the material on those platforms would never have been put there in the first place if the actual owners had expected random restrictions and arbitrary access charges.If they want to radically rewrite the rules like that, then they need to not sell access to any preexisting content unless the original creator explicitly opts in. But we had Reddit, for instance, actively reinstating posts that had been mass-deleted by their authors specifically to prevent that kind of abuse. reply xtracto 14 hours agoparentprevYes and no. ExpertsExchange was notable for having \"walled\" responses and was similar to StackOverflow. In fact, I remember when StackOverflow was launched, it was compared to expertsexchange but \"open\".It is time to move from centralized services to something more distributed and \"micro transaction based\". I know a lot of people in HN will dislike this butn All the ones you mentioned (Q&A forum, News link + commenting aggregator, Git Hosting + Approval Flow + Wiki + Ticketing system ) can and should be implemented in a completely distributed manner: (using Kademilla&#x2F;Bittorrent technologies, IPFS and CryptoTokens for paying micro transactions).Going 100% distributed is the only way these sort of things are going to stay \"open\" and free of corporate greed (we have seen time and time again, original founders may start the project with good intentions, but in the long term, the product gets sold and bean counters take the lead). reply fotcorn 19 hours agoprevA charitable interpretation might be that search requires a fair amount of compute, and is therefore a big denial of service vector.I am not sure how much behavioral data GitHub can gather from logged in user, and how useful that is compared to the code that is there anyway. Maybe to figure out which parts of code are important? But that isn&#x27;t really user-specific. reply marginalia_nu 19 hours agoparentYes, it&#x27;s a real problem for anyone offering any sort of search capabilities. Like, about 0.5% of the traffic to my search engine is human. I&#x27;m not aware of any search engine that doesn&#x27;t have similar stats. reply hskalin 18 hours agorootparentOff topic: how do you determine what percentage of search is coming from humans? reply marginalia_nu 17 hours agorootparentWell about 99% of the search requests I got back when I was using cloudflare couldn&#x27;t get past their bot-mitigation, and of what made it through, at least half looked very automated. reply superkuh 16 hours agorootparentI&#x27;m a human and I can&#x27;t get past cloudflare \"bot mitigation\" with my browser. Bot mitigation actually just means your browser executing the latest bleeding edge javascript functions to make sure your behavior is monetizable. reply marginalia_nu 16 hours agorootparentNo that&#x27;s not actually true at all. The website always worked with text-only browsers, cloudflare or not. Thoroughly tested with the likes of w3m and dillo.Virtually all of the traffic that was intercepted claimed to be modern Chrome or Safari or similar, which should be capable of \"executing the latest bleeding edge javascript functions\".The primary reason why anyone gets shit from bot mitigation is IP reputation, this is far more important (and effective) than looking at browser characteristics. reply crtasm 18 hours agorootparentprevGithub could require captcha for non-logged in users, I suppose. reply dylan604 18 hours agorootparentI&#x27;m a human, yet I am unable to get past Steam&#x27;s captcha. It is not the only site that I cannot prove to not be a robot. I&#x27;m guessing the number of collateral damage is worth it to them. I&#x27;m not a big gamer, and wouldn&#x27;t be a big source of revenue for them anyway. reply Kuinox 17 hours agorootparentSteam has a captcha ? reply mikro2nd 18 hours agoparentprevThis (behavioural data) is precisely Microsoft&#x27;s playbook - no charitable interpretations ought apply. As far as I am concerned, no Open Source project has any justification for still being on the platform as of the day of the MS buyout. It&#x27;s not as though there aren&#x27;t good alternatives just a git clone away. reply yladiz 17 hours agorootparent> This (behavioural data) is precisely Microsoft&#x27;s playbookWhat behavioral data can you glean from a code search like Github&#x27;s? The context is very different than, for example, Google&#x27;s, so is there really much useful data you can get here? reply mikro2nd 3 hours agorootparentFrom a code search in the wild, with no context? Not a lot. From a code search from a person who&#x27;s logged in, identified? Well, probably still not a lot, but it&#x27;s another factoid about that person to hang onto the knowledge graph. reply SnowflakeOnIce 18 hours agoparentprevAnother factor: anonymous faceted regex search across a huge volume of code allows bad actors to find hardcoded credentials and gain access to additional systems, without a good audit trail.But yes, there are multiple good explanations for why they would lock down the API. reply bilkow 18 hours agoprevI&#x27;ve been using sourcegraph when I&#x27;m not logged in, which has also the bonus of being a lot better (at least when compared to the old search).It&#x27;s as simple as appending the repo URL, starting from github.com: e.g. sourcegraph.com&#x2F;github.com&#x2F;rust-lang&#x2F;rust&#x2F; (you can try searching for unit on this repo) reply dontupvoteme 19 hours agoprevPart of microsoft&#x27;s AI moat is about controlling competitions ability to use information on github.I would not be surprised if they start restricting git clone next. reply beeboobaa 19 hours agoparent> I would not be surprised if they start restricting git clone next.I would, considering the amount of CI systems this would break. Say goodbye to large parts of NPM, go package management, Jenkins scripts, etc. reply kkielhofner 19 hours agorootparentDon’t forget Dockerfiles!That would be pure chaos. reply ljm 19 hours agoparentprevCould you still call code open source (in line with the repo’s license) if the code is gated behind a login, or even a paid subscription?IIRC GitHub is already rate limited to slow down excessive cloning or API usage. reply wongarsu 18 hours agorootparentI don&#x27;t see a real issue with forcing you to make a free account and log in. After all it&#x27;s also permissible to only offer source code in the form of shipping you a CD in return for postage fees. If people don&#x27;t like how the code is provided they are free to upload it somewhere else.A paid subscription could be crossing the line. Or maybe it would be fine as long as no profits go to the entity that provided you with the binary. Hard to tell. reply g-b-r 17 hours agorootparentIt requires accepting additional terms, seems pretty likely to be a breach of the obligation to provide the sources (if it&#x27;s the only way to obtain them). reply josephcsible 18 hours agorootparentprevYes. Open source doesn&#x27;t require that you distribute the software to anyone. It only requires the the people you do distribute it to (if any) all have access to the source code under an open source license. reply g-b-r 17 hours agorootparentIf the binaries are publicly available anywhere the sources need to be as well (or at least be provided upon request, but I don&#x27;t think many developers would like to deal with that) reply dontupvoteme 19 hours agorootparentprevAPI Usage yes as it requires a token, but git clone requires no auth and still seems to work behind datacenter IPs (e.g. mullvad) meaning that there is little stopping someone from mass cloning.It&#x27;s a perverse reality but it seems that in order to keep some ecosystems open one has to take actions which are resource-wasteful (though I would argue in the larger picture it will save resources) reply rcxdude 19 hours agorootparentprevYes. You can put a piece of code behind a paywall even. The only requirement under GPL and similar copyleft licenses is to make the code available to those you make the software available to (which can be only paying users, if you so wish), and to allow them to redistribute it under the same terms (which tends to mean that if you do make it pay-only, one of your users can just publish the code if they wish). Absolutely nothing in any commonly used open source license requires anyone to post something publicly and freely. reply anthk 18 hours agorootparentThe user can ask for the code without paying any fees. reply rcxdude 17 hours agorootparentYes (though the GPL does allow you to make the code available via post and charge a reasonable postage fee), if they&#x27;re a user in the first place. You don&#x27;t need to give the code to anyone you didn&#x27;t distribute the software to. reply anthk 14 hours agorootparentVvia postmail, but today with ubiquitous internet connections that way has no sense. reply VWWHFSfQ 19 hours agorootparentprevYes of course because neither of those things are relevant to the license. reply yladiz 18 hours agoprevEven if I agreed with the position of person who posted the question (which I don&#x27;t really, because search is not necessarily cheap and could be used for a DOS style attack), why do people feel the need to be like this? Like, what benefit is saying, \"Is it not enough to monetize every bowel movement, you now feel the need to track which individual lines of code I&#x27;m browsing?\" here? Chill out.The underlying argument isn&#x27;t even that good. Besides that repos themselves are _not_ gated behind a login and so an open repo is able to be accessed publicly, including cloning the repo, if the point is that the author wants his repos and code to be useful to the public, then if there is ever a need to interact beyond just downloading the code and searching, such as creating an issue or making a PR, that contributor would necessarily need to log in. reply g-b-r 17 hours agoparentYes, there is very often a need to quickly search in source code (to figure out how something works, to discuss about it in places other than GitHub issues...).Cloning is usually fast but not with very large repositories, and you might not even have enough space on your current device to clone them.By the way if a lot of people will end up cloning everytime instead of logging-in (after all logging in to GitHub is a pain with the 2fa) I don&#x27;t think their systems will have less load.And not everyone might want to have a GitHub account reply amadeuspagel 18 hours agoprevHackerNews: Can no longer view the frontpage without seeing a complaint about a free and ad-free service not being provided to users who are not logged in. reply mattstir 18 hours agoparentIf it&#x27;s free, then you are the product. That&#x27;s an adage that holds true in most cases, and in this case, your code is being used to train Copilot, among other things. Even if that weren&#x27;t the case though, and the service really was as benevolent as you seem to believe it is, would you be sitting there clapping as the service gets worse for a decent subset of users? Is that exciting and interesting to you? reply tentacleuno 18 hours agoparentprevDo you have any references for this claim? I browse HN pretty much everyday, and \"I can&#x27;t do X without logging in\" doesn&#x27;t seem to be a popular type of post. The only example I can think of is Reddit&#x27;s API changes, which were a while ago and loosely related to logging in. reply beyang 18 hours agoprevIf folks want to continue searching open source, https:&#x2F;&#x2F;sourcegraph.com&#x2F;search does not require sign in and also includes major projects that are not on GitHub.(Full disclosure: I&#x27;m the Sourcegraph CTO) reply potamic 13 hours agoparentWhy did you decide to allow search without a sign in but not for cody? reply charcircuit 16 hours agoparentprevThank you. I don&#x27;t want to sign into my personal github account on my work computer, so I always use sourcegraph if I need to search a repo.I remember longing for the ability for github&#x27;s search to rival git clone + grep, but I never expected a login wall to come with it. IIRC I expected the login wall to just be because the feature was in beta and would be removed when it became the primary search. reply daft_pink 19 hours agoprevI suspect they are trying to block bots like everyone running a website with useful data these days. reply ajsnigrutin 18 hours agoparentThe bots can still do a git clone and index everything, this just inconveniences normal users working on \"some other\" PC (or browser or incognito tab), where they are not logged in, and&#x2F;or don&#x27;t want to log in (coworkers PC, 2fa, whatever). reply arp242 12 hours agorootparentA lot of bot traffic is just mindless \"follow any link\" traffic, not specialized bots to do X. It really is hugely pointless and wasteful to have tons of these bots request tons of comparatively expensive search links. reply mmahemoff 18 hours agorootparentprevMaybe if the bot operators have the resources, but it&#x27;s far from trivial to keep an up-to-date mirror of every project on Github, especially if Github is actively putting up barriers to prevent it. Once login is required, it becomes much harder to bypass rate limits because the company can rate-limit signups from unknown domains, enforce 2FA, etc. reply ajsnigrutin 18 hours agorootparentThey can do all this (rate limits, etc.) for unregister accounts already, and most users would never notice that (since a human only does a few searches per time unit), but they decided to require a login anyway. reply Turing_Machine 18 hours agorootparentprev> The bots can still do a git clone and index everythingOf course they can, but then they&#x27;re gonna be chewing up their own disk space and bandwidth for anything after the initial hit. I think the real problem is that the bots hit the GitHub servers over, and over, and over again. reply Ayesh 18 hours agoparentprevI run a small website that tracks releases of a software, it probes releases every few hours, with each run consuming a several hundreds of API requests. GitHub API token limits are pretty generous. There is no paid offerings to increase the limits, so I suppose if you ask GitHub nicely, they will increase the limits given a reasonable justification. reply dylan604 18 hours agorootparent>it probes releases every few hoursI can&#x27;t think of any software I&#x27;ve ever used that I was this concerned about release schedule. Even if I was a user of your site, I might check it daily, but even that is doubtful. reply stilist 19 hours agoprevMy problem with code search is that when I’m searching I’ll randomly get dumped to github.com&#x2F;search with the search blanked, and anything I put in the search box at that URL is ignored. I have to go back in my history until I’m in the normal GitHub UI again. My search has also been discarded there, but at least I can interact with it. reply zackmorris 16 hours agoprevYou all talk a big game, but your actions are ineffectual. You don&#x27;t consider the leverage you have to \"encourage\" Microsoft to turn public GitHub search back on.Real leverage looks like all of us banding together to boycott Microsoft&#x27;s most profitable income streams. Not GitHub. But cloud services like Azure, and Office, gaming, etc:https:&#x2F;&#x2F;www.nasdaq.com&#x2F;articles&#x2F;these-2-revenue-streams-acco...It&#x27;s about making unilateral ensh*ttification decisions like this so expensive for parent companies that they become unthinkable. The board and investors sometimes need to be reminded how expensive these decisions really are.We&#x27;ve all seen stock prices tank by half in one day since the Dot Bomb. A weak signal starts a big wave in this age of algorithmic trading.A high-profile company publicly switching to AWS&#x2F;GCP to avoid eating MS&#x27;s sh@t sends a strong signal.More importantly, after everything that MS has put us through over the decades, divesting is FUN! :-) reply lloydtao 19 hours agoprevas an alternative, you may add `1s` after `github` in your address bar to open the repository with a browser-based VS Code, and then use Ctrl + Shift + F to search across all files reply benterix 18 hours agoparentA useful tip, thank you, but I think it will ask you to authenticate which is the OP&#x27;s main gripe.EDIT: No, I was wrong, you can search it without authentication. reply jve 19 hours agoparentprevWhy do it via 3rd party if you can just change github.com to github.dev and get 1st party VSCode? Or better yet, just press \".\" (dot) character on keyboard and VSCode will pop up.But that is repository search only, of course and not github-wide.https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;codespaces&#x2F;the-githubdev-web-base... reply lloydtao 18 hours agorootparentironically, if i&#x27;m not mistaken, github.dev requires being logged in reply Andoryuuta 18 hours agorootparentprevgithub.dev also requires being logged into github.com. reply saagarjha 16 hours agoprevI see a lot of people upset about the openness of the web or whatever, and I am too, but I don&#x27;t see many who point to specific harms that this causes. Of course, this leads others to believe that the complaints are nitpicky and lazy. Since I use this feature heavily, I feel like I might weigh in why this really sucks.First off, I have a GitHub account. But I&#x27;m often in contexts where signing into it is annoying, and intentionally so (it lets me push code!) I usually don&#x27;t want it on my work machine, even though I search GitHub a whole lot from there. Some of it is of course related to my job directly, which you might plausibly make the argument for that my employer should somehow compensate them for a free service that indirectly makes them money, but a lot of it is literally my work computer being a second machine that I work on, with open source code running on it, and usually these searches are to contribute value back to the community–either because I want to file a bug against it later, or maybe even deciding whether it is worth my while to contribute code to it. And I do this all the time from other devices, too: I might have logged into GitHub on my phone once, but my iPad? My mom&#x27;s computer? Being able to search GitHub is an excellent way for answer people&#x27;s tech questions at any time, much like if Google asked you to sign in everywhere it would be a massive pain.Second, and also quite important, is that I can&#x27;t use GitHub links as a way of pointing people at code anymore. I frequently (check my history!) will post a comment like \"yeah the Foo project uses bar API a bunch like this, [GitHub search link]\". It&#x27;s a very quick and very direct way of sharing this information. Of course I have no idea if the people on the other side are logged in or not, which means that if you put them behind a login wall I will slowly stop sharing these, because people will complain that they can&#x27;t see what I sent them. It&#x27;s the same way I hesitate to send people links to Twitter&#x2F;X these days, because whether the content will be accessible is a coin flip. And I&#x27;m definitely not going to ask people to clone the repo (on what, their phones?) to see what I&#x27;m seeing, so I might as well link to Sourcegraph instead. reply behindsight 17 hours agoprevBeen using https:&#x2F;&#x2F;grep.app for thatOriginal HN Discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22396824Sourcegraph works too: https:&#x2F;&#x2F;sourcegraph.com&#x2F;search reply adocomplete 18 hours agoprevThis is pretty disappointing on GH&#x27;s part. They seem to be moving further and further away from things that made the platform great.https:&#x2F;&#x2F;sourcegraph.com&#x2F;search has a really powerful code search can be a pretty powerful alternative and allows you to search codebases without logging in and across different code hosts. reply h1fra 18 hours agoprevIt&#x27;s fair to assume it&#x27;s very costly to run and has been abused by bad actors. I&#x27;m actually surprised it was open to begin with. reply spoiler 16 hours agoprevSearching code server-side (especially with the newly added intellisense-ish features) is probably expensive, yet they think they deserve such a feature for free, when code searching&#x2F;browsing isn&#x27;t GitHub&#x27;s primary goal.Why not either learn to use local code search tools and clone a repo, or just log in and search then? Are either of these things such a hurdle?I sympathize that the person opening the post is frustrated by the chain of PEBKAC events that unfolded, but that&#x27;s not really an issue with GitHub.The issue opener is behaving like an entitled toddler by making his problems \"societal\" problems. Honestly, the lack of self awareness with some people is astounding. reply kazinator 17 hours agoprevGithub has not allowed site-wide code search without a login for years.What seems to be new new is that you can&#x27;t search even a single repo.You can just clone it and use your local tools.If the code is hosted or mirrored elsewhere, and elsewhere uses GitWeb, you can use GitWeb&#x27;s grep search. reply anonacct37 13 hours agoprevI have a simple guess. We are in a post zirp (zero interest rate policy) world. I&#x27;ve seen very large companies embracing degrowth.Quite possibly Microsoft told it&#x27;s subsidiary GitHub that they needed to spend less money and this is how they save resources while impacting the users they care about the least. reply Dowwie 13 hours agoprevYou also can&#x27;t sort code-filtered search results by last-indexed anymore. The removal of this feature imposes a new tax on searching popular domains of work involving rapidly changing APIs and library preferences, such as deep learning. reply bArray 9 hours agoprevI have another one: You can&#x27;t logout of GitHub without enabling 2FA. You can&#x27;t log a ticket about it either. reply Jean-Papoulos 18 hours agoprevThis issue blows it way out of proportions. The code is still freely available, Github just doesn&#x27;t want to give away resources for free because it&#x27;s a for-profit company. If you&#x27;re mad at a for-profit company for not giving away things for free, you&#x27;re simply delusional. reply g-b-r 16 hours agoparent> This issue blows it way out of proportions. The code is still freely available, Github just doesn&#x27;t want to give away resources for free because it&#x27;s a for-profit company. If you&#x27;re mad at a for-profit company for not giving away things for free, you&#x27;re simply delusional.GitHub is still giving away resources for free, they&#x27;ve done it forever because it was their business model: attract as many people as possible through open source repositories, a fraction of which will then use their non-free services (and more recently, also to train AI models).Now that they&#x27;re so dominant (and owned by Microsoft) they can afford to worsen the services, but people have every right to be pissed about it and push developers to move their repositories. reply greybox 16 hours agoprevI imagine Microsoft view the code on their platform as an asset, as they can use it all to train AI that they can sell on. They don&#x27;t want anyone else doing the same thing with the code on their platform. reply renegade-otter 18 hours agoprevGitHub search is not exactly supported by ads. It was a much more, uh, baffling decision to have limited public access to Twitter, which is literally ad impressions. I guess I can&#x27;t comprehend that level of business genius. reply niam 18 hours agoparentHas there been an update on whether the \"they literally couldn&#x27;t afford their cloud hosting bill\" theory was bunk? reply 0xbadcafebee 18 hours agoprevControversial opinion: when a service provider makes antagonistic changes to its service, it&#x27;s actually a good thing, as it pushes people to find different solutions and not depend too heavily on one service provider reply myaccountonhn 10 hours agoparentSometimes yes… sometimes it progresses way too slowly. reply zelphirkalt 16 hours agoprevYes, this is standard now on Github. I noticed this weeks or even a few months ago. Now I often resort to cloning repos and running my own dumb grep. Thanks for being helpful, Github. reply wruza 16 hours agoprevAlso this ide-in-browser by default(?) broke the ui. I can’t even h-scroll code on mobile without wasting half of the phone’s charge. reply phailhaus 18 hours agoprevThis doesn&#x27;t seem to be much of a problem given that making an account is free. reply Cthulhu_ 18 hours agoparentYes, but does come with some checks and balances: an e-mail account (allowing them to filter out any dodgy domains or throwaway services, a captcha (bot detection of sorts anyway, may be hidden, would also be used for public search), a barrier to entry via their signup procedure, etc. And then they would have a unique token - your username &#x2F; ID - to help detect flooding &#x2F; scraping, set a limit of e.g. 100 searches per hour or whatever they have deemed normal and acceptable behaviour. reply lakpan 18 hours agoprevTo be fair, I’m surprised it was open in the first place. For a website that does not have any ads, having its enormous amount of data not only readily available but also searchable felt like a bad choice.What I hope never happens is the actual enshittification of the site for logged in users. reply g-b-r 16 hours agoparentWhy a bad choice?? reply mkl95 18 hours agoprevThis is annoying at worst. I search code without being logged in 3 or 4 times a year, and I&#x27;m very active on GitHub. reply g-b-r 17 hours agoparentIf you&#x27;re very active on GitHub you probably stay usually logged in any case reply tjoff 17 hours agoparentprev... try to look at it from a lens other than your own. reply mkl95 17 hours agorootparentCreating an account and logging in does not seem like a big ordeal. reply asabla 19 hours agoprevI wonder how much scraping for various purposes, but maybe more directed towards training LLM&#x27;s, has affected this change? reply g-b-r 16 hours agoparentTo train LLMs it seems asinine to try to scrape things instead of making a clone.The only automated searches I could envisage to MAYBE be happening are those looking for vulnerabilities, and there are probably only a few dozen actors doing it. State actors seem actually more likely to use clones of everything. reply from-nibly 7 hours agoprevWas going to say that the comments on the link is a dumpster fire, but it looks like HN is not much better. reply asylteltine 18 hours agoprevI really don’t mind at all and neither should you reply mattstir 18 hours agoparentWelp, everyone pack up. This one person has decided that there&#x27;s nothing to mind here. reply gsich 18 hours agoprevKPI driven development. reply Zambyte 19 hours agoprevThis is honestly very low on the list of things I dislike about Microsoft GitHub. git clone + rg still works fine anyways. reply fyokdrigd 16 hours agopreveveryone here will forget we only have encryption today because openbsd was in Canada.every usa based project had to quit shipping any cryptography code. but who cares about history. who cares you can only participate in some open source project today if you have an account with usa companies? nobody cares. screw the Iranian engineering student. and hope the usa doesn&#x27;t outlaw encryption again. reply api 16 hours agoprevSo it begins...Now that the entire programming world has just about hard coded GitHub into the very center of everything, it&#x27;s time to start enshittifying. reply vasvae3 19 hours agoprevThis has been happening since... forever?I guess code search requires a lot of processing power so they&#x27;d rather know who&#x27;s doing it in case they need to be throttled. reply nuc1e0n 19 hours agoparentNo. It was possible previously. The whole search feature on github seems to have been rewritten recently with several regressions in functionality. reply riv991 19 hours agorootparentThey wrote a pretty interesting blog about the rewrite.https:&#x2F;&#x2F;github.blog&#x2F;2023-02-06-the-technology-behind-githubs... reply nuc1e0n 19 hours agorootparentI wasn&#x27;t aware of the specifics. I was only going by the UX changes. Good to know more detail. Thanks reply aaronmdjones 19 hours agorootparentprevI stopped being able to do GitHub code search without being logged in several months ago. The search bar would search through open issues; clicking code on the left in the results prompts me to login. Has been this way since at least June. reply henriquecm8 19 hours agorootparentprevYeah, I used to use github to search for codes that I don&#x27;t remember in what repository I used, by searching for keywords that I can remember, but that doesn&#x27;t work anymore. reply doublerabbit 19 hours agoprevNothing other than Microsoft&#x27;s attempt to stop AI from learning repo code. reply 1231232131231 19 hours agoparentI doubt they would use the code search feature instead of just cloning the repo and feeding it to the AI. reply dontupvoteme 19 hours agorootparentThey probably want to use user info on what they search for, what they use, etc to better filter between good&#x2F;bad popular&#x2F;not-popular code for finetuning things.If people can search anonymously it gets a lot harder to datamine reply doublerabbit 19 hours agorootparentprevI don&#x27;t doubt it.If you wished to build a ML bot that taught switches&#x2F;loops; instead of cloning every repo all you&#x27;d need to do is search for switches&#x2F;loops within X lang. reply yodon 19 hours agorootparentVery inefficient way to do data access for more realistic training objectives. reply doublerabbit 18 hours agorootparentWhy else do you use search? If not to find a specific piece of code. reply beeboobaa 19 hours agorootparentprevThat would be ridiculously slow and inefficient. reply doublerabbit 18 hours agorootparentAnd downloading a blind repo isn&#x27;t? reply beeboobaa 18 hours agorootparentMuch less so. It can easily be parallelized without running into rate limits of the proprietary search API. Then once you&#x27;ve cloned it once it&#x27;s on your system and you won&#x27;t have to talk to github again. So much more sensible. reply doublerabbit 18 hours agorootparentSensible isn&#x27;t a thing when it comes to malicious activity. Same could be used to cheat the search.Downloading a repo and than neither knowing the repo has the syntax you wish to learn is still going to be more resourceful than having a webpage thrown at you with all search results. replyodyssey7 18 hours agoparentprevRetrieval-augmented generation could be the specific AI use case they’re guarding against if they’re restricting search but not cloning. reply skilled 19 hours agoprevI still can’t get over the fact that GitHub blatantly disabled regular search in favour of some Git mumbo jumbo nonsense that forbids you from looking up recent code. Any new technology, be it an API or otherwise, you can’t explore it on GitHub because “search by recency” is no longer a thing.What an asinine thing to do. My GitHub usage has dropped exactly to 0% for this very reason. I know I am not alone either since their org forums is filled with complaints about this.Not going to use any strong words but I want to. Disgraceful UX choice. reply simonw 19 hours agoparentThere&#x27;s a forum thread about that here: https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;community&#x2F;discussions&#x2F;52932#discussi...> We are thinking about way to make this work in the new code search system, but don&#x27;t have a date for when it will be possible. reply zxt_tzx 18 hours agoprev [–] To those arguing that GitHub needs to do this because search is resource intensive, why can&#x27;t GitHub put their code search behind something like Cloudflare Turnstile?See e.g. Troy Hunt doing this for HaveIBeenPwned: https:&#x2F;&#x2F;www.troyhunt.com&#x2F;fighting-api-bots-with-cloudflares-...I have had my own issues with Cloudflare Turnstile (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38412057), but in principle, I don&#x27;t see why can&#x27;t this be done replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub has implemented a new policy that requires users to be logged in to search for code, leading to frustration among users.",
      "Some users argue that this change is unnecessary and hostile towards open-source principles.",
      "GitHub justifies the login requirement as a security measure to protect their servers from anonymous overload, but some users believe Microsoft's influence may be a factor."
    ],
    "commentSummary": [
      "GitHub now requires users to be logged in to search for code, citing the resource-intensive nature of the new code search feature.",
      "Some users speculate that this change may be a strategic move to drive registration and increase user numbers.",
      "Mixed opinions exist regarding the change, with some dismissing complaints as laziness or incompetence, while others raise concerns about potential socioeconomic issues, unauthorized access, security risks, code protection, scraping by AI companies, hosting open-source projects, and Microsoft's acquisition of GitHub. Discussions also include alternative platforms and search methods."
    ],
    "points": 335,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1701093410
  },
  {
    "id": 38436419,
    "title": "Broadcom Layoffs Hit VMware Employees After Acquisition",
    "originLink": "https://www.businessinsider.com/broadcom-vmware-layoffs-employees-face-job-cuts-acquisition-2023-11",
    "originBody": "Broadcom CEO Hock E. Tan Lucas Jackson/Reuters Redeem now Some VMware employees learned Monday their positions would be eliminated. Broadcom announced it closed its $69 billion acquisition of VMware on November 22. VMware had already started cutting jobs prior to the deal closing. Some VMware employees learned on Monday that their positions would be eliminated following Broadcom's announcement last week that its deal to acquire the company had closed. Broadcom first announced in May 2022 that it would acquire VMware for $61 billion and assume $8 billion of its net debt. The company announced that it closed the deal on November 22, shortly after receiving regulatory clearance from China. It also had to receive regulatory clearance from other countries including the US and the UK. Employees whose positions were eliminated received an email on Monday, viewed by Business Insider, that read: \"Broadcom recently completed its acquisition of VMware. As part of integration planning, and following an organizational needs assessment, we identified go-forward roles that will be required within the combined company. We regret to inform you that your position is being eliminated and your employment will be terminated.\" \"We would like to thank you for your dedication and service. We want to make this transition as smooth as possible, including offering you a generous severance package and providing you a non-working paid notice period,\" the email continued. Currently, it's unclear exactly how many employees will be affected by the cuts. Broadcom did not immediately respond to a request for comment. VMware — which, as of February, employed more than 38,000 people — had already cut jobs before the acquisition closed, BI previously reported. VMware sent a letter to employees in September saying that they would be offered a job by Broadcom, offered a transitional role, or receive a severance package. Some VMware employees speculated that Broadcom could spin out certain units. Several top VMware executives have left the cloud-computing company over the past year. Some VMware employees said they worried about a culture clash with Broadcom, especially since Broadcom requires workers to return to the office. They also said some deals had slowed as clients waited for news about VMware's fate. Got a tip? Contact this reporter via email at rmchan@businessinsider.com, Signal at 646.376.6106, or Telegram at @rosaliechan. (PR pitches by email only, please.) Other types of secure messaging available upon request. Sign up for notifications from Insider! Stay up to date with what you want to know. Subscribe to push notifications Read next Was this article valuable for you? Additional comments Email (optional) Receive a selection of our best stories daily based on your reading preferences. Submit Watch: How tech layoffs could affect the economy M&A Layoffs",
    "commentLink": "https://news.ycombinator.com/item?id=38436419",
    "commentBody": "Broadcom lays off many VMware employees after closing acquisitionHacker NewspastloginBroadcom lays off many VMware employees after closing acquisition (businessinsider.com) 317 points by mfiguiere 15 hours ago| hidepastfavorite180 comments nerdjon 13 hours agoFirst, question:> Several top VMware executives have left the cloud-computing company over the past year.Since when is VMWare a \"cloud-computing\" company?I am now reaching out to friends to see if everyone is ok, this sucks. I don&#x27;t really get doing layoffs as soon as the acquisition finishes. Wouldn&#x27;t you first expect that people are going to leave just due to them being bought and then do additional cuts if necessary after that?I feel like that has always been the strategy with acquisitions minus redundancy like HR.Otherwise this is a big moral hit, first being bought and then an acquisition at the same time? Who would stay after that? reply ultrarunner 13 hours agoparent> Since when is VMWare a \"cloud-computing\" company?I think most VPS hosting providers are running on VMWare. Whether that counts as \"the cloud\" or not is up to you, but virtualization within datacenter is pretty standard.For what it&#x27;s worth, every experience I&#x27;ve ever had with Broadcom in the embedded space has been negative. So this came initially as a disappointment, and now I&#x27;m disheartened to see my expectations were accurate. reply mianos 10 hours agorootparentI worked at one of the biggest private cloud providers in Australia.It was VMWare VCentre&#x2F;VSphere end to end for all private clients.There were some great GUIs and great management but if you wanted serious API access it was the one of the most hacked up thing I have ever seen in my career. reply gemstones 7 hours agorootparentOh my lord, I remember dealing with a VSphere API early in my career. They didn&#x27;t have a concept of service accounts so the official advice for our nightly automation was:Pay a sysadmin to be up at night, then before your job is meant to run, have them log into the VSphere GUI, open devtools, click refresh to trigger a GET request, copy the cookie, paste the cookie into our custom software, and send it in the cookies of our API requests.When we asked if there was an easier way, they said that we could probably use Selenium to automate that.Needless to say, we did not sign the contract, and moved wholesale into AWS. reply mianos 7 hours agorootparentThat&#x27;s about it. Although, instead of using Selinium, you could work out the XML that the GUI runs from and `POSTS`, then make scripts pretend to be the GUI. Then maybe going into the internal postgres database that runs it to get the data that is not on the GUI.Not funny. :) reply PH95VuimJjqBqy 2 hours agorootparentprevAs someone with a vast amount of experience automating vsphere, this is made up bullshit.API access got a lot better once they introduced the powershell API (which can also be used via C#) but the solution you&#x27;re describing is the result of people not knowing what they&#x27;re doing. reply genmud 2 hours agorootparentIt looks like the automation api was introduced in 2021. (!!!)I stopped using it in like 2014 or 2015 after years of requesting improvements (like better ways to automate things without stuff breaking all the time). reply mianos 1 hour agorootparentMe too. I have not touched it for over 2 years and big clients sre unlikely to make huge upgrades quickly anyway.It sounds great they have arrived at the 20th century. Powershell and C#? Don&#x27;t tell me it is SOAP and all XML? I kinda like the way most cloud apis are just REST and json under the hood with vendor supported python packages that work well on all major platforms. replyjSully24 8 hours agorootparentprevIn my experience in these situations (I was always being acquired), the acquiring company is at least partially counting on cost saving from getting rid of departments like finance, marketing, sales and legal as they believe they can do that with their own teams. The acquiring company also will have their own metrics for proper sizing (employee count and spend) for engineering, support, product, etc which is likely lower than the company being acquired. But in this case VMWare is pretty mature so those metrics maybe ok Can’t say I support this, but the bean counters love it.Edit: “ Broadcom has not returned CRN’s request for comment about the job cuts. The company, which closed its acquisition of VMware Wednesday, had previously stated it could find $250 million in synergies once it completed the deal.” Synergies is code for the redundancies I mentioned above. reply throwaway2037 3 hours agorootparentnext [–]SynergiesAlso: Less competition. reply redundantly 12 hours agorootparentprev> I think most VPS hosting providers are running on VMWare.This is highly unlikely. VMware is too expensive for a VPS provider to achieve profitability. reply swozey 8 hours agorootparentIt&#x27;s usually proxmox or virtuozzo, or some other kvm front end. I&#x27;ve worked at 4 huge webhosts and they never used vmware for vps&#x2F;shared servers. Only our dedicated customers paying for support (think RAX) used vmware and that was their own. reply refulgentis 12 hours agorootparentprevIt seems highly unlikely they&#x27;re not successful and selling for $64 billion. I don&#x27;t get it either :&#x2F; reply brianwawok 11 hours agorootparentF500 companies are paying them millions in licenses to save 10x their hardware cost. reply antod 7 hours agorootparentprev> I think most VPS hosting providers are running on VMWare.Out of date info, but last time I really looked there used to be 3 tiers of VPS provider:* VSphere with SAN and blades (I think there was an HP reference implementation for this - many providers seemed to use it). These were by far the most expensive, and not really very good performance for the money due to SAN speeds and ESX not cpu scheduling multi core VMs that well.* Xen&#x2F;KVM. Mid range in terms of cost, and less fault tolerant than the SAN backed VSphere stuff. But usually out performed it.* VServer &#x2F; OpenVZ at the very cheap end. These were more like early LXC containers than VMs. The price point meant that you got what you paid for, and there were tech limitations with what you could do with the kernel (they were sharing the host kernel). Security wasn&#x27;t great either.I doubt the VMWare providers are as common now as they were 15yrs ago though. Even back then, they weren&#x27;t the most common though. reply bastardoperator 10 hours agorootparentprevI worked at some fairly large hosting companies before cloud took over and never once saw vmware. It was almost always virtuozzo&#x2F;openvz reply grepfru_it 8 hours agorootparentThere are two classes of hosting companies, enterprise and small business. Enterprise hosting companies serve enterprise fortune500 customers. They are VMWare. The ones catering to small business are using everything but VMware because of costs. They are not typically large, digital ocean and linode aside. reply depereo 12 hours agorootparentprevI don&#x27;t know if it&#x27;s &#x27;most&#x27; but there&#x27;s plenty of large private-cloud B2B setups offering vm-as-a-service &#x2F; esxi-as-a-service or something frontended by vmware cloud director. Definitely to the tune of billions of annual revenue worldwide. reply 1letterunixname 4 hours agorootparentprevVPSes use Xen and such, not VMware because of $. VMware was born out of the need to increase utilization of expensive enterprise servers by putting many virtual machine guests on fewer physical hosts. reply alwaysrunning 13 hours agoparentprev>I don&#x27;t really get doing layoffs as soon as the acquisition finishes.Planning layoffs is typically part of the M&A process, so when it completes, they typically already know who is going to be let go and drop the axe at that point. reply alephnerd 13 hours agorootparentThe people making the cutting decisions are also a removed from individual IC level visibility. These cuts are often done based on financial metrics per product line.You could be an amazing engineer and have a well engineered product, but still get laid off because the product didn&#x27;t get market traction.I&#x27;ve been on both sides of the equation. It sucks but such is life sadly. reply gitfan86 12 hours agorootparentAnd ability doesn&#x27;t fit in the spreadsheet they use to calculate their target profitability.They are going to cut expenses by 40% and increase costs of their products by 20%. reply bluGill 12 hours agorootparentThat is the point of lay offs. They are not saying you are bad they are saying they don&#x27;t need someone in your position.sometimes a company will let managers in a different department that is hiring know about coming lay offs and ask&#x2F;force them to take good people. this is rare though. reply alephnerd 12 hours agorootparent> sometimes a company will let managers in a different department that is hiring know about coming lay offs and ask&#x2F;force them to take good people. this is rare though.Depends on the kind of layoff. If it&#x27;s strategic M&A related this happens fairly often. If it&#x27;s purely a cost cutting hail mary, then not as much. reply comprev 12 hours agorootparentprevThere&#x27;s probably some legal loophole by which _technically_ the employees have a contract with a new company after the merge, since their old employer no longer exists on paper.This means employees get let go with legal bare minimum compensation and have little (if any) legal position to fight back.Immoral but not illegal. reply hn_throwaway_99 12 hours agorootparent> This means employees get let go with legal bare minimum compensation and have little (if any) legal position to fight back.I don&#x27;t know what country you&#x27;re posting this from, but in the US none of this matters, and it does not apply. When someone is let go there is no \"legal bare minimum compensation.\" There are some notice requirements (e.g. the WARN act), and many companies will pay out compensation in lieu of notice if the WARN act applies, but an acquisition makes no difference to these notice requirements. reply comprev 12 hours agorootparentIn the UK we do have some protections depending on length of time with the company. There is some framework for redundancy payments.The US culture of \"at will employment\" (i.e fire you on the spot for little reason) gives me anxiety and I don&#x27;t even live there! reply bluGill 12 hours agorootparentThere are pros and cons. The ability to lay someone off quickly means someone in the US can risk hiring a bunch of people on a risky bet - if&#x2F;when they give up on the project they can let the people go and cut their losses. Some of those risky bets pay off though and the people are not laid off.Note that I said lay off not fire above. There is an important legal difference. reply Isthatablackgsd 10 hours agorootparentThere are both side of the coin when it comes to at-will employments with layoff. They can use it to get rid of high paying employees to hire lowest quality of employee for little pay that will affects the quality of their product. And they don&#x27;t care because it looks good on their balance sheet. And there is no recourse for high paying employee against the company (unless they have a signed contracts that have a clause to prevent this).My former job laidoff few people because of high paying salary. They got high salary because positions required a specialized and niche knowledge for their jobs. They are good at what they do which earned their justification for high paying. It took the company a year to hire a barely qualified person because they don&#x27;t want to increase the pay after declined job offers from multiple potential candidates. reply chii 10 hours agorootparentprev> an important legal difference.so what is the difference?a layoff is due to business \"loss\" or changes? How is that really any different to a firing? reply bluGill 9 hours agorootparentFire is for cause and that means they will tell others when asked. This also means more paperwork to prove you are bad. That is they want someone doing the job you are supposed to do,but for whatever reason not you.Laid off just means they don&#x27;t need the job done anymore, and has nothing to do with how good or bad you are. reply dataflow 7 hours agorootparent> Laid off just means they don&#x27;t need the job done anymore, and has nothing to do with how good or bad you are.Eh, in theory maybe. Not really in practice. When layoffs happen, companies often prioritize laying off lower performers. And they often still need the jobs done, just fewer people.Of course none of these are legal differences. The legal difference I know of is that being fired for cause affects whether you are eligible for unemployment insurance benefits. reply 0cf8612b2e1e 10 hours agorootparentprevEspecially notable in an easy to fire environment when health insurance is tied to employment. Everything feels higher stakes when there is minimal social safety net. reply sgerenser 8 hours agorootparentEver since the ACA passed, you can jump right onto a marketplace plan with very little out of pocket (based on your income, which is $0 if you’re unemployed!). And they can’t exclude preexisting conditions anymore. Would be even better if employer-based health insurance went away completely, but at least it’s better than the old status quo. reply comprev 7 hours agorootparentprevTo clarify my earlier comment I know \"to fire\" and \"to let go\" are quite different - the former implies you broke contract, be it illegal means or poor performance reasons.\"to let go\" means redundancy.It appears some countries see little difference between the two and others have a clear distinction and legal process. reply brianwawok 11 hours agorootparentprevSure is nice when you hire a bad employee reply mschuster91 10 hours agorootparentIn Germany, we have a 6 month \"probationary period\" during which either side can terminate the employment contract. That&#x27;s more than enough time to check if you got a dud. reply olliej 11 hours agorootparentprevsure is nice when you have a bad employer reply nxm 10 hours agorootparentThen find an alternate employer? reply olliej 8 hours agorootparentThe whole problem is employers abusing employees by firing without any reason, and using the threat of firing employees if they complain. That is used to force employees to accept unsafe and illegal working conditions, because if you&#x27;re an employee who&#x27;s choice is \"report workplace problems\" or \"pay rent\" you&#x27;re kind of forced into accepting unsafe or unreasonable work places.[1]Because it turns out that is what employers do. That&#x27;s why most countries have have laws preventing employers from arbitrarily dismissing employees unless you have an actual reason.If you have a bad employee, you can fire them. You don&#x27;t need at will employment for that. Unless you mean \"I don&#x27;t like this employee, who is doing their job properly\" in which case you don&#x27;t have a bad employee, you&#x27;re an asshole.[1] If you&#x27;re ever fired and your employer claims they&#x27;re allowed to do so because \"at will\" employer, note that \"at will\" only covers \"legal\" reasons. Retaliation, bias, anti-pregnancy, anti-child, anti-religion, etc are all illegal reasons. Always ensure that everything you communicate with your employer is in writing. As @nxm is demonstrating your only safe option is to assume that your employer is your enemy. replyumanwizard 12 hours agorootparentprevIn the U.S. (where VMware is based), virtually all jobs can fire you for no reason at will. Contractually guaranteed jobs are rare. reply late2part 9 hours agorootparentprevWhy is it immoral to fire employees that the owner thinks are not needed? reply Fatnino 9 hours agoparentprevIf Broadcom lets the VMware employees decide to leave on their own, they will be losing the top performers and those with highest prospects of getting a job elsewhere quickly. They end up left with the less good employees.But if instead Broadcom proactively sheds the underperformers, then the desired employees are reminded to be scared of unemployment by the carnage. And at the same time the market is flooded with a bunch of people looking for the same jobs a high performing employee would have, so now it&#x27;s harder to jump ship.All this is disastrous for morale, but no worries, the beatings will continue until morale improves. reply Spartan-S63 10 hours agoparentprevBack in 2020, VMware pivoted hard into the cloud computing&#x2F;software space with the acquisitions of Carbon Black, Heptio, and Pivotal—I was a Pivot prior the acquisition. They rolled all these acquisitions into a single business unit to sell (formerly) Pivotal Cloud Foundry (PCF), Pivotal Container Service (PKS), and other products in an on-prem, enterprise PaaS offering.I left shortly after the Pivotal acquisition closed, so I didn&#x27;t see firsthand how it unfolded, but from folks I talked to, it seemed that VMware batted teams around and really mixed the whole thing up. reply grepfru_it 8 hours agorootparentPicked up where you left off. It seems that VMware was pulled apart and its carcass was sold off reply bogomipz 5 hours agorootparentprev>\" They rolled all these acquisitions into a single business unit to sell (formerly) Pivotal Cloud Foundry (PCF), Pivotal Container Service (PKS), and other products in an on-prem, enterprise PaaS offering.\"So what happened to Heptio exactlY? Were they combined with Pivotal and spun-off? reply natbennett 4 hours agorootparentTheir staff got smooshed together with Pivotal and a few other acquired odds-and-ends and largely set to work building a suite of Kubernetes-related products under the “Tanzu” brand. This included a Kubernetes manager (TKG) and an application platform (TAP) that were supposed to replace the BOSH-based Kubernetes installer (TKG-i, formerly known as PKS) and application platform (TAS, formerly known as PCF, better known as Cloud Foundry).They got stirred around a few times via reorg, then the Heptio founders retired. Last I heard everyone in Tanzu was trying to get themselves reclassified as somehow part of the Cloud Foundry&#x2F;BOSH stuff because it has significant revenue and was believed to be safe(r) from Broadcom layoffs. reply jabroni_salad 13 hours agoparentprevvmware is very popular in the private IAAS space. If you have special legal requirements but are too small a fish for the big players to spend time catering to, this is where you end up. IAAS operators that are competent at operations and business logic but not necessarily having the engineering talent needed to run a kvm fleet. reply manuelabeledo 9 hours agoparentprev> I don&#x27;t really get doing layoffs as soon as the acquisition finishes.It is pretty standard procedure. Buyer usually absorbs leadership first, then they gut sales, then marketing after that. Leadership is kicked out or leaves after a while.They are treated like redundancies, even if they aren&#x27;t so.I feel for the VMWare folks. I may have my issues with their pricing, but their products have historically been quite solid, while Broadcom strategy is, well, just \"buy and spread\". reply hello_moto 12 hours agoparentprevVMWare is used in the cloud as an alternative option from docker container.VMWare is also diving into K8S space, I&#x27;m sure they have some sort of \"managed\" service offering that bundles their K8S solution.VMWare owns Spring Framework... that&#x27;s a cloud toolkit. reply dilyevsky 12 hours agorootparentVmware bought heptio and with that things like their managed control plane (a la Google’s Anthos) and other k8s stuff. I think most of those folks are gone now though reply natbennett 12 hours agorootparentThe headline folks from Heptio are gone but the products and staff are still there. They’ve gotten a lot of internal transfers from other parts of VMware. reply rf15 12 hours agorootparentprevThanks for pointing out the Spring connection... I wonder how that will impact their development. reply ivanche 58 minutes agorootparentIf it&#x27;s true that they&#x27;ll lay off Oliver Drotbohm and Dan Vega then \"seriously bad\" could describe the impact. reply farnulfo 9 hours agorootparentprevhttps:&#x2F;&#x2F;x.com&#x2F;odrotbohm&#x2F;status&#x2F;1729231722498425092?s=20 reply geodel 10 hours agoparentprev> Since when is VMWare a \"cloud-computing\" company?Well they have this Pivotal Cloud &#x2F; Cloud Foundry &#x2F;VMware Tanzu crap. I know this isn&#x27;t what people mean when they talk about \"cloud company\" but it works as \"private cloud\" in enterprises. reply Stranger43 3 hours agorootparentWe have had sales people trying to push this to us for the last few years and so far it&#x27;s been ridiculously priced and nobody seems to come up with plausible real life success stories.My impression is that vmware have had almost no traction for any product beyond stock vsphere and even there the API&#x27;s see very little use as most customers prefer to point and click in the web interface and are basically happy with what vmware delivered a decade ago in terms of features, but people in this market are also loyal to vmware as it&#x27;s the known way to get virtualization to smaller server rooms. reply grepfru_it 8 hours agorootparentprev>crapWhat do you suggest as a replacement to CF and Tanzu? reply palemoonale 7 hours agoparentprevOur Full hosting service &#x2F; IaaS &#x2F; PaaS builts on ESXi &#x2F; vSphere HA, with custom or OTS orchestrators. It is quite common actually in service provider clouds, at least the ones that are more traditional and not relying on fancy, nwer stacks. reply IshKebab 12 hours agoparentprev> Since when is VMWare a \"cloud-computing\" company?That&#x27;s like 90% of what they do now.They actually make that really clear with little cloud icons on the \"Product\" menu on their homepage. reply natbennett 12 hours agoparentprevThey sell the tools to build private clouds. reply calderwoodra 12 hours agoprevMy friend has been feeding me information about the acquisition for almost a year now.They work at VMware and they knew about the layoffs months ago. Apparently another layoff is coming in 3 months or so with a better package and my friend was estatic to make it past this round and to be included in the next round. reply pokstad 10 hours agoparentThe new American dream: holding out for the better layoff package. reply geraldwhen 10 hours agorootparentIs that not your dream? Why work for years when you can receive years worth of pay in a single paycheck?A good layoff package can advance retirement age significantly. reply bagels 10 hours agorootparentIt&#x27;s more typically a month or two of pay. reply LargeTomato 9 hours agorootparentI only pocket about 20% of my paycheck after mortgage and other expenses. 4 paychecks is 20 months of extra pay. reply zlg_codes 9 hours agorootparentLook at Mr. Moneybags over here with living expenses under 50%... reply coredog64 6 hours agorootparentprevPer Blind, the first round is getting two months severance plus two months of garden leave to allow WARN Act compliance. reply winrid 5 hours agorootparentprevI have worked at software places where it was 2 weeks severance, even if on family leave. reply geraldwhen 7 hours agorootparentprevIt’s not where I work. The packages are truly envious! reply sgift 8 hours agorootparentprevPersonally, I would prefer to stay as long as I can in one place, and work on something I really like until retirement. But, I admit, these kind of jobs are rather rare in the modern economy (even though I&#x27;m not in the US and from my experience it&#x27;s a bit better for someone with my wishes here in Germany). reply waveBidder 7 hours agorootparentprevSome people aren&#x27;t completely alienated from the value of their labor. reply rilindo 14 hours agoprevAs a public cloud engineer, it feels like this is a mistake to cut deeply right when cloud repatriation is starting to be a thing. reply alephnerd 14 hours agoparentBroadcom&#x27;s strategy is to remove sustaining engineering or deprecate product lines with limited uptake by F1000 customers.Also, VMWare has A LOT of fat to trim. I&#x27;ve worked closely with teams and alumni of Broadcom&#x2F;Avago, VMWare, CA Technologies, and Symantec, and honestly, the cuts Broadcom does are pretty reasonable. reply Icathian 13 hours agorootparentI can&#x27;t speak for the whole org, but my small corner of VMware got cut far more than was reasonable. We already ran very lean, and with the losses there will be no new features on our product for a very long time, if ever. Best we can hope for is maintenance unless they&#x27;re planning to staff back up down the line (they&#x27;re not). reply alephnerd 13 hours agorootparent> Best we can hope for is maintenanceThis is probably what&#x27;s going to happen. Companies like Broadcom and PANW want pretty high margins on their product lines, so they&#x27;re pretty mercenary in getting it done.If you got an offer letter, you should probably survive if you can justify your value over the next few months (in my experience w&#x2F; M&A) reply Icathian 13 hours agorootparentI got an offer letter, but I&#x27;ve already got an offer elsewhere and will be leaving shortly. I&#x27;m not interested in picking up the pieces of the mess they&#x27;ve made. It just makes me very sad, my team was amazing and deserved better. reply alephnerd 13 hours agorootparentYep. If I was in your shoes I&#x27;d leave as well. If you got a very competitive stock grant, that means they really want you. If not, best to leave elsewhere. reply gwbas1c 13 hours agorootparentprev> VMWare has A LOT of fat to trimOh, VMware is the 2nd shortest job in my career, primarily for that reason.I don&#x27;t want to say much in a public forum, but my best lesson from working at VMware is how to sniff out an incompetent engineer early in a job interview: I just ask coding questions that a VMWare engineer would get wrong.For example:I have an engineer write some very basic manual SQL data mapping code. If the candidate really pushes back, or just can&#x27;t figure it out, I move on.I ask a question where the engineer should use an enum instead of a string. If the engineer uses an enum, I move on. reply throwaway5959 13 hours agorootparent> I ask a question where the engineer should use an enum instead of a string. If the engineer uses an enum, I move on.I’m confused by this. You’re saying you move on if the engineer uses an enum as you would expect? reply akira2501 12 hours agorootparentIncompetent managers seem to love shorthand \"competency\" tests like this. reply gamblor956 12 hours agorootparentprevYou&#x27;re not confused. As the OP indicated, he was once a VMWare engineer, so he gets his own questions wrong. reply icelancer 10 hours agorootparentTakes one to know one. Checkmate. reply FirmwareBurner 13 hours agorootparentprev>I have an engineer write some very basic manual SQL data mapping code.Do people usually have the SQL language so well memorized that they can pull it out of their head on command? I swear to god I&#x27;m not the only engineer who regularly goes to the cheat sheet. reply JeremyNT 11 hours agorootparent> Do people usually have the SQL language so well memorized that they can pull it out of their head on command? I swear to god I&#x27;m not the only engineer who regularly goes to the cheat sheet.I think the key insight for you here is that there are many jobs where the only technical skill used is SQL. You probably should know this stuff like the back of your hand for such roles. Old, slow moving, large companies have many people doing this sort of work.Contrast with a generic \"developer\" at some startup who uses SQL as only one part of much larger and more complex applications.As a \"developer\" you have to deal with so much obscure syntax in your life (actual programming languages, shells, dockerfiles, kubernetes yaml, helm, god knows how many 3rd party APIs, ci&#x2F;cd definitions, other rando DSLs...) that you won&#x27;t be able to keep everything in working memory. reply ecshafer 8 hours agorootparentIronically working at a big old company I wrote almost no SQL, because the database was DB2, so we had dedicated Data developers to write stored procedures in COBOL &#x2F; SQL. While at a small shop and a more modern tech company I wrote tons of SQL. reply sgift 8 hours agorootparentSame, my best experiences with getting data out of DBs - in an earlier job where I had to work with many external customers directly - were the big ones, cause they always had dedicated Database dev teams that could prepare the data in a way that worked for me and the database. Contrast this with the small shops, which - if at all - had someone as a db admin (usually in addition to their normal job) and could only provide database access and \"here, figure it out\", which meant I had to write all the SQL myself.But I think that supports the GPs point. At big companies you are more specialized, which means if you get hired into the database dev team you better know SQL really well, cause the non-database devs won&#x27;t even get access to it and instead depend on you. reply SoftTalker 13 hours agorootparentprevAs with any language, developers who use SQL a lot do memorize it. Maybe not every bit of it but all the commonly used bits. I used to write SQL and PL&#x2F;SQL every day at work and I got to the point where I rarely needed to consult the manual. reply FirmwareBurner 12 hours agorootparent>As with any language, developers who use SQL a lot do memorize it. If you&#x27;re hiring for a SQL developer I guess it makes sense, buit I use SQL about once every 6-12 months or so, so if you hit me with SQL queries on the spot I&#x27;ll fail your interview for sure.Honestly, I find interviews that revolve around memorizing and regurgitating some programing language syntax trivia a bit shit.Wouldn&#x27;t it be better testing for critical thinking and problem solving skills instead of shit people google anyway? reply SoftTalker 12 hours agorootparentIf you&#x27;re not hiring a SQL developer I agree it doesn&#x27;t make sense to ask them memory questions about SQL. But it does make sense to ask them memory questions about whatever it is they claim to know. A developer who has to google the basics is not going to be very good at problem solving. reply vel0city 11 hours agorootparentprevAny time I&#x27;ve participated in a technical interview as an interviewer we&#x27;ve had the candidate have full access to a computer and web browser.I don&#x27;t expect everyone to have everything memorized or get it right on the first try, but if you don&#x27;t even know what questions to ask or what resources to confer with, I&#x27;m usually concerned if the candidate will be a good fit. The candidate can&#x27;t be expected to instantly warp to the solution but they need to show they can begin to find the path. reply rf15 12 hours agorootparentprevI have to use SQL like 1-3 times a month and can confirm that the syntactic inconsistencies of the language make me go to the cheat sheet regularly. reply cmrdporcupine 13 hours agorootparentprevPeople doing whiteboard coding should not have to get the syntax 100% (or even 80%) correct. It should be enough to just show conceptual understanding of what kinds of syntactical elements and concepts are present in the language. When I trained for giving interviews @ Google, this was emphasized, at least. It&#x27;s not about getting the syntax exactly right, it&#x27;s about getting the algorithm right. Though obviously if someone is getting something fundamental wrong, it raises eyebrows.If I were interviewing for database&#x2F;SQL work, I&#x27;d be personally probing around understanding of the relational data model, not specifics of SQL syntax. How would you model this table? What&#x27;s a join? What&#x27;s normalization? What&#x27;s wrong, conceptually, with the following schema? etc reply bluGill 12 hours agorootparentWhen I give coding interviews I often jump in with \"it doesn&#x27;t compile because you forgot the semicolon at\". The tools we use don&#x27;t make it easy for me to fix such mistakes and it doesn&#x27;t pay to waste their time to search out a mistake I (as an observer) already happened to know - I want to see their algorithm work so we can move on (or see how they debug why it isn&#x27;t working yet depending on what phase we are in). I don&#x27;t really care how long it takes you to find that missing semicolon - we all make that mistake once in a while and there isn&#x27;t much to learn from it. reply financltravsty 9 hours agorootparentprevIdeally, yes you should be able to understand and recall all of the basic commands:- DDL: create, drop, alter, truncate- DML: insert, update, delete, call, explain, lock- TCL: commit, savepoint, rollback- DQL: select- DCL: grant, revokeIf you&#x27;ve used them before, you&#x27;ll have a good enough intuition to understand what each does and when to bring them out of the toolbox. Some database engines have extended versions of these or other ways to achieve the same result, but the underlying concepts are the same.From there, you should be able to create a basic SELECT statement, and understand the basic flags:- FROM- WHERE- GROUP BY- HAVING- ORDER BY- LIMITThen understand joins and how this is all just relational calculus&#x2F;algebra with sets.Honestly, I couldn&#x27;t list these off the top of my head, but I know what each one does when I see them, and I&#x27;ll instinctively reach for them when I&#x27;m writing SQL. But once you know all of this, you know 80% of the SQL you will ever need, and won&#x27;t have to reach for an ORM. It only takes one weekend to grok SQL, but it will serve you for the rest of your career. reply didibus 12 hours agorootparentprevIncompetent engineers and fat to trim I feel are seperate issues.Layoffs seem pretty bad at identifying the incompetent from the competent from my experience. It seems to cut on all front, you&#x27;re not sure if you&#x27;re left with the best or the worse ones, but you saved a lot of payroll cost either way.Or if you try to handicap teams by understaffing them, even competent engineers will need to reduce the quality of their output. This is when fat is trimmed in a bad way, where instead of getting rid of low ROI venture&#x2F;projects&#x2F;services&#x2F;products and focus on the important, you just tighten the budgets accross the board for example. reply stillwithit 13 hours agorootparentprevAs an EE, when someone explains they need SQL, all the other hallucinated semantic handlebars to compute, I move on.Abacus, slide rule, grid paper, mechanical pencil, or gtfo!IT is data librarian work. I compute with raw materials of reality, not 1970s semantic babble. Software as an industry has a lot of fat to cut.…The argument can work from various contexts. Not the flex you think it is. reply throwaway5959 7 hours agorootparentYou should really check out Agile if you want to understand why software is so messed up now. It’s been perverted to basically mean “rush out product features every two weeks that product managers think are interesting” in many places. reply adolph 12 hours agorootparentprevIf said candidate&#x27;s whiteboard response doesn&#x27;t use at least two main spurving bearings, culture fit comes into question. reply broast 13 hours agorootparentprevUnfortunately it sounds like you&#x27;re filtering on things that can be easily taught. reply esafak 11 hours agorootparentI think the idea is that if they don&#x27;t know something basic you can forget about the hard stuff. reply alephnerd 13 hours agorootparentprevI used to do that as well when I was still a SWE and then a PM. VMWare has good talent (when I first became a PM a long time ago, I was mentored by a very smart VMWare PM alum), but it also has a lot of empire building (when I first became a SWE a large portion of my management was VMWare EM alumni who weren&#x27;t that impressive).Also, a lot of their early management feel like one hit wonders in my experience working with them.It reminds me of how PANW was before Arora joined and cleaned house, and Symantec&#x2F;CA before Broadcom cleaned house. reply FireBeyond 13 hours agorootparent> Also, a lot of their early management feel like one hit wonders in my experience working with them.And many of those one hits were ideas crafted and handed to them for implementation (still valuable and important), but left to distill ideas on their own? Not so much. reply wmf 14 hours agoparentprevWhy bother repatriating to VMware? It&#x27;s probably more expensive. reply alephnerd 14 hours agorootparentMost F1000s use ESXi by default. You can concentrate only on direct sales to F1000s and make more than enough revenue.Look at Zscaler and Crowdstrike for examples - both follow this strategy.As a business, you optimize for Net Revenue after Cost of Operation. It&#x27;s always good to drop customers if it costs more to support them than the revenue you get. reply crmd 12 hours agorootparentEvery single one of those F1000’s has a highly competent CTO office team with an effectively unlimited hc and budget to find a competitive alternative platform to ESX. I don’t know if Broadcom is gonna make their money back before open source gets good enough. reply alephnerd 12 hours agorootparent> Every single one of those F1000’s has a highly competent CTO office teamI agree> effectively unlimited hc and budgetI disagree.A company like Google or FB can build in-house tooling simply because they have entire dedicated teams of engineers to manage their environments in house to meet niche needs. A F10 like ExxonMobil or UnitedHealth cannot justify a FB size engineering footprint when their margins are much lower.> find a competitive alternative platform to ESXYep. The issue is ESX is actually pretty good at getting the job done. Your alternatives from a supportability standpoint are HyperV from Microsoft (which will probably eat up the smaller ESX customers), Citrix Hypervisor (owned and operated by ex-Broadcom leadership), and IBM RedHat&#x27;s KVM (which requires you to work with IBM for Professional Services).At the end of the day, you as a CTO or Platform team don&#x27;t want to be fully OSS. Not because OSS is crap software (anything but), but because a pure OSS play doesn&#x27;t provide you a dedicated support engineering team if shit hits the fan nor SLAs and monetary compensation if shit breaks.This is why most OSS core companies max out revenue via a Professional Services play. RedHat is a notable example of this. reply crmd 8 hours agorootparentIt doesn’t take Facebook level engineering to architect a substitute for VMware in 2023. Mid market IT contractors are replacing ESX for their clients en mass with proxmox. It’s only a question for enterprise until open source gets good enough. reply riemannzeta 11 hours agorootparentprevI have heard this colorfully and memorably referred to as \"the need for a single throat to choke.\" reply alephnerd 11 hours agorootparentPretty much reply sithadmin 12 hours agorootparentprevThere isn&#x27;t a &#x27;competitive alternative platform&#x27; that achieves feature or performance parity with ESXi. reply wmf 14 hours agorootparentprevRight, but those companies don&#x27;t benefit from repatriation. Their on-prem IT is more expensive than public cloud. reply alephnerd 14 hours agorootparent> Their on-prem IT is more expensive than public cloud.Not really. Most of those companies have lax Cloud Cost Management hygiene and&#x2F;or the Cloud Platform team is separate from the Infra&#x2F;Servers team.If you are F250 you can probably negotiate a nice discount if you are able to fully migrate to a single cloud, but most companies don&#x27;t want to keep all their eggs in a single basket. This means larger organizations cannot avail competitive discounts on public cloud.By bundling Private Cloud (VMWare ESXi), Network Security (VMWare NSX), APM (CA Wiley), Endpoint Protection (Symantec Enterprise), and Data Security (Symantec Enterprise) I can purchase 5 critical pieces of Enterprise Infrastructure using a single PO. This is critical at large organizations as any PO above $30k almost always requires CFO or Comptroller approval, and a single PO to Broadcom satisfies your Infra, DevOps, and Security needs (which are all cost centers if you aren&#x27;t a tech company).Btw, Broadcom themselves is almost entirely on GCP [0]. Almost all their infra and products are served using a GCP stack on the backend.[0] - https:&#x2F;&#x2F;www.broadcom.com&#x2F;blog&#x2F;broadcoms-transformation-journ... reply tempnow987 12 hours agorootparentI looked at VMware \"in the cloud\". It was around $50,000 - $100,000 per year for 48 cores. I&#x27;ve defended AWS pricing many times, mostly because of ability to burst etc. But the VMware workloads are often much flatter in terms of demand - ie, many systems just left running so the AWS value - while there of course, isn&#x27;t as obvious to me. reply alephnerd 12 hours agorootparent\"In the cloud\" options are often much more expensive than what they should be due to customer expectations. The margins are much higher than in on-prem (where 80% \"discounts\" are common). reply tempnow987 11 hours agorootparentSure. That said, the cost &#x2F; benefit tradeoffs often seem pretty good for AWS. The cost is low enough to other costs on a project that gain on velocity is worth it. In others gains on reliability, maintenance savings etc.Ended up doing a small on-prem solution. VMWare for 6 CPUs x 32 cores = 192 cores runs about $3K&#x2F;year for the software side which is a good deal to get started. That leaves about $240K&#x2F;year or so to cover other costs. Not a slam dunk necessarily, but the on-prem store with vmware is not unreasonable. reply alephnerd 11 hours agorootparent> the cost &#x2F; benefit tradeoffs often seem pretty good for AWSIf it&#x27;s greenfield I&#x27;d agree. There&#x27;s a reason why most companies founded after 2008 have a heavy public cloud presence.The issue is if you are a large brownfield deployment (like most F1000s), a \"Cloud Transformation\" takes forever and is costly.It can be done - for example Capital One and Broadcom - but it requires executive buy-in to respect engineering leadership and build a solid DevOps&#x2F;Platform team.I know if I was to found my own company tomorrow, I&#x27;d be entire cloud first because of velocity and ease of scalability, but you can&#x27;t expect a company like UnitedHealth Group to transition to an entirely cloud first environment within a 2-3 year timeframe as even a minor outage represents millions of dollars lost a minute and litigation.Over the next 10-15 years we&#x27;ll see a large number of non-tech first companies becoming multi-cloud, but in 2023, it&#x27;s still work in progress. reply wmf 13 hours agorootparentprevWho do you work for BTW? reply alephnerd 13 hours agorootparentPE&#x2F;VC in the Enterprise space now. I used to be a PM and SWE and a staffer for a hot second. Not at Broadcom but have worked closely with their team and alumni. The Enterprise Infra space is a small world.Edit: Also, I didn&#x27;t realize I&#x27;m replying to an actual legend in the systems&#x2F;networking space. I read some of your papers when I was an undergrad and later as an early career SWE. replyghaff 14 hours agorootparentprevYeah. Repatriation is probably an overstated trend even if companies are generally getting smarter about where and how they run workloads. But, while it&#x27;s hard to migrate companies off VMware to a different on-prem solution, I&#x27;m not sure that you&#x27;ll see workloads being repatriated going onto VMware (unless the company already has a large installed base). reply gwbas1c 13 hours agoparentprev> cloud repatriationYou can repatriate VMWare VMs from someone else&#x27;s datacenter to your datacenter. You might even be able to do it with little downtime. (I vaguely remember that VMWare has 0-downtime movement of a VM among physical hardware.) reply nolist_policy 12 hours agorootparentThere&#x27;s nothing special about live migration, really. The major implementations (VMWare, Hyper-V, KVM&#x2F;Qemu, Xen, ...) support it for a long time, including shared-nothing migration. reply woleium 13 hours agorootparentprevits called vmotion iirc, and it costs more money than we had when i looked at it reply auspiv 13 hours agorootparentprevvMotion. works great reply redwood 13 hours agoparentprevHave you seen evidence that it&#x27;s a thing? I&#x27;ve seen evidence of it being a useful threat, just as multi-cloud is. The company that most publicly \"did it\" ended up plateauing as they stopped innovating on anything other than that backend migration and plus they likely would have gotten the cost they looked for from the hyperscalers if they&#x27;d continued to negotiate. But maybe I&#x27;m missing something? reply DaiPlusPlus 13 hours agorootparent> Have you seen evidence that it&#x27;s a thing?Only anecdotally - though the stuff I&#x27;ve seen is more about undoing the mistake of letting anyone at the company run-up massive bills on cloud-only big-data-processing (CosmosDB, Data Lake, etc).The only other \"repatriation\" movement I&#x27;ve seen are orgs who got burned by limitations and glitches in things like OneDrive&#x2F;GoogleDrive - again, this doesn&#x27;t really come under VMware&#x27;s remit. reply FireBeyond 13 hours agorootparentThe other big thing to realize is that for VMware, and Red Hat (I used to work on the OpenShift team) is that cloud might be big, but our last estimates were that 80% of our workloads and capacity were on-prem&#x2F;hybrid, not cloud. reply AmericanChopper 12 hours agorootparentprevI’ve had the pleasure of contracting on a cloud migration project, and then a little while later an on-prem migration for the same project. I helped that company build a cost model for comparing their on-prem and cloud workloads, and they ended up moving quite a bit back on-prem. Even for the workloads they didn’t move, having a better way of comparing on-prem and cloud TCOs helped them negotiate much better pricing for their remaining cloud workloads.One legitimate challenge is that it’s actually rather difficult to measure the TCO of each option. I’ve seen companies pushed to get better at that as they experience cloud sticker shock, and as large enterprise pushes more and more work to the cloud, I’d expect to see them continue to improve in this area. reply scarface_74 13 hours agoparentprevDo you have any evidence that this happening besides random anecdotes? reply asylteltine 10 hours agoparentprevCloud repatriation is a myth perpetuated by consulting companies to get more hours. No one is seriously considering moving off the cloud reply Jordanpomeroy 10 hours agorootparentFalse reply allenrb 8 hours agorootparentprevI honestly cannot tell if this comment is a joke or not. reply ska 14 hours agoprevLeast surprising headline of the day? reply p1esk 12 hours agoparentAre there any surprising headlines today? reply givemeethekeys 14 hours agoprevAcquiring and gutting is the Broadcom way. reply outside1234 14 hours agoparentBut this isn&#x27;t semiconductors.This will just end up with Microsoft taking all of their customers away. reply stackskipton 14 hours agorootparentAs former admin of both, HyperV is nowhere near as good as VMware ESXi offering. Also, switching Hypervisors at F1000 is such a massive project that no risk adverse manager is going to sign off on it.I&#x27;ve been at F1000 with ESXi clusters, they had 20 racks full of blades&#x2F;switches&#x2F;SANs that made up 4000 core cluster. They will ride or die ESXi no matter how hard VMware screws them over. reply Stranger43 2 hours agorootparentIf my experience is anything to go by those clusters will fade and die over the next 20 years just like the equally critical and irreplaceable unix(tm) vms and mainframe system is in the process of doing.But it will be a long process and a lot of money will be made by vmware and partners even as the market wound down and move to whatever it is that going to replace it(im willing to bet it wont be kubenetes as we know it today). reply NewJazz 13 hours agorootparentprevMiddle management is getting squeezed in a lot of firms. Risk averse management won&#x27;t cut it, IMO. reply bluGill 11 hours agorootparentprevToday... However if Microsoft (or someone unknown) works hard for a while they can build a more compelling offering and get F1000 to switch over the next 10-20 years. reply stackskipton 11 hours agorootparentSure but until then, they can squeeze the hell out of F1000s.Problem with creating competitive offering is F1000 are not going to take risk on you. Last thing CIO wants to remember for is someone who greenlit moving core infrastructure to product that went under. They also need to provide decent support so startup today has about 15 years before they even think of breaking into this market.So it&#x27;s got to be a major player like Microsoft who seems to not care about this market because Azure. Nutanix is there but they are all in hyperconverged meaning traditional compute&#x2F;storage separated hardware would have to be thrown out. Red Hat exited in 2020. So I don&#x27;t see a competitor on the horizon. reply bluGill 9 hours agorootparentA CIO will not move core infrastructure, but they will often let you try things in a lab, and if that goes well move non core things off, then as confidence grows the core goes too. That is why I said 10 to 20 years.Unless broadcom has made the CIO (or CFO...) mad, then it is all hands on deck to replace them.. I know one company (by personal conversation with insiders, but I don&#x27;t think they want to be named) replacing otherwise good tools because broadcom now owns them and the license fees are too high reply OrvalWintermute 10 hours agorootparentprevHowever awesome some of their products are, I&#x27;ve not been seeing VMware displacing very cost conscious customers running homogeneous workloads of Windows on HyperV, or Linux on RHEV due to the attractiveness of the licensing. reply stackskipton 9 hours agorootparentI didn&#x27;t say HyperV&#x2F;RHEV was bad, just VMware is gold standard of virtualization. reply The_Colonel 13 hours agorootparentprevThat&#x27;s like saying all Oracle customers are leaving for Postgres. VMWare moat is deep. reply factlogic 13 hours agoprevIt also has to be reported that those who are staying as a part of the newly formed Vmware (under broadcom) has been offered a very lucrative and generous pay packages and welcome grants . Especially at levels 5 and above it reaches close to a million dollars of RSU . This is in addition to existing VMware stock that employees have. reply alephnerd 13 hours agoparentYep. Broadcom pays Google level but with much less fat and demanding an AWS style work ethic. reply stingraycharles 7 hours agorootparent“with much less fat and demanding an AWS style work ethic”What does that mean? I can’t tell if that’s a positive or a negative. reply natbennett 14 hours agoprevIs this reporting that Broadcom is implementing the layoffs that were announced through the offer letter process? Or are they laying off people who they already gave an offer letter? reply x3n0ph3n3 13 hours agoparentPeople who were not given an offer letter. reply scaramanga 1 hour agoprevSurprised their union agreed to this given the extraordinary profits being made... reply mobilio 14 hours agoprevLink:https:&#x2F;&#x2F;archive.md&#x2F;V9Y67 reply deburo 14 hours agoprevCost cutting or killing a competitor (does they own another virtual host company?)? reply alephnerd 14 hours agoparentBroadcom is almost 50% Hardware and 50% Enterprise SaaS (CA Technologies&#x27; APM product line, Symantec&#x27;s EPP product line, and now VMWare&#x27;s entire private cloud product line) by revenue.I wouldn&#x27;t be surprised if Broadcom did this acquisition in order to pull of a HP&#x2F;HPE type split. Broadcom alumni did a similar thing with Citrix, TIBCO, and NetScaler merging into the \"Cloud Security Group\" reply cbsks 14 hours agoprevI interviewed at VMware a couple of years ago. Bullet dodged! reply chihuahua 11 hours agoparentI interviewed there a few weeks ago. They mentioned the pending acquisition, and said they had been trying to fill as many positions as possible before that event. They would continue interviewing candidates, but if they wanted to make an offer, that offer would have to wait until after the acquisition completed.I found a job elsewhere in the meantime, so it&#x27;s not relevant to me anymore. reply geodel 13 hours agoparentprevI think they allow(ed) permanent remote positions. For someone who care (like me!) it maybe worth quite a bit. So far I am stuck with this RTO mandate in most places. reply brcmthrowaway 11 hours agorootparentHave you found public companies with generous stock comp that allow full remote? reply chihuahua 11 hours agorootparentDepending on what exactly qualifies as \"generous\": ZipRecruiter, GE Healthcare, Cloudflare, Atlassian. reply unmole 3 hours agorootparentprevArista has fully remote positions globally. reply TimMontague 9 hours agorootparentprevNVIDIA has fully remote positions. reply HenryBemis 13 hours agoprevReading the dear John letter it reminded me of two of the companies I worked in the past that wanted to reduce their staff, and they were looking for volunteers. In both cases I found better AND god a very decent payout. I understand that someone in their late 50s may not see this as a blessing, but imagine getting another job and a 15% raise and 5-10-20 salaries (gross) in your pocket (as a great sign-off bonus). I see this a free money! reply ghaff 13 hours agoparentOr someone in their late 50s might be ready to retire (or work part-time independently) for the right buyout offer. reply bluGill 11 hours agorootparentI know I&#x27;m hoping that in my late 50s &#x2F; early 60s my company goes through a round of voluntary layoffs. It remains to be seen how good my 401k and other retirement plans do over the years in between - I can&#x27;t afford to retire today, but there is a good chance that by 60 I could do so. Having see several relatives die at 64-65 recently I&#x27;m not interested in working longer (family history suggests overall I&#x27;ll have an average lifespan: 78-79 - but the normal bell curves start showing death at 64 is not an outlier) reply ghaff 10 hours agorootparentIt absolutely depends.Someone can be in a situation where they can prefer their job than an alternative. But, for a lot of people, well it&#x27;s \"work\" even if they&#x27;re generally OK with it. And if they can afford to do something they prefer--especially given possible keeping toe in the river options--why not? reply outside1234 14 hours agoprevProbably going to lose the other half promptly as well to be honest. I mean, seriously, who wants to work for Broadcom? reply natbennett 14 hours agoparentI know a surprising number of people who hope that Broadcom will be better managed than VMware and that it’ll be easier to get good work done there. Mostly folks who came into VMware through acquisitions and then had layers and layers of VMware management piled on top of them. reply jsdwarf 14 hours agoparentprevThere are golden handcuffs called RSUs, shares that are distributed to employees over a longer period of time. The longer you stay, the more shares you get ;-) reply brcmthrowaway 12 hours agoparentprevBRCM comp is about 50% stock, and the stock has been a runaway hit in the last few years. So a lot of people.They also gave crazy 4 years of RSU grants in one hit right before COVID. reply uxp8u61q 12 hours agoparentprevAbout 20,000 people according to Wikipedia... reply ReptileMan 14 hours agoprevIn my low labor cost neck of the woods VMware employ a lot of people. I wonder if they will pick up hiring and shift work from US&#x2F;UK here or will decimate the current offices too. Or both and work current people here to the bone. It&#x27;s broadcom after all. reply bdcravens 13 hours agoprevIn related news, water is still wet. reply vkdelta 13 hours agoprevthey might as well add NCNR (No Cancellation, No Return) and million dollars engagement NREs while they are at it.if you have BRCM as your key supplier, you better plan to keep them for life as otherwise. reply 1letterunixname 4 hours agoprevFun fact 0: Much of VMware&#x27;s practical success was born out of its sales and consulting arm that was born out of the subsuming of an East Coast consulting company. (A long time ago, I found a deep link to an IT shop redirect mentioned somewhere on early documentation or in a support article.)Fun fact 1: Between VMware and Citrix, there was an unspoken gentleman&#x27;s agreement that piracy in the service of the industry and functional interoperability was OK. Licensing became nag-based because of the pushback from customers who experienced hard licensing restrictions interfering with critical business operations.Fun fact 2: One of VMware&#x27;s office moves was blocks away around Palo Alto. Legend has it, some servers were kept running by migrating to&#x2F;from standalone UPSes and datacenter PDUs one PSU at a time.Fun fact 3: VMware Workstation was usable and interesting in 2000.Fun fact 4: VMware&#x27;s early story is almost as desperate as Dropbox&#x27;s and Cisco&#x27;s. All 3 could make interesting Netflix docudramas.Fun fact 5: VMware&#x27;s ~2015 microkitchens were better than Facebooks. Breadracks of jars of tasty snacks from wall to wall.Fun fact 6: Most of VMware Workstation and Fusion yearly major versions skew to UI&#x2F;UX and theme changes rather than functionality improvements because of incredibly lean, under-resourced teams.Disclaimer: Once upon a time (c. 2004), {{big name university}} paid me to go to in-person training and pass the VCP exam. LOL. reply nektro 12 hours agoprevwhen will these layoffs stop? reply Ancalagon 11 hours agoparentProbably not until interest rates come down or plateau for a long time at the least.Arguably this layoff was more because of the merger though and less because of the economy at broad. reply gopher2000 9 hours agoparentprevThis is standard procedure for Broadcom and acquisitions. Not necessarily a symptom of industry conditions. reply HDThoreaun 11 hours agoparentprevWhen tech companies get back to pre pandemic staffing levels? reply gnicholas 12 hours agoparentprevWhen morale improves. reply gsich 12 hours agoprev [–] Broadcom will ruin it. Nothing good comes out of that company. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Broadcom has finalized its acquisition of VMware, leading to potential job cuts for some VMware employees.",
      "The exact number of employees affected remains uncertain.",
      "VMware had already been implementing job cuts prior to the acquisition.",
      "Broadcom emailed employees whose positions were being eliminated, offering a severance package and a non-working paid notice period.",
      "Speculation arose among VMware employees about the possibility of certain units being spun out by Broadcom.",
      "Concerns were raised about potential clashes in company culture and the impact on business deals."
    ],
    "commentSummary": [
      "Broadcom's acquisition of VMware has led to layoffs, raising concerns about timing and employee morale.",
      "The discussion highlights challenges with VMware's vSphere API and the preference for REST and JSON-based cloud APIs.",
      "Other topics include the use of VMware by VPS hosting providers, the impact of layoffs on retirement age, and the importance of understanding SQL in software engineering."
    ],
    "points": 317,
    "commentCount": 180,
    "retryCount": 0,
    "time": 1701110907
  },
  {
    "id": 38432127,
    "title": "The Relationship Between zlib, gzip, and zip Explained",
    "originLink": "https://stackoverflow.com/questions/20762094/how-are-zlib-gzip-and-zip-related-what-do-they-have-in-common-and-how-are-they",
    "originBody": "Stack Overflow About Products For Teams Stack Overflow Public questions & answers Stack Overflow for Teams Where developers & technologists share private knowledge with coworkers Talent Build your employer brand Advertising Reach developers & technologists worldwide Labs The future of collective knowledge sharing About the company Loading… current community Stack Overflow help chat Meta Stack Overflow your communities Sign up or log in to customize your list. more stack exchange communities company blog Log in Sign up Home Questions Tags Users Companies Collectives Explore Collectives Labs Discussions Teams Stack Overflow for Teams – Start collaborating and sharing organizational knowledge. Create a free Team Why Teams? Teams Create free Team Collectives™ on Stack Overflow Find centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives Teams Q&A for work Connect and share knowledge within a single location that is structured and easy to search. Learn more about Teams Get early access and see previews of new features. Learn more about Labs How are zlib, gzip and zip related? What do they have in common and how are they different? Ask Question Asked 9 years, 11 months ago Modified today Viewed 462k times This question shows research effort; it is useful and clear 1247 This question does not show any research effort; it is unclear or not useful Save this question. Show activity on this post. The compression algorithm used in zlib is essentially the same as that in gzip and zip. What are gzip and zip? How are they different and how are they same? compression zip gzip zlib Share Improve this question Follow Follow this question to receive notifications edited Sep 10, 2016 at 3:36 Francisco Corrales Morales 3,70611 gold badge3737 silver badges6464 bronze badges asked Dec 24, 2013 at 13:48 Abhishek JainAbhishek Jain 10k55 gold badges2727 silver badges4040 bronze badges 0 Add a comment3 Answers Sorted by: Reset to default Highest score (default) Trending (recent votes count more) Date modified (newest first) Date created (oldest first) This answer is useful 3137 This answer is not useful Save this answer. +500 This answer has been awarded bounties worth 500 reputation by Matthieu Show activity on this post. Short form: .zip is an archive format using, usually, the Deflate compression method. The .gz gzip format is for single files, also using the Deflate compression method. Often gzip is used in combination with tar to make a compressed archive format, .tar.gz. The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications. Long form: The ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware. It is an archive format that stores files and their directory structure, where each file is individually compressed. The file type is .zip. The files, as well as the directory structure, can optionally be encrypted. The ZIP format supports several compression methods: 0 - The file is stored (no compression) 1 - The file is Shrunk 2 - The file is Reduced with compression factor 1 3 - The file is Reduced with compression factor 2 4 - The file is Reduced with compression factor 3 5 - The file is Reduced with compression factor 4 6 - The file is Imploded 7 - Reserved for Tokenizing compression algorithm 8 - The file is Deflated 9 - Enhanced Deflating using Deflate64(tm) 10 - PKWARE Data Compression Library Imploding (old IBM TERSE) 11 - Reserved by PKWARE 12 - File is compressed using BZIP2 algorithm 13 - Reserved by PKWARE 14 - LZMA 15 - Reserved by PKWARE 16 - IBM z/OS CMPSC Compression 17 - Reserved by PKWARE 18 - File is compressed using IBM TERSE (new) 19 - IBM LZ77 z Architecture 20 - deprecated (use method 93 for zstd) 93 - Zstandard (zstd) Compression 94 - MP3 Compression 95 - XZ Compression 96 - JPEG variant 97 - WavPack compressed data 98 - PPMd version I, Rev 1 99 - AE-x encryption marker (see APPENDIX E) Methods 1 to 7 are historical and are not in use. Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use. The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all. Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8. (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.) The ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures. Around 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats. This greatly expanded the use of the .zip format. In the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities. Unix compress was designed to compress a single file or stream, appending a .Z to the file name. compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders. Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents. That implementation has not been so challenged in the last 20+ years. The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence). gzip appends a .gz to the file name. gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data. The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time. Though compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file. In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress. This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format. The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files. .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives. Unlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed. Shortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time). So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif. That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy. In order to promote widespread usage of the PNG format, two free code libraries were created. libpng and zlib. libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications. zlib was adapted from the gzip code. All of the mentioned patents have since expired. The zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams. Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer. The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses. Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data. zlib is now in wide use for data transmission and storage. For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format. Different implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores. Share Improve this answer Follow Follow this answer to receive notifications edited 12 hours ago answered Dec 24, 2013 at 18:03 Mark AdlerMark Adler 104k1313 gold badges120120 silver badges159159 bronze badges 31 176 This post is packed with so much history and information that I feel like some citations need be added incase people try to reference this post as an information source. Though if this information is reflected somewhere with citations like Wikipedia, a link to such similar cited work would be appreciated. – ThorSummoner Oct 16, 2015 at 16:29 1812 I am the reference, having been part of all of that. This post could be cited in Wikipedia as an original source. – Mark Adler Oct 16, 2015 at 16:38 682 FYI: Mark Adler is an American software engineer, and has been heavily involved in space exploration. He is best known for his work in the field of data compression as the author of the Adler-32 checksum function, and a co-author of the zlib compression library and gzip. He has contributed to Info-ZIP, and has participated in developing the Portable Network Graphics (PNG) image format. Adler was also the Spirit Cruise Mission Manager for the Mars Exploration Rover mission. (wikipedia) – Isaac Hanson Dec 9, 2015 at 17:23 133 gzip was created to replace Unix compress. zip is not superior to tar + gzip on Unix, for several reasons. (When you see .tar.gz files, that's what they are.) First, tar + gzip compresses better than zip, since the compression of the next file can use history from the previous file (sometimes referred to as a \"solid\" archive). zip can only compress files individually. Second, tar preserves all of the Unix directory information, whereas zip was not designed to do that. (Later extensions to the zip format with Unix-specific extra blocks tries to remedy this problem.) – Mark Adler Feb 21, 2016 at 17:19 115 You seem to be confusing formats with implementation. The 7-Zip implementation of the deflate format can get something like your quoted 2% to 10% better compression than gzip with the very same deflate format (while taking much more CPU time to do so). The 7z LZMA2 format offers on the order of 40% better compression. – Mark Adler Mar 17, 2016 at 14:34Show 26 more comments This answer is useful 62 This answer is not useful Save this answer. Show activity on this post. ZIP is a file format used for storing an arbitrary number of files and folders together with lossless compression. It makes no strict assumptions about the compression methods used, but is most frequently used with DEFLATE. Gzip is both a compression algorithm based on DEFLATE but less encumbered with potential patents et al, and a file format for storing a single compressed file. It supports compressing an arbitrary number of files and folders when combined with tar. The resulting file has an extension of .tgz or .tar.gz and is commonly called a tarball. zlib is a library of functions encapsulating DEFLATE in its most common LZ77 incarnation. Share Improve this answer Follow Follow this answer to receive notifications answered Dec 24, 2013 at 13:55 Niels KeurentjesNiels Keurentjes 41.5k99 gold badges9999 silver badges137137 bronze badges Add a commentThis answer is useful 38 This answer is not useful Save this answer. Show activity on this post. The most important difference is that gzip is only capable to compress a single file while zip compresses multiple files one by one and archives them into one single file afterwards. Thus, gzip comes along with tar most of the time (there are other possibilities, though). This comes along with some (dis)advantages. If you have a big archive and you only need one single file out of it, you have to decompress the whole gzip file to get to that file. This is not required if you have a zip file. On the other hand, if you compress 10 similiar or even identical files, the zip archive will be much bigger because each file is compressed individually, whereas in gzip in combination with tar a single file is compressed which is much more effective if the files are similiar (equal). Share Improve this answer Follow Follow this answer to receive notifications edited Sep 27, 2018 at 13:19 Konrad Rudolph 533k133133 gold badges940940 silver badges12191219 bronze badges answered Dec 24, 2013 at 14:03 Tim ZimmermannTim Zimmermann 6,17233 gold badges3030 silver badges3636 bronze badges 3 12 You are overstating the point. If people wanted random-access compressed archives, they could create \".gz.tar\" files instead of \".tar.gz\" files. They don't, because most people aren't that interested in random access. There is a big community around the .warc.gz web archiving format, and they need random access, so they compress each web page separately. You use this format every time you look at a webpage in the Internet Archive Wayback Machine. – Greg Lindahl Jun 18, 2016 at 14:57 12 \".gz.tar\" does not offer random-access as the tar format is not capable to randomly access its entries. You need to go through all the entries from the beginning to get the one specific entry wanted, even worse: you need to go through all the entries until the end because the same file may be archived several times (in several versions) on several places in the same archive - and there is no means to figure it out except to read the whole archive entry by entry. – Min-Soo Pipefeet Jul 23, 2018 at 19:48 1 @Min-SooPipefeet right, because tar means \"tape archive\", and (unless you're playing with something like DECtape) tapes are nothing if not sequential! – RonJohn Aug 7, 2022 at 20:07 Add a commentYour Answer Reminder: Answers generated by Artificial Intelligence tools are not allowed on Stack Overflow. Learn more Thanks for contributing an answer to Stack Overflow! Please be sure to answer the question. Provide details and share your research! But avoid … Asking for help, clarification, or responding to other answers. Making statements based on opinion; back them up with references or personal experience. To learn more, see our tips on writing great answers. Sign up or log in Sign up using Google Sign up using Facebook Sign up using Email and Password Submit Post as a guest Name Email Required, but never shown Post Your Answer Discard By clicking “Post Your Answer”, you agree to our terms of service and acknowledge that you have read and understand our privacy policy and code of conduct. Not the answer you're looking for? Browse other questions tagged compression zip gzip zlib or ask your own question. The Overflow Blog Are remote workers more productive? That’s the wrong question. Featured on Meta Update: New Colors Launched We're rolling back the changes to the Acceptable Use Policy (AUP) Temporary policy: Generative AI (e.g., ChatGPT) is banned Beta test for short survey in banner ad slots starting on the week of... Collectives updates: new features and ways to get started with Discussions Linked 97 How do I gzip compress a string in Python? 8 creating a zip file from an object directly without disk IO 6 Unzip or inflate php://input stream? 5 How to make a .zip file using zlib 9 Compress into .zip file with node.js core zlib module 4 With node's api/zlib I'm getting errno: -3, code: 'Z_DATA_ERROR' 2 Is DEFLATE used in gzip and png compression the same? 2 How to process a zipfile using Julia 0 How to change deflate stream output format(raw, zlib, gzip) when use zlib? -2 Zip fails after PHP download See more linked questions Related 3 What are the different zlib compression methods and how do I force the default in Java's Deflater? 91 Deflate compression browser compatibility and advantages over GZIP 3 In gzip library, what's the difference between 'uncompress' and 'gzopen'? 4 zlib/gzip interpreter 2 What is the difference between zlib's gzip compression and the compression used by .NET's GZipStream? 29 zlib: Differences Between the `deflate` and `compress` Functions 0 does using gzip on command line have the same result as using zlib programmatically? 3 Implementation discrepancies between GNU Gzip and Python Gzip 3 How do the different compression levels of gzip differ? 6 What is the relationship between zlib and minizip? Hot Network Questions Is to be able to describe something to be able to judge that it's true or not? Steps to install a natural gas ceiling garage heater Alternative to polling interrupt flag from main loop? Mishustin's circle problem What gives Egypt the authority or leverage to act as a mediator between Hamas and Israel? Why is it the indefinite article? Shouldn't it be 'the' here? Mathematical papers and the IMRaD-structure: how comparable are they with each other? What is a good journal for submitting my article on a conjecture in theoretical statistics, re: ancillary complement for correlation? What happens if an MCU boosts its own supply voltage while running? How can I word the rules for a custom Magic item that allows a caster to cast a spell (but still requires the spell slot and component)? why is it impossible to infer the surface temperature of Venus by spectroscopy observation from earth？ Annual bonus changed this year. Is there anything I can do about it? Catch Block Inside of a Trigger Does Not Get Hit When a SP With Dynamic SQL Errors Out - Causing the Larger Transaction to Fail Rearrange words to make a sentence Relatively prime numbers Null checking with primary constructor in C# 12 What's wrong with this picture? Impossible circles. \"James flicked a peanut at her.\" — What can \"flicked\" mean here? Why is the graph size decreasing in 'Column'? When has the scaffolding been more important than the completed building? Shielding bottom of PCB in THT design How is GNFS the best factoring algorithm when its time complexity exceeds brute-force? What does it mean for a liquid to \"crack\"? Short story about alien invaders encountering prairie dogs more hot questions Question feed Subscribe to RSS Question feed To subscribe to this RSS feed, copy and paste this URL into your RSS reader. Stack Overflow Questions Help Products Teams Advertising Collectives Talent Company About Press Work Here Legal Privacy Policy Terms of Service Contact Us Cookie Settings Cookie Policy Stack Exchange Network Technology Culture & recreation Life & arts Science Professional Business API Data Blog Facebook Twitter LinkedIn Instagram Site design / logo © 2023 Stack Exchange Inc; user contributions licensed under CC BY-SA. rev 2023.11.27.1523 Your privacy By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy. Accept all cookies Necessary cookies only Customize settings",
    "commentLink": "https://news.ycombinator.com/item?id=38432127",
    "commentBody": "How are zlib, gzip and zip related?Hacker NewspastloginHow are zlib, gzip and zip related? (stackoverflow.com) 281 points by damagednoob 20 hours ago| hidepastfavorite72 comments ctur 19 hours agoWhat a great historical summary. Compression has moved on now but having grown up marveling at PKZip and maximizing usable space on very early computers, as well as compression in modems (v42bis ftw!), this field has always seemed magical.These days it generally is better to prefer Zstandard to zlib&#x2F;gzip for many reasons. And if you need seekable format, consider squashfs as a reasonable choice. These stand on the shoulders of the giants of zlib and zip but do indeed stand much higher in the modern world. reply michaelrpeskin 15 hours agoparentI had forgotten about modem compression. Back in the BBS days when you had to upload files to get new files, you usually had a ratio (20 bytes download for every byte you uploaded). I would always use the PKZIP no compression option for the archive to upload because Z-Modem would take care of compression over the wire. So I didn&#x27;t burn my daily time limit by uploading a large file and I got more credit for my download ratios.I was a silly kid. reply EvanAnderson 14 hours agorootparentThat&#x27;s really clever and likely would have gone unnoticed by a lot of sysops! reply cpeterso 6 hours agorootparentAnother download ratio trick was to use a file transfer client like Leech Modem, an XMODEM-compatible client that would, after downloading the final data block, tell the server the file transfer failed so it wouldn’t count against your download limit.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;LeechModem reply lxgr 12 hours agoparentprev> These days it generally is better to prefer Zstandard to zlib&#x2F;gzip for many reasons.I&#x27;d agree for new applications, but just like MP3, .gz files (and by extension .tar.gz&#x2F;.tgz) and zlib streams will probably be around for a long time for compatibility reasons. reply pvorb 11 hours agoparentprevI think zlib&#x2F;gzip still has its place these days. It&#x27;s still a decent choice for most use cases. If you don&#x27;t know what usage patterns your program will see, zlib still might be a good choice. Plus, it&#x27;s supported virtually everywhere, which makes it interesting for long-term storage. Often, using one of the modern alternatives is not worth the hassle. reply emmelaich 15 hours agoprevFun fact: in a sense. gzip can have multiple files, but not in a specially useful way ... $ echo meow >cat $ echo woof > dog$ gzip cat $ gzip dog $ cat cat.gz dog.gz >animals.gz$ gunzip animals.gz$ cat animals meow woof reply ericpauley 7 hours agoparentImo all file formats should be concatenable when possible. Thankfully ZStandard purposefully also supports this, which is a huge boon for combining files.Fun fact, tar-files are also (semi-) concatenable, you&#x27;ll just need to `-i` when decompressing. This also means compressed (using gz&#x2F;zstd) tarfiles are also (semi-)concatenable! reply koolba 15 hours agoparentprev> ... but not in a specially useful way ...It can be very useful: https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;crfs#introducing-stargz reply DigiDigiorno 13 hours agorootparentIt is specially useful, it is not especially&#x2F;generally useful lolIt could be a typo, though I think when we say something \"isn&#x27;t specially&#x2F;specifically&#x2F;particularly useful\" we mean \"compared to the set of all features, specifically this subset feature is not that useful\" not that the feature isn&#x27;t useful for specific things reply emmelaich 8 hours agorootparentIndeed! I should have written \"especially\" not \"specially\" reply billyhoffman 8 hours agoparentprevWARC files (used by the Internet Archive to power the Wayback machine, among others) use this trick too to have a a compressed file format that is seek-able to individual HTTP request&#x2F;response records reply lxgr 12 hours agoparentprevWow, that&#x27;s surprising (at least to me)!Is there a limit in the default gunzip implementation? I&#x27;m aware of the concept of ZIP&#x2F;tar bombs, but I wouldn&#x27;t have expected gunzip to ever produce more than one output file, at least when invoked without options. reply tedunangst 11 hours agorootparentIt only produces one output. It&#x27;s just a stream of data. reply lxgr 11 hours agorootparentAh, I somehow imagined a second `cat` in there. That makes more sense, thank you! reply abhibeckert 8 hours agorootparentprevThe limit is it doesn&#x27;t do filenames or other metadata — it&#x27;s limited to contents. reply cout 17 hours agoprevInteresting -- I did not realize that the zip format supports lzma, bzip2, and zstd. What software supports those compression methods? Can Windows Explorer read zip files produced with those compression methods?(I have been using 7zip for about 15 years to produce archive files that have an index and can quickly extract a single file and can use multiple cores for compression, but I would love to have an alternative, if one exists). reply ForkMeOnTinder 17 hours agoparent7zip has a dropdown called \"Compression method\" in the \"Add to Archive\" dialog that lets you choose. reply pixl97 16 hours agoparentprevUntil windows 11, no, windows zip only seems to deal with COMPRESS&#x2F;DEFLATE zip files. reply melagonster 20 hours agoprevFor people who first read this: the sweet part is in the comments :) reply dcow 19 hours agoparentWhat’s even more sad is that the SO community has since consequently destroyed SO as the home for this type of info. This post would now be considered off topic as it’s “not a good format for a Q&A site”. You’d never see it happen today. Truly sad. reply barrkel 18 hours agorootparentThing is, it could only be that way in its early days, when the vanguard of users came to it from word of mouth, from following Joel Spolsky or Coding Horror or their joint podcast. The audience is much bigger now and with the long tail of people, the number willing to put effort into good questions is too low, and on-topicness is a simple quality bar which can improve the signal to noise ratio. reply s_dev 16 hours agorootparentThey had a voting system. By having mods decide what was and what wasn&#x27;t a &#x27;good&#x27; question undermined the whole point of the voting system. Mods should use their powers to filter out hate&#x2F;spam&#x2F;trolling&#x2F;egregiously off topic issues not determine relevance&#x2F;usefulness. As others have pointed out SO was a site with great answers but awful for asking questions. This is why ChatGPT is eating SO for breakfast.Even if a question was super similar to one that was previously asked has value in exactly that it might be phrased slightly better and be a closer match to what people were Googling. reply dcow 18 hours agorootparentprevExcept, I doubt anybody would argue that a lower signal to noise ratio has improved the site. (Plus, has the actual metric even improved and how is it measured?) And, did anybody ever stop to ask whether S:N should even be the champion metric in the first place, at a product level? With a philosophy of “Google is our homepage”, I honestly don’t understand why S:N even matters since search pretty effectively cuts out noise. I guess it makes a mod’s life easier though. The site is less useful today than it’s ever been. The road to hell… reply Dalewyn 18 hours agorootparentprevVery broadly, I find the quality&#x2F;value of a given thing is inversely proportional to how many people are involved.So with regards to the internet: The 90s and early 00s were great, then the internet became mainstream and it all just became Cable TV 2.0. reply zxt_tzx 18 hours agorootparentprevRelatedly, I have seen the graph showing the dip in SO traffic by ~30% if I&#x27;m not wrong (and the corresponding hot takes that attribute that to the rise of LLMs).I know most people are pessimistic that LLMs will lead to SO and the web in general to be overrun by hallucinated content and AI-training-on-AI-ouroboros, but I wonder if it might instead allow for curious people to query an endlessly patient AI assistant about exactly this kind of information. (A custom GPT perhaps?) reply dcow 18 hours agorootparentGPT info tools will fully replace SO in most dev workflows if it hasn’t already. reply norenh 16 hours agorootparentAnd what will GPT info tools learn from, once the public curated sources are gone? reply dcow 13 hours agorootparentProbably the great swaths of documentation out there that for most use cases people need not waste time sifting through if a computer can do it faster... reply hawski 13 hours agorootparentprevIsn&#x27;t it fun, that ChatGPT&#x27;s success poisoned the well for everyone else? :) reply dylan604 16 hours agorootparentprevBy then, AGI will be ready, right? reply twic 18 hours agorootparentprevA rephrasing of this might be on-topic on retrocomputing: https:&#x2F;&#x2F;retrocomputing.stackexchange.com&#x2F;q&#x2F;3083&#x2F;21450But almost nobody reads that. reply BeetleB 16 hours agorootparentprevThis is somewhat revisionist. They would mark stuff like this as off topic even in the early days. reply miyuru 17 hours agoparentprevHis stackexchange profile is a gold mine itslef.https:&#x2F;&#x2F;stackexchange.com&#x2F;users&#x2F;1136690&#x2F;mark-adler#top-answe... reply dustypotato 19 hours agoprevFound this hilarious:> This post is packed with so much history and information that I feel like some citations need be added> I am the reference(extracted a part of the conversation) reply gmgmgmgmgm 18 hours agoparentThat&#x27;s disallowed on Wikipedia. There, you must reference some \"source\". That \"source\" doesn&#x27;t need to be reliable or correct, it just needs to be some random website that&#x27;s not the actual person. First sources are disallowed. reply bombela 17 hours agorootparentI learned this when I tried correcting the wikipedia page on Docker. I literally wrote the first prototype. But this wasn&#x27;t enough source for wikipedia. And to this day the English page is still not truthfull (interestingly enough, the french version is closer to the truth). reply nerdponx 17 hours agorootparentYou could publish a little webpage called \"An historical note about the Docker prototype\" under your own name, which you could then cite on Wikipedia.I think it makes perfect sense as a general and strict policy for an encyclopedia. It would simply be too hard to audit every case to check if it&#x27;s someone like you, or a crank. reply bombela 14 hours agorootparentMaybe I should write the story as a comment on hacker news, and link to it ;)Joke aside, I should probably take up on your advice. reply a1369209993 11 hours agorootparentYes. Do this (make sure it&#x27;s not a top-level submission) and cite the HN comment specifically. Stupid rules deserve stupid compliance. reply nerdponx 10 hours agorootparentWhy would a top-level submission not be valid on Wikipedia? reply andrewf 7 hours agorootparentprevThat may fall afoul of the \"reputably published\" requirement at https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:No_original_research...Basically, Wikipedia wants a primary source&#x27;s claims to be vetted by a third party, either a \"reputable publisher\" or a secondary source. reply nerdponx 4 hours agorootparentDoesn&#x27;t that disqualify just about any personal blog as a source, or any academic preprint (like on Arxiv)? I can&#x27;t imagine that&#x27;s widely enforced. reply 3836293648 3 hours agorootparentWell, stuff like the ArXiv is exactly what they don&#x27;t want cited reply gmgmgmgmgm 2 hours agorootparentprevyou&#x27;d have to publish under another name. reference to the author&#x27;s blog is also disallowed reply dTal 16 hours agorootparentprevI don&#x27;t see how requiring someone to set up a little webpage filters out cranks. If anything I might expect it to favor them. reply nerdponx 14 hours agorootparentThe idea is that it&#x27;s a separate, distinct source, which exists outside of and independently from the encyclopedia itself, and can be archived, mirrored, etc. Its veracity and usefulness can then be debated or discussed as needed. reply kibwen 17 hours agorootparentprevAnd that&#x27;s for good reason. Encyclopedias are supposed to be tertiary sources, not primary sources. Having an explicit cited reference makes it easier to judge the veracity of a statement compared to digging through the page history to figure out if a line was added by a person who happens to be an expert. reply msla 16 hours agorootparentAnd then there&#x27;s impostors, which people who denigrate sourcing rules never seem to even think of. reply JadeNB 16 hours agorootparentprev> And that&#x27;s for good reason. Encyclopedias are supposed to be tertiary sources, not primary sources. Having an explicit cited reference makes it easier to judge the veracity of a statement compared to digging through the page history to figure out if a line was added by a person who happens to be an expert.But why is a reference to \"[1] Blog post by XXX\" (or, even worse, \"[1] Blog post by YYY based on their tentative understanding of XXX\") a more authoritative source than \"[1] Added to Wikipedia personally by XXX\"? Of course, Wikipedia potentially has no proof that the editor was actually XXX in the latter case; but they have even less proof that a blog post purporting to be by XXX actually is. reply kibwen 15 hours agorootparent> Wikipedia potentially has no proof that the editor was actually XXX in the latter case; but they have even less proof that a blog post purporting to be by XXX actually is.Wikipedia is not an authoritative identity layer, it provides no proof of identity and is thus strictly weaker than any other proof you can come up with. If you don&#x27;t trust any arbitrary website that Wikipedia cites, then you have no more reason to trust any arbitrary Wikipedia editor.As for what tertiary sources are and why they prefer not to cite primary sources in the first place, Wikipedia goes over this in their own guidelines: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:No_original_resear... reply tyingq 19 hours agoparentprevMaybe a spoiler, but the \"I\" in \"I am the reference\" is Mark Adler:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mark_Adler reply signaru 17 hours agorootparentIt&#x27;s awesome how he is active on stack overflow for almost anything DEFLATE related. I once tried stuffing deflate compressed vector graphics into PDFs. Among other things, it turns out an Adler-32 checksum is necessary for compliance (some newer PDF viewers will ignore its absence though). reply whalesalad 18 hours agoparentprevReminds me of when I was inadvertently arguing here on HN with the inventor of the actor model about what actors are reply demondemidi 18 hours agorootparentThat sounds like something I’d do too. If that makes you feel better. reply chupasaurus 6 hours agorootparentprevOh, I remember that conversation, it was fun. reply FartyMcFarter 18 hours agoparentprev\"I&#x27;m the one who knocks\". reply matheusmoreira 13 hours agorootparent\"I am the hype.\" reply HexDecOctBin 15 hours agoprevIs there an archive format that supports appending diff&#x27;s of an existing file, so that multiple versions of the same file are stored? PKZIP has a proprietary extension (supposedly), but I couldn&#x27;t find any open version of that.(I was thinking of a creating a version control system whose .git directory equivalent is basically an archive file that can easily be emailed, etc.) reply pizza 9 hours agoparentNew versions of zstd allow you to produce patches using the trained dictionary feature reply wiredfool 18 hours agoprevThe real question is: how are zlib and libz related? reply o11c 17 hours agoparentzlib is the name of the project. libz is an implementation-detail name of the library on Unix-like systems. reply pdw 16 hours agorootparentSimilar: Xlib and libX11. reply kissgyorgy 10 hours agoprevIf you are interested in implementation details, how to unpack&#x2F;decompress them, check out these Python implementations:- https:&#x2F;&#x2F;github.com&#x2F;onekey-sec&#x2F;unblob&#x2F;blob&#x2F;main&#x2F;unblob&#x2F;handle...- https:&#x2F;&#x2F;github.com&#x2F;onekey-sec&#x2F;unblob&#x2F;blob&#x2F;main&#x2F;unblob&#x2F;handle...- https:&#x2F;&#x2F;github.com&#x2F;onekey-sec&#x2F;unblob&#x2F;blob&#x2F;main&#x2F;unblob&#x2F;handle... reply exposition 11 hours agoprevThere&#x27;s also pzip&#x2F;punzip (https:&#x2F;&#x2F;github.com&#x2F;ybirader) for those wanting more performant (concurrent) zip&#x2F;unzip.Disclaimer: I&#x27;m the author. reply Dwedit 10 hours agoprevSee a highly upvoted answer in a question about zlib related things, suspect it was probably posted by Mark Adler, and turn out to be correct. reply readyplayernull 7 hours agoprevgzip can be used to (de)compress directories recursively in a variable:FOO=$(tar cf - folderToCompressgzipbase64)echo $FOObase64 - dzcattar xf - reply raggi 13 hours agoprevThe answer is good, but is missing a key section:Salty form: They&#x27;re all quite slow compared to modern competitors. reply levzettelin 13 hours agoparentWhat are some of those modern competitors? reply raggi 12 hours agorootparentFor zlib compatible workloads, there are cloudflare patches and chromium forks, intel forks, and zlib-ng which are compatible but >50% faster. (I think the cloudflare patches eventually made it into upstream zlib, but you may not see that in your distro for a decade).lz4 and zstd have both been very popular since their release, they&#x27;re similar and by the same author, though zstd has had more thorough testing and fuzzing, and is more featureful. lz4 maintains an extremely fast decompression speed.Snappy also performs very well, with zstd and snappy having very close performance with tuning to achieve comparable compression levels.In recent years Zstd has started to make heavy inroads in broader usage in OSS with a number of distro package managers moving to it and observing substantial benefits. There are HTTP extensions to make it available which Chrome originally resisted but I believe it&#x27;s now finally coming there too (https:&#x2F;&#x2F;chromestatus.com&#x2F;feature&#x2F;6186023867908096).In gaming circles there&#x27;s also Oodle and friends from RAD tools which are now available in Unreal engine as builtin compression offerings (since 4.27+). You could see the effects of this in for example Ark Survival Evolved (250GB) -> Ark Survival Ascended (75GB, with richer models & textures), and associated improved load times. reply scq 12 hours agorootparentprevzstd is over 4x faster than zlib, while having a better compression ratio.http:&#x2F;&#x2F;facebook.github.io&#x2F;zstd&#x2F; reply encom 16 hours agoprev [–] (2013) replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Stack Overflow is a platform where developers and technologists can ask questions and share knowledge.",
      "The summary includes information about compression algorithms used in zlib, gzip, and zip formats.",
      "It discusses the pros and cons of using gzip files and includes details about Mark Adler, a notable figure in data compression and file formats."
    ],
    "commentSummary": [
      "The post explores the usefulness of different compression formats such as zlib, gzip, zip, Zstandard, and squashfs.",
      "It discusses the benefits and debates surrounding the concatenation of tar files and the limitations of certain compression methods.",
      "The decline of Stack Overflow and the potential rise of AI tools in developers' workflows are also mentioned, along with challenges in finding reliable information and the trustworthiness of curated sources like Wikipedia."
    ],
    "points": 281,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1701092355
  },
  {
    "id": 38433563,
    "title": "Fatty Acid in Beef and Dairy Boosts Cancer Fighting Immune Response, Study Finds",
    "originLink": "https://biologicalsciences.uchicago.edu/news/tva-nutrient-cancer-immunity",
    "originBody": "News Nutrient found in beef and dairy improves immune response to cancer Scientists at UChicago discover that trans-vaccenic acid (TVA), a fatty acid found in beef, lamb, and dairy products, improves the ability of immune cells to fight tumors. November 22, 2023 Research by Jing Chen, Chuan He, and team suggests that TVA could have potential as a nutritional supplement to complement clinical treatments for cancer. (NIH) By Matt Wood Assistant Director of Communications, Biological Sciences Division Trans-vaccenic acid (TVA), a long-chain fatty acid found in meat and dairy products from grazing animals such as cows and sheep, improves the ability of CD8+ T cells to infiltrate tumors and kill cancer cells, according to a new study by researchers from the University of Chicago. The research, published this week in Nature, also shows that patients with higher levels of TVA circulating in the blood responded better to immunotherapy, suggesting that it could have potential as a nutritional supplement to complement clinical treatments for cancer. “There are many studies trying to decipher the link between diet and human health, and it’s very difficult to understand the underlying mechanisms because of the wide variety of foods people eat. But if we focus on just the nutrients and metabolites derived from food, we begin to see how they influence physiology and pathology,” said Jing Chen, PhD, the Janet Davison Rowley Distinguished Service Professor of Medicine at UChicago and one of the senior authors of the new study. “By focusing on nutrients that can activate T cell responses, we found one that actually enhances anti-tumor immunity by activating an important immune pathway.” Finding nutrients that activate immune cells Chen’s lab focuses on understanding how metabolites, nutrients and other molecules circulating in the blood influence the development of cancer and response to cancer treatments. For the new study, two postdoctoral fellows, Hao Fan, PhD and Siyuan Xia, PhD, both co-first authors, assembled a “blood nutrient” compound library consisting of 255 bioactive molecules derived from nutrients. They screened the compounds in this new library for their ability to influence anti-tumor immunity by activating CD8+ T cells, a group of immune cells critical for killing cancerous or virally infected cells. After the scientists evaluated the top six candidates in both human and mouse cells, they saw that TVA performed the best. TVA is the most abundant trans fatty acid present in human milk, but the body cannot produce it on its own. Only about 20% of TVA is broken down into other byproducts, leaving 80% circulating in the blood. “That means there must be something else it does, so we started working on it more,” Chen said. To see that a single nutrient like TVA has a very targeted mechanism on a targeted immune cell type ... I find that really amazing and intriguing. Jing Chen, PhD Jing Chen, PhD The researchers then conducted a series of experiments with cells and mouse models of diverse tumor types. Feeding mice a diet enriched with TVA significantly reduced the tumor growth potential of melanoma and colon cancer cells compared to mice fed a control diet. The TVA diet also enhanced the ability of CD8+ T cells to infiltrate tumors. The team also performed a series of molecular and genetic analyses to understand how TVA was affecting the T cells. These included a new technique for monitoring transcription of single-stranded DNA called kethoxal-assisted single-stranded DNA sequencing, or KAS-seq, developed by Chuan He, PhD, the John T. Wilson Distinguished Service Professor of Chemistry at UChicago and another senior author of the study. These additional assays, done by both the Chen and He labs, showed that TVA inactivates a receptor on the cell surface called GPR43 which is usually activated by short-chain fatty acids often produced by gut microbiota. TVA overpowers these short-chain fatty acids and activates a cellular signaling process known as the CREB pathway, which is involved in a variety of functions including cellular growth, survival, and differentiation. The team also showed that mouse models where the GPR43 receptor was exclusively removed from CD8+ T cells also lacked their improved tumor fighting ability. Finally, the team also worked with Justin Kline, MD, Professor of Medicine at UChicago, to analyze blood samples taken from patients undergoing CAR-T cell immunotherapy treatment for lymphoma. They saw that patients with higher levels of TVA tended to respond to treatment better than those with lower levels. They also tested cell lines from leukemia by working with Wendy Stock, MD, the Anjuli Seth Nayak Professor of Medicine, and saw that TVA enhanced the ability of an immunotherapy drug to kill leukemia cells. Focus on the nutrients, not the food The study suggests that TVA could be used as a dietary supplement to help various T cell-based cancer treatments, although Chen points out that it is important to determine the optimized amount of the nutrient itself, not the food source. There is a growing body of evidence about the detrimental health effects of consuming too much red meat and dairy, so this study shouldn’t be taken as an excuse to eat more cheeseburgers and pizza; rather, it indicates that nutrient supplements such as TVA could be used to promote T cell activity. Chen thinks there may be other nutrients that can do the same. “There is early data showing that other fatty acids from plants signal through a similar receptor, so we believe there is a high possibility that nutrients from plants can do the same thing by activating the CREB pathway as well,” he said. The new research also highlights the promise of this “metabolomic” approach to understanding how the building blocks of diet affect our health. Chen said his team hopes to build a comprehensive library of nutrients circulating in the blood to understand their impact on immunity and other biological processes like aging. “After millions of years of evolution, there are only a couple hundred metabolites derived from food that end up circulating in the blood, so that means they could have some importance in our biology,” Chen said. “To see that a single nutrient like TVA has a very targeted mechanism on a targeted immune cell type, with a very profound physiological response at the whole organism level—I find that really amazing and intriguing.” The study, “Trans-vaccenic acid reprograms CD8+ T cells and anti-tumor immunity,” was supported by the National Institutes of Health (grants CA140515, CA174786, CA276568, 1375 HG006827, K99ES034084), a UChicago Biological Sciences Division Pilot Project Award, the Ludwig Center at UChicago, the Sigal Fellowship in Immuno-oncology, the Margaret E. Early Medical Research Trust, the AASLD Foundation a Harborview Foundation Gift Fund, and the Howard Hughes Medical Institute.",
    "commentLink": "https://news.ycombinator.com/item?id=38433563",
    "commentBody": "Nutrient found in beef and dairy improves immune response to cancer in miceHacker NewspastloginNutrient found in beef and dairy improves immune response to cancer in mice (uchicago.edu) 278 points by elorant 18 hours ago| hidepastfavorite187 comments odyssey7 17 hours agoA key excerpt, in my opinion.“There is a growing body of evidence about the detrimental health effects of consuming too much red meat and dairy, so this study shouldn’t be taken as an excuse to eat more cheeseburgers and pizza; rather, it indicates that nutrient supplements such as TVA could be used to promote T cell activity.”I’m also curious about this being a trans fat. I’ve heard public health campaigning that trans fats are unhealthy, but this research highlights one as a potential cancer therapy. It is apparently another case of un-nuanced popular ideas about health and nutrition not being the whole truth. reply spacephysics 16 hours agoparentThe problem with many red meat studies that show a negative effect, is typically the meat is eaten in the form of fast food cheeseburgers or in the case of dairy, pizza.Red meat alone does not cause heart disease. It’s based off the fraudulent study regarding saturated fat and heart disease, which the sugar industry paid for.More likely, is the breads used in SAD (which have very high amounts of sugar vs other countries’ bread) has a much heavier hand in heart disease.Further, the science about cholesterol and what markers mean what amount of heart disease risk isn’t even a clear science. LSL vs HDL vs triglycerides. It seems we know triglycerides elevated is not good, but it’s not clear on LDL and HDL alone (there’s also the size of the particle that matters in determining cardiovascular risk)Show me a study of a population who eats steaks, vegetables, avoids processed foods, and is athletic and I’ll show you a healthy heart.https:&#x2F;&#x2F;www.nytimes.com&#x2F;2016&#x2F;09&#x2F;13&#x2F;well&#x2F;eat&#x2F;how-the-sugar-in....https:&#x2F;&#x2F;www.npr.org&#x2F;sections&#x2F;thetwo-way&#x2F;2016&#x2F;09&#x2F;13&#x2F;493739074...https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC9794145&#x2F;#:~:tex.... reply resoluteteeth 14 hours agorootparent> It’s based off the fraudulent study regarding saturated fat and heart disease, which the sugar industry paid for.A lot of the recent research claiming that there&#x27;s no link has also been paid for by industry groups. You can&#x27;t just dismiss one and accept the other. reply maerF0x0 7 hours agorootparentThey cited sources, it&#x27;d be fair if you would too. reply p_j_w 15 hours agorootparentprevYou make it sound as though the Lipid Hypothesis has been thoroughly debunked, how many medical bodies agree with this conclusion, though? At the very least, the European Atherosclerosis Society, American College of Cardiology, and Canadian Cardiovascular Society disagree with you. reply spirit557 11 hours agorootparentOne could cite a lot of things against the lipid hypothesis and likewise the other way. Maybe just think about this from an armchair standpoint... Before 1900 heart disease was extremely rare. And what were people eating before 1900? A whole lot of beef, bacon, butter, and eggs. Saturated fat consumption per capita was basically unchanged from 1900 to 1950. But heart disease skyrocketed. Look at this data: https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;FmhBUOpagAA-9Mj?format=png&name=...Consumption of free sugars, pulverized grains, manufactured oils and cigarette smoking took off like crazy after 1900. Type 2 Diabetes was totaly unchecked and out of control. By 1960 heart disease is the #1 killer and what do they blame... Saturated fat, beef... eggs? Come on... You don&#x27;t need a million studies to scratch your head a little at the lipid hypothesis. reply p_j_w 9 hours agorootparent>Maybe just think about this from an armchair standpointWhy?>Before 1900 heart disease was extremely rare. And what were people eating before 1900? A whole lot of beef, bacon, butter, and eggs. Saturated fat consumption per capita was basically unchanged from 1900 to 1950. But heart disease skyrocketed. Look at this data: https:&#x2F;&#x2F;pbs.twimg.com&#x2F;media&#x2F;FmhBUOpagAA-9Mj?format=png&name=...Do you think the scientists and doctors that have examined this issue professionally for 8 or more hours per day over decades haven&#x27;t seen or considered this data?I am not going to \"just think about this from an armchair standpoint\" because, quite frankly, that&#x27;s stupid. I don&#x27;t know what the confounding variables in the data you show might be. I don&#x27;t know the nuances and complexities. I would guess you probably don&#x27;t either. reply maerF0x0 6 hours agorootparentprevIMO the real mistake is assuming that any survey + observational style study constitutes advice for individuals. The unfortunate truth in coaching is there&#x27;s so little broad advice that can be given, that is guaranteed to not have a slice of humanity for which it&#x27;s actually damaging. That is, for the vast majority of the advisable ways of living&#x2F;eating, there are folks out there whom if they adhered to the advice they&#x27;d actually deteriorate. Which is a big part of what medical advisors (doctors, nutritionists, integrative medicine et al) exist, and taking advice from an influencer of 1000s or millions of followers is simply irresponsible.Rather, one needs to first do the science to understand the mechanisms, and 2nd do the labwork to understand the individual.. Then apply the mechanisms which will bring that individual to better health, and iterate too because often times the advice can either go too far, or not far enough. reply eternityforest 5 hours agorootparentprevIs that age standardized? This seems to show a different curve, that does increase a ton, but then peaks and goes back down.https:&#x2F;&#x2F;amjmed.org&#x2F;trends-in-coronary-atherosclerosis-a-tale... reply graphe 15 hours agorootparentprevWhy do they matter? Do you only care about everything they agree on? If not, why are you resistant to the idea they could be wrong? reply p_j_w 15 hours agorootparent>Why do they matter?Because they are the experts in their field. They are the most qualified to not only read through all of the literature, but actually properly understand it, debate it, and come to a reasonably correct conclusion.>why are you resistant to the idea they could be wrong?I&#x27;m not resistant to the idea they could be wrong. It&#x27;s science, the whole thing is built on showing proving previous hypotheses were either wrong or somehow incomplete. However, when it comes to health, I feel I&#x27;d be better served by listening to the consensus of various medical bodies than commenters on the Internet, media outlets, or my own ability to digest the scientific literature. In mathematical terms, I think the expected value of my health outcomes where I listen only to the advice of medical bodies is better than the expected value of my health outcomes where I \"do my own research,\" in spite of the fact that medical bodies may be wrong here or there. I&#x27;d get it wrong more often. reply Phiwise_ 11 hours agorootparentLongevity scientists are mostly just experts in the field of getting duped by pension fraudsters who say what they want to hear: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blue_zone#Scientific_receptionParts of the field are in worryingly deep denial about the number and magnitude of confounding variables a moderate level of skeptical empiricism suggests could be present in their foundational research. As, again, in the specific case that user is worried you&#x27;re glossing over: The lipid hypothesis was probably developed from studies with diets also high in processed foods, simple carbohydrates, added sugar, salt, and vegetable oils, and low in organ meats and heavy-metal-free fish. Don&#x27;t we all agree these things as a staple in a diet will probably negative health outcomes on their own? How can the field be confident enough to publicly assert a model on that? Have they put the rice-fruit-and-legume diet through as rigorous a comparison to the chicken-veg-and-liver diet as they did to the McDonald&#x27;s special diet? It&#x27;d be nice to develop a culture of a straight answer of \"Yes, and here&#x27;s the confirming metaanalysis [1]\" on this question in place of this \"mind-your-seniors\" browbeating. Isn&#x27;t that what science is supposed to be about? reply p_j_w 9 hours agorootparent>Longevity scientists are mostly just experts in the field of getting duped by pension fraudsters who say what they want to hear: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blue_zone#Scientific_receptionYou&#x27;ve cited the debunking of a demographer to try to debunk the work of cardiologists. Okay?>Parts of the field are in worryingly deep denial about the number and magnitude of confounding variables a moderate level of skeptical empiricism suggests could be present in their foundational research. As, again, in the specific case that user is worried you&#x27;re glossing over: The lipid hypothesis was probably developed from studies with diets also high in processed foods, simple carbohydrates, added sugar, salt, and vegetable oils, and low in organ meats and heavy-metal-free fish. Don&#x27;t we all agree these things as a staple in a diet will probably negative health outcomes on their own? How can the field be confident enough to publicly assert a model on that? Have they put the rice-fruit-and-legume diet through as rigorous a comparison to the chicken-veg-and-liver diet as they did to the McDonald&#x27;s special diet?Once again, I, and probably the vast majority of people on HN, aren&#x27;t properly educated to have a comprehensive view of the medical literature on the matter and properly digest it all. I&#x27;m not going to participate in a debate on the merits of guidelines published and signed by thousands of medical and scientific experts from around the world, to even try and assume I could do so with a couple of hours here and there when they&#x27;re looking into these issues for 8 hours a day for years on end would be arrogant on my part.>It&#x27;d be nice to develop a culture of a straight answer of \"Yes, and here&#x27;s the confirming metaanalysis [1]\" on this question in place of this \"mind-your-seniors\" browbeating. Isn&#x27;t that what science is supposed to be about?If you want to challenge someone who might be able to give you an informed answer, your best bet is to talk to an actual expert on the matter. You can find some people in the articles below.https:&#x2F;&#x2F;academic.oup.com&#x2F;eurheartj&#x2F;article&#x2F;41&#x2F;1&#x2F;111&#x2F;5556353 https:&#x2F;&#x2F;onlinecjc.ca&#x2F;article&#x2F;S0828-282X(21)00165-3&#x2F;fulltext https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S073510971... reply spacephysics 15 hours agorootparentprevThat’s the key problem though, in the face of alternative hypothesis they fail to even acknowledge they may be wrong. Coupled with the intense lobbying reply p_j_w 14 hours agorootparent>That’s the key problem though, in the face of alternative hypothesis they fail to even acknowledge they may be wrong.That is not they key problem as you originally laid out. Your original position is that they key problem is they ARE wrong. Further, I don&#x27;t know that I&#x27;ve ever read any sort of paper or written guidelines that explicitly says \"we might be wrong here.\" Should those all be ignored for that reason? The papers you yourself cited in a comment above don&#x27;t explicitly say they may be wrong, yet you have no problem taking them as 100% established fact. This seems like a bit of a double standard and sloppy logic.>Coupled with the intense lobbyingDo you have evidence for corruption in these institutions? reply AlexandrB 14 hours agorootparentprevLobbying by whom? The beef industry certainly does a lot of lobbying itself[1].[1] https:&#x2F;&#x2F;www.opensecrets.org&#x2F;news&#x2F;2017&#x2F;03&#x2F;wheres-the-beef-mea... reply Vegenoid 14 hours agorootparentprevLobbying from the veggie industry persuading doctors to tell people to eat less meat and cheese? reply graphe 13 hours agorootparentprevAccording to whom are they considered experts? After how the government deals with COVID I don&#x27;t trust \"the medical consensus\" which is code for \"things I want you, the mainstream to agree with\". Why would they be \"experts\"? What expertise do they have? It&#x27;s not based on the best credentials, it&#x27;s a job.They&#x27;re nameless government entities. Depending if you feel knowledge always grows you may change your ideas. Waves of ignorance litter the medical field today, as hysteria was accepted as the only disease women could have: the wandering womb theory. Consensus of ignorance is still ignorance.There&#x27;s no real evidence that high red meat is bad. There&#x27;s no evidence that eating more veggies is bad. reply p_j_w 9 hours agorootparent>After how the government deals with COVID I don&#x27;t trust \"the medical consensus\" which is code for \"things I want you, the mainstream to agree with\"I can&#x27;t believe I&#x27;m about to spend time responding to someone who opens with this, but here we go.>It&#x27;s not based on the best credentials, it&#x27;s a job.Where are you getting this information? Looking at the CCS guidelines, for example, one sees that the authors are MDs or PhDs at medical schools.>They&#x27;re nameless government entities.Not a single one of the bodies I cited is a government entity. What are you even talking about? reply dalore 15 hours agorootparentprevThey also have more to lose if they are wrong. They would have to admit they pushed something wrong for so long. So they need to double down. reply p_j_w 14 hours agorootparentI could imagine this sort of thing having an impact on a scattered group here and there, but these are not static groups. Why would a member who joined 10 years ago would give a shit about doubling down for the sake of the reputation of the same group from 40 years ago? reply graphe 13 hours agorootparentLook at the food pyramid. You&#x27;re advocating for the same kind of diet that focuses on. It&#x27;s known to be outdated, wrong and unhealthy. reply p_j_w 12 hours agorootparentThat&#x27;s an example of a body of scientists changing its recommendations when there&#x27;s data available to support it, which is what GP was trying to argue wouldn&#x27;t happen because it&#x27;d make them look bad.I&#x27;m not advocating for any kind of diet, though. I&#x27;m advocating for trusting the consensus opinion of multiple independent groups of scientists to determine what the data does and does not say about what&#x27;s healthy. reply graphe 12 hours agorootparentThere is no consensus. There are different consensuses. That group has a consensus based on false evidence. Nordic countries have another consensus based on real evidence. https:&#x2F;&#x2F;nordstjernan.com&#x2F;news&#x2F;food&#x2F;4668&#x2F; reply orangepurple 14 hours agorootparentprevIt wouldn&#x27;t be the first time a reputable research group threw off an entire field for decades and millions of dollars wasted: https:&#x2F;&#x2F;www.science.org&#x2F;content&#x2F;blog-post&#x2F;faked-beta-amyloid... reply kcplate 10 hours agorootparentprev> Because they are the experts in their field. They are the most qualified to not only read through all of the literature, but actually properly understand it, debate it…Ok…point taken>..and come to a reasonably correct conclusion.Or more likely…probably will not come to the correct conclusion. Every past “accepted” or “settled” scientific consensus now known to be wrong once had its merits tested from the most qualified to not only read through all of the literature, but actually properly understand it, debate it people?In the last few years science has received a black eye because the populace was told we should blindly trust experts. We see now that experts are frequently wrong and affected by politics and money. I can weigh their opinion, but blindly trust? Not any more. reply p_j_w 9 hours agorootparent>Or more likely…probably will not come to the correct conclusion. Every past “accepted” or “settled” scientific consensus now known to be wrong once had its merits tested from the most qualified to not only read through all of the literature, but actually properly understand it, debate it people?So what? You&#x27;re quickly approaching \"Science is a liar sometimes\" territory. Has scientific consensus been wrong in the past? Yes. So what?>We see now that experts are frequently wrong and affected by politics and money.Define \"frequently,\" because I sure as shit haven&#x27;t seen them be wrong more often than not, and I REALLY haven&#x27;t seen them be wrong more often than commentators on the internet. replyodyssey7 16 hours agorootparentprevIt is interesting that this PR pre-empts the natural takeaway many would have — eating more dairy or red meat — by reductively representing the options as “cheeseburgers” and “pizza.”Checking out the research paper[1], the competing interests section declares that Chen “has patents pending on TVA and TVA derivatives.”I don’t think the underlying reasoning is incorrect, but the way it’s presented is interesting.[1] https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06749-3 reply jdkee 16 hours agorootparent\"the competing interests section declares that Chen “has patents pending on TVA and TVA derivatives.”\"That basically destroys his credibility on the topic. reply s1artibartfast 15 hours agorootparentI think it confers credibility on the topic. It is what you would expect if they actually believe it has therapeutic potential.I would be more skeptical if they made claims about benefit but we&#x27;re taking no Acton to commercialize or protect it reply hilux 15 hours agorootparentprevI think that&#x27;s going a bit far.Technology commercialization by universities is the way any academic research (potentially, eventually) reaches the public, and it&#x27;s quite normal for the original researchers to be involved in the commercialization venture, as advisers, or as CEO. They know the topic best. reply Root_Denied 14 hours agorootparentI agree - in cases like this patents are a large part of the motivation behind studying the thing in question, if they didn&#x27;t have patents they wouldn&#x27;t do the research.It&#x27;s up to peer review and replication studies to confirm any effects. Even if there weren&#x27;t any credibility issues with a piece of research this would be true. reply virtuallynathan 11 hours agorootparentprevThere&#x27;s currently a study underway that took 100 people who have been on a low carb high fat diet for, on average, over 4 years. These people had a large increase in LDL when switching to this diet. All participants had CT Angiograms, CAC scans, and other tests at T-0 and after 1 year. Some in the study had LDL levels seen in Homozygous Familial Hypercholesterolemia (>350mg&#x2F;dL).A matched control study with this population and the general population is also underway, with the initial results being presented on Dec 8th at the WCIRDCD Conference.If LDL is independently causal of atheroscelosis&#x2F;CHD, it should be seen in this population. reply bad_user 14 hours agorootparentprev> Show me a study of a population who eats steaks, vegetables, avoids processed foods, and is athletic and I’ll show you a healthy heart.You&#x27;re probably right, but only because the actual problem has always been the excess calories.Excess calories leads to energy poisoning, the cause for diabetes, with high blood glucose and high triglycerides being just energy excess that can&#x27;t be quickly burned or stored.Foods are not equal though. That saturated fat more easily leads to atherosclerosis, compared to monounsaturated fat, is a fact. So, if you eat a lot in excess, being selective matters.Also, lay off the conspiracy theories, it&#x27;s bad for your health. reply sumpit 14 hours agorootparentprevWhen I saw articles&#x2F;videos on the centenarians in Italy or Costa Rica, I noticed that they don&#x27;t shy away from dairy products or grains. They usually prepare their own food themselves, and don&#x27;t eat too much. reply Phiwise_ 9 hours agorootparentYou should know that most of the claims of unusual proportions of very long life from these communities turned out to be bunk. Many researchers spent much more effort on present-day cultural and medical study than they did on discerning historical verification, and so got their numbers inflated by retroactively-produced proof of age in communities small and disconnected enough to not make this their priority a whole hundred years ago (Okinawa), or fraudulently-adjusted age in communities with enough lack of national oversight to enable community-wide retirement fraud (Sicily).The \"Blue Zone Diet(s)\" is still probably better than the American Big Mac and Sugar Cup diet, but it&#x27;s unclear if they&#x27;re actually living longer than people on other potentially wholesome diets. reply oooyay 14 hours agorootparentprevDo I understand correctly that you&#x27;re saying processed meats are a problem but not raw meat you prepare yourself? reply ricardobeat 14 hours agorootparentThis is long known to be true, especially for cured meats. Industrial processing requires additives and preservatives that you’ll never use at home or a restaurant. reply stubish 9 hours agorootparentAnd also, the cooking method can introduce many problematic elements. It often seems that meat is not the problem, but flavour. reply dade_ 16 hours agorootparentprevI did learn something interesting about red meat travelling to South America. so the story is red meat and cancer, people that consume a lot more red meat tend to have more cancer. Brazil and Argentina are favourite examples. Except no one seems to take an account how they prepare red meat.A churrasco or asado is a full day event. It starts with wood, and they get a fire going, and this last for hours and is a big social event. In other words hanging around wood smoke, now I can picture something that’s not red meat that could cause cancer in this scenario, but that never seems to bother researchers. They got the result they wanted and publish. reply oooyay 14 hours agorootparentI can buy some of that. Burn pits in active war zones are a known source of cancer, half being in a constant vicinity of something burning and the other half being the properties of the aerosols produced by the thing being burned. That&#x27;s to say, if the wood is treated it&#x27;ll be more dangerous than seasoned wood.When a family member was diagnosed with cancer I was warned heavily about the difference between cooked meat and charred meat. Charring is linked to cancer where cooked meat is a bit more up in the air. reply heyoni 6 hours agorootparentBut is it the charred meat then or the charring process that causes cancer? reply oooyay 3 hours agorootparentIt&#x27;s the acrylamide: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Acrylamide reply potatopatch 15 hours agorootparentprevThat&#x27;s not really conclusive about anything as not all studies are done in South America..Looking into pork for example, it is recommended as an alternative because it seems to have a lower incidence of many cancer types in studies and it&#x27;s a great meat to smoke. reply djtango 15 hours agorootparentprevinteresting.Teochew people eat a lot of (from my anecdotal experience) and we don&#x27;t have the same culture of charcoal burning our meat. I wonder whether a similar comparison could work for South East vs other parts of the Chinese diaspora... reply monero-xmr 15 hours agorootparentprevI avoid grilled and smoked meat because it seems very obvious to me that infusing meat with carcinogens would be bad for you. I’ve read that colon cancer is elevated in youth and I have a hunch that the widespread grilling and smoking culture has played a part. reply bsder 9 hours agorootparentprev> The problem with many red meat studies that show a negative effect, is typically the meat is eaten in the form of fast food cheeseburgers or in the case of dairy, pizza.Beef is also corn fed and corn finished which changes its nutritional characteristics.If you want actual grass fed and grass finished beef, that&#x27;s very rare and more expensive. reply yieldcrv 13 hours agorootparentprevI’m always amused on how thats supposed to be the smoking gun, as if some other party that didn&#x27;t care would ever allocate time and money into the researchno conflict = no interestif the study is fraudulent due to methods then just stick with that, science is supposed to be reproducible so that nobody has to care about who did the hypothesis and conclusion at all reply caesil 16 hours agoparentprev\"I’ve heard public health campaigning that trans fats are unhealthy, but this research highlights one as a potential cancer therapy.\"Something can help fight cancer and also be terrible for heart health. The body is a collection of complex systems. reply belly_joe 15 hours agorootparentYeah my understanding is that the balance of evidence on moderate alcohol consumption works this way as well (though with cancer and heart health reversed). reply aplusbi 15 hours agorootparentIt was my understanding that many of the studies on moderate alcohol consumption included people in the study who could not drink for medical reasons (or were former alcoholics) which skewed the results for the zero-alcohol groups.Studies on red wine included people whose only source of fruit was red wine, which similarly skewed the results. reply hanniabu 14 hours agorootparentprevStudies say ray guns are great at fighting cancer, but terrile for heart health reply mymacbook 16 hours agoparentprevMaybe the problem with cheeseburgers is the bun and sugar more than the meat or cheese itself? I would imagine grass-fed (and finished) beef is better than ultra processed meat – especially if sugar ends up in the finished product! reply bluGill 17 hours agoparentprevAre all trans fats equal? We are more and more learning that broad categories are not useful as some subsets are better&#x2F;worse than others. reply Modified3019 14 hours agorootparentBroadly speaking, I’d say the evidence decently points to yes most trans polyunsaturated fats are probably harmful. But I definitely agree, that lumping various fatty acids, amino acids and carbohydrates into big groups hides their extreme individual complexity.One possible exception to the “trans fat bad” thing for example is that when I last looked into it several years ago, Conjugated Linoleic Acid (“CLA”) which can be either cis or trans were generally regarded as probably beneficial.Though frankly I’ve no idea how that would be. Regular linoleic acid itself is considered harmful (at least in the historically excessive amounts we are currently exposed to). It suppresses mammalian metabolism (being required by hibernators to induce the start of their winter torpor cycling), is very prone to oxidizing and generating free radicals and lipid peroxides which cause havoc in lipoproteins and blast holes in cardiovascular tissue, and even without all that it’s breakdown products tend to increase inflammation, though this can be paradoxically hidden by linoleic acid’s ability to suppress the immune system. But somehow when you conjugate three of the things together and get CLA, it behaves differently and can seemingly scavenge for free radicals. reply nkozyra 14 hours agorootparentUnless I&#x27;m misremembering a lot of the benefits of CLA did not prove out with subsequent studies and it always came with general health caveats alongside the potential benefits (primarily to visceral abdominal fat, IIRC). reply some_furry 17 hours agorootparentprevIt could also be a category error: Vegetable seed oils seem to correlate strongly with bad health outcomes. Meanwhile, other oils (e.g. olive oil) with similar levels of the same types of fatty acids have better outcomes.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7kGnfXXIKZMIs it the type of fat, or the process through which it&#x27;s extracted, that&#x27;s most important? I don&#x27;t know if the answer is as clear as we&#x27;d like it to be.Going back to the article: Isn&#x27;t it possible that the trans fat in question is actually toxic like other trans fats, but in a way that&#x27;s narrowly beneficial in this circumstance? reply hombre_fatal 17 hours agorootparent> Vegetable seed oils seem to correlate strongly with bad health outcomes.Based on what evidence? Here&#x27;s a good roundup of our best evidence, and it shows that seed oils improve health outcomes: https:&#x2F;&#x2F;www.the-nutrivore.com&#x2F;post&#x2F;a-comprehensive-rebuttal-...Since you like Youtube videos, here&#x27;s a good talk between Gil Carvalho and Simon Hill that also goes through the evidence: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_ri4S7MLu3AAnother summary of the evidence on seed oils by Layne Norton: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=L2fSaFnt0FMI skimmed your video and it&#x27;s the same old low-carb narratives with very little evidence cited which should always be a red flag. People who can&#x27;t point to our best metaanalyses or human health outcome research on something to back their point will generally spin narratives because it&#x27;s all they have left to do. reply cubefox 16 hours agorootparentWhen vegetable&#x2F;seed oils are turned into solid fats, like shortening or margarine, trans fats (in particular, partially hydrogenated oils) occur. They have only recently been restricted in the US, at least to some extent. They cause heart disease.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trans_fat#Health_risks> Partially hydrogenated vegetable oils were an increasingly significant part of the human diet for about 100 years, especially after 1950 as processed food rose in popularity.[66] The deleterious effects of trans fat consumption are scientifically accepted.> The most important health risk identified for trans fat consumption is an elevated risk of coronary artery disease (CAD).[72] A 1994 study estimated that over 30,000 cardiac deaths per year in the U.S. are attributable to the consumption of trans fats.[30] By 2006 upper estimates of 100,000 deaths were suggested.[73] A comprehensive review of studies of trans fats published in 2006 in the New England Journal of Medicine reports a strong and reliable connection between trans fat consumption and CAD, concluding that \"On a per-calorie basis, trans fats appear to increase the risk of CAD more than any other macronutrient, conferring a substantially increased risk at low levels of consumption (1 to 3% of total energy intake)\".[60] reply azakai 16 hours agorootparentThat&#x27;s true but it&#x27;s a separate matter. Yes, vegetable and seed oils can be unhealthy if they are processed into trans fats, but otherwise they can be healthy.Whole Foods, for example, banned trans fats 20 years ago, but they sell many products with vegetable and seed oils, and those are generally healthy. reply fngjdflmdflg 13 hours agorootparentprevTwo YouTube videos and a blog has the same credibility as one YouTube video.>certain types of vegetable oil and hydrogenated oil shortened the survival of stroke-prone spontaneously hypertensive rats by decreasing platelet number, increasing hemorrhagic tendency and damaging kidney functions, which could not be accounted for by their fatty acid and phytosterol compositions.[0]The whole article is worth a read. Note this is about vegetable oils in general but the discussion at hand is about lipids from meat vs lipids from other sources.[0]Medicines and Vegetable Oils as Hidden Causes of Cardiovascular Disease and Diabetes https:&#x2F;&#x2F;karger.com&#x2F;pha&#x2F;article&#x2F;98&#x2F;3-4&#x2F;134&#x2F;272344&#x2F;Medicines-a... reply simplify 15 hours agorootparentprevIs it just me, or is the nutrivore website unbearably slow? It&#x27;s taking several seconds to render on each chunk of page scrolling, and I&#x27;m on a maxed out m2 reply RandomWorker 9 hours agoparentprevReally the problem is with the headline, which is probably all most people will see and read. The title should probably say TVA improves cancer fighting immune system. Is dairy and beef the only source ? My sense is no.https:&#x2F;&#x2F;pubchem.ncbi.nlm.nih.gov&#x2F;compound&#x2F;trans-Vaccenic-aci...Reviewing pubmed it seems to be a product from ecoli. reply Dalewyn 15 hours agoparentprevThe moral of all these tales is that the best diet is composed of a reasonable balance of varied foods that you like, not something that takes the meter to +&#x2F;-11.Too much of anything is bad, as is too little. reply tazjin 14 hours agoparentprevYes, eat less cheeseburgers and pizza, and more steak. reply hombre_fatal 17 hours agoprevThey isolated trans-vaccenic acid and gave it to mice with tumors.If this informs your dietary preferences but our best metaanalyses on human health outcomes do not, you need to check your epistemic standards. reply CobaltFire 17 hours agoprevSo I suffer from multiple autoimmune conditions and moving to a pescatarian diet (vegan + fish) has had a large moderating effect on my symptoms.I’m trying it based on some research for Neu5gc (that I first saw referenced here on HN) because… well it seemed harmless to try.This makes me wonder if this fatty acid may also&#x2F;instead be a&#x2F;the casual factor.Human bodies have so many confounding factors tracing these things is very difficult. Very happy to see more research. reply zackmorris 15 hours agoparentA I&#x27;ve gotten older, I&#x27;ve found that my overall health mostly starts in the gut. I got hit with a form of chronic fatigue syndrome from leaky gut in 2019 that was so bad that I burned out and lost the ability to work for about 6 months. A variation of the Autoimmune Protocol (AIP) diet is what worked for me: * Avoid wheat and most grains (rice and corn are fine) * Avoid milk and young cheese (cheddar is fine but YMMV) * Avoid FODMAPs like beans, other legumes and nightshades * Get back to macro nutrients like bacon & eggs for breakfast, meat & rice, etc * Get back to micro nutrients like spinach for iron and a thyroid supplement for iodine * Seek holistic approaches like ashwagandha to restore hormone balance * Build gut health with probiotics and get treated for parasites if necessary * Look into supplements like glutamine and colostrum which rebuild the gut * Decrease caffeine and stress which lower the activation threshold for nerves in the gut to purge its contents * Drink a gallon of water per day, from one large container if necessary, to accommodate ADHD&#x2F;autism symptoms * Exercise by alternating weightlifting and walking each day, about 60 minutes ideallyEach food damages the villi in the small intestine at a different rate, but the gut only regenerates twice per week. Gluten causes some of the worst damage, which remains hidden until middle age when the ability to regenerate starts to decrease.Now I believe that GMOs and processed foods are so damaging not due to stuff like preservatives, but because low cost means a higher percentage of fillers like gluten.It took 3-5 years but I&#x27;m able to eat FODMAPs again, tolerate milk every day and even eat wheat a couple of times per week. I consider myself to be in remission but must be mindful of what I eat.I&#x27;m also hopeful that long COVID studies will reveal some of the disease mechanisms at work in these types of undiagnosable conditions. Gaslighting (not believing patients as they describe symptoms) is still a huge problem in western medicine. reply datameta 14 hours agorootparentSome thoughts. About the bacon, try to avoid frying it too hard. In general heavily fried food contain acyclic aromatic hydrocarbons which are inflammatory and carcinogenic through increased oxidative stress. With that said, I very lightly fry my eggs and don&#x27;t feel negative effects. With supplements like Ashwaghanda and St. Johns Wort make sure to check that they don&#x27;t have an interaction with medication - these two in particular tend to interact widely. Indeed proper water intake is hugely important. And as for walking, even 7000 steps per day can be considered light-medium exercise. reply zackmorris 9 hours agorootparentThanks, good points! I&#x27;ve been migrating away from pork on my journey towards vegetarianism, but it really helped me in the beginning. It has something like tryptophan that works as an antidepressant for me. Also good advice on supplements, please talk to a doctor before taking any besides a daily multivitamin. It&#x27;s contraindicated for stuff like multiple sclerosis where a stronger immune system just causes more damage. reply temeritatis 14 hours agorootparentprevLooks like a solid approach. I&#x27;ve mostly gravitated in the same direction after my last \"mystery disease\". I would add yoga&#x2F;meditation and mindfulness to that list as well as the importance of sleep. Sleep seem to impact me more than anything else. I used to be up all night working and coding. I like the night. But it seems more and more obvious to me how bad this is for my wellbeing. That, and also to get enough natural sunlight during the day, but this is also intimately linked with sleep of course. reply datameta 14 hours agorootparentMindfulness meditation is hugely important. The amount of gain from 10 minutes a day is clearly noticeable after a week or two. I personally often feel better right after do it (even though that isn&#x27;t expressly the goal). reply mentos 16 hours agoparentprevFor what it’s worth I started developing auto immune symptoms after my exposure to Covid (nerve pain in my face, legs, feet, inner ear was burning and had a lack of balance, fatigue, trembling) only thing that resolved it was exercise (moderate to heavy weight lifting) and a strict carnivore diet over 3 months. I’d probably put more stock into the exercise but my working theory was low testosterone and poor diet was preventing me from recovering.3 years later and if I go a few months without exercise the nerve pain starts to come back in my face heh reply helsinkiandrew 18 hours agoprevInterestingly also in Sea Buckthorn: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Vaccenic_acid reply tuukkah 17 hours agoparent> Its stereoisomer, cis-vaccenic acid, is found in Sea Buckthorn (Hippophae rhamnoides) oil.An isomer (different configuration of atoms) can have totally different biochemical effects though: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stereoisomerism reply speedylight 17 hours agoprevMy body must have exceptional anti cancer properties considering the sheer quantity of cheese I consume daily. reply cies 16 hours agoparentFrom the article:> There is a growing body of evidence about the detrimental health effects of consuming too much red meat and dairy, so this study shouldn’t be taken as an excuse to eat more cheeseburgers and pizza; rather, it indicates that nutrient supplements such as TVA could be used to promote T cell activity. reply tremon 16 hours agorootparent>> \"Considering the sheer quantity of cheese I consume daily, I should have gotten cancer many times over now. Therefore, my body must have exceptional anti-cancer properties.\" reply iteratethis 11 hours agoprevCut sugar, sauce and highly processed food.I know they have a fatal attraction, quite literally. But if you manage to get rid of all that junk, you can pretty much eat what you want from the selection of real foods like whole wheat bread, fruits and vegetables, eggs, meats. Mix and match a reasonable balance between those, and you strike a good balance.But to start with: sugar is your enemy. reply wtcactus 1 hour agoparentI would say that&#x27;s more cultural that you state.I absolutely have no attraction for processed food or sauce (well, except for olive oil, which can be considered a sauce I guess).Sugar, well, I do enjoy some home made cakes, but little more.My father is the same, I&#x27;m quite sure he never ate an hamburger in his life. He does love the sweets though.Anecdotally, he is 73 and has absolutely no health issues, doesn&#x27;t take any medicine at all.Also, we do eat a lot of fish and meat here. Every single meal has either one of those as the main food. reply progne 18 hours agoprevnext [–]Immune function analysis displayed significantly lower phagocytosis of monocytes and granulocytes in an older vegetarian population (vs. older nonvegetarians (PIt is a multivariate situation [...] It could well be that it is not diary or meat but the amount of glucose&#x2F;fructose one eats which really affects this.Your missing puzzle piece is the fact that we do multivariate adjustment models to adjust for cofounders.You might as well ask \"How do we know whether it&#x27;s the cigarettes or the fact that people who don&#x27;t smoke also tend to exercise and eat vegetables? I guess we can never know!\" reply scythe 16 hours agorootparentIt is an epistemic error to say \"we did something about X, therefore X is not a problem\". There are always characteristics of a population that aren&#x27;t modeled. This is why randomization is considered so important. reply ryuhhnn 18 hours agoparentprevThere might be an impact, but it’s not clear what the impact is. I’m no biologist, but I’d wager your volume of immune cells changes quite a bit over your lifetime. I’d be more interested in seeing the long-term health outcomes of vegetarians vs. non-vegetarians. reply mtalantikite 17 hours agoparentprevThis comment is from a completely different paper and is looking at various blood count levels of 174 women. The submitted paper seems more interesting and is investigating TVA in particular as a potential supplement to complement other cancer treatments. reply metabagel 16 hours agoprevI suspect that the way the body processes food is modified significantly by exercise. So, it&#x27;s not the food you are eating, but rather it&#x27;s the food you are eating combined with how active or sedentary your lifestyle is. I think that exercise has a marked effect on the gut&#x27;s microbiome. reply atlgator 14 hours agoprevIt&#x27;s shocking how little we know about what is \"good\" and what is \"bad\" for the body. There are too many confounding factors around meat, such as using processed meats and meats cooked with seed oils.For me, eating as clean as possible is the best approach. Find meat that isn&#x27;t loaded up with hormones, antibiotics, or mold toxins. Find non-GMO vegetables that aren&#x27;t loaded with pesticides. Limit carbs and dairy as much as possible. No stimulants, barbiturates, or other drugs. reply shuntress 14 hours agoparent\"It&#x27;s shocking how little we know about how food affects the body. That&#x27;s why I follow this trendy diet based on assumptions about how food affects the body.\" reply atlgator 12 hours agorootparentIn the last 70 years, the human diet has become polluted with chemicals and shifted to refined carbs, concurrent with the rise of heart disease and diabetes. And I&#x27;m the one following the \"trend\" by eliminating those things? Not sure you know what that word means. reply sethammons 12 hours agorootparentprevhow is that a trendy diet? It looks like what we assume people have mostly eaten for the last 100k years. reply huytersd 14 hours agoparentprevThe only way to actually know is with “dispensable” humans and long term studies. That’s not ethical and never going to happen so I guess we’ll never know. reply whoiscroberts 7 hours agoprevI stopped eating most processed foods and now mostly consume beef in the form of steaks, chicken and vegetables. Never felt better. I also stopped eating mass amounts of sugar and carbs. reply jnellis 14 hours agoprevReports like this always remind me of that scene in Sleeper.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=485Em2JF34M reply idlewords 17 hours agoprev*In mice reply dang 11 hours agoparentGrr, another one. We&#x27;ve inmiced the title above. Thanks! reply idlewords 1 hour agorootparentI&#x27;m sure someone here would be willing to whip up an automated inmicer script. It might be a fun little project. reply codingdave 17 hours agoprevWith all the information available and research about nutrition really impacts our lives... I find myself wondering if there really is a benefit for a layman like myself to keeping up, or if the overall best advice still sits at: \"Eat food. Not too much. Mostly plants.\" reply krunck 17 hours agoprev(Nutrient in beef) ≠ beef reply Phiwise_ 9 hours agoparentThe cynic in me says that this is more correct as a statement about the author&#x27;s patent possesion than it is about the items themselves. reply capitainenemo 13 hours agoprevHuh. From reading the article it doesn&#x27;t seem like a nutrient and more a signaling thing (activates a receptor on the cell&#x27;s surface).Could this be a hormesis effect? For example, the body reacts to things like exercise or UV by amping up repair and immune system. Perhaps the body is amping up its immune system in response to consuming meat&#x2F;milk due to carcinogen or disease risks? reply teslabox 14 hours agoprevThe current top comment [0] on this submission said he was \"curious about this being a trans fat\". This would be a result of biohydrogenation, where the bacteria in the ruminants stomachs will add hydrogen to the unsaturated oils.First link for biohydrogenation: https:&#x2F;&#x2F;www.megalac.com&#x2F;resources-advice&#x2F;fats-advice&#x2F;71-bioh...Biohydrogenation is a totally different process from the hydrogenation used to make margarine from seed oils.Ruminal microbe of biohydrogenation of trans-vaccenic acid to stearic acid in vitro - https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3305423&#x2F;[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38434521 reply roschdal 14 hours agoprevMilk cures cancer. reply EGreg 15 hours agoprevDon’t vegetarians live far longer and meat cooking brings carcinogens? Also meat of older animals ages you more quickly reply sundarurfriend 15 hours agoparentI&#x27;d love to hear what sources these claims are from. I&#x27;m vegetarian, and every time I&#x27;ve come across these \"vegetarians live longer\" claims it&#x27;s from dubious sources that cite random tiny studies in isolation.I could believe modern factory farming, preservation, and mass-preparation techniques make meat much less healthy (than it used to be in our ancestral days), but a general \"meat cooking brings carcinogens [to a level that matters, beyond plant cooking does]\" sounds pretty unlikely too. reply shinryuu 15 hours agorootparentThe most famous study is the seventh adventist study. The study in question is not tiny. About 70k people participated.https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4191896&#x2F;https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S000291652... reply HNDen21 14 hours agorootparentThey also don&#x27;t smoke or drink.. I am sure that chops off a few years reply hanniabu 14 hours agorootparentprevNeed to control for healthy habits also associated with healthier diets, such as exercising, not smoking, etc. It should also eliminate any participants with clearly unhealthy habits like poor diets consisting of fast food and living extremely sedentary lifestyles (different from just not exercising). reply hilux 15 hours agorootparentprevThere is no large long-term double-blind controlled study comparing a vegetarian&#x2F;vegan diet to some omnivore diet - and there never will be.The only diet studies are observational, and they are unreliable in all kinds of ways. The famous Blue Zones book has about the validity of Rich Dad Poor Dad or The Millionaire Next Door. But the book continues to sell, and at a macro level, its advice is not terrible, but it&#x27;s also not \"the only way.\"Surprisingly (to me), there is some evidence that some people are genetically better disposed to eating a vegetarian diet than others. reply fithisux 17 hours agoprevIf it works, we will see it eventually used. reply TheRealPomax 17 hours agoprevLike almost all of these publications, for HN purposes this title really needs \"in mice\" explicitly added to the end, because there is no reason for a Nature publication to put it in, but a lot of folks will never read the paper. reply _ink_ 18 hours agoprev“There is early data showing that other fatty acids from plants signal through a similar receptor, so we believe there is a high possibility that nutrients from plants can do the same thing by activating the CREB pathway as well,” he said. reply pkd 17 hours agoparentAlso important to point out given the headline:\"Focus on the nutrients, not the foodThe study suggests that TVA could be used as a dietary supplement to help various T cell-based cancer treatments, although Chen points out that it is important to determine the optimized amount of the nutrient itself, not the food source. There is a growing body of evidence about the detrimental health effects of consuming too much red meat and dairy, so this study shouldn’t be taken as an excuse to eat more cheeseburgers and pizza; rather, it indicates that nutrient supplements such as TVA could be used to promote T cell activity. Chen thinks there may be other nutrients that can do the same.\" reply cnity 17 hours agorootparentIt&#x27;s an interesting and weirdly loaded angle to suggest that the only red meat foods are unhealthy (by picking unhealthy foods as examples). Consider this alternative phrasing:> this study shouldn’t be taken as an excuse to eat more venison, cabbage, and mash.Is it really the best thing to be finding the correct soup of isolated nutrients to take in a multi-pill, as opposed to admitting that perhaps it can be healthy to include some red meat sources in your diet? Cutting out red meat is clearly not some panacea. reply herunan 17 hours agoparentprevSo eating food improves immune response to cancer. Next. reply szundi 17 hours agorootparentWhy the downvote? Correct observation reply Buttons840 17 hours agorootparentBecause the \"next\" meme is the antithesis of what HN strives to be.For example, fasting might help fight cancer, so there&#x27;s obviously more nuance than just \"eating fights cancer, next!\". reply jader201 16 hours agorootparentI still think the point of the “next” comment is that we didn’t really learn anything — except that there’s more to learn. To your point, way too much nuance here, and it sounds like the title — and maybe article — is clickbait. reply em500 17 hours agorootparentprevThe comment appears at odds with several items in HN guidelines, like- Be kind. Don&#x27;t be snarky. Converse curiously; don&#x27;t cross-examine. Edit out swipes.- Comments should get more thoughtful and substantive, not less, as a topic gets more divisive.- Please don&#x27;t sneer, including at the rest of the community.- Please don&#x27;t post shallow dismissals, especially of other people&#x27;s work. A good critical comment teaches us something. reply AnimalMuppet 17 hours agorootparentprevBecause this identified a specific nutrient, and a specific pathway for its action. The \"eating food improves immune response\" is an absurd characterization. reply beebeepka 17 hours agorootparentI think it was just a short version of \"eating both kinds of food\" replyjwmoz 17 hours agoprevWhat&#x27;s sad is I don&#x27;t believe any of these research papers anymore. Both ways can be found good or bad. It&#x27;s impossible to deal with. reply swatcoder 17 hours agoparentThe mistake is trying to to scry personal life guidance out of research papers. Research papers just describe experimental results, and sometimes unsound ones.They&#x27;re meant to feed further research and bring prestige to the authors, not to help intellectuals hyperoptimize their longevity or whatever.It&#x27;s a widespread mistake, especially among intellectuals who take pride in being able to read jargon, but what you&#x27;re calling \"impossible to deal with\" is actually just the outcome of trying to use things in a way they&#x27;re not meant to be used.If you really want guidance (you actually don&#x27;t need much), either look to traditions if you&#x27;re comfortable with that or turn to institutional guidelines if you want something that feels more evidence-based and modern. Neither of those is unassailable either, but at least then you&#x27;re using things as intended. You&#x27;ll have less grief and frustration that way.Which is probably good for you. reply sundarurfriend 15 hours agorootparent> turn to institutional guidelines if you want something that feels more evidence-based and modern.\"feels\" is the right word here, unfortunately. Often, these are heavily out of date, influenced by power struggles inside the institutions and by external incentives, and get watered down by the need for generality and for avoiding liability. So the average person is left with very little to go by, which is why people either flock to study after study, choose \"bro science\" instead, or just stick to traditions even in places where they&#x27;re not helpful. reply agumonkey 13 hours agorootparentprevthere&#x27;s a strange phenomenon when you try to dig deeper than current accepted textbooks and read pubmed, you end up in less certainty. Science behave like a lens, validated data (yet about to be obsolete potentially) is in focus, illogical is fuzzy, but unproven is fuzzy too. reply smrtinsert 17 hours agorootparentprevUpvoted to raise awareness. The average DYOR type is lacking a lot of context with regard to research goals and process. There aren&#x27;t absolutes, there are snapshots or milestones of understanding given specific criteria - at best. reply bratwurst3000 16 hours agorootparentprevThis comment sums up so many problems with the interpretation of science so good. Thanks reply rubslopes 17 hours agoparentprevRelevant chart: https:&#x2F;&#x2F;fivethirtyeight.com&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;01&#x2F;foodp...From: https:&#x2F;&#x2F;fivethirtyeight.com&#x2F;features&#x2F;you-cant-trust-what-you... reply airstrike 17 hours agorootparentNah. \"Beef\" is too broad a category. There are lean cuts and high fat cuts, there are many methods of preparation and countless ways to season. Someone eating maple glazed brisket with a side of extra crispy bacon and buttered scrambled eggs is very different from someone else eating a grilled sirloin seasoned with salt and black pepper.This crusade against red meat is honestly tiring.I subscribe to \"eat a little bit of everything in moderation. Mostly what&#x27;s available in nature and what someone in a farm or at home could produce. Be mindful of the total number of calories. Try not to burn or overcook food in general. Exercise a little bit most days. Don&#x27;t neglect sleep.\"(Which to me reads like a lot of common sense advice that my grandparents already knew and followed with no struggle) reply soco 17 hours agorootparentI believe the crusade against red meat comes mostly from the environmental footprint which is pretty much the same for sirloin and brisket, and significant even for industrial grown cattle. reply kjkjadksj 16 hours agorootparentIf that were truly the case the crusaders would be trying to stymie the meat industry on environmental legislative grounds. Instead all we get is some abstracted sense of blame at the consumer level, no real action, no real teeth, no real change to industry or society. reply hombre_fatal 16 hours agorootparentHow do you suggest people change a massive, incumbent, dominant animal ag industry married to the government (e.g. billions of subsidies per year, ag-gag laws) without changing personal attitudes about animal ag and consumer level behavior? And why can&#x27;t they do both at once?Animal-free products from nut milks to meat alternatives are a growing industry, so clearly something is working without having to somehow do the impossible task of changing the government and doing it independent of popular opinion. reply jononomo 15 hours agorootparentBeef is subsidized in the US? That is definitely news to me. replyncann 17 hours agorootparentprevSomeone made a site for \"cure or cause cancer\" before, too bad it&#x27;s not available anymorehttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=1495213 reply brigadier132 16 hours agorootparentprevThis just looks like a chart of calorie density. How much of this can be explained by increased bmi as a result of over consumption of these foods? reply wyager 17 hours agorootparentprevThe output of nutrition science (using the word \"science\" loosely - the epistemic standard is not very high) must be examined with the understanding that the field is extraordinarily biased by religious and political considerations, e.g. https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;327179700_The_Globa...In particular, there are a number of political and religious factions opposed especially to red meat. reply INTPenis 17 hours agoparentprevYeah I stopped paying attention to cancer research. Someone should aggregate all the cancer headlines just to show people how many of them there are.I lost my SO 6 years ago to a weird form of cancer that fed on her estrogen, so doctors tried to put her into menopause to stop its growth. That&#x27;s hell on earth, like taking away the soul of a woman.So for years every single cancer headline made me react, but after a while you get jaded because you never see them lead anywhere. reply apwell23 17 hours agorootparent> weird form of cancer that fed on her estrogen,my father is currently dealing with male equivalent of this (prostate cancer) . Yea taking away a harmone is a slow and painful march to death. I don&#x27;t wish this on anyone. reply instagib 15 hours agorootparentAnd doesn’t work all the time either. So you’re left with worse choices. reply armchairhacker 15 hours agoparentprevThe main things you need to know:- \"Eat whole food, not too much, (maybe) mostly plants\". Do some form of exercise for some time at least 5&#x2F;7 days.- Listen to your body and do what makes you feel best (e.g. types of foods, meal timing and # of meals per day, types of exercises and amounts). I really believe that people&#x27;s bodies work differently enough that even excluding extreme cases, no one diet is best for everyone- Genetics, some people are just unlucky or luckyAlso for cancer, 99% of things have increase your risk by some amount; including every food, because simply metabolizing causes chemical reactions which (when unlucky) cause cancer. The important things to watch out for are those which carry a very noticeable, substantial increase, like benzene and radiation; studies show that red meat and nitrites correlate with increased risk, but it&#x27;s ambiguous how significant they are especially with the other correlated factors (like bad eating in general). reply jrm4 16 hours agoparentprevNot impossible, just a potential \"lower quality of information?\"I can&#x27;t speak for anyone else, but I&#x27;ve grown to understand the following:My grandma, with whom I share a bunch of DNA, lived to be a healthy 100 and drank milk to the very end; which is why I&#x27;m probably going to do the same. The quality of that \"datapoint\" for me is (almost objectively) much higher than ANY study could ever produce? reply willsmith72 17 hours agoparentprevto me that&#x27;s encouraging. science has never been black and white.the mixed results can be a sign that we don&#x27;t understand something properly. there&#x27;s nuance and depth adding noise to the resultsit will only get better with more peer-reviewed studies and analysis reply wyager 17 hours agorootparentThere&#x27;s a difference between \"not black and white\" and \"replication crisis\". Nutrition studies suffer from a replication crisis. reply amanaplanacanal 17 hours agorootparentQuite a bit of nutrition research is simply useless. They ask a bunch of people what they ate and then try to correlate that with health outcomes. You will never learn anything that way, though it might give you areas for further research.The next step up is a randomized controlled trial, but instead of choosing health outcomes they choose measurements of lab tests as their endpoints. Someone’s cholesterol went up or down by some small number. What did we learn? Nothing.Unfortunately good research is outrageously expensive, so it doesn’t get done. reply szundi 17 hours agorootparentprev100 more years and we know more. Under we I mean someone else. reply yjftsjthsd-h 17 hours agorootparentprevMixed results can also be evidence that everything is fraudulent and nobody is even pretending to try and replicate things. And as a layperson, even honest noise isn&#x27;t useful. reply traviswingo 17 hours agoparentprevThat’s simply because confirmation bias heavily influences perspective. If I want something to be true, I wouldn’t be hard pressed to find research or conduct my own that could make a solid argument for it. reply bluGill 17 hours agoparentprevOften context matters. Something can be good for one cancer and bad for another. Or be good for cancer but bad for heart disease. Or bad for cancer but good for heart disease. Something may cause cancer if you force feed far more than a human could reasonably consume, but harmless at normal quantities.which is why so many of us just say \"moderation and eat a varied diet\". Once in a while science settles on something that is true for everyone, but real science is hard. reply mcpackieh 15 hours agoparentprevI&#x27;ve given up on &#x27;scientific&#x27; nutritional advice and now go with my intuition informed by my personal observations. What I have observed is that people who eat lots of carbs seem to get fat, people who eat lots of processed foods look terrible all around, people who are vegan tend to look like cancer patients, people who are vegetarian but also eat eggs and&#x2F;or fish look reasonably healthy, and people who eat lots of roasted meat, vegetables and eggs seem to have the best outcomes. reply wappieslurkz 10 hours agorootparentSo all the (well known) people in this Wikipedia list \"tend to\" look like cancer patients to you?https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_vegans reply jononomo 15 hours agorootparentprevThis has also been my observation. I am convinced at this point that a diet that is 95% beef, fish, and eggs is the optimal diet for humans. reply 1270018080 17 hours agoparentprevMy least favorite genre is the 100 papers saying X is bad to eat and 100 saying X is good to eat. Just have common sense and eat everything in moderation. Food absolutists are weird&#x2F;annoying. Especially the trad-fascist \"seed oils and soy bad, red meat good\" crowd. Just a bunch of dorks. reply amanaplanacanal 16 hours agorootparentThis might sound like the naturalistic fallacy, but if people have been eating things in quantity for hundreds of thousands of years, the worst outcomes have probably been weeded out of the gene pool.If it’s new in the last 10,000 years (the beginning of agriculture) we have at least some hope that the worst problems have been dealt with, although we do have lactose and gluten intolerance as counter examples.If it’s new to the human diet in the last hundred years, maybe it’s fine, and maybe it isn’t. It has to pretty obviously kill you for people to notice.Animals, fruit, and root vegetables have been in the human diet since before humans left Africa. That’s my rule of thumb. reply Vegenoid 14 hours agorootparentYes, but people are often interested in nutrition not to avoid &#x27;worst outcomes&#x27;, but to reduce risk factors as they age. Evolution isn&#x27;t so good at selecting for diets that push the median life expectancy from 75 to 85. reply im3w1l 15 hours agorootparentprevAgriculture may have existed for 10k years or something, but it&#x27;s not like everyone switched over to that lifestyle immediately. There have also been peoples that lived as nomadic herders and even hunter gatherers into even modern to almost-modern times. reply amanaplanacanal 14 hours agorootparentYes. My ancestry is mostly British isles, and it looks like agriculture took hold there only about 6000 years ago. Possibly arrived with the Indo-Europeans? reply jdminhbg 15 hours agorootparentprev> eat everything in moderationThe entire point of diet advice and nutritional science is trying to find out what \"in moderation\" means for different foods. The amount of donuts that is moderate and the amount of broccoli that is moderate are not equal. reply fsflover 15 hours agoparentprevAFAIK research papers are for researchers. For ordinary people, there is, e.g., Harvard School of Public Health. reply moose44 16 hours agoprevThis comment section is a perfect example of how little we know about health. reply swatcoder 16 hours agoparentAlternately, it&#x27;s example of how absurdly some obsess over it.Excepting bad luck and pretty-obvious excess, almost everybody will have about 6-7 decades of adulthood and maybe 1-2 decades of slowing down and feeling less capable in the last of those. If you can come to terms with that, and many do, there&#x27;s not a lot to worry about.All the obsessing people get up to here is a kind of \"premature optimization\" that often sends people flailing around between research trends without any of the rigor or continuity that might have yeilded benefit anyway. It&#x27;s a lot of time, effort, and mental energy stolen from the most vibrant and impactful stage of life in questionable hope of squeezing a teensy bit more out of the least. reply grecy 16 hours agoparentprev\"Eat food, mostly plants, not too much.\" reply sp3000 15 hours agorootparentFrom Dr. Robert Lustig in his book, Metabolical (highly recommended):\"Michael Pollan (full disclosure, he’s a friend), in his now-famous New York Times Magazine article, espoused seven simple words: Eat food, not too much, mostly plants. Three separate clauses, but I think that each clause is misleading. Eat food doesn’t take into account that some people may do better on a low-fat diet, while others may do better on a high-fat diet. Not too much doesn’t say how you are supposed to moderate that, as it doesn’t take into account food addiction or what generates satiety. And mostly plants doesn’t take into account that Coke, French fries, and Doritos are all plant-based. If you buy your organic, all-natural, GMO-free tortilla chips at Whole Foods, you’re still stuffing your liver and starving your gut—you’re just paying more for the privilege.\" reply grecy 14 hours agorootparent> doesn’t take into account that Coke, French fries, and Doritos are all plant-basedBy the very definition Pollan gives, none of those are food, they are lab concoctions that have been invented in the last few generations. Like the advice given about McDonalds by all the doctors and nutritionists in Super Size Me, a healthy person should never eat them.When Pollan says \"eat food\", he means \"real\" things that exist naturally. [Note 1]He also introduces the \"grandma test\" which is to say you shouldn&#x27;t eat anything your grandma wouldn&#x27;t recognize as food. A glass of black fizzy liquid sugar? no. Drink water.[Note 1] I often think about this like the periodic table. Many of the elements that have been on the table forever (low atomic numbers) can be found just lying around on the ground (Gold, Copper, etc.). They are \"real\".Almost all the ones added in the last few decades (nihonium, moscovium, tennessine) (high atomic numbers) never exist naturally and must be concocted in a lab under extremely specific circumstances. Often they only exist for a fraction of a second. They are \"fake\".Can you find an Apple, spinach, meat or fish out in nature? Yes - that&#x27;s \"real\" food.Can you find Doritos or coke out in nature? No - that&#x27;s not food. reply helicalmix 15 hours agorootparentprevThis feels like an overly complex response to a statement that&#x27;s intentionally simple though.The whole point of Pollan&#x27;s statement is to have simple guidelines that ignore optimizations in favor of directionally good flexibility, which makes me feel like Lustig either misses Pollan&#x27;s point, or just believes in a completely different philosophy. reply jononomo 15 hours agorootparentprevI have the opposite view. Eat food, entirely beef, as much as you&#x27;d like. reply grecy 14 hours agorootparent> entirely beef, as much as you&#x27;d like.That is a very good way to eat thousands of calories per day in excess of base metabolic need, which for nearly everyone will result in significant weight gain. reply helicalmix 12 hours agorootparentwhich is good, if you want to gain weight reply jononomo 10 hours agorootparentprevSpoken like someone who thinks theoretically but without any practical experience with the diet!What actually happens is that you get satiated a LOT faster because beef is so nutrient dense, so you end up eating fewer calories and most people only eat two meals a day because they just stay satiated for so long. You also never experience the drowsiness that normally comes after eating.This is a zero carb diet which is high in protein, so you&#x27;ll find yourself effortlessly putting on muscle and losing belly fat.When I say \"entirely beef\", that means \"you can&#x27;t put anything in your mouth that is not literally part of a cow\" -- so that means no junk food, no sugar, no alcohol, not calories through drinking anything. It is the ultimate elimination diet because it is nutritionally complete yet ridiculously simple.People call it \"the lion diet\" and if you actually try it you&#x27;ll end up in phenomenal shape and you&#x27;ll feel like you&#x27;re running on jet fuel. reply moose44 7 hours agorootparentAs someone who&#x27;s experimented with an animal-based diet—I can attest. reply grecy 6 hours agorootparentprevI&#x27;ve never gone full meat, but I&#x27;ve certainly eaten 5000+ clean(ish) calories a day while working out HARD and put on a significant amount of muscle. Happily eat a 28 oz steak for dinner, then 1000 calorie breakfast, then a thousand calorie shake..I have to ask, where are you getting your fibre, vitamins and minerals from?Please tell me you&#x27;re at least taking a multivitamin. replyanonu 16 hours agoprevTomorrow we&#x27;ll see an article that says: \"beef consumption tied to higher rates of cancer\". Sorry to be cynical.I&#x27;ll never forget - back when Google News was more about an amalgamation of all articles from around the web - there were 2 articles in the \"coffee\" category: \"Coffee consumption linked to cancer\" and then \"Coffee consumption linked to lower cancer\".The answer is really moderation. Michael Pollan puts it well: \"Eat food, not too much, mostly plants\". reply moffkalast 16 hours agoparentWell sometimes the solution to cancer is more cancer. Larger animals like elephants and whales don&#x27;t die from cancer because their cancer gets cancer before it manages to spread. These hypertumors then compete with and deprive the original cancer of blood flow, so they all end up starving.When you think about it we mostly cure cancer with ionizing radiation, which is ironically notorious for causing it in the first place. reply jononomo 15 hours agoprevI only eat beef and eggs. reply mcpackieh 15 hours agoparentThe Vince Gironda diet. reply jononomo 15 hours agorootparentexactly reply karol 17 hours agoprev reply rambojohnson 17 hours agoprev [–] Red meat has been demonstrated to have carcinogenic properties for quite some time. It&#x27;s uncertain whether the discovery of this obscure \"nutrient\" justifies the associated risks... reply chainwax 17 hours agoparentThis has been called into question recently.https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;36216940&#x2F;https:&#x2F;&#x2F;www.healthdata.org&#x2F;research-analysis&#x2F;library&#x2F;health-... reply brigadier132 17 hours agoparentprev [–] Those studies that link red meat to cancer are all epidemiological and causal relationships can&#x27;t be drawn from them. For example, one possible confounder could be that population of people that eat red meat also eat higher amounts of fast food or they might have higher bmi than those that don&#x27;t.Also, always note how the studies never say \"red meat causes cancer\", they say \"red meat is linked to cancer\". Linked being the key word. reply rockinghigh 15 hours agorootparent> Those studies that link red meat to cancer are all epidemiological and causal relationships can&#x27;t be drawn from them.There is an extensive body of studies that show a direct link between amines produced from exposing meat to high temperature in animals. You can disagree on the dosage used in the studies but I wouldn&#x27;t ignore them completely. Here are a few:Sugimura T, Wakabayashi K, Nakagama H, Nagao M. Heterocyclic amines: Mutagens&#x2F;carcinogens produced during cooking of meat and fish. Cancer Science 2004; 95(4):290–299.Ito N, Hasegawa R, Sano M, et al. A new colon and mammary carcinogen in cooked food, 2-amino-1-methyl-6-phenylimidazo[4,5-b]pyridine (PhIP). Carcinogenesis 1991; 12(8):1503–1506.Kato T, Ohgaki H, Hasegawa H, et al. Carcinogenicity in rats of a mutagenic compound, 2-amino-3,8-dimethylimidazo[4,5-f]quinoxaline. Carcinogenesis 1988; 9(1):71–73.Kato T, Migita H, Ohgaki H, et al. Induction of tumors in the Zymbal gland, oral cavity, colon, skin and mammary gland of F344 rats by a mutagenic compound, 2-amino-3,4-dimethylimidazo[4,5-f]quinoline. Carcinogenesis 1989; 10(3):601–603.Ohgaki H, Kusama K, Matsukura N, et al. Carcinogenicity in mice of a mutagenic compound, 2-amino-3-methylimidazo[4,5-f]quinoline, from broiled sardine, cooked beef and beef extract. Carcinogenesis 1984; 5(7):921–924.Ohgaki H, Hasegawa H, Suenaga M, et al. Induction of hepatocellular carcinoma and highly metastatic squamous cell carcinomas in the forestomach of mice by feeding 2-amino-3,4-dimethylimidazo[4,5-f]quinoline. Carcinogenesis 1986; 7(11):1889–1893.Shirai T, Sano M, Tamano S, et al. The prostate: A target for carcinogenicity of 2-amino-1-methyl-6-phenylimidazo[4,5-b]pyridine (PhIP) derived from cooked foods. Cancer Research 1997; 57(2):195–198. reply Moldoteck 17 hours agorootparentprev [–] Don&#x27;t those studies accout for such types of influence? reply mdhen 16 hours agorootparentThey try, but not successfully. It is called the healthy user effect. If something is already considered unhealthy by conventional wisdom then people trying to be healthy dont do that thing. And it&#x27;s extremely difficult to account for because the differences between those people and everyone else are very large and difficult to lock down, since they will tend to do lots and lots of things that you are supposed to do for good health, but most people dont. reply brigadier132 16 hours agorootparentprevThey try isolate confounders using statistical methods but these are not intervention studies and they rely on self reported data. There are obvious limitations with such approaches. You could imagine that people who are careful enough with their diet to completely avoid red meat also probably do physical activity of some sort. How many people that avoid red meat in their diet also eat at mcdonalds?They aren&#x27;t locking people up in a resort and strictly monitoring and controlling what they ingest. reply hombre_fatal 16 hours agorootparentWould you also bite that bullet with regard to the association between cigarettes and lung cancer? We don’t have RCTs.How do we know that it’s not just the fact that people who smoke also don’t exercise? Well, it’s simple: we trivially adjust for exercise. Just like we have prospective cohort research on unprocessed vs processed red meat: we can design studies that measure it.(This is aimed at your sibling comment) If you’re saying there’s a confounder we don’t know about in play: then what is it and what evidence would you bring to the table to justify its inclusion in our inference model? Note: that evidence would have to be at least as good as the evidence you’re rejecting for not having that magical secret confounder.That said, even before getting to cancer outcomes, I would replace red meat with another protein source just to avoid saturated fat. And if I was only moved by cancer outcomes and refused to change my high red meat diet, I would ensure I&#x27;m eating lots of veg&#x2F;fruit to insulate myself against the risks which is what you see in the Tomorrow Alberta Project prospective cohort analysis among the highest red meat eaters who also ate the most fruit&#x2F;veg. reply amanaplanacanal 16 hours agorootparentprev [–] They try to. Unfortunately you can’t account for all of them, because we don’t know all of them.Once it’s published that X is healthy or Y is unhealthy then people change their lifestyle in accordance. You can’t tell if some other thing that people do because they heard it is healthy is the real cause. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scientists at the University of Chicago have discovered that a fatty acid called trans-vaccenic acid (TVA) found in beef, lamb, and dairy products can enhance the ability of immune cells to fight tumors.",
      "Higher levels of TVA in the blood were associated with improved response to immunotherapy in cancer patients.",
      "This research suggests that TVA could be used as a nutritional supplement to enhance the effectiveness of cancer treatments, and there may be other nutrients from plants that activate immune cells in a similar way."
    ],
    "commentSummary": [
      "The discussions explore various aspects of nutrition and health, such as the effects of red meat, dairy, and cooking methods, as well as the link between certain foods and cancer.",
      "Different perspectives and viewpoints are presented, emphasizing the complexity and ongoing debate in this field.",
      "Topics also include the validity of scientific studies, the role of experts in providing health advice, and the significance of considering individual factors and context in research."
    ],
    "points": 278,
    "commentCount": 187,
    "retryCount": 0,
    "time": 1701099713
  },
  {
    "id": 38431482,
    "title": "$10M AI-MO Prize: Develop AI models to win gold in math competition",
    "originLink": "https://aimoprize.com/",
    "originBody": "AI-MO Prize About Supporters Get Involved $10mn AI Mathematical Olympiad Prize Launches 🌍 XTX Markets is launching a new $10mn challenge fund, the Artificial Intelligence Mathematical Olympiad Prize (AI-MO Prize). The fund intends to spur the development of AI models that can reason mathematically, leading to the creation of a publicly-shared AI model capable of winning a gold medal in the International Mathematical Olympiad (IMO). 🏆 The grand prize of $5mn will be awarded to the first publicly-shared AI model to enter an AI-MO approved competition and perform at a standard equivalent to a gold medal in the IMO. There will also be a series of progress prizes, totalling up to $5mn, for publicly-shared AI models that achieve key milestones towards the grand prize. The AI-MO Prize [..] will help compare different AI problem solving strategies at a technical level, in a manner that will be accessible and appealing to the broader public. view more Terence Tao, UCLA I am sure that many people will be following the AI-MO Prize with great excitement, to see when, in the future, AI will match the world's brightest young minds. view more Gregor Dolinar, IMO President 📈 The prizes will be designed by an AI-MO Advisory Committee, including mathematicians, AI and machine learning experts, and experienced Olympiad problem-setters. The first AI-MO approved competitions will open to participants in early 2024. There will be a presentation of progress at the 65th IMO, which will be held in Bath, England in July 2024. © XTX Markets 2023, All rights reserved EnquiriesTermsPrivacy Policy",
    "commentLink": "https://news.ycombinator.com/item?id=38431482",
    "commentBody": "$10M AI Mathematical Olympiad PrizeHacker Newspastlogin$10M AI Mathematical Olympiad Prize (aimoprize.com) 271 points by jasondavies 21 hours ago| hidepastfavorite222 comments gregdoesit 20 hours agoI was recently in Palo Alto, and bumped into a newly founded startup (I don&#x27;t remember the name unfortunately) who set themselves the grand the vision of exactly this: winning a gold medal on the international Olympiad using AI. Their plan was to build mostly on LLMs as a start, and iterate as they go. In their barebones office space, they had a poster with a countdown of the number of weeks till the event: it was 36 at the time.It sounded interesting to wonder how far they could go with this kind of approach. I thought they were aiming for the moon: but also respected the boldness and determination. They had the funding to operate for at least a year, and were very focused to get there.Seems like this prize will supr hundreds (or thousands) of teams competing in exactly this space. Perhaps it will have a similar effect like the $1M Netflix Prize in 2009 for recommendations algorithms! reply anonylizard 20 hours agoparentWell math solving is exactly what the rumored Q* is aiming towards too. I don&#x27;t think it&#x27;ll take more than 2 years before some LLM + RL system can take the gold medal.I think companies like OpenAI are aiming for something far more ambitious, like solving a millennium prize problem (even with human assistance). That&#x27;s the kind of news release that&#x27;ll add another $100 billion to your market cap. reply dimask 20 hours agorootparentTheir current ambition is to be able to solve school math, which is quite far away from solving unsolved conjectures or math olympiads. I really doubt that any of this is within LLM&#x2F;transformer scope, except maybe in some auxiliary sense to other, much different architectures. reply bilater 15 hours agorootparentExponential progress. Hard to wrap your mind around.An analogy: If it takes 20 years to create AGI equivalent to the village idiot. It take another couple of hours to go from that to Einstein. reply dimask 12 hours agorootparentI don&#x27;t disagree, as in that if we achieve school math in a way that is not mere overfitting over a language-based training set, going to more advanced mathematics is definitely conceivable. My problem is that people are talking about solving unsolved conjectures while we are not even in a point where we know how to tackle math at all.Imo we are not currently in the beginning of an exponential curve re solving math with AI, and def not on the path of AGI. I understand that if one believes that we are on the path to AGI soon then we shall have these math-AI advancements quite soon, but I disagree with the premise. reply anonylizard 19 hours agorootparentprevArt isn&#x27;t an easier problem than math. An artbot would have sounded more sci-fi than a mathbot only 2 years ago.Yet it only took the AI world 1.5 years to go from drawing child scribbles to replicating top artists with like 90% similarity (I can barely tell the difference between AI and human drawn art anymore with the new NovelAI model). It won&#x27;t be long before AI starts to go superhuman in art skills.It won&#x27;t take long from a school-math model to math olympiad model (I&#x27;d say 1 year is enough), and going to unsolved conjectures won&#x27;t be that long either (2-3 years?). We know from AlphaGo that its possible to make AI systems far superhuman at solving some abstract math problem. reply ekianjo 19 hours agorootparent> Art isn&#x27;t an easier problem than mathNot sure. Art is about approximate pattern recognition and if you have a large enough dataset it seems that you can definitely reproduce some of that.For math... it involves consistent reasoning from A to Z - which does not allow for any kind of mistake in the way. In Art you won&#x27;t feel too bad if the shadows or the lights are a little weird or if a character has 7 fingers instead of 5 on one hand, but this kind of mishaps break everything in Math. reply anonylizard 19 hours agorootparentFingers are already mostly solved. The latest models draw hands better than most human artists.I think artists would disagree with your assessment of &#x27;approximate pattern recognition&#x27;. Its more like：1. Given a set of words describing what the user wants. 2. Arrange pixels in a grid 3. That maximizes the user ratingOn one hand it is tolerant of small errors. On the other hand its an extremely broad problem. Also to get a good user rating, it has to do 99 things right for every 1 thing it draws wrong. reply larodi 18 hours agorootparentWell this thing about fingers, etc in drawings. Lets put it this way - for us mere mortals the generative images look very much okay. To artists and people who actually draw something, well ... they very often spot inconsistencies in the whole production, including how fingers, arms, overall body posture, etc is presented. So it is exactly what we can expect - good enough on average, but actually a mediocre result of commonality. Thing is the wide audience chews in mediocrity all the time, and nobody seems to have been able to change this for ages... reply disgruntledphd2 18 hours agorootparentIt&#x27;s a weighted average over the prompts and data. Mediocrity is exactly what we&#x27;d expect from such an approach.Now, if we end up seeing mastery, that would be extremely interesting. reply sdwr 12 hours agorootparentThis is a very mediocre understanding of deep learning :) reply disgruntledphd2 31 minutes agorootparentWhat else is it then? Basically all statistical models are weighted averages. I&#x27;m not sure why deep learning would be any different. reply michaelt 15 hours agorootparentprev> Mediocrity is exactly what we&#x27;d expect from such an approach.No no, you see I prompted the AI with \"masterpiece, photorealistic, 35 mm photography, cinematic, dslr, volumetric lighting, trending on artstation, 4k, 8k, hyper-detailed, epic digital painting by Greg Rutkowski\" &#x2F;s reply Der_Einzige 17 hours agorootparentprevUsing controlnet fixes all of these problems and makes it where even real artists won’t be able to tell AI art apart from real art. reply jszymborski 17 hours agorootparentprev> Art isn&#x27;t an easier problem than math.Someone can tell you when a math problem is solved. Someone else can&#x27;t tell you when you&#x27;ve successfully art&#x27;d with remotely the same degree of confidence. In so far as they can, however, many experts claim that AI cannot, indeed \"do an art\".Also, it&#x27;s easier to formulate a hard math question (there are plenty of unsolved problems), but that&#x27;s (IMHO) harder to do for art. Sure, you may think this is the first time the phrase \"Astronaut riding a Llama and holding an avocado\" was writ, but those are all well represented concepts in the dataset. For more abstract prompts, there really isn&#x27;t a way to verify \"correctness\". reply japoco 18 hours agorootparentprevI think art is much easier for LLM-style AI models to do compared to writing. To make a nice picture you just need to place pixels near each other in a way that looks good, and we all know LLMs are phenomenal at this. Good text on the other hand is not just text that has a good flow and fits the prompt. It must follow a line of thought, and LLMs don’t do that by design, even though we could argue wether they have that capability as an emergent one, but I don&#x27;t believe that at all. reply dxbydt 16 hours agorootparentprev> Art isn&#x27;t an easier problem than math.If I was asked to drawa. the emptiness in your heartb. the lack of furniture in your roomc. your empty bank balanced. starvatione. object returned by a python function with no return...I could just submit an empty sheet of paper, & an artist would argue that my empty sheet of paper represents any&#x2F;all of the above.Now, if I turn in the same empty paper at a math qualifier and argue that it represents the infinite set of real and complex numbers, ergo the answer to the posed qual problem must be in there, I&#x27;ll get kicked out of that phd program in a jiffy. reply civilitty 16 hours agorootparent> I could just submit an empty sheet of paper, & an artist would argue that my empty sheet of paper represents any&#x2F;all of the above.And they&#x27;d be taken about as seriously as the ads taped to a urinal in a Museum of Modern Art washroom. reply godelski 15 hours agorootparentprev> Art isn&#x27;t an easier problem than math.In a way it is, in a way it isn&#x27;t. You have to remember what is easy for machine isn&#x27;t going to correlate to what is easy for us humans. Look at AI art. Closely. No, closer than that. All the detail is fucked up. Not just the hands, but the tiniest of things. Strokes, lighting, reflections, and consistency, and all that. But can I turn my friend into a convincing werewolf? Yes. Can I turn my cat into a human or Wonder Woman? No. The system isn&#x27;t a \"fancy copier\" but it is a compression algorithm and the aforementioned tasks were only possible because lots of work training LoRAs, textual inversions, control nets, and so on (you could seriously improve GANs, VAEs, hell, even Boltzman Machines could probably do pretty well were any of these given the same research investment that diffusion has received. GANs come close but nuances like GANs having a magnitude fewer parameters).But let&#x27;s look at math, can I consistently add numbers? No. The problem is that in math, all those tiny intricate details matter. Not only that, they matter at every single step. The thing here is that these are still pattern recognition machines. But they aren&#x27;t generalized machines. You can&#x27;t really derive out all of math from probability distributions (or at least cleanly, but still not convinced you can). The thing is that for math to work in AI we have to address the elephants in the room: math. Yeah, math. ML people don&#x27;t like it. But we gotta address the axioms in the room that we&#x27;re operating under. How do we move on from machines operating on manifolds? How do we make it so data are not distributional? How do we move away from a number of unmentioned axioms remains a large open problem in AI research. One that does not get anywhere serious enough of a conversation, especially within the community. Sure, maybe transformer circuits can learn some addition by learning how to do FFTs and add in the FFT space, but you&#x27;re not going to get to Abstract Algebra that way. Ideally the AI can solve problems that have no algorithms, pun intended. reply jack_riminton 17 hours agorootparentprevNovelAI isn&#x27;t for drawing it&#x27;s for writing, did you mean something else? reply RandomLensman 19 hours agorootparentprevAlphaGo solved Go? reply anonylizard 19 hours agorootparent\"Superhuman at solving\" != \"solved\".AlphaGo didn&#x27;t solve go (Ie, can the first mover guarantee a win?). However, it understood go at a far, far superior level to any human.A mathbot doens&#x27;t have to solve math in general. It merely has to be better at solving math than any human mathematician to be considered ASI. And it only has to be better than the &#x27;average&#x27; human mathematician to be extremely useful in accelerating math research. reply tromp 19 hours agorootparent> solve go (Ie, can the first mover guarantee a win?).Solving Go means determining by what margin the first player can win, or equivalently, at what komi for white the game is a theoretical draw.That also makes it dependent on the exact rule set used.E.g. 2x2 go is a +1 first player win with the Tromp-Taylor rules of Go, while the Japanese rules are not even sufficiently formalized to allow scoring a 2x2 game. reply RandomLensman 19 hours agorootparentprevWhat mathematics was AlphaGo solving? What do you mean by solving there? reply dimask 19 hours agorootparentProving that the first player has a winning strategy, or the optimal strategy for both players leads to draw. reply falcor84 19 hours agorootparentJust to nitpick, while unlikely, it is theoretically possible that the second player has a winning strategy reply k2enemy 18 hours agorootparentTo nitpick a little further, it actually is not possible that the second player has a winning strategy. For that to be the case, P2 would need a winning path of play no matter what P1&#x27;s first move is.Suppose that P1 passes on their first move (which is a valid move). Then P2 has a winning path of play in which they put down the first stone. But P1 could have made that move and then they would be on the winning path. reply godelski 15 hours agorootparentI&#x27;m not a game theorist, in RL, nor a big Go player; but I am having a hard time finding this argument convincing. Isn&#x27;t the whole reason Go is impressive is because the enormous set of possible moves? Like we know that no computer could run ever game in the lifetime of the universe were it to even perform millions of moves a second. So of the I understand this number to be north of 10^500 for possible legal and playable games. So the difference of 1 doesn&#x27;t seem meaningful. Is there something I&#x27;m missing or a more convincing argument? Because even if player 1 always locks out 90% of those possible future moves, that&#x27;s still an absurdly large search space and it doesn&#x27;t seem like it is meaningfully different. reply recursivecaveat 12 hours agorootparentIt&#x27;s a proof by contradiction, like the halting problem proof. It doesn&#x27;t rely on the actual playable games at all, but what the existence of a winning strategy would imply. If there was a guaranteed winning strategy for player 2 it would be contradictory because player 1 could execute it by passing their first turn then using the winning P2 strategy. In that game both players can&#x27;t be guaranteed to win, so there must be some flaw in P2&#x27;s supposed guaranteed win strategy. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Strategy-stealing_argument reply godelski 10 hours agorootparentI am familiar with strategy stealing and things like tit-for-tat. But even the wiki article you linked suggests that Go is not a symmetric game, which is the requisite condition for strategy stealing to work (which was my underlying belief albeit (very) poorly worded). The wiki suggests both ladder and ko fights create an asymmetry as well as central control. Not to mention Komi explicitly making it asymmetric.First player does not always have the advantage. Nim is the best example where the setup can either be the first player winning game (nim-sum of the sizes of the heaps is not zero) or the second. My understanding is also that Chess (another perfect information turn-based game) is not shown solved or even has proven first player advantage (though in practice it looks so).So I get the argument, I just don&#x27;t buy it. I would be inclined to lean towards that direction, but it&#x27;s a tough claim theoretically and probably not meaningful in practice (unless a generalized strategy such as strategy stealing can be employed otherwise a lookup table is impractical as it&#x27;d contain more bits than atoms in the universe even for 100 move games).I think we have to consider far more than strategy-stealing which is not even a generalizable strategy to two-person perfect information turn-based games.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nim#Proof_of_the_winning_formu... reply recursivecaveat 8 hours agorootparentThe passing is the important part. In chess you can&#x27;t pass so going first might be bad, you could be the first to reach a forced zugzwang. If you have the option of passing your first turn, then having a turn before the 2nd player gets to go is at worst neutral for you. Likewise in Nim you can&#x27;t pass and taking your turn might be bad for you unlike Hex. reply godelski 6 hours agorootparentSorry, I&#x27;m not quite following, there seems to be a disconnection.First, I thought going first in chess is generally considered an advantage. Even the wiki article states that. Or at least says there&#x27;s a 10% increased win rate.Second, I still don&#x27;t get why passing is the important aspect. I thought the important aspect is symmetry. I mean I can understand this in nim since that symmetry is that killer aspect that makes for the easy analysis of a solution.When I said I&#x27;m not a game theory person I didn&#x27;t mean I have no game theory experience but that&#x27;s not what I study. I&#x27;m on the mathy side of ML but not so much in RL. You can use math with me if that makes things easier (in fact, I love math. Please do. RL notation doesn&#x27;t scare me but rather weirds me out that it scares others) because I think we&#x27;re getting lost in the conditions. reply recursivecaveat 2 hours agorootparentOh yeah, in chess going 1st is 100% definitely an advantage. For example, pawn to a4 is a really bad opener because its so close to throwing away that advantage by passing your first turn. Its just that the rules permit that taking a turn can force you into a worse position than you started, and you can&#x27;t prove the starting position isn&#x27;t like that. So from a proof perspective the empirical truth that going first is really good is not particularly helpful. The passing thing is weird because it empirically is a bad idea, it just closes a theoretical loophole.I&#x27;m not a mathematician myself, just got into this stuff when I was working on a boardgame solver. I find it difficult to map the &#x27;symmetric game&#x27; definition \"the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them\" onto a turn based game, but if it can work for Hex it must be compatible.If you consider a strategy to be \"a decision tree for how to place stones, when I&#x27;m playing as 2nd\", then there&#x27;s perfect symmetry between being 1st and choosing to immediately pass, and being 2nd. The possible strategies and resulting payoffs are the same. You add on top the extra move, which the possibility of passing means is at worst neutral for 1st player, and they cannot be at a disadvantage.More intuitively for me: being allowed to pass your first move is the same as getting to pick which side you want to play. There&#x27;s no way the side who can pick to play 1st or 2nd at their option can be at a forced loss to the side who just has to accept their decision. The picker would just pick the other side and now they have a forced win.(I&#x27;m always assuming above any kind of infinite-pass-standoff is a draw, and not some kind of weird other thing).If I haven&#x27;t expressed my thoughts clearly enough that&#x27;s probably about as well as I can manage I&#x27;m afraid. reply godelski 1 hour agorootparent> I find it difficult to map the &#x27;symmetric game&#x27; definition \"the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them\" onto a turn based game,Yeah so probably the better way to think about it might be with the payoff matrix. Because symmetry is actually about the strategy. That&#x27;s why there are the notes about the laddering in Go. But the payoff of a symmetric game is actually when A = -A^T. So if we have a 2x2 game a symmetric zero-sum one is where the payoff matrix might look like [[0, 1], [-1, 0]] Where we&#x27;re like an inverse-identity matrix (actually anti-symmetric) but the diagonals are opposite. Maybe it is best to think about this from a geometric perspective, this symmetry here (in this specific example) is a rotation matrix. That&#x27;s what it does when applied to another matrix. Recall our standard form is [[cos(theta), -sin(theta)],[sin(theta), cos(theta)]]. Pretty easy to get our matrix from there if you remember that cos(90)=cos(180)=0 and sin(90)=1 but sin(180)=-1. So our angle of rotation is 180 degrees (or pi radians). You could also see that if we made the two columns vectors we&#x27;d see they pointed in opposite directions. That&#x27;s the symmetry! Okay, yeah, maybe that&#x27;s confusing lol. But I find it helpful to see matrices as transforms and I wish this was stated a bit more clearly and often.So now that we maybe understand that, symmetry is about a __strategy__, not a player. Because our payoff matrix is strategy based. For example, our strategy for rock-paper-scissors is to pick each outcome 1&#x2F;3 of the time, which gives us this symmetric payoff. But if we pick rock every time we don&#x27;t get that payoff, right? So it&#x27;s actually not about who goes first or second but also includes the strategy aspect.At least that&#x27;s my understanding which a lot is prompted by this conversation (thanks!)The reason I&#x27;m finding the go argument hard is thinking of a basic \"entropy\" based strategy (it&#x27;ll serve you well in boardgames, especially when sight reading). The idea is if you don&#x27;t know the best move, play the move that gives you the most future moves. It&#x27;ll trick you into thinking that this strategy is actually simple, it isn&#x27;t. So in the game of Go, this isn&#x27;t reasonably different from making a random move! Because there are just so many. And realistically your strategy is going to be the composition of many different strategies. Like you said, pull out the decision tree but we can actually abstract this a bit more and have a decision strategy tree that&#x27;s a superset to our strategy that&#x27;s a response (e.g. a ladder is set up so we play the laddering strategy). The reason I&#x27;m not buying the argument isn&#x27;t about the logic, it is about the possible move sets. Even with super-ko (the board cannot return to a state it has previously been at any time in the game (must be fun to keep track of...)). So forgetting about all the extras that are played in go, passing shouldn&#x27;t result in a meaningful change in the number of possible strategies. But this argument might actually be an argument in favor of symmetry, not against it. Coming back to Chess, we know that game __is not__ symmetric. Why? Because white has different strategies than black. If instead the first \"move\" is to flip a coin and that decides who is white and who is black, then the game actually becomes symmetric. Kinda wild...I didn&#x27;t read this, but a glance suggests that black dominates in smaller gameshttp:&#x2F;&#x2F;erikvanderwerf.tengen.nl&#x2F;pubdown&#x2F;thesis_erikvanderwer...falcor84 18 hours agorootparentprevGood point, I actually wasn&#x27;t that they could pass the first move reply daveguy 18 hours agorootparentprevI would assume this is possible for any sufficiently complex game. Would you mind answering a few questions from someone near-completely ignorant about Go?Does second mover in Go have some sort of artificial benefit in scoring or playing? As in -- is there something to compensate for moving second?On the face it seems like first mover would have an advantage in any turn-based game. But maybe, in some games, seeing an opponent&#x27;s strategy is more helpful than executing the strategy.Also, are there examples of real games where second mover can always win? (Real as in, not made up with weird rules just to demonstrate it&#x27;s possible.) reply gizmo686 17 hours agorootparent> Does second mover in Go have some sort of artificial benefit in scoring or playing? As in -- is there something to compensate for moving second?Yes. The second player typically gets an extra 6.5 or 7.5 points. replyeddtries 18 hours agorootparentprevI have a couple friends who did the Math tripos at Cambridge (so a pretty high level!) who work in tech and have unanimously said they have 0% expectations of an LLM doing a millennium problem anytime soon reply Enginerrrd 17 hours agorootparentYeah, millennium problems almost certainly require truly novel nontrivial ideas to solve.That&#x27;s a tough thing for AI to do.On the other hand, Terrence Tao had an interesting article on his blog a while back where he was trying to solve a problem and asked chatGPT about it in a high-level strategy sense. ChatGPT suggested several reasonable approaches, one of which turned out to work.That&#x27;s nowhere near solving a millennium problem, but it is very interesting and suggests fairly sophisticated conceptual understanding of mathematics nevertheless.Current architecture and training methods I don&#x27;t think are enough to get there. However, with enough compute, I can plausibly envision some sort of meta training of LLMs using an analogy to GANs where one network tries to synthesize new correct ideas and the other shoots them down as not novel, not correct, or not sufficiently interesting.Such an approach I think could perhaps work, but the compute needed would probably be pretty high. reply jacobr1 17 hours agorootparentAnd also if you could combine that with some kind of representation software like Lean that can validate proofs, perhaps you can generate some kind of targeted search of the problem space and with a combination of a general high level strategy and maybe brute force of some sub-problems, find novel solutions. My understanding is that is the common human approach: gain some kind of intuition of problem, try a few things and then iteratively refine. Sometimes that works, sometimes you need to find a new starting point. It seems plausible we could automate that workflow, with likely mixed but still useful results. reply eddtries 17 hours agorootparentprevI’m not trying to understate the stuff you can do with LLMs or formal proof tooling that could be attached to randomly try things till it reaches a solution to a novel problem, but some solutions you see to lesser problems are sometimes so pie-in-the-sky I think stumbling on one is barely better than a random walk. And I think mathematicians like Tao are far better at narrowing that down. As an aide I can see it’s use, I just don’t believe we’re going to have LLMs & tooling solve these grand problems until compute power is orders of magnitudes better, and even then I’m still sceptical. BUT I’m not a mathematician and this is based on my intuition from pub talks :) reply paulddraper 13 hours agorootparentprevAsk yourself what mathematicians do today...They decompose problems, solve specialized subsets, examine more general cases, use existing proofs, do some numerical analysis, etc. reply auntienomen 13 hours agorootparentYeah, but none of that gets you a solution to a Millennium problem. One needs an AI to do something like what Peter Scholze did in inventing perfectoid spaces or Perelman in introducing his entropy function. You can treat axiom invention as a game, I suppose, but the space of possible moves is uhh rather large. reply Davidzheng 17 hours agorootparentprevThe field of LLM reasoning is far from stable atm. I think it&#x27;s pretty hard for anyone to give confident predictions about what they can or cannot do in 5 years. In that light, I am skeptical about any claims that they cannot do something anytime soon. reply eddtries 17 hours agorootparentWhat does an LLM really do? And can it create sometimes entirely novel math that goes against its training set to solve something, and know it’s right using a proof tool that may not even accept that? I agree but on a different order of magnitude of years. reply antoinexp 19 hours agorootparentprevMight depend on the terms you have in mind, but current consensus seems to be more like 4-5 years as we speak on https:&#x2F;&#x2F;www.metaculus.com&#x2F;questions&#x2F;6728&#x2F;ai-wins-imo-gold-me... reply auntienomen 15 hours agorootparentThat&#x27;s 4-5 years for solving Olympiad problems. Those are just very tricky high school math problems. They have solutions and can generally be solved by applying some combination of standard tricks. It&#x27;s very much the sort of thing an LLM should be good at.Solving Millennium problems is a whole different ballgame. It&#x27;s not known if these problems are solvable within ZFC axioms. (In one case, the Yang-Mills prize, stating the problem mathematically is part of the challenge.) All of the obvious applications of known tricks have been tried and failed. To solve such problems, one probably has to invent new and surprising mathematical definitions, building a framework in which the problem becomes solvable. This is something that LLMs will be crap at; the process of invention is not represented in any training data we have access to. reply EVa5I7bHFq9mnYK 14 hours agorootparentWe can look at it this way: there are ~1000 chess Grandmasters and one World Champion. It took very short time for AI to go from beating an average GM to beating World Champion.There are ~1000 MO winners and 1 (one) Millenial problem solver ... reply auntienomen 14 hours agorootparentWe can. But doing so frames math research as the same sort of activity as math problem solving.. it&#x27;s not. Many imo champions struggle to do any successful math research. And many successful math researchers (e.g., all of the most recent batch of Fields medallists) never did the Oympiad at all. reply EVa5I7bHFq9mnYK 13 hours agorootparentYet the one and only Millenial problem solver was a MO winner too, so there is some overlap. reply auntienomen 13 hours agorootparentSample size 1.Here&#x27;s what Andrew Wiles, the only other person to have solved a Millennium-class problem has to say of math competition: \"Let me stress that creating new mathematics is a quite different occupation from solving problems in a contest. Why is this? Because you don&#x27;t know for sure what you are trying to prove or indeed whether it is true.\" reply EVa5I7bHFq9mnYK 12 hours agorootparentSample size > 1 for sure, Terence Tao comes to mind, as well as Dr. Maryam Mirzakhani.Nobody argues it&#x27;s the same, after all MO problems are designed to be solved in ~an hour, but we are talking about mental capabilities. reply c7b 18 hours agoparentprevI&#x27;m sure lots of people have been thinking&#x2F;working along that direction. I think the idea of combining LLMs with formal verification&#x2F;proof assistant tools (Lean, Coq, Isabelle,...) is particularly interesting. Anyone is aware of any major groups working on open source solutions for this? reply alecco 19 hours agoparentprevI wish there was a serious study of Dunning-Kruger effect in Silicon Valley. reply larodi 19 hours agorootparentyour sense of humor is much appreciated. couldn&#x27;t have said it better. hah :) basically i would suggest the study to expand to everyone doing anything titled AI at this moment. reply eddtries 18 hours agorootparentprevAnd HN! reply goosinmouse 15 hours agorootparentHN about anything finance related is funny. Basically same level as reddit&#x2F;gamestonks reply queuebert 17 hours agorootparentprevWhile I get your point and agree, there apparently is no Dunning-Kruger effect. It was determined to be yet another case of faulty data analysis in behavioral psych.[1]1. https:&#x2F;&#x2F;economicsfromthetopdown.com&#x2F;2022&#x2F;04&#x2F;08&#x2F;the-dunning-k... reply lupire 17 hours agorootparentThat paper was thoroughly rebutted when posted to HN.But Gwern has written about some earlier debunkings of the D-K effect, ad furthermore D-K was never about the popular misconception of D-K (\"incompetent people think they are more competent than competent people\"). reply alecco 15 hours agorootparentprevI don&#x27;t think that&#x27;s a mainstream take. But I can settle with:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Overconfidence_effectOr evenhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grandiose_delusions reply queuebert 9 hours agorootparentThat first article cites more behavioral psych research, which has the absolute worst track record of data analysis. After so many retractions and scandals, I find it difficult to know what to believe anymore. reply jasfi 19 hours agoparentprevI&#x27;m not sure if such a system would qualify, the prize specifically mentions a model that could solve specific types of problems.I&#x27;m building something myself that I hope will be able to work similarly: https:&#x2F;&#x2F;aiconstrux.com reply mlengineerio 12 hours agoparentprevCan you share the name of the startup? I&#x27;m also in Palo Alto and tinkering LLM for Match competition too (math.llmlab.io). reply auggierose 20 hours agoparentprevI don&#x27;t think anything came out of the Netflix prize, did it? reply anonylizard 20 hours agorootparentI don&#x27;t think the actual winning algorithm itself was used, because real world systems have more constraints&#x2F;requirements than what the recommender was trained on. But that was in 2009, pre deep-learning&#x2F;AI summer, and $1 mil clearly helped stimulate interest in that area.Today we see multiple billion dollar recommender systems, like Tiktok. Netflix ironically benefits the least from recommenders due to the nature of its dataset (Very expensive, low sample size). reply jacobr1 17 hours agorootparentMy understanding was that their research on what drove engagement shifted quite a bit. Things like social proof, and product patterns like auto-loading the next episode to binge drove engagement metrics. Recently there were some articles about their team custom-identifying which cuts of a video to show as a trailer maximized engagement on a personal level. In some sense that is a recommendation, but it is a broader problem space. reply gorkish 14 hours agorootparentI have a more cynical take; the recommendations declined when Netflix started producing their own content. Prior to this, what constituted a \"good recommendation\" was aligned between Netflix and the customer, but afterwards not so much.Today Netflix is in the \"how do we get our customers to use our service as little as possible but still pay us every month\" phase of their mediacom hypocracy. From a business standpoint, that is their best optimization. They are AOL&#x2F;TW from 20 years ago. reply rcpt 16 hours agorootparentprevSocial proof on Netflix? reply jacobr1 15 hours agorootparentYeah, the top10 in your region carousels get high engagement reply rcpt 16 hours agorootparentprevNetflix got enormous amounts of R and D for dirt cheap reply lern_too_spel 16 hours agorootparentprevMatrix factorization was popular in industrial recommender systems for years afterwards, and its ideas are still useful today. https:&#x2F;&#x2F;sifter.org&#x2F;~simon&#x2F;journal&#x2F;20061211.html reply tomatoadventure 16 hours agoparentprevThat sounds really cool, but I am having trouble finding any information about them to get in touch :( reply dist-epoch 20 hours agoparentprevWhat&#x27;s his plan on being allowed to compete?Or will he be doing it after the fact, when the questions are published. reply lupire 17 hours agorootparentAfter the fact. reply svat 20 hours agoprevHow does this relate to the \"IMO Grand Challenge\" https:&#x2F;&#x2F;imo-grand-challenge.github.io&#x2F; ? Is this a new name &#x2F; formalization, with prize money attached, or is it entirely independent? (E.g. I see Kevin Buzzard and Leonardo de Moura listed on both that page and at https:&#x2F;&#x2F;aimoprize.com&#x2F;supporters) reply dwrensha 19 hours agoparentThe IMO Grand Challenge is \"formal to formal\" -- a solver is given the problem specified in the Lean programming language, and must produce a solution in Lean. To see more concretely what this setup might look like, check out https:&#x2F;&#x2F;github.com&#x2F;dwrensha&#x2F;compfiles.The AI MO prize is \"informal to informal\" -- a solver is given a problem in natural language and must produce a solution in natural language.My belief is that the best way to get to \"informal to informal\" is to first solve \"formal to formal\", but not everyone thinks so. reply lupire 17 hours agorootparent\"Informal to formal\" is harder than \"formal to formal\". These puzzle problems are quite simple (for computers) if you have a formalization.\"Informal to informal\" is so far snake oil. reply dwrensha 17 hours agorootparent> These puzzle problems are quite simple (for computers) if you have a formalization.That may be true someday, but it&#x27;s not yet! That&#x27;s exactly what the IMO Grand Challenge is about, and nobody has gotten close to solving it. reply lupire 17 hours agorootparentDo you have formal problems with formal solutions (so, you know the formalization is sufficiently complete) that cannot be found by existing (powerful) computer search techniques? reply dwrensha 16 hours agorootparentAbout half of the problems in Compfiles have complete solutions. They are marked by the checkmarks in the list at https:&#x2F;&#x2F;dwrensha.github.io&#x2F;compfiles&#x2F;index.html .As far as I know, based on published systems like LeanDojo [1] and Magnushammer [2], computers today can only solve a small handful of the very easiest of these problems (like maybe Imo1959P1).[1] https:&#x2F;&#x2F;leandojo.org&#x2F;[2] https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.04488 replysvat 3 hours agoparentprevAnswering my own question: Alex Gerko (who&#x27;s behind the $10M prize here) says:> \"It came about as me checking on the status of IMO Grand Challenge (which was launched in 2019 https:&#x2F;&#x2F;imo-grand-challenge.github.io) and deciding that it&#x27;s time to give this idea a boost\"https:&#x2F;&#x2F;twitter.com&#x2F;AlexanderGerko&#x2F;status&#x2F;172920793662562733... reply openquery 18 hours agoprevCan automated theorem provers solve mathematical olympiad problems in a reasonable time given enough compute?LLMs are quite good at generating semantically correct language. I remember reading a paper about extending the planning capabilities of GPT-4 by using a Planning Domain Definition Language [0]. By that same logic could an LLM not translate the olympiad problem into a form suitable for a theorem prover?[0] https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.11014.pdf reply govg 13 hours agoparenthttps:&#x2F;&#x2F;imo-grand-challenge.github.io&#x2F;This is a similar contest where the plan is exactly as you describe - to develop a way to solve formal descriptions in Lean of IMO problem. reply lacker 13 hours agoparentprevNo, IMO problems are much too hard for the current generation of theorem provers. reply 7373737373 20 hours agoprevIt would be cool to have a Patreon-like system for math proofs. But to reward solvers appropriately and at scale, the award conditions and evaluation would have to be very formalized and specific.This seems to be one potential, actually useful application of blockchains which support general purpose computing - if you can port a proof verifier onto them, you give anyone the ability to commit to (and claim) proof bounties.Now, precisely formalizing specific conjectures and ensuring the proof system is expressive enough but doesn&#x27;t allow for the introduction of any new assumptions is another problem... reply xavxav 20 hours agoparentA blockchain provides nothing of value here, what you need is a mechanized proof, and once you have that the blockchain in no way contributes to the trust.The trust in a Coq proof comes down to \"do you believe that the 8kloc kernel faithfully implements CiC+extensions and is this metatheory a sound type system?\". As it is today, anyone could claim or commit a proof bounty by posting a Coq &#x2F; Lean file &#x2F; project online, all that&#x27;s required is an email. reply 7373737373 19 hours agorootparentSure, any centralized group, ideally a non-profit could run it just as well, and do so without potential complications arising from computation limits.But they&#x27;d have to be entrusted with all the funds, and the financial side may be more difficult to implement. What could be a contract call would involve more real life logistics.Not to imply that the points or whatever would have to represent anything more than kudos&#x2F;bragging rights.It would be interesting to see which problems have the most professional interest, say if every math PhD got 1 million points to commit. reply lupire 16 hours agorootparentOverhead of block chain is greater than overhead of centralization.We already have rich math guys like Simons throwing money at people doing math. reply eddtries 18 hours agorootparentprevFirst person to submit a mechanised proof gets the payout to the address that submits the code? People can pool money into a pot? No one manages the funds individually and on a set date funds return to senders? reply cottonseed 19 hours agoparentprevI actually prototyped a system like this, mostly as an exercise to learn about crypto. You can&#x27;t feasibly host or verify proofs on-chain, so you need external trusted verifiers (e.g. oracles). Making sure the oracles can&#x27;t front-run proof submission is a challenge. Standard formal proof system (like Lean) are sufficiently expressive, although they weren&#x27;t built for this and need to be modified to make sure a proof hasn&#x27;t introduced any additional axioms, as you note. The proof system also becomes a point of attack, so you&#x27;d probably want multiple, independent verifiers (which themselves have been formally proved correct). I believe these exist for some proof systems, although I&#x27;m not sure about Lean&#x27;s kernel.Ultimately, I don&#x27;t think this is really practical, and investing in AI proof agents is the way to go. reply 7373737373 19 hours agorootparent> You can&#x27;t feasibly host or verify proofs on-chainI came to the same conclusion with existing systems, full on-chain verification would not be economically feasible.But perhaps a special-purpose chain specifically made for this may not have the same limitations. Or Truebit-like oracle systems may be possible, where external verifiers can dispute other external verifier&#x27;s assertions of correctness by running only the (potentially) wrong steps on-chain.The frontrunning may be avoided by submitting hash(accountidsecret large random numberproof) first, then once that&#x27;s finalized the full proof with the random number. The random number so the proof can&#x27;t be brute forced from the hash. Payouts have some reasonable delay so if someone tries to frontrun the second step by intercepting the full proof and submits both steps (possibly with a varied proof) before the second step of the first person is finalized, the first person can still prove with a reference to their first step and their working proof that they were first. This still requires some thought regarding (forced) congestion in relation to the transaction cost and bounty size.Metamath has the same problem of the system accepting new axioms anywhere, and the same $a statement being used for definitions. I have some hopes for Metamath Zero (https:&#x2F;&#x2F;github.com&#x2F;digama0&#x2F;mm0), which is a related system which may be able to fix this.I think this idea is rather synergistic with proof agents. I think more people would consider developing these if enough bounties were credibly committed. It might speed up and greatly increase the number of formalized proofs. reply kalverra 19 hours agorootparentprevChainlink has solved this. Worth taking a look if you&#x27;re still interested in this sort of thing. reply byteware 18 hours agoparentprevtalking as someone who is building such a system, why would an avarage participant of the network give their own money, i dont think they would for the same reason i&#x27;m not running a bounty program with my dollars, the only use of having it on the blockchain is rewards without using your own money (so it&#x27;s economics depends on the eventual value of the coin), for that it has to be bound to the minting process, but a purely algorithmic system for determining who gets how much based on their proofs seems elusive (think of the infinite possible proofs 1+2=3, 1+3=4...), we are going for having a central authority doing the minting for proofs (as voting based on money would hardly reflect mathematical experties) reply 7373737373 17 hours agorootparentI&#x27;d like to personally put 5-10 bucks toward the solution of the Collatz conjecture, just for fun (and to see machine learning people grind their GPU-teeth on it).No idea about the minting process, there seems to be an infinitude of possibilities. reply baq 18 hours agoparentprevI don&#x27;t think you need the blockchain to post a verifiable, cryptographically signed proof of any theorem. A simple email is good enough for that.Blockchains are append-only databases, sometimes (usually?) with &#x27;slow&#x27; thrown in somewhere. reply eddtries 18 hours agorootparentFirst person to submit a mechanised proof gets the payout to the address that submits the code? People can pool money into a pot? No one manages the funds individually and on a set date funds return to senders?That’s an app on top of the append only DB with some logic that either requires IRL groups to manage it, or a blockchain contract. I prefer the latter tbh! reply colesantiago 19 hours agoparentprev> This seems to be one potential, actually useful application of blockchain...No.Blockchain and anything crypto has absolutely no use case at all other than speculation, please stop suggesting this solution in search of a problem. reply pixel8account 1 hour agorootparentOne: anonymous[1] method of payment online. Maybe this is not a problem you like, but your statement is trivially false.[1] pseudonymous (in case of BTC) if we&#x27;re being pedantic. reply andyjohnson0 20 hours agoprevAs the parent of a young adult currently half way through their maths undergrad, this kind of fills me with foreboding.I know that proof assistants etc have existed for quite a while now, but what with this and the murmours about OAI&#x27;s Q* model, I do wonder what will happen to maths as a human endeavour - and as a enabling skill for jobs that can financially support people like my child. reply slibhb 20 hours agoparentI wouldn&#x27;t worry about it. We&#x27;re going to need humans with specialized mathematical training in the loop.Besides, these things have a way of surprising us. Before compilers, people wrote machine code by hand. It would have been reasonable to think that compilers would reduce the demand for programmers, but the opposite happened. reply pixl97 19 hours agorootparent>There isn’t a rule of economics that says better technology makes more, better jobs for horses. It sounds shockingly dumb to even say that out loud, but swap horses for humans and suddenly people think it sounds about right. reply Der_Einzige 16 hours agorootparentYes there is actually: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Okishio%27s_theorem reply pixl97 16 hours agorootparentEh, kind of, but maybe not.It talks about rate of profit over the economy as a whole. It says nothing about the distribution of said profits. Its assumed that human labors are the ones also reaping some of those profits because they are doing labor, hence wages from those labors remain stable. If for some reason there was 0 labor available to you the same premise could hold true, the rate the AI is earning could remain stable, you&#x27;re just shit out of luck. reply Davidzheng 19 hours agorootparentprevIt&#x27;s highly unclear to me what will happen. Probably learning math will be more accessible. But it&#x27;s unclear how many humans will be involved at the forefront of math research. For example you can imagine a situation where in some parts of math large communities work in a direction guided by a few \"visionaries\". This sort of thing will not really happen after the commodization of theorem provers. Like instead of an advisor telling a PhD student a problem, they can just plug it in. On the other hand, in some sense the advisor and the student will be equal in the fact that the only thing you will need to do is ask questions?(last part is also speculative) reply falcor84 19 hours agorootparent>Probably learning math will be more accessible.I&#x27;m not quite sure what you mean. Isn&#x27;t math already pretty much the most accessible thing that could be imagined? I can&#x27;t think of any story of someone in the past century who wanted to study math but was unable to, except for reasons prohibiting any sort of academic study whatsoever (e.g. girls in Taliban controlled areas). reply Davidzheng 19 hours agorootparentI don&#x27;t think research math is that accessible! Granted theoretically it&#x27;s possible to just read papers and do research--and possibly it&#x27;s theoretically as accessible as possible. In practice people learn better with teachers and access to experts at the research level. If this pans out, we can commoditize access to experts (or even experts). reply falcor84 18 hours agorootparentBut even in 1913 a brilliant mathematician from the middle of nowhere could access a world class mentor. I would assume that this is so much easier these days. What would stand in the way of a modern day math prodigy from getting accepted into a good program?Or is it about making this more accessible to students who are \"merely good\" rather than brilliant? reply Davidzheng 18 hours agorootparentyes I mean more accessible to the general public yes. reply michaelt 15 hours agorootparentprevSome people imagine AI will soon be a bit like being able to go to a university professor&#x27;s office hours to ask questions and get detailed expert advice on any part of the undergraduate curriculum you&#x27;re struggling with.Except it&#x27;ll be available 24&#x2F;7 and you won&#x27;t have to pass exams, spend $$$$, attend full time, and live in dorms and be in your late teens or early 20s to get access. reply c-cube 8 hours agorootparentAnd you&#x27;ll have to fact check every answer to make sure it&#x27;s not a \"hallucination\". reply andyjohnson0 19 hours agorootparentprev> Before compilers, people wrote machine code by hand. It would have been reasonable to think that compilers would reduce the demand for programmers, but the opposite happened.That analogy occurred to me too. But I&#x27;m not sure what the corresponding higher-level domain is that mathematicians might have to migrate to - in the way that assembly programmers started using high level languages. Writing prompts for maths LLMs, or wrangling teams of them, is hardly going to be well paid enough to facilitate a decent life in this era of late capitalism, or even pay uni fees.And even if there are such higher-level domains, its not certain that they are compatable with available human cognitive ability or limits.Lots of maths grads currently go into tech&#x2F;finance&#x2F;lifescience&#x2F;whatever. I&#x27;m a dev and I can see those fields being eaten alive by this stuff. I don&#x27;t want my kid to end up as a 2030-equivalent of a fully qualified assembly language programmer. reply margorczynski 19 hours agorootparentprev> Besides, these things have a way of surprising us. Before compilers, people wrote machine code by hand. It would have been reasonable to think that compilers would reduce the demand for programmers, but the opposite happened.I see this reasoning a lot but for me it kinda screams \"correlation is not causation\". As time passed and technology advanced it was simply more widely used, both on a consumer and business level. Very well may be that if we had to write machine code by hand we would need 10x more developers and a salary of $1kk&#x2F;year would be average at best.Do you really think if we get some AI agent that can write a wholly working application based on natural language specification that wouldn&#x27;t significantly reduce the need for human devs? reply BeetleB 18 hours agoparentprevI hate to break it to you but most people with math degrees aren&#x27;t doing math jobs today. The demand for such skills is low. While such an AI may displace some of the few jobs that do involve heavy math, it&#x27;s likely your child will not be in that category.I wouldn&#x27;t worry too much. reply auggierose 20 hours agoparentprevI wouldn&#x27;t worry. What this means is that mathematics as a skill is going to be back big time, because now you can actually use it everywhere.Mathematics departments have been closing down for a while now, I think. I think this trend will reverse now. Mathematics itself will change in the process, but for the better. reply anonylizard 20 hours agorootparentThis. Math proofs are useless in 99.99% of situations because they are far too expensive to actually use in production. Only something like AWS would use formal proofs to verify some system property for reliability.With some super-math Q* bot, a mathematician could presumably create actual proofs&#x2F;simplifications for complex real world problems&#x2F;systems at very affordable time and costs (in weeks not years).The mathematician in this case is far less skilled than the bot, but that doesn&#x27;t detract from their market value. Most programmers are way less skilled&#x2F;smart than the library authors that they rely on, that doesn&#x27;t stop them from earning $$$, because they are useful. reply electrozav 19 hours agorootparentThere&#x27;s a massive corpus of useful math proofs, just most of them have been implemented and are second nature now reply Davidzheng 19 hours agorootparentprevI don&#x27;t think it&#x27;s true that \"math departments have been closing for a while now\" reply sdenton4 19 hours agorootparentCorrect - math departments are doing fine. Not quite as fine as CS departments, which get showered with piles of money and their grad students poached to make money in industry. But so long as other university departments want their students to know some combination of calculus, linear algebra, and statistics, math departments will continue humming along. reply auggierose 19 hours agorootparentprevIt&#x27;s an impression I had, I don&#x27;t know any hard data. For example:https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;math&#x2F;comments&#x2F;l7yyir&#x2F;not_joking_uni...https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;math&#x2F;comments&#x2F;15of64l&#x2F;rumor_west_vi... reply Davidzheng 19 hours agorootparentI think it&#x27;s a very unfair assessment to make from these two examples. I think it&#x27;s like saying Silicon Valley has been shutting down for a while and providing two examples of startups. I will say from my personal impression as someone in pure math, I disagree with this statement. reply auggierose 16 hours agorootparentI don&#x27;t know what \"fair\" has to do with that. These are two examples that caught my attention, and I might be wrong that this constitutes a trend. If you have data that says otherwise, please share. reply Davidzheng 16 hours agorootparentI think the funding in pure math is not going down in recent years. Nor are math departments much smaller than they were say ten years ago. Otherwise I&#x27;m sure how best to give evidence of math departments not shutting down! Do I list healthy math departments? reply auggierose 15 hours agorootparentI guess this would be a case of applied math ;-)Joking aside, of course it is not on you to provide evidence. I would probably start with seeing how many universities have pure math departments over a time axis from now back to 2000 or so.It might even be that the total funding increases, but is more centralised in the big universities. So a total funding timeline would also be good. replydxbydt 16 hours agoparentprevPerhaps this might cheer you up - https:&#x2F;&#x2F;statmodeling.stat.columbia.edu&#x2F;2015&#x2F;03&#x2F;17&#x2F;1980-math-...Or perhaps not. In that blog, Gelman speaks of Gregg, who ended up as a GS VP, and says -math olympiad = high school basketball starpro mathematician = NBA playerGoldman Sachs VP = sports hustlerI actually worked with Gregg in fixed income at that time :) Gelman&#x27;s blogpost received sufficient notoriety, atleast within GS & the IB community. reply EVa5I7bHFq9mnYK 13 hours agorootparentYet GS VPs salary is closer to NBA player&#x27;s salary, while math pro&#x27;s salary is closer to sports hustlers&#x27; ... reply adhocmobility 20 hours agoparentprevI think its pretty clear that in the coming decade intelligence and cognitive labor is going to become very cheap. So your kid should develop some skills outside of that to stay competitive in the job market. reply RugnirViking 20 hours agorootparentgeneral intelligence will become cheap? and so your suggestion is to develop skills outside of skills? reply Davidzheng 19 hours agorootparentprevPersonally i think markets will be so different from now that this question of where you&#x27;ll have skills for jobs is less important than asking what will a job mean in 2040. But ok, maybe this is still a minority opinion. reply skepticATX 19 hours agorootparentprevI think it&#x27;s pretty clear that cognitive labor will become even higher value in the future, as our tools get better and allow us to become more productive. reply nickfromseattle 20 hours agorootparentprevAny suggestions? reply therealdrag0 2 hours agorootparentPlumber reply realharo 18 hours agorootparentprevStockpile metals and rare earth elements. Natural resources will be the beginning and the end of trade.(not actually serious, at least not yet) reply airstrike 20 hours agorootparentprevAI Software Engineering? Baker? reply Der_Einzige 16 hours agorootparentprevYou speak the truth but people here will hate you for it. reply daxfohl 18 hours agoparentprevIn a way, all math proofs already exist. Humans just have to determine what is worth looking for and uncover them. AI will help us do the latter. But we still have to do the former ourselves.It will still require sound mathematical knowledge and understanding, to know what are the interesting questions to ask. Even if AI knows all the answers, it doesn&#x27;t change anything because the answers already exist anyway. reply andyjohnson0 18 hours agorootparentThat kind of Platonism also implies that all possible computer programs already exist: because any program is merely a very large natural number, the set of which has infinite cardinality. I&#x27;m not sure how helpful this is for the practice of humans doing mathematics or software development though. reply tomp 19 hours agoparentprevYou do realize that we still learn (and not unnecessarily) addition despite having had calculators for 50+ years?Math shapes your mind, that’s why we learn it. reply alecco 18 hours agoprevVery cool of XTX. They are a solid company with very smart people.But wouldn&#x27;t a model capable of doing this be currently worth hundreds of millions? A billion? reply ackbar03 18 hours agoparentThat&#x27;s why they only offer 10 million.If you want, you can offer 100mio, I&#x27;ll join your competition instead reply dwrensha 19 hours agoprev> $5mn will be awarded to the first publicly-shared AI modelThank for you the emphasis on openness! reply Yhippa 19 hours agoprevI&#x27;m asking this question out of ignorance: if you were able to do this, why would you make it public for $10MM instead of keeping it private and exploiting it. Say, in algorithmic trading models? reply falcor84 19 hours agoparentI&#x27;m answering this out of naivete: some people want to benefit humankind over their personal interests. reply thomastjeffery 18 hours agorootparentIn that case, why would you be motivated by a $10m prize? reply falcor84 18 hours agorootparentTo clarify, I as a relatively well-off adult would consider donating towards such a prize pool with the explicit intent of drawing people who aren&#x27;t as willing or able to give money themselves towards solving a problem that I believe would benefit humankind. That is, I have a personal interest in that outcome (especially as compared to that tech being closed and controlled by governments&#x2F;corporations) and would be willing to donate money to motivate others to join this effort and make this model open source. reply fooker 16 hours agoparentprevTwo reasons: (1) These are two different problems. Human mathematicians tend to be good at both, but there&#x27;s no reason to believe (yet) that this generalization can be achieved with software.(2) Trading has second order effects, no matter how good your algorithm is, if you apply it at any kind of scale the market reacts to it and you are left with something that doesn&#x27;t work or cause large losses in the worst case. reply nathanfig 19 hours agoparentprevSome private enterprises would undoubtedly ignore this prize, but a lot of academic researchers (including students) would be more than motivated by this prize. reply BeetleB 18 hours agoparentprevI see no reason to believe such a machine would be helpful for algorithmic trading. Why should it?How well do current gold medalists do in trading? reply Yhippa 14 hours agorootparentI was curious as to why an algorithmic trading firm is sponsoring this. What&#x27;s in it for them? reply govg 13 hours agorootparentA lot of finance companies sponsor events &#x2F; prizes like this simply as a means of advertising and PR. If you come across this prize as a math student, now XTX is in your head and maybe you&#x27;ll look them up and decide to intern there. And 10m is a drop in the bucket for such goodwill and PR, especially because anyone who has the skills to win this can surely be hired and make them as much money. reply eddtries 18 hours agorootparentprevAlameda Research had some gold medalists iirc! reply lupire 17 hours agorootparentYes, but they stopped doing math puzzles when they started trading. Math puzzles aren&#x27;t how they trade. reply eddtries 16 hours agorootparentMaybe I’m being dumb here but isn’t the brain of one of those people the same, basically, as the model? So if a model can do X and is valued so highly, surely it’s value is pinned on a known brain that matches its skills? reply logicchains 16 hours agoparentprev>Say, in algorithmic trading modelsHigher level math is nothing like the math used in trading algorithms. It wouldn&#x27;t be any more useful than a top tier PHD graduate. reply seafoam 19 hours agoprevIs XTX good for the $10M ?There are a lot of people who got burnt by FTX grants and prizes ... reply eb2 19 hours agoparentYes, Alex Gerko is the UKs biggest tax payer and regularly donates to schools&#x2F;universities math departments reply WhitneyLand 19 hours agoprevWinning a gold medal here will really be a leap forward for AI creativity and deep insight into problem solving.AI already is already contributing in substantial ways to research.What’s tantalizing here is this next level would take a huge step toward accelerating the pace of advancements in many fields.It will be a milestone in moving past AI being a mere tool in scientific progress to something much greater. reply haltist 18 hours agoparentI sense you are a techno-optimist. reply WhitneyLand 18 hours agorootparentMaybe somewhat, but is anything I said that much of a leap?Do you consider AI to have already considered substantially to the advancement of science?For example AlphaFold, Weather forecasting, Algorithm optimization, etc reply haltist 18 hours agorootparentI believe the universe is a computer executing an algorithm. In fact, the universe is AI. This is a common belief among many techno-optimist church goers. People themselves are basically just numbers, aka algorithms, so future AI&#x2F;numbers (people) will definitely contribute to scientific progress. reply ShamelessC 11 hours agorootparentAre you serious?edit: you’re either trolling or mentally ill. If it’s the latter, I sincerely apologize. Hope things get better for you soon.edit edit: I saw your comments about the art project. Thank God. I didn’t want to live in a world where someone like that existed. replyDer_Einzige 16 hours agorootparentprevTechno optimism is good and should be the default. I wish a16z didn’t ruin that phrase. Techno optimism should evoke Star Trek, not Silicon Valley vibes reply haltist 16 hours agorootparentThe inevitable endpoint is the same, abundance of all resources (almost vacuously) thanks to AI&#x2F;numbers&#x2F;people. It is not absurd at all to believe society can be regulated and managed by a large computer with the right software. Society, after all, is just a bunch of numbers, algorithms, and arithmetic. The communists tried to do this some time ago but they just didn&#x27;t have a big enough computer with the right software (AI). They were on the right track and ironically their mission will be achieved by the capitalists doing nothing other than pursuing monetary profit. reply oglop 15 hours agoprevThat’s, interesting I think. I’m not sure wha true domain of problems are in that arena. I’ve found great success using LLMs for teaching abstract algebra but have noticed when I switch to analysis or topology things get more wonky.My advice, and I have zero understanding or care why the above happens, is to lean into algebra and train less on analysis. You’ll get there faster it seems. reply Imnimo 12 hours agoprevIt feels like 10M is a fraction of what it would cost to train such a model. Even if we assume an eventual winner would be willing to release the model and forgo potential profits, does this prize really motivate development if it doesn&#x27;t even cover costs? reply daxfohl 18 hours agoprevI&#x27;d still call this fancy autocomplete. I imagine the jump from this, to \"come up with an interesting new branch of math to explore\", is a long way off. reply ak_111 18 hours agoparentI totally disagree. I think it is more likely we will have quantum chips running your iphone before this prize gets cashed (and I am a bit of a quantum computing sceptic).For comparison, even field medalists sometime struggle with getting gold medal in the time allocated or identify the trick needed in solving an IMO question.The difficulty difference between solving an IMO level 6 problem and solving an open math problem is much smaller than most imagine. reply killerstorm 18 hours agoparentprevBy that definition 99.99% people are also fancy autocomplete.How many people you know who came up with an interesting new branch of math? reply daxfohl 15 hours agorootparentYeah I don&#x27;t disagree with this. But such people do exist. My expectation is that IMO gold is a couple years off. But inventing new math and explaining why it&#x27;s interesting and useful is much much farther away.I can&#x27;t decide whether I hope I&#x27;m wrong or not. reply esafak 18 hours agoparentprevFancy autocomplete is made possible by understanding.Can you imagine correctly autocompleting a proof to a Millennium Prize problem any other way? reply WhitneyLand 19 hours agoprevBefore gold at the math Olympiad, it would be nice if GPT 4 could merely evaluate an expression like this into an exact solution :3 + 1 &#x2F; (3 + 1 &#x2F; (3 + 1&#x2F;3)) reply dmd 19 hours agoparentWorks for me... https:&#x2F;&#x2F;cloud.typingmind.com&#x2F;share&#x2F;29d28743-b6a7-4716-b1aa-a... reply WhitneyLand 18 hours agorootparentThat’s helpful thank you.Would you mind trying it 4 times and tell how many succeed?I see it work occasionally but seemsProblem 1. Determine all composite integers n > 1 that satisfy the following property: if d1, d2, . . . , dk are all the positive divisors of n with 1 = d1To determine all composite integers \\( n > 1 \\) that satisfy the given property, we need to closely examine the condition set forth: for a composite number \\( n \\) with its divisors \\( d_1, d_2, \\ldots, d_k \\) where \\( 1 = d_1Now, let&#x27;s define a new function g: K → R^n such that g(x) = (f(x), f(x), ..., f(x)) for all x in K. In other words, each component of g is equal to f(x) for all x in K.That&#x27;s hilarious reply dj_mc_merlin 20 hours agorootparentprevYeah, this is actually a really good example of its weaknesses.. it&#x27;s the equivalent of a bullshit essay in math form. Completely tautological, with just a bit of the correct definitions thrown it to seem like it understands. reply devit 19 hours agorootparentprevIn my first attempt at the problem it made this mistake:\"The condition fails for i=1 since 1 does not divide p+q unless p+q is a multiple of n, which is not generally true\" (1 divides everything)In the second it gives up:\"However, this conclusion is based on heuristic reasoning and examples\"Third attempt it makes this mistake:\"If the immediate next divisor, di+1 , is not a multiple of p (for instance, it could be q or a product involving q), then p does not divide di+1. Hence, p will not divide the sum di+1+di+2 in such a case, violating the condition.\" (the fact that p does not divide d_i+1 does not imply that it does not divide d_i+1, d_i+2)Then it gives up again: \"However, this conclusion is based on heuristic reasoning and examples\"Then it makes this basic logic mistake: \"However, since p and q are distinct primes, p does not divide q, and it&#x27;s not guaranteed that p divides q+d_i+2 , especially if d_i+2 is not a multiple of p. Therefore, for such n, the condition fails.\" (the fact that \"it&#x27;s not guaranteed\" doesn&#x27;t imply that it&#x27;s false)Overall it proves that prime powers have the property, conjures that non-prime-powers don&#x27;t, proves that p*q doesn&#x27;t have it, but completely fails at coming up with a proof strategy that proves that non-prime-powers don&#x27;t have the property. reply Davidzheng 19 hours agorootparentprevGPT-4+code is surprisingly good. https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;ad42d09a-b366-4b21-b701-782fc8... \"This observation leads to a hypothesis: only the powers of prime numbers satisfy the given condition. To verify this hypothesis, we need to consider why powers of primes might uniquely satisfy the divisibility condition and then prove or disprove it.\" reply spi 19 hours agorootparentprevA decent start? It says absolutely nothing about how to solve it, except repeating the question. The part where it tries out the few first numbers is entirely wrong, given that 6 does _not_ satisfy the condition (2 does not divide 3+6=9) and 8 _does_ (2 does divide 4+8=12).Amusingly, the list of the integersThe IMO are a bit more involved...Answer:> The IMO are a bit more involved... e.g. IMO 2023:> Problem 1. Determine all composite integers n > 1 that satisfy the following property: if d₁, d₂, ..., dₖ are all the positive divisors of n with 1 = d₁To determine all composite integers 𝑛 > 1 that satisfy the given property, we need to closely examine the condition set forth: for a composite number 𝑛 with its divisors d₁, d₂, ..., dₖ where 1 = d₁Is there a clear point of departure when AI can no longer handle mathematical reasoning?I&#x27;m more interested in the point where AIs are presenting proofs far beyond human capability. I&#x27;m imagining a day when an AI says it has solved some interesting problem and when we ask for the proof it spits out a 4 million page document. What are we supposed to do with that? What&#x27;s the role of humans in that world? reply lupire 17 hours agoparentprevYet AI \"models\" ar terrible at calculation. reply userforcomment 15 hours agoprevGreat marketing stunt, nice work by XTX HR! reply amai 18 hours agoprevJust combine GPT-4 and Wolfram Alpha? reply arcza 20 hours agoprevOf course XTX is sponsoring this reply supergirl 10 hours agoprevnot sure there is much point to this. currently no model is even at the level of primary school math. it will take decades until it can win a gold medal in maths. it won&#x27;t happen overnight. it will happen slowly. will they still give out the prize in 20 years from now, when it won&#x27;t seem like such a breakthrough? reply rvz 20 hours agoprevGreat, a worthy competition which being first and correctness completely matter and may the best AI model win. Hopefully DeepMind doesn’t enter otherwise they would smoke everyone else other than OpenAI.Speaking of which, I’ll give you a 1% chance of winning with this paper from OpenAI: [0] with the MATH dataset: [1]EDIT: Why the downvotes? I&#x27;m trying to help you here and give you a starting point to win the competition? What&#x27;s wrong with helping others?[0] https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.20050[1] https:&#x2F;&#x2F;github.com&#x2F;hendrycks&#x2F;math reply haltist 19 hours agoprev [–] We will solve math with math and this project will succeed as foretold by prophets of techno-optimism like Marc Andreesen, Elon Musk, and Bill Gates. All we need is the right architecture, aka mathematical formula for giving all of math a smooth manifold structure. How this works for discrete structures like integers is left as an exercise for the reader and future AI which will figure out how to improve themselves and deal with discrete and non-smooth mathematical problems.I can win this challenge by the way for $80B. I already know what architecture is required to solve math with math but I need the money to buy the GPUs. You might think such a recursive application of math is logically circular but it is not and all I need is $80B to prove it (pun intended).Claude did not like this comment at all, ironically: I do not have enough context to fully evaluate those claims or determine if that approach would work. Solving all of mathematics is an extraordinarily ambitious goal that would require fundamental theoretical advances we do not yet possess. While future AI systems may someday make significant progress on longstanding mathematical problems, making definitive claims about solutions requires rigorous mathematical proof and analysis beyond optimistic speculation. I&#x27;d encourage focusing discussion on specific mathematical questions or areas of research rather than making broad, unsupported assertions about solving all of mathematics. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "XTX Markets is introducing the AI-MO Prize, a $10 million challenge fund to create AI models that can win gold medals at the International Mathematical Olympiad (IMO).",
      "The grand prize of $5 million will be awarded to the first publicly-shared AI model that achieves the level of a gold medal in an approved competition.",
      "Progressive prizes will also be given for models that reach specific milestones, and the competition will open to participants in early 2024 to compare AI problem-solving strategies."
    ],
    "commentSummary": [
      "The summary addresses the potential of AI to win a gold medal at the International Mathematical Olympiad, showcasing its capabilities in the field of mathematics.",
      "It explores the advancements and limitations of AI in art and math, shedding light on the potential applications of AI technology.",
      "The impact of AI on various fields, including math education and cognitive labor, is discussed, highlighting its value and contributions. The closing of math departments in universities and the future of intelligence are also touched upon."
    ],
    "points": 271,
    "commentCount": 222,
    "retryCount": 0,
    "time": 1701087906
  },
  {
    "id": 38441710,
    "title": "Introducing godforsaken.website: A Decentralized Social Network on Mastodon",
    "originLink": "https://godforsaken.website/@Shrigglepuss/111482466182637440",
    "originBody": "Create accountLogin Recent searches No recent searches Search options has: media, poll, or embedis: reply or sensitivelanguage: ISO language codefrom: userbefore: specific dateduring: specific dateafter: specific datein: all or library godforsaken.website is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more godforsaken.website: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.1 SearchLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=38441710",
    "commentBody": "New Outlook is good, both for yourself and 766 third partiesHacker NewspastloginNew Outlook is good, both for yourself and 766 third parties (godforsaken.website) 263 points by commoner 7 hours ago| hidepastfavorite72 comments withinrafael 5 hours agoMicrosoft replaced the old Mail app in favor of the new Outlook Desktop app, and I can&#x27;t even login despite reporting the issue for a year now. (At the time, I was even a Microsoft MVP getting briefed on the stuff way before launch.) That got me to finally stop using Outlook for good. Users beware. reply I_Am_Nous 5 hours agoprevI used to use Evolution until MAPI was removed, then Thunderbird+Exquilla for a bit, but now Thunderbird+IMAP is the sensible way to go. I make all my email rules server-side using OWA, and Thunderbird has had most of the OAUTH2 growing pains sorted out for a while now.We use Teams, which handles all the calendar stuff I used to need my mail client to do for me. The only risk seems to be Microsoft silently changing something and Thunderbird being left to figure out what to fix. I no longer need to access proprietary Public Folders, as that functionality has been swallowed up by Sharepoint Online.I&#x27;ve used Linux at work for the past decade, in spite of it being a Microsoft shop -- some days I would wrestle with HyperV or whatever ricketysticks thing Microsoft bolted on top of HyperV, some other rare days I&#x27;d get to play in the terminal all day.I could tolerate using Windows if I was forced to. But I absolutely love using Linux. Who even needs Outlook specifically these days? reply basemi 1 hour agoparentThunderbird add-on `owl-for-exchange` is another way to go for a little price (10€&#x2F;y). reply JeremyNT 4 hours agoparentprevFWIW, Evolution works quite well with O365 when using the EWS provider these days. You might want to give that another look if you aren&#x27;t wedded to Thunderbird. reply I_Am_Nous 4 hours agorootparentEvolution was always rock solid for me, but these days so is Thunderbird. I&#x27;ll have to give it a go sometime, thanks for the suggestion :) reply cglong 4 hours agoparentprevAs someone considering switching to Linux for work, why not use OWA always? It&#x27;s a full PWA, so you can access your data offline, and it&#x27;s flowing through Exchange anyway. reply vanous 4 hours agorootparentOne reason is good extension support, for example for fast keyboard navigation and mail moving in TB...https:&#x2F;&#x2F;addons.thunderbird.net&#x2F;en-US&#x2F;thunderbird&#x2F;addon&#x2F;nosta... reply I_Am_Nous 4 hours agorootparentprevOWA is fine for quick emails, usually, but I like old fashioned stuff like being able to view my message headers. Also, I use Firefox instead of Chrome so I don&#x27;t get the PWA benefits.It&#x27;s gonna suck when Microsoft finally kills connectivity or login support for the Teams for Linux Electron app lol reply jmmv 4 hours agorootparentprevOne reason is because OWA is really slow on not-great hardware... reply ggm 6 hours agoprevI had a time on a 13-00 phoneline today which said your call is important to us, you are \"undefined number\" caller on this queue and it made me very happy. I suspect 766 is \"suspiciously accurate\" and some of them are JV with Microsoft. reply bitwize 6 hours agoparentUnbelievable. You, [SUBJECT NAME HERE], must be the pride of [SUBJECT HOMETOWN HERE]. reply ggm 6 hours agorootparentImagine being the voice over artist having to say: \"undefined number\" \"not a number\" \"floating point overflow\" \"the back-end ESS-QEW-ELL server has returned an error\" \"missing semicolon or comma on line [1,2,3,4,..... 1231,1232,1233...]\" reply justinator 5 hours agorootparentI wonder if anyone has done a mysql injection exploit via text to speak over an automated phone line.\"Spell your name\"\"F-R-E-D-double-quote-semicolon-D-R-O-P-space-T...\" reply sundvor 4 hours agorootparentLittle Bobby Drop Tables is all grown up now! :-) reply userbinator 4 hours agorootparentprevI&#x27;m certain that the majority of the time it&#x27;s a TTS generating those lines. These days, probably something involving AI. reply smsm42 3 hours agorootparentOr a Perl script sold as AI because you can charge more this way. reply I_Am_Nous 5 hours agorootparentprevThe device is now worth more than the combined incomes and organs of [SUBJECT HOMETOWN HERE] reply moring 1 hour agorootparentI could never decipher these lines (they just sounded like \"mumble mumble\" to me), and now it makes sense. Have a cake as a \"thank you\"! reply AraceliHarker 6 hours agoprevAfter Windows 10, Microsoft&#x27;s policy seems to be \"I will give you free updates, but in return, you will be a Windows tester, watch ads, and provide your privacy.\" reply donmcronald 3 minutes agoparentOffice is even better. Look at the release cadence of the different MS365 tiers. Small businesses pay to be the canary for enterprise. reply 3dGrabber 3 hours agoparentprevThe End Game is to make Windows “Cloud Only” [1]Then they will have the users by their balls. Charge money for every click. Insert ads where they see fit. Dynamically adjust prices to squeeze the max amount of money.Time for quarterly reports? “Sorry our Excel servers are under ‘heavy load’, Excel now costs 4x the usual hourly rate…”[1] https:&#x2F;&#x2F;www.techradar.com&#x2F;news&#x2F;could-windows-12-become-micro... reply b6z 1 hour agorootparentThat sounds too negative. How about that:\"The prize-winning Excel service is in high demand right now. We apologize for any delay this might cause. Do you want to become an &#x27;Excel Prime Plus User&#x27; with preferential treatment? Just upgrade your subscription now and increase your productivity!\" reply tomComb 5 hours agoparentprevAfter paying for windows - that’s the part that really makes it sting. reply userbinator 4 hours agorootparentAs usual, Pirates of the LTSC have a better experience. reply yurrzz 6 hours agoprevNew Outlook is great &#x2F;shttps:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;sysadmin&#x2F;comments&#x2F;13lo7u6&#x2F;list_of_n... reply deafpolygon 2 hours agoparentI use the new Outlook, and I didn&#x27;t use the majority of features on the old Outlook. reply kahirsch 3 hours agoparentprevWow. reply FrostKiwi 5 hours agoprev> Features removed [...] PST file supportNo way! That kills of so many long term inboxes I know of. Over a decade of customer conversations in there in some cases. This will kick off a painful set of IT department phone calls and an even more painful migration if needed reply lnxg33k1 4 hours agoprevThese market forces really are great, they really made Microsoft to stop abusing dominant positions to get advantage in other segments, stop showing ads, etc. hooray for deregulation and innovation reply eastbound 4 hours agoparentMarket forces are great. Everyone for whom it was important spent the extra money and went to macOS. Don’t remember seeing Candy Crush in my start menu on macOS for one decade, and I distinctly remember the quality of browsing (no obnoxious popups, no Askbar, not having to download software like 7zip or Notepad++ from dodgy websites) was my argument back then.Maybe it’s not so important to you that you’d spend the extra money. reply skullone 3 hours agorootparentI&#x27;ll criticize Apple when it&#x27;s due, but I&#x27;m glad the OS isn&#x27;t filled with absolute trash all over the place on a fresh install. I can&#x27;t even bring myself to build a new PC for gaming, because whenever I turn on a Windows box I get disgusted by what it&#x27;s become. reply lnxg33k1 3 hours agorootparentYou guys seem to ignore that apple is an ads broker itself so no idea what are you on about https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;apple-is-an-ad-company-now&#x2F; reply russelg 3 hours agorootparentI think they&#x27;re probably talking about junkware like Candy Crush and Tiktok being pre-installed (or at least preexisting in the start menu)... reply wiseowise 2 hours agorootparentprevOwned iPhone for more than two years now. Haven’t seen a single ad pushed onto me via home, settings or browser. reply lnxg33k1 3 hours agorootparentprevFuck the poor reply wiseowise 2 hours agorootparentUbuntu is free. (As in beer) reply xattt 1 hour agorootparentI can’t use Thunderbird with my university email, because the Owl plug-in developer cornered the market on an Outlook plug-in with a subscription model.Subscription software is my hill to die on. reply jhutch 1 hour agorootparent*won’t replyfirekvz 5 hours agoprevthe funny thing about outlook is not even mentioned here, it seems that they have absolutely no spam filter? I get more spam (and the stupid obvious one, like 2004 phishing methods) into my outlook inbox than into my gmail throwaway email intentionally created to receive spam and crap signups, it just baffles me reply BenjiWiebe 5 hours agoparentThey definitely have anti spam. It&#x27;s great at triggering on emails from a self hosted email server.I&#x27;ve been maintaining one for a few years and have pretty good deliverability. 95% of the time when there&#x27;s an issue it&#x27;s Outlook&#x2F;Hotmail marking me as spam. And no I don&#x27;t do email marketing or newsletters. reply RagnarD 3 hours agoprevDo yourself a favor and use eM Client instead. It works extremely well, including playing nicely with Exchange. It was the only Window client I found that could completely replace Outlook.https:&#x2F;&#x2F;www.emclient.com&#x2F; reply xattt 1 hour agoparentShhhh, if too many people start using it, Microsoft will do something to put it out to pasture. reply quickthrower2 6 hours agoprevAccept! Then the next day send a right to be forgotten request. reply I_am_tiberius 6 hours agoparentIf they do it like Paul Graham with Hackernews, then they just remove the link between the person and the data. reply throwboatyface 6 hours agorootparentUsing a different identifier isn&#x27;t sufficient to satisfy a right to be forgotten request. The user&#x27;s identity could still be inferred from comments, for instance. reply godelski 6 hours agorootparentSo does that mean that if we request Dang will nuke our accounts (honest question, because it&#x27;s something I&#x27;ve been thinking about. Maybe or maybe not write a Tell HN) reply I_am_tiberius 6 hours agorootparentHe does not, that&#x27;s the point. I think YC is not on the legal side regarding this topic. reply godelski 5 hours agorootparentMaybe it is time to write that manifesto. I love the community but the environment has changed underneath us and faster that I, even someone in ML, expected. reply tzs 5 hours agorootparentprevWhich law(s) do you think they are not on the legal side of?The most common law people cite for deletion rights is GDPR but I&#x27;m not sure GDPR applies. GDPR&#x27;s territorial scope is defined in Article 3.For controllers or processors not \"in the Union\", which I think is the case for YC, GDPR applies if the processing activities are related to:\"(a) the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union; or(b) the monitoring of their behaviour as far as their behaviour takes place within the Union.\"People in the Union can make HN accounts and post, but that doesn&#x27;t mean that YC is offering goods or services to data subjects in the Union. Recital 23 of GDPR elaborates on offering goods or services (edited to split into two paragraphs to make it easier to read):> In order to determine whether such a controller or processor is offering goods or services to data subjects who are in the Union, it should be ascertained whether it is apparent that the controller or processor envisages offering services to data subjects in one or more Member States in the Union.> Whereas the mere accessibility of the controller’s, processor’s or an intermediary’s website in the Union, of an email address or of other contact details, or the use of a language generally used in the third country where the controller is established, is insufficient to ascertain such intention, factors such as the use of a language or a currency generally used in one or more Member States with the possibility of ordering goods and services in that other language, or the mentioning of customers or users who are in the Union, may make it apparent that the controller envisages offering goods or services to data subjects in the Union.I think YC could make a decent case that they do not envisage offering such services to people in the Union. reply lmm 4 hours agorootparent> > Whereas the mere accessibility of the controller’s, processor’s or an intermediary’s website in the Union, of an email address or of other contact details, or the use of a language generally used in the third country where the controller is established, is insufficient to ascertain such intention, factors such as the use of a language or a currency generally used in one or more Member States with the possibility of ordering goods and services in that other language, or the mentioning of customers or users who are in the Union, may make it apparent that the controller envisages offering goods or services to data subjects in the Union.> I think YC could make a decent case that they do not envisage offering such services to people in the Union.I very much doubt it. YC has an explicit list of European-based companies in their portfolio; they have posted job ads on HN for positions that were based exclusively in EU countries, and continue to do so as far as I can tell. PG is on the record talking about using HN from the UK (no longer part of the EU, but it was at the time). reply andrewmcwatters 6 hours agorootparentprevHe will not, I have asked. Instead, he does what throwboatyface has mentioned. It&#x27;s bad form. reply temp112123 6 hours agorootparentprevAs an aside- If you ask nicely they&#x27;ll edit&#x2F;remove the PII from the comments. reply hammock 6 hours agorootparentprevWhat do you mean by that? reply I_am_tiberius 6 hours agorootparentIf you ask hackernews to delete your comment(s), they refuse to do so. Instead, they just set the username of the comment to deleted. reply I_Am_Nous 5 hours agorootparentFrom what I have seen, the servers for hackernews are a bit delicate. At least when OpenAI does something stupid...if our usernames are a primary key ID a single update to remove the username is far less costly than actually dropping all the comments.Or they just want to have an archive for some background monetization scheme, anything is possible! reply junon 6 hours agorootparentprevThis is such a different case I don&#x27;t even know where to begin. Dang does an incredible job moderating the site. reply saagarjha 6 hours agorootparentYou can do an excellent job moderating this site while also choosing to not do something. reply I_am_tiberius 6 hours agorootparentprevThis has nothing to do with Dang. It&#x27;s not the moderators who decide this - it&#x27;s company policy, most likely defined by pg. reply ddingus 5 hours agoprevI dislike it. No data file support is a big deal. reply g-b-r 6 hours agoprevIt&#x27;s the sort of cookie notice (which also attempts to embed gdpr consents and notices) used by a lot of sites, I haven&#x27;t checked who the vendor is but they began declaring the number of third parties a few days ago.I think I ran into it five times so far, three of which had a party count similarly around 700, and two above 2000.The counts maybe include the third parties to which the third parties relay data etc.I haven&#x27;t checked if they&#x27;ve done it for some upcoming legal requirement or out of their own good heart (the legal requirement would in all likelihood be an nth draft of the ever \"upcoming\" ePrivacy regulation).Fairly eye-opening in any case. reply jruohonen 6 hours agoprevWith this much sharing, I&#x27;d reckon that a third life time award is coming. But I wonder whether the same sharing is going on for business users? reply yieldcrv 6 hours agoprevwhen you optimize packet size and then add 766 analytics libraries reply deafpolygon 2 hours agoprevJesus christ on a motorbike.Makes me want to switch away - why do they do this? reply zrezzed 6 hours agoprevIf you care about strongly about privacy... choose your email provider accordingly.If Outlook is your company&#x27;s email provider... the third parties are the concern of your IT department.If you&#x27;re not hit by one of those two conditions... why are you using Outlook? This feels like unwarranted outrage bait. reply artzmeister 6 hours agoparentRegardless of the reason, calling out such behaviour is always good, more so since many millions use outlook still. reply bee_rider 4 hours agoparentprevWhere is the option to make sure nobody I email uses outlook? reply ParetoOptimal 5 hours agoparentprev> If Outlook is your company&#x27;s email provider... the third parties are the concern of your IT department.Still your concern given whatever information is shared about you and your job to the 766 third parties. reply itake 6 hours agoparentprevhotmail used to be a popular choice. it can be hard to completely switch email providers. Many websites still don&#x27;t support updating email. reply freetanga 5 hours agorootparentJust pay for one which sees you as a client, not eyeballs. Enable pop3 or redirect on old account. Change as many web sites as possible.Bonus point if you get your own domain. Bitwarden and others allow creation of individual users on the fly on your domain, while a catchall on the email server collects this. reply johndhi 6 hours agoprev [–] It&#x27;s hard for me to read the text on mobile.Is this a cookie notice (required by EU e-privacy directive), or is this a sub processors notice (required by GDPR)?Both would be insane. I don&#x27;t see how they could possibly use that many vendors. Must also include all Microsoft subsidiaries? reply hammock 6 hours agoparentYou can pinch the screen and zoom in to make the image larger reply simongray 6 hours agoparentprev [–] It&#x27;s GDPR. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website godforsaken.website is a decentralized social network built on Mastodon.",
      "Users can create accounts, login, and search using different options.",
      "The website offers information about the platform, server statistics, and various features like following profiles, hashtags, favoriting, sharing, and replying to posts. Users can also interact from their accounts on different servers."
    ],
    "commentSummary": [
      "Hacker News discussions encompass various subjects such as problems with the Outlook Desktop app, frustrations with Microsoft Teams, and concerns regarding privacy and data protection regulations.",
      "Users on the platform express dissatisfaction with Microsoft's policies and engage in conversations about the pros and cons of different email providers, prioritizing privacy and security.",
      "The discussions highlight the importance placed on finding alternative email management options that address users' concerns."
    ],
    "points": 264,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1701139930
  },
  {
    "id": 38438261,
    "title": "Understanding AI Monosemanticity: Unraveling the Complexity of Neural Networks and the Pursuit of Interpretability",
    "originLink": "https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand",
    "originBody": "Share this post God Help Us, Let's Try To Understand AI Monosemanticity www.astralcodexten.com Copy link Facebook Email Note Other God Help Us, Let's Try To Understand AI Monosemanticity Inside every AI is a bigger AI, trying to get out Nov 27, 2023 130 Share this post God Help Us, Let's Try To Understand AI Monosemanticity www.astralcodexten.com Copy link Facebook Email Note Other 87 Share You’ve probably heard AI is a “black box”. No one knows how it works. Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want. But God only knows what goes on inside of it. This is bad for safety. For safety, it would be nice to look inside the AI and see whether it’s executing an algorithm like “do the thing” or more like “trick the humans into thinking I’m doing the thing”. But we can’t. Because we can’t look inside an AI at all. Until now! Towards Monosemanticity, recently out of big AI company/research lab Anthropic, claims to have gazed inside an AI and seen its soul. It looks like this: How did they do it? What is inside of an AI? And what the heck is “monosemanticity”? [disclaimer: after talking to many people much smarter than me, I might, just barely, sort of understand this. Any mistakes below are my own.] Inside Every AI Is A Bigger AI, Trying To Get Out A stylized neural net looks like this: Input neurons (blue) take information from the world. In an image AI, they might take the values of pixels in the image; in a language AI, they might take characters in a text. These connect to interneurons (black) in the “hidden layers”, which do mysterious things. Then those connect to output neurons (green). In an image AI, they might represent values of pixels in a piece of AI art; in a language AI, characters in the chatbot response. “Understanding what goes on inside an AI” means understanding what the black neurons in the middle layer do. A promising starting point might be to present the AI with lots of different stimuli, then see when each neuron does vs. doesn’t fire. For example, if there’s one neuron that fires every time the input involves a dog, and never fires any other time, probably that neuron is representing the concept “dog”. Sounds easy, right? A good summer project for an intern, right? There are at least two problems. First, GPT-4 has over 100 billion neurons (the exact number seems to be secret, but it’s somewhere up there). Second, this doesn’t work. When you switch to a weaker AI with “only” a few hundred neurons and build special tools to automate the stimulus/analysis process, the neurons aren’t this simple. A few low-level ones respond to basic features (like curves in an image). But deep in the middle, where the real thought has to be happening, there’s nothing representing “dog”. Instead, the neurons are much weirder than this. In one image model, an earlier paper found “one neuron that responds to cat faces, fronts of cars, and cat legs”. The authors described this as “polysemanticity” - multiple meanings for one neuron. The three images that most strongly activate neuron 4e:55 Some very smart people spent a lot of time trying to figure out what conceptual system could make neurons behave like this, and came up with the Toy Models Of Superposition paper. Their insight is: suppose your neural net has 1,000 neurons. If each neuron represented one concept, like “dog”, then the net could, at best, understand 1,000 concepts. Realistically it would understand many fewer than this, because in order to get dogs right, it would need to have many subconcepts like “dog’s face” or “that one unusual-looking dog”. So it would be helpful if you could use 1,000 neurons to represent much more than 1,000 concepts. Here’s a way to make two neurons represent five concepts (adapted from here): If neuron A is activated at 0.5, and neuron B is activated at 0, you get “dog”. If neuron A is activated at 1, and neuron B is activated at 0.5, you get “apple”. And so on. The exact number of vertices in this abstract shape is a tradeoff. More vertices means that the two-neuron-pair can represent more concepts. But it also risks confusion. If you activate the concepts “dog” and “heart” at the same time, the AI might interpret this as “apple”. And there’s some weak sense in which the AI interprets “dog” as “negative eye”. This theory is called “superposition”. Do AIs really do it? And how many vertices do they have on their abstract shapes? The Anthropic interpretability team trained a very small, simple AI. It needed to remember 400 features, but it had only 30 neurons, so it would have to try something like the superposition strategy. Here’s what they found (slightly edited from here): Follow the black line. On the far left of the graph, the data is dense; you need to think about every feature at the same time. Here the AI assigns one neuron per concept (meaning it will only ever learn 30 of the 400 concepts it needs to know, and mostly fail the task). Moving to the right, we allow features to be less common - the AI may only have to think about a few at a time. The AI gradually shifts to packing its concepts into tetrahedra (three neurons per four concepts) and triangles (two neurons per three concepts). When it reaches digons (one neuron per two concepts) it stops for a while (to repackage everything this way?) Next it goes through pentagons and an unusual polyhedron called the “square anti-prism” . . . Source: https://en.wikipedia.org/wiki/Square_antiprism#/media/File:Square_antiprism.png . . . which Wikipedia says is best known for being the shape of the biscornu (a “stuffed ornamental pincushion”) and One World Trade Center in New York: Freedom Tower confirmed as fundamental to the nature of thought itself; America can't stop winning. After exhausting square anti-prisms (8 features per three neurons) it gives up. Why? I don’t know. A friend who understands these issues better than I warns that we shouldn’t expect to find pentagons and square anti-prisms in GPT-4. Probably GPT-4 does something incomprehensible in 1000-dimensional space. But it’s the 1000-dimensional equivalent of these pentagons and square anti-prisms, conserving neurons by turning them into dimensions and then placing concepts in the implied space. The Anthropic interpretability team describes this as simulating a more powerful AI. That is, the two-neuron AI in the pentagonal toy example above is simulating a five-neuron AI. They go on to prove that the real AI can then run computations in the simulated AI; in some sense, there really is an abstract five neuron AI doing all the cognition. The only reason all of our AIs aren’t simulating infinitely powerful AIs and letting them do all the work is that as real neurons start representing more and more simulated neurons, it produces more and more noise and conceptual interference. This is great for AIs but bad for interpreters. We hoped we could figure out what our AIs were doing just by looking at them. But it turns out they’re simulating much bigger and more complicated AIs, and if we want to know what’s going on, we have to look at those. But those AIs only exist in simulated abstract hyperdimensional spaces. Sounds hard to dissect! God From The Machine Still, last month Anthropic’s interpretability team announced that they successfully dissected of one of the simulated AIs in its abstract hyperdimensional space. (finally, we’re back to the monosemanticity paper!) First the researchers trained a very simple 512-neuron AI to predict text, like a tiny version of GPT or Anthropic’s competing model Claude. Then, they trained a second AI called an autoencoder to predict the activations of the first AI. They told it to posit a certain number of features (the experiments varied between ~2,000 and ~100,000), corresponding to the neurons of the higher-dimensional AI it was simulating. Then they made it predict how those features mapped onto the real neurons of the real AI. They found that even though the original AI’s neurons weren’t comprehensible, the new AI’s simulated neurons (aka “features”) were! They were monosemantic, ie they meant one specific thing. Here’s feature #2663 (remember, the original AI only had 512 neurons, but they’re treating it as simulating a larger AI with up to ~100,000 neuron-features). Feature #2663 represents God. The single sentence in the training data that activated it most strongly is from Josephus, Book 14: “And he passed on to Sepphoris, as God sent a snow”. But we see that all the top activations are different uses of “God”. This simulated neuron seems to be composed of a collection of real neurons including 407, 182, and 259, though probably there are many more than these and the interface just isn’t showing them to me. None of these neurons are themselves very Godly. When we look at neuron #407 - the real neuron that contributes most to the AI’s understanding of God! - an AI-generated summary describes it as “fir[ing] primarily on non-English text, particularly accented Latin characters. It also occasionally fires on non-standard text like HTML tags.” Probably this is because you can’t really understand AIs at the real-neuron-by-real-neuron level, so the summarizing AI - having been asked to do this impossible thing - is reading tea leaves and saying random stuff. But at the feature level, everything is nice and tidy! Remember, this AI is trying to predict the next token in a text. At this level, it does so intelligibly. When Feature #2663 is activated, it increases the probability of the next token being “bless”, “forbid”, “damn”, or “-zilla”. Shouldn’t the AI be keeping the concept of God, Almighty Creator and Lord of the Universe, separate from God- as in the first half of Godzilla? Probably GPT-4 does that, but this toy AI doesn’t have enough real neurons to have enough simulated neurons / features to spare for the purpose. In fact, you can see this sort of thing change later in the paper: At the bottom of this tree, you can see what happens to the AI’s representation of “the” in mathematical terminology as you let it have more and more features. First: why is there a feature for “the” in mathematical terminology? I think because of the AI’s predictive imperative - it’s helpful to know that some specific instance of “the” should be followed by math words like “numerator” or “cosine”. In their smallest AI (512 features), there is only one neuron for “the” in math. In their largest AI tested here (16,384 features), this has branched out to one neuron for “the” in machine learning, one for “the” in complex analysis, and one for “the” in topology and abstract algebra. So probably if we upgraded to an AI with more simulated neurons, the God neuron would split in two - one for God as used in religions, one for God as used in kaiju names. Later we might get God in Christianity, God in Judaism, God in philosophy, et cetera. Not all features/simulated-neurons are this simple. But many are. The team graded 412 real neurons vs. simulated neurons on subjective interpretability, and found the simulated neurons were on average pretty interpretable: Some, like the God neuron, are for specific concepts. Many others, including some of the most interpretable, are for “formal genres” of text, like whether it’s uppercase or lowercase, English vs. some other alphabet, etc. How common are these features? That is, suppose you train two different 4,096-feature AIs on the same text datasets. Will they have mostly the same 4,096 features? Will they both have some feature representing God? Or will the first choose to represent God together with Godzilla, and the second choose to separate them? Will the second one maybe not have a feature for God at all, instead using that space to store some other concept the first AI can’t possibly understand? The team tests this, and finds that their two AIs are pretty similar! On average, if there’s a feature in the first one, the most similar feature in the second one will “have a median correlation of 0.72”. I Have Seen The Soul Of The AI, And It Is Good What comes after this? In May of this year, OpenAI tried to make GPT-4 (very big) understand GPT-2 (very small). They got GPT-4 to inspect each of GPT-2’s 307,200 neurons and report back on what it found. It found a collection of intriguing results and random gibberish, because they hadn’t mastered the techniques described above of projecting the real neurons into simulated neurons and analyzing the simulated neurons instead. Still, it was impressively ambitious. Unlike the toy AI in the monosemanticity paper, GPT-2 is a real (albeit very small and obsolete) AI that once impressed people. But what we really want is to be able to interpret the current generation of AIs. The Anthropic interpretability team admits we’re not there yet, for a few reasons. First, scaling the autoencoder: Scaling the application of sparse autoencoders to frontier models strikes us as one of the most important questions going forward. We're quite hopeful that these or similar methods will work – Cunningham et al.'s work seems to suggest this approach can work on somewhat larger models, and we have preliminary results that point in the same direction. However, there are significant computational challenges to be overcome. Consider an autoencoder with a 100× expansion factor applied to the activations of a single MLP layer of width 10,000: it would have ~20 billion parameters. Additionally, many of these features are likely quite rare, potentially requiring the autoencoder to be trained on a substantial fraction of the large model's training corpus. So it seems plausible that training the autoencoder could become very expensive, potentially even more expensive than the original model. We remain optimistic, however, and there is a silver lining – it increasingly seems like a large chunk of the mechanistic interpretability agenda will now turn on succeeding at a difficult engineering and scaling problem, which frontier AI labs have significant expertise in. In other words, in order to even begin to interpret an AI like GPT-4 (or Anthropic’s equivalent, Claude), you would need an interpreter-AI around the same size. But training an AI that size takes a giant company and hundreds of millions (soon billions) of dollars. Second, scaling the interpretation. Suppose we find all the simulated neurons for God and Godzilla and everything else, and have a giant map of exactly how they connect, and hang that map in our room. Now we want to answer questions like: If you ask the AI a controversial question, how does it decide how to respond? Is the AI using racial stereotypes in forming judgments of people? Is the AI plotting to kill all humans? There will be some combination of millions of features and connections that answers these questions. In some case we can even imagine how we would begin to do it - check how active the features representing race are when we ask it to judge people, maybe. But realistically, when we’re working with very complex interactions between millions of neurons we’ll have to automate the process, some larger scale version of “ask GPT-4 to tell us what GPT-2 is doing”. This probably works for racial stereotypes. It’s more complicated once you start asking about killing all humans (what if the GPT-4 equivalent is the one plotting to kill all humans, and feeds us false answers?) But maybe there’s some way to make an interpreter AI which itself is too dumb to plot, but which can interpret a more general, more intelligent, more dangerous AI. You can see more about how this could tie into more general alignment plans in the post on the ELK problem. I also just found this paper, which I haven’t fully read yet but which seems like a start on engineering safety into interpretable AIs. Finally, what does all of this tell us about humans? Humans also use neural nets to reason about concepts. We have a lot of neurons, but so does GPT-4. Our data is very sparse - there are lots of concepts (eg octopi) that come up pretty rarely in everyday life. Are our brains full of strange abstract polyhedra? Are we simulating much bigger brains? This field is very new, but I was able to find one paper, Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. The authors say: Through a suite of experiments and analyses, we find evidence consistent with the hypothesis that neurons in both deep image model [AIs] and the visual cortex [of the brain] encode features in superposition. That is, we find non-axis aligned directions in the neural state space that are more interpretable than individual neurons. In addition, across both biological and artificial systems, we uncover the intriguing phenomenon of what we call feature synergy - sparse combinations in activation space that yield more interpretable features than the constituent parts. Our work pushes in the direction of automated interpretability research for CNNs, in line with recent efforts for language models. Simultaneously, it provides a new framework for analyzing neural coding properties in biological systems. This is a single non-peer-reviewed paper announcing a surprising claim in a hype-filled field. That means it has to be true - otherwise it would be unfair! If this topic interests you, you might want to read the full papers, which are much more comprehensive and interesting than this post was able to capture. My favorites are: An Introduction To Circuits Toy Models of Superposition Distribution Representations: Composition & Superposition Towards Monosemanticity: Decomposing Language Models With Dictionary Learning In the unlikely scenario where all of this makes total sense and you feel like you’re ready to make contributions, you might be a good candidate for Anthropic or OpenAI’s alignment teams, both of which are hiring. If you feel like it’s the sort of thing which could make sense and you want to transition into learning more about it, you might be a good candidate for alignment training/scholarship programs like MATS. Subscribe to Astral Codex Ten By Scott Alexander P(A|B) = [P(A)*P(B|A)]/P(B), all the rest is commentary. Subscribe 130 Share this post God Help Us, Let's Try To Understand AI Monosemanticity www.astralcodexten.com Copy link Facebook Email Note Other 87 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=38438261",
    "commentBody": "Let&#x27;s try to understand AI monosemanticityHacker NewspastloginLet&#x27;s try to understand AI monosemanticity (astralcodexten.com) 258 points by bananaflag 12 hours ago| hidepastfavorite123 comments lukev 11 hours agoThere&#x27;s actually a somewhat reasonable analogy to human cognitive processes here, I think, in the sense that humans tend to form concepts defined by their connectivity to other concepts (c.f. Ferdinand de Saussure & structuralism).Human brains are also a \"black box\" in the sense that you can&#x27;t scan&#x2F;dissect one to build a concept graph.Neural nets do seem to have some sort of emergent structural concept graph, in the case of LLMs it&#x27;s largely informed by human language (because that&#x27;s what they&#x27;re trained on.) To an extent, we can observe this empirically through their output even if the first principles are opaque. reply wahern 10 hours agoparent> Neural nets do seem to have some sort of emergent structural concept graph, in the case of LLMs it&#x27;s largely informed by human language (because that&#x27;s what they&#x27;re trained on.) To an extent, we can observe this empirically through their output even if the first principles are opaque.Alternatively, what you&#x27;re seeing are the structures inherent within human culture as manifested through its literature[1], with LLMs simply being a new and useful tool which makes these structures more apparent.[1] And also its engineers&#x27; training choices reply Kapura 9 hours agoparentprevThis gets at the fundamental issue I have with AI: We&#x27;re trying to get machines to think, but we only have the barest understanding of how we, as humans, think. The concept of a \"neuron\" that can activate other neurons comes straight from real neurology, but given how complex the human brain is, it&#x27;s no surprise that we can only create something that is fundamentally \"lesser.\"However, I think that real neurology and machine-learning can be mutually reinforcing fields: structures discovered in the one could be applied to the other, and vice versa. But thinking we can create \"AGI\" without first increasing our understanding of \"wet\" neural nets is the height of hubris. reply hibikir 6 hours agorootparentAll the efforts to get computers to play chess mostly like humans do were made to look insufficient by a large enough neural network that took zero knowledge of human position evaluation, which just played against itself an outrageous number of games.The plane is in many ways lesser than birds, and we barely understood aerodynamics when the wright brothers gave us a working plane: While the bird is efficient, we had a whole lot more thrust. Our planes are far better than they were back then, but not because we understand bird better, but because we focused on efficiency of the simpler designs we could build. The Formula 1 car doesn&#x27;t come from deep understanding of the efficient, graceful running movements of the cheetah.Our results in medicine have been far and ahead of our understanding of biochemistry, DNA, and in general, how the human body works. The discovery of penicilin didn&#x27;t require a lot of understanding: It required luck. Ozempic and Viagra didn&#x27;t come from deep understanding of the human body: Resarchers were looking for one thing, and ended up with something useful that was straight out unintended.We might be able to build AGI with what we know of brains, we might not. But either way, it&#x27;s not because we need to understand the human mind better: Do we have enough compute for a very crude systems that we know how to build to overcome our relative lack of understanding?So if you ask me, what is the height of hubris is to think that working engineering solutions have to come from deep understanding of how nature solves the problem. The only reasonable bet, given the massive improvements in AI in the last decade, is to assume that the error bars of any prediction here are just huge. Will we get stuck, the way self-driving cars seem to have gotten stuck for a bit? Possibly. Will be go from a stochastic parrot into something better than humans, the same way that Go AIs went from pretty weak to better than any human in the blink of an eye? I&#x27;d not discount it either. I expect to be surprised either way. reply PeterStuer 2 hours agorootparentIn AI research there are different branches with different goals and motivations. You have applied research, where the goal is to engineer systems that are better at soving certain needs. You also have scientific endeavours, usually interdisciplinary, typically crossed with biology, psychology and philosophy, where AI models are created and used as models of understanding human or animal intelligence.The former is where as you describe where better understanding of biology might help but is not a prerequisite for progress, but in the latter it is not just needed but the goal.Now I know this is a bit of a caricature as both of these disciplines are in practice poorly deliniated and often intermixed. It&#x27;s easy to find hubristic examples to mock on both sides, but there&#x27;s value and brilliance to be found in each respectively as well. reply galaxyLogic 3 hours agorootparentprev> height of hubris is to think that working engineering solutions have to come from deep understanding of how nature solves the problem.It is basically understood how \"nature solves the problem\". It does that by evolution. And what is machine learning and neural networks but artificial evolution? reply valval 2 hours agorootparentNothing of the sort -- it&#x27;s best characterized as learning, like it says in the name. reply sxg 8 hours agorootparentprevAGI and human intelligence don&#x27;t necessarily have to work the same way. While many of us assume that they must be very similar, I don&#x27;t think there&#x27;s any basis for that assumption. reply swatcoder 8 hours agorootparentThat&#x27;s correct that they don&#x27;t need to work the same way. I would say most people that have earnestly thought about it assume they won&#x27;t.But given the exponentially greater chemical, biological, and neurological complexity of human brains, the millions of years of evolutionary \"pre-training\" that they get imbued with, and the years of constant multimodal sensory input required for their culmination in what&#x27;s called human intelligence, it takes an extremely bold assumption to believe that equivalent ends can be met with a stream of fmadds driving through in an array of transistors.It&#x27;s hard to overstate how big a gulf there is in both the hardware and software between living systems and what we currently see in our silicon projects. Neural network learning in general and LLM&#x27;s in particular are like crude paper airplanes compared to the flight capabilities of a dextrous bird. You can kinda squint and see that they both move some distance through the air, and even marvel at it for a bit, but there&#x27;s a long long long long way to go. reply kelseyfrog 6 hours agorootparentAnd yet no bird ever broke the sound barrier. Humans make things that exceed anything in nature. Thought will only be another example. reply bomewish 1 hour agorootparentExceed in some regards but not others right? Like, let’s not imagine the plane is better at being a bird than the bird. It’s an evocative set of associations though. Makes me think we’ll end up creating the Concorde of intelligence but be no closer to understanding how our own works… reply fuy 1 hour agorootparentprevThis basis is our limited data set: we have only 1 example of human level intelligence, so it&#x27;s natural to assume there&#x27;s something unique, or at least significant about the way it works. reply mitthrowaway2 7 hours agorootparentprev> We&#x27;re trying to get machines to think, but we only have the barest understanding of how we, as humans, think... But thinking we can create \"AGI\" without first increasing our understanding of \"wet\" neural nets is the height of hubris.That depends on the landscape of the solution space. If the techniques that work much better than anything else happen to be the same techniques used within our own brains, then we might find them, through aggressive experimentation and exploration, before we even realize that it&#x27;s also how our own brains work too. reply dboreham 7 hours agorootparentprevHard disagre. Imho humans suffer under a delusion that their thinking is like a computer whereas it is actually much more like an LLM. So if you ask humans[1] to figure our how they think, the answer will be a) long time coming and b) wrong.[1] possibly bhudist monks excepted. reply _as_text 11 hours agoprevI just skimmed through it for now, but it has seemed kinda natural to me for a few months now that there would be a deep connection between neural networks and differential or algebraic geometry.Each ReLU layer is just a (quasi-)linear transformation, and a pass through two layers is basically also a linear transformation. If you say you want some piece of information to stay (numerically) intact as it passes through the network, you say you want that piece of information to be processed in the same way in each layer. The groups of linear transformations that \"all process information in the same way, and their compositions do, as well\" are basically the Lie groups. Anyone else ever had this thought?I imagine if nothing catastrophic happens we&#x27;ll have a really beautiful theory of all this someday, which I won&#x27;t create, but maybe I&#x27;ll be able to understand it after a lot of hard work. reply zozbot234 9 hours agoparentReLU is quite far from linear, adding ReLU activations to a linear layer amounts to fitting a piecewise-segmented model of the underlying data. reply drdeca 9 hours agorootparentWell, at all but a finite number of points (specifically all but one point), there is a neighborhood of that point at which ReLU matches a linear function...In one sense, that seems rather close to being linear. If you take a random point (according to a continuous probability distribution) , then with probability 1, if look in a small enough neighborhood of the selected point, it will be indistinguishable from linear within that neighborhood.And, for a network made of ReLU gates and affine maps, still get that it looks indistinguishable from affine on any small enough region around any point outside of a set of measure zero.So... Depends what we mean by “almost linear” I think. I think one can make a reasonable case for saying that, in a sense it is “almost linear”.But yes, of course I agree that in another important sense, it is far from linear. (E.g. it is not well approximated by any linear function) reply KhoomeiK 10 hours agoparentprevYou might be interested in this workshop: https:&#x2F;&#x2F;www.neurreps.org&#x2F;And a possibly relevant paper from it:https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=Ag8HcNFfDsg reply blovescoffee 7 hours agoparentprevWhat? The very point of neural networks is representing non-linear functions. reply jimsimmons 9 hours agoparentprevEverything is something. Question is what this nomenclature gymnastics buys you? Unless you answer that this is no different than claiming neural networks are a projection of my soul reply rlupi 2 hours agorootparentCould looking at NN through the lens of group theory unlock a lot of performance improvements?If they have inner symmetries we are not aware of, you can avoid waste in searching in the wrong directions.If you know that some concepts are necessarily independent, you can exploit that in your encoding to avoid superposition.For example, I am using cyclic groups and dihedral groups, and prime powers to encode representations of what I know to be independent concepts in a NN for a small personal project.I am working on a 32-bit (perhaps float) representation of mixtures of quantized Von Mises distributions (time of day patterns). I know there are enough bits to represent what I want, but I also want specific algebraic properties so that they will act as a probabilistic sketch: an accumulator or a Monad if you like.I don&#x27;t know the exact formula for this probabilistic sketch operator, but I am positive it should exist. (I am just starting to learn group theory and category theory, to solve this problem; I suspect I want a specific semi-lattice structure, but I haven&#x27;t studied enough to know what properties I want)My plan is to encode hourly buckets (location) as primes and how fuzzy they are (concentration) as their powers. I don&#x27;t know if this will work completely, but it will be the starting point for my next experiment: try to learn the probabilistic sketch I want.I suspect that I will need different activation functions that you&#x27;d normally use in NN, because linear or ReLU or similar won&#x27;t be good to represent in finite space what I am searching for (likely a modular form or L-function). Looking at Koopman operator theory, I think I need to introduce non-linearity in the form of a Theta function neuron or Ramanujan Tau function (which is very connected to my problem). reply seanhunter 2 hours agorootparentprevI would argue that there are a few fundamental ways to make progress in mathematics:1. Proving that a thing or set of things is part of some grouping2. Proving that a grouping has some property or set of properties (including connections to or relationships with other groupings)These are extremely powerful tools and they buy you a lot because they allow you to connect new things in with mathematical work that has been done in the past. So for example if the GP surmises that something is a Lie group that buys them a bunch of results stretching back to the 18th century which can be applied to understand these neural nets even though they are a modern concept. reply kdmccormick 4 hours agorootparentprev> what this nomenclature gymnastics buys you????Are you writing off all abstract mathematics as nomenclature gymnastics, or is there something about this connection that you think makes it particularly useless? reply mathematicaster 10 hours agoparentprevI did a little spelunking some time ago reacting to the same urge. Tropical geometry appears to be where the math talk is at.Just dropping the reference here, I don&#x27;t grok the literature. reply erikerikson 11 hours agoprevBefore finishing my read, I need to register an objection to the opening which reads to me so as to imply it is the only means:> Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want.This isn&#x27;t the only way. Back propagation is a hack around the oversimplification of neural models. By adding a sense of location into the network, you get linearly inseparable functions learned just fine.Hopfield networks with Hebbian learning are sufficient and are implemented by the existing proofs of concept we have. reply janalsncm 10 hours agoparentThis is true. We use backpropagation not because it’s the only way or because it’s biologically plausible (the brain doesn’t have any backward passes) but because it works. Neural networks aren’t special because of any sort of connection to the brain, we use them because we have hardware (GPUs) which can train them pretty quickly.I feel the same way about transformers vs RNNs: even if RNNs are more “correct” in some sense of having theoretically infinite memory it takes forever to train them so transformers won. And then we developed techniques like Long LoRA which make theoretical disadvantages functionally irrelevant. reply erikerikson 5 hours agorootparentI agree that doing effective things is effective and we should do effective things when they prove valuable. These things here are generally simplifications or even distillations which can be faster and more efficient which are excellent attributes of solutions. My objection isn&#x27;t that we develop great systems but that we don&#x27;t forget that other branches in the solution space exist. Particularly here because the more rich enough may help yield the next generation of solutions. reply bilsbie 10 hours agorootparentprev> developed techniques like Long LoRA which make theoretical disadvantages functionally irrelevant.How’s that? reply janalsncm 9 hours agorootparentContext windows used to be tiny e.g. 768 or 1024. That meant sliding windows of limited context if you had any reasonably sized input. If your context window is 32k or even 100k, a lot of inputs will fit into the context window entirely.The reason huge context windows weren&#x27;t possible in the past is that memory requirements were quadratic with input length. Long LoRA lets us use less memory for our context windows, or use the same memory footprint for larger context windows. reply dekhn 7 hours agoparentprevI think it would be really exciting if somebody could show that ANNs that more resembled biological neurons could learn function approximation as well as (or better than!) current DNNs. However, my understanding of math and engineering suggests that for the time being, the mechanisms we currently use and invest so much time and effort into will exceed more biologically inspired neurons, for utterly banal reasons. reply erikerikson 5 hours agorootparentYet brains remain superior in many regards. Still, by all means, let&#x27;s celebrate the advances we make! reply blovescoffee 7 hours agoparentprevBack propagation isn&#x27;t a hack. It&#x27;s a triumph. It&#x27;s powering the revolution we&#x27;re experiencing. reply erikerikson 5 hours agorootparentIt has yield some excellent results that we should celebrate and use. Yet the consensus of the field, when I was properly informed on such things, was that we developed back propagation while under a misunderstanding that linearly inseparable functions (e.g. XOR) could not be learned by Hebbian learning in Hopfield networks. This was a correct result in the context of his assumptions but too limited and&#x2F;or over applied conclusion from Minsky&#x27;s work. reply gmuslera 11 hours agoprevAt least the first part reminded me of Hyperion and how AIs evolved there (I think the actual explanation is in The Fall of Hyperion), smaller but more interconnected \"code\".Not sure about actual implementation, but at least for us concepts or words are not pure nor isolated, they have multiple meanings that collapse into specific ones as you put several together reply daveguy 11 hours agoprevAll this anthropomorphizing of activation networks strikes me as very odd. None of these neurons \"want\" to do anything. They respond to specific input. Maybe humans are the same, but in the case of artificial neural networks we at least know it&#x27;s a simple mathematical function. Also, an artificial neuron is nothing like a biological neuron. At the most basic -- artificial neurons don&#x27;t \"fire\" except in direct response to inputs. Biological neurons fire because of their internal state, state which is modified by biological signaling chemicals. It&#x27;s like comparing apples to gorillas. reply og_kalu 9 hours agoparent>None of these neurons \"want\" to do anything. They respond to specific input.Yes well, your neurons don&#x27;t \"want\" to do anything either.>Maybe humans are the same, but in the case of artificial neural networks we at least know it&#x27;s a simple mathematical functionSo what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevant. Silicon, biology, pulleys and gears. all can be arranged to make the same or similar computations. If you genuinely believe the latter, it&#x27;s fine. The point is that \"simple\" mathematical function is kind of irrelevant. Either the brain computes and any substrate is fine or it doesn&#x27;t.>Also, an artificial neuron is nothing like a biological neuron.They&#x27;re not the same but \"nothing like\" is pushing it a lot. They&#x27;re inspired by biological neurons and the only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it, not because we can&#x27;t.>Biological neurons fire because of their internal state, state which is modified by biological signaling chemicalsBrains aren&#x27;t breaking break causality. The fire because of input. reply airgapstopgap 8 hours agorootparentComments like this are incredibly grating. You condescend to the interlocutor for making a mistake which only exists in your own mistaken world model. Your confidence that neurons and ANN weights and «pulleys and gears» are all equivalent because there is, in theory, an intention to instantiate some computation, and to think otherwise is tantamount to belief in magic and broken causality, is just confused and born out of perusing popular-scientific materials instead of relying on scientific literature or hands-on experience.> The fire because of input.No they do not fire because of input, they modulate their firing probability based on input, and there are different modalities of input with different effects. Neurons are self-contained biological units (descended, let me remind you, from standalone unicellular organisms, just like the rest of our cells), which actually have an independently developing internal state and even metabolic needs; they are not merely a system of logic gates even if you can approximate their role with a system of equations or an ANN. This is very different, mechanistically and teleologically. Hell, even spiking ANNs would be substantially different from currently dominant models.> So what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevantStop dumbing down complex arguments to some low-status culture war opinion you find it easy to dunk on. reply og_kalu 8 hours agorootparent>Your confidence that neurons and ANN weights and «pulleys and gears» are all equivalent because there is, in theory, an intention to instantiate some computation, and to think otherwise is tantamount to belief in magic and broken causality, is just confused and born out of perusing popular-scientific materials instead of relying on scientific literature or hands-on experience.Computation is substrate independent. I&#x27;m not saying neurons and ANN weights and «pulleys and gears» are the same. I&#x27;m saying it does not matter because what you perform computation with does not change the results of the computation. If the brain computes, then it doesn&#x27;t matter what is doing the computation.>No they do not fire because of input, they modulate their firing probability based on input, and there are different modalities of input with different effects. Neurons are self-contained biological units (descended, let me remind you, from standalone unicellular organisms, just like the rest of our cells), which actually have an independently developing internal state and even metabolic needs; they are not merely a system of logic gates even if you can approximate their role with a system of equations or an ANN. This is very different, mechanistically and teleologically. Hell, even spiking ANNs would be substantially different from currently dominant models.Yes, a neuron is firing because of input. To suggest otherwise is to suggest something beyond cause and effect directing the workings of the brain. If that is genuinely not the case then feel free to explain why, rather than an ad hominin attack on someone you don&#x27;t even know.> So what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevant>Stop dumbing down complex arguments to some low-status culture war opinion you find it easy to dunk on.I personally don&#x27;t care if that&#x27;s what anyone believes. The intention is not to attack anyone.If you believe in a soul or the non religious equivalent, that&#x27;s fine. We just have different axioms.If you don&#x27;t believe in a soul(or the equivalent) but somehow think substrate matters then you need to explain why because it makes no sense. reply kdmccormick 3 hours agorootparentI am not well versed in any of this, but from reading the counterarguments, I think two good points are being made:* Analogies aside, neurons are quite different than NN nodes, because each neuron has an incredibly complex internal cellular state, whereas an NN node just has an integer for state.* A brain is not a \"function\" in the way that a trained LLM model is. Human life is not a series of input prompts and output prompts. Rather, we experience a fluid stream of stimuli, which our brain multiplexes and reacts to in a variety of ways (speaking, moving, storing memories, moving our pupils, releasing hormones, etc). That is NOT TO SAY a brain violates causality; it&#x27;s saying that the brain is mechanically doing so much more than an LLM, even if the LLM is better at raw computation.None of this IMO precludes AGI from happening in the medium term future, but I do think we should be careful when making comparisons between AGI and the human brains.Rather than comparing \"apples to gorillas\", I&#x27;d say it&#x27;s like comparing a calculator to a tree. Yes, the calculator is SIGNIFICANTLY better at multiplication, but that doesn&#x27;t make it \"smarter\" than a tree, whatever that means. reply airgapstopgap 1 hour agorootparentI do not even think any of this has much of impact on AGI timelines. Human brain cells are not a superior substrate for computing \"intelligence\". They just are what they are; individual cells can somewhat meaningfully \"want\" stuff and be quasi-agents unto themselves, they do much more than integrate and fire. Weights in an ANN are purely terms in an equation without any inner process or content. reply red75prime 5 hours agorootparentprev> Stop dumbing down complex argumentsIt&#x27;s not dumbing down. It&#x27;s extracting the crux of the matter that the complexity of arguments is trying to hide, perhaps unintentionally. Either the brain implements a function that can be approximated by a neural network thanks to universal approximation theorem, or the function cannot be approximated (you need arguments for why it is the case), or magic. reply morsecodist 4 hours agorootparentThis is technically true but kind of misses the point in my opinion. A neural network can approximate any function in theory but that doesn&#x27;t mean it has to do so in a reasonable amount of time and with a reasonable amount of resources. For example, take the function that gives you the prime factors of an integer. It is theoretically possible for a neural network to approximate this for an arbitrarily large fixed window but is provably infeasible to compute on current hardware. In theory, a quantum computer could compute this much faster.This is not to say that the human brain leverages quantum effects. It&#x27;s just a well known example where the hardware and a specific algorithm can be shown to matter.I also think it&#x27;s strange to describe the brain as implementing a function. Functions don&#x27;t exist. We made them up to help us think about building useful circuits (among other things). In this scenario, we would be implementing functions to help us simulate what is going on in brains. reply Notatheist 10 hours agoparentprevI feel anthropomorphizing is perfectly reasonable especially in this context. How would you like it described? reply kibwen 9 hours agorootparentIt&#x27;s one thing to deliberately anthropomorphize in order to construct an analogy.It&#x27;s another thing to anthropomorphize by accident or by illusion, as per pareidolia: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pareidolia . Just as in pareidolia, where the human brain is primed to \"see\" a human face in a certain pattern of light and shapes, it seems that human brains are primed to \"see\" a human intelligence in the output of an LLM, because our brains are pattern-matching on \"things that look like human speech\". But that&#x27;s a reason to not anthropomorphize LLMs, precisely because people are inclined to do so without thinking. reply lacrimacida 9 hours agorootparentprevAnthropomorphizing is valid in relation with laguage, human language. Other than that anthropomorphizing would be somewhat valid with other human outputs but ideally it is understood it’s just one of the lenses in the toolkit, that it has pros and cons just like any idea or any tool. reply janalsncm 10 hours agorootparentprevThere’s so much baggage attached with it. The only thing a neural network “wants” is to minimize its loss function. That’s all. reply mitthrowaway2 8 hours agorootparentThe neural network doesn&#x27;t want to minimize its loss function any more than you want to maximize your inclusive genetic fitness.The neural network training process wants to minimize the neural network&#x27;s loss function. The neural network, if it \"wants\" anything, will have such wants as were embedded in its weights through the process of minimize its loss function, which will mostly be \"wants\" whose satisfactions correlate with reduced loss function. Of course this is using the term \"want\" in a behaviourally-descriptive sense, not a subjective-experience sense.For example, AlphaGo&#x27;s training routine wants to minimize AlphaGo&#x27;s loss function. AlphaGo wants to beat you at Go. reply srcreigh 9 hours agorootparentprevYou must not believe in human free will then. Or maybe you don’t believe the brain is the key aspect of human intelligence. Or believe the brain is more than just a organic NN reply janalsncm 9 hours agorootparentI’m saying that neural networks are so different from brains that anthropomorphizing them runs a real risk of glossing over important differences.Terms like “free will” and “intelligence” are too fuzzy to talk about precisely unless we’re on the exact same page regarding what we mean. And applying our imprecise definitions to machines is not doing us any favors. reply bobbylarrybobby 7 hours agorootparentprevAll humans “want” to do is obey the laws of physics. reply csours 9 hours agorootparentprevCan we anthropomorphize the loss function then? reply dragonwriter 9 hours agorootparentThat&#x27;s like anthropomorphizing utility functions in place of the humans that (are conjectured to) have them, isn&#x27;t it? reply aatd86 10 hours agoparentprevDo you mean that artificial neurons are inherently passive while biological neurons are inherently active i.e. they would act in spite of external input?Just wondering if I understood you, I don&#x27;t know anything on the subject. reply baobabKoodaa 10 hours agorootparentThat was already addressed in the comment you&#x27;re responding to. They wrote that in the case of artificial neural networks \"at least we know [there is no god inside the machine]\". With regards to humans, we don&#x27;t know. reply TacticalCoder 8 hours agorootparentThere are a great many possibilities between:- a fully deterministic machine (even if the interface and the way OpenAI let people access ChatGPT make it seem like it&#x27;s non-deterministic, there are fully deterministic models out there, who not only only respond to inputs but also always respond the exact same thing given the same inputs [query, seed, ...]),and:- god existsThere could be chaos at work when human thinks. There may be interferences at play, say because whatever element that traveled trillion of kilometers just traversed our brain.While a fully deterministic machine that always respond to the same input in the same way is just that: a deterministic machine.P.S: I don&#x27;t know about other LLMs like Falcon 180b but image-generation models like StableDiffusion are fully deterministic. I think a model is using a broken design and shall quickly hit limitations if it cannot be queried in a deterministic way (and its usecases are certainly limited if repeatability is not achievable). If you want different answers, use a different seed or a different query. But the same query+seed should always give the exact same output. reply syntaxfree 7 hours agorootparentAlso not all notions of God are dualistic — where you get to (notionally) talk to the guy using “you” and “I”. India’s Advaita Vedanta, Ibn Arabi’s version of Sufism and even some sects of Hasidism all hold that everything is in God. I haven’t found the Christianity that does this, but if it was discovered under Islam is probably thinkable here.Are these different ideas of God entirely? Yes: in India there are however many gods and Brahman says these are lesser precisely because they don’t include the whole universe. reply recursivecaveat 6 hours agorootparentprevFor the record LLM are theoretically fully deterministic, but non-deterministic in practice. First, some randomness is deliberately set via &#x27;temperature&#x27;, and second some randomness comes from things like the order of floating point operations when you divide the model on your GPU(s) without being super careful about it. reply red75prime 59 minutes agorootparentLLMs output probability distribution of the next token. And that automatically makes them non-deterministic. You can make their output deterministic by greedy sampling, fixing the seed of a pseudorandom generator, or by computing exponentially growing probability distribution of all possible continuations, but it doesn&#x27;t change the fact that LLMs produce probability distributions you need to sample somehow to get a definite result. reply aatd86 9 hours agorootparentprevThat wasn&#x27;t clear to me because having internal state doesn&#x27;t mean that any process is running. That&#x27;s the difference between memory and CPU. reply pengstrom 10 hours agorootparentprevI&#x27;d say &#x27;in spite&#x27;-ness is pretty spot on for life in general reply benchaney 8 hours agoparentprevCan you be more specific about what particular anthropomorphizing you object to? The only place the author uses the word want is in describing the wants of humans. reply gremlinunderway 7 hours agoparentprevI think you&#x27;re being a bit pedantic here.Neurons also don&#x27;t \"respond\" to specific input either. They can&#x27;t speak or provide an answer to your input.These are all just abstract metaphors and analogies. Literally everything in computer science at some point or another is an abstract metaphor or analogy.When you look up the definition and etymology of \"input\", it says to \"put on\" or \"impose\" or \"feed data into the machine\". We&#x27;re not literally feeding the machine data, it doesn&#x27;t eat the data and subsist on it.You could go on and on and nitpick every single one of these, and I don&#x27;t think the use of \"want\" (i.e. anthropomorphizing the networks to have intent) is all that bad. reply ShamelessC 10 hours agoparentprevWait what are you referring to specifically? Any anthropomorphism in the article is _clearly_, clearly the author&#x27;s admitted simplification due to the incredible density of the subject matter.Given that, I honestly can&#x27;t find anything too upsetting.In any case, anthropomorphism is something I don&#x27;t mind, mostly. Is it misleading? For the layman. But the domain is one of modeling intelligence itself and there are many instances where an existing definition simply makes sense. This happens in lots of fields and causes similar amounts of frustration in those fields. So it goes. reply janalsncm 10 hours agorootparent> Humans also use neural nets to reason about concepts. We have a lot of neurons, but so does GPT-4.I feel this is an abuse of the language. Biological neurons and ANN neurons aren’t the same or even all that similar. Brains don’t do backprop for example. Only forward passes. There’s a zoo of neurotransmitters which change the behavior of individual neurons or regions in the brain. Unused neurons in the brain can be repurposed for other things (for example if your arm is amputated). reply og_kalu 9 hours agorootparent>Biological neurons and ANN neurons aren’t the same or even all that similar.They&#x27;re not the same but they&#x27;re definitely similar.>Brains don’t do backprop for example.We&#x27;ve developed numerous different learning algorithms that are biologically plausible, but they all kinda work like backpropagation but worse, so we stuck with backpropagation. We&#x27;ve made more complicated neurons that better resemble biological neurons, but it is faster and works better if you just add extra simple neurons, so we do that instead. Spiking neural networks have connection patterns more similar to what you see in the brain, but they learn slower and are tougher to work with than regular layered neural networks, so we use layered neural networks instead.The only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it.The secret of bird flight was wings. Not feathers. Not flapping. reply tsimionescu 8 hours agorootparent> The only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it.And yet, there&#x27;s no ANN that&#x27;s as good at interacting with the real world as the simplest worms we&#x27;ve studied, despite having many times more neurons than those worms have cells.We are clearly still missing some key pieces of the puzzle for intelligence, so claiming that the difference between ANNs and biological neurons is irrelevant is quite premature. We are far away from having an airfoil moment in AI research. reply janalsncm 8 hours agorootparentprevMy point isn’t about whether ANNs work, it’s that they’re so fundamentally different (both locally at the level of the neuron and globally at the level of the brain) that calling them both “neurons” is pretty imprecise.Silicon simulations of brains may “suffer” from being faithful but this also discounts the advantages that brains have. As I mentioned for example, brains can repurpose neurons for other tasks. Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.Brains also generally do not suffer from catastrophic forgetting in the same way that our simulations tend to. If I ask you to study a textbook on cats you won’t suddenly forget the difference between cats and dogs. reply greiskul 6 hours agorootparent> Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.Since the other comment already went for the evolution and structure angle, I&#x27;ll go for the other part. What single example? What test have you seen done on the brain capacity of few weeks old fetuses? Our brains start learning patterns in the world before we are even born. How much input does a baby receive every single second from it&#x27;s eyes and ears and every other sense?Even when you are \"analyzing\" a new object for the first time, you receive a continuous stream of sensory input of it. Our brain even requires that to work, if you put a single frame different in a fast enough display, most times you won&#x27;t even notice the extra frame and your brain will just ignore it. reply og_kalu 8 hours agorootparentprev>Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.There is not a single brain on earth that is the blank slate a typical ANN is. \"Brains generalize from one example\" is pretty dubious. Millions of years of evolution matter.>As I mentioned for example, brains can repurpose neurons for other tasks.Isn&#x27;t this just a matter of the practical distinction between training and inference and not some fundamental structural limitation ?>Brains also generally do not suffer from catastrophic forgetting in the same way that our simulations tend to.This suggests CF may well be a simple matter of scale - https:&#x2F;&#x2F;palm-e.github.io&#x2F;Since individual anns are much closer to synapses, we don&#x27;t have anything near the scale of the brain yet. reply janalsncm 6 hours agorootparent> There is not a single brain on earth that is the blank slate a typical ANN is.Of course structure matters, but biological neurons have far more degrees of freedom than those in ANNs. The fact that we even need to keep differentiating between the two is an indication that classifying both as “neurons” is not accurate.> Isn&#x27;t this just a matter of the practical distinction between training and inference and not some fundamental structural limitation?It’s a difference in capabilities of the things themselves. A biological neuron organically seeks out new connections. Sure we could program that into an ANN somehow but the fact that nodes in an ANN don’t have this capability out of the box is a fundamental difference.> CF may well be a simple matter of scaleFor a moment, a big enough network might be able to mirror an entire brain with the lottery ticket hypothesis. But if it takes two or ten or a thousand ANN neurons to simulate the degrees of freedom of a biological neuron, are they really the same? replylainga 10 hours agoparentprevWait till you find out how we talk about evolutionary adaptation... a couple analogies and elided concepts here and there and you&#x27;d swear Lamarck had smothered Darwin in his cradle reply jjw1414 9 hours agorootparentYes, Lamarck is alive and well. I frequently hear colleagues in biology with years of experience making lazy statements such as, \"Humans evolved to walk because...\" as if evolution is a conscious act. In many cases, these people are using the term evolution as shorthand for, \"Random mutations gave rise to new gene variants (alleles), and over numerous generations, natural selection leads to an increase in the prevalence of beneficial variants, specifically those that provide a reproductive advantage. The genetic variants that combined to enable bipedal locomotion provided such an advantage to the human population\". Evolution is a passive, natural process driven by genetic variation, environmental changes, and the differential reproductive success of individuals with advantageous traits. This in turn results in population changes. Evolution doesn&#x27;t involve a conscious choice or direction. Evolution is the result of the cumulative effects of these factors over long periods of time. reply dekhn 8 hours agorootparentWhile I generally agree with you, is it really \"lazy\" to say \"humans evolved to walk to\"? Usually in the context, it&#x27;s not being used to claim there is some sort of intent or purpose, but rather to shortcut the (rather verbose) description you gave.Also I can think of some counterpoints to yours: the people who bred teosinte into corn (or any wild grain into a domesticated one) appear to be making conscious choices or direction- that is, they used their intelligence and reasoning from observed examples of pairings to conclude that they could make improved specimens based on selective breeding (without knowing about random mutations of natural selection!).And if we start to modify human germline then would also be an example of evolution with conscious choice or direction (assuming the modifications became fixed in the population). reply Sharlin 1 hour agorootparentprevEven in math, the most exact of all fields of study, people in practice are being nonrigorous and ambiguous and take shortcuts all the time. That’s just how communication works, because it would be incredibly grating and inefficient to try to be 100% unambiguous when everybody already knows what you mean. reply hoseja 1 hour agorootparentprevSocial, memetic evolution is at least partially Lamarckian. reply drones 9 hours agoparentprevAll this anthropomorphizing of humans strikes me as very odd. None of these humans \"want\" to do anything. They respond to specific input. Maybe artificial neural networks are the same, but in the case of humans we at least know it&#x27;s a simple reaction to neurotransmitter signals. reply arolihas 9 hours agorootparentYou&#x27;re probably just being tongue in cheek here, but I still wanted to add that we humans do actually \"want\". Desires, intentions, these are real things. You can try to represent them as loss functions or rewards in some mathematical model but that isn&#x27;t the original thing. Consider this excerpt from https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=7094#comment-1947377> Most animals are goal-directed, intentional, sensory-motor agents who grow interior representations of their environments during their lifetime which enables them to successfully navigate their environments. They are responsive to reasons their environments affords for action, because they can reason from their desires and beliefs towards actions.In addition, animals like people, have complex representational abilities where we can reify the sensory-motor “concepts” which we develop as “abstract concepts” and give them symbolic representations which can then be communicated. We communicate because we have the capacity to form such representations, translate them symbolically, and use those symbols “on the right occasions” when we have the relevant mental states.(Discrete mathematicians seem to have imparted a magical property to these symbols that *in them* is everything… no, when I use words its to represent my interior states… the words are symptoms, their patterns are coincidental and useful, but not where anything important lies).In other words, we say “I like ice-cream” because: we are able to like things (desire, preference), we have tasted ice-cream, we have reflected on our preferences (via a capacity for self-modelling and self-directed emotional awareness), and so on. And when we say, “I like ice-cream” it’s *because* all of those things come together in radically complex ways to actually put us in a position to speak truthfully about ourselves. We really do like ice-cream. reply Zambyte 8 hours agorootparent> but I still wanted to add that we humans do actually \"want\".I would also like to add that the subject of conversation is artificial and natural neurons, which humans, though contain some, are not.If a NN is trained to do something, it can be equally considered as \"wanting\" to do that thing within the autonomy it is afforded, as much as any human. reply TerrifiedMouse 8 hours agorootparent> If a NN is trained to do something, it can be equally considered as \"wanting\" to do that thing within the autonomy it is afforded, as much as any human.Human wants are driven by instinct though - i.e. our preferences; if you like women, you like women, if you don’t, you don’t.Our “output” is in service to those wants.Current AI doesn’t have instinct &#x2F; preprogrammed goals - except for goal-driven AIs but the hyped up LLMs aren’t such AIs. Their output isn’t motivated by any goal - a LLM can’t deliberately lie to you to get you to do something; it lies because it doesn’t differentiate between what’s true and what’s false. reply Zambyte 6 hours agorootparent> Human wants are driven by instinct thoughWhat is instinct physically?> a LLM can’t deliberately lie to you to get you to do something; it lies because it doesn’t differentiate between what’s true and what’s false.A LLM also can&#x27;t do multi-step reasoning, yet here we are. reply bongodongobob 8 hours agorootparentprevIf my grandmother had wheels, she&#x27;d be a bike.Fish don&#x27;t like ice cream and we don&#x27;t feel the need to spawn. It&#x27;s because of how we are built. reply oivey 9 hours agorootparentprevThis is the definition of anthropomorphism: “Anthropomorphism is the attribution of human traits, emotions, or intentions to non-human entities.” By definition the point you’re making doesn’t make sense. reply EMM_386 6 hours agorootparentprev> None of these humans \"want\" to do anything. They respond to specific inputWeird, because I&#x27;m pretty sure I had the choice whether to respond to this comment.It wasn&#x27;t the light waves hitting my retina from an HN post, leading to nerves firing and neurotransmitters all coming together to post this.I posted it because I have free will. I almost didn&#x27;t.Unless you truly feel that there is no free will, and that reality is just a bizarre movie we have to experience .... well, then ... we&#x27;ll disagree. reply mitthrowaway2 4 hours agorootparentAt some point after the light waves hit your retina, did one or more of the atoms in your body not obey the exact same physical laws as the atoms outside your body? reply DonsDiscountGas 8 hours agoprevIsn&#x27;t this also(just?) a description of how high-dimensional embedding spaces work? Putting every kind of concept all in the same space is going to lead to some weird stuff. Different regions of the latent space will cover different concepts, with very uneven volumes, and local distances will generally be meaningful (red vs green) but long-distances won&#x27;t (red vs. ennui).I guess we could also look at it the other way; embedding spaces work this way because the underlying neurons work this way. reply nl 10 hours agoprevPersonally I find the original paper much better written and easier to understand: https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2023&#x2F;monosemantic-features&#x2F;... reply zone411 2 hours agoparentIf you have a background in ML, then yes, a paper is almost always better. I&#x27;ve recommended papers over sources like Towards Data Science etc. here. However, for laypeople, I doubt it would be as effective - they&#x27;d need to look up terms like MLP, ReLU, UMAP, Logit, or even what an activation function is, and they are the target audience of this post. reply error9348 10 hours agoprev> No one knows how it works. Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want.There is a distinction to be made in \"knowing how it works\" on architecture vs weights themselves. reply shermantanktop 11 hours agoprevAs described in the post, this seems quite analogous to the operation of a bloom filter, except each \"bit\" is more than a single bit&#x27;s worth of information, and the match detection has to do some thresholding&#x2F;ranking to select a winner.That said, the post is itself clearly summarizing much more technical work, so my analogy is resting on shaky ground. reply turtleyacht 12 hours agoprevBy the same token, thinking in memes all the time may be a form of impoverished cognition.Or, is it enhanced cognition, on the part of the interpreter having to unpack much from little? reply throwanem 11 hours agoparentDarmok and Jalad at Mar-a-Lago. reply kibwen 8 hours agorootparentShaka, paying to build the walls. reply dekhn 7 hours agoparentprev>By the same token, thinking in memes all the time may be a form of impoverished cognition.I would recast this: any thinking is a linear superposition of weighted tropes. If you read TVTropes enough you&#x27;ll start to realize that the site doesn&#x27;t just describe TV plots, but basically all human interaction and thought, nicely clustered into nearly orthogonal topics. Almost anything you can say can be expressed by taking a few tropes and combining them with weights. reply aatd86 11 hours agoparentprevSome kind of single context abstract interpretation maybe. reply s1gnp0st 11 hours agoprev> Shouldn’t the AI be keeping the concept of God, Almighty Creator and Lord of the Universe, separate from God-This seems wrong. God-zilla is using the concept of God as a superlative modifier. I would expect a neuron involved in the concept of godhood to activate whenever any metaphorical \"god-of-X\" concept is being used. reply Sniffnoy 11 hours agoparentI mean, it&#x27;s not actually. It&#x27;s just a somewhat unusual transcription (well, originally somewhat unusual, now obviously it&#x27;s the official English name) of what might be more usually transcribed as \"Gojira\". reply s1gnp0st 11 hours agorootparentAh, I thought the Japanese word was just \"jira\". My mistake. reply postmodest 11 hours agorootparentThat&#x27;s an entirely different monster. reply eichin 11 hours agorootparentIndeed, but not an entirely unrelated one though - per https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jira_(software)#Naming the inspiration path was Bugzilla -> Godzilla -> Gojira -> Jira (which is why Confluence keeps correcting me when I try to spell it JIRA) reply VinLucero 11 hours agorootparentprevI see what you did there. reply csours 9 hours agoprevI feel like we&#x27;re a few more paradigm shifts away from self-driving cars, and this is one of them - being able to actually understand neural nets and modify them in a constructive way more directly - aka engineering.Some more: cheaper sensors (happening now) better sensor integration (happening now, kind of) better tools for ml grokking and intermediate engineering (this article, kind of) better tools for layering ml (probably the same thing as above) a new model for insurance&#x2F;responsibility&#x2F;something like this (unsure) better communication with people inside and outside the car (barely on the radar) reply Merrill 9 hours agoprevWhen LLMs are trained on text, are the words annotated to indicate the semantic meaning, or is the LLM training process expected to disambiguate the possibly hundreds of semantic meanings of an individual common word such as \"run\"? reply OkayPhysicist 9 hours agoparentThe task LLMs are trained on is \"predict the next word\", which elegantly is included for free in your training set of text. Typically no annotation is provided, since that would involve a ton of human labor doing the annotations. reply Merrill 8 hours agorootparentCould you ask a specialized AI to define each of the words in a block of text to automate the meaning extraction? reply chrissnow2023 1 hour agoprevCOOL~ interpreting a big AI with a bigger is like interpreting 42 with Earth~ reply kevdozer1 10 hours agoprevsuch a wonderful article, really enjoyed reading it reply keithalewis 49 minutes agoprev[disclaimer: after talking to many people much smarter than me, I might, just barely, sort of understand this. Any mistakes below are my own]Thanks for the heads up! AI is better at making up stories than humans already. Hodl my beer while I go buy some BSitCoins. reply adamnemecek 10 hours agoprevSuperposition makes sense when you understand all of ML as a convolution. reply blovescoffee 7 hours agoparentYou can just note the fact that two bits can represent four values. reply adamnemecek 6 hours agorootparentCan you explain machine learning through this lens? reply lngnmn2 4 hours agoprevEvery training set will produce a different set of weights, even the same training set with produce different weights with different initialization, leave alone slightly different architectures.So what exactly is the point, except \"look at us, we are so clever\"? reply asylteltine 10 hours agoprevI’ve never understood the hidden layers argument. Ultimately these models are executing code. You can examine the code. Why can’t that be done? reply OkayPhysicist 9 hours agoparentFundamentally, artificial neural networks boil down to some code that can be written, mathematically, asσ(C*σ(B*σ(A*x)))... = yrepeated matrix-vector multiplication, separated by nonlinear functions, σ(). A,B,C etc here are your weights, x is your input vector, y is your output vector. Any nonlinear function will work, some behave better than others, but they must be present, otherwise you could simplify the problem by apply the associative property to the weights side of things, and you&#x27;d effectively have a single layer that can only simulate linear functions. A hidden layer is just any vector that is intermediate to that calculation, for example the result of σ(A*x) would be the first hidden layer.So right off the bat, you obviously have a problem: \"Examining the code\" means examining the weights, which are just gigabyte&#x2F;terabyte sized matrices. Opaque is putting it mildly. The only sane approach is to start from one of our known-meaningful vectors (the input or output), and work our way inwards from there, either seeing what elements of hidden layer vectors have the most significant value when a certain input is applied, or determining what hidden layer vector values produce values closest resembling a desired output.Then you start running into the problems described in the article. reply janalsncm 9 hours agorootparentThis is a really good explanation for a fully connected network. Most models will have more complex architectures so are even more complicated than this. reply OkayPhysicist 9 hours agorootparentI haven&#x27;t been great about keeping up with advances in the field, but to my understanding most if not all architectures in effect merely enforce symmetries upon the network. That is, they can be represented by a fully connected network but in that representation not all weights are free, in that some are fixed (0 or 1) or some are dependent (A(1,1) will also be equal to C(1,1)). reply janalsncm 9 hours agorootparentI don’t blame you, things are pretty diversified so I’m white knuckling it through my own subfield.But as a simple example, convolution would be a little tedious to describe in the general notation you wrote above without getting into the image dimensions, stride, padding etc., not to mention residual layers or norm layers that are commonly used. Then there are things like stop gradient, dropout or even other training targets which are only used during training but not inference. reply OkayPhysicist 8 hours agorootparentConvolution actually can be pretty elegantly translated into multiplication by a matrix with symmetries. It&#x27;s been a few years, but that was an example we had to work out by hand in my Deep Learning course at university.It&#x27;s pretty much the quintessential example of an enforced symmetry, in that it introduces a symmetry against translation. replyafthonos 10 hours agoparentprevThink of it this way: looking at the neurons is like looking at the assembly and trying to understand how a video is played on YouTube. Yes, in theory it’s possible, but realistically no one would be able to do that. At least not in a reasonable amount of time. reply astrange 6 hours agorootparentVideo decoding works by reading and writing pixels to the same buffers you use for displaying them, so you could see a lot of how it works by watching the accesses there. reply drdeca 9 hours agoparentprevIf by “code” you mean “the source code people wrote”, then, that source code by itself does not determine the behavior. The behavior is determined by the numbers.Saying that understanding that code implies understanding how the network works, is like saying “John wrote this emulator for GBA games, therefore he must understand how the game [some GBA game] works”.Except, instead of the game being written in assembly, it was written in malbolge. reply cwillu 9 hours agoparentprev“This is great for AIs but bad for interpreters. We hoped we could figure out what our AIs were doing just by looking at them. But it turns out they’re simulating much bigger and more complicated AIs, and if we want to know what’s going on, we have to look at those. But those AIs only exist in simulated abstract hyperdimensional spaces. Sounds hard to dissect!”“Still, last month Anthropic’s interpretability team announced that they successfully dissected of one of the simulated AIs in its abstract hyperdimensional space.(finally, we’re back to the monosemanticity paper!)First the researchers trained a very simple 512-neuron AI to predict text, like a tiny version of GPT or Anthropic’s competing model Claude.Then, they trained a second AI called an autoencoder to predict the activations of the first AI. They told it to posit a certain number of features (the experiments varied between ~2,000 and ~100,000), corresponding to the neurons of the higher-dimensional AI it was simulating. Then they made it predict how those features mapped onto the real neurons of the real AI.They found that even though the original AI’s neurons weren’t comprehensible, the new AI’s simulated neurons (aka “features”) were! They were monosemantic, ie they meant one specific thing.Here’s feature #2663 (remember, the original AI only had 512 neurons, but they’re treating it as simulating a larger AI with up to ~100,000 neuron-features).” reply quickthrower2 7 hours agoparentprevWith these models, the Pytorch code is an interpreter, and the 100Gb of weights is the code. So you have say a 10G (10 weights = 1 LOC) LOC codebase, all in assembly, no comments. G&#x27;luck.Oh and it doesn&#x27;t always use logic. So there are no ORs and ANDs and IFs, just +, *, exp, max, etc. reply ggm 5 hours agoprev [–] As long as you read it with the skeptics \"yes, but it&#x27;s not intelligence\" its a good read.It&#x27;s when you read it with the \"at last, I can understand how reasoning and inference with meaning is going to emerge from this\" you have a problem.It&#x27;s a great read but what you bring to it, informs what you take from it. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Understanding artificial intelligence (AI) is a significant challenge due to the complexity of its neural networks, containing billions of neurons.",
      "Researchers are making progress in deciphering AI's inner workings, but the concept of superposition, where a few neurons can represent multiple concepts, adds to the difficulties.",
      "The article explores the need for transparency and safety in AI, discussing interpretability techniques, the potential development of an interpreter-AI, and the concept of feature synergy in convolutional neural networks (CNNs)."
    ],
    "commentSummary": [
      "The discussion covers various aspects of artificial intelligence (AI) and its connection to human cognition, including the complexity of the human brain and potential advancements in neurology and machine learning.",
      "Topics also include the use of group theory in neural networks, different techniques in AI, and the comparison between biological neurons and artificial neural networks.",
      "The conversation explores the impact of substrate on computation, the anthropomorphization of neural networks, determinism in machines, and the differences between biological brains and artificial neural networks.",
      "Additionally, the concept of evolution and the challenges in understanding neural networks are discussed, along with the limitations and potential of AI and the training of language models.",
      "The conversation ultimately underlines the complexities, differences, and potential advancements in AI and human cognition."
    ],
    "points": 258,
    "commentCount": 123,
    "retryCount": 0,
    "time": 1701119086
  },
  {
    "id": 38434470,
    "title": "Unlocking JavaScript Framework Lock-In with Web Components",
    "originLink": "https://jakelazaroff.com/words/web-components-eliminate-javascript-framework-lock-in/",
    "originBody": "Web Components Eliminate JavaScript Framework Lock-in November 27, 2023 #javascript #react #solid #svelte Table of Contents What’s a Web Component? Scaffolding Layout Adding Todos Todo Items Filtering Todos Moving Forward Reading List We’ve seen a lot of great posts about web components lately. Many have focused on the burgeoning HTML web components pattern, which eschews shadow DOM in favor of progressively enhancing existing markup. There’s also been discussion — including this post by yours truly — about fully replacing JavaScript frameworks with web components. Those aren’t the only options, though. You can also use web components in tandem with JavaScript frameworks. To that end, I want to talk about a key benefit that I haven’t seen mentioned as much: web components can dramatically loosen the coupling of JavaScript frameworks. To prove it, we’re going to do something kinda crazy: build an app where every single component is written with a different framework. It probably goes without saying that you should not build a real app like this! But there are valid reasons for mixing frameworks. Maybe you’re gradually migrating from React to Vue. Maybe your app is built with Solid, but you want to use a third-party library that only exists as an Angular component. Maybe you want to use Svelte for a few “islands of interactivity” in an otherwise static website. Here’s what we’re going to create: a simple little todo app based loosely on TodoMVC. As we build it, we’ll see how web components can encapsulate JavaScript frameworks, allowing us to use them without imposing broader constraints on the rest of the application. What’s a Web Component? In case you’re not familiar with web components, here’s a brief primer on how they work. First, we declare a subclass of HTMLElement in JavaScript. Let’s call it MyComponent: class MyComponent extends HTMLElement { constructor() { super(); this.shadow = this.attachShadow({ mode: \"open\" }); } connectedCallback() { this.shadow.innerHTML = ` Hello from a web component!p { color: pink; font-weight: bold; padding: 1rem; border: 4px solid pink; }`; } } That call to attachShadow in the constructor makes our component use the shadow DOM, which encapsulates the markup and styles inside our component from the rest of the page. connectedCallback is called when the web component is actually connected to the DOM tree, rendering the HTML contents into the component’s “shadow root”. This foreshadows how we’ll make our frameworks work with web components.1 We normally “attach” frameworks to a DOM element, and let the framework take over all ancestors of that element. With web components, we can attach the framework to the shadow root, which ensures that it can only access the component’s “shadow tree”. Get it? Foreshadows? Foreshadows? Like shadow DOM? ↩ Next, we define a custom element name for our MyComponent class: customElements.define(\"my-component\", MyComponent); Whenever a tag with that custom element name appears on the page, the corresponding DOM node is actually an instance of MyComponent!const myComponent = document.querySelector(\"my-component\"); console.log(myComponent instanceof MyComponent); // true Check it out: There’s more to web components, but that’s enough to get you through the rest of the article. Scaffolding Layout The entrypoint of our app will be a React component.2 Here’s our humble start: Technically, we’re using Preact in compatibility mode, because I couldn’t figure out how to get Vite’s React preset to work. It turns out that build tooling gets tricky when you try to use four different frameworks in one codebase! ↩ // TodoApp.jsx export default function TodoApp() { return ; } We could start adding elements here to block out the basic DOM structure, but I want to write another component for that to show how we can nest web components in the same way we nest framework components. Most frameworks support composition via nesting like normal HTML elements. From the outside, it usually looks something like this:On the inside, there are a few ways that frameworks handle this. For example, React and Solid give you access to those children as a special children prop: function Card(props) { return {props.children}; } With web components that use shadow DOM, we can do the same thing using theelement. When the browser encounters a , it replaces it with the children of the web component.is actually more powerful than React or Solid’s children. If we give each slot a name attribute, a web component can have multiple s, and we can determine where each nested element goes by giving it a slot attribute matching the ’s name. Let’s see what this looks like in practice. We’ll write our layout component using Solid: // TodoLayout.jsx import { render } from \"solid-js/web\"; function TodoLayout() { return ( ); } customElements.define( \"todo-layout\", class extends HTMLElement { constructor() { super(); this.shadow = this.attachShadow({ mode: \"open\" }); } connectedCallback() { render(() => , this.shadow); } } ); There are two parts to our Solid web component: the web component wrapper at the top, and the actual Solid component at the bottom. The most important thing to notice about the Solid component is that we’re using named s instead of the children prop. Whereas children is handled by Solid and would only let us nest other Solid components, s are handled by the browser itself and will let us nest any HTML element — including web components written with other frameworks! The web component wrapper is pretty similar to the example above. It creates a shadow root in the constructor, and then renders the Solid component into it in the connectedCallback method. Note that this is not a complete implementation of the web component wrapper! At the very least, we’d probably want to define an attributeChangedCallback method so we can re-render the Solid component when the attributes change. If you’re using this in production, you should probably use a package Solid provides called Solid Element that handles all this for you. Back in our React app, we can now use our TodoLayout component: // TodoApp.jsx export default function TodoApp() { return (Todos); } Note that we don’t need to import anything from TodoLayout.jsx — we just use the custom element tag that we defined. Check it out: That’s a React component rendering a Solid component, which takes a nested React element as a child. Adding Todos For the todo input, we’ll peel the onion back a bit further and write it with no framework at all! // TodoInput.js customElements.define(\"todo-input\", TodoInput); class TodoInput extends HTMLElement { constructor() { super(); this.shadow = this.attachShadow({ mode: \"open\" }); } connectedCallback() { this.shadow.innerHTML = ``; this.shadow.querySelector(\"form\").addEventListener(\"submit\", evt => { evt.preventDefault(); const data = new FormData(evt.target); this.dispatchEvent(new CustomEvent(\"add\", { detail: data.get(\"text\") })); evt.target.reset(); }); } } Between this, the example web component and our Solid layout, you’re probably noticing a pattern: attach a shadow root and then render some HTML inside it. Whether we hand-write the HTML or use a framework to generate it, the process is roughly the same. Here, we’re using a custom event to communicate with the parent component. When the form is submitted, we dispatch an add event with the input text. Event queues are often used to decouple communication between components of a software system. Browsers lean heavily on events, and custom events in particular are an important tool in the web components toolbox — especially so because the custom element acts as a natural event bus that can be accessed from outside the web component. Before we can continue adding components, we need to figure out how to handle our state. For now, we’ll just keep it in our React TodoApp component. Although we’ll eventually outgrow useState, it’s a perfect place to start. Each todo will have three properties: an id, a text string describing it, and a done boolean indicating whether it’s been completed. // TodoApp.jsx import { useCallback, useState } from \"react\"; let id = 0; export default function TodoApp() { const [todos, setTodos] = useState([]); export function addTodo(text) { setTodos(todos => [...todos, { id: id++, text, done: false }]); } const inputRef = useCallback(ref => { if (!ref) return; ref.addEventListener(\"add\", evt => addTodo(evt.detail)); }, []); return (Todos ); } We’ll keep an array of our todos in React state. When we add a todo, we’ll add it to the array. The one awkward part of this is that inputRef function. Ouremits a custom add event when the form is submitted. Usually with React, we’d attach event listeners using props like onClick — but that only works for events that React already knows about. We need to listen for add events directly.3 This process is easier with other frameworks. With Svelte, for example, we can use the on: directive to listen to arbitrary events emitted from any HTML element, including web components. ↩ In React Land, we use refs to directly interact with the DOM. We most commonly use them with the useRef hook, but that’s not the only way! The ref prop is actually just a function that gets called with a DOM node. Rather than passing a ref returned from the useRef hook to that prop, we can instead pass a function that attaches the event listener to the DOM node directly. You might be wondering why we have to wrap the function in useCallback. The answer lies in the legacy React docs on refs (and, as far as I can tell, has not been brought over to the new docs): If the ref callback is defined as an inline function, it will get called twice during updates, first with null and then again with the DOM element. This is because a new instance of the function is created with each render, so React needs to clear the old ref and set up the new one. You can avoid this by defining the ref callback as a bound method on the class, but note that it shouldn’t matter in most cases. In this case, it does matter, since we don’t want to attach the event listener again on every render. So we wrap it in useCallback to ensure that we pass the same instance of the function every time. Todo Items So far, we can add todos, but not see them. The next step is writing a component to show each todo item. We’ll write that component with Svelte. Svelte supports custom elements out of the box. Rather than continuing to show the same web component wrapper boilerplate every time, we’ll just use that feature! Here’s the code: import { createEventDispatcher } from \"svelte\"; export let id; export let text; export let done; const dispatch = createEventDispatcher(); $: dispatch(\"check\", { id, done }); {text}dispatch(\"delete\", { id })}>With Svelte, the tag isn’t literally rendered to the DOM — instead, that code runs when the component is instantiated. Our Svelte component takes three props: id, text and done. It also creates a custom event dispatcher, which can dispatch events on the custom element. The $: syntax declares a reactive block. It means that whenever the values of id or done change, it will dispatch a check event with the new values. id probably won’t change, so what this means in practice is that it’ll dispatch a check event whenever we check or uncheck the todo. Back in our React component, we loop over our todos and use our newcomponent. We also need a couple more utility functions to remove and check todos, and another ref callback to attach the event listeners to each . Here’s the code: // TodoApp.jsx import { useCallback, useState } from \"react\"; let id = 0; export default function TodoApp() { const [todos, setTodos] = useState([]); export function addTodo(text) { setTodos(todos => [...todos, { id: id++, text, done: false }]); } export function removeTodo(id) { setTodos(todos => todos.filter(todo => todo.id !== id)); } export function checkTodo(id, done) { setTodos(todos => todos.map(todo => (todo.id === id ? { ...todo, done } : todo))); } const inputRef = useCallback(ref => { if (!ref) return; ref.addEventListener(\"add\", evt => addTodo(evt.detail)); }, []); const todoRef = useCallback(ref => { if (!ref) return; ref.addEventListener(\"check\", evt => checkTodo(evt.detail.id, evt.detail.done)); ref.addEventListener(\"delete\", evt => removeTodo(evt.detail.id)); }, []); return (Todos{todos.map(todo => ())}); } Now the list actually shows all our todos! And when we add a new todo, it shows up in the list! Filtering Todos The last feature to add is the ability to filter todos. Before we can add that, though, we need to do a bit of refactoring. I want to show another way that web components can communicate with each other: using a shared store. Many of the frameworks we’re using have their own store implementations, but we need a store that we can use with all of them. For that reason, we’ll use a library called Nano Stores. First, we’ll make a new file called store.js with our todo state rewritten using Nano Stores: // store.js import { atom, computed } from \"nanostores\"; let id = 0; export const $todos = atom([]); export const $done = computed($todos, todos => todos.filter(todo => todo.done)); export const $left = computed($todos, todos => todos.filter(todo => !todo.done)); export function addTodo(text) { $todos.set([...$todos.get(), { id: id++, text }]); } export function checkTodo(id, done) { $todos.set($todos.get().map(todo => (todo.id === id ? { ...todo, done } : todo))); } export function removeTodo(id) { $todos.set($todos.get().filter(todo => todo.id !== id)); } export const $filter = atom(\"all\"); The core logic is the same; most of the changes are just porting from the useState API to the Nano Stores API. We did add two new computed stores, $done and $left, which are “derived” from the $todos store and return completed and incomplete todos, respectively. We also added a new store, $filter, which will hold the current filter value. We’ll write our filter component with Vue. import { useStore, useVModel } from \"@nanostores/vue\"; import { $todos, $done, $left, $filter } from \"./store.js\"; const filter = useVModel($filter); const todos = useStore($todos); const done = useStore($done); const left = useStore($left);All ({{ todos.length }}) Todo ({{ left.length }}) Done ({{ done.length }})The syntax is pretty similar to Svelte’s: the tag at the top is run when the component is instantiated, and thetag contains the component’s markup. Vue doesn’t make compiling a component to a custom element quite as simple as Svelte does. We need to create another file, import the Vue component and call defineCustomElement on it: // TodoFilters.js import { defineCustomElement } from \"vue\"; import TodoFilters from \"./TodoFilters.ce.vue\"; customElements.define(\"todo-filters\", defineCustomElement(TodoFilters)); Back in React Land, we’ll refactor our component to use Nano Stores rather than useState, and bring in thecomponent: // TodoApp.jsx import { useStore } from \"@nanostores/react\"; import { useCallback } from \"react\"; import { $todos, $done, $left, $filter, addTodo, removeTodo, checkTodo } from \"./store.js\"; export default function App() { const filter = useStore($filter); const todos = useStore($todos); const done = useStore($done); const left = useStore($left); const visible = filter === \"todo\" ? left : filter === \"done\" ? done : todos; const todoRef = useCallback(ref => { if (!ref) return; ref.addEventListener(\"check\", evt => checkTodo(evt.detail.id, evt.detail.done)); ref.addEventListener(\"delete\", evt => removeTodo(evt.detail.id)); }, []); const inputRef = useCallback(ref => { if (ref) ref.addEventListener(\"add\", evt => addTodo(evt.detail)); }, []); return (Todos {visible.map(todo => ())}); } We did it! We now have a fully functional todo app, written with four different frameworks — React, Solid, Svelte and Vue — plus a component written in vanilla JavaScript. Moving Forward The point of this article is not to convince you that this is a good way to write web apps. It’s to show that there are ways to build a web app other than writing the entire thing with a single JavaScript framework — and furthermore, that web components actually make it significantly easier to do that. You can progressively enhance static HTML. You can build rich interactive JavaScript “islands” that naturally communicate with hypermedia libraries like HTMX. You can even wrap a web component around a framework component, and use it with any other framework. Web components drastically loosen the coupling of JavaScript frameworks by providing a common interface that all frameworks can use. From a consumer’s point of view, web components are just HTML tags — it doesn’t matter what goes on “under the hood”. If you want to play around with this yourself, I’ve made a CodeSandbox with our example todo app. Reading List If you’re interested, here are some good articles that dive even deeper into the topic: Chris Ferdinandi wrote about wrapping his own UI library Reef with a web component in Reactive Web Components and DOM Diffing. Andrico Karoulla wrote a great overview of how to write framework-agnostic components aptly titled Writing Components That Work in Any Framework. Thomas Wilburn shows how to use web components to build “languages” within HTML in Chiaroscuro, or Expressive Trees in Web Components. Maxi Ferreira wrote a wonderful article called Sharing State with Islands Architecture that goes into detail more about custom events and stores. The official Astro documentation has a page on sharing state between islands using Nano Stores. Although it doesn’t explicitly mention web components, the HTMX essay on hypermedia-friendly scripting brings up events and islands as ways for client-side scripting to interact with hypermedia-driven web applications. Like what you read? Subscribe to my RSS feed , or follow me on Mastodon or Twitter .",
    "commentLink": "https://news.ycombinator.com/item?id=38434470",
    "commentBody": "Web Components Eliminate JavaScript Framework Lock-InHacker NewspastloginWeb Components Eliminate JavaScript Framework Lock-In (jakelazaroff.com) 248 points by jakelazaroff 17 hours ago| hidepastfavorite249 comments djrenren 16 hours agoI&#x27;ve found webcomponents to be really good at encapsulating anything that doesn&#x27;t directly query application state.Specifically there are two type of components that really thrive as web components (as opposed to react):1. Highly interactive components - Components that implement complex interaction management (but sort of agnostic to application state) are ideal web components. You don&#x27;t need to mess around with `useEffect` or `useMemo`. You get really tight control of rendering and state updates.2. Highly internally stateful components - Components that track a lot of state that doesn&#x27;t escape the component, work great as web component. You get strong encapsulation without a framework requirement.React conflates two concepts that I think are better when separated: templates, and components. Templates provide a \"re-render the world\" approach, and components encapsulate state and interaction patterns. Conflating these two things causes the standard useEffect and useMemo headaches. It also means that any consumer of your component, must also use your templating system (react&#x27;s render function).The `lit` library does this separation extremely well, allowing you to implement components using templates if you want, or not. And consumers do not care how the internals are implemented. reply paulddraper 14 hours agoparent> React conflates two concepts that I think are better when separated: templates, and components.Whether a React code base conflates them depends on the code base, but the more familiar terminology is container vs presentation. [1][1] https:&#x2F;&#x2F;www.patterns.dev&#x2F;react&#x2F;presentational-container-patt... reply djrenren 14 hours agorootparentYou can absolutely follow a discipline like this which recreates this separation in a framework that lacks it reply willio58 13 hours agoparentprevI completely agree. Recently I listened to a podcast where Brad Frost talked about web components being useful for design systems, as you can make a button in a web component and have that defined no matter what JS framework the dev team (or teams) want to use company-wide.One issue I see though and I almost feel dumb for saying it, I don&#x27;t like how web component code looks. Using innerHTML feels really weird when your team is so used to using JSX for react components for example. I don&#x27;t think this would stop me from researching web components more but does anyone feel similarly or have a solution or obvious answer for me in this area? reply zaphar 12 hours agorootparentYou don&#x27;t have to use innerHtml. The ShadowRoot pretty much acts like a document. So you can also use appendChild. Combined with a template element you can completely avoid innheHtml. You could even use jsx to create the template element I suppose.That said I think this is mostly a non-issue. reply nbe 13 hours agorootparentprevlit [1] provides declarative templates for web components for example.[1] https:&#x2F;&#x2F;lit.dev&#x2F; reply itronitron 12 hours agorootparentprevFor me, it feels weird to have four file types I need to manage in order to define a UI, instead of just one. reply djrenren 16 hours agoparentprevForgot to mention, the knock-on effect of this thought process is that if you want to adopt web components, you still need a templating approach. And you&#x27;ll find that most of your code is templates, and only a few are really components. reply troupo 16 hours agoparentprev> You don&#x27;t need to mess around with `useEffect` or `useMemo`. You get really tight control of rendering and state updates.You don&#x27;t need those in React either. Whatever you do in Web Components can probably (most likely) be done in React.After all, Web Components are a solidified 2010-era design. The reason React has hooks now because people have moved on, and are exploring other ways of building stuff. The only mistake React did was to keep reactivity on component level (hence all the hooks and their weird rules) when most people moved on to more granular reactivity. Hell, even Angular did. reply djrenren 16 hours agorootparent> You don&#x27;t need those in React either.I mean... sometimes you do, that&#x27;s why they exist. But yeah, most of the time you don&#x27;t.> Whatever you do in Web Components can probably (most likely) be done in React.Of course! React is a good and powerful framework. But everything you do in React can be done in Angular 1.0 or even backbone.js. As always with frameworks it&#x27;s about the productivity &#x2F; performance ratio for your team (or for libraries, the ratio for your consumers).> After all, Web Components are a solidified 2010-era design.A lot of the web components related APIs are being actively developed including constructable stylesheets, shadow DOM APIs and more. Regardless, the era of design is not a great point in either the \"pro\" or \"con\" column, and is usually an ambiguous shorthand for the actual quality being critiqued. reply paulddraper 14 hours agorootparent> I mean... sometimes you do, that&#x27;s why they exist.They exist because they&#x27;re useful, but you can write traditional React components that look a lot closer to Web Components, if you think that&#x27;s better. reply jongjong 11 hours agoparentprevI find that they can work very well with state but since state is typically passed via attributes (downstream), it means that components can only share state with each other via strings. I found this to be an advantage, not a drawback because simple interfaces are at the core of my coding philosophy and strings make it infeasible to pass complex instances or functions to other components. reply djrenren 11 hours agorootparentWell, this is only sort of true. When you’re writing HTML you’re restricted to attributes but custom elements are JavaScript objects and are free to respond to property updates on the object. Relying solely on strings is a limitation of your templating engine.For example lit-html templates support syntax like: reply dbingham 16 hours agoprevOne of the linked articles [1] makes an interesting claim that feels like a reach to me, but I&#x27;d be interested in hearing HN&#x27;s take on it:> At this point React is legacy technology, like Angular. Lots of people are still using it, but nobody can quite remember why. The decision-makers in organisations who chose to build everything with React have long since left. People starting new projects who still decide to build on React are doing it largely out of habit.I&#x27;m going to withhold my own thoughts about that quote, cause I&#x27;m curious what everyone else thinks.Wrt. Web Components, I played around with Web Components about half a decade ago and they felt promising but not ready for prime time yet. They were pretty messy and felt like they did a poor job of encapsulation. (At least at the time.) I haven&#x27;t looked at them recently, but the code examples in these articles look much better (and cleaner) than what I remember.[1] https:&#x2F;&#x2F;adactio.com&#x2F;journal&#x2F;20618 reply menssen 16 hours agoparent\"Lots of people are still using it, but nobody can quite remember why.\"I can remember why. This, and every other article I&#x27;ve ever read arguing to replace React with Web Components, completely misunderstands the point of React. It isn&#x27;t about JSX. It isn&#x27;t about encapsulation. It isn&#x27;t about reusability.It is about enabling a design pattern where *the user interface is a pure functional transformation of the application state.*I kind of feel like people get tripped up by the fact that \"virtual DOM\" and \"shadow DOM\" sort of sound similar. They have literally nothing to do with each other. The React \"virtual DOM\" allows you to *completely re-output the entire user interface* on every state change, which is not possible with any other design pattern, and is not possible without a framework, because actually re-rendering the entire tree on every state change isn&#x27;t performative (or usable).Anybody who is advocating an alternative to React needs to do one of two things:(A) Make a convincing argument that a different design pattern is better. Some things we&#x27;ve tried, which most people think are worse:1. Using the DOM as your data model (jQuery)2. Manually writing virtual representations of every view (Backbone)3. Auto-magically two-way binding some data structure with the DOM (Angular 1)4. Observables (Ember, maybe Angular 2+?)(B) Advocate a framework other than React that uses the same pattern as React, but improves the usability. I think the two places there is the most room for improvements are:1. Animations2. useEffect() in generalI have not seen anybody successfully do either A or B.Recommended reading: https:&#x2F;&#x2F;acko.net&#x2F;blog&#x2F;get-in-zoomer-we-re-saving-react&#x2F; reply gitaarik 31 minutes agorootparentThe reason React uses a virtual DOM is because when React started, there were no (advanced) HTML templates yet. And it made it easy to setup listeners on elements, instead of manually adding it with `addEventListener()` and possibly remove them again with `removeEventListener()`. So the virtual DOM was really a game changer.But Lit templates solve these problems in a more browser integrated way, without the need of a virtual DOM. How you manage the state is free to your choice, that is also not something exclusive to React and your favorite pattern can also be used with Lit. I wrote a tiny state management library (LitState [0]) which makes it very easy for multiple components to share the same state and stay in sync. I personally find it much more convenient and cleaner than any other state library I&#x27;ve used before. And it integrates very nicely with Lit.[0]: https:&#x2F;&#x2F;github.com&#x2F;gitaarik&#x2F;lit-state reply WorldMaker 14 hours agorootparentprev> 4. Observables (Ember, maybe Angular 2+?)Don&#x27;t forget Knockout which was the OG. You can also make the case that Svelte, Vue, and Qwik all are different takes on Observables (at least as much as Angular 2+ is) all with more or less magic and more or fewer escape hatches from Observable best practices to imperative(-looking) code.I got a \"What if we did Knockout but with with the compile-time benefits of TSX and Pure RxJS Observables?\" itch a couple months back and have made some wild progress on it. I certainly don&#x27;t think it is ready yet to advocate as an alternative to React, but it&#x27;s probably in an interesting A4+B2 tangent on your map right here, and I find that interesting. reply jeremycarter 12 hours agorootparentI remember 2012, ASP.NET MVC Razor pages, combined with individual page knockout Js. Actually worked quite well. Infact that system is still running for that company. Sure we could have made a angularJS 1.0 SPA at the time, but knockout meant we only had to apply the pattern to the pages that needed it. reply WorldMaker 12 hours agorootparentI even used Durandal some in those cases where we really did want a SPA, but Knockout was good enough. That was something I appreciated about Knockout too was that the SPA framework was around it, not a part of it or implemented inside of it. Same with routing frameworks like Crossroads.js if you wanted something lighter than Durandal for just boring hash-navigation somewhere in the boundary spectrum between full SPA and MPA.It was something that was also attractive about early React that React was similarly just a view engine and could do MPA or SPA or things in between and it didn&#x27;t try to have the full kitchen sink. I don&#x27;t know exactly where React stopped being that, but it always feels harder to argue that current React is \"just\" a view engine.In the larger context here with this article, libraries that are just view engines should be great for building the internals of Web Components. (I&#x27;m hoping the view engine I&#x27;ve been working on might serve that role well, though with a dependency like RxJS I&#x27;m not sure if it would be towards the top of the list for many developers. It will certainly feel bigger than Lit, for example, no matter how well it tree shakes.) reply recursive 12 hours agorootparentprevI got the same itch. Here&#x27;s my thing. https:&#x2F;&#x2F;mutraction.dev&#x2F; reply WorldMaker 12 hours agorootparentInteresting, thanks. Your mutation tracker isn&#x27;t enough like \"pure\" RxJS Observables for my tastes and what I&#x27;ve been doing with my itch, but you captured a few of the things I&#x27;m covering in my system and are probably the next closest I&#x27;ve seen to what I&#x27;ve been doing (and I tried to research a deep dive). I think the only other thing is that I took an approach that observable change bindings look different from static HTML-like attributes. I did that in part because that&#x27;s how Knockout used to do it, and also in part because I think it should better facilitate SSG&#x2F;SSR&#x2F;progressive enhancement when I get back around to those ideas (it&#x27;s not a current priority, but definitely an idea I&#x27;m tracking). reply recursive 11 hours agorootparentI was also influenced quite a bit by knockout. SSR was explicitly not a goal of mine. To me, SSR as a feature in a framework like this is mostly interesting in the context of server&#x2F;client continuity like rehydration. But I have a feeling the constraints imposed by anything like this aren&#x27;t going to be worth it. If I was doing pure server rendering, flat text templates seem like the sweet spot, since it&#x27;s fundamentally not interactive. Render to html string and ship it. And if I was doing that, I probably would not choose to use javascript or observables&#x2F;signals. reply WorldMaker 10 hours agorootparentKnockout was designed in the heyday of the \"progressive enhancement\" era so it seems hard for me to claim a Knockout influence&#x2F;inspiration if I&#x27;m not at least contemplating \"progressive enhancement\" of some sort. SSR isn&#x27;t really my goal on that front but progressive enhancement&#x2F;SSG (static site generation) certainly is a side goal for me (that&#x27;s what I would like to have to self-host my own documentation site in something resembling the framework itself, rather than Jekyll or Docusaurus or whatever), but again not a main priority. It feels like the case for me where there is a lot of bleedover between SSR, SSG, and progressive enhancement and solving one basically solves all of them.> I probably would not choose to use javascript or observables&#x2F;signals.This seems to be a case where we differ. In one obvious part because I don&#x27;t tend to like signals and think they are very different from observables (because they are harder to encapsulate and offer fewer operators and tend to bleed too many accidental imperative escape hatches). Observables, to me, are just as useful to describe \"open this file, read its contents, convert it from Markdown to HTML, then bind it to innerHTML here\" on the server side as \"fetch this URL, read its contents, convert it from Markdown to HTML, then bind it to innerHTML here\" on the client side. Very different forms of interactivity, but there is still an interactivity there that a good observable pipeline can describe. Especially when you start to get into the idea that the difference between \"open this file\" and \"fetch this URL\" is tiny and can be dependency injected. Same component, slightly different inputs, slightly different expected outputs (you expect the server side binding to complete, whereas you expect the client side one to live for some time and continue to respond to different URLs from input). reply recursive 8 hours agorootparentI don&#x27;t know much about the state of the art SSR&#x2F;SSG, but in my mind, a whole document can be built synchronously, with a depth-first traversal of the templates&#x2F;components. From that perspective the power and the constraints of reactive UI seem like overkill and limiting at the same time.But anyway, the more UI frameworks, the more ideas get out there, the better. I&#x27;m hoping that some day in my lifetime, web UI gets \"solved\" and I&#x27;m not willing to believe that React is the answer. Does your project have a name yet? I&#x27;ll keep an eye out. reply WorldMaker 5 hours agorootparentJust because on most modern hardware file I&#x2F;O has millisecond latency and feels synchronous doesn&#x27;t mean it is synchronous. It might feel like overkill to use Observables instead of Promises and I&#x2F;O event loops or even thread-blocking faux synchronous file system calls, but there is still an asynchronous world there where it can be nice to have the full power of Observables. To be fair, my love affair with Observables started in C# in \"backend\" applications, so that&#x27;s always been the natural fit for me and frontend and UI work has been the \"side hustle\" of taking stuff that I love in the backend side of the house and putting it to even more use.I&#x27;m calling my view engine Butterfloat, and I only just finished the first documentation pass, so be gentle, but feedback is very welcome: https:&#x2F;&#x2F;github.com&#x2F;WorldMaker&#x2F;butterfloat replyapi 12 hours agorootparentprevI used Knockout to implement the very very very first ZeroTier network control dialog. Nice paradigm, but I think React beat it when things got really complex. reply pests 21 minutes agorootparentprevuseEffect is overused. People use it to compute derived values when that is not its intended use.How many times have you seen the equivelent to a fullname = firstName + lastName inside a useEffect? Its outstanding. :( reply breadwinner 16 hours agorootparentprev> It is about enabling a design pattern where the user interface is a pure functional transformation of the application state.This is only true for read-only components.There is no \"pure functional\" when you introduce state and interactivity. See here: https:&#x2F;&#x2F;mckoder.medium.com&#x2F;why-react-is-not-functional-b1ed1... reply menssen 16 hours agorootparentIt is possible with React to write an application where components have no internal state, every component is \"read only,\" and all UI changes are state transitions in (something like) a redux store.This is rarely done, because there are pragmatic reasons (e.g., animations) not to, but it is possible.The other mistake alternatives make is to try to make components having internal state \"easier.\" It should not be easier! Every single useX() is a statement that \"I am violating the proper design pattern of this application,\" and it&#x27;s a feature not a bug of React that you have to be obvious and intentional about it. reply dexwiz 15 hours agorootparentIf one of the most popular APIs is a violation, then maybe the pattern is broken. reply gherkinnn 14 hours agorootparentI agree.useEffect should have been named useDangerousSideEffectAndThisIsNotALifecycleHook and the js influencers should have never compared useEffect to component lifecycles and the React team should have updated their docs and not wait 4.5 years to do so and the dependency array should absolutely not be optional. reply DougBTX 15 hours agorootparentprevIt’s a misremembering of history. The point of all the “pure functional” discussion was that rendering the UI now shouldn’t depend on how the UI was rendered earlier. The idea was to move away from the “create then update” paradigm to just “render”. React would be responsible for which elements need to be created and which can be updated in place.To achieve that, it doesn’t matter whether the state is local to a component or global across the application. reply breadwinner 14 hours agorootparent> React would be responsible for which elements need to be created and which can be updated in place.That works for simple, read-only components. The moment the component starts managing its own interactivity you end up needing state, and at that point things break down. If you need to change props, you have to essentially recreate the component by changing key [1], and at that point, what is the benefit of React?[1] https:&#x2F;&#x2F;legacy.reactjs.org&#x2F;blog&#x2F;2018&#x2F;06&#x2F;07&#x2F;you-probably-dont... reply marcosdumay 15 hours agorootparentprevThe idea of a reactive framework is that every interface component is either an event creator with no representation and that can update your application state, or an state viewer that has no inputs but can represent your state.It&#x27;s decoupling those two that brings all the power.But the idea doesn&#x27;t represent at all the web. So react is a mess of complex code trying to make the things you can create on the web behave like stateless pure components. Yet, it mostly works, and the react does indeed implement the idea that the interface is a pure functional transformation of the application state. (But I do disagree on claiming this the \"entire idea\", it&#x27;s half of it at most.) reply tshaddox 15 hours agorootparentprevI too remember building interactive web sites with jQuery. It was never fun creating, for instance, an interactive list view where each item can be modified in client state and new items can be added. This was the problem for which React provided a major \"Aha!\" moment. reply TheHiddenSun 1 hour agorootparentprevSolid.js comes to mind- not a framework, but a library- can do fine gained state updates without unnecessary re-renderings (reactivity)- uses dom instead of vdom to produce best in class SPA performance- uses jsx like reactPerformance claim &#x2F; proof: https:&#x2F;&#x2F;krausest.github.io&#x2F;js-framework-benchmark&#x2F; reply techpression 11 hours agorootparentprevI would say SolidJS does everything better than React while keeping with the same general mindset. React spent way too much time worrying about DX to the point that now DX is really bad. Fine grained granularity is such a breath of fresh air to work with after the very large and very heavy brush that is the React “render the world every time anything changes” method (yes they try to be smart about it, but so far haven’t succeeded, the next compiler might solve it, but then we’re back to a black box of incomprehensible code soup to debug). reply vmfunction 11 hours agorootparentThank you. Finally someone mentions Solid, it got all the good part of React, and none of the bloated parts.Personally, JSX is the mainly thing that makes React attractive. As JSX is just modern E4X. It really ought to be put into the ES standard again instead of this Web Component stuff.Another good JSX implementation (SSR) is: https:&#x2F;&#x2F;nanojsx.ioIf you just want something light JSX, then this seems the way to go for now. reply wayfinder 9 hours agorootparentprevI have to disagree.It can&#x27;t be that that design pattern because React did not invent it. Every server-side templating library is a pure functional transformation of application state. I can tell you that many people also ported that design pattern to JS.What tripped library writers up is once they applied that pattern to JS, there is no way to actually define the transformation because to do that, you either write a template or the code-equivalent (i.e. using DOM as your data model). And that&#x27;s where JSX came in.Also, I think you have patterns completely confused.Backbone is NOT in the same class as jQuery or React. Backbone is more like Redux, mobx, or React hooks because it&#x27;s a data model library. In Backbone, you define \"models\" and \"collections\" -- basically, you use Backbone to store your application data in memory.Backbone did have a views component to it but it actually could not generate any HTML. Look at this example from their docs: var Bookmark = Backbone.View.extend({ template: _.template(...), render: function() { this.$el.html(this.template(this.model.attributes)); return this; } });It literally has jQuery and underscore.js! Backbone had no templating&#x2F;HTML generation ability so when you used Backbone, you combined the fat Backbone classes with whatever horrific HTML-generation method you used to create a huge monster.You can actually use Backbone.js with React since they are not in the same class. You would not, of course, because mobx is basically the same style of data modeling as Backbone.js but with almost no boilerplate. reply pcthrowaway 13 hours agorootparentprev> (B) Advocate a framework other than React that uses the same pattern as React, but improves the usability. I think the two places there is the most room for improvements are:> 1. Animations> 2. useEffect() in generalI&#x27;m not normally one to \"shill\" a web framework, and I still mainly use (and love) React, but I&#x27;m curious if you&#x27;ve tried Svelte, because it definitely manages animations better than React, and has a different idea of state that&#x27;s more ergonomic for many use cases (though I won&#x27;t go so far as to say it&#x27;s definitively \"better\" than useEffect) reply QuadmasterXLII 15 hours agorootparentprevWith modern browsers, \"completely re-output the entire user interface on every state change\" is kinda viable. I recently wrote a trebuchet simulator app (hastingsgreer.github.io&#x2F;jstreb) without a framework. instead I wrote a \"rebuild UI\" function that I call on every new state, and the user experience is super snappy reply nightpool 13 hours agorootparentWeirdly I see that changing the \"Projectile\" selection to \"P3\" and then back to the original \"P4\" made the range drop way down, even though the new value was identical to the old value. But changing the \"Main Axel\" value made it jump way back up: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;boo5Xw3So maybe some kind of \"non-functional\"&#x2F;reused state issue exists regardless? reply QuadmasterXLII 12 hours agorootparentOh, I don’t like the look of that. This may be a valuable lesson in “I should have just used a framework” reply stickfigure 15 hours agorootparentprevIt doesn&#x27;t seem to work at all in Chrome, so maybe not the best example. reply QuadmasterXLII 14 hours agorootparentAh, global variables in modules have to be declared with var etc in chrome, but firefox and safari let it slide if you just assign. Fixed, but I guess I’m gonna have to set up a test suite reply Vinnl 12 hours agorootparentprevTo add to that, one of the (largely justified) criticisms of React is that it, or at least the way it is commonly used, can have negative performance impact, and thus negatively impacts user experience. However, user experience is broader than just performance, and critically also includes that the application has as few bugs as possible and works well in the first place, and I feel that the model you describe greatly helps there, and it makes unit testing and strict static typing feasible to boot.(Admittedly, these benefits haven&#x27;t been thoroughly researched, as far as I&#x27;m aware, so it still mostly relies on gut feeling and experience.) reply breadwinner 16 hours agorootparentprev> Make a convincing argument that a different design pattern is better.Simple Model-View-Controller pattern works well for JavaScript, just as it does for ASP.NET Core, JSP and JSF, Ruby on Rails, Django and so on.Want proof? Browse this code:https:&#x2F;&#x2F;github.com&#x2F;wisercoder&#x2F;eureka&#x2F;tree&#x2F;master&#x2F;webapp&#x2F;Clie...This is the application: https:&#x2F;&#x2F;github.com&#x2F;wisercoder&#x2F;eureka reply jitl 15 hours agorootparentThe server side style of MVC you describe is a far stretch from how MVC is practiced in retained-mode UI frameworks like UIKit, Cocoa, or Backbone. It’s possible to make this style work fine with careful design and planning; Apple built web versions of Pages and Keynote using SproutCore (which evolved into Ember?) in 2013-era.In fact back then, everyone’s big app was MVC - usually Backbone. I worked on Airbnb’s host-side web app called “Manage Listing”, it had probably 30+ models, 150+ views, 100+ controllers in Backbone. There were many bugs in this scale of app around making sure the DOM reflected the latest change to some model. the engineering team regarded the more complicated screens as a nightmare to maintain in our fast paced environment with many teams touching the code.Engineers at Airbnb started to adopt React as a solution to the problems we all experienced with MVC - not because it was a “hot new thing”. When React came out, it was widely regarded as weird - it was more like “eww, this smells of PHP and needs a weird compiler, but pure function of state is a lot better than fiddling DOM manually…”. Eventually we replaced Manage Listing backbone views with React components one by one until there was no backbone left.React is still kinda weird but it does solve this problem the best out of the modern frameworks. It’s happy to over-render by default and prefers a correct DOM-for-state over everything else. It’s clear thought that the mental models popularized by React are worth it - now Apple and Google are switching to the React model in their own frameworks. reply breadwinner 12 hours agorootparentMVC is a proven and popular technology. It works very well, otherwise it wouldn&#x27;t be so popular. I notice you allude to \"problems we all experienced with MVC\" without mentioning any. Surely if there are so many problems, you would be able to mention one? reply jitl 11 hours agorootparentI mentioned the main problem in the second paragraph:> There were many bugs in this scale of app around making sure the DOM reflected the latest change to some model. The engineering team regarded the more complicated screens as a nightmare to maintain in our fast paced environment with many teams touching the code.I&#x27;ve seen this same issue in Cocoa&#x2F;UIKit&#x2F;Android views. Older Cocoa in particular has a lot of hairy wiring up of signals and first responders and delegates. I think it&#x27;s less of an issue there because the rate of change is typically lower; in web land we expect to deploy continuously and with a very high number of engineers, anything targetting the app store will usually deploy at most weekly (2 orders of magnitude fewer deploys) and with many less engineers (usually an order of magnitude at least. My hypothesis is the difference in change rate is why this kind of bug was more of an issue for web developers. reply stickfigure 12 hours agorootparentprevAs parent pointed out, the MVC in server-side development only shares a name with the MVC in GUI development. Yes, MVC is a proven and popular name. But it means different things to different people. reply breadwinner 12 hours agorootparentWhy would it be different? The concept of Models is the same whether it is client-side or server-side. These are objects that encapsulate business logic. The concept of Views is also the same. These are responsible for rendering the screen. The concept of controllers? That too is the same. Controllers determine application flow from one screen to the next. reply jitl 11 hours agorootparentThe server is typically not a retained-mode kind of abstraction. Incremental view maintenance in retained-mode MVC systems is the main source of bugs. If you render the view from scratch on every request, it&#x27;s more similar to React style immediate-mode UI than something like UIKit where you end up handling many UI events and keeping little bits of the UI up-to-date with model changes incrementally. reply breadwinner 11 hours agorootparentYou can render the view from scratch on every user interaction, even in MVC. In fact, you can use React for that purpose. \"Lots of people use React as the V in MVC.\" See that quote on this page: https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;react&#x2F;tree&#x2F;015833e5942ce55cf31ae...Personally, I have not run into large amounts of bugs of this type, and I have written plenty of MVC code using VanillaJS. reply mardifoufs 9 hours agorootparentprevWell React is also a proven and popular technology, so there must be a reason why those using it chose it instead of using MVC. reply tyingq 16 hours agorootparentprevFair, though you can find plenty of stuff on the internet using React, but where the app&#x2F;page itself performs abysmally. Meaning maybe there is some merit in approaches that are easier for lesser skilled teams to deliver in. reply menssen 16 hours agorootparentThere is always space for options for \"fast and easy\" vs \"robust and maintainable.\"Nothing will ever beat the following as tech demo for how easy it is to make interactive UIs, but anybody who ever tried to reason through a large Angular 1 application would seriously hesitate to want to use it for something much more complicated.{{ inputValue }} reply MrPatan 16 hours agorootparentprevPreach. Angular 2+, btw, can (and should) be used the same way. V=f(S) is the only sane way to live.Just don&#x27;t change application state from a component lifecycle function and you&#x27;re golden. reply wayfinder 16 hours agoparentprevAs someone who does remember why because they&#x27;ve been building sites since the 90s, it&#x27;s because the React team created a preprocessor (JSX) to let you embed HTML tags in JavaScript. This fixed the problem that other languages like Java, Python, etc. don&#x27;t have because those languages has assemblies&#x2F;packages so they can put HTML templates in separate files and load them in your app. There is zero standard way to do that in JS and every custom way you invent looks like trash.Doing something like this.shadow.innerHTML = `your HTML` with web components is terrible. document.createElement() is terrible. $(&#x27;div&#x27;).appendChild() is terrible.is terrible. HTML(Div(Strong(text))) is terrible*. Storing templates as JSON and writing a one-off loader is terrible. template is terrible. I&#x27;ve done it every possible way and they are all terrible.JSX fixed all that.The biggest previous attempt at something like JSX was E4X where you could embed XML straight into JS, which was kinda nice except it added the entirety of XML and its complexity. I creamed when it came out but then I tried it and it was not it. (E4X ended up surviving a little longer in Adobe Flash though.)E4X: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ECMAScript_for_XML*This is what JSX compiles to behind the scenes. reply zelphirkalt 16 hours agorootparentInteresting. Yet I find that React HTML—that is not actually HTML, but a look-alike, a dialect if you want—to be exactly what I dislike about React. That is because it still encourages people to put JS in their HTML in their JS in their ... And we are almost back to crazy PHP land of \"I treat HTML as a string and blend everything together into one big lump.\", instead of using a templating engine, that treats HTML in separate files, or making use of a DSL, that treats HTML as structured data, say for example SXML. Also there are issues with that custom HTML-look-alike preprocessor, because you cannot write class=\"...\", because then somehow it gets syntactically confused, because \"class\" is now a keyword in JS.This all feels rather half-baked. Why can&#x27;t the parser&#x2F;prprocessor distinguish between that \"class\" and \"class\" in JS source code? We can have reusable HTML snippets (some may call components) with normal templating engines easily. Look at something like Jinja2 and how to reuse blocks and macros. Yes, some React component can encapsulate its own interactive behavior. That is only because we are already writing JS though, so we can already write frontend logic. But do we actually want that? Coupling state and behavior? I think we might not. Writing a script that gets served only on those rendered templates, where it is needed is not so hard either, when using a normal templating engine. reply nightski 15 hours agorootparentExcept it&#x27;s not half baked. Because guess what - now you need a new template language that supports constructs like conditionals, loops, etc... You basically end up implementing a half-baked version of javascript with an unfamiliar syntax. Even worse they need to interact with state. Many template engines introduce some sort of \"data binding\" abomination which is a complex way of saying how you want the state to interact with the view.React is simple. Here is your data, and here is a function that transforms that data into a view.Templates are terrible, and that is a hill I am willing to die on. reply wayfinder 16 hours agorootparentprevYour criticism was the biggest one of React when it came out -- mixing view code with your controller code. Big no no. Everyone loved the MVC pattern (model-viewer-controller where you separate these things in different places) and what React did was a big no no.But you know, if you are trying to put HTML templates in separate files, you have to now send extra HTTP requests. That&#x27;s an even bigger no no.In other languages, you can just put your template in a separate file and load it from your package&#x2F;.jar&#x2F;assembly. Super easy and most importantly, 100% standard and provided by the environment.So people just settled with React because it worked and no successfully invented a standard bundle format for the web.F&#x27; it we said. reply nostrademons 15 hours agorootparent> But you know, if you are trying to put HTML templates in separate files, you have to now send extra HTTP requests. That&#x27;s an even bigger no no.It&#x27;s not actually, with HTTP&#x2F;2. The separate requests are all multiplexed over the same connection, often with prefetching, and compressed together so that there&#x27;s little to no overhead vs. putting them in the same file.This illustrates a common failure point for web frameworks though. They encode a lot of folk knowledge about how the web works, but that folk knowledge becomes obsolete as browsers change.Another prominent React example fits into this category: the virtual DOM. React assumed that DOM manipulations were slow and Javascript manipulations were fast, and so they built up an internal representation of the DOM in Javascript so they could manipulate that one and diff the change to apply it to the real DOM. This was true from approximately 2008 (when V8 introduced JS JITting) to 2013 (when Blink&#x2F;Webkit&#x2F;Firefox all moved to a dirty-bit system for DOM manipulations), which not coincidentally is when React was designed. But since then, DOM manipulation has been just pointer swaps and a dirty bit flip, and if you&#x27;re careful not to invoke any operation that triggers a reflow and repaint you can make your modifications directly in the DOM for roughly the same cost as making them in JS. Since React is declarative it could easily have enforced the no-reflow rules without the virtual DOM, but because DOM manipulations were expensive when its designers learned web programming, it introduces an unnecessary concept. reply wayfinder 12 hours agorootparentOh I&#x27;m aware, but I&#x27;m talking historically.But HTTP&#x2F;2 is not the right example. You&#x27;re missing some things on the web timeline.Long before HTTP&#x2F;2 (time in web terms), and after React (and Angular, etc.) came about, people created actually-popular bundler toolchains that can bundle arbitrary files together and then incrementally load them as needed. This effectively fixed the multiple HTTP request problem long before HTTP&#x2F;2 existed. Efficient template files have been possible for years. (Granted, not anything standard though.)The issue right now is that someone has to make a full toolchain with strong developer support to make this a de-facto and popular way to distribute apps. Just because you have a bundler or HTTP&#x2F;2 doesn&#x27;t mean you have a nice way to make use of it. You need a nice library. Your usage examples need to look pretty. It needs to work with other toolchains. It needs to look like your library will be supported for a long time, just like how React was supported by Facebook.You can&#x27;t just write a blog post about how it&#x27;s possible because you&#x27;re basically asking your readers to invent and maintain a library or worse, roll their own internal library that will confound people at their company. reply chatmasta 15 hours agorootparentprevMultiplexing requests doesn&#x27;t help when there is a logical dependency between them, e.g. some javascript code needs to download and then execute just to determine what template files it needs to subsequently download. That can&#x27;t be parallelized. reply nostrademons 13 hours agorootparentIf you&#x27;re in a position where you could&#x27;ve written the template inline in your source file, this is a static dependency, independent of any page content, and can be handled through build systems and prefetching. reply QuadmasterXLII 15 hours agorootparentprevCan I borrow some knowledge? What sort of operations trigger a reflow and repaint? reply nostrademons 13 hours agorootparentShort answer: anything that modifies the DOM, followed by a call which measures geometric properties of the DOM. Also you get an automatic reflow&#x2F;repaint when control leaves the Javascript event handler that triggers a modification (this is how the browser&#x27;s UI is updated). You can batch up a bunch of modifications without triggering a reflow and then have it account for all of them on next reflow, though, which is what React tries to achieve with the Virtual DOM and what you get for free after ~2013.Slightly longer answers:[modifies] https:&#x2F;&#x2F;developers.google.com&#x2F;speed&#x2F;docs&#x2F;insights&#x2F;browser-re...[measures] https:&#x2F;&#x2F;gist.github.com&#x2F;paulirish&#x2F;5d52fb081b3570c81e3aAlso there&#x27;s a bunch of exceptions where you can modify things without triggering a full reflow. Anything that takes elements out of normal flow (position: fixed, position: absolute, float:, overflow: hidden) creates a layout boundary where changes inside only reflow the containing element. Any element with a transform: or opacity: property gets rendered as a 2D texture on the GPU, and then you can do subsequent transform&#x2F;opacity animations purely on the GPU without touching the CPU. Used to do this to make performant animations on mobile devices. reply lioeters 12 hours agorootparentprev\"Generally, all APIs that synchronously provide layout metrics will trigger forced reflow &#x2F; layout.\"> What forces layout&#x2F;reflow. The comprehensive list.> All of the below properties or methods, when requested&#x2F;called in JavaScript, will trigger the browser to synchronously calculate the style and layout*. This is also called reflow or layout thrashing, and is common performance bottleneck.https:&#x2F;&#x2F;gist.github.com&#x2F;paulirish&#x2F;5d52fb081b3570c81e3a reply throwaway_1987 14 hours agorootparentprev>But you know, if you are trying to put HTML templates in separate files, you have to now send extra HTTP requests.Most projects that I have been involved with have some kind of build system that does all sort of magic. It should be possible to include the template in the JS file during the build process reply nicoburns 12 hours agorootparentYep, and that&#x27;s what was being done before React existed. JSX took over because people (including me) prefer to have the full power of JavaScript for their templates rather than a half-based templating language. reply tubthumper8 13 hours agorootparentprev> Also there are issues with that custom HTML-look-alike preprocessor, because you cannot write class=\"...\", because then somehow it gets syntactically confused, because \"class\" is now a keyword in JS.> This all feels rather half-baked. Why can&#x27;t the parser&#x2F;prprocessor distinguish between that \"class\" and \"class\" in JS source code?Remember that React is \"just JavaScript\", JSX is a syntax transformation of JavaScript. When you use `className` in your JSX, you&#x27;re using the standard DOM Attribute [1]. You wouldn&#x27;t use `class` either in vanilla JS to set the CSS class, you&#x27;d use `className`.https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Element&#x2F;cla... reply vitaflo 15 hours agorootparentprev100% agree. Like you I’ve been building sites since the mid 90s and always was pulling my hair out at all the convoluted ways we would attempt to handle HTML in these JS frameworks. As someone who knows HTML and CSS and vanilla JS, as soon as I saw React all I could think of when I saw JSX was “finally!”.Yeah it’s another framework with its own quirks but it lets me think about the UI in the same way I’ve thought about it for 25 years. For someone like me that’s a godsend. reply gitaarik 22 minutes agorootparentThen you would love Lit [0]. It is like JSX, but then using more native html template technologies.[0]: https:&#x2F;&#x2F;lit.dev&#x2F; reply xtracto 12 hours agorootparentprevI remember using PHP and CodeIgniter in the mid-2000s. IIRC PHP had that template-interpolation thing solved since the first day: You populated your Views with PHP code. reply breadwinner 16 hours agorootparentprevWhat if you could have the best of both worlds? What if you could use JSX templates and use standards-based web components? You can.Here&#x27;s an example of using a web component in a JSX template (look for zx-listeditor): https:&#x2F;&#x2F;github.com&#x2F;wisercoder&#x2F;uibuilder&#x2F;blob&#x2F;master&#x2F;WebCompo...Here&#x27;s how the web component is implemented, also using JSX: https:&#x2F;&#x2F;github.com&#x2F;wisercoder&#x2F;uibuilder&#x2F;blob&#x2F;master&#x2F;WebCompo... reply wayfinder 16 hours agorootparentNot saying that you can&#x27;t. I&#x27;m just explaining why React won and how it was blatantly obvious that it was going to win when it came out (that is, if you were there in the before-times). reply breadwinner 16 hours agorootparentAgree with you on that. React was a breath of fresh air compared to what we had in the before-times, such as Angular, Ember and so on. The canonical demo of Ember.js was two-way data binding. React did away with two-way data binding and introduced JSX syntax. This was a major step forward. Code suddenly looked sane and readable. Then they took two steps backward when they introduced hooks. Code went back to being unreadable. reply chatmasta 15 hours agorootparentprevAlso, the entire OP article is demonstrating this... I&#x27;m not sure many of the comments here actually read it... reply marcosdumay 15 hours agorootparentprev> templateIt&#x27;s new and has no significance for the history. But I think it&#x27;s relevant to point out thatisn&#x27;t horrible.And that those articles about web components are doing a really bad job of using constant strings instead of this.shaddow.innerHTML = getElementById(\"myhtml\").innerHTML. reply wayfinder 10 hours agorootparentThe implementation is new but as far as the UX goes, it&#x27;s very similar to . That said, the UX is going to be slightly better forsimply because it&#x27;s is an official standard and so IDEs and tooling will be more likely to support it without additional extensions or plugins. reply marcosdumay 5 hours agorootparentIt&#x27;s much more descriptive and obvious, and it&#x27;s just plain HTML, so anything that assists you on writing HTML will assist you inside the templates.It&#x27;s bad because it&#x27;s inline and your templates have no obvious place to go. You also can&#x27;t import them on the fly (or rather, it&#x27;s as idiomatic to import as any HTML). And obviously, because they have no effect on HTML and can only be used in javascript reply WorldMaker 14 hours agorootparentprev> HTML(Div(Strong(text))) is terrible*> This is what JSX compiles to behind the scenes.Oh, it is much worse than that. By classic default it is: React.createElement(&#x27;div&#x27;, { &#x2F;* props and attributes ... *&#x2F; }, React.createElement(&#x27;strong&#x27;, { &#x2F;* ... *&#x2F; }, text))Recent updates and other JSX frameworks simplified the name of `React.createElement` to just `jsx` or `h`. I&#x27;ve seen projects that manually write `h` calls like that everywhere, and it is indeed terrible.(Source: I&#x27;ve just completed a bunch of crazy TSX work and got pretty familiar with the compiled forms.) reply wayfinder 10 hours agorootparentYeah, you&#x27;re right. I was kind of simplifying it. API-wise, it&#x27;s more like document.createElement() but I didn&#x27;t want to asterisk that because document.createElement() is actually a thing and I didn&#x27;t want people to think it was a simple layer on top of document.createElement. reply nostrademons 15 hours agorootparentprevI liked the templates-as-JSON approach much more than JSX. HTML is terribly verbose, doesn&#x27;t fit well with the rest of Javascript, doesn&#x27;t let you use JS language features like destructuring or iteration on the component tree, and doesn&#x27;t let you factor out common properties into their own data structure. Would much rather see: { tag: &#x27;div&#x27;, padding: &#x27;2px&#x27;, position: &#x27;absolute&#x27;, content: [myText] }than ${myText}The main reason I ended up settling for JSX was just convention: I don&#x27;t want to be the one with my own unique format for UI trees unless I can convince everybody else to use my format.Note that Jetpack Compose has a very similar declarative reactive paradigm as React, but represents components with Kotlin language mechanisms. IMHO I like it much better, because it&#x27;s much more composeable. reply tshaddox 16 hours agoparentprevFascinating that React simultaneously receives criticism as being1) boring, old technology that has been around for so long that no one even remember why it was chosen in the first place, and2) extremely fast-churning, bleeding edge software that is constantly changing and breaking because the JS community is more interested in chasing trends than building robust stuff. reply pwdisswordfishc 16 hours agorootparentReact has merely aged, not matured. reply paulddraper 14 hours agorootparentprevSame people who say:1) the ecosystem keeps chasing shiny tech that is constantly changing, not boring solutions2) everything should be written in Rust reply mattlondon 15 hours agoparentprevWould anyone pick react these days, if they did not already use it&#x2F;know it&#x2F;think-it-is-what-they-should-use-because-everyone-else-does?I think that is the point. The original \"pitch\" for react in my mind was that it was lightweight and simple. I don&#x27;t think you can say that any more, and I think a lot of rough edges have been identified (e.g. hooks fiasco, app state management, dependency-hell etc to name just 3) after years of use.It&#x27;s the same cycle that always happens. Old thing ossifies and grows fat overtime trying to be all things to all people. Then some new thing appears that starts with a clean slate and no prior expectations that proves popular as a result reply JacobThreeThree 12 hours agorootparentIt&#x27;s as you describe, but also the spec and browser support landscape has changed. reply robin_reala 16 hours agoparentprevReact “won” because Facebook spent three years flying devrel to every web conference on earth to persuade other people that Facebook’s problem space was applicable to small agencies.Back in reality, React is slow and massively overbuilt for the majority of stuff that most people are building.If you played around with Web Components a long time ago then you probably played with the v0 spec, which was somewhat different to the current version. reply JamesSwift 16 hours agorootparentReact \"won\" I think because it trojan horsed itself as being \"just the view\" but then morphed into a hand-rolled kitchen-sink solution, and inertia kept people around. reply jeremycarter 12 hours agorootparentAccurate quote. Source: I was there. reply paulddraper 15 hours agorootparentprev!!!If React won, it was precisely because it was not \"overbuilt.\"It was simpler (deliberately) than Backbone, Angular, Vue, Svelte, etc.You can implement a React clone in a few hundred lines, a la Preact. reply levitate 14 hours agorootparentI would not say that React won over svelte because of simplicity. Hate what you will about implicit behaviors in svelte, but the syntax is simple and intuitive.React won by being early and different. Svelte came out three years after React and got (unfortunately) steamrolled. reply paulddraper 14 hours agorootparentThere two notions of \"simplicity.\"One is that the implementation is very simple&#x2F;transparent, e.g. C.The second is that it allows users to have very simple code, e.g. Python.React is mostly the first with a bit of the second. Svelte is the reverse. reply troupo 16 hours agorootparentprev> React “won” because Facebook spent three years flying devrel to every web conferenceYou do know that Google sponsored and ran things like Polymer conf? That it builds lit? That Google&#x27;s devs have literally overrun and overruled most web specs committees? That Google has spend hundreds of millions of dollars promoting Web Components?And yet here we are. reply robin_reala 16 hours agorootparentPolymer was incredibly even slower that React and even more overbuilt, and it had the added negative that it tried to enforce a visual style as well, which was too much for most people. Google couldn’t spend its way out of that hole.As for Web Component specs, we got v1 because of Apple and Mozilla: Google would happily have stayed with v0. reply pseudosavant 15 hours agorootparentprevI&#x27;m glad to have read this today. I&#x27;m going to look into how I could use web components in my next side project. I&#x27;d really like something that worked easily with SSR and client-side, maybe using htmx.In retrospect, I think I avoided web components because of Google and their aggressive dev relations pushing Polymer. It felt so one-sided that it didn&#x27;t seem like a web standard. It felt more like a Google \"standard\" like NaCL. Having to ship Polymer to support non-Google browsers felt even more heavy weight than React. reply troupo 13 hours agorootparent> I&#x27;d really like something that worked easily with SSR and client-sideThen you should skip Web Components :) reply robin_reala 13 hours agorootparentFor sure. SSR is one thing that React has done well, and that Web Components don’t have a great story for. reply pseudosavant 13 hours agorootparentI&#x27;m pretty sure that just shipping the HTML works. The nice thing if I use web components is that I can just have my backend create HTML that uses those components. I can create elements whatever way I want on the backend or the front-end so long as it is HTML. reply robin_reala 12 hours agorootparentYes, if you stick to custom elements. If you want shadow dom encapsulation then you’ll need to wait for declarative shadow dom to turn up in all browsers. replydjrenren 15 hours agoparentprevAs someone who just had to answer this for my startup, the value of React is that it&#x27;s robust and immensely hire-able. If you&#x27;re looking for frontend developers, the one thing you can always expect is at least passable React knowledge.Everything else, even if it has a better technical fit for your project, is likely riskier for your organization. reply pseudosavant 15 hours agorootparentSome of the most productive teams I&#x27;ve worked with (not as a dev) have been for tech stacks I personally don&#x27;t enjoy using. Java, .NET&#x2F;C#, React, etc. In practice, I&#x27;d rather quickly hire a local senior Java&#x2F;C# dev for 1X than scour the interwebs to hire a Golang dev that requires relocation for 1.5-2X.If I was going to learn a language though, I&#x27;d probably learn Golang. Their rate is so much higher. reply Draiken 14 hours agorootparentprevI find this argument so absurd. If you know JavaScript and basic programming, you can use any front-end framework.We&#x27;re hyper-specializing people in a specific framework that then don&#x27;t even understand the basics of JavaScript itself.I already find the divide between frontend and backend developers unnecessary the majority of the time. This takes it a step further and in turn creates monstrosities like the leftpad debacle.Can we stop with this nonsense? Hire good developers and let them spend 5 minutes to learn the god damn framework of the week. If your hire can&#x27;t learn React in a week, I have some bad news for you. reply davedx 15 hours agoparentprevReact isn&#x27;t legacy technology. What a perfidious statement.I can easily justify the reasons I still use React, but I can&#x27;t be bothered writing it out every time some gimmicky front-end tech hits HN. reply sesm 14 hours agorootparentEvery alternative to React roughly fall into one of 3 categories:1. Go back to reactive templates.2. Sprinkle directives over plain HTML a-la early Angular 1.3. HTML over the wire.All those ideas pre-date React and React won against them back in 2014. reply nicoburns 12 hours agorootparentThere is also4. The React model but with compiler support to improve the ergonomics (and with better static checks). reply mustaflex 16 hours agoparentprevWeb components are a bit verbose by themselves but used with a library like lit.js you can build a framework agnostic component library. Angular and Vue support WC out of the box and they have a wrapper for React. I think the only thing missing(or not mature enough right) now is SSR. There are probably libraries other than lit.js that take the pain out of writing WC.I was skeptical at first but I&#x27;ve seen in production a lit.js&#x2F;WC component library used with React and Angular successfully in the banking sector and it works surprisingly well. reply austin-cheney 16 hours agoparentprevAs a full time JavaScript developer of 15 years it boils down to 2 reasons.1. Eases candidate selection. There is no uniform baseline of competence in software generally and certainly nothing exists for web development. Its often the blind leading the blind, so outsource everything to a tool. At the very least, people that cannot use that tool are then not qualified to be there if you discount absolutely everything else. After all, it isn&#x27;t as though employers are willing to train new developers to perform as required, so they need to turn this into a commodity as much as possible.2. Composition. Most of the people who do web development cannot write original software, plan, or organize at the level required by modern applications. Employers still need people to do trivial things to get text to appear on screen. As the demand for these trivial tasks still exists employers still need to hire people to do this work, and so they attempt to outsource the parts that require higher intelligence.These beg the obvious question: What will happen when employers realize they don&#x27;t need to overpay developers to do this work when half of it can be pushed into content management systems and the other half can be pushed into AI? reply gustavus 16 hours agoparentprevIt sounds like the common thing that someone in the Frontend space would say, that a technology less than 5 years old says the technology is old.I prefer the terms stable, mature, or hardened.People use React&#x2F;Angular because large products are forced to have a way to organize things across multiple developers and time. Now don&#x27;t get me wrong I think these JS bloated websites are an abomination that have largely made the web worse and not better, especially since UX \"engineers\" feel a compulsive need to change the layout every 6 months and now even simply scrolling through a page involves 50 web requests, and takes a full 3 minutes to load the next page... but I digress. reply meiraleal 13 hours agoparentprevYes, I agree with him. What&#x27;s the reason to use React? It is over-engineered and too complicated for no clear benefit. It takes longer to build an app using react just because you have to deal with things that are exclusive to react reply troupo 16 hours agoparentprev> They were pretty messy and felt like they did a poor job of encapsulation, ultimately. (At least at the time.)They are still messy. https:&#x2F;&#x2F;w3c.github.io&#x2F;webcomponents-cg&#x2F;2022.html reply earthboundkid 16 hours agoprevNo, they just lock you into one framework per component. I don&#x27;t know why people say obviously false stuff like this about web components besides they just really, really want it to be true. Yeah, if you&#x27;re willing to have the same framework on the page ten times, you can just chuck them all in separate script tags using the custom element API. But if you care about making a performant page with progressive enhancement, etc. this is a bad strategy.Even the name \"web components\" is basically just marketing fluff. When people say \"web component\", they mean using two particular DOM APIs: customElement and shadow DOM. Those are both pretty niche APIs. There are some cases where they are helpful, but 99% of the time, they don&#x27;t add much to a project versus good old querySelectorAll and normal CSS. reply jakelazaroff 16 hours agoparentAuthor here — I&#x27;m not sure what you mean by \"lock you into one framework per component\". The point is that you can encapsulate framework code within web components, not that every component you write should be a web component.Let&#x27;s say you&#x27;re writing a Vue app and you really want to use a library that&#x27;s only available as a React component. You can wrap that library in a web component and use it in your Vue app just like you would any other HTML element. The rest of your app can continue being Vue, using normal Vue components. reply mmis1000 15 hours agorootparentThe biggest problem of web component is that it don&#x27;t support most state injection(for things that rely on contexts, for example: tabs) and template instantiation(render function of react or scoped slot in vue for example), which literally every current popular web framework does. You may pass string or number as string to it. But anything more complicate that can&#x27;t be easily serialized? Good luck.You may add another layer of abstraction on the top of it. But that only make you another framework (like polymer) instead of actually using web components.In other word. Web component is useful to encapsulate an &#x27;app&#x27; (for example: a interactive map viewer that accept an address) inside another app. But is mostly useless to encapsulate anything other than a simple ui&#x2F;input component. There is basically 0 functionality to inter-blend between parent and child components of different frameworks. reply Devasta 15 hours agorootparentprevThat sounds awful to be honest. A recipe for total loss of standardization within an org. reply recursive 12 hours agorootparentAny sufficiently large org already has all the technologies. reply jdmg94 15 hours agorootparentprevexcept you can&#x27;t use it as a normal component, you can&#x27;t adjust its CSS normally, you have to `&::part()` your way around and hope for the best. Accessing through refs is a complete blackbox. I work with web components daily and it is a hindrance to my daily work, it is very common for people at my org to just re-write a component instead of using its web component version. reply mmis1000 14 hours agorootparentI also wonder about this. The webcomponent make the style completely immutable unless you add part selector specifically… how on the earth this is even useful? Imagine using an ui library that you can&#x27;t change the style at all. That sounds like a total joke to me. That isn&#x27;t even a sane default that is useful to most web folks that make page base on layout that designers gave. reply jakelazaroff 14 hours agorootparentI find it very useful for embedding interactive demos on my blog that are mostly independent of the styles for the rest of the site (for example, the article here). But for what it’s worth, there is discussion of an “open-stylable” shadow DOM mode that addresses those concerns: https:&#x2F;&#x2F;github.com&#x2F;WICG&#x2F;webcomponents&#x2F;issues&#x2F;909 reply EnergyAmy 11 hours agorootparentShadow DOM was a mistake. It&#x27;s a 0.1 version that was unfortunately pushed out as a completed standard. Basic stuff is missing like your link. The entire thing should be deprecated and we should start from scratch.If you want to embed something that doesn&#x27;t use your page&#x27;s styles, we already have iframes. Shadow DOM is just a half-assed recreation of that. reply jakelazaroff 10 hours agorootparentHow would you build, say, a component library like Shoelace [1] using iframes?[1] https:&#x2F;&#x2F;shoelace.style&#x2F; reply earthboundkid 7 hours agorootparentYou could use srcdoc=\"...\" to simulate a component, except that iframes don&#x27;t have automatic height adjustment, which is the thing they should actually fix.In any event, shadow DOM is just a worse version of scope selectors. There&#x27;s not really a good use for it now that you can just do a style reset on a donut selector inside an import layer. It can all be done declaratively with CSS instead of using an imperative JS API that has a ton of gotchas. reply jakelazaroff 6 hours agorootparentThere are a lot of things you can do with shadow DOM that you can’t with iframes or scope&#x2F;donut selectors:- transclude other elements (technically you can with iframes, but hopefully it’s clear why making something like this breadcrumb component would be extremely awkward [1])- control and react to the display of child elements via s- call methods on custom element objectsetc etc. It’s fine if it doesn’t solve any problems you personally have, but that doesn’t mean it’s not useful.[1] https:&#x2F;&#x2F;shoelace.style&#x2F;components&#x2F;breadcrumb replyoblak 16 hours agoprevI&#x27;ve been fought by people insisting that if we&#x27;re using a framework, we should be using it for everything, if it has it, even when both agree that doing it natively is actually less cumbersome. All that in the name of consistency. I think middle ground is a good solution.On topic: I don&#x27;t think WebComponents are going to \"make it\" until someone builds a nice framework on top of them. React, Vue, Svelte, etc. solve a number of problems that are not directly solved by Web Components. State management, rendering, routing - right now, these are, imho, the high level areas that need solid solutions for an UI app to function in any sane way. How much of that is solved by going Web Components? reply spankalee 16 hours agoparentI work on Lit, which I would hesitate to call a framework, but gives a framework-like DX for building web components, while trying to keep opinions to a minimum and lock-in as low as possible.It&#x27;s got reactivity, declarative templates, great performance, SSR, TypeScript support, native CSS encapsulation, context, tasks, and more.It&#x27;s used to build Material Design, settings and devtools UIs for Chrome, some UI for Firefox, Reddit, Photoshop Web...https:&#x2F;&#x2F;lit.dev if you&#x27;re interested. reply veeti 16 hours agorootparentHow do we know this won&#x27;t be killed off by Google in six months? reply spankalee 16 hours agorootparentHonestly, you don&#x27;t. It&#x27;s used for many Google projects like Chrome, parts of YouTube, Maps APIs, Collab, the Google Store, and a ton of internal things, so I think it&#x27;s unlikely, but I understand Google&#x27;s reputation.This is one reason why we care so much about low coupling and lock-in, and a small, easy-to-understand codebase. You should be able to migrate away from Lit very easily, and fork or maintain it if necessary. We&#x27;re also trying to build up our non-Google contributors. reply matt7340 14 hours agorootparentI&#x27;ve read that Angular is also extensively used for Google internal projects. How do the teams choose between Angular and Lit? reply troupo 5 hours agorootparentprev> It&#x27;s used for many Google projects like Chrome, parts of YouTube... so I think it&#x27;s unlikely,When Custom Components v0 was barely released Google re-wrote Youtube with Polymer. Where&#x27;s Polymer now?> You should be able to migrate away from Lit very easilyAnd that should comes from which part of lit? Custom DSL? Custom decorators? Custom data bindings? Custom tasks?As I wrote elsewhere, Preact, Svelte, Vue, and Angular can all be compiled to web components. How easy do you think it would be to convert app code between each of these frameworks? Convert that code to and from lit? reply troupo 16 hours agorootparentprev> lock-in as low as possible.As in:- reactivity. Specific to lit- declarative templates. Specific to lit- SSR. Specific to lit.- context. Specific to lit.- tasks. Specific to lit\"minimal\" and \"low lock-in\".Please do not hesitate to call it a framework. If you call React a framework, then lit is definitely a framework. reply spankalee 16 hours agorootparentLit&#x27;s features are internal to each component (except context which is being developed as an open community protocol with the Web Components Community Group) and there is no coupling between components written in Lit. So you can port from Lit to something else component-by-component.Lit is also modular. The template library, lit-html, is usable independently and used by other web component and non-web component libraries. The reactive custom element base class ReactiveElement can be used to build web components with a different template system like Preact.So yes, \"lock-in as low as possible\". reply troupo 13 hours agorootparentThis is just splitting hairs in an attempt to pretend that lit isn&#x27;t a framework (or framework-like lib), or that it&#x27;s somehow unopinionated, or that it somehow prevents you from lock-in.Almost all of the things you listed are specific to lit, and lit only. So, people who will develop with lit will be locked in to lit. Because it&#x27;s not like you can just pop the code you wrote with lit into stencil or ionic, and will just work.> So you can port from Lit to something else component-by-component.I personally saw a huge project ported from Angular to React basically doing the same. It&#x27;s not a testament to lit. It&#x27;s what people have been doing since time immemorial. reply hmcdona1 11 hours agorootparentI mean they have to be for now - you&#x27;re just being semantic for the sake of it a bit. I will say as a team that utilizes Lit for our design system web components (which none of our users even need to know or care about no matter their framework btw). The Lit team are huge advocates of aligning with native standards (now or what they might be in the future) and working to establish or push them forward. The goal of the project for a lot of these issues truly seems to be to eventually not need them to be a part of Lit at all. reply troupo 5 hours agorootparent> you&#x27;re just being semantic for the sake of it a bit.No. I&#x27;m calling it as it is. I don&#x27;t pretend that something isn&#x27;t a framework when it has all the same things that the frameworks they so love to vilify do.> The Lit team are huge advocates of aligning with native standardsWhich of the things that lit provides are native standards current or future? Its template DSL? Its custom decorators? Its data binding system? Tasks? Directives?Not to mention the usual workarounds like support for SVGs https:&#x2F;&#x2F;lit.dev&#x2F;docs&#x2F;api&#x2F;templates&#x2F;#svg reply meiraleal 13 hours agorootparentprev> I personally saw a huge project ported from Angular to React basically doing the same.Do Angular and React components talk to each other? Lit and other Web component frameworks can share components. To refactor Lit components into other web component framework or raw web component is orders of magnitude easier than converting AngularReact. reply troupo 5 hours agorootparent> Do Angular and React components talk to each other?You can make them talk to each other. Depends on what exactly you need, how components and projects are structured etc.> Lit and other Web component frameworks can share components. To refactor Lit components into other web component framework or raw web component is orders of magnitude easier than converting AngularReact.You&#x27;re putting a false equivalency between \"talking to each other\" and \"refactoring one code base into another\".Preact, Svelte, Vue, and Angular can all be compiled to web components. How easy do you think it would be to convert app code between each of these frameworks? Convert that code to and from lit? replyearthboundkid 16 hours agoparentprev\"Web Components\" are the name for a dream, not an actual technology. The actual technologies involved -- customElement and shadow DOM -- are pretty crappy to use directly, and even when hidden behind a framework&#x2F;library don&#x27;t buy you that much. But people want it to be true that there is such a thing as a \"Web Component\" so the dream lives on. reply o11c 15 hours agoparentprev> I&#x27;ve been fought by people insisting that if we&#x27;re using a framework, we should be using it for everything,Remember the definition of a framework: a framework is just a library that does not play well with others. reply jdmg94 15 hours agoparentprevStencil does it, but the resulting web components are still not great to work with because of shadow dom and the missing standards you can&#x27;t expect from custom elements. You want inline styles? hope your dev remember to drill those down. reply toasted-subs 16 hours agoparentprevSvelte compiles into WebComponents. reply steve_taylor 8 hours agorootparentThat’s news to me and probably everyone who uses Svelte including its developers. reply beebeepka 16 hours agorootparentprevThat&#x27;s good to know. I really liked what I saw last time I played with it but it&#x27;s hard to find many job listings where people are using svelte. Almost everything is react and surprisingly, at from what I can tell, it&#x27;s getting even bigger share in job postings I used to see. Hooray for uniformity, I guess. reply tannhaeuser 16 hours agoprevIn case anyone was wondering, JavaScript per se isn&#x27;t replaced at all. All mentioned examples still use event listeners, querySelectorAll, ES modules, and what not. Most importantly, you need to subclass HTMLElement and register your custom element using JavaScript in the first place eg: class MyElement extends HTMLElement { } window.customElements .register(&#x27;my-element&#x27;, MyElement)(the hyphen in the name is idiosyncratically required by the custom element spec).As such, \"HTML web components\" (aka custom elements, which have be around for many years) are targeted at developers not hypothetical web users&#x2F;authors. Then what is the point? When you&#x27;re relying on JavaScript anyway, you have already much greater freedoms - syntactically and otherwise.Custom element declarations and custom vocabularies can make sense as a means to organize and isolate authoring concerns when you&#x27;re creating hypertext documents. The fact alone that you have to use JavaScript for registering custom elements make them a non-starter though (ie consider an authoring tool which surely doesn&#x27;t want to execute arbitrary JavaScript in client docs).It&#x27;s not that there&#x27;s no precedent either. SGML has a mechanism for parametric macro expansion where your element is syntactically replaced into an arbitrary markup fragment, with type-checked arguments (= attributes at the call site, just as with custom elements), and with type-checking the resultant expansion in context (ie. checking whether the expansion adheres to the content model at the expansion site), also having of course the capability to include JavaScript. Moreover, SGML actually can serialize&#x2F;re-parse such markup whereas HTML&#x27;s preliminary APIs for fragment parsing won&#x27;t deal with contextual tag inference at all. reply runarberg 16 hours agoparentFor me, being able to offer my library as a web component is kind of neat. Sure I can ask users of my library to import it then query select math inputs, then apply my library on these, and remember to apply it also on DOM change.import lib from \"&#x2F;path&#x2F;to&#x2F;my&#x2F;lib.js\"; for (const target of document.querySelectorAll(\".target\")) { lib(target); } &#x2F;&#x2F; TODO: Watch for DOM change and apply lib. Some InputOr I can simply ask them to import my web component module and use the custom element:Some inputThe latter certainly feels like a superior API for my users. reply svieira 15 hours agorootparentOr you can do a little more work in your lib for your users and offer:import lib from \"the-lib.co.uk\"; lib({ target: \".target\" }); If your users might not want you to watch the _whole_ DOM for performance reasons:import lib from \"the-lib.co.uk\"; lib({ target: \".target\", root: \".all-the-targets-show-up-here\" }); And if your users might already have a mutation observer in their apps:import lib from \"the-lib.co.uk\"; lib({ target: \".target\", observer: theAppObserver });reply runarberg 14 hours agorootparentIt may just be me, but all of these seem a lot inferior API design then a simple web component. Particularly if I’m asking my users to setup a mutation observer.Aside: Don’t you need to pass the library function into the callback while constructing the mutation observer? import lib, { mutationObserverCallback } from \"https:&#x2F;&#x2F;my-lib.example\"; lib({ target: \".target\", observer: new MutationObserver(mutationObserverCallback), });Honestly, this feels like so much magic compared to a simple: Some Inputwhere I handle update in my library by listening to the slotchange event. reply itslennysfault 15 hours agoprevI like the idea of web components, but really dislike html and js being intermingled (the reason I don&#x27;t like React&#x2F;JSX). In web components its even worse because its HTML as a string which means it has no validation and isn&#x27;t syntax aware. I much prefer the angular approach where you have separate css,html,ts files, and would love a web component framework that could work similarly.Searching for this I found some kinda hacky solutions on Stack Overflow which use fetch to load html&#x2F;css at runtime. reply djrenren 14 hours agoparentWeb Components are really just the minimal APIs required to allow the implementation of new HTML elements. This includes things like:- Responding to being attached to the DOM- Encapsulating DOM nodes (the Shadow DOM)- Scoped styling (Shadow DOM + User defined stylesheets + CSS parts)It is definitively not a templating engine. It doesn&#x27;t provide any new APIs for creating DOM nodes, or mutating them a la React or handlebars or lit-html. Templating is basically the \"next step up the stack\". Using shadowDom.innerHTML = `...`; is basically a stand-in for having an actual templating engine.There is work going on, developing a native templating system in the browser which may interest you. It&#x27;s called the DOM Parts proposal and you can find info on it here: https:&#x2F;&#x2F;github.com&#x2F;WICG&#x2F;webcomponents&#x2F;blob&#x2F;gh-pages&#x2F;proposal... reply jacobsimon 14 hours agoparentprevI like React but I actually agree with you re: JSX, it helps me to remember that JSX is &#x27;just javascript&#x27; - it&#x27;s a syntactical shorthand for creating a component tree, which then happens to be rendered into HTML. reply yohannparis 14 hours agoparentprevYou can use `` for that purpose and keep your separation of concerns okay. reply MrDresden 14 hours agorootparentAs a non web dev I had to look this up and yeah, this looks like a very sane and clean way to separate concerns and get proper handling from IDEs:https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Web_compone... reply acheong08 13 hours agoparentprevRather than putting the HTML in JS, you can fetch HTML in the JS.E.g. https:&#x2F;&#x2F;github.com&#x2F;g-utils&#x2F;WebComponentFactory reply fuzzy2 14 hours agoparentprevThen… use Angular Elements?Using innerHTML isn’t specific to Web Components in any way. It was and is a quick & dirty way to dynamically insert HTML. reply claytongulick 15 hours agoparentprevIf you use a good rendering library, like lit-html then (at least in vscode) you get very nice syntax highlighting, completion and validation: html`` reply itslennysfault 15 hours agorootparentI was actually just looking at that after posting this (lit is new to me). That&#x27;s a pretty cool solution. I&#x27;d still prefer an option to have the html in its own file, but that is just my taste. I know a lot of people feel exactly the opposite. reply claytongulick 12 hours agorootparentMaybe a bit pedantic - but I prefer using lit-html, not lit.Lit is a bit too \"frameworky\" for my taste, I generally just use vanilla classes that extend HTMLElement and have a render function that renders the lit-element template to the light dom.Something like: import {html, render} from &#x27;lit-html&#x27;; class AppComponent extends HTMLElement { connectedCallback() { this.template = () => html``; this.render(); } render() { if(this.template) render(this.template(), this) } } customElements.define(&#x27;app-component&#x27;, AppComponent); reply itslennysfault 10 hours agorootparentPerfect, and perfect timing. I was recently tasked with creating a small standalone web app and everything feels like overkill to me. This might be the perfect fit. Thanks for the explanation. reply paulddraper 15 hours agoparentprev> its HTML as a stringCan you explain what you mean by that? reply itslennysfault 15 hours agorootparentThis is the example from the article, and pretty common practice for web components. As far as an IDE is concerned this is just a string of text (it doesn&#x27;t know&#x2F;care that it is html). This makes it more error prone as the IDE can&#x27;t catch syntax errors in the html. connectedCallback() { this.shadow.innerHTML = ` Hello from a web component!p { color: pink; font-weight: bold; padding: 1rem; border: 4px solid pink; }`; } reply paulddraper 14 hours agorootparentWell, you can use `innerHTML` and a string, but you don&#x27;t have to.But there several alternatives:1. Use DOM APIs (createElement).2. Fetch the HTML as a separate resource.3. Have the HTML in a separate file and bundle it with a bundler.4. Use React&#x2F;JSX.5. Use a preferred templating library of your choice.You can do whichever of these you like the most reply thfuran 15 hours agorootparentprevHtml as a string literal in code rather than the html being in a separate .html file. reply IshKebab 15 hours agorootparentprevReact uses JSX&#x2F;TSX which is much better because it is syntax checked and even type checked in the case of TSX. reply preommr 16 hours agoprevWeb components are a half-baked solution. It only becomes useful with another framework (even if it&#x27;s very lightweight) like lit. At which point I might as well use vue&#x2F;react&#x2F;svelte since I am already brining in a library. Tech like preact is really lightweight as well, and given the popularity of react, is enough to offset any benefits web components have.I would really like it if we reached a point where a web project only had a single dependency that&#x27;s a bundler like vite that you point to an index file, and that&#x27;s it. reply OJFord 16 hours agoparentI think we&#x27;re going in the opposite direction though - everyone wants to hide what&#x27;s going on and use some magic CLI that just takes care of everything for you, don&#x27;t worry about how or when or what to do if it goes wrong, just run `flyctl deploy`, `wrangler publish` (Cloudflare), `npx wizzle wazzle` :sparkles: :tada: never `git commit -m &#x27;(chore): [...]&#x27;` again!Oh and here&#x27;s a readme example of how to set that up if you&#x27;re using pnpm v42 with sveltekit v2 and webpack v99. For any other combination of the 5 packagers, 19 bundlers, and 56 frameworks commonly in use and giving examples like this recommending each other - have fun. reply reidjs 15 hours agoparentprevI use them for a basic website (no build step) for things like navigation components and page templates. They are great for that, but not much else in my experience. Once you have a complex UI or multiple devs on a project it’s probably better to just use a full framework reply codegeek 15 hours agoparentprev\"half-baked solution\"Can you elaborate further ? Anything that removes dependencies from external libraries is a huge plus for me so I am curious as someone who is not great at JS. reply troupo 13 hours agorootparent> Can you elaborate further ?As you read the following, keep in mind that at this time Web Components have been in development for almost 12 years.The core is just three standards, CustomElements, Shadow DOM and HTML Imports (already deprecated and removed in favor of JS-only imports). And people will go out of hteir way to sell you the idea that this is lightweight, all that you need etc.However.They&#x27;ve already spawned half a dozen new web standards just to deal with issues they inflicted on themselves. None of these issues are present in any other solution&#x2F;lib&#x2F;framework that exists.These range from the fact that web components cannot participate in forms (fixed with a new spec: https:&#x2F;&#x2F;web.dev&#x2F;articles&#x2F;more-capable-form-controls) to their inability to share stylesheets (fixed with a new spec: https:&#x2F;&#x2F;web.dev&#x2F;articles&#x2F;constructable-stylesheets) to whatever else (hard to keep track).They will need at least 20 more new web standards to fix other issues that, once again, are not a problem for literally every other framework, library, or hand-written code under the sun: https:&#x2F;&#x2F;w3c.github.io&#x2F;webcomponents-cg&#x2F;2022.html.Among my favorite ones: a web component button cannot be a submit button in a form; you cannot reference an id inside a shadow root, and that breaks ARIA.To call them half-baked is an understatement. They are badly thought-out, badly implemented APIs with no forethought or visible planning, and literally no end goal in sight. The people building this met to hash out what more is needed for them to be complete only last year, 11 years into development. All development before that looked like ad-hoc patches by people surprised that a yet another thing doesn&#x27;t work, but is sorely needed. reply tauchunfall 16 hours agoprevI recently rebuild some parts of a large APM (application performance monitoring) solution to find out how much JavaScript is needed to have similar functionality (minus the backend, of course), e.g. faceted search.It&#x27;s interesting how much of a component system one can build only by using ``, ``, and ``. For the remaining interactive elements that can&#x27;t be build easily without JavaScript I plan to use Web Components.For me this is a reasonable 80-percent solution. But for web applications with high requirements I would use React and React-Aria components&#x2F;hooks. reply megaman821 16 hours agoprevI don&#x27;t feel like web components and React should compete for most people. Web components feel pretty good at making leaf components, especially when they will be used with multiple technologies. I tried using React to only make a single component for use within a larger React app and within a Django site, and it was not as smooth as a web component. Also, I tried to make a web component app and there was way more friction and less tooling than a React app. As it stands now, I wouldn&#x27;t use React for single component nor web components for an entire app. reply riquito 14 hours agoprevIt&#x27;s not all that shiny. Web components have global names (you should pretty much apply a prefix&#x2F;namespace if you want to work with others) and managing multiple version of the same component in the same page is an issue in any non trivial codebase (either use a different name per version or fix all breaking changes at once during the upgrade, unless the draft about scoping web elements became standard https:&#x2F;&#x2F;github.com&#x2F;WICG&#x2F;webcomponents&#x2F;blob&#x2F;gh-pages&#x2F;proposal... ) reply auggierose 15 hours agoprevAs far as I can see, web components take only strings as arguments. That alone disqualifies them from being a serious component framework. I&#x27;ve looked at them only for a short time last month, because I was excited about a web native component framework, but they provide basically nothing I would expect from such a framework. They are not even in the same arena as React. reply djrenren 15 hours agoparent> They are not even in the same arena as React.This is 100% correct. I think a lot of the disappointment about web components comes from a mismatch in expectation between the spec writers and what people when they think \"components\".Web Components allow you to make new HTML elements. That is, you can make new kinds of DOM nodes out of other DOM nodes. This is powerful, but the resultant API is still a DOM-level API, and not a full templating system like React.On the other hand, because all these templating systems (React, lit, svelte, etc.) ultimately boil down to a series of DOM manipulations, this allows you to create a kind of element that can be plugged into all of these systems.Custom elements are a lot more akin to the C ABI. Than a fully featured framework. reply WickyNilliams 11 hours agoparentprevStrings as _attributes_, yes. But that&#x27;s par for the course when it comes to html (boolean attributes aside).However, anything can be passed as a _property_. React operates on properties by default (hence `htmlFor` and `className`, not `for` and `class`). In fact, react&#x27;s upcoming improved WC support will first do an `in` check on a property name before falling back to attribute. This is similar in vue etc.So yes if you&#x27;re only authoring plain html you&#x27;re confined to primitives, but if you&#x27;re using a framework or WC lib, they almost certainly support properties reply auggierose 11 hours agorootparentI see, interesting. That&#x27;s certainly helpful. Still, I don&#x27;t see what problem web components solve. New custom html tags? Nice, but useless. And if I have to use a framework or WC lib on top of web components... Well. Nothing gained, it is just switching one framework for another. I am not against that at all, in fact I am writing my own framework for specific reasons. But let&#x27;s not pretend that web components are a solution to anything, or that there are specific reasons why it needs to exist. reply socketcluster 11 hours agoprevThe main problem with the Shadow DOM which makes people avoid it is the inability to use CSS externally to style the elements which are generated internally by the component...This is unfortunate because the Shadow DOM is essential if you want to work with child components slotted into it from the outside. This opens up many use cases.There is one alternative which is not being considered; it&#x27;s possible for components with a shadow DOM to inject their elements into the Light DOM (where they can be styled&#x2F;skinned externally with CSS), you just have to make the user slot a div (or other element) from the outside to act as a &#x27;viewport&#x27; so that the component can inject its children into it (instead of injecting them into its Shadow DOM).With this approach, you can still access slotted elements. It&#x27;s an ideal pattern for situations where you want the component to generate child elements from a slotted template.I wrote an article about this approach here: https:&#x2F;&#x2F;dev.to&#x2F;jondubois&#x2F;web-components-the-template-viewpor... reply djrenren 11 hours agoparentThis is a great approach for components that want to inherit all styling from their context.In cases where you want to introduce just a little styling, the CSS Parts API is really cool: https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;CSS&#x2F;::part reply socketcluster 10 hours agorootparentThanks for pointing this out. I should probably mention this approach in my article for cases when you need to work with slotted elements but don&#x27;t want to inherit all page styles. reply aitchnyu 5 hours agoparentprevIs there a comparison between light and shadow dom for webcomponents? I kinda want the simplicity of applying light dom css. Svelte doesnt support slots in light dom, I want to know the other caveats of light dom. reply r3trohack3r 16 hours agoprevFor folks looking for a lightweight JS class that manages the lifecycle of Web Components, I have this little Template class I use in many of my front-end projects. It&#x27;s able to manage the application state by pushing the state down through the application and bubbling changes up.It&#x27;s a 161-line file that&#x27;s easy to reason about and modify as needed.https:&#x2F;&#x2F;github.com&#x2F;retrohacker&#x2F;template reply xutopia 16 hours agoprevI&#x27;ve seen the entire industry move towards React&#x2F;Angular type frameworks and it&#x27;s invariably been a really bad thing. Large projects end up with front end specialists and backend specialists and very few actual full stack devs. What ends up happening is that no one person can do an entire feature anymore.I&#x27;m all for web components! reply lopis 16 hours agoparentThis is not my experience at all. Why can&#x27;t a full stack dev know react or angular? reply codegeek 15 hours agorootparentGP didnt say that. They said lot of devs become \"front end\" or \"back end\" specialists which means that they cannot do full stack. Full stack devs knowing React is fine. The problem is that most \"React devs\" don&#x27;t know full stack and most \"backend devs\" don&#x27;t know full stack. reply zztop44 10 hours agorootparentWhy does being a front end or back end specialist mean you can’t do full stack? If I’m a full stack dev that’s a back end specialist I can still write a complete feature myself just fine, thank you very much. But of course, if I’m having trouble with some part on the front end I might ask a front end specialist colleague for help, and vice versa. reply schwartzworld 16 hours agorootparentprevApparently because learning anything more than the minimum necessary to put pixels on a page makes you a specialist? reply joshstrange 9 hours agoparentprevIn your opinion what is the “really bad thing” that React&#x2F;Angular have resulted in? Is it just the fewer full stack devs? I mean I’m a full stack dev still while using Vue. But also specialization (at a certain scale) is nearly unavoidable. reply shadowgovt 16 hours agoparentprevThe concerns, constraints, and behavior of server-side headless software and a viewer that fetches and renders data from a remote backing store are entirely different.Is it a bad thing that organizations specialize on those concerns? Seems like the most obvious place to slice responsibility, even if it means every feature is split across two teams. After all, the performance of the backing store is impacted not by neigbhoring logic in the UI infrastructure but by neighboring state in the backing store; it should be someone&#x27;s job to be looking at the backing infrastructure holistically and not sliced as \"frontend-backend feature A, frontend-backend feature B, etc.\" reply djrenren 14 hours agoprevPlease remember that Web Components are much more akin to an ABI for web projects and not a full-featured framework.Any web component easily plugs into React, svelte, lit, etc. Existing components written in those frameworks can pretty easily be wrapped in a web component. It&#x27;s a common base-layer we can all use.If you attempt to build a web app using just web components and no libraries, you&#x27;ll quickly find that you&#x27;re doing lots of manual DOM updates (no templating engine) and lots of manual state management (no data management API).As an ABI, it&#x27;s wildly successful. It does, in fact, just work. It&#x27;s just very low level. reply iteratethis 11 hours agoprevI don&#x27;t think there&#x27;s a need to cause a stir and generate hate for either Web Components or React.We should hate both equally.Either approach is frankly an embarrassing way to build complex interactive applications. Web tech is just way too low level, batteries not included. We&#x27;ve added mountain of complexity to deal with it but it doesn&#x27;t hide that the fundamentals are broken and that productivity is low. reply sohzm 16 hours agoprevlit.dev is awesome.I am a backend dev so I don&#x27;t really want to get into the intricates of React or Angular. But if I need some reactivity for some complex element, I use lit. It&#x27;s awesome reply davetron5000 15 hours agoprevWeb Components are not a replacement for React. They are a thing you would use to build React if you were to do so today. To use only Web Components, you will be writing a lot of low-level code using the browser&#x27;s API, or you will have to use a framework built on Web Components reply troupo 13 hours agoparent> They are a thing you would use to build React if you were to do so today.No, you wouldn&#x27;t. Very few JS frameworks (however new they are) use web components as their foundation for many, many, many reasons. reply willsmith72 16 hours agoprevi did this back in 2019 to make the transition smoother between angular.js and react> there are ways to build a web app other than writing the entire thing with a single JavaScript frameworktotally agree, and reducing rewrites or making converting them into \"strangler\" rewrites is a huge winon the other hand, it reminds me of the microservices hype a few years back. Just because you can, doesn&#x27;t mean you should. Uniform languages, frameworks, coded patterns make things SO much smoother for an organisation.I would repeat the author&#x27;s words of warning: \"you should not build a real app like this!\". you should seriously think hard before adding this level of complexity to any production app reply dang 11 hours agoprevRecent and related:Web components will outlive JavaScript frameworks - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38012662 - Oct 2023 (244 comments) reply yohannparis 17 hours agoprevThank you for the great read! The last few weeks have seen a lot of thought on HTML web components, and I&#x27;m really happy about it. reply jdmg94 15 hours agoprev> We’ve seen a lot of great posts about web components lately.where? I work at a fairly big org that some time ago decided to bet on web components. Everyone I work with (including me) hates it. reply jatwork 15 hours agoprevDo we see a future where the web development free of these frameworks? reply sesm 14 hours agoparentWhen browsers implement some API to efficiently synchronize state with UI a-la React. reply byyll 15 hours agoprevI wonder how the preview on hover was made. reply adam_arthur 15 hours agoprevPeople discussing writing Web Components by hand are missing the point.Web Components, once",
    "originSummary": [
      "Web components offer benefits in terms of avoiding JavaScript framework lock-in.",
      "The article showcases the process of using web components alongside popular frameworks like React, Solid, Svelte, and Vue.",
      "Code examples and additional resources are provided for further exploration of web components."
    ],
    "commentSummary": [
      "The passage explores discussions and debates on web development topics like Web Components, React, JSX, and other frameworks.",
      "It covers the limitations, advantages, and criticisms of these technologies, along with alternative approaches and opinions.",
      "State management, templating, component creation, and design patterns are among the topics discussed in the conversations."
    ],
    "points": 248,
    "commentCount": 249,
    "retryCount": 0,
    "time": 1701103295
  },
  {
    "id": 38433083,
    "title": "Top 5 Database Shirts: A Fashionable Tribute to Promoting Tech",
    "originLink": "https://www.cs.cmu.edu/~pavlo/blog/2016/07/my-favorite-database-shirts.html",
    "originBody": "My Favorite Database Shirts Posted on Tuesday July 26, 2016 at 09:31 AM TL;DR I like wearing database shirts. MongoDB's original shirt from the early 2010s is the vanguard. You should never opt for a cheaper material. When I was an undergrad, I was really into shirts from obscure indie and punk bands (see example #1[1] and example #2[2] from 2004). It was my (pretentious) way of trying to be different by wearing shirts from bands that broke up after only one album[3]. I'm older now and I don't run basement shows anymore. So now that same zeal that I had back then for band shirts is now channeled towards database shirts. I now wear shirts from database companies that broke up and no longer exist (e.g., Akiban, Xeround). Promoting your database system or start-up with a shirt is almost as important as getting the thing to actually run. As I've told my students several times, in the world of databases you don't sell the steak, you sell the sizzle. Over the years, I've collected a number of shirts from database companies (mostly newer NoSQL and NewSQL start-ups). A good shirt can get people to feel like your DBMS is going to solve all of their application's database problems. Of course this means you have to have good branding and logo design. But all of these efforts should be in the name of working towards the goal of making an impressive shirt that people will want to wear and promote your system. In this article, I provide a list of my top five favorite database shirts. I have other ones that I like a lot (and ones that I think are dreadful[4]), but the shirts listed below are the ones that feel are a step above the others. Eventually we will conduct a more scientific evaluation of these systems, but I thought that it would be fun to compare them based on their shirts. I evaluate the shirts based on the following metrics: Material Quality: How soft and thick is the shirt? Does it feel sturdy and a high quality? Fit: How well does the shirt fit on someone with a normal body shape? Design: Is the artwork on the shirt interesting and something that one could wear in public? Per Margo Seltzer's guidelines, extra points are given to companies that provide a women's cut shirt. I recognize that companies typically make nicer items for their employees (e.g., MemSQL has Patagonia jackets). The shirts in this review are only ones that the companies give out to non-employees (e.g., customers, professors). #1 — MongoDB Brand: American Apparel 50/50 Shirt Material: 50% Polyester / 50% Cotton Year: 2010 As far as I know, this is the original shirt released by MongoDB. This was the hot item when I got it a few years ago. I consider this to be the gold standard for a database shirt. MongoDB spared no expense in getting the plush American Apparel 50/50 cotton blend. The cut is flattering. The design is clean and direct (with three colors). It's not entirely obvious that this is a tech shirt, so you can wear it in public. They get also points for using a non-traditional shirt color (i.e., it's not black, white, or gray). Over the years, the shirt has held up well and has gotten softer. This is an example of what all database companies should aspire to have with their own shirts. A lot of companies go cheap and get Hane's Beefy-T, but these shirts feels like sandpaper on my delicate body. It shows that paying a little extra to get a nicer quality shirt is worth it because it makes people want to wear it. I remember when they would come to recruit at Brown and the next day there would be non-CS people walking around campus with MongoDB shirts. I know that they made women's cut versions too, because a lot of girls would wear them to the gym. The success of Eliot Horowitz and his crew with this MongoDB shirt inspired me to make a shirt for my own DBMS before I went on the job market. I attribute this shirt to a lot of my own early success. #2 — NuoDB Brand: Next Level Material: 50% Polyester / 25% Cotton / 25% Rayon Year: 2013 This is the only long sleeve t-shirt that I have in my top five list. I also consider this to be one of the best shirts ever put out by a database company. I would have ranked this #1 except that I think that the MongoDB shirt above had a larger impact on their popularity. This NuoDB shirt is high quality and soft. Next Level shirts are just as good as American Apparel. It's also rare to have a t-shirt with a hood. The design is bright and interesting. This has the best artwork of all the shirts because it's not just their logo slapped on a shirt. NuoDB actually had somebody design this and it shows. Most people would not know that this shirt is for a database company. I always wear this shirt when I have to fly long distances. It's sort of my \"thunder shirt.\" I also have the light gray variant of it that they gave my wife (it has different artwork). She loves hers equally as much as I love this one here (although it is not a women's cut). I think they told me that these were a limited edition. NuoDB's new shirts are good, but not nearly as nice as this one (but they do have women's sizes). #3 — Snowflake Brand: Canvas Material: 50% Polyester / 25% Cotton / 25% Rayon Year: 2016 The Snowflake team (smartly) followed the MongoDB playbook with this shirt. This is the highest quality database shirt that I have received in the last year. The database fam agrees with this assessment. The companies name is on the front and their logo is on the back. All of the printing is in white. They also have a nice sky blue version as well. Although I have only worn this shirt for a few months, it already buttersoft. It is even softer than the MongoDB one above. The only issue is that I have been asked whether the shirt is for a ski resort. It is not obvious to non-database people that this is a tech company. That can be good or bad depending on your lifestyle. #4 — Altibase Brand: Next Level Material: 90% Cotton / 10% Polyester Year: 2016 This shirt is a bit banal, but I like it because Altibase is a (slightly) obscure DBMS. It is a Korean system that was one of the first in-memory DBMSs from the 1990s. This is the most nerdy of all the shirts listed here (even though all database shirts are by definition nerdy). I like it because it has a vintage aesthetic that reminds me of the stolid tech companies from the 1980s. It says upfront that it's a relational DBMS so there are never any questions about what the shirt means. It also tells you that their DBMS means business since it's \"enterprise grade.\" The shirt material is a bit thinner than the others. I have avoid putting this shirt in the dryer because I am afraid that it will start to tatter. The printing is two-color and has set in nicely into the fabric. #5 — VoltDB Brand: Hanes Nano-T Material: 100% Cotton Year: 2015 This last one is the only Hanes shirt that I will wear. This is not as bad as Beefy T. The primary red color is uninteresting. No database company should ever use this kind of shirt. With that said, the reason why it makes my top five list is that it is a limited edition shirt put out by VoltDB in celebration of Mike Stonebraker winning the 2014 Turing Award. So it's special to me for that reason. It has a picture of Mike's beautiful visage and an INSERT query about him becoming a Turing Laureate. It is also the only shirt in this listing where I have both men's and women's version (shown in the photo here). I have the other VoltDB Warhol-style Stonebraker shirt, but it's too large so I can't wear it. I am a big proponent of putting a picture of your face on a shirt. Disclosure: I was on the team that wrote the system that later became VoltDB and Stonebraker was one of my PhD advisers, so I am probably biased because of that. Attention Companies: If you want me to review your database shirt in the future, you can send a men's medium and/or a women's small shirt to: Andy Pavlo Dept. of Computer Science Carnegie Mellon University Gates-Hillman Center 9019 5000 Forbes Avenue Pittsburgh, PA 15213-3891 Footnotes Welcome the Plague Year Sleepytime Trio This was well before the term \"hipster\" came into existence. I was going for the \"I'm not a CS nerd\" look at a nerd school. It did help me meet my future wife. Coincidentally, the Akiban and Xeround shirts are actually two of the ugliest ones that I have. And now they no longer exist. Think about that...",
    "commentLink": "https://news.ycombinator.com/item?id=38433083",
    "commentBody": "My favorite database shirtsHacker NewspastloginMy favorite database shirts (cmu.edu) 223 points by k-rus 18 hours ago| hidepastfavorite127 comments Sohcahtoa82 16 hours agoMy favorite t-shirt has the Azure logo in the style of the Amazon logo on the front, and the AWS logo in the style of the Azure logo on the back.https:&#x2F;&#x2F;github.com&#x2F;jogerj&#x2F;misbrands&#x2F;blob&#x2F;master&#x2F;azure.svghttps:&#x2F;&#x2F;github.com&#x2F;jogerj&#x2F;misbrands&#x2F;blob&#x2F;master&#x2F;aws.svg reply redhale 4 hours agoparentThis repo, and the other forks of its original which have even more \"misbrands\", are going to have me ordering some custom sticker sheets here shortly, thanks for sharing!The AWS&#x2F;Azure and Java&#x2F;JavaScript ones are good, but the Vim&#x2F;VS Code one is gold. reply easton 12 hours agoparentprevDid you have that made or is there somewhere to buy it?This is the exact kind of dumb shirt I’m looking for reply Sohcahtoa82 11 hours agorootparentI had CafePress make it for me.I originally tried SpreadShirt, but they rejected the design due to copyright. reply jdelman 17 hours agoprevI disagree with nearly this entire post, which is fun. I find that the poly blend shirts that the author loves (especially like the MongoDB shirt) are clingy and sweaty. Softness is low on my list of shirt priorities - comfort and fit are high. I don&#x27;t know how you could see \"mongoDB\" and not think it is tech, but maybe I&#x27;m too mired in tech to have an outsider&#x27;s perspective. Also, that dark heathered grey color is everywhere in tech shirts right now and it just screams tech. The only one of these with a good design is the Altibase shirt, but it suffers from having the ugliest color.Give me black, 100% cotton, heavyweight Hanes Beefy-Ts with retro-looking logos. reply airstrike 17 hours agoparentSomewhat off-topic but for those who want a black, 100% cotton and lighterweight t-shirt, IMHO nothing compares to Intimissimi[0]They run a bit small so I recommend one size larger than your regular size. They&#x27;re currently on sale from Black Friday so only $14.30 each. (Large Black is now out-of-stock, sorry!)They also make many other varieties like v necks, long sleeve, modal&#x2F;cashmere&#x2F; etc.(Claimer: I&#x27;m not affiliated or getting a referral from this... I just think they&#x27;re a lesser known brand in the US)__________0. https:&#x2F;&#x2F;www.intimissimi.com&#x2F;us&#x2F;product&#x2F;short_sleeve_crew_nec... reply airstrike 14 hours agorootparentAlso worth noting that the only review on the website is very accurate despite being a 1-star review: \"The quality of material is great. Neck is too low. Too lenghty\".These are exactly the reasons why I like this shirt. Great material, neck is not too tight, probably an inch longer than many other fitted t-shirts, so your underwear isn&#x27;t showing all the time. reply loxias 12 hours agorootparentVery good to know, now I won&#x27;t waste $15. ;-)This whole comment thread is teaching me that apparently there isn&#x27;t &#x2F;any&#x2F; objective truth when it comes to t shirt comfort.Before today I figured that was an objective fact that the cheap Hanes ones were unloved -- neck always stretched out, too short and stout.I&#x27;m a fan of LA apparel (aka american apparel) as well as some bella+canvas ones. That stretched out neck just gives me flashbacks to middle school, ugh. reply hodgesrm 12 hours agorootparent> This whole comment thread is teaching me that apparently there isn&#x27;t any objective truth when it comes to t shirt comfort.That&#x27;s true of many things and has been for a long time. \"De gustibus non disputandum est,\" as the Romans said. \"It&#x27;s useless to argue about taste.\" reply heresie-dabord 6 hours agorootparent> \"It&#x27;s useless to argue about taste... or T-shirt comfort.\"I updated your statement for the remainder of modern history. ^_^ reply airstrike 10 hours agorootparentprevFWIW I don&#x27;t think Intimissimi shirts have a quote-unquote \"stretched out neck\", just a tiny bit of extra room... I hadn&#x27;t even noticed it until I read the review reply ljm 15 hours agoparentprevThe author calls a hoodie a shirt. I’m not sure I trust their opinion on the fit and comfort of database-branded apparel. reply munificent 12 hours agorootparentI strongly disagree with the claim that it&#x27;s a hoodie simply because it has a hood. Many hooded garments are not hoodies. To be a hoodie, it must be made out of thicker sweatshirt material.A long-sleeve T-shirt with a hood is just a long-sleeve T-shirt with a hood. reply mathgeek 11 hours agorootparentIndeed. Anyone who lives near a beach in a warm climate is likely familiar with these long sleeve hooded “tees”. reply abhibeckert 9 hours agorootparentAs someone who lives near a beach in a warm climate... I&#x27;ve never seen anyone wear one of these nor have I seen them for sale in stores.I can&#x27;t imagine when I&#x27;d wear one either - if I want protection from the sun I&#x27;ll wear a hat or sunscreen - not a hood. reply __MatrixMan__ 6 hours agorootparentMy whole climbing crew has made the switch from hats and sunscreen to sunshirts. They fit under a helmet, and they feel less cancery than repeatedly slathering on some goo and crossing your fingers. It&#x27;s one thing if you&#x27;re free to roam about to a more comfortable area, but if you&#x27;re stuck on belay duty on the sunny part of the face, you&#x27;ll be happier with a sunshirt.And then you&#x27;re used to it, and you start wearing them elsewhere too, because why bother with lesser protection?They may be more popular at higher altitudes than at the beach though. Beach folk have a bit more atmosphere protecting them. reply bitvoid 6 hours agorootparentprevI typically have really short hair, so I wear one with the hood on when it&#x27;s chilly inside, but not so chilly that I want the heater on or want to wear a sweater. reply llbeansandrice 6 hours agorootparentprevI wear poly ones that are spf50 all the time when I’m fishing so I don’t need to worry about applying sunscreen to large parts of my body multiple times a day. reply bitvoid 15 hours agorootparentprevTo be fair, it&#x27;s not really a hoodie either as I feel that implies it&#x27;s a sweater or has the thickness of one. It really is just a long sleeve shirt with a hood attached. reply stvltvs 12 hours agorootparentWhat is the distinction in your mind? reply bitvoid 6 hours agorootparentThe same distinction between a long sleeve t-shirt and a crewneck sweater. I don&#x27;t know how they&#x27;re made, but my layman distinction would be \"thickness\". reply llbeansandrice 6 hours agorootparentprevTees are made of a tee shirt material like jersey knit while hoodies are made from a sweatshirt material like terry cloth or a cotton fleece. reply matsemann 13 hours agoparentprev> don&#x27;t know how you could see \"mongoDB\" and not think it is techI disagree with the author in that I would be comfortable wearing a mongodb shirt in public. Mongoloid is a slur, and at least in Norway it&#x27;s often shortened to \"mongo\". reply icelancer 5 hours agorootparentI wouldn&#x27;t be comfortable wearing a MongoDB shirt in public either. I mean, it&#x27;s awful technology. reply jjgreen 9 hours agorootparentprevIs that a racial slur (i.e., Asian), or about Down syndrome? The latter was used in the UK many years ago, I&#x27;ve not heard it for 25 years (?), so \"mongo\" wouldn&#x27;t raise an eyebrow here now. reply matsemann 1 hour agorootparentIt&#x27;s kinda both. Mongoloid here used to refer to people with Downs, by comparing them to the (non existent) Asian race of Mongoloids due to both often having eyes with epicanthic fold. Of course I didn&#x27;t know this when using it as a kid back in the days, and I&#x27;ve luckily not heard it used for ages, but I think all of my peers would remember it if used on a t-shirt. reply vladvasiliu 5 hours agorootparentprevIn France, that would be related to Down syndrome. Technically, in these parts it would be \"mongol\", with an l, like the people of Mongolia.AFAICT it&#x27;s not exactly used against people with actual Down syndrome, but rather as a synonym for \"idiot\" and the like. reply stvltvs 12 hours agorootparentprevInteresting, in the States, that term is no longer used very much and was never shortened AFAIK. reply selimthegrim 10 hours agorootparentI’m pretty sure it was shortened, but as a name for a character reply jjgreen 9 hours agorootparentIn \"Blazing Saddles\" ... reply leetrout 17 hours agoparentprevFollow on, my biggest frustration with these super thin, clingy shirts is how _everyone_ is showing their nipples through them now. reply CrazyStat 16 hours agorootparentWhy does seeing nipples through a tshirt bother you so much? reply nerdponx 13 hours agorootparentIt&#x27;s just a weird fleshy body part that I&#x27;m not used to seeing and generally don&#x27;t want to see on other people. Also the feeling of a plasticky \"technical\" shirt on my own nipples is very unpleasant. reply 0xbadcafebee 14 hours agorootparentprevThelephobia - fear of nipples reply hotnfresh 16 hours agorootparentprevWell—T-shirts did start out as underwear. reply jjgreen 9 hours agorootparentprevNipple covers are a thing reply bubblethink 16 hours agoparentprev>I don&#x27;t know how you could see \"mongoDB\" and not think it is techDepends on the age. MongoDB may not have been such a common name back when the tshirt came out. I have a couple of Palantir t-shirts (american apparel, 100% cotton I think) that have held up over a decade and are really comfortable. One of them says, \"Save the shire\" and Palantir. I don&#x27;t think people would have known that it&#x27;s a tech tshirt back then. reply mulmen 14 hours agorootparentEverything about that organization offends me. They took their name from a series of books they apparently never read.How is Palantir supposed to save the shire? By trampling the rights of the Hobbits in addition to destroying their environment?The Palantir show a narrow view of events and lead to their users to ruin. It’s like they read the cautionary tale as an instruction manual.They chose an accurate name for what their product does, but I can’t understand how a person with that clarity of thought would decide to actually make one. reply dbt00 12 hours agorootparentPalantir is a Quenya word meaning far-seeing. The last successful king of Numenor was Tar-Palantir for example.The palantiri were corrupted by Sauron and limited to only show things that he wanted you to see. This was extremely well known inside Palantir and was deliberately talked about as something we should all consider the risk of.The origin story of Palantir was the intelligence failures that led to the 9&#x2F;11 attacks not being caught. The goal was to prevent the total eradication of civil liberties that would necessarily follow another successful attack of that magnitude.Palantir’s software was rejected by organizations performing dragnet style mass data collection.This is the most pointless hn comment I’ll ever leave, but you shouldn’t assume other people are ignorant or acting out of malice.Also the tshirts were comfy as fuck and extremely well designed. reply mulmen 8 hours agorootparentI don’t doubt the good intentions. I do question the outcome. reply selimthegrim 10 hours agorootparentprevThey signed a contract with the city of New Orleans! reply pc86 13 hours agorootparentprevThis seems unrelated to the comfort and fit of their shirts, but maybe not? reply mulmen 8 hours agorootparentI have been in the same room as someone wearing a Palantir shirt and I was uncomfortable. reply tannhaeuser 11 hours agoparentprev> I don&#x27;t know how you could see \"mongoDB\" and not think it is techIdk but mongo-anything has clearly insulting connotations hasn&#x27;t it? I&#x27;m not a fan of cancelling language at all but still find it very surprising a product with that name could advance that far in enterprise computing. I guess I had hoped the article would explain the joke I was missing here.Update: ads on a CMU site? reply ourmandave 15 hours agoparentprevMy holy grail was the Hanes ComfortSoft 100% cotton T. Available in 6-paks at the local Walmart. They start soft and just get better.But they changed the fabric a while ago and it&#x27;s not the same.Still have a few that are getting pretty threadbare. reply gammarator 6 hours agorootparentWhat you want now is Hanes Premium slim fit, which is a cotton&#x2F;poly blend similar to the old ComfortSoft.Watch out because some Hanes Premium are 100% cotton, which is not what you want at least at this price point. reply a_t48 13 hours agoparentprevBlack 100% cotton tends to collect lint and cat hair, doesn’t it? reply djbusby 17 hours agoparentprevThat shirt, with long sleeves. My go-to for 20 years. reply dminor 18 hours agoprevThe 50&#x2F;50 shirts are great. I&#x27;ve got a stripe shirt that first turned me on to them and I went online and bought a bunch of blank ones in different colors.My current company found a similar shirt from bella+canvas that is also great. It and the stripe shirt are the only vendor shirts in my regular rotation. reply leetrout 17 hours agoparentI will toss out a second vote for bella+canvas being a modern equivalent to the american apparel of yesteryear.What is very frustrating is I have 2XL shirts from 2007-2009 that fit better than a 4XL from 2022. I&#x27;ve tried to narrow it down to brands and blends and the 50&#x2F;50 or tri-blend shirts have held their true sizes for longer but somewhere along the way every brand has seemed to have gotten smaller by full inches. In underarmor polos I now buy their tactical line to get the equivalent of their \"loose\" fit from yesteryear. It is so frustrating and sometimes a huge money waste buying shirts.The longest lasting shirts in my closet are some Hurley and Billabong shirts from Hot Topic and similar from 2010 or earlier.For any conference organizers or swag buyers reading this, please start offering up to 4XL in any cheap brand shirts, especially 100% cotton, even if it is preshrunk. reply dylan604 17 hours agoparentprevWouldn&#x27;t it just be easier to find 50&#x2F;50 t-shirts without the branding and not be some tool of a walking billboard rent free?Edit: Ignore this. Reading comprehension clearly needs work reply cocoflunchy 17 hours agorootparentIsn&#x27;t that exactly what dminor did? reply dylan604 17 hours agorootparentYou&#x27;re right. I misread \" went online and bought a bunch of blank ones in different colors.\" by completely reading past \"blank ones\" and thought they went to get different colors of the branded shirt.reading comprehension was always a skill set lost on me reply jprd 14 hours agoparentprev> Hanes Beefy-TMy current company has been using bella+canvas of late, and I wear them all_the_time, they are so comfortable that I worry about their durability.I&#x27;m also super, super big into the collars not stretching out. I can&#x27;t stand that ¯\\_(ツ)_&#x2F;¯ reply zellyn 17 hours agoprevFor some reason the Google Fiber T-shirt that Google gave out in Atlanta is almost indestructible. I&#x27;ve worn mine routinely for years, and still see other people wearing them sometimes. Not sure exactly what the secret was, but boy would I love to know! reply powerset 17 hours agoparentMust be the quality of the fiber reply zellyn 15 hours agorootparenttouché reply danielvf 17 hours agoparentprevI&#x27;ve gotten several Google Fiber t-shirts from a local thrift store, and I can confirm - great shirts! I think they are NextLevel. reply zellyn 15 hours agorootparentNextLevel as a superlative, or https:&#x2F;&#x2F;www.nextlevelapparel.com&#x2F;? reply zmj 10 hours agoparentprevYup. I’ve had one of the North Carolina Google Fiber shirts for well over a decade.I walked by the office a few weeks ago. It’s vacant and looked trashed inside. There’s a metaphor there. reply jroseattle 17 hours agoprevIt&#x27;s been 20+ years but I loved this shirt for it&#x27;s SQL agnostic-ity:https:&#x2F;&#x2F;www.pinterest.com&#x2F;pin&#x2F;lol-code-geek-select-from-user...(This shows the shirt you could buy once-upon-a-time from thinkgeek, which appears to be no more...)I believe they used beefy-t brand shirts, which was like the nicest thing you could get for t-shirts at the time. (Personal opinion, don&#x27;t @ me.) reply leetrout 17 hours agoparentI think my wife finally tossed all my old thinkgeek shirts. I had a huge collection from my college years including \"There are only 10 types of people in this world...\", \"man love\" and \"there&#x27;s not place like 127.0.0.1\".Who has filled this void? I know xkcd had&#x2F;has some geeky shirts and they shut down too. reply blacksmith_tb 13 hours agorootparentHmm, I too miss the good ol&#x27; days, though I have more old tattered Threadless shirts than ThinkGeek. I suppose something like shirt.woot.com might be where the (nerdy) kids today look? reply easton 12 hours agorootparentprevCotton Bureau has some, although they are kind of pricy. reply quickthrower2 14 hours agoparentprevnext [–]SELECT * FROM SITES WHERE FLAGS LIKE &#x27;%BACKBUTTONHIJACKED%&#x27; reply mcoliver 17 hours agoprevMy favorite I found on Amazon: I keep all my dad jokes in a Dadabase reply graypegg 6 hours agoprevCypress, the test runner, was offering up a free t-shirt if you attended any webinar in 2019. The test engineer on my team at the time got us all in on leaving the webinar open in the background. We got a full team set of Cypress shirts! Was the uniform for demos for the next couple sprints hahaI still have a couple (we did it multiple times) and actually really like them. Same 50&#x2F;50 blend as the mongo shirt. reply CoastalCoder 17 hours agoprevIt was so cool to see Mike Stonebraker&#x27;s face on a t-shirt!Back in grad school, I was lucky enough to be on a research group that he helped lead. He was totally unpretentious, and at the time I had no idea what a big deal he was in the database world. reply throw0101c 17 hours agoprevThe History Guy recently posted \"A Brief History of the T Shirt\":* https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Ds_xyrZCSo8Generally: started out as undershirts, became work shirts (especially in the USN during WW2), and then folks just started wearing them on a daily basis post-war, mostly in &#x27;misfit&#x27; &#x2F; counter-culture groups in the 1950s (especially amongst teenagers), and the hippy culture embracing them in the 1960s. reply KnlnKS 10 hours agoprevWhile I don&#x27;t have a MongoDB shirt. I do happen to have a pair of MongoDB Shoes (from the Vans brand). They&#x27;re quite good and probably some of the best swag I&#x27;ve gotten from a tech brand. reply icelancer 5 hours agoprevI got that MongoDB shirt at a conference in Seattle about 10-13 years ago as he stated. He ain&#x27;t kidding. It&#x27;s a great shirt.Now I gotta go find it in my closet... reply sbuttgereit 16 hours agoprevI&#x27;ve gotten a couple PostgreSQL t-shirts over the years when I volunteered for this or that.My favorite one is the one for the release of PostgreSQL 8.4. It&#x27;s simply blue on white and the design is after one of the boxes from the periodic table of the elements: with \"Pg\" in the middle and (as I recall, it&#x27;s not in front of me now) and an \"atomic weight\" of 8.4.The other was for the release of PostgreSQL 9.0 when replication was introduced. It&#x27;s a blue shirt with white print. That banner feature is the prominent text on the front and a herd of Elephants charging at the viewer like you might see a stampede of horses do on a western movie&#x27;s poster.Good designs celebrating milestone events for the project. reply montanalow 16 hours agoprevIf anyone would like a free PostgresML T-shirt, we just did our first run. Feel free to email me with your shipping info and size. It&#x27;d also be nice to get to know you a bit if your email address isn&#x27;t obvious. reply geffchang 6 hours agoparentIf you ship the Philippines, would love to get one. First time to hear about PostgresML. Email add in profile. reply Icathian 15 hours agoparentprevEchoing the other commenter, I&#x27;d be very interested but I can&#x27;t find an email address to ping you.I&#x27;m a database geek and would happily rep your stuff! reply reducesuffering 16 hours agoparentprevYou need to put your email in your profile. People aren&#x27;t going to comment it. reply ecshafer 17 hours agoprevI don&#x27;t like MongoDB as a technology, but I have that T-Shirt and its super comfortable. reply monknomo 13 hours agoparentMe too, though mine is getting on towards too threadbare to wear in public reply computerfriend 17 hours agoparentprevSame, and I didn&#x27;t realise I was holding the number one best database t-shirt in my wardrobe. reply danielvf 17 hours agoprevLike the author, my favorite t-shirt of all time is an early 2010&#x27;s American Apparel blend shirt, (though mine is a Stripe CTF3 winner shirt.) However, American Apparel changed something mid decade, and I&#x27;ve never been able to get anything close to as comfortable as those shirts.My current standard shirt are the NextLevel tri-blends, which are almost as good, and can be ordered plain online, or you can find them with all the logos you want in thrift store near techie areas. reply xbar 15 hours agoprevNone of those compare to my Legend of ZDLRA: A Link Between Databases shirt but I’m grateful to have a forum to review such things. reply TuringNYC 18 hours agoprevI am biased since I worked at KineticaDB, but the KineticaDB t-shirts were the best swag I&#x27;ve ever received. I have a closet full of them, 3x&#x2F;yr across the three years I was there. I wore some socially and fashionably, and others at the gym. They used high-end fabrics comfortable for day and night.I think about ROI on marketing and this must be incredible w&#x2F;r&#x2F;t Employee alignment. Imagine having employees wear your brand day in day out! reply dylan604 17 hours agoparent3x&#x2F;yr X 3years == closet fullhow small is your closest? i&#x27;ve heard how small apartments can be in places like NYC&#x2F;SF, but if the space provided for your clothes is filled by 9 t-shirts, i&#x27;m going to need to re-evaluate small again. reply Z_Z 17 hours agorootparentleave it to hackernews to debate the semantics of figures of speech reply TuringNYC 15 hours agorootparentprevThe article was on T-Shirts, but I also have 3x * 3 hoodies, Patagonia vests, Fleeces, the works.The 3x * 3 was meant to convey the variety (we did unique prints each IRL meeting) reply mattsears 9 hours agoprevIf you like Ruby, there&#x27;s a bunch of nice shirts (and more) at https:&#x2F;&#x2F;popruby.com reply shaunxcode 4 hours agoprevNever realized how much I would like a datomic shirt until now. reply nofinator 16 hours agoprev+1 for how great the Snowflake t-shirt is (ranked #3). I wore it a lot during the COVID era, and the poly blend is still soft and has withstood a lot of washing and drying.When we signed onto Snowflake in 2019, a week later a surprise HUGE box of swag arrived, with a dozen shirts and lots of other things. Our team and corner of the office became The Place To Be for a while. reply stickfigure 16 hours agoprevI&#x27;m in the print space and this post is weird. I have that exact same green MongoDB on the AA 50&#x2F;50, as well as a couple other MongoDB shirts printed on different blanks. Somebody picked the design, picked a blank, and printed a batch. Next week somebody else will pick a different blank, maybe even for the same design. reply ifaxmycodetok8s 17 hours agoprevI had that exact MongoDB shirt. Got it from a hackathon back in 2014. Was such a soft shirt. Loved it. reply turtlebits 15 hours agoprevTableau used to give employees t-shirts multiple times a year (I had at least 15 by the time I left). I had a favorite I&#x27;m now reminded used \"Next Level\" shirts. Time to go buy a bunch of their blank t-shirts. reply antifa 9 hours agoprevIf you&#x27;re a leftist of any kind, my favorite is this parody shirt called OurSQL: https:&#x2F;&#x2F;www.redbubble.com&#x2F;i&#x2F;t-shirt&#x2F;OurSQL-red-and-yellow-by... reply harrisonjackson 12 hours agoprevBefore clicking through I had that mongodb shirt in mind as my personal favorite.I got a few of them at aws reinvent in like 2012.2nd favorite is a Cassandra one of similar make. reply erwinkle 16 hours agoprevI have like 6 of those MongoDB tshirts and they are SO COMFY reply bawolff 5 hours agoprev> \"Promoting your database system or start-up with a shirt is almost as important as getting the thing to actually run. As I&#x27;ve told my students several times, in the world of databases you don&#x27;t sell the steak, you sell the sizzle...A good shirt can get people to feel like your DBMS is going to solve all of their application&#x27;s database problems.\"I can&#x27;t tell if this is sarcasm or serious, but either way is a sad reflection on the state of the DB industry. reply ellisv 16 hours agoprevDid Andy update something about this post (from 2016)? reply apavlo 16 hours agoparentNo. I&#x27;ve been meaning to do an updated article but I&#x27;ve been unfortunately too busy. A bunch of companies send us shirts to give out to students:* https:&#x2F;&#x2F;twitter.com&#x2F;andy_pavlo&#x2F;status&#x2F;1659019035818729472* https:&#x2F;&#x2F;twitter.com&#x2F;andy_pavlo&#x2F;status&#x2F;1335045678876270592* https:&#x2F;&#x2F;twitter.com&#x2F;andy_pavlo&#x2F;status&#x2F;1125465168023048193* https:&#x2F;&#x2F;twitter.com&#x2F;andy_pavlo&#x2F;status&#x2F;996191088372322304* https:&#x2F;&#x2F;twitter.com&#x2F;andy_pavlo&#x2F;status&#x2F;862320227601850368My highlights from the last 6-7 years:* DuckDB (European embroidery!)* Materialize (like Snowflake)* Yellowbrick (wild designs)* Timescale (old logo was popular)-- Andy reply _dan 13 hours agoprevI have a MongoDB shirt of the same vintage and I can confirm they are indeed soft AF. reply geophile 16 hours agoprevI worked at Akiban for nearly its entire history, and I don&#x27;t think I even saw one of their t-shirts. reply achileas 12 hours agoprevStill waiting for a vendor to hand out some branded y-back tanks reply laxd 12 hours agoprevPostgreSQL? And OracleDB as a joke christmas present? reply adamretter 18 hours agoprevPersonally I love my Redis shirt and my RocksDB shirt - both excellent materials and colours. reply Galanwe 6 hours agoprevI have that same MongoDB shirt from FOSDEM 2010.I remember it vividly as that year, the infantilization of developers by companies was at its peak.MongoDB had a stand with hired girls in pump up bras distributing tee shirts. Google had its stand with the same girls proposing some \"funny quizz\" to win some blinking LED toy and an interview slot. reply nlavezzo 14 hours agoprevI&#x27;m partial to my FoundationDB shirts :) reply DonHopkins 14 hours agoprevI used to have a really nice tie dyed Microsoft DirectX t-shirt that I got as swag from the Microsoft booth at CGDC the year it was released, that I loved to wear to Linux and open source software conferences, to make people&#x27;s heads explode. reply nemo44x 10 hours agoprevMongoDB deserves credit with bucking the trend of sans-serif fonts that it seems like every tech company has gone with. Their choices of colors too - brown and green are earthy and unique in the tech landscape.Very bold choices by them and a reflection on their direction of being a very different type of database. reply ChrisArchitect 16 hours agoprev(2016) reply bigdict 18 hours agoprevIs this guy seriously running clickbait ads on an academic subdomain? reply craigkerstiens 18 hours agoparentI love the \"this guy\" piece... Meanwhile he&#x27;s been the most public person in academia talking about databases over at least the last 5 years, maybe the last 10. He&#x27;s not only done an awesome job of talking about foundational pieces of databases, but also examining new databases that have come up over the last 10 years or so. He&#x27;s course is quite open as well, so it&#x27;s not just the student base that gets to take advantage - https:&#x2F;&#x2F;15445.courses.cs.cmu.edu&#x2F;fall2023&#x2F;The shirts he&#x27;s generally helped promote and publicize those companies so he has things to hand out to his students, TAs, graduate students. reply bhickey 15 hours agorootparentBack in grad school he got put on probation a second time after hiring a magician off craigslist for admitted students day. The magician showed up drunk and lost his dove in the building. reply achileas 12 hours agorootparentWow I like Andy even more now reply bhickey 11 hours agorootparentHere&#x27;s his first trip to probation: https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~pavlo&#x2F;slides&#x2F;graffiti-dc401-oct12.pd... reply snapetom 13 hours agorootparentprevYes, the glory of social media. One of the most brilliant minds in databases, and he&#x27;s \"this guy.\" reply bigdict 15 hours agorootparentprevnot talking about the shirts reply Andoryuuta 17 hours agoparentprevThose ads you are seeing above the comments are from the comment service, Disqus. Unless you pay 12$&#x2F;month, the comment box iframe will inject ads.These are not ads directly added by \"this guy\", but rather by his choice of using Disqus for adding comments to the site. reply nlarew 18 hours agoparentprevSeems like everyone here is running an Adblocker! The post uses Disqus for comments and has the built-in (garbage) ads enabled.I think it&#x27;s likely that the OP uses an adblocker too and never even saw the ads on his own site. reply ydant 17 hours agorootparentGood point about the importance of checking your own page with your ad blockers and similar turned off.It&#x27;s something I imagine a lot of people would forget - I don&#x27;t think the thought ever occurred to me for a personal site.Part of having browsed the web with ad blockers turned on for so long is I tend to forget ads exist for the most part - I definitely forget how pervasive they are. reply quickthrower2 14 hours agoparentprevLol all the people with adblockers saying \"leave him alone there are no ads!\"For extra evil Discus should really pop up an ad blocker blocker. reply 123pie123 18 hours agoparentprevwhats clickbait about it?it looks like blog about how he likes his t-shirtsI can&#x27;t see any ads - although my firefox extentions will be blocking them if there was any reply KomoD 13 hours agorootparentThere&#x27;s ads. reply rafram 18 hours agoparentprevThere are no links to buy any of the shirts on the page. It&#x27;s a tongue-in-cheek comparison of startup swag. reply cbb330 17 hours agorootparentScroll to the bottom. reply lopkeny12ko 17 hours agorootparentAre you talking about the footnotes at the bottom? There are no links to buy the shirts there either... reply KomoD 13 hours agorootparentNo, the ads injected by Disqus reply siva7 17 hours agoparentprevHate to be that guy but you must be fun at a party. reply lai 17 hours agoprev [–] Does anyone know of a place to buy techie t-shirts made by next level apparel? reply thesh4d0w 15 hours agoparent [–] You can just buy the screenprinting blanks, unless you really want the logos. I personally get them from wordans.com. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author expresses their enthusiasm for wearing database shirts and ranks their top five favorites, highlighting the significance of using quality materials and designs.",
      "They recognize the role of shirts in promoting a company's brand and discuss their personal connection to some of the shirts.",
      "-"
    ],
    "commentSummary": [
      "The comment thread covers a range of topics related to t-shirts, such as favorite designs, preferred brands, materials, and styles.",
      "Controversies surrounding certain t-shirt names and the availability of larger sizes are also discussed.",
      "Mention is made of tech companies using t-shirts for promotional purposes, and the challenges faced in finding specific brands."
    ],
    "points": 223,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1701097668
  },
  {
    "id": 38439817,
    "title": "Popular Science Magazine Ends Print Edition After 151 Years",
    "originLink": "https://www.theverge.com/2023/11/27/23978042/popular-science-digital-magazine-discontinued",
    "originBody": "Science/ Tech/ Business After 151 years, Popular Science will no longer offer a magazine After 151 years, Popular Science will no longer offer a magazine / Popular Science magazine shifted to an all-digital format a couple of years ago, and now even that’s gone. By Emma Roth, a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Nov 27, 2023, 7:35 PM UTC| Share this story A November 1920 issue of Popular Science. Image: Popular Science via Wikimedia Commons After 151 years, Popular Science will no longer be available to purchase as a magazine. In a statement to The Verge, Cathy Hebert, the communications director for PopSci owner Recurrent Ventures, says the outlet needs to “evolve” beyond its magazine product, which published its first all-digital issue in 2021. PopSci, which covers a whole range of stories related to the fields of science, technology, and nature, published its first issue in 1872. Things have changed a lot over the years, with the magazine switching to a quarterly publication schedule in 2018 and doing away with the physical copies altogether after 2020. 1/5 The November 1921 issue of Popular Science. Image: Popular Science via Wikimedia Commons 1/5 The November 1921 issue of Popular Science. Image: Popular Science via Wikimedia Commons In a post on LinkedIn, former PopSci editor Purbita Saha commented on the magazine’s discontinuation, stating she’s “frustrated, incensed, and appalled that the owners shut down a pioneering publication that’s adapted to 151 years worth of changes in the space of a five-minute Zoom call.” Layoffs have impacted journalists on the science beat particularly hard in recent weeks. National Geographic cut the remainder of the magazine’s editorial staff in June, followed by Gizmodo laying off its last climate reporter, and CNBC shuttering its climate desk last week. “PopSci is a phenomenal brand, and as consumer trends shift it’s important we prioritize investment in new formats,” Herbert tells The Verge. “We believe that the content strategy has to evolve beyond the digital magazine product. A combination of its news team, along with commerce, video, and other initiatives, will produce content that naturally aligns with PopSci’s mission.” PopSci laid off several employees earlier this month, leaving around five editorial staff members In addition to dropping its magazine format, PopSci laid off several employees earlier this month, leaving around five editorial staff members and “a few” workers on the publication’s commerce team, according to Axios. The digital media group Recurrent Ventures acquired PopSci in 2021 and named its third CEO in three years just one week before the layoffs hit. PopSci will continue to offer articles on its website, along with its PopSci Plus subscription, which offers access to exclusive content and the magazine’s archive. However, its discontinuation marks the end of an era, and the other cuts across the science journalism field won’t make it easier to stay up to date on the state of our climate or dive into fascinating stories that you might not otherwise come across without the media outlets that bring them to our attention. Most Popular This company just put the air in Apple’s MacBook Air The best Cyber Monday deals you can still get After 151 years, Popular Science will no longer offer a magazine The best Cyber Monday deals for some of The Verge staff’s favorite stuff Digital car keys are here. Are we ready? Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=38439817",
    "commentBody": "After 151 years, Popular Science will no longer offer a magazineHacker NewspastloginAfter 151 years, Popular Science will no longer offer a magazine (theverge.com) 222 points by sohkamyung 10 hours ago| hidepastfavorite111 comments mindcrime 8 hours agoOn a related note... one of my favorite parts of magazines like Popular Science and Popular Mechanics was always the ads in the back of the magazine. You could always find all sorts of weird shit advertised back there. Anti-gravity mechanisms, Tesla coil kits, ultrasonic weapons, \"free energy\" stuff, plans to build your own log-splitter, forge, plasma cutter, whatever.And one of the top places selling some of the cooler stuff was an outfit called \"Information Unlimited\". They, sadly, went the way of the dodo recently themselves. Apparently due to the death of the owner. :-(https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38329274 reply Loughla 7 hours agoparentI bought plans to build a log splitter back in the day.It sucked until I upgraded it with a hydraulic cylinder from an old d4. Now it can split granite I think. reply HPsquared 7 hours agoparentprevAlso the ads on old VHS recordings.I don&#x27;t think many people save ads these days, I guess the equivalent is banner ads caught in screenshots, that kind of thing. reply Kye 1 hour agorootparentThen there&#x27;s the rare ad meme: https:&#x2F;&#x2F;knowyourmeme.com&#x2F;memes&#x2F;its-more-likely-than-you-thin... reply godzillabrennus 7 hours agorootparentprevI saved this ad when I was a kid: https:&#x2F;&#x2F;i.imgur.com&#x2F;npMzu3i.jpgLoved the slogan.Vintage tech ads can be fun. reply bcraven 7 hours agoparentprevA relevant podcast:https:&#x2F;&#x2F;slate.com&#x2F;podcasts&#x2F;decoder-ring&#x2F;2023&#x2F;08&#x2F;the-homemade... reply bbarnett 6 hours agoparentprevThis is really sad! When I was a kid, in the early 80s, I had their catalog, and even called them.Some guy talked to me, happily for 20 minutes about their products. I suspect it was the owner, seemed like a nice sort, and seemed to like spreading enthusiastic joy about science. reply Scottn1 2 hours agorootparentI agree, real sad.I too was a pre-teen in the early 80&#x27;s and I remember how neat it was to have these \"secret\" catalogs and&#x2F;or ads in the back of magazines like Popular Science where I could place some cash (or ask mom for a check while I gave her my cash) in an envelope, put it in the mail and in a few weeks I would have these cool sci-fi gadgets in my hand to play with. I was the only one in my friends group who really knew about this stuff apparently (or was the only one geeky enough to think it was cool.)It wasn&#x27;t just Tesla Coils either. Mail order was in it&#x27;s golden-years in the 80&#x27;s and early 90&#x27;s before the internet. I remember ordering everything from electronic components like caps&#x2F;breadboards, to magic&#x2F;card tricks, meters, oscilloscopes etc - all in the many different catalogs I had. I used to even order all kinds of fireworks for the 4th of July through the mail as well back then. Not just snakes and smoke-bombs either, but full on Black Cat (the best) firecrackers, bottle rockets and fountains. Literally send in the money and few weeks later I&#x27;d have them dropped off at my front door.Nowadays it is even more amazing as all this stuff is now available in a day or two with free shipping. Heck, I ordered two board games on Amazon the evening of the 19th for with the family the upcoming holiday week and the doorbell rang about 10a the next DAY with them!But nothing beats that feel of a pre-teen and his secret catalogs ordering with his own saved up cash in an envelope and anxiously waiting for two weeks for some off-the-wall gadget to arrive. reply underseacables 9 hours agoprevI was a subscriber for maybe 25 years, but the magazine got progressively light on content and heavy on ads. The articles became more like blurbs. This is sad to see but not unexpected for print media. reply casefields 8 hours agoparentWe should look at this as an old long term relationship where you only remember the good times. The magazine we all grew up reading isn’t the same one it was today. reply huytersd 9 hours agoparentprevIt’s sad because you know they were internally fighting that change but just had no choice, financially. reply Nifty3929 10 hours agoprevThis makes me sad, but also a long time coming. I remember reading them cover-to-cover as a kid several decades ago, but the quality went down, ads went up, etc. and I haven&#x27;t read one now in 10 years or more. reply Nifty3929 10 hours agoparentJust a couple of headlines I grabbed from their home page just now:\"Our favorite cannabis vaporizer is $90 off for Cyber Monday\"\"2024 BMW G 310 R review: A starter bike you won’t outgrow\"\"You can still get the best Apple products at the best prices … if you act fast\"\"20+ luxury items that are less pricey for Cyber Monday\"\"This is your last chance to save $100 on an Xbox Series X during Cyber Monday\"I really just think there is no consumer market left that would support well written, long-form science journalism. So the choice is either to close up shop, or act as a front for retail outlets. reply nerdponx 9 hours agorootparent> I really just think there is no consumer market left that would support well written, long-form science journalismNautilus is trying to be that. It&#x27;s honestly not very good writing or editing, but I still enjoy the experience of getting a science magazine in the mail every once in a while and skimming through it. reply jqcoffey 9 hours agorootparentprevScientific American is still in print. I guess I don’t know for how long, but I love it. reply paradox460 6 hours agorootparentSA has been on the decline for decades. It was fluff in the 2000s, its now somehow less than fluff. reply ars 9 hours agorootparentprevScientific American publishes nonsense in the guise of science. They are not a reliably publication anymore.https:&#x2F;&#x2F;michaelshermer.substack.com&#x2F;p&#x2F;scientific-american-go...https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=6202https:&#x2F;&#x2F;medium.com&#x2F;lessons-from-history&#x2F;the-shameful-decline... reply gorwell 6 hours agorootparentIronically captured by a religion. reply VHRanger 9 hours agorootparentprevOr fund with charity money, like Quanta reply Aeolun 9 hours agorootparentprevThis is quite sad. I’m wondering if I should blame mobile phones for all of this… reply atonse 9 hours agoprevI don’t know if this is a self selecting group on HN but I’d think a disproportionate amount of us read it growing up.Even the sections that talked about cool (and futuristic) stuff were great.This and Popular Mechanics, I’m not sure how we got them in our house in India growing up but I fondly remember both of them.Have we lost something? Where will future hackers get their inspiration from? Maybe a million different YouTube channels and proliferation of cheap ways to learn and tinker are fueling the next generation. Let’s hope so. reply nerdponx 9 hours agoparentI loved Pop Sci growing up, but in hindsight I felt like the content was already on the decline by the early 2000s. That, or it was never that great and I was just getting more aware as I got older.I remember that most of the things I read about never even came close to reality, and that was pretty disappointing. It seemed like they were more interested in writing speculative sci-fi than reporting on the current state of scientific research. It just wasn&#x27;t interesting. The only thing that really caught my attention was coverage of the X Prize and Spaceship One. That stuff was cool because it existed in real life, it wasn&#x27;t just a bunch of artist renderings about what the year 2030 might look like.Can&#x27;t speak for Popular Mechanics. I probably should have been reading that all along. reply gumby 8 hours agorootparent> I loved Pop Sci growing up, but in hindsight I felt like the content was already on the decline by the early 2000s. That, or it was never that great and I was just getting more aware as I got older.I remember realizing it was mostly bull and weak articles when I was about 13…around 1977. Didn’t stop me from reading it but I felt it was more SF speculation from some current science news rather than actual science. That was OK. reply mauvehaus 8 hours agorootparentTurns out sci-fi speculation is more marketable than sci-reality.I still remember thinking how cool it would be if the Lockheed Venturestar came to fruition. Like, Wow! Would that have been cool or what?Maybe it&#x27;s ok that it&#x27;s a lot of speculative stuff after all? Maybe there&#x27;s something to be said for reading about the wild-ass ventures that are high-risk and high reward. Incremental progress gets you a long ways, but the occasional moonshot can really move the needle. Watching the two Falcon Heavy boosters land simultaneously was the kind of thing you feel like you&#x27;d have read about 10 years previously in Popular Science reply jhbadger 7 hours agorootparentprevI seem to remember most all the articles about how airships are on the verge of becoming practical again -- in the 1970s, 1980s, 1990s, 2000s... Don&#x27;t get me wrong, I think airships are neat, it&#x27;s just after seeing this promise unfulfilled so many times, I just doubt that it will happen. reply Xcelerate 7 hours agorootparent> just after seeing this promise unfulfilled so many times, I just doubt that it will happen.Well buckle up:https:&#x2F;&#x2F;www.sfchronicle.com&#x2F;bayarea&#x2F;article&#x2F;worlds-largest-a... reply jhbadger 6 hours agorootparentNeat. Hopefully they won&#x27;t go bust right away like all the rest. This sort of article was exactly what was printed decade after decade -- company X has an airship prototype and will test it shortly but either they never get one aloft, or it gets damaged in a storm and they run out of funding to build another. reply jboy55 6 hours agorootparentprevI bought some from the 30s, 40s and 50s. Each magazine was 300 pages of Radio or TV repairman training courses, and a dozen pages with some fluff about the &#x27;future&#x27; or advertisement article about a new tool. reply kevstev 7 hours agorootparentprevMy wife worked there for a bit in the mid 2000s and I did and still do read just about anything I can get my hands on, but I found it really hard to get through an issue of pop sci. It was mostly optimistic fluff with the lightest of technical detail. And tbh if I wasn&#x27;t reading it, I can&#x27;t imagine who was- I imagine it was mostly subscribed to by dentists offices. reply Terr_ 8 hours agorootparentprevI remember excitedly reading their early articles on e-ink, but that only materialized in a very niche way decades later. reply JKCalhoun 8 hours agoparentprevPopular Science, Popular Mechanics — even just the covers of the magazines exuded a kind of optimism. It was going to be a brave new future ... you can learn to do this yourself ... you can understand it ... you can make this in your garage...Unfortunately the future kind of caught up with it.Or maybe as an adult when I became an engineer some of the mystique of tech evaporated when it became my job five days a week?But in the 70&#x27;s and 80&#x27;s when I was perhaps of a more impressionable age (or when maybe the future itself was more measurable) it fueled my curiosity and did impart a kind of optimism that will be missed. reply colechristensen 7 hours agorootparentI got the impression that the quality was on a long slow decline from initially nearly professional science quality articles to something not quite as bad as the modern discovery channel.Surely the mystique evaporates for “popular” publications targeted at people with less experience, but that just means you need to level up what you consume. Physics Today is something to look into today.The media i want to consume is mostly of the “this was written for people on the level of graduate students, but ones in a different field”. I can’t find a lot of it reply bart_spoon 6 hours agorootparentThis has been true for a lot of publications I used to like to read. Wired, Time, Sports Illustrated all are much, much worse today than I remember them being 20 years ago. I don’t know if that’s me or a genuine change in their quality. reply swells34 6 hours agorootparentThis is quite in line with what I remember as well (and have verified by looking through many old editions recently). I think this is just the outcome of modern business practices in the US. It&#x27;s a race to the bottom of quality, while making that bottom line top out. reply colechristensen 5 hours agorootparentEveryone takes the short term safe course, make a thing a little bit worse to make a little bit more money, don’t take risks with things that might be high quality that maybe only a few will appreciate.The result is a long slow decline which is profitable until you’re dying and can’t climb back out. This is, I am sure, an explicit strategy of some. The whole business model of taking something good and lowering quality while relying on the good will from the past until it’s all out and you’re bankrupt… except you’ve been doing whatever you could to extract profit from the whole process and just leaving the business bankrupt. replygeerlingguy 7 hours agoparentprevThere are a few magazines by (I think?) the publishing arm of Raspberry Pi: Hackspace and MagPi... they are a little pricey and mostly cover small electronics and programming-related projects, but they&#x27;re in a similar vein.I wish there were still some decent magazines like these I remember reading through in my youth, but one big challenge for the publishers is the \"maker\" &#x2F; \"mechanic\" these days is much more limited to hobbyists.In my Dad&#x27;s generation, you kinda had to have a certain level of mechanical ability if you ever wanted to own a house and car&#x2F;bike and not live in squalor.Nowadays, if you even have the money for a home, it seems like 99% of things are made to be replaced, and for anything that needs fixing, it&#x27;s impossible to fix it yourself without some pretty strong technical abilities (and a lot of time&#x2F;patience).There&#x27;s also a lot more emphasis on practicality, and less on sci-fi-type aspirational content in general. reply basch 7 hours agorootparentI can recommend BBC Science Focus as a half way decent replacement for Popular Science.https:&#x2F;&#x2F;www.sciencefocus.com&#x2F;magazinewatch for it on sale at https:&#x2F;&#x2F;www.discountmags.com&#x2F;magazine&#x2F;bbc-focusit’s also free with an Apple News or Bundle subscription. https:&#x2F;&#x2F;apple.news&#x2F;TUNNn0U24SRC5LF2bxqxDNA reply nyokodo 8 hours agoparentprev> Have we lost something? Where will future hackers get their inspiration from? Maybe a million different YouTube channels and proliferation of cheap ways to learn and tinker are fueling the next generation. Let’s hope so.I can watch rocket launches live streamed on a handheld device, then consume expert or very well informed layman analysis shortly afterwards. There’s probably 10 substacks for every scientific and engineering specialization written by experts. I can look up primary sources, even preprints, and aggregate a range of expert opinions on them. I can watch a million impeccably animated explainer videos on any topic. It’s harder to choose sources due to the plethora of choices, and the printed medium is tactile and nice, but in practically every other way now is vastly better! reply musicale 7 hours agorootparent> It’s harder to choose sources due to the plethora of choicesThat&#x27;s sort of why print magazines are&#x2F;were helpful and approachable - they&#x27;re an easy entry point with curation, discoverability, and an editorial staff. They&#x27;re also somewhat self-contained rather than being an endless rabbit hole of attention-grabbing links (those generally being limited to the cover.)Arguably the best thing about HN and good aggregation blogs is that they provide curation and discoverability as well as some degree of editorial commentary. And hopefully they&#x27;re not managed by attention-maximizing algorithms.The abundance of options and content - even if you were to limit yourself to a specific category, such as peer-reviewed articles in a particular field, or a particular medium, such as youtube videos - from online ources can be completely overwhelming. reply jvanderbot 9 hours agoparentprevI longed for the days of snipping programs from magazines, or reading how deep internals were implemented in some computer or technology. My whole life I felt like I was born to late to be part of a hacker movement.Now I have GPT-4 and finally my curiosity can be satisfied quickly. The world got so small again. I feel like I can _build_ again. reply ericskiff 7 hours agorootparentThis sums up my optimism about AI.I finally have a patient, infallible teacher without a direct agenda to game SEO to sell ads or a product.It feels like the earliest days of the web when I consumed every page of howstuffworks.com and surfed webrings out of curiosity. reply pwg 8 hours agoparentprev> I don’t know if this is a self selecting group on HN but I’d think a disproportionate amount of us read it growing up.I am a member of that group. This was one of my favorite magazines growing up. And I do have to say I felt a twinge of sadness to see this article on HN just now. reply nytesky 7 hours agorootparentI remember reading over breakfast cereal in the morning, and I won a quiz bowl competition because I had read about buckminster fullerene (buckyball form of carbon). reply gjsman-1000 8 hours agorootparentprevCount me here. I received a subscription to them and Popular Mechanics in 2010 for my birthday. I’m still subscribed to PopMech but the writing is on the wall with the bimonthly releases. reply sneak 9 hours agoparentprevMore people are reading more things more often than ever before.Indie hackers (as well as anyone else) can now cheaply publish their own magazines to the globe.I don’t think we lost something other than paper and postage.Pretty soon (in the scheme of things) they’ll stop printing books and paper money too.(I say this as a vinyl and book collector. It’s just facts. Our great grandchildren will not use physical books.) reply renegade-otter 9 hours agorootparentI am reading The Shallows, and I am not so sure. Having replaced my paper books with iPad Kindle versions years ago, I am seriously thinking of going back to paper.https:&#x2F;&#x2F;www.amazon.com&#x2F;Shallows-What-Internet-Doing-Brains&#x2F;d... reply sneak 7 hours agorootparentI read books off of an SD card inserted into an e-ink device that has never had the wifi turned on.The problems with internet overuse are orthogonal. reply nerdponx 9 hours agorootparentprevI don&#x27;t think it will disappear entirely. Just like vinyl records, cassette tapes, and CDs, some people deliberately choose to buy dead tree media for one reason or another. Maybe print magazines will go first, and maybe print newspapers at some point in the future, but actual books will probably never go away. reply HomeDeLaPot 9 hours agorootparentYes. Books might go the same way the horse has gone. No longer dominant, but still occupying niches. reply sneak 7 hours agorootparentprevVinyl records, cassette tapes, and CDs will also be unused in 100 years. Cassettes and VHS will by then be unreadable, just as most floppies are now. Hell, in 100 years, there won’t even be the mp3 codec anymore.You don’t need to tell me that people deliberately choose dead trees; I am one of them: I collect books and have more than most collectors. Same goes for vinyl.The writing is on the wall, however. In 100 years it won’t be a thing, just as fountain pens and wax seals aren’t today.Yes, they will still exist, and yes, people into whatever type of historical reenactment that is will still have them, but there will be no industry around them like there is now. reply mfragin 7 hours agorootparentYeah I understand your point, but vinyl is kinda sacred.I mean, of all the things you mentioned, it&#x27;s by far the most likely to still be usable in the future. Even in a post-apocalyptic world people might figure out how to play vinyl. MP3s? Not a chance.Full disclosure: I never liked cassettes. Oh, and I made the mistake of choosing Beta over VHS.... reply bawolff 9 hours agorootparentprev> Pretty soon (in the scheme of things) they’ll stop printing books and paper money too.That seems unlikely to me. E-books have really terrible UX. Paper books aren&#x27;t going anywhere until that is fixed. reply dumbfounder 8 hours agorootparentI can&#x27;t carry a library of 1000 physical books in my pocket and pull them out while on the toilet. I think you have it backward. reply hattmall 7 hours agorootparentBut that&#x27;s not really how books work though. I don&#x27;t ever really need to read more than 1 book at a time. I&#x27;m all for e-readers, have a couple and use them, but it&#x27;s a cost saving measure. I would vastly prefer physical books for the interface. reply dumbfounder 6 hours agorootparentWow, a lot of people taking me very literally. Most people take their phones everywhere, giving them instant access to just about every book ever created. You don’t need a light to read. You just need that thing you already have in your pocket. reply bawolff 5 hours agorootparentThat&#x27;s nice and all, and in a pinch maybe convinent, but most of the time im not looking to read in random places.I appreciate that might not be true for everyone, use cases vary. For me its essentially being better at a feature i dont use. reply tzs 6 hours agorootparentprevMy library is across the hall from my toilet, so I can simply get one of my couple thousand physical books on the way to the toilet. reply dumbfounder 6 hours agorootparentYou only poop at home? reply tzs 5 hours agorootparentActually, since my employer switched to work from home about 5 years ago I don&#x27;t think I&#x27;ve pooped elsewhere. reply bawolff 5 hours agorootparentprevYou only read when you poop? reply bigstrat2003 8 hours agorootparentprevThat&#x27;s not exactly useful. The ability to read a book without any electrical charge, however, is. reply dumbfounder 6 hours agorootparentBeing able to pull out any book, anywhere, any time, is not useful? What happens at night in your electricityless world, are you reading by candlelight? Or you can just light the page you just read on fire, that way your book is also your fuel. reply nl 5 hours agorootparentprevAs someone who reads quickly a lot on planes (and sees a lot of others doing the same) the ability to have multiple books on a Kindle is much more important than lack of access to electricity for a few days.(I also have a large physical book library.) reply willywanker 5 hours agorootparentpreve-readers can go for days to weeks on a full charge depending on how often you use them. Definitely much longer than the average smartphone. reply nl 5 hours agorootparentprevWhat don&#x27;t you like about the e-book UX? I used to think this but after using them for a few years I now prefer all aspects except two:* Can&#x27;t flick between two points in non-fiction books easily* Can&#x27;t show them off on a bookshelf reply bawolff 5 hours agorootparentPhones give eyestrain after long use. E-ink is much better but not as good as paper.In a way i can&#x27;t really explain, paper books are more relaxing to read. reply nl 4 hours agorootparentI actually prefer e-ink to paper because I can adjust the type size and layout.I have mixed feelings about tablets. I do like that I can have a nightmode colorscheme for night reading. I don&#x27;t think reading on a phone is a great experience generally. reply tourmalinetaco 8 hours agorootparentprevOr until the book publishers condition the market enough to accept expensive DRM-filled ebooks with no physical copies, due to better margins and no secondary market as an alternative. Similar to the video game industry, which used to provide physical copies that you had full control over and now is moving more and more of the user base into digital stores for their “convenience”. reply lacrimacida 6 hours agorootparentHackers and pirates will be around one way or another. reply echelon 9 hours agorootparentprev> I don’t think we lost something other than paper and postage.Popular Science was nostalgic. It was a callback to an earlier time, and it tracked within its pages the series of changes led to today. It&#x27;s like losing a species to extinction, albeit not quite as drastic or irrecoverable.I remember the magazine fondly. Once I got into my later teens, it felt a little too general audience for my taste. But it always made me wonder about the possibilities of science and engineering. Jetpack futures, cloud cities, and millennium ships.> Indie hackers (as well as anyone else) can now cheaply publish their own magazines to the globe.It won&#x27;t be long and even blogs and news websites will see the margin set to zero. LLMs will ingest and rephrase (with accuracy) all kinds of content. There will be no money in running CNN or the New York Times.> Our great grandchildren will not use physical books.It&#x27;s sad to think that the libraries will get garbage collected, but the books they store will be little more than anachronisms. Perhaps they can be turned into public internet cafes. reply JackFr 6 hours agorootparentI had an argument with my wife about funding our public libraries. (We live in NYC which I believe supports 3 out of the top 10 public library systems in the country. Brooklyn, Queens and Manhattan maintain separate library systems.)I am happy for my tax dollars to support the system, but the bottom line is that branch libraries exist as a free, under-resourced Internet cafe for the indigent. That might be a good use for the public fisc, but the librarians and books are completely ignored.We have three children the youngest of whom just finished high school and we were forced to admit none of us has been in our neighborhood branch in more than 8 years.At the same time I haves seen two exhibits at the main reading room in the past 18 months. reply lupusreal 10 hours agoprevGoogle Books has complete editions of Popular Science available for free from 1872 to 2010:https:&#x2F;&#x2F;books.google.com&#x2F;books&#x2F;about&#x2F;Popular_Science.html?id...https:&#x2F;&#x2F;www.popsci.com&#x2F;diy&#x2F;use-popsci-google-books-archive&#x2F; reply JKCalhoun 8 hours agoparentI wish they were accessible offline as PDFs. reply abdusco 43 minutes agorootparentLibrary Genesis has almost all releases available. reply android42 10 hours agoprev> After 151 years ... needs to “evolve” beyond itsI can&#x27;t be the only one who noticed \"151\" and it definitely sounds like the author was alluding to it (pokemon) as well. Coincidentally Popsci and Nintendo Power were the only magazines I ever read and owned physical copies of. reply HumblyTossed 7 hours agoparentI don’t get the reference. reply android42 6 hours agorootparentpokemon 1st gen had 151 pokemon, making it a pretty recognizable number for pokemon fans, and nothing else of comparable notoriety for 151, and the word choice of \"evolve\" plus the added emphasis of the quotes in the article, I admit it was subtle and may not have been intended, but my mind instantly made that connection, but on second thought I guess they could have also simply intended \"evolve\" as a subtle science pun as well reply tkanarsky 10 hours agoprevPopSci was how I learned to read English as a kid in the early 2000s. RIP to an old friendIn addition to dropping its magazine format, PopSci laid off several employees earlier this month, leaving around five editorial staff members and “a few” workers on the publication’s commerce team, according to Axios.Does this mean they no longer have any writers? Or are they saying that the editorial staff has been cut to five? Do they just oversee freelance journalists? reply MR4D 5 hours agoprev> PopSci will continue to offer articles on its website, along with its PopSci Plus subscription, which offers access to exclusive content and the magazine’s archive.So they’ll still publish, just not in “magazine format”. Freakin clickbait. God I hate the Verge. reply JWLong 8 hours agoprevI ran to the mailbox for these. Sad day. And yet, as others have said, “It felt inevitable.”Popular Science was never the bastion of journalistic integrity, and yet this leaves me worried about print publishing in general. If a periodical can’t make it, what happens to print news? reply rjh29 7 hours agoparentPrint news is only a matter of time, right? The US daily newspaper circulation declined 30% between 2016 and 2020.UK newspapers are dropping 10-20% in circulation in 2023 -alone-. Most of our magazines are already dead or extremely low quality.Japan is doing well though. The Yomiuri Shimbun is published twice daily and has a morning circulation of 7 million. Magazines and periodicals are very much alive, including 500+ page manga magazines and the weekly Famitsu gaming magazine. reply pyrophane 8 hours agoprevAt least National Geographic, Wired, and 2600 all still have print editions. reply sourcecodeplz 7 hours agoprevOnce the Internet came I didn’t really read anywhere else but there. reply interroboink 10 hours agoprevEnd of an era. Though at the same time, I guess I&#x27;m surprised it didn&#x27;t happen sooner.I think the covers of Popular Science were the main thing I remember about it. It seems only a couple were by Norman Rockwell, but a lot of them had a similar vibe, which I enjoy. reply jamestimmins 9 hours agoprevBetween this and today&#x27;s news about Sports Illustrated using AI-generated authors, today marks an unexpected decline in two the defining magazines of my childhood! reply dottjt 10 hours agoprevI remember I used to fervently read this in the library in high school (~2007). Haven&#x27;t read it since lol. reply HumblyTossed 7 hours agoprevSigh. I’m getting old. Everything I once loved is dying. reply pico303 6 hours agoparentI agree. Started subscribing to magazines again because trying to read articles online is such a nightmare. reply liquidpele 6 hours agoparentprevIt’s the ciiiiiiiircle of liiiiife…We lost magazines, and gained YouTube channels that are WAY better. reply nimish 8 hours agoprevWhere will I get my crank science ads on the H-bond angle of water? reply marifi 8 hours agoprevAs someone who never had a subscription, but would always thumb through when given the opportunity, that&#x27;s a bummer. reply benrapscallion 7 hours agoprevOnly those who were paying for a print subscription have the right to protest this move. And the shuttering of other newspapers and magazines. reply zombiwoof 7 hours agoprevwithout Popular Science Popular Mechanics , I&#x27;m not sure my childhood would have even existed reply xvector 9 hours agoprevEvery time I got this magazine as a child it was like Christmas! reply Covzire 10 hours agoprevWas the only magazine subscription my Dad had growing up, always loved paging through it as a kid.RIP. reply lasc4r 10 hours agoparentNot sure if we ever had a subscription but I read them a lot as a kid. Along with popular mechanics. In a waiting room nothing even comes fucking close. Or came close. RIP. reply paulpauper 10 hours agoprevAbout time. I don&#x27;t mean this negatively, but I mean it was inevitable. this is seen elsewhere too. either magazines going away or shrinking to pamphlets. putting content online is much more scalable, cheaper, faster, etc. reply JohnFen 10 hours agoparentThey moved entirely online a couple of years ago. Now they&#x27;re just shutting up shop entirely.> putting content online is much more scalable, cheaper, faster, etc.But it&#x27;s a much worse format (in my opinion) for reading something like PS. When they moved online, I stopped reading them because the media itself removed too much value for me. reply massysett 10 hours agorootparentYeah there is definitely value in the stapled sheaf of pages. I still read The Economist this way. It’s nice having a contained space that says: in the judgement of the editors, this is the most important stuff for the time period this issue covers. Online is a constant dribble of stuff—no snapshots.And putting magazines online in PDF is just miserable. The Economist does a not bad job of putting magazines in the app in readable format. reply Aeolun 9 hours agorootparentI think that’s the whole value of magazines right? You pay them to curate for you.If the only thing they curate is print ads, then all the value is already gone. reply 15457345234 6 hours agorootparentprevThey&#x27;re still self-owned and haven&#x27;t fallen into the grasp of toxic &#x27;brand management&#x27; companies. reply justinzollars 10 hours agoprev [–] Maybe some rich VCs should buy this thing, and convert it into a tech publication that is pro tech. reply eropple 9 hours agoparentIf \"tech\", as you define it here, was doing things that were good for people, would a party-apparatus publication be necessary?This is obvious, of course, but rarely do socially valuable innovations require captive media to substantiate them to the body public. When the body public actually wants to hear about them, there&#x27;s money to be made by educating them.Real \"are we the baddies\" energy here. reply h2odragon 9 hours agorootparentAlso, \"Wired\" has been filling that slot reply manicennui 9 hours agoparentprevAnd fill it with even more ads and clickbait articles. reply sadhorse 10 hours agoparentprevVC tend to invest in things going the opposite way. reply excitom 9 hours agoparentprevIt will be interesting to see if the revival of the printed Rock & Roll magazine Creem https:&#x2F;&#x2F;www.creem.com will find enough Boomer nostalgia to succeed. If so, then maybe PopSci too. Not holding my breath, however. reply qudat 9 hours agoparentprev [–] HN eats that lunch replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Popular Science magazine, after 151 years of publication, has ceased its print edition and transitioned to an all-digital format.",
      "The owner of the magazine, Recurrent Ventures, has stated that the organization needs to expand beyond the magazine and prioritize other ventures such as news, commerce, and video content.",
      "The discontinuation of the magazine has resulted in layoffs of several employees and highlights the difficulties faced by science journalism in today's media industry."
    ],
    "commentSummary": [
      "Popular Science magazine, after 151 years, has decided to discontinue its print format and focus solely on publishing articles on its website.",
      "This decision has sparked conversations concerning the decline of print media and the growing preference for online content.",
      "People have different views on this shift, with some expressing nostalgia for the unique advertisements found in magazines, while others argue that digital formats are more convenient.",
      "There is skepticism regarding the decline in the quality of popular publications and concerns about the future of physical books.",
      "While some believe that vinyl records and physical books will still hold value, others predict that these formats will eventually become obsolete.",
      "The discussions also touch on the potential transformation of libraries and the significance of curated platforms in navigating the abundance of online content.",
      "Overall, opinions vary, with a mixture of optimism and doubt regarding technological progress, the future of print media, and the potential revival of certain magazines."
    ],
    "points": 222,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1701126364
  }
]
