[
  {
    "id": 38826283,
    "title": "Happy New Year Hacker News: The Only Website Worth Visiting Multiple Times",
    "originLink": "https://news.ycombinator.com/item?id=38826283",
    "originBody": "I spend too much time on HN. But of all the places on the internet, this is the only place which feels worth visiting multiple times a day!Wishing everyone a great 2024!",
    "commentLink": "https://news.ycombinator.com/item?id=38826283",
    "commentBody": "Happy New Year HN!Hacker NewspastloginHappy New Year HN! 1267 points by thunderbong 15 hours ago| hidepastfavorite246 comments I spend too much time on HN. But of all the places on the internet, this is the only place which feels worth visiting multiple times a day!Wishing everyone a great 2024! naitgacem 14 hours agoI used to have friends in uni with whom i could share very nerdy&#x2F;technical conversations. near graduation we no longer get together sadly. this has taken a toll on me. HN is a place that feels like home, even if I&#x27;m usually just lurking around and overhearing conversations. Thanks everyone for sharing! reply AlbertCory 12 hours agoparentI think sometimes you have to realize that it&#x27;s OKAY if you&#x27;re always the one to keep in touch, and no one else seems to make any effort.As long as they&#x27;re happy to talk when you call, or they show up at the group event when you badger them enough: that&#x27;s fine. Some people are just not initiators, and it&#x27;s either you make the effort, or you lose touch with them. You can&#x27;t insist on reciprocity. reply taylorius 8 hours agorootparentIs it OK? I&#x27;m genuinely not sure. I have been considering a new year&#x27;s resolution not to keep propping up relationship that aren&#x27;t reciprocated. Are they \"not initiators\" in all relationships, or just with you? Why do you conclude that it&#x27;s ok (genuinely interested)? reply kibwen 7 hours agorootparentIt&#x27;s entirely natural that some people become \"initiators\" as a relationship develops. At the beginning of a relationship, person A will initiate X% of the time, and person B will initiate (100-X)% of the time. Unless X is exactly 50 (unlikely), this means one person will naturally initiate more than the other. And then, over time, the person who initiates less will realize that the other person tends to initiate, and will come to expect it.Notably, that doesn&#x27;t mean that the person who initiates less doesn&#x27;t value your company! (Of course, it also doesn&#x27;t mean that they do value you, only that the frequency of initiation is not a good proxy for the health of the relationship.) reply behnamoh 7 hours agorootparentprevI&#x27;m with you. What I realized is when you take the lead and throw a party or happy hour at a bar or something, your social status increases as well. Most people are followers, few are leaders. If you feel lonely because you don&#x27;t get invited to parties or events, it might be that they don&#x27;t think of you as a fellow-follower. It could be that you&#x27;re actually the leader type and could benefit from that. reply mastazi 5 hours agorootparentprevI am one of those people that could be described as \"not intiators\". I genuinely appreciate when people get in touch. I am sorry that I&#x27;m almost never the intiator, it&#x27;s a trait I unfortunately have and I&#x27;m working on getting better. reply prashantsengar 4 hours agorootparentI used to be the one who would never initiate and would feel sad when a relationship just drifted away.Now I am the initiator most of the times. reply AlbertCory 7 hours agorootparentprevIt CAN be ok. Or maybe \"never initiating\" is a sign that they just don&#x27;t care. I&#x27;ve had it shake out both ways. reply qp11 5 hours agorootparentprevCuz the Buddha said its the path to nirvana. Disconnection breaks you out of the cycle of suffering.In today&#x27;s info overload overwhelming reality, its natural that lot of people are going to disconnect. reply andirk 9 hours agorootparentprevThat&#x27;s me. I&#x27;ve been my friends&#x27; glue to some extent since we moved away from high school. I like to think they appreciate it, as I appreciate when they reach out. Happy New Year all HN! reply BlackjackCF 7 hours agorootparentAs the non-glue friend - YES, we do appreciate it, so so much. reply sidlls 2 hours agorootparentprevNope. If someone doesn’t reciprocate that’s an indication of their level of interest. This is true for friendships and romance. If the other party is always a passive recipient of attention, they aren’t interested in the relationship. It’s pointless to invest one’s time and emotions into such a relationship. reply luqtas 9 hours agorootparentprevhave you checked on the veracity of this type of contact? last time for me it was a good time with an old one; but continuing would be just nostalgic bazinga...anyway i&#x27;m surviving and happy new year [+2 days] nerds!!!!!!!11 reply AlbertCory 9 hours agorootparent> the veracity of this type of contactyou mean, are they really good friends or not? One is always reassessing that, I guess. replypasswordoops 14 hours agoparentprevSame happened to me. We drifted apart, live in different cities. But nearly 20 years later, with life settled and kids grown we&#x27;re rekindling via messaging. I hope firstly, you get your group back, and second it doesn&#x27;t take 20 years reply quickthrower2 45 minutes agoparentprevI am jealous, as people at my uni seemed to be mostly there for the cheap alcohol and parties and probably to avoid working. I imagined uni would have watercooler chats about interesting stuff. But the incentives aren’t aligned. For that you need niche programming language meetups! Or maybe PhD but I decided against that. reply mentos 7 hours agoparentprevThe best 6 months of my life was studying abroad at a foreign university where I would stay up late with 30 other class mates in a lab solving comp sci course problems and then playing quake over lan til midnight.Nothing will ever replace that magic time but HN comes close. Thank you guys happy new year HN! reply toomuchtodo 13 hours agoparentprev> HN is a place that feels like homeIndeed, the only third place I feel worth participating in (for me of course, ymmv) and enjoy. Appreciate y’all. Thank you dang for all of the hard work. reply koliber 12 hours agoparentprevIf you’re ever in need to geek out, hit up someone who posted something interesting and ask them if they’re up for chatting. It’s not the most comfortable thing to do, but I’ve had a some great conversations with random people from HN. You’d be surprised how often it works out.Btw, open invite. Contact info in profile. reply op00to 12 hours agoparentprevI had a solid email thread with friends from college going for these types of conversations for about 10 years after graduation, but with kids and moving around that all ended. reply waltbosz 4 hours agoparentprevI too miss having friends to nerd out with. reply erikaww 14 hours agoparentprevwhy don&#x27;t you start something where you live? reply ethbr1 14 hours agorootparentParent should if they can! But there&#x27;s also a big difference in intentionality between meeting someone on the quad &#x2F; outside your room and starting a meetup with friends (or new friends!) scattered across town.Would also suggest looking for social clubs. I&#x27;ve found the \"general social hangouts + minimal focus on a shared interest\" are great for people time.Avoids the monomaniacal over-focus of a single-interest activity, while still providing a bridge with random strangers (\"You like thing? I like thing!\"). reply apwell23 13 hours agorootparentprevsocial anxiety. reply naitgacem 14 hours agorootparentprev> somethingcan you elaborate? reply mooreds 14 hours agorootparentMonthly lunches?Meet up to play a tabletop game?Go for a hike?Have a group receive a tech presentation once a quarter?Lots of options, most just take your time and emails&#x2F;contacting possibly interested folks. reply naitgacem 14 hours agorootparentI think the biggest issue here is \"possibly interested folks\". although I&#x27;ll keep that as a food for thought. reply mooreds 13 hours agorootparentYes, that&#x27;s the crux, for sure. Meetup or local slacks&#x2F;discords are where I&#x27;d start looking. You could also email current or former coworkers, if you have that info. reply baz00 14 hours agorootparentprevIf it&#x27;s like around here (London, UK) the last thing people want to talk about is the day job! reply zer00eyz 13 hours agorootparentThe Bay Area in the early 2000&#x27;s and even into the early 2000-teens was NOT like this.You could go out to lunch and have a technical chat and the people at the table next to you might chime in with a solution!When you get enough passionate people and pack them into one location things get interesting for them (networking, friendships etc)... Much of the passion is gone (lots of people see tech as a path to a paycheck), and everyone wants to WFH. reply mooreds 12 hours agorootparentThere&#x27;s still good stuff going on at local meet ups. I have experienced it myself.Honest truth is there were always folks without passion in tech. reply baz00 12 hours agorootparentMeetups are bloody amazing. I keep meeting middle aged divorced women there with loose sense of morals and a drinking problem, which is perfect :) reply baz00 12 hours agorootparentprevThat&#x27;s because it&#x27;s not a good vision for the future, it&#x27;s literally either building shit that everyone knows doesn&#x27;t matter or plugging away at pointless automation on overcomplicated piles of steaming crap. Or the next fad.People have lost the vision and do not understand the soul of the machine is to improve our state of existence not enslave us further.Fuck &#x27;em. I&#x27;m taking the money and doing what makes me feel good (wine, floozies and travel). reply wsintra2022 9 hours agorootparentAh good ol’floozies reply d0mine 12 hours agorootparentprevI can&#x27;t talk with my colleagues exactly because I&#x27;d like to talk about tech 24&#x2F;7. HN helps. reply jll29 11 hours agorootparentHow about starting a reading group, say a Linux kernel and device driver source code reading group? ;) reply seabass-labrax 9 hours agorootparentI&#x27;d join - not joking either! I&#x27;m willing to bet that most technically-minded people have devices that act strangely, and yet are powerless to fix them due to, say, the difficulty of getting GDB to connect via the non-existent serial port to capture the stack trace of a once-a-week glitch. I&#x27;m in that category with a few of my devices right now, so a group where people could explain device drivers to each other would be really interesting to me. reply baz00 12 hours agorootparentprevBeen there. Wears off when you get to your early 40s. reply eatonphil 14 hours agorootparentprevI&#x27;ve run a few book clubs online and in NYC this last year. And a Discord for folks in the area in systems programming. And a systems programming coffee meetup in NYC. Stuff like that is what I might guess Erik means! :) reply datadrivenangel 14 hours agorootparentprevTech Meetup groups! reply behnamoh 7 hours agoparentprevhonestly that has made me double think the definition of friendship. they say you know people in hardships. well, I got to know some of my supposed friends when they went on the job market this year and were acting like jerks. after they got jobs, they turned back to normal. but I don&#x27;t think of them the same as before anymore. reply theusus 13 hours agoparentprevOn the same page reply stakent 3 minutes agoprevIt turns out time flies fast. Over 14 years on HN.Being there I&#x27;ve been exposed to some life changing things.Thanks!All the best in 2024! reply klelatti 14 hours agoprevAnd huge thanks to dang for all his work over the year! reply sillysaurusx 3 hours agoparentAnd to the rest of the moderation team. One of the downsides of the job is that they don’t get much recognition. It’s a massive workload too, at least for most people. reply bookofjoe 14 hours agoparentprevI second that emotion. reply tomcam 13 hours agorootparentHeartily thirded! reply sjfjsjdjwvwvc 12 hours agorootparentFourthed from free feelings reply giantg2 10 hours agorootparentFifthed from the bottom of my fifth. reply chanana 9 hours agorootparentsixth&#x27;ed reply seabass-labrax 9 hours agorootparentSeventhed - I&#x27;ve written to HN&#x27;s email address and received a personal response from Daniel the same day. You could have the greatest, most expensive commercial support contract and it wouldn&#x27;t get you a Daniel! reply tomcam 8 hours agorootparentUniversally true AFAIK. He has never failed to respond to me within minutes. Baffling. reply 7steps2much 7 hours agorootparentHonestly, dang is what makes this place so great! One more thumbs up from me! reply behnamoh 7 hours agorootparentprevEighthed - @dang is so responsive with emails too. reply skydhash 4 hours agorootparentNinthed. HN has been my most loved place on the internet and a catalyst to learn so many things about tech. reply giantg2 8 hours agorootparentprevI was hoping you&#x27;d continue with a drinking play on words regarding a sixtel of beer. replyartiscode 14 hours agoprevHappy New Year everyone! HN will always remain in my heart and mind. 5 years ago I moved to Amsterdam to work on a super interesting R&D project that taught me a lot about GPS, coordinate systems, algorithms, and sadly the importance of having a short commute. I spent an hour and a half to get in either direction. That was demotivating and made me depressed and tired. HN was how I passed time, first on the train, then on the bus, reading curated articles and through thoughtful comments. I couldn&#x27;t have managed without you all. Once again, I wish you all a Happy New Year and luck in all your endeavours! reply mooreds 13 hours agoparentAre you still doing that commute? reply artiscode 13 hours agorootparentNo. I got homesick after a year and a half and moved back. I&#x27;ve been working remotely ever since with no commute, which I find awesome! reply einpoklum 10 hours agorootparentSorry if this is a nosy question, but - I lived in Amsterdam; and needing that long of a commute sounds weird, since public transport is pretty good, and you can bike some of the way. Even farther from the center, where it&#x27;s cheaper, it should still not be _that_ long... can I ask what was your commute route? reply bruce343434 10 hours agorootparentIt might also depend on mental model. Some people don&#x27;t count the actual door-to-door time. They just look at the time the train takes between stations. reply artiscode 2 hours agorootparentprevOur office was in Amsterdam Oost and I lived in Almere Filmwijk. An hour and a half is the average door-to-door time. Sometimes odds played in my favour and I could get home in 1h10min. The commute started by taking the bus or cycling from Filmwijk to Alemere Centrum, which took roughly the same amount of time. Then I would take IC or Sprinter to Amsterdam Muiderpoort, take another bus or walk, which, again, took relatively the same amount of time. It was impossible to rent anything in Amsterdam itself with an academia salary of 45k, a non-working wife and two children. I mostly took the bike on days when the weather allowed, but my bike was in Almere. I purchased a cheap(stolen) bike in Amsterdam for 75 euros, but it got stolen the same day I left it at the station.TL;DR Yes, you can live right next to the office if the funds allow it. My budget for rent was 1200 euros per month, which makes renting within the ring almost impossible.edit: spelling reply turquoisevar 5 hours agorootparentprevProbably working elsewhere in the Netherlands (e.g., Rotterdam, Utrecht, Arnhem, etc.) but landing in Amsterdam because that’s the only city in the Netherlands that rings a bell to most. reply artiscode 2 hours agorootparentHelaas pindakaas, the other way around. I lived in unbeknownst Almere and commuted to Amsterdam and back. replyOnewildgamer 5 hours agoprevWith all the turmoil in reddit, twitter and other social media, me and all of us here are so grateful there exists a forum which goes beyond the usual small talk, beyond known circles, with sincere and original thoughts no where else to be found on earth. A big thank you to all the mods and a very happy new year to everyone!Cheers reply SnowingXIV 14 hours agoprevHappy new year! Likewise, this is generally the only remaining site that I feel better after I leave it and more informed. No ads, excellent moderation, useful discussion and links. No gamification and chasing the modern web. Years of lurking and then years of sparse posting it still feels the same.Here is to hoping this doesn’t change. Keep it up dang. reply danieldk 14 hours agoparentHappy New Year to you too!Likewise, this is generally the only remaining site that I feel better after I leave it and more informed.Hacker News, but also lobster.rs reply susiecambria 7 hours agoprevHN is an interesting place for a non-techie to hang. . . I read about a lot of things which are not found in my world (quilting and public policy advocacy on human services&#x2F;rural issues) and I am thrilled each and every time one of you shows such interest in something I&#x27;m more familiar with. And your joy!Thank you all, thank you HN, for being so welcoming and a genuinely fun and safe place to spend time. reply cableshaft 2 hours agoparentWhat&#x27;s something cool going on in the world of quilting right now? I get a gift card at a quilting store for a family member every year but not too familiar otherwise. The store near me looks like a cool space, with lots of room to work on quilts in it. Kind of like a makerspace, a little. reply defend 14 hours agoprevTime to ship 2024 to production. Happy new year, fellow hackers. May your code always work on the first try and never regress. reply datadrivenangel 14 hours agoparentMidnight deploys are such a pain, why can&#x27;t we deploy the new year during business hours? reply quickthrower2 42 minutes agorootparentAt least we stagger the transition across world time zones. reply ethbr1 14 hours agorootparentprevWe tried delaying new year deployment in 2020, 2021, and 2022... it didn&#x27;t go so well.So back to the tried and true pipeline! reply layer8 14 hours agoparentprevIt feels more like an eternal beta to be honest. reply fsniper 10 hours agoparentprevIt&#x27;s not a great idea pudding to production at this time of year! Particularly when there are no requirements, unit tests or functional tests available!?! It&#x27;s &#x2F;dev&#x2F;urandom all around reply tiberius_p 13 hours agoprevHN is my goto place whenever I feel bored, sad, lonely, without purpose, lacking inspiration etc. It&#x27;s amazing how a single website with such a minimalistic design can condense so much quality content and quality people in a single place for the whole world to enjoy free of charge. Thanks and Happy New Year! reply yaa_minu 44 minutes agoprevHappy New Year! Just as many people here have said already, HN is really one of the few places online where one can find thoughtful discourse devoid of flame baits. Many a times, I read the comments before even following the links if at all. I&#x27;ve learned so much through HN. I would say that not only technical knowledge but also my English has improved massively all thanks to this forum. reply tus666 14 hours agoprevSo will this be the year of the Linux desktop? reply diggan 14 hours agoparentI&#x27;m fairly confident that yes, this will be the year of the Linux desktop:2024 = 202420 x 3 + 16 = 76, which is the ASCII value of &#x27;L&#x27;.24 x 3 + 12 = 84, which is the ASCII value of &#x27;T&#x27;.= LT = Linux Torvalds reply abhinavstarts 7 hours agorootparentIt&#x27;s fun I know. I want to know why you multiplied them with 3 and then added by 16 & 12 respectively. Any relation reply zdc1 6 hours agorootparentI assume its because using 16 & 15 hasn&#x27;t worked since 2023, and likewise only 16 & 9 will work for 2025 reply seanthemon 14 hours agorootparentprevI&#x27;m so thankful for the mathematicians of HN! Hear hear! Year of Linux! reply quickthrower2 39 minutes agorootparentAlways has been (the linux desktop is the terminal!) reply c0balt 8 hours agoparentprevIt will take a few more years for Chromebooks to take over parts of the Windows market share. It&#x27;s sad but the Steam Deck and Chromebooks are most likely the closest the Linux Desktop will ever come in the next decades. reply layer8 14 hours agoparentprevNope, I’ve been informed it’s the year of the dragon. Looking at the list, there doesn’t seem to be a year of the Linux desktop. reply gigglesupstairs 14 hours agorootparentSo, Dragon Linux? reply moffkalast 14 hours agoparentprevEvery year is the year of Linux desktop :P reply einpoklum 10 hours agorootparentDesktop Linux is:* Easy (and gratis) to obtain* Easy to install, these days, on most machines* Behaves nicely out of the box - usually, with most popular distributions* Has good software catering to most user&#x27;s daily needs, and is well-internationalized and well-localized for the most part.* Looks nice :-)So, yes, I&#x27;d say that these days, every year is the year of the Linux Desktop. reply skydhash 4 hours agorootparentInstalled linuxmint on an old mac mini 2011 and it work quite well. When I think about the tiny desktop format, I believe you can put linux mint on it and got a great general desktop that way, more reliable than a laptop. reply SushiHippie 12 hours agoparentprevMaybe the decade of linux desktop reply maxrf 14 hours agoparentprev¯\\_(ツ)_&#x2F;¯ reply BonitaPersona 9 hours agoparentprevagain! reply ryukoposting 8 hours agoprevLikewise. This has been a hell of a year, for better and worse (mostly worse). HN is a reliable place where I can have reasonable conversations about stuff I love doing. It&#x27;s a shining example of what I wish the internet was. reply chrisallenlane 7 hours agoparentHope 2024 is better for you! reply gigatexal 14 hours agoprevHN has literally changed my life — it was the who’s hiring a few years back that got me my first startup job here in Germany and the rest is history.I do spend too much time here but I tend to learn a lot. reply purplerabbit 6 hours agoparentChanged my life as well. Accelerated my assimilation of good software principles, management theory, business strategy, and salary negotiation. Accelerated my career by like 40%. And lots of entertainment along the way. Thanks HN! reply asabla 14 hours agoprevI still find it a bit crazy how familiar HN still feels after so many years I&#x27;ve been reading posts here. Most of that time was me lurking and latter me starting to interact with the community.It&#x27;s always nice to get your world view shattered by someone more knowledgeable or getting a point of view you wouldn&#x27;t get anywhere else. Stay the same HN and kudos to dang for keeping up with all the comments and posts being submitted.Happy new years y&#x27;all reply maxboone 14 hours agoprevHappy New Year everyone, huge thanks dang for your work and everyone for the civil and interesting discussions.HN feels like one of the few places similar to how early internet was. reply ivanstojic 51 minutes agoprevThanks for being here over the years. The faces changed some, stayed the same some, but it’s good people and good conversation!May you all have a happy new year, and many more! reply FredPret 14 hours agoprevI love HN. Dang does an amazing job moderating this place.My new-years resolution: to not post a single political thing; interesting technical things only. Let&#x27;s see how that goes in a US election year! reply block_dagger 14 hours agoprevThanks for being a great community we can all escape to throughout the day. Best wishes for intellectual riches in 2024. reply Simon_ORourke 14 hours agoparentSeconded! Happy MMXXIV reply replwoacause 14 hours agoprevThis is the only social media I use, but the special thing about it is that it doesn’t really feel like social media. The content is edifying and the community is well run thanks to dang. Happy New Year everyone! reply MarcusE1W 14 hours agoprevHappy new Year from Germany, we are not quite there yet, but in the other years it always worked and so I am confident we will make it this year as well.Time for one more Dinner For One ;-) reply hacb 10 hours agoparentI hope you enjoyed your dinner! I&#x27;m currently in bed, sick as hell while family and friend are having fun downstairs. I guess it&#x27;s my illness quota for the year :)Happy new year ! reply dylanwenzlau 3 hours agoprevDespite a minor grievance about not making HN mobile friendly sooner, I have extreme respect for long lasting high quality things, and this is one of them.Happy new year hacker news reply cableshaft 2 hours agoparentHow is it not mobile friendly? I check it on the phone periodically and have no problems. Have much more issues with Reddit. reply 1f60c 14 hours agoprevHappy sum(n**3 for n in range(2, 10))to all! May we continue to learn from one another. reply avmich 14 hours agoparentnext [–]+&#x2F; ^&3 [ 2 + i. 8Happy New Year from J language :) reply ksec 6 hours agoprevHappy New Year HN!Still waiting for the Annual 2024 Prediction thread to show up. I think csomar used to does it every year [1] so i dont want to start one and steal his tradition.@Dang. Since we already have an official monthly Who is Hiring thread why dont we have an official yearly prediction thread as well?[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34125628 reply TheAceOfHearts 6 hours agoparentA yearly prediction thread might be fun, it&#x27;ll be cool to look back on it and see if anyone was right about major events. Personally I&#x27;m hoping that in 2024 we&#x27;ll achieve AGI and have a robust definition of what that means. Maybe I&#x27;m just a little bit too ambitious though. I was hoping that we&#x27;d get there in 2023 but progress seems to have stalled more than I had expected. reply tomcam 13 hours agoprevdang is the Philippe Petit of moderators. This place is what it is almost solely due to the tightrope act he performs as the best moderator on the Web. reply CM30 9 hours agoprevHappy new year folks! It&#x27;s just turned 2024 here in the UK, and I&#x27;m hoping it&#x27;s a great year. reply ta-run 13 hours agoprevHappy New Year everyone!HN has been very insightful and although I don&#x27;t post often, just lurking and reading the articles&#x2F;comments makes me realise how shit I amAnd although I&#x27;m not in the best frame of mind right now, I hope 2024 will be a year where I can change my career around. Much love to everyone here reply jwindle47 8 hours agoprevHappy 2024 :). I&#x27;ve gotten so much out of this site. 2024 marks 10 years for me as an SWE officially. Starting with a soulless internship in a defense contractor, working at many early stage startups, and ending with a FAANG.The one constant? This site. I never post, but I hope to more. Love the productive discussions that I always see on this site. It&#x27;s a breath of fresh air after spending any time on X, LinkedIn, Facebook, etc. reply shreezus 11 hours agoprevWe’re incredibly fortunate to be living through one of the greatest time periods imaginable right now. Happy 2024, onwards and upwards reply TimCTRL 14 hours agoprevHappy New Year HN! From Uganda! reply mtmail 14 hours agoprevThe 6th &#x27;Happy new year&#x27; submission today made it. The first 5 from other users didn&#x27;t get enough upvotes. You won the karma lottery. reply layer8 14 hours agoparentIt takes some time for the New Year to build up momentum, starting from the international date line. reply bloomingeek 3 hours agoprevAdmittedly, I don&#x27;t understand about half of what is discussed on this site. However, I have learned and been fascinated by many topics and replies! This is a great site for the curious of mind. Happy 2024! reply kjuulh 14 hours agoprevHappy new years everyone.I started regularly browsing Hacker News around the start of corona when I graduated from university as well. It has been an amazing few years, and I look forward to the coming year.A lot has changed these last years, but somehow Hacker News always remains a solid place to procrastinate and&#x2F;or learn new things.When I tell other developers that if they want to keep up to date with tech, I tell them about Hacker News, it always goes like this: Come for the articles stay for the community. Thanks to everyone making Hacker News both an interesting and welcoming space to hang out! reply jacquesm 9 hours agoprevHappy New Year to all of you as well. 15 years and counting here, can&#x27;t believe it&#x27;s been that long already.2023 has been a pretty wild ride, a lot of our possible futures are riding on some key decisions that will be made in 2024, I hope they will all fall in the way that will lead to a more stable and peaceful world. reply terhechte 9 hours agoparentSame, clocking in at 13 years and I still visit this place multiple times a day and enjoy it. Also, learned life-changing things here. reply crims0n 14 hours agoprevHappy New Year! This is my favorite community, and in a lot of ways feels like a last bastion for our kind. reply lumax 8 hours agoprevOne of my favourite questions to ask people is where&#x2F;how they discover new interesting things to read. If I ask myself this question, HN is definitely a major part of the answer.Thanks everyone for sharing from this lurker and happy new year! reply botulidze 5 hours agoprevHappy New Year to everyone here!My 2024 resolution is to stop opening multiple HN tabs and never finishing majority of them to read through :-) reply bloopernova 12 hours agoprevHere&#x27;s to another year of: Emacs, and org-mode Firefox, and the inexplicable lack of vertical tabs adoption in Chrome zsh, fzf, ripgrep awesomeness, fd, asdf&#x2F;rtx, btop, and cool shell stuff Rust everywhere Factorio! Frivolous Nerdery! Oh and some LLM&#x2F;AI stuff might also be popular reply dc_ist 2 hours agoprevHappy New Year HN! May 2024 bring you love, peace, and prosperity. reply go2europa 13 hours agoprevHappy new year, from Connecticut in USA. A fellow lurker on here, this is the only news source I&#x27;ve deemed worthwhile to consistently follow since I first learned about it from my college roommate. More than that, the content and community here is always amazing.Much appreciation to dang and the people who help make this place what it is. reply shaunxcode 2 hours agoprevFor those who walk the straight edge: resist. Hold fast. This is the year we move beyond text. reply lagrange77 13 hours agoprevHappy successful solar orbit revolution, fellow space travellers! reply xbar 13 hours agoparentLet&#x27;s go round again! reply grumblepeet 14 hours agoprevHappy New Year all. HN has been there for me to read, learn, get cross with comments, and generally entertaining me for several years. I may not always agree with or understand articles that are linked, but I come back every day. So Happy New Year and thanks all. Esp dang for keeping the place civil. reply wdfx 14 hours agoprevHappy new year from a Londoner currently in Reykjavik :fireworks: reply joisig 13 hours agoparentHappy New Year from an Icelander in Garðabær (10 minutes by car south of Reykjavík... our definition of a suburb ) reply meheleventyone 10 hours agoparentprevHappy new year, just about to head up to Hallgrimskirkja for the fireworks with my family. reply nixass 14 hours agoparentprevYou got some real natural fireworks over there. Hope the sight is nice (have no idea if Grindavik is still erupting) reply wdfx 14 hours agorootparentI think it calmed down the last week or so but tourism isn&#x27;t exactly encouraged around that area.We&#x27;re in the capital with plenty of man made fireworks :) reply fairytale 2 hours agoprevHappy New Year everyone! I hope 2024 brings us more love, peace and knowledge. reply pototo666 5 hours agoprevThank you HN.When I don&#x27;t want to work or code, HN can always motivate me to do more. reply blameitonme 13 hours agoprevHappy New Year Everyone.this was the best year in my life, hope the next is even better for me as well as y&#x27;all <3 reply arjonagelhout 12 hours agoprevHappy new year everyone! Everyone who posts and comments here has inspired me in so many ways.Books, programming tricks, views on life, interesting blog posts, and all that without any distractions.hn.algolia.com and news.ycombinator.com are my go to, everyday. reply glapworth 9 hours agoprevHappy new year all! I’m proud to be an understated member of this community, only commenting briefly but yet coming back here every day to see the some of the most inspiring and incredible comments. I hope 2024 is as good for you all as it can possibly be. Personally I’m looking forward to a new year that is as awesome for my family and myself as I hope for everyone else. Good luck everyone in 2024! reply gustavopezzi 9 hours agoprevI agree.Hacker News is pretty much the only website I personally still visit, and it has very kind to our blog posts in the past. :)Happy 2024 from the entire team at pikuma.com. reply aEJ04Izw5HYm 7 hours agoprevHappy 2024, in London UK I hope to meet others in the new year at Hacker News London meetup: https:&#x2F;&#x2F;meetu.ps&#x2F;c&#x2F;4ZYdt&#x2F;5Tyc&#x2F;d reply eks391 5 hours agoparentWhere are these advertised? This is the first I&#x27;ve heard of a meetup reply IAmGraydon 5 hours agoprevHappy new year, everyone. Wishing you all the success in the world this year.A special thank you to dang for helping to uphold this bastion of intellectual discourse in a sea of noise. reply mgd 9 hours agoprevWishing everyone the best for the new year. This is a wonderful place amongst the craziness of the modern internet reply hannasm 2 hours agoprevI hate to generalize but it&#x27;s pretty good here. reply rismay 1 hour agoprevHappy new years! reply hackernj 12 hours agoprevHappy 10 - 9 + 8 * 7 * 6! &#x2F; 5 &#x2F; 4 + 3 * 2 + 1! reply jasoneckert 11 hours agoprevDIS chu&#x27; DatIvjaj! (Happy New Year in Klingon). reply phtrivier 11 hours agoprevWe learned the value of immutability, so I don&#x27;t celebrate new year any more, I just append days to each other and celeb OOM reply mckirk 14 hours agoprevI can only second that.And by the way, it always makes me happy to see a &#x27;thunderbong&#x27; submission reach the front page. To me that&#x27;s a reassuring sign that we haven&#x27;t been gentrified yet ;) reply zerojames 14 hours agoprevHappy new year, everyone!Thank you dang for all the work you do to help maintain HN! reply Brajeshwar 6 hours agoprevHey Everyone,Wishing a Happy New Year to your team, family, and you. Best of luck to 2024; I&#x27;m sure it will be way better than 2023. reply einpoklum 10 hours agoprevWishing us all a less-sad year from here in occupied Palestine. I&#x27;m not in Gaza, I&#x27;m in the Israeli part, where the police prevents demonstrations against the war, social media posts lead to arrests and prosecutions, and people are awash with bloodlust and pain. 2024 is not going to be a good year, but we can at least hope less people will be killed this year and we don&#x27;t experience the completion of an ethnic cleansing. reply upmostly 14 hours agoprevI have a fantastic feeling about 2024...HN is somewhere that feels like home to me. As it does for a lot of people. I&#x27;m grateful to have it in my life.All the best. reply pylua 14 hours agoprevThanks everyone. I have learned so much here and have a backlog of interesting articles to consume. All the best to everyone in 2024. reply cod1r 5 hours agoprevHappy new years! I&#x27;m still sitting here coding XD... TO MOAR CODING IN 2024!!! reply klysm 6 hours agoprevA toast to all the time this year I have spent thinking through date time programming and lamenting calendars reply larodi 7 hours agoprevI find myself checking HN on parties, incl. this NY event with lots of people. but there is always time to check HN.HNY, all HN! reply ChrisArchitect 12 hours agoprevDespite increasing piles of duplicate content HN remains my favourite place to stay on top of news and get into the nitty gritty of topics via discussions on new and old things, especially all our classic tech humour and insights. Cheers all reply mox111 8 hours agoprevHappy new years you crazy people. Hear’s to many failed ideas, to many outrageous schemes, to many goals gone amiss. I love you lot xx reply hdhuwgdue2 13 hours agoprevAway from main pc so throaway account used. Happy New Year everyone - even or maybe especially those I disagreed with this year. You guys make this place worth coming back more often than productivity dictates. Woo. Lets do this. reply drexlspivey 14 hours agoprevI was expecting this at midnight UTC reply dwhitwor 7 hours agoprevYou too TB. I find it very chill knowing that someone else finds HN as delightful as I do. :) reply ErikAugust 14 hours agoprevHere’s a countdown for those on the East Coast: https:&#x2F;&#x2F;cronster.app&#x2F;@erik&#x2F;new-years-day-2024&#x2F;?1 reply Lukeisun 14 hours agoprevHappy New Years! I have visiting here regularly since August and it has been great :) reply testplzignore 14 hours agoprevNo leap seconds in 2023. Here&#x27;s hoping that IERS blesses us again in 2024! reply giantg2 10 hours agoprevI hope everyone has a better 2024 than me. Probably a lower wish than most people think. Mine&#x27;s going to suck, for multiple reasons. reply koliber 12 hours agoprevHappy 2024. This year was full of surprises. Hard to tell which way they will go. Be curious, be adaptive, smile, and spread the love. Everything else will fall into place. reply deadbyte 8 hours agoprevLong time lurker, but I get so much value from this community and wish everyone well for the year ahead! reply op00to 12 hours agoprevThis is the place where I consistently leave in a better mood than I arrived. Thank you everyone for showing me I don’t have to be an asshole to get my point across! reply anArbitraryOne 7 hours agoprevMy resolution is to write fewer stupid comments on here. Failing so far reply monkeydust 12 hours agoprevHappy new year. Like OP I&#x27;m here multiple times a day and take a lot out and try to give back. Here&#x27;s to more of this next year. reply hardlianotion 10 hours agoprevHappy New Year to you all. This year will be the year of open source contributions. This time I mean it. reply irrational 7 hours agoprevOne more year and the century is a quarter of the way over. reply tamimio 12 hours agoprevHappy new year everyone, been lurking in this site since 2009? I think, so hope everyone has a wonderful year! reply chris-orgmenta 14 hours agoprevHere&#x27;s to a year ahead of compassion and progress.Hope y&#x27;all have fun, and thanks for everything positive that you do, whatever it is. reply defart 10 hours agoprevHappy new year from Croatia! reply sjfjsjdjwvwvc 12 hours agoprevHappy new year from Brazil <3 HN is really the only social media that is worth visiting reply seydor 14 hours agoprevi&#x27;ll be extremely cautious on underdetermined wishes and will request a new year that s better than last year. reply dwhitwor 7 hours agoprevYou too TB. I thought I was the only one. :) reply agurha 10 hours agoprevHappy new year from NYC! reply jviotti 11 hours agoprevHappy New Year everybody from Argentina, and may your wildest dreams come true! reply jll29 11 hours agoprevHappy New Year, all!$ xlock # ...about to go to the balcony now to watch the fireworks reply gessha 14 hours agoprevHappy New Year!I hope this year brings (physical and financial) health to you and your family as well as fun times. reply Chio 14 hours agoprevHappy new year! Committed to making this new year my, and your, best year so far!Thanks for being a great community. reply parvatzar 14 hours agoprevWishing Everyone a very Happy New Year 2024. Looking forward to more new learnings on HN as before! reply xg15 10 hours agoprevA happy and successful year 2024 to you all as well! reply mmaunder 14 hours agoprevAgreed. Happy new year to you and the community here. Have an excellent and lucky 2024. reply life-and-quiet 9 hours agoprevHappy New Year! Thank you to everyone who contributes to the spirit of this place and keeps us a healthy community. reply nhatcher 14 hours agoprevHappy new year! I have learned a lot from this community and plan on continuing so reply sidcool 14 hours agoprevHappy new year fellow hackers. Hope 2024 brings less bugs and more mental peace. reply achempion 12 hours agoprevHappy New Year Everyone! HN is amazing, good luck everyone in 2024 reply vinniepukh 13 hours agoprevI learn so much here every year! Next year, I want to contribute more. reply sangupta 14 hours agoprevHappy New Year everyone - wishing you all a healthy and happy 2024. reply marginalia_nu 14 hours agoprev2023 was a good one for me, let 2024 be a great one for all! reply eddieweng 9 hours agoprevGrateful for HN! Happy New Year! reply norparsec 11 hours agoprevHappy New Year Everyone! Thanks for building awesome projects, sharing interesting articles, and always helping out. reply masterj122517 2 hours agoprevHappy New Year reply moneywoes 14 hours agoprevluckily set the bar really low for 2023.can only go up from here reply crtasm 9 hours agoprevHappy New Year to you all! reply matanyall 14 hours agoprevYeehaw, here&#x27;s to another year gone by! reply sgammon 13 hours agoprevI love HN! Happy new year everybody. reply AISnakeOil 14 hours agoprevThanks for making my work day better :) reply ninjamayo 11 hours agoprevHappy New Year to everyone! reply rmrf100 7 hours agoprevHappy New Year! reply szundi 11 hours agoprevHNY! reply im_so_dumb 9 hours agoprevFeliz año nuevo ! reply tunnuz 14 hours agoprevAnd to you. reply fernandotakai 14 hours agoprevhappy new year!it was a great year for me, and i totally hope it was a great year for all of you. reply dylukes 14 hours agoprevHappy new year to all! Cheers! reply denysvitali 12 hours agoprevHappy New Year everyone! reply digitalbreed 11 hours agoprevHappy New Year HN! reply sportstuff 14 hours agoprevCheers reply 2f0ja 13 hours agoprevhappy ny everyone, thanks for making this a wonderful community reply agurha 10 hours agoprevHappy new year! reply nektro 5 hours agoprevhappy new year! reply nobrains 14 hours agoprevHappy new year from UAE. reply jmkni 12 hours agoprevHappy New Year pal! reply razor6ack 13 hours agoprev(HN)y 2024 everyone! reply whisper_yb 11 hours agoprevHappy new year! reply matheusmoreira 8 hours agoprevHappy new year! reply TriangleEdge 14 hours agoprevHappy New Year <3 reply gnarlouse 10 hours agoprevI’ll be honest, I don’t know what we’re celebrating.The planet has been on a slow roll into oblivion. 2024 will be the hottest year in recorded human history and we’ll probably pass the irreversibility threshold for average global temperature.There are multiple extremely unsettling ongoing global humanitarian crises: Chinese-Uyghur, Russo-Ukrainian, the latest Israeli conflict*.What exactly are we celebrating?*note: I’m not taking sides. There are atrocities perpetrated by both sides, and grievances experienced by both sides. It just makes me sick generally to think about. reply pmichaud 10 hours agoparentWe are the unbroken line of billions of years of life, and today we&#x27;re still alive, unbroken. May it always be so. reply gnarlouse 4 hours agorootparentI’m pretty sure those scorching heatwaves are going to break India at some point. I could be wrong. reply antoineMoPa 13 hours agoprevHappy new year! reply ericmay 14 hours agoprevHappy New Year! reply api 14 hours agoprevThis is one of the only places I still post. I post a little on Mastodon and occasionally lobste.rs but that&#x27;s about it.I attribute its retaining of quality to banning memes and other low-effort junk and severely limiting (both as policy and culturally) politics and culture war flame wars. Also being text only helps a lot. reply kerv 14 hours agoprevHappy new year! reply vpjosh 14 hours agoprevHN is my favorite place on the web. Happy NYE HN!josh :) reply EGreg 11 hours agoprevLet&#x27;s do some great things this coming year!If you&#x27;re reading this, I&#x27;m curious what you plan to accomplish in this coming year. Write a comment below and tell us a little bit about your main goals to accomplish by this time next year! reply gtirloni 6 hours agoprevHN is the best. I&#x27;m really grateful it exists. Thanks to you all. reply brador 14 hours agoprevHN and Skimfeed, my two dailies after quitting social media. reply baz00 14 hours agoprevHNY to everyone here. While I don’t agree with some of you this is probably one of the finest communities left on the net. Hat tip to you all. reply bozhark 14 hours agoprevCheers HN reply weekin2day 6 hours agoprevhappy new years fam reply knaik94 11 hours agoprevhappy New Year reply dimpalo 14 hours agoprevhappy new year -3H reply PIPO115 14 hours agoprevHappy New Year reply utybo 14 hours agoprevI discovered HN this year, and my takeaway from it all is: great links, questionable debates in the comments.Happy new year everyone! reply fortran77 14 hours agoprev...to those who observe New Year&#x27;s Day on January 1st. reply justinl33 9 hours agoprev [–] Everyone drop your nerdiest NY resolutions. I’ll start: turn my to do list into a priority queue and never let it exceed a length of 25 replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author acknowledges their excessive use of Hacker News (HN) but believes it's the only website worth frequenting.",
      "The author sends well wishes for the year 2024.",
      "No substantial or significant information is provided in the text."
    ],
    "commentSummary": [
      "Users on Hacker News engage in discussions about building relationships, Linux, and expressing gratitude and aspirations for the future.",
      "The overall tone of the discussions was positive and focused on personal growth.",
      "The community serves as a platform for sharing experiences and fostering a sense of community among tech enthusiasts."
    ],
    "points": 1267,
    "commentCount": 246,
    "retryCount": 0,
    "time": 1704047614
  },
  {
    "id": 38823101,
    "title": "Customizing SteamOS: Challenges, Updates, and Mods",
    "originLink": "https://iliana.fyi/blog/build-your-own-steamos-updates/",
    "originBody": "December 29, 2023 How I forked SteamOS for my living room PC SteamOS 3 (“Holo”) is the Arch-based Linux distribution built for the Steam Deck, Valve Software’s portable PC gaming device. It’s a very interesting Linux distribution even when you only focus on how it updates itself: updates are performed atomically by downloading a new read-only root filesystem to an inactive partition, then rebooting into that partition. But consumers can also run steamos-devmode to unlock the root filesystem, put the pacman database in working order, and give them a working Linux distro with a normal package manager. This A/B atomic updates system is pretty standard for OSes these days, but there’s a lot going on in SteamOS that makes them work even with heavy customization by the end-user. I wanted to explore that while still being able to make changes to the root filesystem images. steamos-devmode is the easy way out; I wanted to make a proper fork. Here’s how I did it. I use these instructions, but am providing them here to help publicly document some of what SteamOS does. I’m not responsible for you breaking your Steam Deck, and if you send me questions asking why these steps don’t work for you I won’t answer. Why? I don’t own a Steam Deck. A bunch of my friends do, but they know better than to let me have root access on a device they actually like using. What I do have is a recently-built living room PC that I wanted to play games on… and SteamOS seemed like a reasonable choice. It almost even worked perfectly out of the box, although I think that is primarily because I built a computer that looks vaguely similar to a really heavy, battery-less Steam Deck1. The one thing that didn’t work was resume from suspend. Other distributions running on my computer using mainline or stable kernels did. Eventually, I found the sources for Valve’s kernel (it’s weird, I’ll explain when we get there) and started a git bisect, leading me to a commit that seems to fix resume from suspend on Steam Deck hardware, but ultimately breaks it on mine. Needing to revert this commit and do my own build is the ultimate reason I headed down this path. At several points in this process my partner asked if this made more sense than just using Arch or something else directly. I still don’t know the answer, although I think I still prefer relying on a Valve-tested set of packages than whatever’s current in the Arch repos. If I’m going to have to tinker with a Linux distro for running games, it may as well be one that people actually test their games on. I apparently have a tendency to make poor choices like this, because you are reading the second post in what has become a series on installing Linux distros onto systems they’re not ready for yet. SteamOS in a nutshell A SteamOS system has eight partitions. The stage 1 bootloader is stored on the EFI system partition, along with metadata files that describe the two A/B partition sets and how to choose which one to boot. Each of the two partition sets contains a stage 2 bootloader (GRUB), the root filesystem, and a /var partition. Finally, there’s a single home partition that fills the rest of the disk. Number Start (sector) End (sector) Size Code Name 1 2048 133119 64.0 MiB EF00 esp 2 133120 198655 32.0 MiB 0700 efi-A 3 198656 264191 32.0 MiB 0700 efi-B 4 264192 10749951 5.0 GiB 8304 rootfs-A 5 10749952 21235711 5.0 GiB 8304 rootfs-B 6 21235712 21759999 256.0 MiB 8310 var-A 7 21760000 22284287 256.0 MiB 8310 var-B 8 22284288 4000797319 1.9 TiB 8302 home When the system boots, a number of other pseudo-filesystems get mounted. Almost a dozen directories, including /var/log, /root, and /nix, are bind-mounted from /home/.steamos/offload to keep their data persistent. Perhaps my favorite detail of SteamOS is how it handles /etc: it mounts an overlayfs on top of it, with modifications persisted at /var/lib/overlays/etc/upper. This allows persisting the usual things that need to be persisted in /etc (e.g. the machine-id file, NetworkManager connections) while allowing updates to untouched configuration files. Most Linux package managers have similar behaviors around /etc, only updating config files that haven’t been changed from their defaults, but Valve’s approach makes this work with the A/B partition system without any package manager logic. A system update is started when the Steam client, or a user in a terminal, runs steamos-update. This runs a Python program, steamos-atomupd-client, which sends a request containing the current OS information and the user’s update channel configuration to the URLs in /etc/steamos-atomupd/client.conf in order to determine whether there is a new update. If there is, the servers respond with a path to a RAUC bundle, which the client downloads and runs rauc install on. RAUC verifies the signature of the bundle and looks for the rootfs.img.caibx file, then runs casync extract to download all the necessary pieces of the new image and write them to the inactive rootfs partition. RAUC then runs a post-install script that selectively2 synchronizes data from the active /var partition to the inactive /var partition, and modifies the stage 1 bootloader configuration on the EFI system partition to boot into the newly-written partition set. Patching the kernel Valve uses a heavily-modified Linux kernel in SteamOS. We can know this because we can readily download the sources. It’s a little more convoluted than git clone, but not by much. Their pacman mirror can be found in /etc/pacman.d/mirrorlist, and the sources used for current (as of writing) SteamOS images are in the sources/holo-3.5 and sources/jupiter-3.5. Right now, the current stable image’s kernel is 6.1.52-valve9-1-neptune-61, whose source lives at https://steamdeck-packages.steamos.cloud/archlinux-mirror/sources/jupiter-3.5/linux-neptune-61-6.1.52.valve9-1.src.tar.gz. This is a 2.9 GiB tarball. Why is it that big? Because there’s an entire Linux Git tree in here. $ tar xvf linux-neptune-61-6.1.52.valve9-1.src.tar.gz linux-neptune-61/ linux-neptune-61/config linux-neptune-61/config-neptune linux-neptune-61/PKGBUILD linux-neptune-61/archlinux-linux-neptune/ linux-neptune-61/.SRCINFO linux-neptune-61/archlinux-linux-neptune/hooks/ linux-neptune-61/archlinux-linux-neptune/branches/ linux-neptune-61/archlinux-linux-neptune/HEAD linux-neptune-61/archlinux-linux-neptune/config linux-neptune-61/archlinux-linux-neptune/description linux-neptune-61/archlinux-linux-neptune/objects/ linux-neptune-61/archlinux-linux-neptune/refs/ linux-neptune-61/archlinux-linux-neptune/info/ linux-neptune-61/archlinux-linux-neptune/packed-refs linux-neptune-61/archlinux-linux-neptune/info/exclude linux-neptune-61/archlinux-linux-neptune/refs/tags/ linux-neptune-61/archlinux-linux-neptune/refs/heads/ linux-neptune-61/archlinux-linux-neptune/objects/pack/ linux-neptune-61/archlinux-linux-neptune/objects/info/ linux-neptune-61/archlinux-linux-neptune/objects/pack/pack-6e0e2b73767937e4f217e55f6d3628af296eecfc.idx linux-neptune-61/archlinux-linux-neptune/objects/pack/pack-6e0e2b73767937e4f217e55f6d3628af296eecfc.pack linux-neptune-61/archlinux-linux-neptune/objects/pack/pack-6e0e2b73767937e4f217e55f6d3628af296eecfc.rev ... This was likely generated by makepkg --allsource from a PKGBUILD with: _tag=6.1.52-valve9 # ... source=( \"$_srcname::git+ssh://git@gitlab.steamos.cloud/jupiter/linux-integration.git#tag=$_tag\" config # Upstream Arch Linux kernel configuration file, DO NOT EDIT!!! config-neptune # Jupiter: the neptune kernel fragment file (overrides 'config' above) ) So we can’t clone directly from their private GitLab repo or link to various commits, but we can get regular snapshots of every tag with full commit history in the repo from their makepkg sources. This is very useful for, say, bisecting which commit breaks suspend on your living room PC. The source tarball isn’t a working Git tree that you can cd into and start hacking on. It’s a bare repository, which you can clone into a normal working tree3. My recommendation is to maintain your own branch of changes, tag your releases, and push them to your favorite Git host so that you can use it in the PKGBUILD file. Here’s mine! wget https://steamdeck-packages.steamos.cloud/archlinux-mirror/sources/jupiter-3.5/linux-neptune-61-6.1.52.valve9-1.src.tar.gz tar xvzf linux-neptune-61-6.1.52.valve9-1.src.tar.gz git clone linux-neptune-61/archlinux-linux-neptune/ linux-neptune cd linux-neptune git switch --create my-branch 6.1.52-valve9 Fix up your PKGBUILD (pointing to your Git repo, not mine): --- a/PKGBUILD +++ b/PKGBUILD @@ -3,5 +3,5 @@ pkgbase=linux-neptune-61 -_tag=6.1.52-valve9 +_tag=6.1.52-valve9-iliana1 pkgver=${_tag//-/.} pkgrel=1 @@ -19,5 +19,5 @@ options=('!strip' '!debug') _srcname=archlinux-linux-neptune source=( - \"$_srcname::git+ssh://git@gitlab.steamos.cloud/jupiter/linux-integration.git#tag=$_tag\" + \"$_srcname::git+https://git.iliana.fyi/linux#tag=$_tag\" config # Upstream Arch Linux kernel configuration file, DO NOT EDIT!!! config-neptune # Jupiter: the neptune kernel fragment file (overrides 'config' above) Then makepkg should spit out a package. (Tip: makepkg MAKEFLAGS=-j$(nproc), or updating /etc/makepkg.conf, is a good idea if you are not building in a tiny virtual machine.) This same general process should apply for any SteamOS-specific packages; all the ones I’ve looked at similarly use a Git repository as their first source. To make the next steps easier, I set up a pacman repo containing my package outputs. This also helps the steamos-devmode tool work properly if you choose to run that in the future. This is very simple: put the packages in a directory, run repo-add $REPO_NAME.db.tar.zst [PACKAGES...] in that directory, and upload the directory to a web host somewhere. Repacking the root filesystem The update client is perfectly-readable Python, and the sources for the rest of the packages on the system can be found adjacent to Valve’s pacman repos, but I haven’t yet found any release engineering scripts. Reverse engineering these would be fraught, time-consuming, and beyond what I believe my attention span would allow, so I decided to take the existing root filesystem and “repack” it to fit my needs. If you want my scripts without the explanations and commentary, you can find them at https://git.iliana.fyi/fauxlo/tree/. Getting the root filesystem The normal way to get a copy of the SteamOS root filesystem image is to buy a Steam Deck or download the recovery image from https://store.steampowered.com/steamos/download?ver=steamdeck, both of which require agreeing to the Steam End User License Agreement4. The methods I describe below don’t make you do this, but now you’re aware of it. First, we need the root filesystem. You could install SteamOS, run an update, and pull it off the disk, but that is kind of obnoxious, especially if you don’t have any hardware to install it on. Every build of a Steam Deck image can be found at https://steamdeck-images.steamos.cloud/steamdeck/, but to find the current release version you can look at what appears to be a fallback URL for the updates system, https://steamdeck-atomupd.steamos.cloud/meta/steamos/amd64/snapshot/steamdeck.json (or this path for the preview channel). As of writing, the current stable version is https://steamdeck-images.steamos.cloud/steamdeck/20231122.1/. To download the root filesystem, we follow the same steps steamos-atomupd-client does: first, download the RAUC bundle (the .raucb file). Then extract the rootfs.img.caibx from inside; these RAUC bundles are SquashFS filesystems with a signature at the end, so you can either mount it or use unsquashfs (from squashfs-tools) or p7zip to extract it without mounting. Finally, use casync to fetch the image using the .castr store adjacent to the .raucb bundle: casync extract \\--store=https://steamdeck-images.steamos.cloud/steamdeck/20231122.1/steamdeck-20231122.1-3.5.7.castr \\rootfs.img.caibx rootfs.img The casync store URL is the RAUC bundle URL, but with .raucb replaced with .castr (this is hardcoded in steamos-atomupd). Here is a script I use to do all this. The adjacent .img.zip and .img.zst files are not the root filesystem, sadly, but are separate bootable recovery images: $ sgdisk --print disk.img Disk disk.img: 15125000 sectors, 7.2 GiB [...] Number Start (sector) End (sector) Size Code Name 1 34 131071 64.0 MiB EF00 esp 2 131072 393215 128.0 MiB 0700 efi-A 3 655360 11141119 5.0 GiB 8304 rootfs-A 4 11141120 11665407 256.0 MiB 8310 var-A 5 11665408 15124966 1.6 GiB 8302 home You could extract the rootfs partition and use it in the next steps, but for some reason it’s not a bit-for-bit copy of the image that is downloaded via RAUC and casync, and doing this doesn’t save you from having to use those tools as we need them to turn our repacked image back into an update bundle. Mounting the root filesystem First, we should randomize the filesystem UUID. If you update from a currently-released SteamOS image to your customized one without randomizing the filesystem UUID, you will end up with two distinct filesystems with the same UUID. This can cause problems. btrfstune -fu rootfs.img Valve is currently using Btrfs images with zstd compression, so to maintain that compression as we change the image, we need to mount it with the appropriate option: mkdir rootfs mount -o compress=zstd rootfs.img rootfs SteamOS uses Btrfs’s readonly subvolume property; clear that flag: btrfs property set -ts rootfs ro false Modifying certain packages, such as the Linux kernel, triggers scripts that want /dev and /proc, so mount those: mount -t devtmpfs dev rootfs/dev mount -t proc proc rootfs/proc It’s also a good idea to prevent writes to directories that will be mounted by the booted system. We can mount tmpfs to these directories: mount -t tmpfs tmpfs rootfs/tmp mount -t tmpfs -o mode=755 tmpfs rootfs/run mount -t tmpfs -o mode=755 tmpfs rootfs/var mount -t tmpfs -o mode=755 tmpfs rootfs/home In this example we’ll be installing packages via pacman repositories in a chroot (using pacman’s --sysroot option). Networking functions fine in a chroot, but name resolution still relies on a correct /etc/resolv.conf, so bind mount one in: mount --bind \"$(realpath /etc/resolv.conf)\" rootfs/etc/resolv.conf Replacing packages To add your custom repository, make it the first repository entry in /etc/pacman.conf. This will ensure your packages override any newer-versioned ones from Valve’s repositories. It also allows your packages to be reinstalled by steamos-devmode if you run that on this image. I used this stanza: [fauxlo] Server = https://fauxlo.ili.fyi/pacman/$arch SigLevel = Never SigLevel = Never allows the packages to have no signatures. If you want to GPG-sign your packages, go for it, but that’s beyond what I have patience for. If you do end up installing GPG-signed packages, you’ll need to populate the pacman keyring. I think it’s best to avoid messing with the empty keyring in /etc/pacman.d/gnupg by populating a new keyring on a tmpfs: chroot rootfs pacman-key --gpgdir /tmp/gnupg --init chroot rootfs pacman-key --gpgdir /tmp/gnupg --populate # These start a gpg-agent, which we need to stop before we can unmount at the end. chroot rootfs gpgconf --homedir /tmp/gnupg --kill all Then add --gpgdir /tmp/gnupg to your pacman incantation. Then, we install: pacman --sysroot rootfs --noconfirm -Sy linux-neptune-61 In my script, I avoid using -y and instead synchronize my repository’s database behind pacman’s back before running the install command: curl -Ro rootfs/usr/lib/holo/pacmandb/sync/fauxlo.db \\https://fauxlo.ili.fyi/pacman/x86_64/fauxlo.db This keeps the state of the other repositories on disk frozen at the same point in time when the image was originally built. I don’t think this actually matters, but it reduces the changes that show up if I diff my image against Valve’s. Changing the build ID steamos-atomupd reads from /lib/steamos-atomupd/manifest.json, or if that is somehow missing, /etc/os-release, to determine the version and build ID of the current image. It will refuse to perform an update if the server says the available update’s build ID is the same as the current image. It’s also good to know what image you’re running. The build ID must be of the form YYYYMMDD.N. If it isn’t, steamos-atomupd exits with a Python traceback upon encountering it. To avoid having to remember to count up manually, I set N to a timestamp; either HHMMSS or a Unix timestamp would work fine. Update the buildid field in rootfs/lib/steamos-atomupd/manifest.json and the BUILD_ID field in rootfs/etc/os-release with whatever you pick. You can steal from the Bash script I wrote to do this. --- a/lib/steamos-atomupd/manifest.json +++ b/lib/steamos-atomupd/manifest.json @@ -4,7 +4,7 @@ \"variant\": \"steamdeck\", \"arch\": \"amd64\", \"version\": \"3.5.7\", - \"buildid\": \"20231122.1\", + \"buildid\": \"20231219.55534\", \"checkpoint\": false, \"estimated_size\": 0 } --- a/etc/os-release +++ b/etc/os-release @@ -11,4 +11,4 @@ LOGO=steamos VARIANT_ID=steamdeck VERSION_ID=3.5.7 -BUILD_ID=20231122.1 +BUILD_ID=20231219.55534 Keep a copy of the updated manifest.json handy, as it’s useful in building the updates server later. Changing the update URLs and signing keys RAUC uses X.509 certificates to establish trust. The trusted certificate lives at /etc/rauc/keyring.pem. You can make an overcomplicated PKI scheme, such as the one generated in RAUC’s tests, but a simple self-signed certificate is fine. Install your new certificate at rootfs/etc/rauc/keyring.pem. You’ll need to modify the URLs in rootfs/etc/steamos-atomupd/client.conf with your own: --- a/rootfs/etc/steamos-atomupd/client.conf +++ b/rootfs/etc/steamos-atomupd/client.conf @@ -1,5 +1,5 @@ [Server] -QueryUrl = https://steamdeck-atomupd.steamos.cloud/updates -ImagesUrl = https://steamdeck-images.steamos.cloud/ -MetaUrl = https://steamdeck-atomupd.steamos.cloud/meta +QueryUrl = https://fauxlo.ili.fyi/updates +ImagesUrl = https://fauxlo.ili.fyi/ +MetaUrl = https://fauxlo.ili.fyi/meta Variants = rel;rc;beta;bc;main Other changes You can make pretty much any change you want at this point as long as you don’t run out of space in a 5 GiB Btrfs image. For instance, if you want your SteamOS device to be resolvable as hostname.local on your network, you could remove rootfs/usr/lib/systemd/resolved.conf.d/00-disable-mdns.conf. This can be overridden with a configuration in the /etc overlay, but it’s kind of a pain in the ass. In general, my philosophy here is that you should avoid making changes that are trivial to perform without modifying the image. You could install Firefox in the root filesystem this way, instead of using Flatpak or Nix, but then you’d need to repack the image every time you want to install a Firefox security update. Unmounting the root filesystem Mark the filesystem read-only once again: btrfs property set -ts rootfs ro true Discard any unused blocks: fstrim -v rootfs Then, unmount. --recursive is particularly helpful here to take care of all the pseudo-filesystems we mounted in: umount --recursive rootfs Creating the RAUC bundle First we need to create the casync store and blob index. We can do this with: mkdir bundle casync make --store=rootfs.img.castr \\bundle/rootfs.img.caibx rootfs.img The RAUC bundle needs two other files. The first is manifest.raucm: cat >bundle/manifest.raucm bundle/UUID With those three files: $ ls bundle manifest.raucm rootfs.img.caibx UUID we can now call rauc bundle: rauc bundle \\--signing-keyring=cert.pem --cert=cert.pem --key=key.pem \\bundle rootfs.img.raucb Upload rootfs.img.raucb and rootfs.img.caibx to the web server specified by ImagesUrl in rootfs/etc/steamos-atomupd/client.conf. These need to be in the same directory. Final update server setup The web server you used for QueryUrl and MetaUrl in rootfs/etc/steamos-atomupd/client.conf will need to serve a JSON file. This doesn’t need to be fancy; what I do is write a live.json file with these contents: { \"minor\": { \"release\": \"holo\", \"candidates\": [ { \"image\": { \"product\": \"steamos\", \"release\": \"holo\", \"variant\": \"steamdeck\", \"arch\": \"amd64\", \"version\": \"3.5.7\", \"buildid\": \"20231219.55534\", \"checkpoint\": false, \"estimated_size\": 0 }, \"update_path\": \"rootfs.img.raucb\" } ] } } Note that the object at .minor.candidates[0].image should be the same as /lib/steamos-atomupd/manifest.json in your image. update_path is what the updates client will append to your ImagesUrl to download the bundle. I use the following Caddy configuration to rewrite the requests steamos-atomupd makes to QueryUrl and MetaUrl to the above live.json: root * /var/www/fauxlo.ili.fyi rewrite /updates /live.json rewrite /meta/*/*/*/*.json /live.json rewrite /meta/*/*/*/*/*.json /live.json file_server browse The real SteamOS QueryUrl and MetaUrl seem to have quite a bit more logic to them, but this is sufficient to get steamos-atomupd to find the new update. It has logic to avoid updating if it’s already running the image advertised as currently available. Updating! Once you have all this in place, you can update an existing SteamOS installation to this by modifying /etc/rauc/keyring.pem and /etc/steamos-atomupd/client.conf. (No steamos-readonly disable required, as your changes will land on the /etc overlay; after you run steamos-update, consider cleaning those changes out of /var/lib/overlays/etc/upper.) You can also probably install your modified SteamOS by modifying one of Valve’s recovery images, replacing their rootfs with your own. I haven’t tested this, but I also haven’t seen anything that would contradict this. Thanks for reading! 2023 has been a busier-than-usual year for my blog, and I’m pretty happy about getting more writing out there. If you’ve found any of my blog posts helpful, give a trans person all of your money. See you next year! Footnotes The similarities don’t end with just having an AMD CPU and GPU and an NVMe SSD; similar to how Valve says you shouldn’t open your Steam Deck because it will immediately make it less structurally resilient, you also shouldn’t open my living room PC because you might damage the precision-bent PCI slot cover plate keeping the graphics hovering above the case fans I had to use to replace the GPU fan shroud that wouldn’t fit in the case. ↩︎ The sync_var_mountpoints function in /usr/lib/rauc/post-install.sh has an excellent comment explaining why some files need to be excluded when synchronizing /var between the partition sets. ↩︎ You could also use git-worktree(1) for this: cd linux-neptune-61/archlinux-linux-neptune then git worktree add -B my-branch ../../linux-neptune 6.1.52-valve9. ↩︎ If anyone from Valve is reading this, you might want to update your EULA to point to somewhere more useful than https://gitlab.steamos.cloud/, which has barely any public repositories and certainly does not have a “list of contained packages along with their respective FOSS or proprietary licenses as well as source code for FOSS packages”. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=38823101",
    "commentBody": "I forked SteamOS for my living room PCHacker NewspastloginI forked SteamOS for my living room PC (iliana.fyi) 426 points by muterad_murilax 23 hours ago| hidepastfavorite214 comments techknowlogick 14 hours agoI love this kind of deep dive into customizing the software&#x2F;OS on a device you own. Glad that \"Tivoization\" isn&#x27;t a concern for the steamdeck.The most interesting part of the article was the mention of a &#x2F;nix partition, as I didn&#x27;t realize the steamdeck supports nixpkgs, after researching it more, they do indeed (not installed by default, but at least it is possible without having to fork an entire os to get it on the device). reply frutiger 12 hours agoparentnix has always been installable onto any *nix OS without requiring you to “fork an entire OS”.You can put the nix store in any writable location and modify $PATH to point to the symlinks directory. reply rollcat 9 hours agorootparentNot if your &#x2F; filesystem lacks the &#x2F;nix directory to use as a mountpoint, and happens to be a read-only image. Something that needs a workaround on macOS. reply frutiger 9 hours agorootparentIt has been a while since I last used it, but I was under the impression that the store location was customisable and thus could be anywhere? reply figomore 8 hours agorootparentNix store can be anywhere. But if it’s not &#x2F;nix Nix will not use the cache (already compiled packages) and compile every package needed. reply alchemist1e9 3 hours agoparentprevDoes anyone know what they are using from nixpkgs? I’d be curious since I’m a heavy nixpkgs user myself but unfortunately I don’t have a steamdeck. reply mat0 16 hours agoprevWhat a thorough and interesting post. I would personally never do something like this. The most tinkering I&#x27;ve ever done with Linux was in my RaspberryPi era and that&#x27;s 1% at most. So props to the author reply dixie_land 16 hours agoparentI was in a similar situation as the author: for quite a while I had to build my own Redhat kernel for a very obscure case: by pass RMRR check to pass GPU to a windows VM. (similar to https:&#x2F;&#x2F;github.com&#x2F;kiler129&#x2F;relax-intel-rmrr ; not my repo)The root issue can only be addressed by ROM updates from the manufacturer but I&#x27;m running an old DL360 that&#x27;s no longer supported by HPE.The patch itself is only one line change but updating the kernel is a pain since I have to : - get SRPM (there&#x27;s no git repo) - unpack SRPM, apply patch - rebuild and install reply cyanydeez 10 hours agoparentprevif you want a tv, soon there may be no real choice. reply quaffapint 18 hours agoprevI actually just ordered a GPU for my unRaid NAS server just to be able to do Steam Headless via a nice docker image(1) and then use Moonlight (for example) as a client on my Windows laptop. If it works, it&#x27;s much better than buying yet another piece of desktop hardware just to play games when my NAS is just sitting there idle most of the time. Just need to make sure I keep the power level setting on the Nvidia card to idle when not in use (hopefully a nvidia-persistenced call will do it).1: https:&#x2F;&#x2F;github.com&#x2F;Steam-Headless&#x2F;docker-steam-headless reply sdl 14 hours agoparentI spent some (too much) time trying to get pretty much the same thing running using GOW [1]. Was quite a bit harder than I thought, requiring a hdmi dummy plug to get the xserver config right etc.1: https:&#x2F;&#x2F;github.com&#x2F;games-on-whales&#x2F;gow reply quaffapint 12 hours agorootparentGood call out - this does require a dummy plug as well. reply sevagh 15 hours agoparentprevThis looks great. I currently use Sunshine + Moonlight, I&#x27;ll test Steam Headless performance soon. reply moondev 12 hours agoparentprevAnother alternative, launch a kvm with GPU passthrough and use cloudinit to launch sunshine and the game, or just use the monitor directly.https:&#x2F;&#x2F;kubevirt.io&#x2F;user-guide&#x2F;virtual_machines&#x2F;host-devices...Declarative cloud native game launching! kubectl apply -f crysis.yaml reply bormaj 14 hours agoparentprevThis is really interesting! Do you notice any limitations on input lag or video quality when streaming over a local network this way? reply goda90 12 hours agoparentprevOh nice. I&#x27;ve been day dreaming of setting up a server with turn based, hot seat enabled games (like Civilization) and a browser based way to remote into them so that friends and I can play long turn games from anywhere at any time. reply Kerbonut 17 hours agoparentprevThat is crazy, thanks for sharing! reply exitb 18 hours agoprevThere already are distributions based around elements of SteamOS, geared towards PCs and controller-based usage. ChimeraOS works for me quite flawlessly, including Steam Deck add-ons, like EmuDeck. reply sillywalk 11 hours agoprevAnybody else sort of miss that Netscape meteor shower favicon? reply dkga 9 hours agoparentI certainly do! reply Aldipower 11 hours agoparentprevYes! reply jokethrowaway 15 hours agoprevInteresting read! The A&#x2F;B upgrade sounds a bit overkill, you can always just pop up a live distro or install a recovery system (on an old version) in a partition in case something goes wrong.I recently moved to Arch after a few years of NixOS (preceded by years of Arch) and I think the fears of the author are misplaced.Arch is definitely a very serious and mature distro and I&#x27;d trust them more than Valve.The quality of the packages available for Arch is what made me move from NixOS. The main repos are updated really fast and AUR has a lot of useful packages. reply embik 15 hours agoparent> The A&#x2F;B upgrade sounds a bit overkill, you can always just pop up a live distro or install a recovery system (on an old version) in a partition in case something goes wrong.You and I can, the overwhelming majority of computer users cannot. Valve clearly focuses on building for the average person, something that Linux distributions (as much as I love them) still don’t really do (well).The system automatically recovering from a failed upgrade is essential in a low-maintenance OS at this point. reply stavros 15 hours agorootparentI can too, but I have better things to do than fix boot issues on my Steam Deck. I just want it to work. reply darkstar999 15 hours agoparentprevNo way steam deck users should be expected to boot a live distro to fix a botched upgrade. It needs to be seamless and behind the curtain. reply yjftsjthsd-h 9 hours agoparentprev> The A&#x2F;B upgrade sounds a bit overkill, you can always just pop up a live distro or install a recovery system (on an old version) in a partition in case something goes wrong.Could, sure, but we have the technology to make it unnecessary and disk isn&#x27;t that expensive, so why not? reply ParetoOptimal 14 hours agoparentprev> The quality of the packages available for Arch is what made me move from NixOS.Can you give some examples of this please?I generally find the NixOS packages high quality. reply dataangel 11 hours agorootparentUnless you care about packages from lang package managers like pip... reply ParetoOptimal 10 hours agorootparentYou can get those, but its more work on NixOS if not already packaged.I&#x27;m undecided on what I think about this since... I frequently get bitten by reproducibility issues of pip packages.Anything packaged in nixplgs generally rarely fails for me, especially in the more complex cases of cuda&#x2F;pytorch.I suppose I&#x27;m more likely to want \"pytorch except\" where that except is a newer dependency or build flag improving performance. reply bsimpson 14 hours agoparentprevThe Steam Deck is essentially a Chromebook for video games, so ChromeOS&#x27;s unbreakable partition scheme seems like a reasonable idea. reply bsimpson 15 hours agoprevI recently got my hands on a gaming handheld (the Legion Go) and have used it to get more exposure to Linux. I&#x27;d historically avoided it, because it seemed like a perpetual tinker timesink with limited compatibility with things I&#x27;d actually want to use. Reading about immutable filesystems and how traditional Linux gives root willy-nilly to all sorts of random software piqued my curiosity.I&#x27;m using NixOS, which can indeed be a tinker timesink, but is good for exploration. You can easily try different components, and then completely remove them (aside from some ~&#x2F;.config pollution) if you don&#x27;t want to keep them. It&#x27;s also trivial to patch things before you install them (such as adding some kernel patches to make Linux usable on esoteric hardware like a gaming handheld).There&#x27;s a NixOS community called Jovian that&#x27;s reconstructing Valve&#x27;s random SteamOS tarballs into tagged commits on GitHub, which you can browse as if you were a Valve employee. They&#x27;ve made it so you can install your own copy of SteamOS atop NixOS by adding a few lines to your Nix configuration. They&#x27;re clearly Linux experts, and you can see from the source that you&#x27;re getting Valve&#x27;s packages unadulterated, save for simple adaptations like introspecting instead of hardcoding the power button location.So, if you want a pure SteamOS experience without hosting your own mirror of Valve&#x27;s update system (or if you want to be able to browse Valve&#x27;s source without downloading a 3GB tarball), give Jovian a try.Install instructions: https:&#x2F;&#x2F;jovian-experiments.github.io&#x2F;Jovian-NixOS&#x2F;getting-st...Mirrors of Valve&#x27;s source: https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;Jovian-Experiments&#x2F;repositories?type... reply ParetoOptimal 14 hours agoparentI&#x27;m also successfully using Jovian-NixOS on my Steam Deck without issue, highly recommended. reply toxicunderGroov 16 hours agoprevbazzite.gg alsof does this very well. On AMD hardware it did 120hz VRR out of the box and u can alpha test HDR support. reply iotku 11 hours agoparentEven more relevant is that you can \"fork\" Bazzite relatively simply and add any missing packages or configuration you need to your own custom image and let GitHub actions do most of the infra work for youhttps:&#x2F;&#x2F;universal-blue.org&#x2F;guide&#x2F;fork-your-own&#x2F;And yes, you can roll back to previous images as its an \"immutable\" OS as well should issues arise reply bsimpson 14 hours agoparentprevBazzite (and Immutable Linux as a whole) is fascinating.I&#x27;m not deep enough in their weeds to perfectly explain it in a concise HN comment, but it&#x27;s all about having a read-only known-good Linux distro at the root and then layering packages on top, taking much inspiration from server-side containers. It&#x27;s supposed to be both more secure and more reliable&#x2F;reproducible&#x2F;customizable than traditional Linux. You just write in a container manifest which packages you want. When an upgrade comes out, it runs the upgrade, then reinstalls your packages on top. reply goncalossilva 12 hours agoparentprev2023 was the first year I gamed exclusively on Linux according to Steam&#x27;s year in review, including some of this year&#x27;s titles. Most of that was on the Steam Deck or on a virtual machine with GPU passthrough running Bazzite. It is really well made. reply freedomben 15 hours agoparentprevI&#x27;m shocked that Bazzite isn&#x27;t more well known. It&#x27;s exactly what I dreamed about but didn&#x27;t know existed until recently reply jauntywundrkind 14 hours agoparentprevHadn&#x27;t heard of Bazzite.> Bazzite is an OCI image that serves as an alternative operating system for the Steam Deck, and a ready-to-game SteamOS-like for desktop computers, living room home theater PCs, and numerous other handheld PCs.https:&#x2F;&#x2F;github.com&#x2F;ublue-os&#x2F;bazzite&#x2F;Worth visiting the readme even if not interested. There&#x27;s a huge list of included stuff, and a lot of it seems really cool.and helpful (for gamers or streamers mostly). reply denysvitali 11 hours agoprevTIL: SteamOS is based on Arch Linux. This is so cool! reply userbinator 7 hours agoprevliving room PCDo people not use the term HTPC or \"media center\" anymore? reply justinsaccount 18 hours agoprevTIL about RAUC (https:&#x2F;&#x2F;rauc.io&#x2F;) I had been wondering how valve implemented the A&#x2F;B update scheme. reply jamies 16 hours agoprevIf you&#x27;re interested in running SteamOS on a Linux PC, I&#x27;d recommend: https:&#x2F;&#x2F;github.com&#x2F;HoloISO&#x2F;holoiso reply slimsag 12 hours agoparent> No. Not even questionable. If you have an NVIDIA GPU, You&#x27;re on your own. Latest Valve updates for Steam client including normal and Jupiter bootstraps have broken gamepadui on NVIDIA GPUs, and if so, no support will be provided for you.Bummer. This rules out 76% of steam users, according to their hardware surveys. reply rollcat 9 hours agorootparentNVidia on Linux is an unholy mess, and always has been (at the very least since 2004, which is my earliest memory of fighting it). It&#x27;s true even on NVidia&#x27;s own SoC (Jetson).It almost feels like they&#x27;re trying hard to make the experience worse for everyone: users, OS developers, app developers, hardware developers... I don&#x27;t know what to make of it, if you want NVidia you should pick an OS other than Linux (I&#x27;ve heard FreeBSD actually works fine), if you want Linux pick a GPU other than NVidia. reply slimsag 9 hours agorootparentI tried PopOS! recently and nvidia GPU worked 100% fine out of the box, without doing anything. reply attentive 4 hours agorootparentDoes it do video hardware decoding in the browser? - I tried it and it didn&#x27;t do it on nvidia. reply smoldesu 12 hours agorootparentprevThat description is pretty hyperbolic. The SteamOS UI (eg. the Steam Deck-looking part) is very broken on Nvidia right now, but the actual gaming part (eg Proton and the Steam launcher) works fine. If you just want to play mouse-and-keyboard in desktop mode, recent Nvidia cards are generally pretty cooperative. reply slimsag 12 hours agorootparentWell, I&#x27;m not smart enough to know if it&#x27;s hyperbolic but it&#x27;s a pretty damning statement right there in the README. Certainly enough to turn me away from ever trying it on one of my machines. reply smoldesu 6 hours agorootparentYour loss. replybarbariangrunge 16 hours agoprevTangential: anyone have experience with unity and&#x2F;or unreal on Linux these days? Last I checked (2-3 years ago), they technically worked but we’re janky and buggy. Is it improved? reply itsboring 6 hours agoparentUnity has been quite solid for me on Linux lately. It’s mainly just minor annoyances like the project settings window being too small when you open it so you have to resize it, little stuff like that. Nothing that has prevented me from getting the job done. I still prefer to use it on Linux because the glitches annoy me less than, well, using windows.Unreal works okay for me, but I’ve had to submit a few patches upstream to work out some Wayland issues. Other than that, it’s about as bloated&#x2F;buggy&#x2F;slow as it is on windows. Most of the time if I think there’s some Linux-specific issue I’ll open the same project on windows only to discover it was the same. reply calamari4065 11 hours agoparentprevUnity is only a little more janky and buggy than it is on Windows.I had a lot of trouble getting the unity editor working on my steam deck, but that may have been due to using an editor version from 2021 (for unrelated reasons). It seems to behave fine on a normal desktop environment though. reply arminiusreturns 5 hours agoparentprevI moved to Godot and haven&#x27;t looked back. reply barbariangrunge 4 hours agorootparentHow is performance? reply shmerl 4 hours agoprevValve might need some not yet upstreamed kernel features for Steam Deck, but what is the ustpream kernel missing otherwise for gaming? I use it without any issues.As far as I know they also prefer to upstream things in general. I think AMD&#x27;s amd-pstate &#x2F; amd-pstate-epp and related work was kicked off becasue of Steam Deck, but it all went upstream. reply coffeebeqn 19 hours agoprevI was totally blown away by how good Proton is in the post Steam Deck world. I now play Steam games on my Linux laptop almost daily because they “just work” even when the only listed supported platform is Windows reply jorvi 19 hours agoparentI’ll dissent.After hearing people be ecstatic, I thought I’d go full-in on Linux gaming. I have a pretty bog-standard gaming PC that is very Linux-compatible (Intel i5 + Radeon 6800XT) and on there Apex Legends has horrid frame pacing issues, Mirror’s Edge doesn’t work with wireless Xbox controllers. You lose out on a lot of GPU suite features that Windows has. Gnome doesn’t support VRR. Etc etc.There’s so many small issues it’s held me back from deleting my Windows partition. Maybe in a year or two?That said, these things work flawlessly on the Deck. reply COGlory 18 hours agorootparentFYI Valve is primarily deploying KDE, which does support VRR. They can&#x27;t really control what dumb decisions the GNOME folks make.For Mirror&#x27;s Edge, were you using Steam Input? reply jorvi 17 hours agorootparentYeah, except I prefer the cleanliness of Gnome over how scattershot and buggy KDE feels, so I’m SoL. I’ve even looked into launching games into their own little Gamescope instance, but if you don’t run Gamescope as your main window manager, you lose most of its benefits.> For Mirror&#x27;s Edge, were you using Steam Input?Yes. The problem lies in the fact that only the Xone driver properly supports the Xbox wireless adapter, but it doesn’t play nice with Mirror’s Edge. Xpad and XpadNeo do work, but those require USB or Bluetooth.And me having to tweak a million things tells why gaming on Linux still sucks, aside from Deck’s blessed config. I don’t want to deal with a thousand papercuts, I want to boot my system and play. Windows is still closer to that experience than Linux. reply prmoustache 11 minutes agorootparent>Yeah, except I prefer the cleanliness of Gnome over how scattershot and buggy KDE feels, so I’m SoL.You know that linux distros are multiusers&#x2F;multiseats systems right? You can perfectly use Gnome as default desktop and live switch to a dedicated gaming user with kde plasma desktop that is only used to launch games.Kde plasma shoudln&#x27;t be buggy if only used as a game launcher and disable baloo file indexing if you want to limit kde memory usage to minimum. reply OJFord 17 hours agorootparentprev> Yeah, except I prefer the cleanliness of Gnome over how scattershot and buggy KDE feelsBut if it&#x27;s the difference between gaming working or not for you, wouldn&#x27;t you rather use it? Surely you barely interact with it anyway while gaming, only to get into Steam?If this is a machine you use for something else too, you could just have a gaming user that logs in to KDE and your normal user that uses Gnome? reply eropple 17 hours agorootparent> Surely you barely interact with it anyway while gaming, only to get into Steam?I&#x27;m a Linux desktop user and I drop into a game once in a while while I&#x27;m waiting for another meeting or waiting for a build to finish or whatever. My work desktop doesn&#x27;t use VRR (the just-for-games PC uses Windows), otherwise I&#x27;d be in the same boat as &#x27;jorvi because it quite matters to me that games on my desktop integrate into everything else at a passable level. For me, GNOME does a better job of integrating my different activities than KDE (which wasn&#x27;t always the case! I was a KDE3 user for a long time!), so I use GNOME. And it remains an unsolved pain in the ass that the Linux desktop experience isn&#x27;t coherent enough to mean that we should only be thinking about desktop environments if we want to.Coherent, holistic switching between tasks is a thing that people are allowed to want and attempting to convince people that they don&#x27;t is a bad look.> If this is a machine you use for something else too, you could just have a gaming user that logs in to KDE and your normal user that uses Gnome?This is a really sad observation on the state of the Linux desktop. Still. reply bee_rider 16 hours agorootparent>> If this is a machine you use for something else too, you could just have a gaming user that logs in to KDE and your normal user that uses Gnome?> This is a really sad observation on the state of the Linux desktop. Still.It seems like a somewhat odd observation, is it really necessary to have another user to do this? I can easily switch between Gnome, i3, and Sway on my system, I mean that’s going between X and Wayland, no issues… maybe KDE and Gnome have some specific incompatibility though? Odd.Anyway, at least there’s a workaround. If Gnome is a hard requirement, how is Windows even a candidate? reply OJFord 42 minutes agorootparent> It seems like a somewhat odd observation, is it really necessary to have another user to do this? I can easily switch between Gnome, i3, and Sway on my systemOk, sure, it was just the first solution that came to mind. On mine logging in launches straight into sway. I think only in the first session (to allow recovery in case of some related issue) so I suppose I could switch tty and then manually launch whatever DE.But to me I think the odd observation is that it doesn&#x27;t need to be a different user if we&#x27;re talking about still having to log in again anyway.Unless you mean some kind of session saving, swaymsg exit, and then launch the other one? But then you have to maintain whatever session saving (probably different in each) solution and what have you really gained. reply eropple 15 hours agorootparentprev> maybe KDE and Gnome have some specific incompatibility thoughIt&#x27;s a layer down from the DE itself, it&#x27;s the window manager beneath it. GNOME ships Mutter and KDE ships KWin. GNOME is pretty tightly tied to Mutter; KDE is less tied to KWin, but KWin also tends to support shinier features than Mutter does anyway so I don&#x27;t know why you wouldn&#x27;t use it anyway.> It seems like a somewhat odd observation, is it really necessary to have another user to do this?Strictly no, but having to have another login session, period, is bonkers to me. It&#x27;s reasonable to respond to that suggestion with incredulity.> If Gnome is a hard requirement, how is Windows even a candidate?For me, it&#x27;s not. At the moment it&#x27;s inertia, because Windows has legit become the best Linux dev environment I know of with WSL2. I originally switched back to a Linux desktop because I was working on some hardware stuff that benefited from being on a Linux platform, but I&#x27;m certainly not tied to it past that. reply zeta0134 16 hours agorootparentprevHonestly it&#x27;s the reverse for me, but I guess that&#x27;s down to personal preference. \"Gnome\" apps keep updating with the \"new\" GTK style, which means the title bar becomes a conglomeration of a bunch of weird controls, the familiar dropdown menus vanish, everything gets moved into a tiny little hamburger menu and, often, the layout breaks in subtle ways.The calculator app just recently did this, and now I have to type and enter one line of numbers before the text control realizes it&#x27;s too small and resizes itself. That first line of numbers is nearly invisible. Happens again every time it&#x27;s opened.I&#x27;m not sure who decided that desktop apps need to look and feel like touchscreen-first mobile apps, but I don&#x27;t particularly like it. KDE still feels like a desktop environment, so it&#x27;s my strong preference. I&#x27;ll put up with a very slightly less polished experience if it means stuff stops rearranging itself just for the sake of change every couple of weeks.(Aside from KDE, Cinnamon is pretty solid and less feature packed, maybe give it a whirl?) reply jwells89 11 hours agorootparentHamburger menus are among my greatest gripes with GNOME. In apps with any functionality at all they end up being poorly organized junk drawers filled with odds and ends, and because they have to be somewhat short to be effective, functions that don’t fit in them either get buried or cut.What makes this all worse is that GNOME has acres of space reserved at the top of the screen with its statusbar, most of which is empty and doing absolutely nothing. It could house a macOS-style global menubar (as Unity did for fullscreened windows) with room to spare… Though global menubars aren’t everybody’s cup of tea I think many would agree they’re better than the alternative of oversimplified hamburger menus, and they would help achieve the clean look GNOME is going for without so dramatically impeding functionality. reply CamperBob2 15 hours agorootparentprevThe calculator app just recently did this, and now I have to type and enter one line of numbers before the text control realizes it&#x27;s too small and resizes itself. That first line of numbers is nearly invisible. Happens again every time it&#x27;s opened.OT, but I recently started using a Python REPL as a calculator, leaving it open full time in a window. It&#x27;s pretty great. Haven&#x27;t touched an actual calculator, or a calculator app, in weeks. reply Zekio 17 hours agorootparentprevyou could try Xow it supports the wireless dongles for xbox controllers if that is what you are trying to use reply Phelinofist 16 hours agorootparentIsn&#x27;t Xone the new version of Xow and Xow is no longer maintained? reply barbariangrunge 16 hours agorootparentprevJust chiming in to say gnome is wonderful to use and I miss it every time I have to use something else reply Audiophilip 15 hours agorootparentprevMay I ask what \"SoL\" stands for? (Not a native English speaker.) reply bigstrat2003 13 hours agorootparentI&#x27;ve always heard it as \"short on luck\". reply 30 15 hours agorootparentprevShit out of Luck reply ho_schi 17 hours agorootparentprevhttps:&#x2F;&#x2F;gitlab.gnome.org&#x2F;GNOME&#x2F;mutter&#x2F;-&#x2F;merge_requests&#x2F;1154You can install it upon Arch from AUR.Putting that aside:Windows users always find a reason not to switch to Linux because some missing feature. In two years? There will be another new feature or game on Windows. I remember people insisting on using Windows because it support their „3D-Shutter glasses“ or their card from Nvidia. Either you want use Linux or not :)Why are many features initially only available on Windows?First. That is wrong. Important features like cgroups, namespace and containers&#x2F;Flatpak where novelly developed upon Linux.Second? MBAs only look at past numbers. So Windows often get traditional Windows stuff first. You make guess it, innovative companies care about what will be possible in future. Valve for example.The MBA style thinking is also in many consumers. Still buying Nvidia? Because they were faster in the paper sheet? I prefer the cards which works well with Linux, so AMD or Intel. Frames actually generated are more worth than problems with proprietary drivers.PS: Linux has maybe won the war against drives. Seems like Nvidia open most stuff slowly and feature land in the nouveau-module or mesa. A decade to late. I’m already in Team AMD ;) reply luma 15 hours agorootparentYou&#x27;re using the phrase \"MBA thinking\" to mean \"making decisions based on your personal use case and identifying solutions which match\".I&#x27;m not sure how this is a bad thing. I don&#x27;t run Linux to run games because Windows is a better supported platform for running games. I&#x27;m not \"looking at past numbers\", I&#x27;m looking at the situation in front of me as it exists, setting aside my personal feeling on what might have been and instead focusing on what actually exists, today, for the problem I am looking to solve today. reply ho_schi 13 hours agorootparent„Today“…MBA-Thinking.I don’t want a huge problem tomorrow. reply hhh 16 hours agorootparentprevI don’t want to tweak stuff for a week to get a comparable experience playing games to a fresh W11 install. reply ho_schi 13 hours agorootparentI want tweak stuff to make it fit better for me. I don’t want tweak stuff to get it initial usable.Likewise the rationale why people buy the Steamdeck. Other than Windows it just works. And it is tweakable. reply developerDan 16 hours agorootparentprevWhy don’t people like Linux? Because it takes 8 bloody commands to do something as simple as add a new drive whereas Windows you can just open disk utility and format. I bounced off Linux a few weeks ago over this. It’s for people who want to tinker more than actually use the system. reply bisby 16 hours agorootparentThat&#x27;s untrue though. Linux has a disk utility (I use gparted personally). And you can surely do it on the command line in a single command.On Linux you could automate that task. How would you propose automating \"open disk utility and click a few buttons\" on Windows?This is less of a \"Linux can&#x27;t\" issue and more of a \"I quickly know how to do it on Windows after years of experience and I don&#x27;t know how to do it on Linux.\" Linux not being identical to Windows isn&#x27;t a flaw. No one blames you for not wanting to relearn, but pretending like Linux is bad because your Windows muscle memory doesn&#x27;t apply is nonsense. reply developerDan 15 hours agorootparentWhen I Googled how to complete this task I came across multiple results all of which suggesting to use a string of CLI commands. GParted was suggested in some of the results but it wasn’t installed by default on the distro I was recommended (Lubuntu) so I had to punch in even more commands to get it installed. Then after creating the partition it was still unusable until I mounted the drive (which wasn’t clear until after Googling why I can’t use it). Mounting required yet more commands. I did a cursory glance at the GUI buttons on GParted and didn’t see a simple mounting option. If you can’t mount in GParted then my claim still stands that it’s much more effort, and obscure, than Windows which automatically “mounts” the drive so to speak, when you create the partition. reply ParetoOptimal 14 hours agorootparent> GParted was suggested in some of the results but it wasn’t installed by default on the distro I was recommended (LubuntuYou got a less than stellar recommendation based on your desire for parity with ease of use with windows. Lubuntu is a more niche distro aimed at lower resource usage at the expense of the ease of use you are looking for.If you had installed KDE, you&#x27;d likely have explored the start menu and found gparted or typed &#x27;disk&#x27; into search and found gparted. reply iknowstuff 12 hours agorootparentprevI swear the biggest problem with linux is the nerds pushing newbies towards esoteric garbage distros instead of established and widely supported ones like straight up Ubuntu with Gnome. reply bisby 10 hours agorootparentThe biggest problem with linux is definitely finding the right distro. Ubuntu is awful. With their move towards \"snaps everything\" it just keeps getting worse. Canonical is basically Linux&#x27;s Microsoft. A lot of the \"established\" stuff on Ubuntu is just duct tape that actually makes it worse overall. Ubuntu might have an easier getting started experience, but it&#x27;s not a good long term experience. reply ziml77 10 hours agorootparentprevThere&#x27;s still other things that are absurd to do under Linux. Like turning off write caching for removable drives. Unless something has changed in the past couple years, you need to either manually edit fstab per-drive or setup udev rules.(Not write caching for removable drives should just be the default. Windows hasn&#x27;t used write caching on removable drives for 20 years. It also presents a toggle in the drive properties if you really do want it on though.) reply accelbred 16 hours agorootparentprevOn Gnome, you open the disk manager. You click format.In fact a lot of things are easier. On windows, you need a third party tool to install an iso onto a disk. On Gnome, you open disk manager, right click disk, click restore from image. reply freedomben 15 hours agorootparentprevYou bounced quickly then without trying very hard. Gnome comes with a GUI disk utility tool pre-installed that is easy enough for a Windows user ;-) reply developerDan 15 hours agorootparentI was recommended a distro that doesn’t use Gnome (Lubuntu). The system I was working with is very old and some light research made it seem like Gnome is pretty resource heavy. reply chronogram 13 hours agorootparentHow old are we talking about? 10+ years ago I was running Gnome3 on decent hardware of the time, and everything was snappy[0]. Now all the OS software got faster since then, so everything is still snappy on that thing despite that hardware now being old. Similarly that laptop came with Windows 7 and that was snappy, and the Windows 2021 LTSC on it is also snappy[0].0: I care about responsiveness, so I&#x27;ve always disabled animations on every device, so I have no experience if some animations can run at 60fps on some hardware and 30fps on others. reply ParetoOptimal 14 hours agorootparentprevI think a lot of times recommending Lubuntu or other niche distros to first-time linux users is a mistake.Instead one should recommend using KDE or Gnome and turning down all of the graphical settings if needed to improve performance. reply mattl 13 hours agorootparentI think it comes from the idea that you should install Linux to get more time out of aging hardware. reply ParetoOptimal 13 hours agorootparentYeah, but for first time users I maintain it&#x27;d be better to risk potential slowness than \"fast but unacceptable user experience\". reply mattl 11 hours agorootparentFully agree. I don’t really know what the user groups are doing these days? Are installfests still a thing? replyLegitShady 14 hours agorootparentprevto be fair a fresh W11 install still doesn&#x27;t have a usable taskbar or start menu - you have to install explorer patcher to get those back. reply brnt 12 hours agorootparentLooking back, that may have been my switching point: when setting up my distro of choice took less time then windows after a fresh install. Life without package managers, even now that there is chocolatey, is just unnecessary pain. And as a DE, windows had no edge over something like KDE. reply ho_schi 1 hour agorootparentAn operating-system without a package-manager is not maintable.It hard to install, update or remove software. The validation of the system isn’t possible. Aside from the missing security.On Windows changing a setting has become a horrible task for me. Where is it? Why does the search doesn’t find it? Should I try to find the Win32-Dialog from Win2k or is this setting in WinUI3 the same? Why I have to sit in front of an installer wizard an „click through it“?On Steam I can do a file check of a game and it verifies its integrity? My wish. All package-managers on Linux should provide that :) reply hhh 11 hours agorootparentprevboth are perfectly usable reply FirmwareBurner 15 hours agorootparentprev>Windows users always find a reason not to switch to Linux because some missing feature.Because the OS is a tool, not a religious&#x2F;political statement.Therefore I&#x27;ll use it if it works the way I need it and it solves my problem, or not use it if it doesn&#x27;t work the way I want it and ends up creating more problems for me than it solves. Simple. reply Arnavion 16 hours agorootparentprev>First. That is wrong. Important features like cgroups, namespace and containers&#x2F;Flatpak where novelly developed upon Linux.I get your overall point, but the first \"process containers\" code that later became cgroups was merged to the kernel in 2007. Windows came out with the Job Objects API in Windows 2000 (NT 5.0) in 2000. reply jxf 16 hours agorootparentIMO, the Job Objects API was not really suitable to use in production settings; it had many weird edge cases, so although it looked similar to cgroups it often broke in strange and unpredictable ways. reply cma 16 hours agorootparentprev> were you using Steam Input?Steam Input is rapidly becoming the Google Play Services of the desktop linux world. On Steam Deck for a long time you couldn&#x27;t even use the touchpads without the Steam client running. reply bravetraveler 17 hours agorootparentprevDid you try out &#x27;gamescope&#x27;? This is something you find on the Deck but not &#x27;for free&#x27; with Steam on other Linux.I find it helps with pacing. It also supports VRR with a commandline argument, &#x27;--adaptive-sync&#x27;.VRR may need support in the environment to work, I&#x27;m not sure. Sway&#x2F;wlroots does it fine. Presumably KDE does&#x2F;can too since that&#x27;s what the Deck uses in &#x27;desktop&#x27; mode (otherwise, gamescope).edit: I see in another post - you have! Agreed on KDE being scattershot. I hope the Gnome people clear things up for you. I wouldn&#x27;t go so far as to suggest i3&#x2F;Sway, even though I&#x27;m happy with them reply colordrops 16 hours agorootparentCould you provide details on how you got gamescope working with sway? What is the full command line you used? I believe I ran into problems with it conflicting with XWayland or something like that. reply bravetraveler 16 hours agorootparentSure thing! Here&#x27;s an example command line (from Steam): env DXVK_ASYNC=1 SDL_VIDEODRIVER=x11 gamemoderun taskset --cpu-list 0-7,16-23 gamescope -W 3840 -H 2160 -r 160 -o 160 --borderless --fullscreen --rt --steam -- %command%Gamescope has become odd with the introduction of &#x27;--expose-wayland&#x27;.I think the &#x27;SDL_VIDEODRIVER=x11&#x27; part may be key; I didn&#x27;t need this before, but now I often do. Not every game requires it. It&#x27;s weird.It helps if something crashes because \"wayland isn&#x27;t available\". Adding &#x27;|& tee &#x2F;tmp&#x2F;game.log&#x27; is useful for debugging.Pinning (taskset)&#x2F;gamemode stuff left for context. This example gives a game the cache-rich threads on a 7950X3D.Beyond &#x27;--adaptive-sync&#x27;... I believe VRR calls for the feature to be enabled on the output in Sway.See &#x27;man 5 sway-output&#x27;, looking for &#x27;adaptive_sync&#x27; for more info on thatedit: One last note. I&#x27;m on Fedora - the libraries here are so new that Flatpak-based Steam tends to work best. reply ParetoOptimal 14 hours agorootparentprev> After hearing people be ecstatic, I thought I’d go full-in on Linux gaming. I have a pretty bog-standard gaming PC that is very Linux-compatible (Intel i5 + Radeon 6800XT) and on there Apex Legends has horrid frame pacing issuesApex Legends run flawlessly for me, but only on KDE&#x2F;X11 with Nvidia reflex enabled[0].If you are on Radeon though, I bet the problem is your window manager. I have the frame pacing issues on:- hyprland&#x2F;wayland (even with no_direct_scanout = true; and floating game windows) - KDE&#x2F;waylandI also had a weird issue using gamescope as my DM where apex got resized into a tiny frame in the top left that was like 200 pixels or so wide.> That said, these things work flawlessly on the Deck.Likely due to running into these graphics driver -> WM and similar compatibility issues and fixing them. The other performance improvements from kernel changes probably don&#x27;t hurt either.0: Requires unreleased proton-ge build: https:&#x2F;&#x2F;github.com&#x2F;GloriousEggroll&#x2F;proton-ge-custom&#x2F;pull&#x2F;104... reply bee_rider 16 hours agorootparentprevI’m not sure why people are trying to convince you; Linux is free so there really isn’t any benefit to us Linux users or to the Linux developers if you switch…Valve should be the only one that is worried about your opinion here. I think they develop SteamOS as a backup plan, though, in case Microsoft ever starts to take their own App Store seriously. reply freedomben 15 hours agorootparentThat is surely part of the consideration, but certainly not all. Some engineers at Valve (especially the head honcho Gabe Newell) are legit Linux people (Debian IIRC). They believe in it, and I love them for it reply Adverblessly 14 hours agorootparentI don&#x27;t dispute your claims, but I remember very clearly that back then it seemed obvious that SteamOS was a response to the Microsoft Store and a fear that Microsoft would mandate that all software on Windows come from the Microsoft Store.While that was obviously speculation, at least the dates match up (October 26, 2012 for Microsft Store launch and December 13, 2013 from SteamOS launch according to Wikipedia) reply freedomben 14 hours agorootparentAgree based on my memory. I think the Microsoft store threat is what finally tipped the scale. It took it from \"we kind of support linux because we like it\" to \"we support linux because it&#x27;s important business insurance for us in case Microsoft goes Apple (or Xbox or whatever example you want) and monopolizes app distribution on Windows. reply bee_rider 15 hours agorootparentprevAnd it left them well-positioned for the steam deck, I wonder if they were thinking about that when they started steamOS, or if it is just an example of the natural advantage that openness gives you.Anyway, agree—I wasn’t trying to belittle Valve’s motivations, just wanted to include a thought about why they seem to be happy serving both platforms. reply beebeepka 18 hours agorootparentprevCannot relate much. My 5800x3D and 6800XT deliver an outstanding Linux gaming experience. I don&#x27;t play EA games, though. I do play some fast paced shooters that don&#x27;t need VRR since you can manually cap fps to your liking. Also, it was my understanding that gnome has support for adaptive sync.May i ask what driver features are you missing? I only want some decent fan control instead of relying on random scripts off github. AMD has to release some sort of GUI panel for sure. reply cassianoleal 18 hours agorootparent> I only want some decent fan control instead of relying on random scripts off github. AMD has to release some sort of GUI panel for sure.Have you tried CoreCtrl [0]?> My 5800x3D and 6800XT deliver an outstanding Linux gaming experience.I have a 7900XTX and performance under Linux has been at least on par with Windows, sometimes better (though not by much).> May i ask what driver features are you missing?I&#x27;m not GP but I&#x27;d love to see frame gen and stuff like anti-lag and upscaling integrated into amdgpu with some sort of official way of setting it (though looking at Adrenaline it might actually be best if it&#x27;s left up to the community to create the GUIs).[0] https:&#x2F;&#x2F;gitlab.com&#x2F;corectrl&#x2F;corectrl reply tigeroil 18 hours agorootparentprevSimilar specs but run Windows here, part of the reason being that I noticed that the ray tracing performance is just awful on Linux compared to Windows. I found I get slightly better framerates in most games in Linux, but anything that uses raytracing goes from \"just about usable with FSR\" on Windows to \"totally unplayable\" on Linux.I&#x27;m told it&#x27;s better in Mesa 23.3 though, haven&#x27;t tested. reply tigeroil 18 hours agorootparentprevdid you try playing mirrors edge through steam? I ask because steam input really does work wonders reply blizzard_dev_17 19 hours agoparentprevI&#x27;m doing the same. Playing games I bought 10 years ago for the first time. reply gclawes 19 hours agoparentprevThat&#x27;s awesome. How does proton treat anti-cheat software or DRM? reply jcastro 19 hours agorootparentWorks if the developer enables it.For example: Halo Infinite works fine, but Destiny and Call of Duty don&#x27;t. reply jwells89 19 hours agorootparentYep. DRM’d online stuff and VR mainstays (Beat Saber, primarily) are the two sets of games that are keeping me tethered to Windows at the moment. VR games can be played via a Windows VM with GPU passthrough but for DRM’d online games you don’t really have any other option, at least if you don’t want to get banned. reply Cyph0n 18 hours agorootparentI also run a Windows VM for gaming. One thing to note is that some games have (robust!) VM detection checks on launch, so you can’t even run them in the first place. Valorant is one example. reply diggan 18 hours agorootparent> I also run a Windows VM for gamingWhat do you use for the VM? Last time I checked, I couldn&#x27;t find any free&#x2F;FOSS VM tooling that allows me to do GPU pass-through on a Linux Host to Windows Guest. reply SXX 17 hours agorootparentIt seems like you haven&#x27;t looked into it much since it&#x27;s was feasible for last 7-8 years..Linux hosts had GPU passthrough working well before commercial software had such options. Nowadays it&#x27;s just work out-of-box with Virt-Manager that just run QEMU under KVM.It&#x27;s been working for years for 99.9% of games excluding some invasive anti-cheats that ban you for VMs, but there literally only a few games that have issue with virtualization. reply Cyph0n 16 hours agorootparentprevI use Proxmox and GPU passthrough works just fine (via QEMU). Note that Nvidia GPUs have less issues with passthrough, at least last I checked. See this guide: https:&#x2F;&#x2F;pve.proxmox.com&#x2F;wiki&#x2F;PCI_PassthroughBut if you’re running a standard distro, there are guides for most them.* Arch guide (excellent resource, applicable to all distros): https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;PCI_passthrough_via_OVMF* Ubuntu: https:&#x2F;&#x2F;ubuntu.com&#x2F;server&#x2F;docs&#x2F;gpu-virtualization-with-qemu-...* Gentoo: https:&#x2F;&#x2F;wiki.gentoo.org&#x2F;wiki&#x2F;GPU_passthrough_with_libvirt_qe...You can also find countless blog posts and videos on setting up GPU passthrough.One excellent resource for gaming use-cases in particular is this subreddit: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;VFIO&#x2F;Last thing to note is that your motherboard can make the process easier if it has good IOMMU support. Basically, you want a MB that puts your PCI slot in a separate IOMMU group. You can find examples by searching for “(MB name) IOMMU groups”. reply Kerbonut 17 hours agorootparentprevBeat Saber does work on Linux if you can deal with VR on Linux’s quirks. reply jwells89 17 hours agorootparentDoing so requires a SteamVR headset, though. I’m using a Quest 2 which only works well with Windows.Up to date compelling SteamVR options are starting to appear however so I might see if I can move over to one of those in the next year or two. reply Filligree 15 hours agorootparentI haven&#x27;t tried the quest link thing in years, and last I did it dropped FPS a lot vs. using SteamVR. Has that improved? reply jwells89 14 hours agorootparentNot sure either but people with Indexes, Vives, etc seem to be competitive on the Beat Saber leaderboards so any loss can’t be too bad. replybravetraveler 17 hours agorootparentprev* depending on the anti-cheat or DRMThis is true for at least EAC&#x2F;EasyAntiCheat. This covers a lot, sure, but not everything! reply Deathmax 15 hours agorootparentBattleye has integrations with Proton as well, but as with EAC, it&#x27;s opt in by the developer, and not every dev enables it. reply bravetraveler 13 hours agorootparentSure - leads more to my point of... check the games you care about for support. I didn&#x27;t really want to start itemizing these things.Most AC used in the competitive Counter-Strike community isn&#x27;t supported, for example. Only first-party VAC. reply 0x457 16 hours agorootparentprevIt depends on both. Anti-cheat used by Destiny 2 is supported, but Bungie needs to allow it first. reply bravetraveler 13 hours agorootparentIndeed. I meant to imply both if I managed otherwise. This is just a casual warning - Linux gaming is great [as much as the industry allows]I didn&#x27;t really want to start itemizing things. Most competitive Counter-Strike anti-cheat isn&#x27;t supported.I&#x27;ve had some luck with the Windows VM approach instead. One may have to disable quite a lot (ie: Hyper-V enhancements) to truly trick them. reply nhkcode 1 hour agorootparentprevIt varies by game. https:&#x2F;&#x2F;areweanticheatyet.com&#x2F; is an interesting resource for that because they also track announcements by developers about whether or not linux support is eventually planned. reply Pannoniae 19 hours agorootparentprevThose games with anti-cheats are anti-player garbage anyway. You don&#x27;t lose much without them. reply daveidol 18 hours agorootparentDo you mean the presence of anti-cheat software makes them anti-player? Because I’d disagree. It’s a lot of work and expense to combat cheats, but is very much appreciated by many players (when it works) reply kaetemi 18 hours agorootparentOr they could just not trust the clients, instead of throwing the problem over the wall. A lot of these games with fancy anti cheat protection the cheat tools basically just tell the server \"spawn me a vehicle right here\" and the server just does it. Garbage. reply yatac42 18 hours agorootparent> A lot of these games with fancy anti cheat protection the cheat tools basically just tell the server \"spawn me a vehicle right here\" and the server just does it.Citation needed. I&#x27;d be quite surprised if it were common for servers of professional games to trust the client in that sense (i.e. allowing it to decide game logic like what gets spawned where).As far as I&#x27;m aware the most common types of multiplayer cheats are* wall hacks, which you could probably prevent by not sending the client any information about objects that the player can&#x27;t see, but that would require the server to calculate the line of sight for every player&#x2F;object, * and aim bots, which I don&#x27;t think you could prevent at all on the server side since they don&#x27;t rely on the bot having access to any information that the player isn&#x27;t supposed to have. They just rely on the bot being better at aiming. I suppose if you did all rendering server side and only sent the rendered graphics to the client (i.e. streaming), that would make it harder for the bot because it&#x27;d now have to do image recognition to find the target, but that just makes it harder, not impossible. Plus, game streaming wasn&#x27;t well received for a reason and anyway, I don&#x27;t think that&#x27;s what you had in mind when you talked about \"not trusting the client\". reply kaetemi 29 minutes agorootparentLook up BF2. Cheat tools would just disable limits locally on ammo requests, vehicle requests, artillery strikes, and so on. Server didn&#x27;t check anything. It had fancy anti-cheat tech. Which was bypassed by just writing and restoring executable memory changes faster than the anti cheat detected.Things are certainly not always as professional as they appear to be.Visibility test is definitely feasible against wallhacks, it&#x27;s not that expensive.Aimbot is an assist cheat, which technically does not violate the physical rules of the game, so you are right that it&#x27;s more difficult to detect. One solution to detect this class of cheating is to record the player&#x27;s movement, and rely on a combination of outlier scores and outlier movement behavior to detect abuse. It&#x27;s not watertight, but neither are any of these client side anti-cheat detection schemes. reply doublepg23 18 hours agorootparentprevWow, it’s awesome you’ve solved the entirety of multiplayer gaming. Here I was thinking anti-cheating measures was a complex topic but it’s great you’ve elucidated me. reply kaetemi 23 minutes agorootparentWell, it&#x27;s definitely not solved by throwing the problem over the wall and praying that anti-cheat tools aren&#x27;t broken as easily as your game. reply thaumasiotes 15 hours agorootparentprev> Here I was thinking anti-cheating measures was a complex topicIt isn&#x27;t. If you play with people you don&#x27;t know, some of them will cheat. If you don&#x27;t want that, stop playing with strangers. reply j1elo 17 hours agorootparentprev\"just\" is (tongue in cheek) a forbidden word in HN. Next thing you might find yourself claiming is that Dropbox is a worthless idea because it&#x27;s \"just\" FTP.Btw tell me exactly how an aimbot that takes the visuals from the player&#x27;s screen and tilts the player&#x27;s cursor so (or not so) slightly towards identified moving targets, are to be avoided from the server. Modern cheating is already a hard-ass problem to solve, much more so if no client-level monitoring is desired. reply kaetemi 18 minutes agorootparentDropbox is a worthless idea (long term) because it&#x27;s not running on my own server. :&#x27;)And exactly. You cannot detect that with client-side anti-cheat nonsense either. Record on the HDMI and output a fake USB mouse, why not? Botting doesn&#x27;t break the physical rules of the game, so you&#x27;re right that it&#x27;s hard to detect. One \"solution\" is to record player movement on the server and detect outliers in behavior and scores. Not perfect (and also very difficult), but just as unreliable as client-side anti cheat nonsense. reply kuschku 16 hours agorootparentprev> Btw tell me exactly how an aimbot that takes the visuals from the player&#x27;s screen and tilts the player&#x27;s cursor so (or not so) slightly towards identified moving targets, are to be avoided from the server. Modern cheating is already a hard-ass problem to solve, much more so if no client-level monitoring is desired.The very same way that you&#x27;d do it on the client. If I run an aimbot on an nvidia jetson devkit, using HDMI in to get the screen image and USB emulation to send inputs, your anticheat has to do the same work regardless if it&#x27;s on the client or the server. reply j1elo 15 hours agorootparentI think that makes sense; but doing it on the client means that your computer has to do the work for you, thus distributing the load among all clients. Doing it on the server would mean that their machine has to do the work for all players.If we complain about companies being too quick closing up their servers when games are not as successful as they hoped... imagine if those servers were x10 or more expensive, due to that kind of analysis for all players. Companies would be much quicker to pull the plug, I guess. replydijit 18 hours agorootparentprevRealistically some trust has to be in the client, otherwise your game will feel horribly sluggish and the corrections will drive you crazy.I can make a game with full server trust to show you if you like. reply charcircuit 18 hours agorootparentprevThat is the way things are going with cloud gaming. reply MrNeon 16 hours agorootparentprevIf my cheat puts my crosshair on the opponent&#x27;s head automatically what about that information is untrustworthy that would make you throw it out? reply serf 16 hours agorootparentprevit&#x27;s anti-player when it is security theatre, which in 95% of cases it is.When I start a game and I see an Easy Anti-Cheat banner I think to myself \"Great now I can be killed by an aimbot while simultaneously hosting a root-kit voluntarily.\"Why do you think these systems are advertised like that, at the forefront of the game load? It&#x27;s so that the developers create a false trust in the playerbase that they&#x27;re doing their damnedest to prevent cheaters, when the reality is that they paid a small amount of cash to a third party to use a system that does a piss-poor job at everything aside from being a symbol of effort and adding incompatibilities where there shouldn&#x27;t be.eac bypassing is trivial to a laymen, that doesn&#x27;t bode well as a defense against people that have made cheating their hobby.and to be clear : I use EAC as the example because to me it symbolizes the &#x27;security theatre&#x27; side of the effort. Real anti-cheat efforts exist, and those should be applauded. EAC ain&#x27;t it, but it&#x27;s the industry standard... worrisome. reply bigstrat2003 13 hours agorootparentprevI personally would far rather have the occasional cheater than have the game install literal rootkits. It&#x27;s absolutely bonkers that people are willing to accept that. reply ekianjo 18 hours agorootparentprevNot every gamer wants an esport experience to have fun reply aseipp 16 hours agorootparentThere&#x27;s nothing \"esports\" about wanting to avoid wallhacks&#x2F;aimbots in games like Tarkov, Rust, or Destiny, which completely ruin the entire game for every player in the lobby in an instant. It has nothing to do with \"esports\" and everything to do with actually being able to play the game. Do you also think it&#x27;s because of \"esports\" when you&#x27;re forbidden from cheating at a game of chess in person? When my friend plays Rust and gets upset because a flying aimbot hacker raids his base, gets banned, and comes back 1 hour later (buying a hot key off some shady 3rd party site), is he thinking \"Damn, esports is really ruining this game\"? No. The players are expected to fundamentally abide by the same rules. That&#x27;s what a game is.Realistically these days with how expensive most of these games are to run and make, if you do not keep cheaters away it can tank the entire project, e.g. Cycle: The Frontier basically had to shut down because they couldn&#x27;t keep cheaters at bay, in a system that heavily relies on player count to remain healthy and fun. Once the cheating gets bad enough, people stop playing the game, which leads to a death spiral: it starts with bad queue times, which leads to people playing other games, and that spiral further diminishes the playerbase beyond a point of no return. Cycle barely made it 12 months and the result was a multi-million dollar project getting flushed down the drain. reply earthling8118 16 hours agorootparentA kernel level invasion of privacy is required to stop flying players? That doesn&#x27;t sound right to me. Not to mention that apparently it isn&#x27;t working if your friend is witnessing it.So players of those games are sacrificing privacy for no security at all by the sounds of it. reply steveklabnik 16 hours agorootparentprevRIP to the Cycle. It deserved better.I am glad that Bungie is going with fog of war for Marathon. And heck, given the features Marathon is getting, maybe someday Destiny can have those nice things too. We&#x27;ll see... reply Wytwwww 18 hours agorootparentprevI assumed that cheating is way more widespread amongst multiplayer gamers? There is a lot less anonymity in esports and if you get caught and blacklisted.. well you just wasted thousands or tens of thousands of hours.It&#x27;s pretty hard to have fun when the server is full of cheaters. reply barbariangrunge 16 hours agorootparentCheating in single player is sort of like modding reply diggan 18 hours agorootparentprev> I assumed that cheating is way more widespread amongst multiplayer gamers?I mean, hard to call cheating in a multiplayer game the same as cheating in a singleplayer game. The former ruins the experience of others, the latter just affects your own session. Hard to be against cheating in a singleplayer context. reply Wytwwww 16 hours agorootparentI was thinking about casual and professional online gamers (yet somehow managed to leave out a word in comment...). Of course \"cheating\" in single player games isn&#x27;t even a real thing reply ijhuygft776 11 hours agorootparentprevBut does it ever work? Its just a game of cat and mouse.... like all other software, bugs will always be present apparently. reply tapoxi 17 hours agorootparentprevThe Valorant community is incredibly in favor of the Vangard anti-cheat that loads as an early kernel mode driver, and the pro&#x2F;pro-am Counter-Strike scene plays on FACEIT because they have a strong Kernel-based anticheat. VAC, and server-side VACnet just doesn&#x27;t cut it. reply infecto 18 hours agorootparentprevI wish the Linux&#x2F;OSS communities were less like this and more welcoming. reply Brian_K_White 17 hours agorootparentWhy aren&#x27;t you saying that the DRM software is the unwelcoming party?That user would happily play that game, but the game publisher doesn&#x27;t want them.Incredible. reply infecto 17 hours agorootparentThe only thing incredible is how upset people are for pointing out that it’s hostile to tell someone the game they enjoy playing is garbage and is not worth playing because it has anticheat. reply Brian_K_White 10 hours agorootparentBecause it&#x27;s not an attack, meanwhile the accusation is. reply ekianjo 18 hours agorootparentprevDRM is not welcoming by default... sounds like you have double standards reply infecto 18 hours agorootparentYou are conflating ideas. I don’t think it will be a productive discussion to go down the road of anticheat systems and DRM. We can all have opinions that are different.What is productive is calling out hostile behavior and comments that do nothing but hurt the ecosystem. I see these type of strong negative opinions in a lot of areas of the Linux community. “Oh you do X, that’s stupid you should not be using the product like that” reply Brian_K_White 17 hours agorootparentBut it&#x27;s the simple facts.The best possible, most correct, most defensible, most world-improving advice to give for dealing with a user-hostile product or service, is to have the strength of will to reject it and live without it, and live the example to show that it&#x27;s possible and you won&#x27;t die.Or at the very least, it is AT LEAST as defensible a stance as \"The more pragmatic&#x2F;adult approach is to give the bully whatever they want than to go without their product or service\".That philosophy is not remotely automatically more correct or more adult or nuanced or any of the self-serving words anyone typically uses to try to grant their idea more legitimacy than it deserves.Calling the principled stance \"hostile\" is itself hostile.You can phrase it in a way that sounds emotional and shortsighted and jeuvenile, and certainly there are many juveniles who are guilty of that.Never the less, rejecting a bad deal is still fundamentally a reaction not an action, a defense not an offense.The publisher promulgating a user-hostile deal is inarguably the offender, the initial hostile actor.You can decide that the bad deal is tolerable for yourself, but that is entirely your weakness and does not make that policy smarter or more correct than that of those that decline. reply infecto 17 hours agorootparentI genuinely appreciate you proving my point.I am not here debating DRM or anticheat. Simple pointing out that telling someone the game they play is garbage because it uses anticheat does nothing but hurts the Linux ecosystem.You can come up with another essay but I don’t think it disproves what I am saying. Telling someone the game they play is garbage is not increasing the Linux user base. I am sure there will be a retort here, “we don’t want those kind of users or related software”. reply Brian_K_White 10 hours agorootparentWho said \"we don&#x27;t want those kind of users\"? The game publishers are saying that!The people you&#x27;re trying to criticize are themselves only rejecting the software and the publishers that use it, and for a completely explicable and defensible reason, not because it&#x27;s the wrong tribal colors or religion.You are consistently neglecting to acknowledge the basic order of operations and ignoring the initial act and offense in order to focus on a reaction that you don&#x27;t like and to excuse the initial act that you personally don&#x27;t have a problem with.I am saying that you only have the right to say that the deal proposed by drm and anticheat systems is acceptable to you, not to go one mm further to say that anyone else is ogbligated to feel the same, and is in any way hostile or harming the ecosystem or anything like that if they don&#x27;t. reply Brian_K_White 17 hours agorootparentprevnext [3 more] [flagged] infecto 16 hours agorootparentSorry I am not going down this low brow path. We can agree to disagree. I just don’t think it helps an ecosystem to tell people the software they want to run is garbage. reply Brian_K_White 10 hours agorootparentI do. I think it helps the ecosystem more than any other reaction. I&#x27;m not sure we can agree to disagree. I don&#x27;t think you are allowing it, and certainly I am not. replyyjftsjthsd-h 15 hours agorootparentprev> What is productive is calling out hostile behaviorOkay; anti-cheat is user-hostile.> “Oh you do X, that’s stupid you should not be using the product like that”Okay, the thing I want is to use a game that I paid for, play it on the machine I own, and run it without giving it any special privileges (certainly not modifying my kernel). I trust that you will support that and not be negative about the way I want to use it? reply infecto 15 hours agorootparentWhat are you even arguing? I am not here debating if drm&#x2F;anticheat is good or bad.I am saying it’s hostile to tell someone who wants to run software but cannot because of a limitation in the OS that it does not matter because it’s garbage anyway. reply IshKebab 18 hours agorootparentprevDRM is not the same as anti-cheat. reply arendtio 17 hours agorootparentThey are not, but both are symptoms of a consumer-disrespecting mindset.- DRM does not serve the consumer, but the producer.- Anti-cheat only serves the consumer if it is well-designed. However, if someone is able to design a game (technically) well, anti-cheat is unnecessary. And if someone cannot design a game, their anti-cheat is often a disservice to the consumer.I don&#x27;t like either DRM or anti-cheat solutions, not because I am not willing to pay the producers, but because I have been burned too many times by dysfunctional solutions. reply eropple 16 hours agorootparent> Anti-cheat only serves the consumer if it is well-designed. However, if someone is able to design a game (technically) well, anti-cheat is unnecessary.That silly \"speed of light\" thing? Just design better. reply 0x457 15 hours agorootparentThere are cheats today that takes your monitor output and act like a hardware mouse. There is nothing you can do with game design about it. reply IshKebab 16 hours agorootparentprev> However, if someone is able to design a game (technically) well, anti-cheat is unnecessary.Nonsense. It&#x27;s completely impossible to stop cheaters these days, but anti-cheat technology definitely raises the bar. It&#x27;s only \"unnecessary\" if you&#x27;re willing to accept a large number of cheaters.Some anti-cheat stuff definitely goes to far but to dismiss the idea entirely is just naïve. reply aeonik 15 hours agorootparentBack in the day we had admins and communities of people. You&#x27;d get to know people more and establish trust. You could have registered brackets and independent tournaments with manual administration and banning for cheaters.It worked pretty good, but all of that was taken away. reply bitwize 11 hours agorootparentprevThe players of the game are willing to put up with DRM and anticheat in order to get the game. By taking a hardline stance against these, the Linux community is being user-hostile. reply Dalewyn 18 hours agorootparentprevA computer should serve its user. If the user is serving his computer, they&#x27;re Linuxing right but otherwise doing it wrong. reply Wytwwww 18 hours agorootparentWell if cheating is going to make the game almost unplayable the outcome is pretty much the same as you deciding to never install it in the first place due to disliking anticheat systems. So I don&#x27;t really see the problem. reply arendtio 17 hours agorootparentObviously, you haven&#x27;t been in a position where you had to patch the anti-cheat solution yourself in order to play the game you paid for.Well-designed games offer limited potential for cheaters by design. An anti-cheat software can help to eliminate the little potential that is left, but often games are designed without cheating in mind and some anti-cheat software is put in place to solve all the issues that were produced by the bad design. reply mainde 16 hours agorootparentI think that there are very few tasks in competitive multiplayer games that humans perform better than machines[1], I don&#x27;t think your statement holds true unless you exclude a huge amount of game genres or you take all the fun out of them. (E.g. no FPSs or ..FPSs with no aiming?)[1] Unless we&#x27;re talking about captcha solving competitions, for now, maybe. :) reply kuschku 16 hours agorootparentYou&#x27;re right in that, if your server rejects inputs that are too fast, too precise, too robotic to be human, bots will emulate the top-playing humans ever more closely.But the question I want to ask is: Is that a problem?If all the bots and cheaters are playing indistinguishable from high-level real humans, where&#x27;s the harm?Or, to quote Westworld: If you can&#x27;t tell the difference, does it matter? reply mainde 16 hours agorootparentUhm, yes, I think it is a problem because unfairly losing isn&#x27;t as fun as fairly losing or fairly winning. Ignorance about the fairness of a game may work in a few instances but would not scale.You don&#x27;t have to reach pro levels, it often only takes small assists to turn a balanced game on its head, ruining someone&#x27;s experience with a game. Repeat often enough and the userbase will leave, feeling cheated or at least demoralised for being unable to compete or improve.And allowing machine-assists, thus leveling the playing field, turns the game into a completely different one that is (imho) drastically less fun whoever may not be interested in (or may be unable to) running&#x2F;coding their bot. reply 0x457 15 hours agorootparentprev> If you can&#x27;t tell the difference, does it matter?There is a difference in skill level distribution. If everyone playing at a highly skilled player level, then it&#x27;s simply not fun and doesn&#x27;t provide an opportunity to get better.Anyways, playing with cheaters isn&#x27;t fun and if you want to play without them then you need anti-cheat and&#x2F;or game to not be free. reply kuschku 8 hours agorootparentBut not everyone is cheating. There will always be enough players that even if you just match players based on their skill level, you&#x27;ll always have someone at your own level to play with.In fact, I&#x27;d like to see the same bots developed by cheaters be used for NPCs as well. replytyfon 18 hours agorootparentprevEAC has a proton build now so for new games it should work at least. reply deadbunny 17 hours agorootparentOnly if the developer enables it. Most don&#x27;t. reply charcircuit 18 hours agorootparentprevEAC has had a wine build for a long time (over a decade?). That doesn&#x27;t mean games enable it. reply pjmlp 16 hours agoparentprevPity that it solidifies Windows as the top PC gaming OS, that all studios should care about.Let Valve do the needful for running them under GNU&#x2F;Linux, if at all. reply nindalf 16 hours agorootparentYou’re missing the bigger picture. Yes, developers really appreciate that their games work seamlessly on the Steam Deck and Linux with no effort on their part. But there are a couple of knock on effects.One is that developers now a specific hardware + software combo to test their games with. Even if it’s the same build they’re sending out, they’re still testing their game on the Deck and fixing issues, leading to a better (but not perfect) experience for Linux gamers. Here’s a video of Swen Vincke, CEO of Larian studios playing a game released by his studio on the Steam Deck - https:&#x2F;&#x2F;youtu.be&#x2F;kzfEkSGa45k. He’s very pleased and promises to test future games released by his studio on the Deck. And he stuck to that promise - Larian released several fixes specifically for the Steam Deck to make Baldur’s Gate III run better. Linux gamers benefit from that.Second, this increases the % of gamers using Linux. After the Deck’s success in the last couple of years Linux is at 1.91% of the respondents of the Steam Hardware Survey for Nov 2023. Linux was at 1.15% 18 months ago. Doesn’t sound impressive, but if that growth continues and it reaches 3-4%, at that point developers will find shipping native Linux builds more attractive. reply pjmlp 15 hours agorootparentValve adocates are the ones failing to learn from OS&#x2F;2 history, \"it does Windows better than Windows\".Studios don&#x27;t care about native GNU&#x2F;Linux, despite the games being shipped with Android&#x2F;NDK, PlayStation POSIX environment, and the available APIs on Switch OS.All of them much easier than porting from Windows&#x2F;XBox, almost straight ports if coming from Android&#x2F;NDK. reply brnt 12 hours agorootparentHaving a desktop OS was a big thing 30 years ago, but now nobody cares anymore. Who interacts with their OS other than launching browsers or apps based on browsers? Not even most coders these days.OSes are irrelevant these days and having basically libwindows.so these days only underlines that. reply pjmlp 2 hours agorootparentThat is why GNU&#x2F;Linux gaming is stuck on 2%, and needs to emulate Windows APIs.Valve only stresses the relevance of Windows. reply brnt 1 hour agorootparentThe Steam user survey isnt a representative number. An API isn&#x27;t emulated, it&#x27;s implemented, it&#x27;s no more or less native than Windows itself.In various benchmarks Proton is now outperforming Windows. There is no need for it anymore, outside some niche applications. reply pjmlp 40 minutes agorootparentStatistics only matter when they make our case, yeah.I know pretty well how Proton works. replydelfinom 15 hours agorootparentprevValve inventing a portable game runtime that just works on all Linux distros without game studios needing an entire department to handle the dependency hell of Linux NIHisn would solve that issue. reply nhkcode 1 hour agorootparentDoes it actually work though? Ironically my experience is that windows api + proton are a more stable target than anything linux native. Even valve doesn&#x27;t get it always right when shipping linux versions of their own games. See https:&#x2F;&#x2F;steamcommunity.com&#x2F;sharedfiles&#x2F;filedetails&#x2F;?id=30358... for example. reply Adverblessly 14 hours agorootparentprevYou mean the Steam Linux Runtime?From a quick search this is the best description of it I found: https:&#x2F;&#x2F;github.com&#x2F;ValveSoftware&#x2F;steam-runtime reply a1o 15 hours agorootparentprevValve funds SDL development, I think reply smoldesu 12 hours agorootparentprevPity that Khronos never got the support they needed to make cross-platform raster APIs a reality. I mean really, what an enormous and crying shame that a successor to a highly-demanded API like OpenGL never emerged. It&#x27;s really quite sad that users never had a corporate champion to resist the allure of a proprietary graphics API. The stage was set for every modern OS to be unified under a new raster library, but the setting was dashed for a petty buck. Quite a tragedy.Ah well, it&#x27;s funny to see people complaining because it really solos out the OS you&#x27;re using. Windows users have native DirectX, Linux users have near-flawless DXVK, and Mac users... well, Mac users get what Apple gives them, and they have to learn to be happy with it. reply pjmlp 2 hours agorootparentJust like PlayStation, XBox and Switch.Game studios aren&#x27;t FOSS indie devs stressing out about 3D APIs.They are used to specific hardware since the Atari and Magnavox. reply psyclobe 18 hours agoparentprevBottles is the end game for wine style containers and windows games. reply 4ggr0 16 hours agoprev2024 will for sure be the year of the Linux Desktop, and it starts in a couple of hours!!! reply Old-Assumption 18 hours agoprev [–] I wanted to do use SteamOS for our LR PC, our kitchen ambiance PC and our MBR PC but instead installed Ubuntu (upgraded to Kubuntu) then disabled Snap because SteamOS which runs KDE and was a great call by Valve, is built on Arch, a bad call IMHO. reply smoldesu 17 hours agoparent [–] > built on Arch, a bad call IMHO.I&#x27;d be curious to hear why. Arch deserves it&#x27;s reputation for poor stability, but for Valve&#x27;s application with OSTree and immutable root should work fine. For users who don&#x27;t want to tinker, they can receive a quality first-party experience with smooth upgrades. Users that do want to tinker are largely funneled into using Flatpak or AppImage, which are much more stable than AUR packages. reply glitchcrab 16 hours agorootparent [–] Can we please stop with the FUD around Arch and poor stability? It&#x27;s an old meme which will never die, but it has no basis in reality. I&#x27;ve been using Arch on my personal and work laptops for probably 7 years now and the only time it had been a problem has been due to layer 8 issues and doing something stupid. I certainly wouldn&#x27;t be using it for work if it was unstable. reply 0x457 14 hours agorootparentIt&#x27;s because plenty of arch users just copy and paste things from arch wiki and stackoverflow without thinking. reply glitchcrab 11 hours agorootparentAgreed, Arch is not a good My First Linux for sure. I would never suggest it to someone without a decent bit of experience under their belt. reply freedomben 15 hours agorootparentprevIt&#x27;s not FUD. If you stay very light then it is very stable, but the more stuff you add, the worse it gets (gnome extensions anybody?)I love Arch, but it is a demanding mistress. If you get behind on updates, you&#x27;re asking for pain. Also it can be very disruptive to suddenly get a new major version of Gnome that breaks extensions you used, or applications, etc.What we instead should say is not that Arch is \"unstable\" because I agree it&#x27;s not, but rather that Arch requires a lot more care and feeding and if you don&#x27;t do that, it can lead to instability reply accelbred 15 hours agorootparentprev [–] I used Arch for years, and left it due to poor stability. Every time I would try to use an AUR app it would be broken and need re-installing. Sure the non-AUR stuff was mostly fine, but a lot of necessary applictions are in AUR, and AUR is touted as a major selling point of Arch. When there was an issue during a system update, recovering the system was a mess. I also cannot call it stable when you can&#x27;t update one application without updating the rest of the system.I switched to Gentoo and it fixed all the issues I was encountering with Arch, and was more stable. Now I&#x27;m on NixOS, which is far more stable than Arch or Gentoo were.Now, that said, the way SteamOS uses it, I don&#x27;t see any issues. With an immutable system, A&#x2F;B updates, and tested images, the compatibility and update issues are solved. Using flatpak for user applications solves the rest of the noted issues. Would be ideal if I could install with Nix instead of Flatpak, but ran into some trouble there. reply glitchcrab 11 hours agorootparent [–] Counterpoint to this; I have many packages from the AUR and I&#x27;ve never had any issues like you describe with them. Both of our viewpoints are polar opposites but they are only a single datapoint each. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discusses their experience with customizing SteamOS for their living room PC, outlining the difficulties they encountered.",
      "They explain the partition system and update process used in SteamOS, providing instructions on modifying the Linux Neptune repository and obtaining the SteamOS root filesystem image for the SteamDeck.",
      "The article also covers packaging custom software using pacman repositories and updating SteamOS installations. The author concludes by suggesting the need for an update to Valve's EULA."
    ],
    "commentSummary": [
      "The discussion covers various aspects of gaming on Linux, such as customization of OS and preferred development environments.",
      "It explores the challenges of performing tasks on Linux compared to Windows and limitations in Linux gaming.",
      "The conversation also touches on cheating in multiplayer games, anti-cheat systems, stability of Linux distributions for gaming, and the impact of the Steam Deck on Linux gaming."
    ],
    "points": 426,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1704019533
  },
  {
    "id": 38823514,
    "title": "Building a Low-Power Server/NAS: Achieving 7 Watts with Intel 12th/13th Gen",
    "originLink": "https://mattgadient.com/7-watts-idle-on-intel-12th-13th-gen-the-foundation-for-building-a-low-power-server-nas/",
    "originBody": "7 watts idle on Intel 12th/13th gen: the foundation for building a low power server/NAS By Matt Gadient We shall start with a bit of history: 2016: Building a Low Power PC on Skylake – 10 watts idle 2019: 9w Idle – Creating a low power home NAS / file server with 4 Storage Drives 2021: (no write-up) – 11 watts using an Intel i3-10320 on a Gigabyte H470M DS3H Not all my systems have been so successful. In 2022 I measured a couple other systems at 19 watts and 27 watts as part of Curbing the “Gas-Guzzling” tendencies of AMD Radeon with Multi-Monitor . While I did manage to get that 27 watt AMD system down in power some time later, not every CPU/motherboard combo is destined for the 10 watt ballpark. — Before going further, the 7 watt figure for this system was before storage has been added. The 7 watts (measured at the wall) includes: Motherboard (Intel H770) CPU (Intel i5-12400) 64GB RAM SSD (booting Ubuntu Server 23.04) PSU (Corsair) C-States set up in BIOS so that it reaches C8 powertop with auto-tune (which disabled the USB keyboard when the port went to sleep) Note that if I don’t allow powertop to disable the keyboard, I get 8 watts measured at the wall. — Let’s get into detailed specs and component choices. This time around I had the following goals: low idle power reasonable CPU performance for compression able to handle 12 hard drives + at least 1 NVMe capacity to (eventually) convert those 12 hard drives to 6 NVMe + 6 SSD SATA keep costs under control – since a motherboard purchase would be required, try to stick with DDR4 and reuse a CPU I already have. Putting together a new system with the hopes of getting in the ballpark of the 10 watt range *measured from the wall* is often not only a challenge, but a bit of a gamble. Sometimes you just have to take your best educated guesses in terms of components, build your rig, tune what you can, and let the chips fall where they may. Motherboard – ASUS Prime H770-Plus D4 Before I begin, here is a quick look at the motherboard layout. The GREEN CPU-connected slots and ORANGE chipset-connected slots will become relevant throughout this write-up. At the time of writing, widely available consumer options were motherboards in the Intel 600/700-series and AMD 500/600-series. One of my goals above was the capacity for an eventual 6 NVMe drives. Digging into deeper details as to why this can be a challenge (feel free to skip this section)… Problem: There are 0 consumer motherboards with 6x M.2 slots that can all be used at the same time in PCIe mode. On AMD the MEG X570S Unify-X Max *looks* like it does, but check the manual and you’ll find that if you try to populate all 6, the last one has to be a SATA variant. The ASRock Z790 PG Sonic also has 6 slots, but you can only use 5 of them (with a legitimate excuse: they offer a Gen5 NVMe slot but it comes with an either/or caveat). Why This Problem Exists: There are chipset lane limitations on consumer boards. Assuming I want the ability to run all M.2 in Gen4x4 and assuming a manufacturer were actually willing to devote all the lanes to M.2 NVMe slots (they’re not), AMD X570 and Intel B760 would max at three M.2 slots, with AMD B650 and Intel H670/Q670/Z690/W680 managing four. Five M.2 slots is possible on AMD X670 and Intel H770 boards. Six on a Z790 board. Beyond that, extraordinary measures like robbing the main PCIE slot of lanes would be required. If sheer M.2 count were desired, manufacturers could run theoretically run lanes in Gen4x2 or add some Gen3 M.2 slots, but at that point they’ve created a *very* niche product. The Solution: PCI-E to M.2 adapters became necessary. Now when searching for a motherboard, it became a matter if adding the M.2 slots included to any available PCI-E slots capable of x4 or higher. My options were now limited to AMD X570, Intel H770, and Intel Z790 motherboards. Note that while using bifurcation is a possibility on some motherboards to get more than 1 NVMe out of the main PCIe slot, I decided not to rely on it. I decided to go the Intel route for a few reasons: Chipset TDP: 600/700-series Intel chipsets all have a 6W TDP, whereas the TDP of the AMD X670 chipset is pretty high (7w+7w). AMD chipset power consumption has concerned me for a while, as previous X570 chipsets had a TDP of 11w and needed a fan. Chipset Speed: Intel H670/Q670/W680/Z690/H770/Z790 chipsets have a DMI 4.0 x8 link to the CPU. AMD X570/B650/X670 have a PCIe 4.0 x4 link to the CPU. Theoretical throughput on the Intel should be twice as much as AMD (16GB/s vs 8GB/s). I already had 64GB of DDR4 that the Intel system could use. AMD 600-series chipsets are all DDR5-only. I already had an Intel 12th Gen CPU. I’ve yet to see any positive discussion around AM5 power consumption. At all. Update: as I was writing this, news actually came out about AMD 7000-series CPUs burning/bulging where the motherboard socket pins meet the CPU. Yeah, sorry AMD, not this time. So Intel it was. After checking out available DDR4 motherboards on the market, I quickly narrowed options to 2 manufacturers: MSI and ASUS. Don’t care about the board comparisons? Feel free to skip this. The enticing MSI boards were the PRO Z790-P WIFI DDR4 and Z790-A WIFI DDR4. Nearly identical on the surface, except the “A” is a little more premium (audio, rear ports, heatsinks, power phases, etc). Pros/cons: Pro: 4x M.2 (Gen4x4) + 1x PCIE Gen5x16 + 1x PCIE Gen4x4 supports a total of 6 Gen4 NVMe Pro: 2x PCIE Gen3x1 extra Pro: 6 SATA ports Con: Intel 2.5G LAN (known to be problematic and buggy) Con: I’m not a fan of the MSI BIOS Con: My current B660 board that results in higher idle consumption than expected is an MSI. Attractive ASUS options were the Prime H770-Plus D4 and Prime Z790-P D4 (optional WIFI edition). Getting into the TUF, Strix, or ProArt was just too expensive. I’ll start by listing pros/cons for the H770-Plus: Pro: 3x M.2 (Gen4x4) + 1x PCIE Gen5x16 + 2x PCIE Gen4x4 supports a total of 6 Gen4 NVMe Pro: 2x PCIE Gen3x1 extra Con: Only 4 SATA ports Pro: 2.5G Realtek Network Adapter (preferable to Intel 2.5G LAN these days) (see comments) The Z790-P D4 is similar except it has more power phases, better heatsinking, more USB ports, extra fan header, and for our purposes…: +1 PCIE Gen4x4 -1 PCIE Gen3x1 Ultimately the ASUS Prime H770-Plus D4 was about $100 cheaper at the time and is what I chose. One upside I’ve found with “cheaper” boards is they tend to have fewer components and thus less vampire power drain at idle, though this isn’t always a certainty. CPU – Intel i5-12400 (H0 stepping) – Alder Lake I already had this CPU as part of a previous desktop build. At the time it was chosen for the desktop system because: it had AV1 hardware decode it had the highest performance available from the Intel lineup of the 12th generation that avoids the E-core silicon overhead in that build, I was getting a new motherboard with 2xDP anyway, and going older-gen didn’t make sense to me. That desktop build turned out to be a disappointment, and ranks as one of my least favorite builds. Some details… I had issues where sometimes only 1 of 2 DP-attached monitors would wake in Linux which meant I had to either pull/reconnect the other DP connector, or manually suspend/resume the system so it could try again. Another issue was that rebooting between Windows/Linux sometimes caused odd issues which necessitated a full poweroff/restart. Hardware decode on Ubuntu using Wayland is still problematic and when programs tried to use it to play video, problems would ensue. Finally, unlike my previous Intel systems which could all be brought down near the 10 watt mark, this one was idling at 19 watts, though I suspected the MSI motherboard I was using may have been a factor. Most of the headaches I experienced were related to the GPU and display. Since I was about to build something server-oriented, that was no longer a factor. MEMORY – 64GB DDR4-3200 Here’s what I used: 2x16GB Kingston HyperX dual-rank (Hynix DJR) 2x16GB Kingston HyperX single-rank (Hynix CJR) This was memory I already had. I ran the 4 sticks of memory at the XMP profile of the dual-rank kit which was 16-18-18-36. Everything else was essentially left to the defaults except that I ran the RAM at 1.25 volts (higher than stock 1.20, but lower than the XMP 1.35v setting). TestMem5 and Memtest86 showed stability at 1.22v, though testing this memory on previous motherboards had shown 1.22v to be unstable, so for a little extra buffer when it comes to stability I boosted the voltage to 1.25v. Boot Drive – Sandisk Ultra 3D 1TB SSD This component wasn’t deliberately chosen. When I wanted a fresh Ubuntu Server install for testing, this happened to be the only SSD I had kicking around that wasn’t currently being used. I was going to be doing a lot of A/B testing on PCIE and NVMe devices, so installing Ubuntu 23.04 to a SATA SSD made sense to keep PCIE slots free. Note that after testing, the main OS was to be run on a Samsung SSD 970 EVO Plus 500GB NVMe. Not much to say except that Samsung stuff tends to reliably go into low power modes. Having used both drives, I can’t measure any power difference between them in my testing. Tom’s Hardware tested the Samsung idle at 0.072 watts (via ASPM/APST), and Anandtech tested the Sandisk Ultra 3D idle to be 0.056 watts (via ALPM). Both are well below the 1W resolution of my Kill-A-Watt meter. PSU – Corsair RM750 As much as this 750W PSU may appear to be overkill for a system intended to sit around 10 watts, when 12 drive motors spin up at the same time, the instantaneous load is likely to be quite high. Seagate states 2A/3A DC/AC peak currents on the 12V rail for one of their 10TB 3.5″ drives. Even peak random read/writes can clock in at over 2A. This bursty power demand has the potential to be problematic if the PSU isn’t up to the task. If an array of 6 drives collectively pull 150-200 watts at the same moment the CPU spikes to pull a peak 120W, that’s a jump from around 10 watts idle to around 400 watts. This could easily cause an instantaneous voltage dip – if it dips enough to cause an immediate crash/reboot it’s probably not a big deal, but if it dips just enough that data is corrupted during a memory refresh or when another drive is mid-write… that’s a more painful problem. Oversizing the PSU to some degree (or adding some in-line capacitors to the power rails) makes sense. Fortunately, despite operating outside of the peak efficiency range, much of the Corsair RM series is pretty efficient across a wide range. Power Measurements – Initial A few important bits: Power measured from the wall Intel PowerTOP was used to auto-tune settings Ubuntu Server 23.04 A few potentially-important BIOS bits: CPU C-states were enabled in the BIOS (C10) ASPM was enabled with everything set to L1 RC6 (Render Standby) enabled Aggressive LPM Support enabled (ALPM) DISABLED: HD Audio, Connectivity Mode, LEDs, GNA Device, Serial Port 9-10 watts was the consumption when the display output was on. 7 watts was the consumption once the display turned off (consoleblank=600 kernel boot parameter for a 600s timer), which is where this system sits most of the week. 8 watts was the consumption if the USB keyboard power management was disabled. If you don’t SSH into the server from elsewhere, spending the extra watt for keyboard use might be necessary. Problematic Power Measurements – Loaded up with spinning rust (spun-down) As mentioned in the beginning, I started with 12 hard drives. Half were 2.5″ and the other half were 3.5″. Because the motherboard only has 4 SATA ports, a SATA controller and a port multiplier were used to handle the remaining drives. Additionally, 4 NVMe drives were used early on: one of them, a Western Digital SN770 had a tendency to get quite hot even at idle which indicates it probably wasn’t going into a low power mode. With all the equipment connected, at idle, with display off, and with the 12 drives spun down to standby, I was shocked to see that my idle power consumption had gone from 7 watts all the way up to a whopping 24-25 watts. Far too much! Something was amiss. Power Consumption Puzzles – High Power Investigation and Diagnosis I disconnected the hard drives and started testing components one at a time. These were fairly crude tests meant to get a rough idea as to the culprit, so numbers here aren’t precise. I quickly discovered that the JMB585 SATA controller I was using caused power consumption to increase by something in the 6-10 watt range (precise measurements in a later section). The controller itself is only supposed to take a couple watts, and the tiny heatsink stayed cool, so there was obviously more going on. Where was the power going? I decided to watch the CPU package C-states. Without the JMB585 SATA controller, the system hit C6. When the JMB585 was reconnected, the best the system hit was C3. Ah ha! But why? Turns out that if a PCIE-connected device won’t go into ASPM L1, the CPU won’t go into as deep a sleep. The JMB585 controller cards don’t seem to have ASPM support. A little further experimentation revealed something else that I hadn’t known, and it has to do with C6 vs C8. The system will only hit C8 if there’s nothing hooked up to the CPU-attached PCIE lanes. In other words, if anything is plugged in to the top PCIE slot or the top NVMe slot, C6 is the maximum. The power consumption difference between C6 and C8 *seemed* to be less than a watt in a simple test. So while C8 would be a luxury, hitting C6 was a must. C3 uses too much power. If SATA controllers were going to prevent the CPU from hitting the best power saving states, I started to wonder whether I should have been looking for a motherboard with 6-8 SATA ports so that I wouldn’t have to rely on add-on controllers… A little searching for SATA HBAs showed that while there aren’t many options here, the ASM1166 SATA controller should support ASPM L1, though the firmware has to be flashed for it to work properly (and to work at all on newer Intel boards). This was something I’d have to order: I have Marvel and JMicron spares, but they don’t support ASPM. I’d actually been avoiding ASMedia for years, but out of necessity they were now getting another chance: I ordered a couple ASM1166 6 port SATA controllers. Aside: BadTLP, Bad! AER Bus Errors from the pcieport Worth a mention… During initial testing with a WD Black SN770 (Gen4 NVMe), I found a problem when the primary (top CPU-attached) PCIE and NVMe ports were used. Running dmesg resulted in output littered with stuff like: pcieport 0000:00:06.0: AER: Corrected error received: 0000:02:00.0 nvme 0000:02:00.0: PCIe Bus Error: severity=Corrected, type=Physical Layer, (Receiver ID) pcieport 0000:00:06.0: PCIe Bus Error: severity=Corrected, type=Data Link Layer, (Transmitter ID) pcieport 0000:00:06.0: AER: Error of this Agent is reported first nvme 0000:02:00.0: [ 6] BadTLP …after much trial-and-error I found that if the “PEG – ASPM” BIOS setting was set to [Disabled] or [L0s] there were no errors. Of course, this was a bad option, as [L1] is crucial for power savings. If [L1] or [L0sL1] were used, the only option was to set the Link Speed of those ports to [Gen3], which didn’t stop the errors, but reduced them substantially. Some research showed the root cause can be any number of things. Because swapping the motherboard or CPU wasn’t a pleasant thought, my best hope was swapping to a different brand of NVMe. I ordered some Crucial P3 NVMe drives. This turned out to be a successful endeavor: with the WD drives replaced by the Crucial drives, I was no longer getting any errors, though keep in mind these are Gen3 drives. Power Consumption Puzzles – Finding L1.1 and L1.2 to be enabled on chipset-connected ports only When I had the 2 Crucial P3 NVMe drives installed in the CPU-connected PCIEx16 slot and the top M2 slot, I noticed higher idle temps than expected. While the NAND sat at about 27-29C, the controllers were reporting 49-50C – much higher than I expected for these particular drives. I moved the one from the PCIEx16 slot to a chipset-connected PCIEx4 slot. An interesting difference between these drives showed up via lspci -vvv: CPU-connected M2 slot: L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1- Chipset-connected PCIE slot: L1SubCtl1: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1 sub-states only seem to get enabled on the chipset-connected slots. Unfortunate, but it does seem to coincide with the available BIOS settings in the screenshot above. Let’s reference that motherboard picture again to show the situation: I put both NVMe drives on chipset-connected PCIE slots. Now both showed L1.1+/L1.2+ and both controller temps were down from the 49-50C range to 38-41C. Unfortunately when attempting various A/B tests using these 2 Crucial NVMe drives with different slot configurations and various BIOS settings, I saw very inconsistent behavior in terms of temperature, though it’s worth noting the JMB585 and an NVMe boot drive were also connected during these tests. For example, both drives might idle at around 40C until a soft reboot at which point 1 (or both) might now idle at the 50C range. Sometimes it seemed possible to keep 1 drive on the CPU-connected M.2 and retain 40C temperatures on both drives as long as the x16 slot wasn’t populated. Presumably I was hitting some sort of bug. The Samsung boot NVMe seemed to keep a consistent idle temperature regardless of what was happening with the Crucial NVMe drives, so I suspected the Crucial drives themselves are at least partly to blame. Interestingly, sometimes one (or both) controller temps would drop all the way down to the 29C range when on the chipset-connected slots. Since trying to find a low-power 4TB NVMe replacement for the Crucial P3 wasn’t a realistic goal, my best hope at this point was that the ASPM-incompatible JMicron JMB 585 was somehow to blame, since it was soon to be replaced with the ASPM-compatible ASMedia ASM 1166. Late Update: I unfortunately didn’t keep track of temperatures throughout the rest of the testing, and heatsinks/airflow between drives have all been jumbled around. But for whatever it’s worth, In the final build, my Crucial P3 controller temps are 31-34C, and NAND temps are 23-24C. Power Consumption Puzzles – Swapping from the JMB585 to the ASM1166. After a couple weeks the ASM1166 arrived. First a couple bits regarding the card which you might find helpful if you’re considering it… I began with a firmware flash – ASM1166 cards often have old firmware which doesn’t work with Intel 600-series motherboards and from what I understand can have issues with power management. Newer firmware can be found floating around in various places, but I decided to grab a copy from SilverStone (“fix compatibility issue” in the Download section of https://www.silverstonetek.com/en/product/info/expansion-cards/ECS06/) and followed the instructions at https://docs.phil-barker.com/posts/upgrading-ASM1166-firmware-for-unraid/ . Note that the SilverStone files had an identical MD5 to firmware I found by following the thread at https://forums.unraid.net/topic/102010-recommended-controllers-for-unraid/page/8/#comment-1185707 . For anyone planning to purchase one of these ASMedia cards, I should note that like most SATA controllers and HBAs out there, the quality really varies. One of my cards had a heatsink that was on a bit crooked: the thermal pad was thick enough to prevent it from shorting nearby components, but be aware that these products can be really hit-and-miss. This is one of the situations where paying a little more to buy from somewhere with a good return policy can be prudent. I did quite a bit of A/B testing, so here is a quick “JMicron JMB585 vs ASMedia ASM1166” in terms of total system power consumption, though it may only be applicable to this platform (or perhaps even this specific motherboard). DRIVELESS First, power consumption without any drives connected to the cards (the SATA SSD boot drive is connected to the motherboard) to get a baseline. PowerTOP used on all devices except for the keyboard (adding +1 watt). Measurements after the display output went to sleep. 8 watts – No SATA controller – C8 power state 9 watts – ASM1166 on a chipset-connected x4 slot – C8 power state 12 watts – JMB585 on the CPU-connected x16 slot – C3 power state 15 watts – JMB585 on a chipset-connected x4 slot – C3 power state 22 watts – ASM1166 on the CPU-connected x16 slot – C2 power state The ASM1166 does well here if plugged into a chipset-connected slot (only +1 watt), but does horribly if connected to the main PCI-E slot (+14 watts) where the CPU package power state plummets to C2. Shockingly, the JMB585 behaves in an opposite manner where it’s consumption is lower on the CPU-connected slot (and it didn’t cause C2) – however, you’ll soon see that things change when drives are actually connected… I did additional testing with the controllers, including playing “musical chairs” with a couple NVMe drives to see if multiple devices would throw a wrench into things, but nothing unexpected took place so I’ll skip those details. ADDING DRIVES With baseline measurements complete, next it was time to actually put some drives on these controllers. The SATA SSD boot drive stayed on the motherboard, 2 NVMe drives were added to the mix (chipset-connected unless otherwise noted), and 4 of the 2.5″ SATA hard drives were placed on the controller. I’ll list the “spun down” consumption after the hard drives went into standby – “spun up” was exactly 2 watts higher in every test while the drives were idle. 10 watts – ASM1166 on a chipset-connected x4 slot – C8 power state 11 watts – ASM1166 on a chipset-connected x4 slot with 1 NVMe moved to the CPU-connected x16 slot – C6 power state 11 watts – 2x ASM1166 on chipset-connected x4 slots, with only 1 NVMe drive – C8 power state 16 watts – JMB585 on a chipset-connected x4 slot – C3 power state 24 watts – JMB585 on CPU-connected x16 slot – C2 power state With 4 drives connected via a chipset-connected slot, the ASM1166 adds +2 watts to system power consumption, whereas the JMB585 adds +8 watts. No contest. An additional benefit is that I was able to use both of the ASM1166 cards in the system, whereas attempting to use both of my JMB575 cards at the same time resulted in the system refusing to boot, though that could be a platform or motherboard-specific issue. There is a trade-off though – I always found the JMB585 to be rock-solid reliable, including when paired with a JMB575 port multiplier. My past experience with ASMedia SATA controllers has been less than stellar: reliability with the ASM1166 remains to be seen, but at the very least it’s a bad candidate for a port multiplier since it doesn’t support FBS (only CBS). A couple other minor hiccups that presented with the ASM1166: When removing/reinserting the NVMe boot drive, a BIOS message appeared claiming that it couldn’t boot due to GPT corruption. The ASM1166 cards had to be temporarily removed for the BIOS to “find” the NVMe boot drive again (after which they could be reinstalled). The ASM1166 cards claim to have a *lot* of ports – this causes additional boot time as Linux has to iterate through all of them. Update: SATA and SSD Brands One of the comments mentioned an older Samsung 840 PRO SSD limiting to C3 whereas a Crucial Force GT SSD allowed C8. While those are older drives, I still found this a bit surprising. It was worth investigating. I used the H770 as a testbed with a Samsung 850 EVO SATA SSD boot drive along with a Crucial P3 NVMe and built a custom kernel to allow the Realtek network adapter to reach L1.2. No ASM1166, just using the Intel onboard SATA. I reached C10 after running powertop with auto-tune and allowing the display to sleep. I then added a Patriot P210 SATA SSD. Stuck at C6. I removed the Patriot P210 and tried a 4TB 2.5″ Seagate SATA HDD. C10. Swapped it for an 8TB 3.5″ Seagate SATA HDD. C10. Swapped that for a 14TB Toshiba SATA HDD. C10. Added a 1TB Sandisk Ultra 3D SATA SSD to the Toshiba HDD. C10. Later test: a 4TB Crucial MX500. C10. The lesson here: we clearly need to be picky about SATA SSDs. Feel free to leave a comment with good/bad ones you come across. Power Consumption Puzzles – Conclusion A few important bits if aiming for low consumption: 1) Motherboard support and BIOS configuration are critical – I’ve had motherboards with very inflexible BIOS’s. On this one, “Native ASPM” and the appropriate L1 states must be enabled (to allow OS-controlled instead of BIOS-controlled) for low power consumption to work. 2) Devices all need to support ASPM L1. Otherwise you’re really rolling the dice. The hardest part here as you might have guessed is finding SATA controllers that support it – if possible, get a motherboard enough sufficient Intel chipset-connected SATA ports to avoid needing a separate card. I should note that finding NVMe drives that have working low-power APST power states under ASPM isn’t always a given and you’ll want to do some research there too. 3) If you can hit the C8 power state, avoid using CPU-attached PCIe lanes (top PCIe and M2 slot). On this specific motherboard, my advice would be to avoiding using them altogether if you can, unless you either need the low-latency full-bandwidth path to the CPU or your devices are so active they never sleep anyway. Recall that BOTH my JMicron and ASMedia SATA cards caused the CPU Package C-State to plummet to C2 if plugged into the x16 PCI-E slot. 4) Measuring power from the wall is the only way to make sure that what you *think* is happening is actually happening. A Kill-A-Watt device will pay for itself over time if you use it – consider that I bought mine in 2006 ($16USD + $14USD shipping at the time through eBay). At that time I found our rarely-used fax machine which was always powered on used 7 watts… just keeping that one device powered off when unused during the next 10 years more than paid for the Kill-A-Watt. Power Consumption when loaded up with a bunch of HDDs Now that a variety of parts have moved in-and-out of the system throughout this process, the current setup is as follows: 1x Samsung 970 EVO Plus NVMe (500GB boot drive) 2x Crucial P3 NVMe (4TB each) 5x Seagate 2.5″ HDD (5TB each – 4TB utilized) 6x Seagate 3.5″ HDD (10TB each – 8TB utilized) 2x ASM1166 cards providing SATA ports Total power measured from the wall (display on, keyboard enabled): 50 watts with all 11 HDD in active-idle 38 watts with the 6x 3.5″ HDD in Idle B 34 watts with the 6x 3.5″ HDD in Idle C 21 watts with the 6x 3.5″ HDD in Standby_Z (spun down) 18 watts with the 5x 2.5″ HDD ALSO in Standby 16 watts with the display output ALSO off 15 watts when PowerTOP is allowed to disable the USB Keyboard Seagate rates standby consumption of these 3.5″ drives at about 0.8w each, and the 2.5″ drives at about 0.18w each. This lines up with what I’m seeing above. My active-idle numbers actually match up pretty well to Seagate specs too. The obvious observation: compared to the rest of the system components, the 3.5″ drives are power-hungry monsters. The HDDs will eventually be replaced with SSDs. With idle consumption as low as it is during HDD standby, there isn’t a major rush and this process will gradually take place as my HDD drives/spares fail and SSD prices fall. The plan for “end game” is for an all-SSD build. Originally the plan was for 1 boot drive, 6xNVMe (likely Crucial P3 4TB) for a RAIDZ2 array, and 6xSATA (likely Samsung 870 QVO 8TB) for the 2nd RAIDZ2 array. Since using the CPU-connected M2/PCIe slots not only brings unpredictability but also comes at a slight C-state/power/temperature cost, I might alter that plan and give up a couple NVMe in the first array and use SATA instead so that I don’t have to touch CPU-connected lanes. Time will tell. Unnecessary Storage Details This part is only worth reading if you’re interested in meticulous details about the storage. Feel free to skip to the final section otherwise. NVMe boot drive As alluded to earlier, this is a Samsung 970 EVO Plus. Currently less than 4GB of the 500GB space is used (a 64GB swap partition exists but always sits at 0 used). It was originally chosen because Samsung had developed a reputation for reliability (which has been falling by the wayside lately), and Samsung also scored well in reviews every time it came to idle power consumption. This drive is almost always idle and both Controller and NAND temps stayed low throughout all testing (20-24C). It may eventually be swapped to a SATA SSD to free up an NVMe port. 2.5″ HDD These drives are used for the primary 6-drive ZFS RAIDZ2 array – the one that gets the most use. One day a week it’s busy with a task that involves reading a few TB over the course of 24 hours. Usage through the rest of the week is sporadic, and the drives spend most of the week spun down. For anyone wondering why piddly 2.5″ drives are used instead of 3.5″ drives, there *is* a reason: power consumption. Power consumption of the 2.5″ Seagate drives is honestly pretty impressive. Spun down they’re each rated at 0.18w, in low power idle they’re rated at 0.85w, and the read/write averages are rated at about 2w. There are plenty of SSDs out there with worse power consumption numbers than this spinning rust. 5TB capacity gives a lot of storage-per-watt. The major downsides to these 2.5″ Seagate drives are: Not great performers. 80-120MB/s peak read/write. To be fair though, many TLC/QLC SSDs fall to these write levels when their SLC cache is exhausted. SMR (Shingled Magnetic recording). Reads are fine, but write performance absolutely plummets when random writes take place – it acts like a QLC SSD without an SLC cache that also doesn’t have TRIM. Low rated workload (55TB/year vs 550TB/year for 3.5″ Exos drives). No configurable error recovery time (SCT ERC), and these drives can hang for minutes if they hit an error while they relentlessly try to re-read the problematic sector. Ubuntu needs to be configured to wait instead of trying to reset the drive after 30 seconds. Higher error rates if they heat up (I’ve had to replace a few and have discovered they don’t like running hot). Typical HDD pain points (slow to spin up, etc). To be absolutely fair to Seagate, these are sold as external USB backup drives. Pulling these 15mm tall drives out of the enclosures and using them as RAID members in a NAS isn’t exactly using them as intended. The ultra low power consumption is tremendous, but there are obvious trade-offs. Long term, these 2.5″ 4/5TB drives will slowly be replaced by 4TB SSD drives (possibly all NVMe). SSDs in 4TB capacity started to become available on the consumer end in 2021/2022 at about 4-5x the cost of the spinners. Less than 2 years later they’ve dropped to about 2x the cost, and I expect decent brands to last more than 2x as long as the Seagate spinners. If availability of the Crucial P3 (Gen3) model remains, I’ll likely keep with this model despite being limited to Gen3 speeds. I strongly considered the Crucial P3 Plus (Gen4), but power consumption in reviews was higher despite very few situations where performance was notably higher as well. My biggest concern with the P3 Plus (Gen4) was that if I had issues with ASPM/APST, Tom’s Hardware showed it with a 0.3w idle power premium over the P3 (Gen3) for the 2TB model. I prefer “worst-case scenario” power to be as low as possible. 3.5″ HDD Used in the secondary 6-drive RAIDZ2 array – a backup array that’s spun up for about 2 hours a week where it receives constant heavy writes. Power consumption of the 3.5″ Seagate drives is about what you’d expect. These 10TB drives are rated at about 0.8w each in standby, 2-5w idle, and 6-9w reading and writing. Two concerns here: These are rated to collectively pull about 45-50 watts when writing. That’s a bit of extra UPS load I don’t really want if a lengthy power outage takes place during the backups (I stick with consumer 1500 watt UPS’s). These are rated to collectively pull about 4.8 watts when in standby. Again, some UPS load I wouldn’t mind shaving off. Long-term these drives will likely be replaced by Samsung 870 QVO 8TB SATA drives. The 870 QVO sports 0.041w/0.046w idle with ALPM, 0.224w/0.229w idle without, and 2.0-2.7w during a copy (according to Toms/Anandtech). Price-wise, the Samsung 8TB SATA SSD is currently a fair bit more expensive than 8TB spinners (closer to 3x the cost) so unless these drives start to see more frequent use for some reason, replacement with the SSDs will almost certainly wait until I’ve run out of spares. NVMe Cache Drive Replacing my spinning rust with SSDs is a process that will likely take a while. In the meantime, ZFS has a couple options to make use of high-speed storage (typically SSD) in front of slower storage: “Special” Allocation Class – allows you to create a vdev specifically for metadata and for “small” blocks if desired. A cache drive, known commonly as an L2ARC. If you create the “special” vdev at pool creation, all your metadata (and optionally, small blocks of a size you choose) will go on the “special” vdev instead of your spinning rust. Very fast file listings and directory traversal whilst keeping the spinning rust for the files themselves. Yes, you can “ls” a bunch of directories without waking your HDDs from sleep. Biggest downside is that because all your metadata is on this vdev, if it ever dies, access to all your data is essentially gone. So it really should be at least mirrored. Maybe even a 3-way mirror. Say goodbye to a few ports. The L2ARC is a bit different. It’s essentially a level 2 cache. When the cache in RAM gets full, ZFS will copy some of the blocks to the L2ARC before it evicts that content from RAM. The next time that data needs to be accessed, it’ll be read from the L2ARC instead of the disk. One benefit compared to the “special” vdev is that you’re fine with only 1 SSD – if there’s a problem with the data in the L2ARC (bad checksum, drive dies, etc), ZFS will just read the content from the original disk. Also, once the L2ARC is full, ZFS will just start again at the beginning of the L2ARC SSD and overwrite stuff it wrote before which has some pros (old data never accessed anymore) and cons (data that was frequently accessed and will need to get written to the L2ARC again). You can also add/remove L2ARC devices from the pool at your leisure – want to add a 64GB SSD, 500GB SSD, and 2TB SSD? Go right ahead – ZFS will distribute blocks among them. Need to remove the 500GB SSD from the pool a few days later and use it elsewhere? Go right ahead. The biggest downside to the L2ARC is that if you forget to specify “cache” when adding the device, you probably mucked up your pool. It’s also imperfect: even with careful tuning it’s hard to get ZFS to write EVERYTHING you want to the L2ARC before it gets evicted from memory. At the same time, depending on your data, the L2ARC may see a lot of writes, and you may have to carefully watch the health of your SSD. In the past I’ve used the “special”, used L2ARC, and have used both at the same time (you can even tell the L2ARC not to cache things already contained in the “special” vdev). This time around I simply went with an L2ARC on a 4TB NVMe: once all the other 2.5″ drives have been replaced by SSD and the speed benefits of an SSD cache no longer apply, I can simply remove this cache device (though theoretically having 1 L2ARC cache drive handling the bulk of reads *would* allow the other NVMe drives to stay in low power mode more…). Conclusion – Regrets? Second-guessing? What could have gone differently? Unlike the ASRock J4005 build where I realized part way through that I’d kneecapped myself in a number of ways, I don’t get the same sense here. This time I ended up with low idle power AND a pretty capable system that should be flexible even if repurposed in the future. I’m quite happy with my component choices, though I’d be curious to know how the MSI PRO Z790-P DDR4 (one of the other motherboards I considered) would do in comparison. Functionality-wise the MSI has the advantage of 6xSATA ports, but it comes with the obvious downside of the notorious Intel 2.5G networking chip. The MSI also has a PS/2 port and I’ve never actually checked to see if PS/2 keyboard power consumption is lower than USB (recall that I save 1 watt if I allow powertop to shut down the USB keyboard port). And of course it would be interesting to compare the ASPM and ALPM settings, and to see if the snags I hit with CPU-attached PCIe/M.2 slots exist in the same way. While this system currently sits in the 15-16 watt range when idle with drives in standby, once all HDDs are replaced with SSDs, I’d expect idle consumption of around 10-11 watts which isn’t bad for 72TB worth of drives, 64GB of RAM, and a pretty decent processor. Update: Recent Linux kernels disable the L1 power saving modes of most Realtek NICs which prevents the CPU from entering decent C-states, thus increasing power consumption by quite a lot. While there are workarounds, moving forward I’ll likely limit myself to motherboards containing Intel 1 Gigabit network adapters (perhaps moving to Intel 2.5 Gigabit when it becomes clear they’ve worked out all the kinks). You can find further details about the Realtek NIC situation in the comments below. 66 CommentsLeave a Comment Sort by OldestSort by Newest Anonymous on May 14, 2023 - click here to reply Hi Matt, great piece! I guess a lot of headaches could have been avoided if you found a board with more sata ports on it! On my end I never managed to get my chip beyond C3. I purposefully tried to reduce the amount of excess components (like those SATA controllers, I'd read about how hit and miss they could be). I'll double check my BIOS settings to make sure that I've enabled all the relevant things you mentioned in your piece. Geert on May 22, 2023 - click here to reply Hi, Very interesting article, many thanks. So you don’t care about ECC, some say it’s a must for an always on server especially with ZFS. Also NVME’s seem to burn more fuel than SSD’s. I am looking for a frugal ECC motherboard but did not find anything yet, W680 boards are hard to get. In the meantime I am running Unraid on a J5040 Asrock board with two 1TB SSD’s in mirror and 3 mechanical WD’S that are sleeping most of the time. The system burns 19 watt at idle, its was 16-17 watt (C6) before adding an Asmedia Controller (4). I will replace the old seasonic PSU by Corsair soon. Regards Geert Hamun on July 4, 2023 - click here to reply What OS did you use ? Matt Gadient on July 4, 2023 - click here to reply Ubuntu Server 23.04 Anonymous on August 9, 2023 - click here to reply Amazing article Matt. This has inpired me a lot. Since there's no write up, what do you think about Gigabyte H470M DS3H with i5 for low power low profile media server with 30-40TB media? Matt Gadient on August 9, 2023 - click here to reply I actually used it for a period of time as the NAS. As a media server the CPU would lack hardware AV1 decode, but aside from that I suspect it would be fine. Keep in mind that if the 2nd M.2 slot is populated, only 5 of the 6 SATA ports will work. If I recall correctly the BIOS on the H470M DS3H also hid a few options (like forcing IGPU or dedicated GPU) unless put in CSM mode. Additionally it would randomly hang on the boot screen with an error if I had an LSI SAS HBA installed, necessitating another restart attempt - regular SATA controllers worked fine though. Putting aside those weird little nuances, I found it to be reliable and it worked great and I quite like the board. Robert on August 15, 2023 - click here to reply Hi Matt, thanks for the interesting read. I am trying to minimize power consumption on a NAS system with two 3,5'' HDDs and that is also running the OS and some virtual machines on two SSDs. With a Intel J4205 board I and 2 WD Red 6 TB the system crashes a few minutes after I set the HDDs to sleep. By crashing I mean everything is off and I need to repower the system. Did you ever encounter something like this? The system is running normally at 25 W, with the HDDs powered down it is at 15 W. Power supply is some 250 W I had flying around here. Is it possible, that the ATX power supply switches off due to small load? Matt Gadient on August 15, 2023 - click here to reply Some older power supplies shut off if load is too low (some BIOS's actually have a setting for a dummy load to combat this). Some really old power supplies go out of voltage spec if load on a rail is very low. Power line fluctuations can be more problematic at very low loads as well. Robert on August 22, 2023 - click here to reply Small update, I ordered a new 300W in the 50€ range. Fun fact, the power consumption is 1 W less than before, either due to higher efficiency or because the cooling fan is running less. The low power use case is also fine now. Ahmed on August 30, 2023 - click here to reply ECC could be important to have for a system running 24/7 and handling important NAS data (regardless of using ZFS or not it is still a nice feature to have ECC for NAS). Do you plan to publish a similar article but for a system with ECC support and low idle power consumption that would still be compatible with Linux (I think low idle power consumption from AMD is not the best for Linux as an example). I am planning to make such a build soon myself and I would like to know if I should start making my build in the next month or two or maybe wait a bit to read your publications which would provide some useful in sites that can help me make a better educated decisions on the components choice. Nevertheless, thank you very much for the thoroughly written article. You did an impressive job here highlighting the important parts of building a very efficient low power NAS. Matt Gadient on August 30, 2023 - click here to reply Nothing planned in the short term in regards to ECC. I generally just stick to Kingston memory and run it rigorously through Memtest86 and TestMem5 before putting it into use. Were it possible to get ECC in a current low-power low-cost platform I'd go for it, but for me it would be more of a nicety than a necessity. In any case, best of luck with your build! Olivier on September 6, 2023 - click here to reply Hi Matt, Thanks for your very detailed and informative NAS articles! I'm putting mine together with an i3-10500T, 16GB and 4xHDD. For the power supply, I found an Antec EarthWatts 380W EA-380D (80+ Bronze) reconditioned for 25€. Is it worth it in your opinion? or is it too old? If you have another model to recommend, I'd love to hear from you. Thanks in advance. Matt Gadient on September 6, 2023 - click here to reply I normally buy the Corsair RM or SF series these days due to their very good efficiency at idle. But those are a bit expensive. I've always liked the EA-380D power supplies a lot (though I recently gave one of my last ones away), so if that sits nicely in your budget I'd say go for it. xblax on September 11, 2023 - click here to reply That article helped me to decide for a B760M-K D4 mainboard with i3-1200 for my home server upgrade after seeing here what low power consumption is possible. I upgraded from a FM2A88M-HD+ with AMD A4-4000 and was able to reduce idle power from 40W to 15W, which means the new hardware will basically pay for itself in a couple of years. I also got a 970 Evo Plus (2TB) as the boot drive and can confirm that it must be connected to the chipset in order to reach low package C-States (C8). What I found interesting is that the difference between package C3 and C8 was much bigger when then SSD is connected to the chipset. I believe that's because the chipset itself will only go into deep sleep states when all attached devices support ASPM and SATA Link Power Management is active. Connecting the SSD to the CPU PCIe did only increase power consumption by ~2W (Package C3 vs C8), while having not having ASPM on a device connected to the chipset seems to take additional 5W just for the chipset but has the same effect (C3) to the package C-State. One interesting thing worth noting is that I have a PCIe 1.1 DVB-C Capture card connected to the chipset. Even though ASPM is listed as a capability for the card by lspci and I booted the kernel with pcie_aspm=force it didn't get enabled for that card. I had to force-enable ASPM via setpci, see https://wireless.wiki.kernel.org/en/users/documentation/aspm - seems to work without issues. That helped me reaching that 15W idle power. Unfortunately the capture card still takes ~5W, otherwise I currently only have 2x4TB HDD from Toshiba connected which spin down when idle. Btw. Sata Hot Plug must be disabled for all ports, or otherwise the package will only reach C6. danwat1234 on September 15, 2023 - click here to reply Looks like you aren't a fan of USB-connected drives, could have used a hub or two and some enclosures. Good writeup! Anonymous on September 22, 2023 - click here to reply Hey, Super great article, thanks for all these informations. I’m planning building my nas. As the power consumption is the main topic, what do you think about the following build (but I’m kinda a noob about the system and what’s possible and/or the limitation of such a low tdp chip) ? Asrock N100M micro ATX (with the new Intel® Quad-Core Processor N100 (up to 3.4 GHz) with a 6W tdp. As there are only 2 sata ports, the idea is to add a SAS HBA card with 8 additional SATA ports on the 1 x PCIe 3.0 x16 Slot. For the storage it would be 1 M2 (the one from the motherboard) for TrueNas OS, 2 SSD mirroring sata for VM, docker, … and 8 HDD Seagate EXO 7200 rpm drives as a final step (2 at the beginning and then evolving based on the need). For the power supply, a Seasonic Focus PX 550W - Modular 80+ Platinum ATX and finally a unique stick of 32GB of ram (non ECC). Many thanks in advance Matt Gadient on September 22, 2023 - click here to reply I've actually considered the N100 recently, which seems to be the latest darling of the mini PC world. Only 1 memory channel on the N100 boards, but for the majority of situations where memory bandwidth isn't critical that's perfectly fine. The biggest issue I've found over the past few years is these ASRock onboard-CPU boards have gone up in price to the point where a cheap motherboard + CPU is often within reach, along with more PCI-E lanes, more onboard SATA, and similar power consumption as long as you can reach high c-states. But I'd snap up the ASRock N100 quickly if the price were right. Note that the x16 slot runs at x2 so you'll max out at 1GB/s throughput on a PCIe 2.0 card, and 2GB/s on a PCIe 3.0 card - unlikely you'd hit those speeds under normal usage anyway across a bunch of HDDs but it's something to be aware of on these boards. On the SAS HBA card, I'd suggest looking around to see what idle power consumption others are seeing on the specific card you're considering: the popular ones often pull a few watts while doing absolutely nothing. Not sure how *BSD handles the cards, but of the few that seem to have ASPM enabled by default, Linux eventually seems to disable it in the kernel at some point due to issues. That said, this is a situation where the ASRock N100 might fare better than a separate CPU/motherboard combo as I'd expect it to be less sensitive to c-state implications of an expansion card, though this is just a guess based on what I saw with my ASRock J4x05 boards and may not apply to N100. The Seasonic PX 550W looks like a great choice. Overall looks like a solid build! paldepind on September 23, 2023 - click here to reply Thanks for a great post full of helpful information. Do you have any tips for identifying motherboards that can achieve low power usage? People sometimes recommend ITX motherboards but I haven't found any measurements about how many watts ITX vs ATX usually saves. Now, ITX wouldn't have worked for this build, but ATX doesn't seem to have been a significant source of power consumption anyway. In general, it seems very hard to figure out which motherboards are power-efficient and which are not? What do you mean with \"the E-core silicon overhead\" and why did you try to avoid it? I understand the CPUs with E-cores are probably more complex, but I would've thought that the E-cores could lead to lower power usage when the CPU is doing non-intensive tasks at low load. Again, thanks for the great info. I hope to be able to build a system with similar power efficiency. Right now I have a Gigabyte Z790 UD AX motherboard and an i5-13500 system that I can not get below 28W. Matt Gadient on September 23, 2023 - click here to reply In terms of low power motherboards, my general rule of thumb is that lower component counts tend to result in lower power consumption. This is not a robust rule, but it usually holds up well enough here. A quick \"sniff test\" is looking at the number of power phases (something manufacturers advertise heavily): lots of phases running at high switching frequencies are great for hard core overclockers, but for low power we want few phases switched at such a low frequency that if it the motherboard has a MOSFET heatsink it's mostly decorative. The advantage to ITX is that it tends to limit the component count, but it's not strictly necessary - last week I actually repurposed the \"Intel i3-10320 on a Gigabyte H470M DS3H\" I mentioned at the beginning and got it down to 6 watts idle (headless, no keyboard, onboard Intel i219V 1GbE network only, c-states in BIOS, 3 Samsung SATA SSDs 840/860/870, Corsair RM850 power supply, Ubuntu Server with powertop). It's a very utilitarian motherboard. I won't do a separate write-up because the board is no longer available, but 6 watts on that MicroATX Gibabyte H470 board and 7 watts on the ATX ASUS H770 board in this write-up are my best 2 results so far and notably neither were ITX. Something else I just noticed: both these boards only have 6 power phases. As to the \"E-core silicon overhead\", a lot of details can be found at https://www.hwcooling.net/en/the-same-and-yet-different-intel-core-i5-12400-duel-h0-vs-c0/ , but I'll try to summarize. The i5-12400 comes with 6 P-cores and 0 E-cores enabled, commonly referred to as 6+0. However, it came in 2 variants: a \"C0\" stepping which was originally an 8+8 that had cores fused off to become a 6+0, and an \"H0\" stepping which was manufactured directly as an 6+0 and never had any E-core hardware inside to begin with. In the tests (page 5 of that article), the C0 used up to 16 watts more power than the H0 depending on the benchmark, including almost 11 watts more at idle. Now it's always possible their C0 sample had other contributing issues causing power leakage, or that there's some other variable at play, but either way the 2 chips that had physical E-Cores hardware inside didn't fare well in the idle test. Because I focus on extremely low idle consumption for most of my systems, I can't justify buying any of the combined P/E-core chips until I see some data that shows chips with E-cores doing under 10 watts idle. And I simply haven't yet. This is an area where Intel is very much under threat these days: the AMD Mini PCs are now getting down to about 6-7 watts idle power consumption for a Ryzen 9 7940HS ( https://youtu.be/l3Vaz7S3HmQ?t=610 ) and if AMD brings this type of APU design to the desktop side or someone like ASRock starts to package some of these impressive HS chips in a custom motherboard, Intel could quickly lose the low-idle-power market. paldepind on September 27, 2023 - click here to reply Thanks a lot for the great reply 🙏. There's not a lot of info out there on this kind of stuff, so you sharing your knowledge is very valuable and appreciated. I can see that the motherboard I bought is probably not ideal (it advertises a lot of phases). Load more comments (46 more to see!) Leave a Comment You can use an alias and fake email. However, if you choose to use a real email, \"gravatars\" are supported. You can check the privacy policy for more details.",
    "commentLink": "https://news.ycombinator.com/item?id=38823514",
    "commentBody": "7 watts idle – building a low powered server&#x2F;NAS on Intel 12th&#x2F;13th genHacker Newspastlogin7 watts idle – building a low powered server&#x2F;NAS on Intel 12th&#x2F;13th gen (mattgadient.com) 289 points by ryangibb 21 hours ago| hidepastfavorite151 comments sandreas 15 hours agoThere is a german forum thread with a google docs document listing different configurations below 30W[1]. Since there are very different requirements, this might be interesting for many homeserver &#x2F; NAS builders.For me personally I found my ideal price-performance config to be the following hardware: Board: Fujitsu D3417-B2 CPU: Intel Xeon 1225 V5 (better the also compatible 1275v6, but its way more expensive) RAM: 64GB ECC RAM (4x16GB) SSD: WD SN850x 2TB (consumer SSD) Case: Fractal Design Define Mini C Cooling: Big block no name, passively cooled by case fan Power: Pico PSU 120W + 120W Leicke power supply Remote Administration via Intel AMT + MeshCommander using a DP Dummy PlugI bought this config used VERY CHEAP and I am running Proxmox - it draws 9.3W idle (without HDDs). There are 6 SATA ports and a PCIe port, if anyone would like to add more space or passthrough a dedicated GPU.It may be hard to get, but I paid €380,00 in total. Does not work very well for Media Encoding, here you should go for a Core i3 8100 or above. Alternatively you could go for the following changes, but these might be even harder to get for a reasonable price: Boards: GIGABYTE C246N-WU2 (ITX), Gigabyte C246-WU4 (mATX), Fujitsu D3517-B (mATX), Fujitsu D3644 (mATX) Power: Corsair RM550x (2021 Version)Cheap used Workstations that are good servers are Dell T30 or Fujitsu Celsius W550. The Fujitsu ones have D3417(-A!) boards (not -B) having proprietary power supplies with 16 power pins (no 24pin ATX but 16pin). There are Adapters on Aliexpress for 24PIN to 16pin (Bojiadafast), but this is a bit risky - I&#x27;m validating that atm.Ryzen possibilities are pretty rare, but there are reports that the AMD Ryzen 5 PRO 4650G with a Asus PRIME B550M-A Board is drawing about 16W Idle.Hope I could help :-)[1]: https:&#x2F;&#x2F;goo.gl&#x2F;z8nt3A reply kogepathic 8 hours agoparent> The Fujitsu ones have D3417(-A!) boards (not -B) having proprietary power supplies with 16 power pins (no 24pin ATX but 16pin). There are Adapters on Aliexpress for 24PIN to 16pin (Bojiadafast), but this is a bit risky - I&#x27;m validating that atm.They work just fine. The pinout is well known [1]. You can also adapt a normal ATX PSU if you boost 5VSB to 11V.Fujitsu boards are great, and very inexpensive to purchase in the EU. Someone has even reverse engineered the license for the KVM features of their remote management (iRMC S4&#x2F;S5) [2][1] https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20200923042644&#x2F;https:&#x2F;&#x2F;sector.bi...[2] https:&#x2F;&#x2F;watchmysys.com&#x2F;blog&#x2F;2023&#x2F;01&#x2F;fujitsu-irmc-s4-license&#x2F; reply sandreas 7 hours agorootparentOh this is pretty interesting, thank you very much. You mean that the Bojiadafast adapters work fine?If so, I wonder if there is a hit in efficiency because of the required step-up &#x2F; step-down converters in that adapter.However, on my board there seems to be an unsoldered 24pin connector, that could be just used as is with a little soldering, but since it is on-hold my replacement system if my ...-B variant dies, I&#x27;m not willing to risk too many experiments :-) reply manmal 13 hours agoparentprevFor anybody reading this - I think it’s a great config, but would be careful around pico PSUs in case you want to run a bunch of good old spinning disks. HDDs have a sharp power peak when spinning up, and if you have a couple of them in a RAID, they might do so synchronously, potentially exceeding the envelope. reply sandreas 7 hours agorootparentYou&#x27;re right. 6 spinning disks and a discrete GPU won&#x27;t work with 120W. I would recommend to get the Corsair RM550x 2021 (or similar) when using a NAS with more than 2 disks or a discrete GPU.I personally don&#x27;t need more than 10TB space, so 2 10TB Seagate EXOS in ZFS mirror would just work fine with 120W as long as you don&#x27;t run Prime95 the whole time. You might go up to Pico PSU 150 then. reply agilob 12 hours agorootparentprevto go deeper, depending on a file system, some FS won&#x27;t let HDDs go to sleep, so they always consume power for max RPM. reply scns 9 hours agorootparent> some FS won&#x27;t let HDDs go to sleepWhich ones? reply agilob 26 minutes agorootparentbtrfs and zfs at least, probably most frequently used FSs for RAID reply ksjskskskkk 12 hours agoparentprevb550m with a amd5 pro from 2023 (will double check models and post on that forum)i get 9w idle and amd pro cpus have ECC support which is a requirement for me on any real computer. i disable most components on the board. it&#x27;s bottom tier consumer quality.best part is when i need to burn many more watts the integrated gpu is pretty decent reply Jedd 9 hours agorootparentThe same way backups are most rigorously performed by people who&#x27;ve lost data, ECC is a non-negotiable requirement for people who&#x27;ve suffered slow data corruption via silent memory failures.It surprises me that people are happy with 64GB+ builds of non-ECC, especially for NAS (ie. very long term storage, where corruptions probably wouldn&#x27;t be noticed for years).Periodically I look at replacing my small fleet of HP micro Gen8&#x27;s, which use Xeons, have 4 x 3.5\" bays (with proper h&#x2F;w RAID1), but max out at a frustratingly low 16GB. They&#x27;re quite robust, but because of their age - and HP - a horse veterinarian approach to component failures is usually indicated.A Ryzen + ECC whitebox build is massively appealing, but almost everyone&#x27;s build-out includes caveats like &#x27;check the datasheet of the mobo and CPU&#x27; (because series aren&#x27;t consistent), and about half the time a terrifying disclaimer to the effect of &#x27;ECC is present &#x2F; enabled in the kernel, but I can&#x27;t tell if it&#x27;s actually functioning properly&#x27;. reply harshreality 3 hours agorootparentOlder AGESA (part of the mobo firmware that handles system initialization) versions had a bug that prevented the chipset from recognizing and utilizing ECC ram properly even though the chipsets should support it. Check any motherboard in question for a firmware update which includes at least AGESA 1.0.0.5 patch C.https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;truenas&#x2F;comments&#x2F;10lqofy&#x2F; reply sandreas 7 hours agorootparentprevThis. Additionally, there is the price point of modern AMD \"workstationish\" systems vs. used Intel \"real server\" systems. Efficiency together with more performance is great, but if I had to pay double the price for a system that offers lots of performance I won&#x27;t need anyway I&#x27;m not willing to pay the price.Used Intel Systems are just way cheaper, because nobody seems to want them anymore... Everyone wants a >= 8 core ryzen :-) I personally don&#x27;t need this for a little Proxmox &#x2F; NAS kind of system.However, I&#x27;m hoping for frame.work to announce official ECC support[1] on their Notebook boards (pretty likely this will never happen). I would love to just use the coolermaster case with small efficient modern notebook hardware for 600 bucks just because I don&#x27;t need additional harddisks and I would just buy one to support the company.[1]: https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;ecc-error-correcting-code-def... reply vardump 9 hours agorootparentprevIndeed. ECC is non-negotiable, even for \"consumer\"-grade servers. Heck, also for workstations. Less&#x2F;no mystery software malfunctions. reply akvadrako 7 hours agorootparentprevWhat type of ECC is a requirement? All DDR5 has on-die ECC but it&#x27;s hard to get real ECC with error reporting. Since errors are pretty rare (about 1 &#x2F; year per 100GB) and you sacrifice to get it, seems like a hard choice. reply amarshall 7 hours agorootparentOn-die ECC is not ECC memory. On-die ECC only corrects certain errors (e.g. not transport errors), and is absolutely necessary with DDR5 else such errors would be intolerably frequent.True DDR5 ECC memory exists, and is not hard to find—just look for “server” RAM. reply wtallis 6 hours agorootparentI wish JEDEC and the memory manufacturers had not decided to present on-die ECC as being a \"feature\" of DDR5 memory, when it has far more to do with the generation of the fab process than the generation of the memory interface. reply ksjskskskkk 3 hours agorootparentprevanecdotal: run the exact same system and tasks on desktops with ddr4 ecc and laptops without ecc ddr5 (they technically have amd pro apu and sodimm sockets, but i can&#x27;t find the damn memory to buy)the laptops see two unexplainable crashes per 6 mo. reply bb88 10 hours agorootparentprevThis is the setup I was looking for, the ECC support is really a requirement for a NAS. reply CommanderData 10 hours agoprevGreat hardware but when the software is a job to administer I have a hard time justifying builds like these.My Synology NAS for example has 8 GB RAM and a J4150 processor. Runs about 15 containers, Wireguard and on top DSM (which is Synology&#x27;s OS). Usually idles around 1-3%.Software makes all the difference - DSM has been by far the biggest benefit and surprise to me and I would be deprived of time with anything else. I&#x27;m running TrueNas as a second backup server but it no way compares to DSM. Sometimes I don&#x27;t want to trawl through logs and trial and error just to get a basic CRON setup to backup a file off another server, there&#x27;s countless examples where DSM has just worked.I really think Synology is missing a trick here, they clearly have software that is miles ahead of everything else and customizable should you need to. They should be more like Microsoft of the NAS world, making DSM run on non Synology platforms, or at least making it easier to do yourself. It&#x27;s a great OS and it sells itself and can easily be a way to up sell stuff like Active backup for business. reply walterbell 7 hours agoparenthttps:&#x2F;&#x2F;xpenology.org&#x2F;> Xpenology is a bootloader for Synology’s operating system, called DSM (Disk Station Manager), and is used on their NAS devices. DSM is running on a custom Linux version developed by Synology ... Xpenology creates the possibility to run the Synology DSM on any x86 device like any PC or self-built NAS. So, you can benefit from the powerful multimedia- and cloud features of DSM without buying the hardware NAS from Synology. Many people prefer this because they can pick out their own (more powerful) processor and RAM to handle things like transcoding video. reply CommanderData 2 hours agorootparentMy point about Synology being the MS of NAS is actually about this. Spend a little development time making DSM more portable but with a caveat of no support on non-Synology hw.I think we&#x27;d still arrive at a weird place where we see people buying Asustor or other NAS hw just to run DSM even if it&#x27;s not supported and but not a complete hack.People buy convenience and DSM offers everything NAS related it really well.I suppose they have internal plans to make DSM go the other way and lock out attempts like Xpenology. They aren&#x27;t late either as other rival is still miles behind. reply matthewfcarlson 9 hours agoparentprevI suspect focusing on a set of hardware and making it work really well is part of what makes the OS so good. Hardware configs explode in a combinatorial fashion, making it impossible to test everything once you have more than a few options. reply thelittleone 8 hours agoparentprevAgree, its a super nice UI and UX. First time I used them was around 2010 and it was already a joy to use. Would be kind of nice to have a Synology DSM front end to cloud IaaS. reply ksec 3 hours agoparentprevMy biggest problem with Synology continues to be their Kernel version being very old. They were still shipping version 4.4 this year and only the new product this year gets version 5.10.And you dont get Kernel version upgrade in between DSM. reply Dalewyn 2 hours agorootparentIs that really a problem as long as Synology supports it (we&#x27;re paying money, after all)? reply ksec 1 hour agorootparentSo far it hasn&#x27;t but it does mean we dont get up to date BTRFS version. Which is what worries me most. reply buro9 1 hour agorootparentAre you sure? Synology do a very bespoke BTRFS and backport a lot, even though it&#x27;s incredibly complex to do so. Their BTRFS is not standard, or rather it&#x27;s not the standard version for a given kernel version. reply intrasight 6 hours agoparentprevI think of them as the Microsoft of NAS. Everyone I know with a NAS uses Synology. Most because I suggested it. reply agumonkey 8 hours agoparentprevdsm being diskstation manager ? reply CommanderData 1 hour agorootparentYes, it&#x27;s the OS that ships with their hardware. DSM 7 being their latest. reply Dalewyn 6 hours agoparentprevI deeply appreciate Synology handling away all the Linux jank I would otherwise have to deal with myself. Easily worth the price tag of buying one of their NASes.Specifically, I have a DS1520+ with five 16TB Seagate Iron Wolf Pro HDDs in a RAID6 config (have another, sixth identical HDD as a cold spare in the closet) and it has been running absolutely flawlessly for the now two years I&#x27;ve had it.My track record with Linux installations otherwise is \"I keep killing them by just breathing on them, god damn.\". I&#x27;m a walking Linux genocide horror show. reply CommanderData 1 hour agorootparentConvience is nice and something they do really well. Rocksolid for me for 3+ years. A recent issue with a my failed cache drive. Within 2 days the issue triaged and investigated by their devs. The experience was great. reply jepler 11 hours agoprevAuthor seems to have built 5 systems from 2016 to 2023, or around every other year.Some parts (e.g., RAM) are re-used across multiple buildsIt&#x27;s interesting to wonder: How much $$ is the hardware cost vs the lifetime energy costs? Is a more power-hungry machine that would operate for 4 years better than one that would operate for 2 years?The motherboard + CPU is USD 322 right now on pcpartpicker. At USD 0.25&#x2F;kWh (well above my local rate but below the highest rates in the US), 36W continuous over 4 years is also about $315. So, a ~43W, 4-year system might well be cheaper to buy and operate than a 7W, 2-year system. reply buro9 1 hour agoparentI average a fully loaded NAS every 8 years apparently. I don&#x27;t really expand their storage, as by this time the network and software is out of date, so I buy a new one and make the current the backup to the new, and then dispose of the one that was the old backup.It&#x27;s interesting, the upgrade interval has held for 20 years already and so it becomes easy to understand the amortisation, 16 years of use, but really only 8 as the primary and 8 as the backup. reply hrdwdmrbl 10 hours agoparentprevFun tinkering like that is not about saving money. It&#x27;s about the journey, not the destination. reply 1-6 10 hours agorootparentAt least the author documents their journey well. If many people who read this save energy by learning good fundamentals of system build, that alone goes a long way. CPU&#x2F;mobo makers are also probably taking notes. reply ThatMedicIsASpy 15 hours agoprev7950X3D, X670E Taichi, 96GB 6400MHz CL32, 2x4TB Lexar, 4x18TB Seagate Exos X18, RX570 8G, Proxmox.Idle no VM ~60-70W.Idle TrueNAS VM drives spinning ~90-100W.Idle TrueNAS & Fedora Desktop with GPU passthrough ~150WIn a few weeks the 570 is replaced by 7900 xtx. The RAM adds a lot of W. 3-5W per 8GB of RAM depending on the frequency is common for DDR5.I was expecting around 50-100W for Proxmox+TrueNAS. I did not consider the power draw of the RAM when I went for 96GB. reply MrFoof 14 hours agoparentYou can go down to 50W idle, but it requires some very specific hardware choices where the ROI will never materialize, some of which aren’t available yet for Zen4.I have…* AMD Ryzen 7 PRO 5750GE* 128GB ECC DDR4-3200* Intel XL710-QDA2 (using QSFP+ to a quad-SFP+ passive DAC breakout)* LSI 9500-16i* Eight WD 16TB HDDs (shucked)* Two 2TB SK Hynix P41 Platinum M.2 NVMe SSD* Two Samsung 3.84TB PM9A3 U.2 NVMe SSD* Two Samsung 960GB PM893 SATA SSDSo that’s the gist. Has a BMC, but dual 40GbE and can sustain about 55GbE over the network (in certain scenarios, 30-35GbE for almost all), running TrueNAS scale purely as a storage appliance for video editing, a Proxmox cluster (on 1L SFFs with 5750GEs and 10GbE idling at 10W each!) mostly running Apache Spark, a Pi4B 8Gb k3s cluster and lots more. Most of what talks to it is either 40GbE or 10GbE.There is storage tiering set up so the disks are very rarely hit, so they’re asleep most of the time. It mostly is serving data to or from the U.2s, shuffling it around automatically later on. The SATA SSDs are just metadata. It actually boots off a SuperMicro SuperDOM.——The Zen 3 Ryzen PRO 5750GEs are unicorns, but super low power. Very tiny idle (they’re laptop cores), integrated GPU, ECC support, and the memory protection features of EPYC. 92% of the performance of a 5800X, but all 8C&#x2F;16T flat out (at 3.95GHz because of an undervolt) caps at just under 39W package power.The LSI 9500-16i gave me all the lanes I needed (8 PCIe, 16 SlimSAS) for the two enterprise U.2 and 8 HDDs, and was very low idle power by being a newer adapter.The Intel dual QSFP+ NIC was deliberate as using passive DACs over copper saved 4-5W per port (8 at the aggregation switch) between the NIC and the switch. Yes, really. Plus lower latency (than even fiber) which matters at these transfer speeds.The “pig” is honestly the ASRock X570D4U because the BMC is 3.2W on its own, and X570 is a bit power hungry itself. But all in all, the whole system idles at 50W, is usually 75-80W under most loads, but can theoretically peak probably around 180-190W if everything was going flat out. It uses EVERY single PCIe lane available from the chipset and CPU to its fullest! Very specific chassis fan choices and Noctua low profile cooler in a super short depth 2U chassis. I’ve never heard it make a peep, disks aside :) reply ThatMedicIsASpy 14 hours agorootparentI&#x27;m looking at an LSI 9300-16i which is 100€ (refurbished) including the cables. I just have to flash it myself. Even a 9305 is triple the cost for around half the power draw.My build is storage, gaming and a bunch of VMs.Used Epyc 7000 was the other option for a ton more PCIe. I have no need for more network speed. reply MrFoof 14 hours agorootparentYep. 9300s are very cheap now. 9400s are less cheap. 9500s are not cheap. 9600s are new and pricey.As I said, you can&#x27;t recoup the ROI from the reduced power consumption, even if you&#x27;re paying California or Germany power prices. Though you can definitely get the number lower!I had this system (and the 18U rack) in very close proximity in an older, non-air conditioned home for a while. So less heat meant less heat and noise. I also deliberately chased, \"how low can I go (within reason)\" while still chasing the goal of local NVMe performance over the network. Which makes the desire to upgrade non-existent, even 5+ years from now.Not cheap, but a very fun project where I learned a lot and the setup is absolutely silent! reply ianai 14 hours agorootparentprevIs the “e” for embedded? Ie needs to be bought in a package? I’m not seeing many market options. reply MrFoof 14 hours agorootparentNope. The extra E was for \"efficiency\", because they were better binned than the normal Gs. Think of how much more efficient 5950Xs were than 5900Xs, despite more cores.So the Ryzen PRO line is a \"PRO\" desktop CPU. So typical AM4 socket, typical PGA (not BGA), etc. However they were never sold directly to consumers, only OEMs. Typically they were put in USFF (1L) form factors, and some desktops. They were sold primarily to HP and Lenovo (note: Lenovo PSB fuse-locked them to the board -- HP didn&#x27;t). For HP specifically, you&#x27;re looking at the HP ProDesk and EliteDesk (dual M2.2280) 805 G8 Minis... which now have 10GbE upgrade cards (using the proprietary FlexIO V2 port) available straight from HP, plus AMD DASH for IPMI!You could for a while get them a la carte from boutique places like QuietPC who did buy Zen 3 Ryzen PRO trays and half-trays, but they are long gone. They&#x27;re also well out of production.Now if you want one, they&#x27;re mostly found from Taiwanese disassemblers and recyclers who part out off-lease 1L USFFs. The 5750GEs are the holy grail 8-cores, so they command a massive premium over the 6-core 5650GEs. I actually had a call with AMD sales and engineering on being able to source these directly about a year ago, and though they were willing, they couldn&#x27;t help because they were no longer selling them into the channel themselves. Though the engineer sales folks were really thrilled to see someone who used every scrap of capability of these CPUs. They were impressed that I was using them to sustain 55GbE of actual data transfer (moving actual data, not just rando network traffic) in an extremely low power setup.-- -----Also, I actually just logged in to my metered PDU, and the system is idling right now at just 44.2W. So less than the 50W I said, but I wanted to be conservative in case I was wrong. :)44.2W that has over 84TiB usable storage, with fully automagic ingest and cache that helps to serve 4.5GiB&#x2F;sec to 6.5GiB&#x2F;sec over the network ain&#x27;t bad! reply justinclift 5 hours agorootparent> using the proprietary FlexIO V2 portThey&#x27;re also available cheaply on Aliexpress, and reportedly work ok.Well, there are different models (with different numbers of PCIe lanes), so very much a case of \"do your research first\". :) reply ianai 13 hours agorootparentprevNice! Wish they were easier to obtain!! reply MrFoof 11 hours agorootparentAgreed! Despite being PCIe 3.0, these were perfect home server CPUs because of the integrated GPU and ECC support. The idles were a bit higher than 12th gen Intels (especially the similarly tough to find \"T\" and especially \"TE\" processors) mostly because of X570s comparatively higher power draw, but if you ran DDR5 on the Intel platform it was kind of a wash, and under load the Zen 3 PRO GEs won by a real margin. Plus you really could use every scrap of bandwidth and compute these chips could muster. You use ALL the chip. :)My HP ProDesk 405 G8 Minis with a 2.5GbE NIC (plus the built in 1GbE which supported AMD DASH IPMI) idled at around 8.5W, and with the 10GbE NICs that came out around June, are more around 9.5W -- with a 5750GE, 64GB of DDR4-3200 (non-ECC), WiFi 6E and BT 5.3, a 2TiB SK Hynix P31 Gold (lowest idle of any modern M.2 NVMe?), and modern ports including 10Gb USB-C. Without the WiFi&#x2F;BT card it might actually get down to 9W.The hilarious thing about those is they have an onboard SATA connector, but also another proprietary FlexIO connector that can take an NVIDIA GTX 1660 6GB! You want to talk a unicorn, try finding those GPUs in the wild! I&#x27;ve never seen one for sale separately! If you get the EliteDesk (over the ProDesk) you also get a 2nd M2.2280 socket for mirroring.I have three of those beefy ProDesk 805 G8 Minis in a Proxmox 8 cluster, and it mostly runs Apache Spark jobs, sometimes with my PC participating (how I know the storage server can sustain 55GbE data transfer!), and it&#x27;s hilarious that you have this computerized stack of napkins making no noise that&#x27;s fully processing (reading, transforming, and then writing) 3.4GiB&#x2F;sec of data -- closer to 6.3GiB&#x2F;sec if my 5950X PC is also participating. I don&#x27;t need the cloud, we have cloud at home!-----If you want a 5750GE, check eBay. That&#x27;s where you&#x27;ll find them, and rarely NewEgg. Just don&#x27;t get Lenovo systems unless you want the whole thing, because the CPUs are PSB fuse-locked to the system they came in.4750GEs are Zen 2s and cheaper (half the price), and pretty solid, but I think four fewer PCIe lanes. Nothing \"wrong\" with a 5750G per se, but they cap more around 67-68W instead of 39W.Just if you see a 5750GE, grab it ASAP. People like me hunt those things like the unicorns they are. They go FAST! Some sellers will put up 20 at a time, and they&#x27;ll all be gone within 48 hours.-----I really look forward to the Zen 4 versions of these chips, and the eventual possibility of putting 128GiB of memory into a 1L form factor, or 256GiB into a low power storage server. I won&#x27;t need them (I&#x27;m good for a looooong time), but it&#x27;s nice to know it&#x27;ll be a thing.Intel 15th gen may be great too, as it&#x27;s such a massive architecture shift plus a new process node. Intel also tends to have really low board chipset power consumption, and really low idles.Obscenely capable home servers that make no noise and idle in the 7-10W range are utterly fantastic. replyhypercube33 15 hours agoparentprevI really want the Ryzen Embedded and or Epyc 3000(?) series that has dual 10gbe on package for something like a NAS but both are super expensive or impossible to find. reply ThatMedicIsASpy 15 hours agorootparentAsRock Rack B650D4U-2L2T&#x2F;BCM, 2x10G, 2x1G, IPMIFor less power consumption Ryzen 8000 is coming up (wait for Jan 8th, CES) and the APU tend to be monolithic and draw a lot less power than the chiplets. reply tw04 15 hours agorootparentEven that uses Broadcom 10gbe, not the embedded AMD Ethernet. It’s really strange, I can only assume there’s something fatally wrong with the AMD Ethernet. reply AdrianB1 13 hours agorootparentOr it just tells the customers of this kind of equipment want proven solutions instead of other (novelty) options, so the manufacturers build their products with that in mind. Stability and support are very important to most buyers. reply tw04 11 hours agorootparentIf that were the case I’d expect to still see at least SOME products utilizing the AMD chipset, even if budget focused. I have literally not seen a single board from any MFG that utilizes the built in NIC. Heck there are Intel Xeon-d chipsets that utilize both the onboard NIC and external Broadcom to get 4x for cheap. reply j45 15 hours agorootparentprevIt may be possible to install, or add an external 2.5 or 10GbE device.Either way, it&#x27;s awful there is not more 10 GbE connectivity available default. There&#x27;s no reason it shouldn&#x27;t be the next level up, we have been at 1 &#x2F; 2.5 for far too long. reply ThatMedicIsASpy 14 hours agorootparentYou can find what you desire but you always have to pay for it.ASUS ProArt X670E-Creator WIFI, 10G & 2.5G at 460€10G simply isn&#x27;t that cheap. The cheapest 5 port switch is 220€. Upgrading my home net would be rather expensive. reply vetinari 13 hours agorootparentWhat makes is more expensive is insisting on 10GBase-T. 10G over SFP+ is not that expensive; the cheapest 4 port switch (Mikrotik CRS305) is ~130 EUR. reply selectodude 7 hours agorootparentOptics aren’t free either. replyeurekin 15 hours agoparentprevWhat about networking? Did you go over 1gbit? reply ThatMedicIsASpy 15 hours agorootparentIt has 2.5G. There are X670E with 10G if you desire more.My home net is 1G with two MikroTik hAP ax3 which are connected with the single 2.5G poe port they have (and one powers the other). reply dist-epoch 11 hours agoparentprev> 3-5W per 8GB of RAMI think that&#x27;s wrong. It would mean 4*6=25W per DIMM.I also have 48GB DDR5 DIMMs and HwInfo shows 6W max per module. reply MochaDen 16 hours agoprevLow-power is great but running a big RAID long-term without ECC gives me the heebee jeebies! Any good solutions for a similar system but more robust over 5+ years? reply rpcope1 15 hours agoparentI think the trick is to go with a generation or two old Supermicro motherboard in whatever ATX case you can scrounge up, and then use either a low power Xeon or a Pentium&#x2F;Celeron. Something like the X11SAE-F or X12SCA-F (or maybe even older) is plenty, though maybe not quite as low power. I still use an X9SCA+-F with some very old Xeon for a NAS and to run some LXC containers. It idles at maybe 20-30W instead of 5, but I&#x27;ve never had any issues with it, and I&#x27;m sure it&#x27;s paid itself off many times over. reply sgarland 6 hours agorootparentEven better, Supermicro will pick up the phone&#x2F;answer emails, even if you bought a years-old secondhand server. They have the manuals, and are more than happy to help you out.Love my X9 and X11 boards. reply faeriechangling 16 hours agoparentprevEmbedded SOCs like AMDs which are used by Synology etc such as AMD V2000.If you want to step up to being able to serve an entire case or 4U of HDDs, you’re going to need pcie lanes though, in which case w680 with i5-12600k and a single ecc udimm and a SAS HBA in the pcie slot with integrated Ethernet is probably as low wattage as you can get. Shame w680 platform cost is so high, am4&#x2F;zen2 is cheaper to the point of still being viable.You can also get Xeon, embedded Xeon, am5, am4 (without an iGPU).There’s nothing inherently wrong with running a raid without ecc for 5 years, people do it all the time and things go fine. reply eisa01 16 hours agorootparentBeen thinking to just get a Synology with ECC support, but what I find weird is that the CPUs they use are 5+ years old. Feels wrong to buy something like that “new”Same with TrueNas mini reply faeriechangling 14 hours agorootparentFor the most part, these are computers which are meant to stick around through 2-4 upgrade cycles of your other computers. Just doing various low power 24&#x2F;7 tasks like file serving.You could be like “well that’s stupid, I’m going to make a balls to the wall build server that also serves storage with recent components” but the build server components will become obsolete faster then the storage components, it can lead to incidental complexity to try and run something like windows games on a NAS operating system because you tried to consolidate on one computer, being forced to use things like ECC will compromise absolute performance, you’ll want to have the computer by your desk potentially but also in a closet since it has loud storage, you’re liable to run out of pcie lanes and slots, you want to use open cooling for the high performance components and a closed case for the spinning rust, it’s all a bit awkward.Much simpler is to just treat the NAS as an appliance that serves files, maybe runs a plex server, some surveillance, a weather station, rudimentary monitoring, and home automation. Things for which something like a v2000 is overkill. Then use breeding edge chips in things like cell phones and laptops. Then have the two computers do different jobs. Longer product cycles between processors makes things like support cheaper to maintain for long term periods of time and offer low prices. reply dannyw 4 hours agorootparentprevServing files is not compute intensive at all. reply hypercube33 11 hours agorootparentprevI have a 3u Nas I built in 2012 or something with a two core sempron running windows and using storage spaces and it still holds up just fine. reply cpncrunch 15 hours agorootparentprevIt depends what your requirements are. Ive been using a low end synology box for years as a home dev server and it is more than adequate. reply jhot 14 hours agoparentprevI&#x27;m running truenas on a used e3 1245 v5 ($30 on ebay) and an Asus workstation Mobo with 32 GB ECC and 4 spinning drives. Not sure individually, but the nas along with a i5 12400 compute machine, router, and switch use 100W from the wall during baseline operation (~30 containers). I&#x27;d consider that hugely efficient compared to some older workstations I&#x27;ve used as home servers. reply NorwegianDude 12 hours agorootparentI&#x27;ve been running a E3-1230v3 for over 10 years now. With 32GN ECC, 3 SSDs and 4 HDD and separate port for IPMI I&#x27;m averaging 35 W from the wall with a light load. Just ordered a Ryzen 7900 yesterday, and I guess the power consumption will be slightly higher for that one. reply tyingq 15 hours agoparentprevIf you&#x27;re on a budget, a used HP Z-Series workstation supports ECC ram. A bare-bones one is cheap, though the ECC memory can be expensive since it&#x27;s not the (plentifully available) server type RDIMMs. Not a low-power setup either :) reply ianai 16 hours agoparentprevAgree. Didn’t even see ECC discussed.Apparently this board supports ecc with this chip: Supermicro X13SAE W680 LGA1700 ATX MotherboardCosts 550.One option is building around that and having some pcie 4.0 to nvme boards hosting as many nvme drives as needed. Not cheap though but around home affordable. reply ThatMedicIsASpy 16 hours agorootparentYou need workstation chipsets to have ECC on intel desktop CPUs.And yes they start at around 500. reply philjohn 15 hours agorootparentIf you go back a few generations, the C246 chipset can be had on boards costing 200, and if you pair it with an i3-9100T you get ECC as well as pretty damn low power usage. reply ianai 14 hours agorootparentYou are limited to pcie 3.0 speeds there though. But good suggestion. reply philjohn 9 hours agorootparentThat&#x27;s true, but if your goal is low power, that&#x27;s not necessarily going to be a bottleneck - even if you dedicate all 16 PCIe lanes to NVMe storage it&#x27;s going to be more than fast enough for 99% of home server needs. replyphiljohn 15 hours agoparentprevThat&#x27;s why I went with an i3-9100T and an Asrock Rack workstation board, ECC support (although UDIMM vs RDIMM) reply a20eac1d 15 hours agorootparentThis sounds similar to a build I&#x27;m planning. I cannot find the workstation mainboards at a reasonable price though. They start at like 400€ in Europe. reply philjohn 12 hours agorootparentThere&#x27;s an Asus one that&#x27;s available as well, the ASUS C246 PRO - it&#x27;s about 250 GBP.I did build mine 2 years ago, so the 246 motherboards are less available now, the C252 is another option which will take you up to 11th gen Intel. reply j45 15 hours agoparentprevI would never run a self-hosted nas when a synology&#x2F;qnap are available as a dedicated appliance for around the same price.The hardware is much more purpose equipped to store files long term and not the 2-3 years between consumer SSDs&#x27;It&#x27;s not to say self-hosting storage can&#x27;t or shouldn&#x27;t be done, its just about how many recoveries and transitions have you been through, because it&#x27;s not an if, but a when. reply dbeley 11 hours agorootparentThe hardware is basically the same as self-hosted NAS, the motherboard could even be of a lower quality. The software though is closed source and most consumer NAS only get support for 4-5 years which is outrageous. reply dannyw 4 hours agorootparentYou&#x27;re not buying from the right brand.Synology supports their hardware for about 10 years since release. They are the \"Apple\"-like of NAS. reply Jedd 7 hours agorootparentprevI bought a QNAP about a decade ago under the same assumption, but my experiences [0] there means I&#x27;m unlikely to buy a SOHO-level storage appliance ever again.The tl;dr of my rant was around shortcomings in NFS permission configuration, and a failure of the iSCSI feature (the appliance crashed when you sent it data).Further, these appliances invariably use vanilla RAM sticks, so you&#x27;re exposed to gentle memory-based file corruption you probably won&#x27;t notice for years.So I&#x27;d argue the hardware is &#x27;better equipped&#x27;, and I&#x27;d also argue the software as shipped matches the marketing promises accompanying same.Things have doubtless changed - I&#x27;m sure those bugs are long gone now - but unless you&#x27;re looking at an ECC appliance, I&#x27;d say you&#x27;re better off building your own white box.[0] https:&#x2F;&#x2F;jeddi.org&#x2F;b&#x2F;brief-rant-on-trying-to-use-iscsi-on-a-q... reply justsomehnguy 6 hours agorootparent> but unless you&#x27;re looking at an ECC appliance, I&#x27;d say you&#x27;re better off building your own white box.Synology actually allows ECC DRAM and even sells it and list which models would accept them.But yeah, at the price of a full featured model with an x86 CPU and SO&#x2F;DIMM RAM and 4+ drives you are in the territory of building your own, with a lot more of control and without DSM (in Synology case) shenanigans.EDIT: actually the biggest problem here is actually finding a good case, because even ATX cases now usually don&#x27;t have more than 2-3 3.5\" bays by default and often don&#x27;t have 5.25\" at all.https:&#x2F;&#x2F;www.synology.com&#x2F;en-us&#x2F;products&#x2F;DDR4 reply justinsaccount 14 hours agorootparentprev> The hardware is much more purpose equipped to store files long termWhat hardware would that be, specifically? The low end embedded platforms that don&#x27;t even support ECC?> how many recoveries and transitions have you been through3 or 4, at this point, using the same 2 disk zfs mirror upgraded from 1TB to 3TB to 10TB. reply neilv 10 hours agoprevGreat work, for lots of storage.If you can fit your storage on an SSD (or RAID-mirrored pair), and don&#x27;t need much compute, you can do a low-power server (like to run little services for yourself) using an SBC like a RasPi, or something like a NUC.Personally, I currently have a couple 1U Atom servers that run fanless except for the Noctua fans that I swapped into the PSUs. Advantages over RasPi include SATA and ECC RAM, and were also easier to buy over Covid. (I also have a 4U GPU server, which is currently off when not in use, because I haven&#x27;t invested in figuring out how to low-power idle it like the article writer has.) reply Paradigma11 4 hours agoprevHow viable would it be to run a home server in a VM on your main PC that is occasionally also used for high performance activities like gaming. I do have 12 cores and 64GB Ram and I would not mind parting with 4C&#x2F;16GB and maybe the onboard GPU (video decoding) but I dont know how the rest of the system might be influenced if there is heavy IO or other stuff. reply marcosscriven 43 minutes agoparentProxmox is good for this. You can have a Windows VM with GPU passthrough. Then you can add any other VMs or LXCs to your heart’s content. reply mattbillenstein 10 hours agoprevEveryone&#x27;s needs are different, but over time with the loss of a couple of disks, I started to hate running RAID5 or 6 with HDDs. It became an exercise in how fast could I replace a disk before the next one died and would the rebuild actually work or not - although, it always did. Also, the hot-swap case&#x2F;cage I had with all the SATA connectors and power connectors seemed kinda flaky - it was very cheap.So a couple years ago, I downsized to a single 2TB SSD in a smallish ATX case - and another in a completely different machine that gets rsync&#x27;d every 4 hours. My nas is now just my last desktop&#x27;s hardware with two SSDs - one boot, one larger storage running plex, smbd, backups of misc stuff on cron, duplicity backup to the cloud, etc on Ubuntu. If I didn&#x27;t have this extra hardware, I&#x27;d probably just run two medium powered NUCs with a nvme boot disk and a bigger SSD for storage.It&#x27;s all very very simple and I have it setup so I can run some LXC containers should I want to do some dev work there, but I usually have other hardware for that anyway. reply nine_k 10 hours agoparentGood thing that 2TB suffices for you! I think RAID NAS boxes seem to make sense at sizes several times this. reply at_a_remove 9 hours agoparentprevI am imagining a NAS appliance designed for high availability and low-touch, like a RAID 10 box with a ton of hot spares, designed to automatically rebuild in case of failure. The hot spares are spun up once a month to prevent stiction. reply CPLX 9 hours agoparentprevI have done a bunch of stuff in the past but most recently I just bought two synology arrays and followed the instructions to set them up in less than an hour and they just worked with literally zero hassle. reply chx 16 hours agoprevWhy not the N100?Even an N305 fits the purpose, the N100 would be even less https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MiniPCs&#x2F;comments&#x2F;12fv7fh&#x2F;beelink_eq... reply arp242 15 hours agoparentN100 wasn&#x27;t yet released when this was written in May (or was only just released).Also the N100 only supports 16G RAM, and this guy has 64G. Number of pcix lanes (9 vs. 20) probably matter for their use case as well. And the i5 does seem quite a bit faster in general.Comparison: https:&#x2F;&#x2F;ark.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;ark&#x2F;compare.html?pro... reply s800 15 hours agorootparentI&#x27;m running a home server on an N100 (ITX model) with a 32GB DIMM, works well. reply adrian_b 14 hours agorootparentIt has been widely reported that Alder Lake N actually works with 32 GB, but for some reason Intel does not support this configuration officially.The same happened with the previous generations of Intel Atom CPUs, they have always worked without any apparent problems with more memory than the maximum specified by Intel. reply cjdell 16 hours agoparentprevI&#x27;m very impressed with my N100 mini PC (fits in your palm) that I bought from AliExpress. Takes between 2-8W and uses just a plain old 12V plug-style power supply with a DC barrel. Perfect for Home Assistant and light virtualisation.Performance is actually better than my quad core i5-6500 mini PC. Definitely no slouch. reply trescenzi 11 hours agoparentprevI bought a tiny, fits in the palm of my hand, N100 box on Amazon for $150[1]. It currently does basically everything I need and idles at 7.5W.I’ve got cloudflare setup for dns management and a few simple sites hosted on it like blogs and Gitea. It has an sd card slot that I use as extra storage.Sure it’s not nearly as awesome as the setup detailed here but I couldn’t recommend it more if you just want a small simple home server.[1] https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;cIzEFPk reply SomeoneFromCA 1 hour agorootparent7.5 W is a bit high idle for N100, IMHO. Power supply is not very efficient I bet. reply NelsonMinar 7 hours agoparentprevAnother happy N100 mini PC user. Mine&#x27;s about 7W when idle, including a couple of spinning USB drives. The most I could get it to pull was about 14W. Really remarkable little systems for a very good price. reply imglorp 16 hours agoparentprevBecause author said they wanted a bunch of disks. reply hrdwdmrbl 16 hours agoparentprev+1 for the Nx00 series of chips. I just bought myself a pre-built mini pc with an N100. Low power, good price, great performance.I wonder if in a few years they might not eat the whole mini PC market. If the price can come down such that they’re competitive with the various kinds of Pis… reply vermaden 15 hours agoprevGood read.Tried something similar in the past:- https:&#x2F;&#x2F;vermaden.wordpress.com&#x2F;2019&#x2F;04&#x2F;03&#x2F;silent-fanless-fre...- https:&#x2F;&#x2F;vermaden.wordpress.com&#x2F;2023&#x2F;04&#x2F;10&#x2F;silent-fanless-del... reply pomatic 8 hours agoprev7 watts idle is unimpressive out of contextIf you have a NAS that is idle most of the time, and want to minimise power consumption, how about an embedded-cpu based WoL generator? Sniff packets destined for the fileserver, which is otherwise in deep sleep, and automagically wake it up when relevant traffic is detected, with the relevant WoL packet. You&#x27;d get sayhome NAS utilizing a bunch of 2.5\" 1TB Seagate drives. I would not repeat the experiment as the downsides in performance was simply not worth the space&#x2F;power savings.5400 drives? How many and how bad the performance was? reply paulmd 12 hours agorootparentprevyup. the problem is really with the SMR drives where they can (seemingly) hang for minutes at a time as they flush out the buffer track. ordinary spin-down isn&#x27;t really a problem, as long as the drives spin up within a reasonable amount of time, ZFS won&#x27;t drop the disk from the array.ZFS is designed for HDD-based systems after all. actually it works notably kinda poorly for SSDs in general - a lot of the design+tuning decisions were made under the assumptions of HDD-level disk latency and aren&#x27;t necessarily optimal when you can just go look at the SSD!however, tons and tons of drive spin-up cycles are not good for HDDs. Aggressive idle timeout for power management was famously the problem with the WD Green series (wdidle3.exe lol). Best practice is leave the drives spinning all the time, it&#x27;s better for the drives and doesn&#x27;t consume all that much power overall. Or I would certainly think about, say, a 1-hour timeout at least.https:&#x2F;&#x2F;www.truenas.com&#x2F;community&#x2F;threads&#x2F;hacking-wd-greens-...However, block-level striping like ZFS&#x2F;BTRFS&#x2F;Storage Spaces is not very good for spinning down anyway. Essentially all files will have to hit all disks, so you have to spin up the whole array. L2ARC with a SSD behind it might be able to serve a lot of these requests, but as soon as any block isn&#x27;t in cache you will probably be spinning up all the disks very shortly (unless it&#x27;s literally 1 block).Unraid is better at this since it&#x27;s a file-level striping - newer releases can even use ZFS as a backend but a file always lives on a single unraid volume, so with 1-disk ZFS pools underneath you will only be spinning up one disk. This can also be used with ZFS ARC&#x2F;L2ARC or Unraid might have its own setup for tiering hot data on cache drives or hot-data drives.(1-disk ZFS pools as Unraid volumes fits the consumer use-case very nicely imo, and that&#x27;s going to be my advice for friends and family setting up NASs going forward. If ZFS loses any vdev from the pool the whole pool dies, so you want to add at least 2-disk mirrors if not 4-disk RAIDZ vdevs, but since Unraid works at a file-stripe level (with file mirroring) you just add extra disks and let it manage the file layout (and mirrors&#x2F;balancing). Also, if you lose a disk, you only lose those files (or mirrors of files) but all the other files remain intact, you don&#x27;t lose 1&#x2F;8th of every file or whatever, and that&#x27;s a failure mode that aligns a lot better with consumer expectations&#x2F;needs and consumer-level janitoring. And you still retain all the benefits of ZFS in terms of ARC caching, file integrity, etc. It&#x27;s not without flaws, in the naive case the performance will degrade to 1- or 2-disk read speeds (since 1 file is on 1 disk, with eg 1 mirror copy) and writes will probably be 1-disk speed, and a file or volume&#x2F;image cannot exceed the size of a single disk and must have sufficient contiguous free space, and snapshots&#x2F;versioning will consume more data than block-level versioning, etc. All the usual consequences of having 1 file backed by 1 disk will apply. But for \"average\" use-cases it seems pretty ideal and ZFS is an absolutely rock-stable backend for unraid to throw files into.)anyway it&#x27;s a little surprising that having a bunch of individual disks gave you problems with ZFS. I run 8x8TB shucked drives (looking to upgrade soon) in RAIDZ2 and I get basically 8x single-disk speed over 10gbe, ZFS amortizes out the performance very nicely. But there are definitely risks&#x2F;downsides, and power costs, to having a ton of small drives, agreed. Definitely use raidz or mirrors for sure. reply jnsaff2 16 hours agoprevI have a 5-node ceph cluster built out of Fujitsu desktops that I got for 50 euro a piece.4 nodes have 8gb ram and one has 16gb.CPU in each is i5-6500.Each has an NVMe that is split for OS and journal and a spinning HDD.The cluster idles at 75W and full load about 120W. That is intense ceph traffic not other workloads. reply Throw839 16 hours agoparentThat fujitsu part is important. Many mainstream brands do not implement power states correctly, Fujitsu seems to be focused on power consumption quite lot. reply skippyboxedhero 16 hours agorootparentThe NUC and Optiplex aren&#x27;t bad either. There are also very good AsRock boards (I can&#x27;t remember what the modern ones are called but H110T is one, I used this for a bit, idled at 6W, laptop memory and power brick). But Fujitsu is the S-tier.In practice, I found I needed a bit more power but you can get some of the Fujitsu boards with a CPU for $30-40, which is hard to beat. reply ThatMedicIsASpy 16 hours agorootparentprevI have a HP ProDesk powertop shows up to C10 which I never reach. Must be the SSD or NVMe I have. But yeah The BIOS in those are super cut down and there are hardly any energy settings I can change. reply paulmd 12 hours agorootparentprevfujitsu has always been underappreciated in the mainstream tbh. there has always been a thinkpad-style cult following (although much smaller) but japanese companies often do a pretty terrible job at marketing in the west (fujifilm being another fantastic example).my university issued T4220 convertible laptops, with wacom digitizers in the screens. I rarely used it but the pivot in the screen made it indestructible, it survived numerous falls hitting the corner of the screen&#x2F;etc because the screen simply flops out of the way and pivots to absorb the energy. I later got a ST6012 slate PC that my uni bookstore was clearing out (also with a wacom digitizer, and a Core2Solo ULV!). Both of them are extremely well-thought-out and competently designed&#x2F;built hardware. Doesn&#x27;t \"feel\" thinkpad grade, but it absolutely is underneath, and featured PCMCIA and bay batteries and other power-user features.https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;Fujitsu-Siemens-Lifebook-T4220...https:&#x2F;&#x2F;www.ruggedpcreview.com&#x2F;3_slates_fujitsu_st6012.htmlThey also did a ton of HPC stuff for Riken and the other japanese research labs, they did a whole family of SPARC processors for mainframes and HPC stuff, and pivoted into ARM after that wound down. Very cool stuff that receives almost no attention from mainstream tech media, less than POWER even.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=m0GqCxMmyF4Anyway back on topic but my personal cheat-code for power is Intel NUCs. Intel, too, paid far more attention to idle power and power-states than the average system-integrator. The NUCs are really really good at idle even considering they&#x27;re using standalone bricks (my experience is laptop bricks are much less efficient and rarely meet 80+ cert etc). A ton of people use them as building blocks in other cases (like HDPlex H1 or Akasa cases), they don&#x27;t have a ton of IO normally but they have a SATA and a M.2 and you can use a riser cable on the M.2 slot to attach any pcie card you want. People would do this with skull canyon f.ex (and HDPlex H1 explicitly supports this with the square ones). The \"enthusiast\" style NUCs often have multiple M.2s or even actual pcie slots and are nice for this.https:&#x2F;&#x2F;www.amazon.com&#x2F;ADT-Link-Extender-Graphics-Adapter-PC...And don&#x27;t forget that once you have engineered your way to pcie card formfactor, you can throw a Highpoint Rocket R1104 or a SAS controller card in there and run multiple SSDs (up to 8x NVMe) on a single pcie slot, without bifurcation. Or there are numerous other \"cheat code\" m.2 devices for breaking the intended limits of your system - GPUs (Innodisk EPV-1101&#x2F;Asrock M2_GPU), SATA controllers, etc.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=M9TcL9aY004 (actually this makes the good point that CFExpress is a thing and is very optimized for power. No idea how durable they are in practice, and they are definitely very expensive, but they also might help in some extreme low power situations.)Personally I never found AMD is that efficient at idle. Even with a monolithic apu you will want to dig up an X300TM-ITX from aliexpress, since this allows you to forgo the chipset. Sadly AMD does not allow X300 to be marketed directly as a standalone product, only as an integrated system like a nuc or laptop or industrial pc, despite the onboard&#x2F;SOC IO being already quite adequate for beige box usage. Gotta sell those chipsets (hey, how about two chipsets per board!?). But OP article is completely right that AMD’s chipsets just are not very efficient. reply Dalewyn 6 hours agorootparent>Personally I never found AMD is that efficient at idle.Since we&#x27;re talking about Japan, Intel is affectionately called the IdleM@ster as a pun on the very popular IdolM@ster franchise.https:&#x2F;&#x2F;twitter.com&#x2F;neko1942&#x2F;status&#x2F;1261661579101143040https:&#x2F;&#x2F;twitter.com&#x2F;Fact_M_Q&#x2F;status&#x2F;1325604249623953408 reply paulmd 2 hours agorootparentthat&#x27;s hilarious, thanksbut yeah, it&#x27;s true, the paradox of intel is that they can get it so wrong in the big picture and execution and integration but sometimes the little details are so right. I225V is a mess. Sapphire Rapids has 700W transients above average. But idle power and interactive scenarios on client processors is a dream. They still tend to be better than AMD on their driver side and general platform validation and stability (as much as that has been a doubtful thing and continues to be going forward, it&#x27;s true). reply eurekin 15 hours agoparentprevWhat effective client speeds are you getting? reply jnsaff2 15 hours agorootparentCurrently only gigabit network and I can easily saturate that.Thinking about chucking in 25gbit cards. reply treprinum 11 hours agoprevMy NAS has Pentium J 4-core and is way under 7W idle, inside some small Fractal case with 6x20TB HDD. Why would you need 12th&#x2F;13th gen for file transfers? reply wffurr 11 hours agoparentFor encoding maybe? OP says “reasonable CPU performance for compression” and also it was a CPU they already had from a desktop build. reply dbeley 11 hours agoparentprevInteresting I assume it&#x27;s with all drives off, how many Watts with some disk usage? reply ulnarkressty 14 hours agoprevAs exciting as it is to design a low power system, it&#x27;s kind of pointless in the case of a NAS that uses spinning rust as storage media - as the author later writes, the HDD power consumption dwarfs the other system components.If one uses SSD or M.2 drives, there are some solutions on the market that provide high speed hardware RAID in a separate external enclosure. Coupled with a laptop board they could make for a decent low power system. Not sure how reliable USB or Thunderbolt is compared to internal SATA or PCIe connections though... would be interesting to find out. reply nabla9 11 hours agoparentYou can shut down HDD&#x27;s when you don&#x27;t use them. sudo hdparm -Y &#x2F;dev&#x2F;sdX reply V__ 13 hours agoparentprevDon&#x27;t they stop spinning when idle? reply layer8 13 hours agorootparentNot by default, but you can have the OS have them spin down after a certain idle period. Doing that too frequently can affect the life time of the drive though. You save maybe 4 Watts per drive by spinning them down. reply Dalewyn 6 hours agorootparentPersonally, I make sure my HDDs (regardless use case) never spin down when idle. The cost in life span isn&#x27;t worth the electricity bill saved. reply Arn_Thor 2 hours agorootparentSame. Also I’ve had some software error out as it waits for drives to spin up (in an NFS share context). reply orthoxerox 13 hours agorootparentprevThey are never idle if the NAS is seeding torrents. reply Palomides 15 hours agoprevamusing to read this very detailed article and not have any idea what OP actually does with 72TB of online storage1gbe seems a bit anemic for a NAS reply dannyw 4 hours agoparentit&#x27;s usually p0rn reply 1letterunixname 15 hours agoprevI feel like an petrochem refinery with my 44 spinning rust units NAS 847E16-RJBOD, 48 port POE+ 10 GbE switch, 2 lights-out and environmental monitoring UPSes, and DECISO OPNsense router using a combined average of 1264W. ]: One UPS is at least reporting a power factor with an efficiency of 98%, while the other one isn&#x27;t as great at 91%.APM is disabled on all HDDs because it just leads to delay and wear for mythological power savings that isn&#x27;t going to happen in this setup. Note that SMART rarely&#x2F;never predicts failures, but one of the strongest signals of drive failures is slightly elevated temperatures (usually as a result of bearing wear).This creates enough waste heat such that one room never needs heating, but cooling isn&#x27;t strictly needed either because there&#x27;s no point to reducing datacenter ambient below 27 C. reply syntheticnature 15 hours agoparentI was looking into water heaters that use heat pumps recently, and a lot of them function by sucking heat out of the room. While water and computers don&#x27;t mix, might be an even better use for all that waste heat... reply sgarland 6 hours agorootparentMy plan for when I build a house (so far with two moves during my serious homelab period, the room layout hasn’t worked) is to build a small server room that vents out to the heat pump water heater. reply alphabettsy 15 hours agoprevVery cool write up. Good timing too as I find myself attempting to reduce the power consumption of my homelab. reply vardump 9 hours agoprevWhat would be the cheapest and lowest power option to get 100G networking (50-100G actual performance is good enough) and at least one 8 lane PCI-e GPU?Plus at least 2x M.2. reply uxp8u61q 15 hours agoprevI know nothing about building NASs so maybe my question has an obvious answer. But my impression is that most x64 CPUs are thoroughly beaten by Arm or RISC-V CPUs when it comes to power consumption. Is there a specific need for the x64 architecture here? I couldn&#x27;t find an answer in TFA. reply adrian_b 14 hours agoparentMost Arm or RISC-V CPUs (with the exception of a few server-oriented models that are much more expensive than x86) have very few PCIe lanes and SATA ports, so you cannot make a high throughput NAS with any of them.There are some NAS models with Arm-based CPUs and multiple SSDs&#x2F;HDDs, but those have a very low throughput due to using e.g. only one PCIe lane per socket, with at most PCIe 3 speed. reply arp242 14 hours agoparentprev> my impression is that most x64 CPUs are thoroughly beaten by Arm or RISC-V CPUs when it comes to power consumptionNot really.ARM (and to lesser degree, RISC-V) are often used and optimized for low-power usage and&#x2F;or low-heat. x64 is often more optimized for maximum performance, at the expense of higher power usage and more heat. For many x64 CPUs you can drastically reduce the power usage if you underclock the CPU just a little bit (~10% slower), especially desktop CPUs but also laptops.There are ARM and RISC-V CPUs that consume much less power, but they&#x27;re also much slower and have a much more limited feature-set. You do need to compare like to like, and when you do the power usage differences are usually small to non-existent in modern CPUs. ARM today is no longer the ARM that Wilson et al. design 40 years ago.And for something connected to mains, even doubling the efficiency and going from 7W to 3.5W doesn&#x27;t really make all that much difference. It&#x27;s just not a big impact on your energy bill or climate change. reply pmontra 13 hours agoparentprevI&#x27;m using an Odroid HC4 as my home server. It has an ARM CPU and it&#x27;s idling at 3.59 W now with a 1 TB SATA 3 SSD and some web apps that are basically doing nothing, because I&#x27;m their only user. It&#x27;s got a 1 GB network card, like my laptop. I can watch movies and listen to music from its disk on my phone and tablet.There is no need to have something faster. The SATA 3 bus would saturate a 2.5 GB card anyway. The home network in Cat 6A so it could go up to 10 GB. We&#x27;ll see what happens some years from now. reply hmottestad 15 hours agoparentprevYou can very easily run docker containers on it. That’s why I went with a ryzen chip in mine.You could always use an rpi if you want to go with ARM, and you’ll want something with ARMv8. reply jauntywundrkind 16 hours agoprevI feel like I see a good number of nas builds go by, but rarely are they anywhere as technical. Nice. reply homero 12 hours agoprevCrucial Force GT supposed to say Corsair reply jeffbee 16 hours agoprevI wonder if this system is connected with wired ethernet or wifi. I found that it makes a large difference on my NAS. With a wired link the SoC can&#x27;t reach a deep sleep state because the ethernet peripheral demands low-latency wakeup from the PCIe root port. This is power management policy that is flowing from the link peer all the way to your CPU! I found that wifi doesn&#x27;t have this problem, and gives better-than-gigabit performance, sometimes. reply skippyboxedhero 16 hours agoparentIf you have an network card over PCIe then there may be an issue with the card. I have never had an issue reaching low sleep state, you can modify WoL behaviour too. Wifi is, again in my experience, uses significantly more power. I have seen 3-5W and usually switch if off. reply jeffbee 16 hours agorootparentI don&#x27;t think it&#x27;s an issue with the card. It&#x27;s a combination of ethernet and PCIe features that make this happen. There is a standard called \"energy efficient ethernet\" that makes it not happen, but my switch doesn&#x27;t do it. reply newsclues 16 hours agoprevI had the same issue of picking a motherboard with limited SATA ports and then having to deal with extra expansion cards.4 is not enough for homelab type servers. reply louwrentius 16 hours agoprev [–] I have the same amount of storage available in a ~9-year-old 24-bay NAS chassis that does 150 Watt idle (with drives spinning).My NAS is powered down most of the time for this reason, only booted (IPMI) remotely when needed.Although the actual idle power consumption in the article seems to be a tad higher than 7 watts, it&#x27;s so much lower, it&#x27;s not such a big deal to run it 24&#x2F;7 and enjoy the convenience.Loved the write-up! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experience in building a low-power server/NAS system using Intel processors and successfully achieves a power consumption of 7 watts.",
      "Challenges in finding a motherboard with enough M.2 slots and recommendations for Intel chipsets over AMD for lower power consumption are discussed.",
      "The article provides detailed choices of motherboard, CPU, memory, and storage components, tips for achieving low power consumption, and recommendations for measuring power consumption as well as future improvements."
    ],
    "commentSummary": [
      "The article and discussion revolve around the construction of low-powered servers/NAS systems using Intel or AMD processors.",
      "Various aspects are explored, including hardware configurations, price-performance options, and alternatives.",
      "Topics such as ECC support, DDR5 memory, Synology's DSM operating system, and pros and cons of different NAS setups are also covered, with users sharing their experiences and preferences for energy-efficient and cost-effective server builds."
    ],
    "points": 289,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1704024652
  },
  {
    "id": 38828040,
    "title": "Bazzite: A Versatile OCI Image for Steam Deck and More, Resolving SteamOS Issues",
    "originLink": "https://github.com/ublue-os/bazzite",
    "originBody": "Table of Contents Features for All Bazzite Images Features for Desktop Images Features for Steam Deck / HTPC Images Features for GNOME Images Features from Upstream Why Showcase Documentation & Newsletters Custom Packages Verification & Metrics Special Thanks Building Your Own Community About & Features Bazzite is an OCI image that serves as an alternative operating system for the Steam Deck, and a ready-to-game SteamOS-like for desktop computers and living room home theater PCs. Bazzite is built from ublue-os/main and ublue-os/nvidia using Fedora technology, which means expanded hardware support and built in drivers are included. Additionally, Bazzite adds the following features: Proprietary Nvidia drivers pre-installed. Full hardware accelerated codec support for H264 decoding. Full support for AMD's ROCM OpenCL/HIP run-times. xone, xpadneo, and xpad-noone drivers for Xbox controllers. Full support for DisplayLink. Includes Valve's KDE themes from SteamOS. LatencyFleX, vkBasalt, MangoHud, and OBS VkCapture installed and available by default Support for Wallpaper Engine. (Only on KDE) ROM Properties Page shell extension included. Full support for Winesync/Fastsync/NTsync. Distrobox preinstalled with automatic updates for created containers. Automated duperemove and rmlint services for reducing the disk space used by wine prefix contents. Support for HDMI CEC via libCEC. System76-Scheduler preinstalled, providing automatic process priority tweaks to your focused application and keeping CPU time for background processes to a minimum. Customized System76-Scheduler config with additional rules. Uses Google's BBR TCP congestion control by default. Input Remapper preinstalled and enabled. (Available but default-disabled on the Deck variant) Bazzite Portal provides an easy way to install numerous applications and tweaks, including installing CoreCtrl and GreenWithEnvy. Nix package manager with Fleek optionally available for install via Bazzite Portal. Brew package manager optionally available for install via Bazzite Portal. Waydroid preinstalled for running Android apps. Set it up with this quick guide. Future releases will offer to set this up for you through Bazzite Portal. (Not available on Nvidia builds) Manage applications using Flatseal, Warehouse, and Gear Lever. OpenRGB i2c-piix4 and i2c-nct6775 drivers for controlling RGB on certain motherboards. OpenRazer drivers built in, Select OpenRazer in Bazzite Portal or run ujust install-openrazer in a terminal to begin using it. OpenTabletDriver udev rules built in, with the full software suite installable via Bazzite Portal or by running ujust install-opentabletdriver in a terminal. GCAdapter_OC driver for overclocking Nintendo's Gamecube Controller Adapter to 1000hz polling. Out of the box support for Wooting keyboards. Built in support for Southern Islands (HD 7000) and Sea Islands (HD 8000) AMD GPUs under the amdgpu driver. A fix is available for a 32-bit Source 1 engine game bug (IE: TF2) that makes the game crash on launch - ujust patch-source1-tcmalloc XwaylandVideoBridge is available for Discord screensharing on Wayland. Webapp Manager is available for creating applications from websites for a variety of browsers, including Firefox. Desktop Common variant available as bazzite, suitable for desktop computers. Automatic updates for the OS, Flatpaks, Nix packages (Via Fleek), and all Distrobox containers. Important ISOs can be downloaded from our releases page here, and a helpful install guide can be found here. If you experience any issues with installing Bazzite, then check out our troubleshoot guide. If you're on an existing Universal Blue image follow these instructions. To rebase an existing upstream Fedora Silverblue/Kinoite ostree system to this image: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite:latest or for devices with Nvidia GPUs: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite-nvidia:latest For users with Secure Boot enabled: Run ujust enroll-secure-boot-key and enter the password ublue-os if prompted to enroll the required key. Steam Deck/Home Theater PCs (HTPCs) Important Devices that are NOT the Steam Deck can still use the bazzite-deck images, but must use an AMD/Intel GPU. Variant designed for usage as an alternative to SteamOS on the Steam Deck, and for a console-like experience on HTPCs, available as bazzite-deck: Directly boots to Gamemode matching SteamOS's behavior. Automatic duperemove greatly trims the size of compatdata. Latest version of Mesa creates smaller shader caches and does not require them to prevent stutter. Able to be booted even if the drive is full. Support for every language supported by upstream Fedora. Uses Wayland on the desktop with support for Steam input. Features ported versions of most SteamOS packages, including drivers, firmware updaters, and fan controllers from the evlaV repository. Patched Mesa for proper framerate control from Gamescope. Comes with patches from SteamOS BTRFS for full BTRFS support for the SD card by default. Ships with a ported copy of SDGyroDSU, enabled by default. Option to install Decky Loader, EmuDeck, RetroDECK, and ProtonUp-Qt, among numerous other useful packages on installation. Custom update system allows for the OS, Flatpaks, Nix packages (Via Fleek), and Distrobox images to be updated directly from the Gamemode UI. Built in support for Windows dual-boot thanks to Fedora's installation of GRUB being left intact. Update break something? Easily roll back to the previous version of Bazzite thanks to rpm-ostree's rollback functionality. You can even select previous images at boot. Steam and Lutris preinstalled on the image as layered packages. Discover Overlay for Discord pre-installed and automatically launches in both Gamemode and on the Desktop if Discord is installed. View the official documentation here. Uses ZRAM(4GB) with the ZSTD compression algorithm by default with the option to switch back to a 1GB swap file and set a custom size for it if desired. Kyber I/O scheduler to prevent I/O starvation when installing games or during background duperemove and rmlint processes. Applies SteamOS's kernel parameters. Color calibrated display profiles for matte and reflective Steam Deck screens included. Default-disabled power-user features, including: Service for low-risk undervolting of the Steam Deck via RyzenAdj and Ryzen SMU, see ryzenadj.service and /etc/default/ryzenadj. Service for limiting the max charge level of the battery, see batterylimit.service and /etc/default/batterylimit. (Works even when the device is off) Built in support for display overclocking. For example, add GAMESCOPE_OVERRIDE_REFRESH_RATE=40,70 to /etc/environment. Ability to use X11 on the desktop if desired by editing /etc/default/desktop-wayland. 32GB RAM mod your Steam Deck? Enjoy double the maximum VRAM amount, automatically applied. (Can you share your soldering skills?) Steam Deck hardware-specific services can be disabled by running ujust disable-deck-services in the terminal, useful for trying this image on other handhelds or for use on HTPCs. More information can be found here on the Bazzite Steam Deck images. Warning Due to an upstream bug, Bazzite cannot be used on Steam Decks with 64GB eMMC storage at this time. Upgrading the storage resolves the issue. Important ISOs can be downloaded from our releases page here, and a helpful install guide can be found here. If you experience any issues with installing Bazzite, then check out our troubleshoot guide. If you're on an existing Universal Blue image follow these instructions. To rebase an existing upstream Fedora Silverblue/Kinoite ostree system to this image: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite-deck:latest GNOME Builds with the GNOME desktop environment are available in both desktop and deck flavors. These builds come with the following additional features: Variable refresh rate support and fractional scaling enabled under Wayland. Custom menu in the top bar for returning to game mode, launching Steam, and opening a number of useful utilities. (Only on Steam Deck builds) GSConnect preinstalled and ready to use. Features optional Valve-inspired themes matching Vapor and VGUI2 from SteamOS. Hanabi extension included to offer similar features to Wallpaper Engine in KDE. Numerous optional extensions pre-installed, including important user experience fixes. Automatic updates for the Firefox GNOME theme and Thunderbird GNOME theme. (If installed) Important ISOs can be downloaded from our releases page here, and a helpful install guide can be found here. If you experience any issues with installing Bazzite, then check out our troubleshoot guide. To rebase an existing ostree system to the desktop release: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite-gnome:latest To rebase an existing ostree system to the desktop with Nvidia drivers release: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite-gnome-nvidia:latest Warning Due to an upstream bug, Bazzite cannot be used on Steam Decks with 64GB eMMC storage at this time. To rebase an existing ostree system to the Steam Deck/HTPC release: rpm-ostree rebase ostree-unverified-registry:ghcr.io/ublue-os/bazzite-deck-gnome:latest Features from Upstream Universal Blue Flathub is enabled by default. ujust commands for convenience. Multi-media codecs out of the box. Rollback Bazzite from any build within the last 90 days. Features from Fedora Linux (Kinoite & Silverblue) A rock solid and stable base. System packages stay relatively up to date. Can layer Fedora packages to the image without losing them between updates. Security focused with SELinux preinstalled and configured out of the box. The ability to rebase to different Fedora libostree images, if desired, without losing user data. Printing support thanks to CUPS being preinstalled. Why Bazzite started as a project to resolve some of the issues that plague SteamOS, mainly out of date packages (despite an Arch base) and the lack of a functional package manager. Despite this project also being image-based, you are able to install any Fedora package straight from the command line. These packages will persist across updates (So go ahead and install that obscure VPN software you spent an hour trying to get working in SteamOS). Additionally, Bazzite is updated multiple times a week with packages from upstream Fedora, giving you the best possible performance and latest features - all on a stable base. Bazzite ships with the latest Linux kernel and SELinux enabled by default with full support for secure boot (Run ujust enroll-secure-boot-key and enter the password ublue-os if prompted to enroll our key) and disk encryption, making this a sensible solution for general computing. (Yes, you can print from Bazzite) Read the FAQ for details on what makes Bazzite stand out from other Linux operating systems. Showcase Documentation & Newsletters Installing and Managing Applications Updates, Rollbacks, and Rebasing Gaming Guide Dual Booting Guide Find additional documentation surrounding the project here. Check out our newsletters that get published on a regular basis for updates on the project. Custom Packages Ported SteamOS and ChimeraOS packages, among others used by Bazzite, are built on Copr in bazzite and bazzite-multilib. Package Status bluezdiscover-overlayds-inhibitduperemoveextestgamescopegamescope-session-plusgamescope-session-steamgamescope-shadersgalileo-muragnome-randr-rustgnome-shell-extension-bazzite-menugnome-shell-extension-caribou-blockergnome-shell-extension-hanabignome-shell-extension-compiz-windows-effecthhdjupiter-fan-controljupiter-hw-support-btrfsmangohudmesapipewirepowerbuttondpython3-hidrmlintryzenadjsdgyrodsusteamdeck-dspsteamdeck-gnome-presetssteamdeck-kde-presetssteamdeck-kde-presets-desktopsteam_notif_daemonudisks2upowervpowerwireplumberxorg-x11-server-XwaylandAdditionally, the following packages are used from other Copr repos: Package Status gcadapter_oc-kmodgnome-vrrhl2linux-selinuxjoycondkernel-fsynclatencyflex-vulkan-layernoise-suppression-for-voiceobs-vkcapturepromptrom-propertiessteamdeck-kmodsystem76-schedulerVTFLibwallpaper-engine-kde-pluginwebapp-managerVerification These images are signed with sigstore's cosign. You can verify the signature by downloading the cosign.pub key from this repo and running the following command: cosign verify --key cosign.pub ghcr.io/ublue-os/bazzite Contributor Metrics Special Thanks Bazzite is a community effort and wouldn't exist without everyone's support. Below are some of the people who've helped us along the way: rei.svg - For creating our logo and overall branding. evlaV - For making Valve's code available and for being this person. ChimeraOS - For gamescope-session and for valuable support along the way. Jovian-NixOS - For supporting us with technical issues and for creating a similar project. Seriously, go check it out. It's our Nix-based cousin. sentry - For assistance with needed kernel patches and for creating the kernel-fsync repo we now use. nicknamenamenick - For being the MVP nearly single-handedly upkeeping our documentation and support literature, and countless cases of helping users. Steam Deck Homebrew - For choosing to support distributions other than SteamOS despite the extra work, and a special thanks to PartyWumpus for getting Decky Loader working with SELinux for us. cyrv6737 - For the initial inspiration and the base that became bazzite-arch. Build Your Own Bazzite is built entirely in GitHub and creating your own custom version of it is as easy as forking this repository, adding a private signing key, and enabling GitHub actions. Familiarize yourself on keeping secrets in github. You'll need to generate a new keypair with cosign. The public key can be in your public repo (Your users need it to check the signatures), and you can paste the private key in Settings -> Secrets -> Actions with the name SIGNING_SECRET. We also ship a config for the popular pull app if you'd like to keep your fork in sync with upstream. Enable this app on your repo to keep track of Bazzite changes while also making your own modifications. Join The Community You can find us on the Universal Blue Discord and also discuss Bazzite on our Discourse forums.",
    "commentLink": "https://news.ycombinator.com/item?id=38828040",
    "commentBody": "Bazzite – a SteamOS-like OCI image for desktop, living room, and handheld PCsHacker NewspastloginBazzite – a SteamOS-like OCI image for desktop, living room, and handheld PCs (github.com/ublue-os) 282 points by goncalossilva 11 hours ago| hidepastfavorite70 comments erulabs 6 hours agoLove it! Living-room media servers will be the Trojan horse that brings self-hosting back into existence, and will inevitably transform the internet back into a more peer-to-peer oriented existence. Once most folks have symmetrical connections and powerful Linux systems, it’s only a software problem keeping people from using the internet as it was intended, equally publishers as they are consumers. reply mikepurvis 2 hours agoparentI&#x27;ve tried at several points to roll my own media center PC&#x2F;server setup and it&#x27;s always been a very far cry from what the fire stick does for like a tenth the price and zero effort.So yeah, I&#x27;m very keen for something plug and play in this space. reply heyoni 5 hours agoparentprevI love this take and absolutely hate what the centralized internet has become. My god, even basic searches don’t work on major platforms. reply sirspacey 4 hours agoparentprevThe Roku of the internet, I love this idea so much. reply Takennickname 4 hours agoparentprevNever thought about it like that. Very possible. reply freedomben 9 hours agoprevGlad to see this finally up here. I posted it a couple weeks ago and was shocked that I hadn&#x27;t heard of it sooner. I expected it to rocket to the top, but instead it just languished (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38642298).I&#x27;ve been blown away by Bazzite. I haven&#x27;t yet (so far at least) run into any downsides of being on Bazzite instead of SteamOS, yet have had numerous upsides. I&#x27;ve wanted my Decks to be on my tailnet for a long time, but that was not an easy proposition. There&#x27;s numerous packages that don&#x27;t flatpak well that I&#x27;ve wanted to install into my base OS but haven&#x27;t been able to until now! An example, I use Remote Play to run the games on my beastly desktop while viewing them on my TV. Something I routinely do is SSH into the host machine and run htop in one tmux pane and nvtop (for my AMD card, yes it works great with AMD now!) in another pane. To me this feels like the difference between driving with a speedometer and tachometer vs. driving without. Such a simple thing, so hard to do with Steam OS, yet so easy with Bazzite. reply ParetoOptimal 7 hours agoparentI also have my steam deck on my tailnet via `services.tailscale.enable = true` on NixOS.I might have to do a comparison of NixOS and bazzite since bazzite has me curious. reply bsimpson 2 minutes agorootparentI wasn&#x27;t sure whether to start with Bazzite or Jovian (NixOS), but I started with Jovian and haven&#x27;t looked back.I&#x27;d be curious to know what you find. reply Cloudef 3 hours agorootparentprevHere is my \"steamdeck os\" https:&#x2F;&#x2F;github.com&#x2F;Cloudef&#x2F;nixos-flake&#x2F;blob&#x2F;master&#x2F;modules&#x2F;s... reply bsimpson 3 minutes agorootparentextest is in a nixpkgs PR and SteamOS is packaged by Jovian. Any reason you&#x27;re doing it all manually? reply sweeter 3 hours agorootparentprevJovianOS is also similar to Bazzite, ChimeraOS and SteamOS but its based on Nix. reply westurner 9 hours agoprevTIL about various things for rpm-ostree distros:gnome-randr-rust: https:&#x2F;&#x2F;github.com&#x2F;maxwellainatchi&#x2F;gnome-randr-rust :> `xrandr` for Gnome&#x2F;wayland, on distros that don&#x27;t support `wlr-randr`Kernel-fsync: https:&#x2F;&#x2F;copr.fedorainfracloud.org&#x2F;coprs&#x2F;sentry&#x2F;kernel-fsync&#x2F;gnome-vrr: https:&#x2F;&#x2F;copr.fedorainfracloud.org&#x2F;coprs&#x2F;kylegospo&#x2F;gnome-vrr&#x2F; : gsettings set org.gnome.mutter experimental-features \"[&#x27;variable-refresh-rate&#x27;]\"obs-vkcapture: https:&#x2F;&#x2F;copr.fedorainfracloud.org&#x2F;coprs&#x2F;kylegospo&#x2F;obs-vkcapt...system76-scheduler: https:&#x2F;&#x2F;copr.fedorainfracloud.org&#x2F;coprs&#x2F;kylegospo&#x2F;system76-s... reply johnchristopher 9 hours agoprevOooh, looks cool ! I was just googling for something like \"pc with steamdeck like specs\" because I don&#x27;t need the portability, I can&#x27;t stand fan noise but I&#x27;d like Steam&#x27;s ease of use. Now there is a path to a \"normal\"&#x2F;quiet&#x2F;dedicated PC to easily run games. But I suppose there won&#x27;t be optimizations for any combinations of GPU&#x2F;CPU&#x2F;RAM like Valve and AMD do for the Steamdeck ?Also:> - Comes with patches from SteamOS BTRFS for full BTRFS support for the SD card by default.Interesting, what advantages does BTRFS bring in gaming&#x2F;steamdeck scenarios ?edit: juste read on https:&#x2F;&#x2F;gitlab.com&#x2F;popsulfr&#x2F;steamos-btrfs:> Btrfs with its transparent compression and deduplication capabilities can achieve impressive storage gains but also improve loading times because of less data being read. It also supports instant snapshotting which is very useful to roll back to a previous state.I guess it&#x27;s for easier rollbacks on the system and maybe rollback different versions of the same game ? reply tw04 9 hours agoparentCopy-on-write filesystems are inherently better for flash media because they never overwrite in place. They always allocate a new block and mark the \"old\" block freed when it is no longer referenced by the active filesystem (or snapshots if supported).Flash Media HATES overwriting data in place because it requires the block to be freed, then re-written.Now, modern flash firmware tries its best to allocate new blocks anyway, so some of it is a wash, but it is overall a better way to write to flash media. reply jcastro 6 hours agoparentprev> But I suppose there won&#x27;t be optimizations for any combinations of GPU&#x2F;CPU&#x2F;RAM like Valve and AMD do for the Steamdeck?I run a homegrown gaming htpc with an R5-5600, Radeon 6800XT, and then the Xbox wireless dongle and 4 Xbone controllers with Bazzite.You&#x27;d be surprised at how much of heavy lifting is done by the kernel and mesa stacks, that&#x27;s where the real work is done. Fedora does a good job pulling in kernel and mesa updates relatively quickly and the steam client handles the proton updates.There&#x27;s also great synergy between Bazzite, ChimeraOS, and Nobara, which are all gaming focused distros. Lots of code sharing and teamwork happening there, which is awesome to see. Everything is open for people to hack on.It acts like a big steamdeck, all the performance overlays work, all the xbox controllers work ootb, fsr works, etc. - you do need to pair the controllers with each controller but that&#x27;s a one time thing. I&#x27;ve personally completed God of War, Horizon Zero Dawn, Baldur&#x27;s Gate 3 and other AAA campaigns in 4k. And then when I need to travel all my game progress is on my deck. It&#x27;s a full multi device experience.But to set expectations: VR and multiplayer games that don&#x27;t opt into EAC or use kernel-level anti cheat are a no-op, as well as anything Epic makes. To me it&#x27;s just like any console platform, you get lots of games, and some games you can&#x27;t play. At this stage in the game both Windows and Linux suffer from the same UX shit show, horrible third party launchers are the worst problem with either set up.Disclaimer: I&#x27;m involved in universal blue but don&#x27;t directly contribute to bazzite. reply happymellon 2 hours agorootparentI&#x27;ve also built myself an HTPC for gaming on, and it works amazing since Valve gave us the Steam OS 3 UI.However I did it a while ago and went down the HoloISO route. Would you recommend that I switch out for something like Bazzite, or are the benefits good but not *that* good that I&#x27;d have to spend a day reinstalling all my games? reply jeroenhd 8 hours agoparentprevI use BTRFS with compression because my SD card is old (like, a decade old) and slow. (De)ompressing the assets takes a bit of CPU time but the slow I&#x2F;O is noticeably faster because of it.Deduplication can be useful if you store the Proton&#x2F;Wine runtimes on the same disk. Different games may need different runtimes, the latest version isn&#x27;t always the best, and a Wine environment without any games inside of it can take up a couple hundred megabytes just for DLLs and other common dependencies. Deduplicating can save a chunk of wasted storage, although with current flash storage prices that&#x27;s probably not worth worrying about in practice.Some people like the checksumming but IMO that&#x27;s not all that useful without ECC memory. reply KyleGospo 9 hours agoparentprevFedora actually defaults to BTRFS, in the case of SteamOS the system is BTRFS out of the box, and only home and the SD card are ext4.Main benefits are compression, and increased read speeds from compressed drives, especially from the MicroSD.BTRFS de-duplication also solves the issue of wine prefixes with similar dependencies taking up more space than needed. reply xvector 9 hours agorootparentWhy is it not the default? Compatibility issues with Windows games somehow? reply xyproto 8 hours agorootparentBTRFS is probably mature and stable by now, but it&#x27;s been a rocky road with several premature declarations of maturity. reply KyleGospo 8 hours agorootparentprevI can only speculate Valve&#x27;s reasoning, but there are a very select few games that require actual case folding and not the simulated case folding that wine offers. I know of literally only one. reply nephyrin 8 hours agorootparentSteamOS dev here - lack of case folding is one (but solvable, we supported development on native case folding for ext4), but general stability issues are the main concern. Our testing with btrfs has not been promising for deploying it in a zero-maintenance manner to many users and finnicky SD cards and have it Just Work, but we&#x27;re keeping an eye on things and weighing where we could contribute. reply jonny_eh 4 hours agorootparentIs there a way for adventurous users to opt into using BTRFS on SD cards? reply KyleGospo 3 hours agorootparenthttps:&#x2F;&#x2F;gitlab.com&#x2F;popsulfr&#x2F;steamos-btrfs reply freedomben 8 hours agorootparentprevCan you help me with some searchable terms for learning about case folding? Is that something specific to games? Specific to disk formats? reply KyleGospo 8 hours agorootparentIt&#x27;s a file system feature, all it means to be case folding is that capital and non-capital letters are treated the same, as NTFS does.All Linux filesystems will by default allow \"TEST\" and \"test\" and \"Test\" to exist in the same folder, which no Windows application is ever intended to handle. Wine works around this by default. reply anticensor 2 hours agorootparentNTFS is also case sensitive, however case folding is done at search time according to default windows settings.Windows apps are expected to handle case sensitivity gracefully in non-FAT filesystems. replyKyleGospo 5 hours agoparentprev> But I suppose there won&#x27;t be optimizations for any combinations of GPU&#x2F;CPU&#x2F;RAM like Valve and AMD do for the Steamdeck ?Every single one of these should be present, along with our own tweaks and changes made upstream at Fedora. reply attentive 8 hours agoparentprevcompression is good, checksums are good.It&#x27;s a shame other FS&#x27;s don&#x27;t have it. reply freetonik 10 hours agoprevSlightly related, but just today I discovered this SteamOS redistribution for generic machines (as long as they don’t have Nvidia graphics): https:&#x2F;&#x2F;github.com&#x2F;HoloISO&#x2F;holoiso reply dicknuckle 6 hours agoparenthttps:&#x2F;&#x2F;chimeraos.org&#x2F; I&#x27;ve been using this one, it updates the system atomically. reply 542458 6 hours agoparentprevOkay, so I read that page and I understand that nvidia graphics are very much a no-go for that distribution. I’m just wondering why? Full disclosure, I know very little about the finer points of GPU compatibility on Linux, but why isn’t it just a matter of installing nvidia’s closed source packages? reply SushiHippie 5 hours agorootparentIIUC As the SteamDeck does only use AMD GPUs they have no incentive to support their UI on Nvidia GPUs. reply puchatek 1 hour agoprevCould this replace a media center like Kodi for playing music and movies or is it just for games? reply b3nji 55 minutes agoparent> Could this replace a media center like Kodi for playing music and movies or is it just for games?I would like to know this too. reply sapphicsnail 9 hours agoprevI&#x27;m competing with this for the attention of my girlfriend and losing. reply evolve2k 10 hours agoprevWhat’s the motivation for the creators of this? And who’s backing it? Doesn’t feel like a weekend hobby project but rather a strategic open source play of some sort.Maybe something Nvidia related? reply KyleGospo 9 hours agoparentHi, I&#x27;m the original creator of this project. I&#x27;m happy to say the motivation is purely organic, we&#x27;ve no backing and have never received donations of any kind, project is totally out of pocket.Originally I wanted something akin to SteamOS that allowed packages to be installed and maintained through updates, and knew Fedora could offer that after using Silverblue for about a year, and it just kept evolving and growing from there.We just launched HDR in testing, and are working on custom kernel signing so we can move that to stable without breaking secure boot support.If you want to support us for the time being, all I can ask is that you give it a try and report any bugs you may find to us. The more users the better! reply attentive 5 hours agorootparentHi, it&#x27;s probably not your focus, but since you have Nvidia specific flavors, it would be great if browsers had hardware video decoding working. Either Firefox or chromium&#x2F;brave. I did a fair bit of distrohopping and yet to find a distro doing that easily for nvidia native drivers. reply KyleGospo 4 hours agorootparentThis should be working on AMD, Nvidia, and Intel. At most you may need to make minor configuration changes which we document in our wiki. reply attentive 4 hours agorootparentThat was my hope but it doesn&#x27;t work on bazzite-gnome-nvidia. I believe I tried bazzite-nvidia as well but it&#x27;s been a while. Mind pointing me to a relevant doc? - it&#x27;s not googleable. reply KyleGospo 3 hours agorootparenthttps:&#x2F;&#x2F;universal-blue.org&#x2F;images&#x2F;nvidia&#x2F;#video-playback reply janice1999 9 hours agorootparentprevWhen you say \"we\" do you mean other members of the Fedora community? Do any of you work at RedHat? Just wondering how you got a complex project like this up and running. reply KyleGospo 8 hours agorootparent\"We\" is myself and two others that call ourselves maintainers, and the group at Universal Blue who brought us into their circle and shared knowledge. My personal repo at https:&#x2F;&#x2F;GitHub.com&#x2F;KyleGospo&#x2F;Bazzite is a time capsule of the project at it&#x27;s beginning when it was only me.Universal blue has a ton of different people contributing to it, but there&#x27;s no corporate backing there either. reply janice1999 8 hours agorootparentThanks for the info. Looks really interesting. reply ParetoOptimal 7 hours agorootparentprevDo you know of NixOS and jovian NixOS? Did you consider either of those?I may compare them in detail in the future, any thoughts on how they compare? reply aliasxneo 3 hours agorootparentSimilar premise, but not as technically reproducible. You can create and manage your own custom images using the blue-os build systems, it&#x27;s dead simple. Basically you&#x27;re just writing a Dockerfile. Most of the filesystem is read-only like NixOS, but it acts like a traditional Linux system with FHS support.It&#x27;s that last part that made me switch to it from NixOS. I was tired of random things not working because of Nix&#x27;s opinions on how things should work. I run Bluefin Linux with Fleek and get all the benefits I liked about Nix with none of the nonsense. reply brirec 10 hours agoparentprevIt’s a variant of Universal Blue (https:&#x2F;&#x2F;universal-blue.org) reply Apocryphon 5 hours agoprevBehold: Bazzite on the trash can Mac Pro!https:&#x2F;&#x2F;youtu.be&#x2F;te1AEj_RA64 reply stavros 7 hours agoprevThis looks interesting, but what do I lose by switching to it? Does the Steam interface still work? The hardware? The games? reply JustinGarrison 5 hours agoparentYep! Everything works as it does with SteamOS but better (more fixes) and a better desktop environment.Installation is pretty quick too! Here’s a video I made showing the process https:&#x2F;&#x2F;youtu.be&#x2F;doQW1FyAISQ reply figmert 4 hours agoprevAny one know if there is a reason not to install this on my SD? The Waydroid installation is very interesting. How well does it work? Would you consider this a bit bloated? reply yjftsjthsd-h 10 hours agoprev> Bazzite is an OCI image that serves as an alternative operating system for the Steam DeckI&#x27;m not quite following - this is a host OS that runs things in containers, or the OS inside a container? reply gclawes 10 hours agoparentFedora&#x27;s OSTree recently-ish started supporting using OCI containers as the content-addressed image backend in addition to their original git-like one:https:&#x2F;&#x2F;coreos.github.io&#x2F;rpm-ostree&#x2F;container&#x2F; https:&#x2F;&#x2F;fedoraproject.org&#x2F;wiki&#x2F;Changes&#x2F;OstreeNativeContainer... reply curt15 10 hours agorootparentAre OCI images content-addressed? reply pixelatedindex 9 hours agorootparentWhat does content-addressed mean? reply gclawes 7 hours agorootparentThe identifier for the content (image layers + manifest) is the hash of the content, and the has serves as an \"address\", the registry repo + tag is basically a pointer to a hash. reply justincormack 10 hours agorootparentprevYes reply jcastro 10 hours agoparentprevThe OS is delivered to you via a container (via the github registry), but can also run things in containers.The OS itself is run on the bare metal. reply jabbequbs 4 hours agoprevWhat does OCI mean in this context? My best guess is Oracle Cloud Infrastructure, but that doesn&#x27;t seem right... reply mkotowski 3 hours agoparentI guess this refers to OCI image format from Open Container Initiative: https:&#x2F;&#x2F;opencontainers.org&#x2F;Here is Containerfile from the repo: https:&#x2F;&#x2F;github.com&#x2F;ublue-os&#x2F;bazzite&#x2F;blob&#x2F;main&#x2F;Containerfile reply brirec 10 hours agoprevWhat’s with the 0 in the title? reply tavavex 6 hours agoprevMakes me wonder - how viable is it to install actual SteamOS on devices that aren&#x27;t the Steam Deck (and use the Deck UI and all)? Are there any mods or something to make that work? reply sebazzz 9 hours agoprevHow does this compare to Winesapos? reply SubiculumCode 4 hours agoprevI decided to ditch windows. Installed Budgie, and I installed Steam.What is the advantage of Steam OS or Bazzite over Ubuntu variant + Steam? reply westurner 7 hours agoprevWhat could be merged into Fedora and e.g. Gnome? reply attentive 8 hours agoprev [–] 10Gb of \"everything is a Flatpak\" isn&#x27;t exciting. reply KyleGospo 8 hours agoparent [–] The only flatpaks here are ones installed by the user after setup, or ones installed by Fedora (Firefox for instance, you don&#x27;t want your browser updating only when your OS does).Steam, gamescope, and nearly every other listed feature are native packages. reply attentive 5 hours agorootparent [–] Brand new first boot Bazzite Gnome. That&#x27;s 7Gb of flatpaks. $ flatpak list --columns=size,namegrep MB| grep -Po &#x27;\\d+\\.\\d+&#x27;| paste -sd+bc 7025.2 $ flatpak list --columns=size,namewc -l 54 $ df -h &#x2F; Filesystem Size Used Avail Use% Mounted on &#x2F;dev&#x2F;sda3 232G 15G 214G 7% &#x2F; reply nicknamename 5 hours agorootparent [–] The GNOME images have more Flatpaks preinstalled than the KDE Plasma images currently.KDE image Flatpaks: https:&#x2F;&#x2F;github.com&#x2F;ublue-os&#x2F;bazzite&#x2F;blob&#x2F;main&#x2F;system_files&#x2F;d...vs.GNOME image Flatpaks: https:&#x2F;&#x2F;github.com&#x2F;ublue-os&#x2F;bazzite&#x2F;blob&#x2F;main&#x2F;system_files&#x2F;d...If you&#x27;re looking to save a few gigabytes, then I suggest using the KDE variants. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bazzite is an OCI image that provides an alternative operating system for the Steam Deck and other devices, resolving issues with SteamOS.",
      "It includes various features and customization options, support for AMD GPUs, and fixes for bugs in Source 1 engine games.",
      "Bazzite supports Discord screensharing on Wayland, offers a Webapp Manager, and has different variants for desktop computers and the Steam Deck. It is a community-driven project with documentation and newsletters, and users can build their own custom version."
    ],
    "commentSummary": [
      "The post explores various operating systems, including Bazzite, NixOS, ChimeraOS, and SteamOS, highlighting their respective benefits.",
      "The BTRFS file system is discussed in relation to gaming scenarios, but limitations are mentioned regarding compatibility with VR, certain anti-cheat measures, and games from Epic.",
      "The post also covers the development and features of ChimeraOS, discussing its potential as a media center. Bazzite and Universal Blue are mentioned as projects for AMD, Nvidia, and Intel.",
      "The installation and functionality of Bazzite as an alternative operating system for the Steam Deck are explained, with comparisons made to Winesapos.",
      "The user suggests using KDE variants to save storage space and provides links to compare the number of Flatpaks in KDE and GNOME images."
    ],
    "points": 281,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1704061316
  },
  {
    "id": 38823817,
    "title": "Using Email Addresses as \"Permanent\" Account IDs: A Flawed Approach",
    "originLink": "https://utcc.utoronto.ca/~cks/space/blog/tech/EmailAddressesBadPermanentIDs",
    "originBody": "Chris Siebenmann :: CSpace » blog » tech » EmailAddressesBadPermanentIDs Welcome, guest. Email addresses are not good 'permanent' identifiers for accounts December 30, 2023 Every so often someone needs to create a more or less permanent internal identifier in their system every person's account. Some of the time they look at how authentication systems like OIDC return email addresses among other data and decide that since pretty much everyone is giving them an email address, they'll use the email address as the account's permanent internal identification. As the famous saying goes, now you have two problems. The biggest problem with email addresses as 'permanent' identifiers is that people's email addresses change even within a single organization (for example, a university). They change for the same collection of reasons that people's commonly used names and logins change. An organization that refuses to change or redo the email addresses it assigns to people is being unusually cruel in ways that are probably not legally sustainable in any number of places. (Some of the time there will be some sort of access or forwarding from the old email address to the new one, but even then the old email address may no longer work for non-email purposes such as OIDC authentication. And beyond that, the person won't want to keep using their old and possibly uncomfortable email address with you, they want to use their new current one.) The lesser problem is that you have no particular guarantee that an organization won't reuse email addresses, either in general or for particularly desirable ones that get reused or reassigned as an exception because someone powerful wants them. Sometimes you sort of have no choice, because account recovery has to run through the email address you have on file, but at other times (such as in theory with OIDC), you have some form of internal ID that is supposed to be unique and permanent, which you should use. Even if you have to remember an email address for account recovery, you want your internal identifier for accounts to be meaningless. This will make your life much simpler in the long run, even if this is never exposed to people. (There are also security issues lurking in the underbrush of reading too much into email addresses, cf (via).) (One comment.) Written on 30 December 2023. « Your kernel panics in ZFS on Linux probably aren't actual kernel panics Switching Linux software RAID disks around in (early) 2023 » These are my WanderingThoughts (About the blog) Full index of entries Recent comments This is part of CSpace, and is written by ChrisSiebenmann. Mastodon: @cks Twitter @thatcks * * * Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web Also: (Sub)topics This is a DWiki. GettingAround (Help) Search: Page tools: View Source, Add Comment. Search: Login: Password: Atom Syndication: Recent Comments. Last modified: Sat Dec 30 23:22:46 2023 This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "commentLink": "https://news.ycombinator.com/item?id=38823817",
    "commentBody": "Email addresses are not good &#x27;permanent&#x27; identifiers for accountsHacker NewspastloginEmail addresses are not good &#x27;permanent&#x27; identifiers for accounts (utoronto.ca) 264 points by throw0101b 20 hours ago| hidepastfavorite382 comments buro9 19 hours agoThere is no good identity.Emails change, people lose access to old emails.People dislike usernames, they want to be able to choose non-unique ones rather than end up with user53267 or something inane.People lose devices, just storing a secret UUID in their cookie, or using a passkey from their device isn&#x27;t going to work.There is no ideal solution except to blend a variety of things together, for some people email is pretty stable for long time and they like it as the identity, for others their usernames are stable and they prefer that as the identity... though I know of no-one that has had the same primary device for more than years (not decades) so perhaps that one will never work.I do think this is important though, where it comes up a lot is a work email account, a first.last@company.com, and how all of the vendor software utilises \"Sign in with Google\", and it&#x27;s the email address they then store in the vendor app as the identifier...People get married, people get divorced, people transition, people move culture and choose new names... names change, and so do email addresses.Perhaps OIDC and the like needs a new extension: a standard API to change a username, and a standard API to change an email address. reply poulpy123 16 hours agoparentThe oldest email that I can connect to is more than 20 years old. I&#x27;m not using it anymore but it&#x27;s older than any of my phone number or physical address. I don&#x27;t think we can do better, except with official indentifiers like I&#x27;d card or social security number reply kube-system 14 hours agorootparent> I&#x27;d card or social security numberAnd even those change reply lostlogin 13 hours agorootparentAnd most the world doesn’t have them. reply szszrk 3 hours agorootparentBecause most world has a generic identity numbers for residents, not those related to social security: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;National_identification_number reply turquoisevar 12 hours agorootparentprevOr won’t allow them to be recorded by private entities for privacy reasons even when they do. reply seydor 19 hours agoparentprevPeople also have a right to lose everything and start a new life. this is something that people could do just a few decades ago reply popcalc 18 hours agorootparentRegistered sex offenders in many U.S. states lose this right.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eWPtAJS1kro reply stevenpetryk 18 hours agorootparentthat makes sense reply diggan 18 hours agorootparentDoesn&#x27;t some states have really arbitrary rules for what constitutes \"enough\" to be called a sex offender? Things like visiting a prostitute, urinating in public, consensual sex between two teenagers and more requires you to register as a \"sex offender\" in some states. Should that suddenly mean you shouldn&#x27;t be able to start anew online? reply javajosh 17 hours agorootparentHow is urinating in public a sex offense? I&#x27;m releasing urine not having sex with it. reply codetrotter 17 hours agorootparentBecause in order to pee you have to expose your genitalia. I suppose the laws were made so that anyone exposing genitalia could be charged for it.That being said, it is completely stupid that people are actually made to register as sex offenders when they really were just peeing. Doubly so when they did so without anyone other than themselves seeing their private parts. reply kshacker 15 hours agorootparent> in order to pee you have to expose your genitaliaThey should visit India once. Of course many people will pee against an object (wall, tree) but at times not so hiddenIncidentally saw this funny discussion yesterday (the comments are funny): https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;unitedstatesofindia&#x2F;comments&#x2F;18ufbu...I think these United States have gone way above and beyond their duty as far as calling people sex offenders (not saying that Indians are right) reply popcalc 10 hours agorootparentYou just need to go as far as Budapest. reply bigstrat2003 16 hours agorootparentprevBecause the law is badly written sometimes. reply pc86 16 hours agorootparentprevPee in public within a certain distance of a school and you&#x27;ll probably find out. reply mythrwy 9 hours agorootparentprevI know a guy that happened to. He was drunk and urinating by the side of the road in plain view and wound up registered as a sex offender. West Texas.This is his story that I&#x27;ve not independently verified, but I don&#x27;t think he was an actual sex offender from what I knew of him. He was certainly a drunk though. reply pacifika 16 hours agorootparentprevSource? reply fantasybroker 16 hours agorootparenthttps:&#x2F;&#x2F;slate.com&#x2F;news-and-politics&#x2F;2014&#x2F;08&#x2F;mapped-sex-offen... reply philwelch 17 hours agorootparentprevYes. If you don’t think some offenses should be punished with sex offender registration, we can discuss changing those particular statutes, but that’s no reason to allow violent rapists and child molesters to evade registration. reply AnthonyMouse 14 hours agorootparentEliminating registration requirements so we don&#x27;t have to institute a tracking system and prevent anyone else from starting over when >99% of the public is not a sex offender is not \"evading registration\", it&#x27;s eliminating it.If someone is still a danger to the public then maybe don&#x27;t release them from prison. reply jrockway 14 hours agorootparentI think it&#x27;s a cost&#x2F;benefit thing. People react strongly to sex crimes, and many people don&#x27;t agree with the state the 20 years in prison or whatever was enough, so this allows the state to spend $0 on incarcerating someone and still placate those voters.I don&#x27;t know why we don&#x27;t do this for all crimes, though. Why no murderer registration list or bad-check-writer registration list? I guess those we outsource to the private sector. reply AnthonyMouse 13 hours agorootparentPeople want rapists and pedophiles to burn in hell. They don&#x27;t feel the same about bad check writers, and first degree murder already carries the death penalty in the relevant jurisdictions, but sex crimes generally aren&#x27;t punishable by death.It&#x27;s probably because sex crimes are extremely hard to prove. If there is a murder, somebody is dead and somebody killed them. It&#x27;s not that ambiguous. Consent can be extremely ambiguous. It drives people mad because you want every rapist to be six feet underground and every falsely accused innocent person to be free but there are all too may cases, plausibly the majority of cases, where there is no way for the system to conclusively distinguish them.And then we hesitate to kill rapists because we&#x27;re uncomfortable about that when it&#x27;s so easy to get it wrong, but if we don&#x27;t, people feel the guilty ones are being insufficiently punished. This is kind of a recipe for ending up with bad laws. reply pseudalopex 11 hours agorootparentSex offender registries are not limited to rapists and pedophiles. Jurisdictions without death penalties have sex offender registries. And some other crimes receive longer prison sentences. reply AnthonyMouse 10 hours agorootparentSex offender registries are for rapists and pedophiles. Putting people on them who were arrested for urinating in public is the sort of negligence that rarely gets addressed because it doesn&#x27;t affect a powerful lobby. But that kind of intractable bureaucratic scope creep is another argument for not having them.The sentence isn&#x27;t the longest of any crime for the same reason it isn&#x27;t death (or, for the jurisdictions without the death penalty, the same as their penalty for murder). reply pseudalopex 9 hours agorootparentSex offender is not a natural category. Registration statutes specify relevant crimes. People who urinated in public were compelled to register because statutes specified genital exposure. Buyers and sellers of consensual sex between adults were compelled to register because statutes specified consensual sex between adults.The difficulty of proving rape is why rape conviction rates are low. Imprisonment for rape is shorter than imprisonment for murder because most people consider murder worse than rape. reply AnthonyMouse 8 hours agorootparentPeople who urinated in public were compelled to register because statutes specified genital exposure in order to cover flashers but were overly broad and non-discretionary, causing an absurd result. Statutes covered flashers and prostitution because of scope creep; once you create a machine people want to use it for things. It&#x27;s fairly obvious that there would be no registry to begin with if there were no rapists or pedophiles.> The difficulty of proving rape is why rape conviction rates are low.Even despite the low conviction rate, rape convictions are reversed at a higher rate than other crimes, because it&#x27;s so difficult in those cases to know the truth.> Imprisonment for rape is shorter than imprisonment for murder because most people consider murder worse than rape.Many countries historically imposed the death penalty for rape and a few still do. Typically the ones less concerned about proof beyond a reasonable doubt in general.People viscerally hate rapists and, even more, pedophiles. Murder is nominally worse but not by much. The penalty for rape and second degree murder are typically about the same. In multiple US states the penalty for rape is higher. reply lostlogin 13 hours agorootparentprev> People want rapists and pedophiles to burn in hell.Do they? A quick mental scan of well know paedophiles suggests that famous ones face limited repercussions. reply AnthonyMouse 13 hours agorootparent\"Powerful people evade justice\" is a different issue. That doesn&#x27;t really depend on what they did. reply philwelch 6 hours agorootparentprevThat’s what criminal background checks are for. Lots of criminal convictions have lifelong consequences, many of which are meant to prevent recidivism. People who are convicted of violent felonies are barred from owning firearms, people who are convicted of securities fraud are barred from working in positions where they are capable of committing securities fraud, and people who are convicted of sexual offenses are kept away from children. All of these restrictions seem reasonable to me. reply philwelch 7 hours agorootparentprevI would gladly support life sentences or even the death penalty for rapists and child molesters, but unfortunately a lot of people like to complain about “mass incarceration” so most states don’t have the ability to fund the necessary prison facilities. If we’re going to let them out of prison, the least we can do is to try and keep these people away from children. reply scotty79 16 hours agorootparentprevIt&#x27;s kind of weird that sex offences are treated specially. I&#x27;d like to know if I live around someone who gave in to plain violence as well or if someone stolen something significant enough.Either we are giving people second chance without stigma or we are tracking everyone forever. I&#x27;m fine with both. reply spencerchubb 16 hours agorootparentprevSex offenders should lose rights. This video is trying to draw sympathy for a 19 year old hooking up with a 14 year old, and that is plain evil. reply fantasybroker 16 hours agorootparent\"Sex offender\" at least in the US is a ridiculous classification that includes people who peed in a fountain while drunk. I don&#x27;t think they should be punished for the rest of their life and be on a public list with serial rapists. reply spencerchubb 9 hours agorootparentOkay. I can agree that that subset of sex offenders shouldn&#x27;t be defined as sex offenders. reply noidea_ 15 hours agorootparentprevNo it doesn&#x27;t. There are zero people on the registry for urinating in public. This is a myth that pedophiles like to claim.You&#x27;ll find thousands of articles about it. You won&#x27;t find a single case. reply fantasybroker 14 hours agorootparent> No it doesn&#x27;t. There are zero people on the registry for urinating in public. This is a myth that pedophiles like to claim.Yes it does. Puppy killers like to claim that it doesn&#x27;t. &#x2F;sHere are a few examples with names: https:&#x2F;&#x2F;www.menshealth.com&#x2F;trending-news&#x2F;a19541024&#x2F;you-might...There is a ton of posts from lawyers confirming the fact that in many states you can get convicted for public urination and put on the sex offender list. For example, here is a post from a Texas-based lawyer: https:&#x2F;&#x2F;www.craiggreeninglaw.com&#x2F;can-you-really-become-a-sex...The reason it&#x27;s so hard to estimate the real number of these cases is that the crime is listed as indecent exposure or lewdness. reply goatlover 10 hours agorootparentprevThis isn&#x27;t reddit. And that&#x27;s a terrible form of argument. replytoomuchtodo 18 hours agorootparentprevHighly dependent on jurisdiction. reply newsclues 18 hours agorootparentprevOh really?As someone that once faced serious jail time for plants, I think that would have been a nice option, but I wasted two years of my life in court&#x2F;etc. reply micromacrofoot 18 hours agorootparentprobably should have learned that you forfeit certain rights when you commit a crime reply direwolf20 18 hours agorootparentlike possessing the wrong non-toxic plant reply talldatethrow 16 hours agorootparentA police officer once told me officers liked having something concrete and easy to catch someone with. His mode of thinking was \"if someone is willing to risk jail for something as simple as weed, imagine what else they&#x27;d do for a greater reward\" reply sowbug 15 hours agorootparentIn my first year of law school, my criminal-law professor often mentioned that a fundamental value of the US justice system is to \"punish bad acts, not bad people.\" I took my professor&#x27;s statement on faith, but 30+ years later I wish I&#x27;d asked for a citation. It seems like your cop friend would have twisted the value to \"use bad acts as an excuse to punish bad people.\" I&#x27;m sure it&#x27;s easy to find plenty of Americans who think that&#x27;s an excellent mission for their neighborhood police force. Which makes me sad, because it means that one&#x27;s \"badness\" isn&#x27;t defined by one&#x27;s actual choices.I agree that bad people do exist. And I&#x27;m sure that prosecuted crimes are committed disproportionately by those we&#x27;d define as \"bad people.\" But I don&#x27;t think that bad acts committed by good people are any better than those committed by bad people. And I don&#x27;t think that a bad person who resists committing bad acts should be treated worse than a good person who also resists committing them. reply mock-possum 15 hours agorootparentprevSounds like exactly the sort of person that should not be a cop reply talldatethrow 14 hours agorootparentMaybe. He was pretty convincing. The above goes along with his statement on \"non violent drug offenders\". He thinks it&#x27;s a false concept, because again most people risking jail time transporting 5lbs of weed also do other bad things, but you just can&#x27;t catch them doing it or prove it. If a drug dealer doesn&#x27;t get paid by someone, they usually ARE violent with that person. If someone snitches to the police, they are violent with that person. If you insult a drug dealer casually on the street, they are violent with that person. They do carry weapons with them often. But catching them being violent or using their weapons is pretty hard. Catching them with the 5 lbs is easier. And then they get labeled non violent. reply AnthonyMouse 14 hours agorootparent> If a drug dealer doesn&#x27;t get paid by someone, they usually ARE violent with that person. If someone snitches to the police, they are violent with that person. If you insult a drug dealer casually on the street, they are violent with that person.All of this is caused by the drugs being illegal, which prevents them from seeking justice in the usual way if someone steals from them or rips them off etc.If you were to legalize drugs and release them, they would no longer have any incentive to resort to violence.And the same logic applies to people in other situations. If you regularly get shaken down by two bit thugs and the police do nothing no matter how many times you report it, you&#x27;re likely to eventually take matters into your own hands, even though that isn&#x27;t exactly legal. So should the police use any excuse they can to arrest you, for the situation they caused, because people in that situation are disposed to commit a violent crime the state can&#x27;t even prove? reply talldatethrow 12 hours agorootparentOh yes, you often hear about small time drug dealers taking the person who wronged them to small claims court for other matters. reply AnthonyMouse 12 hours agorootparentAbout as often as you hear about it for anyone else.It&#x27;s called bargaining in the shadow of the law. If you have the ability to take someone to court, they know that too, and know you&#x27;d win, so they&#x27;re willing to settle up with you peacefully rather than both of you incurring the costs of going to court.When a small time drug dealer&#x27;s car needs service covered under warranty, they don&#x27;t go in and threaten the dealership with violence, they drop off the car and expect the dealership to honor the contract. Which the dealership does in the same way as they do for anyone else, because the dealership doesn&#x27;t know anything about their extracurricular criminal activity and can&#x27;t use it to renege on their obligations by threatening to turn them in to the police. reply MeImCounting 14 hours agorootparentprevThis is really hilarious to read.You must not have met many weed dealers before have you? Specifically the examples where a \"drug dealer\" would be violent are so divorced from my own lived experience that it really makes me wonder if this is how some of society really thinks weed dealers act like. reply talldatethrow 12 hours agorootparentIs it possible your weed dealer acts differently with you then he does in the other parts of his life? He&#x27;s clearly ok with jail time. Are you? If not, seems like you guys are slightly different people and you may not know him well. reply MeImCounting 11 hours agorootparentWell, having lived with and generally hung out with these \"drug dealers\" for most of my youth the idea of them secretly having a set of circumstances where they would act violently that they keep secret from their family and friends is certainly possible I suppose.Its also a really funny idea.I get it you think to be a weed dealer you need to be some kind of hardened criminal who commits all sorts of various crimes or whatever, but im here to break it to you that the hollywood criminal drug dealers youre imagining dont really exist. Many people do all sorts of illegal things because we live in a country with many overly restrictive and draconian laws. Very few of those people are the dangerous criminals you are imagining. Most of them are regular people who dont have the religious reverence for arbitrary laws that some other memebers of our society have. reply throwaway902984 14 hours agorootparentprev> He thinks it&#x27;s a false concept, because again most people risking jail time transporting 5lbs of weed also do other bad thingsThat isn&#x27;t the case IME, most people in the black market dealing with weed are pretty non-violent up until you are talking millions of dollars, which 5lbs will never be. They tend to only deal in weed or weed derivatives at those levels as well. It just doesn&#x27;t make economic sense to do otherwise. What little bloods and crips style stuff of favela fame is a lot less common now too from what I can tell. reply talldatethrow 12 hours agorootparentPeople robbing a weed store shot and killed a police officer in Oakland a few days ago. Nice people. reply pseudalopex 11 hours agorootparentPeople robbing a car dealership shot a police officer in Ohio a few months ago.[1] What do you think this proves about car dealers?[1] https:&#x2F;&#x2F;www.nbc4i.com&#x2F;news&#x2F;local-news&#x2F;columbus&#x2F;bank-robbery-... reply throwaway902984 10 hours agorootparentprevRegular ol&#x27; white market stores getting robbed is like any other smash and grab robbery, but you&#x27;re not associating weed with violence rn any more than you are jewelry stores with violence. replypc86 16 hours agorootparentprevSo change the law, but \"this thing shouldn&#x27;t be illegal\" is a completely separate discussion from \"breaking this law should carry no penalty.\" reply sowbug 15 hours agorootparentWhat penalty was appropriate for people who engaged in miscegenation in the United States before June 12, 1967? reply lostlogin 13 hours agorootparent> What penalty was appropriateA life sentence.Silliness aside, I didn’t know this was a thing. Yikes.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Miscegenation reply fantasybroker 16 hours agorootparentprevNot sure why you have such a strong opinion on this. There are countless examples of people suffering immense harm for trivial things that are considered unlawful in some jurisdictions. And no, \"so change the law\" is not the full answer, that&#x27;s just blaming the victim. reply pc86 16 hours agorootparentNot sure why you think I have such a strong opinion.You are not a victim if you break the law, and it&#x27;s not victim blaming to say that you should follow the law and work to change it if you disagree with it. Plenty of people never smoked pot when it was illegal then started using it recreationally when it was legal recreationally. If you want to break the law I could care less for something victimless like low level recreational drug use but don&#x27;t be surprised when some regressive jurisdiction nails you for it and punishes you. That doesn&#x27;t make you a victim, it makes you someone who got caught breaking the law. reply MeImCounting 14 hours agorootparentSo your thesis is that just because some arbitrary thing is against some law then nobody should use the word victim to describe people having their lives ruined by the enforcement of said law? Because laws are somehow divine and perfect by virtue of them just existing or something? Nobody has ever been the victim of any repressive system of government or \"justice\" because it was \"the law?\"Many people are victims of unjust laws from laws against pot to laws against homosexuality. Thousands of people across the US and across the world are routinely victimized by unjust laws and corrupt police. reply lostlogin 13 hours agorootparentThis.Pick pretty much any disadvantaged group and there will be or have been a law that keeps them in their place. I guess they should just follow the rules and complain via the methods the oppressors specify? reply AnthonyMouse 12 hours agorootparentNot only that, you might be able to make that case in a hypothetical system where most people are law abiding, e.g. because there are few laws and only against well known and unambiguously legitimate offenses like murder and theft. Never in one where laws are numerous and broad and commonly violated but selectively enforced.\"You lose your rights if you break the law\" in the legal system of Three Felonies A Day is equivalent to no one having any rights. reply MeImCounting 12 hours agorootparentprevThe thing is that there is a huge amount of indoctrination from schools to popular media that encourages people to just accept \"Law\" as somehow of divine providence. Many many people go their entire lives without considering the possibility that the primary function of many laws and \"justice\" systems would be to oppress and victimize people. reply pc86 11 hours agorootparentprevI don&#x27;t think getting high and being gay are in any way comparable. reply MeImCounting 11 hours agorootparentThey are comparable in that they can both get you thrown in prison in various places at various times by people who have a religious mentality when it comes to the nature of the \"justice\" system.I hope I can assume that you retract your earlier statement about the term \"victim\" as it relates to people having their lives destroyed by unjust laws.From the miscegenation laws mentioned earlier to anti-weed laws to anti-gay laws to any number of other laws, much of our current system of creating and enforcing laws is based around victimizing various people. reply pc86 9 hours agorootparentMiscegenation and homosexuality aren&#x27;t choices, getting high is. If you&#x27;re punished for being with someone you love you&#x27;re a victim, if you&#x27;re punished for getting high you&#x27;re not. reply MeImCounting 7 hours agorootparentBy your logic it makes perfect sense to say \"Its a choice to be with someone of a different race. You could perfectly well just choose not be with someone of a different race. Since its a choice you are not a victim.\"It seems the line is arbitrarily drawn at \"getting high\". Is this because you dont personally like the idea of other people getting high or is there some other logical basis for your determination?Say holding some religious belief is illegal. By all accounts what religion you believe in is a choice. Is the person who is punished for their religious belief a victim? They could have just chosen not to hold that belief but they \"went against the law\". So theyre not a victim?I am going to disengage from this conversation as I suspect this stance has less to do with any logical reasoning and more with personal conviction. Which is fine, no shade on anyone for their personal convictions. As long as they dont go around trying to force that on other people. ;) reply fantasybroker 11 hours agorootparentprevIt’s a list, not a comparison. reply fantasybroker 16 hours agorootparentprevHow do you keep track of all local, state, and federal laws? replyuxp8u61q 17 hours agorootparentprevOnce you&#x27;ve served your sentence you should recover all your rights. Lifelong punishment is a violation of human rights - those rights that you cannot \"forfeit\" or lose. reply sokoloff 15 hours agorootparentI believe there exist some set of crimes for which lifelong punishment is appropriate. Which crimes are in that set is a fine topic for debate and some may argue that the null set is correct. For me, the set is not-null.(Simple drug possession, of any drug, in any amount, is not among that set for me.) reply uxp8u61q 12 hours agorootparentTake it up with the UNHRC. reply sokoloff 12 hours agorootparentI have way better uses for my time than to argue with them in either direction.They can hold whatever stance they do and I&#x27;ll do the same. reply uxp8u61q 12 hours agorootparentAs if the opinion of a random person on the internet had the same value as the policy of an organization established by the general assembly of the UN. reply sokoloff 10 hours agorootparentIf it helps, I value your opinion, random internet commenter, on equal footing with the UN’s. replypopcalc 18 hours agorootparentprevThe GP didn&#x27;t specify which rights they were referring to. The parent probably assumed they mean natural rights. I think that&#x27;s an understandable assumption. reply mock-possum 15 hours agorootparentprevIt should never have been a crime in the first place. Punishment for breaking an unjust law is an injustice. The state owes restitution to the person you replied to. You should be ashamed for dismissing their suffering so carelessly. reply d0gsg0w00f 14 hours agorootparentIf you can just pick and choose which laws are just and unjust then society breaks down.As another commenter said, it&#x27;s far better to focus your efforts on changing the laws. If they are truly unjust, then changing them will have wide support. We can already see this with the relaxation of Marijuana laws.Even if you personally disagree with something and think it should be changed, you still have to \"read the room\" and see if society agrees with you. If not, then bide your time and wait for society to catch up. replyhibikir 17 hours agoparentprevMy \"favorite\" of those email changes is the self-inflicted contractor postfix change. I&#x27;ve worked at places where a conversion to employee forces the creation of a completely new account, and lacking a simple, single permission systems, the act of converting means spending a good 3 weeks trying to get access to systems one had access the day before.This is extra fun when the company in question does a lot of their business offering complicated accounts to customers, and has an external facing identity solution that deals with all of this easily: just not for their own workers, including those maintaining the external-facing identity system. reply BigTuna 17 hours agorootparent>just not for their own workersThe shoemaker&#x27;s children always go barefoot reply wlonkly 9 hours agorootparentprevIn a thread about email, \"postfix\" really threw me here! But I realize you don&#x27;t mean the email server, but I&#x27;m not sure what you do mean. reply oooyay 17 hours agoparentprevI liked Discords old scheme where you had an email and a display name. Everyone had the numbers so they didn&#x27;t matter. When they switched to unique account IDs I was kind of bummed, but I&#x27;m still curious why they switched. reply sgjohnson 14 hours agorootparent> but I&#x27;m still curious why they switched.Because it was trivial to impersonate someone. The old usernames supported full UTF-8, enabling a wide variety of attacks.And by paying for nitro, you could choose the numbers too, so step 1. Find someone you want to impersonate, copy their username, and replace one character with a visually identical one, but a different one. Step 2 - pay for nitro, and choose the numbers too.As usual, malicious actors are the reason why we can’t have nice things. reply jrockway 14 hours agorootparentI don&#x27;t think they solved the impersonation problem. You can still look exactly like someone else, and you can only figure out that person X isn&#x27;t who you think they are upon investigation. Most people don&#x27;t investigate anything except the display name and profile picture, so are still vulnerable to impersonation.As far as I can tell, this hasn&#x27;t been a big problem in practice. Calls can forge caller ID. Emails can forge the From field. It&#x27;s irritating but isn&#x27;t exactly halting society.I am really not sure why Discord made the change. Wanted to prune inactive users? Wanted to sell Nitro Super Mega Premium to jump ahead in line to get your username of choice? Just bored? reply sgjohnson 14 hours agorootparent> You can still look exactly like someone elseBut you can’t have a username that looks exactly the same anymore.I’m in some 140 discord servers. The amount of spam from people impersonating someone popular has reduced drastically since this change.I wouldn’t call “one click” an investigation. reply forgotpwd16 12 hours agorootparentprevSo the change that mattered is dropping utf-8. Otherwise if was still supported all someone had to do is step 1. reply Groxx 17 hours agorootparentprevYeah, same here. I thought it did a good job of resolving username conflicts without it really being an issue inside active social circles. Everyone can just always use their normal username, and it&#x27;s only a minor friction when sharing your name outside Discord without a link or something. reply brewmarche 19 hours agoparentprevOIDC actually already handles this by requiring the `sub` claim to never be re-assigned and unique: https:&#x2F;&#x2F;openid.net&#x2F;specs&#x2F;openid-connect-core-1_0.html#IDToke...Of course this means that an ID token should not contain an e-mail address under `sub`. reply fauigerzigerk 18 hours agorootparentSo the identity provider could just generate this unchangable ID and let the user link any number of aliases to it, right? reply uxp8u61q 16 hours agorootparentThat is what TFA suggests, yes. reply laserbeam 16 hours agoparentprevMy understanding of the article is:1. Use a guid-like value as your internal identifier. All internal references in your databases to a user should use that.2. Use a second user friendly identifier for the user to login (i.e. Email). Feel free to rebind this if the user needs to change it. Keep a 1-to-1 relationship between the two. reply pc86 16 hours agorootparentThat was my understanding as well but it doesn&#x27;t really address sufficiently what to do in the case of, for example, the user permanently losing access to #2. Sure if I am making the decision to migrate from gmail to some other provider, I can self-coordinate transitioning in your app. But if I lose access and can&#x27;t regain it through my own forgetfulness, or worse I get hacked, the easiest option still seems to be creating a new unique account. reply cpeterso 14 hours agorootparentYou could allow the user to add secondary email addresses or phone numbers to their account for account recovery, but then any of this becomes the weak link for hacking their account. reply mark-r 18 hours agoparentprevI once had my email change because my ISP was bought out. Absolutely nothing I could do about it. The old email was forwarded for 12 months, then cut off completely. reply otteromkram 17 hours agorootparentDid you use that time to update relevant accounts?Not sure what the gripe is here, especially with a one year notification period. reply lapsed_pacifist 15 hours agorootparentThere are accounts which do not allow you to change the email the account is registered to. reply fshr 15 hours agorootparentCan you give a couple examples? I can&#x27;t think of any accounts that don&#x27;t allow an email change. reply darinpantley 6 hours agorootparentI changed my email address in ~200 accounts and only had trouble updating a few: BuiltIn, CPUID, Flickr, Kakao, JCrew, Nord, and Steiger.After I contacted Kakao support, they asked for some documents. Then they called me to verify the details. A day or two later, they updated my online profile with an editable email field, so I was finally able to update my email address.When I contacted JCrew chat support, they performed the update immediately and emailed me a temporary password from noreply@demandware.net.Flickr and Steiger were also happy to help. TBD on the others.So I can&#x27;t think of any accounts that don&#x27;t allow an email change either. But you might have to jump through some hoops. reply saagarjha 17 hours agorootparentprevIt means you lose accounts that use the email address as your identity and don’t let you change it. reply fshr 15 hours agorootparentCan you give a couple examples? I can&#x27;t think of any accounts that don&#x27;t allow an email change. reply saagarjha 12 hours agorootparentSay, for example, systems that directly expose your email address as the identifier that can be used to reach you. \"Send money to saagar@saagarjha.com [using our service]\" kind of sites. reply justsomehnguy 6 hours agorootparentprevNot everything is a phpBB forum.Also:> Select Google Account email. If you can&#x27;t open this setting, it might not be possible to change your email or username.> If your account&#x27;s email address ends in @gmail.com, you usually can&#x27;t change it.https:&#x2F;&#x2F;support.google.com&#x2F;accounts&#x2F;answer&#x2F;19870 reply mark-r 12 hours agorootparentprevI updated all the accounts I could remember, but to this day I don&#x27;t know if there were any I forgot. I figure if I forgot one, it must not have been important.P.S. not a gripe, just adding an anecdata. reply pseudalopex 11 hours agorootparentprev1 of the article&#x27;s points was accounts must be designed to be updated. reply dcow 17 hours agoparentprevPasskeys aren’t device-bound. I think they’ll work just fine.The real problem, though, is that we seem to need digital identity solutions to be perfect as opposed to “good enough”. No solution is perfect and we’ll be stuck on email as long as the enterprise security nuts (who need everything device-bound and vendor attested) and anon-in-the-ether privacy schoolers (who think any stable identifier whatsoever is a heinous crime) are part of the conversation.Imagine if everyone just used mobile drivers licenses issued to whatever self-sovereign wallet the user chooses. Identity issuing, revocation, and recovery is then handled by all the things society has already built to handle meatspace identity. Account recovery involves a trip to your local gov’t office to re-issue your ID credential. Which means you need some chain of trust to your birth certificate. You’re going to treat your mDL credential wallet with a lot more reverence if that’s the recovery flow, so some of these problems solve themselves if we stop using punk short-names everywhere online.Relying parties that need human uniqueness, age, and&#x2F;or nationality guarantees use the mDL verifiable credential. Law probibits relying parties from aggregating and selling&#x2F;transferring information obtained for purposes of authentication from a VC. Ad-tech privacy problem solved.Services that don’t need proof of human uniqueness etc. can just skip the VC part of the equation and use basic passkeys and implement short-name reclamation. reply fantasybroker 16 hours agorootparentPersonally, I like the idea of hardware-tied auth (phone&#x2F;FIDO2 key), that could work for me eventually. However, I am also one of the \"anon-in-the-ether\" people and don&#x27;t want a permanent identity on any public service.A permanent identity comes with many additional mechanics, far beyond just a stable identifier. The biggest one is post karma (including likes): IMO it&#x27;s at the core of almost everything what&#x27;s wrong with the modern web. It introduces vile personal and group incentives and leads to an eventual destruction of any honest conversation. While this mechanic exists on public forums, I won&#x27;t use a permanent identity. reply dcow 23 minutes agorootparentI’d challenge you to live life without a government ID, and without giving your phone number to any “public service”. I’m sure we could be more private than we are, but you aren’t going to get far without your permanent ID. reply pseudalopex 11 hours agorootparentprevStable identifiers were common decades before karma or likes. reply fantasybroker 11 hours agorootparentYup, I used to use them back then. reply pseudalopex 11 hours agorootparentYou said stable identifiers come with karma and likes. But you know they come without also. reply fantasybroker 10 hours agorootparentI mean - yes? I understand what a stable identifier is. Having a unique user ID in a database doesn&#x27;t magically enable likes on all features. But for the purpose of this discussion I limited my opinion to \"public services\", i.e. services with a social aspect, which overwhelmingly have this mechanic (to such an absurd extent that I can \"like\" payments made by people I don&#x27;t know on Venmo). reply pseudalopex 10 hours agorootparent> I understand what a stable identifier is.You do not understand what core is apparently. reply fantasybroker 7 hours agorootparentFrom what I understand \"core\" is a central part of a fruit. replyAnthonyMouse 9 hours agorootparentprev> Imagine if everyone just used mobile drivers licenses issued to whatever self-sovereign wallet the user chooses. Identity issuing, revocation, and recovery is then handled by all the things society has already built to handle meatspace identity.But why do you need to tie it to your driver&#x27;s license? Tie it to whatever kind of account recovery token you like, put that in a safety deposit box at a bank (available for ~$20&#x2F;year), and then you can get access to that with your government ID if all else fails.This requires no new infrastructure to screw up or get broken into and doesn&#x27;t tie your internet activity to your name while still allowing you to use your name to recover your accounts.> Relying parties that need human uniqueness, age, and&#x2F;or nationality guarantees use the mDL verifiable credential.This is variously unnecessary and ineffective.The correct way to do age verification is to ask the client&#x27;s browser if the user is a minor. If the user is a minor, either their parents will have configured the device to answer truthfully or the minor has access to an adult willing to allow it, against which no remote system is secure anyway because the adult has an adult ID.This is the same reason human verification doesn&#x27;t work like that. You still have no idea if you&#x27;re talking to a human, all you know is the device on the other end has somebody&#x27;s ID attached. Individuals who aren&#x27;t supposed to be using AI still have a human ID and criminal organizations not only have their own IDs but also any they can steal.It&#x27;s not clear why proving nationality over the internet is necessary but most of the obvious use cases are dystopian and requiring you to visit a physical office once in your life to prove your nationality to some bureaucracy (after which you can use your account) seems like a minor burden -- and more secure -- than trying to do this.That kind of system is not worth the candle. The tracking risk is large, the benefit is small, there are too many ways to screw it up and it would inevitably be politically compromised and hard to change. reply dcow 48 minutes agorootparentYou can use whatever credential society is willing to trust. Practically that’s a state-issued ID.All I’m arguing is that we should extend the concept of your state ID cryptographically into cyberspace. Amd that it should be used for recovery flows where real identity matters. There’s nothing new here (society already works this way) other than the protocols and specs to agree on document and signature format, which is luckily something we happen to excel at managing.None of your counter suggestions solve any practical problems. A liquor store isn’t going to simply “ask the user” whether they’re of age. They need more formal proof. Users don’t rent safe deposit boxes at banks, and even if they did that would be chained back to your physical ID anyway so your apparent solution isn&#x27;t a real solution.The dystopian worries are hyperbolic and mostly FUD. If a service needs your info and you need the service, you’ll give it to them.Anyway mobile DL is already happening. I’d rather a state that I have at least a modicum of control over be the root of my digital identity than some corporate run email system than can evict me without cause. reply AnthonyMouse 12 minutes agorootparent> You can use whatever credential society is willing to trust. Practically that’s a state-issued ID.Nearly every institution trusts the credentials that it issues. Your employer trusts your ID badge that they issued. Your bank trusts your bank card that they issued. Why does anything else even need to exist?> All I’m arguing is that we should extend the concept of your state ID cryptographically into cyberspace.And then it will be designed poorly but everything will start requiring it because the poor design will allow it to be used as a tracking ID (even if it was claimed not to, because malicious corporations are clever), but once everything is using it the poor design will be difficult to change. See social security numbers (which never should have been public).> A liquor store isn’t going to simply “ask the user” whether they’re of age.A liquor store doesn&#x27;t need to verify identity over the internet because you&#x27;re standing in the liquor store. Unless it&#x27;s an internet liquor store in which case they already have your identity because you&#x27;ve provided them with payment info and a shipping address, and checking ID at the point of sale is useless when it&#x27;s the point of delivery you care about, i.e. you need the delivery driver to check it. Otherwise minors can just buy alcohol with an adult&#x27;s ID unbeknownst to both the seller and the adult, and have it delivered to themselves where nobody checks who receives the package.You can&#x27;t verify age over the internet because you have no way to know if the credentials being used are those of the user or someone else. In person you compare the picture on the ID to their face, or notice that they&#x27;re clearly a child.> Users don’t rent safe deposit boxes at banks, and even if they did that would be chained back to your physical ID anyway so your apparent solution isn&#x27;t a real solution.The bank doesn&#x27;t even know what&#x27;s in the box, and you&#x27;re not required to use a bank if you don&#x27;t want to. You can use any safe place you&#x27;ll still be able to access even if your house burns down etc. A safety deposit box is an example of such a place which is relatively inexpensive. Many people do in fact use them to store important documents -- that&#x27;s one of the main things they&#x27;re for.> The dystopian worries are hyperbolic and mostly FUD. If a service needs your info and you need the service, you’ll give it to them.If you make it easy to demand then services that don&#x27;t need the info will demand it, and then you&#x27;ll give it to them because you need the service. Which is the evil to be prevented, by making it hard to demand, so only services that actually need it will demand it.> Anyway mobile DL is already happening.That which is made can be unmade. Easier if done sooner.> I’d rather a state that I have at least a modicum of control over be the root of my digital identity than some corporate run email system than can evict me without cause.So buy a domain name for $15&#x2F;year to use for your email, which you can point to any third party email service if you don&#x27;t want to host it yourself, and you can point somewhere else if they disappear or become adversarial. Or make it easier for the average person to do this (though it&#x27;s really not that hard). reply pc86 16 hours agorootparentprevI definitely don&#x27;t consider myself a \"privacy schooler\" (whatever that means) but government-issued ID credentials tied to \"self-sovereign wallets\" (whatever that means) sounds like a, pardon my french, absolute fucking nightmare. But agreed with your other point that we&#x27;d probably stop giving ID up as easily if that was the cost of recovery. reply riffraff 15 hours agorootparentYou have government issued id credentials tied to government controlled bank accounts and cards, a wallet per se would not change much. reply xerox13ster 14 hours agorootparentThose are not signifiers for my entire digital life and allowing them to be a single trackable signifier online gives corporations and governments carte blanche to track every movement we make online in perpetuity, forever once that cat is out of the bag.It will be worse than the WEI framework in terms of restricting access to a certain class of people. If you&#x27;re already disadvantaged and don&#x27;t have the ability to provide documents to the DOL proving who you are, how are you supposed to get access to your online identity again to get into a mail account and try to apply for work or housing?Imagine someone steals your real world wallet and gets your online identity credentials, goes posting revenge porn and crypto spam and gets you booted off every platform. You get cancelled and lose your job because your online identity is tied explicitly to meat space and the court of public opinion operates on guilty even after proving innocence.Meanwhile you&#x27;re trying to recover your life--social, physical, and digital--, but can&#x27;t get into any platforms online. None of your accounts work anymore. You can&#x27;t access your backups, or get into your contacts because your device is no longer trusted because it&#x27;s tied to a blocked Microsoft&#x2F;Google&#x2F;Apple account. You can&#x27;t access your house because your IoT security is tied to your online accounts which have been disabled. You can&#x27;t access your physical documents safe. You have to break into your house. You can&#x27;t scan the QR code or NFC to verify identity after providing the alarm code. Police come, and arrest you because you can&#x27;t prove who you are. You&#x27;re crazed about your situation, babbling because of the insanity of it all and look like someone trying to steal a nice homeowner&#x27;s documents.I realize that&#x27;s a pretty extreme Black Mirror level example a bit like Nosedive, but it&#x27;s in the realm of possibility if we go down that route knowing that corporations are already trying to do device attestations. Maybe you&#x27;d have the prescience to have a physical security layer 0 for your IoT security, but many products people purchase won&#x27;t because having to carry a key defeats the purpose of having the tech solution.The scary enough reality is that if there is a single government provided signifier for an individual online, we will inevitably see sweeping tracking and censorship. They do as much as they can possibly do now. Why on Earth would anyone ever think they wouldn&#x27;t do more?No, thanks. reply dcow 33 minutes agorootparentEntirely unfounded FUD!Wallets wouldn’t just let some random thief access all your credentials. They have safeguards like biometric TEE unlock. If you’re being targeted by someone who can get past that, then they could do equal damage with your physical drivers license. Nobody is going to drive by swipe your phone, bypass biometrics, and access your wallet just to post revenge porn. Give me a break!The way you fight companies trying to do device attestation&#x2F;profiling is to provide a system that meets the current needs but controls structurally the philosophy around what you’re identifying (user, not device). And you legally limit behavior, not technically. I am sick of losing every nice thing we had because some privacy wanker says “oh that’s a persistent identifier better neuter it”. I want well regulated identifiers that I control judicially and around which there is a clear legal framework preventing abuse. I don’t want a world where I can’t manage my kid’s phone on my home network because some tin foil hat at Apple decided to change the device’s mac address every day “for privacy”. reply CatWChainsaw 12 hours agorootparentprevYou call that an extreme example, but I say \"watch that happen by 2030 at the rate we&#x27;re going\". It&#x27;s rather frightening how so many people on Hacker News either are completely unaware that this is a thing that any party of power wants to be able to do, or think that society wouldn&#x27;t let it happen, even as parties in power remove all means of leverage against them. reply mooreds 16 hours agorootparentprev> Passkeys aren’t device-bound. I think they’ll work just fine.Depends on the implementation. Some passkeys are device bound. The free ones, typically. Unless you trust Apple and Google to preserve and protect private keys. reply jml7c5 14 hours agorootparentprev>Passkeys aren’t device-bound. I think they’ll work just fine.Is there anywhere to follow progress on this? I don&#x27;t think anyone actually implements import&#x2F;export of passkeys yet. reply rdl 14 hours agorootparentDon&#x27;t they work pretty portably within Apple iCloud keychain or Google equivalent (but only one)? I think some of the legacy password managers are supporting this as well, although my preferred self-hosted vaultwarden option doesn&#x27;t yet (bitwarden has support, but I don&#x27;t think it is in the self hosted version yet, let alone in vaultwarden)There are ways to set passkeys as non-exportable from device I think but that is not the default. reply pseudalopex 11 hours agorootparentBound to devices chosen by Apple or Google is device bound.What does legacy mean to you? The usual meaning of outdated is inapplicable in this case. reply dcow 30 minutes agorootparentNo it’s not. At least that’s not what anybody in the security community means when they say device bound. Device bound implies the key is cryptographically tied to a piece of unique physical hardware. reply CatWChainsaw 12 hours agorootparentprevSorry, how are passkeys not device bound? Every single article I read explaining why they&#x27;re \"better than passwords\" touts precisely this as a strength - your authentication takes place on device, so there&#x27;s no server of passwords for anyone to hack. If you lose the devices your passkeys are paired to, you&#x27;ve locked yourself out of that account for good. reply renonce 16 hours agoparentprevWhat about domain names? Emails are bound to a domain name by definition, and indeed domain name IS designed as a good identity and can be used to host an email. See how domain names are owned by large corporations and trusted since day one and never lost, as long as you keep it carefully. It requires WHOIS information as the authoritative information of domain owner. An account based on username and password is what you usually need to access it, but if you are serious you can always choose a domain registar that is serious about keeping your domain name. It’s not free but it costs nuts compared to the cost of losing it, and let’s not forget that there is a cost behind hosting emails even if it’s given for free. reply talldatethrow 16 hours agorootparentI set up a Twitter account long ago with an email from a domain I used. I let the domain expire later, and now can&#x27;t do the password reset because it&#x27;s pointing to a domain that I don&#x27;t own, and can&#x27;t buy anymore. Basically lost forever. reply earthling8118 15 hours agorootparentThis happened to my primary Google account once and only by sheer luck was the domain available and I was able to recover the account.I&#x27;m the only one to blame because I changed my password at 2 in the morning when notified that someone in another country used it. I didn&#x27;t put my info into a password manager or remember it because I wasn&#x27;t truly awake at the time.When recovering I had to go through a bunch of steps and did great up until the email one. Well, Google was firm about wanting access to that email. Odd, considering the domain wasn&#x27;t registered at the time of requesting a password reset and so I would consider it a security violation to accept that.Needless to say, I am much more stringent on making sure this stuff is set up correctly now. reply renonce 16 hours agorootparentprevYou can buy a domain 10 years at a time. If you forgot to renew it one year ahead, the domain probably wasn’t important enough for you. reply talldatethrow 16 hours agorootparentCorrect it was not. I let it expire. I forgot the Twitter account was registered with it. Later when I needed to relogin and couldn&#x27;t remember the password, I couldn&#x27;t do a password reset. reply prepend 19 hours agoparentprevI think email plus a robust protocol for resolving changes works as good as can get.For important stuff like banks and pensions they also have phone and physical address, so there’s a way to reconcile things like email changes, as rare as they are. reply toomuchtodo 18 hours agorootparentUS example. Financial services orgs have your social security number. Perhaps they should be able to forward a message to the US gov to forward it to the citizen stakeholder through a government messaging delivery platform. This ensures continuity of communication but does not allow orgs to lookup emails with loose data governance (and all that leads to).Login.gov is very good from a federal gov idp perspective, and I’m hoping it slowly develops into supporting a national ID and ubiquitous identity proofing to squash identity fraud but also streamline gov digital service delivery. reply _kb 18 hours agorootparentThis is very similar to what&#x27;s happening with the Australian Digital ID Bill [0].[0]: https:&#x2F;&#x2F;www.digitalidentity.gov.au&#x2F;digital-id-bill reply Rapzid 9 hours agoparentprevPretty much everything is moving to UUID of sorts including Google auth.It&#x27;s tricky because you often need to let people reference username&#x2F;emails for mentions and etc, so you just have to index all of em and translate to UUIDs for references behind the scenes.It gets extra tricky with APIs. Consider AirPlane.dev which let&#x27;s you specificy approvers via email. Now a user changes their name and their email. Well, that \"IaC\" suddenly references an invalid email or worse a different user because jane.doe joined after jane.doe-brown got their new email. reply ianburrell 17 hours agoparentprevAlso, it is good to keep concepts of account ID, public username, and login username separate. By using random account ID, can change the other values. Most accounts want email but don’t have to make it user name. Or people have multiple accounts and makes sense to have email reused.For login, it can help to have multiple methods. Then people can change from OIDC to password, or between providers. reply kentbull 15 hours agoparentprevHave you heard of key event receipt infrastructure (KERI)?It solves the identity problem with decentralized identifiers though the secret sauce is the fractionally weighted multisig for enabling multi-device signing and account recovery with key rotation.See the specification for more details: https:&#x2F;&#x2F;www.ietf.org&#x2F;id&#x2F;draft-ssmith-keri-00.htmlOr the whitepaper: https:&#x2F;&#x2F;github.com&#x2F;SmithSamuelM&#x2F;Papers&#x2F;blob&#x2F;master&#x2F;whitepape... reply pests 10 hours agoparentprevSpotify let&#x27;s you add separate login methods. I have my email+pw set up as well as login with Apple and login with Facebook. They all log into the same account and all have the same permissions once logged in.I think it&#x27;s a good solution. reply BazookaMusic 17 hours agoparentprevLong lasting usernames across websites is the worst for privacy though, unless the username is not public. In general, it&#x27;s best if the unique identifier is only known to the user.Example: https:&#x2F;&#x2F;instantusername.comI&#x27;ve seen quite personal details being leaked because sometimes even smart people don&#x27;t realise how easy it is to cross-reference given a unique username. reply flir 17 hours agoparentprevMultiple identities at the same time, too.This is why I think email addresses are \"good enough\" - you can always spin up a new one for each identity you want to inhabit. reply dcow 17 hours agorootparentSome services explicitly want to disallow this so it’s actually an attack vector in that light. reply zirgs 17 hours agoparentprev> People dislike usernames, they want to be able to choose non-unique ones rather than end up with user53267 or something inane.Google doesn&#x27;t reuse usernames so if they are still around - in a few decades pretty much all unique usernames will belong to dead people. reply j45 19 hours agoparentprevIt’s useful to have your own domain with your own email so it stays with you as long as you like, beyond work emails.Being a tolerated guest who pays little to none in someone’s servers is another issue.Most large email providers are more like digital identity providers, and being a citizen of one of these big digital countries is neither democratic or setup for your long term preferences. reply dotancohen 19 hours agorootparentnext [–]> It’s useful to have your own domain with your own emailUntil you&#x27;ve forgotten to renew, or were to sick too renew, or the domain is hijacked. I&#x27;ve had my domain for over twenty years, and I&#x27;ve come way too close to losing it at least twice. reply input_sh 18 hours agorootparentOr you decide to change the domain, but are too lazy to change the email address... everywhere, so you end up paying for 2-3 domains instead of one just for the email redirects to work. reply dotancohen 18 hours agorootparentOr you have financial troubles and no credit card just as you need to renew...A more permanent way to buy - not rent - domain names would solve many of these issues. And changing ownership of domains should be just as difficult as changing ownership of real estate, the only people benefiting from the current ease of changing domain ownership are speculators. reply j45 18 hours agorootparentThere is at least one domain tld selling “lifetime” domains.I’m not sure why icann wouldn’t let anyone pay for 10-50 years for a domain. reply anonuser123456 17 hours agorootparentprevMost registrars let you prepay. I prepay mine by 5 years in advance, and have a reminder to refresh it yearly. It also is setup to auto-renew 3 months before expiration so if the charge fails… you have 3 months to fix it. reply dotancohen 15 hours agorootparentAnd I just finished almost three months of emergency military service and my credit card was just cancelled before due to my own mistake. Had I been two weeks into that three month window, I would have missed the renew date. reply mixmastamyk 14 hours agorootparentEven after the reg term expires there’s typically a grace period before the domain is unrecoverably recycled. Find a registrar with good policy, though some things are regulated by internet orgs. reply dotancohen 14 hours agorootparentYes, thanks. reply EVa5I7bHFq9mnYK 16 hours agorootparentprevI had my own domain for many years until the email provider of the admin email for the domain (openmailbox.org) decided to shut down. Bye bye my domain. reply solotronics 18 hours agorootparentprevIs there a way to actually \"own\" your domain instead of paying registration fees every year? reply lolinder 18 hours agorootparentYou can pay ahead up to 10 years, which helps. Get in the habit of adding time every year, but you can miss up to 9 in a row before there are any problems. reply _rm 18 hours agorootparentThis, if you can&#x27;t keep a domain registration current despite the 10 year max length, domains aren&#x27;t for you. reply saagarjha 17 hours agorootparentPeople who have been incarcerated? reply xoa 16 hours agorootparentKind of an odd example in this context? Someone who is incarcerated for that length of time is liable to lose any property they fully own anyway given the kinds of monetary damages that tend to go with it. And physical property isn&#x27;t trivial to have kept up for a decade away from it in jail either is it? The only way that happens is if they have it legally isolated from them and someone else who can&#x2F;will be a caretaker, and in that case said person could easily renew a domain as well. If anything domains would relatively speaking seem pretty easy there, no matter what jurisdiction you&#x27;re under there are domains to be had that are under a different hostile jurisdiction, there are registrars that will accept cryptocurrency payments, and costs are relatively very low. Auto-renew from a private bitcoin wallet for $10-20&#x2F;yr on the face of it looks more sustainable and feasible to have work and survive court judgements following a serious felony. And during trial there is time to prep.Nothing is perfect and the domain situation is really far from perfect, and it doesn&#x27;t hurt to consider edge cases. But the Venn diagram intersection there of someone who cares enough to have custom domain that is critical, commits a serious felony, considers having the domain after release a key priority, isn&#x27;t legally barred from it, and doesn&#x27;t or can&#x27;t take any steps towards it, seems kinda small. In that case maybe indeed \"domains aren&#x27;t for you\" but that doesn&#x27;t really take away from its use to the rest of us. reply saagarjha 12 hours agorootparentI think you would get a confluence of many of these when looking at computer crimes, because often a condition of the sentence is being unable to use a computer (and, to some extent, a lot of people who spend time on computers lack physical people in their life who can step in to help out in situations like these…) reply jbverschoor 17 hours agorootparentprevTell that all SSL cert holders lol reply j45 18 hours agorootparentprevI think there’s a few rods that are free to cheap. .cx comes to mind for some reason reply j45 18 hours agorootparentprevBeing able to move your domain between any email hosting provider remains valuable.A domain that important is worth putting multiple recurring yearly calendar reminders up.An email serving your identity is probably worth a bit more investing in.It’s possible to leave a credit card on file to auto renew, renew for maximum years at a time, And lock down the domain enough to prevent hijacking. reply rixthefox 18 hours agorootparentprevCame to the same conclusion myself.The only “safe” email host is the one you run yourself or pay for with actual dollars, not data.The hard part is taking your second paragraph to action. Most people are not ready for that conversation because the major freemail providers have been in service for such a long time that most people really can’t grasp the concept that email is something you have to pay for.I really blame a lot of that on Google from the very beginning. Gmail, and essentially all free mail providers, are what they are today because of the precedent Google set and the only way companies were going to be able to compete with that was to also make their email services free. reply fauigerzigerk 18 hours agorootparentprevIt would be so much easier for normal people if all service providers allowed you to add multiple email adresses or other aliases to the account.You can easily lose access any particular email address, even if it&#x27;s on your own domain. Losing access to all your email addresses and phone numbers at the same time is far less likely. reply cassianoleal 17 hours agorootparent> You can easily lose access any particular email address, even if it&#x27;s on your own domain.In which scenario would this happen, except for loss of ownership of the domain itself? reply fauigerzigerk 17 hours agorootparentI&#x27;m not aware of any other scenario, but losing a domain is easier than you may think.For instance, my main email address is on a domain that is now owned by my company (it wasn&#x27;t originally). If I ever sell the company, I lose access to the domain as well. My wife&#x27;s email address is on that domain too. reply cassianoleal 15 hours agorootparentRight. Yeah I know it&#x27;s not hard to lose posession of a domain. I probably misinterpreted your comment when I thought you meant losing a specific email address but not the domain as a whole.> my main email address is on a domain that is now owned by my companyI never mix work and private. My email addresses on my company&#x27;s domain are for work-related things only. If I ever sell or close the company, it doesn&#x27;t matter. I own a separate domain for personal email. reply fauigerzigerk 12 hours agorootparent>I never mix work and private.That&#x27;s definitely a good idea. My company sort of emerged from a personal activity and things got mixed up. If I could do it again I would handle it differently, but you know, life&#x27;s twists and turns... :-) replynewsclues 18 hours agoparentprev\"There is no good identity.\"Government has failed to adapt with modern times and technology and has failed to provide modern and secure identification and authentication services for citizens.I log in with my bank credentials to access my government tax account, talk about a total failure to do your job from the people still using SIN as an important piece of identity for some of the most important aspects of life.This is a solvable problem. Governments can adapt and use modern technology to provide identity and authentication services, but they do not.In my opinion this is a failure to be responsible for core government services, and I can only speculate why. reply pzmarzly 18 hours agorootparent> Governments can adapt and use modern technology to provide identity and authentication services, but they do not.At least in many EU countries, they are adapting. I&#x27;m a fairly happy user of Irish myGovID (OIDC) and ROS (X509 \"sign this message with your private key\" challenge), and Polish Profil Zaufany (I think OIDC or CAS?).The issues I see are:- Each country has its own system, some documented, some not so much, some use OIDC, some SAML, some something more obscure.- As an individual who moved countries, you end up with multiple accounts.- As a developer you cannot easily register your own OIDC app. Send an email to some ministry and hope for the best. If you aren&#x27;t part of government yourself, you may be out of luck. reply NooneAtAll3 18 hours agorootparentprevthe last thing I&#x27;d want would be connecting every online personality to my real identity reply newsclues 16 hours agorootparentYour aliases don’t need to be publicly disclosed.But the reality is that virtually every online identity is tied to a real identity, just with layers of obscurity added.This hacker news was created with anonymous accounts but it still links back to my real one. reply NikkiA 12 hours agorootparentEvery day I get dozens of &#x27;your account was hacked and we used your webcam to record videos of you jacking off&#x27; bitcoin &#x27;blackmail&#x27; spam to various accounts that have been either hack-harvested or the company went bust and figured selling all their user data was a decent golden parachute.Just imagine if those accounts all had had direct and firm ties to RL identity stored with them. reply FireBeyond 14 hours agorootparentprevThey don&#x27;t, but they will be. If for no other reason than the concrete knowledge that somehow your alias on a site is verifiably connectable to person X will lead to a raft of abuses of the legal system to silence or harass people saying things someone doesn&#x27;t like. And average sites aren&#x27;t going to have the resources or will to fight this. reply azlev 18 hours agorootparentprevIt would be or a chaos or very limiting expect that companies would interact with a lot of govs to get authentication. It&#x27;s way cheaper leave as is. reply LocalH 18 hours agorootparent\"Cheap\" should never be the driving factor for these things. reply jethro_tell 18 hours agorootparentThat is, in fact, the purpose of government, to do the things necessary for a functioning society for which a profit motive does not exist. reply azlev 18 hours agorootparentMaybe I&#x27;m mistaken but China&#x27;s super apps solve this problem with profit motivation. reply gsich 18 hours agorootparentprevIt&#x27;s not a problem though. reply Levitz 16 hours agorootparentIt&#x27;s definitely a problem. Identity theft is way more common in the US than in Europe.If it wasn&#x27;t a problem the SSN system wouldn&#x27;t be used in unintended ways. reply goodpoint 18 hours agorootparentprevIdentity is not centralized and therefore identification and accounting cannot be centralized. reply api 19 hours agoparentprevI usually dislike the idea of inviting government into this space, but if there&#x27;s anything that governments have traditionally done decently well and should do (it&#x27;s usually within their mission statement) its identity. Passports are really the primary identity layer of Earth.Ideally in a perfect world we&#x27;d have governments run OIDC systems similar to the US login.gov and these would delegate from an international master OIDC system at the UN. Everyone would have their citizenship passport ID and their UN ID, and the latter could serve as a \"break glass\" master key to support immigration and also limit the ability of countries to \"digital death penalty\" people.I can think of some dystopian outcomes here, but IMHO they are not worse than the dystopian outcomes that come from corporate monopolist control of digital identity. At least in democracies one has some nominal influence over one&#x27;s government and the latter is bound by the rule of law, and if you don&#x27;t live in a democracy you can (or should be able to) leave.You&#x27;re right that identity is hard, and I think most of why it&#x27;s hard is human rather than technical. One could create a decentralized identity layer from a block chain fairly easily but people would lose their keys etc. reply vlovich123 19 hours agorootparentI don’t know. I wouldn’t say that governments do identity better than anyone else and adding more dependency on it just increases the value of the government ID making it an even more lucrative target to steal&#x2F;forge.Fake ids are a thing and the quality depends on how much you spend. Governments also have reasons to lie about identity themselves (think spies).A true identity solution means being able to cross reference your identity across multiple entities (federal government, state and municipal, employers you’ve worked for, businesses you’ve interacted with, etc etc). reply fragmede 18 hours agorootparentI&#x27;m sure there&#x27;s a criminal underworld where you can just buy a passport to get you into a country, but for the average person, passports are really hard to fake, which makes them good enough as identity for logging into your average dog photo sharing site. reply maxcoder4 18 hours agorootparentWhy would I want to log in to a dog sharing website using my real identity? Maybe I&#x27;m an outlier but I really value the pseudonymity possible thanks to the internet. I can share as much or as little about me as I want, and the dumb things I probably wrote when I was 14 are not immortalized with my name next to them. reply Cyberdog 13 hours agorootparentA dog what web site? reply CatWChainsaw 12 hours agorootparentThey said dogs, not cyberdogs, so you might be safe? replyTimedToasts 18 hours agorootparentprevIt&#x27;ll be a cold day in Cupertino before I accept a UN ID.I&#x27;ll reconsider once the corporate death counts begin to match the governmental ones but until then I&#x27;ll take my chances. reply gossamer 17 hours agorootparentThe temperature in Cupertino is currently 50F. That probably doesn&#x27;t meet your definition of cold though. reply saagarjha 17 hours agorootparentA bit chilly. reply bigstrat2003 16 hours agorootparentprevThat is unironically shorts weather. reply tim333 18 hours agorootparentprev60% of Americans approx don&#x27;t have passports. In India about 93%. It&#x27;s not an ideal solution to logging in to your website.Also in the UK various government types have tried to bring in national ID and the people rebel. People eh? reply klabb3 17 hours agorootparentprev> Everyone would have their citizenship passport ID and their UN ID, and the latter could serve as a \"break glass\" master key to support immigrationImmigration doesn’t work that way. You don’t lose or transition from one identity federation to another. You maintain both, typically for the rest of your life.My personal wishlist is that decision makers and designers of identity systems must include people with real world experience of multiple nationalities, tax residencies, migration and so on.Currently, these systems are already built on false premises and immigrants suffer a lot – not only because of malice but to a large extent because the bureaucrats didn’t think like security-minded engineers. The edge cases are extremely important when it comes to identity, because identity is required for a lot of basic needs. As the world is become more globalized, these issues are a lot more prevalent.> and I think most of why it&#x27;s hard is human rather than technicalYes but I don’t see why that’s so surprising. It’s the identity of humans that’s the problem.FWIW I think email is fantastic as identity, compared to the abysmal state of the alternatives. It doesn’t change when you cross a border like phone numbers. It’s not perfect when it comes to self-sovereignty and account recovery. reply logifail 19 hours agorootparentprev> Passports are really the primary identity layer of Earth.Umm, I&#x27;m not they&#x27;re not that great as a primary identity either.One edge case is that you can have have more than one valid passport for the one nationality. Another is of course that one can have more than one nationality. reply api 19 hours agorootparentI have a Google, Apple, Microsoft, and GitHub OIDC account among others. That&#x27;s a feature not a bug. reply paganel 17 hours agorootparentprevAlso, passports become useless once your government actively turns against you, see the current fate of adult Ukrainian men still residing in Ukraine.The same thing risks happening with this government-approved online identity, I mean, how will the EU bureaucrats “handle” people like me that are openly against the West’s take in Ukraine? Will we get our accounts banned from posting pro-Russia content online? reply logifail 17 hours agorootparent> The same thing risks happening with this government-approved online identity [..]Don&#x27;t forget the Covid QR codes. reply t_mann 18 hours agorootparentprevWe&#x27;ll know soon, something pretty close to what you&#x27;re asking for is about to be rolled out in the EU: https:&#x2F;&#x2F;commission.europa.eu&#x2F;strategy-and-policy&#x2F;priorities-... reply paganel 17 hours agorootparentFortunately that will be another nail in the coffin for the EU, at this point they’re just throwing the proverbial excrements at the wall and seeing what will stick. That’s how you get parties like the AfD at more than 30% of the vote (the NSDAP themselves were just a little over 30% too in early 1933). reply sureglymop 19 hours agorootparentprevI think it&#x27;s not a good idea. What if, due to unforeseen circumstances, you become homeless and passport-less? reply buro9 19 hours agorootparentprevGovernments do a bad job here, not a good one.They restrict people to a single immutable identity, that may not conform to other governments, that may not accommodate different languages and character sets, that are not flexible of gender, that do not reflect relationship types that aren&#x27;t typically monogamous... the list goes on.They offer a poor base implementation that is only sufficient due to the legal identity seldom actually being needed online. Which is a good thing, because identity theft would be so much worse if that was everywhere.In the UK we don&#x27;t have as fixed an idea of an identity as people think, Cherie Blair is also Cherie Booth Q.C. , Elton John is also Reginald Dwight, and for both people, both identities are real identities and sufficient to get bank accounts in the name of, it&#x27;s only when it comes to a tax record and passport that you are reduced to a single identifier, but who is to say that the name on that is the preferred name of a person?My bank account, bank card, accounts on most of my things, do not match my passport and birth certificate. reply bluefirebrand 15 hours agorootparent> They restrict people to a single immutable identityThis is a good thing because this is what people have, one single immutable identity.What you&#x27;ve identified are problems with assumptions about the relationship between various things and identity.A key one is the relationship between name and identity. Two people can have the same name but aren&#x27;t the same person. A person ca change their name but they haven&#x27;t become a new person.I can do anything I want to myself honestly. I can color my hair, have a sex change, lose weight, gain weight, have height augmentation surgery, get piercings grow a beard, change my name, whatever, but I&#x27;m still the same person. That is immutable. I still have the same mother and father, the same birthdate, lived in the same places, attended the same schools, studied the same things, etc.My identity as far as the government is concerned is still tied to a single number. Yeah, they make it hard to change a lot of the other stuff related to that number (for good reason), but ultimately they are pretty good at knowing who I am based on that number. reply api 19 hours agorootparentprevThe alternative is corporate monopolist control, which is what is developing right now with OIDC where Google can \"digital death penalty\" you and lock you out of your life. The biggest realistic risk is this happening \"accidentally\" because some stupid bot at Google or Microsoft decides you are violating ToS, and since these companies have basically zero tech support it takes you weeks to unlock your account if at all.I don&#x27;t actually like governments owning the identity layer, but then again I am of the \"necessary evil\" school of thought regarding most of what governments do. It&#x27;s marginally better than having corporate monopolies own it.I&#x27;m a giant fan of decentralized identity, but there are two insanely hard problems with it:(1) People lose keys, forget passwords, or get them stolen. They also lose or break security key hardware. I&#x27;ve heard stories of people paying people to actually excavate dumps to find lost Bitcoin keys for example.(2) There&#x27;s a chicken or egg problem of getting sites to support any decentralized login scheme without a monopolist like Google pushing it, and the latter have no reason to do so since they want to monopolize the identity layer.Problem (1) is solvable to some extent by having companies that escrow keys for you. Escrowing with them would not be mandatory but they&#x27;d offer a \"break glass\" service for people who are willing to trade a bit of (potential) privacy for it. A good escrowing service would have a terms of service that forbids misuse of your identity.Problem (2) is probably harder. Big tech will actively refuse to support any system that doesn&#x27;t hand them a monopoly or at least an oligopoly.BTW this is one area where cryptocurrency could have found an actual bona fide beneficial use case! But it&#x27;s not as profitable as running scams and casinos so nobody did it. reply swells34 18 hours agorootparentThis whole discussion seems to revolve around \"Who will be the central authority, the permanent and unwaivering center of trust?\". Really a good time to bring blockchain into the discussion, as this is actually something it&#x27;s kind of designed to handle. reply buro9 17 hours agorootparentThat&#x27;s a tool, it still leaves open the question \"Who will be the authority to determine what changes to allow to the blockchain?\", the problem was not solved by introducing blockchain, you just changed the question slightly whilst adding a dependency that wasn&#x27;t required originally. reply api 16 hours agorootparentMoreover proof of work can&#x27;t work for non-currency use cases because it requires an incentive structure for the game theoretics to work. replySolvency 19 hours agoparentprev> they want to be able to choose non-unique ones rather than end up with user53267 or something inane.Disagreed. I&#x27;m 39. I&#x27;ve known hundreds of people (HS, college, etc) and many close friends who willingly made email accounts like \"brijacks85\" (their birth year) or \"sammichelson212\" even when their actual names were still fully available on yahoo&#x2F;gmail&#x2F;hotmail, etc. I used to regularly create email accounts for these people using just their names and then ask \"why didn&#x27;t you just check your own name first?\" and they&#x27;d usually just shrug with total indifference and never use the account I made for them. reply buro9 19 hours agorootparentOh yes, some large number of people are incredibly habitual.But some also large number of people are not. reply ForkMeOnTinder 18 hours agorootparentprevI&#x27;d never use an account someone else made for me either. Who knows if after you created it, you added some recovery questions or a recovery email or saved the login cookie or who knows what else? I&#x27;ll stick with my fresh account made on my own PC through my own connection, thanks. reply mixmastamyk 14 hours agorootparentprevWhen you get the simplest variation of a name on a popular site you’ll receive mail from all the mistaken folks who weren’t careful enough.Similarly, I get a small fraction of the mail of a texas lawyer, because her email address string is a super set of mine, and some percentage of her clients don’t bother or notice the need to add the extra suffix. reply emilfihlman 19 hours agoparentprevThis is just not true.There absolutely is a good identity, and it&#x27;s one provided by countries. reply ben_w 19 hours agorootparentThe UK has no formal universal government provided ID system. The UK does allocate National Insurance numbers, but those are specifically not to be used as ID in part because they don&#x27;t have a face associated with them. Driving licenses exist but are optional, and need regular replacement e.g. when moving address. Names are something you can change on a whim for a bet. Passports have to be updated if your appearance changes significantly, and in any case you don&#x27;t keep the same one if you change nationality. reply lolinder 18 hours agorootparentAnd you can basically say all of the same things about the US. reply buro9 19 hours agorootparentprevmaybe you are in Scandinavia, but many countries do not even have a centralised register of births, deaths, marriages... and so they do not have a centralised and canonical record of identity of all people in the country. reply dfox 18 hours agorootparentFor example Czech republic has central register of all residents. The register is intentionally designed so that it does not provide any kind of identifier that is both long-term stable and globally meaningful (there is a number called RČ broadly equivalent to NIN in other countries, but it is not supposed to be used since 2010).Conceptually there is ZIFO (Basic ID of natural person), that should be globally unique, but this is known only to a subcomponent of the central registry that is run by different govarnment entity than rest of the system. At same time the design of that subcomponent contains provisions for allocating new ID, mainly for handling the cases when that ID was allocated wrong (both multiple IDs for one person and multiple persons sharing same ID), so even that is not necessarily stable ID.Users of that data refer to persons using AIFO, which is specific and meaningful only for particular database (called AIS) and if different databases need to identify particular person as having the same identity they have to call the central translation subcomponent (the API surfaces are designed such that the translation the calling system will not get the result of the translation, which is only sent to the destination system). Even that the AIFOs are meaningless, they are required to be not disclosed to anybody. Alternative IDs that can be used are broadly serial numbers of government issued identitty&#x2F;travel documents, but these are necessarily both revocable and have limited time validity (the aforementioned technically deprecated RČ is essentially a special case of this).I believe that this design comes from some pan-EU initiative related to GDPR. For example according to Wikipedia Austria uses broadly similar, but less fine-grained system.This has interesting issue with regard to things like eIDAS as there is no sane ID that could be included in the qualified personal certificate. One Czech QCA (PostSignum) does not include any kind of personal ID in its personal certificates, second one (I.CA) can optionally include the serial number of identity document that was used for identification. Apparently you can get Swedish qualified certificate that includes Czech RČ in its CN form Zealid. You are supposed to register your certificate into the resident registry yourself, which creates the link between the certificate and your identity. The slight issue with that is that there is no sane way how a third party outside the architecture of these registers can validate that link (you can send them a signed PDF with your data from the resident register, which is apparently what you are supposed to do, but \"sane\" would look different). reply wlonkly 9 hours agorootparentprevSometimes, the country you are in changes even though you are in the same geographical location. I&#x27;ve had my email address for over a decade longer than the current countries of Serbia, Montenegro, Kosovo and South Sudan have existed. reply BrandoElFollito 19 hours agorootparentprevHow do you imagine using that? Having an API for each country and each returning different data? With a 10% adoption?If my country did not like your country I will not be able to connect to your stuff?A never ending must of problems ahead. reply maxcoder4 18 hours agorootparentDoesn&#x27;t it kind of already exists for passports? I can travel with my passport to (basically) any country on earth, so there must be some international support for this. reply BrandoElFollito 14 hours agorootparentThe format of the data on a passport is unified (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Machine-readable_passport) but the problem is to make sure that the one who presents this string of data is actually the one it belongs to. reply zirgs 12 hours agorootparentprevGood for you, but, for example, Israeli passports are not recognised by many countries as legitimate. reply superjan 19 hours agorootparentprevWell the argument not that it it readily available, but that most countries do have an administration of their inhabitants. reply ForkMeOnTinder 18 hours agorootparentSo instead of \"There is a good identity\", GP should have said \"There are about 200 good identities around the world, but if your country happens to not have a single unified ID system you&#x27;re out of luck\" reply BrandoElFollito 14 hours agorootparentThere are 200 good identities in the world with an authentication system closed outside the official entities.I have never seen any endpoint for the general population that can be used to authenticate a citizen reply tecleandor 18 hours agorootparentprevWell, that if your contry keeps a lifelong unique ID for your person. And that&#x27;s only till you migrate, I guess. reply deruta 18 hours agorootparentprevThen, however else you&#x27;d want it to be, states do come and go too, regions change hands... reply azlev 18 hours agorootparentprevHere in Brazil I have at least 4 different IDs :-). reply kijin 19 hours agorootparentprevPeople get new citizenships. They often lose their old citizenships, too, often deliberately for tax purposes. reply zarzavat 17 hours agorootparentThe most common reason people lose their citizenship is that they take on a second citizenship and the first country doesn&#x27;t allow dual citizenship. This means that citizenship limbo exists where you have legally lost your citizenship but can still benefit from it until the first country finds out. Some countries forbid dual citizenship but don&#x27;t enforce it so people end up in this limbo for the rest of their lives.There&#x27;s also a large number of stateless people who don&#x27;t have citizenship at all.There&#x27;s also people who are not technically stateless, but cannot return to their country or obtain any identity documents from it. reply esafak 19 hours agoparentprev> People get married, people get divorced, people transition, people move culture and choose new names... names change, and so do email addresses.Exactly the analogy I had in mind. email primary keys are \"serial monogamy\". Or if you want a mathematical analogy, piecewise constant :) reply Mordisquitos 19 hours agorootparentSpeak for yourself, I&#x27;m an email polygamist! reply MasterYoda900 19 hours agoparentprevWhat if every newborn received a chip implant under the skin (cryptographically unbreakable, unauthorized removal punishable by law), linked to a central government database with the chip’s unique identifier and a profile of the newborn’s DNA signature? reply Joker_vD 19 hours agorootparentThere are reasons why SSN or its equivalents are unpopular as web identities. Can you enumerate those reasons? reply tuwtuwtuwtuw 19 hours agorootparentWhere I live the SSN-equivalent is extremely popular as web identity.Can you explain why it shouldn&#x27;t be? reply lcnPylGDnU4H9OF 19 hours agorootparentNot sure about where you’re from but US institutions often treat the SSN as a secret which should allow account access. Which it’s not by design but nobody wanted to make a better system; it’s why banks have claimed that they are not defrauded by people and instead the fraudsters “steal identities”.To answer the question more directly, it’s treating the identifier like a password that is problematic. reply maxcoder4 17 hours agorootparentOf course this is extremely stupid (basically using the \"username\" as a secret, something everyone in IT and itsec knows not to do). If we started using SSN-equivalent identifiers as online ID problem would solve itself because by then they&#x27;re not a secret anymore.In my country my national ID numbers are nowhere near as problematic as SSN on the US (from what I understand). reply tuwtuwtuwtuw 15 hours agorootparentprevOkay, but that is then about authorization and not about \"identity\". I agree that treating it as a secret is a bad idea.The person said SSN and equivalents, so I guess it depends on what is meant by equivalents.Where I live my personal number is used as identity, but to actually prove I am the owner of it another mechanism is used (private keys embedded in certs). The personal number is very public info by design and can&#x27;t be used as a secret. reply lcnPylGDnU4H9OF 13 hours agorootparentIf my web identity was my personal government tax identification number, I would be worried that one could use that to fraudulently and successfully claim to be me with a fair number of institutions because the authentication mechanism is lacking efficacy. reply tuwtuwtuwtuw 12 hours agorootparentYes, I understand, but you mix up identity with proof of identity.My non-web identity is my name. But me saying that my name is John Doe is not a proof that this is my name. In the same way, me saying that I have identification number 12345678 isn&#x27;t proof that I actually have that number.As I wrote, I have a government issued identification number. This number can be looked up by any citizen in the country since it&#x27;s public info. You can even look it up online - it&#x27;s not secret.But someone knowing the number doesn&#x27;t mean they can prove it is their number, because proof of identity is not in the number itself - for that we use public&#x2F;private keys and other secure mechanisms.I understand that this is not how it works in the US because some organization treats the SSN as secret. But that&#x27;s not an issue with government issued identity number as a form of identity, it&#x27;s an issue with the US system. Other countries does not have the same issue, since they didn&#x27;t mix up identity with proof of identity. reply Joker_vD 12 hours agorootparentMany people don&#x27;t want their online identities be easily connected to their offline identity (or even with each other). Hell, you yourself didn&#x27;t register on HN with your G.I. ID&#x27;s serial number for some reason, did you? reply tuwtuwtuwtuw 12 hours agorootparentSure, but this specific part of the discussion was about it being a bad choice because \"it’s treating the identifier like a password that is problematic\", which I objected to.I do agree that in many cases I would not want to use it as my web identity. (Those cases would also overlap pretty well with cases where I don&#x27;t want to use my own email address, like when signing up for sites like this, reddit, Twitter and similar). replyspacebanana7 18 hours agorootparentprevMost consumer & enterprise web services are international, and making an implementation for every jurisdiction is burdensome.There are also some ugly edge cases - what happens if somebody is too young to get a SSN equivalent (e.g NI numbers in the UK), or somebody expatriates, or if some government allows people to request a change to their SSN-equivalent, or if a customer is a refugee without an SSN equivalent?I do think SSN-equivalent identifiers may be useful for services which are inherently for tax paying adults like some accounting&#x2F;banking services or marketplaces. reply nine_k 19 hours agorootparentprevSome of the chips would malfunction or get destroyed in incidents. You still need an update protocol! reply tim333 17 hours agorootparentprevNot convinced that&#x27;s a vote winner.See for example UK id cards scrapped in 1952 and again in 2010>...very unpopular with the public, and was regarded as an alien imposition on the British way of life. https:&#x2F;&#x2F;www.politics.co.uk&#x2F;reference&#x2F;identity-cards&#x2F; reply Dan42 18 hours agorootparentprevI can&#x27;t believe such satirical gold is getting downvoted. reply goodpoint 18 hours agorootparentThis is HN. There are equal probabilities that the post is not satirical and that the downvoters are not getting the sarcasm. reply extrememacaroni 17 hours agorootparentprevkeep hackernews away from the newborns reply user234683 18 hours agoprevWhat is the best approach to dealing with this problem as an individual?Gmail? You might randomly get locked by some AI algorithm (or you might get banned!), or something else goes wrong, and there&#x27;s no recourse.Yahoo? I recently lost access to mine because they decided to start demanding verification with a deactivated email I haven&#x27;t had access to for 15 years in order to login. Luckily, I had access in an email client, so I was able to migrate all the important accounts off of it.Yahoo&#x2F;AOL&#x2F;Tutanota&#x2F;Protonmail&#x2F;Many others? These ones will auto-delete your account if you don&#x27;t login frequently enough (not protonmail yet, but they allow it in their TOS)Self-host? All self-hosting infrastructure requires an email in the first place. Lose access to that email, lose access to payment reminders, potentially your hosting account. I nearly lost my domain since the payment reminders went to an email that I rarely check because it doesn&#x27;t support IMAP. And there is a greater increase of hacking unless you&#x27;re a professional sysadmin and have plenty of time for maintenance.Duo push? Your phone breaks.SMS verification? Phone breaks, lose access to your plan, compromised employee gives your codes away, etc.I&#x27;ve settled on using my university gmail address since (1) they promise alumni can keep it and (2) if something goes wrong with it (likely losing 2-factor by losing my phone), there is a good alumni support center. There really needs to be a human I can talk to somewhere. Still not sure if this is the best approach; am I still at risk from Google here? reply Horffupolde 18 hours agoparentYou are missing the best solution which is your own domain and hosted email like Gmail. If you get locked out like you said, “just” change providers and you lose at most a couple of hours of emails. reply arp242 18 hours agorootparentYou can lose a domain though, so that&#x27;s not perfect or guaranteed either. That said, it probably is the best option right now. reply layer8 17 hours agorootparentWith domains on auto-renewal, unless you are with an incompetent registrar or there is some legal issue, you won’t lose your domain.I agree that there should be some non-forfeitable right to a permanent personal domain though. reply arp242 16 hours agorootparentCredit cards expire so manual action will be needed at some point, contact details change, people can be in financial troubles and even the ~€10 can be a lot, people can be temporarily indisposed due to illness (ranging from cancer to serious accidents to mental illness), etc. etc.There&#x27;s tons of exceptional circumstances where people can lose access to their domain. Some TLDs have no grace period at all and it can be fairly easy to lose access. For others it&#x27;s larger, but eve",
    "originSummary": [
      "Chris Siebenmann challenges the reliability of email addresses as permanent identifiers for user accounts.",
      "Email addresses can change within organizations and can be reused or reassigned, making them unreliable for long-term identification.",
      "Siebenmann recommends using internal IDs that are unique and meaningless for account identification, even if email addresses are used for account recovery purposes."
    ],
    "commentSummary": [
      "The article highlights the limitations and drawbacks of using email addresses as long-term identifiers for online accounts.",
      "It discusses the challenges and shortcomings of sex offender registries, as well as the classification and punishment of sex offenders within the US justice system.",
      "The conversation explores the importance of secure identification services and the difficulties associated with managing digital identities. It also mentions the potential use of blockchain technology for identity verification, emphasizing the need for secure and trustworthy solutions in managing online identities."
    ],
    "points": 264,
    "commentCount": 382,
    "retryCount": 0,
    "time": 1704028380
  },
  {
    "id": 38825623,
    "title": "Bluesky platform update includes styling rules, but Sony software update causes theater projector issues",
    "originLink": "https://bsky.app/profile/donohoe.dev/post/3khu7w2kz7l2b",
    "originBody": "@donohoe.dev on Bluesky /** * Extend the react-native-web reset: * https://github.com/necolas/react-native-web/blob/master/packages/react-native-web/src/exports/StyleSheet/initialRules.js */ html, body, #root { width: 100%; /* To smooth any scrolling behavior */ -webkit-overflow-scrolling: touch; margin: 0px; padding: 0px; /* Allows content to fill the viewport and go beyond the bottom */ min-height: 100%; } #root { flex-shrink: 0; flex-basis: auto; flex-grow: 1; display: flex; flex: 1; } html { scroll-behavior: smooth; /* Prevent text size change on orientation change https://gist.github.com/tfausak/2222823#file-ios-8-web-app-html-L138 */ -webkit-text-size-adjust: 100%; height: calc(100% + env(safe-area-inset-top)); } /* Color theming */ :root { --text: black; --background: white; --backgroundLight: #F3F3F8; } html.colorMode--dark { --text: white; --background: black; --backgroundLight: #26272D; } @media (prefers-color-scheme: light) { html.colorMode--system { --text: black; --background: white; --backgroundLight: #F3F3F8; } } @media (prefers-color-scheme: dark) { html.colorMode--system { --text: white; --background: black; --backgroundLight: #26272D; } } body { display: flex; /* Allows you to scroll below the viewport; default value is visible */ overflow-y: auto; overscroll-behavior-y: none; text-rendering: optimizeLegibility; background-color: var(--background); -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; -ms-overflow-style: scrollbar; } /* Remove default link styling */ a { color: inherit; } a[role=\"link\"]:hover { text-decoration: underline; } a[role=\"link\"][data-no-underline=\"1\"]:hover { text-decoration: none; } /* Styling hacks */ *[data-word-wrap] { word-break: break-word; } *[data-stable-gutters] { scrollbar-gutter: stable both-edges; } /* ProseMirror */ .ProseMirror { font: 18px -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif; min-height: 140px; } .ProseMirror-dark { color: white; } .ProseMirror p { margin: 0; } .ProseMirror p.is-editor-empty:first-child::before { color: #8d8e96; content: attr(data-placeholder); float: left; height: 0; pointer-events: none; } .ProseMirror .mention { color: #0085ff; } .ProseMirror a, .ProseMirror .autolink { color: #0085ff; } /* OLLIE: TODO -- this is not accessible */ /* Remove focus state on inputs */ .ProseMirror-focused { outline: 0; } textarea:focus, input:focus { outline: 0; } .tippy-content .items { width: fit-content; } /* Tooltips */ [data-tooltip] { position: relative; z-index: 10; } [data-tooltip]::after { content: attr(data-tooltip); display: none; position: absolute; bottom: 0; left: 50%; transform: translateY(100%) translateY(8px) translateX(-50%); padding: 4px 10px; border-radius: 10px; background: var(--backgroundLight); color: var(--text); text-align: center; white-space: nowrap; font-size: 12px; z-index: 10; } [data-tooltip]::before { content: ''; display: none; position: absolute; border-bottom: 6px solid var(--backgroundLight); border-left: 6px solid transparent; border-right: 6px solid transparent; bottom: 0; left: 50%; transform: translateY(100%) translateY(2px) translateX(-50%); z-index: 10; } [data-tooltip]:hover::after, [data-tooltip]:hover::before { display:block; }JavaScript Required This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. Learn more about Bluesky at blueskyweb.xyz and atproto.com. Post Michael donohoe.dev did:plc:n4mr5xclsdxolbgynwxaanq5 All movies cancelled at Alamo Drafthouse cos Sony issued a bad software update for the projectors. Likely affecting other theaters too. 2023-12-31T17:19:32.955Z",
    "commentLink": "https://news.ycombinator.com/item?id=38825623",
    "commentBody": "Sony software updates breaks movie theater projectorsHacker NewspastloginSony software updates breaks movie theater projectors (bsky.app) 214 points by donohoe 16 hours ago| hidepastfavorite108 comments sprocket35 13 hours agoDigital cinema tech here. Sony hasn’t been releasing updates since they exited the business in 2020.This is likely an expired certificate related to the encryption on the movie files. reply crazygringo 12 hours agoparentAre certificates usually&#x2F;often issued to expire at the end of a given calendar year? On the one hand, that would explain this happening on Dec 31.But it&#x27;s slightly weird because it&#x27;s not yet the new year in UTC. This was posted several hours before that happening, and we&#x27;ve still got over 2 hours to go... (And the cinema is in New York, with almost 8 hours to go in local time, so it&#x27;s not a local timezone issue either.) reply mistermo 40 minutes agorootparentThe Dolby&#x2F;Doremi servers certificate ended the Dec 31 too. Our maintenance subcontractor updated them few weeks ago. SO I guess it is common for the industry. Then it is an industry which often play 365d&#x2F;year so, it&#x27;s just another sunday&#x2F;monday. (Add to this our weeks doesn&#x27;t start Sunday&#x2F;Monday, but Wednesday for France by example, and Friday for the US, day of the release of new movies) reply userbinator 12 hours agorootparentprevThis is Sony, and it is the new year in Japan. reply numpad0 7 hours agorootparent(fyi)Japan Standard Time(JST) is UTC+9, single zone and no DST. Not sure of absolute time but Tweet is showing \"Jan 1, 2024 at 2:19\" timestamp on my JST phone. reply FirmwareBurner 11 hours agorootparentprevSony is a Japanese company but it&#x27;s also a multinational company consisting of hundreds of companies they aquired over the years all over the globe. It&#x27;s therefore a relatively small chance is has something to do with Japan specifically. reply swells34 11 hours agorootparentIt likely depends on where the programmer who has the original cert issued was... so quite likely Japan, given the evidence reply joseph8th 5 hours agorootparentThis is the most likely, yup. The certs are unlikely to have a precise end-of-year expiration. It will be whatever the expiration is for the last certs loaded.10 years in cinema IoT, here. Features are encrypted by Key Delivery Messages (KDM), and those are per cinema server&#x2F;projector \"marriage\". No KDM will be considered valid if the server certs are expired.This should make 2024 interesting for me. replysaghm 12 hours agoparentprevHonestly, is that better? Leaving all the devices set to EOL and get bricked or whatever feels pretty wasteful. reply sprocket35 11 hours agorootparentThe studios set the requirements. A certificate that never expires would never fly with their need to control DRM as tightly as possible.If Sony was still in the business, they would offer a certificate renewal for a small fee like the other manufacturers have done.The root problem is that Sony exited the market and left a lot of cinema owners out to dry with the looming cost of $50,000+ per auditorium for replacement projectors. reply saghm 8 hours agorootparent> The root problem is that Sony exited the market and left a lot of cinema owners out to dry with the looming cost of $50,000+ per auditorium for replacement projectors.That&#x27;s the point I was trying to make; if someone accidentally pushed a broken update, that sucks, but its not the first or the last time it will happen, and at least there&#x27;s a clear path to it getting fixed and someone being paid to do it. The cert being expired and the only company who would have been in charge of fixing that not really caring anymore sounds to me worse from pretty much any possible point of view, and selling a product that will stop working if you decide to stop servicing it seems pretty terrible. reply thomaslord 7 hours agorootparentprevAny idea if these projectors could be made to work with other sources that don&#x27;t require DRM? As much as this sucks for movie theaters, it could also be the deal of the century for a home theater build if you know the right people... reply simcop2387 4 hours agorootparentI don&#x27;t think you&#x27;d want to pay the power bill for one, nor would you want to have a large enough screen to not have it burn your retinas. That said I believe some of them do allow that (no clue what models or availability) since you can rent out theaters to play games on your own computer or console, https:&#x2F;&#x2F;www.cinemark.com&#x2F;private-events&#x2F;gaming&#x2F; reply MandieD 2 hours agorootparentIf you’re in Central Texas, one of my high school friends and her husband run The Beltonian in Belton (shocking), a beautiful single-screen theatre built in 1922, renovated to have about 140 modern seats with tables, featuring various classic movies, reasonably-priced pub grub, Central Texas wines and beers, and available for similar rental arrangements (gaming or movies).If you’re not in Central Texas, you likely have a similar, beautifully-restored old theatre available for similar rental arrangements.The Beltonian: https:&#x2F;&#x2F;thebeltoniantheatre.com&#x2F; reply userbinator 10 hours agorootparentprevIMHO a certificate that expires at the same time as the copyright on a film, and DRM that then \"fails open\", makes the most sense. reply coder543 9 hours agorootparentOkay, then I set the date on my computer to year the 2124. Now the DRM is unlocked!You can&#x27;t build a \"time lock\" with just encryption primitives. Even if you could build a time lock with just encryption primitives, we don&#x27;t know when the copyright will expire until the original author has died, since copyright term is life + 70 years.Someone would have to run a server that specifically chooses to start serving the keys on that date, which is an absurd notion given how absurdly long copyright lasts these days.If the federal government were interested in passing a law that required this, I&#x27;m sure the Library of Congress could run such a server, but no such law exists. reply shkkmo 5 hours agorootparent> set the date on my computer to year the 2124. Now the DRM is unlocked!It&#x27;s the same. \"Fail closed\" means you can \"just\" set the system time to be before the expiration date, \"fail open\" means you can just set the system time to be after.Either way, having an expiring decryption key is just security theater that harms users. reply alexvoda 7 hours agorootparentprevNot at all absurd.Maybe not through crypto but otherwise perfectly achievable. No reason to achieve this within a file. If you really want to, some fancy solution involving blockchains and smart contracts is probably possible but there is no need.Require by law all who desire copyright protection to register the work with a governmental agency and place a copy of the work in escrow at an archive as a condition. The archive knows when the copyright expires and starts serving the work to the public from that point forward. This is what to do if a state cares about public domain.The status quo of corporations abandoning works to bit rot, actual rot, misplacement or fires for decades is strictly worse. reply coder543 7 hours agorootparent>> If the federal government were interested in passing a law that required this, I&#x27;m sure the Library of Congress could run such a server, but no such law exists.> Require by law all who desire copyright protection to register the work with a governmental agency and place a copy of the work in escrow at an archive as a condition.You’re just repeating what I already wrote. By all means, call your representatives. reply nagisa 9 hours agorootparentprevNot for the studios. There’s nothing more they would love than to get a continuous and non-ending stream of money for content they themselves have forgotten about owning. reply numpad0 5 hours agorootparentprevThere hasn&#x27;t been cryptographic algorithms nor a DRM that proved good for > 50 years, let alone 70 years + safety margin. Even 3650 days expiry is considered too long. reply prepend 11 hours agorootparentprevHaving a short term cert seems wasteful too.How frequently does Sony change its identity. They should have a 999 year cert expiration and then check a revocation list in the off chance they Sony gets its private keys rooted. reply saghm 8 hours agorootparentI don&#x27;t disagree! I don&#x27;t feel like the exact technical justification really makes a difference here; there were working projectors, and now there aren&#x27;t, and it&#x27;s hard to see how that&#x27;s not directly due to Sony designing things in a way that required them to continue operating that part of the business in order for that not to happen. reply dn3500 12 hours agoparentprevA digital cinema tech with the username \"sprocket35\"? reply Aloha 11 hours agorootparentI mean his comment history is on par with who he says he is - I think it&#x27;s a great name. Just because I work mostly in the digital domain doesn&#x27;t mean I dont long for the analog. reply firtoz 12 hours agorootparentprevThere have been stranger things reply crazygringo 15 hours agoprevSuper curious if there&#x27;s any kind of contractual recourse where theaters can recoup the lost income from Sony.It&#x27;s one thing if a projector breaks mechanically or due to a pre-existing bug; it&#x27;s another thing when an update breaks it.In an age where updates are increasingly the norm, I wonder if there&#x27;s legislation needed to hold manufacturers accountable for updates that break otherwise perfectly-functioning hardware? reply amelius 14 hours agoparent> In an age where updates are increasingly the norm, I wonder if there&#x27;s legislation needed to hold manufacturers accountable for updates that break otherwise perfectly-functioning hardware?Maybe there should be a law that says:1. Upgrades may be performed but never behind the user&#x27;s back.2. In particular, the user determines exactly when an upgrade is performed.3. The user may roll back any update at any time.4. Any services which the software depends on should be compatible with all versions of the updated software.EDIT: 5. Security backports should be made available. However, the user should always be in control over whether they are installed. Sometimes working code is more important than 100% secure code. Also this rule will prevent companies from quickly forcing an update and sweeping security breaches under the rug. reply 542458 13 hours agorootparent1 and 2 - this seems incompatible with how 90% of the population uses software, namely they set it and forget it. Having to manually approve and schedule every single update for everything a user touches would be a) a security nightmare, as most things would never get updated ever and b) a UX nightmare, with a million different things asking for updates.3 - Maintaining a data path forward is tricky enough. Demanding that users be able to downgrade at anytime would be a very tall ask if user data has to survive the downgrade.4 - This seems outlandishly expensive to do. This effectively reads “nobody can ever deprecate an api on anything”. This also seems to be broadly incompatible with fixing certain security vulnerabilities - would everybody have to maintain TLS 1.1 or plaintext api endpoints for old clients? Would a social media network have to maintain api endpoints that leaked more data than users were comfortable with? reply userbinator 12 hours agorootparentIt&#x27;s all an incentive for \"don&#x27;t just churn software, plan well ahead\".This also seems to be broadly incompatible with fixing certain security vulnerabilities - would everybody have to maintain TLS 1.1 or plaintext api endpoints for old clients?Or they would forced to produce an update that doesn&#x27;t do anything other than e.g. upgrade the TLS version --- and has absolutely nothing else. reply stevehawk 7 hours agorootparentit also skyrockets the barrier to entry for software to a point that no one but the big companies could compete. reply userbinator 2 hours agorootparentIt raises the barrier enough to stop the lowest-common-denominator forced-obsolescence crap from becoming popular. If an individual can make software that lasts, big companies have no excuse. reply ToucanLoucan 12 hours agorootparentprev> 1 and 2 - this seems incompatible with how 90% of the population uses software, namely they set it and forget it. Having to manually approve and schedule every single update for everything a user touches would be a) a security nightmare, as most things would never get updated ever and b) a UX nightmare, with a million different things asking for updates.I don&#x27;t see how an automatic update setting is incompatible with 2. If a user says \"go ahead and install updates as needed\" that is the user expressing their desire to receive updates.I also think the phrasing in 1 is a little needlessly aggressive though I believe it comes from a place of frustration. The difference in my mind between saying \"this thing updated behind my back\" and \"this thing updated automatically for me\" is whether the user has registered the update as being beneficial or not, and depending on the device, that&#x27;s a WIDE spectrum. I know my smart outlets update their firmware all the time, and an extremely small handful of times I do notice, because sometimes they end up not reconnecting to the wifi quite right and need to be reconnected. However if they updated and, for example, broke HomeKit support and no longer worked, I&#x27;d be angry the next time I tried to use them.> 3 - Maintaining a data path forward is tricky enough. Demanding that users be able to downgrade at anytime would be a very tall ask if user data has to survive the downgrade.I mean, this is just an engineering problem pure and simple. Most of the time, in my experience, graceful downgrade just isn&#x27;t prioritized because, well, who can even do it for starters? Installing old software oftentimes means you need to do some really intense stuff, like wiping whatever device entirely, so the retention of data is moot.If this was mandated I see no problem with getting it done in my industry. It&#x27;s simply a matter of making it a priority IMO.> 4 - This seems outlandishly expensive to do. This effectively reads “nobody can ever deprecate an api on anything”.With certain products I can definitely see it being an advantage, and the first place my mind goes to is again, smart home products and appliances, automotive hardware, that sort of thing. Large, expensive items that incorporate software that the user interacts with can be an absolute nightmare when the OEM randomly decides that the way something&#x27;s worked for years and years for you is now just not an option, or worse still, locks it behind a paywall. And what are your options here? Buy a new car or dishwasher? Or eat shit and pay them $20 a year that they have not earned and are providing no value for?This is why the newest car I have is a 2018 Corvette, because I know all it&#x27;s software and have access to it, and there&#x27;s no system that&#x27;s going to lock my heated seats behind a Chevrolet Premiere+ subscription where I have to give chevy money to permit my car to engage a damn relay for me. reply windows2020 13 hours agorootparentprevMy take on this is back when software was distributed on media like CDs, new versions were better. Updates were discrete, marketed and expensive. They had to be good!Continuous updates continue to permeate, including into things that are still surprisingly connected to the internet in the first place.I think that in time, forced updates will cause enough trouble that people will become more conscious of and dislike them. For some, one bad update is all it will take.So, I think it&#x27;s worth waiting to see if anti-update competitors appear before regulating this. reply az226 14 hours agorootparentprevWindows 10 forced an updated on my PC and deleted all my personal files. I paid a forensic data firm four digits to get about half my data back. reply at_a_remove 12 hours agorootparentCould you tell us more about this?Were you keeping your personal files in the usual \"Documents\" and \"Videos\" and such laid out by Microsoft? Or somewhere else? reply userbinator 12 hours agorootparentNot the one you&#x27;re responding to, but my guess is this: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18189139 reply zitterbewegung 10 hours agorootparentprevI have a PS3 and Sony removed the ability to load Linux on the device. Basically a class action lawsuit was filed because of the removal . The class action was ruled that they were liable.For number 3 if we allowed for people to roll back any update that can include Teslas where they are doing OTA safety updates so that wouldn’t work out. reply johnchristopher 11 hours agorootparentprev> 1. Upgrades may be performed but never behind the user&#x27;s back.> 2. In particular, the user determines exactly when an upgrade is performed.Haha, at last, yes !! Take that stupid windows XP countdown to reboot !! reply dejj 11 hours agorootparentprevAmelius’ 5 laws to Asimov’s 3.Make them a bit terser, and maybe “Right to repair” will heave them out of the science fiction tarpit. reply tantalor 14 hours agorootparentprevNot gonna happen in a million years reply vlovich123 14 hours agorootparentSome of those suggestions are things that sound good at first glance but are simply not great ideas.For example, support for downgrades means you a security vulnerability can be reintroduced by a malicious user which may not be desirable. Writing software that’s backwards and forwards compatible across all releases can be extremely expensive to impossible (eg a feature in your application that requires a new OS or you need to use a now removed API when running on older releases).There are difficult technical issues involved and trying to legislate specifics may not be the best idea vs other approaches that improve real freedom (eg you have to release sufficient details to your customers that they can write their own software for your hardware). reply thfuran 14 hours agorootparent>For example, support for downgrades means you a security vulnerability can be reintroduced by a malicious user which may not be desirable.And there are regulated industries where a software update could be the fulfillment of a recall. reply amelius 13 hours agorootparentprev> For example, support for downgrades means you a security vulnerability can be reintroduced by a malicious user which may not be desirable.Backports exist because of this reason. Just added them as a requirement to the list of rules above. reply csdvrx 13 hours agorootparentprev> Some of those suggestions are things that sound good at first glance but are simply not great ideas.Actually, yes they are. I&#x27;m not a big fan of legislation, but the upgrade crazyness has to stop at some point!> For example, support for downgrades means you a security vulnerability can be reintroduced by a malicious user which may not be desirableWhat if \"I, the user\" deem it \"desirable\"?I&#x27;m holding to bios with known vulnerabilities so I can work around \"security features\" that are \"for my own protection\" like 1) preventing me from underclocking (to keep the security features of the now-dead SGX) 2) using any M2 WWAN or NVMe that I wantIt&#x27;s gone to a point where it&#x27;s not desirable for me to upgrade, and to prefer the risks that come with an exploit as at least I know my freedom to use my hardware the way I want will not suddenly become limited.Another example: getting root on android with mediatek was considered a \"bug\" and work a mandatory \"upgrade\" that prevent users from being able to get root that way.But I want to be root! reply thfuran 12 hours agorootparentDo you believe that the operator of a deliberately insecure system should be liable if it ends up suborned by a botnet and used to attack someone else&#x27;s system? reply csdvrx 11 hours agorootparent> Do you believe that the operator of a deliberately insecure system should be liableOnly if said operator signed a contract. No contract=no liability.Here in the US, the Supreme Court has made it clear that law enforcement agencies are not required to provide protection to the citizens cf https:&#x2F;&#x2F;www.nytimes.com&#x2F;2005&#x2F;06&#x2F;28&#x2F;politics&#x2F;justices-rule-po...If even the police isn&#x27;t liable, why should I be liable or have any kind of duty to protect your system?Your system, your problem. reply thfuran 11 hours agorootparentDo you also think it&#x27;s fine to shit in other people&#x27;s wells or for companies to dump their toxic waste in the middle of other companies parking lots as long as you&#x2F;they haven&#x27;t signed a contract explicitly agreeing not to? reply csdvrx 10 hours agorootparent> Do you also think it&#x27;s fine to shit in other people&#x27;s wells or for companies to dump their toxic waste in the middle of other companies parking lots as long as you&#x2F;they haven&#x27;t signed a contract explicitly agreeing not to?You are using loaded words to 1) imply I would support some questionable actions where you assume intent and 2) refer to negative externalities, but I&#x27;ll suppose you are not trolling interact in good faith with you.Both the actions you define are ignoring property rights: your well, your parking lot = I can&#x27;t do that, unless you allow me (with a contract!)My well, my parking lot = I can do that, and you don&#x27;t get to say what I do with my property, unless we have signed a contract which creates liability.Many people seem to have a strong desire to be able to force ME to update MY browser&#x2F;operating system&#x2F;bios&#x2F;whatever else to be up to THEIR standards, because it has consequences on THEIR liability.I care a bit about them, but I care way more about MY freedom: I do what I what with MY computers.Note that intent matters: I have no desire to cause bad things to other people. I&#x27;d be very sad if my computer was used as \"part of a botnet\" like someone else said. I might even try to avoid that - but only as far as it puts my freedom first, and there&#x27;s not even a requirement that I try (because I might have better things to do lol)Should bad things like botnets happen, 1) it wasn&#x27;t my intent, as preserving my freedom was my intent 2) the negative externality is sad, but it&#x27;s not my liability: you should secure your property, or said differently \"your problems aren&#x27;t my problem\"This whole interaction feels very strange to me. By any chance, are you European? Europeans seem to have very different concepts of freedom and liability than we do. reply thfuran 9 hours agorootparent>but it&#x27;s not my liability: you should secure your property, or said differently \"your problems aren&#x27;t my problem\"But we&#x27;re talking about you deliberately not securing or even actively reducing the security of your property in a manner that could reasonably be predicted to lead to harm to other&#x27;s property, and that harm occuring, through no fault of the harmed party. You&#x27;re not supposed to store loaded guns unsecured on your front porch (not around here anyways).>By any chance, are you European? Europeans seem to have very different concepts of freedom and liability than we do.No, I&#x27;m from the US.>You are using loaded words to 1) imply I would support some questionable actions where you assume intent and 2) refer to negative externalities, but I&#x27;ll suppose you are not trolling interact in good faith with you.I&#x27;m asking whether you would, not suggesting that you do. But yes, the acts in question were significantly more questionable than those in my previous comment, since your answer to that was more extreme than I expected. So how about the middle ground: Do you believe that the EPA should impose restrictions on companies&#x27; or citizens&#x27; right to dump whatever toxic waste they want into rivers? If you think the physical commons should be protected from predictable harm by negligence or reckless disregard, why not the digital? reply csdvrx 7 hours agorootparent> But we&#x27;re talking about you deliberately not securing or even actively reducing the security of your propertyWe have different preferences about what&#x27;s the right security&#x2F;freedom ratio.> You&#x27;re not supposed to store loaded guns unsecured on your front porch (not around here anyways).I don&#x27;t think the government or anyone has any say about where or how I may keep my guns.People believe they might have a say, so there are laws on the books, but they&#x27;re frequently taken down by the courts.> So how about the middle ground: Do you believe that the EPA should impose restrictions on companies&#x27; or citizens&#x27; right to dump whatever toxic waste they want into rivers?I believe it shouldn&#x27;t, but that&#x27;s just my opinion.You may not believe it, but in terms of efficiency and keeping the environment, it doesn&#x27;t matter (see below)> If you think the physical commons should be protected from predictable harm by negligence or reckless disregard, why not the digital?I don&#x27;t believe in in the physical word, and I don&#x27;t believe it in the digital world either.Regardless of my beliefs, there&#x27;s a nobel prize winner who&#x27;s shown that it doesn&#x27;t matter how the rights are initially assigned, as long as parties can negotiate with no transaction costs.Check the Coase theorem.The EPA imposing restrictions create transaction costs, so I think we&#x27;re better off without them replyhdhuwgdue2 13 hours agorootparentprevRegular bios updates rub me the wrong way. Wife&#x27;s lappy recently decided to update it of its own volition too. I was livid, but thankfully nothing broke. I hate that OS can do it, but I hate more the fact that bios is clearly less reliable.. replymikequinlan 15 hours agoparentprev>hold manufacturers accountable for updates that break otherwise perfectly-functioning hardwareWouldn&#x27;t the license agreement that you agreed to when you installed the software specify any responsibilities of the vendor and define what recourse you might have? Why would government action be needed? reply crazygringo 14 hours agorootparentBecause I have no negotiating power. Every license agreement says there&#x27;s no recourse, so I can&#x27;t pick one that does have it.There&#x27;s a major free-market failure because there&#x27;s no negotiation over the agreement. There&#x27;s no representative for consumers pushing back. So that representative needs to be the government.This is the entire reason for consumer protection laws. reply 4death4 14 hours agorootparentThere is a negotiation over the agreement. If you don’t agree with an agreement, then buy a different product. Thats how all agreements work. It’s just not very fun to negotiate when there’s a large power imbalance. reply phartenfeller 14 hours agorootparentSame logic as why you need worker&#x2F;renter rights, just find another employer&#x2F;apartment. Sorry that the toaster shocked your wife, but you could have just gotten a different one with better safety standards.It is great that the government protects consumers. Otherwise, everyone would need to spend hours researching everything before making a rare purchase. reply hypeatei 13 hours agorootparentYou&#x27;re conflating human rights and safety issues with broken projector software which seems disingenuous.> Otherwise, everyone would need to spend hours researching everything before making a rare purchaseOn this issue specifically, these projectors seem to be in the tens (possibly hundreds) of thousands of dollars so some research and due diligence doesn&#x27;t seem that far fetched. reply Adverblessly 13 hours agorootparentprev> It’s just not very fun to negotiate when there’s a large power imbalance.Indeed, which is why people may choose to band together in a bigger bargaining block to improve their position and possibly even achieve greater power than the other party. For example, they could choose to form a single block that represents the citizens of an entire country. reply 4death4 12 hours agorootparentThat seems like the opposite of the original comment, which was a desire to negotiate on an individual level. reply malfist 14 hours agorootparentprevFind me a theater projector firmware&#x2F;software that doesn&#x27;t have an EULA. Go on, I&#x27;ll wait. reply ncallaway 14 hours agorootparentprevBecause the government has put constraints on what kind of agreements are valid (especially in the domain of sales to consumers).It happens all the time across many domains (look up the Uniform Commercial Code, for more general examples, or laws around vehicle sales).I have less inclination to be involved in business to business transactions, but there’s absolutely a societal debate to be had around what laws and regulations we have on transactions of software.Society runs smoother, with more transactions, and this economic wealth, when consumers can assume a reasonable baseline of behavior that is being regulated by the government. If every purchase and every transaction requires deep due diligence there will be far fewer transactions. reply Levitz 13 hours agorootparentprevBecause government action is consumer action.The government is not some foreign third actor, we live in a democratic society and as such, the way in which we do things is subjected to the desires of the public.If enough people consider the government should intervene, then the government should intervene. reply delecti 14 hours agorootparentprevPrecisely for that reason. Most license agreements require the end-user to waive any recourse. That&#x27;s what customer protection laws are for. reply iancmceachern 14 hours agoparentprevIn my experience in this industry they often have multiple (2-3) projectors in the projection booth for exactly this kind of issue, a bulb goes out, etc. They also play the previews and ads before the movie on a different, cheaper projector.Having 2 Sony projectors wouldn&#x27;t help here though... reply vlovich123 14 hours agorootparentIt would if Sony staggered software releases to sibling projectors. reply csdvrx 15 hours agoparentprev> updates that break otherwise perfectly-functioning hardware?\"Every update is a downgrade\":http:&#x2F;&#x2F;itre.cis.upenn.edu&#x2F;~myl&#x2F;languagelog&#x2F;archives&#x2F;000606.h... reply donmcronald 13 hours agorootparentThis is my experience with Roku TVs. They used to be ok, but they can&#x27;t resist updating them and I have 3 that run noticeably slower than they did originally. It&#x27;s probably a combination of the OS and apps.Even worse, something got updated that broke CEC integration with my sound bar on one of them, so now I can&#x27;t use the built in volume control and need to use 2 remotes instead. I know it&#x27;s a real first world problem, but it infuriates me that they can slowly ruin a TV that I own and I have no recourse.I&#x27;m so sick of the tech industry I hope the whole thing collapses. We need major legislation updates to make tech companies liable for all awful they&#x27;re doing to the world. reply transcriptase 12 hours agorootparentRoku and smart TV manufacturers are infuriating.They choose a processor that’s barely sufficient to run the software it releases with, and proceed release a constant stream of updates with nothing of value to the user. Meanwhile every update has the device running 5% slower, making it noticeably sluggish after a couple years.It almost feels intentional, but I’m sure no bean counter is going to permit spending a few dollars extra per unit for something they probably see as reason for people to upgrade. reply m463 13 hours agorootparentprevI liked this:> Notice, I&#x27;m no Luddite. I don&#x27;t reject technology. I depend on it. reply Xenoamorphous 11 hours agoparentprevDaikin semi-bricked my 3 aircon units in the middle of the summer with a firmware upgrade. reply hypeatei 15 hours agoparentprevMeh, shit happens and maybe software rollback should be codified. Let the compensation be between the two parties involved (Sony and the customer) - similar to SLAs in the cloud. reply malux85 14 hours agorootparentThere is an enormous power imbalance between Sony and the customer which will lead to abuse.They will filibuster and or beaurocrat-ize away any will to pursue lawsuits, or they will offer token trivial compensation (which doesn’t nearly reflect the actual lost income)Your “meh” apathy is what leads to the abuse of power by the larger parties reply hypeatei 14 hours agorootparentI just don&#x27;t see the need for a law which turns a civil issue into a criminal one. There are already existing frameworks for this - contracts. If someone isn&#x27;t comfortable with the terms, then they don&#x27;t use Sony products.If your argument is that Sony is too big and has a monopoly on projectors - then antitrust laws exist. reply handoflixue 14 hours agorootparentAs a society, we&#x27;ve already concluded that contracts are insufficient to cover a huge class of situations (minimum wages, banning non-competes, etc.). Why would you think they&#x27;re sufficient to handle this one?We already have simple systems that handle \"you broke my stuff\" fairly well - why would we want to lean on something as slow and complex as antitrust laws to resolve this? The Epic vs Google lawsuit started in 2020. 3 years is a long time to wait to collect damages for broken projectors. reply hypeatei 14 hours agorootparent> Why would you think they&#x27;re sufficient to handle this one?It&#x27;s a business transaction where contracts are the norm. Sony may not be very flexible on terms, but no one is forced to buy their projectors and agree to the terms.> why would we want to lean on something as slow and complex as antitrust lawsWe would if consumers had no other choice but to buy Sony projectors only - that doesn&#x27;t seem to be the case, though. reply MobiusHorizons 11 hours agorootparent> but no one is forced to buy their projectors and agree to the termsI don’t know how much choice movie theaters have. As I understand it, these projectors read directly from a hard drive, and are heavily regulated to avoid piracy. According to the Wikipedia article [1] there are only 4 approved manufacturers, and until very recently Sony had the only 4k model.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Digital_cinema (see the projectors for digits cinema section) reply verve_rat 14 hours agorootparentprevI think you are confused about how laws work. A law can cover sales and transactions without any criminal penalties. It can layout the ground for civil actions, to be taken by either government entities or the effected parties themselves.Just because a law is created doesn&#x27;t mean a new crime with criminal penalties is created. reply hypeatei 13 hours agorootparentYou&#x27;re right, there doesn&#x27;t have to be criminal penalties. Codifying compensation requirements for buggy software seems like it would need to be very broad and effectively a useless law, though. reply verve_rat 12 hours agorootparentNot really, just a simple law that said software vendor are liable for actual loss caused by their products would have a huge impact.Courts are our mechanism for sorting out the details, not legislation. reply guhidalg 14 hours agorootparentprevI’m sure Sony is sensitive to the PR hit from movie theaters telling their customers that the reason they can’t watch a movie is precisely due to a Sony software update. Next time the consumer is buying a Sony product they’ll think twice about its software reliability. reply ipython 14 hours agorootparentI argue Sony doesn’t care as this isn’t the first time they’ve shafted customers (see below), and it sure as heck won’t be the last.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Sony_BMG_copy_protection_roo... reply guhidalg 11 hours agorootparentThanks for sharing that, my trust is Sony’s software is now lower than before. replylayer8 13 hours agoparentprev> updates that break otherwise perfectly-functioning hardwareArguably the hardware still functions perfectly, it’s the software that’s broken. reply rcdemski 13 hours agoprevDown in Denver too. My money is on a date rollover issue related to DRM. reply sschueller 11 hours agoprevI wonder how much longer until we have a military or medical device that doesn&#x27;t work killing people because a DRM certificate expired... reply throwawaymedic 10 hours agoparentOhh this has already started. Happened to me the other day: machine for rectal air insufflation under x-ray guidance to allow reduction of an intussusception (a condition where the bowel folds in on itself). It primarily affects young children and if the procedure fails they require an open operation. Failure to treat the intussusception eventually results in bowel perforation, peritonitis and potentially death.Went to start the procedure, machine reports it’s ready to work, all set up, assistant presses start, cryptic error messages. No way to fix. Turns out our license has improperly expired for reasons unknown. No way to override.It’s past 5 pm Friday of the New Year’s weekend. No one on call for the company has any idea how to fix. Took three hours repeatedly phoning the company to finally get put through to an engineer who gave us magic series of button presses and codes to get the machine working.We used to have a purely mechanical machine, maintained in house by an on site engineer. Now we have to deal with this. There is only one company making these machines, so no competition. Progress. reply userbinator 7 hours agorootparentIt was (and probably still is) quite common to hire a cracker to remove the DRM from industrial equipment software to prevent any downtime it causes; of course that&#x27;s not a life-support application, and the motive was that downtime for such machinery is often very costly.Of course with the recent Polish train debacle (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38788360) stories of DRM schemes where incomplete defeat causes subtly worse behaviour (rather common in games), and the ability to engineer a system with plausible deniability in mind, like some of Apple&#x27;s hardware-locking attempts, one does wonder whether medical devices may have such logic bombs too. reply waiwai933 5 hours agorootparentCan you share&#x2F;link to any stories of the first? I&#x27;ve never heard of it before and would be keen to read more on it. reply userbinator 3 hours agorootparentIt is not surprisingly difficult to find this stuff now, but there are some stories here: https:&#x2F;&#x2F;www.eevblog.com&#x2F;forum&#x2F;fpga&#x2F;altera-software-dongle-te... reply Maxious 5 hours agoparentprev> Why Repair Techs Are Hacking Ventilators With DIY Dongles From Polandhttps:&#x2F;&#x2F;www.vice.com&#x2F;en&#x2F;article&#x2F;3azv9b&#x2F;why-repair-techs-are-... reply mixmastamyk 11 hours agoparentprevProbably already happened but the witnesses have expired. reply lancesells 14 hours agoprevIs there something where a projector needs to be connected to the internet? This seems as silly as bluetooth speakers like Sonos needing an internet connection.Perhaps someone can share what is needed here and why it&#x27;s connected. reply joseph8th 4 hours agoparentNo, the projector is never connected to the internet. It&#x27;s connected to a cinema server. In the case of Sony, it&#x27;s always a Sony server. This is not usually connected to the internet either. Instead, the usual topology is to have a Theater Management System (TMS) that pre-ingests content in the form of Digital Cinema Packages (DCPs) and then propagates it to each auditorium&#x27;s server.This TMS is also not connected to the internet in most cases. Digital cinema is locked down tight as an ATM. Most theaters have pretty meager on-site IT, so email and thumb-drives and hard-drives still rule.I work in cinema IoT including KDM and DCP delivery and ingestion to TMS or cinema server, and our solution is to have a separate agent inside the private cinema network that can broker communication with cinema devices like projectors, calibrators, and audio processors. Some of these have their own UIs in the local network, or if you&#x27;ve got the company VPN, but in general for monitoring we just rely on SNMP or server API.The cinema servers are different. They all have APIs that provide varying levels of monitoring, control, and automation for the server, itself, as well as connected devices including limited monitoring and control of projectors and audio. They all support RDP or VNC, so if you&#x27;re behind the same firewall you can get to their UI. Same with TMSes... they have UIs that you can access remotely, if you&#x27;re on the company VPN.But the projector itself? Never on the internet. It&#x27;s \"married\" to the cinema server, and will only work with that particular server, based on their respective certs.In Sony&#x27;s case, it sounds like the projector certs have expired, so now they are invalid when used with the updated server certs. reply buro9 14 hours agoparentprevIIUC, movies are delivered to local storage via the internet, and those files are heavily DRMd, the DRM is checked synchronously when films are played. reply 542458 13 hours agorootparentThat sounds roughly correct based on when I worked at a theatre, although back then they mailed you drives. Worth noting that the movies are encrypted and you only get decryption keys at release.But Sony hasn’t made projectors in a while. I suspect this was something like an expired certificate rather than an actual software update. reply joseph8th 4 hours agorootparentIt hasn&#x27;t changed much. Most features are still delivered on drives. They&#x27;re just too damn BIG otherwise.Keys are sent separately, and are valid only for a certain date-range, and for a specific cinema server. In this case, Sony servers only work with Sony projectors, and vice-versa. Each device has its own certs, but for encrypted feature encoding, the standard is Key Delivery Messages, which unlock the feature Digital Cinema Package (DCP). DCPs are a general purpose cinema package, and is also used to deliver unencrypted clips like ads and trailers.But the key is the Key. It&#x27;s only valid for the specific cinema server, and the cinema server is \"married\" to the projector by encryption. This protects against on-site MITM attacks.If there&#x27;s a server update that doesn&#x27;t update the certs on either the server itself, or the projector (in Sony&#x27;s case), then the marriage breaks, and the silver screen stays dark.Nothing anyone else can do about it, either, since any valid certs would have to be issued by Sony, and nobody has the private keys except Sony. reply joezydeco 14 hours agorootparentprevSo maybe this was a key rotation issue instead of what we think of as a software update (e.g. bugfixes)? reply happyopossum 4 hours agoparentprev> as silly as bluetooth speakers like Sonos needing an internet connectionWhere are you getting your information? Sonos speakers that support bluetooth don&#x27;t need an internet connection to use bluetooth. They only need an internet connection when you want them to stream music from the internet. reply rblatz 7 hours agoparentprevSonos isn’t Bluetooth speakers, they have a couple in their portable lineup that do offer Bluetooth, but last I checked the majority don’t offer Bluetooth and to be honest I like it better this way.Not having my phone locked up to only play music is awesome, being able to use voice control to play specific songs and have them play throughout the house is awesome. Bluetooth is a pain, not being able to use Instagram (or any other app that wants control of audio) while my daughter listens to a song sucks. reply Baldbvrhunter 14 hours agoparentprevDRM and no unauthorised screenings outside of approved show times.Although you can use them for non DRM showings. reply rladd 12 hours agoprevThey recently released an update for my 2021 vintage OLED TV, and after applying it it now doesn’t work properly at all. It’s still usable, but only barely. reply sjfjsjdjwvwvc 4 hours agoprevCopyright is holding us all back and needs to be reformed, if not abolished entirely. reply imperialdrive 13 hours agoprevThat&#x27;s terrible. Very curious who pushed that button and why. Just confirmed they&#x27;re closing all locations. reply stephenhandley 5 hours agoprevplease tell me there&#x27;s pictures reply aaron695 9 hours agoprev [–] Lots more info here on the Y2K24 bug -http:&#x2F;&#x2F;www.film-tech.com&#x2F;vbb&#x2F;forum&#x2F;main-forumExpired cert. Some players refuse, some ignore it.Affected - Sony (XCT-M10) &#x2F; GDC SR-1000 - \"15-08-2011 18:10:59 UTC 31-12-2023 00:00:00 UTC (expired)\"There was a known Y2K24 bug that needed software updates to fix around certs, unsure how&#x2F;if related -http:&#x2F;&#x2F;www.film-tech.com&#x2F;vbb&#x2F;forum&#x2F;main-forum&#x2F;26517-dolby-do... reply idlephysicist 5 hours agoparentI love that the forum where that discussion is taking place doesn&#x27;t even use HTTPS. reply joseph8th 3 hours agorootparentThe cinema industry in general is woefully way behind the curve in some respects... Cutting edge in others (like security). reply trebligdivad 7 hours agoparentprev [–] Nice! So that was identified a year ago and it&#x27;s people who haven&#x27;t updated since who hit it? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "@donohoe.dev made changes to the Bluesky platform, focusing on extending the react-native-web reset.",
      "The changes include CSS styling rules, color theming, and additional styling hacks for different elements.",
      "An incident involving a software update for projectors resulted in the cancellation of all movies at Alamo Drafthouse, highlighting potential impacts on other theaters."
    ],
    "commentSummary": [
      "Sony software updates have led to malfunctions in movie theater projectors due to an expired encryption certificate.",
      "This situation raises questions about copyright, DRM, and the responsibility of manufacturers for update-related malfunctions.",
      "The discussion also delves into the drawbacks of software updates, the importance of user consent, and concerns about losing functionality or paying for previously free features."
    ],
    "points": 214,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1704043251
  },
  {
    "id": 38823673,
    "title": "Introducing Bun: A Fast JavaScript Runtime with Tail Call Optimization (TCO)",
    "originLink": "https://www.onsclom.net/posts/javascript-tco",
    "originBody": "Bun, Javascript, and TCO Bun is a JavaScript runtime that just released version 1.0! Now you have three choices for running JavaScript outside of the browser: Node, Deno, and Bun. One of Bun’s selling points is speed! It makes some interesting decisions to acheive this. For one, Bun is programmed using Zig. This results in an exciting universe: Node is made with C++, Deno is made with Rust, and Bun is made with Zig. Isn’t this an exciting battle of system languages?! We will actually focus on something else however. Node and Deno are built on V8, while Bun is built on JavaScriptCore. You might know V8 as the JavaScript engine of Chrome. JavaScriptCore is the engine for Safari. They have lots of interesting differences, but we will be focusing on a niche optimization which JavaScriptCore implements and V8 does not: Tail Call Optimization. Let’s dive in by writing some real code! Imagine you need to implement the following function: /* Returns an array of numbers counting from 1 to amount. Examples: count(3) => [1, 2, 3] count(5) => [1, 2, 3, 4, 5] count(-1) => [] */ function count(amount: number): number[]; Give it a try yourself if you want! I imagine most people will come up with a solution like: function count(amount: number): number[] {let nums: number[] = [];for (let i = 1; i(amount > 0 ? [...count(amount - 1), amount] : []); Its a succinct solution! It might look familiar to recurrence relations from math class. You might be thinking, “It looks like loops can be expressed more elegantly with recursion!” But, now I have something sad to share. Try doing count(100000) (Deno and Bun allow running TypeScript directly). You will get the error Maximum call stack size exceeded. Recursion takes up precious memory on the call stack! There may be commands to increase the call stack size for your program, but there is only so much the OS will allow. Memory on the heap is much less restricted. How can we use recursion without fear of exceeding the call stack? The answer: hope your JavaScript engine implements TCO and write your recursion in a way that can be optimized! The process of rewriting a function to be tail call optmized generally involves moving state to arguments. The recursive call needs to be the last thing in the function’s AST. The TCO version of our recursive function looks like: const count = (amount: number, cur: number[] = []) =>cur.length >= amount ? cur : count(amount, [...cur, cur.length + 1]); Its slightly less succint and elegant, but it can be tail call optimized now! If we run count(100000) with Deno, we still get error: Uncaught RangeError: Maximum call stack size exceeded. With Bun, the program now successfully runs! But there’s still one more problem… This solution is really slow. count(100000) with this TCO solution takes 7 seconds with bun. The original for loop solution takes .01s. How can we get similar performance to the for loop solution while still using recursion? We use mutation: function count(amount: number, cur: number[] = []) {if (cur.length >= amount) return cur;cur.push(cur.length + 1);return count(amount, cur); } This function is starting to look a lot like the orignal for loop solution. It’s not very succinct or elegant anymore. But, it runs count(100000) at .01s as well. Nice! The minimalist part of me really enjoys TCO. It enables a language to express complex and efficient programs without imperative loops. In the case of JavaScript, it means a smaller subset of the language can express all programs. Most beginners are taught loop statements as if they are fundamental or required in every language. But that’s not true. With TCO, you can express any loop statement using recursion and get similar performance. Languages that rely on recursion like LISPs often specify that TCO must be implemented in their language spec. Sadly, TCO is only implemented in JavaScriptCore. Thankfully, Bun and Safari use JavaScriptCore!",
    "commentLink": "https://news.ycombinator.com/item?id=38823673",
    "commentBody": "Bun, JavaScript, and TCOHacker NewspastloginBun, JavaScript, and TCO (onsclom.net) 163 points by tosh 21 hours ago| hidepastfavorite141 comments reactordev 11 hours agoNot to be a dog in the fight but…How is: const count = (amount: number) => (amount > 0 ? [...count(amount - 1), amount] : []);Succinct and better? It’s fewer lines. It uses recursion. But it’s obtuse and illegible syntax diarrhea that takes more time to grok.The for loop example above, while simple, is easily understood. A junior engineer could fix it. Is it faster? No. Is it easier to understand? Yes. I like the final example as well.Look, I’m all for being clever when cleverness is needed but some of you JS&#x2F;TS folks take it to the extreme. I want to be able to read my codebase, not perform archaeological studies. I have the same issue with Rust syntax as well in places.I will praise the article on this though, optimizations do matter and having something execute orders faster will improve your overall experience. It doesn’t have to be cuneiform. If you must be clever, wrap it in an abstraction that is well documented because you don’t live forever and neither will your code, make it easy(ier) on the next person. While you can nest a recursive function in a tertiary statement coro, don’t. Fire is cool but also burns.Btw, bun is blazingly fast. I look forward to more. reply kannanvijayan 9 hours agoparentI’m surprised no one has brought up the fact that this code has quadratic time complexity when the underlying algorithm could be linear.And the fact that it’s allocation behaviour is horrible, leaving behind ‘count-1’ garbage objects, count-squared garbage memory, and forcing the GC to chase after the algorithm.I can accept this example as a contrived scenario to talk about TCO… but even in the presence of TCO one would not want to implement the function this way. reply reactordev 7 hours agorootparentI wasn’t even going to dive into the allocations and garbage this style brings but it’s orders of magnitude slower, with more garbage created, disposed of, than a simple if statement. I’m glad you opened the can of worms. There are times when this doesn’t matter. Reading a config json, or performing your startup bootstrap, but if this was executed during a request or during a frame then you’re going to have a bad time. reply djur 8 hours agorootparentprevThe article doesn&#x27;t explain why (and it should) but it does mention that this version is very slow and offers a version that doesn&#x27;t have the same problems. reply nsonha 2 hours agorootparentprevlooks linear to me? Takes n calls, each is O(1).> the fact that it’s allocation behaviour is horrible, leaving behind ‘count-1’ garbage objectsTopic is TCO, which is kinda about that. reply mhink 11 hours agoparentprev> Look, I’m all for being clever when cleverness is needed but some of you JS&#x2F;TS folks take it to the extreme.To be fair, this is very much a toy example meant to demonstrate tail recursion- although I do agree it&#x27;s not a great example considering you need to understand the evaluation order of ternaries in order to understand why one version is tail-recursive and the other isn&#x27;t.That being said: generally speaking, one might argue that using recursion at all is \"too clever\" and algorithms should just stick to iteration altogether. ¯\\_(ツ)_&#x2F;¯> While you can nest a recursive function in a tertiary statement coro, don’t.Man, why do you gotta call me out like this? :D reply akoboldfrying 7 hours agorootparent>algorithms should just stick to iteration altogetherWell, if your algorithm performs 2 or more recursive calls per function invocation (like, say, quicksort), at least one of them needs to be an actual recursive call that uses additional stack space, since TCO is only able to mitigate a single recursive call per function invocation. (You could simulate your own stack and use iteration... but the only reason to do so in practice would be to escape some artificial restriction on native stack size.) reply reactordev 7 hours agorootparentprev>”Man, why do you gotta call me out like this?”Because you lose the stack trace most of the time. Only God and you know where the Uncaught PromiseRejection happened. It’s just really hard to debug when things go south. This drives me bonkers with the AWS sdk. reply satvikpendem 10 hours agoparentprevIf one is well-versed in functional programming, they would understand the above function just fine, plus it comes with all the benefits of functional versus imperative styles, which I will not repeat here as there are lots of articles on this. Perhaps JS devs should spend more time in functional codebases rather than imperative ones and they&#x27;ll also get used to such code, and I for one am glad to see TCO making this style more efficient compared to traditional for-loops. reply djur 8 hours agorootparentThis style isn&#x27;t efficient in JS, though, because it produces a ton of garbage to collect. TCO doesn&#x27;t really make it more efficient, just possible. reply jeswin 6 hours agorootparentIf we&#x27;re commenting on style, we should exclude corner cases. 10k recursive calls are relatively rarer, and could be outside the style preference. reply imbnwa 7 hours agorootparentprevThis, you need a compiler to solve this unless you plan on doing it by hand reply cowsandmilk 7 hours agorootparentprev> Perhaps JS devs should spend more time in functional codebases rather than imperative onesthe pure functional version takes 700x longer to run than the imperative version; maybe the wins of functional programming just aren&#x27;t there in JS engines today. reply reactordev 7 hours agorootparentprevIf one was well-versed in memory management and how the stack would blow up in production one would not have to repeat them here.Also, if you have to justify cleverness, it’s not clever. Don’t defend the style by grandstanding on “functional is better” or “you don’t know func bro”, how about ask why I feel the imperative style is more legible? Why would this not be the best way to write this? Where can I write this way? Or even “I find this style to be legible to me, but I’m curious about why you think it’s not”. reply satvikpendem 7 hours agorootparentIt&#x27;s not \"clever\" though, it&#x27;s simply how functional programming languages like OCaml and Haskell work. You think imperative is more legible because you learned primarily from imperative languages, I&#x27;m assuming. If you had learned functional programming from the start of your programming life, then you&#x27;d find it perfectly legible. reply reactordev 4 hours agorootparentYou don’t know me or what I’ve learned or know so why presume? The clever bit is the recursive nature of it in addition that it’s functional. I’m pretty sure monads have existed longer than JavaScript and I’m perfectly comfortable reading functional code. The code in the example hides several onion layers of inefficiencies I think need&#x2F;should&#x2F;deserve to be called out. I’m not saying everything should be a class man. I’m saying not everything should be a clever one liner. reply mitt_romney_12 7 hours agoparentprevIf you replace the ternary with an if then it&#x27;s easy to understand for anyone who knows what ... means (which imo every JS developer should know) const count = (amount: number) => { if (amount > 0) { return [...count(amount - 1), amount] } else { return [] }; } reply reactordev 7 hours agorootparentYou missed the point entirely I’m afraid. It’s not about replacing the if with …, it’s about complexity. reply femiagbabiaka 6 hours agoparentprevThe ubiquitousness of c-style makes it more familiar. But that doesn&#x27;t make it better syntactically. And of course what&#x27;s faster or more memory efficient is an implementation detail ultimately, but moving away from the imperative style opens up the opportunity for new and interesting -- and potentially more grokkable, ultimately -- patterns. Rust is actually a great example of this, with things like match and pattern matching.Don&#x27;t be such a language grinch. reply reactordev 4 hours agorootparentTalk to me about lifetimes of Boxed RC Mutex then… because last time I checked, I had to do some insane amount ofto get a concurrent collection synchronized. I’m all for functional. Just don’t over use it in places where a simple function would suffice and try not to use it when dealing with recursive allocations of temporary arrays. Rust has come a long way but I feel like some of their style choices were to feel superior rather than make something functional. Go is a great example of simple, functional, but not quite as robust - as Rust. reply Jarred 18 hours agoprev(I work on Bun)A neat thing about tail calls in JavaScriptCore: there’s an explicit bytecode intrinsic for it. return @tailCallForwardArguments(MyConstructor, this);To use this in Bun, you’d have to start Bun with the environment variable “BUN_JSC_useDollarVM=1” and then $vm.createBuiltin(mySourceCodeString)When using this intrinsic, if any of the arguments are incorrect or it cannot otherwise enable it, the entire process will probably crash. In debug builds of JSC it will have a nicer assertion failure but that is not enabled in release buildsExample code: https:&#x2F;&#x2F;github.com&#x2F;WebKit&#x2F;WebKit&#x2F;blob&#x2F;17351231b4dedb62d81721...also happy to answer any questions about Bun reply CodeGroyper 18 hours agoparentLove bun, thank you so much Mr. Sumner.What is the priority of app router integration for NextJS? reply Jarred 9 hours agorootparentAfter Windows support is further along, but not that long from now reply johnfn 13 hours agorootparentprev> What is the priority of app router integration for NextJS?I&#x27;m not Jarred, but what do you mean by this? I&#x27;m using bun + app router + NextJS and everything seems to work just fine. reply CodeGroyper 11 hours agorootparentOn https:&#x2F;&#x2F;bun.sh&#x2F;guides&#x2F;nextjs it says that bun currently does not implement the API to run the dev server. reply atmin 13 hours agorootparentprevNot OP, but interested: can you deploy on a vps having only bun and not node? reply maxboone 12 hours agorootparentYes reply reducesuffering 11 hours agorootparentI don&#x27;t think you&#x27;re talking about what GP comment is.For clarification, Bun can act as a package manager like npm (probably the case of \"bun + app router + NextJS\" comment), and it can act as a replacement to a node runtime a la Express on a VPS (I&#x27;m assuming this is what you mean by \"yes\"). But Bun currently does not implement enough of the Node API&#x27;s to use Bun as the runtime for a Next.js app.From Bun themselves: \"The Next.js App Router currently relies on Node.js APIs that Bun does not yet implement. The guide below uses Bun to initialize a project and install dependencies, but it uses Node.js to run the dev server.\"https:&#x2F;&#x2F;bun.sh&#x2F;guides&#x2F;ecosystem&#x2F;nextjs replyleptons 9 hours agoparentprevWhen do you see AWS Lambda supporting Bun, if ever? reply Jarred 9 hours agorootparentYou can use it today with containers in Lambda or a custom Lambda runtime, but both approaches have embarrassingly slow cold start. Lambda has special optimizations for Node which are not possible without official support.I have no idea when Lambda would add an official runtime layer. But if you work at Lambda and are interested, feel free to email me - jarred@bun.sh reply MrJohz 20 hours agoprevI think this article actually does a really good job of explaining why TCO hasn&#x27;t really been an important part of the JS ecosystem.There&#x27;s a lot of situations where recursive algorithms are really neat and clear. I don&#x27;t know if this is the best example, but it shows the benefit of being able to split logic into a base case and a recurrence case.But, in my experience, an algorithm that elegantly fits a recursive relationship is rarely one that naturally fits the tail-call paradigm. Often part of the benefit of recursion is that you can store state in the stack - the very thing you need to avoid to use TCO.This means you often need to put in effort to create a tail recursive algorithm. But that often ends up looking a lot like the imperative case anyway - an accumulator outside the loop that you either mutate manually, or update in a tail call. And in my experience, the mutating, imperative version is usually then the easier to read and write (assuming you can keep mutations to a given scope, and not have that state leak all over the place). (In fairness, this might be more familiarity, though.)In the light of this, what is the advantage of TCO? In functional languages without mutation, it&#x27;s pretty important to allow for functions to act on arbitrarily-sized inputs without constantly growing the stack. But if we have mutation, it&#x27;s really just a different way of writing the same code. And if that different way is generally less clear and almost always less performant, it probably isn&#x27;t a very useful choice.Which is why I think TCO hasn&#x27;t really caught on in the other JS engines. It&#x27;s a cool idea, and there&#x27;s definitely a handful of cases where it&#x27;s the useful way to go, but usually you&#x27;ll be better served by writing things in the more traditional JS way. reply tikhonj 12 hours agoparentTail calls also matter for code in continuation-passing style (CPS).This matters for some styles of programming (callbacks&#x2F;promises&#x2F;monads&#x2F;etc), but it&#x27;s also very useful for code emitted by compilers and DSLs. It&#x27;s much easier to encode complex control flow with CPS than just with the basic constructs JavaScript supports.Even for hand-written code it can be pretty important. It&#x27;s easy to imagine rewriting simple tail-recursive functions in an imperative style, but it quickly gets much harder, especially if your problem naturally decomposes into mutually recursive functions: that is, instead of a function calling itself, you have multiple functions calling each other, often with a bunch of additional computation between each call. I&#x27;ve run into this when writing parsers as well as custom algorithms for constraint-satisfaction kinds of problems. (Which are pretty common!) reply tonyg 9 hours agoparentprev> an algorithm that elegantly fits a recursive relationship is rarely one that naturally fits the tail-call paradigm.That&#x27;s because tail calls are structured gotos (with arguments). They&#x27;re not really about recursion or even iteration.> what is the advantage of [proper tail calls]?There&#x27;s a loss of expressiveness [1] without them, similar to the loss of expressiveness when there&#x27;s no garbage collector.> if [using proper tail calls] is almost always less performant, it probably isn&#x27;t a very useful choice.Don&#x27;t assume PTC is less performant. There&#x27;s no intrinsic reason for it to be. After all, the machine should be doing (slightly) less work: a goto instead of a call. Retrofitting it to an existing system can of course lead to a poor implementation, but it is not inherently a slow construct.[1] Felleisen, Matthias. “On the Expressive Power of Programming Languages.” Science of Computer Programming, vol. 17, no. 1--3, 1991, pp. 35–75, https:&#x2F;&#x2F;doi.org&#x2F;10.1016&#x2F;0167-6423(91)90036-W . reply MrJohz 1 hour agorootparentTo be clear, I&#x27;m not talking about PTC&#x2F;TCO in general in this comment, I&#x27;m talking about it specifically in the context of Javascript, and in the context of the common patterns of Javascript usage.That is, PTC doesn&#x27;t just need to be powerful than other forms of call, it needs to be more useful day-to-day than other ways of writing the same logic. I think this article shows that, at least with the common recursive examples, that&#x27;s rarely the case - it is pretty much always clearer and significantly quicker to use loops and Array methods than it is to rely on PTC and recursion.You make a good point that there are uses for PTC outside of recursion, and that there are things you can do with it that can&#x27;t be done with existing constructs. I&#x27;m sure that&#x27;s true, but in my experience, the things people want to do with these tools tend not to fit well with the other features of Javascript, such as the high degree of mutation, and the dynamic nature of the language. At which point, it&#x27;s probably more useful to create a compile-to-Javascript language instead that uses these sorts of features internally.But with the rise of WASM (which has PTC as a proposal that I believe some groups have already implemented), it makes a lot more sense to use WASM as a target instead. There, you have a lot more control over exactly what gets executed and how, along with generally smaller and faster code. The exception right now is obviously DOM manipulation, but with the GC work being done right now, the path is opening up to getting easier access to the DOM directly in WASM. reply pjmlp 20 hours agoparentprevRegardless, it has been approved as part of JavaScript specification and Chrome folks refuse for political reasons to implement it, which given the market domination everyone has helped them achieve is a bummer. reply MrJohz 19 hours agorootparentFirefox have also chosen not to implement it, right? Indeed, my impression was that Firefox were far more against the whole thing than Chrome, who did implement TCO but then removed it for various reasons.My impression is that this is less an issue with any particular implementor, and more with the feature not being fully thought through at the beginning. A lot of the browsers (including the Edge team at the time) ended up running into issues, on top of the UX&#x2F;explanatory problems. That&#x27;s why there was the alternative proposal for explicit tail calls, but those didn&#x27;t go anywhere. reply pjmlp 16 hours agorootparentFirefox no longer matters, hence why I left it out. reply ncruces 14 hours agorootparent1. Complain about market dominance.2. Any 2% of the market doesn&#x27;t matter.3. … reply pjmlp 2 hours agorootparentYes, those 2% only came to be because of folks pushing Chrome and Electron all over the place.Now it is too late to save Firefox. reply hajile 13 hours agorootparentprevThey said it was too hard, but now they’re adding it because of wasm, so their objection doesn’t matter. reply kannanvijayan 8 hours agorootparentI left the spidermonkey team a while back but was there when this issue came up and I was strongly against implementing support for it. For me it wasn’t about it being “too hard” (implementation difficulty wouldn’t have been that bad actually).It was more that it forced the implementation to elide stack frames whenever calls occurred in tail position.That’s a semantic change to program behaviour. And one that messed with the expectations of developers.I would have been fine with some explicit syntax for invoking that behaviour, but it seems the standards process that got it approved never really consulted developers or implementors.Do you know why the standard committee rejected adding explicit syntax for tail returns and instead went the route of declaring that all calls in tail position must lead to stack elision? I never really bothered to find out.Also is spidermonkey now adding TCO to JS because of wasm, or are they adding TCO to wasm? reply pjmlp 2 hours agorootparentMost likely because FP languages that have as part of the language specification don&#x27;t have special syntax for it, it just works, and the debuggers are able to provide a good debugging experience.The only FP languages that require explicit syntax, target the JVM, which lacks the support. reply kannanvijayan 1 hour agorootparentIt wouldn&#x27;t just work for javascript and the way it&#x27;s used, though. It modifies semantics in a way that would affect the behaviour of existing programs in production environments. It would break things that currently work, and it would break tooling that currently works.Common tooling for production error tracking (e.g. Sentry) would be greatly affected by such a behavioural change.Functional languages \"just work\" with those semantics because those expectations were set up ahead of time, and tooling and developer expectations were molded around them. But JS isn&#x27;t one of those languages. The tooling and developer expectations, and gargantuan amounts of existing code, has been written with the expectation that calls create stack frames.> The only FP languages that require explicit syntax, target the JVM, which lacks the support.Support could be added to the JVM just the same as the JSVM though - via a spec change. And just as with the JSVMs, it would be a semantic change that would alter the behaviour of existing programs in ways that would break existing tooling and make developers lives worse. It&#x27;s why both of them resist it. replytrashburger 14 hours agorootparentprevSpiderMonkey is still used in over 2% of the browser market share, and is used in non-Firefox applications too. I&#x27;d say it matters. reply pjmlp 2 hours agorootparentSadly it doesn&#x27;t, in many places being so low means the project acceptance matrix for target browsers no longer needs to contemplate Firefox. reply tomashubelbauer 19 hours agorootparentprevI find it surprising that the JS specification also includes optimizations an engine should implement. I&#x27;m not sure how difficult TCO is to implement, but I just checked if QuickJS supports it or not and it seems to be one of the few ES2020 omissions the author chose to make: https:&#x2F;&#x2F;bellard.org&#x2F;quickjs&#x2F;quickjs.pdf (3.1.1). This tells me it might be non-trivial to implement which if true makes the decision to make it a part of the specification all the more surprising, because that means the specification restricts the types of trade-offs implementers can make when attempting to achieve full compatibility. In other words, until today I was under the impression you could make a fully ES compatible engine, but choose to make it slow for implementation ease. Looks like the spec defines the floor for how (non)-performant the engine implementation can be in some cases. Is this common in other languages&#x27; specs?Edit: Oh, seeing the sibling comment, are TCO and \"tail calls\" different things? If so, I remain unclear on the status of TCO support in QuickJS. reply mananaysiempre 18 hours agorootparentThere’s a bit of a naming confusion around tail calls, but in any case “proper tail calls”, let’s call them that, are not precisely an optimization: they are a guarantee that any number of recursive calls of a certain kind will result in constant and not linear memory consumption. This permits some kinds of programming that can otherwise be quite awkward. If your program takes advantage of them (as e.g. often happens in Scheme), having tail calls is not an optimization issue, it’s an implementation correctness issue.Now tail calls are quite annoying to implement in C on top of a compiler that doesn’t (and in fact I don’t think any mainstream platform has a C ABI that would allow a C compiler to natively support them between functions of arbitrary types). That has always been a (solvable) issue for Scheme-to-C translators, and it was probably a consideration for QuickJS. Chrome’s V8, though, is so far away from interpreting things in C or even translating them to C that I expect that any difficulties the developers have are of a completely different nature.(As an example, LuaJIT does support tail calls but kind of sucks at inferring hot loops written using them, so has really impressively draconian limits on how many you can have before the JIT aborts. They otherwise work, though, as is required for a valid Lua implementation.) reply MrJohz 18 hours agorootparentprevTCO isn&#x27;t really an optimisation per se, at least in the sense that optimisations typically aren&#x27;t observable as part of the semantics of a language. TCO does affect the semantics; it says that a recursive function that recurses only using tail calls will never overflow the call stack. That&#x27;s why it&#x27;s something that might get specified in the specification rather than just left as an implementation detail: it&#x27;s the sort of implementation detail that developers might actually rely on.While in practice it probably improves the performance to have tail calls, theoretically, the spec isn&#x27;t making a point about performance here, only semantics. It would, for example, be allowed to have a spec-compliant JS implementation that handled tail calls very slowly, as long as they are correctly don&#x27;t increase the stack.As to why various implementations don&#x27;t do TCO, my impression is that it&#x27;s a combination of complexity and usability issues that have kept it from being implemented. And if some of the major browsers won&#x27;t implement it, then I can see why other, smaller implementations don&#x27;t see it as a priority. reply EE84M3i 13 hours agorootparentThere&#x27;s another consideration, you can introspect the stack using Error.stack (also Function.caller and friends?). It requires a spec change to say \"it&#x27;s okay to not not includes these stack frames in specific cases\". This information would also presumably not show up in the devtools &#x2F; debugger or sentry, etc.I personally more agree with not implementing it implicitly (although I think there was some discussion about having a special syntax) just because of the potential for confusion during debugging. E.g. if all tail calls are elided, then you could have stack frames that don&#x27;t \"make sense\" (how did function A call function C? well, through function B but that frame is gone, and this could be really difficult in deep stacks). Alternatively, you could use a heuristic but that just raises even more questions. reply hajile 13 hours agorootparentThis solvable enough by activating a shadow stack. You can also just note on the trace pretty cheaply that X function was called by a tail call function. This eliminates any ambiguity about what happened.It’s not a different issue from async functions not having a stack trace and JS devs survived for decades without async traces just fine. reply bsdpufferfish 8 hours agorootparentprev> I find it surprising that the JS specification also includes optimizations an engine should implement.Algorithmic complexity is an essential part of defining an interface. reply basil-rash 18 hours agorootparentprevScheme (what JavaScript was based on) also requires tail call optimizations as part of its spec. reply spankalee 14 hours agorootparentprevWhat \"political\" reasons?My understanding of the situation is that it&#x27;s entirely technical issues that came up after implementing. reply magicalist 13 hours agorootparentYes, what&#x27;s really important here is that TCO was added to the spec before the modern TC39 process was adopted. Had implementation and usage experience been required before it shipped in the spec, this conversation would have been concluded 9 years ago. reply hajile 13 hours agorootparentprevThere were no technical arguments from Google. They implemented tail calls and performance was fine. Furthermore, wasm required them to add the functionality back to the jit.MS complained that windows APIs made it slow, but first, they should fix their OS. Second, chrome didn’t have those same issues on windows. Third, they now use chrome, so it’s no longer an objection.Firefox complained that it was work, but they’re also doing it for wasm.This only leaves the stack trace argument, but the biggest cases for tail calls are where you’d already be forced to a loop and nobody complains that loops don’t have stack traces.Further, you already have the same issues with async stack traces and activating shadow stacks in debug mode works just as well as those async traces.It’s all about chrome devs digging in their heels because they took a side dumb argument and won’t just swallow their pride and implement what developers want and the spec demands. reply chrismorgan 7 hours agorootparentAnything to do with tail calls in WASM is likely to be completely irrelevant to proper tail calls in ECMAScript: they’ll be separate pieces of code with different reasons and balances.Moreover, tail call elimination is explicit in WASM. The initially-defined call and call_indirect instructions aren’t allowed to do tail call elimination, and the tail-call proposal added return_call or return_call_indirect which require it. The situation is thus more like the (now inactive) TC39 Syntactic Tail Calls proposal. No need to worry about the debugging or performance implications.> Firefox complained that it was work, but they’re also doing it for wasm.They said it was impossible to implement across realms with their membrane-based security model. That is: “we could do it in most cases, but you’ve said we have to do it in all cases and we can’t do that without replacing a fundamental and pervasive part of our engine, which we certainly don’t want to do for something we and others aren’t convinced is even a good idea”. But in WASM, any tail calls are to the same WASM program, where this isn’t a problem. reply hajile 7 hours agorootparentWASM runs in v8 on Chrome. If you have to implement the bytecode for one, it should work for both. Chrome used to have semantic tail calls for JS implemented too.FF runs WASM on Spidermonkey. If the security model is broken by JS, there&#x27;s not a great reason to believe it won&#x27;t also be broken for the same JIT when running WASM. reply Rapzid 13 hours agorootparentprev> Firefox complained that it was workLOL, as they do. reply runarberg 14 hours agorootparentprevI think the most cited reason they give is diminished developer experience. That is when developers write a recursive function without intending it to be tail call optimized, they loose their call stack. In theory this makes debugging harder. However I don’t—and others interested in JavaScript design—don’t buy this reasoning. There is little—if any—data showing this happens to developers in real life.I also believe that Google’s representatives at TC-39 (and Mozilla’s to a lesser extent) knows this. Meaning they have other reasons for not implementing TCO. My personal theory is that they are very much against expending the functional paradigm in JavaScript. TC-39 has been very hostile towards any proposals which would expand the functional paradigm in JavaScript. The decision to advance the hack style pipeline operator (which uses a placeholder) over the F# (functional&#x2F;tacit) operator is a prime example of this.This, I believe, is purely political. reply sroussey 10 hours agorootparentHaving worked on debuggers in JS land I can tell you that it would generate lots of bug reports.Does the developer expect a call stack?Does the developer NOT expect a call stack? Maybe they want to see if the optimization was activated.Bug reports either way.That said, all sorts of weird changes happen when a debugger is enabled.The jit will eliminate various pieces of code and remove variables from scopes, but you better put them back if the debugger stops! This one I remember in Firebug. Oops!¯\\_(ツ)_&#x2F;¯ reply odyssey7 13 hours agorootparentprevSadly I sense something amuck against the functional paradigm as well.Paul Graham wrote back in 2001 that Lisp was a secret weapon for building a startup due to the developer productivity gains. https:&#x2F;&#x2F;paulgraham.com&#x2F;avg.htmlAs a corollary, the more lisp-like JavaScript is, the more it reduces barriers to entry for launching a tech company, promoting more software engineers into potential new competitors.Could it be that Java and Python are taught in universities because some members of the industry prefer making it more cumbersome for new grads to launch a company? Having significant barriers to entry is business strategy 101. reply jart 12 hours agorootparentAre you saying that knowing LISP makes you like an instant Paul Graham? If only. reply klyrs 9 hours agorootparentEw, I hope not. I know lisp, I do not want to be Paul Graham. reply odyssey7 12 hours agorootparentprevNice try replyakoboldfrying 5 hours agoparentprev>I think this article actually does a really good job of explaining why TCO hasn&#x27;t really been an important part of the JS ecosystem.Nicely put. TCO is a bit like a guitar solo: more fun to implement than it is useful&#x2F;enjoyable to the intended recipients. reply odyssey7 18 hours agoparentprevI don’t think the example is particularly good. TCO is essential in functional programming, where recursion shines, but the example given is an impure function which mutates its input parameter by calling Array.prototype.push. Basically, this isn’t a demonstration of the paradigm for which TCO is required.We’ll never know how big of a role TCO could have played in JavaScript over the past several years, because it hasn’t been available outside of some narrow contexts. reply MrJohz 17 hours agorootparentI agree that TCO is essential in functional languages where loops and mutability are disallowed or at least heavily discouraged, but Javascript is not one of those languages. In Javascript, both mutation and and loops are very simple, and as I think the article demonstrates, often the clearest ways to express an idea.Where recursion is clearer (for example, when dealing with recursively nested data structures), the clearest expression of a function usually isn&#x27;t tail recursive - that often requires rewriting the function to put more state into the function arguments.This means that tail recursion rarely matches with patterns used regularly in Javascript. That&#x27;s not to say it&#x27;s useless - if you want to keep to a particular functional style, or if you&#x27;re compiling to Javascript from a language that emphasises recursion, then it&#x27;s a useful tool to have. But in the former case, you&#x27;re generally going to struggle with browser engines not being optimised for this style, and in the latest case, you&#x27;re probably better off targeting WASM directly. reply odyssey7 16 hours agorootparentI agree with everything you wrote.There’s an element in this that gets at why V8’s omission of TCO feels to me like suppression of functional programming. JavaScript is an incredible widely used multi-paradigm language, to the point that I would recommend for universities to replace Python and Java with JavaScript in their curriculums. It’s just so practical for building software systems, has amazing tooling, connects you to a rich community, and allows you to cover so much academic ground. JavaScript has been a rocket fuel for promoting a vibrant, diverse software engineering ecosystem.Although you can choose to write a loop and do things imperatively in JavaScript, you don’t have to. The ECMAScript spec is inclusive toward the functional programming paradigm by requiring that engines provide for proper tail calls. Functional programmers should be allowed to have their cake and eat it too! But TCO isn’t implemented in V8, and the workaround is to just switch from functional to imperative programming. reply MrJohz 15 hours agorootparentI don&#x27;t think it&#x27;s the lack of TCO that&#x27;s the thing holding JS back from getting the next great functional language. There&#x27;s no structural pattern matching, there&#x27;s no immutable data structures, there&#x27;s nothing in the way of convenient syntax sugar like currying.You can mimic all of that, but it&#x27;s painful, it&#x27;s not something that Javascript is that great at. Which is fair enough, it doesn&#x27;t need to be perfect at everything, that&#x27;s why there&#x27;s different languages. If you want functional programming in the browser, try using Elm or ReasonML. Both of those are fairly easy to use, I think there are even online playgrounds for both to get started. reply hajile 12 hours agorootparentTC39 hates FP.They have a stage 2 record&#x2F;tuple proposal. Records are far more optimizable than JS classes since they enforce concrete types and can’t add&#x2F;remove fields.Instead, they spent massive amounts of time on private fields which most devs didn’t want (most pushback any JS feature has ever received by a large margin) AND one that completely breaks proxies which are used by all kinds of libraries.Even Java is getting pattern matching before JS.They’ve decided pipe operators are either going to be an abomination or they will be cancelled entirely. So of this completely disregards the opinions and use cases of the FO devs that will be using them.This applies to all the FP proposals on their list. They get delayed, canceled, and neutered while niche or even bad features get implemented instead. reply wk_end 9 hours agorootparentIt’s a shame, because records and tuples are enormously helpful when trying to optimize React, given how it uses reference equality on objects to bust memoization. reply odyssey7 13 hours agorootparentprevI think it’s not as widely recognized as it could be just how forward-looking ECMAScript was in introducing language updates in the mid 2010s. For instance, arrow functions make it natural to work with partial application, a currying-adjacent technique. const times = a => b => a * b const double = times(2) const twiceFour = double(4)As a language feature, a function to curry a given function is different from proper tail calls in that it can be implemented as a JavaScript function rather than needing to be built into the JavaScript engine. Similarly, Haskell’s own curry function is imported from its standard prelude. Leaving this detail up to language users allows a diversity of approaches and vibrancy in the community. Lodash has had an implementation for a while: https:&#x2F;&#x2F;github.com&#x2F;lodash&#x2F;lodash&#x2F;blob&#x2F;0843bd46ef805dd03c0c8d.... (Edit, found better example.)Structural pattern matching can be very elegant and it would be nice if ECMAScript offered it. Object destructuring has been added to the language, and that feature could be seen as a stepping stone for adding structural pattern matching in a later version. Proper tail calls however are more fundamental to the paradigm of functional programming, since they are necessary for iterative algorithms to work as well functionally as they do imperatively. reply MrJohz 9 hours agorootparentIn Haskell, all functions are syntactically and semantically curried. That is, a function that takes two arguments is essentially identical to a function that takes one argument, and returns a new function that returns a second argument. (In fairness, part of this is the laziness of Haskell, and a language like OCaml which also uses currying behaves slightly differently, but still similarly enough.) The `curry` function is not a library implementation of currying, it&#x27;s a way of unpacking tuples into arguments.In Javascript, a function that accepts two arguments is meaningfully different from a function that accepts one argument and returns a new function that accepts a second argument. Both semantically, but more importantly syntactically. It&#x27;s possible to overcome this at the library level as you say, but it usually has performance and usability issues - error messages start looking very funky! It also often doesn&#x27;t play well with other Javascript features, such as the famous `map(parseInt)` issue.That&#x27;s not to say that currying isn&#x27;t possible in Javascript, just that it&#x27;s a feature that doesn&#x27;t play well with the rest of Javascript. If you really want currying when you&#x27;re coding, you&#x27;re probably better off choosing another language - currying will work so much better, and you will have so many more advantages.What I&#x27;ve said is specific to currying, but in my experience, this is generally true. You can use certain functional idioms in Javascript (particularly higher order functions), but it isn&#x27;t really a functional programming language. Adding features to make Javascript more functional makes the language a lot more complex, but it won&#x27;t magically turn Javascript into a good functional language. Therefore, it doesn&#x27;t make sense to include things in Javascript just because they&#x27;re useful in a functional context, it&#x27;s first important to validate that these features are useful in a standard Javascript context. reply hajile 7 hours agorootparentJS works alright with currying, but I think manual currying is the best way to do it with the language (and it&#x27;s not that inconvenient). reply Rapzid 12 hours agorootparentprevShout out to F# in the browser via Fable too. reply miloandmilk 11 hours agorootparentI was looking for an alternative to rescript and literally just started looking at Fable this week - and absolutely love it in conjunction with Elmish. replyo11c 13 hours agorootparentprevThe real problem, of course, is that in the cases where compiler-based TCO is possible, the user can trivially refactor the function to use a loop. reply tonyg 10 hours agorootparentThis is not true.The compiler can trivially turn this into a jump, but the developer doesn&#x27;t have enough context to construct any kind of loop: function ap(f, v) { return f(v); } reply pcl 20 hours agoprev> Recursion takes up precious memory on the call stack! There may be commands to increase the call stack size for your program, but there is only so much the OS will allow. Memory on the heap is much less restricted.This reasoning implies that tail call optimization is some sort of memory-segmentation workaround. This isn&#x27;t accurate.A (non-TCO) recursive approach is more expensive because it uses O(n) bookkeeping memory, whereas the iterative approach uses O(1) memory. Naive recursion will allocate a stack frame for each \"iteration\", whereas the loop approach will not allocate anything per iteration.So, the iterative approach simply uses much less memory for large iteration counts; the placement of the memory in stack space or heap space isn&#x27;t the relevant factor. reply fyrn_ 14 hours agoparentBut without TCO, even if your function doesn&#x27;t have any state of it&#x27;s own, it will use memory on the call stack. I don&#x27;t think anything about what they said implied memory segmentation was the problem. It&#x27;s pretty clear they mean each recusion increases the size of the call stack without TCO. reply rezonant 12 hours agorootparentAgreed, I think the mention of heap space was more to answer the question of someone new to this topic of \"but wait, I can allocate gigabytes of data in an array, why can&#x27;t I allocate 100,000 stack frames?\" reply chrismorgan 18 hours agoprevThis is not actually about Tail Call Optimisation, which is more flexible and optional matter of optimisation, but about Proper Tail Calls, which are a guarantee made in the ECMAScript 6 specification (over implementer concerns objections)—in strict mode, calls in tail position must not create additional stack frames. This is the last piece of ECMAScript 6 that most engines haven’t implemented, because it’s rather controversial: it actually causes some performance problems, and makes debugging harder, and may have security issues (in 2016, Mozilla declared it impossible to implement across realm boundaries due to their security model).https:&#x2F;&#x2F;github.com&#x2F;tc39&#x2F;proposal-ptc-syntax has a lot of useful information about it all, and a proposal to make it explicit in syntax, such as with `return continue …`, though I don’t believe that’s really gone anywhere. The practical situation is that this is the one part of the ECMAScript spec that most implementers ignore, and thus which you can’t depend on. Which says to me that it should either be made optional or be removed from the spec. Not sure if there are any other features similarly disregarded. ECMAScript specification policy was different in those days, they operated more independently of implementers. (HTML was like that once too—that’s roughly why WHATWG was formed, because the actual implementers weren’t happy with how things worked in W3C, so they took matters into their own hands.)(Fun terminology problems here. The term TCO is commonly used for PTC, and PTC is very close to being a subset of TCO, but the mandatory stack frame elision which ruins debugging feels to me like it falls outside of TCO. In various situations, debuggers will mark things like “stack frame omitted” when they’ve optimised one out of existence, but you can generally compile things differently, or something like that, to prevent this. (Compiled&#x2F;dynamic language variation showing up here.) But with PTC, it feels like the engine is kinda not even allowed to know that a stack frames may be absent. So I say PTC and TCO are a little distinct, though PTC is mostly just a subset of TCO. Reminds me of the terminology of tree-shaking versus dead code removal—where the former is essentially a subset of the latter, but that the effects are just slightly different, though I’d say it’s more slight in that case than this.) reply imjonse 19 hours agoprevRelated: Guido van Rossum&#x27;s argument for why there is no TCO in Python.https:&#x2F;&#x2F;neopythonic.blogspot.com&#x2F;2009&#x2F;04&#x2F;final-words-on-tail... reply sbarre 20 hours agoprevTCO in this article = Tail Call Optimization, not Total Cost of Ownership.It makes the article more interesting! reply smarx007 20 hours agoparentActually, as a chiefly Java&#x2F;C# person, I was hoping for the article to cover how Bun helps lower the Total Cost of Ownership of Javascript projects by helping to get off the hamster wheel of infinitely deep npm dependency trees, wild west of constant project deprecations and major version releases with immediate deprecation of previous versions, daily game of CVE alert whack-a-mole etc. Preferably, short of writing all the code yourself. reply sbarre 19 hours agorootparentI also assumed the other TCO when I clicked through, which would have been an interesting take to read for sure (particularly since Bun is so new, so I was wondering how much data would be backing this analysis), but I&#x27;m not a strong functional programmer so any time someone covers functional-adjacent topics, it&#x27;s interesting to me!I&#x27;m with you on wanting to learn and explore more about Bun&#x27;s main advantages though, the bundled tooling and some of the convenience APIs feel like well-planned time&#x2F;effort&#x2F;maintenance optimizations. reply gerikson 20 hours agoparentprevIt&#x27;s also not Tjänstemännens Centralorganisation, the largest white collar union in Sweden, and also the org behind the TCO labelling of computer monitors. reply sbarre 19 hours agorootparentIf you google TCO, at least where I am (North America), \"Total Cost of Ownership\" is the first result - and the dominant one on the first page of results.It&#x27;s also the main definition of TCO that most people are probably familiar with, me included.This felt like a reasonable clarification to make. reply davidy123 10 hours agorootparentI guess for anyone interested in Bun and Javascript, it would be more obvious.User what does \"TCO\" mean in \" Bun, JavaScript, and TCO\"ChatGPT \"TCO\" in \"Bun, JavaScript, and TCO\" refers to \"Tail Call Optimization.\" reply imjonse 20 hours agoprevIt&#x27;s nice, but the downside is that relying on TCO can make your Bun-tested code unexpectedly break with stack overflow error if it ever gets run on Deno or Node. So probably a good idea to comment this part of the code and make sure if it&#x27;s in a library to similarly warn the callers somehow. reply quickthrower2 20 hours agoparentYes the O is more than an Optimisation, it is a change in semantics. reply bryancoxwell 20 hours agoparentprevDoes serverside JS have anything similar to #ifdef? reply vlovich123 20 hours agorootparentYes. You can define a constant (eg IS_SERVER) which your bundler (eg esbuild) replaces with a specific value depending on what environment you’re targeting. There are also ways to detect it using runtime globals that are only defined within the browser although that can be monkeypatched by your own code or 3p libraries you import (not common but could be a compat layer you add to make server side JS behave more like a browser). reply antipurist 20 hours agorootparentprevSomething like if (&#x27;Bun&#x27; in globalThis) ... if (&#x27;Deno&#x27; in globalThis) ... if (&#x27;window&#x27; in globalThis) ...could be used to determine the runtime. reply timw4mail 19 hours agorootparentThat certainly works for Bun and Deno, not sure about what is effective in Node, though. reply basil-rash 18 hours agorootparentChecking for process.versions.{node,bun} works, not sure if Deno has an equivalent. replymidtake 20 hours agoprev> [The tail call optimized] function is starting to look a lot like the orignal for loop solution. It’s not very succinct or elegant anymore.(Brackets mine.)I don&#x27;t know about succinct but I definitely like how it looks a lot more. It&#x27;s very readable, while the ternary operator one looks illegible at that length. It looks shoehorned in. It&#x27;s the programming language equivalent of trying to mumble a whole sentence in two syllables. reply k__ 20 hours agoparentYes, every language should have if-expressions. Don&#x27;t know what they were thinking with the ternary operator. reply JonChesterfield 20 hours agorootparentOne reason I&#x27;ve heard for the statement vs expression distinction, where one deliberately makes a mess of the syntax uniformity, is that it helps diagnostics of programming errors.That is, by demanding spurious restrictions on syntax and generally making code more annoying to write, you get to detect more constructs as probably bad.Line noise - semicolons in C, colons in python - is the same idea. Earlier diagnostics of code that probably wasn&#x27;t intended.I don&#x27;t find this remotely compelling as a language design choice but it&#x27;s the best justification I&#x27;ve come across for why so many have the extra obfuscation between programmer and AST.Specifically, why would one optimise for ease of reading invalid code instead of for ease of reading valid code. Madness. But popular. reply recursive 14 hours agorootparentprevHow are if-expressions different from the conditional operator? I thought one was a concrete implementation of the other. reply djur 8 hours agorootparent\"if\" comes before the condition, \"?\" comes after. For a lot of people this seems to significantly improve readability. reply nsonha 18 hours agoparentprevternary is not the important part here, could write it with if and early return too. reply qudat 8 hours agoprevWhile working on effection@v3 (https:&#x2F;&#x2F;github.com&#x2F;thefrontside&#x2F;effection) we spent a bunch of time ensuring that our delimited continuations could handle deep recursive call stacks in Deno.PR: https:&#x2F;&#x2F;github.com&#x2F;thefrontside&#x2F;continuation&#x2F;pull&#x2F;11TCO would have definitely simplified this issue.What’s worse is hitting maximum memory callstack exception is very tricky to catch and is not reliable across runtimes. So when a user hits it it can be tricky to track down. reply JonChesterfield 20 hours agoprevTCO should be table stakes. An interpreter can do it by noticing that the next thing is a call to itself (the interpreter), a compiler by reorganising the stack frame before the jump.Lots of stuff gets in the way. Destructors, varying type signatures, calling conventions. But for the case where it can happen, it should be considered an implementation error that it does not. Very like a space leak.edit: this ^ was evidently unclear - tail call support in an interpreter means recognising when the interpreter is about to call the interpreter and jumping there, e.g. in an eval-apply setup, you use one frame for the eval-apply-eval-apply skeleton and only spawn more for evaluation of function arguments.Tail call support in an compiler involves changing the calling convention to clean up before jump. At that point it&#x27;s all jumps, whether back to the top of a loop or to some other basic block, because all the world is SSA to a pretty good approximation.Neither of those makes any meaningful distinction between calls-to-self, calls-to-sibling, calls-to-indirect and so forth. The TCO is easier on self calls idea comes from languages where it is hacked before the compiler backend in a setting where goto isn&#x27;t allowed to cross function boundaries. Some C code will have a label at the top named \"self\" or similar and use `goto self;` to force the equivalent of a tail call, some compiler stacks implement TCO by a mechanical version of the same hack and thus can&#x27;t deal with it crossing between different functions.edit2: If you&#x27;re compiling to a target with restrictions on calling convention that preclude cleanup before jump (and doesn&#x27;t sort this out itself) you are out of luck. Pick a better target or live in the doomed world you&#x27;ve created. See e.g. https:&#x2F;&#x2F;v8.dev&#x2F;blog&#x2F;wasm-tail-call for some stuff on wasm trying to fix itself, I haven&#x27;t kept up to date with whether they did or not. reply vitus 19 hours agoparentTCO is about more than self-recursive calls -- you should be able to handle mutual recursion: f = x => x > 0 ? g(x - 1) : 0; g = x => x > 0 ? f(x - 1) : 0;(These functions don&#x27;t do anything other than stress-test the stack, then return 0.)Note that if I changed this to, say, 1 + g(x - 1), this would no longer fall under the purview of tail recursion, where the last instruction before the return is a recursive call.The usual workaround I learned in my SICP days was to add a second variable for accumulating any results. And, indeed, that&#x27;s how the blog post rewrites the recursive function.(The recursive one-liner is still wildly inefficient because JS doesn&#x27;t implement arrays as linked lists, so the spread operator results in a copy for each recursive call, resulting in a quadratic solution. It could be made more efficient by using Array.prototype.push, but then it&#x27;s no longer a pure function.) reply runarberg 13 hours agorootparentI don’t think they picked the best example to showcase this, most JavaScript developers would write this simply as: Array.from({ length: n }, (_, i) => i + 1)But this example does get the point across, so I think it is fine. I do agree though, while reading it the inefficientness was screaming at me. reply djur 7 hours agoparentprevImplicit tail-call elimination outside the context of tail-recursive algorithms causes behavior that is surprising to programmers working in languages where stack traces are used for debugging (i.e. it removes frames from the stack and obscures the path of execution). reply anonymoushn 19 hours agoparentprevTCO is broader than tail calls to the same function, but tail calls to the same function would certainly be a start. reply leeoniya 20 hours agoprevthe reason TCO is not implemented in V8, iirc, is because of developer experience and needing devtools to faithfully dump the call stack that hasnt been flattened, because that&#x27;s what&#x27;s expected of js DX.does JSC&#x27;s implementation solve this? do they exhaust the stack only when devtools are open, or smth? maybe they just keep the last 10 frames in memory for devtools? (that would make most sense to me) reply deredede 7 hours agoparentSee [1] (Filip&#x27;s commit message introducing ShadowChicken is also quite well written if a bit more technical [2]).[1] https:&#x2F;&#x2F;webkit.org&#x2F;blog&#x2F;6240&#x2F;ecmascript-6-proper-tail-calls-...[2] https:&#x2F;&#x2F;bugs.webkit.org&#x2F;attachment.cgi?id=274533&action=revi... reply hajile 12 hours agoparentprevNobody cares that loops don’t have stack frames. Few people cared for decades that async functions lost stack traces.It’s just a bad excuse. reply anonymoushn 19 hours agoparentprevThis is a silly justification I think, in the sense that V8 could implement this in a way that can provide stack traces for all stacks that are short enough for a human to browse while also executing programs that use tail calls as goto correctly. reply pjmlp 20 hours agoparentprevThis is a solved problem on programming languages where it is part of the specification, like Scheme.Basically the debugging info and what actually happens isn&#x27;t 1:1, rather the logical model of the execution. reply twic 19 hours agorootparentWhat is the solution? reply pjmlp 16 hours agorootparentThe debugger has metadata that allows faking the execution flow as if the frames haven&#x27;t been erased, and how deep it happens to be, naturally only when active. reply koito17 11 hours agoprev> Languages that rely on recursion like LISPs often specify that TCO must be implemented in their language spec.This is inaccurate. As far as I can tell, it&#x27;s only Scheme specifications like r5rs that have a hard requirement on tail-call optimization. The Common Lisp specification makes no mention of tail-call optimization, and Clojure -- arguably the most widely used Lisp today -- does not have it either (but it has special forms and macros that let you write recursive code and produce trampolines, thunks, etc. to avoid blowing up the stack).With that said, at least in the case of Common Lisp, it is true that many compilers will implement TCO, even though it is *not* required at all to conform to the specification. reply brabel 20 hours agoprevThe performance difference is not just about TCO, but about creating a new array on each iteration instead of reusing the old one... in Lisp, because it uses cons cells which share structure, appending is very efficient, so the recursive code runs as fast as it can get... not so much with JS arrays. reply ColonelPhantom 10 hours agoparentIsn&#x27;t prepending the efficient operation? Appending would require mutation or copying, prepending only requires a single new cons cell.Appending to a linked list is fine if you can mutate and keep a pointer to the tail, but then again, extending mutable arrays is also amortized O(1) assuming a reasonable allocation scheme. reply brabel 1 hour agorootparentThe common pattern in LISP is to prepend using cons and then reverse the list at the end if order matters.Check this program as an example: http:&#x2F;&#x2F;www.norvig.com&#x2F;java-lisp.htmlRecursion is done with `(cons (nth-digit digits start) words)` and recursion ends with `(format t \"~a:~{ ~a~}~%\" num (reverse words))`. reply odyssey7 17 hours agoparentprevECMAScript continues to evolve and I suspect that there would have been an effort to make the spread operator into a sort of cons in JavaScript. It could have worked for both arrays and objects. However, after seeing that the entry-level bar of proper tail calls couldn’t make it out the door for Chrome and V8, they might have decided not to push it and risk further divergence from a major implementation. reply pjmlp 20 hours agoparentprevEven in Lisp, if performance matters there are other data structures.We are decades away when lists were the only option. reply brabel 18 hours agorootparentIn the example in this blog post, I am quite certain plain LISP lists would still be the right choice today. reply kosolam 19 hours agoprevThis isn’t a good thing if your fancy recursion code works great on bun but is slow in other js runtimes. reply anonymoushn 19 hours agoparentSeems fine if you actually run it on bun. reply 65 14 hours agoprevSo the optimization here is that instead of creating a new array every time you call the count function recursively, you&#x27;re only modifying a single array. Thus the new array recursive function is significantly slower because we&#x27;re storing all those arrays in memory when calling the count function recursively. Do I have that right? reply cj 10 hours agoprevBased on the title, I was hoping for a TCO (Total Cost of Ownership) analysis on Bun&#x2F;Javascript.Or the cost of running a company’s architecture on a “new” (not really new, but quickly evolving re: frameworks) JavaScript. reply snissn 13 hours agoprevI gave bun a test with react apps and found that it installs node_modules much much faster, but starting up a react dev server took the same time reply jart 12 hours agoprevTCO is cool. I remember how enlightening it felt when I studied Scheme and came to understand how it&#x27;s implemented there. reply deredede 7 hours agoparentIf this is not what you&#x27;re referring to, you might be interested in CHICKEN Scheme&#x27;s implementation, which is unlike any other I&#x27;ve seen: instead of compiling tail calls to jumps, they are just regular calls, and the call stack is effectively garbage collected when it would overflow. reply jtlasdfjwfasdf 2 hours agoparentprevAgreed. Scheme taught me that function invocations didn&#x27;t have to be sullied with the risk of stackoverflow, and in-fact what appeared to be recursive was actually iteration and loop constructs were not needed. reply mwkaufma 12 hours agoprevOptimization and functional recursion is nice and all, but the unsung killer-feature of (mandatory) TCO is to (ab)use the stack as a state-machine. Each \"state\" is a function which tail-calls the next state, so it replaces the stack frame. Stack-traces now double as state-inspectors. Substates naturally follow from subroutines.I regularly use this in Lua gameplay scripting, which has proper tail-calls in the language spec, not just as an implementation-optional detail. reply quickthrower2 20 hours agoprevIs it just me but I never use recursion in JS (or imperative languages generally) and it never really comes up as a need? reply anonymoushn 19 hours agoparentFor example if you write a json deserializer that&#x27;s generic over the struct you want to deserialize to (or one in JS that returns a JS object), you&#x27;ll usually use some sort of recursion. My generic radix sort[0] also has recursion in it, which seems like the most straightforward way to write it, but of course any recursive function could be mechanically converted to one that does not use recursion.[0]: https:&#x2F;&#x2F;github.com&#x2F;alichraghi&#x2F;zort&#x2F;blob&#x2F;main&#x2F;src&#x2F;radix.zig#L... reply jiehong 20 hours agoparentprevWe’re using recursion at $company to validate user inputs because it’s a user-defined tree of unknown depth. reply odyssey7 16 hours agoparentprevNitpick: JS is multi-paradigm. reply quickthrower2 12 hours agorootparentYeah I should have been more specific. I mean languages where recursion is idiomatic for many problems because the language is purely functional like Haskell. reply magic_man 18 hours agoprevYou can use a loop and a stack and can get recursion without adding all the overhead of a function call. reply refulgentis 18 hours agoprev [–] I spent the first year or two of my career thinking I needed to Do Recursion because thats what the textbooks said and hey, the language supports tail calls!I learned, in the sense you learn by touching a hot stove, that:A) you can&#x27;t guarantee the compiler sees it as a tail call and optimizes it.B) you are now open to a stack overflow appearing in your software at any given point due to compiler changes &#x2F; your own changes that confuse the compiler, etc.C) qed, don&#x27;t do tail calls. reply kriiuuu 14 hours agoparentBoth Scala3 and Kotlin allow you to annotate functions with @tailrec which will give you a compiler warning if the compiler can’t optimize. And if you use the xfatal-warn compiler flag the Scala code won’t compile if your function is not tail call. reply timw4mail 13 hours agoparentprevIterating over tree structures is a pain without recursion, but that&#x27;s the only situation where I feel I need recursion. reply FpUser 14 hours agoparentprev [–] >\"I spent the first year or two of my career thinking I needed to Do Recursion because thats what the textbooks said\"Being an old fart I&#x27;ve learned long ago to not follow self declared prophets. Recursion is a trade off that comes with the cost. Sometimes it makes sense to use it and sometimes (most of the times) it does not. You do what makes you and your customers happy. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bun is a new JavaScript runtime that is known for its speed and is built on JavaScriptCore, unlike Node and Deno which are built on V8.",
      "JavaScriptCore has a niche optimization feature called Tail Call Optimization (TCO), which allows for recursion without exceeding the call stack.",
      "While TCO can be implemented in Bun by rewriting functions and moving state to arguments, the TCO solution in Bun is slower than the original for loop solution. However, using mutation can achieve similar performance in a recursive solution.",
      "TCO enables programmers to express complex and efficient programs without relying on imperative loops."
    ],
    "commentSummary": [
      "The discussion focuses on the use of recursion and tail call optimization in JavaScript and the pros and cons of functional programming.",
      "Commenters discuss the efficiency and readability of different code examples and the implementation of tail call optimization in various programming languages and browsers.",
      "The conversation also mentions alternative languages like Elm and ReasonML, which are better equipped for functional programming."
    ],
    "points": 163,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1704026439
  },
  {
    "id": 38823304,
    "title": "Rediscovered Carolina African Runner Peanut Brings Excitement to Farmers and Chefs",
    "originLink": "https://nationalpeanutboard.org/news/a-1690s-peanut-is-reborn/",
    "originBody": "HomeA 1690s Peanut Is Reborn A 1690s Peanut Is Reborn Aug 30, 2023 The Carolina African Runner peanut re-emerges, giving farmers and chefs a new culinary adventure. At Clemson University’s Coastal Research and Education Lab, horticulturist Dr. Brian Ward stands for hours at a time carefully hand sorting, shelling and cleaning a small, distinctive and somewhat celebrated peanut known as the Carolina African Runner peanut. And like most rare finds, it has a story behind it. “The Carolina African peanut is so tiny that when it runs through a shelling machine, only about 70% will make it into a pile of good seed, then the sheller has to throw the rest out,” Ward said. Horticulturist Dr. Brian Ward, Clemson University This spring, Ward is bent on safeguarding as many good seeds as possible to provide farmers with enough seeds to sustain the heirloom peanut. So, he’s hand-shelling that last 30%. “It takes about six of us cleaning and separating to get about 20 extra pounds of seed in a day,” Ward said. The necessary tedium to ensure the peanuts are edible helps explain one reason food historians, chefs and artisans assumed this peanut was extinct. Once Upon a Time Brought to the port of Charleston in 1690 by African slaves, the re-emerged Carolina African Runner peanut is known as the first peanut planted in the American colonies. It is denser, sweeter, smaller and had a higher oil content than the peanuts we eat today. Originally, its culinary uses were influenced by West African practices: the peanuts were boiled, ground into meal, candied, mashed or made into a hot beverage, roasted and pressed into oil. In the 1800s, taste buds in the northern cities preferred roasted peanuts, and street and ballpark vendors popularized the larger Virginia snack peanuts. The public quickly deemed Virginias as America’s “eating” peanut. When the boll weevil threatened to wipe out the South’s cotton crop in the 1910s, and cottonseed was unavailable for cottonseed oil production, the Carolina African peanut saw a brief resurgence because of its high oil content. But by the 1930s, the peanut’s small size created additional work at harvest and field workers complained. It was vulnerable to certain diseases in the field, which made it unreliable for growers who needed to produce larger crops. Historians believe the last commercial crops of Carolina African peanuts date from the late 1920s, and it was believed to be extinct by the end of the 1950s. Whether by fate or foresight, a mere 40 Carolina African Runner Peanut seeds were carefully tucked away in cold storage by plant breeders at North Carolina State University (NCSU). A Quest Begins Some 60 years later, food historian Dr. David Shields of the Carolina Gold Rice Foundation, along with Southern food artisans, chefs and financial partners, began a quest to recreate a historic Southern menu. Shields searched through colonial ship manifests and early papers to find proof that in 1690 peanut seeds arrived on our shores. Five years later, a phone call to NCSU’s breeding facility produced the ancestral peanut. “They gave me half of the entire collection,” Ward said. “They just told me it was ‘special seed’ and to take good care of it. It wasn’t until harvest that I learned these were the original Carolina African Runner peanuts everybody thought were extinct.\" In 2013, Ward planted the seeds next to his field lab to watch them closely, and twelve plants made it. The next season, Ward and a South Carolina farmer Nat Bradford multiplied the seeds to 1,200 plants. Other farmers got involved and the first commercial crop of 15 million peanuts was harvested in 2016. Then, bad weather struck the Southeastern growing regions for two seasons, hitting the heirloom peanut fields hard. Flooding and a GPS error greatly reduced the yield and Ward is growing out a smaller two-acre field -- “not for selling, but just for seed.” The Most “Peanutty” of Peanuts “The best part of this discovery is it has cultural and historical significance,” Ward said. “The flavor is ‘wow,’ ‘bam!’ The aroma is more potent than our traditional peanuts today.” Chef Forrest Parker of the Old Village Post House in Mount Pleasant, South Carolina was the first chef who received the peanuts. He calls them “the most peanutty peanut of peanuts,” Ward said. Nat Bradford, an artisan farmer in Seneca, South Carolina, and was given enough of the revived peanut seeds for three-fourths of an acre. “The story behind it is fascinating to me,” Bradford said, “Once you acquire the seed, it’s precious and you’re a steward of it. You don’t want to store it, but to put it back into use, to reproduce it and discover the market potential.” This year, he’s planting five acres and “making it successful enough to be sustainable and profitable,” Bradford said. “I’ve never had a peanut like it.” Bradford admits the price is high right now ($20 per pound) but he said it’s because “there’s so much work involved in cleaning, sorting and shelling. I’ve spent most of my winter evenings with my kids cleaning, picking out debris, shelling them—it’s a labor of love.” Reawakening an Original Superfood Ward sees market potential for the Carolina African peanut. He’s had dozens of farmers interested in growing it and shipped it to 50 or 60 artisan farmers so far. Several chefs throughout South Carolina and Georgia are experimenting with different culinary uses, such as making peanut hummus and peanut brittle or boiled peanuts. As Bradford said, “The difference is the flavor. I want chefs to find new ways to use them, so more people can experience their flavor. It is uniquely amazing.” One peanut butter company, Good Spread, is interested in the heirloom peanut because it reflects their mission of “fighting global malnutrition and using our footprint to protect and enrich the environment and communities we impact,” said Robbie Vitrano, founder and owner of Good Spread. Each jar the company sells provides one packet of peanut butter-based Ready-to-Use-Therapeutic Food (RUTF) to treat malnourished children worldwide. Plus, Vittrano is working on a peanut butter with the heirloom peanut. But why the Carolina African peanut? “We love the cultural narrative of the artisan peanut, and the opportunity to use agricultural practices that protect and improve the environment,\" Vittrano said. \"Peanuts are an original superfood—we believe one way we’ll reawaken people to that is with products that have both an original story and bring new flavors and tastes to our customers.” That’s why many believe the story of America’s original peanut is just beginning. Our Recent News Everithing about peanut Discover more news Officer Slate for 2024 Announced at December Board Meeting The Board approved a new slate of officers for a one-year term beginning on January 1, 2024. Dec 20, 2023 Why You Don’t Need to Fear Sugar Fear-mongering messages around sugar abound. Truthfully, it is not that simple – cutting out one ing... Nov 28, 2023 Hit Your Health Reset Button for the New Year After indulging in food and drink over the December holidays, the New Year is an opportunity to hit ... Nov 28, 2023 Why Fried Turkey Isn’t a Calorie Bomb While you’ll rarely find a registered dietitian advocating for fried foods, there are actually some ... Nov 21, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38823304",
    "commentBody": "A 1690s Peanut is RebornHacker NewspastloginA 1690s Peanut is Reborn (nationalpeanutboard.org) 158 points by mooreds 22 hours ago| hidepastfavorite68 comments wil421 21 hours agoPeanuts are quite unique in many respects. They peg the soil after flowing above ground. The peg shoots down into the soil and forms the peanut.They are nitrogen fixing like most legumes. There are small nodules their roots that harbor nitrogen producing organisms.A few people in my community garden have grown a few smaller plants. I prefer planting snow peas early to give my summer plants a nitrogen boost. reply doctorhandshake 20 hours agoprevThere are a few stories like this related to revival of the Carolina Rice Kitchen heritage cuisine in the book The Third Plate by chef Dan Barber of Blue Hill at Stone Barns. Anson Mills [1] in particular come to mind in regard to the revival effort - they’re growing and selling Carolina Gold Rice, Graham Flour, Bennie seeds, etc, and I would not be surprised if they get involved in this peanut project as well.1 - https:&#x2F;&#x2F;ansonmills.com&#x2F;what_we_do_pages reply 7thaccount 19 hours agoparentI&#x27;ve bought the rice based on someone touting it on HN. It is good. reply bullfightonmars 15 hours agorootparentI did too! Two years ago there was an article about Carolina Gold and I ordered from Anson Mills. I really enjoyed the aged&#x2F;laurel brown rice. It has a deep fragrant flavor, great texture and was very good in baked rice dishes. reply 7thaccount 13 hours agorootparentGood to hear I wasn&#x27;t the only one lol. I found the Carolina Gold to be extremely rich? It&#x27;s hard to describe high quality rice other than it stood out to me and I&#x27;m a Louisiana man who has eaten a fair share of rice. reply doctorhandshake 11 hours agorootparentprevPerhaps it had been built up by Third Plate and also the expectation that comes with Anson’s rather fussy recommended preparation but I failed to have my doors blown off by it. With that said, I regularly buy in bulk Anson’s rice waffle mix (incredible), graham flour (for biscuits), and oats (the best oatmeal). reply mhb 10 hours agorootparentDo you have a link to the rice waffle mix? I don&#x27;t see it on their site. reply doctorhandshake 9 hours agorootparentSure - site nav definitely needs improvementhttps:&#x2F;&#x2F;ansonmills.com&#x2F;products&#x2F;142 reply doctorhandshake 8 hours agorootparentBTW my recommendation is limited to its use in this recipe, which is the sum of my experience with it https:&#x2F;&#x2F;ansonmills.com&#x2F;recipes&#x2F;413 replydoctorhandshake 14 hours agoparentprevUgh autocorrect - ‘benne’ seeds (sesame) reply helsinkiandrew 21 hours agoprev$3.95 for 25 Seeds: https:&#x2F;&#x2F;sowtrueseed.com&#x2F;collections&#x2F;direct-seed-in-spring&#x2F;pr... reply exhilaration 17 hours agoparentCan peanuts grow pretty much anywhere in the U.S.? I&#x27;m sure a lot of us here have grown \"easy\" plants like tomatoes, are peanuts... easy? reply bjorn2run 17 hours agorootparentThat link says no to anywhere, looks like USDA zone 7 is the minimum for outdoor growing:Peanuts have a long growing season and require 100 to 130 depending on the variety of frost-free days to reach maturity. USDA Zone 7 and above should plan on starting seeds for peanuts indoors 4-5 weeks before the last anticipated frost date. Zones 8 and above can start inside to get a head start, or sow directly into your garden beds after your last frost date. reply hansvm 14 hours agorootparentprevIf you&#x27;re not a bit selective with your technique, region, or breed, tomatoes aren&#x27;t de facto easy. Any winter I&#x27;m not careful enough I lose mine. Other people struggle in dry summers, wet summers, or other adverse conditions.Peanuts are IMO similar. They require warm nights and a long growing season, so any northern or moutainous area will need _some_ extra considerations and can&#x27;t rely on cold-weather breeds to paper over the problem. You might want a much longer stint indoors than other plants before tossing them in your garden. Like tomatoes, they&#x27;re not crazy about excess moisture (and where tomatoes just die when they get fungal infections, there&#x27;s a wide range of conditions where peanuts will still produce but the produce will be toxic). Otherwise they&#x27;re pretty easy. reply pvaldes 16 hours agorootparentprevI assume that this is the direct son of another post a few days ago.> Can peanuts grow pretty much anywhere in the U.S.?desert plants, so most probably not in snow or frost areasThey are much better than strawberries for CA at least, but the winning strategy with water if you are a small farmer is not reducing its consume. This would benefit mainly your competitor companies that can use more water and push you off of the road at mid term. Is similar to the problem of not using a big server after having spent solid money building it. You are not saving money or energy, really, and your part of the cake is just reducedEither everybody saves water at the same time, or is just big fishes trying to convince the small ones to surrender part of its quote so there are more resources for them reply mooreds 21 hours agoprevI first heard about these when reading this book, Endangered Eating: https:&#x2F;&#x2F;wwnorton.com&#x2F;books&#x2F;9781324004660If you would like to learn more about various foods on the edge of existence, check it out (no affiliation, just a happy reader). reply Kon-Peki 14 hours agoprevI did some web searching after reading this but didn’t find an answer to my questions:This particular cultivar was brought from Africa to North America and was considered “extinct” once it was no longer planted in North America. But what about Africa? Is it no longer grown in Africa? Why not? reply xhkkffbf 11 hours agoparentMy understanding is that peanuts originated in either Peru or Brazil. At least that&#x27;s what some claim:https:&#x2F;&#x2F;www.aboutpeanuts.com&#x2F;all-about-peanuts&#x2F;origin-histor... reply edbaskerville 10 hours agorootparentThat doesn&#x27;t preclude this cultivar being African in origin. Could have made it to Africa in the 1500s, undergone selective breeding there, and returned to North American via the slave trade later. (See also: North American potatoes coming from Europe, not directly from the Andes.) reply ipcress_file 17 hours agoprevI&#x27;d like to try one, but I&#x27;d limit my expectations.My wife and I stumbled upon a local group selling heirloom tomato plants a couple of years ago. We grew a few and soon realized why they became heirlooms. The nicest thing that can be said is that they must be an acquired taste. reply salad-tycoon 17 hours agoparentThere isn’t one variety of heirloom. There are many. Personally, I like the purple ones. A deeper, smokier flavor. Also pineapple tomatoes are delicious sweet little treats with a subtle pineapple taste. Bakers creek&#x2F;rare seed has a great catalogue. That said, we’ve had duds. Only have two growing seasons under my belt. reply ipcress_file 17 hours agorootparentDefinitely a fair comment. I didn&#x27;t mean to imply that I&#x27;d attempted to grow all of the heirlooms out there! reply phyzome 17 hours agoparentprevThey&#x27;re heirlooms because they worked really, really well for one family in one location for many years. But if you grow them in different conditions, you may get vary different results. reply sweettea 7 hours agoparentprevTomatoe flavors are (in addition to variety) extremely sensitive to nutrients and growing condition. I&#x27;ve had Barrys Crazy Cherry be one of the best flavors ever one year, rich and lowacid and textured and sweet, and absolute watery cardboard the next year - from the same seed batch on a semipro scale. reply chubot 17 hours agoparentprevHeirlooms are less consistent for sure. They can be more sensitive to their growing conditions too.But IMO commodity tomatoes are some of the \"easiest\" things to improve upon. Regular supermarket tomatoes are often flavorless, the farmer&#x27;s market tomatoes are often better (e.g. in California)So heirlooms have higher highs and lower lowsIMO commodity tomatoes are consistent, but consistently \"meh\" ... reply ipcress_file 17 hours agorootparentMy experience is that contemporary varieties of tomatoes grown at home are much better than those grown by the big producers. I&#x27;m not sure why, but our tomatoes are always red all the way through. The ones from the store are often whiter and coarse-grained inside. Actually, the same thing happens with strawberries. reply JoeAltmaier 17 hours agorootparentConventional wisdom says they&#x27;ve been bred for resiliency, predictable ripening, long lasting. Not flavor.One clue is, the gene that ripens tomatoes is also the one that causes the skin to split. So the seeds get spread. But not useful for commercial use, as split tomatoes are worthless to sell. So they use varieties that don&#x27;t express that gene. So they don&#x27;t ripen much. So they are tasteless, no matter how long on the vine.Definitely grow your own, if you care about tomatoes at all and have the room@! reply throwup238 16 hours agorootparentIt&#x27;s not just conventional wisdom, there have been genetic studies: https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.1222218A mutation of the SlGLK2 gene results in significantly less sugar in the fruit while slowing down ripening for transport. reply Texasian 17 hours agorootparentprevThe tomatoes and strawberries you grow at home don’t need to hold up to long distance shipping. Not to mention, the mass market fruits and veg don’t have somebody personally motivated by getting something at the peak of ripeness.Like most things, it’s a different set of priorities and incentives that drive different results. reply buildsjets 16 hours agorootparentprevCommercially grown tomatoes are harvested while they are hard and green, transported and stored in that state, artfically reddened by exposing to them to ethylene in a gas chamber, then presented for sale in the marketplace as \"ripe.\"Would you like to know how \"Not From Concentrate\" orange juice is manufactured? It may not be concentrated, but it involves many industrial processes and can be up to a year old when you buy it.Fake food. reply MostlyStable 11 hours agorootparentI&#x27;m someone who very much believes in home grown food whenever possible. I have a very large garden (larger than probably 80%+ of the country could even hope to have, thanks to living in the country) and I grow a very large proportion of all the vegetables I eat. So I&#x27;m someone who is sympathetic to your view.But I can&#x27;t disagree strongly enough with calling store-bought and&#x2F;or industrialized food \"fake\".It&#x27;s different. It has different tradeoffs. It&#x27;s worse in some ways (and better in others), but what it most definitely is not is \"fake\". reply chubot 7 hours agorootparentprevI didn&#x27;t know about the tomatoes, but it doesn&#x27;t surprise me anymore.I think I learned about Salmon being artifically pink from Hacker News several years agohttps:&#x2F;&#x2F;qz.com&#x2F;358811&#x2F;heres-why-your-farmed-salmon-has-color...On the bright side, essentially everyone I know seems to like local produce, BEFORE I tell them it&#x27;s local. Their taste buds are still attuned, and can tell the difference reply ipcress_file 8 hours agorootparentprevThank you for that explanation. Now I understand why the home-grown versions of the same tomatoes are so different! replyorenlindsey 18 hours agoprevProbably due to selecting for bigger peanuts, they&#x27;ve lost the flavor they used to have. I hope I can try these someday. reply dashtiarian 18 hours agoparentIf you like smaller peanuts, find an Iranian trail mix shop and ask for &#x27;tiny peanuts from Astaneh-ye Ashrafiyeh&#x27;. They&#x27;ve been selected for small size and taste. reply earthling8118 15 hours agorootparentWhat kind of place do you live where you can find a shop with such a niche on top of a niche? A trail mix shop doesn&#x27;t exist in my city, let alone an Iranian one. reply flymasterv 11 hours agorootparentMaybe try Gorp of Persia, down in the granola district? reply raincom 15 hours agoparentprevIndian grocery stores carry both small peanuts (Indian variety) and large peanuts (American variety). 4 lbs small peanuts cost about $5.99; 4 lbs large ones cost about $6.99 reply mantas 17 hours agoparentprevProbably yes. I got hold of some historical apple varieties. Historical as in 19th century. They’re good. Not as sweet as many today’s apples, but the taste is nice IMO. reply jackfoxy 17 hours agoprevWhat is the take on peanut oil from the seed oil bad crowd? IIRC it is usually missing from both the good oil and the bad oil columns. reply smt88 16 hours agoparentI&#x27;ve seen it in the \"bad oil\" column but never the \"good oil\" column. Apparently it has good fats, but also bad fats.I personally use avocado oil for anything that peanut oil would be useful for (like high-temp cooking). reply xeromal 16 hours agoparentprevPeanut oil is used pretty extensively in GA, the state, and I tend to love the flavor it imparts. I&#x27;m curious how it sits on the healthiness chart. My gut feeling is that it&#x27;s somewhere above canola but below avocado. reply erikpukinskis 9 hours agorootparentCanola oil is one of the healthiest oils there is, assuming you don’t have a dietary sensitivity to bogeymen. reply xeromal 8 hours agorootparentI think it&#x27;s higher polyunsaturated fats are what concerns me the most reply RetroTechie 21 hours agoprev“The Carolina African peanut is so tiny that when it runs through a shelling machine, only about 70% will make it into a pile of good seed, then the sheller has to throw the rest out,” Ward said.(in this case the remainder isn&#x27;t thrown out, but hand shelled).Obviously that shelling machine is designed to process bigger peanuts. Would it be difficult to re-design it (or make adaptable) to handle smaller ones?That would seem inevitable if this peanut becomes more popular. Hand shelling 30% of a peanut harvest, in 2024? Come on... reply joshspankit 19 hours agoparent> Would it be difficult to re-design itThe knee-jerk reaction is to say “no, just make all the parts smaller.”, but at some point the peanut and the shell are the same weight and fragments of shell are the same dimensions as a single peanut. Throw in variation on both sides and it becomes a difficult problem most likely requiring a good optical recognition system. reply tudorw 20 hours agoparentprevIn the UK I quite often see peanuts sold in shells, I am sure there would be a market for these in our artisanal markets, people crave novelty! reply msrenee 14 hours agorootparentWe get them in shells in the US as well. I&#x27;d definitely be interested in buying these still in the shell. reply mkoubaa 18 hours agoparentprevIf there&#x27;s a buyer for the tech, someone will invent it reply sowbug 16 hours agoprevFlooding and a GPS error greatly reduced the yield...That is surprising. How would a GPS error affect crop yield? I know about plant hardiness zones, but that&#x27;s it. reply SAI_Peregrinus 15 hours agoparentProbably a GPS-guided tractor went off into the peanut field when it shouldn&#x27;t have. reply tdrnl 15 hours agorootparentOr a GPS problem could have resulted in misapplication of chemicals -- herbicide sprayed onto desired plants instead of between rows for example reply justinl33 14 hours agorootparentprevhttps:&#x2F;&#x2F;youtu.be&#x2F;g9Slg2s06hw?si=JqqPjWuC46M_lAhl reply dotancohen 12 hours agorootparentPlease don&#x27;t post memes to HN. reply op00to 20 hours agoprevMan I want these peanuts now! reply AndrewKemendo 10 hours agoprevThat’s so cool! I’ve been planning on growing peanuts this season.Maybe I can get some of these! reply causality0 20 hours agoprevBrought to the port of Charleston in 1690 by African slaves, the re-emerged Carolina African Runner peanut is known as the first peanut planted in the American colonies.Fascinating that a plant native to and very widely cultivated in South America made it to the American colonies by way of Africa. Were they not grown by indigenous peoples in the North? reply mytailorisrich 19 hours agoparentAccording to Wikipedia it was available all the way North to at least Mexico since apparently that&#x27;s where Europeans (Spanish conquistadors) encountered it. reply xhkkffbf 11 hours agorootparentOne opinion is that it originated in either Brazil or Peru. Maybe the plants made it to Africa after the Europeans discovered America. Maybe they got to Africa by some other path.https:&#x2F;&#x2F;www.aboutpeanuts.com&#x2F;all-about-peanuts&#x2F;origin-histor... reply Solvency 18 hours agoprevSo would this 1960s peanut be more or less deadly to 2023 people with peanut allergies? reply ksherlock 14 hours agoparentThere are 5 peanut proteins (Ara 1 1&#x2F;2&#x2F;3&#x2F;5&#x2F;8) that are associated with peanut allergies, with Ara h 1 being the most common (~90% of people with peanut allergies are allergic to h 1).This study -- https:&#x2F;&#x2F;onlinelibrary.wiley.com&#x2F;doi&#x2F;abs&#x2F;10.1034&#x2F;j.1398-9995....\"Quantification of major peanut allergens Ara h 1 and Ara h 2 in the peanut varieties Runner, Spanish, Virginia, and Valencia, bred in different parts of the world\"concluded:\"The results suggest that peanuts of different varieties, and from different parts of the world contain similar proteins, including Ara h 1 and Ara h 2. Consequently, the IgE-binding properties are similar to a great extent. This indicates that differences in the serology of peanut allergy may not originate from differences in the allergen composition of the peanut.\"So probably no difference with 1690s either. reply PcChip 17 hours agoparentprev1690s, and I would assume so reply Solvency 16 hours agorootparentAssume more...or less... reply PcChip 8 hours agorootparentSorry, I would assume more so, since they said the flavor was more concentrated and peanutty tasting reply paganel 21 hours agoprev [–] Thought I was on the rs sub for a minute, this forum does indeed need a feel-good mascot like Peanut there is. reply tomrod 20 hours agoparent [–] What is the rs sub? reply kalbadia 19 hours agorootparentI guess it&#x27;s the RuneScape subreddit ? for some reasons it seems that Jaggex, the RS parent company have partenered with a third-party company, Peanuts, that sells things to players. And it&#x27;s quite controversial. reply paganel 18 hours agorootparentprev [–] This [1]. There were a few HN readers there, so, yeah.[1] https:&#x2F;&#x2F;www.reddit.com&#x2F;gallery&#x2F;15skh75 replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Carolina African Runner peanut, previously thought to be extinct, has been rediscovered and is now being cultivated by farmers and chefs.",
      "The peanut, brought to the American colonies in 1690, is smaller, sweeter, and has a higher oil content than modern peanuts.",
      "Researchers obtained seeds from a breeding facility and successfully grew a commercial crop, leading to potential market opportunities for farmers and artisan food companies interested in growing and utilizing the Carolina African peanut."
    ],
    "commentSummary": [
      "The article explores the resurgence of a peanut variety from the 1690s called Carolina Gold Rice heritage cuisine.",
      "It delves into the difficulties faced in growing peanuts in the US and the desire for a solution to handle smaller peanuts.",
      "The variability of flavors in heirloom tomatoes and the preference for locally grown produce are also discussed."
    ],
    "points": 158,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1704022105
  },
  {
    "id": 38823482,
    "title": "Sweden's Spectacular Solar System Model",
    "originLink": "https://en.wikipedia.org/wiki/Sweden_Solar_System",
    "originBody": "Toggle the table of contents Sweden Solar System 16 languages العربية Deutsch Español Français Հայերեն Hrvatski Italiano עברית Македонски Nederlands 日本語 Русский Српски / srpski Svenska Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Wikidata item Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikivoyage From Wikipedia, the free encyclopedia You can help expand this article with text translated from the corresponding article in Swedish. (April 2022) Click [show] for important translation instructions. View a machine-translated version of the Swedish article. Machine translation, like DeepL or Google Translate, is a useful starting point for translations, but translators must revise errors as necessary and confirm that the translation is accurate, rather than simply copy-pasting machine-translated text into the English Wikipedia. Consider adding a topic to this template: there are already 280 articles in the main category, and specifying|topic= will aid in categorization. Do not translate text that appears unreliable or low-quality. If possible, verify the text with references provided in the foreign-language article. You must provide copyright attribution in the edit summary accompanying your translation by providing an interlanguage link to the source of your translation. A model attribution edit summary is Content in this edit is translated from the existing Swedish Wikipedia article at [[:sv:Sweden Solar System]]; see its history for attribution. You should also add the template {{Translated|sv|Sweden Solar System}} to the talk page. For more guidance, see Wikipedia:Translation. Swift-Tuttle Halley Earth, Eros, Saltis, Mars, Sun, Mercury, Venus Jupiter Saturn and 5025 PL Uranus Neptune Pluto and Charon Ixion Eris Sedna Termination Shock class=notpageimage| The Sweden Solar System The Sweden Solar System is the world's largest permanent scale model of the Solar System. The Sun is represented by the Avicii Arena in Stockholm, the second-largest hemispherical building in the world. The inner planets can also be found in Stockholm but the outer planets are situated northward in other cities along the Baltic Sea. The system was started by Nils Brenning, professor at the Royal Institute of Technology in Stockholm, and Gösta Gahm, professor at the Stockholm University.[1][2] The model represents the Solar System on the scale of 1:20 million.[3] The system[edit] Avicii Arena, representing the Sun in the Sweden Solar System The bodies represented in this model include the Sun, the planets (and some of their moons), dwarf planets and many types of small bodies (comets, asteroids, trans-Neptunians, etc.), as well as some abstract concepts (like the Termination Shock zone). Because of the existence of many small bodies in the real Solar System, the model can always be further increased. The Sun is represented by the Avicii Arena (Globen), Stockholm, which is the second-largest hemispherical building in the world, 110 m (360 ft) in diameter. To respect the scale, the globe represents the Sun including its corona. Inner planets[edit] Mercury Model just outside the Stockholm City Museum Mercury (25 cm (9.8 in) in diameter) is placed at Stockholm City Museum, 2,900 m (1.8 mi) from the Globe. The small metallic sphere was built by the artist Peter Varhelyi. Venus (62 cm (24 in) in diameter) is placed at Vetenskapens Hus at KTH (Royal Institute of Technology), 5,500 m (3.4 mi) from the Globe. The previous model, made by the United States artist Daniel Oberti, was inaugurated on 8 June 2004, during a Venus transit and placed at KTH. It fell and shattered around 11 June 2011. Due to construction work at the location of the previous model of Venus it was removed and as of October 2012 cannot be seen. The current model now at Vetenskapens Hus was previously located at the Observatory Museum in Stockholm (now closed). Earth (65 cm (26 in) in diameter) is located at the Swedish Museum of Natural History (Cosmonova), 7,600 m (4.7 mi) from the Globe. Satellite images of the Earth are exhibited beside the Globe. An elaborate model of the Moon (18 cm (7.1 in) in diameter) is also on display, about 20 meters from the model of Earth. Mars (35 cm (14 in) in diameter) is located at Mörby centrum, a shopping centre and Stockholm metro station in Danderyd, a suburb of Stockholm. It is 11.6 km (7.2 mi) from the Globe. The model, made in copper by the Finnish artist Heikki Haapanen, is connected by an \"umbilical cord\" to a steel plate on the floor having an Earth image.[4] The globe also features marks that represent some typical Martian chemical elements. Gas giants[edit] Jupiter (7.3 m (24 ft) in diameter) is placed inside the Clarion Hotel located at Stockholm Arlanda Airport in Sigtuna Municipality, 40 km (25 mi) from the Globe. Previously, it was made as a flower decoration, with different flowers representing different zones of the giant gas planet. Today, the planet is depicted as a ring light above a lobby.[5] Saturn (6.1 m (20 ft) in diameter) is placed outside the old observatory of Anders Celsius, in the so-called Celsius Square, in the centre of Uppsala, 73 km (45 mi) from the Globe. Inaugurated during the International Year of Astronomy,[6] the model is a mat with a picture of Saturn, but will eventually grow to crown a school planetarium in the city. In addition, several schools in Uppsala are to provide moons of Saturn: the first completed was Enceladus (diameter 2.5 cm or 0.98 in) at Kvarngärdesskolan.[7] Uranus (2.6 m (8 ft 6 in) in diameter) was vandalized and the new model was reconstructed behind Stora magasinet in Lövstabruk in 2012. It is an outdoor model made of blue steel bars. The rotation axis of the planet is marked in red.[8] 2.5-m representation of Neptune, by the river Söderhamnsån in Söderhamn Neptune (2.5 m in diameter) is located by the river Söderhamnsån in Söderhamn, a coast town with tradition of fishing and sailing (which relates to Neptune being the deity of the seas). Placed 229 km (142 mi) from the Globe, the model is made of acrylic and, at night, shines with a blue light. Trans-Neptunian objects[edit] Pluto (12 cm (4.7 in) in diameter) and its largest moon Charon are placed near the southern of the Dellen lakes, in Delsbo, 300 km (190 mi) from the Globe. The lakes are thought to be formed by a meteorite impact 90 million years ago. The two bodies' sculptures are supported by two gravelike pillars (as Pluto is the deity for death), made up with dellenite, a rare mineral formed at that place by the meteorite impact. Haumea (8.5cm (3.3 in) in diameter) and its moons are depicted in the 2047 Science Centre, Borlänge, 200 km (124 mi) from the Globe. Quaoar (6cm (2.4 in) in diameter) is located in the library in Gislaved, 340 km from the Globe. Ixion (6.5 cm (2.6 in) in diameter), a dwarf planet candidate, is located at Technichus, a science center in Härnösand, 360 km (224 mi) from the Globe. The sculpture is an orb held by a hand with the arm. This plutino was discovered by a team which included scientists from Uppsala. Makemake (7cm (2.8 in) in diameter) is located at Slottsskogsobservatoriet, an observatory in Gothenburg, 400 km (249 mi) from the Globe. 'Oumuamua (0.3 mm (0.012 in) in diameter) is placed in the village of Plönninge, Halland, 440 km (273 mi) from the Globe. Gonggong (7.5 cm (3.0 in) in diameter) is placed near the Tycho Brahe Observatory in Oxie, Malmö, 500 km (311 mi) from the Globe. Eris (13 cm (5.1 in) in diameter) is located at Umestans Företagspark, Umeå, 518 km (322 mi) from the Globe. Made by Theresa Berg, the golden model is inspired by the mythical story of Eris sparking a quarrel between three Greek goddesses with a golden apple bearing the inscription καλλίστῃ (\"to the most beautiful one\"). Sedna (10 cm (3.9 in) in diameter), another dwarf planet candidate, is located at Teknikens Hus, a science center in Luleå, 734 km (456 mi) from the Globe. Other bodies[edit] The dwarf planet Sedna 471926 Jörmungandr a minor planet in Stockholm. The near-Earth Object Eros is located at Mörbyskolan, a school in Danderyd Municipality (where Mars is located), 11 km (6.8 mi) from the Globe. It was created as a Valentine's Day project in gold, modeled after Eros, the god of love. The dimensions are 2 × 0.7 × 0.7 mm (0.98 mm3 or 6.0×10−5 in3). The asteroid 36614 Saltis is located at Saltsjöbaden's Kunskapsskola, a school near the Stockholm Observatory. The asteroid was discovered by A. Brandeker in 2000, using a telescope at the observatory, and the body was named after the observatory's location, Saltsjöbaden. The asteroid Vesta is located at Åva gymnasium, a public secondary school in Täby. The asteroid 306367 Nut (a.k.a 5025 PL for Palomar-Leiden) (0.2 mm (0.0079 in) in diameter) is located in a park in Knivsta Municipality, 60 km (37 mi) from the Globe. It is not a sculpture but a dot on a map of the System, placed in front of Erik Ståhl's monumental cosmic sculptures. Halley's Comet is located at Balthazar Science Center, in Skövde. Inaugurated on 16 December 2009, there are actually four models of the comet: three placed outdoors, based on schoolchildren's drawings, plus one indoors, consisting of a laser passing through a block of glass. Comet Swift-Tuttle is placed at Kreativum, a science center in Karlshamn. The comet's orbit is closest to the Globe in inner Stockholm and farthest in Karlshamn, 390 km (240 mi) away. The Termination Shock is at the edge of the heliosphere: it is the boundary where the solar wind transitions to subsonic velocity. No sculpture currently represents the termination shock, but a foundation for a future sculpture exists at the Institute of Space Physics, 950 km (590 mi) from the Globe, in Kiruna, above the Arctic Circle. List of objects[edit] Object Distance from Globen[9] Diameter[9] Location[9] Coordinates Inauguration date Sun — 71 m (233 ft), the disk 110 m (361 ft), incl. the corona The Avicii Arena in Stockholm 59°17′36.80″N 18°04′59.65″E 19 February 1989 471926 Jörmungandr 1.8 km 0.05 mm Ion Game Design in Stockholm 59°18′34.7″N 18°04′21.9″E 23 september 2023[10] Mercury 2.9 km (1.8 mi) 25 cm (9.8 in) Stockholm City Museum in Stockholm 59°19′11″N 18°04′16″E 1998 Venus 5.5 km (3.4 mi) 62 cm (24.4 in) Vetenskapens Hus 59°21′10.38″N 18°03′30.78″E 8 June 2004 Earth and Moon 7.6 km (4.7 mi) 65 cm (25.6 in) and 18 cm (7.1 in) Cosmonova Riksmuseet in Stockholm 59°22′08.48″N 18°03′12.34″E before 2000[11] (433) Eros 11 km (6.8 mi) 2.0 mm × 0.7 mm × 0.7 mm Mörbyskolan, a school in Danderyd 59°23′38″N 18°02′41″E(36614) Saltis 11 km (6.8 mi) < 1 mm Kunskapsskolan, a school in Saltsjöbaden 59°16′21″N 18°18′17″E 14 January 2010[12] Mars 11.6 km (7.2 mi) 35 cm (13.8 in) Mörby Centrum in Danderyd 59°23′52.58″N 18°02′11.58″E before 2000[11] 4 Vesta 16.4 km (10.2 mi) 2.6 cm Åva gymnasium in Täby 59°26′24″N 18°03′47.16″E 6 September 2017[13] Jupiter 40 km (25 mi) 7.3 m (24 ft) Arlanda airport in Märsta 59°38′58.52″N 17°55′50.38″E before 2000[11] (306367) Nut (5025 PL) 60 km (37 mi) 0.2 mm in Knivsta 59°45′25″N 17°45′57″ESaturn 73 km (45 mi) 6.1 m (20 ft) Celsius square in Uppsala 59°51′34″N 17°38′14″E 2010 (only Titan) Uranus 125 km (77 mi) 2.6 m (8.5 ft) Stora magasinet in Lövstabruk 60°24′31″N 17°52′37″E 13 October 2012[14] Haumea 200 km 10 cm Borlänge 60°29′18.1″N 15°25′51.5″E1P/Halley comet 204 km (127 mi)Balthazar Science Center in Skövde 58°23′14″N 13°51′11″E 16 December 2009[15] Neptune 229 km (142 mi) 2.5 m (8.2 ft) by the river Söderhamnsån in Söderhamn 61°18′07″N 17°03′19″E 29 October 1998[16] Pluto and Charon 300 km (186 mi) 12 cm (4.7 in) and 6 cm (2.4 in) by the lake Dellen South, in Delsbo 61°47′50.13″N 16°32′59.96″E before 2000[11] 50000 Quaoar 340 km 6 cm Gislaved’s library 57°17′46.9″N 13°31′49.8″E 18 November 2017[17] (28978) Ixion 360 km (224 mi) 6.5 cm (2.6 in) Technichus, a science center in Härnösand 62°37′49″N 17°56′12″E 18 April 2002[18] 174567 Varda 370 km 33 mm Bohusläns museum in Uddevalla 58°20′57.4″N 11°55′44.0″E 4 september 2021[19] 109P/Swift-Tuttle comet 390 km (242 mi) < 1 cm Kreativum, a science center in Karlshamn 56°11′39″N 14°51′09″EMakemake 400 km 7 cm Slottsskogsobservatoriet in Gothenburg 57°41′28.3″N 11°56′36.4″E 23 September 2017[20] ʻOumuamua 440 km 0.3 mm Halmstads 56°44′04.8″N 12°44′42.8″E225088 Gonggong 400 km 7.5 cm Tycho Brahe-observatoriet, Oxie 55°32′33.9″N 13°05′04.0″E 23 September 2017[21] (136199) Eris 510 km (317 mi) 13 cm (5.1 in) Företagspark in Umeå 63°50′05″N 20°15′37″E 6 December 2007[22] (90377) Sedna 810 km (503 mi) 10 cm (3.9 in) Teknikens Hus, a science center in Luleå 65°36′59.50″N 22°08′06.00″E 8 December 2005[23] Termination shock 950 km (590 mi) A plate Institute of Space Physics in Kiruna 67°50′27″N 20°24′34.5″EGallery[edit] The Avicii Arena represents the Sun. The rest of the Solar System is scattered in, and north of, Stockholm. Mercury in Stockholm Venus in Stockholm Earth in Stockholm Mars in Stockholm Uranus in Lövstabruk Neptune in Söderhamn Asteroid Vesta in Täby Sedna in Luleå The foundation for the sculpture of the Termination Shock in front of the Swedish Institute of Space Physics in Kiruna. See also[edit]Solar System portal Nine Views Somerset Space Walk References[edit] ^ \"Sweden Solar System: Bakgrund\" (in Swedish). Sweden Solar System. Retrieved 2009-09-15. ^ \"ContactSweden Solar System\". ^ \"Sweden Solar System: English summary\". Sweden Solar System. Retrieved 2009-09-15. ^ Danderyds Kommun: Mars ^ Karlsson, Lars. \"Sweden Solar System - Jupiter ver. 2\". www.astrofriend.eu. Retrieved 2022-03-25. ^ Press release, linked 2009-06-08. ^ List of moons of Saturn assigned to schools in Uppsala (in Swedish). ^ \"Uranus landade i Lövsta\". 14 October 2012. ^ a b c \"Sweden Solar System: Stationer\" (in Swedish). Sweden Solar System. Retrieved 15 September 2009. ^ Asteroid Jormungandr ^ a b c d \"Tours of Model Solar Systems\". Psych.illinois.edu. Retrieved 4 October 2013. ^ Ny Teknik: Saltis invigs i Saltis Archived 23 June 2010 at the Wayback Machine ^ \"Åva gymnasium\". ^ \"Uranus invigdes i Lövstabruk – Upsala Nya Tidning\". 13 October 2012. ^ \"Sweden Solar System: Halleys komet\". Ttt.astro.su.se. 16 December 2009. Retrieved 4 October 2013. ^ Neptunus i Söderhamn ^ Quaoar ^ \"Technichus' Exhibitions\". Technichus home Page. Archived from the original on 14 August 2010. Retrieved 10 May 2010. ^ Varda ^ Makemake ^ Gonggong ^ Umeå kommun: Umeå får en egen himlakropp Archived 29 September 2011 at the Wayback Machine ^ \"Luleå är Sedna. I alla fall om vår sol motsvaras av Globen i Stockholm\". Norrbotten Kuriren (in swedish). Archived from the original on 15 July 2010. Retrieved 10 May 2010. External links[edit] Wikimedia Commons has media related to Sweden Solar System. Sweden Solar System's webpage All objects on Openstreetmap vte Solar System models DevicesAntikythera mechanismArmillary sphereAstrariumAstronomical clockOrrery Eise Eisinga PlanetariumTellurion ModelsAkaa Solar System Scale Model (Akaa, Finland)Gravity Discovery Centre (Gingin, Australia)Kystagerparken (Hvidovre, Denmark)Monument to the Sun (Zadar, Croatia)Nine Views (Zagreb, Croatia)Pajamäki Solar System Scale Model (Helsinki and Espoo, Finland)Planet Lofoten (Lofoten, Norway)Sagan Planet Walk (Ithaca, New York)Somerset Space Walk (Somerset, England)Sweden Solar System RelatedSolar SystemKirkhill Astronomical PillarHistorical models of the Solar SystemNumerical model of the Solar System Retrieved from \"https://en.wikipedia.org/w/index.php?title=Sweden_Solar_System&oldid=1190751100\" Categories: Scale modeling Space art Science and technology in Sweden Buildings and structures in Sweden Tourist attractions in Sweden Solar System models Hidden categories: CS1 Swedish-language sources (sv) Webarchive template wayback links Articles needing translation from Swedish Wikipedia Articles with short description Short description is different from Wikidata Commons category link is on Wikidata",
    "commentLink": "https://news.ycombinator.com/item?id=38823482",
    "commentBody": "Sweden Solar SystemHacker NewspastloginSweden Solar System (wikipedia.org) 150 points by titaniumtown 21 hours ago| hidepastfavorite43 comments sakjur 19 hours agoWalking from Globen (the sun) to Naturhistoriska (the earth) could be made into a quite nice 10 km walk, with possible detours to see much of the old worker&#x27;s town of Södermalm, the old town, the city center around Klara&#x2F;Norrmalm, the urban residential area Sibirien, the university and finally ending up at the science museum Naturhistoriska.The Stockholm novels by Per Anders Fogelström starting with \"City of My Dreams\" largely plays out across that walk, if you&#x27;re interested in the 19th&#x2F;20th century evolution of the city and its working class.The metro&#x27;s art project is also pretty cool if you take the subway when you&#x27;re in Stockholm, with unique artwork at each station (spanning from a few pieces almost hidden away next to the ad billboards to the whole station decorated to an elaborate theme). reply mongol 20 hours agoprevThe sun is represented by the spherical Avicii Arena, which used to be the largest spherical building in the world. That is, until Las Vegas&#x27; Sphere was built. Now the United States have the opportunity to create an even larger solar system model! reply petersumskas 18 hours agoparentThe Las Vegas sphere is pretty large. I’m not sure how it compares to the dome at Siding Springs but in sure that would be a very large scale model. https:&#x2F;&#x2F;www.visitnsw.com&#x2F;destinations&#x2F;country-nsw&#x2F;warrumbung... reply petepete 21 hours agoprevI do find it frustrating on Wikipedia that when you click on the map to enlarge it the markers vanish.I understand why, but the hyperlink should be to a bigger map viewer, not the bare underlying image. reply formerly_proven 17 hours agoparentI dunno why but WP detests interactive features for some reason and the few that do exist are disabled in the mobile view (e.g. collapsible sections). Another example: all those tables where measurements are spelled out twice, in metric and imperial, wasting a huge amount of screen space and making everything harder to read instead of having a \"[mm] [in]\" switch once per table or page. reply pestatije 20 hours agoparentprevyeah, thats definitely a bug in the specs reply dang 11 hours agoprevRelated:There’s an entire solar system hiding inside Sweden - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36687868 - July 2023 (1 comment)The Sweden Solar System (world’s largest model of our planetary system) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32944427 - Sept 2022 (3 comments)The Sweden Solar System - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26424090 - March 2021 (72 comments)Sweden Solar System Photos - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8181696 - Aug 2014 (2 comments)Sweden Solar System - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8167192 - Aug 2014 (40 comments)The Sweden Solar System - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2220903 - Feb 2011 (26 comments) reply simonjgreen 17 hours agoprevI’m actually quite surprised there isn’t a Tom Scott video about this! I half expected the link to take me to his final video and this be the title. reply Findecanor 20 hours agoprevI live close by \"Earth\" and \"Mars\". \"Earth\" is one of the few pieces made to depict its planet somewhat accurately, with the continents correct and the moon at the correct distance and (almost) correct size. Unfortunately the Natural History museum it is in is closed for renovation so you can&#x27;t go there at the moment.Most of the other pieces, are more art than science. \"Mars\" looks more like an old-fashioned soccer ball than the planet, IMHO. reply jenadine 19 hours agoparentIs it a coincidence that the scaled earth&#x27;s path crosses the Natural History museum? reply robin_reala 19 hours agorootparentPresumably: the Museum was built in 1916, and the Globe in 1989. reply workfromspace 16 hours agoprevThere is a smaller version in Zagreb as well: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Nine_ViewsI was there this summer. It was a fun event trying to find them all. reply resolutebat 10 hours agoparentOne more in Zurich: https:&#x2F;&#x2F;www.zuerich.com&#x2F;en&#x2F;visit&#x2F;sport&#x2F;planet-trail reply aetherson 18 hours agoprevI&#x27;ve been interested in scale models of the solar system (or other astronomical systems like say the moon system of Jupiter) for a while, partly to try to teach my kids stuff and partly just because I think they&#x27;re cool.My opinion is that this solar system model is too big to be useful. If you&#x27;re trying to teach people about the scales of space, I think you need something humanly navigable, ideally something where you can see with a single sight-line back to the previous object in the model, or, failing that, where at least you can walk between elements of the model in a short enough time that you can kind of keep yourself on-task and thinking about the model for the whole walk. reply prewett 13 hours agoparentThere&#x27;s one north of Boston [1] that I accidentally found walking one time. Turns out I remembered it from a HN posting some years ago. I&#x27;d already walked some 3 mi or so by the time I walked across Neptune, so couldn&#x27;t go all the way to Topsfield and back. I thought it gave a really good sense of how big space is. It&#x27;s about 1 mi from Neptune to Uranus and another mile or so to Saturn (as far as I got). On my return I walked directly at a fast walking pace, so it took some 20 minutes to from Saturn to Uranus and another 20 minutes to Neptune. (Distances and times are approximate.) At the time I think I calculated my walking speed to be four times the speed of light. It&#x27;s pretty awesome, in the original sense, that going 4x light speed it still takes a long time to get from one outer planet to the next.Wikipedia has quite a list of other models [2], although it doesn&#x27;t seem to include Topsfield, MA.[1] https:&#x2F;&#x2F;www.wickedlocal.com&#x2F;story&#x2F;chronicle-transcript&#x2F;2021&#x2F;...[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Solar_System_model reply pdobsan 8 hours agoparentprev> If you&#x27;re trying to teach people about the scales of space, I think you need something humanly navigableThe Somerset Space Walk [1] seems to be a very nice candidate for your requirements.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Somerset_Space_Walk reply crazygringo 17 hours agoparentprevI dunno... Earth is 4.7 mi from the sun. That&#x27;s walkable.I like that this actually gives you a sense of real meaningful distance and size. This is a sun that&#x27;s still big compared to humans, and a distance to the earth that isn&#x27;t negligible.And then a real lesson about how insanely larger the solar system is when you try to get out to Jupiter and beyond. reply aetherson 17 hours agorootparentIt&#x27;s walkable, but you can&#x27;t see the \"sun\" from the \"earth,\" and it&#x27;s walkable in well more than an hour, at which point I don&#x27;t think that you&#x27;re really keeping the scale in mind.We can always look at images or whatever if we want to get an intellectual sense of the scale of things. I think the advantage of a model is that you feel immersed in it, and I don&#x27;t think you do at this scale.It&#x27;s challenging! It&#x27;s hard to find a location and a scale which gives you this human sense of the sizes involved. At most scales, you have to make the planets pinpricks in size. But, especially for the inner system, you can make it work.EDIT: Here&#x27;s I think something that works.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=zR3Igc3Rhfg reply RetroTechie 12 hours agorootparentprevFor a better perspective on scale, they should do a model where in 1 place you have the sun, its planets, asteroid belts etc, and (to scale!) in a 2nd place you have a model representing the nearest star system (Alpha Centauri).Oh... and of course put in where Voyager, Pioneer & New Horizons probes are these days - after 40+ years of travel at ~15km&#x2F;s. reply aetherson 12 hours agorootparentI think this is near-impossible. The diameter of earth is about 12,700 km. So if the distance between the Sun and Alpha Centauri were \"the maximum distance available on Earth,\" then the scale is 3.25 billion to 1.The sun&#x27;s radius is about 696,000 km, so 7x10^6 meters, so at 3.25 x 10^9:1 scale, it&#x27;s about 2mm across. The planets would all be specks. You could see orbits, though they&#x27;d be small. And that&#x27;s if you place your alpha centauri model literally \"on the other side of the world\" from your sol model. reply jimbobthrowawy 7 hours agorootparentI think you can do very slightly better by using great circle distances rather than straight lines cutting through the earth. After all, I think that&#x27;s what this model is doing. reply aetherson 6 hours agorootparentSure, it could be half the circumference of the earth instead of the diameter. The circumference is pi times the diameter, so half the circumference is a little over 1.5x the diameter -- the sun could be 3mm across instead of 2mm. replygenerationP 15 hours agoparentprev> humanly navigableThis is Sweden; I would bet that there are hiking trails connecting everything on that map, although maybe not a single one for all the planets. Roslagsleden (13 day hikes) + the first 9 etapper of Vikingaleden bring you from (close to) Mars to Uranus. And these are some nice hikes:https:&#x2F;&#x2F;www.roslagen.se&#x2F;roslagsledenhttps:&#x2F;&#x2F;www.roslagen.se&#x2F;vandra-vikingaleden reply low_tech_love 17 hours agoparentprevIs it an actual linear scale? reply aetherson 17 hours agorootparentThe Sweden one? Seems like yes to me. reply crazygringo 17 hours agoprevSeems like there could&#x2F;should be one for New York -- it could use the spherical Hayden Planetarium [1] as the sun. Which would be a conveniently somewhat smaller scale.(To be fair, it does already have scale models of the planets right next to it, just not at scale distances [2].)[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rose_Center_for_Earth_and_Spac...[2] https:&#x2F;&#x2F;commons.wikimedia.org&#x2F;wiki&#x2F;File:Amnh_fg09.jpg reply anatnom 15 hours agoparentAt scale distances from the 27m sphere planetarium (as the sun), earth would be a 24 cm sphere at distance of 2.8 km. From looking at a map, this could be a basketball somewhere a few blocks NNE of Times Square. (Pluto, however, would be 114km away. Dunno where that would be. Maybe Allentown, PA?) reply adonovan 14 hours agoprevI showed this to my brother, who lives in Stockholm. His response: “Typical bloody Stockholmers, thinking everything revolves around them!” ;-) reply patrickwalton 18 hours agoprevThis is so cool! I&#x27;ve wanted to make something like this for years. I did the math for using Spaceport America as the sun and Pluto would have been as far as Wyoming. reply sponaugle 17 hours agoprevThere is a smaller scale version of this on the CU campus:https:&#x2F;&#x2F;www.colorado.edu&#x2F;fiske&#x2F;colorado-scale-model-solar-sy...It is 1:10 Billion, so about a half-mile from the Sun top Pluto. reply JKCalhoun 14 hours agoparentEugene, Oregon has one as well: https:&#x2F;&#x2F;eugenesciencecenter.org&#x2F;exhibits&#x2F;eugene-solar-system... reply mongol 20 hours agoprevInteresting to see Oumuamua there. But it was my understanding that it was just briefly visiting our solar system and will leave us behind?https:&#x2F;&#x2F;www.swedensolarsystem.se&#x2F;en&#x2F;oumuamua&#x2F; reply Giorgi 18 hours agoparentYes, but whole system looks like it is designed as a tourist attraction anyways. reply beAbU 16 hours agoprevIs this scale model accurate to relative sizes of the bodies, or only distance? That&#x27;s something I&#x27;ve not been able to find out. reply dmurray 15 hours agoparentBoth, but they cheated a bit on the Sun. Per TFA:> To respect the scale, the globe represents the Sun including its corona.That&#x27;s not really a meaningful measurement as the corona could extend anywhere from a few hundred thousand to a few million kilometres. If the Avicii Arena represented a more normal measurement of the sun, the rest of the planets should be ~50% greater in diameter and ~50% further dispersed. Sweden would still have room! reply Gare 16 hours agoparentprevBoth, that&#x27;s the whole point of these models. Also size compared to the distance is true to scale. reply jholzer 20 hours agoprevVery cool to see TNOs on this. I never appreciated how far away Sedna is, TIL it&#x27;s not even close to the Kuiper belt! reply VikingCoder 17 hours agoprevIs the Vegas Sphere the largest hemispherical object? reply mongol 11 hours agoparentYes it is. The sun in this model used to be. reply sph 16 hours agoprevIf my calculations are correct, at that scale of 1:20,000,000 the closest star Proxima Centauri is ~2 million km away, or 5.22 times the distance to the Moon.Space is fucking huge. reply deadbabe 16 hours agoprev [–] I’d love to do a road trip visiting all these planets! When is a good time? reply daneel_w 16 hours agoparent [–] Any. We still have four seasons, sort of, so come on over and enjoy either while you still can. reply tuwtuwtuwtuw 12 hours agorootparent [–] Wouldn&#x27;t recommend roadtrip in December unless you like driving in darkness.The most northern part of this solar system is in kiruna, where next sunrise is 2nd of January and that day sun will be upp less than 30 minutes. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Sweden Solar System is a scale model of the Solar System located in Sweden.",
      "It includes representations of the Sun, planets, moons, dwarf planets, and other celestial bodies.",
      "The model is spread out across different locations in Sweden to depict the scale and distances in the Solar System.",
      "It serves as both an educational tool and a popular tourist attraction."
    ],
    "commentSummary": [
      "The Sweden Solar System is being discussed as a scale model of the solar system in Sweden.",
      "Different models, including a walkable one, are being debated.",
      "Participants share information about similar models worldwide, and their accuracy and usefulness for teaching about space are discussed."
    ],
    "points": 150,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1704024263
  },
  {
    "id": 38826187,
    "title": "Inside AMD's Vintage LANCE Ethernet Chip: Exploring Circuitry and Manufacturing",
    "originLink": "https://www.righto.com/2023/12/amd-lance-ethernet-double-poly.html",
    "originBody": "Ken Shirriff's blog Computer history, restoring vintage computers, IC reverse engineering, and whatever Interesting double-poly latches inside AMD's vintage LANCE Ethernet chip I've studied a lot of chips from the 1970s and 1980s, so I usually know what to expect. But an Ethernet chip from 1982 had something new: a strange layer of yellow wiring on the die. After some study, I learned that the yellow wiring is a second layer of resistive polysilicon, used in the chip's static storage cells and latches. A closeup of the die of the LANCE chip. The metal has been removed to show the layers underneath. The die photo above shows a closeup of a latch circuit, with the diagonal yellow stripe in the middle. For this photo, I removed the chip's metal layer so you can see the underlying circuitry. The bottom layer, silicon, appears gray-purple under the microscope, with the active silicon regions slightly darker and bordered in black. On top of the silicon, the pink regions are polysilicon, a special type of silicon. Polysilicon has a critical role in the chip: when it crosses active silicon, polysilicon forms the gate of a transistor. The circles are contacts between the metal layer and the underlying silicon or polysilicon. So far, the components of the chip match most NMOS chips of that time. But what about the bright yellow line crossing the circuit diagonally? That was new to me. This second layer of polysilicon provides resistance. It crosses over the other layers, connected to the silicon at the ends with a complex ring structure. Why would you want high-resistance wiring in your digital chip? To understand this, let's first look at how a bit can be stored. An efficient way to store a bit is to connect two inverters in a loop, as shown below. Each inverter sends the opposite value to the other inverter, so the circuit will be stable in two states, holding one bit: a 1 or a 0. Two cross-coupled inverters can store either a 0 or a 1 bit. But how do you store a new value into the inverter loop? There are a few techniques. One is to use pass transistors to break the loop, allowing a new value to be stored. In the schematic below, if the hold signal is activated, the transistor turns on, completing the loop. But if hold is dropped and load is activated, a new value can be loaded from the input into the inverter loop. A latch, controlled by pass transistors. An alternative is to use a weak inverter that produces a low-current output. In this case, the input signal can simply overpower the value produced by the inverter, forcing the loop into a new state. The advantage of this circuit is that it eliminates the \"hold\" transistor. However, a weak inverter turns out to be larger than a regular inverter, negating much of the space saving.1 (The Intel 386 processor uses this type of latch.) A latch using a weak inverter. A third alternative, used in the Ethernet chip, is to use a resistor for the feedback, limiting the current.2 As in the previous circuit, the input can overpower the low feedback current. However, this circuit is more compact since it doesn't require a larger inverter. The resistor doesn't require additional space since it can overlap the rest of the circuitry, as shown in the photo at the top of the article. The disadvantage is that manufacturing the die requires additional processing steps to create the resistive polysilicon layer. A latch using a resistor for feedback. In the Ethernet chip, this type of latch is used in many circuits. For example, shift registers are built by connecting latches in sequence, controlled by the clock signals. Latches are also used to create binary counters, with the latch value toggled when the lower bits produce a carry. The SRAM cell It would be overkill to create a separate polysilicon layer just for a few latches. It turns out that the chip was constructed with AMD's \"64K dynamic RAM process\". Dynamic RAM uses tiny capacitors to store data. In the late 1970s, dynamic RAM chips started using a \"double-poly\" process with one layer of polysilicon to form the capacitors and a second layer of polysilicon for transistor gates and wiring (details). The double-poly process was also useful for shrinking the size of static RAM.3 The Ethernet chip contains several blocks of storage buffers for various purposes. These blocks are implemented as static RAM, including a 22×16 block, a 48×9 block, and a 16×7 block. The photo below shows a closeup of some storage cells, showing how they are arranged in a regular grid. The yellow lines of resistive polysilicon are visible in each cell. A block of 28 storage cells in the chip. Some of the second polysilicon layer is damaged. A static RAM storage cell is roughly similar to the latch cell, with two inverters in a loop to store each bit. However, the storage is arranged in a grid: each row corresponds to a particular word, and each column corresponds to the bits in a word. To select a word, a word select line is activated, turning on the pass transistors in that row. Reading and writing the cell is done through a pair of bitlines; each bit has a bitline and a complemented bitline. To read a word, the bits in the word are accessed through the bitlines. To write a word, the new value and its complement are applied to the bitlines, forcing the inverters into the desired state. (The bitlines must be driven with a high-current signal that can overcome the signal from the inverters.) Schematic of one storage cell. The diagram below shows the physical layout of one memory cell, consisting of two resistors and four transistors. The black lines indicate the vertical metal wiring that was removed. The schematic on the right corresponds to the physical arrangement of the circuit. Each inverter is constructed from a transistor and a pull-up resistor, and the inverters are connected into a loop. (The role of these resistors is completely different from the feedback resistors in the latch.) The two transistors at the bottom are the pass transistors that provide access to the cell for reads or writes. One memory cell static memory cell as it appears on the die, along with its schematic. The layout of this storage cell is highly optimized to minimize its area. Note that the yellow resistors take almost no additional area, as they overlap other parts of the cell. If constructed without resistors, each inverter would require an additional transistor, making the cell larger. To summarize, although the double-poly process was introduced for DRAM capacitors, it can also be used for SRAM cell pull-up resistors. Reducing the size of the SRAM cells was probably the motivation to use this process for the Ethernet chip, with the latch feedback resistors a secondary benefit. The Am7990 LANCE Ethernet chip I'll wrap up with some background on the AMD Ethernet chip. Ethernet was invented in 1973 at Xerox PARC and became a standard in 1980. Ethernet was originally implemented with a board full of chips, mostly TTL. By the early 1980s, companies such as Intel, Seeq, and AMD introduced chips to put most of the circuitry onto VLSI chips. These chips reduced the complexity of Ethernet interface hardware, causing the price to drop from $2000 to $1000. The chip that I'm examining is AMD's Am7990 LANCE (Local Area Network Controller for Ethernet). This chip implemented much of the functionality for Ethernet and \"Cheapernet\" (now known as 10BASE2 Ethernet). The chip handles serial/parallel conversion, computing the 32-bit CRC checksum, handling collisions and backoff, and recognizing desired addresses. The chip also provides DMA access for interfacing with a microcomputer. The chip doesn't handle everything, though. It was designed to work with an Am7992 Serial Interface Adapter chip that encodes and decodes the bitstream using Manchester encoding. The third chip was the Am7996 transceiver that handled the low-level signaling and interfacing with the coaxial network cable, as well as detecting collisions if two nodes transmitted at the same time. The LANCE chip is fairly complicated. The die photo below shows the main functional blocks of the chip. The chip is controlled by the large block of microcode ROM in the lower right. The large dark rectangles are storage, implemented with the static RAM cells described above. The chip has 48 pins, connected by tiny bond wires to the square pads around the edges of the die. Main functional blocks of the LANCE chip. Thanks to Robert Garner for providing the AMD LANCE chip and information, thanks to a bunch of people on Twitter for discussion, and thanks to Bob Patel for providing the functional block labeling and other information. For more, follow me on Twitter @kenshirriff or RSS for updates. I'm also on Mastodon occasionally as @kenshirriff@oldbytes.space. Notes and references It may seem contradictory for a weak inverter to be larger than a regular inverter, since you'd expect that the bigger the transistor, the stronger the signal. It turns out, however, that creating a weak signal requires a larger transistor, due to how MOS transistors are constructed. The current from a transistor is proportional to the gate's width divided by the length. Thus, to create a more powerful transistor, you increase the width. But to create a weak transistor, you can't decrease the width because the minimum width is limited by the manufacturing process. Thus, you need to increase the gate's length. The result is that both stronger and weaker transistors are larger than \"normal\" transistors. ↩ You might worry that the feedback resistor will wastefully dissipate power. However, the feedback current is essentially zero because NMOS transistor gates are essentially insulators. Thus, the resistor only needs to pass enough current to charge or discharge the gate. ↩ An AMD patent describes the double-poly process as well as the static RAM cell; I'm not sure this is the process used in the Ethernet chip, but I expect the process is similar. The diagram below shows the RAM cell with its two resistors. The patent describes how the resistors and second layer of wiring are formed by a silicide/polysilicon (\"inverted polycide\") sandwich. (The silicide is a low-resistance compound of tantalum and silicon or molybdenum and silicon.) Specifically, the second layer consists of a buffer layer of polysilicon, a thicker silicide layer, and another layer of polysilicon forming the low-resistance \"sandwich\". Where resistance is desired, the bottom two layers of \"sandwich\" are removed during fabrication to leave just a layer of polysilicon. This polysilicon is then doped through implantation to give it the desired resistance. The static RAM cell from patent 4569122, \"Method of forming a low resistance quasi-buried contact\". The patent also describes using the second layer of polysilicon to provide a connection between silicon and the main polysilicon layer. Chips normally use a \"buried contact\" to connect silicon and polysilicon, but the patent describes how putting the second layer of polysilicon on top reduces the alignment requirements for a low-resistance contact. I think this explains the yellow ring of polysilicon around all the silicon/polysilicon contacts in the chip. (These rings are visible in the die photo at the top of the article.) Patent 4581815 refines this process further. ↩ Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest Labels: chips, electronics, reverse-engineering No comments: Post a Comment Older Post Home Get new posts by email: Subscribe About the site Contact info and site index Popular Posts The transparent chip inside a vintage Hewlett-Packard floppy drive Two interesting XOR circuits inside the Intel 386 processor Interesting double-poly latches inside AMD's vintage LANCE Ethernet chip A Multi-Protocol Infrared Remote Library for the Arduino Reverse engineering the barrel shifter circuit on the Intel 386 processor die A dozen USB chargers in the lab: Apple is very good, but not quite the best Apple iPhone charger teardown: quality in a tiny expensive package Teardown and exploration of Apple's Magsafe connector Search This Blog Labels 386 6502 8008 8085 8086 8087 aerospace alto analog Apollo apple arc arduino arm beaglebone bitcoin c# cadc calculator chips css datapoint dx7 electronics f# fpga fractals genome globus haskell HP html5 ibm ibm1401 ibm360 intel ipv6 ir java javascript math microcode oscilloscope photo power supply random reverse-engineering sheevaplug snark space spanish synth teardown theory unicode Z-80 Blog Archive ▼ 2023 (35) ▼ December (4) Interesting double-poly latches inside AMD's vinta... The transparent chip inside a vintage Hewlett-Pack... Two interesting XOR circuits inside the Intel 386 ... Reverse engineering the barrel shifter circuit on ... ► November (2) ► October (3) ► September (1) ► August (2) ► July (3) ► May (1) ► April (2) ► March (4) ► February (5) ► January (8) ► 2022 (18) ► November (3) ► August (1) ► July (1) ► June (1) ► May (1) ► April (4) ► March (2) ► February (3) ► January (2) ► 2021 (26) ► December (4) ► November (2) ► September (1) ► August (1) ► July (2) ► June (2) ► May (1) ► April (2) ► March (4) ► February (4) ► January (3) ► 2020 (33) ► December (2) ► November (3) ► October (2) ► September (4) ► August (5) ► July (2) ► June (3) ► May (4) ► April (2) ► March (5) ► January (1) ► 2019 (18) ► November (3) ► October (2) ► September (3) ► August (1) ► July (4) ► April (2) ► February (1) ► January (2) ► 2018 (17) ► December (1) ► September (4) ► August (1) ► June (1) ► May (1) ► April (1) ► March (3) ► February (1) ► January (4) ► 2017 (21) ► December (5) ► November (2) ► October (3) ► August (1) ► July (2) ► June (2) ► April (2) ► March (2) ► February (1) ► January (1) ► 2016 (34) ► December (2) ► October (5) ► September (8) ► August (2) ► July (3) ► June (4) ► May (1) ► April (1) ► March (1) ► February (4) ► January (3) ► 2015 (12) ► December (2) ► November (1) ► October (3) ► August (1) ► May (2) ► March (2) ► February (1) ► 2014 (13) ► December (1) ► October (1) ► September (3) ► May (2) ► March (1) ► February (5) ► 2013 (24) ► November (2) ► September (4) ► August (4) ► July (4) ► June (2) ► April (1) ► March (2) ► February (2) ► January (3) ► 2012 (10) ► December (1) ► November (5) ► October (1) ► May (1) ► March (1) ► February (1) ► 2011 (11) ► December (2) ► July (2) ► May (2) ► April (1) ► March (1) ► February (3) ► 2010 (22) ► December (2) ► November (4) ► October (3) ► August (1) ► June (1) ► May (2) ► April (3) ► March (4) ► January (2) ► 2009 (22) ► December (2) ► November (5) ► September (1) ► August (3) ► July (1) ► June (3) ► April (1) ► March (3) ► February (2) ► January (1) ► 2008 (27) ► July (3) ► June (1) ► May (3) ► April (4) ► March (10) ► February (6)",
    "commentLink": "https://news.ycombinator.com/item?id=38826187",
    "commentBody": "Interesting double-poly latches inside AMD&#x27;s vintage LANCE Ethernet chipHacker NewspastloginInteresting double-poly latches inside AMD&#x27;s vintage LANCE Ethernet chip (righto.com) 135 points by elpocko 14 hours ago| hidepastfavorite9 comments kens 14 hours agoAuthor here for any questions :-) This is a change of pace from my x86 stuff, just a quick look at a 1982 Ethernet chip that unexpectedly used a second layer of polysilicon for resistors. reply johnklos 13 hours agoparentThe Commodore A2065 ethernet card uses the Am7990, and I&#x27;m still using several of them in various Amiga systems, running both AmigaDOS and NetBSD.While trying to figure out some occasional network stalls, I looked at NetBSD&#x27;s man le(4) page, which had an interesting note:Alas, the Am7990 chip is so old that AMD has \"de-archived\" the production information about it; pending a search elsewhere, we don&#x27;t know how to identify the revision C chip from the date codes.Any chance this information is publicly available elsewhere?Always fascinating writeups, Ken! Thanks :) reply kens 10 hours agorootparentSorry, I don&#x27;t know where you would find that information. reply virgulino 11 hours agoparentprevHappy New Year, Master Ken! Thank you for all the gifts you&#x27;ve given us this year, even on the last day! Cheers, from an island Brazil! reply Genbox 14 hours agoparentprevGreat article Ken. I love the chip&#x2F;die pictures. reply userbinator 8 hours agoprevIf I remember correctly, AMD LANCE is also one of those NICs that became commonly implemented virtually for VMs, along with NE2000, Intel&#x27;s 8257x, and one of the popular Realtek models.The double-inverter loop is like the electrical analog of a \"snap action\" physical mechanism[1]; it&#x27;s bistable, refusing to stay in an intermediate position, and requires force to change its state.[1] When https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pop_it toys became popular recently, the number of people who thought \"that&#x27;s a bistable memory\" upon seeing them was unfortunately not many. reply MenhirMike 9 hours agoprev [–] I&#x27;m always fascinated with die shots that look like artists drawings (black pen on purple&#x2F;blue felt tip marker fields) as shown on a tube TV (slightly blurry&#x2F;color fringe).But no, it&#x27;s not a drawing, it&#x27;s an actual photo. Well, I guess it&#x27;s slightly blurry because of the magnification, but it&#x27;s such a surreal look. Love it! reply kens 8 hours agoparent [–] What I find interesting is that the colors aren&#x27;t \"real\", in the sense that the materials aren&#x27;t colored. The colors are mostly from thin-film interference due to the thin layers of material. And the black lines aren&#x27;t \"real\" either; although the layers look flat, they form three-dimensional structures as they overlap. So many of the black lines show up because they are edges in three dimensions, not because they are actually black. Another consequence is that different chips look completely different due to the manufacturing process that was used, even though the bulk color of the materials is the same.Of course, the images would be sharper if I had a more expensive microscope, since the quality of the optics matters too :-) reply colechristensen 5 hours agorootparent [–] One has to be careful because there are several sources of “color” and it’s hard to argue successfully that any of them are more “real” than the others. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post delves into a vintage Ethernet chip from 1982 and examines its circuitry, storage methods, and manufacturing procedures.",
      "It explores latch circuits and the incorporation of static RAM in the chip, shedding light on the inner workings of the technology.",
      "The article also divulges insights into the transistor construction of the chip as well as mentions two associated patents."
    ],
    "commentSummary": [
      "The article highlights the discovery of double-poly latches in AMD's vintage LANCE Ethernet chip.",
      "The author explores the unique features of the chip based on their findings.",
      "Reader comments express appreciation for the article and share personal experiences with the chip."
    ],
    "points": 135,
    "commentCount": 9,
    "retryCount": 0,
    "time": 1704046855
  },
  {
    "id": 38828804,
    "title": "Harvard Honor Council Member Calls for President Gay's Resignation Over Plagiarism",
    "originLink": "https://www.thecrimson.com/article/2023/12/31/honor-council-member-gay/",
    "originBody": "Sections LEADERSHIP CRISIS NEWS OPINION ARTS BLOG MAGAZINE METRO MULTIMEDIA SPORTS NEWSLETTER EDITOR'S PICK TIPS DONATE STORE NEWSLETTER EDITOR'S PICK TIPS DONATE STORE NEWS House Committee Extends Harvard’s Deadline to Provide Documents About Gay’s Plagiarism Scandal NEWS Harvard Business School Loses Partnership with Lauder Business School Amid Backlash Over Campus Antisemitism NEWS Ten Stories That Shaped 2023 NEWS Former Harvard Corporation Head William Lee ’72 Helped Prepare Gay Ahead of Testimony, Highlighting Complex Dual Roles NEWS ‘Overblown’ or ‘Hypocritical’? Harvard Students Offer Mixed Takes on President Gay Plagiarism Allegations ADVERTISEMENT Op Eds I Vote on Plagiarism Cases at Harvard College. Gay’s Getting off Easy. By An Undergraduate Member of the Harvard College Honor Council, Contributing Opinion Writer I have served as a voting member of the Harvard College Honor Council, the body tasked with upholding the College’s community standards of academic integrity. In my time on the Council, I heard dozens of cases. When students — my classmates, peers, and friends — appear before the council, they are distraught. For most, it is the worst day of their college careers. For some, it is the worst day of their lives. They often cry. It is because I have seen first-hand how heart-wrenching these decisions can be, and still think them necessary, that I call on University President Claudine Gay to resign for her numerous and serious violations of academic ethics. Let’s compare the treatment of Harvard undergraduates suspected of plagiarism with that of their president. A plurality of the Honor Council’s investigations concern plagiarism. In the 2021-22 school year, the last year for which data is publicly available, 43 percent of cases involved plagiarism or misuse of sources. Omitting quotation marks, citing sources incompletely, or not citing sources at all constitutes plagiarism according to Harvard’s definitions. In my experience, when students omit quotation marks and citations, as President Gay did, the sanction is usually one term of probation — a permanent mark on a student’s record. A student on probation is no longer considered in good standing, disqualifying them from opportunities like fellowships and study-abroad programs. Good standing is also required to receive a degree. What is striking about the allegations of plagiarism against President Gay is that the improprieties are routine and pervasive. She is accused of plagiarism in her dissertation and at least two of her 11 journal articles. Two sentences from the acknowledgement section of her dissertation even seem to have been copied from another work. According to the Honor Council’s procedures, the response to a violation depends on the “seriousness of the infraction” and “extenuating circumstances, including the extent to which a student has had similar trouble before.” In other words, while a single lifted paragraph could be blamed on a lapse in judgment, a pattern is more concerning. ADVERTISEMENT In my experience, when a student is found responsible for multiple separate Honor Code violations, they are generally required to withdraw — i.e., suspended — from the College for two semesters. Since the Council was established in 2015, roughly 16 percent of students who have appeared before us have been required to withdraw. It is a serious thing for the Council to render this judgment, and I have never taken any such vote lightly. Voting to suspend a peer with whom I might share a dorm, club, or class is not easy. We have even voted to suspend seniors just about to graduate. But strict sanctions are necessary to demonstrate that our community values academic integrity. Cheating on exams is not okay. Plagiarism is not okay. It may be true that the plagiarism allegations against President Gay fall short of the Faculty of Arts and Sciences’ interim policy on research misconduct. She may not have “intentionally, knowingly, and recklessly” tried to represent the work of her doctoral advisor and others as her own. And there is no evidence that any of her arguments posited as original contributions were plagiarized. But President Gay’s pattern of mistakes is serious, and the Harvard Corporation should not minimize these allegations of plagiarism, as it has readily done. In a Dec. 12 University-wide letter, the Corporation described the alleged plagiarism as “a few instances of inadequate citation.” The letter lauded President Gay for “proactively” correcting her articles by inserting citations and quotation marks. By definition, Gay’s corrections were not proactive but reactive — she only made them after she was caught. And that the Corporation considers her corrections an adequate response is not fair to undergraduates, who cannot simply submit corrections to avoid penalties. When my peers are found responsible for multiple instances of inadequate citation, they are often suspended for an academic year. When the president of their university is found responsible for the same types of infractions, the fellows of the Corporation “unanimously stand in support of” her. There is one standard for me and my peers and another, much lower standard for our University’s president. The Corporation should resolve the double standard by demanding her resignation. Editor’s Note: In order to protect the author from retaliation, and because the proceedings of the Harvard College Honor Council are sensitive and confidential, we made the decision to grant this author anonymity. ADVERTISEMENT Readers should also note that online commenting has been disabled for this piece in an effort to help protect the author’s identity. —Tommy Barone and Jacob M. Miller, Editorial Chairs —J. Sellers Hill, President Want to keep up with breaking news? Subscribe to our email newsletter. TAGS OP EDS ADVERTISEMENT ADVERTISEMENT ADVERTISEMENT ADVERTISEMENT The Harvard Crimson The University Daily, Est. 1873 SECTIONS News Opinion Arts Blog Magazine Videos Sports ABOUT General Diversity & Inclusion Privacy Policy Rights & Permissions RESOURCES Advertising Newsletters Journalism Programs Photo Store CONTACT US Corrections Copyright © 2024 The Harvard Crimson, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=38828804",
    "commentBody": "I Vote on Plagiarism Cases at Harvard College. Gay&#x27;s Getting Off EasyHacker Newspastlogin [flagged] I Vote on Plagiarism Cases at Harvard College. Gay&#x27;s Getting Off Easy (thecrimson.com) 113 points by mcenedella 9 hours ago| hidepastfavorite55 comments justinclift 7 hours agonext [–]In order to protect the author from retaliation, and because the proceedings of the Harvard College Honor Council are sensitive and confidential, we made the decision to grant this author anonymity.Lets see how long that lasts... reply thedailymail 7 hours agoprevAs an academic, the real surprise for me is how thin her publication record is. At my institution someone with only 11 papers would be considered early career, not president material. (I do realize that different fields have different metrics of success.) reply lacrimacida 6 hours agoparentIs it always quantity over quality? Maybe just pumping out quantity exacerbated the plagiarism we’re seeing popping up more and more often. reply craigdalton 4 hours agoprevHas anyone got a link to a list of the alleged plagiarisms? The two alleged examples I saw shared on Twitter were not plagiarism. One alleged example was clearly referenced to the original author. reply Gimpei 6 hours agoprevI dunno. I was in academia for a while and wrote some papers, and in my field at least, the literature review was worthless and really only there to give other people in your field citations. Which is to say that it was not an area of great originality and if you’re citing somebody else’s literature review, you’re really just citing the grammatical structure of the sentence because there’s nothing original in what you’re saying. So I’m not especially bothered by copying a few sentences from there. It just seems a bit sloppy and a bit lazy to me. Which isn’t to say that I’m much of a fan of Gay. And it does seem like the much bigger deal is the thin publication record. Although, at the same time, isn’t administration the natural place for people who don’t like research to go? reply armchairhacker 7 hours agoprevWhat exactly did she plagiarize and where? Politics doesn&#x27;t influence the severity of plagiarism.> When my peers are found responsible for multiple instances of inadequate citation, they are often suspended for an academic year.It would also help if the author or someone could give examples of specific instances and their punishments including this one (with the student anonymous and assignment vague enough that it&#x27;s not identifying).---Accidental plagiarism is real plagiarism, but in practice, not a big deal. Even a few instances like \"two sentences from the acknowledgement section of her dissertation\", over a 100+ page dissertation and 11 journal entries, seem OK to me. I&#x27;m in academia, still I probably don&#x27;t know very much, but my understanding is that individual violations like this often end up with lost points, not even a 0 on the assignment. The author mentions that repeat violations carry harsher penalties, but doesn&#x27;t mention that the next violation is usually made after the student has been cited for the previous one.On the other hand, just because someone went out of their way to hunt down an instance of plagiarism, doesn&#x27;t mean it should be downplayed. If she ever intentionally plagiarized, then I agree with the author. Or even if she only accidentally plagiarized, but it was several egregious instances like paragraphs copied verbatim, I&#x27;m also inclined to agree (then she&#x27;s not malicious, but incompetent). reply mcenedella 7 hours agoprevWhy flagged @dang? reply tacheiordache 6 hours agoparentSubject does not like being talked about or dissected too much.I highly doubt @dang flagged this himself, other people did. It seems like a controll valve that can be used from time to time. Anyone can gang up with a bunch of others to mass flag a story. reply alkhatib 7 hours agoprevAnother interesting read, from \"the most read\" section: https:&#x2F;&#x2F;www.thecrimson.com&#x2F;article&#x2F;2023&#x2F;12&#x2F;29&#x2F;steinberg-weap... reply vecplane 8 hours agoprevIs it feasible to generate comprehensive plagiarism analyses for most or all published works? reply andy99 8 hours agoparentI wrote a paper this year (after about a 15 year hiatus) and apparently submitted papers, this was to an elsevier journal, now automatically go though a plagiarism detector of some kind that gives a score that can&#x27;t exceed some value. We actually failed the first time because we&#x27;d published the paper on Arxiv first, which seems like a big flaw in the system. I don&#x27;t have a lot of faith in, nor do I support automated detection like that, but it does happen systematically now. reply Robotbeat 7 hours agorootparentIt’s not a flaw from Elsevier’s perspective to discourage authors from releasing free pre-prints. reply woodruffw 8 hours agoparentprevNot without false positives, which are unacceptable (unless you&#x27;re willing to put the work saved from plagiarism checking into absolving the innocent).I was forced to submit essays to a plagarism checker in high school (well over a decade ago). The company running this checker had, in their infinite wisdom, included a check for writing complexity under the assumption that a 10th grader who writes like an adult is probably cheating. This was embarrassing for everyone. reply hooverd 7 hours agoparentprevAs long as it&#x27;s available to students so they can check their work for unintentional plagiarism. reply BOOTRACER 3 hours agoprevBlack Privilege plain and simple reply mcpackieh 8 hours agoprev> What is striking about the allegations of plagiarism against President Gay is that the improprieties are routine and pervasive.> She is accused of plagiarism in her dissertation and at least two of her 11 journal articles. Two sentences from the acknowledgement section of her dissertation even seem to have been copied from another work.Presuming the allegations are true, I find it interesting that it went unaddressed for so long. The matter was seemingly systematically ignored for almost 30 years until she pissed off the wrong people by allowing students to protest against Israel. Then people went digging for something to use against her and found this plagarism. From the NYTimes:> After weeks of tumult at Harvard over the university’s response to the Israel-Hamas war and the leadership of its president, Claudine Gay, there was no shortage of interest in a faculty forum with Dr. Gay this week.> In a town hall held over Zoom on Tuesday with several hundred members of the Faculty of Arts and Sciences, Dr. Gay focused on how to bridge the deep divides that had emerged on campus as a result of the war, according to two people who attended and asked for confidentiality because of the sensitivity of the situation.> Faculty members who spoke up in the meeting were largely positive, and there were no questions about Dr. Gay’s academic record after public allegations of plagiarism. The matter wasn’t even raised, one professor said.> But by Thursday, new questions surrounding Dr. Gay’s scholarship had shifted to the forefront, after the university said late Wednesday that it had identified two more instances of what it called “duplicative language without appropriate attribution,” from her 1997 doctoral dissertation.https:&#x2F;&#x2F;www.nytimes.com&#x2F;2023&#x2F;12&#x2F;21&#x2F;us&#x2F;harvard-claudine-gay-p... reply bitcurious 7 hours agoparentThe plagiarism allegations were first surfaced about a year ago, entirely unrelated to Dr. Gay’s embarrassing failure to condemn antisemitism.> Over the past year, the accusations against Gay were a frequent topic of discussion including accusations that she is a serial plagiarist.https:&#x2F;&#x2F;www.wsj.com&#x2F;us-news&#x2F;education&#x2F;behind-the-campaign-to... reply OrvalWintermute 7 hours agorootparentIt isn&#x27;t just her plagiarism.She has a very thin scholarly record. https:&#x2F;&#x2F;www.mindingthecampus.org&#x2F;2022&#x2F;12&#x2F;16&#x2F;the-president-ha...That this this thin record which she has, is apparently partially contaminated reply gruez 7 hours agoparentprev>Presuming the allegations are true, I find it interesting that it went unaddressed for so long. The matter was seemingly systematically ignored for almost 30 years until [...]If the recent youtube plagiarism drama[1] is anything to go by, you can do blatant plagiarism for years, get called out by randoms, and still get away with it.[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=yDp3cB5fHXQ[2] https:&#x2F;&#x2F;www.vulture.com&#x2F;2023&#x2F;12&#x2F;hbomberguy-interview-james-s... reply greatpostman 7 hours agoparentprevIt’s all about Israel and flexing power to remove others, a warning shot to institutions about what will be tolerated.\"To learn who rules over you, simply find out who you are not allowed to criticize.“ reply poulsbohemian 7 hours agoparentprev>Presuming the allegations are true, I find it interesting that it went unaddressed for so long.Multiple German politicians have been forced to resign decades after their dissertations, with similar accusations. It seems like it is a case of rising to a point of public renown that then leads to greater public scrutiny. In some cases, I wonder if it a fake-it-until-you-make-it situation that upon making it, causes the house of cards to collapse. reply nradov 8 hours agoparentprevPlagiarism and research fraud are rife throughout academia. If we subjected everyone to the same level of scrutiny we would find a lot of people who have done far worse than Dr. Gay. I am hopeful that new AI tools will automate this type of investigation and find more instances of academic dishonesty. reply daft_pink 7 hours agorootparentThe president of Harvard should be subject to a higher level of scrutiny related to academic integrity than most people. reply MiguelX413 7 hours agorootparentEveryone should be subject to a very high level of scrutiny. reply pylua 7 hours agorootparentprevThe same ai tools that ironically aid in plagiarism and inability to give credit to their sources. reply AlbertCory 7 hours agorootparentprevDoes Turnitin not meet your standards? reply nradov 4 hours agorootparentI haven&#x27;t used Turnitin for several years. From what I recall it couldn&#x27;t detect cases where the author paraphrased another work without adding any original thoughts. reply newsclues 7 hours agoparentprev“The matter was seemingly systematically ignored for almost 30 years until she pissed off the wrong people by allowing students to protest against Israel. Then people went digging for something to use against her and found this plagarism.”Like much of the metoo witch hunting? reply enriquec 8 hours agoparentprevIs the implication that because she allowed students to call for the genocide of Israeli&#x27;s, she should get off? reply db48x 7 hours agorootparentThe problem really isn’t that students at her institution support genocide. The problem is that when she asked if chanting in support of the murder of Jews was against school policy, she equivocated. She would not have equivocated if the students had been chanting that Blacks should be killed, instead of Jews. reply daft_pink 7 hours agorootparentExactly. What they said was so offensive, not because she spoke in favor of free speech. It was because after being outspoken and sanctioning students for years in regards to hate speech towards other groups, when it comes to murdering Jews, they are suddenly eliciting the importance of freedom of speech.When you promote hate speech as free speech against one group, while punishing hate speech against all the others, it leaves many outside observers with the opinion that you tacitly approve of hate speech against that group and I believe creates a hostile environment in the entity you lead towards members of that group. reply mmcwilliams 7 hours agorootparentprevCan you tell me the dates of the protests where you heard students chanting \"Jews should be killed\"? reply ejb999 7 hours agorootparentThe post said:“when she asked if chanting in support of the murder of Jews was against school policy”She said it depended on the context - do you think she would have said “it depended on the context” if ask the same question about gays or blacks? reply mmcwilliams 6 hours agorootparentThat&#x27;s asking a hypothetical about a hypothetical. The OP made a claim about chants on campus that I didn&#x27;t hear. reply unsupp0rted 7 hours agorootparentprevHere’s a really in 2017 in which white nationalists didn’t chant “non-whites should be killed”.All they chanted was “you will not replace us”.https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;av&#x2F;world-asia-40911744In Harvard’s case, nobody chanted “kill all Jews”. All they chanted was “from the river to the sea”. reply skissane 2 hours agorootparentprevAccording to the transcript of the Congressional hearing https:&#x2F;&#x2F;rollcall.com&#x2F;2023&#x2F;12&#x2F;13&#x2F;transcript-what-harvard-mit-...ELISE STEFANIK: It’s a yes or no question. Let me ask you this. You are president of Harvard, so I assume you’re familiar with the term intifada, correct?CLAUDINE GAY: I’ve heard that term, yes.ELISE STEFANIK: And you understand that the use of the term intifada in the context of the Israeli Arab conflict is indeed a call for violent armed resistance against the state of Israel, including violence against civilians and the genocide of Jews. Are you aware of that?CLAUDINE GAY: That type of hateful speech is personally abhorrent to me.ELISE STEFANIK: And there have been multiple marches at Harvard with students chanting quote, “there is only one solution intifada revolution.” And quote, “globalize the intifada.” Is that correct?CLAUDINE GAY: I’ve heard that thoughtless, reckless and hateful language on our campus, yes.ELISE STEFANIK: So, based upon your testimony, you understand that this call for intifada is to commit genocide against the Jewish people in Israel and globally, correct?CLAUDINE GAY: I will say again that type of hateful speech is personally abhorrent to me.Given that the Second Intifada saw over 800 Israeli civilians murdered in terrorist attacks, it is not unreasonable to interpret \"globalize the intifada\" as call for murdering Israelis and&#x2F;or Jews outside of Israel. And if the President of Harvard admits it happened, we don&#x27;t really need to know the exact dates in order to know that it did happen. reply bluish29 7 hours agorootparentprevSome people consider \"from the river to the sea\" to be a call to genocide of Jews. Because, of course, they ignore the simple fact that majority of Jews don&#x27;t even live in Israel [1]. And to call for a Palestinian state from the river to the sea doesn&#x27;t say that Jews people cannot live inside it. And ironically the likud party which dominated the Israeli government the last couple of decades have almost the same sentence in its first point on its charter [2] and no one calls this a call to kill all Palestinians (Although some say it in public [3][4]).[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Jewish_population_by_country[2] https:&#x2F;&#x2F;www.jewishvirtuallibrary.org&#x2F;original-party-platform...[3] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Death_to_Arabs[4] https:&#x2F;&#x2F;www.politico.com&#x2F;news&#x2F;2022&#x2F;05&#x2F;29&#x2F;israel-jerusalem-ma... reply gruez 6 hours agorootparent>Some people consider \"from the river to the sea\" to be a call to genocide of Jews. Because, of course, they ignore the simple fact that majority of Jews don&#x27;t even live in Israel [1].so it&#x27;s only genocide when you&#x27;re trying to wipe them out entirely? if you only wipe out a quarter, for instance, that&#x27;s not genocide? reply bluish29 6 hours agorootparentI didn&#x27;t say that. This is just you trying to bend my words. I did say that from river to the sea does not mean that Jews living in Palestine will be wiped out. If you think that this the meaning that&#x27;s fine. I just hope that you consider it the same when the other side use almost exactly the same slogan (interesting you ignored that) reply mmcwilliams 6 hours agorootparentprevDo the people chanting that believe that to be a call to genocide or people who don&#x27;t want them to be able to chant that? reply hooverd 7 hours agorootparentprevApparently that&#x27;s a call for genocide and calling for a \"Sinai Solution\" a la the Madagascar Plan is just business as usual. reply wredcoll 6 hours agorootparentprevAnd why do you suppose she did that? Is it perhaps because of the constant attempts to force people to treat \"jews the people belonging to a certain race&#x2F;religion\" and \"the state of israel\" as exactly the same thing? The sheer amount of people who accept these, bad faith, to put it mildly, arguments, is, well, not surprising, but is pretty sad. reply matrix87 5 hours agorootparentSo because those Zionist arguments exist, calling for the murder of Jewish people (not metaphorically, the question was a hypothetical) is something where \"nuance\" matters now?You acknowledge the arguments as bad faith, then turn around and use them to relativize hate speech. Anyone calling for the murder of Jewish people (per the metaphorical) would never accept those arguments in the first place reply zdragnar 7 hours agorootparentprevCharitably, I think the point was that academics overlook it, like the dirty inside secret that everyone knows to not talk about.Once she drew the ire of people outside the normal academic circles, things that should have been controversial (like blatant plagiarism) were brought forward.Further, these people weren&#x27;t looking specifically in the interest of academic integrity, they were looking for any reason at all to get her fired for not unequivocally saying that calling for genocide is outside the student code of conduct.As a result, the allegations have, thus far, been treated as unworthy of much attention- it wasn&#x27;t that they are right, but the accusers have the wrong motivation.I don&#x27;t know how much of this phenomenon is a modern issue, but it does seem to be very prevalent in today&#x27;s society. reply OrvalWintermute 7 hours agorootparentprevI am against the genocide of any peoples.The anti-zionist attacks, the anti-jewish genocide talk, and the anti-palestinian rubbling of Gaza are all deeply troubling. Consequently, I do not fall into either of the two main camps.However, she is the president of a premier university and must be held to account for her serial failures to uphold academic ethics.The Plagiarist must go reply asylteltine 8 hours agoprevnext [2 more] [flagged] woodruffw 8 hours agoparentPlease don&#x27;t insinuate. If you have something to say that you think we know, you should say it. reply matthewdgreen 7 hours agoprev [–] If you screw up parallel parking while taking a driver’s test, you won’t pass. You won’t get a driver’s license. It’s a very harsh treatment. By contrast if you screw up parallel parking in the real world, the police (probably) won’t arrest you and take you to jail for messing up on your first few tries. There’s a reason for this, and it’s related to the reason we have very different standards at the undergraduate and professional levels. reply twbarr 7 hours agoparentBy comparison, if you steal as an infant, you get off easy. If you steal as an adult, you don&#x27;t. Screwing up parallel parking is a mistake, plagiarism is not. reply matthewdgreen 6 hours agorootparentThis isn’t stealing. Using a meaningless sentence isn’t a very big deal. In academia we care about plagiarism because we care very deeply about the misattribution of academic credit. In practice this does not mean borrowing a sentence in an acknowledgements section, which is sad and embarrassing. It means stealing full ideas and written sections to take credit for them. We treat this much more harshly at the early student level to dissuade serious violations later in life. We do this for the same reason we demand exceptional performance on the parking section of the driving test, even though many licensed adult drivers are terrible at parking and society survives just fine. reply tgsovlerkhgsel 7 hours agorootparentprevBased on the definition of plagiarism in the article, I&#x27;m not 100% convinced that only intentional plagiarism counts as plagiarism - it seems like omitting proper markings around something intended to be a quote, or dropping a \\cite line under a section rephrasing an idea mentioned in another paper could be enough to be considered a mild form of plagiarism, to be punished with a \"light\" punishment that \"only\" derails your academic career for half a year.Given the concept of \"self-plagiarism\" and given the treatment I&#x27;ve seen honest students (at a different university) receive for alleged plagiarism (turns out there are only so many unique ways to implement a fizzbuzz-level piece of homework), I&#x27;m not willing to blindly assume common sense here. reply wredcoll 6 hours agorootparentprevDon&#x27;t be silly. If I write a sentence in a paper that happens to be identical to a sentence someone else wrote, I&#x27;ve now committed plagiarism even if I had no way of knowing that sentence even existed! reply messiah_complex 7 hours agoparentprevThe analogy is inapt. It&#x27;s not illegal to fail at parallel parking. Even if it was, it&#x27;s not the type of offense that would get you arrested. You say the standards are different, and they are, but then you conflate the two contexts by treating both consequences as punishments. Failing a test is not a punishment. Failing at parallel parking is not legally or morally blameworthy. reply matthewdgreen 6 hours agorootparentIt’s not illegal to misattribute small portions of a sentence either. There is literally no law against it. We harshly dissuade it at the grade school, high school, and undergraduate levels (meaning, we will give you a bad grade) because we are trying to teach young students not to misuse even a fragment of text with extremely harsh punishments outside of the legal system. We don’t apply the same level of punishment in real life and certainly not through the legal system because it’s not that serious to borrow a few words in the acknowledgement of a thesis. It’s just embarrassing and bad, as long as the concepts are original.ETA: I say this as someone who has had both scientific contributions and entire introductory sections copied verbatim into other paper. That’s plagiarism. A meaningless sentence copied from my work has as much relationship to serious plagiarism as a fart in a car has to a Sarin gas attack. reply messiah_complex 2 hours agorootparentYou&#x27;re using a different definition of plagiarism than Harvard uses. I also think Harvard&#x27;s definition is overly broad, but that&#x27;s not the point. The point is that a student would be punished for engaging in the same conduct that&#x27;s at issue here. The double standard is the problem. The president of Harvard has broken the same rules she has enforced against the University&#x27;s students. For them, the consequences are serious. reply Capricorn2481 7 hours agoparentprev [–] The reason is that those are two different organizations with different rules.Also, this is not true? If you screw up parallel parking you&#x27;re docked points. Lots of people pass with screwing it up, it&#x27;s the most commonly missed part of the test replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An anonymous member of the Harvard College Honor Council has written an article criticizing President Claudine Gay for alleged plagiarism and demanding her resignation.",
      "The author argues that the university's treatment of students accused of plagiarism is stricter than the treatment of the president, citing the Honor Council's procedures and sanctions as evidence.",
      "The author calls on the Harvard Corporation to demand President Gay's resignation in order to address what they perceive as a double standard."
    ],
    "commentSummary": [
      "The article discusses cases of plagiarism in academia, including allegations against the President of Harvard University.",
      "Different perspectives on plagiarism, its severity, and the consequences for individuals accused of plagiarism are explored.",
      "The discussion also touches on topics such as hate speech, freedom of speech, and the interpretation of phrases related to Israel and Judaism."
    ],
    "points": 113,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1704070238
  },
  {
    "id": 38826130,
    "title": "WWII hero's daughter sets record straight, reveals father's heroics",
    "originLink": "https://www.dispatch.com/story/news/local/2023/12/31/bazooka-charlies-granville-daughter-shares-wwii-heroics-in-new-book/72033226007/",
    "originBody": "LOCAL WWII heroics of 'Bazooka Charlie' doubted until Granville daughter sets record straight Alan MillerTheReportingProject.org On a night when Carol Apacki had trouble sleeping, she found herself at the computer typing a search for “Rosie the Rocketer.” That was the name her dad had given his airplane during World War II, and she knew he had done some amazing things with it during the war. The first thing that came up in her search was a very long, technical discussion about whether her father, Maj. Charles Carpenter, actually could have strapped six bazookas onto the wings of his 800-pound reconnaissance plane made of cloth over a frame of welded steel and wood – and used those weapons to take out German tanks. With what little evidence they had and their knowledge of the Piper L-4 Grasshopper’s construction and capabilities, the war enthusiasts cast doubt about whether the stories of his heroics were true. They suggested it was myth at best and perhaps even wartime propaganda. “As for ‘rosie,’ I’m inclined to dismiss it using a highly technical term: ‘Complete Bollocks,’” wrote one skeptic. “During WWII, propaganda, both against the enemy and to bolster the Home Front, was in high gear. Rosie appears to be the result of yet more ‘home-grown hero’ wishes.” The typically mild-mannered and kindly Apacki could feel her face flush. “That made me a little angry,” said Apacki, 81, of Granville, who had plenty of evidence that all of it was true. Her dad, a history teacher before and after the war, was a prolific letter writer, and her mom, Elda, had saved all of his letters and the photos he sent home. So she pulled out box after box of letters, photos and documents and began to school the historians on the true story of a humble soldier and an otherwise quiet family man from Illinois who became known as “Bazooka Charlie.” She shared, with evidence in his own handwriting, that Maj. Carpenter repeatedly flew his military version of a Piper Cub low over enemy troops – who were firing at him with pistols and rifles – and took out German tanks and other armored vehicles by firing missiles at them. “There’s no question he saved countless lives,” Apacki said. The doubters were stunned, and the online conversation caught the eye of Colin Powers of La Pine, Oregon, a retired mechanical engineer and pilot who had restored several planes like the one Apacki’s dad flew. Powers encouraged Apacki to write a story about her dad and offer it to the Experimental Aircraft Association Warbirds magazine. It was a small act inspired by respect for her father and her passion for accuracy. It also led to a book and, almost miraculously, the recovery and restoration of the very plane Bazooka Charlie Carpenter flew on his remarkable missions. The plane is now in the Collings Foundation’s American Heritage Museum in Hudson, Massachusetts. “In 2016, I got a call from a woman named Carol from Ohio,” said James P. Busha, the EAA magazine’s editor and a retired Oshkosh, Wisconsin, police detective lieutenant. “She said she wanted to submit a story about her dad. She said he was a pilot in World War II. “Then she said her father was Charles Carpenter, and she must have thought the phone went dead. I was speechless,” Busha said. “This was a woman talking about her father, but it was a path into history.” After the article appeared, it led to the discovery of her dad’s plane, and Busha contacted Apacki and said, ‘I think there’s a book here.’” During his visit to Granville in 2020 to meet with Apacki and her husband, Ken, Busha said, “I was gobsmacked when I saw all of the letters and documents and diaries.” Apacki’s mother had collected and saved all of it. Apacki was excited that someone wanted to help tell her father’s story, to set the record straight and document his accomplishments. She was thinking it would be a self-published book with enough copies to share with family and friends. “I came back the next day, and she had these lemon cookies for me,” Busha said. “She told me, ‘I thought at first you might be a shyster, but you’re OK.’” Busha and Apacki went on a journey through time and deep into history – including some personal history about the toll the war took on her father, his marriage and his family. Apacki insisted that any book about her dad’s heroics should also include the effects on his mental and physical health – and the home-front heroics of her mother, who stood by him and held their family together during very difficult circumstances. The story was not just about her dad, “but about what can happen to a marriage and a family. It’s many people’s story,” said Apacki, who was born while her father was at war. “I didn’t see my dad until I was 4 – and I didn’t really like him because he was so stern. I’m sure he had (post-traumatic stress disorder).” Some veterans who have read the book or heard Apacki talk about the war’s effects on her dad and her family have been overwhelmed with appreciation for her willingness to be so open. “I had people come up to me in tears – Vietnam veterans – who said, ‘Thank you for telling this story.’” Apacki said the book started as a gift for her family. “I have 13 grandchildren who knew nothing of him. For my family, it has been a wonderful reconnection with their roots.” Her dad grew up as one of six children in a home where his dad drank and gambled so much that he lost the family farm. Adversity led Carpenter as a 17-year-old, to write this personal creed: “I have resolved to exert all of my efforts toward being a nobler and stronger fellow, a gentleman, a scholar, a friend, and a real man. To the best of my ability, I will ever strive to self-control, self-improvement, freedom, wisdom, courage, generosity, truth and true nobility before gods and men. I will be better.” – Charles Marston Carpenter, 1929 Apacki said the creed framed who her father would become, but the war challenged his values. He enlisted after Japan attacked Pearl Harbor in 1941, and by 1944, he was firing missiles at German tanks as U.S. troops swept across Europe. “He was seeing devastation,” she said. “He was brave but not the man he wanted to be. He was a man of peace, and he saw untold deaths and destruction – day after day, endless death and destruction.” In one incredible act of bravery, Carpenter, who had flown over an occupied village, landed near some U.S. troops just outside town and asked why the troops were not moving to rout the few Germans holding it. The troops were under the impression the town was swarming with German troops, so they stayed at bay – until Carpenter jumped onto a U.S. tank, grabbed a 50-caliber machine gun and threatened to use it if they didn’t take the village. His bold move inspired the troops, and they captured the village. He might have been court-martialed for that move if not for Maj. Gen. John Wood, Commander of the 4th Armored Division, who appreciated Carpenter’s moxie and his military skills, particularly his flying. Wood made Carpenter his personal reconnaissance pilot. Apacki said she was stunned when she finally saw the plane she had read so much about. “I couldn’t believe how small it was,” she said. “It looked like a kid’s toy.” Her dad’s letters talked about being shot at by German troops, “and the restorer found bullet holes in the plane.” The Collings Foundation located the plane in a museum in Austria, where it had been used after WWII to pull gliders into the air. The museum operators clearly didn’t know its history and sold it for a song, Busha said. The foundation hired Colin Powers to restore the plane, given his experience rebuilding several Piper L-4s. “I restored it for Collings but also for Carol,” Powers said. When he stripped off the old fabric, he found that “Rosie had bullet holes, and a lot of signatures of the people who built it were on the wooden spars.” It took him a year to restore it, and when it came time to apply the “Rosie the Rocketer” image on the new fabric skin, he invited Apacki’s daughter and Carpenter’s granddaughter to put paintbrush to canvas. Erin Pata is a graphic artist living in California, and Powers said she nailed the “nose art” on the plane. As he was completing the restoration, Powers made a special request of Apacki. “When I finished the restoration, I asked Carol for a photo of her mom to use as a pin-up in the cockpit,” he said. “Carol sent me a picture of her mom and Carol as a child. It was a very special touch.” At the Experimental Aircraft Association’s annual airshow in Oshkosh in July, an event that drew more than 800,000 enthusiasts over seven days, Apacki was a guest of honor and center of attention. She was invited to center stage to talk about her dad and the book, “Bazooka Charlie: The Unbelievable Story of Major Charles Carpenter and Rosie the Rocketer.” She was beaming, Busha said, and taken aback by the several hundred people who came to hear her. “I saw the youngest 80-year-old woman I have ever seen in my life,” he said. “You would have thought I was escorting an 18-year-old girl around. She was giddy. She was the star of the show. I introduced her as Charles Carpenter’s daughter, and she was the rock star.” At the largest aviation event in the world, Busha said, everyone knows the name Charles Carpenter, “or the legend of him.” And now they have a book full of facts about the man, the myth, the legend – the humble World War II hero Carol Apacki knew simply as dad. Alan Miller writes forTheReportingProject.org, the nonprofit news organization of Denison University’s Journalism program, which is sponsored in part by the Mellon Foundation anddonations from readers. Sign up for The Reporting Project newsletterhere.",
    "commentLink": "https://news.ycombinator.com/item?id=38826130",
    "commentBody": "WWII heroics of &#x27;Bazooka Charlie&#x27; doubted until daughter sets record straightHacker NewspastloginWWII heroics of &#x27;Bazooka Charlie&#x27; doubted until daughter sets record straight (dispatch.com) 112 points by alehlopeh 15 hours ago| hidepastfavorite24 comments ilamont 14 hours agoThere was a story from the Vietnam War which sounds totally outlandish - a damaged USAF jet pushing another damaged jet via its tailhook (pressed up against a glass windshield of the pusher) as it leaked fuel and was in danger of crashing in enemy territory. But the account is real, despite sounding so improbable. https:&#x2F;&#x2F;theaviationgeekclub.com&#x2F;pardos-push-how-an-f-4-pushe...For WW2 air crews, I believe gun cameras were standard in many aircraft (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gun_camera), but for an ad hoc weapon like the one described in the article it would less likely, which makes witness accounts or other evidence so important.My grandfather was in a US Navy salvage group in WWII. At the time, to verify an enemy vessel (especially subs) were sunk, they would send down divers in the old fashioned \"heavy gear\" suits to identify the vessels. They would also bring up proof, usually something that could be tied to the vessel, which could be a piece of military-grade equipment or even silverware engraved with the name of the ship.This is what the suits looked like: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Standard_diving_dress reply pdonis 14 hours agoparentWikipedia also has an article on Pardo&#x27;s Push:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pardo%27s_PushThe article addresses a point that occurred to me when reading the Aviation Geek Club article: Air Force planes didn&#x27;t land on carriers, so why would they have a tail hook? But according to the Wikipedia article, all F-4s were built with tail hooks since the plane was originally designed for the Navy and Marines and was not redesigned when the Air Force started using it. reply thedrbrian 12 hours agorootparent>The article addresses a point that occurred to me when reading the Aviation Geek Club article: Air Force planes didn&#x27;t land on carriers, so why would they have a tail hook?Loads of air force aircraft have tail hooks ,they&#x27;re used to stop the aircraft in an emergencyhttps:&#x2F;&#x2F;www.quora.com&#x2F;Why-do-F16s-have-tail-hooks-even-thoug... reply pdonis 12 hours agorootparent> they&#x27;re used to stop the aircraft in an emergencyAh, yes, I&#x27;d forgotten that they have arresting gear on many runways for emergency use. reply throwup238 13 hours agorootparentprevThe titular Bob Pardo passed away a few weeks ago. RIP o&#x27; brave airman! reply FredPret 13 hours agorootparentprevWhat a crazy thing!Weird that he died a couple of weeks ago. But what a pilot. reply kevin_thibedeau 12 hours agorootparentprevSome air force planes have tail hooks for landing with emergency arrestor gear on land. reply WalterBright 14 hours agoparentprevI doubt there were gun cameras installed on B-17s because:1. I&#x27;ve seen a lot of WW2 aviation footage, and none appeared to be from B-17s.2. The combat footage that exists for B-17s was shot(!) for the \"Memphis Belle\" documentary, and some footage for it left on the cutting room floor. This was filmed by a camera crew going along for the ride on several missions. Memphis Belle footage appears in countless WW2 documentaries.3. My dad flew 32 missions in one and never mentioned gun cameras, though he did talk about them when he flew ground attack missions in the Korean War. I have a couple of them from his airplane that the military didn&#x27;t want. reply danielvf 12 hours agorootparentGun cameras on B-17&#x27;s might have been a late war thing? Here&#x27;s a photo of some B-17 crew members with new gun cameras in 1945:https:&#x2F;&#x2F;www.superstock.com&#x2F;asset&#x2F;crew-members-rd-bomb-group-...The B-29&#x27;s and B-26&#x27;s were built from the factory with gun camera fixtures in their turrets as well.That said, I&#x27;ve hard a hard time finding any footage from a B17. reply WalterBright 10 hours agorootparentInteresting. The photo was in March, 1945, just before the end of the war. My dad&#x27;s tour ended in late summer 1944. reply AndrewKemendo 13 hours agoparentprevPlease also appreciate that as a cadet at the Air Force Academy, you live and breathe these stories day and they out because they are real and absolutely insane. They are also taught as the types of things expected of us as military officers.Worth reading about some of the insanity that Lt Lance Sijan went through before his reply globalise83 14 hours agoparentprevYour grandpa probably saw some REALLY unnerving sights down there. I shudder to think about it. reply ceejayoz 14 hours agoparentprev> At the time, to verify an enemy vessel (especially subs) were sunk, they would send down divers in the old fashioned \"heavy gear\" suits to identify the vessels.Just hope you don&#x27;t sink someone over the Mariana Trench. reply flir 12 hours agoparentprevRiffing off the diving suit link: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;William_Walker_(diver) reply nradov 7 hours agoparentprevWhile it might have happened in a few rare circumstances, I guarantee that most enemy vessel sinkings were never verified by divers. The wrecks tended to be too deep for divers to reach regardless of equipment. During the war, divers were needed for more urgent taskings including salvage, construction, and maintenance.Instead US submarines documented evidence of sinkings using a variety of other techniques. They had cameras attached to periscopes and for use on the bridge when surfaced. They could hear breakup noises on the hydrophones. And when possible they would pick up small pieces of floating wreckage. Errors were common.After the war there was a major effort to reconcile sinking claims. The committee relied largely on seized enemy documents to verify what had actually been sunk and how.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Joint_Army%E2%80%93Navy_Assess... reply ChrisMarshallNY 14 hours agoprevThat&#x27;s a great story. I liked hearing about his daughter.My father was affected by PTSD. He never treated it, and I think he never recovered. He was a war hero, and is buried in Arlington[0].I have another friend who is a Vietnam vet (and also a decorated hero), and treated his PTSD. The difference is night and day.My father was quiet, and never talked about the war. We&#x27;re still not sure what earned him his stars.My friend has absolutely no problem talking about his action. He&#x27;s not boastful, but it doesn&#x27;t bother him to talk about it.[0] https:&#x2F;&#x2F;cmarshall.com&#x2F;miscellaneous&#x2F;MikeMarshall.htm reply AndrewKemendo 13 hours agoparentPTSD is under treated if you can actually believe it.I only recently realized I had it and just had decades of coping mechanisms. So there’s often a lot to unpack especially with high performing people before you can even start looking at it. reply livinginfear 12 hours agoparentprevThank you for sharing that link, and your story. Your father led an extraordinary life. reply throw0101a 8 hours agoprevReminds me Desmond Doss and the movie Hacksaw Ridge: while certain elements were fictionalized for the purposes of telling a story on film, some actual events were left because no one would believe they were real and would come off as too fantastical:> Other changes occur near the end of the film, when Doss is placed on a stretcher. In real life, Doss had another wounded man take his place on the stretcher. After treating the soldier, a sniper shot fractured Doss&#x27;s arm, and he crawled 300 yards (270 m)[55] to safety after being left alone for five hours.[56] Gibson omitted that from the film because he felt that the audience would not find the scene believable.[57]* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hacksaw_Ridge reply vasco 11 hours agoprevPeople online will prove that all kinds of things would never be possible, come up with explanations of why pictures are doctored or videos edited or CGI&#x27;ed, even calculate based on physics, and it has happened to me more than once that they could convince me of the impossibility of things I&#x27;ve witnessed myself.Tells you something about what we think we know about history. reply eszed 12 hours agoprevWhat a cool story! I just bought the book referred to in the article. Won&#x27;t link, but search for \"Bazooka Charlie\" on an online book seller site. I&#x27;ll give it to my dad when I finish it. reply WalterBright 14 hours agoprevWhat a great story! Glad it has been preserved. reply tetris11 9 hours agoprevit&#x27;s a really nice story, though the last few paragraphs really read like it was finished off with ChatGPT. Yes, yes, please don&#x27;t lost such things, it detracts from the discussion... reply bvan 10 hours agoprev [–] Such a neat story. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The daughter of WWII hero Major Charles Carpenter, known as \"Bazooka Charlie,\" provides evidence through letters, photos, and documents to support her father's heroic actions during the war, after doubts were raised by war enthusiasts.",
      "Her story catches the attention of a retired mechanical engineer who encourages her to write a book, leading to the recovery and restoration of her father's plane, which is now on display in a museum.",
      "The book also explores the impact of the war on her father's health and the sacrifices made by her mother, garnering appreciation from veterans for her open sharing of her family's story."
    ],
    "commentSummary": [
      "The daughter of \"Bazooka Charlie,\" a WWII hero, defends her father's heroic actions after they were called into question.",
      "The story emphasizes the significance of witness testimonies and evidence in validating extraordinary wartime feats.",
      "The discussion also covers topics like gun cameras on aircraft, the role of divers in verifying enemy vessel sinkings, and the experiences of war veterans dealing with PTSD."
    ],
    "points": 112,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1704046396
  },
  {
    "id": 38825520,
    "title": "How to Build a Small Social Network: Emphasizing Community and User Control",
    "originLink": "https://runyourown.social/",
    "originBody": "How to run a small social network site for your friends Since August 2018 I have run a social network site called Friend Camp for about 50 of my friends. I think Friend Camp is a really nice place, and my friends seem to agree that it has enriched our lives. I'd like to see more places like Friend Camp on the internet, and this document is my attempt to provide some practical guidance as to how you might run a social network site like this. Friend Camp is an intentionally small social network so the home page doesn't look like much if you don't have a login. You can read some of our policies if you like. Who this guide is for If you're tired of Facebook or Twitter or wherever else and have thought that there's got to be a better way, this is for you. If you currently run a social network server for people besides just you, using software Mastodon or Pleroma or whatever else, this is for you. If you have some programming experience, this is for you. If you have no programming experience, this is for you. Social solutions to social problems This document exists to lay out some general principles of running a small social network site that have worked for me. These principles are related to community building more than they are related to specific technologies. This is because the big problems with social network sites are not technical: the problems are social problems related to things like policy, values, and power. There are still some areas where technical progress is needed, and one section of this document discusses some of those areas. Running a social network site is community building first and a technical task second. And while community building is hard work, it's often worth it. This is my pitch to you: using big social media sites is easy, but you pay a steep price for it. You should consider running your own site, which is harder, but can be extremely rewarding. For those of you familiar with the term, I'm discussing federated social media. I find that term nearly useless though and have tried to keep discussion of federation itself to asides like this one. What makes a small social network site different from a big one? Friend Camp looks like this on a desktop web browser: Text obscured to protect the innocent. And like this on phones: The phone view of Friend Camp. It's fairly Twitter-like at first blush: you post short messages, you can \"@\" individual users, you can follow and be followed, you can use hashtags, and you can boost things that you want your followers to see. While Friend Camp is small, the universe of content we have access to is big. We are connected to about 5,000 other communities on the web, allowing us to interact with hundreds of thousands of users if we want. Friend Camp is a modification of a program called Mastodon—I'll get into the specifics later, but I want to say right away that I did not create 99.999% of the software and I'm grateful for the work of the Mastodon authors and contributors. So far, this seems like an underpowered Twitter, but there are some key differences from Twitter: you control the computer that runs the site you can modify the software that powers the site you get to make the rules and policies Next I'll explain each of these in detail. You control the computer that runs the site Big social media sites all run on computers, usually a big complex network of computers. When you type \"facebook.com\" into your browser or load up your Facebook app, what happens is that your phone sends a message to a computer owned by Facebook, which then coordinates with a bunch of other computers owned by Facebook to collect all the information that is then displayed to you. In the context of Facebook, this means that everything you do on Facebook passes through computers that they control. And then they take the information about what you do and sell it to advertisers, directly or indirectly. But a small social network site doesn't need a huge complex network of computers. One computer can be enough. Often it's the kind of thing you can rent for $10 a month, or even run at home on an old computer you have lying around if you want. For example, Friend Camp costs about $30 a month to run, it experiences about 10 minutes of down time each week, and it takes about 2 hours a week of my own time to maintain. I maintain the computer that Friend Camp's software runs on. I'm training another camper to be an administrator as well, so if something happens to me there is still someone who can make sure the site keeps going. The site is funded by campers who can afford to throw a few bucks a month at a Patreon that I run just for us. It's a small price to pay for a nice place to talk to friends on the internet. As a result of these economic realities, Friend Camp does not have the annoyances of big social network sites. We sell no data, we collect no extraneous data, there are no advertisements at all, and no major features get changed unless I talk to the campers about it first. Another result of these economic realities is that we're never going to grow much bigger than 50 active users. But that's okay, because it keeps costs low and we can still talk to people outside of our server. You might ask: how can one small computer replace something as huge as Facebook in my life? Well, the reason is that you control this one computer for your site, and then that computer talks to thousands of other computers, which gives you access to literally millions of people and organizations and videos and all sorts of other things. So what this does is replace thousands of computers owned by a single company with thousands of computers owned by thousands of different people. You can modify the software You don't have access to the code and the algorithms that Twitter uses to run their service. If I wanted Twitter to make it so that the word \"tomato\" always appears in bright red, I would have to submit a request to Twitter, and then they would laugh at me, and that would be the end of it. But on Friend Camp, I have the ability to change our code to do exactly that. You can also hire someone to change your code if you can't do it yourself. Or encourage someone in your community to learn to do it. It involves work, but the point is that it's possible, whereas it's impossible to have access to the Twitter code unless you are an employee of Twitter. A silly example of this kind of rule in action is Dolphin Town, a fully functioning social network site that I started a couple years ago. Like Friend Camp, it's a modification of Mastodon. It behaves just like Mastodon with one important change: the only letter you are allowed to use is \"E\". Dolphin Town in action. Dolphin Town is based on the great oulipo.social, a social network where you can't say the letter \"e\". The reason I can make these modifications is that Mastodon is open source. There are a lot of things that open source software enables, but for our purposes the important thing is that you can open up the software, tinker with it, and put it back together in its newly modified state. I made Dolphin Town to draw attention to something that I think a lot of people miss: if you run a social network based on open source software, you can fundamentally alter how it works to your liking, at least for the version that runs on the computer you own. You get to make the social rules and policies On a big site like Facebook, if you encounter content that you think shouldn't be there, you need to ask Facebook to take it down. On a small site serving a small community, if the content comes from inside your community, then you can navigate it the way you navigate problems in small communities in the real world. You bring up with the rest of your community that someone is behaving in a way that is counter to the values of the community. It's unpleasant, but at least you have some control over that process, and it's something tangible that you can navigate. It's more work than simply letting an army of underpaid and traumatized moderators handle all of that for you. But it's worth it. (You might sense \"it's hard work but it's worth it\" as an ongoing theme.) Wait, who is \"you\"? When I refer to \"you\" here I mean \"your community\" and \"your decision making process\". Some communities may have a single person who is kind of the \"benevolent dictator\". Other communities may run based on consensus or even more formal processes. Some servers are formal cooperatives that use cooperative decision making tools like Loomio to organize their node, which has its own advantages and drawbacks that I won't go into the details of here. Tools like Loomio are important when your group gets to a size that is too big to manage informally. This is why I am very much in favor of keeping these sites really small, like way less than 100 people. It allows decision making to stay informal and means that running the site can be a hobby or a low-intensity community project instead of a full time job. And of course nothing is going to stop a corporation from running one of these sites either! But I don't recommend joining a corporate site; these things will eventually be driven by profit motive to replicate the same bad experience you get on the current social network sites that you hate. Why run a small social network site? The main reason to run a small social network site is that you can create an online environment tailored to the needs of your community in a way that a big corporation like Facebook or Twitter never could. Yes, you can always start a Facebook Group for your community and moderate that how you like, but only within certain bounds set by Facebook. If you (or your community) run the whole site, then you are ultimately the boss of what goes on. It is harder work than letting Facebook or Twitter or Slack or Basecamp or whoever else take care of everything, but I believe it's worth it. Let's go back to Friend Camp. While there are a hundred thousand people we can talk to from Friend Camp, there are only about 50 people with an active Friend Camp login. We call ourselves \"campers\" because we are corny like that. And campers have a special communication channel that lets us post messages that only other campers can see. If I make software that makes the lives of 50 people much nicer, and it makes 0 people more miserable, then on the balance I think I'm doing better than a lot of programmers in the world. Because we're mostly all friends with each other, this extra communications mode is kind of like a group chat on steroids. For our community it ends up being a sort of hybrid between Twitter and a group chat. As a result of having a community layer alongside a more public layer, we have a movie night, a book club, and a postcard club. Campers visit each other when we travel, even if we've never met in person before. We correspond with each other about what we're making for dinner and trade recipes. They're the kind of mundane interactions that you probably don't want to have with perfect strangers but you cherish in a group of people you care about. We are also able to have moderation rules that are hyper-specific to our own values as a community. It lets us maintain an environment that's far more pleasant than you find on most social media sites. And I can make custom features at the request of the people on the site. So if someone wants a particular kind of visual theme, or content filter, or a button to do something that the software can't do right now, I can make it happen, at least within the limits of my own ability. Software can be complicated where necessary There are certain things that are notoriously subtle and complex to communicate to users passively via a user interface. A classic example is security settings: Facebook, for example, offers a lot of decent security options but it can be overwhelming to many users. Of course, by having reasonable defaults you can get past this problem, but any default setting is at best going to work for some but not all users. For example, Mastodon has four levels of privacy available for your posts, ranging from \"a private message to another person\" to \"anyone on the internet can see this\". According to some discussions I've seen, one reason that Mastodon has been reluctant to add a \"local-only\" posting option to its service is that it multiplies that to a combination of eight total privacy settings. The idea is that it would confuse people and they would potentially use it wrong, which could lead to bad security breaches due to user error in dealing with a complex design. I agree with this in a situation where a user tries out Mastodon and has to learn to use it themselves. But in a case like Friend Camp, I can personally walk the user through what these complex features mean. I can tailor the instruction to their personal learning style and to their needs. Plus, I'm available to answer questions about it whenever they come up. I'll discuss this more below in the section \"What you can do today\". All that said, there's no need for overly complicated software. It should be as simple as possible. But in cases like security and privacy I think the ability to granularly control your settings outweighs factors like ease of use. You can have hyper-specific norms Because of their need to have as many users as possible, big social network sites have to try to be everything to everyone. In practice this means they need to limit the number of people they alienate, which means they have to be very careful about what kinds of actions and speech they ban on their network. Every person they ban from their network is another set of eyeballs that could otherwise be looking at advertisements and making them money. In the end, this means everyone is going to be unhappy with the policies of a big site. You can see it right now on Twitter and Facebook: people on the left complain that these sites support right wing extremism, and people on the right complain that these sites support left wing extremism. And for a website with thousands of posts a second from hundreds of millions of potential sources that's aiming to please as many people as possible, both of these viewpoints are absolutely going to be true. There is no way to moderate this effectively, and you can never come up with rules for behavior that make millions of people happy. But on a small social network site, you can set hyper-specific norms. For example, Mastodon allows for \"content warnings\", where you hide content behind a text tag that you define for your post. It's often things like \"nsfw\" or \"mental health discussion\". This acts as a warning so people who don't want to read about that kind of stuff won't click through to see it. On Friend Camp, we generally tag anything related to United States politics as \"uspol\" — we didn't invent this tag, and lots of other servers use it too. I couldn't ever imagine people tagging politics tweets on Twitter, since Twitter styles itself as a news network, for the broadest possible interpretation of the word \"news\". You also don't need to implement some kind of draconian filtering system to have norms: you simply model the behavior you want to see, and remind people to act a certain way when they post outside of those norms. This kind of thing does not work well in a forum of even 200 people, but in the 50-ish range it is entirely doable. Your small social network site can have its own rules about, for example: what speech is acceptable what actions are considered violence what actions are considered protected speech or expression This will be very different for every site and I think that's part of the beauty of this whole thing. I talk more about the specifics of this in the \"What you can do today\" section below. Why to NOT run a small social network site There are many reasons not to run a small social network site. As I've mentioned before and will continue to mention, it's really hard work. You need to at least have time for a hobby or figure out a way to get community support for doing it (maybe people chip in to keep things running; maybe you can trade in-kind services with your community members; etc.). If you choose to run a site like this, it means that people will now depend on you for something that's important to them. People on Friend Camp are very forgiving when we have down time, and sometimes that down time can last hours if I'm asleep or at a movie or otherwise busy. Later in this document I discuss some cases where people had to abandon a site because it got too big and unwieldy to run. Friend Campers are, of course, incredibly patient and gracious and understanding that I'm a person with a life. But none of this is unique to running a social network site. I think all of this stuff applies if you take away the internet and computers entirely. This is exactly the cost of starting a community theater group, activist group, social group, sports club, book club, and so on. Even the bit about \"down time\" applies if you run a physical space like a theater. You can't be there for every performance but if something goes wrong with the building, you'd better have a plan to deal with it. In other words, the costs of running a small social network site end up being more or less the costs of starting, well, anything involving other people. What you can do today This section describes practices that anyone running Mastodon or similar software can do today to make for a cohesive, rewarding, and caring community. This section lays out some rules that have helped make Friend Camp a good place. The high level overview is: keep the number of users on your server very small remember that your job is social first and technical second provide custom onboarding provide group activities provide custom features if you can enforce your code of conduct Keep it small It is absolutely necessary to have no more than 50 to 100 active users who log in to your online community. This is a number I am pulling out of my ass. And eagle-eyed readers will notice that I'm not even settling on a single number here. I know that 50 is a number that works as a cap, and I think anything more than 100 definitely does not work as a cap. In my admittedly limited experience as a human being on this planet, you are not going to be able to form and maintain a cohesive group of more than about 50 to 100 people who all basically agree on values and moderation rules and that sort of stuff. Even as a group where people are mutually looking out for each other, you are not going to be able to meaningfully make sure everyone in that group's wellbeing is maintained. I don't have science to back this up. I have seen this happen in startups, in activist groups, and in internet forums. In the world of startups this is sometimes referred to as the problem of keeping your culture as you grow. I often hear people lament, \"I miss when our company was just ten/twenty/fifty people.\" I can't stress enough how important it is to keep your numbers small. To draw from recent history, witches.town was one of the more popular Mastodon servers in 2017 and 2018. In mid-2018 there were a series of disagreements between the primary administrator and the users. I can't speak to the nature of these disagreements as I wasn't there but there were multiple disagreements about moderation policy, and also the primary administrator (who I believe actually owned the server and its domain) said they were burned out on running the server. In the end, the site was shut down. According to archive.org it had about 2400 registered accounts in April 2018, shortly before the instance was deleted. If we look at instances of similar size today we can extrapolate that there were perhaps 500 active users on the server at the time it went dark. KNZK is another network site that shut down on June 30th 2019 and it had about 3100 registered accounts and 560 active users. It shut down due to maintenance tasks demanding too much time and effort from administrators. I posit that 500 active, invested community members will not be able to achieve a values-based harmony or consensus. It's simply too big to be possible. My assertion is not backed up by any studies I have read but rather my personal experience in online and offline groups of all kinds. You cannot wrangle consensus from 500 people. With that many active, committed community members you will necessarily have at least a few dedicated members who feel investment and ownership in the community who are also extremely unhappy with the direction of the community. People like to bring up Dunbar's number when I talk about this stuff, and I have no idea if that concept is a crock of bullshit or what. For starters the number itself, 150 people, seems a bit on the high side to me intuitively. But if you believe in Dunbar's number then that does lend some credence to this idea. In addition to promoting group cohesion, having a small number of active users means moderation is completely achievable by a single person. With 50 active users I field, on average, one moderation request a month. This is in addition to me being vigilant and blocking bad actors as I see them. Now, this wouldn't be true if I had 50 random people who all signed up. But these are 50 users who are screened and onboarded into our community. Even tricky matters like delineating what is and is not acceptable speech become easier when you're dealing with a small community. For example, there may be some inalienable right for expression of disagreeable political speech, but if it is speech that all the people on your corner of the network agree you don't want to see, you can simply ban it internally and filter or block it externally. And when your community is small, you can use powerful tools that would be irresponsible to wield in a larger community. For example, when I discover a user on someone else's site engaging in speech that violates our local norms, I will often mute or ban the entire server that hosts the user in question on behalf of all the people on Friend Camp. This would be inadvisable on a huge network like Twitter, but on Friend Camp, we have all agreed we don't want to see certain things and we don't want to engage with other servers that allow for those types of speech. Of course, the question is how does this scale beyond those 50 people? Well, that is why I'm writing this guide. I think there should be thousands of these small servers talking to each other. You are the party host Running a small social network is like hosting a party. It requires social intelligence, empathy, and yes, technical skills. You might wonder what aspect of hosting a party is technical. I mean \"technical\" in the sense of \"requiring technique and skill\". Party hosts need all sorts of technical skills: managing RSVPs and calendars, planning and purchasing supplies, cooking and serving, wrangling a music playlist, etc. Not everyone is comfortable hosting a party because these are technical barriers to entry. Similarly, not everyone is comfortable running social network software because of technical barriers to entry. But as with hosting a party, technical skills aren't enough. You need to have social skills, or have someone assisting you who does. For example, I need to maintain an understanding of everyone on the server. I have to remember what their individual needs are, which means that I need to be able to hold all of that information in my head. It's not a task that everyone is suited to, but it's critically important. Similarly, programming computers is an important task that not everyone is suited to do. The big difference is that when it comes to anything dealing with software we tend to overestimate the importance of programmers and underestimate the importance of everyone else. Among other things, my duty is to read the local timeline and catch up on everyone's posts. I need to take the temperature of the network and also provide social lubrication where necessary. (\"Hey, you just mentioned you're traveling to Montreal! So-and-so lives there, maybe you should message them and see if they want to meet up.\") Catching up on all local posts would take forever with hundreds of active users, but with dozens, I only have to read the timeline for 20 minutes a day. Do not fool yourself into thinking that your job as an administrator is primarily technical. It's social first, and technical second. If you want to only focus on the tech, then please find someone who cares deeply about the social organizing side and recruit them as a co-administrator. Provide a custom introduction to your network for every user Because you only have 50 people in your community, you are able to do all sorts of things that would never be possible in a larger group. One really important place where this comes into play is in new user onboarding. When a new user joins Friend Camp, I now schedule a video chat with them (or an in-person chat if they are local). For one to three hours, I introduce them to Friend Camp and walk them through our basic features. I customize the content of the chat with them depending on their background. Generally we discuss the following topics: This does not have to be a video or in person thing. I think video works for many, but there are certainly all sorts of people who can't do that kind of interaction for all sorts of reasons. My recommendation ultimately comes down to: pick a form of onboarding with a personal touch, where you can convey a lot of information and answer questions in a way where the person you're onboarding feels comfortable asking questions. the basics of using Mastodon, tailored to the individual's needs how to find further help if they need it who everyone on the server is (I go down the list and explain in one sentence who every non-private user is, kind of like a party host might introduce guests to one another) our custom software features our group traditions and how to participate in them (movie night, for example) If I know them to be social media power users, I'll spend more time on those kinds of features. If they are not very technical and need more basic training, then I spend more time on that. I'll ask them what kind of phone they use, and give them specific information about iOS or Android as needed. In addition to explaining who everyone else on the server is, I point out specific people who I think they might get along with. Again, it's like being a party host. If you know people in common or have common interests, I will mention that. This is one of those things that a small social network site can provide that a big site could never provide, because it just doesn't scale. Group activities It's important to do things together as a group. Organize whatever makes sense for your community, and allow your community to organize as well. For example, on Friend Camp we have: movie night, book club, postcard club. Not everyone participates in these things, and that's okay too. But the option is there. The people on your site will probably want different activites than these specific ones, but the important thing about this is we are connecting through some kind of shared activity. Keep it interconnected The ultimate goal of the last three sections (hosting a party, custom onboarding, group activities) is to socialize everyone on your server with everyone else on your server. Not everyone has to get along with or even interact with everyone else, but the more positive connections you build between people on your server the better a place it will be. Having a tight-knit group where lots of people know each other is key to the principle of maintaining hyper-specific norms. An interconnected network also helps with conflict resolution, so when things go \"wrong\" it's hardly noticeable. As one camper put it to me: \"Tiny scale conflict resolves between people without formal moderation. Alice says something that Bob didn't like, Bob points it out, Alice agrees and apologizes, and everyone moves on.\" That kind of conflict resolution doesn't easily happen between strangers, even in a group where people share values with one another. But when the chance of any two people being complete strangers is close to zero, this happens way more frequently. Funding Unfortunately, running a social network node requires you to move electrons around on pieces of metal, which means you need metal and electricity. And to have access to those resources, you need some kind of funding. Either you have a benefactor, you get together as a group and buy some equipment and internet service and pay for repairs, or pay a company like Linode some money to host a server for you. We have a Patreon for Friend Camp simply because it's the easiest way for me to collect monthly donations. Campers contribute what they want or what they can. I needed to upgrade our server at some point for various reasons and I made an appeal to the community. I ended up getting the monthly pledges to cover the new server costs for everyone. The monetary costs of running Friend Camp are laid out in this spreadsheet. At the moment, Friend Camp costs $31.00 a month to run. This is not a small cost for a single person, but spread between 50 people it can be very affordable, more like $1 per month per person even if half the active users can't afford to contribute anything. The bus problem For almost a year, Friend Camp had one person with the keys to the kingdom: me. In this first year, if I got hit by a bus, the server would probably run until its first crash and then never come back again. I decided to put out the word to see if a member of the community would be able to volunteer to be co-administrator. Thankfully, one of our own decided to take on the responsibility. I was lucky that there was a person in the community willing to do this and with development experience and experience running a server. It took just a handful of video calls to get them up to speed and within a month they had pushed their first production upgrade to Friend Camp. Not every community is going to have people who are willing and able to take on these duties. But if you have someone willing, there could be a longer mentoring process to get them to the point where they are able to do at least minimal work to maintain the server in the absence of the main administrator. If someone is at least able to reboot the server if things go weird, that's a huge step up from nothing. Ideally you can train a willing person to the point where they can do more complicated administrative tasks. If you have a small social network community that actually really means something to you, it's work that helps keep a thing alive that people deeply care about. Code of Conduct A code of conduct (CoC) is about what actions are acceptable on a network, which is absolutely necessary but not sufficient on its own for establishing a culture. A CoC perhaps implies certain values but it mostly states what is NOT acceptable rather than what IS encouraged. Some codes of conduct do state things like \"please engage with people courteously\" etc. I think a good CoC should be specific enough that it is actively repulsive to some people. And frankly, the more people who are repulsed by it the better. Perhaps simple anti-racist sentiment is enough to repulse a small number of explicitly white supremacist people, but if you add in sentiment like \"no TERFs allowed\", suddenly a lot of people who consider themselves leftists will also decide not to join. As an example, Friend Camp is anti-free-speech, at least in the sense that freedom of speech is commonly understood as a value. This is repulsive to some people on both the left and the right, and it's important that people with that core value find somewhere other than Friend Camp to set up their online home. For those wondering, the RationalWiki definition of freedom of speech is exactly the kind of freedom of speech we take issue with on our server. Again, you may find this abhorrent, but you don't have to join our server, and we're never going to house more than a few dozen people. Again this is a good thing. I remember one code of conduct I read for a chat forum where they stated that \"before posting here, ask yourself if what you are saying is necessary for the health of the discourse\". And I saw that and thought, \"Wow, I would never feel comfortable posting on that forum because I am always doubting the necessity of what I have to say.\" This is good, though. The forum clearly signaled that they wanted a certain kind of person, one who is confident in the necessity of their speech. And that is not me, and I was able to steer clear. Did I feel excluded? Sure. But ultimately that feeling of pre-emptive exclusion is better than joining a community and THEN realizing that I don't belong there. I believe this rule was based on \"The Three Gates of Speech\", which is an idea that has been around for a long time that I think has some serious problems in practice as rules for engagement. Enforcement Valerie Aurora and Mary Gardiner have published a free book called How to Respond to Code of Conduct Reports that I think is required reading for anyone with a code of conduct on their site. The book explains what a CoC is and what it is not and how to make yours actually effective. Some of their recommendations make more sense for very large communities (like conferences of thousands of people) than the small ones I'm advocating for, but most of their recommendations do apply, and the book comes with many real world examples that you can learn from. One reason I'm a big fan of adding a local-only communication layer to these social network sites is that it helps with enforcement. The reason is that it creates serious social repercussions for violating rules to the point where you're kicked off of a server. This is because the person who was kicked out cannot just join a new server and re-follow everyone and continue to participate in or monitor conversations. It's more like being kicked from a group chat. It can be socially devastating, which means there are real consequences to violating group policy. At the moment, Mastodon does not support a local-only communication layer, though modifications like Hometown do. I'm sure there will be friction on Friend Camp but nobody has, to my knowledge, broken our code of conduct or otherwise required admonishment or disciplinary action. When that does happen and it is beyond a simple warning followed by an apology, I will individually solicit opinions from literally every active user on what should be done. Because we keep the server small, I could in theory host video calls with every single user where we discuss what ought to be done. I'm not sure if that last one is a good idea but I have that option, which you can't say for Facebook. This is all possible because we are a small group of people and we will never be a big group of people. Technical recommendations This section might not make a lot of sense if you're not a programmer. Feel free to skip it and move to \"What we need to do in the future\" if that's not you. Ideally you will pick social media software that has features which allow for communication that only your users on your server can see. In the case of Friend Camp, we maintain Friend Camp as a fork of Mastodon and pull in Renato Lond's great work that allows for posts that only the 50 people with logins to Friend Camp can see. I would go so far as to say this is a necessary feature for group cohesion, and my hope is that implementors of decentralized social media software come to understand it's important. If you are a programmer, I suggest that you maintain a fork of whatever open source social network software you end up using so that you can implement custom features for your users. You might end up making purely aesthetic changes, small browser hacks, or complicated new features that you need to maintain version to version. If you implement custom features that your users ask for, it gives them a stronger sense of ownership over the site even if they are not the ones writing the software. Make sure to credit them somewhere too! I try to credit individual users for each suggested feature in the commit messages and also on our server's About page. Provide multiple services for your community members! Probably not email because that is a nightmare but there are other possibilities. I use Mastodon's login to authenticate small helper applications that I write for my users. I just put in a hard coded gateway so that only people from my server have access to them (so their OAuth login account has to originate from the server friend.camp). This means the basic social media login for your server is also a login for many other services; kind of like a default LDAP but not. Examples of services you can provide I only have one auxilary service I provide my users that gets active use but I think it's a big one. Bugle is a bot that any Friend Camper can use that sends a direct message to every other person on the server (unless they have blocked it or have DMs otherwise disabled). This would normally be a horrible idea on a site like Twitter. But because we're a small community and we all know each other, I trust that my users will use it well and sparingly. If it is misused I will give the user a stern talking-to. A stern talking-to is a laughable deterrent for most sites but it works for us because we are so tight knit. What we need to do in the future The current situation for small social network sites is not even close to perfect. There is work that needs to be done to make this idea better and more viable for more kinds of people. Places where we need better tech Most of the problems that exist are social problems with social solutions, and I've tried to lay some of those out above. But there are still unsolved problems where better tech could really help, so I'll try to enumerate those here. Fluidity of identity and the ability to migrate The existence of a server and an administrator implies some local form of centralization. I think this is necessary because most people don't want to run their own network node, and there are fantastic benefits to having a trusted local administrator. That said, the drawbacks are also great and we need to be able to mitigate the drawbacks. What happens if I, as the administrator, violate the social norms of my own community? In other words, what happens if the person with all the real power is the problem? People need to be able to jump ship and migrate their accounts, seamlessly and wholly, to other servers. We do not have a good technical solution for this yet. In my opinion it is the one big area regarding federated social networks where we need to work on technical solutions. If I decide that my values no longer align with a server, I need to be able to take that account to a new server. If the server decides to kick me off, that's fine, but I should in theory be able to set up shop elsewhere. I know that people have been working on this issue, but as far as I'm aware it's nowhere near resolved. As it stands right now, the fact that you can't uproot and take your stuff with you from one site to another means there is a very real kind of lock-in. This helps with enforcement (as I said above) but is a huge liability when it's simply the choice of the individual to move. Let people keep things in the community Most open source social network software right now is designed to get your messages out to as many people as possible. We need support for private communities. For federated social networks this means support for messages that don't federate and can only be viewed by people with access to the server on which the message was posted. Unlike the identity problem, this is a very easy thing to implement technically. It already exists on a minority of open source social network servers. But the big players don't support it and that's causing more harm than good. There is currently a pull request open on Mastodon by Renato Lond that implements exactly this feature. It's derived from glitch-soc, a Mastodon fork. Server forking should be easy It should be easy for, say, 25 members of Friend Camp to pick up and start \"Friend Camp 2\". This could be because Friend Camp is getting too big, or it could be because these people don't like what Friend Camp has become and would like to move en masse. As far as I'm aware there is no work at all being done on this issue, but I suppose a prerequisite to this is a solution to the identity migration problem above. Lean software that doesn't have to scale What if we built software to run on very low spec, low power machines, that was a federated social media server for 50 people but could never grow to support more than that? You could use something like SQLite instead of a \"real\" database because you'd never realistically have to support a lot of writes to the DB. You could run on a raspberry pi. There are some federated servers that fit this bill already. Pleroma is a Twitter style server that extremely light on its use of resources. And Write Freely, a federated Medium-style blogging server, is incredibly lean as well. But even Pleroma and Write Freely are built with thousands of users in mind. What kind of low tech solutions can we enable if we keep our communities intentionally small? This may open up more paths to equitable access by communities with different resources available to them. More on scale Any time I propose a new piece of software to a group of software engineers I'm asked the same question: how will it scale? We are trained as a group to ask this question. I think it's the software equivalent of in manufacturing when someone asks \"What will it cost to produce?\" Since the marginal cost of producing software is effectively zero, it's the scale, the ability for the software to be used by millions or billions of people, that becomes the limiting factor that everyone brings up. Imagine two different software developers. One person writes a piece of software that makes the lives of one million people slightly easier. Maybe it's better routing for navigation software and it shaves 30 seconds off the commute of a million people. Another person writes a piece of software that only ten people ever use, but it tangibly changes their lives for the better in very material ways; maybe they learn a trade that becomes a career. One of these outcomes is not necessarily better than the other, and yet due to myriad factors, only the software with a million users is likely to get funding from entities—whether the context is for profit or not for profit. I'd like to advance the notion that software does not have to scale, and in fact software can be better if it is not built to scale. I hope some of the examples I've given above have illustrated what is possible when software is used by a small number of people instead of a large number of people. Beyond local and public: the neighborhood Right now on federated social networks you have a concept of people on your own \"home\" server (which I'll call local) and people on every other server in the world (which I'll call public). But we need concepts that are more fine grained than local and public. I would like to see groups of servers that band together through a kind of mutual approval system. The group of people on Server A decide that Server B is to be trusted, and vice versa. They approve each other, in a manner similar to friending someone on Facebook, but for a whole server instead of an individual user. Now the 50 people on server A and the 50 people on server B are in a \"neighborhood\" together. Servers that belong to the same neighborhood could share things with each other. For example, the servers could share access to posts that are tagged on either server as \"available to the neighborhood\". Servers could share block lists, since mutual trust between servers probably implies some level of shared values between the people on both servers. A neighborhood would necessarily consist of two or more servers, and could in theory grow to be very large. However, I don't think neighborhoods should be very big themselves. I think there should be a kind of mutual decision-making, so perhaps now that A and B are connected, both A and B have to agree that C is worth connecting to, and C has to agree that A and B are worth it. This makes every extra node you add more difficult, which is the point. Big \"private\" communities should be very very hard to come by — and \"public\" should be where the \"broad\" conversations happen. I also think there shouldn't be concepts of overlapping Venn diagrams of multiple neighborhoods. I loved the concept from Google Plus of circles. The idea was that as a person you have friends, coworkers, college friends, family, IRL neighbors, etc. People you \"know\" on Google Plus could belong to none, one, or more than one of these groups. A childhood friend who you also work with would probably belong to \"friends\", \"childhood friends\", and \"coworkers\". This system sounded great to me… until I tried it in practice. And in practice it was so much tedious digital paperwork to keep it all fresh and updated. I didn't want to manage people into one or more of dozens of categories and in the end I just went for the default \"mutual friends on the social network\" and \"public\" circles. I think many of us can hold three of these groups in our head though, especially if it's not the job of the individual user to constantly maintain it. I'm sure lots of people can hold more than three groups in their head at a time, but I'm picking the smallest useful number in order to reduce the confusion I experienced with Google's \"circles\" feature. So for me an ideal network would be partitioned into three basic levels for posting: local - just the people on my physical server neighborhood - the concept I described above of people who belong to servers that have a mutual trust agreement public - everyone in the world This would be in addition to the usual layers of \"followers only\" and that kind of thing. One way I see this breaking down is: local - tin foil hat stuff, \"I do not want this message to leave this piece of metal\", or perhaps the very intimate like \"only local mutual followers get to see my nudes\" neighborhood - most of your friendly chatter and ideological debate happens here public - the place for things that are inconsequential (cute cats and jokes), or require signal boosting, or where you really want to put an idea in front of the public Conclusion There are a lot of unanswered questions here, and in a lot of ways I am still learning as I go. I don't want to give the impression that I have the answer to everything so I'd like to address some questions that I don't have good anwers to at all. Hard questions How do people join one of these networks if they don't have friends who are already on such a network? This may be where sites with thousands of users come into play. Perhaps people join a giant server but then do it with the understanding that they are there to make friends on smaller servers with the idea of eventually moving there or even starting their own server. This is not very feasible until the account migration stuff I mention in the \"What we need to do in the future\" section is solved. What happens to people who suck? Like what about people who are actively opposed to community cohesion? Will they find a sort of anti-community where they are accepted? Will they simply migrate to the giant networks of thousands or hundreds of thousands of people? Will they set up their own single-person networks? Will this kind of network even be the place for them at all? I don't know. What about equity? For example, how do we get these servers run and operated by people in poor communities? I look to work by people like Bruna Zanolli, going in to Amazon indigenous communities and training the women to run the networks. If you have the privilege to run one of these sites then you might also have the ability to teach people how to run them for themselves. If that's you, then it's really your duty to get other people up and running. The more communities you help get started, the more options your own community will have for people to connect to. I also would love to see granting organizations support this kind of work. Do the technical administrators have to be the same people doing the social organizing? I think the answer as of June 2019 is, sadly, yes. If you have 2 people with root access to the server and 2 people managing the community aspects, you'll end up with imbalances in that group of 4. You will end up with technical administrators who feel like code monkeys who never get the gratitude that the community organizers get, or you'll end up with community organizers who feel like glorified babysitters while the techies have all the real power. You might even end up with a situation where both are true. I think that if you're dedicated to this sort of project though, you could start with something like that 2 and 2, and then the techies could teach the organizers the technical skills, and the organizers could teach the techies the organizing skills. However, if more paid hosting services would support a wider variety of software and forks and features, this reliance on technical people like me will hopefully become lessened and an actual community organizer, or some kind of community organization structure, can be in charge. So what do you, the reader, do next? What you can do next depends on who you are. If you are comfortable with servers and hosting If you've done the system administrator thing, you have many options here. Ask your friends if they'd like to try out a social network run by you. If you can find 5 people who would be interested that's enough. Then get going! There are plenty of technical guides out there: Setting up a Mastodon server for a Twitter-like experience This also works with the various forks, like Glitch Social and Florence I now offer a fork of Mastodon that supports principles outlined in this document called Hometown Setting up a Pleroma server for another Twitter-like experience Setting up Pixelfed for an Instagram-style experience Setting up PeerTube for a YouTube-style experience There are many other servers and services out there as well, but these are the ones that I personally have used and can recommend. Keep in mind that none of these services support any of the \"future\" concepts like neighborhoods. Some of the forks like Glitch Social do allow for local-only posting. If learning the tech stuff is realistically not going to happen Do you have any technical friends? If you do, ask them if they might be willing to do this sort of thing. You can also pay a company like Masto.host to run all the technical stuff for you. If you take my advice and only have a small number of users, then you can probably get away with paying less than $10/month for your hosting. Paying a company necessarily means that you have less control over your system. You won't be able to provide custom features for your users, but if you are not a technical person you probably aren't going to be writing custom features anyway. I have no business relationship with Masto.host but I have spoken to them about the possibility of getting modified versions of Mastodon that include things like local-only posting onto their hosting service. I can't promise anything on this front except that I am working toward it. Running a small social network site is completely possible to do on the side I suppose I'll repeat what I said multiple times in this document, which is that running a small social network site for your friends is hard work, but it's worth it. It is first and foremost the work of community building, and only secondarily is it a technical endeavor. And it's completely possible to do, today, though depending on who you are and what your resources are it's going to be difficult in different ways. Acknowledgments First and foremost I'd like to thank the Mozilla Foundation and the Ford Foundation for the Mozilla Fellowship that's kept me funded for the last ten months. I would not have been able to write this document without dedicated time to think about these problems. I especially want to thank the Mozilla Fellows in my cohort, who have been excellent listeners and collaborators for these past ten months. Code for Science & Society, who are the best possible organization to be embedded with for the work I'm doing. The people at ScuttleCamp and Eyeo Festival, who provided me with thought-provoking conversation and gave me a semi-public forum to test out some of these ideas. The various fediverse and ActivityPub developers who have given their time to answer all sorts of questions I've had. And everyone who's met me in the last ten months and let me talk their ear off about this stuff. I also want to thank the folks at Ink & Switch for publishing their local-first software article, which got me thinking, \"hey, I could write an article too.\" Thank you to Emma Winston for designing the HTML and CSS so that it actually looks good. A special thank you to my husband for being incredibly generous and patient as I've flown around the world talking to people about this stuff. And last but not least, thank you to the Friend Campers, without whom none of this would be happening. I couldn't ask for a better petri dish. About the author Darius Kazemi is a 2018-2019 Mozilla Fellow who lives in Portland, Oregon, USA. He cofounded Feel Train, a creative technology cooperative. He makes weird internet art, writes open source software, gives talks, wrote a book about niche videogame history, is very serious about karaoke, and is blogging about the first 365 IETF Request For Comments (RFC) documents. You can support his work on Patreon, which includes guidebooks like this one, software that implements these ideas, fun internet toys, and more. License This work is licensed under a Creative Commons Attribution 4.0 International License. You can share, copy, remix, and/or adapt this for commercial or non-commercial purposes as long as you credit the author and indicate if changes were made. Please credit \"Darius Kazemi\" with a link to \"https://runyourown.social\". Older versions You are currently viewing version 1.4 of this page, last updated April 27, 2022. Version 1.4: fixed a broken link to Mastodon server installation guide. April 27, 2022. Version 1.3: added information about my Patreon. August 31, 2019. Version 1.2: added information about the Hometown fork. July 26, 2019. Version 1.1: added an aside about video onboarding. July 9, 2019. Version 1.0: original publication. July 8, 2019.",
    "commentLink": "https://news.ycombinator.com/item?id=38825520",
    "commentBody": "Hacker NewsHacker NewsSorry, we're not able to serve your requests this quickly.reload",
    "originSummary": [
      "Running a small social network site offers benefits such as community building and user control.",
      "Open-source software and tailored online environments are advantageous for small social networks.",
      "Platforms like Mastodon and Dolphin Town are recommended for running a small social network.",
      "Managing a small community comes with challenges and responsibilities, but maintaining a positive user experience is crucial.",
      "Implementing code of conduct policies is important, and technical improvements in social network sites are needed.",
      "The concept of \"neighborhoods\" in federated social networks is proposed for improved user experiences.",
      "The future of social networks is discussed, acknowledging contributions from others.",
      "The author provides their own affiliations and licensing information.",
      "Note: This post discusses the benefits, challenges, and recommendations for running a small social network. It also highlights the importance of community management, code of conduct policies, and technical improvements in social network sites. The concept of \"neighborhoods\" in federated social networks is a new and exciting idea proposed by the author."
    ],
    "commentSummary": [
      "The popular website Hacker News is currently facing technical problems that are causing slow response times for user requests.",
      "Users may experience delays or difficulties when trying to access the website or interact with its features.",
      "It is unclear when the technical issues will be resolved, but the website's team is likely working to address the problem."
    ],
    "points": 111,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1704042493
  }
]
