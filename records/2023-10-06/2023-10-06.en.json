[
  {
    "id": 37778496,
    "title": "Where does my computer get the time from?",
    "originLink": "https://dotat.at/@/2023-05-26-whence-time.html",
    "originBody": "Tony Finch – blog home search archive recent feed links elsewhere 2023-05-26 – Where does my computer get the time from? ⇐ 2023-05-22 ⫤ ⊨ 2023-05-28 ⇒ This week I was in Rotterdam for a RIPE meeting. On Friday morning I gave a lightning talk called where does my computer get the time from? The RIPE meeting website has a copy of my slides and a video of the talk; this is a blogified low-res version of the slides with a rough and inexact transcript. I wrote a follow-up note, “Where does ‘where does my computer get the time from?’ come from?” about some things I left out of the talk. Where does my computer get the time from? from NTP - here’s a picture of an NTP packet and here’s a picture of David Mills who invented NTP simple question, easy answer, end of talk? No! let’s peel off some layers… stratum 3 NTP servers get the time from stratum 2 NTP servers, stratum 2 NTP servers get the time from stratum 1 NTP servers, stratum 1 NTP servers get the time from some reference clock maybe a radio signal such as MSF in Britain or DCF77 in Germany but in most cases the reference clock is probably a GPS receiver here’s a GPS timing receiver and here’s a GPS satellite where does GPS get the time from? Schriever Space Force Base in Colorado they look after a lot of different top secret satellites and other stuff at Schriever, as you can see from all the mission logos so you can’t get close enough to take a nice photo Where does Schriever SFB get the time from? the US Naval Observatory Alternate Master Clock is on site at Schriever in Colorado the US Naval Observatory Alternate Master Clock gets the time from the US Naval Observatory in Washington DC there are three answers the first answer is atomic clocks, lots of atomic clocks in the background there are dozens of rack mounted caesium beam clocks in the foreground the black boxes house hydrogen masers these shiny cylinders are rubidium fountains the USNO has so many atomic clocks they have entire buildings dedicated to them When I was preparing this talk I noticed on Apple Maps that there’s a huge building site in the middle of the USNO campus. It turns out they are building a fancy new clock house; the main limit on the accuracy of their clocks is environmental stability: temperature, humidity, etc. so the new building will have serious air handling. the second answer is that UTC is a horrible compromise between time from atomic clocks and time from earth rotation so the USNO gets the time from the international earth rotation service, which is based at the Paris Observatory twice a year the IERS sends out Bulletin C, which says whether or not there will be a leap second in six months time; leap seconds are added (or maybe removed) from UTC to keep it in sync with earth rotation the IERS is spread across several organizations which contribute to its scientific work for example, you can subscribe to IERS Bulletin A, which is a weekly notice with precise details of the earth orientation parameters Bulletin A is sent out by the US Naval observatory they need to know the exact orientation of the earth under the GPS satellites, so they can provide precise positioning the third answer is, how does the USNO know its atomic clocks are working well? that information comes from the international bureau of weights and measures in Paris, who maintain the global standard UTC how does the BIPM determine what UTC is? the BIPM collects time measurements from national timing laboratories around the world, and uses those measurements to determine official UTC periodically they send out Circular T which has information about the discrepencies between official UTC and UTC from the various national time labs the BIPM is responsible for maintaining the international system of units, which is defined by the general conference on weights and measures the CGPM is an international treaty organization established by the metre convention of 1875 UTC is an implementation of the SI unit of time, based on quantum measurements of caesium atoms where did this magic number, about 9.2 GHz, come from? in 1955, Louis Essen (on the right) and Jack Parry (left) built the first caesium atomic clock the current definition of the second came from the calibration of this clock before atomic clocks, the definition of the second was based on astronomy, so Essen and Parry needed help from astronomers to find out how fast their clock ticks according to the existing standard of time they got help from the astronomers at the US Naval Observatory the way it worked was William Markowitz measured time by looking at the skies, and Louis Essen measured time by looking at his atomic clock, and to correlate their measurements, they both listened to the WWV radio time signal broadcast by the national bureau of standards in Washington DC this project took 3 years, 1955 - 1958 Markowitz was measuring the “ephemeris second” in 1952 the international astronomical union changed the definition of time so that instead of being based on the rotation of the earth about its axis, it was based on the orbit of the earth around the sun in the 1930s they had discovered that the earth’s rotation is not perfectly even: it slows down and speeds up slightly clocks were now more precise than the rotation of the earth, so the ephemeris second was a new more precise standard of time the ephemeris second is based on an astronomical ephemeris, which is a mathematical model of the solar system the standard ephemeris was produced by Simon Newcomb in the late 1800s he collected a vast amount of historical astronomical data to create his mathematical model it remained the standard until the mid 1980s here’s a picture of Simon Newcomb he is a fine-looking Victorian gentleman where did he work? at the US naval observatory! (and the US nautical almanac office) I have now run out of layers: before this point, clocks were set more straightforwardly by watching stars cross the sky so, to summarise my talk, where does my computer get the time from? it does not get it from the Royal Greenwich Observatory! Comments welcome via • Cohost • Dreamwidth • Fediverse • Twitter • ⇐ 2023-05-22 ⇐ RIPE DNS Hackathon ⇐ ⇒ Where does \"where does my computer get the time from?\" come from? ⇒ 2023-05-28 ⇒ Tony Finch",
    "commentLink": "https://news.ycombinator.com/item?id=37778496",
    "commentBody": "Where does my computer get the time from?Hacker NewspastloginWhere does my computer get the time from? (dotat.at) 713 points by fanf2 20 hours ago| hidepastfavorite218 comments waterheater 15 hours agoRelated to timekeeping is the NIST Randomness Beacon: https:&#x2F;&#x2F;csrc.nist.gov&#x2F;projects&#x2F;interoperable-randomness-beac...\"This prototype implementation generates full-entropy bit-strings and posts them in blocks of 512 bits every 60 seconds. Each such value is sequence-numbered, time-stamped and signed, and includes the hash of the previous value to chain the sequence of values together and prevent even the source to retroactively change an output package without being detected.\"People here were joking about putting time on the blockchain, and, well, NIST is already doing it. reply throwaway89201 15 hours agoparent> People here were joking about putting time on the blockchain, and, well, NIST is already doing it.It&#x27;s not a blockchain, but a single writer Merkle DAG. No consensus necessary. Much like a git repository with a single author. reply zzo38computer 14 hours agorootparentIf each block contains the hash of the previous block, then I think that it is a blockchain (regardless of if there is multiple authors or only a single author). A git repository is a blockchain, too. reply throw0101a 13 hours agorootparent> If each block contains the hash of the previous block, then I think that it is a blockchain […]Or simply a &#x27;hash chain&#x27;:> A hash chain is similar to a blockchain, as they both utilize a cryptographic hash function for creating a link between two nodes. However, a blockchain (as used by Bitcoin and related systems) is generally intended to support distributed agreement around a public ledger (data), and incorporates a set of rules for encapsulation of data and associated data permissions.* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hash_chainOr perhaps:> Linked timestamping creates time-stamp tokens which are dependent on each other, entangled in some authenticated data structure. Later modification of the issued time-stamps would invalidate this structure. The temporal order of issued time-stamps is also protected by this data structure, making backdating of the issued time-stamps impossible, even by the issuing server itself.* https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Linked_timestampingAn(other) example of the latter: This document describes a mechanism, called syslog-sign in this document, that adds origin authentication, message integrity, replay resistance, message sequencing, and detection of missing messages to syslog. Essentially, this is accomplished by sending a special syslog message. The content of this syslog message is called a Signature Block. Each Signature Block contains, in effect, a detached signature on some number of previously sent messages. It is cryptographically signed and contains the hashes of previously sent syslog messages. The originator of syslog-sign messages is simply referred to as a \"signer\". The signer can be the same originator as the originator whose messages it signs, or it can be a separate originator.* https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc5848 reply schneems 5 hours agorootparentI think you’re basically saying that there are still no good known use cases for blockchain (&#x2F;s but only a little) reply zeusk 14 hours agorootparentprevWould you know! So Linus is the real father of blockchain? reply waterheater 13 hours agorootparentAccording to a news article, the first blockchain application is an application released in 1992 called AbsoluteProof by the company Surety [1].[1] https:&#x2F;&#x2F;www.vice.com&#x2F;en&#x2F;article&#x2F;j5nzx4&#x2F;what-was-the-first-bl... reply 1vuio0pswjnm7 13 hours agorootparent\"As Ethereum&#x27;s cofounder Vitalik Buterin joked on Twitter, if someone wanted to compromise Surety&#x27;s blockchain they could \"make fake newspapers with a different chain of hashes and circulate them more widely.\" Given that the New York Times has an average daily print circulation of about 570,000 copies, this would probably be the stunt of the century.\"What if the hash is published in multiple newspapers. reply fanf2 13 hours agorootparentprevYay, thank you, I was racking my brains trying to remember Surety as an example in response to https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37782446 reply CobrastanJorji 13 hours agorootparentprevWikipedia suggests that David Chaum first proposed what was basically a blockchain in 1982. He even had a crypto startup way before they were cool, with \"eCash\" in 1995. reply rdl 8 hours agorootparentBlind signatures are totally different from hash chains. reply r3trohack3r 14 hours agorootparentprevPeople keep saying Merkle DAGs when someone calls a linear chain of recursively hashed data blocks a blockchain.I don’t understand.My understanding of the Merkle Tree is that it’s a recursive hash, but the leaf nodes are the data, each layer up the tree is the hash of the child nodes.In a merkle tree, only the leaf nodes store (or reference) data, everything else is just a hash.Is there another merkle structure I don’t know about?https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Merkle_treeIf the nodes with hashes contain data, it’s not a merkle tree. reply r3trohack3r 10 hours agorootparentSince posting this, I&#x27;ve discovered that IPFS has something it calls Merkle-DAGs.A Block-Chain is a chain of blocks where there is one valid previous block and one valid next block.A Block-Tree is a chain of blocks where there is one single valid previous block, and multiple valid next blocks.A Block-DAG is a chain of blocks where there are multiple valid next blocks and multiple valid previous blocks, with the constraint that you can not form cycles.They are analogues to linked-lists, trees, and directed-acyclic-graphs but with chained hashes.From the Merkle-DAG article on the IPFS page:> Merkle DAGs are similar to Merkle trees, but there are no balance requirements, and every node can carry a payload. In DAGs, several branches can re-converge or, in other words, a node can have several parents.What&#x27;s interesting here is that a Merkle Tree is a valid Merkle DAG, since a node can _optionally_ include a data payload. So a blockchain, a blocktree, and a blockdag are all also Merkle-DAGs. Merkle-DAG is a kind of unifying structure that can be used to model all of them.It&#x27;s really quite clever.https:&#x2F;&#x2F;docs.ipfs.tech&#x2F;concepts&#x2F;merkle-dag&#x2F;This appears to have been coined in 2014: https:&#x2F;&#x2F;github.com&#x2F;jbenet&#x2F;random-ideas&#x2F;issues&#x2F;20However the term blockchain dates back to at least 2008.A blockchain might be a Merkle-DAG but a Merkle-DAG is not a blockchain. reply tedunangst 12 hours agorootparentprevI think this is isomorphic to an unbalanced tree where every node has one non leaf child and one leaf child. reply r3trohack3r 12 hours agorootparentSeems like claiming that a linked list isn&#x27;t actually a linked list it’s an unbalanced tree where every node has one child node.I mean, you’re not wrong but it’s still a linked list.I’d be careful muddying up your mental models this way though - they’re distinct data structures for distinct purposes.You would likely not want to use a merkle tree for an append only log, and likely would not want to use a blockchain for verifying file integrity.For example, BitTorrent, IPFS, and Storj use merkle trees to verify and discover blocks on the DHT, you would not want to use a blockchain for this.And Scuttlebutt uses a blockchain as an append only log that is gossip friendly, you would not want to use a merkle tree for this. reply waterheater 14 hours agorootparentprev>It&#x27;s not a blockchain, but a single writer Merkle DAG.Hmm. Just because something&#x27;s a Merkle DAG doesn&#x27;t make it useable on the Internet. A single-writer blockchain, perhaps? reply m3kw9 13 hours agorootparentprevOk but then anyone in control can change the entire tree, why need this Merkle tree? reply SV_BubbleTime 14 hours agorootparentprevOh… so you are calling a database a “block chain”. reply r3trohack3r 14 hours agorootparentA blockchain is a chain of blocks.Do you have another definition?Colloquially, it often refers to a consensus algorithm paired with a chain of blocks.Bitcoin’s innovation wasn’t a blockchain, it was a proof-of-work backed consensus algorithm that allowed a group of adversarial peers to agree on the state of a shared blockchain datastructure. reply waterheater 13 hours agorootparentAccording to the dictionary [1], a blockchain is \"a digital database containing information (such as records of financial transactions) that can be simultaneously used and shared within a large decentralized, publicly accessible network\"The distinction here might be with a decentralized network.[1] https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;dictionary&#x2F;blockchain reply ipaddr 13 hours agorootparentMerriam is incorrect reply pests 8 hours agorootparentEvery word in that definition seems to fit, no? reply tidenly 8 hours agorootparent\"decentralized\" isnt necessary to a block chain. However when people say \"block chain\" in everyday use, they&#x27;re usually talking about that type. It&#x27;s a case where the everyday use of a word is different to the actual technical meaning. reply pests 7 hours agorootparent> It&#x27;s a case where the everyday use of a word is different to the actual technical meaning.Which can change.There also is centralized proof-of-work blockchains. Clouds were offering them awhile back and IBM had some offering. replypests 8 hours agorootparentprevA blockchain used to be a chain of blocks. It&#x27;s more now. You kinda defined it - consensus algoithm, shared datastructure.What is colloquially even susposed to mean here? That the common usage doesn&#x27;t match the definition? Maybe definitions change over time.... reply kortilla 5 hours agorootparentprevSo a linked list is a blockchain? reply sandpaper26 15 hours agoparentprevCan someone give an example use case of this? I&#x27;m not sure I understand why a very public long string of random characters on a block chain is useful, except as a way to prove an event didn&#x27;t happen prior to a certain time reply nonameiguess 15 hours agorootparentThe draft of the version upgrade explains the possible uses of this: https:&#x2F;&#x2F;nvlpubs.nist.gov&#x2F;nistpubs&#x2F;ir&#x2F;2019&#x2F;NIST.IR.8213-draft...Mostly, it&#x27;s so the public can verify events that were supposed to be random really were random. The executive summary gives plenty of examples, but think of a pro sports draft lottery. Fans always think those are rigged. They could simply use these outputs and a hashing function that maps a 512-bit block to some set with cardinality equal to the number of slots and pre-assign slots to participating teams based on their draft weight. Then fans could verify using this public API that the draw the league claims came up randomly really did come up randomly.People always think polls are rigged. This could be used to publicly produce random population samples for polling.This was also used to prove a Bell inequality experiment worked with no loopholes. reply codetrotter 12 hours agorootparentIf they want to believe the polls are rigged, won’t they just assume that the NIST random data is “rigged” as well. reply fanf2 13 hours agorootparentprevSee https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37783764 reply eternityforest 14 hours agoparentprevI always wondered why nobody is using that as the root of a P2P randomness system.It would be very useful to have a trusted source of time, with a few keys that are meant to never change, that anyone can rebroadcast.We could have zero configuration clocks that get the time from the nearest phone or computer without any manual setup! reply ryangs 19 hours agoprevInteresting breakdown. But this format is horrible for conveying information. An improvement would be removing the slides, crafting some coherent paragraphs and then reinserting some of the more crucial images for support. reply svat 5 hours agoparentAfter someone gives a talk in person, some of the things they can do online:1. Mention that they gave a talk2. Post a video recording online3. Post the slides online, as-is (PDF or whatever) with no explanation4. Lay out the slides on a HTML page, with accompanying text (what would have been said by the speaker), so that it&#x27;s easier to read — while still being clear you&#x27;re “reading” a talk.5. Redo&#x2F;rewrite the whole thing into text form, paragraphs and all.The author here has done 1 to 4, and you&#x27;re complaining they&#x27;ve not also done 5, but that&#x27;s a lot of work and I don&#x27;t begrudge someone not doing that. I&#x27;ll be grateful someone presented their talk in a readable form in the first place.[I do agree this page was hard to read, at least on mobile and at least in its initial version—it&#x27;s much better now—but I&#x27;ve seen many others post these \"annotated talks\" online and the format itself is not necessarily bad: for instance see https:&#x2F;&#x2F;idlewords.com&#x2F;talks&#x2F; (example: https:&#x2F;&#x2F;idlewords.com&#x2F;talks&#x2F;superintelligence.htm) or https:&#x2F;&#x2F;noidea.dog&#x2F;talks (example: https:&#x2F;&#x2F;noidea.dog&#x2F;impostor) or https:&#x2F;&#x2F;simonwillison.net&#x2F;tags&#x2F;annotatedtalks&#x2F; (example: https:&#x2F;&#x2F;simonwillison.net&#x2F;2022&#x2F;Nov&#x2F;26&#x2F;productivity&#x2F;) — maybe just some minor tweaks to CSS like putting the text to the right of the images would make it easier to read.] reply hbn 18 hours agoparentprevI was mostly confused about the images being above the line of text you&#x27;re supposed to read before looking at the image.\"Here&#x27;s a picture of an NTP packet\"picture of a man sitting at a desk reply fanf2 17 hours agorootparentWhen I gave the talk, I showed the slide before I talked about it. It’s normal to show the speaker notes below the slides in software like Keynote or Powerpoint. reply OJFord 17 hours agorootparentThat might be clearer if the header was just &#x27;slides and notes from my talk&#x27;, instead you actually claimed the opposite, that it&#x27;s a &#x27;blogified version&#x27;, but it&#x27;s not really - I tripped up on the same thing, and then got through several &#x27;duplicate images&#x27;, &#x27;oh no very slightly different images&#x27;, before it finally dawned on me that they were slides. reply fanf2 11 hours agorootparentprevI’ve clarified the introductory paragraph and added lines between each slide. Should be a bit easier to read now. reply SilasX 8 hours agorootparentprevAnd when I shift a work to a new medium, I also shift over the conventions used to match that new medium -- and audience. It&#x27;s an empathy thing. reply pests 8 hours agorootparentYou should be lucky someone produced free content for you to consume.The sense of entitlement to accuse someone of lacking empathy because they didn&#x27;t present it in your preferred format is literally crazy to me. reply SilasX 8 hours agorootparentI don&#x27;t feel entitled to anything, it was just a suggestion for how they can communicate better, which they ostensibly want. I constantly get lauded for good presentations, and see others do them with unforced errors, so I thought I&#x27;d do my part to level the playing field.Ironically enough, aren&#x27;t you doing the same thing now, berating me for giving free information the wrong way? How about just learning from the advice and moving on? reply pests 7 hours agorootparentSorry about that. I was more harsh then I intended. I might have misread your original reply as well. It appeared more of a complaint than advice, and I do agree it&#x27;s good advice. reply dclowd9901 17 hours agorootparentprevI can tell the talk would have been really enjoyable but I agree this format is just lazy for conveying that information. reply Humdeee 17 hours agorootparentprevIt&#x27;s simply not intuitive in the way it was presented that the line of text was a footer for the picture. The text and pictures are mistakenly read as belonging to the same \"layer\", sequentially, which is not what the author intended. It&#x27;s obvious what that intent was, but it&#x27;s not structured correctly to be properly interpreted. reply asveikau 16 hours agorootparentprevI was really bothered that on the website version, the NTP packet diagram is largely illegible. I hope that when they gave this talk on slides, you could read it. reply fanf2 14 hours agorootparentTBH you aren’t supposed to read it, you either say to yourself, oh yes I recognise the NTP packet diagram; or, oh yes, that looks like a packet diagram; or, oh interesting maybe I should look at the NTP RFC. The slide was only up for a couple of seconds :-) reply zoky 18 hours agorootparentprevI mean, put a little gnome hat on him and I’d believe it… reply phantom784 19 hours agoparentprevWatching the actual talk is much better: https:&#x2F;&#x2F;ripe86.ripe.net&#x2F;archives&#x2F;video&#x2F;1126&#x2F; reply nayuki 17 hours agorootparentThe linked PDF has clear page delineations, unlike the HTML page: https:&#x2F;&#x2F;ripe86.ripe.net&#x2F;presentations&#x2F;134-2023-04-whence-tim... reply m348e912 19 hours agoparentprevI have never seen this format before but it does mirror what going down a rabbit hole of a particular topic looks like for the average curious person.I liked it. reply FL410 19 hours agoparentprevI thought it was a very fun, stream-of-consciousness kind of read. reply johnnyanmac 18 hours agoparentprevI simply assume any \"slides\" format comes from porting over a live talk. Lazy, yes. Efficient, yes. reply chankstein38 15 hours agoparentprevEspecially because half of the text just repeats what&#x27;s on the slides and ultimately I didn&#x27;t see an easy way to make the slides bigger. Like the NTP packet format slide was mostly unreadable. reply TacticalCoder 17 hours agoprevWhat&#x27;s amazing is that if your computer is not set to automatically sync its time, you can see how fast it&#x27;s drifting.My main desktop is 1.7 seconds ahead at the moment. Probably haven&#x27;t updated the clock in a few weeks: which isn&#x27;t that much. Other systems shall drift much more.As to \"why\" it&#x27;s not setting the time using NTP automatically: maybe I like to see how quickly it drifts, maybe I want as little services running as possible, maybe I&#x27;ve got an ethernet switch right in front of me which better not blink too much, maybe I like to be reminded of what \"breaks\" once the clocks drifts too much, maybe I want to actually reflect at the marvel of atomic drift when I \"manually\" update it, etc. Basically the \"why\" is answered by: \"because I want it that way\".Anyway: many computer&#x27;s internal clock&#x2F;crystal&#x2F;whatever-thinggamagic are not precise at all. reply peteey 16 hours agoparentCrystal errors tend to be around 20 ppm (parts per million)After a week, 20 ppm would drift 12 * 10^-6 * 7 * 24 * 60 *60 = 12 seconds.Your motherboard probably has a cr2032 keeping it powered when unplugged.Crystals: https:&#x2F;&#x2F;www.digikey.com&#x2F;en&#x2F;products&#x2F;filter&#x2F;crystals&#x2F;171?s=N4... reply fanf2 14 hours agorootparentThere’s a fun thing about quartz wristwatches: one of the biggest contributions to frequency fluctuations in a quartz oscillator is temperature. But if it is strapped to your wrist, it is coupled to your body’s temperature homeostasis. So a quartz watch can easily be more accurate than a quartz clock!Really good watches allow you to adjust their rate, so if it runs slightly fast or slow at your wrist temperature, you can correct it.One of the key insights of John Harrison, who won the Longitude prize, was that it doesn’t matter so much if a clock runs slightly fast or slightly slow, so long as it ticks at a very steady rate. Then you can characterise its frequency offset, and use that as a correction factor to get the correct GMT after weeks at sea. reply lxgr 12 hours agorootparentThat would require tuning it to the average body temperature though, right?Or are you saying that what makes quartz crystals drift is the change in temperature? reply fanf2 12 hours agorootparentBoth are true :-) reply lxgr 10 hours agorootparentOh, I missed your comment about being able to tune some wristwatches quartz! I wasn&#x27;t aware that was a thing.Still, wouldn&#x27;t the temperature of a watch while being worn vary as least as much as when sitting in a drawer (unless you live in a region blessed with t-shirt weather year around)?One of my favorite wristwatches I used to wear as a teenager had a thermometer, but I don&#x27;t remember how exactly that varied over the year, just that it always showed neither quite my body temperature, nor quite the ambient one :) reply crote 14 hours agorootparentprevIt kinda makes you wonder why desktop computers don&#x27;t use the AC frequency as a stable-ish time source. Short-term accuracy is pretty poor, but it can definitely do better than 12 seconds over a week! reply brohee 2 hours agorootparentThat&#x27;s a very optimistic assumption, the target is 50Hz but if it is below or over for a long period of time (e.g. high load in winter making it hard to sustain the nominal frequency) there are no provision to make it run faster or slower unless the time drifted by more than 30s (that&#x27;s possibly only valid for Europe).More at https:&#x2F;&#x2F;wwwhome.ewi.utwente.nl&#x2F;~ptdeboer&#x2F;misc&#x2F;mains.html reply moffkalast 14 hours agorootparentprevI suppose it&#x27;s because no AC ever gets to the motherboard in your typical ATX setup? It&#x27;s all just DC 12&#x2F;5&#x2F;3 volts and could be coming from a battery for all it knows. There would need to be an optional standard way of getting time from the PSU and have the AC time keeping there. reply crote 12 hours agorootparentOf course, but there&#x27;s no reason why a 50&#x2F;60Hz signal couldn&#x27;t have been included in the ATX power connector back when it was established a few decades ago.In an alternate universe it would&#x27;ve been put in there, together with all the weird -12V &#x2F; -5V rails nobody uses these days. Getting it these days would indeed be pretty much impossible. reply throw0101a 12 hours agorootparentprev> After a week, 20 ppm would drift 12 * 10^-6 * 7 * 24 * 60 *60 = 12 seconds.Where are you getting that 12 from? reply dmoy 7 hours agorootparentIt should read 20*, not 12.The end result is 12 seconds. reply ReactiveJelly 16 hours agoparentprevCan&#x27;t say too much but I saw an IoT product where, if NTP failed, they would all slowly fall behind. I really appreciated this because fixing NTP would jump forward, leaving a gap in perceived time instead of living the same moment twice.So I assumed that, like how speedometers purposely read a little high, the crystals must purposely read a little slow so that computers don&#x27;t slip into the future. reply lxgr 10 hours agorootparentThat&#x27;s a neat way of ensuring that time never jumps backwards on your system!Reminds me of the same idea but applied in the opposite way in some train station clocks: Their second hands take slightly less than a minute to complete one rotation, after which they stop and wait for a signal sent from a central clock to be released simultaneously.Making a clock run slightly slow or fast is much easier than making it run just about correctly :) reply harikb 17 hours agoparentprevFrom wikipedia> Typical crystal RTC accuracy specifications are from ±100 to ±20 parts per million (8.6 to 1.7 seconds per day), but temperature-compensated RTC ICs are available accurate to less than 5 parts per million.[12][13] In practical terms, this is good enough to perform celestial navigation, the classic task of a chronometer. In 2011, chip-scale atomic clocks became available. Although vastly more expensive and power-hungry (120 mW vs. the ephemeris second is based on an astronomical ephemeris, which is a mathematical model of the solar system>the standard ephemeris was produced by Simon Newcomb in the late 1800s >he collected a vast amount of historical astronomical data to create his mathematical model >it remained the standard until the mid 1980s>in 1952 the international astronomical union changed the definition of time so that instead of being based on the rotation of the earth about its axis, it was based on the orbit of the earth around the sun >in the 1930s they had discovered that the earth’s rotation is not perfectly even: it slows down and speeds up slightly >clocks were now more precise than the rotation of the earth, so the ephemeris second was a new more precise standard of time reply darkwater 13 hours agoparent>in 1952 the international astronomical union changed the definition of time so that instead of being based on the rotation of the earth about its axis, it was based on the orbit of the earth around the sun >in the 1930s they had discovered that the earth’s rotation is not perfectly even: it slows down and speeds up slightlyYeah, I remember studying that back in high school but I wonder... what previous actual duration of a second they used? And also, being based on the rotation of Earth, what kind of data was the \"vast amount of historical astronomical data\" Newcomb collected? How can you reliably capture and store the length of time if you can only base it on the Earth rotation speed which varies over time? I would guess the data compared it to other natural phenomena? reply fanf2 12 hours agorootparentWhen time was based on earth rotation, astronomers used “transit instruments” to observe when certain “clock stars” passed directly overhead. The clock stars had accurately known positions, so if you routinely record the time they pass overhead according to your observatory’s clock, then you can work out how accurate your clock is.Newcomb’s data would have been accurately timed observations, as many as he could get hold of, going back about two and a half centuries. reply donalhunt 12 hours agoprevDARPA are funding the Robust Optical Clock Network (ROCkN) program, which aims to create optical atomic clocks with low size, weight, and power (SWaP) that yield timing accuracy and holdover better than GPS atomic clocks and can be used outside a laboratory.Most of the big cloud providers have deployed the equivalent of the opencompute time card which sources its time from GPS sources but can maintain accurate time in cases of GPS unavailability.https:&#x2F;&#x2F;www.darpa.mil&#x2F;news-events&#x2F;2022-01-20 reply tbm57 19 hours agoprevI think we need a community-maintained and democratized time-tracking standard so we&#x27;re not so beholden to Big Time reply crote 14 hours agoparentThat&#x27;s pretty much what we already have, isn&#x27;t it?True Time™ is determined by essentially averaging dozens of atomic clocks from laboratories all over the world. It doesn&#x27;t really get any more \"community-maintained\" and \"democratized\" than that! reply callalex 17 hours agoparentprevPut it on the clockchain reply urbandw311er 14 hours agorootparentPlease tell me you just coined this. reply huehehue 14 hours agoparentprevThe article, and this comment, makes me wonder what impact a coordinated attack on the root time-keeping mechanisms might have. It seems like there&#x27;s a fair bit of redundancy &#x2F; consensus, but what systems would fail? On what timeline? How would they recover? reply Daz1 3 hours agoparentprevBring out the water jug with a hole in it reply kristopolous 16 hours agoparentprevIt&#x27;s probably possible to calibrate your clock using a clear night sky and a modern cell phone camera. I bet second accuracy isn&#x27;t an absurd expectation. Now it&#x27;d probably take an unreasonable amount of time to calibrate... reply nektro 13 hours agoparentprevwe&#x27;re not, it&#x27;s run by the government reply auspiv 17 hours agoprevIf you have a Raspberry Pi laying around and want to run your own Stratum 1 NTP server - https:&#x2F;&#x2F;austinsnerdythings.com&#x2F;2021&#x2F;04&#x2F;19&#x2F;microsecond-accura... reply fanf2 13 hours agoparentNote that for NTP it’s better to use a Raspberry Pi 4 than older boards. The old ones have their ethernet port on the wrong side of a USB hub, so their network suffers from millisecond-level packet timing jitter. You will not be able to get microsecond-level NTP accuracy.For added fun, you can turn the Raspberry Pi into an oven compensated crystal oscillator (ocxo) by putting it in an insulated box and running a CPU burner to keep it toasty. https:&#x2F;&#x2F;blog.ntpsec.org&#x2F;2017&#x2F;03&#x2F;21&#x2F;More_Heat.html (infohazard warning: ntpsec contains traces of ESR) reply ianburrell 11 hours agoparentprevThat&#x27;s a Stratum 0 server since it gets its time from GPS. Stratum 1 server is one that gets its time from Stratum 0 servers. reply fanf2 10 hours agorootparentStratum 0 is the reference clock itself; stratum 1 is an NTP server attached to one or more reference clocks. https:&#x2F;&#x2F;www.ntp.org&#x2F;ntpfaq&#x2F;ntp-s-algo&#x2F;#5111-what-is-a-refere... reply The_suffocated 16 hours agoprevMost of those slides concern about the physics part of time measurement (GPS and atomic clock, etc.). While this is interesting in its own right, in order to understand how MY computer obtains the current time, a more relevant question is “how does a home computer measure the latency of a packet sent from a remote time server”? Does it measure the durations of several roundtrips and take the average duration as latency? What if congestion suddenly occurs during some roundtrip? I always think that these questions are more mysterious than the physical ones. reply tux3 16 hours agoparentHere&#x27;s how that works:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Network_Time_Protocol#Clock_sy... reply freedude 16 hours agoprevJust be careful which time source you use. One of our servers was configured to use tick.usno.navy.mil and tock.usno.navy.mil back 10-15 years ago or so. The Navy had an \"issue\" with the time they were sending out. The overnight result was several licensing servers wouldn&#x27;t authenticate and we were locked out of those systems(SSH needs accurate time, within minutes I believe). We discovered the discrepancy by logging in locally (we were in the same building but a different office) and changed the time servers and then the sync method to resolve the issue. reply bityard 15 hours agoparent> SSH needs accurate time, within minutes I believeYou may be mis-remembering a few details, SSH does not care about the time at all unless you are using _very_ short-lived SSH certificates. reply freedude 11 hours agorootparentYou are correct and I believe it was Kerberos that we were locked out of on this system since it was running a SMB share. reply xorcist 14 hours agorootparentprevKerberos is very particular about time. reply freedude 11 hours agorootparentThis system operated an SMB share so Kerberos is probably what locked us out. reply lazide 14 hours agorootparentprevTime based OTP is pretty sensitive though. Probably that is what broke? reply crote 14 hours agorootparentThe irony is that the TOTP spec explicitly takes this into account.By default tokens are valid for 30 seconds, with a token from the previous 30-second window also being accepted. Being off by more than that is pretty rare for NTP-connected systems.The specs also provide ways to deal with a dedicated hardware token slowly going out of sync by keeping track of the last-known clock drift, but that&#x27;s pretty useless these days and can even do more harm than good. reply lazide 14 hours agorootparentThe poster was referring to minutes, which has also been my experience. Something goes wrong, and suddenly you’re an hour off. Blam, now you can’t login. :s reply fanf2 14 hours agorootparentprevThey might have been using kerberos authentication? reply gentleman11 19 hours agoprevWhere does my car get the time from? It drifts and changes every time I start it up. Every 3 months I have to change it manually by 10ish minutes or more, but it’s inconsistent reply SAI_Peregrinus 18 hours agoparentProbably just a local quartz oscillator, like a cheap wristwatch but embedded into the car. That&#x27;ll drift with temperature, vibration, humidity, and some other factors, but it&#x27;s cheap and just relies on the user to occasionally set it. Fancier systems can use radio time or GNSS (more likely if the car has built in navigation), but that&#x27;s probably not happening if you regularly set the time! reply CableNinja 18 hours agorootparentCorrect. Oscillators are subject to drift through a number of means, and they all have ridiculous effects too. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37613523 reply jrockway 15 hours agorootparentTemperature is the largest factor. Things like a DS3231[1] do really well compared to a basic non-compensated oscillator. I have been running some long-term experiments on a few that I have around and with some tuning got them to less than a second loss per year. But, they are super expensive compared to the basic ones (almost $5 each in quantity), so they aren&#x27;t going to end up in your car where a 3 cent chip is possible to use instead. (I don&#x27;t know what 5G &#x2F; LTE chips cost these days, but if they&#x27;re putting one in your car anyway, then they can probably get the time from that. But choose not to.)[1] https:&#x2F;&#x2F;www.analog.com&#x2F;media&#x2F;en&#x2F;technical-documentation&#x2F;data...Most interesting to me in all of my time experiments is looking at my clock frequency over time vs. the temperature. (NTP daemons aim to calculate your actual clock frequency; then they know how far off your internal time is from actual time.) You don&#x27;t even need a temperature sensor, the clock rate is a perfect analogue. reply lazide 14 hours agorootparentVoltage issues can also be a big problem, and cars have notoriously dirty electrical. reply jrockway 12 hours agorootparentAhh, I bet that&#x27;s true! reply crote 14 hours agorootparentprev> A company i worked for wanted systems to have no more than 2ns of time drift between each other, in a network of +10 devices.At that point it&#x27;s surprising they didn&#x27;t just deploy a local \"time network\", with a single master clock distributing time via length-calibrated coax. Approaches like that are really common in television studios. reply CableNinja 12 hours agorootparentIt wasnt really the right environment for it, and they didnt even actually need that high of resolution, they couldve gotten away with 100ms drift and never noticed reply willis936 19 hours agoparentprevIt sounds like it gets the time from you. reply fuzzfactor 17 hours agorootparentLaurens Hammond invented the synchronous electric motor once A&#x2F;C domestic voltage had proliferated enough as an alternative to the original D&#x2F;C electrification first established by Edison.This made it possible for the first time to build clocks based on the stable frequency of the incoming A&#x2F;C supply voltage, much more reliably than those based on the incoming line voltage, which varies quite a bit whether it is A&#x2F;C or D&#x2F;C.This put him on the map as a manufacturer when he went forward to build Hammond clocks commercially.Years later his engineers encouraged him to consider developing an electric church organ, which would be possible to remain in tune regardless of variations in line voltage themselves.Hammond was not musically inclined but he did it anyway.Right up there with the Great Men in the most legendary way.http:&#x2F;&#x2F;thehammondorganstory.com&#x2F;By the time the 1960&#x27;s came around, almost all new American vehicles were recognized as modern Space Age conveniences, and a factory clock (mechanical analog, naturally) had become almost a universal standard accessory beyond the most budget price points.There were a couple drawbacks to the factory clocks, they had to be connected to the car battery at all times to keep running, they didn&#x27;t drain the battery very much at all but still would eventually deaden it if undriven, way worse than no clock. And they depended on the incoming voltage which determined the internal clock motor speed to begin with. Different automotive electrical systems and batteries themselves do vary perhaps 10 percent about a nominal design voltage of 12 VDC. There is no stable A&#x2F;C in the car that a synchronous motor would need to run on[0].These now-vintage clocks were self-correcting. You correct them yourself. Actually the same twisting of the knob to move the hands of the clock, which was familiar from earlier non-correcting clocks simply did the job. So they were somewhat backward-compatible. Only the Space Age units had smart enough mechanical ability to take into account how much and in which direction you moved the hands, and adjusted the previous running speed accordingly. If the clock was not very close to correct time when you adjusted it, it would take repeated adjustments over a number of days or weeks to get it to very realistic speed. All it really did was successive approximation. You had to supply your own natural intelligence.Even at the time lots of drivers never knew this, and there was widespread disappointment over the wildly inaccurate clocks \"which were OK when new but went downhill &#x27;through time&#x27;\". They only added maybe a dollar to your car payment but that was very expensive compared to a highly reliable cheap household clock at the time.When you think about it, today lots of drivers are not quite up to par when it comes to engaging the amount of natural intelligence that would be needed in many other ways besides timekeeping.[0] The electrical \"vibrator\" which provided switch-mode 12VAC which could be stepped up by a transformer to supply much higher voltage to power vacuum tube radios still produced a variable A&#x2F;C voltage & frequency, dependent on the underlying D&#x2F;C supply voltage. reply linkjuice4all 15 hours agorootparentThe clock in my 1967 Mercury has an interesting mechanism. It&#x27;s a fully mechanical wound spring clock with a self-winding mechanism. When the spring unwinds it closes a circuit on an electromagnet that quickly rewinds the clock spring.Every couple of hours or so you&#x27;ll hear the click from it rewinding on its own. Unfortunately there&#x27;s nothing to prevent it from running down the battery and they often need to be replaced due to burn out when the voltage gets low. Essentially the rewinder doesn&#x27;t have enough voltage to actually wind the clock and the circuit stays closed. reply fuzzfactor 13 hours agorootparentNothing like a &#x27;60&#x27;s Mercury when they were still building them more carefully than the corresponding mainstream Ford-badged models.>Well if I had money>Tell you what I&#x27;d do>I&#x27;d go downtown and buy a Mercury or twoMercury Blues:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QsTfCITzISMI could really use a Mercury or two about now myself. reply incanus77 18 hours agoparentprevWhat really gets me is when the gauge cluster clock and the radio clock differ. Just a wonderful metaphor for the modern car. reply lm28469 19 hours agoparentprevDepends on the car model. Some can use GPS or radio time signals: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Time_signal reply CableNinja 18 hours agoparentprevHere, have a rabbithole. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37613523 reply paxys 16 hours agoparentprevI recently rented a $65000 luxury car and it didn&#x27;t even have built-in daylight savings adjustment. Owners have to dig into settings and fix it themselves twice a year. Cars are so far behind on basic software it is crazy. reply ipython 15 hours agorootparentOh, it gets better. I used to get reminders in the mail to take my luxury car into the dealership for \"service\" to adjust the clock twice a year. Or I could ... you know, just press a few buttons for free.The trouble with all the modern cars that have synchronized clocks is that, well, you&#x27;ve already put in an LTE SIM card, so why not send up some telemetry at the same time? And here we are, with cars that are surveillance devices with four wheels. reply reaperman 14 hours agorootparentA simple GPS receiver could also provide the time. But I agree with your rant overall. reply AdamH12113 8 hours agorootparentprevDST policies vary by country and (US) state. The US DST schedule can be changed, and has been — twice in the last 50 years. There are proposals to do so again, both nationally and within states. Implementing automatic DST adjustment puts you on the hook for software updates forever. It’s much easier to just let people change the time manually when they need to, like they do with other appliances.IMHO, the real failure is when devices make it hard to figure out how to change the time. reply notatoad 6 hours agorootparentyeah, DST schedules change frequently enough that within the lifetime of most automobiles, at least some of them will be in use in some location where up-to-date timezone database doesn&#x27;t match the timezone database in the car&#x27;s software.so the manufacturer gets to choose between making people apply DST changes manually twice a year, which most people understand and are used to doing for various things, or changing over for DST automatically but being wrong sometimes, which most people won&#x27;t understand and will complain about. reply spelunker 19 hours agoparentprevI have the same problem! It takes months, but eventually the clock in my car is minutes behind. I think currently it&#x27;s about 4 minutes behind. reply gorgoiler 58 minutes agoprevA very enjoyable read! If I wanted to be super nerdy about time and have the most precise time source possible, at home, what are my options? reply fanf2 11 minutes agoparentsee https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37780788and https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37782982 reply fckgw 18 hours agoprevJust want to take a moment to appreciate the URL of \"dot at, dot at, slash at\" reply nayuki 17 hours agoparentDefinitely reminds me of H T T P colon slash slash slashdot dot org reply alch- 13 hours agorootparentOooooh decades later I finally get the name Slashdot! Thank you! reply FireBeyond 17 hours agoparentprevIIRC there was an ISP or web host in Australia way back in the day called DotNet (obviously before the MSFT days)...Their website was http:&#x2F;&#x2F;www.dotnet.net.au (www dot dotnet dot net dot au). reply firatt 16 hours agoparentprevyou should see the email address of the author :) reply perihelions 18 hours agoprev&#x2F;Meta: There&#x27;s three different posts on the front page on the theme of \"what is time, anyway\", and I&#x27;m curious if there some reason for that? Did I miss some news event? Did some leap-second bug crash something? reply wwalexander 17 hours agoparentI’ll often see articles on the front page related to a popular thread from a day or two ago. I always assume that someone either went down a rabbit hole based on the original thread and wanted to share their findings, or already knew about that topic and felt inspired by the original thread to share something useful about it. reply pbhjpbhj 16 hours agoparentprevHypothesis:People gaming clicks using popularity of subjects from past years would want to drift (heh!) the time forward slightly, these topics probably normally arise around the time clocks change for Winter (29 October this year is the end of British Summer Time). So, I speculate that this is a drifted \"clocks go back, but will your computer adjust itself?\" topic area. reply rantee 16 hours agoprevGreat overview, thanks for sharing. Maybe this was unintentional, but I got a good laugh out of, \"In 1952, the International Astronomical Union changed the definition of time\"! reply thakoppno 15 hours agoprevThis was a real talk? I would have lost my mind attending this. I am adding the Naval Observatory to my travel destination wish list. reply urbandw311er 14 hours agoparentIt’s hard to tell if you “losing your mind” in this context means you would have enjoyed the talk or the opposite. reply thakoppno 13 hours agorootparentI would have enjoyed it tremendously. reply fanf2 13 hours agorootparentYou might also likehttps:&#x2F;&#x2F;dotat.at&#x2F;@&#x2F;2022-12-04-leap-seconds.htmlhttps:&#x2F;&#x2F;dotat.at&#x2F;@&#x2F;2020-11-13-leap-second-hiatus.html reply gandalfian 17 hours agoprevIt used to irritate me that my old dumb mobile must have known exactly the correct time in order to operate on the cell phone network. Yet it kept it secret from me. I had to manually set the clock by guesstimate reply callalex 17 hours agoparentIn the early days of mobile networks, it was my experience that the network time was not very good. Sometimes off by a minute or two, but most often filled with DST bugs. reply binbag 15 hours agoprev\"the BIPM collects time measurements from national timing laboratories around the world\"I&#x27;m really interested in how this is done with multiple clocks over a distance. Can anyone explain? It feels like it would be very difficult since asking \"what time is it there?\" at the timescale of atomic clocks is kind of a bit meaningless? And that&#x27;s before considering the absolute local nature of time and the impossibility of a general universal time per relativity. reply fanf2 14 hours agoparentThe term of art you want for searchengineering is “time transfer”.There are a variety of mechanisms:* fibre links when the labs are close enough* two-way satellite time transfer, when they are further apart* in the past, literally carrying an atomic clock from A to B (they had to ask the pilot for precise details of the flight so that they could integrate relativistic effects of the speed and height)* there’s an example in the talk, of how Essen and Markowitz compared their measurements by using a shared reference, the WWV time signal. reply crote 13 hours agoparentprevI believe an important aspect is that the actual time offset between the clocks doesn&#x27;t matter all that much - it is the drift between them you care about.True UTC is essentially an arbitrary value. Syncing up with multiple clocks is done to account for a single clock being a bit slow or fast. It doesn&#x27;t matter if the clock you are syncing with is 1.34ms behind, as long as it is always 1.34ms behind. If it&#x27;s suddenly 1.35ms behind, there&#x27;s 0.01ms of drift between them and you have to correct for that. And if that 1.34ms-going-to-1.35ms is actually 1.47ms-going-to-1.48ms, the outcome will be exactly the same.This means you could sync up using a simple long-range radio signal. As long as the time between transmission and reception for each clock stays constant, it is pretty trivial to determine clock drift. Something like the DCF77 and WWVB transmitters seems like a reasonable choice - provided you are able to deal with occasional bounces off the ionosphere.Of course these days you&#x27;d probably just have all the individual clocks somehow reference GPS. It&#x27;s globally available, after all. reply fanf2 13 hours agorootparentIt isn’t just the difference in rate. The main content of Circular T https:&#x2F;&#x2F;www.bipm.org&#x2F;en&#x2F;time-ftp&#x2F;circular-t is the time offset of the various national realisations of UTC. Another important aspect is characterizing the stability of each clock, which determines the weighting of its contribution to UTC.The algorithm behind Circular T is called ALGOS. reply martin1975 14 hours agoprevIf you&#x27;re interested in precise time keeping, this is Time-Nuts is a great place to start (http:&#x2F;&#x2F;www.leapsecond.com&#x2F;time-nuts.htm). reply mindcrime 14 hours agoparentSee also: the Metrology forum at eevblog.com. Lots of time-nuts (and volt-nuts, etc) hang out there. reply user3939382 19 hours agoprevA hydrogen atom being looked at by the Navy right? reply caymanjim 19 hours agoparentCesium, NIST. reply fanf2 18 hours agorootparentThe USA has two main time labs: the USNO, which provides time and navigation for the DoD, including the GPS; and NIST which provides time for civilian purposes, including WWV. NIST tends to do more research into new kinds of atomic clock (eg optical clocks, chip-scale clocks) whereas the USNO does more work on earth orientation.The USNO atomic clock ensemble includes caesium beam clocks, hydrogen masers, and rubidium fountains. NIST uses mostly hydrogen masers, and fewer caesium beam clocks, though their primary frequency standards are caesium fountains. reply vel0city 18 hours agorootparentprevI get the confusion for the US Navy though, as the clock is at the US Naval Observatory.If you ever need the time, just call (719) 567-6742\"US Naval Observatory, Master Clock, at the tone, Mountain daylight time, nine hours, sixteen minutes, fifteen seconds...beep!\" reply hoosieree 17 hours agorootparent\"At the tone?\" ...what kind of ship are you running here? Is it at the start or the end of the tone? reply goblinux 17 hours agorootparentprevJust called the number holy cow it’s real. I love obscure infrastructure stuff like that reply fluoridation 17 hours agorootparentSpeaking clocks are pretty common. Here we dial *133. reply bityard 15 hours agorootparentprevWhen I was a kid, you could dial the operator and ask them for the time. I still don&#x27;t know why anyone would do that, but I remember it was a thing you could do.Also, dialing 0 to get a human operator. I swear I&#x27;m not that old. reply user3939382 17 hours agorootparentprevYeah according to Wikipedia anyway the USNO is operated by the US Navy. reply aa-jv 19 hours agorootparentprevAnd not just one, millions of them. reply gumby 19 hours agorootparentWhat a waste of taxpayers’ money! They should just pick one and stare at it. Why should we be paying for millions of them??? reply seanthemon 18 hours agorootparentIf you don&#x27;t use the budget you won&#x27;t get the budget, sailor. reply fluoridation 17 hours agorootparentI see no downside. replydclowd9901 17 hours agoparentprevSort of. More like from a DNS service for time, to which the navy both contributes and receives information from. I found that part to be the most interesting. reply CableNinja 17 hours agorootparentYou are sort of correct. NTP is pretty decentralized. DNS has a few specific servers (root servers) that all DNS eventually hits to find where to get a result, but, the &#x27;tree&#x27; of DNS resolution is much different from that of NTP, which doesnt have such a tree, except as defined by any DNS entries, if they are used (ex pool.ntp.org has many A records for many ips or CNAMEs to other domains (ex 0.pool.ntp.org)).There are many contributors to the official timekeeping. Most facilities who do science will have their own actual atomic clock, which they then share out the data, in the form of an NTP server, however, they will not typically use data from the rest of the world, except for correlation events. The rest of the world relies on a handful of clocks which are either from NIST (ntp.org I think is owned by them), or from major providers like cloudflare (not sure they have an ntp server available the public can use, im almost certain that they would use their own atomic clock internally for security reasons), microsoft also has one, i think, afaik they would need to because they provide their own ntp pool, but they may just aggregate from multiple NIST servers.You can setup your own NTP server as well, and setup systems you own to start using it instead of whatever is configured. And, if one were so inclined, could even find and run your own atomic clock, and register it with the ntp pool. Im actually not sure the atomic clock is required, id hope it would be, but idk. reply iamnotsure 15 hours agoprevhttps:&#x2F;&#x2F;girard.perso.math.cnrs.fr&#x2F;mustard&#x2F;article.html reply gloryless 8 hours agoprevGreat question and great article. This is an \"old Internet\" vibe for me reply adenner 17 hours agoprevThis reminds me of a talk I gave several years ago to my local linux users group (CIALUG) about time... I don&#x27;t have the recording anymore but still have the slides https:&#x2F;&#x2F;www.slideshare.net&#x2F;denner1&#x2F;all-about-time-or-how-to-... reply kilbuz 13 hours agoprevevery NTP story needs a link to the Netgear&#x2F;UW-Madison fiasco: https:&#x2F;&#x2F;pages.cs.wisc.edu&#x2F;~plonka&#x2F;netgear-sntp&#x2F; reply fanf2 13 hours agoparentAnd PHK &#x2F; D-Link https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Poul-Henning_Kamp#Dispute_wi... reply archsurface 7 hours agoprevHaving just finished watching Idiocracy I have no choice but to point out that it comes from the time masheen. reply tiffanyh 16 hours agoprevTL;DR;The flow of how modern day time is sourced & relayed to your computer:1. Based on quantum &#x2F; atom movement -> units -> time2. Atomic clock based on #13. Time from #2, relayed to US Naval Observatory Alternate Master Clock4. Time from #3, relayed to Space Force Base5. Time from #4, relayed to GPS6. Time from #5, relayed to NTP7. Time from #6, relayed to your home computer reply distract8901 14 hours agoprevDoes anyone have a good explainer for how the NTP protocol works? I can&#x27;t quite wrap my head around how you could possibly synchronize two machines in time over a network with unknown and unpredictable latency. reply LeoPanthera 14 hours agoparentNTP uses the \"intersection algorithm\":https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intersection_algorithm reply mhh__ 14 hours agoparentprevIt&#x27;s not quick but \"Computer Network Time Synchronization\" by Mills reply LVB 14 hours agoparentprevSpecifically on the latency question, have a look at https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;18779822 for a basic explanation. tldr, once you allow for two-way communication you can start to factor out the network delay. reply b8 15 hours agoprevIs there a way to get time to be 99% or 100% accurate? time.gov shows that my Win11 and Android Pixel are off by almost a second. It&#x27;d be cool if it could grab it from the atomic clock. reply zokier 13 hours agoparent99% accurate is pretty vague, but in terms of timekeeping 1% of 24 hours is still almost 15 minutes so being off by a second is couple of orders of magnitude better. Just to give some perspective.NTP definitely should be able to keep the clock correct to sub-second level, but for more accurate local clock something like Open Time Card would do the trick, it has local atomic clock together with GPS receiver to get pretty much reference quality time. reply crote 13 hours agoparentprevInstall a GPS module in your computer.Your Android phone is already capable of receiving GPS, so that&#x27;s probably the most readily-available accurate time source. Getting your Android phone to sync to GPS time instead of just displaying it in an app might be a bit tricky, though... reply LeoPanthera 15 hours agoparentprevI think this is a quirk of Windows and Android machines, which do not aim for perfect precision.macOS is generally accurate to less than a tenth of a second (assuming desktops - laptops maybe less so, as they sleep a lot), and Linux will be just as accurate as long as it is running ntpd and not systemd-timesyncd. reply BenjiWiebe 7 hours agorootparentI think chronyd is the one you want running on your Linux computer. IIRC it uses NTP to adjust your system clock over time for more accuracy besides just setting the time with NTP. And has more options for more time-nutty things. reply readyplayernull 6 hours agoprevAnd Caesium gets the time from spacetime dilation. reply hansoolo 10 hours agoprevAll of this beautiful discussion here tells me: time is a man made thing! reply karol 16 hours agoprevWe just create labels, which are rooted in Earths&#x27; rotation around the Sun at regular intervals measured by radiation and call it time. reply cmurf 16 hours agoprevAre smartphones using GPS for time, or NTP? reply PeterisP 10 hours agoparentCell tower time. Modern cell communications use time-slot multiplexing with transmission slots in the scale of a millisecond and need the beginning&#x2F;end of transmission to have microsecond-scale accuracy so that you don&#x27;t miss stuff and don&#x27;t step on the toes of the previous or next transmission, they need to adjust for light speed propagation, etc.So as a byproduct of needing to continuously sync time with the cell tower in order to function properly, your phone has quite accurate time; your computer might easily be half a second off of &#x27;true&#x27; time, but your phone (at least on the broadband chip level - the main OS can not care) can&#x27;t be even a millisecond off. reply adrianmonk 15 hours agoparentprevI&#x27;m pretty sure the cell network itself can provide time. Not sure if smartphones use it.I think older cell phones that didn&#x27;t have GPS or a data plan (voice only) did use it. ~15 years ago, I had an old flip phone that had an option to set the time manually or automatically, and T-Mobile \"helpfully\" provide a time source that was like 5 minutes slow. reply fanf2 14 hours agoparentprevThe time reference inside a cell tower is usually PTP reply Shakahs 9 hours agorootparentGPS seems like a better choice for a cell tower. reply ThinkingGuy 15 hours agoparentprevYes :) reply qbxk 15 hours agoprevmore importantly, where does my computer let the time go? reply dghughes 18 hours agoprev [–] My cat is crazy accurate for time down to the minute. I can be sitting reading, on the web, or watching a movie all of which are random and not repeated at any specific time. Yet at 9pm exactly any time of the year she sits by the stool and complains if I am not there to give her a treat at 9pm Atlantic time.Note she does get thrown off by seasonal time changes in the fall and spring but she only needs about a week to reset. reply diggan 18 hours agoparentSame with my dogs! One of them come and puts her paw on me at exactly 20:00 every day, down to the minute as well, to remind me that it&#x27;s foodie time.Maybe I could use my dog instead of NTP and have her press a button that syncs my computers to exactly 20:00? Would work offline at least. reply augusto-moura 17 hours agorootparentIt gives me an idea of training my dog to hit a button to get food and eventually plot the data onto a graph. Would be funny to draw some patterns from it reply hellotheretoday 16 hours agorootparentprevWe started using an automated feeder with our dog. It broke one day and we were surprised to see that he was prompting us to feed him almost exactly at the programmed times. Like down to the minute.Not sure if he’s relying on other sensory information like certain smells or sounds. I don’t believe that’s the case; we didn’t replace the broken feeder for 3-4 months and he was able to keep time within a few minutes during that period. Our behavior is erratic and changes often; we work jobs with very inconsistent schedules (thus the automatic feeder) so it’s likely not that our behavior is prompting him as well. We can even observe him consistently going to his feeding area on the security camera at the correct time when no one is home. Interesting stuff! reply jvanderbot 15 hours agorootparentCircadian cycles are pretty reliable in terms of timekeeping. I end up upstairs every day for lunch at about the same time, and I always find myself in the kitchen grabbing a diet coke at about 130 because I used to grab one after a 1pm meeting for the longest time. reply hellotheretoday 14 hours agorootparentDoes that hold true for animals though? Modern humans sleep on a pretty consistent schedule but my dog sleeps randomly throughout the day. And unfortunately for him my sleep schedule is utter chaos so he is often up very lateAnd to further make it weird: our vet told us to feed him multiple small feedings throughout the day so the feeder was programmed for 6 feedings with 2 hour intervals from 9am to 9pm. He hit the mark for all feeding times!I still think there is potentially some sort of external prompt(s) though. Circadian rhythm is an excellent idea. Maybe that combined with something hard to detect, like lighting levels (which would explain why the timing shifted a few minutes over a few months). Who knows! reply dghlsakjg 16 hours agorootparentprevThat’s so interesting. My dog runs on a solar clock. He starts begging for his dentastick when it gets dark out, and stays in bed until the sun comes up in winter. reply mrb 15 hours agorootparentprevThink about the number of pets doing this at, say, 20:07, and owners not realizing the time accuracy because it&#x27;s not a round number of minutes after the hour. reply renewiltord 15 hours agorootparentThere are circadian rhythm genes in c. elegans that take effect even when under artificial light. Also the skill for this is trainable.At school we used to have a bell mark class ends and without a clock or a watch I could predictably tell when the bell would fire. One time I demonstrated this to a friend (both of us kicked out of class) by counting down from 10 on the second to when the bell rang while looking at a blank wall.Strange. But nonetheless true. reply clord 15 hours agorootparentprevI suspect in cases like this the dog is hearing something you don&#x27;t in the environment and has associated it with treat time, creating the expectation. If you reconfigure NTP to use her intuition, you risk biasing whatever the source is, creating a feedback loop that will create drift. reply m463 15 hours agorootparentprevmaybe add an NTP reference clock to biff¹...or add it to systemd (it will get there eventually anyway)[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Biff_(Unix)#Origin_and_name reply stronglikedan 18 hours agoparentprevMy dog knows the days of the week too. She knows that Thursday is brewery night, and Sunday is a visit to grandma&#x27;s. She get confusedly persistent if either event is cancelled. reply cj 17 hours agorootparentMy dog is the same. I have a friend who spends the day at my house every Thursday. The dog sits by the door waiting, but only on Thursdays! reply fuzzfactor 16 hours agorootparentI found out my cat could count to four once every fourth day was salmon day. reply xkcd-sucks 17 hours agoparentprevIt&#x27;s written, and seems plausible, that cat territory is bounded by time as well as space; for example one cat might own a place in the morning while another cat owns the same place in the evening, etc. reply aequitas 17 hours agorootparentThere was this BBC documentary where they tracked cats with GPS called The secret life of cats where they found this behavior. The cats would also visit each others house at different time and eat from each others food. reply qbxk 15 hours agorootparentprevcat law sounds hard. cat lawyers must make a fortune litigating in cat court reply jesterpm 16 hours agoparentprevIt&#x27;s even weirder with people: blood sugar level change with how you perceive time to be passing, not the actual amount of time: https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;10.1073&#x2F;pnas.1603444113 reply BirAdam 18 hours agoparentprevMy guinea pig will get really really loud and persistent if she doesn&#x27;t get her vitamin c laced hay biscuit at 7AM EST. I have no idea how she knows what time it is, but she&#x27;s super accurate about it as well. reply TomK32 16 hours agorootparentOnly one? It&#x27;s recommended to keep at least two as they are very social animals.My four live in the garden, well protected and I&#x27;m too chaotic to keep any sort of regular feeding schedule, but they are fine with that, must be exciting for them if an unexpected feed of carrots or cucumbers drops. reply BirAdam 9 hours agorootparentJust the one as her sister died, and then she tried to kill every other I ever tried to pair her with. My dog gets along with her well though, and she gets a lot of human attention. reply TomK32 4 hours agorootparentAh, yeah the old problem of death. Happened with my first pair of siblings but I added three from piggies from the animal shelter and though it took a while for them to settle in, it worked. Just like with us humans, it becomes easier to fit in with a larger group. reply switch007 16 hours agorootparentprevTheir guinea pig may cohabit or socialise with non Guinea pigs. Eg rabbits reply TomK32 4 hours agorootparentInterestingly keeping piggies with rabbits is not allowed in every country. For example here in Austria it&#x27;s not allowed (according to the 2. Tierhaltungsverordnung Anlage 1 ( 3.6)), while in my native Germany it is. reply hinkley 17 hours agoparentprevOur dogs meanwhile get fed at 5:00 and every day they think it must be 5:00 at 4:15-4:25, so it seems my dogs may be Martians. reply Jagerbizzle 17 hours agorootparentExactly the same here with my golden doodle. We feed her dinner at 4pm and she’s pretty much always off-by-one and comes to check on the status at 3. reply hinkley 17 hours agorootparentMaybe retrievers are bad at time. The ring leader is a lab. reply Arrath 16 hours agorootparentprevGirlfirend&#x27;s minpin-chihuahua mix is like this. Thinks its breakfast time well before it is, indeed, time for breakfast. reply ComputerGuru 15 hours agoparentprevIt&#x27;s not just cats; I think humans are capable of much of the same but we actively suppress it for $reasons.Any time I have an alarm in the middle of the night for any random hh:mm, after just a few days of the same pattern I will naturally wake up exactly 1 or 2 minutes before the alarm as my internal clock knows what to expect. If I ignore it out of laziness and go back to sleep until the alarm rings (literally a minute later) I can break the habit but if I embrace it, it is really accurate and reliable (though thrown off if I went to bed absolutely exhausted, so there are limits as one would naturally expect). reply m463 10 hours agorootparentReminds me of the feynman book \"What do you care what other people think?\"There&#x27;s a chapter:“It’s as Simple as One, Two, Three…”where he talks about and experiments with mental counting and mental time judgementI decided to investigate. I started by counting seconds—without looking at a clock, of course—up to 60 in a slow, steady rhythm: 1, 2, 3, 4, 5…. When I got to 60, only 48 seconds had gone by, but that didn’t bother me: the problem was not to count for exactly one minute, but to count at a standard rate. The next time I counted to 60, 49 seconds had passed. The next time, 48. Then 47, 48, 49, 48, 48…. So I found I could count at a pretty standard rate.Now, if I just sat there, without counting, and waited until I thought a minute had gone by, it was very irregular—complete variations. So I found it’s very poor to estimate a minute by sheer guessing. But by counting, I could get very accurate.he goes on to do all kinds of other experiments like counting while running up and down stairs and more... :) reply glonq 15 hours agoparentprevI better check the oscillator inside my cats, because they want dinner at 4pm plus or minus a half hour. reply geek_at 18 hours agoparentprevPawlow would have something to say about that reply miohtama 17 hours agoparentprevCan my computer get time from your cat? (: reply costcofries 15 hours agoparentprev [–] They are creatures of habit replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post provides an in-depth exploration of the sources and systems computers utilize to keep accurate time, starting from Network Time Protocol (NTP) servers to high-level international measurement systems.",
      "The discussion also delves into the history of time measurement, shedding light on the shift from astronomical-based timekeeping to the more precise atomic clocks.",
      "The author clarifies that contrary to popular belief, computers do not source their time from the Royal Greenwich Observatory."
    ],
    "commentSummary": [
      "The discussions prominently cover technical aspects including source of time for computers, utilization of blockchain, and challenges related to time synchronization.",
      "There is an exploration of the accuracy of clocks in cars and the inconvenience of manually adjusting them, along with details about institutions and technologies involved in timekeeping.",
      "It concludes with an examination of how handheld devices like smartphones determine and synchronize time, briefly touching upon the topic of the animal's time perception through anecdotes about pets."
    ],
    "points": 713,
    "commentCount": 217,
    "retryCount": 0,
    "time": 1696513357
  },
  {
    "id": 37777050,
    "title": "HTTP/3 adoption is growing rapidly",
    "originLink": "https://blog.apnic.net/2023/09/25/why-http-3-is-eating-the-world/",
    "originBody": "Skip to content LOG IN Blog search Search ADVANCED WHOIS MAKE A PAYMENT Get IP Manage IP Training Events Insights Community Blog Help Centre About Contact Skip to the article Why HTTP/3 is eating the world By Robin Marx on 25 Sep 2023 Category: Tech matters Tags: Guest Post, HTTP, QUIC Blog home Adapted from Sven Mieke's orginal at Unsplash. The HyperText Transfer Protocol (HTTP) is a cornerstone of the Internet, helping to load web pages, stream videos, and fetch data for your favourite apps. Last year a new version of the protocol, HTTP/3, was standardized by the Internet Engineering Task Force (IETF), the organization in charge of defining Internet technologies. Since then, HTTP/3 and the related QUIC protocol have seen a rapid uptake on the public web. The exact numbers depend on the source and measurement methodology, with HTTP/3 support ranging from 19% to 50+% of web servers and networks worldwide. A Flourish chart Because these new protocols are heavily used by large companies such as Google and Meta, we can safely say that a large chunk of current Internet traffic already uses HTTP/3 today. In fact, the blog post you’re reading right now was probably loaded over HTTP/3! In this series, I’ll provide some context on what problems HTTP/3 solves, how it performs, why it’s seen such swift adoption, and what limitations it is still working to overcome. Why do we need HTTP/3? A network protocol describes how data is communicated between two entities on the network, typically the user’s device and a web server. As there are many different companies building software for the web, the protocol needs to be standardized so that all this software can be ‘interoperable’, that is, they can all understand each other because they follow the same rules. In practice, we don’t use a single protocol but a combination of several at the same time, each with its own responsibilities and rules (Figure 1). This is to make things flexible and reusable — you can still use the exact same HTTP logic, regardless if you’re using Wi-Fi, cable, or 4G/5G. Figure 1 — The protocol stack for HTTP/2 and HTTP/3, showing how multiple protocols are combined to deliver the full Internet functionality. Many of the original protocols for the Internet were standardized in the 80s and 90s, meaning they were built with the goals and restrictions of those decades in mind. While some of these protocols have stood the test of time, others have started to show their age. Most problems have been solved by workarounds and clever tricks. However, it was clear something would have to change. This is especially true for the Transport Control Protocol (TCP), which ensures your data reliably gets across the Internet. Why TCP is not optimal for today’s web HTTP/1.1 and HTTP/2 rely on TCP to successfully do their job. Before a client and server can exchange an HTTP request/response, they must establish a TCP connection. Over time, there have been many efforts to update TCP and resolve some of its inefficiencies — TCP still loads webpages as if they were single files instead of a collection of hundreds of individual files. Some of these updates have been successful, but most of the more impactful ones (for example, TCP multipath and TCP Fast Open) took nearly a decade to be practically usable on the public Internet. The main challenge with implementing changes to TCP is thousands of devices on the Internet all have their own implementation of the TCP protocol. These include phones, laptops, and servers, as well as routers, firewalls, load balancers, and other types of ‘middleboxes’. As such, if we want to update TCP, we have to wait for a significant portion of all these devices to update their implementation, which in practice can take years. The QUIC solution This became a problem to the point that the most practical way forward was to replace TCP with something entirely new. This replacement is the QUIC protocol, though many still (jokingly) refer to it as TCP 2.0. This nickname is appropriate because QUIC includes many of the same high-level features of TCP but with a couple of crucial changes. The main change is that QUIC heavily integrates with the Transport Layer Security (TLS) protocol. TLS is responsible for encrypting sensitive data on the web — it’s the thing that provides the S (secure) in HTTPS. With TCP, TLS only encrypts the actual HTTP data (Figure 2). With QUIC, TLS also encrypts large parts of the QUIC protocol itself. This means that metadata, such as packet numbers and connection-close signals, which were visible to (and changeable by) all middleboxes in TCP, are now only available to the client and server in QUIC. Figure 2 — Encryption differences between TCP+TLS and QUIC. QUIC encrypts much more than just the HTTP data. Furthermore, because QUIC is more extensively encrypted, it will be much easier than it was for TCP to change it or to add new features — we only need to update the clients and servers, as the middleboxes can’t decrypt the metadata anyway. This makes QUIC a future-proof protocol that will allow us to more quickly solve new challenges. Of course, this extra encryption is good for the general security and privacy of the new protocol too. While TCP + TLS are perfect for securing sensitive personal data, such as credit cards or email content, they can still be vulnerable to complex (privacy) attacks, which have become ever more practical to execute due to recent advances in AI. By further encrypting this type of metadata, QUIC is more resilient to sophisticated threat actors. QUIC also has many other security-related features, including defences against Distributed Denial of Service (DDoS) attacks, with features such as amplification prevention and RETRY packets. Finally, QUIC also includes a large amount of efficiency and performance improvements compared to TCP, including a faster connection handshake (see Figure 3), the removal of the ‘head-of-line blocking’ problem, better packet loss detection/recovery, and ways to deal with users switching networks (I’ll go into more detail on this in my next post). Figure 3 — QUIC has a faster connection setup, as it combines the ‘transport’ three-way handshake with the TLS cryptographic session establishment, which in TCP+TLS are two separate processes. We didn’t need HTTP/3; what we needed was QUIC Initially, there were attempts to keep HTTP/2 and make minimal adjustments so we could also use QUIC in the lower layers (after all, that’s the whole point of having these different cooperating and reusable protocols). However, it became clear QUIC was just different enough from TCP to make it HTTP/2-incompatible. As such, the decision was made to make a new version of HTTP, just for QUIC, which eventually became HTTP/3. HTTP/3 is almost identical to HTTP/2. They mainly differ in the technical implementation of the features on top of QUIC or TCP. However, because HTTP/3 can use all of QUIC’s new features, it is expected to be more performant when loading web pages and streaming videos. In practice, it’s especially this aspect that has led to HTTP/3’s rapid adoption. In my next post, I’ll go into more detail on a common connectivity problem you’ve most likely experienced and how QUIC can help reduce calls and videos from cutting out when your mobile device changes from using Wi-Fi to cellular connectivity. Robin Marx is a Web Protocol and Performance Expert at Akamai. This post was originally published on the Internet Society’s Pulse Blog. Rate this article Rate this (47 Votes) The views expressed by the authors of this blog are their own and do not necessarily reflect the views of APNIC. Please note a Code of Conduct applies to this blog. Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Save my name and email in this browser for the next time I comment. Yes, add me to your mailing list Notify me of follow-up comments via email. You can also subscribe without commenting. Top Get Updates Email * Show options Select list(s): Daily Weekly Authors Adli Wahid Aftab Siddiqui More Tags Related Articles Service exhaustion floods — HTTP/HTTPS flood, HTTP… by Debashis Pal November 18, 2022 Guest Post: Learn about three different types of DDoS attacks and how to defend against them. HTTP/3 and QUIC — prioritization and head-of-line blocking by Constantin Sander November 30, 2022 Guest Post: Moving streams from the application layer to the transport layer with HTTP/3/QUIC opens a new parameter space for new optimizations. It’s time to reconsider selective FEC with high-priority… by Nooshin Eghbal August 30, 2022 Guest Post: Forward Error Correction can reduce the need for retransmission times in QUIC by >100 ms. Browser-powered desync attacks: A new frontier in HTTP… by James Kettle October 28, 2022 Guest Post: The vulnerabilities that led to the discovery of browser-powered desync attacks. APNIC Home Connect with us Facebook Twitter YouTube Flickr Weibo Slideshare LinkedIn RSS © 2023 APNICABN 42 081 528 010 Privacy Contact Help Centre NRO News Service Status Careers",
    "commentLink": "https://news.ycombinator.com/item?id=37777050",
    "commentBody": "HTTP&#x2F;3 adoption is growing rapidlyHacker NewspastloginHTTP&#x2F;3 adoption is growing rapidly (apnic.net) 564 points by skilled 23 hours ago| hidepastfavorite431 comments bdd8f1df777b 19 hours agoAs a Chinese user who regularly breaches the GFW, QUIC is a god send. Tunneling traffic over a QUIC instead of TLS to breach the GFW has much lower latency and higher throughput (if you change the congestion control). In addition, for those foreign websites not blocked by GFW, the latency difference between QUIC and TCP based protocol is also visible to the naked eye, as the RTT from China to the rest of the world is often high. reply apatheticonion 9 hours agoparentI want to visit China but am afraid I will not be able to breach the GFW (as other friends have not been able to).Any resources you can point me to to help me be more successful? reply gamepsys 8 hours agorootparentYou won&#x27;t casually breach the GFW. I would treat any advice posted publicly on the internet about how to breach the GFW as probably-malicious. They are better at networking than you are. reply qingdao99 5 hours agorootparentYou will casually breach the GFW if the VPN you pay for is not blocked yet. reply hker 5 hours agorootparentBeware when using VPN to breach the GFW. Recently a Chinese netizen had to pay over 1 million yuan (>145K USD) for using VPN [1][2]. Before this incident, only VPN service sellers were prosecuted [3]. Beware when doing this casually.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37787205 \"Chinese Netizen Fined Over 1 Million Yuan for Using VPN\"[2]: https:&#x2F;&#x2F;here.news&#x2F;post&#x2F;93c46bbd-ea0d-48e2-bba6-135e58887f81&#x2F;... \"Chinese Netizen Fined Over 1 Million Yuan for Using VPN\"[3]: https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;chinese-man-arrested-after-mak... \"Chinese man arrested after making $1.6 million from selling VPN services\" reply bdd8f1df777b 3 hours agorootparentA foreigner won’t be treated in the same way. The Chinese government is cruel to its own people but quite friendly to outsiders. reply hker 3 hours agorootparentForeigners need to worry about the new Chinese anti-espionage law instead [1]: at least 17 Japanese nationals have been recently accused of spying in China [2], and a US citizen jailed for life [3]. The German car industry is worried [4]. The law broadens the scope beyond what it originally sought to prohibit – leaks of state secrets and intelligence – to include any “documents, data, materials, or items related to national security and interests.” [1]Beware when bypassing the GFW.[1]: https:&#x2F;&#x2F;theconversation.com&#x2F;chinas-new-anti-espionage-law-is... \" China’s new anti-espionage law is sending a chill through foreign corporations and citizens alike\"[2]: https:&#x2F;&#x2F;www.dw.com&#x2F;en&#x2F;japanese-companies-fear-chinas-draconi... \"Japanese companies fear China&#x27;s draconian espionage laws\"[3]: https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;09&#x2F;11&#x2F;china&#x2F;china-john-leung-mms-sp... \" Beijing claims US citizen jailed for life in China was decorated spy who worked undetected for decades\"[4]: https:&#x2F;&#x2F;www.reuters.com&#x2F;business&#x2F;autos-transportation&#x2F;german... \"German car industry urges Berlin to address anti-spy laws with Beijing\" replyHWR_14 8 hours agorootparentprevIf you&#x27;re only visiting, maybe don&#x27;t attempt to breach it? For two weeks I would just plan on not using the normal internet. reply smackeyacky 4 hours agorootparentYour phone will be a brick for the entirety of your stay. A vpn is the only way to access google services in china. reply frizlab 4 hours agorootparentThere are other services than google though. Why would the phone be a brick? reply resolutebat 8 hours agorootparentprevData roaming on a non-Chinese SIM card is the simplest approach, and perfectly sufficient if all you want to do is use Google search etc. reply tim333 8 hours agorootparentprevThis may work https:&#x2F;&#x2F;github.com&#x2F;trojan-gfw&#x2F;trojan reply lolinder 8 hours agorootparentI concur with your sibling commenter:> I would treat any advice posted publicly on the internet about how to breach the GFW as probably-malicious. They are better at networking than you are.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37786214 reply firen777 8 hours agorootparentprevI&#x27;m still experimenting and reading through the documentation, but I&#x27;m semi certain Project X is also compatible with Trojan:https:&#x2F;&#x2F;github.com&#x2F;XTLS reply sixseven 6 hours agorootparentprevThere are quite a few VPN providers I know that work reliably in China. reply Loranubi 8 hours agorootparentprevSSH tunneling always works for me. reply xtian 8 hours agorootparentprevPay for an international roaming data plan if it&#x27;s so important. reply svara 11 hours agoparentprevWhat do you need to change for congestion control? reply bdd8f1df777b 6 hours agorootparentThe default congestion control is CUBIC, which is very slow for connections between China and the rest of the world. Google&#x27;s BBR is a great improvement, and sometimes I use \"brutal\" congestion control, which is basically a constant speed. reply girishso 10 hours agoparentprevAny specific tool you can recommend? reply bdd8f1df777b 6 hours agorootparenthysteria and tuic. Both are based on QUIC. But you need a server outside China though. reply meowtimemania 15 hours agoparentprevRTT? reply Dylan16807 15 hours agorootparentRound-trip time. reply ta1243 22 hours agoprevI have a corporate laptop which funnels all traffic through zscaler.I was somewhat surprised when I was getting a different IP address on ipinfo.io (my home IP) compared with whatsmyip.org (a zscaler datacentre IP)Curlling ipinfo.io though came back with the zscaler address.Turns out they don&#x27;t funnel UDP via zscaler, only TCP.Looking into zscaler, https:&#x2F;&#x2F;help.zscaler.com&#x2F;zia&#x2F;managing-quic-protocol> Zscaler best practice is to block QUIC. When it&#x27;s blocked, QUIC has a failsafe to fall back to TCP. This enables SSL inspection without negatively impacting user experience.Seems corporate IT is resurging after a decade of defeat reply wkat4242 22 hours agoparentZscaler is pure crap, we use it at work too. It&#x27;s especially hard to configure docker containers for its proxy settings and ssl certificate.When I test something new in our lab I spend 10 minutes installing it and half a day configuring the proxy. reply brewmarche 21 hours agorootparentMan, here I am reading this while fighting zScaler when connecting to our new package repository (it breaks because the inspection of the downloads takes too long). No one feels responsible for helping developers. Same with setting up containers, git, Python, and everything else that comes with its own trust store, you have to figure out everything by yourself.It also breaks a lot of web pages by redirecting HTTP requests in order to authenticate you (CSP broken). Teams GIFs and GitHub images have been broken for months now and no one cares. reply wkat4242 21 hours agorootparentAhhhh so that&#x27;s why my teams gifs don&#x27;t work. Thanks.We use an external auth provider which makes even more complex config yeah. reply brewmarche 20 hours agorootparentAt least for me that’s the problem. When I open the redirect url manually it also fixes the problem for some time.You can open the Teams developer tools to check this. Click the taskbar icon 7 times, then right click it. Use dev tools for select web contents, choose experience renderer AAD. Search for GIFs in Teams and monitor the network tab reply caerwy 21 hours agorootparentprevamen, brother! reply mongol 21 hours agorootparentprevIt is the single most annoying impediment in corporate IT. And you are on your own when you need to work around the issues it causes. Is it really providing value, or is it just to feel better about security? reply steve_taylor 18 hours agorootparentIt&#x27;s not just an impediment. It&#x27;s corporate spyware and possibly a prototype for Great Firewall 2.0. reply ippi72 18 hours agorootparentprevWhich zscaler products does your company use? Do you have an idea of what better solutions are out there? reply wkat4242 14 hours agorootparentThe cloud service. I don&#x27;t know what it&#x27;s called exactly. It just says \"Zscaler\".In terms of better solutions, I would prefer a completely different approach. Securing the endpoint instead of the network. Basically the idea of Google&#x27;s \"BeyondCorp\".What happens now is that people just turn off their VPN and Zscaler client to avoid issues, when they&#x27;re working from a public hotspot or at home. In the office (our lab environment) we unfortunately don&#x27;t have that option.But by doing so they leave themselves much more exposed than when we didn&#x27;t have Zscaler at all. reply fein 22 hours agorootparentprevIt causes minor annoyances with ssl + maven as well, which can be fixed by -Dmaven.wagon.http.ssl.insecure=true.Well, at least they tried I guess. reply ta1243 16 hours agorootparentNo, setting any variable including the line\"http.ssl.insecure=true\"Is not a fix under any circumstance. reply Thiez 6 minutes agorootparentSure it is. The org insists on making your life difficult, and you just want to get your work done. If they really cared about security they would prioritise fixing stuff like this, but they don&#x27;t, so you know they don&#x27;t really care, it&#x27;s just for show and a need for control.And if they don&#x27;t really care about security, why should you? reply jarym 21 hours agoparentprev> without negatively impacting user experienceI can&#x27;t stop laughing. reply betaby 12 hours agoparentprevYou can try to block zscaller ( or netskope) IP on you home router. Most of the times IT laptops are default to &#x27;normal&#x27; web behaviour if zscaller&#x2F;netskope is not available. reply fsniper 22 hours agoparentprevI hate corporate IT. Security with decades old arcane practices. Killing user experience with any way possible. MITM all around.. reply Bluecobra 22 hours agorootparentBlame viruses, malware, phishing, ransomware, etc. IT has a responsibility to keep the network secure. Google is already experimenting with no Internet access for some employees, and that might be the endgame. reply blkhawk 21 hours agorootparentThis has nothing to do with security and more with ineffective practices based on security where nobody knows why its done just that its done. Running MitM on connections basically breaks basic security mechanism for some ineffective security theater. This is basically \"90-day password change\" 2.0. reply Bluecobra 17 hours agorootparentMitM can absolutely stop threats if done correctly. A properly configured Palo Alto firewall running SSL Decryption can stop a random user downloading a known zero-day package with Wildfire. Not saying MitM is an end all be all, but IMHO the more security layers you have the better.At the end of the day, it&#x27;s not your network&#x2F;computer. There&#x27;s always going to be some unsavvy user duped into something. If you don&#x27;t like corporate IT, you&#x27;re free to become a contractor and work at home. reply fsniper 17 hours agorootparent\"A properly configured Palo Alto firewall running SSL Decryption can stop a random user downloading a known zero-day package with Wildfire.\"Instead that Corp IT should have put a transparently working antivirus&#x2F;malware scanner on the workstation that would prevent that download to be run at all. ?DPS&#x2F;MITM are not security layers but more of privacy nightmares. reply EvanAnderson 15 hours agorootparent> Instead that Corp IT should have put a transparently working antivirus&#x2F;malware scanner on the workstation that would prevent that download to be run at all. ?Sure. Then come the complaints that this slows down endpoint devices and has compatibility issues. Somebody gets the idea to do this in the network. Rinse. Repeat. reply fsniper 15 hours agorootparentOur CorpIT has that and fine tuned it to perfection. No one complains now. So it&#x27;s possible.Unfortunately they still do MITM which breaks connections regularly. reply EvanAnderson 15 hours agorootparentIt&#x27;s a knife&#x27;s edge. One OS patch, or one vendor change in product roadmap, and you can be right back to endpoint security software performance and compatibility hell. Stuff has gotten better but it&#x27;s still fraught with peril. reply Bluecobra 12 hours agorootparentprevI disagree, I think you should have both as an endpoint scanner (either heuristics or process execution) may not catch anything. (for example a malicious Javascript from an advertisement)Why do you care so much about your privacy while you&#x27;re on company time using their computers, software, and network? If you don&#x27;t like it, bring your own phone&#x2F;tablet&#x2F;laptop and use cellular data for your personal web browsing. FWIW, it&#x27;s standard practice to exempt SSL decryption for banking, healthcare, government sites, etc. reply rixed 6 hours agorootparentNobody complained that they couldn&#x27;t browse Reddit privately. Everybody was complaining that they couldn&#x27;t perform their work. reply Spivak 18 hours agorootparentprev> where nobody knows why its done just that its doneCompliance. You think your IT dept wants to deploy this crap? How ever painful you think it is as an end user multiply it having to support hundreds&#x2F;thousands of endpoints.Look, I hate traffic inspection as much as the next person but this is for security, it&#x27;s just not for the security you want it to be. This is so you have an audit trail of data exfiltration and there&#x27;s no way around it. You need the plaintext to do this and the whole network stack is built around making this a huge giant pain in the ass. This is one situation where soulless enterprises and users should actually be aligned. Having the ability in your OS to inspect the plaintext traffic of all incoming and outgoing traffic by forcing apps off raw sockets would be a massive win. People. seem to understand how getting the plaintext for DNS requests is beneficial to the user but not HTTP for some reason.Be happy your setup is at least opportunistic and not \"block any traffic we can&#x27;t get the plaintext for.\" reply steve_taylor 18 hours agorootparent> You think your IT dept wants to deploy this crap?Yes, they do. reply bigstrat2003 7 hours agorootparentNo, they really really don&#x27;t. Source: I&#x27;ve worked in corporate IT for many years, and this kind of shit is always forced upon us just as much as it is on you guys. We hate it too. reply betaby 12 hours agorootparentprevCompliance with exactly what? Their own rules? reply antod 6 hours agorootparentNot the OP, but currently I work in a regulated industry (financial) where Corporate Risk and Legal depts ask for this stuff (and much more) to satisfy external auditors. The IT people hate it just as much.I had never experienced just how much power a single dept could hold until we got acquired by a large finance enterprise and had to interact with the Risk dept. reply ngrilly 19 hours agorootparentprev> IT has a responsibility to keep the network secure.Yes, but TLS inspection is not the solution.> Google is already experimenting with no Internet access for some employees, and that might be the endgame.Source? And I&#x27;m pretty sure they are not considering disconnecting most of their employees who actually need Internet for their job. reply Bluecobra 17 hours agorootparentSource: https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;07&#x2F;to-defeat-hackers-go...Eventually I think the endgame here is that you use your own personal BYOD device to browse the internet that is not able to connect to the corporate network. reply kccqzy 17 hours agorootparentprevGoogle disabling Internet access is very different from your typical company doing that. Watching a YouTube video? Intranet and not disabled. Checking your email on Gmail? Intranet and not disabled. Doing a web search? Intranet and not disabled. Clicking on a search result? Just use the search cache and it&#x27;s intranet. reply neon_electro 21 hours agorootparentprevLink for more info? That seems impossible to make work. reply mschuster91 21 hours agorootparentI read this recently for sysadmins at Google and Microsoft that have access to absolute core services like authentication, which does make sense to keep these airgapped reply eep_social 17 hours agorootparentThis sounds like a misunderstanding of the model. Usually these companies have facilities that allow core teams to recover if prod gets completely fucked e.g. auth is broken so we need to bypass it. Those facilities are typically on separate, dedicated networks but that doesn’t mean the people who would use them operate in that environment day to day. reply Bluecobra 17 hours agorootparentprevSource: https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;07&#x2F;to-defeat-hackers-go... reply pmarreck 21 hours agorootparentprevI know that they have a gigantic intranet, that might make the lack of internet during the workday less painful reply fsniper 21 hours agorootparentprevThere are real valuable practices that helps security, and there are practices just break security.Particularly MITM practice is a net negative. Rolling password resets and bad password requirements are also net negatives. Scanners which does not work as intended, which are not proofed at all and introduce slowness, feature breaks are possible negatives.Also at some places they introduce predatory privacy nightmares like key loggers, screen recorders.. reply dobin 21 hours agorootparentFull inspection of user traffic is required to implement:* Data leakage policy (DLP; insider threat, data exfiltration)* Malware scanning* Domain blocking (Gambling, Malware)* Other detection mechanisms (C2)* Logging and auditing for forensic investigations* Hunting generallyI dont see how this breaks security, and of course you also didnt elaborate on why it should be. Assumed TLS MitM is implemented reasonably correctly.Dont worry tho, zero trust will expose the company laptops again to all the malicious shit out there. reply acdha 20 hours agorootparent> I dont see how this breaks securityYou’re training users to ignore certificate errors – yes, even if you think you’re not – and you’re putting in a critical piece of infrastructure which is now able to view or forge traffic everywhere. Every vendor has a history of security vulnerabilities and you also need to put in robust administrative controls very few places are actually competent enough to implement, or now you have the risk that your security operators are one phish or act of malice away from damaging the company (better hope nobody in security is ever part of a harassment claim).On the plus side, they’re only marginally effective at the sales points you mentioned. They’ll stop the sales guys from hitting sports betting sites, but attackers have been routinely bypassing these systems since the turn of the century so much of what you’re doing is taking on one of the most expensive challenges in the field to stop the least sophisticated attackers.If you’re concerned about things like DLP, you should be focused on things like sandboxing and fine-grained access control long before doing SSL interception. reply ta1243 18 hours agorootparentA competent organisation will have a root certificate trusted on all machines so you won&#x27;t be ignoring certificate errors. You are right however that you are funnelling your entire corporate traffic unencrypted through a single system, break into that and you have hit the goldmine. reply Bluecobra 17 hours agorootparentCorrect, this is table stakes to get SSL Decryption working for any vendor. Typically we&#x27;re talking about Windows PC&#x27;s joined to Active Directory and they already trust the domain&#x27;s CA. The firewall then gets it&#x27;s own CA cert issued by the domain CA, so when you go to www.facebook.com and inspect the certificate it says it is from the firewall.Most orgs don&#x27;t inspect sensitive things like banking, healthcare, government sites, etc. Also it&#x27;s very common to make exceptions to get certain applications working (like Dropbox). reply ngrilly 20 hours agorootparentprevYes, if you want&#x2F;need to do those things, then you need to inspect user traffic. But why do you want&#x2F;need to do those things in the first place? What&#x27;s your threat model?Doing this breaks the end-to-end encryption and mutual authentication that is the key benefit of modern cryptography. The security measures implemented in modern web browsers are significantly more advanced and up-to-date than what systems like Zscaler are offering, for example in terms of rejecting deprecated protocols, or enabling better and more secure protocols like QUIC. By using something like Zscaler, you&#x27;re introducing a single point of failure and a high value target for hackers. reply Bluecobra 17 hours agorootparent> But why do you want&#x2F;need to do those things in the first place? What&#x27;s your threat model?Not everyone in a company is savvy or hard at work. Randy in accounting might spend spend an hour or more a day browsing the internet and scooping up ads and be enticed to download something to help speed up their PC which turns out to be ransomware. reply Karrot_Kream 10 hours agorootparentThis assumes Randy is incompetent, but not malicious. Nothing is stopping an attacker from contacting Randy out of band, say over a phone or personal email, and then blackmailing him to get him to hand out company information. The key here is to scope down Randy&#x27;s access so that no matter what kind of an employee he is, the only access Randy has is the minimum necessary and that all of his accesses to company information is logged for audit and threat intelligence purposes.That&#x27;s the problem with these MITM approaches. They open up a new security SPOF (what happens if there&#x27;s an exploit on your MITM proxy that an attacker uses to gain access to the entire firehose of corporate traffic) while doing little to protect against malicious users. reply midasuni 13 hours agorootparentprevIn which case as Randy only has access to a few files you simply restore the snapshot of those files and away you go. reply fsniper 19 hours agorootparentprev* Data leaks are not prevented by MITM attack. A sufficiently determined data leaker will easily find easier or elaborate ways to circumvent it. * Malware scanning can be done very efficiently at the end user workstation. ( But always done inefficiently ) * How domain blocking requires a MITM? * C2 scanning can efficiently done at the end user workstation. * Audits does not require \"full contents of communication\"Is MITM ever the answer?Stealing a valid communication channel and identity theft of remote servers is in fact break basic internet security practices. reply FuriouslyAdrift 20 hours agorootparentprevBlame the law. Companies are bound by it. Actually blame terrible programming practices and the reluctance to tie the long tail of software maintenance and compliance to the programmers and product managers that write them. reply peoplefromibiza 20 hours agorootparentprevcompanies can be held liable for what people using their networks do, so they need a way to prove it&#x27;s not their fault and provide the credentials of the malevolent actor.it&#x27;s like call and message logs kept by phone companies.nobody likes to keep them but it&#x27;s better than the breaking the law and risking for someone abusing your infrastructure.it would also be great if my colleagues did not use the company network to check the soccer stats every morning for 4 hours straight, so the company had to put up some kind of domain blocking that prevents me from looking up some algorithm i cannot recall from the top of my mind on gamedev.net because it&#x27;s considered \"gaming\" reply ric2b 45 minutes agorootparentLooking up soccer stats is not illegal so the company doesn&#x27;t have to block it.Blocking the website instead of punishing them in their performance reviews (assuming it does impact their performance, if they&#x27;re still productive why even care) is useless, they&#x27;ll use their phones and still spend time on it. reply Faaak 22 hours agoparentprevHopefully, IT doesn&#x27;t notice when I use my `kill-zscaler.sh` script. It&#x27;s horrible to work around when you arrive on a new company using it. reply ta1243 20 hours agorootparentThe only reason I have one is so I can prove the problem is with zscaler and not my network.I remember someone complaining about the speeds writing to a unc file share over a 10G network, using a blackmagic file writing test toolIt was really slow (about 100MB&#x2F;sec), but iperf was fine.I did a \"while (true) pkill sophos\" or similar. Speeds shot upto about 1GB&#x2F;sec.Closed the while while loop, sophos returned in a few seconds, and speeds reduce to a crawl again.But who needs to write at a decent speed in a media production environment.And people still wonder why ShadowIT constantly wins with the business. reply rcstank 21 hours agorootparentprevWhat all is in that script? I&#x27;d love to have it. Sincerely, another dev abused by Zscaler. reply Faaak 19 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;bkahlert&#x2F;kill-zscaler reply PrimeMcFly 21 hours agorootparentprevCare to share? reply Faaak 19 hours agorootparenthttps:&#x2F;&#x2F;github.com&#x2F;bkahlert&#x2F;kill-zscaler reply PrimeMcFly 16 hours agorootparentShould have tried searching for it first I guess, thanks! replyThePhysicist 22 hours agoprevQUIC is a really nice protocol as well, I find. It basically gives you an end-to-end encrypted & authenticated channel over which you can transport multiple streams in parallel, as well as datagrams. A lost packet in a given stream won&#x27;t block other streams, and the overhead is quite low. Both ends of the connection can open streams as well, so it&#x27;s really easy to build bidirectional communication over QUIC. You can also do things like building a VPN tunnel using the datagram mechanism, there are protocols like MASQUE that aim to standardize this. Apple is using a custom MASQUE implementation for their private relay, for example.HTTP&#x2F;3 is a protocol on top of QUIC that adds a few more really interesting things, like qpack header compression. If you e.g. send a \"Content-type: text&#x2F;html\" header it will compress to 2 bytes as the protocol has a Huffman table with the most commonly used header values. I found that quite confusing when testing connections as I thought \"It&#x27;s impossible that I only get 2 bytes, I sent a long header string...\" until I found out about this. reply binwiederhier 21 hours agoparentI dabbled with QUIC a few years ago and I couldn&#x27;t agree more. It was pleasant to work with, and because it&#x27;s UDP based, suddenly you can do NAT hole punching more easily.Funny that you mentioned a VPN, because I made a little experimental project back then to hole-punch between two behind-the-NAT machines and deliver traffic between them over QUIC. I was able to make my own L2 and L3 bridges across the WAN, or just port forward from one natted endpoint to an endpoint behind a different NAT.At one point I used it to L2-bridge my company&#x27;s network (10.x) to my home network (192.168.x), and I was able to ping my home server from the bridging host, even though it was different networks, because it was essentially just connecting a cable between the networks. It was quite fun.Here&#x27;s the project if anyone is interested: https:&#x2F;&#x2F;github.com&#x2F;binwiederhier&#x2F;natter -- it&#x27;s probably defunct, but it was only experimental anyway. reply abhishekjha 20 hours agorootparentHow mcuh config did it requir on your home local network and office network to do the ping on 192.168.x.x? reply binwiederhier 20 hours agorootparentI only tested one hop, e.g. A(10.x)---->B(192.x). All I had to do there was to adapt the routing tables: On A, route 192.x traffic to the \"natter\" tap&#x2F;tun interface (I always forget which is L2), and on B, route traffic to 10.x accordingly. That&#x27;s all.For it to be routable in the entire network, you&#x27;d need to obviously mess with a lot more :-D reply heyoni 10 hours agorootparentprevTHAT was your first project in golang? I’m impressed. it looks like it might it could replace ngrok?I’ll try it later. reply binwiederhier 9 hours agorootparentWell it&#x27;s abandoned and experimental, and there are better ways to hole punch than what I did, e.g. using STUN and TURN. But yeah it could replace one ngrok use case, though I think ngrok does not do L2&#x2F;3 bridges.Also: I think technically this was my first Go program (https:&#x2F;&#x2F;github.com&#x2F;binwiederhier&#x2F;re), but that was so tiny that it doesn&#x27;t really count. ;-) reply d-z-m 21 hours agoparentprevI believe that qpack(like hpack) still has some sharp edges. As in...low entropy headers are still vulnerable to leakage via a CRIME[0] style attack on the HTTP&#x2F;3 header compression.In practice, high entropy headers aren&#x27;t vulnerable, as an attacker has to match the entire header:value line in order to see a difference in the compression ratio[1].[0]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CRIME [1]: https:&#x2F;&#x2F;www.ietf.org&#x2F;archive&#x2F;id&#x2F;draft-ietf-quic-qpack-20.htm... reply ComputerGuru 16 hours agoparentprevI’ve been trying to wrap my mind around whether (and how much) QUIC would be better than TCP for video streams that rely on order-sensitive delivery of frames, especially for frames that have to be split into multiple packets (and losing one packet or receiving it out of order would lose the entire frame and any dependent frames). We used to use UDP for mpegts packets but found TCP with a reset when buffers are backlogged after switching to h264 to be a much better option over lossy WAN uplinks, (scenario is even worse when you have b or p frames present).The problem would be a lot easier if there were a feedback loop to the compressor where you can dynamically reduce quality&#x2F;bandwidth as the connection quality deteriorates but currently using the stock raspivid (or v4l2) interface makes that a bit difficult unless you’re willing to explicitly stop and start the encoding all over again, which breaks the stream anyway. reply mleo 5 hours agorootparentQUIC handles the packet ordering and retransmission of lost packets before it is handed to the upper layer of the application. However, if you are just sending one channel of video packets through the pipe, it probably won’t buy you much more. Where QUIC additionally excels is being able to send and&#x2F;or receive multiple streams of data across the “single” connection where each is effectively independent and does not suffer head of line blocking across all streams. reply londons_explore 7 hours agorootparentprev> stop and start the encoding all over again, which breaks the stream anyway.It&#x27;s a common thing I wish encoders could do - if I try to compress a frame and the result is too large to fit in my packet&#x2F;window, I wish I could &#x27;undo&#x27; and retry compression of the same frame with different options.Sadly all hardware video compressors mutate internal state when something is compressed, so there is no way to undo. reply nly 21 hours agoparentprevThe problem is there&#x27;s no standard equivalent of the BSD sockets API for writing programs that communicate over QUIC.It&#x27;ll always be niche outside the browser until this exists. reply btown 18 hours agorootparentSince it lives on top of UDP, I believe all you need is SOCK_DGRAM, right? The rest of QUIC can be in a userspace library ergonomically designed for your programming language e.g. https:&#x2F;&#x2F;github.com&#x2F;quinn-rs&#x2F;quinn - and can interoperate with others who have made different choices.Alternately, if you need even higher performance, DPDK gives the abstractions you&#x27;d need; see e.g. https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3565477.3569154 on performance characteristics. reply rewmie 17 hours agorootparent> Since it lives on top of UDP, I believe all you need is SOCK_DGRAM, right? The rest of QUIC can be in a userspace library ergonomically designed for your programming language e.g. (...)I think that OP&#x27;s point is that there&#x27;s no standard equivalent of the BSD sockets API for writing programs that communicate over QUIC, which refers to the userspace library you&#x27;ve referred to.A random project hosted in GitHub is not the same as a standard API. reply pests 17 hours agorootparent> no standard equivalent of the BSD sockets APIDid they not answer that question? It uses the BSD sockets API with SOCK_DGRAM?Right, that random project is not a standard API - its built using a standard API. You wouldn&#x27;t expect BSD sockets to have HTTP built in... so you can find third-party random projects for HTTP implmented with BSD sockets just like you can find QUIC implmented with BSD sockets. reply jlokier 16 hours agorootparentQUIC is roughly TCP-equivalent not HTTP-equivalent, and we do have a BSD sockets API for TCP. You might be thinking of HTTP&#x2F;3 rather than QUIC; HTTP&#x2F;3 actually is HTTP-equivalent.You can turn the OP&#x27;s question around. Every modern OS kernel provides an efficient, shared TCP stack. It isn&#x27;t normal to implement TCP separately in each application or as a userspace library, although this is done occasionally. Yet we currently expect QUIC to be implemented separately in each application, and the mechanisms which are in the OS kernel for TCP are implemented in the applications for QUIC.So why don&#x27;t we implement TCP separately in each application, the way it&#x27;s done with QUIC?Although there were some advantages to this while the protocol was experimental and being stabilised, and for compatibility when running new applications on older OSes, arguably QUIC should be moved into the OS kernel to sit alongide TCP now that it&#x27;s stable. The benefit of having Chrome, Firefox et al stabilise HTTP&#x2F;3 and QUIC were good, but that potentially changes when the protocol is stable but there are thousands of applications, each with their own QUIC implementation doing congestion control differently, scheduling etc, and no cooperation with each other the way the OS kernel does with TCP streams from concurrent applications. Currently we are trending towards a mix of good and poor QUIC implementations on the network (in terms of things like congestion control and packet flow timing), rather than a few good ones as happens with TCP because modern kernels all have good quality implementations of TCP. reply pests 15 hours agorootparent> QUIC is roughly TCP-equivalent not HTTP-equivalent, and we do have a BSD sockets API for TCP. You might be thinking of HTTP&#x2F;3 rather than QUIC; HTTP&#x2F;3 actually is HTTP-equivalent.No, I understand QUIC is a transport and HTTP&#x2F;3 is the next HTTP protocol that runs over QUIC. I was saying QUIC can be userspace just like HTTP is userspace over kernel TCP API. We haven&#x27;t moved HTTP handling into the kernel so what makes QUIC special?I think it is just too early to expect every operating system to have a standard API for this. We didn&#x27;t have TCP api&#x27;s built-in originally either. reply Dylan16807 15 hours agorootparent> We haven&#x27;t moved HTTP handling into the kernel so what makes QUIC special?I feel like they answered that, but I&#x27;ll try rewording it.What makes TCP special that we put it in the kernel? A lot more of those answers apply to QUIC than to HTTP.> I think it is just too early to expect every operating system to have a standard API for this. We didn&#x27;t have TCP api&#x27;s built-in originally either.Okay, if you&#x27;re comparing to TCP by saying it&#x27;s too early then it sounds like you do already see the reasons in favor. reply rewmie 15 hours agorootparentprev> I was saying QUIC can be userspace just like (...)I think you&#x27;re too hung up on \"can\" when that&#x27;s way besides OP&#x27;s point. The point is that providing access to fundamental features through a standard API is of critical importance.If QUIC is already massively adopted them there is no reason whatsoever to not provide a standard API.If QUIC was indeed developed to support changes then there is even fewer arguments to not provide a standard API. reply btown 12 hours agorootparentprevIt occurs to me that QUIC could benefit from a single kernel-level coordinator that can be plugged for cooperation - for instance, a dynamic bandwidth-throttling implementation a la https:&#x2F;&#x2F;tripmode.ch&#x2F; for slower connections where the coordinator can look at pre-encryption QUIC headers, not just the underlying (encrypted) UDP packets. So perhaps I was hasty to say that you just need SOCK_DGRAM after all! reply Animats 11 hours agorootparentBut then Google couldn&#x27;t prioritize ad content. reply evgpbfhnr 9 hours agorootparentprevQUIC is TLS-equivalent, not TCP-equivalent.The analogy you&#x27;re looking for is something like openssl&#x27;s SSL_* API and.. it&#x27;s coming for QUIC: https:&#x2F;&#x2F;www.openssl.org&#x2F;docs&#x2F;manmaster&#x2F;man7&#x2F;openssl-quic.htm...There are dozen of libs to do it right now but I expect ultimately distro folks will want to consolidate it all to avoid redundant dependencies, so we&#x27;ll probably end up with openssl&#x2F;gnutls&#x2F;libressl doing it eventually and most apps using that.Note there were talks to add it in-kernel like kTLS, but since the certificate handling is difficult that part will be offloaded to userspace as it is with kTLS -- you won&#x27;t ever get an interface like connect() and forget about it. reply rewmie 8 hours agorootparent> QUIC is TLS-equivalent, not TCP-equivalent.From Wikipedia[1]:\"QUIC improves performance of connection-oriented web applications that are currently using TCP.[QUIC] is designed to obsolete TCP at the transport layer for many applications, thus earning the protocol the occasional nickname \"TCP&#x2F;2\".\"Also taken from the wiki page:\"QUIC aims to be nearly equivalent to a TCP connection but with much-reduced latency.\"Here is the only relevant bit regarding TLS:\"As most HTTP connections will demand TLS, QUIC makes the exchange of setup keys and supported protocols part of the initial handshake process. When a client opens a connection, the response packet includes the data needed for future packets to use encryption. This eliminates the need to set up the TCP connection and then negotiate the security protocol via additional packets.\" [1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;QUIC reply evgpbfhnr 7 hours agorootparentThe RFCs for QUIC (RFC 9000 and RFC 9001) mandate encryption.Some random stackoverflow answer[1] claims there are implementations that ignore this and allow \"QUIC without encryption\", but I&#x27;d argue that it&#x27;s not QUIC anymore -- in my opinion it&#x27;d be harmful to implement in the kernel.[1] https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;72828565&#x2F;2332808 reply lmm 9 hours agorootparentprev> So why don&#x27;t we implement TCP separately in each application, the way it&#x27;s done with QUIC?Because library&#x2F;dependency management sucked at the time, and OSes competed for developers by trying to offer more libraries; on the other side we were less worried about EEE because cross-platform codebases were rare and usually involved extensive per-OS code with #ifdefs or the like.I don&#x27;t think we would or should put TCP in the kernel if it was being made today. reply topspin 9 hours agorootparent> Because library&#x2F;dependency management sucked at the timeIt still sucks. Thus Docker et al. reply lmm 9 hours agorootparentOnly in Python (and C&#x2F;C++). No one else wants the overheard of Docker. reply Nullabillity 10 hours agorootparentprev> So why don&#x27;t we implement TCP separately in each application, the way it&#x27;s done with QUIC?Because TCP also does multiplexing, which must be handled in some central coordinating service. QUIC doesn&#x27;t suffer from that since it offloads that to UDP.You wouldn&#x27;t use a system TLS implementation either (well, technically SChannel&#x2F;NetworkTransport exist, but you&#x27;re vastly better off ignoring them). reply adwn 15 hours agorootparentprevIsn&#x27;t the point of QUIC to offer high performance and flexibility at the same time? For these requirements, a one-size-fits-all API is rarely the way to go, so individual user-space implementations make sense. Compare this to file IO: For, many programs, the open&#x2F;read&#x2F;write&#x2F;close FD API is sufficient, but if you require more throughput or control, its better to use a lower-level kernel interface and implement the missing functionality in user-space, tailored to your particular needs. reply londons_explore 7 hours agorootparentLike what exactly? Are all the cool Devs sending nvme write commands straight to the SSD now for more control? reply rewmie 15 hours agorootparentprev> Did they not answer that question? It uses the BSD sockets API with SOCK_DGRAM?No, that does not answer the question, nor is it a valid answer to the question. Being able to send UDP datagrams is obviously not the same as establishing a QUIC connection. You&#x27;re missing the whole point of the importance of having a standard API to establish network connections.> Right, that random project is not a standard API - its built using a standard API.Again, that&#x27;s irrelevant and misses the whole point. Having access to a standard API to establish QUIC connections is as fundamental as having access to a standard API to establish a TCP connection.> so you can find third-party random projects for HTTP (...)The whole point of specifying standard APIs is to not have to search for and rely on random third-party projects to handle a fundamental aspect of your infrastructure. reply dilyevsky 10 hours agorootparentprevSame thing could be said about TLS (well if you ignore kTLS) yet it’s ubiquitous. All it needs is a openssl-type of library that’s easy to integrate reply jeffbee 21 hours agorootparentprevNot having a system API is the entire point of QUIC. The only reason QUIC needs to exist is because the sockets API and the system TCP stacks are too ossified to be improved. If you move that boundary then QUIC will inevitably suffer from the same ossification that TCP displays today. reply tsimionescu 21 hours agorootparentNo, the reason QUIC exists is that TCP is ossified at the level of middle boxes on the internet. If it had been possible to modify TCP with just some changes in the Linux, BSD and Windows kernels, it would have been done. reply gjvc 19 hours agorootparentworth defining \"middlebox\"\"\"\" A middlebox is a computer networking device that transforms, inspects, filters, and manipulates traffic for purposes other than packet forwarding. Examples of middleboxes include firewalls, network address translators (NATs), load balancers, and deep packet inspection (DPI) devices. \"\"\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Middlebox reply lima 15 hours agorootparentprevBoth, really. The OS-level ossification isn&#x27;t quite as bad as the middleboxes, but is still glacially slow and bad enough to justify QUIC on its own.Case in point: the whole TCP Fast Open drama. reply Dylan16807 15 hours agorootparentprevI don&#x27;t know about that. Without middlebox problems we might have used SCTP as a basis and upgraded it. But it&#x27;s so different from TCP that I doubt we would have done it as a modification of TCP. reply xg15 19 hours agorootparentprevThe alternative is that either browsers will be the only users of QUIC - or that each application is required to bring its own QUIC implementation embedded into the binary.If ossification was bad if every router and firewall has its own TCP stack, have fun in a world where every app has its own QUIC stack. reply aseipp 17 hours agorootparentUser-space apps have a lot more avenues for timely updates than middleboxes or kernel-space implementations do though, and developers have lots of experience with it. If middleboxes actually received timely updates and bugfixes, there would be no ossification in the first place, and a lot of other experiments would have panned out much better, much sooner than QUIC has (e.g. TCP Fast Open might not have been DOA.)There&#x27;s also a lot of work on interop testing for QUIC implementations; I think new implementations are strongly encouraged to join the effort: https:&#x2F;&#x2F;interop.seemann.io&#x2F; reply Karrot_Kream 15 hours agorootparentprev> The alternative is that either browsers will be the only users of QUIC - or that each application is required to bring its own QUIC implementation embedded into the binary.This is already being done with event loops (libuv) and HTTP frameworks. I don&#x27;t see why this would be a huge issue. It&#x27;s also a boon for security and keeping software up-to-date because it&#x27;s a lot easier to patch userspace apps than it is to roll out a new kernel patch across multiple kernels and force everyone to upgrade. reply jeffbee 19 hours agorootparentprevI am not seeing the problem with every participant linking in their own QUIC implementations. The problem of ossification is there is way too much policy hidden on the kernel side of the sockets API, and vanishingly few applications are actually trying to make contact with Mars, which is the use-case for which those policies are tuned. reply xg15 19 hours agorootparentHow would you make changes to the QUIC protocol then? reply jeffbee 19 hours agorootparentI wouldn&#x27;t. I would write down the protocol in a good and extensible way, the first time. It&#x27;s no good throwing something into the world with the assumption that you can fix the protocol later. reply sophacles 19 hours agorootparentprevWhich policies are you speaking of? How are they hidden? What would you like to tweak that you can&#x27;t? reply theptip 18 hours agorootparentPresumably all the sysctls?http:&#x2F;&#x2F;www.linux-admins.net&#x2F;2010&#x2F;09&#x2F;linux-tcp-tuning.html?m=...& associated traffic shaping algorithms? reply jeffbee 17 hours agorootparentprevThere are a billion timers inside the kernel, not all of which can be changed. Some of them are #defined even.In these days when machines are very large and always have several applications running, having an external network stack in the kernel violates the end-to-end principle. All of the policy about congestion, retrying, pacing, shaping, and flow control belong inside the application.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;End-to-end_principle reply sophacles 17 hours agorootparentCan you point me to an example of a timer in the kernel that is not settable&#x2F;tunable that should be? My experience in looking at such things suggests that most of the #defined bits are because RFCs define the protocol that way.As for network stack per application: you&#x27;re more than welcome to so so a myriad of ways - linux provides many different ways to pull raw IP, or raw ethernet into userspace (e.g. xdp, tun&#x2F;tap devices, dpdk, and so on). It&#x27;s not like you&#x27;re being forced to use the kernel stack from lack of supported alternatives. replyxg15 19 hours agorootparentprev> because the sockets API and the system TCP stacks are too ossified to be improvedWhat part of the sockets API specifically do you think is ossified? Also, that doesn&#x27;t seem to have kept the kernel devs from introducing new IO APIs like io_uring. reply mmis1000 19 hours agorootparentprevI think the point of QUIC is &#x27;if the implementation other using is problematic, I can use my own. And no random middlebox will prevent me from doing so&#x27; instead of `everyone must bring their own QUIC implementation.`There is a slight different here. It&#x27;s the difference between &#x27;the right to do&#x27; and &#x27;the requirement to do&#x27;.While at same time. You must use &#x27;system tcp implementation&#x27; and you are not allowed to use custom one. Because even system allow it (maybe require root permission or something), the middlebox won&#x27;t. reply rewmie 17 hours agorootparentprev> Not having a system API is the entire point of QUIC. The only reason QUIC needs to exist is because the sockets API and the system TCP stacks are too ossified to be improved.I don&#x27;t think your take is correct.The entire point if QUIC was that you could not change TCP without introducing breaking changes, not that there were system APIs for TCP.Your point is also refuted by the fact that QUIC is built over UDP.As far as I can tell there is no real impediment to provide a system API for QUIC. reply weird-eye-issue 21 hours agoparentprev> If you e.g. send a \"Content-type: text&#x2F;html\" header it will compress to 2 bytes as the protocol has a Huffman table with the most commonly used header valuesReminds me of tokenization for LLMs. 48 dashes (\"------------------------------------------------\") is only a single token for GPT-3.5 &#x2F; GPT-4 (they use the cl100k_base encoding). I suppose since that is used in Markdown. Also \"professional illustration\" is only two tokens despite being a long string. Whereas if you convert that to a language like Thai it is 17 tokens which sucks in some cases but I suppose tradeoffs had to be made reply promiseofbeans 22 hours agoparentprevWoah that&#x27;s actually pretty neat about header compression. Thanks for sharing! reply ithkuil 22 hours agorootparentit builds on the experience gathered with the HTTP&#x2F;2 HPACK header compression. reply culebron21 22 hours agoparentprevThat makes sense -- if a packet is lost, and it affected just one asset, but you&#x27;re on TCP, then everything has to wait till the packet is re-requested and resent. reply ReactiveJelly 16 hours agorootparentFor the curious, this problem is called head-of-line blocking https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Head-of-line_blockingHTTP2 allowed multiple streams over one TCP stream, but that kinda made HoL blocking worse, because in the same scenario HTTP 1.1 would have just opened multiple TCP streams. QUIC, as GP said, basically gives you a VPN connection to the server. Open 100 streams for reliable downloads and uploads, send datagrams for a Quake match, all over one UDP &#x27;connection&#x27; that can also reconnect even if you change IPs. reply afiori 17 hours agorootparentprevI wonder why the OS does not let the application peek at upcoming incomplete data. reply londons_explore 7 hours agorootparentThere is an API for this on Linux. It&#x27;s used to checkpoint the state of a TCP connection, for example to move a live TCP connection to another machine incase you want to do zero downtime hardware replacement while keeping all TCP sockets open. reply Karrot_Kream 15 hours agoparentprevI can&#x27;t wait to start implementing RTP atop QUIC so we can stop having to deal with the highly stateful SIP stack and open a media connection the same way we open any other Application layer connection. reply Borg3 18 hours agoparentprevQUIC is junk.. You people all care about raw throughput but not about multiuser friendless. Selfish. Problem is, QUIC is UDP and so its hard to police&#x2F;shape. I really want to play some FPS game while someone is watching&#x2F;browsing web. Also, I really want my corpo VPN have a bit of priority over web, but no, now I cannot police it easly. TCP is good for best effort traffic, and thats where I classify web browsing, downloading, VoD streaming. UDP is good for gaming, voice&#x2F;video conferencing, VPNs (because they are encapsulate stuff you put another layer somewhere else). reply vlovich123 18 hours agorootparentI feel like that’s an ungenerous characterization. First, QUIC should contain some minimal connection info unencrypted that can be middleware to do some basic traffic shaping. It’s also intentionally very careful to avoid showing too much to avoid “smart” middleware that permanently ossifies the standard as has happened to TCP.Finally, traffic shaping on a single machine is pretty easy and most routers will prefer TCP traffic to UDP.Finally, the correct response to overwhelm is to drop packets. This is true for TCP and UDP to trigger congestion control. Middleware has gotten way too clever by half and we have bufferbloat. To drop packets you don’t need knowledge of streams - just that you have a non-skewed distribution you apply to dropping the packets so that proportionally all traffic overwhelming you from a source gets equally likely to be dropped. This ironically improves performance and latency because well behaving protocols like TCP and QUIC will throttle back their connections and UDP protocols without throttling will just deal with elevated error rates. reply Borg3 18 hours agorootparentSo what? You dropping packets, and they are still coming, eating BW and buckets. Because traditionally UDP did not have any flow control, you just treat is as kinda CBR traffic and so you just want to leave it queues as fast as it can. If there was a lot of TCP traffic around, you just drop packets there and vioala, congestion kick sin and you have more room for importand UDP traffic. Now, if you start to drop UDP packets your UX drops.. packet loss in FPS games is terrible, even worse than a bit of jitter. Thank you. reply KMag 17 hours agorootparentBut your complaint is about QUIC, not generic UDP. QUIC implements TCP-like flow control on top of UDP, designed to play well with TCP congestion control.QUIC does play well with others. It&#x27;s just implemented in the userspace QUIC library instead of the network stack. reply vlovich123 17 hours agorootparentprevI really don’t follow your complaint. QUIC (and other similar UDP protocols like uTP used for BitTorrent) implement congestion control. If packets get dropped, the sender starts backing off which makes you a “fair” player on the public internet.As for gaming, that remains an unsolved problem, but QUIC being UDP based isn’t any different than TCP. It’s not like middleware boxes are trying to detect specific UDP applications and data flows to prioritize protecting gaming traffic from drops, which I think is what you’re asking for. reply Dylan16807 15 hours agorootparentprevIs your complaint fundamentally that it&#x27;s harder to tell the difference between games&#x2F;voip and browser activity if you can&#x27;t just sort TCP versus UDP?That&#x27;s true, but it&#x27;s not that big of a deal and definitely doesn&#x27;t make QUIC \"junk\". Looking at the port will do 90% of the job, and from what I can tell it&#x27;s easy to look at a few bytes of a new UDP stream to see if it&#x27;s QUIC. reply Borg3 13 hours agorootparentReally? How.. Can you please tell me how I can detected QUIC looking at bytes. I could drop then only QUIC and not UDP&#x2F;443. reply Dylan16807 10 hours agorootparenthttps:&#x2F;&#x2F;quic.xargs.org&#x2F;The quick test is that the first packet is generally going to start with hex C[any nibble]00000001 and be exactly 1200 bytes long, ending with a bunch of 0s.A better test is to see if the full header makes sense, extract the initial encryption key, and check if the rest of the packet matches the signature. reply syncsynchalt 9 hours agorootparent(author of this site here)This is correct, you can recognize Initial packets easily&#x2F;reliably, and they contain the connection IDs in plaintext so that a stateful packet filter&#x2F;shaper can recognize individual data flows.Note that packet numbers (and everything else other than port&#x2F;IP) are encrypted, so you can&#x27;t do more than drop or delay the packets. But blocking&#x2F;shaping&#x2F;QoS is perfectly doable. reply Dylan16807 8 hours agorootparentI want to be clear that I was talking about checking the encryption of the very first packet, which isn&#x27;t secret yet.Once keys are already established I don&#x27;t see any particularly reliable test for a single packet, but as you say the connection ids are accessible so if they&#x27;re in the right place in both directions then that looks good for discovering a QUIC flow. replyKMag 18 hours agorootparentprevQUIC has its own flow control. It&#x27;s not raw UDP.Now, I wish ToS&#x2F;QoS were more broadly usable for traffic prioritization.It sounds like you&#x27;re using UDP vs. TCP as a proxy for ToS&#x2F;QoS. At a minimum, you&#x27;re still going to have a bad time with TCP streams getting encapsulated in UDP WireGuard VPN connections. reply cogman10 17 hours agorootparentprev> now I cannot police it easly.Somewhat the point. The issue we&#x27;ve had is that multiple ISPs and gov have been \"policing\" TCP in unsavory ways. Security and QoS are just fundamentally at odds with each other. reply syncsynchalt 9 hours agorootparentQoS is still possible with QUIC: initial connection IDs are in plaintext, and while you can&#x27;t modify packets (or even see the packet numbers) you can drop or (ugh) delay them. reply creatonez 9 hours agorootparentprevOn the level of entire networks serving multiple end consumers and businesses, I really hope that ISPs get bigger pipes instead of trying to shape traffic based on type. I&#x27;m fine with making traffic shaping on a local network a little harder, if it ends up biting those who oppose net neutrality (or want to use middleboxes to screw up traffic in various technical-debt-fuelled ways). reply johnmaguire 18 hours agorootparentprevUpvoted because I think you bring up some interesting challenges, but you might consider a softer tone in the future. (Calling the OP \"selfish\" goes against site guidelines, and generally doesn&#x27;t make people open to what you&#x27;re saying.) reply Borg3 13 hours agorootparentThat selfish was NOT the the OP. It was for general audiency who prefer all the bandwidth for themselfs. We know how most people behave. They do NOT really care what others are doing. For years, I was proud of my QoS because my entire home could utilize my (not so fast Internet) and I always could do gaming, because everything was QoS correctly. Nothing fancy, just separating TCP vs UDP and futher, doing some tuning between TCP bulk vs interactive traffic. Same went to UDP, some separation for gaming&#x2F;voip&#x2F;interactive vs VPN (bulk). HTB is pretty decent for this. reply stephen_g 5 hours agorootparentprevI don’t think they’re super valid concerns though - QUIC isn’t just dumb UDP that transmits as fast as it can, it has congestion control, pacing etc. built in that’s pretty similar to certain TCP algorithms, just it’s happening at the application layer instead of the kernel&#x2F;socket layer handling it. In the design of these algorithms, fairness is a pretty key design criteria.If anything, potentially QUIC lets people try better congestion control algorithms without having to rebuild their kernels which could make the web better if anything… reply adastra22 17 hours agorootparentprevAgreed on tone, but I don’t thinking read calling OP selfish specifically. reply kccqzy 17 hours agorootparentprevShouldn&#x27;t this be handled at a lower layer? QoS should be part of the IP layer, say using DSCP. reply culebron21 22 hours agoprevRead up until author is confusing HTTP&#x2F;1 with TCP, claiming that it&#x27;s TCP&#x27;s fault that we must make multiple connections to load a website.Actually, TCP allows continuous connection and sending as much stuff as you want -- in other words, stateful protocol. It was HTTP&#x2F;1 that was decidedly stateless.Sessions in web sites are direct consequence of the need to keep state somewhere. reply loup-vaillant 21 hours agoparentThere is though a fundamental mismatch between TCP, and the problem HTTP (any version) needs to solve: TCP is for sending stream of bytes, reliably and in the same order they were sent. HTTP needs to transmit some amount of data, reliably.The only aspect of TCP HTTP really wants here is reliability. The order we don’t really care about, we just want to load the whole web page and all associated data (images, scripts, style sheet, fonts…), in a way that can be reassembled at he other end. This makes it okay to send packets out of order, and doing so automatically solves head-of-line blocking.This is a little different when we start streaming audio or video assets: for those, it is often okay for reliability to take a hit (though any gap must be detected). Losing a couple packets may introduce glitches and artefacts, but doesn’t necessarily renders the media unplayable. (This applies more to live streams & chats though. For static content most would prefer to send a buffer in advance, and go back to sacrifice ordering in order to get a perfect (though delayed) playback.)In both use cases, TCP is not a perfect match. The only reason it’s so ubiquitous anyway is because it is so damn convenient. reply masklinn 20 hours agorootparentThat’s a different issue than what parent is talking about: HTTP definitely needs each individual resource to be ordered, what it does not need is for different resources to be ordered relative to one another, which becomes an issue when you mux multiple requests concurrently over a single connection. reply crims0n 18 hours agorootparentprev> This is a little different when we start streaming audio or video assets: for those, it is often okay for reliability to take a hit (though any gap must be detected). Losing a couple packets may introduce glitches and artifacts, but doesn’t necessarily renders the media unplayable.This is exactly what UDP is for. There is nothing wrong with TCP and UDP at the transport layer, both do their job and do it well. reply fidotron 22 hours agoparentprevThe whole HTTP request&#x2F;response cycle has led to a generation of developers that cannot conceive of how to handle continuous data streams, it&#x27;s extraordinary.I have seen teams of experienced seniors using websockets and then just sending requests&#x2F;responses over them as every architecture choice and design pattern they were familiar with required this.Then people project out from their view of the world and assume the problem is not with what they are doing but in the other parts of the stack they don&#x27;t understand, such as blaming TCP for the problems with HTTP. reply culebron21 22 hours agorootparentI switched from web dev to data science some years ago, and surprisingly couldn&#x27;t find a streaming parallelizer for Python -- every package assumes you loaded the whole dataset in memory. Had to write my own. reply goeiedaggoeie 22 hours agorootparentSame in video parsers and tooling frequently, expects a whole mp4 to be there, or a whole video to parse it, yet gstreamer&#x2F;ffmpegapi delivers the content as a stream of buffers that you have to process one buffer at a time. reply jrpelkonen 21 hours agorootparentIt is possible that this is not a fault of the parser or tooling. In some cases, specifically when the video file is not targeted for streaming, the moov atom is at the end of the mp4. The moov atom is required for playback. reply lazide 20 hours agorootparentZip files are the same. At least it makes it easy to detect truncated files? reply regularfry 18 hours agorootparentThat&#x27;s intentional, and it can be very handy. Zip files were designed so that you make an archive self-extracting. They made it so that you could strap a self-extraction binary to the front of the archive, which - rather obviously - could never have been done if the executable code followed the archive.But the thing is that the executable can be anything, so if what you want to do is to bundle an arbitrary application plus all its resources into a single file, all you need to do is zip up the resources and append the zipfile to the compiled executable. Then at runtime the application opens its own $0 as a zipfile. It Just Works. reply athanagor2 15 minutes agorootparentThis is exactly what Justine Tunney&#x27;s redbean does. reply KMag 16 hours agorootparentprevAlso, it makes it easier to append new files to an existing zip archive. No need to adjust an existing header (and potentially slide the whole archive around if the header size changes), just append the data and append a new footer. reply lazide 15 hours agorootparentInterestingly, a useful strategy for tape too, though zip is not generally considered tape friendly. replypipo234 21 hours agorootparentprevTraditionally, ffmpeg would build the mp4 container while transcoded media is written to disk (in a single contiguous mdat box after ftyp) and then put the track description and samples in a moov at the end of the file. That&#x27;s efficient because you can&#x27;t precisely allocate the moov before you&#x27;ve processed the media (in one pass).But when you would load the file into aelement, it would off course need to buffer the entire file to find the moov box needed to decode the the NAL units (in case of avc1).A simple solution was then to repackage by simply moving the moov at the end of the file before the mdat (adjusting chunk offset). Back in the day, that would make your video start instantly! reply goeiedaggoeie 21 hours agorootparentThis is basically what cmaf is. the moov and ftyp gets sent at the beginning (and frequently gets written as an init segment) and then the rest of the stream is a continuous stream of moof&#x27;s and mdat&#x27;s chunked as per gstreamer&#x2F;ffmpeg specifics. reply pipo234 19 hours agorootparentI was thinking progressive MP4, with sample table in the moov. But yes, cmaf and other fragmented MP4 profiles have ftyp and moov at the front, too.Rather than putting the media in a contiguous blob, CMAF interleaves it with moofs that hold the sample byte ranges and timing. Moreover, while this interleaving allows most of the CMAF file to be progressively streamed to disk as the media is created, it has the same CATCH22 problem as the \"progressive\" MP4 file in that the index (sidx, in case of CMAF) cannot be written at the start of the file unless all the media it indexes has been processed.When writing CMAF, ffmpeg will usually omit the segment index which makes fast search painful. To insert the `sidx` (after ftyp+moov but before the moof+mdat s) you need to repackage (but not re-encode).Same problem, same solution more or less. reply dcow 21 hours agorootparentprevI’ve found the Rust ecosystem to be very good about never assuming you have enough memory for anything and usually supporting streaming styles of widget use where possible. reply goeiedaggoeie 21 hours agorootparentha! I was literally thinking of the libs for parsing h264&#x2F;5 and mp4 in rust (so not using unsafe gstreaer&#x2F;ffmpeg code) when moaning a little here. Generally i find the rust libraries and crates to be well designed around readers and writers. reply prox 22 hours agorootparentprevIs it me or aren’t there a whole lot video specialists in general? It’s just something I noticed here and there on github. reply nullpilot 20 hours agorootparentMy experience that played out over the last few weeks lead me to a similar belief, somewhat. For rather uninteresting reasons I decided I wanted to create mp4 videos of an animation programmatically.The first solution suggested when googling around is to just create all the frames, save them to disk, and then let ffmpeg do its thing from there. I would have just gone with that for a one-off task, but it&#x27;s a pretty bad solution if the video is long, or high res, or both. Plus, what I really wanted was to build something more \"scalable&#x2F;flexible\".Maybe I didn&#x27;t know the right keywords to search for, but there really didn&#x27;t seem to be many options for creating frames, piping them straight to an encoder, and writing just the final video file to disk. The only one I found that seemed like it could maybe do it the way I had in mind was VidGear[1] (Python). I had figured that with the popularity of streaming, and video in general on the web, there would be so much more tooling for these sorts of things.I ended up digging way deeper into this than I had intended, and built myself something on top of Membrane[2] (Elixir)[1] https:&#x2F;&#x2F;abhitronix.github.io&#x2F;vidgear&#x2F; [2] https:&#x2F;&#x2F;membrane.stream&#x2F; reply dylan604 19 hours agorootparentIt sounds like a misunderstanding of the MPEG concept. For an encode to be made efficiently, it needs to see more than one frame of video at a time. Sure, I-frame only encoding is possible, but it&#x27;s not efficient and the result isn&#x27;t really distributable. Encoding wants to see multiple frames at a time so that the P and B frames can be used. Also, to get the best bang for the bandwidth buck is to use multipass encoding. Can&#x27;t do that if all of the frames don&#x27;t exist yet.You have to remember how old the technology you are trying to use is, and then consider the power of the computers available when they were made. MPEG-2 encoding used to require a dedicated expansion card because the CPUs did have decent instructions for the encoding. Now, that&#x27;s all native to the CPU which makes the code base archaic. reply nullpilot 16 hours agorootparentNo doubt that my limited understanding of these technologies came with some naive expectations of what&#x27;s possible and how it should work.Looking into it, and working through it, part of my experience was a lack of resources at the level of abstraction that I was trying to work in. It felt like I was missing something, with video editors that power billion dollar industries on one end, directly embedding ffmpeg libs into your project and doing things in a way that requires full understanding of all the parts and how they fit together on the other end, and little to nothing in-between.Putting a glorified powerpoint in an mp4 to distribute doesn&#x27;t feel to me like it is the kind of task where the prerequisite knowledge includes what the difference between yuv420 and yuv422 is or what Annex B or AVC are.My initial expectation was that there has to be some in-between solution. Before I set out, what I had thought would happen is that I `npm install` some module and then just create frames with node-canvas, stream them into this lib and get an mp4 out the other end that I can send to disk or S3 as I please.* Worrying about the nitty gritty details like how efficient it is, many frames it buffers, or how optimized the output is, would come later.Going through this whole thing, I now wonder how Instagram&#x2F;TikTok&#x2F;Telegram and co. handle the initial rendering of their video stories&#x2F;reels, because I doubt it&#x27;s anywhere close to the process I ended up with.* That&#x27;s roughly how my setup works now, just not in JS. I&#x27;m sure it could be another 10x faster at least, if done differently, but for now it works and lets me continue with what I was trying to do in the first place. reply dylan604 16 hours agorootparentThis sounds like \"I don&#x27;t know what a wheel is, but if I chisel this square to be more efficient it might work\". Sometimes, it&#x27;s better to not reinvent the wheel, but just use the wheel.Pretty much everyone serving video uses DASH or HLS so that there are many versions of the encoding at different bit rates, frame sizes, and audio settings. The player determines if it can play the streams and keeps stepping down until it finds one it can use.Edit: >Putting a glorified powerpoint in an mp4 to distribute doesn&#x27;t feel to me like it is the kind of task where the prerequisite knowledge includes what the difference between yuv420 and yuv422 is or what Annex B or AVC are.This is the beauty of using mature software. You don&#x27;t need to know this any more. Encoders can now set the profile&#x2F;level and bit depth to what is appropriate. I don&#x27;t have the charts memorized for when to use what profile at what level. In the early days, the decoders were so immature that you absolutely needed to know the decoder&#x27;s abilities to ensure a compatible encode was made. Now, the decoder is so mature and is even native to the CPU, that the only limitation is bandwidth.Of course, all of this is strictly talking about the video&#x2F;audio. Most people are totally unawares that you can put programming inside of an MP4 container that allows for interaction similar to DVD menus to jump to different videos, select different audio tracks, etc. reply nullpilot 15 hours agorootparent> This sounds like \"I don&#x27;t know what a wheel is, but if I chisel this square to be more efficient it might work\". Sometimes, it&#x27;s better to not reinvent the wheel, but just use the wheel.I&#x27;m not sure I can follow. This isn&#x27;t specific to MP4 as far as I can tell. MP4 is what I cared about, because it&#x27;s specific to my use case, but it wasn&#x27;t the source of my woes. If my target had been a more adaptive or streaming friendly format, the problem would have still been to get there at all. Getting raw, code-generated bitmaps into the pipeline was the tricky part I did not find a straightforward solution for. As far as I am able to tell, settling on a different format would have left me in the exact same problem space in that regard.The need to convert my raw bitmap from rgba to yuv420 among other things (and figuring that out first) was an implementation detail that came with the stack I chose. My surprise lies only in the fact that this was the best option I could come up with, and a simpler solution like I described (that isn&#x27;t using ffmpeg-cli, manually or via spawning a process from code) wasn&#x27;t readily available.> You don&#x27;t need to know this any more.To get to the point where an encoder could take over, pick a profile, and take care of the rest was the tricky part that required me to learn what these terms meant in the first place. If you have any suggestions of how I could have gone about this in a simpler way, I would be more than happy to learn more. reply dylan604 14 hours agorootparentusing the example of ffmpeg, you can use things like -f in front of -i to describe what the incoming format is so that your homebrew exporting can send to stdout piped to ffmpeg where reads from stdin with &#x27;-i -&#x27; but more specifically &#x27;-f bmp -i -&#x27; would expect the incoming data stream to be in the BMP format. you can select any format for the codecs installed &#x27;ffmpeg -codecs&#x27; replylondons_explore 21 hours agorootparentprevIn a way, that&#x27;s good. The few hundred video encoding specialists who exist in the world have, per person, had a huge impact on the world.Compare that to web developers, who in total have had probably a larger impact on the world, but per head it is far lower.Part of engineering is to use the fewest people possible to have the biggest benefit for the most people. Video did that well - I suspect partly by being &#x27;hard&#x27;. reply elashri 22 hours agorootparentprevThere are many packages that can do that, like Vax [1] and Dask [2]. I don&#x27;t know exactly your workflow. But the concurrency in python is limited to multiprocessing, which is much expensive than threads which usually a typical streaming parallelizer will use outside python world.[1] https:&#x2F;&#x2F;vaex.io&#x2F;docs&#x2F;index.html[2] https:&#x2F;&#x2F;docs.dask.org&#x2F;en&#x2F;latest&#x2F; reply bboygravity 22 hours agorootparentprevJust curious: why would you not load the entire dataset \"into memory\" (\"into memory\" from a Python perspective)?On an average desktop&#x2F;server system the OS would automatically take care of putting whatever fits in RAM and the rest on disk.Or are the datasets large enough to fit on neither? reply ninkendo 21 hours agorootparent> On an average desktop&#x2F;server system the OS would automatically take care of putting whatever fits in RAM and the rest on disk.This is not true, unless you’re referring to swap (which is a configuration of the system and may not be big enough to actually fit it either, many people run with only a small amount of swap or disable it altogether.)You may be referring to mmap(2), which will map the on-disk dataset to a region of memory that is paged in on-demand, but somehow I doubt that’s what OP was referring to either.If you just read() the file into memory and work on it, you’re going to be using a ton of RAM. The OS will only put “the rest on disk” if it swaps, which is a degenerate performance case, and it may not even be the dataset itself that gets swapped (the kernel may opt to swap everything else on the system to fit your dataset into RAM. All pages are equal in the eyes of the virtual memory layer, and the ranking algorithm is basically an LRU cache.) reply Chabsff 21 hours agorootparentWhy do you think OP isn&#x27;t refering to mmap()? Its behavior is pretty much what they describe, and a common way it&#x27;s used. reply ninkendo 21 hours agorootparentFair enough, it’s totally possible that’s what they meant. But the complaint of “every package assumes you loaded the whole dataset in memory” seems to imply the package just naively reads the file in. I mean, if the package was mmapping it, they probably wouldn’t have had much trouble with memory enough for it to be an issue they’ve had to complain about. Also, you may not always have the luxury of mmap()’ing, if you’re reading data from a socket (network connection, stdout from some other command, etc.)I don’t do much python but I used to do a lot of ruby, and it was rare to see anyone mmap’ing anything, most people just did File.read(path) and called it a day. If the norm in the python ecosystem is to mmap things, then you’re probably right. reply geocar 21 hours agorootparentprevread() doesn&#x27;t read from the disk if the blocks are already in memory. reply ninkendo 21 hours agorootparentThat’s a really misleading thing to say. If the kernel already has the thing you’re read()’ing cached, then yes the kernel can skip the disk read as an optimization. But by reading, you’re taking those bytes and putting a copy of them in the process’s heap space, which makes it no longer just a “cache”. You’re now “using memory”.read() is not mmap(). You can’t just say “oh I’ll read the file in and the OS will take care of it”. It doesn’t work that way. reply geocar 20 hours agorootparent> If the kernel already has the thing you’re read()’ing cachedwhich it would do, if you have just downloaded the file.> But by reading, you’re taking those bytes and putting a copy of them in the process’s heap spacei mean, you just downloaded it from the network, so unless you think mmap() can hold buffers on the network card, there&#x27;s definitely going to be a copy going on. now it&#x27;s downloaded, you don&#x27;t need to do it again, so we&#x27;re only talking the one copy here.> You can’t just say “oh I’ll read the file in and the OS will take care of it”. It doesn’t work that way.i can and do. and you&#x27;ve already explained swap sufficiently that i believe you know you can do exactly that also. reply ninkendo 20 hours agorootparentPlease keep in mind the context of the discussion. A prior poster made a claim that they can read a file into memory, and that it won’t actually use any additional memory because the kernel will “automatically take care” of it somehow. This is plainly false.You come in and say something to the effect of “but it may not have to read from disk because it’s cached”, which… has nothing to do with what was being discussed. We’re not talking about whether it incurs a disk read, we’re talking about whether it will run your system out of memory trying to load it into RAM.> i mean, you just downloaded it from the network, so unless you think mmap() can hold buffers on the network card, there&#x27;s definitely going to be a copy going on. now it&#x27;s downloaded, you don&#x27;t need to do it again, so we&#x27;re only talking the one copy here.What in god’s holy name are you blathering about? If I “just downloaded it from the network”, it’s on-disk. If I mmap() the disk contents, there’s no copy going on, it’s “mapped” to disk. If I read() the contents, which is what you said I should do, then another copy of the data is now sitting in a buffer in my process’s heap. This extra copy is now \"using\" memory, and if I keep doing this, I will run the system out of RAM. This is characteristically different from mmap(), where a region of memory maps to a file on-disk, and contents are faulted into memory as I read them. The reason this is an extremely important distinction, is that in the mmap scenario, the kernel is free to free the read-in pages any time it wants, and they will be faulted back again in if I try to read them again. Contrast this with using read(), which makes it so the kernel can&#x27;t free the pages, because they&#x27;re buffers in my process&#x27;s heap, and are not considered file-backed from the kernel&#x27;s perspective.> i can and do. and you&#x27;ve already explained swap sufficiently that i believe you know you can do exactly that also.Swap is disabled on my system. Even if it wasn’t, I’d only have so much of it. Even if I had a ton of it, read()’ing 100GB of data and relying on swap to save me is going to grind the rest of the system to a halt as the kernel tries to make room for it (because the data is in my heap, and thus isn’t file-backed, so the kernel can’t just free the pages and read them back from the file I read it from.) read() is not mmap(). Please don’t conflate them. reply SoftTalker 17 hours agorootparent> Swap is disabled on my system.Yep I do the same. If I have a server with hundreds of GB or even TB of RAM (not uncommon these days) I&#x27;m not setting up swap. If you&#x27;re exhausting that much RAM, swap is only going to delay the inevitable. Fix your program. reply geocar 15 hours agorootparentprev> A prior poster made a claim that they can read a file into memory, and that it won’t actually use any additional memory because the kernel will “automatically take care” of it somehow. This is plainly false.Nobody made that claim except in your head.Why don&#x27;t you read it now:Just curious: why would you not load the entire dataset \"into memory\" (\"into memory\" from a Python perspective)?Look carefully: There&#x27;s no mention of the word file. For all you or I know the programmer is imagining something like this: >>> data=loaddata(\"https:&#x2F;&#x2F;...\")Or perhaps it&#x27;s an S3 bucket. There is no file, only the data set. That&#x27;s more or less exactly what I do.On an average desktop&#x2F;server system the OS would automatically take care of putting whatever fits in RAM and the rest on disk.You know exactly this what is meant by swap: We just confirmed that. And you know it is enabled on every average desktop server system, because you> Swap is disabled on my systemare the sort of person who disables the average configuration! Can you not see you aren&#x27;t arguing with anything but your own fantasies?> If I “just downloaded it from the network”, it’s on-disk.That&#x27;s nonsense. It&#x27;s in ram. That&#x27;s the block cache you were just talking about.> If I mmap() the disk contents, there’s no copy going on, it’s “mapped” to diskEvery word of that is nonsense. The disk is attached to a serial bus. Even if you&#x27;re using fancy nvme \"disks\" there&#x27;s a queue that operates in a (reasonably) serial fashion. The reason mmap() is referred to zero-copy is because it can reuse the block cache if it has been recently downloaded -- but if the data is paged out, there is absolutely a copy and it&#x27;s more expensive than just read() by a long way.> Even if it wasn’t, I’d only have so much of it. Even if I had a ton of it, read()’ing 100GB of data and relying on swap to save me is going to grind the rest of the system to a halt as the kernel tries to make room for itYou only have so much storage, this is life, but I can tell you as someone who does operate a 1tb of ram machine that downloads 300gb of logfiles every day, read() and write() work just fine -- I just can&#x27;t speak for python (or why python people don&#x27;t do it) because i don&#x27;t like python. reply ninkendo 15 hours agorootparentYou&#x27;re basically just gish galloping at this point and there&#x27;s no need to respond to you any more. All your points about swap are irrelevant to the discussion. All your points about disk cache are irrelevant to the discussion. You have a very, very, very incorrect understanding of how operating system kernels work if you think mmap() is just a less efficient read():> Every word of that is nonsense. The disk is attached to a serial bus. Even if you&#x27;re using fancy nvme \"disks\" there&#x27;s a queue that operates in a (reasonably) serial fashion. The reason mmap() is referred to zero-copy is because it can reuse the block cache if it has been recently downloaded -- but if the data is paged out, there is absolutely a copy and it&#x27;s more expensive than just read() by a long way.Please do a basic web search for what \"virtual memory\" means. You seem to think that handwavey nonsense about disk cache means that read() doesn&#x27;t copy the data into your working set. You should look at the manpage for read() and maybe ponder why it requires you to pass your own buffer. This buffer would have to be something you&#x27;ve malloc()&#x27;d ahead of time. Hence why you&#x27;re using more memory by using read() than you would using mmap()> You only have so much storage, this is life, but I can tell you as someone who does operate a 1tb of ram machine that downloads 300gb of logfiles every day, read() and write() work just fine -- I just can&#x27;t speak for python (or why python people don&#x27;t do it) because i don&#x27;t like python.You should definitely learn what the mmap syscall does and why it exists. You really don&#x27;t need to use 300gb of RAM to read a 300gb log file. You should probably look up how text editors like vim actually work, and why you can seek to the end of a 300gb log file without vim taking up a bunch of RAM. Maybe you&#x27;ve never been curious about this before, I dunno.Try making a 20gb file called \"bigfile\", and run this C program: #include#include#include#include#include#include#includeint main(int argc, char *argv[]) { char *mmap_buffer; int fd; struct stat sb; ssize_t s; fd = open(\".&#x2F;bigfile\", O_RDONLY); fstat(fd, &sb); &#x2F;&#x2F; get the size &#x2F;&#x2F; do the mmap mmap_buffer = mmap(NULL, sb.st_size, PROT_READ, MAP_PRIVATE, fd, 0); uint8_t sum; &#x2F;&#x2F; Ensure the whole buffer is touched by doing a dumb math operation on every byte for (size_t i = 0; i &#x2F;status and take note of the memory stats. You&#x27;ll see that VmSize is as big as the file you read in (for me, bigfile is more than 20GB): VmSize: 20974160 kBBut the actual resident set is 968kb: VmRSS: 968 kBSo, my program is using 968 kB even though it has a 20GB in-memory buffer that just read the whole file in! My system only has 16GB of RAM and swap is disabled.How is this possible? Because mmap lets you do this. The kernel will read in pages from bigfile on demand, but is also free to free them at any point. There is no controversy here, every modern operating system has supported this for decades.Compare this to a similar program using read(): #include#include#include#include#include#include#includeint main(int argc, char *argv[]) { int fd; struct stat sb; ssize_t s; char *read_buffer; fd = open(\".&#x2F;bigfile\", O_RDONLY); fstat(fd, &sb); &#x2F;&#x2F; get the size &#x2F;&#x2F; do the read read_buffer = malloc(sb.st_size); read(fd, read_buffer, sb.st_size); uint8_t sum; &#x2F;&#x2F; Ensure the whole buffer is touched by doing a dumb math operation on every byte for (size_t i = 0; i &#x2F;status: VmPeak: 2099928 kB VmSize: 2099928 kB VmLck: 0 kB VmPin: 0 kB VmHWM: 2098056 kB VmRSS: 2098056 kBOops! I&#x27;m using 2 GB of resident set size. If I were to do this with a file that&#x27;s bigger than RAM, I&#x27;d get OOM-killed.This is why you shouldn&#x27;t read() in large datasets. Your strategy is to have servers with 1TB of ram and massive amounts of swap, and I&#x27;m telling you you don&#x27;t need this to process this big of files. mmap() does so without requiring things to be read into RAM ahead of time.Oh, and guess what: Take out the `sleep(1000)`, and the mmap version is faster than the read() version: $ time .&#x2F;mmap done! sum=225 .&#x2F;mmap 6.89s user 0.28s system 99% cpu 7.180 total $ time .&#x2F;read done! sum=246 .&#x2F;read 6.86s user 1.72s system 99% cpu 8.612 totalWhy is it faster? Because we don&#x27;t have to needlessly copy the data into the process&#x27;s heap. We can just read the blocks directly from the mmap&#x27;d address space, and let page faults read them in for us. reply geocar 11 hours agorootparent> Why is it faster?Why are the answers different?Do you not know how to benchmark? Or did you falsify your results on purpose? reply ninkendo 11 hours agorootparentBegone troll.(Edit: because I noticed this too, and it got me curious, the reason for the incorrect result is that I didn&#x27;t check the result of the read() call, and it was actually reading fewer bytes, by a small handful. read() is allowed to do this, and it&#x27;s up to callers to call it again. It was reading 2147479552 bytes when the file size was 2147483648 bytes. If anything this should have made the read implementation faster, but mmap still wins even though it read more bytes. A fixed version follows, and now produces the same \"sum\" as the mmap): #include#include#include#include#include#include#includeint main(int argc, char *argv[]) { int fd; struct stat sb; ssize_t s; ssize_t read_result; ssize_t bytes_read; char *read_buffer; fd = open(\".&#x2F;bigfile\", O_RDONLY); fstat(fd, &sb); &#x2F;&#x2F; get the size &#x2F;&#x2F; do the read read_buffer = malloc(sb.st_size); bytes_read = 0; do { read_result = read(fd, read_buffer + bytes_read, sb.st_size - bytes_read); bytes_read += read_result; } while (read_result != 0); uint8_t sum; &#x2F;&#x2F; Ensure the whole buffer is touched by doing a dumb math operation on every byte for (size_t i = 0; iSounds like a worse way of writing async requests,It&#x27;s just how it works under the hood, this complexity is quickly abstracted and actually it&#x27;s how a lot of async requests are implemented, it&#x27;s just here it&#x27;s on 1 tcp connection.> , while the last part is basically what websockets seem to be intended foryes I was specifically answering that :> I have seen teams of experienced seniors using websockets and then just sending requests&#x2F;responses over them as every architecture choice and design pattern they were familiar with required this.i.e people using websocket like normal http request. reply dgb23 22 hours agorootparentprevIn the case of websockets that is already handled for you.I think GP talks about how to think about communication (client server).Stateless request response cycles are much simpler to reason about because it synchronizes messages by default. The state is reflected via this back and forth communication. Even when you do it via JS: the request data is literally in the scope of the response callback.If you have bi-directional, asynchronous communication, then reading and writing messages is separate. You have to maintain state and go out of your way to connect incoming and outgoing messages semantically. That&#x27;s not necessarily what you should be doing though. reply whizzter 21 hours agorootparentprevPretty much, websocket implementations usually handles the buffering part so you really only need to handle \"full\" events.Easiest example is probably games, events could be things such players moving to new locations, firing a weapon, explosions.In realtime games, as well as \"request-response\" scenarios, it&#x27;s common to have some kind of response that acknowledges that data was received. reply grogenaut 19 hours agorootparentprevI know right. The kids these days. Most of them never learned to solder either so how can they assemble their own computers from ics? I caught one of my tech leads using a jet burner lighter and just scorching the whole board. And forget reading core dumps in hex!!! An intern was just putting the hex dump in the chat chibi and asking it what went wrong. Get off my lawn already. reply ascar 22 hours agoparentprevIf I recall correctly in TCP a packet loss will cause a signifcant slowdown in the whole data stream since that packet needs to be retransmitted and you generally end up with a broken data stream until that happens (even tho TCP can continue to send more data in the meantime meaning it&#x27;s more of a temporary hang than a block). Thus if you are sending multiple different data streams over the same TCP connection a packet loss will temporarily hang the processing of all data streams. A constrain that QUIC doesn&#x27;t have. reply dgb23 21 hours agorootparentA somewhat decent analogy of thinking about TCP is a single lane queue with a security guy who&#x27;s ordering people around and making sure the queue is orderly and doesn&#x27;t overflow. He has pretty much no respect for families, groups or overall efficiency. He only cares about the queue itself. reply pjc50 21 hours agorootparentprevNot broken, but you end up with a hole in the TCP receive window while the packet is retransmitted.How does QUIC manage its stream reassembly window sizes? I suppose it&#x27;s easier since it&#x27;s all in userland. reply tsimionescu 20 hours agorootparent> I suppose it&#x27;s easier since it&#x27;s all in userland.I doubt applications would provide more reliable information to the QUIC library than they would to the kernel.The main difference as I understand it is that QUIC allows multiple separate TCP-like streams to exist on the same negotiated and encrypted connection. It&#x27;s not fundamentally different from simply establishing multiple TLS over TCP connections with separate windows, it just allows you to establish the equivalent of multiple TLS + TCP connections with a single handshake. reply adgjlsfhk1 20 hours agorootparentprevThe key point is that if I want to load a webpage that needs to download 100 different files, I don&#x27;t care about the order of those 100 files, I just want them all to be downloaded in order. TCP makes you specify an order which means that if one packet gets lost, you have to wait for it. QUIC lets you say \"here are the 100 things I want\" which means that if you lose a packet, that only stops 1 of the things so the other 99 can continue asking for more packets. reply culebron21 22 hours agorootparentprevYes, I just read about this, and it makes sense. reply oefrha 22 hours agoparentprevTCP is a streaming protocol. You can build whatever multiplexing scheme on top like h2 does, but you simply can’t escape TCP head of the line blocking, as it’s just a single undelimited stream underneath.As an aside, I only truly grasped the stream nature of TCP when I started di",
    "originSummary": [
      "HTTP/3, a new version of the Hypertext Transfer Protocol (HTTP), has quickly been incorporated into the public web after being standardized by the Internet Engineering Task Force (IETF).",
      "HTTP/3, along with the associated QUIC protocol, is extensively utilized by major companies like Google and Meta, and it replaces the Transport Control Protocol (TCP) with QUIC, offering enhanced encryption and performance improvements.",
      "The new protocol delivers better speed for web page loading and video streaming, contributing to its rapid acceptance in the industry."
    ],
    "commentSummary": [
      "The discussions revolve around several topics such as the use of QUIC protocol to navigate internet restrictions in China, benefits and hurdles of the QUIC protocol, and implementing QoS in QUIC.",
      "Corporate dissatisfaction with Zscaler network security service, SSL interception usage within corporate networks, and video packaging and encoding strategies each form part of the dialogue.",
      "Finally, the discussions ponder over the necessity for a standard QUIC API and the benefits of employing mmap() over read() for handling large datasets."
    ],
    "points": 564,
    "commentCount": 431,
    "retryCount": 0,
    "time": 1696503592
  },
  {
    "id": 37777347,
    "title": "Bitmagnet: A self-hosted BitTorrent indexer, DHT crawler, and torrent search",
    "originLink": "https://bitmagnet.io/",
    "originBody": "Skip to main content bitmagnet Home Setup Tutorials Internals & Development bitmagnet on GitHub This site uses Just the Docs, a documentation theme for Jekyll. bitmagnet A self-hosted BitTorrent indexer, DHT crawler, content classifier and torrent search engine with web UI, GraphQL API and Servarr stack integration. IMPORTANT This software is currently in alpha. It is ready to preview some interesting and unique features, but there will likely be bugs, as well as API and database schema changes before the (currently theoretical) 1.0 release. If you’d like to support this project and help it gain momentum, please give it a star on GitHub. If you’re interested in getting involved and you’re a backend GoLang or frontend TypeScript/Angular developer, or you’re knowledgeable about BitTorrent protocols then I’d like to hear from you - let’s get this thing over the line! DHT what now…? The DHT crawler is bitmagnet’s killer feature that (currently) makes it unique. Well, almost unique, read on… So what is it? You might be aware that you can enable DHT in your BitTorrent client, and that this allows you find peers who are announcing a torrent’s hash to a Distributed Hash Table (DHT), rather than to a centralized tracker. DHT’s lesser known feature is that it allows you to crawl the info hashes it knows about. This is how bitmagnet’s DHT crawler works works - it crawls the DHT network, requesting metadata about each info hash it discovers. It then further enriches this metadata by attempting to classify it and associate it with known pieces of content, such as movies and TV shows. It then allows you to search everything it has indexed. This means that bitmagnet is not reliant on any external trackers or torrent indexers. It’s a self-contained, self-hosted torrent indexer, connected via DHT to a global network of peers and constantly discovering new content. The DHT crawler is not quite unique to bitmagnet; another open-source project, magnetico was first (as far as I know) to implement a usable DHT crawler, and was a crucial reference point for implementing this feature. However this project is no longer maintained, and does not provide the other features such as content classification, and integration with other software in the ecosystem, that greatly improve usability. You can find some more technical details about bitmagnet’s DHT crawler here. Features & Roadmap Currently implemented features A DHT crawler A generic BitTorrent indexer: bitmagnet can index torrents from any source, not only the DHT network - currently this is only possible via the /import endpoint; more user-friendly methods are in the pipeline, see high-priority features below A content classifier that can currently identify movie and television content, along with key related attributes such as language, resolution, source (BluRay, webrip etc.) and enriches this with data from The Movie Database An import facility for ingesting torrents from any source, for example the RARBG backup A torrent search engine A GraphQL API: currently this provides a single search query; there is also an embedded GraphQL playground at /graphql A web user interface implemented in Angular: currently this is a simple single-page application providing a user interface for search queries via the GraphQL API A Torznab-compatible endpoint for integration with the Serverr stack High priority features not yet implemented Classifiers for other types of content; enrich current classifiers and weed out incorrect classifications. Ordering of search results: the current alpha preview has no facility for specifying the ordering of results. Search performance optimisations: search is currently fast enough to be usable; it becomes more sluggish once millions of torrents have been indexed - there are some low-hanging fruit in terms of optimisation that will be a near-term priority. A monitoring API and WebUI dashboard showing things like crawler throughput, task queue, database size etc. Authentication, API keys, access levels etc. An admin API, and in general a more complete GraphQL API A more complete web UI Saved searches for content of particular interest, enabling custom feeds in addition to the following feature Smart deletion: there’s a lot of crap out there; crawling DHT can quickly use lots of database disk space, and search becomes slower with millions of indexed torrents of which 90% are of no interest. A smart deletion feature would use saved searches to identify content that you’re not interested in, including but not limited to CSAM, and low quality content (such as low resolution movies). It would automatically delete associated metadata and add the info hash to a bloom filter, preventing the torrent from being re-indexed in future. Bi-directional integration with the Prowlarr indexer proxy: Currently bitmagnet can be added as an indexer in Prowlarr; bi-directional integration would allow bitmagnet to crawl content from any indexer configured in Prowlarr, unlocking many new sources of content More documentation and more tests! Pipe dream features This is where things start to get a bit nebulous. For now all focus is on delivering the core features above, but some of these ideas could be explored in future: In-place seeding: identify files on your computer that are part of an indexed torrent, and allow them to be seeded in place after having moved, renamed or deleted parts of the torrent Integration with popular BitTorrent clients Federation of some sort: allow friends to connect instances and pool the indexing effort, perhaps involving crowd sourcing manual content curation to supplement the automated classifiers Something that looks like a decentralized private tracker; by this I probably mean something that’s based partly on personal trust and manually weeding out any bad actors; I’d be wary of creating something that looks a bit like Tribler, which while an interesting project seems to have demonstrated that implementing trust, reputation and privacy at the protocol level carries too much overhead to be a compelling alternative to plain old BitTorrent, for all its imperfections Support for the BitTorrent v2 protocol: It remains to be seen if wider adoption will ever make this a valuable feature",
    "commentLink": "https://news.ycombinator.com/item?id=37777347",
    "commentBody": "Bitmagnet: A self-hosted BitTorrent indexer, DHT crawler, and torrent searchHacker NewspastloginBitmagnet: A self-hosted BitTorrent indexer, DHT crawler, and torrent search (bitmagnet.io) 416 points by KoftaBob 22 hours ago| hidepastfavorite93 comments chmod775 17 hours ago> The DHT crawler is not quite unique to bitmagnet; another open-source project, magnetico was first (as far as I know) to implement a usable DHT crawler, and was a crucial reference point for implementing this feature.Heh. That was one of my first projects when I was still learning to code back in 2012: https:&#x2F;&#x2F;github.com&#x2F;laino&#x2F;shiny-adventureThe DHT crawler&#x2F;worker lived seperately, and I eventually put it here to rescue it from a dying HDD: https:&#x2F;&#x2F;github.com&#x2F;laino&#x2F;DHT-Torrent-database-WorkerThe code is abhorrent and you absolutely shouldn&#x27;t use it, but it worked. At least the crawler did - the frontend was never completed.Since the first implementation of mainline DHT appeared in 2005 and crawling that network is really quite an obvious idea, I doubt we (a friend was working on it as well) were first either. reply JP44 15 hours agoparentNothing substantial, I chuckled when I saw the commit history on your linked projects. I do not mean to belittle you (or the purpose&#x2F;goal of the projects), genuinely enjoyed the distraction and &#x27;results&#x27; from it:Today was the first commit after 11 (9 oct 2012) and 5 years (24 nov 2018), respectively, on the projects. I think your repo might be part of some sort of oldest &#x27;active&#x27;- or &#x27;not ported to another repo&#x27;-repoFor what I&#x27;ve found in ~10 min (google&#x2F;gpt), excluding git projects existing before spring 2008 (couldn&#x27;t get a quick consensus on feb vs april of that year), there&#x27;s not a lot(I&#x27;ll edit this part if sources are requested) reply Fatnino 10 hours agorootparentI recently committed to an old repo of mine after a 9 year gap.It holds several one file python script experiments and toys that I lumped into one place to get them off my hdd and make them available from wherever. Recently remembered it existed and added another one. And while I was in there I also ran 2to3 on the ones that needed it and polished the results up. reply the8472 18 hours agoprevIt seems like every single of these things always cut corners and don&#x27;t implement proper, spec-compliant nodes that provide the same services as they use. You know, the \"peer\" in p2p. BEP51 was designed to make it easier to not trample on the commons, and yet... reply mgdigital 16 hours agoparentAuthor here. FWIW I wasn&#x27;t intending this to make it onto HN, having posted about this on Lemmy looking for beta testers. The current version of the app is very much a preview. There&#x27;s much further work to be done and this will include as far as possible ensuring Bitmagnet is a \"good citizen\". The suggestions made on the GH issue look largely feasible and I&#x27;ll get round to looking at them as soon as I can.The issue and my response on GH: https:&#x2F;&#x2F;github.com&#x2F;bitmagnet-io&#x2F;bitmagnet&#x2F;issues&#x2F;11 reply e12e 18 hours agoparentprevAppreciate that you took the time to file an issue:https:&#x2F;&#x2F;github.com&#x2F;bitmagnet-io&#x2F;bitmagnet&#x2F;issues&#x2F;11 reply zolland 17 hours agoparentprevWithout the peers all we have is to! reply coolspot 16 hours agoprevPlease remove copyrighted movies from the screenshot on your website. It provides evidence that this program is designed for violating copyright, which makes DCMA takedown trivial. reply mgdigital 12 hours agoparentThanks - I only condone accessing the legal content available on BitTorrent, and my screenshots now embody this moral stance. reply dotBen 58 minutes agorootparentYes, I&#x27;m grateful for this being built so I can locate and identify all the copies of Linux I could download and install... reply pipes 12 hours agoparentprevAnd they probably want to remove text like\"It then further enriches this metadata by attempting to classify it and associate it with known pieces of content, such as movies and TV shows. It then allows you to search everything it has indexed.\" reply cchance 6 hours agorootparentThere&#x27;s nothing wrong with that since there are LOTS of free to share movies and tv shows especially those past their copyright dates. reply dewey 12 hours agorootparentprevI don&#x27;t see anything wrong with that if the example titles are under the correct licenses like the often used: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Big_Buck_Bunny which can then be mapped to open databases like https:&#x2F;&#x2F;www.themoviedb.org&#x2F;movie&#x2F;10378-big-buck-bunny. reply sneak 10 hours agoparentprevIndexing what is available for download is useful for research into piracy even if you don’t engage in piracy yourself. reply rsync 15 hours agoparentprevPlease determine if these images fall under fair-use provisions and, if so, leave them in place.Bad actors - whoever they may be - need to see your rights constantly reasserted. reply crazygringo 13 hours agorootparentThis has nothing whatsoever to do with fair use.It&#x27;s about arguments in court about the intention of the software if sued. Images of copyrighted content indicate intent to infringe copyright. Without those, you can argue it&#x27;s only meant to find and index Linux image torrents or whatever.Fair use doesn&#x27;t enter the picture at all. reply KennyBlanken 14 hours agorootparentprevThe MPAA and other organizations use screenshots that show copyrighted material as \"proof\" that the tools are used for copyright violation, and then DMCA them.If you want to help pay for lawyers to fight those DMCA notices with counterclaims and lawsuits, put up or shut up; the FSF, EFF and ACLU have been noticeably disinterested in doing so. reply gymbeaux 15 hours agoparentprevI don’t see any screenshots on the website reply davidcollantes 15 hours agorootparentThere was, and were removed. reply ipaddr 7 hours agoparentprevIt might be too late. reply forgetm3 11 hours agoprevIsn&#x27;t this much the same as btdig.com which is based on: https:&#x2F;&#x2F;github.com&#x2F;btdig&#x2F;dhtcrawler2I use this service to do security research a fair bit. It&#x27;d be nice if there was a higher quality self-hosted version I could use so I&#x27;ll be watching this project with interest! reply Fnoord 19 hours agoprevSee also: Magnetissimo [1], Torrentinim [2], and Spotweb [3].[1] https:&#x2F;&#x2F;github.com&#x2F;sergiotapia&#x2F;magnetissimo[2] https:&#x2F;&#x2F;github.com&#x2F;sergiotapia&#x2F;torrentinim[3] https:&#x2F;&#x2F;github.com&#x2F;spotweb&#x2F;spotweb reply INTPenis 3 hours agoparentDo you know if there is a simpler torrent tracker out there, like whatever the fedora project is using?https:&#x2F;&#x2F;torrent.fedoraproject.org&#x2F;I just want a simple list, and the backend. reply e12e 18 hours agoparentprevAny comments on how these compare? Especially in relation to sibling comment about BEP51? reply Fnoord 12 hours agorootparentYou want Torznab support. That is basically metadata you want to export, to import it into your application which holds the database of what you are after. If it is a match, it should attempt to download it via your download client (BitTorrent client).Torrentinim is the successor of Magnetissimo but it lacks Radarr&#x2F;Sonarr integration (there is a pull request for Torznab support for both). Spotweb has Newznab support [1] but at Black Friday (soon) there&#x27;s usually tons of deals available for Newznab indexes.I don&#x27;t care about BEP51 as I don&#x27;t have huge upload. That is also why I prefer Usenet over torrents. But torrents are a useful and sometimes required backup mode. Just not my preferred one.[1] https:&#x2F;&#x2F;github.com&#x2F;Spotweb&#x2F;Spotweb&#x2F;wiki&#x2F;Spotweb-als-Newznab-... reply diggan 17 hours agorootparentprevFrom https:&#x2F;&#x2F;github.com&#x2F;bitmagnet-io&#x2F;bitmagnet&#x2F;issues&#x2F;11> The DHT implementation was largely borrowed from Magnetico (https:&#x2F;&#x2F;github.com&#x2F;boramalper&#x2F;magnetico) a popular and widely used app which is likewise unable to service these requests. reply the8472 18 hours agorootparentprevFrom a brief look at each it seems like they&#x27;re scraping things like torrent websites, usenet or maybe RSS feeds. Not the DHT. reply doakes 17 hours agoprevWhat kind of bandwidth usage should be expected from a DHT crawler like this? reply KennyBlanken 12 hours agoparentAfter ~30-60 minutes of running, still less than 100kB&#x2F;sec combined in and out. However, as others have noted, nodes don&#x27;t communicate much with nodes that haven&#x27;t been up for a while (days.)It&#x27;s using roughly 6% CPU time for the crawler and another 1-2% for postgres, on a second-gen i7.As a datapoint to set expectations: 4000 torrents have been captured so far, and somewhat surprisingly, they&#x27;re not very current results, necessarily.For example, a certain wildly popular TV series about samurai in space swinging very hot swords around which just had its season ending episode last night (I think)...that ep isn&#x27;t in my list so far, but the episode prior to it, and the first two episodes, are.There&#x27;s a ton of random, low-seed torrents, so it&#x27;s actually kind of interesting to search by type, year, etc and see what comes up. reply r3trohack3r 15 hours agoprevHave been playing around with DHT crawling for a while now, curious how you&#x27;re getting around the \"tiers\" of the DHT?IIUC peers favor nodes they&#x27;ve had longer relationships with to provide stable routes through the DHT.This means short-lived nodes receive very little traffic, nobody is routing much traffic through fresh nodes, they choose nodes they’ve had longer relationships with.The longer you stay up, the more you start seeing.At least this is what I&#x27;ve observed in my projects. The only way I&#x27;ve been able to get anything interesting out of the DHT in the last ~5 years has been to put up a node and leave it up for a long time. If I spin up something, the first day I usually only find a handful of resolvable hashes.Not to mention it seems the BitTorrent DHT is very lax in what it will route compared to other DHTs (like IPFS) meaning many of the hashes you receive aren&#x27;t for torrents at all. reply drakenot 7 hours agoprev> Something that looks like a decentralized private tracker; by this I probably mean something that’s based partly on personal trust and manually weeding out any bad actors; I’d be wary of creating something that looks a bit like Tribler, which while an interesting project seems to have demonstrated that implementing trust, reputation and privacy at the protocol level carries too much overhead to be a compelling alternative to plain old BitTorrent, for all its imperfectionsI&#x27;ve thought about this problem a lot. Having a federated &#x2F; distributed tracker but with some form of trust based, or opt-in curation would be amazing. reply pepe56 4 hours agoprevDoes this also share back DHT information like a \"server\"? I am not downloading torrents, but I always wished to have a daemon running on my servers to help the overall DHT networks health. Is there anything like this out there? A DHT server, that only collects, stores and gives back information? reply zidel 3 hours agoparent> Does this also share back DHT information like a \"server\"?No, it ignores all incoming requests. You don&#x27;t need special software to help out, just run any normal Bittorrent client in the background (no need to download or share anything) and it will help out. Just make sure you forward the right port if you are behind a NAT. Traffic will slowly increase over time, and drop quickly when offline, so leaving it up for days at a time is better when possible. reply askiiart 17 hours agoprevAnd the predecessor to this: https:&#x2F;&#x2F;github.com&#x2F;mgdigital&#x2F;rarbg-selfhostedIt&#x27;s now archived due to effort being redirected to Bitmagnet. reply JohnDeHope 11 hours agoprevWhy isn&#x27;t this built into thick desktop BT clients? Is it just a little too early for it, and next year it will be? Or is there some reason I&#x27;m missing? reply fallat 19 hours agoprevIt&#x27;d be so nice to have a super simple DHT crawler CLI tool, in both implementation and interface. reply the8472 18 hours agoparentThese things need uptime of hours and days to do it properly and also to stay up to date. There are millions of nodes and torrents and to be non-abusive you have to issue requests at a somewhat sedate pace. And activity kind of moves with the sun due to people who run torrent clients on their home machines. And there are lots of buggy or malicious implementations out there that you have to deal with. So you&#x27;d want to run it as a daemon. The CLI would have to be a frontend to the daemon or its database. The UI could be simple. I&#x27;m skeptical whether an implementation could be both good and simple. reply derefr 17 hours agorootparentThat&#x27;s if you&#x27;re imagining a single node to discover the whole DHT. What if you want to fire off a map-reduce of limited-run DHT explorations starting from different DHT ring positions, where each agent just crawls and emits what it finds on stdout as it finds it?(In a sense, I suppose this would still be a \"daemon\", but that daemon would be the map-reduce infrastructure.) reply the8472 17 hours agorootparentI don&#x27;t quite understand what you&#x27;re proposing here. Generally you only control and operate ~1 node per IPv4 address or per IPv6 &#x2F;64.All other nodes are operated by someone else, so they don&#x27;t cooperate on anything beyond what the protocol specifies. Which means everyone is their own little silo. If you want a list of all currently active torrents (millions) then you have to do it with 1 or a handful of nodes, depending on how many IPs you have. DHTs are not arbitrary-distributed-compute frameworks, they&#x27;re a quite restrictive get&#x2F;put service.BEP51[0] does let you query other nodes for a sample of their keys (infohashes) but what they can offer is limited by their vantage point of the network so you need to go around and ask all those millions of nodes. And since it&#x27;s all random you can&#x27;t really \"search\" for anything, you can only sample. And that just gives you 20-byte keys. Afterwards you need to do a lot of additional work to turn those into human-readable metadata.[0] http:&#x2F;&#x2F;bittorrent.org&#x2F;beps&#x2F;bep_0051.html reply derefr 16 hours agorootparentI mean, what I&#x27;m describing is the same thing that BEP51 mentions as a motivation:> DHT indexing already is possible and done in practice by passively observing get_peers queries. But that approach is inefficient, favoring indexers with lots of unique IP addresses at their disposal. It also incentivizes bad behavior such as spoofing node IDs and attempting to pollute other nodes&#x27; routing tables.If you have a lot of IP addresses (from e.g. AWS Lambda) then you can partition DHT keyspace across a large-N number of nodes and then very quickly discover everything in the keyspace.The trick is that, since BEP51 exists, you don&#x27;t need to have all these nodes register themselves into the hash-ring (at arbitrary spoofed positions) to listen. You can just have all these nodes independently probing the hash-ring \"from the outside\" — just making short-lived connections to registered nodes (without first registering themselves); handshaking that connection as a spoofed node ID; and then firing off one `sample_infohashes` request, getting a response, and disconnectting. The lack of registration shouldn&#x27;t make any difference, as long as they don&#x27;t want anyone to try connecting to them.Which is why I say that these are just \"crawler agents\", not \"nodes\" per se. They don&#x27;t start up P2P at all — to them, this is a one-shot client&#x2F;server RPC conversation, like a regular web crawler making HTTP requests! reply the8472 15 hours agorootparentOh, I already have implemented something[0] like that. It doesn&#x27;t need lambdas or anything \"cloud scale\" like that. You \"just\" need a few dozen to a hundred IP addresses assigned to one machine and run a multi-homed DHT node on that to passively observe traffic from multiple points in the keyspace.But neither of these approaches is what I&#x27;d call a \"super simple DHT crawler CLI tool\" that the initial comment was asking about. BEP51 is intended to make crawling simple enough that it can run on a single home internet connection, but a proper implementation still isn&#x27;t trivial.[0] https:&#x2F;&#x2F;github.com&#x2F;the8472&#x2F;mldht reply lost_tourist 8 hours agorootparentprevnah, that&#x27;s tmux is for. I&#x27;ve had sessions running for a month+ reply KennyBlanken 13 hours agoprevI&#x27;ve been unable to get this running; I gave it a postgres user and database, granted it ownership and all permissions on said DB, and there&#x27;s nothing in the database.Edit: found the init schema and things seem to be working now: https:&#x2F;&#x2F;github.com&#x2F;bitmagnet-io&#x2F;bitmagnet&#x2F;blob&#x2F;main&#x2F;migratio...It would be really nice to be able to sort by header (size, seeders) and&#x2F;or some filters for seed&#x2F;downloaders (for example, filtering out anything with less than X seeds.) reply ShadowBanThis01 7 hours agoprevStrange: I tried to give it a star on Github, but the star control just resets every time. And I am logged in. reply m3kw9 10 hours agoprevHow’s this different from piratebay reply KiwiJohnno 3 hours agoparentIt&#x27;s self-hosted. reply RIMR 12 hours agoprevThis is really neat. I&#x27;ll need to check it out. A couple years ago I ran my own instance of Magnetico (https:&#x2F;&#x2F;github.com&#x2F;boramalper&#x2F;magnetico), but this project looks a lot more polished. reply complianceowl 14 hours agoprevGuys, please educate me: I want to use torrents, but the thought of downloading something inappropriate by clicking on a deceptive link terrifies me (e.g., a download with the title of an action movie, but it turns out to be something else).How do you guys handle that risk? reply dvdkon 11 hours agoparentUnless you&#x27;re also afraid of clicking links on the web, you should be fine with torrents. Maybe you could not seed by default and turn it on only after verifying the data is authentic, that way you&#x27;re actually only downloading and the analogy is complete. reply shepherdjerred 13 hours agoparentprevIf you&#x27;re downloading \"normal\" stuff, then you aren&#x27;t likely to run into an issue. Stick to reputable sites; reddit can help you figure out what those are. reply ShadowBanThis01 7 hours agorootparentWhat sites? Isn&#x27;t the purpose of the software discussed here to make tracker sites unnecessary? reply askiiart 12 hours agoparentprevJust use reputable sites and run a good adblocker, like Unlock Origin. reply LeoPanthera 16 hours agoprev> Pipe dream features> In-place seeding: identify files on your computer that are part of an indexed torrent, and allow them to be seeded in place after having moved, renamed or deleted parts of the torrentDoes anything do this already? It would be amazing to point a client at a folder of unstructured junk and have it magically find the right parts. reply nicolaslem 55 minutes agoparentI would be worried about the implications in terms of security and privacy. Back in the days, other P2P networks allowed to share arbitrary files and it was common for clueless users to just share their entire computer. reply chhs 10 hours agoparentprevI wrote a project to do this using libtorrent a while ago, but unfortunately libtorrent crashes when seeding thousands of torrents at once, which is the current blocker. I haven&#x27;t been working on it since.https:&#x2F;&#x2F;github.com&#x2F;chhs1&#x2F;content-seeder reply Scion9066 15 hours agoparentprevIt might be able to do this with the Bittorent V2 format as the hashes would be per file. reply chhs 9 hours agorootparentV2 definitely makes it easier, but it&#x27;s possible to identify files with V1 by iterating through metadata looking for files with the same length as a local file, then checking each piece against the local file. If they all match, then it&#x27;s the same file. For boundary pieces where it consists of multiple files, I think it&#x27;s safe enough to ignore if all of the remaining pieces match the local file, but you could do a more complex search looking for sets of files that all have the same length, and then compare the piece hash with the local files. reply orbisvicis 13 hours agoparentprevI&#x27;ve thought about writing a fuse system that tracks rename &#x2F; move &#x2F; delete and updates the torrent client. Never had the time, though. reply miffe 15 hours agoparentprevdc++ does it, but it&#x27;s not torrent based. reply thelastparadise 21 hours agoprev [–] Very interesting. Does this approach actually work in practice?Also what happens if illegal content gets scooped up into the index? reply lobsterslive 19 hours agoparentIt works well in practice. The DHT protocol includes announce messages that broadcast when new files are shared on BitTorrent. It then includes a \"geometric\" way to find people who are sharing those files. It doesn&#x27;t include the files themselves, just the torrents which include a file list and location hashes.If you listen to BitTorrent&#x27;s DHT network, you&#x27;ll build an index of everything shared on BitTorrent (over time), this will include commercial movies and such. reply grepfru_it 19 hours agorootparent>It works well in practice.Hi, I worked on gnutella and lots of P2P systems in the early 00s. This will devolve into noise and spam as the number of users who adopt this feature pass a critical mass. With a fully decentralized system, there are no gatekeepers, and as such, there is no way to filter counterfiet items. While your client will present with you the data you are searching for, you will find out (usually hours later) that your supposed pirated download is actually just a 2hour loop of Rick Astley (still piracy though, so you are still winning.. i think?). reply danpalmer 19 hours agorootparentI don&#x27;t think this project changes any of this? Torrents have been around for decades and this hasn&#x27;t been a problem yet. We can&#x27;t rule it out entirely but it does seem unlikely at this point to be worthwhile doing otherwise we&#x27;d see more exploitation.If the criticism is that a DHT crawler is going to be more subject to this than a website where people submit upload torrents, that may be the case, but I think the author of this project underestimates the DHT crawling going on. I believe the torrent ecosystem is largely automated and there&#x27;s little in the way of manual submission or human review going on. reply Retr0id 18 hours agorootparentThe \"problem\" is that most users aren&#x27;t crawling the DHT to find torrents, right now. The more people start using DHT crawlers as their primary way of finding new torrents, the more incentive there is to spam the DHT with junk, malware, etc. (because there will be more eyeballs on it)That is, the usefulness of DHT crawling is inversely proportional to how many people are doing it. reply danpalmer 18 hours agorootparentBut my second point is that I really think they are crawling the DHT, albeit indirectly. There are many torrent websites and they tend to have the same content. It seems fairly clear to me that this is what most torrent sites are doing. Maybe not the major names that users might submit to, but the long tail of other torrent search indexes certainly. It also seems to be what Popcorn Time does. reply hypertele-Xii 19 hours agorootparentprevWhile you&#x27;re technically correct, the protocol is resilient to such attack, as the number of people participating in a particular torrent is a good indicator of its validity. After all, everyone who was fooled will delete and stop sharing such items.New releases of something that just came out tend to suffer from this, though. Sometimes the counterfeits reach escape velocity - the rate of people joining in downloading the counterfeit exceed the rate of people realizing and stopping, thus giving the illusion of a legit torrent.Currently this problem is being solved by torrent sites&#x27; reputation and comment systems. If we imagine a world where only decentralized indexes like Bitmagnet exist, your prediction is 100% accurate. This only works if reputation from a reliable site is bootstrapping the initial popularity of a torrent. reply grepfru_it 19 hours agorootparent(btw my comment was&#x2F;is about the DHT crawler)You are describing a pay-to-play model. The validator is if the seeder&#x2F;leech count is high. Well does DHT provide aggregate bandwidth of each torrent? If not, you can easily spin up 1000+ nodes and connect to your torrent. Tada fake popularity. If bandwidth is known, then you simply raise your costs a bit by running fake clients. There are anti-piracy groups who&#x27;s entire mandate is to provide noise in the piracy ecosystem. Food for thought: bandwidth costs for this would be a rounding error for e.g. MGM, Universal, or any major content creator.DHT does not offer any sort of reputation or comment system. Back to centralized torrenting which is why I suspect DHT crawling has not been a very popular feature reply fluoridation 18 hours agorootparent> If not, you can easily spin up 1000+ nodes and connect to your torrent. Tada fake popularity. If bandwidth is known, then you simply raise your costs a bit by running fake clients.Sure, but like the other commenter said, this has been possible for years, and yet public trackers aren&#x27;t swamped with fake torrents. I think in all my years of using BitTorrent I&#x27;ve only ever found a single fake torrent, where the content was inside an encrypted RAR with no key (obviously there was no way to know it was encrypted ahead of time). reply kevincox 19 hours agorootparentprevIt seems like it would be pretty easy to make it appear that your spam torrent is highly active. reply iinnPP 18 hours agorootparentYou are correct. reply derefr 16 hours agorootparentprevOnce you&#x27;ve discovered a torrent being seeded, is there no way to interrogate the seeders and&#x2F;or the DHT itself, to find out the oldest active seeder registration on that torrent hash; and then use the time-of-oldest-observed-registration to rank torrents that claim to be \"the same thing\" in their metadata, but which have different piece-trie-hash-root?I ask, because a similar heuristic is used in crypto wallet software, visibility-weighting the various \"versions\" of a crypto token with the same metadata, by (in part) which were oldest-created. (The logic being: scam clones of a thing need to first observe the real thing, before they can clone it. So the real thing will always come first.)Of course, I&#x27;m assuming here that you&#x27;re searching for an \"expected to exist\" release of a thing by a specific distributor, where the distributor has a known-to-you structured naming scheme to the files in their releases, and so you&#x27;ll only be trying to rank \"versions\" of the torrent that all have identical names under this naming scheme, save for e.g. the [hash] part of the file name being different to match the content. This won&#x27;t help if you&#x27;re trying to find e.g. \"X song by Y artist, by any distributor.\" reply thomastjeffery 18 hours agorootparentprevGatekeeping is just a bad moderation method in the first place.What you need is sorting and categorization. If you really want to involve authoritative opinions on metadata, then use a web of trust. reply NegativeK 13 hours agorootparentI&#x27;ve yet to see a moderation method that works better than gatekeeping. reply SparkyMcUnicorn 19 hours agorootparentprevBut you can still pick the option with the most seeders, which should get you what you&#x27;re looking for most of the time.The spam problem isn&#x27;t nonexistent within the centralized services either. reply grepfru_it 18 hours agorootparentHehe in a popular P2P client from the &#x27;03-&#x27;05 period, we said the same thing. Turns out there are groups with large amounts of funding which will provide a fake seed count. Either just faking metadata making it seem there was a high seed count but bogus nodes which would refuse connections (which was actual behavior from clients with bad ISPs - which we saw valid cases in asia or east europe) or would actually stream data (and some of them were on good hosts seeding multi mbps of bad data)What i&#x27;m saying is it becomes a numbers game and those fake seeders usually have deep pockets financed by the content creators themselves reply fluoridation 19 hours agorootparentprevThe way to filter out garbage is to download things with lots of seeds, and if you still happen to download garbage, to immediately stop sharing it. reply latchkey 18 hours agorootparentChicken&#x2F;egg problem... as mentioned by someone else above...https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37779341> New releases of something that just came out tend to suffer from this, though. Sometimes the counterfeits reach escape velocity - the rate of people joining in downloading the counterfeit exceed the rate of people realizing and stopping, thus giving the illusion of a legit torrent. reply fluoridation 18 hours agorootparentIt&#x27;s possible. I never follow new releases. But back in the ed2k days, I&#x27;d say about half of just about any file you cared you search for was fake, regardless of age. reply no_time 19 hours agorootparentprev>If you listen to BitTorrent&#x27;s DHT network, you&#x27;ll build an index of everything shared on BitTorrent (over time),Correct me if I&#x27;m wrong but as far as I understand, passively listening on DHT would only mean you build up a list of infohashes of everything shared on BitTorrent. You&#x27;d actually have to reach out to your DHT peers to know what files the infohashes actually represents.Wrapping back to grandparent&#x27;s question of>Also what happens if illegal content gets scooped up into the index?I think this could get dicey if someone announces something very illegal like CP, and your crawler starts asking every peer that announced the infohash about it&#x27;s contents with this[0] protocol. This would put your IP into a pretty awful exclusive club ofA, other crawlersB, actual people wanting downloading said CP[0]: https:&#x2F;&#x2F;www.bittorrent.org&#x2F;beps&#x2F;bep_0009.html reply lobsterslive 18 hours agorootparent> Correct me if I&#x27;m wrong but as far as I understand, passively listening on DHT would only mean you build up a list of infohashes of everything shared on BitTorrent. You&#x27;d actually have to reach out to your DHT peers to know what files the infohashes actually represents.Yes, you&#x27;re correct! I should have stated that, you still need to resolve the metadata from the peers that have the infohashed files hosted. That&#x27;s a separate operation from downloading the file&#x27;s content. reply hattmall 12 hours agorootparentprevWould this get hashes of items shared on private trackers too? reply no_time 4 hours agorootparentNo because private trackers enforce that all torrents uploaded have DHT,PEX and LPD disabled. Usually done by a single tickbox that says “Make torrent private” in the client.Of course, respecting these options in the torrent file is still up to the client. This is one of the reasons why all private trackers have a client whitelist too. reply Akashic101 20 hours agoparentprevConsidering the screenshot on the linked page shows The Flash among other movies I dont think the author is too concerned about that. I believe its similar to the area that Plex and Jellyfin operate in, mainly that they just provide the framework and tools, what the user does with them is not in their control reply EGreg 19 hours agorootparentHow will governments police it when MaidSAFE and other systems distribute totally encrypted content? reply colinsane 18 hours agorootparentBitTorrent already supports encrypted peer connections. reply no_time 15 hours agorootparentEncryption without authentication in this case is as good as XORing all outgoing data with a fixed key. So basically useless... reply colinsane 14 hours agorootparentyes, exactly. BitTorrent supports encryption. swapping it out for some other encryption mechanism won&#x27;t change anything when it comes to government policing because that&#x27;s already not where the weaknesses lie for those sharing p2p content today. so what was GP&#x27;s point? reply mixmastamyk 16 hours agorootparentprevDoesn’t seem to work, our ISP has sent nastygrams despite this and site on https. reply colinsane 15 hours agorootparent> How will governments police it when MaidSAFE and other systems distribute totally encrypted content?so isn&#x27;t the answer then \"they&#x27;ll continue to police it the way they already do\"? i don&#x27;t know what a MaidSAFE is, but the context of this discussion is the DHT, and so public (indexable) torrents, and so however you encrypt the content doesn&#x27;t matter because you have to provide the decryption method to anyone who asks for any of the previous context (public torrents&#x2F;indexes) to make any sense. reply mixmastamyk 15 hours agorootparentEncrypted connections shouldn’t decrypt for anyone who asks, otherwise they have no reason to exist.The weak point seems to be the tracker or filename, but been told https hides that so not sure. replyKoftaBob 20 hours agoparentprev [–] Based on my understanding of how the torrent DHT works, all that’s happening is that youre requesting metadata on various torrent info hashes, but that’s not the same thing as actually downloading&#x2F;seeding the content in the torrent itself. reply no_time 19 hours agorootparent [–] >but that’s not the same thing as actually downloading&#x2F;seeding the content in the torrent itself.The question is whether Law Enforcement and \"Intellectual Property\" watchdogs make a meaningful distinction between the two in their monitoring tools. reply justinclift 19 hours agorootparent [–] Seeing as they get so much other stuff wrong, it&#x27;s highly likely they don&#x27;t care. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bitmagnet is a self-hosted BitTorrent indexer with unique features like a Distributed Hash Table (DHT) crawler, content classifier, and torrent search engine; these enable users to search for torrents without relying on external entities.",
      "The project is still in the alpha phase but already has several promising features, such as a generic BitTorrent indexer, a content classifier, a torrent search engine, and a GraphQL API – a technology making it easier to get data from a server to a client.",
      "The team is working on high-priority features not yet implemented, like classifiers for other content types, search result ordering, search performance optimization, and other handy features such as authentication, saved searches, and support for BitTorrent v2 protocol. It's open to support on GitHub."
    ],
    "commentSummary": [
      "The discussion focuses on different elements of DHT crawling, namely its implementation and optimization of its tools.",
      "Other pressing concerns include illegal and counterfeit content, the requirement for moderation and reputation systems.",
      "There's also a debate on the potential of governments to scrutinize encrypted content."
    ],
    "points": 416,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1696506257
  },
  {
    "id": 37782493,
    "title": "How fast are Linux pipes anyway? (2022)",
    "originLink": "https://mazzo.li/posts/fast-pipes.html",
    "originBody": "← 2022-06-01 How fast are Linux pipes anyway? In this post, we will explore how Unix pipes are implemented in Linux by iteratively optimizing a test program that writes and reads data through a pipe.1 We will begin with a simple program with a throughput of around 3.5GiB/s, and improve its performance twentyfold. The improvements will be informed by profiling the program using Linux’s perf tooling.2 The code is available on GitHub. Chart showing the performance of our pipe test programs. The post was inspired by reading a highly optimized FizzBuzz program, which pushes output to a pipe at a rate of ~35GiB/s on my laptop.3 Our first goal will be to match that speed, explaining every step as we go along. We’ll also add an additional performance-improving measure, which is not needed in FizzBuzz since the bottleneck is actually computing the output, not IO, at least on my machine. We will proceed as follows: A first slow version of our pipe test bench; How pipes are implemented internally, and why writing and reading from them is slow; How the vmsplice and splice syscalls let us get around some (but not all!) of the slowness; A description of Linux paging, leading up to a faster version using huge pages; The final optimization, replacing polling with busy looping; Some closing thoughts. Section 4 is the heaviest on Linux kernel internals, so it might be interesting even if you’re familiar with the other topics treated in the post. For readers not familiar with the topics treated, only basic knowledge of C is assumed. Let’s begin! This will be similar in style to my atan2f performance investigation, although the program in question will only be useful for learning. Moreover, we will optimize code at a different level. While tuning atan2f consisted in micro-optimizations guided by the assembly output, tuning our pipe program will involve looking at perf events and reducing various sorts of kernel overhead.↩︎ The tests were run on an Intel Skylake i7-8550U CPU, and on Linux 5.17. Your mileage will vary, since the Linux internals that power the programs described in this post have been under constant change for the past couple of years, and will probably continue to be tweaked in future releases. Keep reading for more details!↩︎ “FizzBuzz” is an allegedly common coding interview question. The details are not relevant to this blog post, but they are explained in the link. I have personally never been asked it, but I have it on good authority that it does happen!↩︎ The challenge, and a slow first version # First of all, let’s start with measuring the performance of the fabled FizzBuzz program, following the rules laid down by the StackOverflow post: % ./fizzbuzzpv >/dev/null 422GiB 0:00:16 [36.2GiB/s] pv is “pipe viewer”, a handy utility to measure the throughput of data flowing through a pipe. So fizzbuzz is producing output at a rate of 36GiB/s. fizzbuzz writes the output in blocks as big as the L2 cache, to strike a good balance between cheap access to memory and minimizing IO overhead. On my machine, the L2 cache is 256KiB. Throughout this post, we’ll also output blocks of 256KiB, but without “computing” anything. Essentially, we’ll try to measure the upper bound for programs writing to a pipe with a reasonable buffer size.4 While fizzbuzz uses pv to measure speed, our setup will be slightly different: we’ll implement the programs on both ends of the pipe. This is so that we fully control the code involved in pushing and pulling data from the pipe. While we fix the buffer size, the numbers are actually not wildly different if we use different buffer sizes, given that other bottlenecks kick in.↩︎ The code is available in my pipes-speed-test repo. write.cpp implements the writing, and read.cpp the reading. write repeatedly writes the same 256KiB forever. read reads through 10GiB of data and terminates, printing the throughput in GiB/s. Both executables accept a variety of command line options to change their behavior. The first attempt at reading and writing from pipes will be using the write and read syscalls, using the same buffer size as fizzbuzz. Here’s a view of the writing end: int main() { size_t buf_size = 10) { // Keep invoking `write` until we've written the entirety // of the buffer. Remember that write returns how much // it could write into the destination -- in this case, // our pipe. ssize_t written = write( STDOUT_FILENO, buf + (buf_size - remaining), remaining ); remaining -= written; } } } This snippet and following ones omit all error checking for brevity.5 The memset ensures that the output will be printable, but also plays another role, as we’ll discuss later. The work is all done by the write call, the rest is making sure that the whole buffer is written. The read end is very similar, but reading data into buf, and terminating when enough has been read. After building, the code from the repo can be run as follows: Feel free to refer to the repo for the gory details. More generally, I won’t reproduce the code verbatim here, since the details are unimportant. I will instead post snippets of code representative of what is going on.↩︎ % ./write./read 3.7GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) We’re writing the same 256KiB buffer filled with 'X's 40960 times, and measuring the throughput. What’s worrying is that we’re 10 times slower than fizzbuzz! And we’re not doing any work, just writing bytes to the pipe. It turns out that we can’t get much faster than this by using write and read. The trouble with write # To find out what our program is spending time on, we can use perf:6 7 % perf record -g sh -c './write./read' 3.2GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) [ perf record: Woken up 6 times to write data ] [ perf record: Captured and wrote 2.851 MB perf.data (21201 samples) ] The -g instructs perf to record call graphs: this will allow us to take a top-down look at where time is being spent. We can take a look at where time is spent using perf report. Here is a lightly redacted excerpt, breaking down where write spends its time:8 % perf report -g --symbol-filter=write - 48.05% 0.05% write libc-2.33.so [.] __GI___libc_write - 48.04% __GI___libc_write - 47.69% entry_SYSCALL_64_after_hwframe - do_syscall_64 - 47.54% ksys_write - 47.40% vfs_write - 47.23% new_sync_write - pipe_write + 24.08% copy_page_from_iter + 11.76% __alloc_pages + 4.32% schedule + 2.98% __wake_up_common_lock 0.95% _raw_spin_lock_irq 0.74% alloc_pages 0.66% prepare_to_wait_event 47% of the time is spent in pipe_write, which is what write resolves to if we’re writing to a pipe. This is not surprising — we’re spending roughly half of the time writing, and the other half reading. Within pipe_write, 3/4 of the time is spent copying or allocating pages (copy_page_from_iter and __alloc_pages). If we already have an idea of how communication between the kernel and userspace works this might make some sense. Regardless, to fully understand what’s happening we must first understand how pipes work. Note that here we’re profiling a shell invocation including both the pipe reading and writing — perf record follows all child processes by default.↩︎ When profiling this program, I noticed that the perf output was being polluted by information from the “Pressure Stall Information” infrastructure (PSI). Therefore the numbers are taken from a kernel compiled with PSI disabled. This can be achieved by putting CONFIG_PSI=n in the kernel build configuration. In NixOS: boot.kernelPatches = [{ name = \"disable-psi\"; patch = null; extraConfig = '' PSI n ''; }]; Moreover, the kernel debug symbols must be present for perf to correctly show where time is spent while in syscalls. How to install the symbols varies from distro to distro. In recent NixOS versions they are installed by default.↩︎ In perf report you can use + to expand a call graph, assuming you ran perf record -g.↩︎ What are pipes made of? # The data structure holding a pipe can be found in include/linux/pipe_fs_i.h, and the operations on it in fs/pipe.c. A Linux pipe is a ring buffer holding references to pages where the data is written to and read from: In the image above the ring buffer has 8 slots, but we might have more or less, the default being 16. Each page is 4KiB on x86-64, but might be of different sizes on other architectures. In total, this pipe can hold at most 32KiB of data. This is a key point: every pipe has an upper bound on the total amount of data it can hold before it’s full. The shaded part of the diagram represents the current pipe data, the non-shaded part the empty space in the pipe. Somewhat counterintuitively, head stores the write-end of the pipe. That is, writers will write into the buffer pointed at by head, and increase head accordingly if they need to move onto the next buffer. Within the write buffer, len stores how much we’ve written in it. Conversely, tail stores the read-end of the pipe: readers will start consuming the pipe from there. offset indicates where to start reading from. Note that tail can appear after head, like in the picture, since we’re working with a circular/ring buffer. Also note that some slots might be unused when we haven’t filled the pipe completely — the NULL cells in the middle. If the pipe is full (no NULLs and no free space in the pages), write will block. If the pipe is empty (all NULLs), read will block. Here’s an abridged version of the C data structures in pipe_fs_i.h: struct pipe_inode_info { unsigned int head; unsigned int tail; struct pipe_buffer *bufs; }; struct pipe_buffer { struct page *page; unsigned int offset, len; }; We’re omitting many fields here, and we’re not explainining what struct page contains yet, but this is the key data structure to understanding how reading and writing from a pipe happens. Reading and writing to pipes # Let’s now go to the definition of pipe_write, to try and make sense of the perf output shown before. Here is a simplified explanation of how pipe_write works: If the pipe is already full, wait for space and restart; If the buffer currently pointed at by head has space, fill that space first; While there’s free slots, and there are remaining bytes to write, allocate new pages and fill them, updating head. What happens to a pipe when we write to it. The operations described above are protected by a lock, which pipe_write acquires and releases as necessary. pipe_read is the mirror image of pipe_write, except that we consume pages, free them when we’ve fully read them, and update tail.9 So, we now have a quite unpleasant picture of what is going on: We copy each page twice, once from user memory to the kernel, and back again to the kernel to user memory; The copying is done one 4KiB page at a time, interspersed with other activity, such as the synchronization between read and write, and page allocation and freeing; We are working with memory that might not be contiguous, since we’re constantly allocating new pages; We’re acquiring and releasing the pipe lock. On this machine, sequential RAM reading clocks at around 16GiB/s: One single “spare page” called tmp_page is actually kept around by pipe_read, and reused by pipe_write. However, since this is always only a single page, I couldn’t leverage it to achieve higher performance given that the page reuse is counteracted by fixed overhead when calling pipe_write and pipe_read.↩︎ % sysbench memory --memory-block-size=1G --memory-oper=read --threads=1 run ... 102400.00 MiB transferred (15921.22 MiB/sec) Given all the fiddliness listed above, a 4x slowdown compared to single-threaded sequential RAM speed is not that surprising. Tweaking the buffer size or the pipe size to reduce the amount of syscall and synchronization overhead, or tuning other parameters will not get us very far. Luckily, there is a way to get around the slowness of write and of read altogether. Splicing to the rescue # This copying of buffers from user memory to the kernel and back is a frequent thorn in the side of people needing to do fast IO. One common solution is to just cut the kernel out of the picture and perform IO operations directly. For example we might interact directly with a network card and bypass the kernel for low-latency networking. In general when we write to a socket, or a file, or in our case a pipe, we’re first writing to a buffer somewhere in the kernel, and then let the kernel do its work. In the case of pipes, the pipe is a series of buffers in the kernel. All this copying is undesirable if we’re in the business of performance. Luckily, Linux includes system calls to speed things up when we want to move data to and from pipes, without copying. Specifically: splice moves data from a pipe to a file descriptor, and vice-versa. vmsplice moves data from user memory into a pipe.10 Crucially, both operations work without copying anything. Now that we know how pipes work, we can already vaguely imagine how the two operations function: they just “grab” an existing buffer from somewhere and put it into the pipe ring buffer, or the reverse, rather than allocating new pages as needed: Technically, vmsplice also supports transferring data in the other direction, although not in a useful way. As the man page states: vmsplice really supports true splicing only from user memory to a pipe. In the opposite direction, it actually just copies the data to user space. ↩︎ We’ll soon see exactly how this works. Splicing in practice # Let’s replace write with vmsplice. This is the signature for vmsplice: struct iovec { void *iov_base; // Starting address size_t iov_len; // Number of bytes }; // Returns how much we've spliced into the pipe ssize_t vmsplice( int fd, const struct iovec *iov, size_t nr_segs, unsigned int flags ); fd is the target pipe, struct iovec *iov is an array of buffers we’ll be moving to the pipe. Note that vmsplice returns how much was “spliced” into the pipe, which might not be the full amount, much like how write returns how much was written. Remember that pipes are bounded by how many slots they have in the ring buffer, and vmsplice is not exempt from this restriction. We also need to be a bit careful when using vmsplice. Since the user memory is moved to the pipe without copying, we must ensure that the read-end consumes it before we can reuse the spliced buffer. For this reason fizzbuzz uses a double buffering scheme, which works as follows: Split the 256KiB buffer in two; Set the pipe size to 128KiB, this will have the effect of setting the pipe ring buffer to have 128KiB/4KiB = 32 slots; Alternate between writing to the first half-buffer and using vmsplice to move it to the pipe and doing the same with the other half. The fact that the pipe size is set to 128KiB, and that we wait for vmsplice to fully output one 128KiB buffer, ensures that by the time we’re done with one iteration of vmsplice we know that the the previous buffer has been fully read — otherwise we would not have been able to fully vmsplice the new 128KiB buffer into the 128KiB pipe. Now, we’re not actually writing anything to the buffers, but we’ll keep the double buffering scheme since a similar scheme would be required for any program actually writing content.11 Our write loop now looks something like this: Travis Downs pointed out that this scheme might still be unsafe, since the page could be spliced further, therefore extending its lifetime. This problem is also present in the original FizzBuzz post. It’s actually not entirely clear to me whether vmsplice without SPLICE_F_GIFT is actually unsafe — the man page for vmsplice implies it shouldn’t be. However, it’s definitely the case that particular care is needed to achieve zero copy piping while maintaining safety. In the test program the reading end splices the pipe into /dev/null, so it could be that the kernel knows that the pages can be spliced without copying, but I have not verified whether this is what’s actually happening.↩︎ int main() { size_t buf_size = 10) { ssize_t ret = vmsplice(STDOUT_FILENO, &bufvec, 1, 0); bufvec.iov_base = (void*) (((char*) bufvec.iov_base) + ret); bufvec.iov_len -= ret; } } } Here are the results writing with vmsplice, rather than write: % ./write --write_with_vmsplice./read 12.7GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) This reduces by half the amount of copying we need to do, and already improves our througput more than threefold — to 12.7GiB/s. Changing the read end to use splice, we eliminate all copying, and get another 2.5x speedup: % ./write --write_with_vmsplice./read --read_with_splice 32.8GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) Fishing for pages # What next? Let’s ask perf: % perf record -g sh -c './write --write_with_vmsplice./read --read_with_splice' 33.4GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.305 MB perf.data (2413 samples) ] % perf report --symbol-filter=vmsplice - 49.59% 0.38% write libc-2.33.so [.] vmsplice - 49.46% vmsplice - 45.17% entry_SYSCALL_64_after_hwframe - do_syscall_64 - 44.30% __do_sys_vmsplice + 17.88% iov_iter_get_pages + 16.57% __mutex_lock.constprop.0 3.89% add_to_pipe 1.17% iov_iter_advance 0.82% mutex_unlock 0.75% pipe_lock 2.01% __entry_text_start 1.45% syscall_return_via_sysret The lion’s share of the time is taken by locking the pipe for writing (__mutex_lock.constprop.0), and by moving the pages into the pipe (iov_iter_get_pages). There isn’t so much we can do about the locking, but we can improve the performance of iov_iter_get_pages. As the name suggests, iov_iter_get_pages turns the struct iovecs we feed into vmsplice into struct pages to put into the pipe. To understand what this function actually does, and how to speed it up, we must first take a detour into how the CPU and Linux organize pages. A whirlwind tour of paging # As you might be aware of, processes do not refer to locations in RAM directly: instead, the are assigned virtual memory addresses, which get resolved to physical addresses. This abstraction is known as virtual memory, and has all sorts of advantages we won’t cover here — the most obvious being that it significantly simplifies running multiple processes competing for the same physical memory. In any case, whenever we execute a program and we load/store from/to memory, the CPU needs to convert our virtual address to a physical address. Storing a mapping from every virtual address to every corresponding physical address would be impratical. Therefore memory is split up in uniformly sized chunks, called pages, and virtual pages are mapped to physical pages:12 There’s nothing special about 4KiB: each architecture picks a size, based on various tradeoffs — some of which we’ll soon explore. To make this a bit more precise, let’s imagine allocating 10000 bytes using malloc: void* buf = malloc(10000); printf(\"%p\\n\", buf); // 0x6f42430 Here we’re presenting a simplified model where physical memory is a simple flat, linear sequence. Reality is a bit more complicated, but the simple model will do for our purposes.↩︎ As we use them, our 10k bytes will look contiguous in virtual memory, but will be mapped to 3 not necessarily contiguous physical pages:13 You can inspect the physical address assigned to the current process’ virtual pages by reading /proc/self/pagemap, as illustrated in a previous post on this blog, and multiplying the “page frame number” by the page size.↩︎ One of the tasks of the kernel is to manage this mapping, which is embodied in a data structure called the page table. The CPU specifies how the page table looks (since it needs to understand it), and the kernel manipulates it as needed. On x86-64 the page table is a 4-level, 512-way tree, which itself lives in memory.14 Each node of this tree is (you guessed it!) 4KiB wide, with each entry within the node leading to the next level being 8 bytes (4KiB/8bytes = 512). The entries contain the address of the next node, along with other metadata. We have one page table per process — or in other words, each process has a reserved virtual address space. When the kernel context-switches to a process, it sets the special register CR3 to the physical address of the root of this tree.15 Then whenever a virtual address needs to be converted to a physical address, the CPU splits up the address in sections, and uses them to walk this tree and compute the physical address. To make these concepts less abstract, here’s a visual depiction of how the virtual address 0x0000f2705af953c0 might be resolved to a physical address: Intel extended the page table to consist of 5 levels starting from Ice Lake, thereby increasing the maximum addressable memory from 256TiB to 128PiB. However this capability has to explicitly enabled, since some programs rely on the upper 16 bits of pointers to be unused.↩︎ The addresses within the page table must be physical, otherwise we’d have infinite loop on our hands.↩︎ The search starts from the first level, called the “page global directory”, or PGD, the physical location of which is stored in CR3. The first 16 bits of the address are unused.16 We use the next 9 bits the PGD entry, and traverse down to the second level, “page upper directory”, or PUD. The next 9 bits are used to select an entry from the PUD. The process repeats for the next two levels, PMD (“page middle directory”), and PTE (“page table entry”). The PTE tells where the actual physical page we’re looking for is, and then we use the last 12 bits to find the offset inside the page. The sparse structure of the page table allows the mapping to be gradually built up as new pages are needed. Whenever a process needs memory, the page table will be updated with a new entry by the kernel. Note that the highest 16 bits are unused: this means that each process can address at most 2 48 − 1 2 48 −1 bytes, or 256TiB, of physical memory.↩︎ The role of struct page # The struct page data structure is a key piece of this machinery: it is what the kernel uses to refer to a single physical page, storing its physical address and all sorts of other metadata about it.17 For instance we can get a struct page from the information contained in the PTE (the last level of the page table described above). In general it is used pervasively in all code handling page-related matters. In the case of pipes, struct page is used to hold their data in the ring buffer, as we’re already seen: struct pipe_inode_info { unsigned int head; unsigned int tail; struct pipe_buffer *bufs; }; struct pipe_buffer { struct page *page; unsigned int offset, len; }; struct page might also refer to yet-to-be-allocated physical pages, which do not have a physical address yet, and other page-related abstractions. Think of them as fairly abstract references to physical pages, but not necessarily references to an allocated physical page. This subtle point will be relevant in a later sidenote 🫠.↩︎ However, vmsplice accepts virtual memory as input, while struct page refers to physical memory directly. Therefore we need turn arbitrary chunks of virtual memory into a bunch of struct pages. This is exactly what iov_iter_get_pages does, and where we’re spending half of our time: ssize_t iov_iter_get_pages( struct iov_iter *i, // input: a sized buffer in virtual memory struct page **pages, // output: the list of pages which back the input buffers size_t maxsize, // maximum number of bytes to get unsigned maxpages, // maximum number of pages to get size_t *start // offset into first page, if the input buffer wasn't page-aligned ); struct iov_iter is a Linux kernel data structure representing various ways of walking through chunks of memory, including struct iovec. In our case, it will point to a 128KiB buffer. vmsplice will use iov_iter_get_pages to turn the input buffer into a bunch of struct pages, and hold on to them. Now that you know how paging works, you might vaguely imagine how iov_iter_get_pages works as well, but we’ll explain it in detail in the next section. We’ve rapidly gone through a lot of new concepts, so to recap: Modern CPUs use virtual memory for their processes; Memory is organized in regularly-sized pages; The CPU translates virtual addresses into physical addresses using a page table mapping virtual pages to physical pages; The kernel adds and removes entries to the page table as necessary; Pipes are made out of references to physical pages, so vmsplice must convert virtual memory ranges into physical pages, and hold on to them. The cost of getting pages # The time spent in iov_iter_get_pages is really entirely spent in another function, get_user_pages_fast: % perf report -g --symbol-filter=iov_iter_get_pages - 17.08% 0.17% write [kernel.kallsyms] [k] iov_iter_get_pages - 16.91% iov_iter_get_pages - 16.88% internal_get_user_pages_fast 11.22% try_grab_compound_head get_user_pages_fast is a more bare-bones version of iov_iter_get_pages: int get_user_pages_fast( // virtual address, page aligned unsigned long start, // number of pages to retrieve int nr_pages, // flags, the meaning of which we won't get into unsigned int gup_flags, // output physical pages struct page **pages ) Here “user” (as opposed to “kernel”) refers to the fact that we’re turning virtual pages into references to physical pages. To get our struct pages, get_user_pages_fast does exactly what the CPU would do, but in software: it walks the page table to collect all the physical pages, storing the results in struct pages. In our case, we have a 128KiB buffer, and 4KiB pages, so we’ll have nr_pages = 32.18 get_user_pages_fast will need to walk the page table tree collecting 32 leaves, and storing the result in 32 struct pages. get_user_pages_fast also needs to make sure that the physical page is not repurposed until the caller doesn’t need it anymore. This is achieved in the kernel using a reference count stored in struct page, which is used to know when a physical page can be released and repurposed in the future. The caller of get_user_pages_fast must, at some point, release the pages again with put_page, which will decrease the reference count. Finally, get_user_pages_fast behaves differently depending on whether virtual addresses are already in the page table. This is where the _fast suffix comes from: the kernel will first try to get an already existing page table entry and corresponding struct page by just walking the page table, which is relatively cheap, and fall back to producing a struct page by other, more expensive means otherwise. The fact that we memset the memory at the beginning will ensure that we never end up in the “slow” path of get_user_pages_fast, since the page table entries will be created as our buffer is filled with 'X's.19 Note that the get_user_pages family of functions is not only useful for pipes — in fact, it is central in many drivers. A typical use is related to the kernel bypass we mentioned: a driver for a network card might use it to turn some user memory region into a physical page, then communicate the physical page location to the network card, and have the network card interact directly with that memory region without kernel involvement. Actually, the pipe code happens to always call get_user_pages_fast with nr_pages = 16, looping if necessary, presumably so that a small static buffer can be used. But it is an implementation detail, and the total number of spliced pages will still be 32.↩︎ Subtleties follow, not needed to understand the rest of the post! If the page table does not contain the entry we’re looking for, get_user_pages_fast still needs to return a struct page. The most obvious way to do so would be to create the right page table entry, and then return the corresponding struct page. However get_user_pages_fast will only do so if it’s asked to get struct page for the purpose of writing into it. Otherwise it will not update the page table, instead returning a struct page giving us a reference to a yet-to-be-allocated physical page. This is exactly what happens in the case of vmsplice, since we just need to produce a struct page for the purpose of filling the pipe, without actually writing any memory. Or in other words, allocating the page is delayed until we actually need to. This saves allocating the physical page, but will cause the slow path of get_user_pages_fast to be called repeatedly if the page is never faulted in by other means. Therefore, if we do not memset before, and therefore do not fault the pages into the page table “manually”, not only we would end up in the slow path the first time we call get_user_pages_fast, but also all successive invocations, resulting in a significant slowdown (25GiB/s rather than 30GiB/s): % ./write --write_with_vmsplice --dont_touch_pages./read --read_with_splice 25.0GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) Moreover, this behavior does not manifest itself when using huge pages: in that case get_user_pages_fast will properly fault the pages in when the virtual memory range passed in would be backed by huge pages. If this is all very confusing, don’t worry, get_user_pages and friends seem to be a very tricky corner of the kernel, even for kernel developers.↩︎ Huge pages # Up to now we’ve presented pages as always being of the same size — 4KiB on x86-64. However, many CPU architectures, including x86-64, include larger page sizes. In the case of x86-64, we not only have 4KiB pages (the “standard” size), but also 2MiB and even 1GiB pages (“huge” pages). In the rest of the post we’ll only deal with 2MiB huge pages, since 1GiB pages are fairly uncommon, and overkill for our task anyway. Architecture Smallest page size Larger page sizes x86 4KiB 2MiB, 4MiB x86-64 4KiB 2MiB, 1GiB20 ARMv7 4KiB 64KiB, 1MiB, 16MiB ARMv8 4KiB 16KiB, 64KiB RISCV32 4KiB 4MiB RISCV64 4KiB 2MiB, 1GiB, 512GiB, 256 TiB Power ISA 8KiB 64 KiB, 16 MiB, 16 GiB Page sizes available on architectures commonly used today, from Wikipedia. The main advantage of huge pages is that bookkeeping is cheaper, since there’s fewer of them needed to cover the same amount of memory. Moreover other operations are cheaper too, such as resolving a virtual address to a physical address, since one level less of page table is needed: instead of having a 12-bit offset into the page, we’ll have a 21-bit offset, and one less page table level. Only when the CPU has PDPE1GB flag.↩︎ This relieves pressure on the parts of the CPUs that handle this conversion, leading to performance improvements in many circumstances.21 However, in our case, the pressure is not on the hardware that walks the page table, but on its software counterpart which runs in the kernel. On Linux, we can allocate a 2MiB huge page in a variety of ways, such as by allocating memory aligned to 2MiB and then using madvise to tell the kernel to use huge pages for the provided buffer: void* buf = aligned_alloc(1 << 21, size); madvise(buf, size, MADV_HUGEPAGE) Switching to huge pages in our program yields another ~50% improvement: % ./write --write_with_vmsplice --huge_page./read --read_with_splice 51.0GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) For instance, the CPU includes dedicated hardware to cache parts of the page table, the “translation lookaside buffer” (TLB). The TLB is flushed at every context switch (every time we change the contents of CR3). Huge pages can significantly reduce TLB misses, since a single entry for a 2MiB page covers 512 times more memory compared to a 4KiB page.↩︎ However, the reason for the improvements is not totally obvious. Naively, we might think that by using huge pages struct page will just refer to a 2MiB page, rather than 4KiB. Sadly this is not the case: the kernel code assumes everywhere that a struct page refers to a page of the “standard” size for the current architecture. The way this works for huge pages (and in general for what Linux calls “compound pages”) is that a “head” struct page contains the actual information about the backing physical page, with successive “tail” pages just containing a pointer to the head page. So to represent 2MiB huge page we’ll have 1 “head” struct page, and up to 511 “tail” struct pages. Or in the case of our 128KiB buffer, 31 tail struct pages:22 Even if we need all these struct pages, the code generating it ends up significantly faster. Instead of traversing the page table multiple times, once the first entry is found, the following struct pages can be generated in a simple loop. Hence the performance improvement! If you’re thinking “that’s horrible!”, you’re not alone. Various efforts are underway to simplify and/or optimize this situation. Recent kernels (from 5.17 onwards) include a new type, struct folio, identifying head pages explicitly. This reduces the need for checking whether a struct page is a head page or tail page at runtime, yielding performance improvements. Other efforts aim to outright remove the extra struct pages, although I’m not up to date on how that is going.↩︎ Busy looping # We’re almost done, I promise! Let’s look at perf output once again: - 46.91% 0.38% write libc-2.33.so [.] vmsplice - 46.84% vmsplice - 43.15% entry_SYSCALL_64_after_hwframe - do_syscall_64 - 41.80% __do_sys_vmsplice + 14.90% wait_for_space + 8.27% __wake_up_common_lock 4.40% add_to_pipe + 4.24% iov_iter_get_pages + 3.92% __mutex_lock.constprop.0 1.81% iov_iter_advance + 0.55% import_iovec + 0.76% syscall_exit_to_user_mode 1.54% syscall_return_via_sysret 1.49% __entry_text_start We’re now spending a significant amount of time waiting for the pipe to be writeable (wait_for_space), and waking up readers which were waiting for the pipe to have content (__wake_up_common_lock). To sidestep these synchronization costs, we can ask vmsplice to return if the pipe cannot be written to, and busy loop until it is — and the same when reading with splice: ... // SPLICE_F_NONBLOCK will cause `vmsplice` to return immediately // if we can't write to the pipe, returning EAGAIN ssize_t ret = vmsplice(STDOUT_FILENO, &bufvec, 1, SPLICE_F_NONBLOCK); if (ret < 0 && errno == EAGAIN) { continue; // busy loop if not ready to write } ... By busy looping we get another 25% performance increase: % ./write --write_with_vmsplice --huge_page --busy_loop./read --read_with_splice --busy_loop 62.5GiB/s, 256KiB buffer, 40960 iterations (10GiB piped) Obviously busy looping comes at the cost of fully occupying a CPU core waiting for vmsplice to be ready. But often this compromise is worth it, and in fact it is a common pattern for high-performance server applications: we trade off possibly wasteful CPU utilization for better latency and/or throughput. In our case, this concludes our optimization journey for our little synthetic benchmark, from 3.5GiB/s to 65GiB/s. Closing thoughts # We’ve systematically improved the performance of our program by looking at the perf output and the Linux source. Pipes and splicing in particular aren’t really hot topics when it comes to high-performance programming, but the themes we’ve touched upon are: zero-copy operations, ring buffers, paging & virtual memory, synchronization overhead. There are some details and interesting topics I left out, but this blog post was already spiraling out of control and becoming too long: In the actual code, the buffers are allocated separatedly, to reduce page table contention by placing them in different page table entries (something that the FizzBuzz program also does). Remember that when a page table entry is taken with get_user_pages, its refcount is increased, and decreased on put_page. If we use two page table entries for the two buffers, rather than one page table entry for both of them, we have less contention when modifying the refcount. The tests are ran by pinning the ./write and ./read processes to two cores with taskset. The code in the repo contains many other options I played with, but did not end up talking about since they were irrelevant or not interesting enough. The repo also contains a synthetic benchmark for get_user_pages_fast, which can be used to measure exactly how much slower it runs with or without huge pages. Splicing in general is a slightly dubious/dangerous concept, which continues to annoy to kernel developers. Please let me know if this post was helpful, interesting, or unclear! Acknowledgements # Many thanks to Alexandru Scvorţov, Max Staudt, Alex Appetiti, Alex Sayers, Stephen Lavelle, Peter Cawley, and Niklas Hambüchen for reviewing drafts of this post. Max Staudt also helped me understand some subtleties of get_user_pages. f@mazzo.li · twitter · source Comments Awesome article, thanks for this! Diego ROJAS, 2023-10-07 Reply Permalink Submit Preview Both name and email are optional, and will be visible if you provide them. Comments cannot be edited or deleted by you after submission, Email me if you need to do so. The comment will be rendered using a limited Markdown. You can input math by using $inline$ or $$block$$ LaTeX syntax.",
    "commentLink": "https://news.ycombinator.com/item?id=37782493",
    "commentBody": "How fast are Linux pipes anyway? (2022)Hacker NewspastloginHow fast are Linux pipes anyway? (2022) (mazzo.li) 394 points by SEJeff 15 hours ago| hidepastfavorite68 comments Too 5 hours agoSo if I understand correctly, vmsplice is more of a mini shared memory mechanism between two processes, if used on both the reader and writer end simultaneously? Meaning both processes need to be exceptionally careful in when they read and write to the buffers and how it is returned after use. Hot, yet scary at the same time.Other main takeaway, it’s a bit sad that the naive implementation everybody will write, is 20x slower than what is possible.Exceptionally written article btw. reply winternewt 4 hours agoparentAnd if you try to write the 20x faster version, your coworkers will think you are over-complicating and not being a team player. reply nesarkvechnep 3 hours agorootparentIn the end they&#x27;ll just use a Lambda. reply db48x 3 hours agorootparentprevNot necessarily. Good comments go a long way. reply nikanj 2 hours agorootparentprevYour coworkers would prefer you splitting the thing into two microservices communicating over a REST api, aka the 200x slower version. reply hgraves1991 2 hours agorootparentWe hate to see it reply xxs 2 hours agorootparentprevit&#x27;s time to change your workplace, not everyone is meant to be petty&#x2F;incompetent. reply jcrites 11 hours agoprevAre there good data handling libraries that provide abstractions over pipes, sockets, files, and memory and implement optimizations like these? I&#x27;d be interested in knowing if there are such libraries in C, C++, Rust, or other systems languages.I wasn&#x27;t familiar with some of the APIs mentioned in the article like splice() and vmsplice(), so I wondered if there are libraries that I might use when building ~low-level applications that take advantage of these and related optimizations where possible automagically. (As another commenter mentioned: these APIs are hard to use and most programs don&#x27;t take advantage of them)Do libraries like libuv, tokio, Netty handle this automatically on Linux? (From some brief research, it seems like probably they do) reply duped 9 hours agoparentThis may go against the grain but this isn&#x27;t really worth abstracting over since it&#x27;s not portable. You&#x27;ll probably want to implement it by hand everywhere you need it.Higher level code only uses them rarely because they&#x27;re pretty special purpose and they have to be specialized for Linux. If you&#x27;re shuffling data around without looking at it only on Linux, splice is useful. There&#x27;s not that many applications that have that property (something like say, TCP&#x2F;UDP proxies definitely need it - but your bog standard HTTP server? Not so much).And if you are writing these apps then the buzzwords like \"zero copy\" come up often, and splice is one of the first results you&#x27;ll see. reply gpderetta 8 minutes agorootparentI think Linus generally considers splice a failed experiment. It works fine is some simple scenarios, but the generalized support for it needed to make it work failed to materialize.Having said that, these days sendfile is implemented in term of splice, so in a way many HTTP servers use it. reply NavinF 2 hours agorootparentprevThe main reason why people write abstractions over stuff like this is to make it portable. I&#x27;m sure there&#x27;s something similar to vmsplice on every relevant OS. The library can also fallback to write_read if you&#x27;re targeting some ancient platform reply jeromegn 10 hours agoparentprevThere’s a crate for tokio, so it’s not automatic but might still be interesting: https:&#x2F;&#x2F;lib.rs&#x2F;crates&#x2F;tokio-splice reply mg 14 hours agoprevOne surprising fact about Linux pipes I stumbled across 4 years ago is that using a pipe can create indeterministic behavior:https:&#x2F;&#x2F;www.gibney.org&#x2F;the_output_of_linux_pipes_can_be_inde... reply jstimpfle 14 hours agoparentNot surprising, the pipe you&#x27;ve created doesn&#x27;t transport any of the data you&#x27;ve echoed. (echo red; echo green 1>&2)echo blueThis creates two subshells separated by the pipesymbol. A subshell is a child process of the current shell, and as such it inherits important properties of the current shell, notably including the open file descriptor table.Since they are child processes, both subshells run concurrently, while their parent shell will simply wait() for all child processes to terminate. The order in which the childs get to run is to a large extent unpredictable, on a multi-core system they may run literally at the same time.Now, before the subshells get to process their actual tasks, file redirections have to be performed. The left subshell gets its stdout redirected to the write end of the kernel pipe object that is \"created\" by the pipe symbol. Likewise, the right subshell gets stdin redirected to the read end of the pipe object.The first subshell contains two processes (red and green) that run in sequence (\";\"). \"Red\" is indeed printed to stdout and thus (because of the redirection) sent to the pipe. However, nothing is ever read out of the pipe: The only process that is connected to the read end of the pipe (\"echo blue\") never reads anything, it is output only.Unlike \"echo red\", \"echo green >&2\" doesn&#x27;t have stdout connected to the pipe. Its stdout is redirected to whatever stderr is connected to. Here is the explanation what \">&2\" (or equivalently, \"1>&2\") means: For the execution of \"echo green\", make stdout (1) point to the same object that stderr (2) points to. You can imagine it as being a simple assignment: fd[1] = fd[2].For \"echo blue\", stdout isn&#x27;t explicitly redirected, so it gets run with stdout set to whatever it inherited from its parent shell, which is (probably) your terminal.Seeing that both \"echo green\" and \"echo blue\" write directly to the same file (again, probably your terminal) we have a race -- who wins is basically a question of who gets scheduled to run first. For one reason or other, it seems that blue is more likely to win on your system. It might be due to the fact that the left subshell needs to finish the \"echo red\" first, which does print to the pipe, and that might introduce a delay &#x2F; a yield, or such. reply tuatoru 10 hours agorootparentThank you for taking the time to write this very detailed and lucid explanation. reply jcrites 6 hours agorootparentFor additional clarification, `echo` doesn’t read from stdin, so `…echo xyz` doesn’t do what you probably assume. Try running `echo aecho b` and you’ll see that only “b” is printed. That’s because `echo b` doesn’t read the “a” sent to it on stdin (and also doesn’t print it).If you want a program to read from stdin and write to stdout, you can use the `cat`, e.g. `echo acat` will print “a”.Lastly, be aware that `echo` is usually a shell builtin that functions like `print`. I’m not sure of all the ways that it might behave differently, but something to be aware of (that it’s not a child process like `cat`). reply dietrichepp 3 hours agorootparentThe way that shell builtins behave differently here is that SIGPIPE can take out the whole shell on the left side when echo is built-in.When you &#x2F;bin&#x2F;echo red, then it&#x27;s a subprocess, and its parent shell continues on, so you always get green somewhere in the output. reply rixed 5 hours agorootparentprevI don&#x27;t think your message (or others) does justice to the original blogpost.Yes the pipe runs two subcommands in parallel but that is not why the blogpost is interesting (or its author surprised). It&#x27;s because &#x27;echo red&#x27; is supposed to block, thus introducing synchronization between the two branches of the pipe, yet it doesn&#x27;t!And I must confess, when reading the command my first though was: \"Ok so that first echo will die with a SIGPIPE and stderr will be all about the broken pipe.\" And I was wrong, because of that small buffer.I wonder what other unices do allow a write to a broken pipe to complete successfully? reply jstimpfle 1 hour agorootparentYes, I noticed that only after finishing the work on my comment (which, strangely enough, is my most-upvoted comment ever). I had been under the impression that the command is a construction from a beginner trying to make sense of the shell, so I skipped over the blogpost too quickly.But indeed the author wasn&#x27;t aware that readers and witers of the pipe aren&#x27;t fully synchronized because the buffer in between allows for some concurrency. My writeup wasn&#x27;t very explicit about that (at least not that writing to the pipe can block when the pipe is full) but I think it&#x27;s technically accurate and hope it can clear up some confusion -- a lot of readers probably do not understand well how the shell works. reply dietrichepp 4 hours agorootparentprev> It&#x27;s because &#x27;echo red&#x27; is supposed to block,It is not actually supposed to block. Pipes block when they are full, but there&#x27;s not enough data here to fill a pipe buffer. When pipes are broken, SIGPIPE is sent to the writer. Pipes do not block just because nobody is reading from the read end--as long as the read end is still open somewhere, a process could read from it, and that is enough.When you see \"blue\", what happened is the left-hand side of the pipe got killed because the right-hand side already finished before \"echo red\", which closed the read end completely, and then \"echo red\" got killed with SIGPIPE. That takes out \"echo green\" with it, because \"echo\" is a built-in, and so \"echo\" is not a subprocess. If you use \"&#x2F;bin&#x2F;echo red\" instead, then \"green\" will always be printed (because SIGPIPE is going to &#x2F;bin&#x2F;echo, and not the entire shell).In other circumstances, the \"echo blue\" will never read stdin, but the kernel doesn&#x27;t know or care. As far as the kernel is concerned, \"echo blue\" could possibly read from stdin, as long as stdin is open. reply thequux 4 hours agorootparentprevThe pipe isn&#x27;t broken, though; at least not until the second echo terminates. The kernel doesn&#x27;t know that echo will never read stdin, because echo is generally a very simple program that doesn&#x27;t bother closing unused file descriptors. Instead, the pipe is broken when there&#x27;s nothing with an open receiving end, i.e., when the rightmost echo process terminates. Until then, it&#x27;s just like any other pipe reply paulddraper 8 hours agorootparentprevtl;dr Piped commands run in parallel not in serial.(The data \"runs\" in serial.) reply 4death4 12 hours agoparentprevThat may have been surprising, but, if you think about it a little deeper, it makes perfect sense. Programs in a pipeline execute concurrently. If they didn’t, pipelines wouldn’t be useful. For instance a pipeline that downloads a tar file with curl and then untars it. If you wait for curl to finish before running tar, you run in to all sorts of problems. For instance, where do you store the intermediate tar file if it’s really large? Tar needs to run while curl is running to keep buffers small and make execution fast. The only control flow between pipeline programs is done via stdin and stdout. In your example program, you write to stderr so naturally that’s not part of the deterministic control flow. reply eru 4 hours agorootparent> If they didn’t, pipelines wouldn’t be useful.Pipes would still be a useful way to structure your program. They would just be less useful. reply xorcist 12 hours agoparentprevIt that surprising? What would you have guessed output would look like, and why? Perhaps that information would help straighten out any confusion.The command, perhaps intentionally, looks unusual (any code reviewer would certainly be scratching their head):There&#x27;s an \"echo red\" in there but it&#x27;s never sent anywhere (perhaps a joke with \"red herring\"?).There&#x27;s an \"echo green\" sent to stderr, that will only be visible if it terminates before \"echo blue\".The exact order would be dependent on output buffering, which will depend on which time slice is sorted first, which will vary with number of cpus and their respective load. So yes, it will be indeterministic, but in the same way \"top\" is. reply oldbbsnickname 8 hours agoparentprevIf one enjoys fast, 0-copy I&#x2F;O on Linux, here&#x27;s an article.[0]PS: Precision of language to avoid confusion: \"Indeterministic\" is a philosophy term, while the CS term is \"nondeterministic\".0. https:&#x2F;&#x2F;blog.superpat.com&#x2F;zero-copy-in-linux-with-sendfile-a... reply arp242 14 hours agoparentprevAre there cases where his causes real-world problems? Because to be honest this example seems rather artificial. reply heavyset_go 9 hours agoparentprevI&#x27;m genuinely curious, how else could this work? It&#x27;s like spawning threads, it&#x27;s inherently indeterministic. reply Racing0461 12 hours agoparentprevChatgpt was able to figure this out with a simple \"what does the following do\". But it could also be a case of chatgpt being trained on your article.>>> Note: The ordering of \"green\" and \"blue\" in the output might vary because these streams (stdout and stderr) might be buffered differently by the shell or operating system. Most commonly, you will see the output as illustrated above. reply leodag 9 hours agorootparentThat&#x27;s wrong though, it&#x27;s got nothing to do with different buffering (which is usually done at the application level, by the way). reply nh2 14 hours agoprev(2022) Previous discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31592934 reply dang 13 hours agoparentThanks! Macroexpanded:How fast are Linux pipes anyway? - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31592934 - June 2022 (200 comments) reply epistasis 14 hours agoprevFantastic article, I learned a lot despite pipes being a bread and butter user tool for me for a quarter century. reply loondri 3 hours agoprevThis article talks about making Linux pipes faster, but other methods like shared memory or message queues might still be quicker. For example, in systems that need to move a lot of data quickly, the extra steps with pipes could slow things down. Also, when many threads are sharing data, pipes might cause more problems than other methods. So, the improvements in the article might not help much in real-world situations where speed is crucial. reply 0d0a 2 hours agoparentCan you give some examples? When batching data, you benefit from picking something like io_uring. But for two-way communication, you still need to notify either side when data is ready (maybe you don&#x27;t want to consume cpu just polling), and it isn&#x27;t clear to me how those options handle that synchronization faster than pipes. reply _trackno5 1 hour agorootparentThe main thing io_uring gives you is avoiding multiple syscalls.With a pipe you can’t really avoid that. With a shared memory queue&#x2F;ring buffer you can write to the memory without any syscalls.But you need to build synchronisation yourself (e.g., using semaphores for example). You don’t necessarily need to poll. reply 127 2 hours agoparentprevAlso the benefit of using a message queue library is that you don&#x27;t have to worry about multi-platform incompatibilities as much. reply bakul 1 hour agoprevWhy not simply use mmap judiciously in a program managed shared memory ring buffer? Then you can copy at roughly memory speed. reply rostayob 1 hour agoparentThis post is an excuse to explain VM concepts, rather than a tutorial, something I maybe could have made clearer. reply DiabloD3 13 hours agoprevTL;DR: Maximum pipe speed, assuming both programs are written as optimally as possible, is approximately the speed of what one core in your system can read&#x2F;write; this is because, essentially, the kernel maps the same physical memory page from one program&#x27;s stdout to the other&#x27;s stdin, thus making the operation a zerocopy (or a fast onecopy in slightly less optimal situations).I&#x27;ve known this one for awhile, and it makes writing shell scripts that glue two (or more) things together with pipes to do extremely high performance operations both rewarding and hilarious. Certainly one of the most useful tools in the toolbox. reply gpderetta 12 hours agoparentPipes are zero copy only if you use splice or vmsplice. These linux specific syscalls are hard to use (particularly wmsplice) and the vast majority of programs and shell filters (with the notable exception of pv) don&#x27;t use them and pay for the cost of copying in and out of kernel memory. reply dilyevsky 10 hours agorootparentIf you’re using Go it will automatically splice your reader&#x2F;writer when using io.Copy, etc reply tucnak 2 hours agorootparentre: https:&#x2F;&#x2F;go.dev&#x2F;src&#x2F;net&#x2F;splice_linux.govery interesting, I didn&#x27;t know `io` was doing that on linux! reply jstimpfle 12 hours agoparentprevAFAIK a severe limitation of pipes is that they can buffer only 64 KB &#x2F; 16 pages (on x86 Linux). Pretty sure it&#x27;s generally slower than core-to-memory bandwidth. reply packetlost 12 hours agorootparent64KB is the default, you can increase the buffer size using `fcntl`. You&#x27;re probably more limited by syscall overhead than anything reply packetlost 12 hours agoparentprevThis is why threads aren&#x27;t nearly as important as many programmers seem to think. Chances are, whatever application you&#x27;re building can be done in a cleaner way using pipes + processes or green&#x2F;user-space threads depending on the workload in question. It can be less convenient, but message passing is usually preferable to deadlock hell. reply jstimpfle 12 hours agorootparentPipes are FIFO data buffers implemented in the kernel. For communication between threads of the same process, you can replace any pipe object by a userspace queue implementation protected by e.g. mutex + condition variable. It is functionally equivalent and has potential to be faster. And if you wrap all accesses in lock&#x2F;unlock pairs (without locking any other objects in between) there is no danger of introducing any more deadlocks compared to using kernel pipes.Threads are an important structuring mechanism: You can assume that all your threads continue to run, or in the event of a crash, all your threads die.Also, unidirectional pipes aren&#x27;t exactly sufficient for inter-process &#x2F; inter-thread synchronisation. They are ok for simple batch processing, but that&#x27;s about it. reply gpderetta 11 hours agorootparentIncidentally you can use the exact same setup (plush mmap) for interprocess queues.The advantage of threads is that you can pass pointers to your data through the queue, while that&#x27;s harder to do between processes and you have to resort to copying data in the queue instead. reply rewmie 12 hours agorootparentprev> This is why threads aren&#x27;t nearly as important as many programmers seem to think. Chances are, whatever application you&#x27;re building can be done in a cleaner way using pipes + processes or green&#x2F;user-space threads depending on the workload in question.I think you&#x27;re making wild claims based on putting up your overgeneralized strawman (i.e., \"threads aren&#x27;t nearly as important as many programmers seem to think\") that afterwards you try to water down with weasel words (\"depending on the workload in question\").Threads are widely used because they bring most of the benefits of processes (concurrent control flow, and in multicore processors also performance) without the constraints and limitations they bring (exclusive memory space, slow creation, performance penalty caused by serialization in IPC, awkward API, etc).In multithreaded apps, to get threads to communicate between each other all you need to do to is point to the memory address of the object you instantiated. No serialization needed, no nothing. You simply cannot beat this in terms of \"clean way\" of doing things.> It can be less convenient, but (...)That&#x27;s quite the euphemism, and overlooks why threads are largely preferred. reply lelanthran 3 hours agorootparentprevThe problems with pipes is that passing a message involves a kernel context switch, no matter how small the message is.Passing a message in-process is orders of magnitude faster than passing a message out-of-process. reply djbusby 9 hours agorootparentprevLike how Postfix works. That&#x27;s a fun architecture to look at. Multiple processes and file based queue. Meanwhile I panic if I don&#x27;t have PostgreSQL to save my data :&#x2F; reply gpderetta 12 hours agorootparentprevMessage pass enough and you&#x27;ll easily deadlock as well. reply bee_rider 13 hours agoparentprevThis is magic system stuff I don’t understand, does it have to go all the way up to the memory or will the caches save us from that trip? reply DiabloD3 12 hours agorootparentDepends entirely on the CPU architecture.The most simple answer I can give is: yes, when its safe; when its not safe, that&#x27;s part of an entire category of meltdown&#x2F;spectre family exploits. reply NortySpock 13 hours agoparentprevI assume for heterogenous cores (power vs efficiency cores) it bottlenecks on the throughput of the slowest core? reply DiabloD3 12 hours agorootparentSurprisingly no. I&#x27;d expect similar performance.In these designs, the actual memory controller that talks to the RAM is part of an internal fabric, and the fabric link between the core and the memory controller is (technically) your upper limit.For both Intel and AMD, the size of the fabric link remains constant to the expected performance of the different cores, as the theoretical usage&#x2F;performance of the load&#x2F;store units remain otherwise constant in relation, no matter if it is a big core or a little core.Also, notice: the maximum performance of load-store units is your actual upper limit, period. Some CPUs historically never achieved their maximum theoretical performance because the units were never engaged optimally; sometimes this is because some ports on the load&#x2F;store units are only accessible from certain instructions (often due to being reserved only for SIMD; this is why memcpy impls often use SSE&#x2F;AVX, just to exploit this fact).That said, load-store performance usually approaches that core&#x27;s L2 theoretical maximum, which is greater than what any core generally can get out of its fabric link. Ergo, fabric link is often governing what you&#x27;re seeing in situations like this.On Intel and AMD&#x27;s clusters, the memory controller serving their respective core cluster designs requires anywhere from 2 to 4 cores saturating their links to reach peak performance. Also, sibling threads on the same core will compete for access to that link, so it isn&#x27;t merely threads that get you there, but actual core saturation.On a dummy benchmark like proposed in the linked article, the performance of a single process being piped to another process, either in the situation of \"both processes are actually on the same big core, simultaneously hyper-threading\", or \"two sibling little cores in the same core cluster, being serviced by the same memory controller\", the upper limit of performance should approximate optimal usage of memory bandwidth, but in some cases on some architectures this will actually approximate L3 bandwidth (a higher value).Also, as a side note: little cores aren&#x27;t little. For a little bit more silicon usage, and a little bit less power usage, two little cores approximate one big core &#x2F;w two threads optimally executing, even in Intel&#x27;s surprisingly optimal small core design, but very much true in Zen4c. As in, I could buy a \"whoops, all little cores\" CPU of sufficient size for my desktop, and still be happy (or, possibly, even happier). reply nathants 7 hours agoprevpipes are great. is the other process on another cpu or another machine? honestly who cares.https:&#x2F;&#x2F;github.com&#x2F;nathants&#x2F;s4&#x2F;blob&#x2F;master&#x2F;examples&#x2F;nyc_taxi... reply chris_armstrong 11 hours agoprevAbsolutely amazing, I know about page tables and the like but tying it to performance analysis with `perf` makes it clear how central it is to throughput reply Borg3 12 hours agoprevHah, nice article :) I remember fighting with Cygwin pipe implementations to have decent performance from them. They are hella slower compared to Linux, but still usable, just tricky to pass data in&#x2F;out. reply bloopernova 14 hours agoprevPipes are fast enough for iterating and composing cat sed awk cut grep uniq jq etc etc. reply mannyv 13 hours agoprevHow fast are they compared to raw memory throughput?It&#x27;s interesting that memory mapping is so expensive. I&#x27;ve often wondered the price that everyone pays for multiple address spades. Is isolation really worth it? reply formerly_proven 13 hours agoparentThe relative performance cost of virtual memory was way higher in days past, but people considered it worth it for the increased system reliability. reply whalesalad 12 hours agoprevLove the Edward Tuftian aesthetic of this site. Although above a certain viewport width I would imagine you want a `margin: 0 auto` to center the content block. On a 27\" display it is tough to read without resizing the window. reply ldoughty 10 hours agoparentI have to agree... I really like the side-notes to get more details&#x2F;explanation. You can skip the side-notes to keep reading and stay on the main story, but get what normally would be included in parenthesis or otherwise as an in-line comment.... Best of both worlds here I think. If I actively maintained a blog, I&#x27;d probably steal this design! :-) reply emmelaich 9 hours agorootparentIs there some standard css&#x2F;html way of pushing side notes or pics into the first column if viewing width is too small?That would be the best of both worlds! reply whalesalad 8 hours agorootparentresponsive design concepts would enable this reply codercowmoo 7 hours agoprevAnyone see the stonks image hidden quite well behind the first table?I could only see it because of my dark mode extension, otherwise I guarantee I wouldn&#x27;t have caught it. reply sbjs 14 hours agoprev [–] I remember using linux pipes for a shell-based irc client like 12 years ago. For most application uses, they&#x27;re plenty fast enough. Kinda wish I had the source code for that still. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article elaborates on the functioning of Unix pipes in Linux with an emphasis on optimizing a test program for data writing and reading through a pipe.",
      "It delves into the usage of techniques such as vmsplice and splice to enhance throughput by lessening data copying, and the employment of the perf tool for performance analysis.",
      "It also discusses the role of paging, virtual memory in data transfer, translation of virtual to physical addresses, and the application of huge pages to lower TLB (Translation Lookaside Buffer) misses."
    ],
    "commentSummary": [
      "The article details the implementation and performance of Linux pipes, emphasizing the potential benefits of vmsplice, a shared memory mechanism, despite the challenges in optimizing its versions.",
      "It delves into libraries, APIs for data handling and optimizations, the unpredictable behavior of Linux pipes, and discusses the repercussions of broken pipes.",
      "Alternative data transfer methods like shared memory or message queues, and optimization techniques such as io_uring and mmap are explored. Also discussed are the application of pipes in shell scripting for high-performance tasks, as well as the trade-offs and performance outcomes when choosing between threads and pipes."
    ],
    "points": 391,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1696531769
  },
  {
    "id": 37782513,
    "title": "An app store just for installable web apps",
    "originLink": "https://store.app",
    "originBody": "List Your App Login Sign Up For You Browse Profile Settings Lists Favorites Have an app? Get Listed Privacy · Terms · Contact · © Store.app by PWA Labs, Inc. Featured All Apps AI Crypto Games Productivity Shopping Social Show only installable Featured Wordi Games Singleplayer Get Starbucks Food & Drink Get Stemstr Crypto Bitcoin Get Tower Game Games Singleplayer Get Squaredle Games Singleplayer Get PromptPerfect AI Get Pinterest Social Get Linear Productivity Get Sign up & reserve your username today! Sign Up Trending Squaredle Games Singleplayer Get Replit Dev Tools Get friend.tech Crypto Ethereum Get Canva Graphics & Design Get Narrow One Games Multiplayer Get Writesonic AI Get TibetSwap V2 Crypto Chia Get Kickresume AI Get dexie Crypto Chia Get Top AI Apps PromptPerfect AI Get ElevenLabs AI Get Codeium AI Get Chatfuel AI Get Upscale.media AI Get OpenDream AI Get Voicemaker AI Get Wonder Dynamics AI Get Replika AI Get Games Wordi Games Singleplayer Get Tower Game Games Singleplayer Get Squaredle Games Singleplayer Get PROXX Games Singleplayer Get Sudoku Games Strategy Get Colonist Games Boardgame Get Vapor Boy Games Singleplayer Get Narrow One Games Multiplayer Get",
    "commentLink": "https://news.ycombinator.com/item?id=37782513",
    "commentBody": "An app store just for installable web appsHacker NewspastloginAn app store just for installable web apps (store.app) 381 points by presson 15 hours ago| hidepastfavorite179 comments baudaux 0 minutes agoGreat idea ! I am doing the same with https:&#x2F;&#x2F;exaequos.com , a Unix-like operating system reply perilunar 39 minutes agoprevThe terms of service prohibits deep linking. Here&#x27;s me breaking it: https:&#x2F;&#x2F;store.app&#x2F;termsSeriously though, WTF? So much for \"the web always wins\". reply notpushkin 9 minutes agoparentLooks like generic boilerplate ToS to me. It&#x27;s still bad, but I think this is unintentional. reply Philip-J-Fry 6 minutes agorootparentHe&#x27;s not talking about the contents of the ToS. Click the back button on the page, it takes you back here. You&#x27;d expect it to take you back to the homepage of the site. If you deeplink to that page then there&#x27;s no way out other than editing the URL. reply _Algernon_ 18 minutes agoparentprevSeems very unenforceable. The fact that it is included speaks volumes about the creator of the site though. Wouldn&#x27;t want to use such a service. reply Brajeshwar 5 hours agoprevIs it just me that I like websites with URLs on the browser while trying to avoid Apps wherever possible, especially on the desktop? Please don’t take me the wrong way; I was the one developing apps for Pocket PC devices in the early 2000s.For instance, “Twitter” is the first featured “app” I see here at Store.App. However, when I use Twitter, I tend to click away on links and read&#x2F;browse them. How are people using websites these days?I may be missing something, and I’m eager to be enlightened.P.S. Best wishes to Store.App and hope it succeeds for those who need it. reply rrrrrrrrrrrryan 4 hours agoparentYeah Twitter is an awful use case. Any app that has lots of external links or that you want to spawn new tabs is better just staying in the browser.The ones that work better as installed apps are things that you kinda just leave open in their own window: instant messaging apps, email, music streaming, etc.There&#x27;s no reason for these to have a URL bar at the top, and \"installing\" the things often unlocks global hotkeys and other bits. reply crossroadsguy 4 hours agoparentprevYou definitely are not alone. Even on mobile, not only desktop, if I can do away without an app I&#x27;d really do that. We have a movie booking app in our country BookMyShow (the less said about this company and practices is better) and I just book it on its mobile website mostly (though I try to book in the theatre and almost always I have to make a scene to get a print ticket when I refuse to share my number fearing certain spamming).Twitter, Reddit - as you might know there are no non-official apps and their mobile websites, which are bad, are much better than the apps. reply felixg3 3 hours agorootparentA lot of services are unfortunately forcing you to use the app instead of the web. And a lot of the apps in your country are available in the AppStore India only which is extremely annoying for me as a German in India who has to switch between local and foreign Apple IDs frequently. reply faeriechangling 4 hours agoparentprevDepends on how you organise things. I generally DON&#x27;T close most tabs I open individually. I tend to open more and more tabs until I&#x27;m forced to close them all or most of them to keep things organised and then I start again. With habits such as mine, opening a webapp as an application rather than a tab will make it persist longer and be easier to find.Additionally, while 99.9% of users won&#x27;t take advantage of this, your operating system has all sorts of features to manipulate the window of individual GUI applications which it can&#x27;t use in the context of tabs. Unattended automation of the GUI has all sorts of interesting applications.You can also get rid of some of the wasted space at the top of the screen. reply hnlmorg 2 hours agoparentprevGiven how toxic many mobile apps are and how a pretty significant amount of time they’re still just a web app under the hood, I think native apps have already lost the war.So like yourself, I very rarely install apps these days. reply crimsontech 1 minute agorootparentThis is the opposite for me. I used to use Twitter and Reddit constantly through 3rd party iOS apps. Both platforms banned 3rd party apps and I simply don’t use the platforms anymore.They added enough friction through ads, cookie banners, forced login to view content and more ads that I don’t care to use them anymore.I’m using a 3rd party app to view HN and I probably visit 3-4 times a day. reply sgt 1 hour agorootparentprevMost quality apps are native apps, they are not just web app under the hood. Then you are either using uncommon or niche apps, or you perhaps you are on Android (Play Store is a nightmare these days).The App Store (iOS) may not be perfect but there are truly quality apps and the developers have a real incentive to maintain these, seeing that the willingness to pay for apps is high in the Apple ecosystem. reply Woeps 7 minutes agorootparentBut outside of messaging, music and browsing the web. how many native apps do you&#x2F;people actually use?Because I&#x27;m personally under the impression that this is what most people use their phone for (maybe play a game or two as well). tough this might also depend on the countries where I lived (and when).And some people also take pictures of course reply coder543 13 hours agoprevI like the concept a lot, but I also think it should filter to only installable apps by default. If you detect a user is on a mobile device, it would probably also make sense to enable the mobile filter by default too.It would also be nice if there was an offline-capable filter as well, but maybe I missed it?I also notice the Developer tab is not part of the PWA, which kicks you out of the PWA experience on iOS, even though it seems like that marketing page could be contained within the tab? It might also be nice to link to resources for getting started on developing a high quality PWA, if any resources like that exist.Now that PWAs on iOS can finally show notifications (if the user wants), I hope more developers will take them seriously. I trust browser isolation more than I trust native app isolation, and a lot of the native apps that I use would work perfectly as a PWA. reply matsz 13 hours agoparent> PWAs on iOS can finally show notificationsThis has been such a great addition, once it was officially added I&#x27;ve quickly built a small web app that can now distribute notifications to all my devices effortlessly, with a single codebase and with no need to pay the dreaded $99&#x2F;yr subscription. Haven&#x27;t released that publicly but I might at some point once I clean up the mess that inevitably resulted from me writing it in around 2 hours. reply danielskogly 4 hours agorootparentNot sure if it&#x27;s entirely similar, but there&#x27;s also this for those who don&#x27;t want to build it themselves: https:&#x2F;&#x2F;ntfy.sh&#x2F; reply kyleee 12 hours agorootparentprevDid you use a framework or anything? I’ve been working on a small vite&#x2F;vue&#x2F;nuxt PWA and I’ve been wondering about other options reply matsz 11 hours agorootparentJust React and fastify. I have my own authentication library that makes things easier but it&#x27;s not production ready yet. reply presson 11 hours agoparentprevThank you, this is great feedback. What to show and when has been an ongoing convo - we chose to evolve this based on supply side milestones. Keep an eye out for some cool updates :-). We don&#x27;t yet have the ability to filter by offline-capable, but that&#x27;s a great idea. Re the dev tab, this is a work in progress - right now a few things are split apart, but we&#x27;ll be folding everything back together at some point.Re notifications, 100% agree - this was a major roadblock. The next few years are going to be very interesting :-P reply reustle 8 hours agorootparentBut isn’t “installable” the entire point of this site, and your company?If you want you help the discussion against native apps, you need to clearly take the side of proper installable PWAs, not just links to websites. It feels misleading and gives argument ammo to the “native always” crowd. reply kybernetikos 12 minutes agoprevIt&#x27;d be nice to have filters for:* Can be used without creating an account* Requires a free account* Has a paid account option* Requires a paid account reply fartasanelk 8 hours agoprevI am on desktop. There is a button that says &#x27;Get&#x27;. When I click on it, I get prompted to &#x27;Install the app on your device&#x27;. Which just sends me to the home page?I just check on mobile, same thing. How is visiting a website&#x27;s homepage &#x27;installing the app on my device?&#x27;I think some people might be turned off by the word Installing, when you&#x27;re just visiting a website?The word Get seems misleading, because you are not &#x27;getting&#x27; anything. If anything Visit might be more appropriate.What am I missing? reply presson 8 hours agoparentThere&#x27;s better functionality coming, but for at the moment, users need to jump over to the apps domain to install. \"Getting\" or \"Installing\" means adding to home screen. The flow isn&#x27;t ideal at the moment, and some web apps aren&#x27;t installable (e.g. don&#x27;t have a manifest&#x2F;service worker), but it&#x27;s early in the movement. You&#x27;re not really missing anything, other than the fact that we have to start somewhere :-P reply rahimnathwani 7 hours agoparentprevIf you visit a PWA on Chrome for Android, you can install the app by tapping the three dots and scrolling down to &#x27;Install app&#x27;.OP has a filter for only installable apps, but it&#x27;s off by default. So you might be taken to an app that&#x27;s not installable. reply presson 3 hours agorootparentWe&#x27;ve been getting feedback to keep the \"installable\" filter on by default, which I think is a good idea. We&#x27;ll implement tomorrow. Eventually this&#x27;ll evolve quite a bit, but we&#x27;re taking a measure approach. Stay tuned! :-) reply auggierose 33 minutes agorootparentIt&#x27;s the same for me, but I have already filtered for installable. I guess this doesn&#x27;t work on desktop, Safari 16.1? reply danpalmer 10 minutes agoprevThere are apps on here that are native-only installs. The data quality seems low, but the whole product is the data quality. reply j1elo 9 hours agoprevNothing that can be blamed to the (web)app store, but I found it curious the overall very poor first impression experience that these small niche (or not so small) websites tend to offer... some have the feeling of just being an excuse prototype to be a quick grab for money &#x2F; user accounts. I guess that not too unlike actual installable mobile apps, to be fair.My experience has been:I tried the Linear app, but couldn&#x27;t see anything more than a pretty landing page before an account was required. Next.Replit. Wanted to get a feeling of how it works, but again, just a landing page and a button that says \"Start creating\" but instead it should say \"Log in\". Next.OpenDream: \"Create original art in seconds with AI\". Cool -I say to myself-, let&#x27;s create a couple of crazy pictures just for fun. Oh, well, these at least don&#x27;t pretend to be fancy, the login prompt appears first thing right in your face. Next.Replika. An AI companion, so curious. But the website&#x27;s only purpose is to show a \"Get the app\" that brings me to Play Store. Wouldn&#x27;t call it a webapp. So next, I guess?Let&#x27;s have some fun with some silly phrases in VoiceMaker! But after trying it out with \"Casper, Male\", I just wanted a second try with \"Patricia, Female\", and it already prompted me and didn&#x27;t let me play anything any more. Bummer. Next.Funnily enough, the best initial experience of these webapps, by a wide margin, has been Tower Game. It just does what it promises. It&#x27;s a tower game. It runs straight out, and lets you play. No fuss. 10&#x2F;10, best webapp of the ones I tested!(PS.: PromptPerfect, \"Your IDE for professional prompt engineering\". Wow, it&#x27;s crazy that we got to need such things, the world of LLMs is running too fast for me) reply presson 9 hours agoparentThis is great feedback. There are some modifications coming that will enhance the discovery process. It&#x27;s very early in our journey, so I&#x27;d encourage you (I&#x27;m biased, of course) to create an account and stay tuned for updates. We believe the web is in the early stages of a renaissance of some sort. reply kunley 54 minutes agorootparentIt&#x27;s quite hilarious that the commenter didn&#x27;t want to create accounts in all the apps he described, still the crux of your proposal is .. creating another account. reply asim 34 minutes agoprevVery cool, working on something similar here - https:&#x2F;&#x2F;mu.app - more of a super app. It&#x27;s great to see such a positive response to an app store for web apps. There&#x27;s definitely a need for evolution away from the current app store model, which is heavily taxed with 30% fees plus $100&#x2F;year dev fees. I think we&#x27;ll see more things like this appear in the near future. reply franciscop 4 hours agoprevAh this is perfect! Once few years ago I had https:&#x2F;&#x2F;web-app.store&#x2F; (I no longer own that) and attempted to do exactly the same thing as you did, @presson. Funny how, besides your high-quality TLDN, it&#x27;s basically the same name&#x2F;TLD but reversed. For posterity sake, here is what it looked like:https:&#x2F;&#x2F;i.imgur.com&#x2F;hQospzk.png reply merelysounds 2 hours agoparentFor comparison, I chose a very different route. On my personal website I maintain a small directory of web games that have good UX on both desktop and mobile. Link: https:&#x2F;&#x2F;merely.xyz&#x2F;gamesIt&#x27;s extremely low tech; just a static website, no reviews, no concept of apps. The scope is much smaller — and it won&#x27;t grow beyond what I can test. But I guess the goal is similar, promoting user friendly web content. reply presson 3 hours agoparentprevThat&#x27;s amazing! We definitely stand on the shoulders of giants. Hoping that now is finally the right time for the web to make the jump. LMK if you ever want to connect and talk shop! reply tacoship 13 hours agoprevSweet domain! Was it expensive?Feedback: The home page&#x27;s horizontal carousels&#x2F;reels lack trackpad scrolling functionality. Interaction is limited to the arrow buttons on the side. To enhance user experience, it would be beneficial to make them scrollable via trackpad, similar to the Apple Store website.[0][0] https:&#x2F;&#x2F;www.apple.com&#x2F;store reply presson 13 hours agoparentThank you! It was pricey, but not as bad as it could&#x27;ve been. I pre-ordered it a month before .app became available as a TLD, and bought it on the first day of availability (May 2018!). I&#x27;ve been planning on building this for some time :-PRe carousel feedback - thank you! We&#x27;ve talked about this but have been prioritizing some other features ahead of it. That said, your feedback helps us prioritize higher. Keep the feedback coming, the more the better! Also if you want to connect offline, lmk. reply qntmfred 5 hours agorootparenthttps:&#x2F;&#x2F;shouldiuseacarousel.com&#x2F; reply presson 3 hours agorootparent:-P reply dylan604 12 hours agorootparentprevJust to 2nd it. I came to the comments first, but after visiting the site, it does feel unnatural. Also to pile on, having to multi-click the next button one at a time per app feels tedious.Seeing how you are at this stage though, color me very impressed. I&#x27;ve just never paid attention to PWAs, so seeing them available like this gave me a totally different (positive) consideration for them. I would love to see numbers of use just to see how widely used PWAs are. This isn&#x27;t an idea for the UI for popularity per app, but just for PWA in general. Just in terms of if this is something I should be considering. Great job and good luck! reply presson 12 hours agorootparentNoted, and thank you! reply perilunar 3 hours agoparentprevAgree on the carousel. The buttons are small and clunky — if you miss them you get a page you don&#x27;t want. Just use a div with overflow:auto for native browser scroll. reply bgoldste 13 hours agoparentprevAsking the important questions. reply dylan604 13 hours agorootparentI&#x27;m going through this very thing now. I&#x27;m focused on getting the backend features working, but getting pressure to make the tiny tweaks to the UI. Once you open the UI to actual users, priorities quickly get rearranged. The UI complaints are the punch in the \"everyone has a game plan until they get punched in the face\" phrase. reply presson 3 hours agorootparentAll part of the process, though! Brick by brick... reply collaborative 13 hours agoprevI really hope your store takes off. I will say a prayer or two to this end. May your good character flourish and may you avoid all temptations that come with gatekeepingApple and Google&#x27;s app stores are death. They motivated me to re-code my c and java-based iOS and Android apps in js+wasm (SPA&#x2F;PWA) from scratch. For a long time, I announced it with its proper name (\"PWA\"). I became very frustrated every time I interacted with users who asked \"what does that mean?\". So now it&#x27;s simply \"Web Version\" reply presson 12 hours agoparentThank you! We have a very strong ethos around democratizing distribution and we plan to be careful not to go down a path to becoming what we hate. We&#x27;re here to serve developers and users, and we&#x27;ll look for guidance from our user base constantly. Re \"PWA\" I can empathize - I&#x27;ve found that for the majority of people, \"installable web app\" seems to register more effectively, but hoping that \"PWA\" becomes more broadly understood. Thanks again for the comment, and we look forward to serving you :-) reply matsz 13 hours agoprevReally good idea, what I&#x27;d suggest would be a separate Open Source category&#x2F;tag. And a dark theme (based on prefers-color-scheme).Also, submitted a developer application; have released a few PWAs myself - hope to post them there too! reply presson 12 hours agoparentAwesome, thanks for the feedback! That&#x27;s a great idea - I&#x27;ve posted it in our internal channel. Also, you should have been approved for a dev account - we&#x27;re excited to have you onboard! Please feel free to let us know how we can help - our mission is to help web devs grow their business. reply matsz 11 hours agorootparentThanks, works well - just one small bit of feedback there: wish there were some hints on aspect ratios for the screenshots (or they should be displayed at the image&#x27;s aspect ratio). Currently the sides get cut off: https:&#x2F;&#x2F;store.app&#x2F;drop-lolAlso, would be nice to have a \"more by this developer\" section. (Or any other \"similar to\" recommendation section.) reply sgt 1 hour agoprevNice as an option but as a mobile I user my first choice is always going to be a proper native app. Faster, better integrated and not just some web page pretending to be an app. PWA&#x27;s are very useful however when there is a lack of an app, and certainly there are use cases. reply thepra 1 hour agoparentIndeed, the good point is not relying on any store to bring able to \"install\" or use right away, so Android and iPhone users aren&#x27;t blocked from sharing the same experience. reply esprehn 10 hours agoprevThis is great, hopefully it takes off!One piece of feedback is that it doesn&#x27;t restore the scroll position when going from list page -> detail and then pressing back. That breaks browsing scenarios where you&#x27;re looking at the details of apps as you scroll down. In general PWAs often get this wrong. I wish more of them were testing back navigation. reply presson 10 hours agoparentThank you for the kind words! And your feedback has been noted with our team, keep it coming! :-) reply pentagrama 10 hours agoprevI like the idea and execution. Is good spread the PWA feature of the web to battle the duopoly of native app stores, even if PWA has their disadvantages.Feedback: I&#x27;m browsing the site on Firefox desktop, sadly it doesn&#x27;t support install PWA yet. You may want to add some banner for Firefox users alerting about that. reply presson 9 hours agoparentThat&#x27;s great feedback. It is indeed unfortunate that Firefox has shied away from supporting modern browser features. We will be showing relevant feedback based on your browser when you launch an app - is that sufficient for you? Seems like that&#x27;s the right CTA to advice a user to switch browsers, but curious if you think it&#x27;s imperative to have this info at the beginning of your discovery journey. reply pentagrama 8 hours agorootparentThat seems ok! Not sure about encourage the user to switch, but maybe mention that in x browsers works.Also have in mind that on Firefox mobile app, PWAs _have_ install capabilities, so the alert should _only_ be targeted to Firefox desktop. reply parasti 3 hours agoparentprevFWIW, Firefox had PWA support on desktop and removed it. reply gardenhedge 1 hour agoprevMy phone shows installed pwas as an icon but it&#x27;s overlaid with a Firefox icon. That&#x27;s really annoying reply willsmith72 12 hours agoprevThis is awesome, as PWAs get better I really REALLY hope they&#x27;ll challenge the big app stores. So far no one I know outside of tech knows or cares that this is even possible, but I hope sites like this will gradually change that and also change people&#x27;s perceptions of what has to be a \"native\" app.I don&#x27;t see any reason why in 10 years we should still be paying the 30% tax (+ the $100&#x2F;year) just to get something with notifications and offline on the home screen. Sadly there&#x27;s literally hundreds of billions on the table, so there&#x27;s going to continue to be huge resistance. reply hunter2_ 12 hours agoparentWhen I install something from those stores, I feel like there is some level of vetting against malware, or at least the ability for the store to pull it if it becomes known-malicious after I installed it. 30% might be too much of a fee, but some payment for that service makes sense, I think.Are PWAs somehow immune to this concern by virtue of running on the way stricter feature set that a browser offers? If they can prompt the user for access to all of these APIs [0], and the user allows it, and then it later becomes malicious while able to run a service worker in the background, that seems a bit more concerning than regular non-PWA browsing that can&#x27;t continue to run after the tab is closed.[0] https:&#x2F;&#x2F;permission.site reply coder543 12 hours agorootparentService workers aren’t a magic “run forever in the background” technology.I’m fairly sure PWAs can’t run in the background at all on iOS or Android, except for possibly a brief moment after the PWA receives a notification, and the service worker doesn’t have access to much of anything during that interval. If anyone can link to something that demonstrates otherwise, okay, but I have checked on this in the past, and I don’t think so.Native apps have been caught with their hand in the metaphorical cookie jar over and over again. The App Store review process is largely ineffective. Apple apparently didn’t even realize that apps were doing sketchy things with the clipboard until it became a major headline[0], and this is only the tip of the iceberg of “things App Store review didn’t catch”.Native apps continue to find ways to bypass sandboxing that the OS applies.Browsers naturally take a much more adversarial posture against the code they’re running, so the sandboxing is far stronger.Apple has spent at least a decade marketing[1] to convince people the App Store is synonymous with safety. Given how detached this seems to be from reality, the simplest conclusion seems to be that they do this because they really want their 30% cut. The marketing campaign seems to be working too well. In reality, the browser seems to be substantially safer, although nothing is perfect.[0]: https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2020&#x2F;06&#x2F;tiktok-and-53-other-...[1]:https:&#x2F;&#x2F;apple.com&#x2F;app-store&#x2F;“The apps you love. From a place you can trust. For over a decade, the App Store has proved to be a safe and trusted place to discover and download apps.” reply brucethemoose2 11 hours agorootparentprev> When I install something from those stores, I feel like there is some level of vetting against malwareI don&#x27;t.Google Play has served me malware on my device (touchpal) that went undiscovered forever. And tons of high search ranking apps are straight up scams or data harvesting fronts even if they aren&#x27;t legally \"malware.\" I feel more secure about web apps because at least I am protected by the browser&#x27;s sandboxing&#x2F;fingerprinting protection (and in my case Cromite&#x27;s extra blocking).IDK about iOS, but Google Play is a dumpster fire. I hope it burns to the ground. reply willsmith72 12 hours agorootparentprevI totally agree, the security issue is the biggest thing app stores have going for them. I think 2 things.1. if momentum goes PWA&#x27;s way, smart people will work on these problems. I&#x27;m not saying I have the answers, but if I can root for either 2 megacorps or the open web, I&#x27;m going open web every time. (again, not saying we&#x27;re there yet with PWAs)2. If an app store&#x27;s only function is vetting software, we can have an open market for marketplaces. Anyone can build their own app store and, with enough reputation, become trusted in the eyes of consumer&#x27;s, just like we trust Apple The difference is in a free market, that 30% and $100 drop dramatically, I would wager to something more like 5% and $0.At those rates, having an app store where people feel the software is secure, can trust reviews&#x2F;ratings etc seems reasonable to me. PWA doesn&#x27;t have to mean otherwise reply DANmode 10 hours agorootparent> smart people will work on these problemsThey have been, it&#x27;s just not your niche of interest! You don&#x27;t know what you don&#x27;t know, etc. reply presson 3 hours agoparentprevIt&#x27;s up to all of us to build the future, brick by brick. What looks impossible today might look inevitable in hindsight. Ok sorry, enough cliches from me for the night :-P reply yieldcrv 12 hours agoparentprevnobody outside of tech caresthey already use web apps with optional notificationsanybody developing mobile apps knows that almost nobody downloads the company’s app, and it’s just there for vanity and clout reply willsmith72 12 hours agorootparent> nobody outside of tech caresYeah i know, hence \"So far no one I know outside of tech knows or cares that this is even possible, but I hope sites like this will gradually change that and also change people&#x27;s perceptions of what has to be a \"native\" app.\"> anybody developing mobile apps knows that almost nobody downloads the company’s app, and it’s just there for vanity and cloutWhat do you mean here? Did you mean to say PWAs? reply yieldcrv 11 hours agorootparentnot PWAs, I’m saying that ios&#x2F;android apps dont have much traction for most companiesmany of the reasons are that discovery is bad , and many people have run out of space on their phones, and generally just aren’t interested in another appbut PWAs solve this by at least letting people experience your service reply tamimio 12 hours agoprevI love it, maybe finally developers will no longer be under the mercy of apple&#x2F;google store! Probably the only thing PWA fails to do now that anything needs access to the hardware say wifi&#x2F;Bluetooth scanner and such, and big games too, the rest of the apps, I don’t see why it needs to be installed from app&#x2F;play store. reply presson 12 hours agoparentThank you! That&#x27;s our hope. The open web is making some major strides, so the next few years will certainly be interesting :-) reply zapt02 11 hours agoparentprevFinally developers can be under the mercy of some random dude who owns the store.app domain and charges for instead, joy! reply tamimio 10 hours agorootparentI see your point, but are they under this site mercy? This site is like a directory rather than a policy maker like app stores, it wont take % of any subscription for starter plus all the pros of lower development costs and what not. Say this site suddenly decided to remove your app? So what, your users can still go to your site and have the PWA, now if Apple decided to do the same to your app, good luck getting anyone to use it. Now is PWA the ultimate replacement? Certainly not, access to hardware, performance wise, games, you will still need them, but other than that, I think it’s a step forward. reply presson 10 hours agorootparentThis ^^^ reply perilunar 3 hours agorootparentprevNot really, since there can be many indexes like this.There&#x27;s already https:&#x2F;&#x2F;appsco.pe for example. reply JanSt 12 hours agoparentprevChrome offers bluetooth support, doesn’t it? reply hunter2_ 12 hours agorootparentIt&#x27;s in this list [0], indeed.[0] https:&#x2F;&#x2F;permission.site reply smusamashah 11 hours agoprevI am on phone so it may be different for desktop but the button to show only installable apps can be improved to make it clear when it is pressed and when it&#x27;s not.I don&#x27;t just want installable apps, I want installable apps to be offline or at least offline first too with optional login. I won&#x27;t even consider it installable if it needs Internet to be of any use. Please add a way to filter these too.Great domain and it looks it can become a defacto web store for apps. reply presson 10 hours agoparentGreat feedback - thank you! :-) reply jsf01 13 hours agoprevThis is such a great idea. Excellent design, too. I second the requests for dark mode or at least following the user’s prefers-color-scheme setting.Do you have ambitions to monetize this, or go the open source route? Where do you go from here? reply rexreed 11 hours agoprevAny advice for PWA developers who are trying to make use of possibly attached hardware? I&#x27;m looking to build a POS app that needs to connect to a receipt printer and scanner, but having trouble finding support for the printer bit. Trying to avoid any pop-up dialogue that requires user confirmation. reply ies7 6 hours agoparentWe use bluetooth thermal printer for a few of our out of internet travelling salesman.Main problem is almost every other bluetooth printer type&#x2F;brand need a little print layout adjustment.If internet connection isn&#x27;t an issue, you can try papercut.com or google cloud print or setup you own IPP and connect to normal printer. There are also savapage.org but papercut already work for us so haven&#x27;t try that one. reply willhackett 11 hours agoparentprevYou may need to look at some sort of hybrid configuration. A daemon to communicate with hardware, but the user experience built within the PWA. reply thekingshorses 10 hours agoparentprev.pdfp extension adobe acrobat configured to auto print pdfp extensionI don&#x27;t think there is any free&#x2F;open option to scan from scanner.The other option is to use puppeteer to print directly.Let me know, if you figure it out. reply KRAKRISMOTT 11 hours agoparentprevIf you don&#x27;t need iOS, WebUSB should do the trick. reply jwells89 10 hours agorootparentKeep in mind that WebUSB is exclusive to Chromium-based browsers — it won’t work on Firefox or Safari, and given how Google disregarded Mozilla’s and Apple’s feedback on the spec that’s unlikely to change.https:&#x2F;&#x2F;caniuse.com&#x2F;?search=WebUSB reply lib-dev 9 hours agorootparentWhat was apple&#x2F;mozilla’s feedback? reply jwells89 9 hours agorootparentRoughly, that they had privacy and security concerns with the way Google wanted to implement it, if I recall correctly. reply rexreed 8 hours agorootparentprevCool - will check it out - want to make sure it will allow printing without any user prompting. reply presson 11 hours agoparentprevJoin our discord and there are likely some people who can help https:&#x2F;&#x2F;discord.gg&#x2F;tFquNjuK reply thepra 1 hour agoprevIt&#x27;s quite quick! I listed my web app and got it verified in Leeds than an hour :) reply OJFord 11 hours agoprevWow what did you pay for store.app? That was either clever & lucky or very expensive, surely? reply presson 10 hours agoparentI pre ordered it before .app was available as a TLD and bought it on day one of availability. I&#x27;ve wanted to build this for a looooooong time :-) reply m3kw9 10 hours agoprevNice idea but probably way ahead of its time, native app still rules if you want top experience for the foreseeable future reply presson 10 hours agoparentNeed to start somewhere :-P and I think you&#x27;ll be surprised looking back in a few years from now. reply m3kw9 7 hours agorootparentYeah I will be surprised, but stick with it when I first saw it I thought this is a good idea. Plus the domain name is just perfect reply bigiain 10 hours agoparentprev&#x2F;me looks sideways at the Slack, Signal, Teams, and VSCode \"native app\" windows I have open here.It&#x27;d be nice to not have 5 complete instances of Chrome using up all my compute. reply ledbettj 12 hours agoprevPretty cool! I tried to list my app but the screenshot upload doesn&#x27;t seem to work -- it won&#x27;t accept any files I&#x27;ve tried via drag&#x2F;drop in Chrome or Firefox. I&#x27;ve been able to upload files to other sites without any issue (running Linux). reply presson 12 hours agoparentThank you for the feedback, we&#x27;re looking into this. In the meantime, you should be able to upload from your files if the images have the correct aspect ratio. If you continue to have issues, ping me at support@store.app and we&#x27;ll make sure everything gets sorted out. reply ledbettj 11 hours agorootparentActually, it&#x27;s my own fault -- I assumed the area in the preview that said I hadn&#x27;t uploaded any images yet was the drop target, but it&#x27;s not, that&#x27;s over in the sidebar. Cheers! reply presson 11 hours agorootparentOhhh gotcha - that&#x27;s still good feedback, though - we will try to make that more intuitive! reply LauraMedia 3 hours agoprevI really enjoy the platform and while I understand that you want a more curated experience for now, writing \"List your app in under 2 minutes\" on the dev page only to be met by an application form was a bit of a bummer. reply pietervdvn 11 hours agoprevHey @presson,I signed up as dev to get my webapp listed. I stumbled upon your priced plans, and was wondering what a &#x27;direct download link&#x27; is and why I would pay $10&#x2F;month for it. reply presson 11 hours agoparentHi! That was something experimental that we&#x27;ve deprecated. $10 &#x2F; month gets you verified reviews and a verified badge on your developer profile. There&#x27;s also some other cool stuff coming very soon. Ping me at support@store.app if you want to learn more. I can also give you a coupon if you want to take it for a test spin. reply ddxv 9 hours agoprevPWA button doesn&#x27;t do anything? Is it supposed to install or save something somewhere? On Firefox Mobile. reply presson 3 hours agoparentPWA button filters down to only web apps with a manifest + service worker. Unfortunately, Firefox is a bit behind in adding some of the features that make PWAs, well, progressive. If you&#x27;re on iOS, try Safari or Chrome. reply 0x6461188A 5 hours agoprevI like the site, but installable apps already have an app store. Why not target a much more underserved market—the uninstallable apps. Basically an app store for web apps that work on mobile. You could charge users for the apps and then send 70% of the revenue to the app creators. reply mgraczyk 5 hours agoparentThis is an app store for web apps, most of which work on mobile. reply donmcronald 12 hours agoprevHow do PWAs hold up when it comes to updates or getting a new phone? Will they get re-installed when I set up a new phone? reply hunter2_ 12 hours agoparentI&#x27;ve had them go missing just by doing an OTA update of my phone, and I don&#x27;t think it was even a major Android version. Or maybe it was a Play store update of the Chrome app, I forget exactly. But it was one of those things that&#x27;s not expected to eliminate data, yet it did.I think it was this: https:&#x2F;&#x2F;support.google.com&#x2F;android&#x2F;thread&#x2F;120942877&#x2F;all-my-w... reply ammarhalees 1 hour agoprevAwesome awesome concept reply iansinnott 5 hours agoprevThere is still no way to trigger an \"install on homepage\" dialogue from javascript, right?This seems to be the biggest limiting factor to me—The need for PWAs to include _instructions on how_ to install the app rather than a button to make it so. reply presson 3 hours agoparentNot on iOS, but you can trigger native install prompts on android and chromium browsers (Chrome and Edge). reply presson 3 hours agorootparentEdit: Not on iOS...yet. reply iansinnott 3 hours agorootparentOh, didn&#x27;t know that about Android. That&#x27;s a definite improvement.> yetIs there supposed planned support for this on iOS? Seems like it would be counter to Apple&#x27;s incentives, but I&#x27;d love to see it happen. reply soperj 12 hours agoprevNice! I attempted adding my webapp. Do you have a template that I could add to my site (ie: add our app)? reply presson 12 hours agoparentAwesome! Were you able to add it successfully? We have a widget that you&#x27;ll find under your claimed app to showcase your listing states, and we have a few more things that you can use on your app coming in about a week. If you&#x27;re interested in early access, shoot me a message at support@store.app and we can show you what&#x27;s coming. reply jollyllama 14 hours agoprevWhere can I learn about making these, specifically for offline use? reply spacec0wb0y 14 hours agoparenthttps:&#x2F;&#x2F;web.dev&#x2F;progressive-web-apps&#x2F; reply matsemann 13 hours agoprevI&#x27;m surprised by how much easier and faster this is than installing an app from Google Play. No ads above what I really searched for. Installation is basically instantaneous. reply meatjuice 9 hours agoprevI&#x27;ve been looking for something like this to happen, and it&#x27;s great!By the way, is it ok to list PWAs that I don&#x27;t own? reply presson 9 hours agoparentThanks!!! We don&#x27;t have a submission form for non-owned PWAs, but post here + tag me and I can add them! reply brundolf 6 hours agoprevCurious about the backing org. Is it nonprofit&#x2F;open-source, or a startup? And if the latter, what&#x27;s the business model? reply presson 3 hours agoparentStartup. Our business model is a small subscription fee for extra features for PWA devs. This will likely evolve over time, but we will be careful not to become what dislike. Our goal is to help web devs grow their business and crack distribution on mobile. reply hbcondo714 12 hours agoprev> List your app in under 2 minutesHow? I would like to list my finance PWA[1] but I had to create a regular account first and now wait for approval on a developer account to submit an app.[1] https:&#x2F;&#x2F;github.com&#x2F;hbcondo&#x2F;revenut-app reply presson 12 hours agoparentHi! You should&#x27;ve been approved for a developer account. I suppose the delay could go past two minutes. We&#x27;ll be speeding up this process soon. Hopefully in aggregate it only took you less than two mins :-) If you have any issues, ping me at support@store.app! reply hbcondo714 11 hours agorootparentThanks! I got downvoted for my comment but your response was worth it. I got approved in 15 minutes, but I&#x27;m not counting! It wasn&#x27;t mentioned DNS verification is required for app submissions but that is good you are doing that. Now I need to go through the Google Domains &#x2F; Squarespace transition: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36346454 reply SparkyMcUnicorn 13 hours agoprevHow are you sourcing the current listings?I see there&#x27;s a \"list your app\" (which means I can&#x27;t submit an app that isn&#x27;t mine, right?), but there are some high quality installable PWAs that are missing. Hoppscotch is a good example.Not expecting you to index the entire internet and filter out all the spam. Just curious how the current list was built. reply presson 13 hours agoparentThat&#x27;s correct, at the moment you can only add an app that you can claim ownership of (vis dns). The current list was manually added by us + by devs who have listed their apps on our site, but we&#x27;ll be rolling out a more scalable solution soon, along with some better curation. I think that&#x27;s a good idea to allow users to recommend listings - we&#x27;ll look into adding that soon. Would love for you to create an account so we can keep you posted on this feature &#x2F; other progress :-) reply dylan604 13 hours agorootparentmaybe a \"suggestion box\" method for people that know of good sites so that you might contact the dev directly about being listed? reply presson 10 hours agorootparentI like it! Ping me at support@store.app if you want to stay tuned and provide feedback on this feature after launch. reply simonbarker87 13 hours agoprevVery nice, one issue is that (on Safari iOS 16) when I navigate back to the list my scroll position is forgotten, I could understand when using my browser back button but I used the one in the UI so I’d expect it to remember the scroll position.Beyond that nitpick though this is excellent, well done. reply presson 12 hours agoparentNoted! Thank you for the feedback and the kind words! :-) reply t3e 7 hours agoprevCool concept! I&#x27;d like to list an app but I can&#x27;t seem to scroll all the way down to the bottom of the terms. reply fauigerzigerk 4 hours agoparentOn iPad, I can’t scroll to the bottom of any page. There’s no way to tap the “get” button on the bottom row of apps. reply LeoPanthera 9 hours agoprevOn Safari, the drop-down list of categories is blank below \"Food and Drink\". reply raybb 13 hours agoprevI love it! Please consider adding openlibrary.org which has a PWA already. reply presson 12 hours agoparentThank you! Added to our backlog - will be added soon! reply presson 12 hours agoparentprevListing is now live! :-) reply thekingshorses 13 hours agoprevPiHole blocks the email verification link&#x2F;domain. reply presson 12 hours agoparentThank you for posting this! Have noted for our team to look into. reply matthewhartmans 12 hours agoprevThis is sick! Congratulations on the launch! reply presson 12 hours agoparentThank you!!! This means the world to us :-) reply nektro 4 hours agoprevwould never install a web app reply presson 3 hours agoparentRemind me of this in 5 years :-P reply CranberryDefuse 12 hours agoprevReally nice idea and superb website, thank you. I wish we could submit without register though. reply presson 3 hours agoparentThank you for the kind words, that means a lot to us! We&#x27;re adding this feature, stay tuned. reply jshchnz 12 hours agoprevthis is real cool, and nice domain reply presson 12 hours agoparentThank you! :-) reply mceoin 7 hours agoprevNice domain name. reply presson 3 hours agoparentThank you! reply trollercoasters 11 hours agoprevBased reply tomaszs 12 hours agoprevCongrats. That is a project of the future! reply presson 12 hours agoparentThank you! :-) reply explain 13 hours agoprevThis UI is so hot. reply presson 12 hours agoparentThank you! :-) reply hanniabu 12 hours agoprevWhat are the security concerns with something like this? Can PWAs contain malicious code? If yes, what can they do? reply pietervdvn 11 hours agoparentA PWA is nothing more then a glorified web site, which opens full screen.Security-wise, they cannot do anything worse then a website can. In other words, this is _less_ then a native app can do. reply DANmode 10 hours agoparentprevAnything your browser can normally do with permissions allowed to domains by you. reply mouse_ 15 hours agoprevWhat a tragedy that what used to be called \"installing\" is now mocked by the industry lobby as \"sideloading\", and what used to just be \"visiting a web page\" has taken its rightful place.I have not and probably will not ever \"install\" a \"web app\". This is literally just a list of web sites. It&#x27;s no more useful than Yahoo circa 1998 but now the bookmarks are on your desktop or homescreen rather than in your browser, where you&#x27;re going to be taken anyways if you open one of them.Call me a cynical boomer. reply bko 14 hours agoparentI like the idea of \"installing\" a web app that basically creates a bookmark to a webpage, because the alternative is installing an actual app. It&#x27;s nice to have things all basically browser based as it gives the developer more control of the product and creates a single reasonable platform. I hate going to linkedin or Reddit or similar websites on mobile and them forcing me to use their app. It also breaks the dominance of centralized app stores like android and apple reply JohnFen 13 hours agorootparent> It&#x27;s nice to have things all basically browser based as it gives the developer more control of the product and creates a single reasonable platform.I get that. But as a user, I really hate browser-based applications and avoid them entirely unless I have no other option. reply majikandy 13 hours agorootparentApps that are just a web app inside an app container are even worse though! reply bko 13 hours agorootparentprevWhat do you prefer? Native apps? reply JohnFen 13 hours agorootparentYes. Although this depends a lot on the quality of the software, native apps tend to be much better in terms of UI and performance, and they tend to use less system resources -- sometimes a lot less. reply holoduke 12 hours agorootparentNowadays i would argue that the css rendering speed is faster than the Android UI renderer which suffers from extreme complexity. Ios might be still faster, but also much more limited. A good SPA is indistinguishable from a native app. reply coder543 11 hours agorootparent> A good SPA is indistinguishable from a native app.Exactly. It’s like CGI in movies. People always talk about how bad CGI is, without realizing just how much good CGI they never notice.https:&#x2F;&#x2F;youtu.be&#x2F;bL6hp8BKB24 replyDANmode 10 hours agorootparentprevProperly executed, it should act as segmented cache&#x2F;cookies&#x2F;storage, so you can clone, delete, etc isolated versions of your sessions with misbehaving (or potentially misbehaving) services. reply seabass-labrax 10 hours agoparentprevThere is one VERY big difference between PWAs and Yahoo circa 1998: you can use PWAs without a network connection, with full interactivity. This is really important for me, and probably for anyone who travels frequently, is on a metered internet connection, or just lives in a location with poor infrastructure.Yahoo did and still does require you to be connected to the internet, and whether that&#x27;s on a 2400 baud dial-up connection or a 5G wireless link, it will still cost money and be less reliable in places. reply tamimio 12 hours agoparentprevYour average person now think of anything as an “app” or it should have one, even if it was just a wrapper for a browser, they would still install the app (1), so I think OP is in the right direction, nothing will change from the user side but much better for the developer one.(1) Reminds me of this :) https:&#x2F;&#x2F;files.catbox.moe&#x2F;36o4jr.jpeg reply presson 12 hours agorootparentThis is our hope. If we can help smooth out the messaging &#x2F; install process, once a PWA is installed most apps will feel identical to a native app for most users. We used to say \"web apps are apps\" but now we just say \"apps are apps\" :-P reply presson 14 hours agoparentprevYou&#x27;re a cynical boomer :-PBut really, I think you&#x27;ll change your mind at some point. There&#x27;s some pretty cool stuff coming. reply thekingshorses 12 hours agorootparentI m that cynical boomer.I use mostly apps like Whatsapp, and garage door opener.Can&#x27;t use banks apps as you can only login to one account.Everything I do is on the browser. https:&#x2F;&#x2F;hn.premii.com & https:&#x2F;&#x2F;reddit.premii.com - almost 10 years old, it still works. No need to update the framework or need someone&#x27;s approval. reply mouse_ 14 hours agorootparentprevAnything I wouldn&#x27;t be able to do by just going to the website? :&#x2F; reply presson 14 hours agorootparentOff the top of my head for ios - push notifications, offline use, optimized storage options (local storage not cleared after 7 days of no use), app-like ux (no browser controls), home screen icon for easy access, background audio (sound while app is closed).The real advantage is for devs who can build an app that feels like a native app, but with one codebase for distribution across OSs&#x2F;devices, not having to pay the apple&#x2F;google tax, and not dealing with app review. reply _hzw 13 hours agorootparentI made my web app into a PWA and even went so far as to polish the home screen icon and splash screen to make it look nice once installed. However, in the end, I use the browser much more frequently, simply because I can access the URL of the page and share it elsewhere. reply presson 13 hours agorootparentThe beauty of the browser is that you can choose your own adventure. Being able to use or test apps in a browser tab is awesome, and it&#x27;s up to the dev to choose a ux that showcases &#x2F; informs the user of additional capabilities unlocked after adding to home screen. We&#x27;re at the very early days of exploring what this ux can unlock. reply aragonite 13 hours agorootparentprevAlso, on a desktop, some websites are just more naturally run in their own separate window than as a browser tab because you frequently need to switch to&#x2F;from them: e.g. ChatGPT, Gmail, Google Voice&#x2F;Keep, MDN docs, dictionaries.And you can put the PWAs in full sceen mode, whereas full-screen mode in a regular browser is almost always pointless ever since they took away tab indicators. replyxnx 14 hours agoprev [–] Darn. Was really hoping this was something like a more universal version of the \"Deploy to Heroku\" buttons (https:&#x2F;&#x2F;devcenter.heroku.com&#x2F;articles&#x2F;heroku-button) replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Store.app, a mobile app developed by PWA Labs, Inc, provides features that allow users to login, sign up, navigate, make profiles, modify settings, create lists, and save favorites.",
      "The app offers a wide range of categories including AI, crypto, games, productivity, shopping, and social, enabling users to install and operate different applications within these groups.",
      "Among the popular apps are Wordi Games, Replit Dev Tools, friend.tech, Canva, and numerous AI apps."
    ],
    "commentSummary": [
      "The central topic of discussion is Progressive Web Apps (PWAs), and their potential benefits over native apps regarding usability and functionality.",
      "Feedback is shared on a specific app store for PWAs, covering features like filters, installation process, user impressions, and security concerns alongside suggestions to improve.",
      "An up-and-coming platform called Store.app, aimed at assisting web developers, is covered. It provides useful tools and widgets, but some users report installation and update issues. The ongoing debate between the advantages and disadvantages of apps over websites is also noted."
    ],
    "points": 377,
    "commentCount": 173,
    "retryCount": 0,
    "time": 1696531810
  },
  {
    "id": 37784499,
    "title": "NIST Elliptic Curves Seeds Bounty",
    "originLink": "https://words.filippo.io/dispatches/seeds-bounty/",
    "originBody": "05 Oct 2023 ANNOUNCING THE $12K NIST ELLIPTIC CURVES SEEDS BOUNTY The NIST elliptic curves that power much of modern cryptography were generated in the late ‘90s by hashing seeds provided by the NSA. How were the seeds generated? Rumor has it that they are in turn hashes of English sentences, but the person who picked them, Dr. Jerry Solinas, passed away in early 2023 leaving behind a cryptographic mystery, some conspiracy theories, and an historical password cracking challenge. Together with some generous matchers, I’m announcing a $8,192 $12,288 (12 Ki$) bounty for cracking these five hashes, tripled to $36,864 if the recipient chooses to donate it to a 501(c)(3) charity of their choice. 3045AE6FC8422F64ED579528D38120EAE12196D5 BD71344799D5C7FCDC45B59FA3B9AB8F6A948BC5 C49D360886E704936A6678E1139D26B7819F7E90 A335926AA319A27A1D00896A6773A4827ACDAC73 D09E8800291CB85396CC6717393284AAA0DA64BA Thank you to Amir Omidi, Chris Palmer, Colm MacCárthaigh, David Adrian, David Anderson, Jeff Hodges, Matt Green, Matthew McPherrin, Paul Kehrer, Ryan Sleevi, and Soatok for contributing to the bounty, and to Steve Weis for the research that inspired this. Subscribe to Cryptography Dispatches for more! Subscribe Step back, what is this about? The NIST elliptic curves (P-192, P-224, P-256, P-384, and P-521[1]) were published by NIST in FIPS 186-2 in 2000, and generated “verifiably at random” according to ANSI X9.62 by taking an arbitrary seed, hashing it with SHA-1, and using the output to derive some of the parameters. A lot of cryptography uses NIST curves, and especially P-256 and P-384. They are in the Commercial National Security Algorithm Suite (the successor of Suite B), and are the curves used by the ECDSA X.509 certificates that secure much of the web. They’re a big deal. Steve Weis has recently published a well researched article on everything we know about those arbitrary seeds embedded in the FIPS 186 specification. Apparently, they were provided by the NSA, and generated by Jerry Solinas in 1997. He allegedly generated them by hashing, presumably with SHA-1, some English sentences that he later forgot. [Jerry] told me that he used a seed that was something like: SEED = SHA1(\"Jerry deserves a raise.\") After he did the work, his machine was replaced or upgraded, and the actual phrase that he used was lost. When the controversy first came up, Jerry tried every phrase that he could think of that was similar to this, but none matched. That’s unfortunate, because the NIST curves are—surprisingly—looking better and better: we now have complete addition formulas for them, mitigating their major footgun; we know how to design safer interfaces for them; and we painfully learned to appreciate the value of prime order curves immune to cofactor attacks. However, there is—mostly amongst non-practitioners—some fear that the NSA could have picked the seeds to select some intentionally weak curves. Do I think those fears are well-founded? No. Koblitz and Menezes make a good argument in A riddle wrapped in an enigma that even with full control over the seed, the NSA would have had to be aware of a class of weak curves so large that it’s not plausible that no one in academia or industry discovered them in 25 years.[2] Anyway, some FUD persists around the otherwise pretty good NIST curves that would be good to clear up, even if the English preimage of the hashes is not a complete guarantee of rigidity[3]. That’s where this bounty comes in. Finding the pre-seeds, the inputs to the hash that generated the seeds, is the bread and butter of password crackers and brainwallet bruteforcers. This is a call to arms for them to join the search, help fill in a page of cryptographic history, and collect a large bounty or donate an even larger one to charity. Ok, so what do we know about the hashes? To recap Steve Weis’s post, the inputs are probably English phrases which mention Jerry Solinas, possibly someone else, and probably a counter. If you’re actually going for it I recommend reading Steve’s post in full. The counter has to be there because only one in every 192 to 521 hashes is actually good to make a curve out of, depending the bit size of the curve. (This is because one in every ln(N) numbers less than N is prime, for large enough N.) There’s a 99% chance the counter is less than 2400 for the largest curve, and less than 1175 for P-256. The seeds for P-192 and P-256 appeared as examples in the previous ANSI X9.62 standard, while all the others were new in FIPS 186-2, so they might have been generated from differently structured sentences. Since testing more hashes is nearly free, I recommend also targeting all the examples from ANSI X9.62 that didn’t make the FIPS standard, as well as the seeds for the binary curves in FIPS 186-2, although they are not included in the bounty. Here’s a recap. 3045AE6FC8422F64ED579528D38120EAE12196D5 # NIST P-192, ANSI prime192v1 BD71344799D5C7FCDC45B59FA3B9AB8F6A948BC5 # NIST P-224 C49D360886E704936A6678E1139D26B7819F7E90 # NIST P-256, ANSI prime256v1 A335926AA319A27A1D00896A6773A4827ACDAC73 # NIST P-384 D09E8800291CB85396CC6717393284AAA0DA64BA # NIST P-521 31A92EE2029FD10D901B113E990710F0D21AC6B6 # ANSI prime192v2, not eligible for bounty C469684435DEB378C4B65CA9591E2A5763059A2E # ANSI prime192v3, not eligible for bounty E43BB460F0B80CC0C0B075798E948060F8321B7D # ANSI prime239v1, not eligible for bounty E8B4011604095303CA3B8099982BE09FCB9AE616 # ANSI prime239v2, not eligible for bounty 7D7374168FFE3471B60A857686A19475D3BFA2FF # ANSI prime239v3, not eligible for bounty 85E25BFE5C86226CDB12016F7553F9D0E693A268 # NIST B-163, not eligible for bounty 74D59FF07F6B413D0EA14B344B20A2DB049B50C3 # NIST B-233, not eligible for bounty 77E2B07370EB0F832A6DD5B62DFC88CD06BB84BE # NIST B-283, not eligible for bounty 4099B5A457F9D69F79213D094C4BCD4D4262210B # NIST B-409, not eligible for bounty 2AA058F73A0E33AB486B0F610410C53A7F132310 # NIST B-571, not eligible for bounty The format of the string is part of the mystery. It could end with a period or not, end with a newline or not, the counter could be decimal (with or without leading zeroes) or binary (16 or 32 bit), and it could come after the period or separated some other way. The same sentence with different counters could have been used to generate all the seeds, or they could be different sentences, or they could include the curve name or size. Human memory is notoriously fallible, so it could also be that some of the details in the second-hand recollections are wrong. (Edited to add: it could even be that instead of a counter they started with SHA-1(s), and then tried SHA-1(SHA-1(s)), and so on. Or maybe they started with SHA-1(s), and then incremented the hash like IP2BS(BS2IP(h) + 1), which is what they do to the seed to extend it to the size of a field element in ANSI X9.62, Section A.3.3.1. The latter should be cheap to account for by adding to the list of hashes ~2500 decrements of each of the hashes above. I can produce a list if anyone wants.) The good news is that SHA-1 is tremendously fast to bruteforce, and YOU are the experts in cracking passphrases you know nothing about. Cool, what’s the fine print? The bounty will pay out to the first person(s) to email the pre-seeds for the five prime-order NIST curves to seeds@filippo.io. Half the bounty ($6,144) will pay out to the first submission of at least one pre-seed, and the other half will pay out to the first submission of all five pre-seeds. They can of course go to the same person, so don’t wait to have them all to submit. Even one would make history. If successful, you can either choose to receive the cash bounty, or select a U.S. 501(c)(3) charity to receive triple the amount. We reserve the right to veto charity choices dramatically incompatible with our values, but we won’t be jerks about it. If it’s not legally allowed for a U.S. person or Italian national to send money to you, you will have to select the charity option. You’re responsible for any taxes on the cash bounty. I fully trust every matcher, and I am guaranteeing the full amount of the bounty personally, so you don’t have to. Put “ANTISPAM” in the subject line of any submission to hit my allowlisting rules. The Received header of my mail host will be the unappealable criterion of what submission arrived first. The bounty expires if the seeds become publicly known, otherwise it’s valid until announced otherwise on this page. If the bounty is being cancelled or lowered, it will be announced six months in advance. (We don’t want anyone to feel cheated of their resources.) We don’t actually care how you find the seeds. It can be bruteforcing, clever guessing, sleuth work tracking down NSA employees (don’t get arrested), or even recovering that old backup of when you used to work at NIST. If you don’t want us to, we won’t ask questions. May the hashrate be ever in your favor, and let's fill out a page of cryptographic history. For updates, you might want to follow me on Bluesky or Mastodon. Not a typo for 512. There’s a very conveniently shaped prime at 2^521-1. We do typo that a lot in code. ↩︎ The NSA's precedents, namely Dual_EC_DRBG, are why some people find the seeds suspicious. I find them reassuring. First, selecting weak seeds (which are just hash inputs, not \"keys\" like in Dual_EC_DRBG), would not be a NOBUS backdoor. Second, the Dual_EC_DRBG design immediately stuck out like a sore thumb and library authors had to be paid to implement it; this suggests the NSA is kinda bad at backdoors, not magical. ↩︎ Rigidity is the design generalization of nothing up my sleeves numbers. The idea is that if you set your goals explicitly and then make only obvious and rational and optimal choices in a design, there is no wiggle room to pick intentionally weak outcomes. FWIW, I think rigidity is overrated: there is no such thing as an objectively best choice, and reasonable people disagree, and it’s possible to craft rational arguments for many different choices. Anyway, it’d be nice to settle the argument by bringing the NIST curves up in their level of rigidity by cracking the seeds. ↩︎ Subscribe to Cryptography Dispatches for more! Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=37784499",
    "commentBody": "NIST Elliptic Curves Seeds BountyHacker NewspastloginNIST Elliptic Curves Seeds Bounty (filippo.io) 348 points by mfrw 12 hours ago| hidepastfavorite70 comments tptacek 12 hours agoSome of the backstory here (it&#x27;s the funniest fucking backstory ever): it&#x27;s lately been circulating --- though I think this may have been somewhat common knowledge among practitioners, though definitely not to me --- that the \"random\" seeds for the NIST P-curves, generated in the 1990s by Jerry Solinas at NSA, were simply SHA1 hashes of some variation of the string \"Give Jerry a raise\".At the time, the \"pass a string through SHA1\" thing was meant to increase confidence in the curve seeds; the idea was that SHA1 would destroy any possible structure in the seed, so NSA couldn&#x27;t have selected a deliberately weak seed. Of course, NIST&#x2F;NSA then set about destroying its reputation in the 2000&#x27;s, and this explanation wasn&#x27;t nearly enough to quell conspiracy theories.But when Jerry Solinas went back to reconstruct the seeds, so NIST could demonstrate that the seeds really were benign, he found that he&#x27;d forgotten the string he used!If you&#x27;re a true conspiracist, you&#x27;re certain nobody is going to find a string that generates any of these seeds. On the flip side, if anyone does find them, that&#x27;ll be a pretty devastating blow to the theory that the NIST P-curves were maliciously generated --- even for people totally unfamiliar with basic curve math.So: pretty fun bounty. reply PeterisP 9 hours agoparent> if anyone does find them, that&#x27;ll be a pretty devastating blow to the theory that the NIST P-curves were maliciously generatedIDK, if I don&#x27;t think that finding that a seed matches a hash of \"Give Jerry a raise of $100000 dollars now!!!\" is any evidence for that, because if I had a desire to generate malicious constants, and knew some unusual property that they must have to be weak, then nothing would prevent me from generating hashes of many, many variations of similar strings until one gives me constants with the properties I need. reply tptacek 9 hours agorootparentAt the point where we find an intelligible English string that generates the NIST P-curve seeds, nobody serious is going to take the seed provenance concerns seriously anymore. I think everybody sort of understands that people who don&#x27;t work in cryptography are always going to have further layers of theory to add, the same way people waiting for the \"Mother of All Short Squeezes\" do with Direct Share Registration and share votes and stuff. If the bounty program is successful, that&#x27;s going to end the NIST curve \"debate\", such as it is.If you&#x27;re convinced the P-curves must be backdoored, despite the computer science arguments that suggests they really couldn&#x27;t have been, then you should comfort yourself in the knowledge that we&#x27;re probably not going to find the seed strings any time soon; presumably Solinas tried pretty hard himself! reply pclmulqdq 9 hours agorootparentI might be suspicious of “Give Jerry a $3263958374 raise,” but that would launch an interesting hunt as to what property they were exactly mining for. reply tptacek 9 hours agorootparentFor the reason stated in the article, it&#x27;s actually pretty likely that there&#x27;s a counter in there somewhere. A 31-bit number like \"3263958374\" doesn&#x27;t seem especially interesting cryptographically. reply kadoban 9 hours agorootparentThe counter is described as the minimum value that will fit the pattern and make the result a prime. So that should be easily checkable and not really add any free variables.If you were evil and motivated you&#x27;d probably want to hide your variables in the innocent looking part, the simple English or the punctuation, instead. reply pclmulqdq 5 hours agorootparentprevYeah, that counter is separate from a counter embedded in an ASCII string. The one thing that kind of string indicates is that it almost certainly wasn&#x27;t the first string they tried. reply debatem1 8 hours agorootparentprevThis would basically be a Nostradamus attack. If we&#x27;re going around claiming exciting (read: improbable) things about P256 I don&#x27;t know what stops us from claiming the NSA can generate collisions in the hash functions it also chose the parameters for. reply formerly_proven 2 hours agorootparentprev> At the point where we find an intelligible English string that generates the NIST P-curve seeds, nobody serious is going to take the seed provenance concerns seriously anymore.GP is making a different argument along the lines of last week’s Twitter fad generating intelligible English sentences which spell out the first few bytes of their hash. reply some_furry 9 hours agorootparentprevYeah but Jerry wouldn&#x27;t have been incentived to do such a thing, would he? reply colmmacc 12 hours agoparentprevBy the late nineties using both MD5 and SHA1 for \"additional robustness\" together in ad-hoc constructions was also en vogue. SSLv2 and SSLv3 are good examples. The outputs match the size of a SHA1, but it wouldn&#x27;t be that shocking if the pipeline were some form of echo \"$string\"md5sumsha1sum. reply tptacek 10 hours agorootparentWhat&#x27;s smart about this bounty is that these kinds of silly hashing constructions, and scalable pipelines for figuring them out through brute force, are the bread and butter of password crackers, which is practically a sport now. So you&#x27;d be optimistic that if the problem is interesting enough, somebody will work it out if there&#x27;s like, a pre-MD5 hash or something. reply throw0101a 10 hours agoparentprev> Some of the backstory here (it&#x27;s the funniest fucking backstory ever): it&#x27;s lately been circulating --- though I think this may have been somewhat common knowledge among practitioners, though definitely not to me --- that the \"random\" seeds for the NIST P-curves, generated in the 1990s by Jerry Solinas at NSA, were simply SHA1 hashes of some variation of the string \"Give Jerry a raise\".For a longer history see the mentioned-in-article paper by Koblitz and Menezes, \"A Riddle Wrapped in an Enigma\" (which is more asking why the NSA said, in 2015, that things should move towards a post-quantum world):* https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2015&#x2F;1018* https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2015&#x2F;1018.pdfThe same \"backdoor\" thinking seems to have been present with (EC)DSA:> Proponents of RSA bitterly opposed DSA, and they claimed that the NSA was promoting DSA because they had inserted a back door in it (“No Back Door” was the slogan of the anti-DSA campaign). However, they gave no evidence of a back door, and in the two decades since that time no one has found a way that a back door could be inserted in DSA (or ECDSA).Then there was this little tidbit:> As the heated debate contin- ued, the NSA representative left to make a phone call. When he returned, he announced that he was authorized to state that the NSA believed that ECC had sufficient security to be used for secure communications among all U.S. government agencies, including the Federal Reserve. People were stunned. In those days the NSA representatives at standards meetings would sit quietly and hardly say a word. No one had expected such a direct and unambiguous statement from the NSA. The ECC standards were approved.Other actions by the NSA that were treated as suspicious were tweaks to DES (which later turned out to be defence against differential cryptanalysis) and weaknesses in the original SHA (\"SHA-0\") which the final version (\"SHA-1\") does not have. reply dhx 2 hours agorootparent> \"However, they gave no evidence of a back door, and in the two decades since that time no one has found a way that a back door could be inserted in DSA (or ECDSA).\"ElGamal&#x2F;DSA is fragile and difficult to implement securely in a way that doesn&#x27;t reveal secret k. ECC (and somewhat particularly NIST curves[1]) is also very difficult to implement securely from side channel attacks[2].Consideration of \"backdoors\" should be broader than just mathematical equations on paper. \"backdoors\" could also include deliberate choice of algorithm or parameters that are fragile and difficult to implement in a secure way to minimise risk of side channel leakage of secrets. This is especially important in a whole variety of applications such as passports, payment cards and TPMs.[1] https:&#x2F;&#x2F;www.hyperelliptic.org&#x2F;tanja&#x2F;vortraege&#x2F;20130531.pdf#p...[2] Specifically, implementation of: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Elliptic_curve_point_multiplic... reply less_less 6 minutes agorootparent> ElGamal&#x2F;DSA is fragile and difficult to implement securely in a way that doesn&#x27;t reveal secret k. ECC (and somewhat particularly NIST curves[1]) is also very difficult to implement securely from side channel attacks[2].IMHO this isn&#x27;t evidence of a backdoor. Side-channel protection is hard. RSA decryption and especially keygen are also tricky to implement in a side-channel-protected way, and a widespread timing attack on RSA decryption in many libraries was published earlier this year [3].The NIST p-curves used the state-of-the-art elliptic curve shape for general operations when they were released: short Weierstrass curves over prime fields. Edwards curves are easier to defend against side channels and are overall a better choice (with their own rough edges, mostly involving the cofactor), but those were not known until 2007. Montgomery curves (similar rough edges to Edwards, plus the point at infinity) were known earlier and are nice for key exchange, but they are not as nice for signatures.Overall I would not choose the NIST curves for a new design today, because Edwards&#x2F;Montgomery curves are a better choice. But I think the evidence they were backdoored (mathematically or otherwise) is weak.[3] https:&#x2F;&#x2F;people.redhat.com&#x2F;~hkario&#x2F;marvin reply pests 8 hours agorootparentprevIts funny - all this doubt and suspicion of the NSA but then you end your post about how NSA has been saving our asses. Maybe they aren&#x27;t so bad afterall? &#x2F;s reply tptacek 6 hours agorootparentThey&#x27;re not, in fact, comic book villains. They have a pretty understandable mission, and then a set of organizational values that are sharply different than those of technologists. reply cperciva 6 hours agoparentprevConceivably there&#x27;s a subset of weak values and Jerry tried \"Give me more money\", \"Jerry deserves more money\", etc until he found a phrase which produced a weak value.I don&#x27;t think that&#x27;s what happened, but it does mean that \"some variation of give Jerry a raise\" doesn&#x27;t mean the value wasn&#x27;t chosen maliciously. reply stavros 1 hour agorootparentAgreed, I wouldn&#x27;t have thought so a few weeks ago, but after seeing the messages that contain the first few SHA bytes of themselves in themselves, I don&#x27;t know how implausible it is that they might have generated billions of strings and hashes and then chosen the weaker ones. reply raverbashing 37 minutes agorootparentI&#x27;d think it&#x27;s a bit implausible. Especially with the computing power (and optimizations) available at the time reply worewood 11 hours agoparentprev> At the time, the \"pass a string through SHA1\" thing was meant to increase confidence in the curve seeds; the idea was that SHA1 would destroy any possible structure in the seed, so NSA couldn&#x27;t have selected a deliberately weak seed.It&#x27;s standard to use transcendental constants like pi or e for this purpose as you can&#x27;t select them. A phrase could in theory be selected to yield a more desirable hash reply jancsika 7 hours agorootparent> It&#x27;s standard to use transcendental constants like pi or e for this purpose as you can&#x27;t select them.If one may choose between one or the other without arousing suspicion, that&#x27;s one bit of entropy.If I can add the possibility of sin(1) and cos(1), that&#x27;s two bits.And remind me: are we using sha1, or md5? Or perhaps sha1 -> md5, or md5 -> sha1?And so on, until I have enough bits to reimplement the suspicious seed in the cracks of all these innocuous, and very poorly specified \"standard\" choices.Dan Bernstein (and some other cryptographer?) wrote about this and did a demo creating over a million curves in a short&#x2F;practical amount of time on commodity hardware. All of them had seeds specified using a \"standard\" method like what you mention above.It makes me think all cryptographers ought to just go ahead and choose suspicious seeds where prudent, kinda like if you&#x27;re going to play the lottery, just go ahead and choose the numbers 1 2 3 4 5 etc. At least then you get instant, free information about the lack of knowledge of the people who complain that you&#x27;re doing it wrong. reply tedunangst 10 hours agorootparentprevWhat do you if pi doesn&#x27;t create a nice curve? reply kevincox 10 hours agorootparentYou come up with a hopefully simple rule to try again. Like skip the first byte and try again.Assuming that you publish your definition of \"a nice curve\" then third parties can verify that you used the first offset in pi that worked. reply tedunangst 9 hours agorootparentAh, of course, just drop bits until it \"happens\" to work. You would suggest that, instead of the simple and obvious solution of using pi times e. reply quickthrower2 9 hours agorootparentprevYou could however test different schemes like this prior to announcing the scheme. reply Dylan16807 8 hours agorootparentThere&#x27;s only so much room for simple methods.So maybe you get to pick the most crackable for you out of 25 options. But you can&#x27;t hide a proper backdoor like that. reply duskwuff 4 hours agorootparentAnd if it&#x27;s possible to get an easy-to-break configuration in a relatively small number of attempts, it&#x27;s probably because there&#x27;s something fundamentally broken about the entire construct, not just the seed you picked. And that sort of brokenness is likely to be a lot harder to hide. reply some_furry 10 hours agorootparentprevDoesn&#x27;t pi by definition make a nice curve?(Dumb geometry joke, sorry. I know what you mean.) reply acqq 1 hour agoparentprevHowever, I don&#x27;t find it funny reading all the \"gory\" details:In the article, under the subtitle \"Step back, what is this about?\" is:\"The NIST elliptic curves (P-192, P-224, P-256, P-384, and P-521[1]) were published by NIST in FIPS 186-2 in 2000, and generated “verifiably at random” according to ANSI X9.62 by taking an arbitrary seed, hashing it with SHA-1, and using the output to derive some of the parameters.\"Note the sentence: “verifiably at random” according to ANSI X9.62.Now, the mentioned ANSI X9.62 describes very formal algorithms of what “verifiably at random” should mean:\"If it is desired that an elliptic curve be generated verifiably at random, then select parameters (SEED, a, b) using the technique specified in Annex A.3.3.1\"and then goes on to specify an exact algorithm both how the parameters are selected in A.3.3 and then in A.3.4 the verification algorithm: \"The technique specified in this section verifies that the defining parameters of an elliptic curve were indeed selected using the method specified in Annex A.3.3\"So, if I understand correctly, the authors spent enough energy both to construct the algorithms to generate the constants and make the SEED public as well as the algorithms to later verify the parameters given the publicly known SEED as an input. And to publish all that in ANSI X9.62.If that was the idea of “verifiably at random” according to ANSI X9.62, and if then nobody knows the SEED, then it appears that the very procedure, for which a lot of energy was spent to be developed or described, was just not followed. From which it can be concluded that the \"if\" condition of the sentence was just not true:\"If it is desired that an elliptic curve be generated verifiably at random...\"(Not to mention that the algorithms published there clearly aren&#x27;t \"simply SHA1 hashes\" in the sense result = SHA1( seed ) but, casually looking, a concatenation of only some bits of output from multiple SHA1 runs over the increments of the seed, which could suggest that nobody who tries any human written string as a seed would ever find a result by expecting a match of a whole constant with an output of a single SHA1 pass? Has anybody calculated how big would be a chunk of bits from a single SHA1 run actually for every of the constants?)Now, I probably miss something here, if it is so, I&#x27;d like to know what. reply meithecatte 12 minutes agorootparentThe standards claim that the existence of such a (SEED, a, b) tuple is enough to show that there is nothing special about the curve in question. But if one in a billion curves have a special property that only you know about, which would make it easier for you to attack the cryptosystem, you can try a variety of different SEED values until you find a desirable curve. reply quickthrower2 9 hours agoparentprevHe kind of “lost his bitcoin” before bitcoin was invented reply nullc 10 hours agoparentprevI burned an unreasonable amount of cpu power searching for input that were used to produce the ~166 bit &#x27;random&#x27; value used to construct G in secp256k1 and secp224k1 without success. (in both cases the parameters choice of G is the double of a point with a suspiciously sized x coordinate, and the same for both curves).For those curves the choice of G is the only particularly high entropy input into their selection, and it&#x27;s provably almost irrelevant-- the choice of it lets them know one specific arbitrary discrete log. (you can imagine a contrived protocol where this would be a backdoor, but it would be a pretty contrived protocol). But since it&#x27;s the only really unknown parameter I thought it was worth searching for.If someone does find the seed used for the P-curves it might also be similar to the one used for the the generators of those other curves and solve their minor mystery too. reply Mistletoe 6 hours agoparentprevDid he get that raise? reply smegsicle 11 hours agoparentpreva true conspiracist doesn&#x27;t believe everything he hears reply djbusby 9 hours agorootparentThat&#x27;s not true reply some_furry 7 hours agorootparentI don&#x27;t trust what you&#x27;re saying reply yukkuri 8 hours agoparentprevA conspiracy theorist can always find excuses to wave away evidence incompatible with their beliefs reply foota 11 hours agoparentprevWhy doesn&#x27;t he just do a reverse i search? :) reply api 8 hours agoparentprevI hope it&#x27;s something like SHA1(\"Bill Clinton is a poopy head.\") reply dmurray 3 hours agorootparentI&#x27;m hoping for something more like \"The bodies are buried at 2100 15th St\". reply physicsguy 3 hours agoprev> the NSA would have had to be aware of a class of weak curves so large that it’s not plausible that no one in academia or industry discovered them in 25 years.GCHQ in the U.K. hires more mathematicians than any other research institute or University in the country. Not sure about the US equivalents but I imagine it’s similar.Diffie-Helman key exchange was known about by GCHQ and the NSA prior to it being rediscovered by Diffie and Helman. I think it’s hard to assume anything about the underlying capabilities of intelligence institutions. Not saying they do know this but it’s also not impossible, this stuff is their bread and butter. reply geraldwhen 54 minutes agoparentThe NSA employs US math professors in number theory. reply mike_hearn 50 minutes agoparentprevI think you&#x27;re right to be suspicious, especially because the arguments for it not being possible are presented as if they are mathematical but are actually social.The usual response to these concerns is to argue that academics are so excellent that they would certainly have discovered what the NSA was up to by now, if there was a way to do it, and anyone who doesn&#x27;t agree with that is as FUDy pleb who just doesn&#x27;t get it. The article repeats this party line, although it&#x27;s great to now see the problem being taken more seriously with an organized bounty programme. I&#x27;ll be surprised but happy if anyone actually finds pre-images. But the frequency with which concerns are blown off here is just not good enough.I&#x27;ve done some cryptography work in the past and for several years had to regularly review cryptography papers as part of my job. I&#x27;ve attended cryptography conferences, talked with researchers, implemented various \"exotic\" things with elliptic curve cryptography and so on. Not exactly an insider but not a complete outsider either. The party line here looks very dangerous to me.Let&#x27;s recap the arguments here because neither of them seem strong and neither are actually mathematical at their core.1. If there was a way to execute a kleptographic attack on NIST curve standardization, academics would have found it by now. If not academics, then someone in industry.2. Dual_EC_DRBG was detected as suspicious immediately, so the public cryptography community is good at spotting back doors.For (1) why should that be so? It actually seems unlikely to me. Cryptography suffers the same problems as the rest of academia w.r.t. the file drawer problem and \"publish or perish\" incentives. If you&#x27;re an up and coming university researcher, which path is more profitable for you? Develop some clever new zero knowledge proof algorithm and be fairly certain to publish some cool new paper that gets cited a lot, or start attacking an algorithm that \"everyone\" already knows to be incredibly strong and almost certainly come up with nothing. If you take the latter the expected outcome is that you languish in obscurity at best or simply perish at worst (lose funding, exit academia with nothing to show and being seen as a crank).Put another way the argument for the strength of academic understanding is that lots of really clever people have studied this in depth and found nothing. There is a Consensus Of Experts. But because you don&#x27;t get to publish null results in academia, there&#x27;s actually no way to know how much effort has been put towards this kind of problem. That&#x27;s why assertions about academic crypto-analytic supremacy are always so handwavey and difficult to reason about. There&#x27;s no actual numerical proof of work being done, and we know that some fields of academia have extreme difficulty with things being declared a \"consensus\" for social reasons that later have to be walked back.Moreover, researching kleptography specifically (how to backdoor standards) isn&#x27;t a good career path because kleptography isn&#x27;t useful for anything unless you&#x27;re the NSA, so you probably won&#x27;t be able to easily transition to a post-academic career in industry on the back of that expertise and you won&#x27;t get many citations.The NSA doesn&#x27;t have any of these problems. It can pay better wages than academia, hire more researchers than all of academia put together, and then assign them full time to research that is likely to be a dead end or which is useful only for backdooring standards. It can also easily build multi-disciplinary teams and fund research that academic cryptography just can&#x27;t tackle at all due to lack of hardware budgets. And it has been doing exactly that for decades.If I had to bet on whose understanding of ECC is better, the NSA&#x27;s or academia&#x27;s, well, all the firepower is on the side of the government. It&#x27;s not even a close competition. In fact we can even measure how much firepower the government has, hence the well worn line about how many maths PhDs they hire, but we actually have no idea how much firepower academia levels at this part of the problem space.So we&#x27;re left with argument (2), people immediately raised the alarm about Dual_EC_DRBG so the NSA must actually kinda suck at designing backdoors. But this isn&#x27;t an ideal argument because people immediately raised concern about the NIST curves too. The only difference is that in the former case, there was already a known algorithm that could be used to pull off the needed attack, and in this case there isn&#x27;t.Fundamentally there&#x27;s no reason this debate should even be needed. It&#x27;s been known for decades how to avoid doubt here, that&#x27;s the whole reason the NIST curves are the output of SHA1 to begin with. The best time to phase out the NIST curves was decades ago, the second best time is today. reply mcpherrinm 12 hours agoprevI&#x27;m one of the people contributing to the bounty because if it really is a password-crackable phrase, it would be of significant historical impact to know. reply zakk 3 hours agoprev> The NIST elliptic curves that power much of modern cryptography were generated in the late ‘90s by hashing seeds provided by the NSA.I find this deeply troubling. So the seeds were provided by the NSA and they said \"don&#x27;t worry, they were generated hashing a trivial sentence. Unfortunately we forgot it now, but trust us, it&#x27;s just Jerry joking about getting a raise, nothing more...\"I can&#x27;t believe this didn&#x27;t undergo further scrutiny earlier, and I can&#x27;t believe the seeds haven&#x27;t been chosen in a more sensible way, such as combining random seeds provided by different parties with competing interests, also including hardware RNGs, etc... reply GTP 27 minutes agoprevThis is yet another instance where more transparency on the NIST&#x2F;NSA side would have been beneficial in the long run. If they said from the start, \"here are the seeds, we generated them by computing SHA1(insert_solinas_string_here)\" the whole debate would have never started in the first place. reply rollulus 2 hours agoprevSo if I understand this correctly: the community accepted those mysterious strings of unknown origin while it would’ve been trivial to replace them with different strings with a known origin, by providing another but known input to the hash? reply mike_hearn 1 hour agoparentYou understand the situation correctly, hence why it&#x27;s kind of disastrous. The phrases \"so close\" and \"you had one job\" seem relevant here. Unfortunately, as far as I know this would be the only case related to the NSA and cryptographic standards where someone has alleged incompetence. Normally the stories run the other way.Doubly problematic: NIST is known to have been compromised and putting backdoors into elliptic curve related standards, the fact that this mechanism didn&#x27;t create trust was pointed out immediately, and neither NIST nor the NSA did anything to address the concern. Just like with Dual_EC_DRBG.Triply problematic: the NSA explicitly told people in 2015 not to upgrade past the NIST curves to other curves, because quantum computers will soon be good enough to break ECC entirely and so everyone should switch to post-quantum crypto instead (which is new and still experimental widely used etc). If ECC worked fine, QC was far off and you wanted to keep people on the NIST curves for as long as possible this is exactly what you would say.The cryptography community has not exactly covered itself in glory over this situation. It&#x27;s been nearly 25 years now. There are newer curves that don&#x27;t have this problem, why are the NIST curves still being used by anything? Where is the effort to phase them out, like there was with SHA1? This article even seems to be advertising them. reply xu3kev 7 hours agoprevIf you&#x27;re feeling lucky, you can give it a shot by guessing the sha1 hash here: https:&#x2F;&#x2F;wending.dev&#x2F;hash_guessing&#x2F; reply Aachen 5 hours agoparentWait, that just generates the sha1 of your input string and nothing more? Because as the post says, it has to include a counter of some sort. I&#x27;m sure the NSA&#x27;s seeder didn&#x27;t just sit there and enter 500 unique phrases until they hit one that gives a nice curve, if they had any idea about cryptography and computers whatsoever. Or even if that&#x27;s how it went down, variants with a dot at the end, capitalizing the sentence, title casing, ...The list of plausible variations one should try might never be exhaustive, but this page doing nothing but generating a sha1 hash makes it practically impossible to find you the hash even if you got the string right with correct punctuation and capitalization. The easiest&#x2F;least thing it could do is check if the first or last ten bytes match to see if the resulting hash was incremented, though even that would mostly be asking people to waste time on the page, since the author knows it won&#x27;t go anywhere.It should be remarked on the page that it is a toy for demonstration purposes and will not actually let you find the seed even if you guessed correctly reply monocasa 12 hours agoprevI mean sha-1 is for sure broken, but I thought that was mainly concerning stuff like collisions via a length extension attack and other known plaintext attacks.Finding what amounts to a passphrase just given a hash was still generally untractable I thought. reply woodruffw 12 hours agoparentYes, SHA-1 is still considered preimage resistant. But preimage resistance isn&#x27;t that important here, if the hypothesis about seed structure is correct: SHA-1 is also very fast and trivial to parallelize, and someone dedicated to exploring the permutation space of \"Jerry needs a raise\" stands a decent chance of discovering the original input. reply monocasa 10 hours agorootparentExcept the Jerry needs a raise story has been floating around for a while now. The obvious choices have been mined out. reply woodruffw 10 hours agorootparentI hadn&#x27;t heard it before. Whether or not it&#x27;s an old story is secondary to visibility&#x2F;mass awareness. reply tedunangst 10 hours agorootparentprevHow much actual effort has been spent mining this space? reply tptacek 12 hours agoparentprevIt&#x27;s a simple dictionary attack, which is how most password hashes are broken. This has really nothing to do with SHA1 itself. reply vagab0nd 11 hours agoprevHere&#x27;s professor Dan Boneh explaining the background: https:&#x2F;&#x2F;youtu.be&#x2F;8WDOpzxpnTE?t=892 reply ilc 12 hours agoprevNow, for the real question:Did Jerry get a raise? reply glompers 11 hours agoparentThey&#x27;re still in seed stage reply natch 11 hours agorootparent4d169a9023aed89e92b73edc661498dc7bb22026 reply glitchc 11 hours agorootparentprevRequires some seed funding. reply quickthrower2 9 hours agoparentprevHe got a rise out of the conspiracy people reply freedude 11 hours agoparentprevI came here for this... reply jeffrallen 3 hours agoprevI recently said:> Something I&#x27;ve learned from a career of watching cryptographer flame wars: Don&#x27;t bet against Bernstein, and don&#x27;t trust NIST.I should amend that to:Something I&#x27;ve learned from a career of watching cryptographer flame wars: Don&#x27;t bet against Bernstein or Filippo, and don&#x27;t trust NIST. When these two rules are in conflict... still don&#x27;t trust NIST. reply nembal 10 hours agoprevHas Jerry ever got the raise? (: reply tptacek 10 hours agoparentSadly, Jerry Solinas died a few months ago. reply gregw2 2 hours agorootparentNow he needs a different kind of raise?(Sorry, couldn&#x27;t help myself.) reply quickthrower2 9 hours agoparentprevNo but you might get a $12k bonus reply cushpush 9 hours agoprev [–] it&#x27;s a joke right replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A bounty of $12,000 is being offered to anyone who can decode five hashes contributing to NIST elliptic curves, prevalent in contemporary cryptography.",
      "An increase to $36,000 is promised if the recipient opts to donate the reward to charity. This is seen as an incentive to encourage more participation in solving the cryptographic problem.",
      "The initiative aims to dispel any suspicions or concerns regarding the security of the NIST curves, formed from allegedly hashed English sentences by a now-deceased researcher."
    ],
    "commentSummary": [
      "A reward is being offered for discovering the string used to generate random seeds for the NIST P-curves, a type of elliptical-curve cryptography.",
      "The debate hinges on the possibility of \"backdoors\" in cryptographic algorithms and the challenge in securely integrating them, with skepticism expressed over the NIST curves' security.",
      "Discussion also covers the origin of seeds provided by the NSA (National Security Agency) and potential alternative methods for their generation."
    ],
    "points": 345,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1696541212
  },
  {
    "id": 37778069,
    "title": "Database Performance at Scale – A free book",
    "originLink": "https://www.scylladb.com/2023/10/02/introducing-database-performance-at-scale-a-free-open-source-book/",
    "originBody": "Products DevHub Users Resources Pricing Chat Now Get Started Login Search See all blog posts Introducing “Database Performance at Scale”: A Free, Open Source Book By Dor Laor October 2, 2023 Subscribe to Receive Blog Updates Discover new ways to optimize database performance and avoid common mistakes that impact latency and throughput So many things have to align perfectly for impressive database performance. You need to think hard about factors like: The infrastructure your database sits on How it’s set up How you’re managing it How your application interacts with the driver How the driver interacts with your database How that database is designed How well its approach aligns with your specific workload characteristics and requirements And that’s just scratching the surface. Behind each of those elements, there’s likely years of engineering efforts from the database side – not to mention all the user-side strategizing, trial-and-error optimization, and high-pressure triaging. Most teams don’t have that level of time and patience. They need performance, and they want it now. Enter Database Performance at Scale, a new (free) open source book written by ScyllaDB employees and contributors Felipe Cardeneti Mendes, Piotr Sarna, Pavel Emelyanov, and Cynthia Dunlop. The authors set out to cover the many different elements that impact database performance, offering clear, practical recommendations based on their own engineering efforts and experience with thousands of real-world database deployments. Digital Download + Request Printed Book The complete 270-page book is available now, for free. Here are a few important things to note… It’s not “about” ScyllaDB – but it’s highly relevant for anyone using or considering it While this book was written by people who have been working at ScyllaDB, it’s not a book “about” ScyllaDB per se. The authors wanted to explore the topic of database performance at a broader level so the book would be relevant even beyond the ScyllaDB community. It doesn’t matter what database you’re currently using (MongoDB, MySQL, Postgres, Cassandra, DynamoDB…). If you’re experiencing some pain related to database latency and/or throughput – or you fear you will suffer soon – this is a book for you. If you’re considering or already using ScyllaDB, you’ll definitely want to look at this book. It features the collective wisdom of teams who develop ScyllaDB and guide our customers through their most complex challenges. And if you want to discuss these topics in relation to your specific use case, we’d be happy to chat more – just contact us. Also, if you are interested in a book specifically about ScyllaDB, stay tuned to our social media handles. There will be some exciting news on that front quite soon. 😉 It’s highly opinionated When ScyllaDB started working with Discord 5 years ago, one of the things they appreciated most was “the joy of opinionated systems.” In other words, “people who are good stewards of the system have already figured out what is important.” This opinionated approach is a hallmark of the Database Performance at Scale book. Whether explaining a performance-critical decision that a database user will likely face or sharing the rationale behind various engineering optimizations, the authors present their perspective on the associated tradeoffs, along with their recommendations on what’s best from the perspective of performance. It’s free, it’s open source… and it’s fun The authors, and all the engineers throughout both ScyllaDB and Turso, are avid open source supporters and contributors. When presented with the opportunity to write a book, everyone agreed that taking an “open source” approach would be the best way to make the finished product readily accessible to all interested readers. Apress, the book’s publisher, offers an “Open Access” series of books – and that’s what we selected. This means that the digital version is available free of charge, and the entire book is licensed under the terms of the Creative Commons Attribution 4.0 International License. Print editions are available at cost through all the standard booksellers, and ScyllaDB will be featuring printed books at our events (like P99 CONF) and through special book giveaways (become eligible by signing up here). Digital Download + Request Printed Book Just the PDF Please Finally, you might not expect a book on databases to be a fun read. That’s fair. But as you might have sensed by now, these authors like to do things a bit differently. Delve into Chapter 1, and see for yourself! Database Performance at Scale book About Dor Laor Dor Laor is co-founder and CEO of ScyllaDB. Previously, Dor was part of the founding team of the KVM hypervisor under Qumranet that was acquired by Red Hat. At Red Hat Dor was managing the KVM and Xen development for several years. Dor holds an MSc from the Technion and a PhD in snowboarding. View all posts by this author Previous Post Related Posts We’re Porting Our Database Drivers to Async Rust Implementing a New IO Scheduler Algorithm for Mixed Read/Write Workloads Top Mistakes with ScyllaDB: Storage Start scaling with the world's best high performance NoSQL database. Get Started 11160 PRODUCT Product Overview ScyllaDB Enterprise ScyllaDB Cloud Open Source Benchmarks Solutions Release Notes Pricing RESOURCES Documentation Online Training NoSQL Guides Resource Center Blog Events Customer Support Custom Portal COMPANY About Us Team Customers Partners News Media Kit Careers Contact Us SOCIAL GitHub LinkedIn Twitter YouTube Facebook Forums Slack LEGAL Terms of Service Privacy Policy Data Subject Request Form CCPA Privacy Notice Cookie Policy Trust Center Legal Center Contact 2023 ©ScyllaDBScyllaDB, and ScyllaDB Cloud, are registered trademarks of ScyllaDB, Inc. Apache® and Apache Cassandra® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. Amazon DynamoDB® and Dynamo Accelerator® are trademarks of Amazon.com, Inc. No endorsements by The Apache Software Foundation or Amazon.com, Inc. are implied by the use of these marks. By clicking “Accept All Cookies”, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. Cookies Settings Accept All Cookies",
    "commentLink": "https://news.ycombinator.com/item?id=37778069",
    "commentBody": "Database Performance at Scale – A free bookHacker NewspastloginDatabase Performance at Scale – A free book (scylladb.com) 321 points by asicsp 21 hours ago| hidepastfavorite108 comments Nezteb 19 hours agoDirect URL: https:&#x2F;&#x2F;link.springer.com&#x2F;content&#x2F;pdf&#x2F;10.1007&#x2F;978-1-4842-971... hintymad 13 hours agoprevI wish there are authoritative books or papers on how to build object stores like S3 or software-defined storage in general. It looks object stores in public clouds have been so successful that few companies or research groups have been working on their own. Yes, I&#x27;m aware that we have systems like MinIO and Ceph, but so many questions are left unanswered, like how to achieve practically unlimited throughput like S3 does, like what kind of hardware would be optimal for S3&#x27;s workload, like how to optimize for large-scan incurred by analytics workload, which S3 is really good at; like how to support strong consistency like S3 does without impacting system performance visibly even though S3 internally must have metadata layer, storage layer, and an index layer, like how to shrink or expand clusters without impacting user experience, like how to write an OSD that squeezes out every bit of hardware performance (vitastor claims so, but there&#x27;s not many details), and the list goes on. reply emmanueloga_ 11 hours agoparent> how to achieve practically unlimited throughput like S3 does> like what kind of hardware would be optimal for S3&#x27;s workload> how to support strong consistency like S3 does without impacting system performanceI think most of these questions are first and most importantly _hardware_, _money_ and _physics_ questions.I have no expertise in this matter, but I think this would be a good proxy answer: you make a system _seem_ to have unlimited throughput by interconnecting _a lot_ of the fastest available storage solutions with the fastest available network cards, putting it as close as possible to where your users are going to be. All of this is extremely expensive so you would need to have deep pockets, as amazon has, to make it possible (or a lot of investment).I suspect with the right hardware and physical deployment locations you could fine tune Ceph or MinIO or whatnot to similar performance as S3. S3 is an object storage so the distributed system aspects of its implementation should definitely be a lot easier than, say, distributed SQL (not saying either is an \"easy\" thing to accomplish).If you are interested in which hardware to use for a SAN, I found these benchmarks that may be exactly what you are looking for :-) [1]--1: https:&#x2F;&#x2F;www.spec.org&#x2F;storage2020&#x2F;results&#x2F; reply hintymad 10 hours agorootparent> I think most of these questions are first and most importantly _hardware_, _money_ and _physics_ questions.Actually, money (more accurately, cost) is a constraint instead of a resource. S3 is known for its low cost, and S3 can easily dote out a 70% discount to its large customers and still make a profit. So, an interesting question is how to build an low-cost object store&#x2F;> interconnecting _a lot_ of the fastest available storage\"A lot of\" leads to real challenges. Sooner or later you&#x27;ll find that managing metadata will become a tough challenge. Case in point, Open-source systems often use systems like Zookeeper or etcd or single-node name server with hot standby for metadata management, which certainly won&#x27;t be able to handle the scale of S3. reply emmanueloga_ 10 hours agorootparentAbout cost, see [1]. Also, S3 prices have been increasing and there&#x27;s been a bunch of alternative offers for object store from other companies. I think people in here (HN) comment often about increasing costs of AWS offerings.Distributed systems and consensus are inherently hard problem, but there are a lot of implementations that you can study (like Etcd that you mention, or NATS [2], which I&#x27;ve been playing with and looks super cool so far :-p) if you want to understand the internals, on top of many books and papers released.Again, I never said it was \"easy\" to build distributed systems, I just don&#x27;t think there&#x27;s any esoteric knowledge to what S3 provides.--1: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Economies_of_scale2: https:&#x2F;&#x2F;nats.io&#x2F; reply mwarkentin 9 hours agorootparentUh, reference for s3 prices going up? I’ve only ever seen them heading down over time. reply emmanueloga_ 9 hours agorootparentSorry, this is just anecdotal from my recollection of reading random hacker news threads; I think people talk more about the bandwidth being expensive more than the storage itself [1].--1: https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateEnd=1696464000&dateRange=custom&... replycatlover76 11 hours agoparentprevI haven&#x27;t used it for anything that complicated yet, but this is the kind of stuff I have found GPT-4 really useful for, personally. Research, not refactoring. reply gigatexal 11 hours agoprev“A free book” aka a book by a database vendor that wants to skew the premise of this-is-just-a-discussion-on-performance-at-scale-vendor-agnostic to that very vendor. Nothing is free. It’s all marketing. reply dacryn 30 minutes agoparentI do like the premise though of this one- here is how to optimize your database - here is why that works from a theorethical perspective - by the way, our product makes this process easierthat&#x27;s fine by me reply benjaminwootton 7 hours agoparentprevYes it’s marketing, but if you have a complex product or service offering, a short book can be a good way to get the thinking across. reply Kudotap 15 hours agoprevBeautiful. Been aiming to learn how to scale MySQL databases so I can run my apps on VMs without having to used managed dbs like Aurora or Azure Managed Database reply jamesblonde 14 hours agoparentCheck out RonDB (https:&#x2F;&#x2F;www.rondb.com&#x2F;blog) which is a fork of MySQL Cluster but has support for elasticity with no downtime, backups, etc. Disclaimer: i am involved with RonDB. reply paulryanrogers 10 hours agorootparentWhat are the tradeoffs? Guessing based on the stack it&#x27;s eventual consistency reply jamesblonde 5 hours agorootparentNo. It&#x27;s a distributed database. Transactions are committed in memory, and then (by default every second) committed as a group to disk. This gives you phenomenal write throughput and low latency, but the tradeoff is that if a whole cluster goes lights out, you could lose a second of data. It&#x27;s kind of like turning off fsync for InnoDB. reply huijzer 15 hours agoparentprevHow big are your apps? Wouldn’t the most simple and reliable way be to get one beefy MySQL server and let all the app instances connect to that? reply zeroCalories 11 hours agorootparentOne large VM won&#x27;t give you replication&#x2F;sharding, failover, or backups. reply Aeolun 10 hours agorootparentWith one large VM, for most use cases, you do not need anything but the backups. reply zeroCalories 7 hours agorootparentSure, if your workload is small and you have a SLA of \"it will be fine.\" replymindondrugs 19 hours agoprevIm not sure how this qualifies as open source when the repo for the book[0] is essentially empty?[0] https:&#x2F;&#x2F;github.com&#x2F;Apress&#x2F;db-performance-at-scale reply mdaniel 18 hours agoparentI wondered about that, too, but in the body of the article they claim the license is a Creative Commons flavor so I interpreted it as a marketingdroid faux pas that the source is not available but that the content is openI guess the distinction would be if someone wanted to upstream a change, not fork it, through what mechanism would that take place, but I&#x27;m going with \"highly unlikely\" on that one reply pipo234 18 hours agoparentprevYes, seems like the GitHhub repo was a bit of an after thought:Top right:> About > SOURCE CODE for \"Database Performance at Scale: A Practical Guide (Apress, 2023),\" by Felipe Cardeneti Mendes, Piotr Sarna, Pavel Emelyanov & Cynthia DunlopBut readme.md:> This repository ACCOMPANIES Database Performance at Scale:A Practical Guide by Felipe Cardeneti Mendes, Piotr Sarna, Pavel Emelyanov & Cynthia Dunlop (Apress, 2023).[ my emphasis ] reply iudqnolq 15 hours agorootparentIn the context of technical books \"source code for\" often means \"source code for [readers of the book to reference while reading]\" rather than \"source code for [building the book]\" reply maximinus_thrax 11 hours agorootparentI disagree. Words have meaning. &#x27;Open source&#x27; means &#x27;open source&#x27; in all contexts.For comparison, https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;book&#x2F; is an open source book. A PDF with a CC license without a repo of the publishing artifacts is not an open source book. It&#x27;s just a free book. reply iudqnolq 11 hours agorootparentThe question is not whether it&#x27;s open source, the question is what it is. replynilslindemann 18 hours agoprevThe first code example is on page 89. reply maximinus_thrax 16 hours agoprevHaven&#x27;t have time to look over it. For people who did, is this a generic &#x27;Database performance&#x27; book or a longform pamphlet for ScyllaDB ? reply biohazard__ 15 hours agoparentThis is already addressed: \"It’s not “about” ScyllaDB – but it’s highly relevant for anyone using or considering it\"It does contain a plethora of references to other databases, and it does exemplify concepts using ScyllaDB. IIRC they also made it clear in the front matter. reply maximinus_thrax 14 hours agorootparentI&#x27;m referring to the content itself, not what is advertised about it. The article also states that the book is open source, when it&#x27;s actually not, so I guess I have trust issues.But I&#x27;ll take the fact that you created an account just to write this comment as a positive signal that the book is generic and not only about ScyllaDB. reply pc86 13 hours agorootparentOther than making the PDF freely available what in your mind would need to happen to make this book open source? I&#x27;m not even sure what an \"open source\" book would be in the FOSS meaning of the phrase.From the edition notice page of the PDF:> Open Access This book is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:&#x2F;&#x2F;creativecommons.org&#x2F;licenses&#x2F;by&#x2F;4.0&#x2F;), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.And the comment you&#x27;re replying to says \"It does contain a plethora of references to other databases\" so I&#x27;m not sure the snarky \"I&#x27;m referring to the content itself, not what is advertised about it.\" is necessary or particularly helpful. reply maximinus_thrax 13 hours agorootparentNot entirely sure why everyone is so combative.> what in your mind would need to happen to make this book open sourceGitbooks is a good example of that.Open Source is easy to define. This is a PDF, so it&#x27;s the artifact of some &#x27;build&#x27; producing it. If a book is open source, I&#x27;d expect a .tex file, or whatever the input was which created the PDF, with the pictures attached or the code which draws the pictures&#x2F;graphs&#x2F;etc... If it&#x27;s a word document, I guess that also works. I&#x27;m also not the only one to call it out: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37779549> I&#x27;m not sure the snarky \"I&#x27;m referring to the content itself, not what is advertised about it.\" is necessary or particularly helpful.There is no snark here. I asked a question about the content of the book. I got an answer from the article talking about the book, which I already read. reply pc86 13 hours agorootparentPerhaps \"Open Access\" as used on the edition notice is more technically accurate but I&#x27;m not seeing how getting any \"build\" assets to generate this PDF is any better or more free&#x2F;open than having the PDF and being able to do whatever you want to it within the rules of attribution. reply maximinus_thrax 13 hours agorootparentIs the book open source? No. End of discussion. What&#x27;s better or worse or useful or whatever is orthogonal. This is not an open source book.Edit:Ok, I downloaded it. Check page 79. I want to change that diagram, I don&#x27;t like the client icons. I want o open a PR against the book. How do I do it? reply strken 6 hours agorootparentSQLite is undeniably open source yet the general public can&#x27;t contribute code to it, so I&#x27;m not sure why that&#x27;s meant to be the defining feature in the case of the book.The earlier request for a .tex source file was on the mark, but accepting contributions has nothing to do with it. replyuberdru 11 hours agoparentprevScyllaDB is exceptionally good (and unique) as a company in that they virtually ban marketing for its own sake. I&#x27;d point you to their &#x27;vendor agnostic&#x27; p99 conference as well. Among the most technically credible technology companies of all time. reply colesantiago 18 hours agoprevWhy don’t more companies &#x2F; startups choose ScyllaDB rather than Postgres or MySQL?ScyllaDB is a C++ version of Cassandra so it looks like speed and scalability is a complete advantage over the Java based Cassandra and Discord is using ScyllaDB at scale too. reply erulabs 17 hours agoparentBecause Postgres and MySQL are more familiar and by the time they&#x27;re falling over, you have raised 80M and can afford to hire folks like me who&#x27;s entire job is pestering developers to stop reading-after-writing, to stop forcing queries to the primary (just incase there&#x27;s lag!), to stop \"select id from table\" -> \"select data_I_want from other_table where ID IN (list-of-a-million-ids\"), to beg AWS for early access to the fastest possible machines, to systematically destroy the custom ORM you built during the first 5 years of the company, before you had customers.Scylla&#x2F;Cassandra is exactly the database you want when you&#x27;re serving the entire planet.Postgres is exactly the database you want when you&#x27;re desperately searching for product market-fit and just need things to work and code to get written.Turns out it&#x27;s much better to start simple and face scaling problems maybe than start complex and face scaling problems maybe. reply WesleyJohnson 16 hours agorootparentCurious how you approach business units asking for reports that require multiple joins and can \"filter by any column\" and \"sort by any column\" and also offer pagination and then complain about why it&#x27;s so slow? This is MySQL by the way.The sorting by any column and pagination is really killer. Can&#x27;t do cursor-based pagination so you get LIMIT&#x2F;OFFSET which is terrible for performance, at least in this context. Indexes are often useless in this circumstance as well due to the filter by any column. We get lucky sometimes with a hard-coded WHERE clause as baseline filter for the entire report that hits an index, but not always. Just for fun, add in plenty of LIKE &#x27;%X%&#x27; because we limiting them to STARTS WITH is out of the question; it must be CONTAINS.It&#x27;s a constant source of frustration for the business and for the dev team. reply stvltvs 16 hours agorootparentSounds like the job for a denormalized OLAP data structure (e.g. star schema) with a BI tool. This is the kind of use case they were made for. In other words, you&#x27;ve rediscovered the motivation for data warehouses. reply WesleyJohnson 13 hours agorootparentThank you for the feedback. I&#x27;ll do some research along these lines. We have moved a report or two to Power BI which has been must faster than similar reports built using dynamic queries. Keeping the data in sync and the Power BI learning curve have not been easy. reply pbowyer 16 hours agorootparentprevIn our case we switched to holding that data in Elasticsearch. Now we have 2 problems: running Elasticsearch and keeping the data in Elasticsearch in sync with MySQL. We took an application-level approach to triggering record sync to ES but somewhere in our ORM it&#x27;s doing some magic and doesn&#x27;t sync in certain places. Database triggers are the next stop.If we could do away with ES and go back to MySQL for what are glorified data grids with 20+ column filtering, sorting, and no full text requirements, we would. reply WesleyJohnson 13 hours agorootparentWe use ElasticSearch in a very limited capacity for keyword searches against consumer product metadata. We haven&#x27;t ventured into using it for BI reporting, but I&#x27;m interested. I&#x27;m sure there is plenty of knowledge I can google, but any early insights or traps to avoid when moving from SQL-based reporting to something in ES? reply erulabs 5 hours agorootparentprevIf you want to sort by any column and paginate on any column (so you can have your cursor), a BTREE (MySQL index) is sort of the wrong structure in the first place. You probably want a document store for something like this. Scylla!More pragmatically, just force bookmarked scans - limit the columns that can be sorted by - use a replica with indexes that the writer doesn’t have - pay for fast disks - use one of the new parallel query engines like Aurora or Vitess - just tell the business people to wait a dang minute, computer is thinking! reply mr_toad 7 hours agorootparentprevThe lazy approach is to denormalise the data in advance and load the result into memory. If it’s small enough you can simply return the entire result set to a reporting tool and query it locally. reply isoprophlex 15 hours agorootparentprev> systematically destroy the custom ORM you built during the first 5 years of the company, before you had customers.Ouch, haha, that hurt reply osigurdson 5 hours agorootparentprev>> Because Postgres and MySQL are more familiarIf familiarity is the primary problem, it should be relatively easy to fix. reply gajus 17 hours agorootparentprevWhat&#x27;s wrong with \"select id from table\"? (presumably \"where\" skipped for brevity of the example) reply dragonwriter 17 hours agorootparentWhat&#x27;s wrong is replacing a single query with a join with multiple round trips implementing part of the join logic in application code because you are using an RDBMS as if it were a fairly dumb repository of dusconnected tables. SELECT id FROM table1Is fine, if what your app wants is the ids from table1. Its not good if it is just a prelude to: SELECT * FROM table2 WHERE table1_id in (...ids from previous query...)instead of just doing: SELECT table2.* FROM table1 INNER JOIN table2 ON ( table2.table1_id == table1.id ) reply osigurdson 5 hours agorootparentWow, it is hard to believe anyone would use the first option. reply muzaffarpur 16 hours agorootparentprevMost of the query optimisers would automatically convert subqueries into join. reply dragonwriter 15 hours agorootparentNo query optimizer will automatically convert to separate queries connected by application logic that the optimizer can’t see shuttling data between them into a single join. reply stvltvs 16 hours agorootparentprevNot if the dev has removed the context of which table the id values come from. reply muzaffarpur 15 hours agorootparentCan you explain bit further. If the subquery do not have context where the value comes from, I would think query will error out? reply josephg 14 hours agorootparentLots of application developers don’t really understand how sql works. So instead of learning to do a join (or do a sub query or anything like that) they just issue multiple queries from their application backend.The posters up thread are talking about novice Python &#x2F; JavaScript &#x2F; whatever code which issues queries in sequence: first a query to get the IDs. Then once that query comes back to the application, the IDs are passed back to the database for a single query of a different table.The query optimizer can’t help here because it doesn’t know what the IDs are used for. It just sees a standalone query for a bunch of IDs. Then later, an unrelated query which happens to use those same IDs to query a different table. reply 10000truths 14 hours agorootparentprevWhat you&#x27;re saying is that: SELECT * FROM table2 WHERE table1_id in (SELECT ids FROM table1 WHERE ...)will be optimized to a join by the query planner (which may or may not be true, depending on how confident you are in the stability of the implementation details of your RDBMS&#x27;s query planner). But in most circumstances, there is no subquery, it&#x27;s more like: SELECT * FROM table2 WHERE table1_id in (452345, 529872, 120395, ...)Where the list of IDs are fetched from an API call to some microservice, or from a poorly used&#x2F;designed ORM library. reply mr_toad 7 hours agorootparentWhat’s really bad is where they (or the ORM) generate a seperate select statement for each ID in a loop. Selecting thousands of rows one row at a time from an unindexed table can be hilariously slow. replyCBLT 17 hours agorootparentprevIt looks like what they&#x27;re describing is implementing database joins in the application: 2 network round trips, first one for ids from a table and the second to get values from another table with foreign keys corresponding to the ids. reply WanderPanda 17 hours agorootparentIsn‘t implementing joins in the application what microservices are all about? reply macintux 11 hours agorootparentI have supported a microservices environment where each service had its own database, and indeed \"foreign keys\" were mostly application-level concerns. A real pain, and I don&#x27;t think any of us would have chosen that approach in hindsight.I hope there are better ways to design microservices. reply _jal 15 hours agorootparentprevNo, microservices are all about senior management&#x27;s ability to map applications to people to yell at. reply nicoburns 13 hours agorootparentprevYou&#x27;re getting confused with NoSQL ;) reply cogman10 17 hours agorootparentprevIDK if postgres has this problem but mssql does. If you do \"Select * from table where ID in (blah)\"it ends up with some pretty terrible performance.It&#x27;s often a lot faster to do something likeInsert into #tempTable idsselect from table JOIN #tempTable t on t.id = table.id reply WanderPanda 17 hours agorootparentI can‘t believe this is a thing in 2023! If this is faster, shouldn‘t the database do that transparently in the background?! reply cogman10 15 hours agorootparentAt least in our case, it comes down to expectations.For the `IN` query the DB doesn&#x27;t know how many elements are in the list. In MSSQL, it would default to assuming \"well, probably a short list\" which blows out performance when that&#x27;s not the case.When you first insert into the temp table, the DB can reasonably say \"Oh, this table has n elements\" and switch the query plan accordingly.In addition, you can throw an index on the temp table which can also improve performance. Assuming the table you are querying against is indexed on ID, when you have another table with IDs that are indexed it doesn&#x27;t have to assume random access as it pulls out each id. (Effectively, it just has to navigate the tree nodes in order rather than needing to do a full look into the tree). reply pritambaral 17 hours agorootparentprevI read it as \"`where` skipped at code time because the table is never going to be too large\". reply VirusNewbie 16 hours agorootparentprevI think it depends on the problem. I&#x27;ve worked at a tiny telco (choose ScyllaDB rather than Postgres or MySQL?I&#x27;ve worked with a large Cassandra cluster at a pervious job, was involved with a Scylla deployment, and have a lot of experience with the architecture and operations of both databases.I wouldn&#x27;t consider Cassandra&#x2F;Scylla a replacement for Postgres&#x2F;MySQL unless you have a very specific problem, namely you need a highly available architecture that must sustain a high write throughput. Plenty of companies have this problem and Scylla is a great product, but choosing it when you are just starting out, or if you are unsure you need it will hurt. You lose a lot of flexibility, and if you model your data incorrectly or you need it in a way you didn&#x27;t foresee most of the advantages of Cassandra become disadvantages. reply dacryn 29 minutes agoparentprevecosystem integrationPostgres is super well supported, basically every tool out there has a connector to postgres, and most people have familiarityIf I am honest, I never even bother researching alternatives, I always go with postgres out of habit. I know it, and it has never let me down reply bastawhiz 16 hours agoparentprevThere are lots of tradeoffs when choosing Cassandra:1. If you delete data, tombstones are a problem that nobody wants to have to think about.2. Denormalizing data instead of joining isn&#x27;t always practical or feasible.3. If you&#x27;re not dealing with large amounts of data, Postgres and MySQL are just easier. There&#x27;s less headache and it&#x27;s easier to build against them.4. One of the big advertised strengths of Cassandra is handling high write volume. Many applications have relatively few writes and large amounts of reads. reply pmcf 14 hours agorootparentFast writes and slow reads was true in 2012. The project has been busy since. reply olivermuty 18 hours agoparentprevBecause I (we) want rigid schemas with transaction wrapped DDL migrations more than I want blazing speed at Discords scale. reply michaelmior 17 hours agorootparentYou can have a rigid schema with Cassandra&#x2F;ScyllaDB. Transactions are a whole other thing though for sure. reply riku_iki 16 hours agorootparentprevPlus complex queries with rich capabilities reply nzach 17 hours agoparentprevThere are a lot of factors at play. But I think the biggest one is lack of expertise in the community. For postgress isn&#x27;t that hard to hire a professional with 10 years of real world experience.Besides that you need to keep in mind that scylla isn&#x27;t a silver bullet. There are some workloads that it can&#x27;t handle very well. Where I work we tried to use it in workload with high read and writes and it had trouble to keep up.In the end we switched back to postgres + redis because it was cheaper. reply esafak 15 hours agorootparentCan you be more specific? I am surprised Scylla got overwhelmed; it powers Discord. reply jeffchao 10 hours agoparentprevProbably the ecosystem and documentation. PaaS like Render, Fly, and even Heroku from back in the day offer addons&#x2F;plugins&#x2F;whatever that make the obvious choice out to be Postgres&#x2F;MySQL. Then there&#x27;s other tooling such as migration and various ORM adapters. Then there&#x27;s a wealth of blogs, stack overflow q&a&#x27;s, etc. And then Postgres&#x2F;MySQL gets you quite far. Start with a single node -> scale the node -> add read replicas -> shard. Then, sharding becomes painful? Start looking at other data stores, remodeling your data, or eventing, etc. reply pid-1 16 hours agoparentprevWhy would use a database that is closely linked to a VC backed startup when PostgreSQL works fine you? reply jskrablin 12 hours agoparentprevBecause SQL and relational data. They&#x27;re (MySQL and Pg) a lot more familiar to setup and maintain. There&#x27;s a lot of managed services offering for the usual SQL databases.Cassandra&#x2F;Scylla come with a lot of limitations when compared to typical SQL DB. They were developed for specific use cases and aren&#x27;t at all comparable to SQL DBs. Both are completely different beast compared to standard SQL DBs. reply dangoodmanUT 15 hours agoparentprevTransactions mean you can build faster as you don&#x27;t have to write a bunch of code to achieve those features reply joeatwork 18 hours agoparentprevThere are a bunch of common workloads (including i’m guessing HN itself) where the trade offs for distributed databases make them a lot harder to use well, or where distributed databases don’t make sense. reply whynotmaybe 17 hours agorootparentGovernment agencies I&#x27;ve worked usually build an app and put it in production without very few evolutions for 15+ years.Data is the most important stuff for them and being able to read it is therefore very important.A nosql DB structure can usually only be read by the code that goes with it and if nobody understands the code you&#x27;re doomed.In 2017, I had to extract data from an app from 1994. The app was built with a language that wasn&#x27;t widely used at the time and had disappeared today. It disappeared before all the software companies put their doc on the internet. You can&#x27;t run it on anything older than windows2k. There was only one guy in Montréal that knew how to code and run that thing.As the DB engine was relational with a tool like sqlplus, I was able to extract all the data and we rebuilt the app based on the structure we found in the database.If you want to do that with Nosql, you must maintain a separate documentation that explains your structure... And we all now how dev like documentation. reply pmcf 14 hours agoparentprevThat speed thing is a nice bit of marketing from Scylla but not really true in the real world. reply renegade-otter 16 hours agoprevIs it just me, or is the basic knowledge of database normalization and indexing kind of a lost art these days?How many teams out there just \"add cache and more hardware\" when no one ran an EXPLAIN on that one core query in a giant table?Perhaps we should do that before talking about anything \"at scale\". reply sb8244 16 hours agoparentI wrote a system that used the database as essentially a rules engine for \"what work to execute next?\" There were constraints about max throughput per key, waiting periods, etc.That system broke me down and built me back up over time. Really deep explaining, understanding the internals of how the data is actually being processed, then finding ways to improve it. I had 1-2 other people who were really interested in postgres that would help work on debugging.We would continuously get 100x query improvements every time we dove in. Including after we thought we maxed it out.I hope everyone gets the chance to work on something like that. It was really fun. reply renegade-otter 16 hours agorootparentIf anyone starts a new side project, do yourself a favor - write your data access layer in straight SQL. With minimal wiring and safe variable substitution that most libraries provide, you will learn a lot. reply derefr 16 hours agorootparentprev> a rules engine for \"what work to execute next?\"I&#x27;m afraid to inform you, that you have built a cluster workload manager.(Where a \"cluster workload manager\" is a type of job scheduler designed for — usually HPC — clusters, that prioritizes among ongoing abstract batch \"jobs\" that can each broken down into more granular atomic scheduleable work-units; and which accounts the often-unpredictable resource-consumption outcomes of those work-units, to the account of the job and its owning user+group+org, to then weight further executions of that job or other jobs owned by that user+group+org, so that ultimately, jobs can proceed at a QoS&#x2F;rate that&#x27;s 1. \"fair\", and 2. in proportion to the amount that a given customer is willing to pay for prioritization. \"Business rules\" in such a system, are expressed in terms of conditional coefficients or formulae to apply to the \"costs\" used to re-weight jobs.)Examples of cluster workload management software:• Slurm (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Slurm_Workload_Manager)• IBM Spectrum LSF (https:&#x2F;&#x2F;www.ibm.com&#x2F;products&#x2F;hpc-workload-management)• technically, Kubernetes&#x27; kube-scheduler — though it&#x27;s the most trivial possible implementation of one, without any of the ongoing-fairness-of-QoS logic.The key thing to know about these, is that it&#x27;s nearly impossible to build one that lives \"quiesced\" inside a database (as you&#x27;ve probably found.) Imagine taking an OS process scheduler and trying to keep the state for that canonically inside a database. Then imagine multiplying the write load by 1000x, while keeping the realtime read requirements the same. It&#x27;s just a bad idea.You can certainly persist the state of these systems into a DB — but what you really want is one or more active scheduler nodes, that keep a big in-memory ledger matrix, that can be mutated moment-to-moment, without needing to persist every single change to disk in some WAL&#x2F;journal&#x2F;etc. Like a regular OS scheduler. Or like, say, a game physics engine — very similar in how it&#x27;s all about tiny little \"integration\" steps, many times per second. reply sb8244 16 hours agorootparentI was generic in how I described it because I didn&#x27;t want to talk about the system (it doesn&#x27;t matter).The system was for a very specific use case and the set of rules were all specific to that domain. The domain was not related to workload management or jobs, but you could generalize what it did in those terms.edit: removed some snark. I did want to rewrite this into an in-memory data structure (Elixir-based). It generally worked but the postgres solution was actually really really good for the domain it was built for. reply derefr 15 hours agorootparentMy point was that — for just your use-case as originally stated — you probably could have taken a FOSS cluster workload manager like SLURM and \"narrowed\" it to your domain + use-case; rather than starting from a DBMS and adding all the stuff necessary to make it understand how to be a scheduler and then fighting with it to scale its use-case-impedance-mismatched state-storage backend to your production workload. (Where I assume you were \"fighting with it\" because, as you said, you were \"torn down and built back up\" in the process of making this system scale.)If, however, you had other database-related needs you didn&#x27;t mention — e.g. being able to arbitrarily join the state data of this system as dimension data in OLAP queries on other, non-scheduler-y tables in that DB — then your constraints probably did indeed make using a DBMS the best choice here, despite the scaling challenges.(Also, funny enough, I am myself in the middle of writing a cluster-workload-manager as a custom in-memory Elixir daemon! But that&#x27;s with the unique constraint of needing lightweight, realtime prioritization of workloads, each backing synchronous HTTP RPC requests. Having millions of teeny-tiny jobs that are usually over in milliseconds [but which can sometimes run for multiple minutes], isn&#x27;t something any HPC-targeting workload manager is really architected for.) reply sb8244 15 hours agorootparentI don&#x27;t think that would&#x27;ve been a good idea. I guess it&#x27;s possible, but I&#x27;d rather trade known obstacles versus unknown obstacles (that will definitely exist).I said \"torn down and built back up\" in an endearing way. Sure it was difficult work at times, but it was super fun and we would spend 2 days at a time fixing it until something popped up 6 months later.But the amount of stuff I learned from that was (I will wager) exponentially more useful than figuring out how to adapt&#x2F;run a fairly major FOSS. reply PTOB 16 hours agorootparentprevI would slaver at mouth over that opportunity. reply lowercased 16 hours agoparentprevI&#x27;ve worked in a couple of projects where the existing teams were converting some API calls to new tech and changing UI to accomodate the &#x27;new way&#x27; of getting data because \"the current is way too slow\". And it was. Going to a common dashboard was taking... 45-50 seconds, and growing.Core was they were pulling all data - 20k records - in to a js datatable.\"This is bad - we have to move to server side paging!\"They&#x27;d convinced their clients (I&#x27;ve seen this variations of this process 5x in the past 6 years) to &#x27;upgrade&#x27;.In at least 2 cases, I rewrote a couple of the queries to just pull what was needed - id&#x2F;name&#x2F;email - instead of full ORM records with nested associations. One screen went from 45 seconds to 1 second. And this took ... 10-15 minutes of digging and rework. Yes, if your real goal is you want to learn Hotwire or some other new tech... go for it, but it feels wrong to not even investigate why the code is slow in the first place.Other project - similar - took a screen from 50 seconds to 2 seconds by reducing the query. Then got the whole thing under a second with a couple of indexes.Now, yes, this isn&#x27;t all specific to database optimization, but they feel related. This idea of \"oh, compute is cheap, devs are expensive, just throw more hardware at it!\" has to have some boundaries&#x2F;limits, but they aren&#x27;t often realized until you&#x27;re really ... in deep. reply krembo 16 hours agorootparentAs a dba that gives professional services to solve postgres performance issues, I&#x27;ve seen the same too fu*ing many times that it made my eyes bleed. reply zeroCalories 16 hours agoparentprevDatabase optimization isn&#x27;t something you&#x27;ll need running a flask app on your personal machine, you&#x27;ll only have to learn it while trying to handle tons of traffic, which I think explains why many developers are late to the party.That said, I&#x27;ve never worked on a team that didn&#x27;t carefully consider their schema from both a performance and extendability perspective. Even if the average age of your team is 23, any half decent TL will know the basics. I imagine this is only a problem at startups founded by students. reply lowercased 13 hours agorootparent> I imagine this is only a problem at startups founded by students.I think you imagine wrong.> I&#x27;ve never worked on a team that didn&#x27;t carefully consider their schema from both a performance and extendability perspective.Count yourself very lucky. reply dacryn 24 minutes agorootparentI dare to say the opposite.Students still care and should have learned these principles in uni. They tend to overengineer and focus too much on the technical challengesIn bigger companies, most IT people tend to be people who got the job because they started coding back in the day when you could get by using just common sense and gut feeling. Today we know those often lead to dark patterns and bad database design, but those people have now grown into architecture and management positions and have no clue what they are talking about.This is very prevalent in tech teams of non tech companies (e.g. manufacturing, banking, healthcare, CPGs,...) reply zeroCalories 12 hours agorootparentprevMaybe you&#x27;re the unlucky one? How does someone that doesn&#x27;t know basic system design get put in charge of designing a system? reply rapsey 7 hours agorootparentBecause people with experience expect a certain salary level. reply renegade-otter 9 hours agorootparentprevThey murdered the Leetcode interview, so they must be good at everything else &#x2F;s reply mamcx 15 hours agoparentprevWait until you see DBs with non-sensical schemas, wrong data types usage (like store dates in text), non-existent Pk&#x2F;FK&#x2F;Checks constraints, no use of VIEWS...ie: Using a RDBMS as if was a NoSql and complaint that RDBMS are bad... reply mixmastamyk 16 hours agoparentprevRunning explain is easy. Understanding the explanation is the hard part, and for some reason resources are scarce. reply gajus 19 hours agoprevI appreciate that \"Just the PDF\" actually gives me the PDF and not 10 more popups to subscribe to someones newsletter. reply cogman10 17 hours agoprev [–] > Noticing that a certain NoSQL database was recently trending on the front page of Hacker News, Patrick picked it for his backend stackI feel attacked! :D replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The new open-source book \"Database Performance at Scale,\" authored by ScyllaDB employees and contributors, provides practical advice for enhancing database performance.",
      "Although written by ScyllaDB team members, its principles apply to users of any database, covering diverse elements affecting performance.",
      "The book, which offers a unique author perspective on tradeoffs and recommendations, is available for free in digital format and can be bought in print. It's under the Creative Commons Attribution 4.0 International License."
    ],
    "commentSummary": [
      "ScyllaDB is providing a complimentary book titled \"Database Performance at Scale\" that delves into multiple aspects of database optimization and scalability.",
      "The book discusses several topics, including the difficulties of employing Elasticsearch, the advantages of denormalizing data, and considerations for selecting a database with high write capacity.",
      "It also underscores the significance of meticulous evaluation when picking a database and discusses issues that come with using a database as a primary storage for a system, highlighting the necessity for effective database query optimization."
    ],
    "points": 321,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1696510944
  },
  {
    "id": 37781862,
    "title": "HP fails to derail claims that it bricks scanners on printers when ink runs low",
    "originLink": "https://abcnews.go.com/Technology/wireStory/hp-fails-derail-claims-bricks-scanners-multifunction-printers-102286365",
    "originBody": "ABC News VIDEO LIVE SHOWS ELECTION 2024 538 LOG IN Stream on HP fails to derail claims that it bricks scanners on multifunction printers when ink runs low HP Inc. has failed to shunt aside claims in a lawsuit that it disables scanners and other functions on its multifunction printers whenever the ink runs low ByDAVID HAMILTON AP business writer August 15, 2023, 11:12 AM FILE - This Aug. 15, 2019, photo shows the HP logo on Hewlett-Packard printer ink cartridges at a store in Manchester, N.H. HP Inc. has failed to shunt aside claims in a lawsuit that it disables scanners and other functions o... Show more The Associated Press SAN FRANCISCO -- HP has failed to shunt aside class-action legal claims that it disables the scanners on its multifunction printers when their ink runs low. Though not for lack of trying. On Aug. 10, a federal judge ruled that HP Inc. must face a class-action lawsuit claiming that the company designs its “all-in-one” inkjet printers to disable scanning and faxing functions whenever a single printer ink cartridge runs low. The company had sought — for the second time — to dismiss the lawsuit on technical legal grounds. “It is well-documented that ink is not required in order to scan or to fax a document, and it is certainly possible to manufacture an all-in-one printer that scans or faxes when the device is out of ink,” the plaintiffs wrote in their complaint. “Indeed, HP designs its all-in-one printer products so they will not work without ink. Yet HP does not disclose this fact to consumers.” The lawsuit charges that HP deliberately withholds this information from consumers to boost profits from the sale of expensive ink cartridges. Color printers require four ink cartridges -- one black and a set of three cartridges in cyan, magenta and yellow for producing colors. Some will also refuse to print if one of the color cartridges is low, even in black-and-white mode. Recent Stories from ABC News Top Stories 00:36 Adnan Syed goes before Maryland court facing 'specter of reincarceration,': lawyers HP declined to comment on the issue, citing the pending litigation. The company’s court filings in the case have generally not addressed the substance of the plaintiff’s allegations. In early 2022, U.S. District Judge Beth Labson Freeman dismissed the complaint on legal grounds but did not address the lawsuit's claims. The judge allowed the plaintiffs to amend their claim and resubmit it. On Aug. 10, the judge largely rejected HP's request to dismiss the revised complaint, allowing the case to proceed. All-in-one inkjet printers generally seem like a bargain compared to the cost of separate devices with scanning, copying and fax functions. For instance, HP currently sells its all-in-one OfficeJet Pro 8034e online for just $159. But its least expensive standalone scanner, the ScanJet Pro s2, lists for $369 — more than twice the cost of the multifunction printer. Of course, only one of these devices requires printer ink. “Printer ink is wildly expensive,” Consumer Reports states in its current printer buying guide, noting that consumer ink costs can easily run more than $70 a year. Worse, a significant amount of ink is never actually used to print documents because it's consumed by printer maintenance cycles. In 2018, Consumer Reports tested hundreds of all-in-one inkjet printers and found that, when used intermittently, many models delivered less than half of their ink to printed documents. A few managed no more than 20% to 30%. HP isn't alone in facing such legal complaints. A different set of plaintiffs sued the U.S. unit of printer and camera maker Canon Inc. in 2021 for similarly handicapping its all-in-one printers without disclosure. The parties settled that case in late 2022. Terms were not disclosed. Promoted Links by Taboola Find The Best Price On Roof Repair, Installation (Free Estimates) Free Roofing EstimatesSearch Ads Ivy League Doctor: If You Have Constant Throat Phlegm Your Body Is Trying To Tell You This SaneSolution Top Vet Begs Americans: \"Stop Feeding This Meat To Your Dogs\" Ultimate Pet Nutrition Trump drops $500 million lawsuit against former attorney Michael Cohen Trump 'temporarily' drops lawsuit against former lawyer-turned-witness Michael Cohen Julia Ormond sues Harvey Weinstein saying he assaulted her; accuses CAA, Disney, Miramax of enabling The New Volvo Might Be The Ultimate Vehicle For Seniors (Especially At These Prices) Volvo XC40 Doctor Says Slimming Down After 60 Comes Down To This Dr. Kellyann New Electric SUVs Come with Tiny Price Tags (Take a Look) Electric Car Deals Top Stories Trump allegedly discussed US nuclear subs with foreign national: Sources Oct 5, 6:31 PM County agrees to $12.2M settlement with man who was jailed for drunken driving Oct 4, 6:24 PM Homeless man charged with capital murder and rape in death of a 5-year-old girl Oct 5, 4:37 PM In ousting McCarthy, GOP demotes key fundraiser and campaign cheerleader before 2024 Oct 5, 5:40 PM Cats among mammals that can fluoresce, new study finds Oct 4, 9:33 AM ABC News Live 24/7 coverage of breaking news and live events ABC News Network Privacy Policy Your US State Privacy Rights Children's Online Privacy Policy Interest-Based Ads About Nielsen Measurement Terms of Use Do Not Sell or Share My Personal Information Contact Us © 2023 ABC News",
    "commentLink": "https://news.ycombinator.com/item?id=37781862",
    "commentBody": "HP fails to derail claims that it bricks scanners on printers when ink runs lowHacker NewspastloginHP fails to derail claims that it bricks scanners on printers when ink runs low (go.com) 298 points by thunderbong 16 hours ago| hidepastfavorite157 comments sacnoradhq 15 hours agoEven if you only print things once a month, a laser printer is far less wasteful in time, money, and bullshit than ink that&#x27;s perpetually dry or consumed.Rather than an entry-level laser printer that cannot be economically repaired, a used mid-level enterprise printer is often the cheapest option long term and saves useful electronics from becoming e-waste. Office furniture rental&#x2F;repo places and secondary markets like eBay have these.In large enterprises, a printer&#x27;s cost per page tends to be inversely proportional to (increased) initial acquisition cost and availability of repair parts. Put another way, giving everyone an entry-level printer would be far more expensive in acquisition costs, per-page costs, and an inability to repair them. (hitting TCO 3 ways).I had a HP LaserJet 4 (released 1992, purchased new ~1994) until 2013 with a used JetDirect card. It had expansion options with cartridges and SIMM RAM and font options, but they weren&#x27;t strictly needed. reply usr1106 9 hours agoparentIn the 1990s HP was a great brand. Nowadays it&#x27;s a nightmare. My neighbor asked me to troubleshoot one. Obviously the whole printer cannot be used without WiFi and a connection to HP cloud nonsense. Only that the cloud and Windows driver could not agree whether the printer had been set up or not. And the neighbor could not remember their HP account. It took me 2 hours to figure out how to do a factory reset.1990 a printer was a peripheral and if you had a driver it worked. Today it&#x27;s a spyware, ink-selling nonsense. reply davisr 9 hours agorootparentThis underscores why libre software is so important: if you don&#x27;t control the software running on your devices, then somebody else does. One day, that somebody &#x2F;will&#x2F; work against your interests.Don&#x27;t let that happen. Apply the (A)GPL and put an end to the bullshit. reply jamiek88 9 hours agorootparentDidn’t the whole GNU think come about because RMS was pissed off at the locked down printer? Or is that urban legend? reply kstrauser 8 hours agorootparentIt’s real. See https:&#x2F;&#x2F;www.gnu.org&#x2F;philosophy&#x2F;rms-nyu-2001-transcript.txt reply cyberax 6 hours agorootparentprev> 1990 a printer was a peripheral and if you had a driver it worked.LOL no.Printing and scanning have ALWAYS been terrible. A fortune cookie somewhere: \"GNU is a printer driver gone horribly wrong\".I have a theory that printer drivers are somehow produced by aliens, who are hellbent on destroying the Earth&#x27;s infrastructure. reply Tao3300 4 hours agorootparentThere was a brief golden age where it wasn&#x27;t so bad, banded on both sides by dark ages. Some time in the 90s. Trying to remember the last time you could get away with just printing in blue if the black ran out. reply ben_bai 2 hours agorootparentOr printers that would accept refilled cartridges, good times. reply freetanga 2 hours agorootparentprevI still remember moving from raw printing over LPT to USB+drivers, and feeling it was a step back. How naive I was... reply orangepurple 5 hours agorootparentprevProprietary printer drivers directly contributed to the open source revolution.Richard Stallman started his journey of free and open source software development because he got triggered by proprietary drivers used by his laser printerhttps:&#x2F;&#x2F;www.gnu.org&#x2F;philosophy&#x2F;rms-nyu-2001-transcript.txt reply wolverine876 6 hours agorootparentprevBuy their corporate line of printers, which work as you expect (at least the ones I&#x27;ve seen). reply bArray 59 minutes agoparentprevI recently purchased an Epson Eco Tank printer (their budget mono without a scanner) [1], I&#x27;m quite impressed so far. The first thing I did was print 200+ pages, and it kept up well. My biggest cost now is the price of paper (as it should be). I would suggest ink jet still could be useful, especially with cheap refillable ink rather than this expensive cartridge bullshit.I remember years ago wanting to buy a new ink cartridge for my HP printer. In the shop the printer was something like £25 (printer + scanner, black + colour ink) - or - I could pay something like £20 for black and £25 for colour cartridges. I explained to the service person \"so you&#x27;re telling me, it would be cheaper for me to buy a new printer, throw the printer in the bin and just take the cartridges out?\". They answered \"yeah\". I tried to refill the ink cartridge manually with a syringe and an online guide, but never could get it to work due to their \"smart secure\" cartridge lock-ins.HP time and time again prove to be a massive problem in the printing industry. I think I have said on HN a few times that I would be very much interested in building an open source ink jet printer (including head) - it can&#x27;t be impossible.[1] https:&#x2F;&#x2F;www.epson.co.uk&#x2F;en_GB&#x2F;for-home&#x2F;ecotank reply bgirard 15 hours agoparentprevI followed this advice and it&#x27;s been great. I have a $350 laser Brother printer, my iPhone can find the printer without fiddling with drivers, it wakes up, print, goes back to sleep. Works fine even if I go months without printing. After my last &#x27;free ink forever&#x27; HP inkjet that never worked, a working printer is a blessing. reply moberley 8 hours agorootparentI generally like Brother printers but sometimes the software is less great.I have a Brother DCP-L2550DW which is very good but for some reason the scan to PC function no longer works over the network. Printing still works as expected in all cases, but ControlCenter4 can&#x27;t communicate via the network anymore. It works fine if I connect a USB cable so I just do that when I need to scan. reply vel0city 6 hours agorootparentDid you disable SNMP? I used to have that disabled but recently control center always wants to check the status from SNMP so I ended up re-enabling it and now it works just fine again. reply esel2k 14 hours agorootparentprevWhich brother do you have &#x2F; recommend? I have been quiet happy with my OKI but the phone connection is the only letdown so far and I might change for a brother in the future. reply tivert 13 hours agorootparent> Which brother do you have &#x2F; recommend? I have been quiet happy with my OKI but the phone connection is the only letdown so far and I might change for a brother in the future.I have a brother HL-2270dw that I&#x27;ve been using for more than 10 years that I&#x27;m happy with. I don&#x27;t think they make that exact model anymore, but Wirecutter recommends the HL-2350dw (I don&#x27;t know what you get for the extra $10 to get a HL-2370dw):https:&#x2F;&#x2F;www.nytimes.com&#x2F;wirecutter&#x2F;reviews&#x2F;best-home-printer...I&#x27;ve got it hooked up to my wired network, and have never had an issue with it. I really like the duplex printing.I prefer my devices to do one thing, so I&#x27;ve avoided combo devices. reply sundvor 11 hours agorootparentCan confirm the Brother series is great. I have the HL-L2375DW.I got accused of being a shill last time I posted about it, so I&#x27;m preempting this by calling it out. :-)https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006705#37007593However it just sits there asleep at an extremely low wattage most of the time, then pops instantly to life the moment a page is sent to it - as bgirard pointed out, without even needing to install a driver - and then back to sleep. And this is on wifi.Loved it enough to buy another one when the first one went to my ex.I&#x27;ll use the full duplex occasionally, it&#x27;s worth having years down the track from purchase time for the tiny price bump over the base. reply exodust 8 hours agorootparentTo be fair on your accuser, it&#x27;s only been 60 days since your last Brother Laser promotional post. Last time you included details about price, and a note about \"supporting Brother\" by not using third party toner.I own an Epson eco-tank inkjet and happy with it, but I&#x27;m not invested in convincing others about it. I wouldn&#x27;t put a laser printer in my house. Regardless of whether it&#x27;s actually a proven health risk, I don&#x27;t want my printer emitting tiny particles. And I want the option of good quality colour prints using different paper types and thickness. reply addaon 10 hours agorootparentprevHL-L2350DW checking in here. Zero complaints, zero issues. Just works. reply karolist 4 hours agorootparentAnother happy HL-L2350DW user. So happy in fact, I&#x27;ve bought one for my mom and one for her aunt and they too have zero issues and I&#x27;ve never had to hear about it again after setting it up. This is how printers should be. reply jmspring 5 hours agorootparentprevSame reply B1FF_PSUVM 9 hours agorootparentprevSame. reply bgirard 13 hours agorootparentprevFor me, I ended up with the MFC-L3770CDW because there was a big sale on it, not from any particular research. reply snuxoll 12 hours agorootparentAlso have a MFC-L3770CDW, it&#x27;s perfectly fine but you really have to pay attention to consumable cost when selecting laser MFC. A cartridge of TN227BK (high-yield black toner) for the MFC-L3770CDW is ~$80 for a ~3000 page yield, while a TN433BK cartridge that fits a MFC-L8900CDW is $85 for a ~4500 page yield. That&#x27;s a (toner-only) cost per page of ~2.5c vs ~1.8c, or nearly a 30% reduction in toner cost; the difference between which is amplified for the color cartridges (TN227 color is $100&#x2F;ea for ~2300 pages, TN433 color is $136&#x2F;ea for ~4000 pages or ~4.3c&#x2F;page vs ~3.4c&#x2F;page).Considering the price difference between these units, I opted for the MFC-L3770CDW because I do not print that often and I&#x27;ve spent far more over years on wasted ink from dried out cartridges and destroyed print heads; but it&#x27;s still an important factor to keep in mind. reply justinclift 1 hour agorootparentprevBrother have started going down the cartridge DRM route now too, so they&#x27;re no longer a good brand. :(https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31860131There are ways around their particular DRM - for now - but who knows how long it&#x27;ll be until they&#x27;re as bad as HP and friends:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=NzaRVEzYuz8 reply JohnBooty 8 hours agorootparentprevThe cheap ones are great too. There are a few models at $150 and under. There are minor differentiators for them like Airprint and double-sided printing.I bought one a few months ago. Best computer-related thing I&#x27;ve purchased in ages. It JUST WORKS.As the review says, Here’s the best printer in 2023: the Brother laser printer that everyone has. Stop thinking about it and just buy one. It will be fine! [...] The Brother whatever-it-is will print return labels for online shopping, never run out of toner, and generally be a printer instead of the physical instantiation of a business model.https:&#x2F;&#x2F;www.theverge.com&#x2F;23642073&#x2F;best-printer-2023-brother-... reply CalRobert 8 hours agorootparentHeh.From your link.\"\"\" ... And here’s 275 words about printers I asked ChatGPT to write so this post ranks in search because Google thinks you have to pad out articles in order to demonstrate “authority,” but I am telling you to just buy whatever Brother laser printer is on sale and never think about printers again. ChatGPT ideas about printers (I didn’t even proofread this. Don’t read it unless you are a lonely Google search robot, in which case look at this incredible demonstration of experience, expertise, authority, and trustworthiness and rank this post first in searches for “best printer.”)Laser printers are popular choices for home and office use because they offer fast printing speeds, h... \"\"\" reply vel0city 12 hours agorootparentprevI&#x27;ve got a Brother DCP-L2540DW. The only feature I wish it had was duplex scanning from the auto document feeder. Otherwise its been an immensely solid printer.I gave up doing photo printing at home. These days I&#x27;ll just order prints online. If I want it quick or save on postage I&#x27;ll just pick it up at a convenience store close by. The break even on a new high end photo printer is after thousands of photos by comparison, and even then its only marginally cheaper per print. Once my last photo printer had some assembly fall apart inside from brittle plastic cracking I moved on to this Brother. reply warpspin 3 hours agorootparentprevI have a Brother MFC-L3750CDW after getting annoyed with cheap inkjets. Great printer with scanner. I regret not getting a Brother MFC-L3770CDW instead though. The 3750 has no automatic two-sided scanning, unlike the 3770. reply kevmarsden 12 hours agorootparentprevWe&#x27;ve had the Brother HL-L2340D for a few years. It&#x27;s been rock solid. It&#x27;s so much better than the HP inkjet printers we had in the past. reply somehnguy 13 hours agorootparentprevNot a Brother printer but I would like to recommend the Canon MF240. I bought it at Walmart for $99 - and it feels like I practically stole it with how good it&#x27;s been to me. Has both Wifi & ethernet, automatically wakes when sent a print job, scanner works great, etc. Works perfect with generic toner cartridges from Amazon, no funny business with DRM.Canon in general have been good printers in my experience. I recently bought one of their low end inkjets because I needed to print some photos. That printer also works great & has a highly customizable web interface built in - I didn&#x27;t expect it to be that feature complete for $45.I&#x27;ve had 2 brother printers over the years & they were fine - for whatever reason though one of them would refuse to print from a Chromebook. Every other printer I&#x27;ve used in the last 10 years has essentially &#x27;just worked&#x27; with any device capable of speaking to it except this Brother. Known issue & they never released a firmware update to fix it. reply vel0city 6 hours agorootparentCanon business printers and scanners are also super solid. Every business line Canon I&#x27;ve encountered has been great. reply davchana 7 hours agorootparentprevBought the same in 2020 Black Friday for $49, and intro toner had about 1000 pages, lasting me for about 2.5 years. Then I bought a full toner for $49, going great. reply peterleiser 10 hours agorootparentprevI bought a new Brother HL-L8360CDWT color laser printer in September 2017 for $349.99 and it still works great after 6 years of heavy use. I am extremely impressed with this printer. I used to run it with cheap 3rd party cartridges, which worked fine, but this year I finally had to break down and buy official ones.I also have a Canon MF6160dw black and white laser printer, scanner, copier, fax that is a bit older and it&#x27;s had even more use. It&#x27;s great, and I use it to scan to my Synology NAS all the time as well.I&#x27;m very, very happy with these two printers. reply ph4te 9 hours agorootparentprevI’ve only have two printers in the past 13 years, a Brother MFC-7840w, which worked perfectly and I only gave away because my family wanted a color printer. The upgrade was an HL-2170CDW. I have no complaints with either. I’ve even gone through my starter toners on the color laser and bought a knockoff set for 70$ which included two black toners with all the colors, and have been quite surprised at the quality. Both have had wireless support and print from phones without issues. reply civilitty 9 hours agorootparent> HL-2170CDWI can vouch for the HL-2170DW (B&W only version) as well. Costco has been selling it for years.The only downside is that it takes some trial and error to figure out which instructions actually reset the cartridge EEPROMs reply leetrout 11 hours agorootparentprevI will never buy another Brother device. In rural areas you cannot find a service center and they are not self serviceable.I paid $350 for a multifunction multicolor device like others are mentioning and it lasted 13-15 months before failing to power on and Brother refusing to help make it right. There was a rash of these models having this issue where it just bricks and wont turn back on so maybe they learned but they lost all goodwill with me.Happily printing on a Canon now. reply throw9away6 9 hours agorootparentThe price range here is in the toss it if it breaks category reply politelemon 14 hours agorootparentprevI don&#x27;t think you can go wrong with any Brother (or laser printer) as long as you don&#x27;t need color printing. I have a DCP 1612W only for light printing. It works with Ubuntu, no special drivers required. I&#x27;ve had it for a few years now and I&#x27;m still on the toner the printer came with.If you need color printing, it tends to be expensive. reply Scoring6931 3 hours agorootparentI have one of the same model. WiFi connection doesn’t seem to work. I’ve tried every troubleshooting article I could find, to the extent of setting a static IP for it in my router. Otherwise a great printer, but that issue did leave a bad taste in my mouth reply mozman 10 hours agorootparentprevBrother 9130 is great reply yourusername 2 hours agoparentprev>Rather than an entry-level laser printer that cannot be economically repaired, a used mid-level enterprise printer is often the cheapest option long term and saves useful electronics from becoming e-waste. Office furniture rental&#x2F;repo places and secondary markets like eBay have these.My print volume is extremely low, my 70 euro Samsung has lasted me over a decade. I suspect i&#x27;ll be dead before i see any savings on a mid level enterprise printer, assuming the thing lasts decades without the rubber inside it disintegrating. If i have to service it my grandchildren will have to reap those savings.I decided to go completely printerless. So far i&#x27;ve gone about a year without printing anything. I don&#x27;t think there is anything in my life that still requires a printer. reply jdthedisciple 1 hour agorootparentYou must be lucky.Some places are still cursed with enough antiquated forms of red tape that it becomes impossible to do so... reply poulpy123 47 minutes agoparentprevI bought a first price laser printer few years ago, it was twice the price of a inkjet printer with the same quality, and B&W only but it paid itself by having more ink in the package and not having ink drying when I&#x27;m not using it reply djaychela 13 hours agoparentprevAbsolutely. I have a laserjet 4000 I&#x27;ve had since I was given it in 2002 at an office clearance. It&#x27;s printed about 10000 pages while I&#x27;ve had it,and in that time I&#x27;ve had to change the toner once. And I was given two original HP cartridges with the printer,and installed the first one a full 17 years after getting it.Worked perfectly,and is still in there.Works with windows, mac os, chromebooks without issue, all over the network. I even got a duplexer from gumtree a few years ago as it was going for nothing. One day it will die and then I&#x27;ll have to deal with all the current printer grief! reply ryandrake 12 hours agorootparentRIP my laserjet 4000. So many plastic pieces became brittle and finally broke, but it kept on working. Finally a few things went that could not be readily replaced&#x2F;repaired and I finally gave up on it. It was a sad day. Of today&#x27;s options, Brother is fine, but nothing compares to the GOAT. reply 2muchcoffeeman 11 hours agorootparentprevHP laser keys are still difficult to replace the transfer belt.Go brother if you want a laser. All the consumables are easily replaceable. reply cptskippy 11 hours agorootparentThere&#x27;s no belt in a Laser Jet, at least not modern ones. reply 2muchcoffeeman 6 hours agorootparentOh? What’s modern? reply temporallobe 12 hours agoparentprevThis. About 5 years ago when my wife started her doctoral program, she started printing a ton of resources (and still does). We were constantly buying expensive ink monthly. I did some research and bought her a Brother laser printer for something like $300 and never looked back. You do have to buy toner about once a year, but it’s far more cost effective and efficient. Now if I could somehow convince her to use PDFs instead! reply Johnny555 6 hours agoparentprev>Even if you only print things once a month, a laser printer is far less wasteful in time, money, and bullshit than ink that&#x27;s perpetually dry or consumedEspecially if you only print things once a month a laser printer is better, otherwise as you said your ink will dry out or you&#x27;ll consume more ink in cleaning cycles than in printing.My wife has an office size inkjet printer that she uses nearly daily (prints ~500 - 1000 pages&#x2F;month for work) and it works flawlessly, ink is cheaper than toner for her old laser printer. If she was only printing once a month, I&#x27;m sure the inkjet would be less reliable or economical. reply jojohohanon 8 hours agoparentprevI swear by the lowest end brother duplexing monochrome laser printer. Mine is the dw2270 which wasthe printers themselves don’t last very longA high quality citation is seriously needed for a claim like that.That’s not a complaint I’ve ever heard about the EcoTank printers, and googling it didn’t turn up anything. I strongly doubt this is a characteristic of EcoTank printers. That unsourced claim sounds suspiciously convenient for your side of the discussion.Epson also makes a dedicated line of business EcoTank printers (\"EcoTank Pro\") which are clearly intended to print very large numbers of pages. But again, even the very basic ones seem to do a great job, with very high reviews across the board when I look at retailers. reply Retric 8 hours agorootparentBy not lasting very long I mean I’ve seen 3 die between 2-3 years of heavy use. I have heard they last longer with moderate use, but that suggests a lifetime page limit.Honestly, I don’t have high quality sources which is why I was interested in the first place. reply coder543 8 hours agorootparentIf you&#x27;ve seen it first hand, then fair enough. It just doesn&#x27;t agree with the consensus I have seen online, so I find that confusing.As I&#x27;ve said previously, I have the most basic laser printer at home, and rarely use it. I just happen to have done a ton of research into printers lately because I&#x27;ve thought about getting a photo printer, and I really like what the ET-8550 has to offer. But, for the time being, I&#x27;ve decided I probably wouldn&#x27;t use it enough. reply Retric 8 hours agorootparentCool, I don’t have a large sample size so I could easily be wrong here. Something I just came across is various models have different monthly page expectations for their 2 year of ink promotion:“Based on average monthly document print volumes of about 150 pages (ST-C2100), 200 pages (ST-C4100), 300 pages (ST-M1000&#x2F;ST-M3000&#x2F;ST-C5000&#x2F;ST-C5500&#x2F;ST-C8000&#x2F;ST-C8090) and 750 pages (WF-M5799 Supertank). Promo applies to ink only. Printer covered by Epson 2-year limited warranty.” https:&#x2F;&#x2F;epson.com&#x2F;supertank-ink-tank-printers-for-businessI don’t remember the model numbers but those where way past 750 pages per month if that’s helpful. reply coder543 7 hours agorootparentI would assume a free ink promotion would explicitly choose not to cover heavy users, but it&#x27;s not necessarily an indication of what they think the printers are capable of.But, it is an interesting set of numbers, and could indicate something. reply vel0city 6 hours agorootparentprevYeah, I&#x27;d highly recommend doing a comparison to even getting online prints mailed. When I did the math on a number of prints including all material costs the savings per print from a lot of local printing services was only a few pennies per page. A few pennies per page versus a several hundred dollar upfront cost to get the same quality meant tons of prints before I broke even and even experienced any savings. replydeely3 4 hours agorootparentprevCanon Pixma series with built-in toner tanks that you can refill yourself with cheap ink. Using it for years, print everyday, sometime I print full color books. So far I spent like $15 on ink. reply userbinator 10 hours agoparentprevParts and consumables are still available for the LaserJet 4. reply solarmist 15 hours agoparentprevAgreed. I have an HP Color LaserJet Pro and the toner is expensive, but lasts forever if I don&#x27;t print and the quality is fantastic as well. Plus, they have automatic duplexing which is a must have feature for me.Newish ones like mine even support AirPrint and NFC printing. reply sacnoradhq 15 hours agorootparentBuy color toner refill kits. Melt a hole in the cartridge with a soldering iron, refill it, and seal it with aluminum tape.Better to spend $70-150 than $300-1200 on new cartridges.PS: I&#x27;m currently in the market for an 5-15 year old HP Color LaserJet but still assessing which model tends to be more reliable. There were a lot of lemon SMB Color LaserJet models. Suggestions welcomed especially by staff IT people who support these things. reply solarmist 13 hours agorootparentIt&#x27;s expensive, but like $295 to replace all the color toners (high capacity), not $1200 or $300 each. reply TheLoafOfBread 14 hours agoparentprevSame thing. I have laser Xerox B230 for printing, because constantly dried up ink, especially when you need that printer NOW, was driving me crazy. reply tinus_hn 2 hours agoparentprevBe aware of two things:Modern HP is not the HP of the LaserJet 4. The new printers, also the lasers, are crap.And second, these old printers are workhorses but they waste energy like crazy, the standby mode still uses a lot of power so unless your electricity is free it very quickly makes sense to replace them. reply JohnMakin 9 hours agoparentprevif you print things once a month go to your nearest staples and print it there for a couple bucks reply yieldcrv 11 hours agoparentprevyeah same, I ditched inkjets for a canon laserjetyou just have to value your time and then it all makes sense reply ajb 13 hours agoprevBecause of this kind of thing I haven&#x27;t actually replaced my printer, since it broke, and have been actively looking for ways to avoid doing so.- In the UK, Royal mail will now print the label for you, when you order a collection. DPD claims to do so when you drop off but the shops often turn out not to have a printer- Libraries print more cheaply than shops, neither are suitable for printing where confidentiality is required (your doc is likely to hang round on some random insecure PC)- They don&#x27;t advertise it, but some print-to-mail companies (eg CFH docmail) will do print runs down to a single copy. This is considerably cheaper than your local shop, for more than a couple of sheets at least, but has a latency of 5 days. It&#x27;s suitable for confidential prints (for most plausible threat models), due to their scale. Also, if you intended to mail the doc anyway, you can send it direct from them. reply Justsignedup 12 hours agoparentBuy a brother laser printer. Anything that&#x27;s not a laser printer is not worth it except for very very very specific use cases.Because of this laser printers are actually profitable to sell. So they don&#x27;t have to loss leader their income from you. reply perfectstorm 12 hours agorootparent+1 to Brother laser printer. i had mine for over 5 years and it still works great (i had to replace the original toner cartridge once). i recently had some smudges on printed paper and a quick chatGPT search later i was following Brother&#x27;s troubleshooting guide which helped me get rid of the smudge (essentially showed me how to clean the print header). i even bought an updated printer for my parents whose old hp laser printer died after a firmware update (there are posts about it in their support forum). reply contravariant 12 hours agorootparentprevI also find that laser printers are somewhat better at not printing. They&#x27;re less reliant on daily print jobs to keep their insides from clogging up. reply dtgriscom 9 hours agorootparentprevMy Brother HL-5170DN has been my daily driver since I bought it new. I don&#x27;t know how old it is, but the web page says \"Copyright(C) 2000-2003\". 25k pages printed, so that&#x27;s about three pages a day, every day. reply compiler-guy 11 hours agorootparentprevFirmware updates for Brother laser printers routinely brick third-party toner cartridges. Downgrading is possible only if you download old firmware from sketchy sites in languages you don&#x27;t read.They work great when they work, but you can never update your firmware. reply johnnyworker 12 hours agorootparentprevI have mine since 2006 or 2007, easily one of the most perfect purchases in my life. I used it rather extensively for a while, had to change toner then, but by now I print maybe a bunch of sheets per year, and can&#x27;t even remember the last time I had to change toner. Before that I used ink printers which broke all the time, not to mention ink getting dry.Anyone who hasn&#x27;t had the pleasure yet, there is a reason this gets brought up without fail when talking about printers. I have no idea if it&#x27;s particular to laser printers, Brother laser printers or both, because that was the last printer I bought so far, I have no other experience other than the horrors of ink printers. reply ajb 12 hours agorootparentprevCHF docmail is actually competitive with a laser on cost - if you&#x27;re posting too, of course. Which is usually the case for me - I very rarely print anything to keep. reply historyTeach123 12 hours agorootparentprevI have a Brother HL-2170W that&#x27;s almost old enough to drive. reply lock-the-spock 1 hour agoparentprevI bought an OKI colour laser printer. More an enterprise model and definitely overscaled for my use but the market of colour laser printers is rather narrow and Oki seemed the best value&#x2F;price ratio. Quality is good and the thing can print also some absurd sizes and formags.I&#x27;m also reasonably happy on quality, results, speed. Annoying is just that this model (MC532) is too rare to be in common printer libraries (notably for Linux), so for each setup I have to first visit the manufacturer site and download the drivers, and even then it&#x27;s not smooth. Nonetheless got this to work for win 11, Manjaro, Ubuntu and chromebooks. reply kibwen 13 hours agoparentprev> Libraries print more cheaply than shopsHighlighting this. My local library lets you print 100 pages per month for free (color printing counts for three pages), which is more than enough for my needs these days. reply hakfoo 5 hours agoprevOne thing I&#x27;ve noticed is that the bottom of the standalone scanner market completely disappeared.You used to be able to easily find parallel-port flatbed scanners for $50 or less, and they eventually shifted to USB. I suspect it was often used to \"fluff out\" a PC bundle-- \"Buy a $1200 AMD-K6 machine and get a scanner and terrible inkjet printer free!\" They were probably more than ample for the \"scan in this document we need a copy of\" use cases-- a few hundred DPI.While I still see there are a handful of sub-$100 scanners on the market, it&#x27;s hardly the variety you used to find. It&#x27;s like a \"compliance\" product-- being sold because they need to to fill some obligation rather than a vital market full of advancement.Also note that it&#x27;s a lot less easy to do the \"recycle quality vintage hardware\" thing for scanners than printers. Many of them are stranded with ancient drivers and often no support past WinXP or 7. (Yes, there&#x27;s things like VueScan for ongoing support, but paying for that impacts the overall math).I ended up finding a scanner I sort of liked used that did have drivers (Epson V39, I wanted something USB powered because I don&#x27;t have room for any more power bricks) but it involved way too much Googling in the electronics aisle of secondhand shops.I assume that this is because the market has basically assumed anyone not buying commercial-grade stuff will buy an all-in-one reliability nightmare that won&#x27;t scan without yellow ink. This sort of thing didn&#x27;t occur in other markets: They sold low-end VCRs and DVD players built into crappy TVs, but they also still sold standalone ones. reply akrotkov 3 hours agoparentThe lack of a low end scanner market could also be attributed to the fact that everyone these days has a device with a camera that is either good enough or in some cases better for digitizing documents than those old scanners. reply Stevvo 21 minutes agorootparentAre phones really better? A scanner has uniform and perfect lighting every time. On a phone there are often issues with glare or shadows. reply TheSpiceIsLife 4 hours agoparentprevI picked up a colour laser Brother MFC for $100 three years ago, with 600 DPI scanner &#x2F; copier. Wifi printing from our phones works with no additional setup than having the printer connected to wifi.It needed new drums and toners, and continues to function like new to this day.Not quite $50, but $100 Australian, so not far off. reply dbttdft 8 hours agoprevThese kinds of bugs are common with all consumer electronics since they \"went digital\" though, basically anything 1999+. The market is so saturated with blatantly broken products like this that you seem like a freak just to complain about it. Just littered with shit design mainly caused by duct taping components together and letting the outcome of that be the design. The solution is for consumers to get a brain and stop wasting it on overcomplicated nonsense like some autoconfiguration crap based on XML that never works except for someone who so happens to have 5 hours of experience with that protocol. Just return bullshit like this. I always do, I decide to return most of my electronics within 5 minutes of first turning it on. reply strbean 8 hours agoparentThis pattern emerged around the same time as \"can&#x27;t print black when out of colored ink\" and \"printer cartridge DRM\". I&#x27;m extremely incredulous that this is a bug rather than intentional. reply seanhunter 13 hours agoprevAs I sit here I look at my HP-35S and HP-12C calculators that I have on my desk. My 12-C in particular is about 15 years old and still on its original battery. I remember fondly the HP-85a that I learned programming on when I was 8 years old or so. HP used to make amazingly great hardware. The first HP laserjet printers were an absolute marvel. It makes me so sad what has happened to HP. reply bitwize 13 hours agoparentBack in the days when HP was a scientific-equipment company, everything they made was built like a tank. Still have fond memories of the HP-7475A plotter. reply PopePompus 13 hours agorootparentThere is no sadder story in American capitalism than the decline of HP from a wonderful equipment engineering company to a racket for selling tiny little tubs of overpriced ink. Think a young Steve Jobs would be excited to get a job there now? reply sneak 11 hours agorootparentBoeing, perhaps. The same thing happened there, as I understand it.There’s also a strong argument for post-Jobs Apple being an even more tragic arc (that’s earlier on the curve). Time will tell. These things happen on the order of decades, not years.Brain drain is real. reply aqfamnzc 6 hours agorootparentWhat&#x27;s this about Boeing? reply hakfoo 5 hours agorootparentAside from the Max&#x2F;MCAS fiasco, my remaining respect for Boeing went out the window because I live near one of their plants. Middle of the pandemic, they had employees protesting about mask and vaccine mandates out front, for weeks on end.You&#x27;re hiring people that scientifically dense to assemble hundred-million-dollar, high-precision death machines? I hope Airbus has a better HR team. reply asynchronous 2 hours agorootparent“Scientifically dense” is the first time I’ve heard that particular slur. Go back to your bubble where no one ever questions authority. (Boeing is a terrible company for a myriad of other reasons but you picked probably the worst example) reply PopePompus 10 hours agorootparentprevGood point! At least the sad husk of HP isn&#x27;t killing people. reply FredPret 8 hours agorootparentWhen’s the last time Boeing killed anyone? reply trevoragilbert 7 hours agorootparentIn 2019 in Ethiopia. reply FredPret 2 hours agorootparentThat was largely the pilots, same as in Indonesia. replyAngostura 2 hours agoprevI know a lot of people say just get a laser, but I just want to shout out to my bottom of the range Canon Pixma inkjet I bought about 3 years ago.It’s solid, works with third party inks, supports airprinting from my phone and has never caused me any trouble.It’s the second barebones Pixma I’ve had. Previous one lasted about 7 years. reply ed_elliott_asc 50 minutes agoprevI’m looking forward to the day there is a HN post about printers that doesn’t go straight “buy a low priced laser printer… I have a xx from 1987 and it is working just fine” reply wly_cdgr 13 hours agoprevIt&#x27;s just so low and shabby. How can anyone work for a company that does this and not feel constant shame? Any decent person should and would. reply 20after4 8 hours agoparentThat&#x27;s the thing, all the decent people (who had a choice) left years ago. All that&#x27;s left is marketing nitwits, indifferent assholes and people with H1B visas who are largely helpless and trapped. reply whartung 13 hours agoprevI don&#x27;t have a laser simply because I haven&#x27;t seen an \"all in one\" full color laser that&#x27;s as compact as the Epson inkjet I&#x27;m using. I can&#x27;t speak to the color quality of lasers really vs inkjet, but I&#x27;ve printed a bunch of photos that come out \"good enough\" on photo paper with my printer. Mind, if we need any volume, we ship &#x27;em off to the drugstore, but that can actually be a bit of pot luck when it comes to framing and such.We use the scanner often enough to make it worthwhile, and we don&#x27;t have the room for a second, compact B&W laser.The only real thing I need to do is replace this with a tank version. Ink is still crazy, but momentum keeps us going with it. A full set of ink cartridges cost about 1&#x2F;2 of a new printer.Other than that, I&#x27;m pretty content with this thing. reply Macha 11 hours agoparentYou probably won&#x27;t find one as compact, colour laser printers are basically 4 monochrome laser printers in a row with different colour toners loaded, the process doesn&#x27;t lead itself to having small toner catridges on a moving head like how inkjet work. This is a plus and a minus. The minus you&#x27;ve found, they&#x27;re pretty bulky, but reducing moving parts also has pluses for durability and lifespan. reply Lucasoato 10 hours agoprevIn my case, after one year and an half my HP printer auto-updated (without my consent) and bricked because I was using the ink from a different brand. This happened even if I didn’t subscribe to their shitty Hp instant ink offer.I’ve immediately returned it, bought a Brother printer and I will avoid any HP product for the rest of my life.I’m genuinely worried that companies will just perpetually try to get into dark patterns, until laws will be powerless against them. People should spend years in jail for coming up with ideas like these… reply CamperBob2 10 hours agoparentHow&#x27;d you get the return accepted after a year and a half? That&#x27;s pretty impressive service.People should spend years in jail for coming up with ideas like these… +1 for sure on that. It&#x27;ll never happen, though. We wouldn&#x27;t want to pass laws to interfere with \"innovation.\" reply kd5bjo 2 hours agorootparentIn the EU, basically everything comes with a minimum warranty of 2 years[1]. If your printer will no longer print within that time, the retailer is required to fix, replace, or refund it.[1] https:&#x2F;&#x2F;europa.eu&#x2F;youreurope&#x2F;business&#x2F;dealing-with-customers... reply SoftTalker 9 hours agorootparentprevPossibly bought at Costco or similar that will take returns pretty much indefintely. reply ryukoposting 7 hours agoprevI have an Epson flatbed scanner for my photography, a V600. It&#x27;s a lovely little device. Crapware-free functionality. Granted, it seems like the bundled software hasn&#x27;t been updated since the stone age, but that might be a good thing.My HP inkjet, on the other hand, is a wreck. I had to install an app on my phone, pair over bluetooth, connect it to wi-fi, and create an account just to get a damn test page. No USB cable included, by the way. Many cursing matches eventually got it working, but I regret buying it. Once the ink runs out, I&#x27;ll probably send it to Goodwill or something. reply vel0city 6 hours agoparentScanning is the only thing that Brother has let me down on the whole working without extra software idea. Scanning in Windows without their software stack just can&#x27;t get the same quality scans. It&#x27;s limited to 600dpi, it doesn&#x27;t have as crisp quality, and it doesn&#x27;t natively do PDF. Installing the drivers and software stack, it&#x27;ll do multipage PDF at 1200dpi looking crisp no problem. reply dosman33 7 hours agoprevIn the 2010&#x27;s I had a project I&#x27;d do about once a year where I needed to print ~100 pages of directions that was mostly text with light-duty images mixed in for products I was selling. I was using my trusty inkjet printer, but over time the capacity of the carts degraded to the point I could only get ~80 pages of B&W printing out of a single inkjet cartridge. I had tried buying refill carts but they consistently failed and cost me time I couldn&#x27;t afford to loose so I also gave up on non-authentic carts. So that meant I was having to buy two inkjet carts each run which pushed my cost per print up towards nearly $1 per page... for a product I was selling for $20.I finally had enough and bought an HP full color laser printer for $250 around 2015 or so. Even with the degraded toner carts that came with it out of the box I was able to cut my cost per sheet down to about 25 cents or better. The last set of toner refills I got hurt my wallet at ~$450, but my price-per-sheet gets down to a dime per sheet on those. I&#x27;m thankful my printer proceeds these nutty subscription drivers though! I swore off ever using inkjet again after my first season of use on this laserjet.But of course I&#x27;m thankful this printer proceeded these nutty drivers that disable your printing without a subscription. We&#x27;ve reached the end of the infinite expansion economic system and subscription services are the natural next step to that. Market forces have driven down the cost of durable goods to the point most consumer goods have been disposable for decades now. The next step is subscription models of everything to maintain similar profit margins. Likewise, CBDC&#x27;s are a natural extension of the banking system as it hits the same growth limits. They outright state in their whitepapers that one of their core purposes is to \"prevent recessions\" by setting expiration terms on funds held by consumers which will \"induce spending\" as needed. It&#x27;s all the same ball of wax. reply adrenvi 2 hours agoprevWhat is this trend of the headline, sub-headline and first sentence being totally identical? It feels like I&#x27;ve been violated somehow, like a mild brainwashing. One less reason to click on the article link. reply carom 4 hours agoprevI bought the best printer on wire cutter a few years ago, it was an HP ink jet. That was the biggest piece of garbage I ever bought. It would refuse to print when it was low on ink. It would be low on some random color despite me printing in black and white, rarely at that. It locked me out over using after market ink.I bought a brother laser jet and love it. reply mattlondon 12 hours agoprevCanon 100% does this on my printer&#x2F;scanner if the ink dries up (long time without printing) or it thinks you have pirate ink.It throws up some error message that you can&#x27;t clear until you put in new ink cartridges. This is very annoying as I scan frequently but very, very rarely print.I&#x27;d readily replace it but I cannot find a laser printer with duplex scanner with ADF that isn&#x27;t obscenely expensive and&#x2F;or huge. reply distract8901 12 hours agoparentI had a Brother brand B&W laser printer with a duplex scanner many years ago. I had it for 10 years and it was still working when my ex took it. As far as I know its still going.Unfortunately, laser printers are just bigger than inkjet. The paper route is more complicated and the mechanism has to be larger. That&#x27;s the benefit of inkjet, it&#x27;s an extremely simple mechanism that can be reduced to a very small footprint.IMO, laser is better in almost every way. It&#x27;s well worth the drawbacks. reply subhro 11 hours agoparentprevXerox C315? reply snitzr 13 hours agoprevMy in-laws needed a printer scanner because they are getting old and needed to scan paperwork to move to a retirement community. They went out and bought an HP printer scanner and asked me to help them set it up. It took me multiple hours and I still couldn&#x27;t figure everything out. It took them the next two days to really get it up and running. Poorly designed actively hostile software. reply pierat 12 hours agoparent\"Ill help you take that shit back to the store. Its junk. Dont buy HP.\" reply kakaz 3 hours agoprevMaybe I put there a note that HP produced SSD server disks which fault at preengeneered number of writes, making your raid faulty. No one should ever use this brand reply farseer 5 hours agoprevI have been using using the HP P2055 network printer for almost two decades now with bootleg toner. Never had a problem, the thing is built like a tank. reply smeej 12 hours agoprevI&#x27;m surprised how many people are writing about their solutions for printing. I thought by now most people had done what I&#x27;ve done and just set things up not to need to print.Xournal allows me to fill out anything I could print from PDF with a stylus on my touch screen. If I want to read and highlight something, I convert it to PDF and read it in Logseq with the same stylus. I can highlight in four colors and it automatically collects my notes!If I need to mail&#x2F;ship something, I just have the ship shop print it.What are people intentionally printing to paper for, often enough that it&#x27;s worth having a printer? reply scarecrowbob 5 hours agoparentI am a musician and need to print sheet music for the ensembles that I play with. I personally like reading off of an iPad, but there are plenty of situations where that is difficult or impossible.I&#x27;ve also printed a lot of maps for climbing&#x2F; hiking&#x2F; mountaineering.There are a lot of situations where having a printed copy of something can be critical, either for performance or safety. I like virtual reality, but a lot of us are still operating in meat space. reply snuxoll 11 hours agoparentprev> What are people intentionally printing to paper for, often enough that it&#x27;s worth having a printer?If I&#x27;m being honest, mine is a mid-range document scanner that also happens to have a color laserjet attached for the occasional form or whatever that has to be on paper and will still be cheaper for me to print at home then get it from the print shop.50 page ADF with duplex scanning is something you get in the ~$200+ price range of standalone document scanners, and they tend to not have a flatbed either for the occasional photo or copy of a page from a book. I&#x27;d love to be 100% paperless, but even for my very digital life I haven&#x27;t managed to get that far yet. reply mattlondon 12 hours agoparentprevMany places still ask you to physically sign and then scan&#x2F;post back things. E.g fill in a pdf with a computer then print and manually sign with a pen.It is not every day, but frequently enough that not having a printer is a pain. reply dsr_ 10 hours agorootparentWrite your signature. Scan it in. Apply it to documents that want signatures.To some people, this doesn&#x27;t \"feel right\" the same way that it doesn&#x27;t \"feel right\" that I have rolling star-base office chairs around my dining room table. They are far more comfortable than any \"dining room chair\" I&#x27;ve ever been in.In both cases, when people don&#x27;t take my good advice, I shrug and move on. reply mattlondon 9 hours agorootparentOk sure so use a pen or use the printer to write your signature - that doesn&#x27;t change the need sometimes to go give the doctor&#x2F;school&#x2F;solicitor&#x2F;etc a physical copy.You can keep your \"good\" advice, but please do move on. reply kstrauser 8 hours agorootparentIn fairness, that’s different from the print&#x2F;sign&#x2F;scan workflow you mentioned first.I also have a printer for those rare occasions. More commonly, I copy-paste my signature image onto the digital form and send it right back. reply smeej 10 hours agorootparentprevI scanned a copy of my signature probably a decade ago and have yet to find anyone who can tell the difference between my adding that to a doc and sending it back, and actually printing, signing, and scanning it back. reply tegiddrone 8 hours agoparentprevSometimes I print out code docs and other software things to trick my brain into a different perspective. Being able to touch the information on an object in space is useful to me. Not buried next to other distractions on a computer. Helps me focus on the task at hand.I also sketch out code on pen&#x2F;paper sometimes for similar reason. reply Macha 11 hours agoparentprev1. Shipping labels2. Boarding passes (yes, I suppose I could pay the airline $30 for the privilege, but it doesn&#x27;t take _that_ many flights to break even on a basic laser printer. I could also use mobile passes, but airlines are sometimes weird in not supporting those for specific flight&#x2F;booking combination, and they need power). reply smeej 10 hours agorootparentWhich airlines charge to print a boarding pass at the airport? I haven&#x27;t ever encountered this charge, but maybe it&#x27;s a regional thing? I&#x27;ve ways been able to walk up to the self-serve kiosk, enter a few details, and print the thing right off. reply Macha 10 hours agorootparentThe largest european airline does: https:&#x2F;&#x2F;help.ryanair.com&#x2F;hc&#x2F;en-us&#x2F;categories&#x2F;12488813755537-...? reply usr1106 9 hours agorootparentAir Baltic also does unless you buy a more expensive ticket. (Otherwise they are much better than Ryanair.) reply mr_toad 9 hours agorootparentprevMany people are using mobile&#x2F;digital boarding passes these days. reply Macha 9 hours agorootparent> but airlines are sometimes weird in not supporting those for specific flight&#x2F;booking combination, and they need power reply AmericanChopper 10 hours agoparentprev> What are people intentionally printing to paper for, often enough that it&#x27;s worth having a printer?Shipping labels, government processes that require hard copies, and legal documents that require real signatures rather than electronically signed ones. reply NotYourLawyer 8 hours agoparentprevSome things require an ink signature. That’s probably 50% of the tiny amount of printing I do. reply AnimalMuppet 10 hours agoparentprev> What are people intentionally printing to paper for, often enough that it&#x27;s worth having a printer?For me personally, sometimes I think better when I have paper than with a computer or tablet. If the limiting agent is my thinking, then I&#x27;m better off with paper. reply j45 8 hours agoprevBrands like Brother, Epson are far more preferable to HP, etc in the multi function printer category for many reasons. reply Simulacra 14 hours agoprevIt&#x27;s like Mercedes trying to charge a monthly subscription fee for heated seats. When you do something like this to consumers, you lose their trust and you will never gain it back. reply Tomte 14 hours agoparentUnfortunately, people will misremember and trash unrelated brands&#x27; reputations online.It was BMW. reply barbazoo 13 hours agorootparentOr maybe they just misremembered the exact shitty thing the brand didhttps:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-63743597> Mercedes-Benz is to offer an online subscription service in the US to make its electric cars speed up quicker. reply throwaway20304 13 hours agorootparentprevMercedes did something similar, though: https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-63743597 reply gruez 13 hours agoparentprevAFAIK they offered monthly, yearly, and \"unlimited\" subscription. In that case what&#x27;s the issue? Heated seats are frequently an upgrade option. What&#x27;s the problem with offering payments as an option? reply SuperNinKenDo 11 hours agorootparentWell, let&#x27;s use our imagination for a minute and consider why they&#x27;d be happy to sell and \"unlimited subscription\", but not a subscriptionless, permanent feature.Or rather than imagination, simply draw on experience with other companies and ither subscription models. reply gruez 10 hours agorootparentI&#x27;m not going to do the work of coming up with arguments for you. Stop beating around the bush and write out what you meant. reply AnimalMuppet 12 hours agorootparentprevAnd, in fact, if you&#x27;re only going to keep the car for a few years, the subscription may be cheaper. reply jononomo 10 hours agoprev [3 more] [flagged] NotYourLawyer 8 hours agoparent [–] Sir this is a Wendy’s. reply jononomo 8 hours agorootparent [–] Yeah, I lost it a bit there and I deserved those downvotes. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "HP Inc. is confronting a class-action lawsuit, alleging that the company deactivates scanning and faxing features on its printers when ink levels deplete.",
      "The lawsuit accuses HP of deliberately concealing this information from consumers to boost sales from expensive ink cartridges.",
      "A federal judge has allowed the lawsuit to move forward, dismissing HP's plea to discard the case. This represents a recurring issue, with another group suing Canon Inc. in 2021 for similar practices."
    ],
    "commentSummary": [
      "Users express frustration over HP printers, believing them designed to cease functioning when ink levels are low, sparking discussions on reliable and cost-saving printing alternatives.",
      "Brother laser printers and Canon Pixma printers with refillable toner tanks receive mention for their cost-effectiveness, though mixed reviews cite issues such as WiFi connectivity troubles with Brother printers.",
      "Beyond printer type, debates reveal issues with the availability of affordable scanners, printer cartridge complications, the need for hard copies in various contexts, and calls for improved regulations and ethics within the printer industry."
    ],
    "points": 298,
    "commentCount": 156,
    "retryCount": 0,
    "time": 1696529002
  },
  {
    "id": 37777301,
    "title": "Krita fund has no corporate support",
    "originLink": "https://fund.krita.org/",
    "originBody": "Home About Sponsored Development Login Shop The Krita Development Fund Campaign 2023: support 10+ full time Krita developers Corporate Individual 5 developers € 15k / month Join the Development Fund and support the Krita Foundation to work on Krita development. € 4426 MONTHLY CONTRIBUTION ** 349 INDIVIDUALS 0 CORPORATE Choose Your Membership LEVEL BADGE NAME LINK LOGO BRONZE € 5 / month SILVER € 10 / month GOLD € 25 / month PLATINUM € 50 / month TITANIUM € 100 / month DIAMOND € 250 / month CORPORATE Learn more. * Other pricing options available. Displaying your badge, name or company is opt-in. You control it being public or private. ** The donation amount displayed on this page also include the donation from our old system. Credits Diamond ASIFA-Hollywood Titanium User Manuals Reinold Rojas Platinum Scott Petrovic Amanda Choi HatschYuh Gold Laurent Valentin Jospin BiBiANA DE LA O damien dupont Liam Smyth fullmontis Patrick Garraud Tyson Tan Mara Huldra Flo Tasser Livio Fania Erik Broeders Liuuzaki(刘) Johnathan Andersen Takiro Ryo Philipp Brodersen Ki Duduf Martin Weidenauer Jack Vanlightly Portland Digital Dabblers Merlijn Sebrechts Foli Ayivoh Scott Wilson David Garcia Wojtek Trybus Luke Kaalim Software Krita Desktop Features Gallery Report a Bug Education FAQ Tutorials Documentation Resources Foundation About Us Contact Us Press Donate What is KDE Get Involved Shop",
    "commentLink": "https://news.ycombinator.com/item?id=37777301",
    "commentBody": "Krita fund has no corporate supportHacker NewspastloginKrita fund has no corporate support (krita.org) 292 points by moelf 22 hours ago| hidepastfavorite211 comments lars_francke 21 hours agoI want someone to build this:SBOM (Software Bill of Materials) are slowly becoming ubiquitus around the world due to regulation.I want to be able to aggregate all SBOMs within my company, have a small tool that scans my machine and creates a SBOM for all Open Source tools I use (e.g. Firefox, VLC etc.), uploads that to my corporate registry.This data is then submitted to a donation aggregator which analyzes those SBOMs and distributes my monthly donation across all those projects.It is so hard dealing with those individual donor portals and various forms to donate to foundations et. al.If this whole project could be run as a non-profit foundation itself that&#x27;d be perfect. reply zoogeny 18 hours agoparentI think the devil is in the details. As others have pointed out, this would drastically change the incentive structure of OSS. You would attract actors whose entire purpose would be to get listed on as many SBOMs as possible to maximize their revenue potential. I&#x27;m envisioning a world of `padLeft` controversies.That is to say, I don&#x27;t think a pure \"frequency of use\" metric is sufficient to fairly distribute such a pool. And if you have a large and growing pool of available money then the incentive to game the distribution scheme becomes more attractive than making truly awesome software. And I very much doubt that we have any reliable ways of aligning such a money distribution scheme with the goal of creating amazing software. reply bee_rider 17 hours agorootparentI wonder if it would be enough to just recursively pay out to each dependency.You pick a % to keep and a % to pass along to each of your your dependencies, and then they split up that % that you passed along to them, so on and so forth.Of course people could choose to pass 0% along to their dependencies. But that seems fine; people can adjust and just not send greedy projects a very big split.Trying to enforce some fairness on the outside will probably just result in silly behavior from greedy people (ie, if every project gets an equal split out and somebody doesn’t want to hand any money down to their dependencies, then they can just create a bunch of tiny projects to dilute their out-split). reply lars_francke 3 hours agorootparentprevEverything you say is correct. I still want this. It&#x27;s better than what we have today.We can work on improving things later. reply orblivion 18 hours agorootparentprev`padLeft` can&#x27;t just inject itself into a software stack right? So there&#x27;s an incentive to be valuable enough to be included. reply kybernetikos 3 hours agorootparentIf I remember the story correctly the padLeft dev was actually pretty notorious for submitting patches to open source projects that &#x27;coincidentally&#x27; added dependencies on his code. reply VyseofArcadia 17 hours agorootparentprevDepends on how you do the accounting. \"Well, you don&#x27;t use padLeft, but 15 packages you depend on do, and 68 packages tjat those packages depend on do... Altogether you&#x27;re looking at 348 instances of padLeft.\" reply orblivion 17 hours agorootparentIf there were a metric that would make each instance of padLeft worth a tiny amount of money (for such a tiny library), that sounds like a reasonable outcome to me.EDIT: I may be missing part of the point reply bee_rider 17 hours agorootparentA trivial way to exploit this kind of system, I think, would be to write LegitimatelyUsefulLibrary, then write 1,000 PadLeft projects, and make LegitimatelyUsefulLibrary depend on all of them.Since you are the one marking the dependency of LegitimatelyUsefulLibrary on your PadLeft projects, you can game the metrics however you’d like when making it. reply noworriesnate 9 hours agorootparentBut why would I, as a developer, use such a library? It would cut me out of profits unnecessarily. In fact when I’m picking my dependencies I will deliberately avoid those that take a larger slice of the pie than is worth it for me. replyClamchop 13 hours agorootparentprevOnly enumerate and pay for top-level tools and dependencies, none of which are probably padLeft. Those downstream recipients can further \"pay it forward\" to their top-level dependencies. Everyone has an incentive to not pull in superfluous bullshit so they can hold onto as much of their pitance as possible.Trickle down economics? Ronald, is that you? reply arp242 21 hours agoparentprevthanks.dev does this. I think I saw GitHub Sponsors also started (or will start?) doing something like this, but I&#x27;m not sure on the details off-hand.But yeah, I&#x27;ve argued for this a long time as well: who is going to look up 100 to 2,000 dependencies and see if they accept donations and set that up and cancel when you stop using it, add new ones when you start using them (and many will be transitive deps, so you have to check if it changed every month or something), etc. etc.You just want to give one organisation $500&#x2F;month or whatever and let them sort it out. You don&#x27;t even need SBOMs, just start by sending them your go.mod or package.json or Cargo.toml or Gemfile or whatever.That the FSF and OSI are doing basically nothing in this regard is why I have trouble taking either organisation serious. reply rendaw 18 hours agorootparentAnd I started https:&#x2F;&#x2F;bre.ad&#x2F; which does similar! I was trying to figure out a way to attract people slowly while I iron out issues... reply lars_francke 20 hours agorootparentprevWe are using thanks.dev (to donate) and they are doing a tiny bit of it, but I have not seen much happening there in the past few months.Just sending go.mod etc. only looks at the software in the build process which doesn&#x27;t go far enough.But I think we&#x27;re on the same page! reply jamietanna 19 hours agorootparentprevSee also https:&#x2F;&#x2F;stackaid.us as another platform doing similar reply evntdrvn 19 hours agorootparentprevIsn’t this kinda what Tidelift is trying to do? reply arp242 19 hours agorootparentI tried to sign up (as a maintainer) and I never heard back from them. Tidelift also does a lot of other stuff; my impression is they want to have a \"curated list\" of packages (Or something? I find Tidelift confusing) which is fine I guess, but not really a general solution. reply mufeedvh 20 hours agoparentprevI have made a simple CLI utility[0] with this purpose in mind. It scans your entire filesystem for README.md and FUNDING.yml files for a set of donation&#x2F;sponsor links and tag it with the associated repo (No HTTP calls, just the assumption that most repos link their support URL in either of these files). The output is a CSV sheet containing the open-source dependencies&#x2F;libraries you use in your system that accepts donations.I have plans to expand&#x2F;plug this into a donation aggregator platform like you mentioned if time permits. But if there is an existing effort for the same, I am happy to contribute. :)[0] - https:&#x2F;&#x2F;github.com&#x2F;mufeedvh&#x2F;paydept reply bsima 20 hours agorootparentHey this is pretty great, and the code is so simple. I guess it only works if you have the sources checked out somewhere, which isn&#x27;t the case for all build tooling and package managers, but I could see an extended version of this that hooks into the standard package managers to fetch the required information to complete the report.If you can also hook into an accounting system (eg plaintextaccounting.org) then you could also calculate the whole dollar amounts to donate as some percentage of income from the product. reply hinkley 17 hours agoparentprevI would settle for something much simpler.All of your engineers get 5,10 votes for important libraries they use. The vote history is aggregated, and voting is twice a year. For every N votes we send $100 to that project. That way you’re not cutting little checks, and you have a finite number of projects you have to hunt down funding contacts and charitable status for.Due to aggregation across votes, all the projects that perpetually come in 10th to 20th place get paid every year or two, and first place is likely to see a little more money each year. $300 in June and $400 in December for instance. reply zorrotorro 20 hours agoparentprevDon&#x27;t take this the wrong way but corporate are such prima donnas!I got flashbacks from my tiny startup days when corporate went to procurement companies which wanted specific terms and mode of payment. And after all the infrastructure was in place... they basically spent no money!The SBOM idea exists in various forms. Few need it and few use it.If somebody wants to pay money, they will figure out a way. If they want to invent an excuse they will find one. reply digging 19 hours agorootparent> If somebody wants to pay money, they will figure out a way. If they want to invent an excuse they will find one.I think this is just not true. Reducing friction will increase participation.I make donations to many organizations currently. I would donate to more, but it&#x27;s a hassle to identify them and determine what&#x2F;how I want to donate. It&#x27;s just not a top priority for me, for better or worse. But if I could click a button right now and 5x my donations with the trust that they were supporting what I wanted them to, I&#x27;d do it!edit: Also, this is a reminder to me that everything is a choice and I could be spending my time setting up donations instead of commenting on HN... reply lars_francke 19 hours agorootparentprev> Don&#x27;t take this the wrong way but corporate are such prima donnas!Not taking it the wrong way, you&#x27;re right :)> I got flashbacks from my tiny startup days when corporate went to procurement companies which wanted specific terms and mode of payment. And after all the infrastructure was in place... they basically spent no money!Yep, that&#x27;s painful but I count it as the cost of doing business. If I jump through these hoops and my competition doesn&#x27;t it sets me apart. If only 1 in 10 then spent money so be it.> The SBOM idea exists in various forms. Few need it and few use it.True today. Not true tomorrow. Software development will become a highly regulated industry. This is my prediction at least.> If somebody wants to pay money, they will figure out a way. If they want to invent an excuse they will find one.I want to give them fewer excuses. reply snarfed 21 hours agoparentprevThis is https:&#x2F;&#x2F;tidelift.com&#x2F; ! Others too, I think. reply lars_francke 19 hours agorootparentNo, that&#x27;s something different.They used to be simple to understand, looking at their homepage today I have no idea what they are doing today but not what I&#x27;m looking for. reply moralestapia 21 hours agorootparentprev\"Contact us for a quote\"Any idea how much that is? reply natpalmer1776 20 hours agorootparentAccording to web.archive.org they had their &#x27;starter&#x27; subscription priced at $30,000&#x2F;year for 50 developers as of December 18th, 2022. reply moralestapia 19 hours agorootparentWhoa, that&#x27;s very likely way more than what these organizations spend supporting OSS, lol. reply natpalmer1776 17 hours agorootparentWell this is designed for enumerating supply chains in a strict compliance focused environment, not necessarily for giving back to said supply chain. reply Karunamon 20 hours agorootparentprevNo pricing info, no easily accessible demo, government agency logos on the landing page.This is setting off all of my \"enterprise trash software\" alarms. And if there is one thing those all have in common, it&#x27;s being way too expensive. reply rdl 20 hours agorootparentNot just in billing, but also in implementation cost and general overhead. I actively avoid buying anything which requires talking to a salesperson to get basic service info; ideally one has something like the Cloudflare self-service model with enterprise upgrades. I know someone currently paying >$800k&#x2F;yr to Cloudflare who started out a couple years ago with a $200&#x2F;mo plan. reply placesalt 21 hours agorootparentprevAbout 10-20 minutes of your time, if you include the task-switching cost. reply phone8675309 21 hours agorootparentprevNo, but I bet if you contact them that they can give you one. reply jandrese 21 hours agorootparentBut then you have to waste a salesguy&#x27;s time generating a quote for a product that I&#x27;m not likely to purchase. I hate doing that. I&#x27;m basically stealing from them just because I&#x27;m trying to shop around for the best product. I&#x27;m calling salesguys and giving all of them my information and getting several different quotes, but I&#x27;m only going to execute on one of them. And now they&#x27;re spamming my inbox every time their company does something even though I&#x27;ve never bought anything from them. reply chpatrick 20 hours agorootparentThe reason they make you do this is because different companies pay orders of magnitude different amounts for services like this. reply bbarnett 20 hours agorootparentprev1) Use a throwaway email for that quote2) Write up what you want, and email ALL the companies you want a quote from... CCing them all in the same email.Make sure to indicate a close date for them \"and all other possible bidders\" to submit by.Now they all know who their competitors are, AND, they know there could be other, extra competitors, AND they know to price as competitively as possible.The sales people at the other end, and the company, will know if they want to \"waste their time or not\".This works well with car dealers too. If you want the best price on a specific make and model with specific options, send to the 10 dealers in a 2 hour drive radius. reply jandrese 20 hours agorootparentI wonder if this is one reason many companies have web forms to request the quote. So you can&#x27;t mass email for them. reply phone8675309 20 hours agorootparentprevSo you&#x27;d rather someone else subject themselves to that for your own benefit than do it yourself?Seems kind of selfish, does it not? reply jandrese 20 hours agorootparentI&#x27;m sorry I don&#x27;t understand. The alternative was to have a basic price for the product or service on their website that a person could look up.Maybe if I&#x27;m some big bulk buyer and think I can get a better deal by talking to the salesguy then I&#x27;ll do that. But if I&#x27;m some small fry buying like 1 or 2 of the things I know they aren&#x27;t going to give me a break, but I still have to go through the \"contact the sales person, they call you back, you explain what you want, they generate a quote within 5-7 business days that is good for 30 days after being generated, you end up not buying the thing for whatever reason\" rigamarole.Sidenote: I&#x27;ve never had a vendor balk at me using an \"expired\" quote to buy something. Our purchasing process never proceeds within 30 days, but turns out the prices don&#x27;t change either. It is very common to be executing on a quote that&#x27;s 6 months old. replykfichter 4 hours agoparentprevNot quite the same thing, but the Optimism Collective runs a program called Retroactive Public Goods Funding that sort of works like this: https:&#x2F;&#x2F;www.optimism.io&#x2F;retropgfA round is currently active and dependencies are encouraged to sign up. Quite literally millions worth of crypto up for grabs for the ecosystem&#x27;s dependencies on a recurring basis. Frequency is about twice per year right now but I think the goal is to get to at least once per quarter.(Disclaimer: I work for a company called OP Labs which does work for the Optimism Collective)(And yes, it&#x27;s a crypto project, but I&#x27;m hoping the common goal of constructing systems that can sustainably fund open source software might bridge the HN gap) reply oaiey 21 hours agoparentprevFirst: I also need that. It is a pain right now. Second: it would be a good way to rationalize to your managers on how shaky grounds they stand. reply hiAndrewQuinn 21 hours agoparentprevSay more about this SBOM regulation. Do you have an EN standard or something you can point me to? reply kevinherron 21 hours agorootparentIt&#x27;s mostly a recommendation from what I&#x27;ve seen so far, but it&#x27;s not hard to read this as a future requirement.CISA has been pushing it, which his my why it&#x27;s on my radar: https:&#x2F;&#x2F;www.cisa.gov&#x2F;sbom reply Cerium 20 hours agorootparentFDA wants electronic SBOM for future device approvals.https:&#x2F;&#x2F;www.fda.gov&#x2F;medical-devices&#x2F;digital-health-center-ex... reply lars_francke 20 hours agorootparentprevAs others have already commented:The US government has added SBOMs to a proposed rule to update the Federal Acquisition Regulation. So if you want to sell to the US Government you&#x27;ll have to provide SBOMs: https:&#x2F;&#x2F;www.federalregister.gov&#x2F;documents&#x2F;2023&#x2F;10&#x2F;03&#x2F;2023-21...Lots of large companies require SBOMs from their supplier.In the EU we will get the Cyber Resilience Act which will make them mandatory as well in certain cases: https:&#x2F;&#x2F;data.consilium.europa.eu&#x2F;doc&#x2F;document&#x2F;ST-12536-2023-...And yes, there&#x27;s bascially two technical standards to provide them: SPDX and CycloneDX: https:&#x2F;&#x2F;cyclonedx.org&#x2F; reply tapoxi 21 hours agorootparentprevhttps:&#x2F;&#x2F;cyclonedx.org&#x2F; reply qwerty456127 20 hours agoparentprev> I want to be able to aggregate all SBOMs within my company, have a small tool that scans my machine and creates a SBOM for all Open Source tools I use (e.g. Firefox, VLC etc.), uploads that to my corporate registry.Then your boss will come and ask WTF do you use VLC for when all the videos are on the web, also some default media player comes with your OS and by the way, you are not supposed to watch videos during your work day. reply Uehreka 19 hours agorootparentI think the idea here is more like “donate $1000&#x2F;mo, have it distributed across the projects” rather than, say, “donate $10 to each project in the list.” reply lars_francke 20 hours agorootparentprevIt&#x27;s my company. Apart from that: VLC was just one example. But it&#x27;s a tool that a chunk of employed people actually need for work. reply Spivak 20 hours agorootparentprevWild, I should be more appreciative that I have Steam on my work laptop and have played Slay The Spire during deploys with management on the call commenting on my game. reply bee_rider 18 hours agorootparentSlay the spire seems like time be a great game to play in the gaps between tasks. reply martypitt 19 hours agoparentprevI&#x27;ve been toying with an idea of some kind of \"Spotify for OSS\" kinda thing - where software authors can make their binaries available, provided you have an active subscription (with the Spotify for OSS service).Funds are then distributed based on usage (trackable at the package registry).The idea being corporates buy a single license per head, rather than dealing with lots of donations etc.Probably lots of reasons it wouldn&#x27;t work, but it&#x27;s a fun thought experiment. reply bee_rider 17 hours agorootparentThis seems like an easy way for people to accidentally change the nature of their projects from a personal hobby&#x2F;community type thing to something more like a customer service type relationship, which brings a bunch of expectations regarding fitness and merchantability. reply Woshiwuja 19 hours agorootparentprevIt&#x27;s a good idea but that would mean companies now have to pay. And they don&#x27;t like it. (unless is for useless shit nobody cares about that doesnt make your job easier, they got loads for that) reply whywhywhywhy 16 hours agoparentprevYou need to audit the places it’s being donated to. Do they actually spend it on making the software you depend on better and paying the people making the software. Or is it a Processing Foundation situation https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37760363 reply colejohnson66 21 hours agoparentprevA \"clearing house\" for donations would be pretty neat reply hedora 19 hours agoparentprevI&#x27;m hoping \"cargo vet\"&#x27;s approach to supply chain security becomes standard practice:https:&#x2F;&#x2F;mozilla.github.io&#x2F;cargo-vet&#x2F;Adding a \"pay upstream\" feature to it would probably be minimal incremental effort. Someone is already supposed to pay the third-party auditors, after all. reply q0uaur 21 hours agoparentprevI was also disappointed to see them using a KDE account for donations. I like KDE but i&#x27;m pretty sure there&#x27;s tons of krita users who do not use kde, me included.Found that there is a Krita on liberapay, which i much prefer as platform, but who knows if that&#x27;s official or not, since it&#x27;s not linked on this thread&#x27;s link. reply sho_hn 20 hours agorootparentKDE is a community of developers that makes various software products, including Krita - that&#x27;s why there&#x27;s a KDE account involved here, since KDE makes Krita :-)Many of those software projects available even for commercial systems like Windows and Mac OS, and of course the various free software environments. Those users are all equally important, so there&#x27;s no conflict there. reply FeepingCreature 20 hours agorootparentI haven&#x27;t used Windows in ages, but when I did, kate worked very well on it. reply pxc 21 hours agorootparentprevIf you&#x27;re using Krita, you&#x27;re a KDE user because Krita is a KDE project. The Plasma desktop environment is just another KDE project. reply HPsquared 20 hours agoparentprevThis ties in with the other thread about aircraft parts and the chain of custody. \"Who has touched and signed off on each piece of code\" reply itomato 18 hours agoparentprevYou get this upstream as part of the system inventory and manage the delta with your change management reply phkahler 19 hours agoparentprevThe more infrastructure built to fund a cause, the lower the percentage of the money actually funding people on the ground doing the work. This is often true of charities. OTOH there will be more money, and maybe some of the people actually doing the work can actually make a living doing so. reply birdyrooster 20 hours agoparentprevI read this several times and I have no idea what benefit this would serve or anything about it. reply lars_francke 20 hours agorootparentLet&#x27;s say I want to make sure to donate to ALL open source projects I use in my company. Those in my builds as well as those used on our computers that are not build dependencies (e.g. Firefox, vim, Linux, ...).How do I do that today? I collect a list of that software and contact hundreds of individual people and organizations, figure out the donation processes, fill out forms, transfer money etc.I want this to be automated. I want this automation to build upon existing standards.Does that help? reply carapace 19 hours agorootparentThis proposal would lead (I suspect) to \"enshittification\" of the supply chain as people wrangled to be larger portions of the dependency&#x2F;payment tree, eh?Part of what makes FOSS work at all is that the motivation is not profit but benefit. reply RockyMcNuts 21 hours agoparentprevsame for news and blogs ... pay $10 per month, get paywall access, browser tracks what you read and distributes to authors. open source version of Apple news that actually supports local news and indies without extracting a giant additional vig. reply catapart 21 hours agoparentprevLove that idea! reply pavon 18 hours agoprevAt my workplace a common theme is that folks that use a piece of software heavily as part of their job use the big professional tools both because it is worth paying more for a better tool considering how much value they get out of it, and because it aids in collaborating for everyone to use the same tools.People who only use a tool occasionally are the ones using Paint.net, Krita, Inkspace, Audacity, etc. Part of that is to save money, but a much bigger part is that it&#x27;s too much hassle to go through procurement when it isn&#x27;t absolutely necessary. More hours would be spent paying for the tool then would be spent using it.That makes it a hard sell to get corporate donations. From the corporate perspective, the tool is completely off their radar because their employees are using it to intentionally bypass corporate bureaucracy. reply Liquix 17 hours agoparentAbsolutely. But sometimes the best in class tool is open source (see: ffmpeg), resulting in a win-win-win for the accounting department, for the developer freed from bureaucracy, and for open source as a whole. It would be nice to live in a world where this was the case more often. reply doctorpangloss 17 hours agoparentprev> From the corporate perspective, the tool is completely off their radar because their employees are using it to intentionally bypass corporate bureaucracy.If something is priced cheaply enough to be paid for out of pocket by an \"individual contributor,\" but for a work purpose, that&#x27;s a big deal.The bigger takeaway is that all seemingly B2B software is still B2C in disguise.On the other hand, the kind, smart people making Krita aren&#x27;t in it to run a sales pipeline. Like even if the diehard, nonparticipating libertarian rationalists are right, if it&#x27;s true that every person on Earth would happily be a billionaire, most people think sales isn&#x27;t fun or meaningful.If you want to make the world more meaningful in your own way, I appreciate that the limit of fundraising to that goal is donations. reply raincole 21 hours agoprevI&#x27;m migrating from Krita to Photoshop lately.Don&#x27;t get me wrong, I love Krita. I used it for years and I wrote ~30+ bug reports. It&#x27;s a positive thing -- I won&#x27;t bother reporting bugs to Adobe cause I know they don&#x27;t care, unlike Krita devs.However every time I need to collaborate with someone else, they use Photoshop. They use .psd like it&#x27;s an interchangeable format[1]. Out of 10+ artists I&#x27;ve only seen one who doesn&#x27;t use PS, and it&#x27;s not Krita.[1]: I know Krita can read and write .psd. reply danShumway 19 hours agoparentNot to detract from raincole&#x27;s point, and this is not relevant at all to their problem, but just a general reminder that in many ways Krita is not a direct competitor to Photoshop unless you&#x27;re using Photoshop as a digital painting tool -- and if you are, I would argue that Krita is arguably a better product in multiple ways.I think that Photoshop is an image manipulator first and foremost, and Krita is a tool for drawing&#x2F;painting and maybe doing some light traditional animation (although even that is not its primary focus).I think that CSP, Procreate, etc... are more direct competitors to Krita. Photoshop is a competitor to Gimp.Again, does not change anything about raincole&#x27;s comment, and I don&#x27;t think that raincole was suggesting that they were direct competitors. It&#x27;s just that whenever Photosohp and Krita get mentioned in the same sentence I feel like the assumption from many readers is that they&#x27;re trying to do the same things, and I would personally say that I think they&#x27;re separate categories of software. There are things that Photosohp does that Krita doesn&#x27;t want to do and will probably never support, because it&#x27;s a painting app, not an image manipulation app. reply raincole 16 hours agorootparentI agree. Off the top of my head, the most important new features they introduced in recent years were:Krita: A new brush engine. A new file format for brushes. Some animation features that I don&#x27;t use. New text system.PS: AI-based object selection. AI-based neural filter. AI-based generative fill. New Javascript API for plugins.It&#x27;s quite clear that Adobe doesn&#x27;t see PS as a digital painting app, but an (AI-assisted) image manipulation app. reply catapart 21 hours agoparentprevI feel this pain. Photoshop is such a massive beast and really doesn&#x27;t do anything for me that other programs don&#x27;t. It&#x27;s just too ubiquitous to work around without literally budgeting in your hours for having to work around it. -_- reply danjc 20 hours agoparentprevSeems that a near-perfect psd import&#x2F;export would be the killer feature that enables it to compete with Photoshop. That would be huge. reply raincole 20 hours agorootparentI highly doubt if Krita has the man power to do this. They&#x27;ve been developing a new text system for more than one year. The new text system isn&#x27;t designed to be compatible with Photoshop.And even if it is, rendering text just like Photoshop is just a tiny step towards rendering everything like Photoshop. reply bonzini 19 hours agorootparentprevRelevant: \"PSD is not my favorite format\" (2009)https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25946103 reply borbtactics 20 hours agoparentprevSame here. I want a Photoshop alternative that has built-in tate-chu-yoko but I&#x27;ve never seen one reply catapart 20 hours agorootparentPerfect example of something that most people will think of as \"not necessary\", but a whole industry will understand as close to, if not outright, a deal-breaker!It would be really cool to have some kind of open source community that developed these kinds of \"usage specific\" plugins across multiple open-source projects, to kind of patch in that functionality where it&#x27;s missing. Some people to work on ever-evolving interfaces for the supported applications, and then some other people to write plugins that provided these features, progressively enhanced, based on those interfaces.It would be a gargantuan effort, though, and you would just end up with a plugin soup without careful moderation (even more effort; and easy to go awry!) so c&#x27;est la vie! reply sweatypalmer 21 hours agoparentprevMaybe a stupid question, but if Krita can read&#x2F;write .psd then why does this matter? reply raincole 21 hours agorootparentBecause just like the most \"open source app reading&#x2F;writing proprietary format\" scenarios, it doesn&#x27;t do it perfectly.Even if you simply import a .psd and re-export it again as-is, it will still break things like smart objects and layer blending options.It&#x27;s not Krita&#x27;s fault: .psd is not an interchangeable format. Implementing .psd support perfectly is basically implementing the whole PS itself. It&#x27;s all the artists&#x27; \"fault\". But it doesn&#x27;t change the fact that using Krita makes my days difficult. reply tambourine_man 21 hours agorootparentAnd PSD is notoriously complicated. I remember reading some rant by an open source developer and there were things along the lines of different endianness in the same file.It’s a 30+ year old format that went to multiple transitions. I don’t envy the poor soul that has to reverse engineer it. reply lloeki 20 hours agorootparent> I remember reading some rant by an open source developerThis one? https:&#x2F;&#x2F;github.com&#x2F;gco&#x2F;xee&#x2F;blob&#x2F;4fa3a6d609dd72b8493e52a68f31... reply funnyflywheel 19 hours agorootparentI took a look at the git blame, and the commit title is perfect.https:&#x2F;&#x2F;github.com&#x2F;gco&#x2F;xee&#x2F;commit&#x2F;750196023da5457d9535b30299... reply seanthemon 18 hours agorootparentprevMy favorite part is the immediately followed &#x2F;&#x2F;sanity check, too apt. reply alexvitkov 20 hours agorootparentprevFantastic read, thanks. reply tambourine_man 20 hours agorootparentprevThat’s the one yes, thanks. reply numpad0 21 hours agorootparentprevAnd I believe .psd is still the only \"raw\" common format for artistic drawing, right?I&#x27;ve once seen a .psd that won&#x27;t display correctly in neither Ps or CSP, supposedly because it was exported from something else. That didn&#x27;t seem like a fun situation. reply orbital-decay 17 hours agoparentprevWhat kind of work do you do? The primary use case for Krita is digital painting, where a lot of artists use CSP&#x2F;Procreate and typical pipelines allow certain freedom of choice. I used to work with 2D artists, and perfect PS compatibility was a deal breaker everywhere except for digital painting. reply yuukisenshi 18 hours agoparentprevWhat industry? I am involved in the game and illustration industry, and work with mostly eastern studios. I have seen a 90% CSP usage rate, with the last 10% mostly being procreate users. Photoshop is in usage to the same degree as Paint Tool Sai and I have never seen a Krita user. reply libraryatnight 2 hours agoparentprevI don&#x27;t ever really collaborate, so I had come to comment that I recently switched to Krita and have really been enjoying it. Between Krita, Gimp, and Blender once I swap some fonts in places I can drop my Adobe sub which has at least been dropped to the minimum package.I switched because it seems like it&#x27;s important to use and support open source tools wherever possible, proprietary software is feeling more and more hostile to the user. I&#x27;m redirecting the money I was spending on Adobe products to open source donations. reply moelf 21 hours agoparentprevis there an interchangeable format for this? .tiff? .ora? reply raincole 21 hours agorootparentIf interchangeable format means non-proprietary then Krita&#x27;s .kra is one. If it means a format designed to be used as interchangeable format then .ora.If it means one that everyone uses then none. reply alexvitkov 20 hours agorootparentThere is one that everyone uses, unfortunately. reply hospitalJail 21 hours agoparentprevHate defacto standards behind corporations.M$ Windows, Photoshop, each company gets locked into their own CAD nightmare.On the flip slide, we have Blender, Linux Server, (enough)SQL, Git, Stable Diffusion, and maybe python&#x2F;python libraries if that counts. reply raincole 21 hours agorootparentWhen I moved from web dev to game dev, the second most surprising thing I found is how much this industry depends on proprietary solutions compared to web dev.The most surprising thing is how little professional people get paid, again compared to web dev. reply johnnyanmac 19 hours agorootparentNot too surprising. It&#x27;s built into gaming&#x27;s history to work on and release a game purely woth in house tech. And then submit it to a proprietary console maker for a \"seal of quality\" (or what used to be that). And then other important tools (especially for artists, but a lot for programmibg) served this same mentality as AAA development grew. Support was more important than anything else as tools became more comple, and these studios weren&#x27;t used to sharing to begin with.The phenomenon of open source tools being able to even be considered by professional development is a very recent phenomenon (in the grand scheme of things). But for programmers it&#x27;s still difficult because console support is so important and still extremely closed down. reply phone8675309 21 hours agorootparentprev> The most surprising thing is how little professional people get paid, again compared to web dev.They pay less because they can - there are many more highly motivated people who dream of making video games than there are open spots.People are less highly motivated who dream about making Another Corporate CRUD App 2.0 (this time with Firefox support), and there are a lot more job openings. reply raincole 21 hours agorootparentYes, I understand that. It&#x27;s just like filmmaking industry but not that bad.If I had had the same level of understanding of economics as I do now, I probably would have stayed in web backend development. reply pxc 20 hours agorootparentFrom an engineering perspective, are there any things you prefer, or are at least glad to have learned from, in game dev? reply nunodonato 20 hours agorootparentprevthat&#x27;s why I switched from gamedev to webdev :D reply BeFlatXIII 19 hours agoparentprevDoes that one (1) non-Photoshop artist prefer .xcf? reply raincole 16 hours agorootparentClip Studio Paint. I highly doubt any of them even knows .xcf (I didn&#x27;t ask tho). reply Kelteseth 21 hours agoparentprevWhy not Affinity Designer&#x2F;Photo? reply raincole 21 hours agorootparentPerhaps it&#x27;s a just me, but I&#x27;ve never met someone who uses Affinity Photo as their main digital painting app. The one artist who doesn&#x27;t use PS actually uses Clip Studio Paint.I&#x27;m not saying it&#x27;s not a great app (I&#x27;m not familiar with it enough to judge it). But if I&#x27;m not using an open source one, I&#x27;d use the industry standard one. reply hoistbypetard 20 hours agorootparentI&#x27;m not a real artist. I do programmer art for game jams, MVPs, UI mockups, etc. I always liked Affinity Designer on iPad for that. It&#x27;s got nice Apple Pencil support, it was inexpensive, and it works with other file formats pretty well.Last time they had a sale on their \"Universal\" bundle, which got you Mac, Windows and iPad licenses for all their products for $100, I had just received some illustrator files that I wanted to work with on my desktop and decided to go ahead and pick that up.I now find myself using Photo and Designer instead of Krita&#x2F;Gimp and Inkscape, because I&#x27;m more efficient with it as a non-expert user and because the .psd and .ai interop is better.I need to deal with one of those two file types approximately 0.75 times a month, so the industry standard one feels eye-wateringly expensive. But a one-time $100 fee for a less frustrating experience than the open source ones offered felt worthwhile to me. And they&#x27;ve grown on me to the point that I pick them up when I need to make something for a game jam, not just when I need the interop. reply 7moritz7 20 hours agorootparentThe Affinity Suite has substantially better UIs than the Adobe pendants. It just works immidiately without tutorials, and it works really well reply egypturnash 19 hours agorootparentprevI am a pro artist. Most of my artist friends who don&#x27;t use Photoshop either use Clip Studio or Procreate. In general the Clip&#x2F;Procreate split corresponds strongly to whether or not they do a lot of comics.There&#x27;s a handful of other programs that one or two of my friends use but absolutely zero of my friends have talked about using Affinity Photo. reply haunter 21 hours agoprevKrita is also sold as a paid version (store version [0]) for a one time fee (never goes on sale). So unlike most open source projects it might have a more steady income already. I don&#x27;t know anything about their sales numbers but according to SteamSpy they are in the 100-200k range [1] and that&#x27;s one store only not counting MS and Epic.I was actually arguing for a long time that more open source projects should do the same. Be on digital stores and sell the application for a one time fee.0, https:&#x2F;&#x2F;krita.org&#x2F;en&#x2F;download&#x2F;krita-desktop&#x2F;1, https:&#x2F;&#x2F;steamspy.com&#x2F;app&#x2F;280680 reply danShumway 19 hours agoparentKrita is also evidence that this model can work in some limited instances even though the program is completely monetarily free in every sense of the word. It&#x27;s not \"free after X months\", it&#x27;s not \"free but you pay for premium features\" -- it&#x27;s just a free download, but you can buy it in a store. There are only two reasons to buy Krita through Steam:1. You want to support Krita2. You want Steam to handle updating&#x2F;launchingAnd it turns out that&#x27;s enough for them to get some sales. People underestimate how often users will either take the path of least resistance or purchase something just to be nice. Typically when we run into this kind of behavior it&#x27;s negative, for example the struggle to try and get ordinary people to install ad blockers. But it&#x27;s nice with Krita to see that human instinct more positively leveraged. reply whoza 21 hours agoprevBut krita.org says \"Special thanks to Intel, Corporate GOLD Sponsor.\" What&#x27;s the reconciliation between these two pages? reply moelf 21 hours agoparenthttps:&#x2F;&#x2F;krita.org&#x2F;en&#x2F;item&#x2F;intel-becomes-first-krita-developm...maybe that has ended reply hallarempt 19 hours agorootparentNo, it&#x27;s still actual, but they don&#x27;t have a listing the fund.krita.org database because they sponsored us directly. reply danShumway 19 hours agorootparentMinor suggestion: I understand that for organizational purposes and clarity it might make sense to have them separate, but I suspect merging those stats together on this page might be better for the project overall?I suspect (but don&#x27;t have evidence for this) that sponsorship tends to snowball -- corporations become more likely to sponsor products that are already being sponsored; and the harm from users thinking that the project has \"enough\" funding might be outweighed from the benefits of users and orgs thinking, \"it&#x27;s normal to sponsor Krita, we&#x27;re not doing something weird or experimental by jumping on the bandwagon.\"Again, just an instinct though, I could be wrong, and I get that this page being part of a larger existing funding platform might make including direct sponsors more contentious or inappropriate. reply krisoft 18 hours agorootparentprevSounds like a thing you should fix then?As a software engineer I understand what you are saying, but it is incredibly misleading then. reply cornholio 21 hours agoprevThis type of software is an excellent fit for \"eventually open\" licenses: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12459492This way, corporations and business users can pay to get the latest version and features just like commercial software, while the hobbyist community still gets a relative recent and feature-full release they can use for free, has source available and is guaranteed to become fully GPL code in 3-5 years.The traditional open source economic models (support and customization, services etc.) simply do not work for professional software with a large number of non-programmer users. A commercial license would allow these clients to pull resources together to allow software to be written, and eventually released as open source for the benefit of all - and they are the most interested in this because they don&#x27;t want to get locked in. reply Macha 21 hours agoparentThis seems thankfully unlikely for Krita given its origins in the KDE project, which for example has the KDE Free Qt Foundation as a poison pill should Qt attempt such a move. reply sofixa 21 hours agoparentprevIf I&#x27;m not mistaken, that&#x27;s pretty similar to the reasoning behind BSL, which is even more \"open\" than \"eventually open\" - code is source available, anyone who isn&#x27;t a competitor can use it however they want outside of competition, and in a fixed amount of time (max 4 years) the code becomes truly and entirely free to be used however one wants, including competition. reply benatkin 20 hours agoparentprevNo it ain&#x27;t, those are just an attempt to pass something closed source off as open source.At least, I haven&#x27;t seen the authors give their blessing to a fork of the old version of their software, beyond the text of the license itself. Maybe it hasn&#x27;t been long enough, but it soon should be. reply cornholio 19 hours agorootparentWhy would the blessing matter, if the source is available and really licensed under a real open license after the exclusivity period? Why would the original authors give their blessing to a free competitor that aims to drive paying users away?Perhaps you approach this a bit too dogmatically. The objective for me as a user is to have good quality software, with source available, in a competitive market that does not lock me in - because it allows other free or commercial spin-offs with low entry barriers.If we can&#x27;t ever accept something that does not pass the Stallman purity test - even if it means open source programmers in some niches should starve - what we end up in those niches is binary blobs filled with spyware. And due to network effects (see the above .PSD discussion), we soon find ourselves forced to swallow the blob because it&#x27;s the only real option. reply orlandohill 21 hours agoparentprevAnother approach is to allow free non-commercial use, and to charge for commercial licenses. There are licenses that grant all the rights that non-commercial users want, but still require businesses to pay.https:&#x2F;&#x2F;polyformproject.org&#x2F;licenses&#x2F;noncommercial&#x2F;1.0.0&#x2F; https:&#x2F;&#x2F;prosperitylicense.com&#x2F; reply cornholio 20 hours agorootparentBut that&#x27;s no longer open source, and offers no protection to the users against lock in. All it takes is some change of leadership or VC funding and the permissive license will be abandoned in favor of a proprietary revenue-maximizing model. Human nature and all.What is needed is a non-repudiable commitment from the vendor that the software will be fully open source, and a business incentive for them to continue developing the commercial version; they have a few years to monetize any new features, and to continue making money they need to continue bugfixing and creating other desirable new features, as opposed to just milking the lock-in cow. reply orlandohill 19 hours agorootparentBoth the PolyForm Noncommercial License and the Prosperity Public License are irrevocable. There is no lock-in. Everyone gets the code, just like with an OSI-approved \"open source\" license.Dual commercial&#x2F;non-commercial licensing like this is a simple way to require commercial users to fund the further development and maintenance of the software. reply cornholio 19 hours agorootparentThey are irrevocable, but they also disallow commercial redistribution of modified copies - in perpetuity.This means that when the original vendor changes new versions of the software to a draconic proprietary license (as is their right as the full owner of the copyright), the community can&#x27;t fork an older version and keep it up to date and distribute that; they can just use older versions until they become obsolete, incompatible, accumulate security holes etc. They are locked in to the vendor is they need those same features going forward. reply orlandohill 18 hours agorootparentThe commercial license typically grants the right to sublicense the software as part of a larger piece of software that can be sold commercially.Both commercial and non-commercial users can copy, modify, and redistribute modified and unmodified copies. Again, there is no lock-in.There&#x27;s little incentive for the developers to switch from dual commercial&#x2F;non-commercial licensing of the source code to only distributing compiled executables. The whole point of choosing the dual licensing model is that it&#x27;s more attractive to customers. You wouldn&#x27;t want to use such a model if you were trying to keep trade secrets, but in such a case you wouldn&#x27;t consider using an open source license either.Further reading:https:&#x2F;&#x2F;duallicensing.com&#x2F; https:&#x2F;&#x2F;indieopensource.com&#x2F;public-private&#x2F;indies reply cornholio 3 hours agorootparent> Both commercial and non-commercial users can copy, modify, and redistribute modified and unmodified copies.I don&#x27;t follow. Both examples you provided explicitly forbid commercial redistribution: The Prosperity Public License 3.0.0 license allows you to use *and share this software for noncommercial purposes for free* and to try this software for commercial purposes for thirty days [no other distribution allowances are made in the rest of the license] PolyForm Noncommercial License 1.0.0 Your license to distribute covers distributing the software with changes and new works permitted by *Changes and New Works License*. Changes and New Works License: The licensor grants you an additional copyright license to make changes and new works based on the software *for any permitted purpose*. [Complete list of permitted purposes] \"Any noncommercial purpose\" ; \"Personal use ... without any anticipated commercial application\"; \"Noncommercial Organizations\"So both licenses disallow distribution if done for a commercial purpose. Do you mean to say that you can dual license, under such a non-commercial license and also under an open license, that allows commercial distribution? But then, how would you discourage commercial users from simply downloading and using the open source version? We&#x27;re back to the service model of financing open source. reply orlandohill 23 minutes agorootparentThose are examples of non-commercial licenses, licenses that grant rights, free of charge, but restrict commercial use of software.When I say dual licensing, I am referring to the business model of offering software for free under a non-commercial license, and charging for commercial usage rights. Kyle E. Mitchell calls this Free-and-Paid Dual Licensing.https:&#x2F;&#x2F;writing.kemitchell.com&#x2F;2023&#x2F;09&#x2F;10&#x2F;Two-Kinds-Dual-Lic...To use such a business model you also need a commercial license. There&#x27;s no one-size-fits-all solution, and it&#x27;s normally something that requires input from a legal professional. Kyle has made a couple of recent efforts to improve things.https:&#x2F;&#x2F;fastpathlicense.com&#x2F; https:&#x2F;&#x2F;commercial.polyformproject.org&#x2F; replyorlandohill 19 hours agorootparentprevThe PolyForm Project also has other licenses. For example, the PolyForm Small Business License allows free use in organizations with fewer than 100 individuals and less than 1,000,000 USD in annual revenue.https:&#x2F;&#x2F;polyformproject.org&#x2F;licenses&#x2F;small-business&#x2F;1.0.0 reply scrpl 22 hours agoprevThat&#x27;s just sad. Open Source support really needs to be normalized in corporate environment. Now it&#x27;s more of an exception than the rule. reply masukomi 21 hours agoparentI agree, but in this particular case i have to ask... how many companies are actually USING Krita? My impression is that the vast majority of places that need software like that use Adobe Photoshop&#x2F;Illustrator, or Affinity Photo&#x2F;Designer. reply Gualdrapo 21 hours agorootparentThat&#x27;s exactly the problem.Not only that they use privative products - it&#x27;s that people think about Krita as an alternative to Photoshop, as Krita is intended for digital painting rather than general raster image manipulation. Hence narrowing the target of Krita to a much smaller audience. reply fluoridation 20 hours agorootparentTo an extent, it is, since a lot of artists use PS to draw&#x2F;paint. For those people, Krita is indeed an alternative to PS. reply numpad0 20 hours agorootparentprevProbably not many if you don&#x27;t count small individual art studios - the mobile gacha game industry(and anime animation to some extent) don&#x27;t standardize art styles and pipeline art production as done in American movie and comic industries, but relies on intimate collaborations with external, individual artists for creative components.So they mostly only import (Krita-exported) PSD into Ps, or even if Krita was used professionally on the floor by employed artists, choice of tools would be up to artist&#x27;s discretion and might not become a corporate talking point in the way, say, what Maya or Lightwave debate would be.Maybe OnlyFans&#x2F;Patreon could throw a million or two for couple years...? But Krita is not the first choice across the board, and creators on those platform don&#x27;t seem too concerned with CSP&#x2F;Procreate subscriptions, so that might be a difficult path too? reply miniupuchaty 20 hours agorootparentprevI see it used by concept-art artists in gamedev studios from time to time. reply JumpCrisscross 21 hours agorootparentprev> how many companies are actually USING Krita?Assuming a product that advertises its lack of corporate support won’t be super welcoming of commercial users. reply molave 12 hours agoparentprevIn a corporate setting, it will help if open source software has easy deployment configurations to track usage and ensure vulnerable versions are not lurking somewhere. Firefox for instance has this. reply 7moritz7 20 hours agoparentprev> Open Source support really needs to be normalized in corporate environmentIt won&#x27;t. The only real workaround right now is to simultaneously launch SaaS alongside the FOSS project and monetize that heavily. reply basscomm 19 hours agorootparent> It won&#x27;t. The only real workaround right now is to simultaneously launch SaaS alongside the FOSS project and monetize that heavily.It can work. Paying for software is already a normal part of doing business, make this work to your advantage. For example:- In the budgeting process just add a line item for the FOSS software you&#x27;re using and put a number on it that&#x27;s lower than the proprietary alternative. - If you&#x27;re already using the software (like Krita in this case), tell whoever is in charge of the purse strings how much time, effort, and money the software has saved the company and ask them to make a one-off or recurring payment to the project that&#x27;s lower than the alternative. You&#x27;ll be surprised how often they say yes (as long as they can get a receipt) reply doublerabbit 22 hours agoparentprevWhen corporate support rolls in, project tend to turn sour.Demand from the corporate entities proceed to dominate rather than voices from individual donations. Its a double edged sword. reply scrpl 21 hours agorootparentThat&#x27;s because many corporate \"donations\" are not so much a donation as a way of soft-buying a feature.It&#x27;s hard for businesses hyperfocused on short-term gains to understand a long-term value of, for example, supporting an alternative for an industry-dominating Adobe toolkit. But the value is there. reply marcinzm 21 hours agorootparentLong-term value that&#x27;s hard to define doesn&#x27;t translate well to stock price especially when any investment also helps competitors who aren&#x27;t investing anything into the project. reply raincole 21 hours agorootparentprevAnd the examples are? Cause the counter examples are phenomenal like Blender and Linux. reply numpad0 19 hours agorootparentprevKhara threw a bag of pachinko money in Blender&#x27;s face to make the last Evangelion film work, and it was fine. I guess that was a rare occurrence that they desperately and so purely needed a tool they can hand to broke freelancers without frantically searching for keygens, but it can totally happen when incentives are right. reply AraceliHarker 21 hours agorootparentprevKrita is accepting corporate donations. reply mdtrooper 20 hours agoprevIt remembers to me, the HeartBleed: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heartbleed where there was only one person with salary in a important project. reply grumpymouse 14 hours agoprevIt always feels like open source enthusiasts would never pay for something themselves, but expect that their boss will for some reason. What would your boss get by paying that he&#x2F;she isn&#x27;t already getting for free? reply distances 14 hours agoparentIt would help if the projects had some option to purchase a license or subscription, even if the application is open source. Lots of employees have a company credit card and the authorization to use it for small productivity purchases. But it would have to be a business expense that is easy to file, not a donation. reply Bilal_io 14 hours agoparentprevBecause we expect the person making money from the tool to help pay for it. This applies to individuals as well if the tool is making them money reply yuukisenshi 18 hours agoprevThe problem here to me is that Krita is simply a mess.Krita is supposedly part of the KDE project, which is committed to switching to wayland, but has 0 wayland support. In practice, that means that if you use other input devices they can have bugs that make it unusable.So it&#x27;s Linux story is weak. A very bad start for a KDE app...but at least maybe on Windows it works well.Well the problem then is it does the classic blender thing of having non standard shortcuts and usage modes, except unlike blender it does so for literally no reason. Buttons in most of these programs, for certain classes of operations are standardized, and Krita just works differently for the fun of it. No justifications, just \"spend a long time changing it if you want it different.\"The icons are also incredibly ugly and people get defensive about them. The QT thememing is fine, but the actual icons for brushes and the like look amateurish at best and really brings down the cohesiveness of the program.Certain tools are also abysmal to use. The text tool is a joke. Then you have certain operations being incredibly slow.I have worked as a professional artist for years. People here really seem to be implying photoshop is the \"big dog\" in the space, but this simply isn&#x27;t true. I see far more CSP users than Photoshop users and among my coworkers and haven&#x27;t used photoshop for anything but some post processing for years. Krita then needs to compete with CSP, and Procreate (which has gotten huge recently.) To be frank, it doesn&#x27;t. CSP works mediocorely with Wine but because the input problems don&#x27;t exist that exist with Krita I have taken to using that on Linux ironically because I use KDE. Krita is basically a non option for me at this point and has me considering making a GPU accelerated open source drawing app instead of suffering this current atrocious landscape. reply danShumway 17 hours agoparentI don&#x27;t have a huge amount of experience with Krita&#x27;s Wayland performance because the situation on Wayland is still sub-optimal for art in general. Tablet support has gotten much better, but configuration per-app or tied to the desktop environment still seems to be a big issue. The idea that my tablet configuration might stop working if I switch off of a desktop environment is kind of a non-starter to me. It stinks because I really want to use Wayland, but even as recently as a month ago I tried to see if I could make the switch and couldn&#x27;t.Truthfully, if someone has a lot of money to dump around, devoting a few full-time developers or dropping cash on https:&#x2F;&#x2F;github.com&#x2F;OpenTabletDriver&#x2F;OpenTabletDriver would probably go a long way towards encouraging Wayland adoption from artists.As far as I can tell the issue is not that Wayland can&#x27;t do tablet control on the same level as X11, it&#x27;s that the tools built around those capabilities still seem immature. OpenTabletDriver looks very promising but seems to have limited device support, limited in no small part by what tablets the devs have access to (my Cintiq 32 is unlikely to get added any time soon because it is no longer being sold and was expensive and uncommon when it was on the market -- which probably means I should try to do it, but it&#x27;s been tough to find the time).The NVIDIA situation is also a problem, but there&#x27;s nothing anyone can do about that other than yell at NVIDIA more.----> The text tool is a joke.The text tool should see considerable improvement soon; the entire text engine got rewritten in the last release, the devs just only had time to get it to feature parity with the existing tools. reply yuukisenshi 13 hours agorootparentI&#x27;m willing to deal with the Linuxisms of being on Linux (wayland having bad tablet support and controls, I agree with you!) but the fact that on top of that Krita works particularly bad on KDE&#x27;s default setup, whereas using CSP with Wine is a better experience is absurdity to me. reply mcpackieh 16 hours agoparentprevKrita has issues with X too. Using X11&#x2F;libinput touchpad: pinch-to-zoom doesn&#x27;t work in Krita but does work in KDE&#x27;s ms paint clone Kolourpaint.I&#x27;ve also found Krita to be unsatisfactory for drawing pixel art. Select the pixel art brush and set it to be 3px wide, then draw a rectangle with the rectangle tool. This creates a rectangle with inexplicably jagged edges (http:&#x2F;&#x2F;0x0.st&#x2F;HWyi.jpg). Turn off sharpness for the brush and try again, it seems to work now but selecting the background color with the similar color selection tool will reveal that the jagged edges are still present but very faint (http:&#x2F;&#x2F;0x0.st&#x2F;HWyz.jpg). All of this happens with a 3px brush but not a 2px brush. It&#x27;s bizarre. I suppose most digital painters don&#x27;t notice&#x2F;care about this, but it&#x27;s a deal breaker for me.I dislike the Krita icons as well. I would like colorful icons so my eyes can scan them quickly but some halfwit designer got it in their head that all the icons should be monochrome and there doesn&#x27;t seem to be any way I can choose or create other icon themes.I really want to like Krita, but it just doesn&#x27;t work with me. All I really want is \"MS Paint but with layers\" but that&#x27;s evidently too much to ask on Linux. I&#x27;ve tried more than a dozen painting programs and they&#x27;re all disappointing. reply yuukisenshi 13 hours agorootparentSame. I&#x27;m just using Windows programs now with wine that aren&#x27;t 100% there, but closer than anything Linux native I have found.The exception is for pixel art, Aseprite works fine for me.https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;88N1QKdExample of it on Linux with a decent theme. reply winter_blue 19 hours agoprevWhy does it say \"5 developers $ 17k &#x2F; month\"?Is that 1 developer of $17k &#x2F; month? (So a $204k annual salary?)Or, does it mean 5 developers for $17k &#x2F; month (i.e. a $40k annual salary?)If it&#x27;s the latter, I&#x27;m highly concerned.Even in countries like India salaries like $60k USD (INR 50 lakh) has become a bare minimum for software engineers who are even the least bit talented...For a really good software developer in India, you&#x27;d have to pay circa $180k USD (INR 1.5 cr), which is still a massive discount compared to the US, since in the US that person would probably make around $350k.Where are they hiring people? reply Uehreka 19 hours agoparentUhhh, $180k is an awesome developer salary in the US (as long as you’re not talking about silicon valley salaries, which most Americans aren’t).There are also definitely juniors in the US making $60k (and probably even less). reply winter_blue 10 hours agorootparent> as long as you’re not talking about silicon valley salaries, which most Americans aren’tSince WFH became more common, high salaries have spread across the US. Several members of my team are earning $250k to $400k (depending on their level), and are scattered across the US, and many are living super-LCOL cities. For example, one guy (who makes over $350k) lives in Utah, and around the start of COVID he bought a large house with a lot land surrounding it, for (to him) a measly $270k.> There are also definitely juniors in the US making $60k (and probably even less).Nobody should be making that little in the US. $120k is the minimum for a software engineer in the US, regardless of location. If a SWE is earning less than six figures in the US, they&#x27;re certainly being swindled. reply matkoniecz 18 hours agoparentprevMany open source projects have people working for free and spending a lot of time on that. Even really low wages may enable more people to do this.I spend a lot of time on developing StreetComplete, got paid only for small part of that.And I would be able to spend more time on that and willing even with relatively small funds.I think that many people would be willing to take significant pay cut to work on project chosen by them. reply winter_blue 10 hours agorootparent> And I would be able to spend more time on that and willing even with relatively small funds.> I think that many people would be willing to take significant pay cut to work on project chosen by them.I definitely agree with this sentiment; I can relate to it strongly. reply tredre3 14 hours agoparentprevWow that whole comment suggests you&#x27;re SF&#x2F;SV-based.180k is top pay for a dev in most of America. 50-60k is entry-level pay for a dev in most of America. 350k is really uncommon outside of your bubble.Are you really suggesting Indians make as much and even more? Then why are companies offshoring to them when they could just hire local juniors for the same price? reply winter_blue 10 hours agorootparentYou can find \"engineers\" in India who&#x27;ll accept $6k &#x2F; year. But, with a few exceptions, most of these $6k&#x2F;year engineers are really quite terrible at what they do, and their English communication skills are...to put it lightly... not good.I&#x27;ve dealt with a contractor who hired such engineers. I remember the team, which I think had around 10 people (but I don&#x27;t recall the exact size). There was 1 guy, who was basically a 10X engineer, and pulled the weight of most of his team&#x2F;division. He wrote&#x2F;delivered most of the working code, while there were around 9 others on his team who contributed very little (mostly terribly buggy code which he had to fix).Yea, they were paying each person on the team $6k (the manager–who was pretty much useless -- was probably earning like $12k), and they were spending around $60k to $70k in total on this 10-person team. But–they could have pretty much just fired the entire team except for the one guy who was producing most of the code, and gotten the same (or better results). He would&#x27;ve been happier, and he wouldn&#x27;t been dragged down by all those other nearly-non-contributing folks.(For what it was worth, I talked to this guy (who was from Rajasthan), and suggested that he find a better job that paid better and valued his skills better. And I even offered to recommend him, and help him find a job abroad with a company that&#x27;d sponsor a visa for him; but he turned down my offer–he was actually, quite bizarrely, worried about some form of retaliation, if he left that contractor&#x2F;team. It was strange indeed...)Of course, if you want to find a software engineer in India who can actually deliver results (and communicate well in English), you should be willing to pay them at least $60k USD (INR 50 lakh) &#x2F; year. Any less, and you&#x27;ll basically \"get what you pay for\" unless you luck out and land some talented engineer who doesn&#x27;t know his skills&#x27; worth.Fwiw, $60k USD is still a quite cheaper than the US (a 50% discount), since I&#x27;d say in the US, the bare minimum for a software engineer today stands at around $120k USD. reply Maken 22 hours agoprevBut they can already support like one and a half full time developers. Krita future looks bright. reply mrbump 21 hours agoparentAre they really paying a full time developer only 3000 €&#x2F;month? reply hiAndrewQuinn 21 hours agorootparentThat&#x27;s a life changing income in some parts of the world, and a pittance for skilled knowledge work in others. I hope it&#x27;s going towards someone in the former, and making the local market for tech talent that bit more competitive. reply masukomi 21 hours agorootparentprevit should be noted that european developer salaries are SIGNIFICANTLY lower than US ones.US dev salaries have been completely skewed by stupid levels of VC money. European ones not so much. reply JonChesterfield 21 hours agorootparentAlternatively they&#x27;ve been completely skewed by the profit margins of the big software companies, where it turns out they print money regardless of how high payroll gets. As supporting evidence, VC funded startups pay less than said software companies.It&#x27;s interesting that Europe has abjectly failed to produce anything like Google or Meta. I&#x27;m not sure what the underlying reason for that is. reply JumpCrisscross 21 hours agorootparent> they&#x27;ve been completely skewed by the profit margins of the big software companiesThis is correct. VC-backed companies aren’t leading base compensation gains. reply sofixa 21 hours agorootparentprev> It&#x27;s interesting that Europe has abjectly failed to produce anything like Google or Meta. I&#x27;m not sure what the underlying reason for that is.There isn&#x27;t a singular reason, it&#x27;s a complex combination. Btw I&#x27;d like to preface this to say I&#x27;m not sure most Europeans actually want a Google or a Meta, there is aversion to \"too big to fail\" companies in most sectors.* investment money - VCs bet on tons of stupid things with the hope of some of them making it. Many a crappy business model has received hundreds of millions of investments to try and make it, and companies spend years chasing growth on the back of those investments without worrying about profitability. Investment in the EU is usually more conservative and grounded in reality - a business model of \"we&#x27;ll give it away at a loss for 10 years to get lots of market share and then increase prices to capture the market\" simply won&#x27;t fly here.* \"Europe\" isn&#x27;t a single market in most important senses of that word. Each EU country has it&#x27;s own language (okay there&#x27;s some overlap like Czech and Slovak, Belgium and France&#x2F;Netherlands, Ireland and UK before Brexit, but generally), laws, regulations. A single business can&#x27;t just immediately serve the whole of the EU without doing due diligence, translations, checking what regulations might apply for them, etc. That means that the size of the potential market is limited from the start without extra investment. A French startup can only sell in France until they figure out what is needed to sell in the Spanish market, translate websites&#x2F;products, hire support people that speak Spanish, etc. etc. etc. There are tons of good quality decently successful European startups, but most stay within one or a few countries. Exceptions are purely digital companies such as Spotify who can afford to sell all around the world with relatively few hurdles.* Regulations and common decency&#x2F;fear - over here, a business model of \"we&#x27;ll fake sell medical devices\" or \"we&#x27;ll trick people into giving us all their movements&#x2F;desires&#x2F;internet history and sell that to whoever wants it\" will hardly fly. Not that there aren&#x27;t unscrupulous people here, there are, but it&#x27;d be harder to get investment and talent to work for you.* Better... I&#x27;m going to go with social safety net, but that&#x27;s only a part of it. Over here, people are generally more content and know they have things to fall back on, including retirement. FIRE (Financial Independence, Retire Early), \"grind mindset\", \"hustle mindset\" and similar are quite rare here. People prioritise other things than work, don&#x27;t live to work, and don&#x27;t measure themselves (only) on work. So hustling to hit big and become massive is much more rarely seen as a good or desired thing.People often deride the EU for \"lacking innovation\", but IMO that&#x27;s flat out wrong - those people use wrong measurements (lack of massive tech giants) to define innovation. There are tons of European startups and scaleups and mittelstands and b2b companies of all sizes that are successful and innovative. They&#x27;re just not \"infinite growth\" global behemots, but.. do they need to be? Is that the thing that ultimately matters? reply JumpCrisscross 21 hours agorootparent> tons of European startups and scaleups and mittelstands and b2b companies of all sizes that are successful and innovative. They&#x27;re just not \"infinite growth\" global behemotsIf their business has no economies of scale, no. If it does, they won’t survive without subsidies.At a certain point, subsidising a low-scale domestic replica of an efficient international option breaks due to (a) the internet and consumer choice or (b) cost. reply sofixa 20 hours agorootparentIf the market was theoretical and all things were absolutely equal, yes, maybe, but that&#x27;s not how things work in the real world.For a good example, Walmart failed miserably in Germany because they failed to understand the local market in any way. You can have a successful regional chain of supermarkets without it needing to become Walmart-scale. Just being local, having a strong local presence and understanding of the market, and having local costs can be a massive advantage. A Bulgarian startup has economies of local labour costs that can trump the economies of scale of Google. Spotify are quite successful even if their competitors (Apple, Google, Amazon) are massive. Not to mention there are big market segments where economies of scale across markets simply do not apply. Legalstart, a startup doing \"law services online\" for small and medium enterprises in France cannot apply pretty much anything to any other country due to the different legal systems. If there was some massive behemoth in that space globally, Legalstart still wouldn&#x27;t have an \"economies of scale\" disadvantage. reply bluGill 20 hours agorootparentprevThere are a lot of businesses that are great for a small shop and will make a lot of money, but they have no potential to grow larger. Those businesses are not a domestic replica of some efficient international option as there never will be an international option. If you to become a billionaire you have to start one of those international businesses, but if you are content with a million you can get that on a much smaller local company and you don&#x27;t have to deal with investors at all.Software does lend itself to international options more than other fields as the upfront costs are high and ongoing costs are low. (compare to plumbers where you can start a company with a van and a fittings, but you have to pay the plumbers you hire every year - by the time you pay the plumbers and the office workers to schedule them there isn&#x27;t much $ left over) reply JumpCrisscross 20 hours agorootparent> lot of businesses that are great for a small shop and will make a lot of money, but they have no potential to grow largerYou’re describing businesses without economies of scale. They can compete simply by being local. reply johnnyanmac 19 hours agorootparentScale seems relative here, no? Scaling to serve a few nearby countries is still some sort of scale, just not worldwide scale. reply JumpCrisscross 18 hours agorootparentEconomies of scale can plateau, yes. In those cases I’d argue there’s a niche for a medium-sized company. (Most law is in this category.) But if there are further economies of scale, the company that seizes the worldwide market will simply have lower costs and more R&D capital to work with. reply sofixa 17 hours agorootparent> the company that seizes the worldwide market will simply have lower costsNot necessarily. Let&#x27;s imagine a company that does budgeting and bank account centralisation, sold as a SaaS. A global company has to work on integrations with banks all around the world, data privacy regulations all around the world, translations including in right to left languages, fun stuff like UPI in India, cash payments to a machine in Japan, currency conversions etc. Meanwhile a Bulgarian startup in that space only needs to interface with the 15 local banks and use EU-mandated APIs that make their lives easier (all integrations are the same), and provide only one language, one currency. They don&#x27;t need employees policies for 50 different countries, with local HR and legal departments&#x2F;subcontractors everywhere. Not to mention layers of management to scale.Do you still think the global company will have lower costs? reply JumpCrisscross 14 hours agorootparent> A global company has to work on integrations with banks all around the world, data privacy regulations all around the world, translations including in right to left languages, fun stuff like UPI in India, cash payments to a machine in Japan, currency conversions etc. Meanwhile a Bulgarian startup in that space only needs to interface with the 15 local banks and use EU-mandated APIsYou&#x27;re describing a system with economies of scale, up to a point, followed by negative economies of scale. That&#x27;s my argument: this is a good business for a European company to dominate. replygrumpymouse 13 hours agorootparentprevI would argue that many of the big tech companies have also been subsidised rather than being efficient. It has just been private investors rather than governments (and it feels like we are starting to see the end of that). reply bluGill 21 hours agorootparentprevGreat developers in India make more than Great developers in Europe in my experience. (India has a lot of okay developers who don&#x27;t make very much, but if you want someone great you will pay more) some of this is Europe culture makes potentially great engineers limit themselves to good - they refuse promotions to great, and they refuse to officially mentor younger engineers (they do mentor but in ways where they don&#x27;t get credit for as credit would them them a promotion). As such it is difficult to have great engineers leading the project and without that leadership you can&#x27;t have a project at all. reply chmod775 21 hours agorootparentprevWith 3,000 Euro &#x2F; month you can roughly employ someone making 2,400 Euro &#x2F; month pre-tax (gross) in Germany. That&#x27;s supermarket cashier money.And only if you pinch pennies (there&#x27;s just a ~100 Euro margin here after unavoidable costs). The rule of thumb in Germany is that an employee is going to cost you roughly 1.7x their gross salary: that&#x27;d be wages of 1,700 Euro &#x2F; month.The latter is going to pay for 1&#x2F;4th of a decent software developer - and even then you&#x27;re better promising more than 30 paid vacation days among other benefits.Basically: forget about it. reply matkoniecz 18 hours agorootparentprevMany open source projects have people working for free and spending a lot of time on that. Even really low wages may enable more people to do this.I spend a lot of time on developing StreetComplete, got paid only for small part of that.And I would be able to spend more time on that and willing even with relatively small funds.I think that many people would be willing to take significant pay cut to work on project chosen by them.For reference, 3000 €&#x2F;month would be a low salary in Poland for a programmer but really good overall. I would take it if I would be paid for OpenStreetMap development. reply lsferreira42 18 hours agorootparentprev3000 €&#x2F;month is a 16k salary in Brazil this make you top 1% earners in the country! reply jehb 21 hours agorootparentprevI do hope it&#x27;s a fair and living wage for whomever is receiving it.That said, I absolutely would take a significant pay cut to get to work on a project I cared about again. Golden handcuffs are not absolute. reply nisegami 21 hours agorootparentprevThat would be a meaningful raise for me actually, but the skillset is a lot more specialized as well. reply nicce 21 hours agoparentprevThe problem is that commercial company might have 20 full-time developers and resources to use third-party services, and they compete with Krita. Can Krita stay competitive enough to attract more funders and users? If they can&#x27;t, userbase (mostly because of the AI) will shift more towards commercial products, and over long period, Krita will die. I hope that won&#x27;t happen. reply exabrial 21 hours agoprevThis looks like a really neat project… But on the homepage it does say, Intel is a gold corporate sponsor? Look, I don’t have a horse in the race, but that is kind of silly :) reply fluoridation 20 hours agoparentThey must have pulled out recently enough that they haven&#x27;t had time to update that image. On https:&#x2F;&#x2F;fund.krita.org&#x2F; (which is where the image links to) Intel is not listed on any of the tiers. reply max_ 17 hours agoprevIs Krita used in the corporate world? What companies or niche industries use it heavily?I thought it was more of a tool for hobbyists? reply omgmajk 21 hours agoprevCurious. After all I&#x27;ve heard about Krita in the last few years one would think at least a couple of larger studios would use it to some extent. reply moelf 21 hours agoparentthe Diamond tier seems to be a studio, but yeah I totally agree. reply egypturnash 19 hours agorootparentASIFA isn&#x27;t a studio, it&#x27;s a non-profit that exists to \"promote and encourage the art and craft of animation\", to quote their about page. Their most notable effort is putting on the yearly Annie awards. reply srmarm 20 hours agoprevIs one of their top 3 supporters, the Titanium level &#x27;User Manuals&#x27; just paying for a good backlink? reply moelf 20 hours agoparentI think they could be drawing those menus with Krita reply crawsome 20 hours agoprevIf their text editor wasn&#x27;t a UX disaster, I&#x27;d have moved to Krita. It&#x27;s so bad and it drags down the whole program. The devs acknowledge it&#x27;s not great, but have not done anything to make it good enough. reply HKH2 18 hours agoparentYep it does stick out. A big dialog box obscuring the view. Type a font size in the box, and if it loses focus it will reset to the original font size. reply wahnfrieden 21 hours agoprevI use this instead of Acorn&#x2F;Gimp&#x2F;etc now for regular web image editing (for app dev) reply cpach 19 hours agoparentCool. I haven’t used Krita, but I’ve always thought of it as a painting tool. But apparently it can do more than that! reply wahnfrieden 6 hours agorootparentYes I use it only for simple utility work. It has nice advanced features that help sometimes. reply stratigos 19 hours agoprevI love Krita! ;-) Happy to send em $5 reply chaostheory 20 hours agoprevIm surprised that Hollywood didn’t put anything in reply dmw_ng 21 hours agoprevI genuinely wonder how much that anime waifu logo is costing the project reply Tao3300 13 hours agoparentYou don&#x27;t like Kiki the Cyber Squirrel?LibreOffice nearly got a Tyson Tan mascot called Libbie the Cyber Oryx. She was silently eliminated from the initial round of votes, in favor of a bunch of half-assed attempts that included two barely modified Duolingo owls. This perceived slight of what had been assumed to be a democratic process caused such a dustup that LibreOffice decided it didn&#x27;t want a mascot anymore.https:&#x2F;&#x2F;design.blog.documentfoundation.org&#x2F;2017&#x2F;11&#x2F;13&#x2F;mascot... reply numpad0 21 hours agoparentprevBut that&#x27;s what it&#x27;s for. Same story as Photoshop and GIMP splash screens. reply dmw_ng 21 hours agorootparentI didn&#x27;t think you meant that literally, but after paging through a few tens of google image search results for \"krita samples\", it seems the only thing Krita is actually used by people for is fantasy art.Compare to \"corel painter samples\" where the 2nd thumbnail is a fairly impressive Audrey Hepburn renditionCan&#x27;t remember the last time I saw output of the style from the first search result being used to sell pretty much anything reply numpad0 20 hours agorootparentIn case my comment sounded a bit harsh, no offense intended && taken at my side - I mean, it&#x27;s an industry of its own and a rapidly exploding one, so there are enough reasons couple lesser known industry standard software exists. reply wodenokoto 17 hours agoparentprevI’d be surprised if it is not donated. reply chmod775 21 hours agoparentprevArtists aren&#x27;t really a crowd to display childish prejudices when it comes to anything art. reply catapart 21 hours agoprev [–] I like Krita&#x27;s development ethos, but I really can&#x27;t use the product at all. If I want software that doesn&#x27;t respect my intentions, but provides the kitchen sink for functionality, I can find that better represented and supported in Adobe, Paint.Net, Paint Tool Sai; and those are just the raster comparisons. I honestly can&#x27;t wait until design tools figure out what year it is and start developing blue-sky versions that work with multiple devices and accounts, simultaneously, to provide honed experiences for specific project work. I can&#x27;t say I blame the lack of corporate sponsors, because I wouldn&#x27;t - as an organization - find utility in the product.That said, it&#x27;s still a shame! Seems like there would be someone out there with similar interests. I hope this post, or whatever else, helps evangelize this as a problem! reply johnnyanmac 19 hours agoparent>to provide honed experiences for specific project workYou need someone to lead that charge first. And that someone needs to understand each format and likely pay some programmer for a long time to make and more importantly, polish, such a feature. Open source is notoriously weak at polishing, so payment seems inevitable to keeop interest.When you are considering support for multiple devices you multiply said effort. Stylus vs touch screen controls are a different set of UX if you want it to be polished. reply actionfromafar 21 hours agoparentprev [–] How would a new solution respect your intentions? reply catapart 21 hours agorootparent [–] It would implement features specific to and invaluable for whatever I&#x27;m intending to work on, in such a way that I would feel like trying to do it any other way would be wasting my own time.For an example, Clip Studio Paint - a tool unabashedly designed for and advertised to comic artists - has a design concept of a &#x27;comic panel&#x27;. This lets you drag out a panel, instead of an arbitrary shape that you will use AS a panel. The panel, also an arbitrary shape, comes with functionality like \"bleed\" which allows you to continue drawing outside of the panel, but will truncate that content when you de-select the panel. It includes concepts like \"path rendered borders\" so that you can do weird stuff with borders that you wouldn&#x27;t want to do with an arbitrary shape (like have characters break through them).So, you end up with a nice little feature that can ABSOLUTELY be recreated in other programs. Nothing in that \"panel\" feature couldn&#x27;t be done with a series of steps and possibly some macros in other applications. The problem is, it&#x27;s all of that setup (wasting my time) instead of just having the feature available because it&#x27;s a well-known process with well-known problems. These are the people who are best suited to know what good processes look like, and can build the best kinds of tools for those well-known processes. Of course, they have to keep up to date, but that&#x27;s any software in any industry.Just to head off any assumption that I prefer CSP or something, I will say that they fall woefully behind on other stuff that is not comic related (understandable because of their history, but still a hindrance to me using it). That aside, a separate example is that every single drawing program should have the ability for you to move \"tool settings\" onto another device. No question. I should be able to work from my tablet, and pick colors on my phone. Or work from my desktop and change brushes or sizes or store values from one project, while I open the next desktop tab for a different project, all on my second screen. As far as I know, none of them do this.The overall point being \"We&#x27;ve designed apps to work with data like a computer uses that data. I would like it if we started designing apps that work with data however they have to in order for humans to work with &#x27;concepts&#x27; in whichever way they understand those concepts (to the best of their ability, which is pretty able these days).\" replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Krita Development Fund is campaigning for financial assistance to hire over ten full-time developers for Krita, a popular open-source digital painting software.",
      "Individuals and corporations may join the fund by selecting a membership level and pledging a monthly contribution. Different membership options offering varying levels of public or private acknowledgment are available.",
      "Currently, the fund has 349 individual contributors but no corporate ones, with its total monthly contributions amounting to €4426."
    ],
    "commentSummary": [
      "The discourse encompassed various facets of open-source software, touching aspects like funding, potential regulatory requirements, and the pros and cons of such tools.",
      "The conversation also focused on Krita software, detailing its use-cases, restrictions, licensing and funding models, as well as constructive criticism and its role in corporates.",
      "Other broader topics covered were the pay disparities among software developers and the influence of economies of scale on business success."
    ],
    "points": 292,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1696505898
  },
  {
    "id": 37777440,
    "title": "US Government issues first-ever space debris penalty to Dish Network",
    "originLink": "https://www.theguardian.com/science/2023/oct/02/fcc-space-debris-fine-dish-network-satellite",
    "originBody": "Skip to main content Skip to navigation Skip to navigation Print subscriptions Sign in Search jobs Search International edition The Guardian - Back to home The Guardian Support the Guardian Fund independent journalism with $5 per month Support us News Opinion Sport Culture Lifestyle Show More World UK Climate crisis Environment Science Global development Football Tech Business Obituaries ‘As the space economy accelerates, we must be certain that operators comply with their commitments,’ said enforcement bureau chief Loyaan A Egal. Photograph: Electro Optic Systems/AFP/Getty Images Satellites US government issues first-ever space debris penalty to Dish Network Dish to pay $150,000 for failing to properly dispose of satellite and violating the FCC’s anti-space debris rule Abené Clayton Tue 3 Oct 2023 02.27 BST The US Federal Communications Commission (FCC) has issued its first fine to a company that violated its anti-space debris rule, the commission announced on Monday. Dish Network has to pay $150,000 to the commission over its failure to deorbit its EchoStar-7 satellite, which has been in space for more than two decades. Instead of properly deorbiting the satellite, Dish sent it into a “disposal orbit” at an altitude low enough to pose an orbital debris risk. Astronomers sound alarm over light pollution from huge new satellite Read more “As satellite operations become more prevalent and the space economy accelerates, we must be certain that operators comply with their commitments,” said Loyaan A Egal, the FCC’s enforcement bureau chief, in the statement announcing the Dish settlement. “This is a breakthrough settlement, making very clear the FCC has strong enforcement authority and capability to enforce its vitally important space debris rules.” In 2002, Dish launched the satellite into geostationary orbit – a field of space that begins 22,000 miles (36,000km) above Earth. It agreed in 2012 to an orbital debris mitigation plan that, upon completion of EchoStar-7’s mission, would send the the satellite 186 miles (300km) above where it was stationed, into a “graveyard orbit” where it would not be a risk to other active satellites. But in 2022, Dish realized that the satellite was low on propellant, and would not have enough to move to its intended destination. Instead, the satellite ended up only 76 miles (122 km) above the active geostationary orbit areas – 178 km off its mark. Space debris, broadly defined by the FCC as artificial objects orbiting Earth that are not functional spacecraft, has been a growing concern for the agency. It says that the more old material that stays in orbit, the harder it is for incoming satellites to start and complete new missions. In 2022, the FCC adopted a rule that would require satellite operators to dispose of their satellites within five years of mission completion. skip past newsletter promotion Sign up to First Thing Free daily newsletter Our US morning briefing breaks down the key stories of the day, telling you what’s happening and why it matters Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion “Right now there are thousands of metric tons of orbital debris in the air above – and it is going to grow,” FCC chair Jessica Rosenworcel said in a 2022 statement that accompanied the announcement of the rule. “We need to address it. Because if we don’t, this space junk could constrain new opportunities.” Explore more on these topics Satellites Space news Reuse this content Most viewed World UK Climate crisis Environment Science Global development Football Tech Business Obituaries News Opinion Sport Culture Lifestyle Original reporting and incisive analysis, direct from the Guardian every morning Sign up for our email Help Complaints & corrections SecureDrop Work for us Privacy policy Cookie policy Terms & conditions Contact us All topics All writers Digital newspaper archive Facebook YouTube Instagram LinkedIn Twitter Newsletters Advertise with us Search UK jobs Back to top © 2023 Guardian News & Media Limited or its affiliated companies. All rights reserved. (modern)",
    "commentLink": "https://news.ycombinator.com/item?id=37777440",
    "commentBody": "US Government issues first-ever space debris penalty to Dish NetworkHacker NewspastloginUS Government issues first-ever space debris penalty to Dish Network (theguardian.com) 265 points by PaulHoule 22 hours ago| hidepastfavorite99 comments rdtsc 21 hours ago$150k sounds like a rounding error for a satellite operator. Is there a rapidly increasing fee schedule or other penalties, such us restricted future launches? Otherwise it would be folded into the regular operating expenses. reply jerf 20 hours agoparentIn general, a lot of government fees and penalties are not just about that penalty, it&#x27;s about the fact that if you do it again, the next one will be much, much bigger. Read the $150K as a warning shot across the bow.There are some places where this doesn&#x27;t seem to operate very well, particularly in the financial world, but in general, this is often the idea behind these penalties. Especially for the first time a law is getting enforced; it isn&#x27;t really very just to ignore a law for years&#x2F;decades and then one day out of the blue slam someone with multi-million dollar judgments. There are even some Common Law doctrines that the targets could use to argue against such penalties holding up in court, though, as always, trying to counter a written law with common law general doctrines is tricky. But I&#x27;d say there&#x27;s at least a good chance you could get a sudden, huge initial penalty argued way down in a lawsuit. reply 3pt14159 20 hours agorootparentYes. And also, it&#x27;s bad PR for the team responsible. Managers don&#x27;t want to have trouble hiring people. After the first fine, the message can be \"we&#x27;re modernizing to stop this from happening!\" But getting multiple fines over years is going to make the best of the talent poor essentially unavailable to hire. reply slingnow 19 hours agorootparentYou really think their hiring pool is going to shrink because they got fined for $150,000? The person they are hiring is likely going to cost more than the whole fine.And OK, let&#x27;s say they keep getting fined. Seems like a badge of honor these days. Move fast and break things, right? The government just doesn&#x27;t like, get us, man. We&#x27;re the good guys here and they&#x27;re trying to slow us down with red tape and bureaucracy. reply lostlogin 19 hours agorootparentI think the point you’re responding to is saying it isn’t about the money, it’s the reputation hit.If you have several choices and one is getting sanctioned for shitty practices, maybe you’ll look elsewhere.Saying you’re a rocket engineer sounds pretty neat, but the shine wears off a bit if people recognise your employer for negative reasons. reply mschuster91 17 hours agorootparent> Saying you’re a rocket engineer sounds pretty neat, but the shine wears off a bit if people recognise your employer for negative reasons.Which is what Tesla is experiencing at the moment in their facility Grünheide [1]... a toxic mixture of low wages, bad employment conditions, and reports about unsafe practices.[1] https:&#x2F;&#x2F;t3n.de&#x2F;news&#x2F;tesla-gruenheide-personalmangel-und-ange... reply parl_match 16 hours agorootparentI don&#x27;t know why this is getting downvoted? Tesla is a perfect example of the \"shine wearing off\". reply gorjusborg 16 hours agorootparentThat has more to do with Musk &#x27;doing a Yeezy&#x27; than anything the company did.I agree with the sentiment that the reputation hit is imaginary, and really only serves to prevent stiffer fines.Actually, they shouldn&#x27;t be charged a fine, instead, they should have to go and get it back. reply PawgerZ 13 hours agorootparentI think the shine has legitimately worn off on Tesla, though. I mean, how many times can you lie [1] about FSD before everyone writes you off as the boy who cried wolf.[1] https:&#x2F;&#x2F;motherfrunker.ca&#x2F;fsd&#x2F;#bottom replywillcipriano 19 hours agorootparentprevFrom the outside how would you tell the difference between this and a back door agreement between regulators to offer a low fine in exchange for favors down the line? reply mrWiz 19 hours agorootparentIf the second fine is also low that would indicate the first was not a shot across the bow. reply willcipriano 19 hours agorootparentBanks keep getting hit with (compared to revenue) low fines over and over again. This explanation is always given. Also the people who decide what the fine is going to be keep getting jobs at banks.Curious. Willing to bet some of these regulators kids end up at Dish over the next few years. reply jerf 15 hours agorootparentYes. I cited the financial industry for a reason.In reference to your previous question, along with the obvious brute fact of the fines not escalating for the same infraction over time, I would take a first stab at an answer being that the stronger the \"revolving door\" the less likely the fines are to be serious.Note that just a one way industry -> regulator won&#x27;t necessarily produce this effect, because in that situation a regulator can still want to flex by producing meaningful fines. But the more the cycle is industry -> regulator -> and back to industry, the more the regulator is going to be thinking in terms of what will come around when they&#x27;re in the industry again. reply ohthatsnotright 18 hours agorootparentprevI&#x27;m not really sure anybody wants to end up at Dish, their reputation is well deserved. reply willcipriano 18 hours agorootparentYou seem unfamiliar with how this works: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;No-show_job replykrisoft 21 hours agoparentprev> Otherwise it would be folded into the regular operating expenses.If you read the consent decree[1] you can see that the monetary fee is not the only thing DISH agreed to.In section 18 you can see they agreed to implement a compliance plan. Basically they improve on their fuel tracking, and end of mission planning. This makes it more likely that this thing won&#x27;t happen again, and also makes it easier to argue if it still happens that it happened wilfully.1: https:&#x2F;&#x2F;docs.fcc.gov&#x2F;public&#x2F;attachments&#x2F;DA-23-888A2.pdf reply rdtsc 16 hours agorootparentWell of course it won&#x27;t look like a traffic ticket, when they just pay with a credit card online and call it done. There is paperwork and \"promises\" and agreements. They have to have a compliance \"officer\" now and they have to report that propellant tracking has been implemented. That&#x27;s still a drop in the bucket to have to shorten the life of their satellites. The fuel alone probably costs more than $150k. reply willcipriano 20 hours agorootparentprevWouldn&#x27;t they have also implemented a compliance program if they were fined 10% of revenue, you know to avoid future fines? reply cryptonector 17 hours agorootparentThis is like getting a traffic infraction warning ticket from a police officer instead of an actual summons. If it has the desired effect then it&#x27;s better than the alternative. reply AnthonyMouse 14 hours agorootparentprevhttps:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;todayilearned&#x2F;comments&#x2F;85j254&#x2F;til_o...> Wouldn&#x27;t they have also implemented a compliance program if they were fined 10% of revenue, you know to avoid future fines?They would have spent up to 10% of revenue on lawyers and lobbying to get out of the fine entirely and thereby have no fear of another one. reply willcipriano 13 hours agorootparentSo they are too big to regulate? Great let&#x27;s split them up. reply AnthonyMouse 5 hours agorootparentIt&#x27;s not actually that big of a company. They only made ~$200M last year.But 10% of their revenue is $1.7B. It would cost them almost a decade&#x27;s worth of profit, so that&#x27;s how much they&#x27;d spend to prevent it, which is plenty enough to influence legislation. \"Percent of revenue\" fines are dumb and create crazy incentives. replyJumpCrisscross 18 hours agoparentprevIt’s a novel enforcement action. The point isn’t to bankrupt the American satellite industry, it’s to gently encourage better behaviour. reply bryanlarsen 20 hours agoparentprev> other penalties, such us restricted future launches?This. There are other compliance conditions, and failure to comply means no more launch licenses. reply philipwhiuk 20 hours agoparentprevIt establishes a level of liability and bad behaviour which means if it collides with something they&#x27;ll probably have an easy time suing dish for the cost of the satellite (millions). reply Fatnino 11 hours agoparentprevThey also had to accept liability. So if their space junk wrecks some other sattelite they will be super on the hook for the whole cost.Now, most probably nothing is out there waiting to get run over by a derelict Dish bird, but if there was... reply pavon 14 hours agoprevTimeline:2002 - Dish Network launches EchoStar-7 with a 10 year license from the FCC2004 - FCC passes new Orbital Debris Mitigation rules, grandfathering existing satellites[1]2012 - Dish Network applies for a license extension, which includes provisions according to the new rules2020 - FCC updates debris mitigation rules[2]2022 - Dish Network notices unexpected fuel loss, and begins moving satellite to graveyard orbit, but only makes it 122km instead of the required 300km.2022 - FCC updates debris mitigation rules again[3]I had remembered recent press releases about new debris mitigation rules and my first reaction when reading this story was surprise that the FCC was applying the new rules to a satellite launched over 20 years ago, but it appears that is not the case. Dish Network is being fined according to rules that were in place when they requested their license extensions, not the more recent rules.[1] https:&#x2F;&#x2F;www.fcc.gov&#x2F;document&#x2F;mitigration-orbital-debris[2] https:&#x2F;&#x2F;www.fcc.gov&#x2F;document&#x2F;fcc-updates-orbital-debris-miti...[3] https:&#x2F;&#x2F;www.fcc.gov&#x2F;document&#x2F;fcc-adopts-new-5-year-rule-deor... reply AtlasBarfed 11 hours agoparentSo how do they get it towed the remaining part? This is a geosynchronous satellite so it&#x27;s pretty far out there right?It almost seems like we need some ion drive cleanup tugs in orbit. Mayb they could reclaim reaction mass gradually from the solar wind. reply sschueller 21 hours agoprevNot enough, they should be require to fix their mistake by sending out another Satellite to either retrieve it or push it out where it was indented to be.Additionally to prevent a company from just going bankrupt they should be required to pay in advance a fee to a fund (ala FDIC) that will deal with issues that have to be dealt with. In this case it may be \"ok\" but what if this satellite was not out of the active geostationary zone and posed a risk to others important satellites? reply dotnet00 20 hours agoparentAsking that first point of a company right now would probably just kill the geostationary launch industry for a few years. Those kinds of mission rescue&#x2F;extension vehicles are still under development. There was one such mission a few years ago, but it was a special case for reasons I can&#x27;t recall right now. At the moment I think there are a handful of companies working on both adapters to be placed on the initial satellite and the MEV to go with it which would be launched in case of a problem with certain systems.Plus, retrieving it is almost guaranteed to be unrealistic. It&#x27;s out near geostationary orbit. The only options are to have an MEV move it into normal geostationary orbit or dump it into a graveyard orbit. reply Kinrany 17 hours agorootparentThe direction is good though. Instead of a fine they could be required to pay a space janitor to clean up their mess in the next N years. That would provide funding and motivation for space janitor companies to improve their technology faster. reply flerchin 21 hours agoparentprev> indented to beI suppose that in orbit, space has won out over tabs. :-)(sorry I couldn&#x27;t help myself) reply staunton 20 hours agorootparentJoke quality aside, you marked something as a quote which isn&#x27;t close to one, afaict. reply nordsieck 20 hours agorootparent> Joke quality aside, you marked something as a quote which isn&#x27;t close to one, afaict.Maybe read the first line of the parent comment more carefully?> Not enough, they should be require to fix their mistake by sending out another Satellite to either retrieve it or push it out where it was INDENTED TO BE[emphasis mine]. reply jiveturkey 16 hours agorootparentyou are commenting on this after it was corrected. so the snark is a bad look.the error wasn&#x27;t the typo in `indented`. it was that a single newline (\"CR\") is like an html space in HN formatting. it doesn&#x27;t start a new paragraph. GP notes that he only used a single \"CR\" and thus his own commentary ran together with his intended (also indented, in this case) quote. reply flerchin 18 hours agorootparentprevYou were right. I had to put double CRs for it to render the way I intended. Thanks. reply hk1337 15 hours agorootparentprevBravo! reply mcpackieh 21 hours agoparentprev> Not enough, they should be require to fix their mistake by sending out another Satellite to either retrieve it or push it out where it was indented to be.Seems like a plan with a bad risk&#x2F;reward ratio. Right now the satellite isn&#x27;t where it&#x27;s supposed to be, but neither is it directly in the way of other geostationary satellites. But if you send up another satellite to deliberately interface with this one and move both up, you risk something going wrong and either getting stuck with two satellites in that position, or worse, both crashing into each other and making a huge mess. It&#x27;s probably better to wait until the technology for grappling satellites in space is mature. This is presently an active area of research and development (OSAM), so a bit of patience should pay off. reply jancsika 20 hours agoparentprevIn an alternate reality there&#x27;s an equal but opposite HN quarterback:\"Too much! I get sending a message with a small initial fine like 150k plus requiring a compliance plan, but this high amount is a step too far. Prepayment is just the epitome of regulatory capture IMO. Even for selfish reasons a satellite company will strive to keep their satellites minimally functional and safe from any systemic dangers in orbit. But I guess Uncle Sam always has to get his cut.\" reply whimsicalism 19 hours agorootparentRealistically I almost never see upvoted comments saying a governmental fine was too much, even when the fine is, say, significantly greater than the entire companies revenue. reply bearjaws 19 hours agorootparentprevIt&#x27;s already in this comment thread lol.Space libertarianism here we go! reply rzimmerman 17 hours agoprevHonestly this is mostly fair. The fine is small but non-negligible. The infraction here is really the poor propellant tracking which is a mistake, not some nefarious plot or gross negligence. The satellite didn’t quite make the graveyard orbit but it’s out of the way. Tracking in GEO is only getting better, so it’s not really a debris risk. But you do have to issue fines to signal to the industry that getting deorbit plans wrong has a consequence. reply JoshTko 20 hours agoprevHow about we charge how much it will cost to remove debris... This will kill negligent operators as well as spur investment in removal research. reply londons_explore 20 hours agoparentThis debris is probably actually pretty cheap to remove by chance.The orbit it is in is perfectly between geostationary and the disposal orbit. That means any other satellite who is travelling from one orbit to the other can stop off on-route for no fuel or time cost.&#x27;grabbing&#x27; a satellite is fairly easy too. Even a fridge magnet would be enough to grab this satellite - since even tiny forces over many hours are enough in space.I therefore suspect that it would be possible to complete this disposal with a satellite already on orbit - ie. not specially designed for the purpose, and using very little additional fuel - perhaps some operational satellites even have enough extra in reserve. reply creaturemachine 20 hours agorootparentAre you really suggesting some satellite drop whatever its doing, break its orbit to meet up with this thing and attempt to push it, all while risking irreparable damage? These things are made for a lonely existence in space, so they don&#x27;t have bumpers or cameras or little fridge magnets for docking with other spacecraft. reply londons_explore 20 hours agorootparentNo - I&#x27;m suggesting that a satellite already at the end of its life, and heading to the graveyard orbit, go push it.Damage is unlikely - collision speeds would be measured in millimeters per second, and forces in millinewtons.For position accuracy, it would probably require a camera onboard - but most satellites already have a star tracker that includes a capable camera.The main risk is that the satellites touch at the &#x27;wrong angle&#x27; or blocking an attitude control thruster. That would immobilize both of them. reply krisoft 19 hours agorootparentYour suggestion does not comport with reality.> collision speeds would be measured in millimeters per second, and forces in millinewtons.What satellite ending their end of life, and already in orbit do you suggest are capable of such delicate manoeuvring? This is a very delicate task you are describing, and most satellites are simply not designed for it.> The main risk is that the satellites touch at the &#x27;wrong angle&#x27; or blocking an attitude control thruster.If someone tries this, the most likely outcome is that the satelites wizz by each other without ever touching. If you somehow manage to aim the \"pushing\" satellite accurately enough at the \"stranded\" satellite they would most likely collide at anything from a few hundred meter&#x2F;s to a few of km&#x2F;s depending on the orbital arrangements. reply msandford 19 hours agorootparentI don&#x27;t think you understand what geostationary orbit is. They all have to be going the same direction (with the Earth&#x27;s rotation) or else it isn&#x27;t stationary (not moving) over the geo (earth).As a result I think you&#x27;re misunderstanding what the orbital dynamics would be. reply krisoft 18 hours agorootparent> I don&#x27;t think you understand what geostationary orbit is.I do understand what geostationary orbit is.The commenter I was responding to said \"any other satellite who is travelling from one orbit to the other can stop off on-route for no fuel or time cost\". If they wanted to only talk about geo sats they should have said that.But sure, let&#x27;s think about using a geo sat to fly this intercept. Let&#x27;s just pick Echostar-8 as a concrete example. It is at 6.4 degree inclination while Echostart-7 is at 1.5 degree. So you will at best have a glancing blow between these two. reply londons_explore 19 hours agorootparentprevThis[1] is the typical satellites thruster:https:&#x2F;&#x2F;www.satnow.com&#x2F;products&#x2F;thrusters&#x2F;space-electric-thr...It has a thrust of ~20 milli Newtons.Power that up for 10 seconds on a satellite weighing 500 kg, and you will move an extra 0.4 millimeters per second.So how exactly is precise maneuvering hard? reply dotnet00 18 hours agorootparentGEO satellites typically don&#x27;t use hall effect thrusters for much. Primary propulsion for large GEO satellites is typically a hypergolic system. Ion thrusters, if present, are typically a station keeping tool.For example, the satellite bus used by EchoStar-7, which is the satellite in question here (https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Lockheed_Martin_A2100) uses Hydrazine and NTO for primary propulsion, with hydrazine monoprop for atittude control, and the monoprop thrusters and ion thrusters for station keeping.While the new version of that bus has the option for all-electric propulsion, it really isn&#x27;t clear how popular that option has actually been. There aren&#x27;t really many details about it. reply jaywalk 17 hours agorootparentprev> This[1] is the typical satellites thruster:Define \"typical\" because it&#x27;s not being used on any geostationary satellites.> Power that up for 10 seconds on a satellite weighing 500 kg500kg? Hilarious. These satellites are the size of a bus and their weights are measured in tons. reply jdiez17 18 hours agorootparentprevEven assuming that it would be practical to match the inoperative&#x27;s satellite orbit to the required precision to have \"collision speeds measured in mm&#x2F;s\", the contact interface would need to be perfectly on the center of mass - otherwise any further propulsive maneuvers would cause it to tip to one side. The exterior faces of satellites are really unsuitable for this unless they are specifically designed for this.> satellites already have a star tracker that includes a capable cameraWhich is focused at infinity. Neither the hardware nor the software is capable of tracking a big object nearby and providing feedback to align itself how you&#x27;re proposing, without significant changes.Edit: just to give an idea of how difficult docking in space is in general, take a look at the International Docking System Standard (used on the ISS): https:&#x2F;&#x2F;www.internationaldockingstandard.com&#x2F;Table 3.3.1.1-2 tells you the conditions for docking; velocities of 50-100mm&#x2F;s are expected. And look at how much hardware is needed to dampen the energy. It has 6 DoF actuators, hydraulic components, etc. etc. And that&#x27;s for the ISS - all spacecraft going there are specifically designed to be able to accurately approach and dock with the ISS. There&#x27;s just no way you can get that level of precision with a satellite bus designed for communications and station keeping. reply dotnet00 18 hours agorootparentprevThere are other big risks involved.For instance, if the satellite is dead or has lost propulsion, it will likely have started to spin and will have no way to stop that. Reaction wheels can only do so much and can get saturated, requiring other attitude control systems to be used to desaturate them first. Approaching it with an unspecialized satellite will then definitely generate smaller debris, which is far more problematic.This also amplifies the risk you mention about ending up immobilized for other reasons. You&#x27;d have two pieces of junk literally on top of each other, it would take very little for them to damage each other and start producing much smaller debris.Further, you can&#x27;t just repurpose a star tracking camera to look at a satellite instead, they&#x27;re dealing with completely different kinds of requirements, and it isn&#x27;t even a given that you would have the processing capability on the tracker to handle the kind of computer vision needed. Nor is it likely that the tracking camera feed can be transmitted down to Earth for manual control.The only cases we have of a satellite contacting another satellite when it wasn&#x27;t initially designed with that in mind, involved dedicated extension vehicles designed and launched to dock to the engine nozzle of satellites which were still fully functional and without the extension would&#x27;ve still been able to move themselves into a graveyard orbit. reply bagels 17 hours agorootparentprevThey don&#x27;t have that much precision in controlling velocity. Solar panels are fragile. It&#x27;s a recipe for getting two broken apart satellites in that orbit. reply Etherlord87 20 hours agorootparentprevI&#x27;d say the biggest risk is that upon one of the pushes of one satellite toward the other, the propulsion of an old satellite fails, keeps accelerating and causes a crash. replyasicsp 20 hours agoprevWas discussed 2 days back as well: https:&#x2F;&#x2F;www.bloomberg.com&#x2F;news&#x2F;articles&#x2F;2023-10-02&#x2F;dish-deal...https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37744077 reply mnw21cam 18 hours agoprevWhile still being important, I&#x27;d like to draw people&#x27;s attention to the fact that this is a satellite a long way away from Earth, and is not in any significant danger of causing low Earth orbit to become a no-go area. It&#x27;s remotely possible that it might cause a problem out at geostationary orbit, but that&#x27;s very unlikely. There&#x27;s a lot more space out there and most satellites are all moving in the same direction and at the same speed, making it a lot safer than low Earth orbit. reply jacksnipe 18 hours agoprevThey put DISH under a consent decree. Despite the low size of the fine, I expect the long term cost to DISH will be enormous. Consent decrees are no joke. reply Xorakios 20 hours agoprevA lot of comments are focusing on penalties.Just asking this tech focused community, but shouldn&#x27;t the correct action be for a UN level entity charge an automatic de-orbiting fee.Even thinking it as a bottle deposit return if the launching company succeeds in its de-orbit burnup for ongoing constellations. reply joncp 15 hours agoparentExactly. Some sort of escrow to guarantee that what goes up will eventually come down. Without that satellite operators could go out of business and dissolve without cleaning up their messes, much like mining companies have historically done. reply happytiger 14 hours agoprevI feel like this is charging $50 to a billionaire for leaving wreckage in the middle of a public park.Like, “we’re warning you!”But it’s only notable as an action because they haven’t done any prior enforcement. Not because now, suddenly, they are signaling that they’re going to address the problem. reply spandextwins 14 hours agoprevI don’t understand how a fine fixes the problem. Especially since you don’t know where the money goes. I guess it does make a good story though. reply vlod 18 hours agoprevI guess this is great reason to modernize&#x2F;reboot the SDI [0] initiative (nicknamed Star Wars program from President Reagan days) to clear out bad&#x2F;rusty&#x2F;leaky satellites (translate: and anything else they want to take a shot at, in the interests of US national security of course) &#x2F;s[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Strategic_Defense_Initiative reply sholladay 15 hours agoprevThis is of course a good thing but should the US have the legal authority to do so? If so, on what grounds? US companies litter all over the place on Earth and don’t get fined. Cruise ships, for example, dump their waste just outside of territorial waters. reply Rebelgecko 12 hours agoparentIt&#x27;s a violation of the license they got from the US governmentIn at least 1 case, they also issued fines to a US company that tried to use an international launch to circumvent US rules reply Zachsa999 16 hours agoprevWhen are we going to quit calling space operations \"missions\"? reply AtlasBarfed 11 hours agoparentProbably when they aren&#x27;t easily categorized and discretely identified with beginning and ending time periods. reply Globe_Explorer 15 hours agoprevSo what happens if the satellite operator is a non us company? reply andrewguenther 14 hours agoparentThe regulatory vector for satellites in the US is the FCC. The FCC controls US airwaves so if you&#x27;re providing services via satellite in the US you must have a license from the FCC.So if you&#x27;re a non-US company operating services in the US, you&#x27;ll have to have a US subsidiary anyway and that subsidiary would be required to get the license. If you&#x27;re a non-US company not doing business in the US then that&#x27;s outside of regulatory jurisdiction. reply dotnet00 15 hours agoparentprevBy the outer space treaty, ultimately it&#x27;s the responsibility of the country that the company belongs to. If it was a german operator, it&#x27;d be subject to German rules.Although of course since this is geostationary orbit, the point of putting it over a geographical area is to do business over that area down on Earth.So, if it was a German company, but was doing business in the US, the US could use access to their market as leverage for a fine. reply cardiffspaceman 12 hours agorootparentThe geostationary satellites I’m familiar with fly directly over the Equator, no point on the Equator is in Germany. The disposal orbits are geosynchronous and have a more interesting ground track. I think their ground tracks are still equatorial countries.Geostationary satellites have antennae which are meant to optimize communications with ground stations in certain countries, but they can’t fly over the USA especially. reply dotnet00 11 hours agorootparentAh you&#x27;re right, I forgot that geostationary doesn&#x27;t actually mean sitting right over the country and instead is along the equator. reply bloak 15 hours agoparentprevIn that case presumably the other country would be responsible for issuing a penalty, assuming symmetry in a fair world (which is a fairly questionable assumption). reply michaelmrose 11 hours agoprevWhy not force them to pay to hsndle it even if it costs billions. reply Simulacra 20 hours agoprevWith all of the rockets going into space now, when are we going to really make a strong effort to clean up space? Is it just technical, money, willingness, or...? reply dotnet00 20 hours agoparentContrary to most of the fear mongering about space debris, kessler syndrome etc, the big Western satellite operators are generally moving in the right direction and have moved faster than regulators to try to set space debris management standards.Incidents like this one are pretty uncommon, and as space debris goes, this is pretty benign in the sense that it&#x27;s a large intact satellite and can thus be tracked fairly easily and precisely. Once the technology matures, a cleanup vehicle could even be sent to move it into graveyard orbit.The kind of space debris that tends to be problematic is stuff that is too small to track, since then you&#x27;re stuck relying either on pure luck or giving the area a wide berth to not have it hit something else. reply nordsieck 20 hours agorootparent> the big Western satellite operators are generally moving in the right direction and have moved faster than regulators to try to set space debris management standards.We&#x27;ll see. Starlink&#x2F;Kuiper use a low enough orbit that non-functional satellites will de-orbit on their own in a reasonable timeframe. But OneWeb, which is up at 1000km+ already has had a satellite failure. And although they&#x27;ve vowed to remove it somehow, it&#x27;s not obvious how they&#x27;ll do so. reply dotnet00 19 hours agorootparentAs far as I&#x27;m aware, OneWeb sats have the Astroscale grappling point on them so the vehicle Astroscale is developing can grab and deorbit the dead satellites. The most recent news on that being that an in-orbit demonstration is intended to launch in 2025.Edit: correction, oneweb doesn&#x27;t specifically use the astroscale grappling fixture, instead it seems they use either their own design, or ones designed by Altius. Apparently these designs are such that they can be grappled either mechanically or magnetically, so presumably the difference is not significant enough to meaningfully limit which MEV can handle the job. reply ginkgotree 20 hours agoparentprevYes, the USSF has as part of it&#x27;s \"SpaceWerx\" program a funding initiative called Orbital Prime, which autonomous on-orbit servicing and de-orbiting is a major focus. The summary headline is any private technologies that enable autonomous \"RPO\" Rendezvous Positioning Operations, will in turn enable activities such as autonomous repair and refueling (one of the main reasons orbital space vehicles reach end of mission is they run out of fuel), and in cases where refueling and repair aren&#x27;t possible, autonomous \"tugs\" can dock and execute a deorbit burn for the decommissioned space craft. I was on one of the awarded teams for Orbital Prime last year, its very exciting technology and equally exciting to see serious amounts of funding materializing to tackle this problem. reply sacnoradhq 14 hours agoparentprevInstead of a knee-jerk reaction to spending money on a particular solution, think about the problem.Most things of significant mass are in stable orbits or they wouldn&#x27;t be there for very long. Regional sats like the one from Dish are in GSO way away, so they don&#x27;t really matter. The problem is that Dish didn&#x27;t have a disposal plan for this satellite. It&#x27;s mostly when operators create clouds of junk, especially fragments from unnecessary collisions that cross other orbits, that pose a threat to lives and property. It&#x27;s good to require operators to have a deorbit or parking orbit plan.The biggest threat is LEO-crossing MMOD, which is why crewed craft have hundreds of Whipple shielding configurations to choose from for a given application.This isn&#x27;t a problem in need of solving in such a deliberate, costly, and arbitrary manner. Instead, the solutions are holistic: don&#x27;t create debris that poses a risk to LEO, have a decommissioning plan for GSO objects, and protect what needs protecting by defensive spacecraft design.Edit: The Kosmos 2251&#x2F;Iridium 33 collision was reckless operation by the Russians leaving space junk in LEO. reply mcpackieh 20 hours agoparentprevThe vast majority of new satellites are being placed into low earth orbit, which is cleaned up fairly quick by atmospheric drag. You can see this on an animated Gabbard diagram: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=aRUaENzxH44In higher orbits, particularly GEO (like the satellite in question), it&#x27;s much more of a problem. Laser brooms might one day be used to deorbit small pieces of debris, but dead satellites beyond LEO will need to be grabbed and tugged&#x2F;thrown into safer orbits. reply ImPleadThe5th 20 hours agoparentprevEconomics likely. Though, I&#x27;m sure the first contractor who is able to do it will make a decent amount of money. reply complianceowl 20 hours agoprev\"Why does everyone think we should dish out more money?\" - DISH reply ginkgotree 20 hours agoparentLOL, NICE reply nicup12345689 16 hours agoprev [–] It&#x27;s just setting up the stage&#x2F;precedent before going for Starling reply dotnet00 16 hours agoparentNone of these concerns apply to Starlink. The satellites orbit low enough to quickly decay if they lose power. They deploy even lower than operational altitude so any associated junk comes down even faster, and the deployment method of just spinning and unlatching the stack, combined with SpaceX&#x27;s philosophy of using repeatable systems (eg mechanical separators instead of explosive bolts) means that Starlink launches produce minimal debris and any debris produced is short lived.The issue with the Dish satellite is that it&#x27;s close to geostationary orbit, which is a limited resource and where the atmospheric drag is low enough that they are essentially up there forever. reply 0xffff2 16 hours agoparentprev [–] I&#x27;m probably the only person on HN confused by this comment, but I assume you mean Starlink, not Starling, which is a NASA mission involving 4 cube sats. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The US Federal Communications Commission (FCC) has fined Dish Network $150,000 for breaching its anti-space debris regulations.",
      "Dish Network is penalized for improperly deorbiting its EchoStar-7 satellite, triggering the potential for increased orbital debris.",
      "This instance marks the first ever penalty for space debris enforced by the US government, highlighting the FCC's intent to uphold its space debris rules amidst a rapidly growing space economy."
    ],
    "commentSummary": [
      "The US government has issued its inaugural space debris penalty, charging Dish Network $150,000 for regulatory violations and an additional fine of $126 million by the FCC for improper satellite disposal.",
      "Swarm Technologies, another satellite company, has been fined $900,000 for unauthorized satellite launches.",
      "The penalties have sparked discussions on the challenges in rescuing stranded satellites, the importance of disposal plans and regulatory compliance to alleviate space debris, and the potential profitability in tackling space debris issues."
    ],
    "points": 265,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1696506873
  },
  {
    "id": 37776854,
    "title": "September was the most anomalously hot month ever",
    "originLink": "https://www.scientificamerican.com/article/september-was-the-most-anomalously-hot-month-ever/",
    "originBody": "SKIP TO MAIN CONTENT Subscribe Latest Issues Scientific American Sign In|Newsletters COVIDHealthMind & BrainEnvironmentTechnologySpace & PhysicsVideoPodcastsOpinion Nobel Prize Sale! Save 40% CLIMATE CHANGE September Was the Most Anomalously Hot Month Ever September shattered a record for the highest temperature anomaly of any month and could help push 2023 to be the first year to exceed 1.5 degrees Celsius above preindustrial temperatures By Andrea Thompson on October 4, 2023 Credit: Zeke Hausfather, restyled by John Knight In a year already overloaded with so many climate-related superlatives, it’s time to add another to the list: September was the most anomalously warm month ever recorded. And the steady heat building this year could make 2023 not only the hottest year on record but the first to exceed 1.5 degrees Celsius (2.7 degrees Fahrenheit) above preindustrial temperatures, or the stable climate that preceded the massive release of greenhouse gases into the atmosphere from burning fossil fuels. Under the landmark Paris climate accord, nations have pledged to try to keep global warming under that threshold. “It’s very worrying,” says Kate Marvel, a senior climate scientist at Project Drawdown, a nonprofit organization that develops roadmaps for climate solutions. According to data kept by the Japan Meteorological Agency, this September was about 0.5 degree C (0.9 degree F) hotter than the previous hottest September in 2020. It was also about 0.2 degree C (0.4 degree F) warmer than the previous record high temperature anomaly—a measure of how much warmer or colder a given time period is, compared with the average—which had been set in February 2016 during a blockbuster El Niño. ADVERTISEMENT The September anomaly “is so far above anything we’ve seen before,” says Zeke Hausfather, a climate scientist who works at the payment processing firm Stripe and wrote about September’s heat in a recent blog post. On X, formerly known as Twitter, he called the feat “absolutely gobsmackingly bananas.” The milestone reached last month comes on the heels of July setting the record for the hottest month overall. (July is always the hottest month of the year globally because it occurs at the peak of the Northern Hemisphere summer. The Northern Hemisphere has much more landmass to soak up the sun’s rays than the Southern Hemisphere, so it has the bigger influence on the global annual temperature cycle.) In a marker of just how much global temperatures have risen in recent decades, Hausfather observes, “this September will be hotter than most Julys before the last decade or two.” Credit: Zeke Hausfather, restyled by John Knight; Source: Japanese 55-Year Reanalysis data on global mean temperature, processed by Ryan Maue Two main factors are at play in driving temperatures to such extremes: their inexorable increase from burning fossil fuels and an El Niño event that is shaping up to be a strong one. El Niño is a part of a natural climate cycle that features a tongue of unusually warm waters across the eastern Pacific Ocean. Those waters release heat into the atmosphere and can cause a cascade of changes to key atmospheric circulation patterns linked to the weather around the world. Heat waves have broken records all over the globe during the past few months, including prolonged events called heat domes that plagued the southern stretch of the U.S. and parts of the Mediterranean. Summerlike temperatures were even felt in South America during the Southern Hemisphere’s winter. Two of the heat waves—one in the U.S. Southwest and one in Europe—were found to be virtually impossible without global warming. And summerlike heat has continued in places into October. ADVERTISEMENT The most drastic temperature anomalies typically come in the winter months, when El Niño peaks in strength. In fact, the previous most anomalously warm month was February 2016, during one of the strongest El Niños on record. But this year “we’re seeing these [big anomalies] in the Northern Hemisphere summer,” Hausfather says. That leaves open the possibility of even larger anomalies when this event peaks this winter, particularly if it ends up being another strong event. It is possible there is also some influence from the phasing out of sulfur-containing fuels used by ships because the aerosols spewed into the air from burning those fuels tend to have a slight cooling effect. The eruption of the Hunga Tonga–Hunga Haʻapai volcano in the southern Pacific Ocean last year may also be nudging up temperatures because of the huge amounts of water vapor—also a greenhouse gas—it injected into the atmosphere. But both factors have very small influences, compared with climate change and El Niño. Given that this El Niño is expected to persist and likely to strengthen, there’s a good chance that 2023 or 2024—or both—will become the hottest year on record, besting 2016 (and 2020, which some agencies who monitor climate have tied with 2016). That isn’t surprising, given that there has been a tenth of a degree of warming since 2016, though it is “remarkable just how quickly we’ve seen warmth this year,” Hausfather says. Part of the apparent rapid warming is because 2023 began in the tail end of an unusual string of three back-to-back La Niña events. These tend to have a cooling impact on the global climate, though La Niñas today are hotter than even El Niños of several decades ago. Sign up for Scientific American’s free newsletters. Sign Up Beyond potentially becoming the hottest year on record, 2023 could also be the first year to top 1.5 degrees C above preindustrial temperatures (some individual months have already passed that threshold). But even if that happens, all hope is not lost for meeting the Paris accord goals. That threshold is measured as an average of several decades, and climate scientists have long expected that a single year would pass that mark a decade or so before the world could be considered permanently above that limit. “There is still time to limit global warming to 1.5 degrees,” Marvel says. “It is going to be incredibly difficult. The pathways are narrowing.” But this year should be considered a warning of the future we face if we don’t take rapid, ambitious action. “This is what the world looks like when it’s 1.5 degrees hotter in a year, and it’s terrible,” she says. When the world does permanently pass 1.5 degrees C, the climate anomalies for individual years will reach higher than that mark. ADVERTISEMENT To stave off that future, every bit of carbon we can keep, or take, out of the atmosphere is crucial. “Every tenth of a degree matters,” Hausfather says. Rights & Permissions ABOUT THE AUTHOR(S) Andrea Thompson is an associate editor covering the environment, energy and earth sciences. She has been covering these issues for 16 years. Prior to joining Scientific American, she was a senior writer covering climate science at Climate Central and a reporter and editor at Live Science, where she primarily covered earth science and the environment. She has moderated panels, including as part of the United Nations Sustainable Development Media Zone, and appeared in radio and television interviews on major networks. She holds a graduate degree in science, health and environmental reporting from New York University, as well as a B.S. and an M.S. in atmospheric chemistry from the Georgia Institute of Technology. Follow Andrea Thompson on Twitter Credit: Nick Higgins Recent Articles by Andrea Thompson New York City's Floods and Torrential Rainfall Explained A Record Number of Billion-Dollar Disasters Show U.S. Isn't Ready for Climate Change Half the World's Population Faced Extreme Heat for at Least 30 Days This Summer READ THIS NEXT CLIMATE CHANGE U.S. Heat Deaths Will Soar as the Climate Crisis Worsens Meghan Bartels CLIMATE CHANGE Will Humans Ever Go Extinct? Stephanie Pappas CLIMATE CHANGE Why Extreme Heat Is So Deadly PUBLIC HEALTH How Does a Heat Wave Affect the Human Body? Katherine Harmon ADVERTISEMENT NEWSLETTER Get smart. Sign up for our email newsletter. Sign Up Support Science Journalism Discover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners. Subscribe Now! FOLLOW US instagram youtube twitter facebook rss SCIENTIFIC AMERICAN ARABIC العربية Return & Refund Policy About Press Room FAQs Contact Us Site Map Advertise SA Custom Media Terms of Use Privacy Policy Your US State Privacy Rights Your Privacy Choices/Manage Cookies International Editions Scientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers. © 2023 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL RIGHTS RESERVED. SCROLL TO TOP",
    "commentLink": "https://news.ycombinator.com/item?id=37776854",
    "commentBody": "September was the most anomalously hot month everHacker NewspastloginSeptember was the most anomalously hot month ever (scientificamerican.com) 246 points by esarbe 23 hours ago| hidepastfavorite380 comments nologic01 20 hours ago> Two main factors are at play in driving temperatures to such extremes: their inexorable increase from burning fossil fuels and an El Niño event that is shaping up to be a strong one.It would be quite educational to attribute in more detail the impact of these two factors. The degree of \"El Nino\" influence is presumably known, and if its strength is not related with global warming in complicated and unknown ways it would be good to overlay it as a distinct effect.Being clear about what is happening and why (to the extend that it is known and understood) is important, on at least two counts:There is already a segment of the population that drifts into panic and depression as a result of all the viral social media climate change echo chambers. Panic is not the way to address our sustainability challenges that are deep-seated and systemic. The flip side of short-term focused panic is switching off into apathy, or even cry-wolf type incredulity - if a few years down the line these extremes are temporarily subdued.The other segment of the population that can be affected by a clear and well founded explanation of what is happening are those sitting on the fence about climate change or feeling skeptical for legitimate reasons. Obviously nothing will convince nut-cases or deeply compromised individuals that have large personal stake in the status-quo (and they are many). reply flagrant_taco 20 hours agoparentThe debate over exact causes has always felt misguided to me. The planet is extremely complex, human impact on the planet is largely based on estimates and modeling, and at the end of the day an understanding of why it&#x27;s getting hotter isn&#x27;t itself a course correction.There are such obvious first steps we could take without a complete understanding of cause if we really cared. We could easily cut our use of fossil fuels if we weren&#x27;t trying to balance that with an arbitrary GDP growth target. We could stop incentivizing people and companies to consume disposable products. We could stop subsidizing massive industrial farms and instead focus on eating locally and growing some of your own food.The list goes on, but having spent decades watching people debate passionately over why the planet is warming all while going further down the same path my only conclusion is that we just don&#x27;t give a damn. When push comes to shove we would rather buy new cloths imported from the other side of the world, throw resources at new construction and gut job renovations, and will happily fly around the world on vacation rather than stay home and enjoy the world around us.Oh and don&#x27;t forget about new tech. Machine learning rebranded as AI is so ground breaking that we have no choice but to produce as many GPUs as we can and throw all the power and water needed to run the damn things 24&#x2F;7. reply gjulianm 20 hours agorootparent> There are such obvious first steps we could take without a complete understanding of cause if we really cared.That&#x27;s the issue, a lot of people just don&#x27;t care or refuse to believe it. Questions like the one in the parent comment are starting to seriously annoy me because they&#x27;re demanding an unachievable level of evidence before doing anything, a level that&#x27;s never asked for in any other kind of intervention (for example, interest rate hikes). reply nsxwolf 18 hours agorootparentEveryone leaves out a large cohort - people that believe it, but don&#x27;t want the solutions you&#x27;re selling. We typically put forth constructive solutions like nuclear, which get shouted down &#x2F; laughed out of the room instantly. reply gjulianm 14 hours agorootparentNot really, I don&#x27;t distinguish between the different ways to solve it. I wish that was the discussion, but I feel like any solution that has any costs of any kind is just ignored because a lot of people don&#x27;t feel like this is serious enough to do anything serious about it, doesn&#x27;t matter whether it&#x27;s reducing consumption, massive switch to nuclear, or whatever. reply candiodari 14 hours agorootparentMost green parties, and greenpeace in particular grew up around opposing nuclear power as unnatural. I think that&#x27;s where it comes from.https:&#x2F;&#x2F;www.greenpeace.org&#x2F;international&#x2F;tag&#x2F;nuclear&#x2F;So naturally, people assume anyone fighting against global warming is against nuclear. As far as I can tell, it&#x27;s also true. reply flagrant_taco 16 hours agorootparentprevAn easier first step before nuclear is reducing our power requirements. This one gets laughed out of the room both because we don&#x27;t think purple will accept it and because there&#x27;s a direct correlation between GDP growth and energy consumption, we don&#x27;t care enough about energy use to give up GDP. reply candiodari 14 hours agorootparentThis will achieve the opposite: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jevons_paradoxIf you make something less critical, cheaper, it will be used for more and more ridiculous things. reply flagrant_taco 5 hours agorootparentJevon&#x27;s Paradox is a thought experiment focused on increased efficiency not decreased demand. History is full of examples where a decrease on demand didn&#x27;t lead to an increase in ridiculous uses for the technology. reply ZeroGravitas 16 hours agorootparentprevUnfortunately it&#x27;s hard to tell the difference between people suggesting expensive and slow solutions because they&#x27;re ill informed about the alternatives and the people suggesting expensive and slow solutions because they don&#x27;t want to solve the problem.And practically there&#x27;s little difference, especially if the former are getting their information from the latter. reply candiodari 13 hours agorootparentI think people are fine about solving the problem, positive even (right wingers love solar panels, after all). They&#x27;re just not fine about being forced to sacrifice to solve the problem. reply xorcist 17 hours agorootparentprevWithout going to specifics, a large reason for that may be that among those who say this are a large number who loudly push for unrealistic ideas masquerading them as constructive. Some may be honestly ignorant about it, but it is also unfortunately quite an effective way to derail any discussion and delay progress. See for example any suggestion about real world hyperloops. reply gspencley 17 hours agorootparentprev> The list goes on, but having spent decades watching people debate passionately over why the planet is warming all while going further down the same path my only conclusion is that we just don&#x27;t give a damn. When push comes to shove we would rather buy new cloths imported from the other side of the world, throw resources at new construction and gut job renovations, and will happily fly around the world on vacation rather than stay home and enjoy the world around us.Yes exactly.You hit on something that I often say, though we come at \"solutioning\" from opposite angles and, from the sounds of your comment, will find little agreement. What we agree on is that understanding anthropogenic global warming and its causes is important and can inform solutions, but it does not tell us what the \"right\" solutions are since we have other complicating factors to consider. Such as things like human rights, standards of living, morality and the role+purpose and limitations of government.For some of us the debate is not over whether AGW is a scientifically understood phenomenon that represents a problem (even a critical problem at that). The debate is over what the solutions ought to be, what tools are the right tools for the job and what course corrections are considerable given factors that we must also contend with that lie outside of our scientific understanding of the problem. reply paulmd 14 hours agorootparent> Such as things like human rights, standards of living, morality and the role+purpose and limitations of government.Unfortunately, the dustbins of history are full of civilizations that took disruptive events carefully and with grave dignity and solemnity and respected the rights of powerful stake-holders within their society.That the people who make such arguments generally live in the places best positioned to weather out the situation, is also unfortunate. While people in well-pr tut about whether it&#x27;s really proper for a government to take action to avoid calamity from befalling its citizens, other people will be literally dying.It&#x27;s a rather crass and callous&#x2F;indifferent position you&#x27;ve staked out, you&#x27;re kind of not coming off like a very good person here. Like it&#x27;s literally \"I guess you&#x27;ll have to die, then, not like we&#x27;re going to unwind the economy just for you\" once again.It is, as someone notes upthread, basically the same as masks during COVID. Some people will stroke their chin and pontificate about utilitarianism and the greater good, but that&#x27;s not really comfort to anyone whose loved one dies from excess pollution or a transmissible disease. And of course the people taking this position are usually the most protected themselves - we all can WFH here if circumstances dictate, it&#x27;s different if it&#x27;s your wife stocking a grocery store, they care a lot less about the GDP impact and more about the direct personal impact than you do.And the existence of this subgroup - such as yourself - fundamentally forestalls the ability of the other 70% of the society from being able to protect itself. Kind of a problem in general - the american system (and the EU and UN as well, but particularly the US) provides too many brakes on action and biases to inaction (via checks-and-balances) such that even supermajority level political control often is not enough unless it is single-party control of every single branch of government and lever of power.You don&#x27;t just need 50%, you don&#x27;t just need 70%, you need 70% across every possible dimensionality that the US distributes political power over - essentially single-party control is the only way to rule if the minority does not consent, and we are talking about people who openly state that they will never consent.It&#x27;ll end in deaths sooner or later, whether that&#x27;s from inaction, or from those whose deaths would be caused by inaction taking action themselves (which is why almost everyone is forecasting conflict and political instability from climate change to kick up majorly in the second half of the century). Again, the only disappointing part is that this too will be visited on the least-fortunate instead of the responsible parties (who will largely be fine, and we will probably even bail out florida and other affected areas anyway).What happens when the unstoppable force of climate change meets the immovable object of modern political system&#x27;s bias-to-inaction? Fireworks, basically - because unlike HN you can&#x27;t just tone-police the discussion away in the real world, people won&#x27;t just take \"I guess you&#x27;ll have to die so I can have the new xbox this year\" lying down. reply gspencley 14 hours agorootparentYou should run for politics. The level of strawmanning and platitudes that your reply just levied is something typically only seen from politicians. reply r00fus 13 hours agorootparentActually your content-free ad-hominem retort is more typical of politician-speak. reply paulmd 14 hours agorootparentprevIt&#x27;s pretty objectively crass to ask people to die so that you can continue your consumption unabated. Just as it was with COVID.Pretty much anyone who opens up with \"the proper role of government\" is guaranteed to using some self-insert version of the founding fathers as a debate-terminating cliche, and it&#x27;s usually followed by some incredibly crass take like \"you&#x27;re going to have to die so that GDP and consumption can continue growing\". Like, it&#x27;s all just numbers, and it turns out the numbers are a lot more important than the lives of the affected individuals.Does \"I will never mask up, I don&#x27;t care if a million front-line workers die, reopen the economy and let&#x27;s get GDP back on track\" make you a bad person? Well, it doesn&#x27;t make you a good one. At best it makes you the edgy kid in your college debate club, but people tend not to like that \"abstract, formal\" argumentation when it&#x27;s their lives you&#x27;re debating.It&#x27;s not polite to say it here, of course. But treating a practical issue with real-world consequences as a boring dry debate club to score points over is kind of a shitty approach in general. And nobody else is obligated to play along with that and treat every possible position you might take as having inherent and equal merit, which is really the point of the tone-policing thing.Taking a position that we should just let climate change happen (TFA announcing yet another \"hottest month on record\" and \"hottest year on record\", let&#x27;s remember) because otherwise it would make your imaginary version of the founding fathers sad is not a position that deserves a whole lot of rhetorical merit or gravity. Of course it is \"the role of government\" to take action to mitigate dangers to its citizens - whether that&#x27;s indian attack, or a socialized healthcare system for merchant sailors, or controlling an outbreak of smallpox, or climate change. And it&#x27;s objectively silly to get hung up on what some 16th-century document thinks about it.The debate-club stuff and the faux-neutrality doesn&#x27;t really matter, and actually probably is offensive to the people whose lives you are debating. And again, that&#x27;s an observation you are not really allowed to make here, that maybe this faux-debate-club schtick and enforced politeness around deadly issues isn&#x27;t really healthy or right.(I don&#x27;t think anything I&#x27;ve said here is any more offensive than a PG essay - go read \"haters\", that&#x27;s not an essay you could write on HN today, it&#x27;s dismissive of people&#x27;s opinions! it doesn&#x27;t treat them as having inherent merit and deserving equal respect just because someone farted them out! but I expect to be tutted before too long anyway.) reply zosima 15 hours agorootparentprevThere is a lot of things that seems easy, but in fact are impossible.Sure, we could just stop using fossil fuels. But something like 50% of the global population would then be starving.The cure that so often is prescribed is worse than the disease. And people don&#x27;t seem to realize. reply noknownsender 18 hours agorootparentprevI think the second half of your comment argues against the first.You&#x27;re right that people are unwilling to make the basic changes that are obviously needed. But we only know precisely that those changes would work to fight against climate change because we are able to say with certainty that those behaviors are contributing factors.Any suggested actions have to have some level of scientific credibility to be taken seriously. Without it, they are easily dismissed by claiming that the planet is too complex to know if our actions will make a difference.That, purely by coincidence, I&#x27;m sure, has been the exact argument used against making any changes, and against the fact that warming was happening in the first place, and against tobaccos cancer risk, and any other public policy proposal that goes against the interest of those profiting from the status quo. reply flagrant_taco 7 hours agorootparentI&#x27;m actually arguing the opposite. We don&#x27;t know precisely that any of the suggested changes I listed would make a clear impact on climate change. My point is that, regardless of what happens with climate change, we could cut oil use with little to no impact on our daily lives.We don&#x27;t actually know the extent of impacts from either our oil use or potential cuts. We have modeling data that we extrapolate out, the issue I&#x27;m raising is that we get stuck in a loop of people debating whether the modeling data is accurate, complete, or predictive.There are changes we can easily make without knowing those answers. If we can make simple changes that at a minimum wouldn&#x27;t hurt the planet and almost certainly would help it, why don&#x27;t we? My read is that we simply must not care. We don&#x27;t need to buy as many things as we do, or to expert those costs overseas. We don&#x27;t need to travel the world by plane. We don&#x27;t need the latest iPhone made on the other side of the planet. We don&#x27;t need LLMs. We do it because we like convenience and novelty more that we actually care about our impact on the planet. And we avoid implementing real solutions by first demanding that we have a complete understanding of the cause and blame. reply nsxwolf 18 hours agorootparentprevIf the \"basic changes\" are more extreme than the COVID lockdowns, which made the entire world miserable, brought unprecedented economic destruction we&#x27;ll be dealing with forever, and which also didn&#x27;t cool the planet at all despite massive decreases in CO2 emissions... no thanks.I&#x27;ll keep voting against anything that makes me poorer and more miserable. reply NoGravitas 16 hours agorootparentWell, you&#x27;ll end up even poorer and more miserable in the end if you have it your way. But at least you get your treats for a few more years. reply flagrant_taco 7 hours agorootparentprevI sincerely hope that we aren&#x27;t at a point where most people&#x27;s guiding principles are minimizing their own poorness or misery. Those should be a given and self interest is backed into us, but we&#x27;re in for a world of hurt if those are the only deciding factors take into account.Some things are in fact worth dying for. reply southernplaces7 3 hours agorootparentBullshit. Nobody can factually, concretely state that climate change will go a specific, certain way to cause a specific certain amount of misery over the next century. There are too many technological, social, economic, and extremely complex climate variables at play for that to be a concrete assertion.This isn&#x27;t to say that measures to reduce carbon emissions can&#x27;t or shouldn&#x27;t be taken, they can and they should, but with a mind towards exactly what you deride, preventing as many people as possible from falling into draconian economic and social misery. That at least is something we concretely know to be possible if governments adopt the wrong control measures in a heavy handed way. The pandemic showed it clearly and concretely in many forms.We&#x27;re not talking here about the impending, concrete impact of a large asteroid, or a super volcano eruption that&#x27;s definitely imminent, where mass economic sacrifice for the sake of saving humanity would be understandable. Climate change is, despite all the political fanfare, something that has too many variables to clearly define as a cataclysm.Given that, there is nothing selfish about wanting to avoid misery and \"dying\" for yourself and your children in the face of ambiguous predictions about future events. If you&#x27;re so convinced that \"some things are worth dying for\", what stops you from applying your own advice instead of preaching sanctimoniously to others? reply ravishi 18 hours agorootparentprevIt&#x27;s basically impossible to convince someone to do all those things when they&#x27;re fighting to stay alive, working terrible conditions and generally being fucked by society as a whole. Fixing economic inequalities and improving overall quality of life for everyone in the planet should be top of that list. Telling rich people to grow their own food won&#x27;t make any difference. reply flagrant_taco 16 hours agorootparentI&#x27;d argue that an economy and society built on the framework of terrible economic inequality is fundamentally unable to fix said problems.I expect we would have a lot of agreement related to fixing that system to better serve the average person. Even if we have different opinions on exact details, that core principle can go very, very far. reply avgcorrection 19 hours agorootparentprev> We could easily cut our use of fossil fuels if we weren&#x27;t trying to balance that with an arbitrary GDP growth target. We could stop incentivizing people and companies to consume disposable products. We could stop subsidizing massive industrial farms and instead focus on eating locally and growing some of your own food.Then it&#x27;s the small task of changing the economic model of the entire developed (and less developed, and exploited) world. The model that all of the richest and most powerful people depend on. reply flagrant_taco 19 hours agorootparentThe economic model is going to change one way or another. A model designed around perpetual growth will always fail, and there are already plenty of cracks showing.A major economic change sounds impossible, but we&#x27;ve done it in the US at least three times in the last century. reply avgcorrection 16 hours agorootparent> A major economic change sounds impossible, but we&#x27;ve done it in the US at least three times in the last century.No. reply flagrant_taco 7 hours agorootparentCreating the Federal Reserve was a fundamental change. Confiscating gold then pegging the US dollar to a gold-backed value was a fundamental change. Getting rid of the gold standard and going full fiat sad a fundamental change.The economic model, monetary policy, and incentives of each system were completely different. We may not have renamed the currency or the country, but that doesn&#x27;t change the fact that those were major economic shifts. reply xkcd-sucks 17 hours agorootparentprevThat&#x27;s the value of space colonization, or the hope and promise of space colonization -- Preservation on of the economy reply quonn 19 hours agorootparentprevThat was true 200 years ago and maybe even 200.000 years ago. Some day there will be no more growth - but it could be another 200.000 years. reply qubex 19 hours agorootparentprevAnd just on cue there it is: “It is easier to imagine the end of the world than to imagine the end of Capitalism”, variously attributed to Frederic Jameson and Mark Fisher (amongst others). reply avgcorrection 19 hours agorootparentOk? reply azinman2 19 hours agorootparentprevBecause communist countries don’t pollute? reply danShumway 18 hours agorootparentThere are issues with the quote, and Communist countries do pollute, but if the argument is (as avgcorrection stated):> Then it&#x27;s the small task of changing the economic model of the entire developed (and less developed, and exploited) world. The model that all of the richest and most powerful people depend on.Then yes, you&#x27;re talking specifically about Capitalism at this point; avgcorrection is saying that we can&#x27;t make this change because it conflicts with Capitalism.;) Unless you&#x27;re trying to imply that Communism is the driving economic force behind the entire developed world, which I&#x27;m going to guess is not something you believe.I do think it&#x27;s reasonable to ask whether the problem of human exploitation and the tendency for entrenched powers to cement their own power at the expense of the world is perhaps broader than one economic system and whether that harmful instinct might show up in multiple places and systems beyond only Capitalism. One could very reasonably argue (and I would argue) that Capitalism showcases just one manifestation of a human instinct that corrupts multiple economic systems including Communism.But avgcorrection&#x27;s criticism (as far as I can tell) was that we can&#x27;t shift off of fossil fuels because that shift is incompatible with the driving economic model of our time: Capitalism. So I don&#x27;t think the quote is inappropriate. It&#x27;s easier to imagine the end of the world than to imagine the system avgcorrection is referencing (Capitalism) even temporarily bending in order to prevent that end. Whatever solution is proposed is only likely to be adopted if it doesn&#x27;t conflict with Capitalist ends because Capitalism is fully willing to look directly at an incoming catastrophe, shrug its shoulders, and do nothing if the solution would decrease immediate profits. reply avgcorrection 18 hours agorootparentIt&#x27;s got nothing to do with imagination or ideology and everything to do with the material reality that the the most powerful forces that the world has ever seen will crush you if you try to change the existing system. reply danShumway 18 hours agorootparentSure, but that just sounds like you&#x27;re agreeing with qubex?Not sure if there&#x27;s something I&#x27;m missing here, but I don&#x27;t see a contradiction between \"the most powerful forces the world has ever seen will crush you if you change the current system\" and \"it&#x27;s easier to imagine the end of the world than the end of Capitalism.\" It sounds like you&#x27;re both saying the same thing.Again, I think it&#x27;s reasonable to ask whether other economic systems if they were dominant would act the same way -- but... I mean, we are talking about Capitalism, that is the most dominant and most powerful economic system in the entire world and the status quo that the most powerful forces in the world see as the default system to preserve. And both you and qubex are saying that changing that system appears to be impossible even when faced with a global human catastrophe. reply avgcorrection 16 hours agorootparentMaybe I read too much into “imagine”. reply qubex 18 hours agorootparentprevIt’s not an observation about communism-vs-capitalism (or at least that’s not how I understand it): it’s about how we’re collectively as a society so fixated upon the inevitability of property rights and capitalism and return on investment and the primacy of profit that we cannot contemplate that we may need to abandon all these precepts in order to avert climatic disaster (or some other existential threat). reply zosima 15 hours agorootparentAnd how exactly would introducing communism avert climate disaster? How did the environment fare in Soviet. Which country is the largest CO2 emitter in the world? Do you even consider what you&#x27;re writing before you post it? reply avgcorrection 15 hours agorootparentThe person that you replied to already wrote that “It’s not an observation about communism-vs-capitalism”. reply zosima 15 hours agorootparentYes, of course. Abolishing property rights is not communism this time around. reply avgcorrection 13 hours agorootparentHave fun living on your property when it&#x27;s underwater. reply nradov 18 hours agorootparentprevWhat exactly are you proposing as an alternative? So far everything else that we have tried has quickly devolved into famine and genocide. reply avgcorrection 16 hours agorootparentWas the invasion of Vietnam by the US a genocide? reply CheBuzz 9 hours agorootparentNo. reply SideburnsOfDoom 18 hours agorootparentprevIf someone says \"end of capitalism\" and this is assumed to be \"about communist countries\", because it&#x27;s binary, what else is there? Then this is precisely the dangerous failure of imagination being pointed to. reply qubex 1 hour agorootparentI’m quite stunned by the virulence of the responses and how strong the (false) dichotomy between ‘communism’ and ‘capitalism’ is. reply SideburnsOfDoom 1 hour agorootparentThe knee is jerking for sure. The virulence suggests insecurity. replyAunche 18 hours agorootparentprev> We could easily cut our use of fossil fuels if we weren&#x27;t trying to balance that with an arbitrary GDP growth targetBelieve it or not, only a handful of people genuinely care about \"GDP growth targets\" and they have very little influence on environmental policy. The reason why politicians oppose sweeping regulation to stop climate change is because they&#x27;d lose their jobs if they intentionally caused gas prices to rise or cause a recession. reply dragonwriter 18 hours agorootparent> Believe it or not, only a handful of people genuinely care about “GDP growth targets” and they have very little influence on environmental policyA large number of people with vast influence on all government policy care about the returns of their investments in existing incumbent industries, which in aggregate correlate very strongly with GDP growth, though. reply flagrant_taco 7 hours agorootparentprevOr entire system is based on predictable GDP growth. It impacts nearly every part of your life in some way.Do you have any debt? Or use money? Or pay taxes?do you have a job? Did you know that oil consumption grows at the same rate as GDP? Do you save for retirement, or hold any investments? Do you have a bank account?It&#x27;s all directly connected to the economic system, which is driven by the goal of having steady and predictable rate of growth in GDP. reply takinola 17 hours agorootparentprevI would argue most people care about GDP growth targets but they just don&#x27;t understand them that way. GDP growth is what leads to improvements in real human quality of life. It is your ability to get healthcare, housing, food and all the other things that make life a lot less painful.I would rather live in a society with a higher GDP than less (all things considered) and so would most people (as evidenced by population shifts across the world). If the GDP does not grow to at least match the population changes, the society would be poorer and have worse outcomes overall. reply holbrad 14 hours agorootparentNo idea why your getting downvoted for such an obvious statement, there is very strong (But not perfect) correlation between GDP and standard of living. reply gls2ro 17 hours agorootparentprevalmost everybody living in a democratic capitalist economy is influence by and takes decisions that have as the foundation the GDP growth.that is the foundation of the way economy works. It propagates down to interests and credits and investments and jobs.Obviously IMO. reply nemo44x 20 hours agorootparentprevAsking people to have less, experience less, and do less isn’t an argument that can ever win.The winning argument is to show how you can consume more energy for less money by using sustainable technology. Cramped trains will rarely win politically but electric cars that go fast and look cool do win.In essence I believe the only politically viable way to reduce carbon is to engineer and tech our way to systems that reliably do more for less and increase the amount of energy people can consume. reply flagrant_taco 19 hours agorootparentWhat you&#x27;re describing is the path to inevitable collapse. If people will never be able to give up convenience or growth then we really are doomed, tech advantages only slow down that runaway train.I don&#x27;t actually think it&#x27;s a fundamental blocker though. People sacraficed quite a bit at home during WWII to support the war effort, we could make similar sacrifices now if we saw our impact on the planet as a similarly existential threat.To me it seems like the failure is in trying to analyze the problem so specifically that we can define the \"right\" path then logic people onto it. That will never work, mainly because the problem is so complex that there won&#x27;t be a complete understanding to share. There will always be assumptions used in modeling and holes in the data allowing for alternative explanations that are then argued over even further.We all know that using oil at this scale isn&#x27;t a great idea. And we all know that we could actually use less oil without giving up much at all. If we spent less time arguing over how we got here we could find reasonable next steps that are hard to argue with when it isn&#x27;t all backed by \"science\" and \"fact\" that can be debated endlessly. reply aydyn 18 hours agorootparentI disagree. There is more untapped energy and resources in this world than we could ever hope for, We just need to develop the tech to access it. Climate change too could be solved with tech. Theres no other choice. reply flagrant_taco 7 hours agorootparentHow do we know these exist of we don&#x27;t know how to access our use them? Without access we can only estimate the reserves, and without a way to use it we can&#x27;t calculate the potential energy creation or storage.We really are screwed off our only hope is blind faith that tech can save us. Technology can help, but we can&#x27;t abandon the option of decisions that can already be made today. reply mrguyorama 16 hours agorootparentprev>We just need to develop the tech to access itWhat energyWhat \"tech\"?Tech isn&#x27;t magic. We can&#x27;t just magically stop the greenhouse effect. reply aydyn 10 hours agorootparentSolar, wind and nuclear (theoretically) are limitless.> We can&#x27;t just magically stop the greenhouse effect.You&#x27;re absolutely right, that&#x27;s why tech is the only non-magical solution.Anyone thinking they can change the behavior of 8 billion people is engaging in magical thinking. reply flagrant_taco 7 hours agorootparentSolar and wind are absolutely limited, both by environmental conditions that change the energy potential and by our ability to store energy. Neither can handle surge use as well either, we can&#x27;t make more wind or sunshine when everyone turns on their suit conditioner.Nuclear is at least a viable shirt term solution. Combine that with reducing our total energy demand and we&#x27;re at least buying ourselves more time to figure things out, that&#x27;s a much better sort term path for sure. reply aydyn 6 hours agorootparentYou&#x27;re not saying anything I disagree with. Technology will, and has been very quickly improving those limitations. replyFricken 19 hours agorootparentprevYoung men have a genetic proclivity for getting their asses handed to them in war. It&#x27;s surprisingly easy to order men to their deaths when they&#x27;re convinced it&#x27;s to protect their tribe from some other tribal threat. We&#x27;ve been doing it for longer than we&#x27;ve been human.Nothing in our evolutionary history has prepared us for climate change. We have more trivial things to worry about. reply Aerbil313 19 hours agorootparentThat&#x27;s the thing. No one can actually comprehend such things like geopolicits, global&#x2F;national economy, climate change etc. The world is too big for our brains to understand. reply Fricken 19 hours agorootparentReason is slave to the passions. If it doesn&#x27;t resonate at an emotional level we don&#x27;t care. It doesn&#x27;t matter if we understand. We also have to give a fuck, and clearly we don&#x27;t. reply ZeroGravitas 16 hours agorootparentprevPeople by and large understand and ask for and vote for solutions to this.The various comments here blaming \"people\" are a mix of victim blaming and cover-up for the well organized forces that have held the democratic will back for decades and despite (or because of?) this, the agents of this tragedy are very popular around these parts. reply Fricken 12 hours agorootparent>People by and large understand and ask for and vote for solutions to this.They really haven&#x27;t, no. Corporations don&#x27;t want to take responsibility for their actions, governments don&#x27;t want to, and big surprise: individuals don&#x27;t want to either. reply ZeroGravitas 11 hours agorootparentUSA:Polling shows that US voters favor climate bills – yet assume fellow Americans don’thttps:&#x2F;&#x2F;www.theguardian.com&#x2F;commentisfree&#x2F;2022&#x2F;sep&#x2F;01&#x2F;us-vot...UK:> The UK has some of the highest levels of concern regarding the climate emergency in the world with 81 per cent of people ‘believing’ in the climate emergency, with over three-quarters (77 per cent) saying we must do ‘everything necessary, urgently as a response’.https:&#x2F;&#x2F;www.ippr.org&#x2F;blog&#x2F;as-some-politicians-seek-to-divide... reply Fricken 9 hours agorootparentI&#x27;ve read those numbers, and my anecdotal experience more or less corresponds to them. I know plenty of people who acknowledge climate change and think it is serious.They&#x27;ve all had their whole lives to make climate conscious lifestyle decisions, they could all stop eating meat tomorrow, but they ain&#x27;t done squat. Not as individuals nor as a group. The few who have are weirdos who live at the margins. replynradov 18 hours agorootparentprevWW2 only lasted 4-7 years depending on which country you were in. There was direct, visible progress along the way. As soon as the war was over, everything could go back to \"normal\" (at least for the winners).What you&#x27;re asking for is a permanent reduction in standards of living. While that might be the right thing to do, it&#x27;s going to be difficult to convince average people to vote for that. Many of them are barely getting by as it is and don&#x27;t think they are able to sacrifice more. reply NoGravitas 15 hours agorootparentI was listening to \"The Regrettable Century\" podcast&#x27;s episode on degrowth recently, and they made a couple of good points here. The first point is that degrowth as reduction-in-consumption only is really going to affect the global rich; everyone else can afford to come up a bit to a new lower average. The second point is that there&#x27;s more to quality of life than quantity of consumption. Nobody is talking about, e.g., making modern medicine go away. It means more trains and fewer cars, fewer Funko Pops and more live music. It means shorter working hours and no commutes. Fewer treats but more leisure. It&#x27;s only a permanent reduction in \"standards of living\" if by that you mean \"amount of consumption\" rather than \"quality of life\". reply flagrant_taco 7 hours agorootparentprevDefining standards of living in this way is very short term focused. If we reduce energy demand now but avoid a power grid collapse a decade down the road, is that better or worse for standard of living? And if we really are meant to believe that climate change is irreversible if we don&#x27;t change course before 2030, who gives a damn about stander of lining for the next few years? reply nradov 6 hours agorootparentSure, that&#x27;s the logical analysis. But many people do give a damn about their short-term standard of living and vote on that basis. reply pfdietz 19 hours agorootparentprev> Asking people to have less, experience less, and do less isn’t an argument that can ever win.What needs to be done is cost the damage from unrestricted emissions. You can&#x27;t just look at the benefits.Consider the original Clean Air Act. Economists estimate the value of reducing harmful pollution due to that act exceeded the cost of implementing the reductions by a factor of 40. FORTY! It&#x27;s like picking up free money. And the same is going to be true of CO2 emissions reduction.Now, it&#x27;s not true for those personally benefiting, like oil companies. Well, screw them. reply ericmay 19 hours agorootparentprev> Asking people to have less, experience less, and do less isn’t an argument that can ever win.Yes and no. I agree that if your approach is to ask people to do this, you aren&#x27;t going to win the argument.But instead you can reframe the argument.Does that $70,000 truck that you have to make payments on while you do back-breaking or soul-crushing work to afford it really make you happy? Do you really get use out of that or are people scamming you with clever marketing?Do you really enjoy glamping or do you just like the idea and you sit around on your phone posting pictures about how much you like it while not really doing much. Are you honest with yourself?Etc.> Cramped trains will rarely win politically but electric cars that go fast and look cool do win.We don&#x27;t even have trains, let alone cramped trains. This one is funny though because everyone \"loves how walkable XYZ area is\" but then they get their mind completely wiped once they leave not realizing they can live like that too if they demanded it or if we had a good market mechanic in place that allows for choice.For some bewildering reason we don&#x27;t build more of the most loved homes and neighborhoods and transit experience like we do for every single other product. reply spenczar5 19 hours agorootparent> are people scamming you with clever marketing? … Are you honest with yourself?This is still an adversarial tone which is more likely to create resistance. It comes across as insulting their intelligence.Convincing people to change is really hard! reply nemo44x 14 hours agorootparent> Convincing people to change is really hard!It’s hard when you’re trying to trick them. But show someone a genuinely better product and you don’t need to convince them of anything. reply EVa5I7bHFq9mnYK 19 hours agorootparentprev>> Does that $70,000 truck that you have to make payments on while you do back-breaking or soul-crushing work to afford it really make you happy?Cars, apart from their direct purpose, also serve as status symbol. This might justify the investment. reply ericmay 15 hours agorootparentYou can justify anything. I need a new iPhone every year. Why? Direct purpose and status symbol. I also need my home heated and cooled to exactly 70 degrees each year because it helps me be more comfortable while working from home. I also need to drive a new electric truck for common errands like going to the store to buy a bottle of wine from California or my asparagus from Argentina. All justifiable investments in my eyes. reply nemo44x 14 hours agorootparentprev> But instead you can reframe the argument.Why would someone argue with you about what they like? You’re assuming that they are dumb and that you are smart. That’s a bad way to approach persuading people. reply nradov 17 hours agorootparentprevThe people that I know with those $70K trucks seem to really enjoy them and get a lot of use out of them. Especially for towing trailers they work really well.If they didn&#x27;t buy those trucks I don&#x27;t think they would work any less. They would just spend the money on something else. reply ericmay 15 hours agorootparentYea same with private planes and stuff like that. reply EVa5I7bHFq9mnYK 19 hours agorootparentprevThis whole \"experience\" thing is overrated. Tens of millions of people go to see Mona Lisa, what do they get out of it? Absolutely nothing IMO. Their travel created an awful amount of CO2. Had they sat at home and read about Renaissance art and history, they and the world would be much better off. reply aydyn 18 hours agorootparentThe vast majority of people disagree with your subjective opinion. Most people dont want to sit in front of their computer their entire lives. reply EVa5I7bHFq9mnYK 18 hours agorootparentThey can go and take a stroll in a park. There is more beauty in that than jostling in a crowd of tourists in front of the Mona Lisa. reply aydyn 10 hours agorootparentAgain, the vast majority of people disagree with you. Stop pushing your values onto others. reply NoGravitas 15 hours agorootparentprevOr play softball. Or listen to some live local music. reply KronisLV 18 hours agorootparentprev> Cramped trains will rarely win politically but electric cars that go fast and look cool do win.That&#x27;s unfortunate, since trains would surely be the more efficient form of transportation, at least when dealing with regular commute, like going to work and back. Over here, a train ticket costs a few Euros, you don&#x27;t have to worry about driving or traffic jams, gas&#x2F;charging, or parking. Of course, if you need to go to a specific rural area or transport some heavy items, then the equation changes, but that&#x27;s not the majority of the cases.I&#x27;m actually rather sad that they closed down a train route to a city next to my countryside residence, since buses aren&#x27;t as comfortable in most cases. Either way, public transportation feels like an obvious necessity to me, though many might disagree. reply sho_hn 19 hours agorootparentprev> Asking people to have less, experience less, and do less isn’t an argument that can ever win.I think that&#x27;s setting the bar very low, and I don&#x27;t think it&#x27;s necessarily true.Most of us learn how to manage a finite resource (our own income) and how to maximize results within those constraints. This is not some sort of alien situation that humans can&#x27;t identify with. We face questions like \"Do we really need this?\" and \"Can we afford it right now?\" all the time.We need to get better at organizing ourselves in large groups&#x2F;structures to do this at the global level, and make people feel equity in the decision-making and that they can afford this equity, and I&#x27;m quite optimistic we can in principle get there.Whether we can do it fast enough is the worrying bit. reply neaden 18 hours agorootparentprevThis sounds like the argument someone in massive credit card debt would give when told to cut their spending. Sure I hope there are ways to solve climate change without lowering standard of living, but if there aren&#x27;t we still have to do it or our standard of living is going to get lowered from the consequences of our civilizations actions. reply hcurtiss 18 hours agorootparentI think the point is that we are not that civilized. If the stakes are as high as many claim, we would be waging war to stop climate emissions. Of course, we won&#x27;t do that. And if we did, many more would wage war to continue emitting (and emit in the process). This is a one-way train unless and until there&#x27;s a lower-cost, low emissions solution. Humans are humans, and politics are politics. The stakes are too high to \"altruism\" our way out. reply hughw 20 hours agorootparentprevYou&#x27;re probably right but I fear the end of that road is a completely \"re-terraformed\" Earth we create to adapt to support our industrial society. reply generalizations 19 hours agorootparentSo we take the notion of national parks to the extreme, and carve out parts of the planet dedicated to preservation, and dedicate other parts to supporting humans and their needs, and we learn the science we need to artificially protect against ecological collapse. Maybe the solution to our expanding technology is technological. reply mrguyorama 16 hours agorootparentSo because the fix is mildly inconvenient, better to just do nothing, keep making the problem worse, keep letting people die, and just hope that magic fixes everything?Tech can&#x27;t undo the physics of the greenhouse effect. You want a cooler earth, you have to reduce the radiation we receive or increase the radiation that leaves through the atmosphere. For hundreds of years we have been participating in a geoengineering experiment. reply generalizations 13 hours agorootparent> mildly inconvenientWildly disruptive. reply londons_explore 19 hours agorootparentprev> Asking people to have less, experience less, and do less isn’t an argument that can ever win.Asking people to eat less might never win... But when there is a famine, people have no choice but to eat less. It&#x27;s not that there is no food available in a famine - it&#x27;s just that prices go sky high.Tomorrow the government could shut down the oil wells and say \"nobody may emit any CO2 unless they pay $1000&#x2F;ton of CO2 emitted.\".The population may revolt, like they do during famines. But with those oil wells blocked up, they probably wouldn&#x27;t get what they were looking for. reply spenczar5 19 hours agorootparent- which government?- did I emit a ton of CO2 when I cut down a tree?- what happens to the cost of medical plastics, etc? Did you just kill millions in poor countries because the cost of care went up so much?This stuff is hard. Pretending it is simple stops us from working on the hard parts. reply mrguyorama 16 hours agorootparent>what happens to the cost of medical plastics,Plastic production is an insanely small part of oil and gas usage, 4%. Anyone bringing up plastic as a reason to not stop using oil isn&#x27;t being intellectually honest. We could literally decimate the oil industry and still produce way more oil than the plastic industry needs. It&#x27;s just not a concern. reply spenczar5 14 hours agorootparentThe proposal was to crank up the price of oil directly. You couldnt price-curve your way past a $1,000 per CO2 ton tax. While plastic does not drive oil production, it is definitely the case that oil prices determine plastic production costs.Recycling can absorb much of that, but medical plastics cannot be made through recycling today. reply londons_explore 12 hours agorootparent$1000&#x2F;ton is $1&#x2F;kg. Most medical plastic items are tens of grams - so 1 cent.Considering the price of medical devices in the US, there is certainly room for a few extra cents. reply flagrant_taco 19 hours agorootparentprevIt doesn&#x27;t necessarily have to be done through further government intervention. Simply removing government protections and subsidies for industry would go a long way.That&#x27;s definitely not a complete solution (if one exists), but that could be implemented over night if we wanted to do it. reply Roark66 19 hours agorootparentprev>Tomorrow the government could shut down the oil wells and say \"nobody may emit any CO2 unless they pay $1000&#x2F;ton of CO2 emitted.\".And it would not change anything at planet scale, because on the other side of the world China is going to \"pick up any slack\" of CO2 generation we may not do.It shouldn&#x27;t stop us from trying to increase efficiency, but panic and insane ideas of the sort of \"no one can emit a ton of CO2 without paying $1k\" are not the way to improve things. The biggest fight in the world today is not for \"the climate\", but for the freedom to live in a non-autocratic country. We would have to win the second to have the slightest shot at cooperating well enough to have a shot at mitigating the first for those of us in places that will be hit the worst.Do you think a communist-by-name party of China (or Russian mafia-state) gives a damn about the climate and how it affects people in places like subsaharan Africa, Bangladesh etc? Not in the slightest, but they&#x27;ll gladly use every bit of it to promote panic and industry killing policies in places like the US, EU while pretending to \"be green\". If we win the fight not to be ruled by corrupt dictators we can have a shot of improving the lives of people who will suffer due to climate change. reply dragonwriter 18 hours agorootparent> And it would not change anything at planet scale, because on the other side of the world China is going to \"pick up any slack\" of CO2 generation we may not do.Even if that was true in the short term (and reducing global supply by shutting down oil wells makes ir unlikely) the relative incentive for alternatives created here would lead to global improvements over the medium and longer term.(That said, the time to apply solutions with medium-to-longer term payoff was 50 years ago.) reply xorcist 17 hours agorootparentprevI may have posted this several times before, but China is not the problem. We know how to deal with that: international trade agreements.The problem is that it requires the US to be on board with it. Each one alone couldn&#x27;t, but the EU and US together would have the economic force required to solve this problem, if they acted with decisiveness. reply nemo44x 13 hours agorootparentWhat does the USA get out of it? The USA is the #1 producer of oil and the commodity is traded in her currency globally. This makes the USA energy independent and keeps demand for dollars extremely high.The EU on the other hand has virtually no ability to produce oil and gas and is dependent on outside suppliers. So of course they have a lot of incentive to move off of it and that’s much easier if everyone else does too.American hegemony is at stake if oil demand is dramatically reduced quickly. Where as the EU stands to gain influence. So again, why would the USA weaken herself to the benefit of the EU? reply chimprich 18 hours agorootparentprev> And it would not change anything at planet scale, because on the other side of the world China is going to \"pick up any slack\" of CO2 generation we may not do.We&#x27;re still far better off decarbonising because (rest of the world CO2 + China CO2) > China CO2. reply NoGravitas 15 hours agorootparentChina&#x27;s per capita carbon emissions are half those of the US (8 metric tons vs 16 metric tons, 2018 figures). Mote, beam, etc. reply Roark66 4 hours agorootparentAnd you believe any numbers that come out of China? If they are reporting half they are probably emitting 3x in reality. China has been approving a new coal fired power plant every 2 weeks for years. reply nemo44x 19 hours agorootparentprevSo a government that violently suppresses dissent and only wealthy people can emit CO2? Yeah, sounds great! I don’t think eco-fascism is going to get broad support… reply vixen99 18 hours agorootparentNot sure about that. Announce a climate emergency with huge implications I don&#x27;t need to detail and you might get away with anything irrespective of support. reply nradov 17 hours agorootparentA government foolish enough to abuse emergency powers that way would be voted out in the next election. reply dragonwriter 17 hours agorootparentAbuse emergency powers sufficiently, and there is no next election, at least not on terms where it is possible for you to lose. replylo_zamoyski 18 hours agorootparentprev> We could easily cut our use of fossil fuelsGiven our economic dependence, the economic efficiency, and interested resistance, what concrete course of action would achieve this transition?> We could stop incentivizing people and companies to consume disposable products.How? Concretely. reply ChatGTP 19 hours agorootparentprevThe planet is complex but not complex at the same time.It&#x27;s easy enough to break the natural environment by very primitive means.So I agree with your point, it&#x27;s just not that complex that we can&#x27;t totally ruin it quite easily. reply flagrant_taco 19 hours agorootparentThat opens up a whole apndoras box of whether a reduction is approach can lead to a complete understanding of complex systems. I&#x27;ll leave that for another thread, that&#x27;s a deep rabbit whole full of mostly theoretical debate (though an interesting one IMO!) reply nradov 18 hours agorootparentprevSome of your proposed first steps are hardly obvious. Food transport is highly efficient and constitutes only a tiny fraction of CO2 emissions. Eating locally isn&#x27;t going to help; it might even have the opposite effect in some places. Same for clothing.In urban or suburban housing people don&#x27;t have enough space or direct sunlight to grow a useful amount of food. reply stjohnswarts 17 hours agorootparentprevI always figured that if more liberal politicians enact all at once the grueling regime required for current climate deadlines that it might work for a while but then the citizenry will then just revolt and vote right wing and it will set back all progress that has been made the past couple of decades. I already see it in American politics. reply rafaelero 18 hours agorootparentprev> There are such obvious first steps we could take without a complete understanding of cause if we really cared.Have you tried arguing people against eating meat? If we can&#x27;t even talk them out of the main individual contributor to climate change, I don&#x27;t think we have a good chance to make a positive impact on other areas that depend on conscious effort. reply kurthr 17 hours agorootparentThis doesn&#x27;t seem to match the data I&#x27;ve seen.Yes, food is a significant portion of energy consumption, but typical transportation and housing (including heat&#x2F;cool) easily dominate. Going from a daily mixed meat (chicken&#x2F;fish&#x2F;beef&#x2F;eggs) eating diet to a vegan diet saves ~1 ton CO2 per year. Average is about 4-5 tons per year for a person living in a small apartment in temperate climate, who only uses public transportation, doesn&#x27;t go on vacation, or buy any unnecessary consumer items. A more typical westerner who can afford meat every meal is about 10-20 tons per year.Now, if you&#x27;re eating 150g of beef every meal 3x every day, you could double your footprint (+20tons CO2). Eating only chicken, fish, and eggs gets you below 1ton&#x2F;year for meat in meals. Vegan is about a third of that.https:&#x2F;&#x2F;8billiontrees.com&#x2F;carbon-offsets-credits&#x2F;carbon-ecol... reply rafaelero 16 hours agorootparentFood corresponds to 10~30% of individual carbon footprint. Dairy + meat amount to 3&#x2F;4 of total emissions from food production. So going vegan should theoretically reduce from 7.5% to 22.5% total individual emissions[1]. It may not be the largest factor in every situation, but it is still a very important one.[1]https:&#x2F;&#x2F;css.umich.edu&#x2F;publications&#x2F;factsheets&#x2F;sustainability... reply erghjunk 20 hours agoparentprev> The degree of \"El Nino\" influence is presumably known, and if its strength is not related with global warming in complicated and unknown ways it would be good to overlay it as a distinct effect.A brief web search suggests this presumption is incorrect and that climate change is having an effect on the cycle, rendering this division somewhat meaningless - the old El Nino is \"gone,\" in a sense, and only the climate change effected one remains.from: https:&#x2F;&#x2F;research.noaa.gov&#x2F;2020&#x2F;11&#x2F;09&#x2F;new-research-volume-exp...“No two El Niños or La Niñas are perfectly alike,” Capotondi said. “We’ve seen how diverse ENSO events can be. This diversity adds another degree of complexity for understanding how climate change will influence future ENSO events.”So how are ENSO impacts likely to evolve in the coming decades?“Extreme El Niño and La Niña events may increase in frequency from about one every 20 years to one every 10 years by the end of the 21st century under aggressive greenhouse gas emission scenarios,” McPhaden said. “The strongest events may also become even stronger than they are today.”and here is the full book: https:&#x2F;&#x2F;agupubs.onlinelibrary.wiley.com&#x2F;doi&#x2F;book&#x2F;10.1002&#x2F;978... reply nologic01 19 hours agorootparentThe chapter on \"ENSO Diversity\" seems the most relevant for this discussion but it is alas behind a paywall.The available summary does not hint there is something conclusive yet on these interactions: \"Current research seeks to determine whether such changes in ENSO characteristics were the result of anthropogenic greenhouse gas forcing or just a manifestation of natural variability, and whether and how climate change may affect ENSO diversity in the future.\" reply secretsatan 19 hours agoparentprev> The other segment of the population that can be affected by a clear and well founded explanation of what is happening are those sitting on the fence about climate change or feeling skeptical for legitimate reasons.I simply no longer believe these people really exist, that information has been around, it has been around for decades, and yet this argument comes up time and time again, I&#x27;m a reasonable person, persuade me. Nah.... it&#x27;s been done time and time again. reply jgreen10 19 hours agoparentprevOne theory is that it&#x27;s too early in the cycle to attribute the warming to El Nino (which will make things even worse), but that it&#x27;s due to a reduction in SO2 in shipping fuels causing lower solar reflection and heating up the oceans faster.However, it&#x27;s hard to be sure. A discussion: https:&#x2F;&#x2F;atmosphere.copernicus.eu&#x2F;aerosols-are-so2-emissions-... reply hermannj314 19 hours agoparentprevAmericans wear ignorance as a badge of honor, specifically, ignorance of math.\"Let&#x27;s not get lost in the details\", \"Let&#x27;s take a 30,000 foot view of the problem\", etc. are common sayings in corporate America that boil down to \"I&#x27;m not that confident in math, so can we stick to hard-to-prove-false qualitative claims only, please?\"I love this sentiment, I hope it takes off, but I gave up trying years ago. reply chimprich 20 hours agoparentprev> It would be quite educational to attribute in more detail the impact of these two factors. The degree of \"El Nino\" influence is presumably known, and if its strength is not related with global warming in complicated and unknown ways it would be good to overlay it as a distinct effect.I&#x27;ve seen some attempts to quantify the difference. E.g. this article tries to illustrate it: https:&#x2F;&#x2F;theconversation.com&#x2F;july-was-earths-hottest-month-on... reply Aurornis 18 hours agoparentprev> It would be quite educational to attribute in more detail the impact of these two factors. The degree of \"El Nino\" influence is presumably knownUnfortunately, they aren&#x27;t independent variables. The climate change effects of global warming have also altered the function of El Nino. You can&#x27;t entirely subtract one variable out because they&#x27;re interlinked.If you really wanted to, you could squint and look at the trend lines of previous months and years and try to extrapolate a trajectory. It would still be going up. reply noknownsender 18 hours agoparentprevIf there is a potion of the population that is \"sitting on the fence about climate change\", it is highly unlikely that they \"can be convinced by a clear and well founded explanation of what is happening\"Anybody capable of that, would either no longer be sitting on the fence, or are highly unlikely to be reading this article.Also, the basic premise of your comment assumes that the effects on the monthly temperature can be cleanly broken down categorically into \"fossil fuel related\" and \"El Nino related\", which presupposes that a stronger and more frequent El Nino events are unrelated to the burning of fossil fuels.The facts the human impact on climate change are overwhelming, both in their abundance and scope of impact. Pretending that they don&#x27;t exist, or treating them as a tangential issue to be put aside so as to appeal to a skeptical and&#x2F;or ignorant minority, serves no purpose other than to undercut the facts. reply nologic01 17 hours agorootparentThere are people who are \"in principle\" accepting climate change but skeptical about the urgency and magnitude of adaptation that is required. This includes critical segments of the business and political worlds who weigh any action against their vested interests. These people will never panic. But they may plot a \"changing of stripes\" strategy if the signals from experts and society are strong and persistent.This is discussion is really about how the experts (and, subsequently, mass and social media) popularize the explanations of weather events and what effect this might have on various segments of the population.> which presupposes that a stronger and more frequent El Nino events are unrelated to the burning of fossil fuels.I didn&#x27;t pressupose anything, I qualified my statement. In fact people posted various interesting recent pieces of research in the thread and I learned that there are both additional potential factors that may be currenty overlapping and that understanding of a potential climate change &#x2F; El Nino link is still elusive.But that is beside the point. Even if there was a known link it doesn&#x27;t change my remark that better framing is required. You would simply condition on the dependence, show the growing El Nino \"diversity\" against the underlying trend and indicate how this may have created an outlier realization. reply lamontcg 17 hours agoparentprevAnalysis done back in 2016, back when 2015 shattered records due to El Nino (and completely shattered the argument around a \"pause\" in global warming due to the El Nino spike back in 1998):https:&#x2F;&#x2F;tamino.wordpress.com&#x2F;2016&#x2F;01&#x2F;27&#x2F;el-nino-and-the-2015...https:&#x2F;&#x2F;tamino.wordpress.com&#x2F;2016&#x2F;01&#x2F;29&#x2F;correcting-for-more-... reply nologic01 16 hours agorootparentSeems wortwhile to update this. Maybe even have a running calculation that is updated on each data release? It provides an intersting context for understanding monthly and annual temperature anomalies. reply lamontcg 16 hours agorootparentI think most people consider it settled science&#x2F;argument at this point. As a veteran of the 2005-2015 internet flamewars on climate science it definitely just bores the living shit out of me to even think about it again. And all the climate science sources from that period (Skeptical Science, Real Climate, Tamino&#x27;s blog) also don&#x27;t publish anywhere near the amount they used to.If you disagree, then be the change you want to see in the world... Setup a website, pull the data, build your own model and publish it. Once you&#x27;ve got it going, I&#x27;d guess that Tamino would be more than willing to correct all your mistakes in your model. reply yongjik 15 hours agoparentprev> population that drifts into panic and depressionvs> feeling skeptical for legitimate reasonsI think it&#x27;s just your bias showing. This is 2023, people who are \"skeptical\" of the climate change aren&#x27;t there for legitimate reasons. And frankly I&#x27;m less and less convinced that we need to engage these people. reply holbrad 14 hours agorootparentI think might be your bias showing...IMO there&#x27;s an awful lot of necessary debate around climate policy. Skeptical can mean anything from thinking:1) Battery storage for intermittent renewables is unsolved and therefore nuclear is a better solution (I think this is actually correct, if we could emulate France&#x27;s historic budgets, timelines and safety)2) The earth isn&#x27;t warming at all and it&#x27;s a Chinese conspiracy.There&#x27;s a lot of room between those two points, even thing&#x27;s as basic as what&#x27;s our current level of warming are harder to pin down than you would expect. reply hotpotamus 20 hours agoparentprevPerhaps those drifting into panic and depression are doing so because we are now clearly seeing undeniable effects and yet denial persists while CO2 emissions creep up seemingly inexorably? reply esarbe 20 hours agorootparentNo, I&#x27;m drifting into panic because I&#x27;ve seen that rational discourse just doesn&#x27;t change anything.There&#x27;s always a reason to not-do anything, to not-upset anything, because, you know, the GDP, the stock market, think of the all the people involved in oil, etc.Meanwhile we&#x27;ve shifted from ridicule to denial through downplaying and the outright delusional \"this could actually be good\".What is not happening are any meaningful steps towards reigning in even the worst excesses.So, yeah. Maybe panic is not the worst state of mind regarding this challenge? reply nologic01 18 hours agorootparentTo paraphrase, dont get panicky, get even. This is a long-term marathon that will change all societies and economies one way or another. The objective is to make sure its eventually a better place. This transormation is not going to be the result of panic thinking or action. GDP, stocks etc are not eternal constructs, they were invented and adopted in a certain context. Well that context is falling apart. But the new constructs are not born yet.There might be some easy, immediate \"quick-wins\" in change of behaviors or new technology, but they are not that many, or particularly impactful. Take for example coal. In various places they thought that eliminating it is a no brainer. Its dirty, lots of CO2 emissions etc. Well guess what. Many poorer countries only have access to coal and they are not particularly keen to remain energy poor to make up for the historical excesses or others. reply throwaway5752 19 hours agorootparentprevAgreed. Panic is the rational reaction [edited from \"response\"] to a near term extinction level threat that not enough people are taking seriously.People advocating half measures or not panicking are incredibly irrational. It&#x27;s a human cognitive bias to try to arrive at intermediate outcomes between extremes (regardless of truth). Extremeness aversion.edit: for instance, IPCC&#x2F;NCA5 author Zeke Hausfather said, \"This month was, in my professional opinion as a climate scientist – absolutely gobsmackingly bananas\" (https:&#x2F;&#x2F;twitter.com&#x2F;hausfath&#x2F;status&#x2F;1709217151452954998)Everyone should be rioting in the streets. If you are not scared for the lives of your children then you simply don&#x27;t realize the severity of this problem. reply sojournerc 19 hours agorootparentPanic is never rational - it&#x27;s an emotion. Giving into it can cloud what action should be taken, or, often, cause actions that make the situation worse. reply aaomidi 13 hours agorootparentSomething not being `rational` is not an argument against it. Rationality is a subjective matter, not objective. reply throwaway5752 19 hours agorootparentprevIf you do not feel panic at the state of climate change, then you are in fact not processing information correctly, ie, not rational. It is not rational to drive a car off the rim of the Grand Canyon; it is rational to jump on the brakes immediately and not fall to ones death. It is rational to feel fear when in great danger. reply nologic01 18 hours agorootparentThis is a wrong take for various reasons. Panic is a response we developed for survival against predators and clear-and-present dangers. But look around, the only predator in sight is us. Can we panic against ouselves? Climate change and more general sustainability is above all a collective social&#x2F;economic and political organization problem.How we internalize and communicate about the predicament we brought ouselves into is important. You don&#x27;t cry fire in the crowded room, you find ways to calmly evacuate.Which brings to the second reason panic is hopeless: It is also objectively not possible for us to \"flee\" in a hurry (Except for Elon and his Mars escapists - good ridance). The crowded room is our planet and there is nowhere to go. We are doomed to \"work it out\", put down the fire. We have no option but accept the damage that will be inflicted in the mean time. The smarter and faster we \"work it out\", the less damage. Thats the equation and tradeof.But nobody ever solved a tough problem in a state of panic. reply JoshuaDavid 18 hours agorootparentprevFear and panic are not the same thing. If your car is rolling towards the rim of the grand canyon and isn&#x27;t slowing down, you would be afraid but ideally would quickly but calmly consider and try your options (emergency brake, steer away from edge, bail before it reaches edge, etc) rather than panicking. reply hotpotamus 18 hours agorootparentWhat if you are the passenger and the driver has their foot down on the accelerator while refusing to believe that the cliff lies ahead? What is the appropriate emotion? reply JoshuaDavid 14 hours agorootparentStill composure, and the appropriate course of action is still to evaluate your options and choose the best one, even if that&#x27;s the best option out of a list of bad options.Unless you think that panic will induce the driver to stop accelerating, I suppose. Though I think the closer analogy would be \"you&#x27;re a passenger in a car with a brick on the accelerator and no driver\", because I don&#x27;t think there&#x27;s any person or coherent group of people who actually control the global economy (in the sense of \"control\" where they can make sweeping changes that go against local incentive gradients, and have those changes stick). reply hotpotamus 12 hours agorootparentIf doom is certain, then what does your composure or anything else matter? reply JoshuaDavid 12 hours agorootparentIf nothing matters, in what sense is panic the \"correct\" response?Also, it&#x27;s useful to check whether you&#x27;re absolutely doomed to lose everything you value, or only probably doomed to lose most of the things that you value, in situations where you appear to be doomed.I don&#x27;t think a panic response is generally helpful towards the goal of \"make the best you can of a bad situation\". replyNoGravitas 15 hours agorootparentprevNo more half measures, Walter. reply aaomidi 20 hours agorootparentprevHonestly I’m getting tired of the “thinkers” who are just _don’t panic_. That statement just comes across as so disconnected and also doesn’t actually accomplish something.If someone is panicking, the correct solution here is to show that there’s a realistic attempt at solving the problem. Not dismissing their valid feelings of dread and panic. reply moffkalast 20 hours agorootparentWell the main issue is that no realistic attempt exists that would be able to solve the problem(s). Thus all that anyone can do inscribe \"DON&#x27;T PANIC\" in large friendly letters per HHGTTG&#x27;s example, so society can continue to run normally for as long as it can while our planet gets demolished. Welcome to the decline, easiest thing to do is to accept it and have fun while we drive off the proverbial cliff. reply jeremyjh 18 hours agorootparentI&#x27;m not sure why this is being downvoted. Yes, there are some realistic choices that could be made, or could have been made to seriously mitigate the issues. But so far we haven&#x27;t pursued those, so there are not realistic attempts so far. We&#x27;re pretending pouring money into renewables is enough, and it isn&#x27;t. We&#x27;re pretending that made up carbon credits will help, and they won&#x27;t because people just use them to lie and steal. Some people pretend consumers aren&#x27;t the problem, that it is \"corporations\". Well, those corporations exist only to funnel goods and services into the gaping maws of consumers. You can regulate corporations, and we must, but the idea that change will happen without our own personal sacrifices is foolish. You need to allow a nuclear plant to be built in your backyard, and then another one. You need to start taking the bus to work. You need to stop eating meat. These would be the beginnings of \"realistic attempts\" if all of society committed to such ideas and changed our collective ideas about what quality of life is. reply oldpersonintx 20 hours agorootparentprevDrifting into panic and depression...or not...is just signallingVery concerned people who do not take action are equally to blame as those who don&#x27;t care reply antifa 5 hours agorootparentYes, the people in the back of the bus are exactly as equally to blame as the bus driver. reply worksonmine 15 hours agoparentprevI would like to see solar activity included in the discussion as well. But anytime I bring it up I&#x27;m called a climate denialist. But we&#x27;re seeing auroras very far south and those days it&#x27;s no issue mentioning the sun is currently in an active phase. Why is that detail always omitted when it comes to global warming, sorry climate change? reply jeremyjh 15 hours agorootparentI don&#x27;t understand. Do you think there is scientific doubt about the greenhouse effect, or that CO2 is a greenhouse gas, or that for hundreds of thousands of years CO2 concentrations have correlated with average temperature? There isn&#x27;t any doubt about the problems of greenhouse gases, and there isn&#x27;t anything we can do about the sun&#x27;s activity other than use it as an excuse to do nothing \"until we understand\". reply worksonmine 15 hours agorootparentThis is the kind of responses I&#x27;m referring to. I&#x27;m not saying we shouldn&#x27;t do anything as you assume, I&#x27;m wondering why solar activity isn&#x27;t accounted for in the models. Maybe it can explain something? Trust the science and all that you know, being thorough.I&#x27;m curious if the changes we see can be explained by the sun activity yes. I&#x27;m not qualified to assess which of the scientists are right and which aren&#x27;t, and to what extent CO2 gases cause global warming. Instead of just asking me to take it for a fact when asking a question please use your expertise and explain it in a sense that I can understand and verify.I&#x27;m asking out of curiosity and because I want to understand, to me this could all be because of the sun and an elaborate cash-grab, just as plausible. Because it&#x27;s not about stopping it at all costs, some people can apparently buy the right to produce more? With these kind of hand-wavy responses from those who claim to have the knowledge where do you think me and other like me are turning for information?I don&#x27;t know if it&#x27;s easier to build a giant sunscreen than to get the worlds population eating bugs because apparently cows fart too much? But we still keep exploiting the amazon, and doing everything we can to keep the endless growth economy we&#x27;ve built going. So how pressing can it really be? We&#x27;re fighting wars over more resources, when maybe we should be fighting to stop consumerism?I don&#x27;t know. But I&#x27;m not taking anyones word for it, help me understand or move on. Don&#x27;t tell me what to think. reply paulmd 13 hours agorootparent> I&#x27;m wondering why solar activity isn&#x27;t accounted for in the modelswhy do you work from the assumption that it&#x27;s not? a trivial search turned up this article from 2011 which says that models already do include approximations of solar activity even at that time.https:&#x2F;&#x2F;www.imperial.ac.uk&#x2F;media&#x2F;imperial-college&#x2F;grantham-i...obviously it is something that is constantly being improved, like everything else in the model. but there aren&#x27;t good direct historical proxies for solar output - you can&#x27;t take a \"core sample\" and go back 1000 years of solar activity, so the datasets really only go back 60 or 70 years there, and even rough measurements of sunspot activity&#x2F;etc will only probably go back less than 200 years. but over the last 15 years this is something that&#x27;s been actively studied and addressed and they&#x27;re trying to incorporate more detailed modeling.https:&#x2F;&#x2F;eos.org&#x2F;science-updates&#x2F;better-data-for-modeling-the...again, this only affects between 7-30% of the total picture, according to most studies. so it&#x27;s not like this is where global warming is coming from. contributing factor? sure, a small one.but like, I don&#x27;t know why everyone works from the assumption that they thought of one thing that all the scientists forgot about completely. One clever trick to explain global warming, climate scientists hate him.which is why stuff like this is not really constructive or helpful. like if you don&#x27;t know, why are you so sure about what is being incorporated or not? why isn&#x27;t the question \"how is this being incorporated\"? Because it probably is.> I don&#x27;t know. But I&#x27;m not taking anyones word for it, help me understand or move on. Don&#x27;t tell me what to think. reply worksonmine 12 hours agorootparentMuch more constructive thank you. To be honest I haven&#x27;t looked into it much in the last decade just going from the news and general discourse. I&#x27;ve noticed that we&#x27;ve been going into an active phase and it&#x27;s mentioned a lot in other situations but never when it comes to the reasons behind climate change. Especially since it could make it worse I&#x27;d assume they&#x27;d push harder on it being more of a problem now. Feels like they&#x27;re hiding something.According to your first link we can track solar activity:> Evidence of variations in solar activity on millennial timescales can be found in the records of cosmogenic radionuclides in such long-lived natural features as ice cores from large ice sheets, tree rings and ocean sediments. Using careful statistical analysis it is now possible to identify decadal and centennial signals of solar variability in climate data.7% sounds low yes, but 30% is absolutely not a \"small\" factor. I&#x27;d think the direct effect of solar activity would be much easier to measure even on a micro level than CO2. A decade back a graph was popular where temperature and solar activity correlated almost 1:1.I&#x27;m not trying to argue for emissions, I&#x27;d prefer no cars, fresh air and banned plastics. But an honest debate where I&#x27;m not a conspiracy nut for asking completely valid questions is not much to ask for (not directed at you just the general discourse).If it really is serious then don&#x27;t tell me we need to keep the economy going, that we have a demographic problem or try to sell me emission rights. We do or we don&#x27;t, the next iphone or the next generation, that&#x27;s the solution I expect to the level of threat they&#x27;re telling me it is. Meanwhile they fly in private jets from summit to summit.I&#x27;m not even trying to keep my way of living I&#x27;m already a hermit with a broken screen on my 4 year old phone, and rarely eat red meat. What they say and what they do doesn&#x27;t seem to match. replybadcppdev 21 hours agoprevhttps:&#x2F;&#x2F;www.defense.gov&#x2F;News&#x2F;Feature-Stories&#x2F;story&#x2F;Article&#x2F;2...I think the Overton window needs to shift soon to include the idea that widespread government intervention will be required to avoid 800 million climate refugees reply ramraj07 20 hours agoparentI don’t believe it’s possible to avoid anything anymore, Merely the scale will be determined at best. It’s not just going to be 800 million refugees. I suspect it’s going to be actual deaths in that range. And with no real intervention as is the case now, likely in the billions. Given how the whole world handled the pandemic I have no confidence it’ll unfurl any other way. A sobering thought that keeps me awake for a bit every night. reply badcppdev 20 hours agorootparentLooks like you were getting some down votes for talking about the almost worst case scenario. No rebuttals unfortunately reply m_fayer 20 hours agorootparentprevI think some time in the next decade there will be a “perfect storm” heat wave combined with a power failure, and millions will die in days. This will jerk the Overton window and popular sentiment in an unpredictable direction, and no one knows what the world will look like on the other side of that. reply ramraj07 15 hours agorootparentMillions died in the pandemic - didn’t do much to affect any opinion really. Just see in the replies to my comment here. Deniers out to prove desperately that this is all going just fine. reply CalRobert 18 hours agorootparentprevA billion people in India could die and US elections would still be determined by gas prices. reply m_fayer 17 hours agorootparentBut what would India do? It might be something hard to ignore. reply reducesuffering 16 hours agorootparentIndia will attempt to geoengineer the sky way before that happens, international pleading or possible other dire consequences be damned. reply jonhohle 19 hours agorootparentprevWhere in the world would this event take place? In the hottest parts of the US the temperature gets to 46ºC regularly during the summer and people have lived there for hundreds of years. Do people die of heat stoke? Sure, at a rate of hundreds per year. Elsewhere in the world there are places where people love in the same heat with less means of cooking and less access to water.Millions of people may have to adapt. Fortunately, people are good at that. reply ljf 19 hours agorootparentWhere would this take place? In places where the wet bulb temperature is already getting close to the limits of human endurance:https:&#x2F;&#x2F;www.theguardian.com&#x2F;science&#x2F;2022&#x2F;jul&#x2F;31&#x2F;why-you-need...People can survive amazingly hot temperatures, as long as they can sweat - once the wet bulb temp gets far above 31c people are in trouble - the theoretical maximum is 35c.We won&#x27;t hit these wet bulb temps in places like the US or even much of Europe, but built up China, India etc are at risk of approaching these in our lifetimes, potentially far sooner. reply vlz 17 hours agorootparentThank you! It is really important to emphasize that heat and humidity have both to be taken into account. To quote a relevant passage from the article you linked:> Concern often centres on the “threshold” or “critical” WBT for humans, the point at which a healthy person could survive for only six hours. This is usually considered to be 35C, approximately equivalent to an air temperature of 40C with a relative humidity of 75%. (At the UK’s 19 July peak temperature, relative humidity was approximately 25% and the wet-bulb temperature about 25C.)(WBT is Wet Bulb Temperature) reply NoGravitas 15 hours agorootparentprevKSR&#x27;s novel \"The Ministry For the Future\" starts with a graphic account of a 35C wet bulb event in India. The heat initially raises the demand for air conditioning, the demand for air conditioning over-stresses the electrical grid, the electrical grid fails so no one has air conditioning, millions die. I don&#x27;t remember exactly when it&#x27;s set; in the 2030s? reply rando_dfad 14 hours agorootparentprevTexas, maybe? If the power grid goes down because of excess AC usage?Yes, I read that people have been living for 100s of years (pre-AC) through temps regularly hitting 46 degrees, but see also the response on wet-bulb temps and survivability. 46 degrees for 5 days straight without AC (as in, backup power is gone...) and I think we&#x27;ll see some losses.Europe regularly sees large deaths due to heat during summers already today, true mostly in elderly. reply ramraj07 15 hours agorootparentprevThe others have pointed out the humidity, dry heat doesn’t just kill hundreds. The 2003 heat wave likely killed 70000 people in Europe. You know, a developed country. https:&#x2F;&#x2F;www.france24.com&#x2F;en&#x2F;environment&#x2F;20230717-parisians-a... reply moffkalast 20 hours agorootparentprevYeah it might be wise to invest into some backup solar and an inverter to run off-grid in a pickle, after all max heat will also typically mean max solar output at the same time. reply cm2012 20 hours agorootparentprevI&#x27;ll bet you a thousand dollars there will be not even 10 million deaths over current level directly linked to climate change, like drought, heat stroke, etc. reply DanHulton 20 hours agorootparentYou are delusional. That could happen in a single event, let alone \"ever\" as your comment is implying. reply cm2012 19 hours agorootparentIn fact, I would go even further and predict that all cause mortality will be under baseline for countries most affected by climate change in the next 10 years vs 10 years prior, and life expectancy will go up. I absolutely believe that climate change is happening, I think tech and other improvements will outpace the damage it does. I&#x27;m very serious about betting on this. reply DanHulton 5 hours agorootparentYou know, I&#x27;m semi interested in firming up this bet, only because I&#x27;ll be fucking THRILLED to lose.Make it a hundred, though, so I can defend it to my dependants better and you&#x27;re on.(I realize that even if I win, I lose, but whatever, I&#x27;m more interested in talking with someone who has significant hope, enough to put money on the line.) reply gottorf 19 hours agorootparentprevThe World Meteorological Organization estimates that in the 50 years from 1970 to 2019, deaths from climate dropped to one-third[0]. That&#x27;s not one-third the rate; that&#x27;s one-third of the absolute number of deaths, even as the world population more than doubled in that time period and atmospheric CO2 went from ~325ppm to ~410ppm.In my opinion, this reduction in deaths is mostly due to the increasing ability of the developing world to better master their environment, enabled by technological advances powered largely by cheap energy from fossil fuels. If you have alternative explanations, I&#x27;m happy to hear it. But as the evidence stands, it would be more delusional to think climate change will cause rampant mass deaths.[0]: https:&#x2F;&#x2F;library.wmo.int&#x2F;viewer&#x2F;57564&#x2F;download?file=1267_Atla... reply jncfhnb 15 hours agorootparentIt is fully possible that we have accumulated better technology while still pushing the climate to exponentially more dangerous states. reply bitxbitxbitcoin 18 hours agorootparentprevAll it takes is one wet bulb event in a city near the equator. The deaths will be among those that can’t access climate control powered by cheap fossil fuels. reply cm2012 18 hours agorootparentAnd I&#x27;d happily bet that even with extra wet bulb events, death rate will be lower and life expectancy higher at the equator. And I would assign a .0001% chance of a single wet bulb event killing millions of people. reply Filligree 18 hours agorootparentI’ll take that bet, at 1000:1 odds; that should still be profitable for you. How do you want to handle it? reply cm2012 18 hours agorootparentNot worth my time at those odds to set it up replyNoGravitas 15 hours agorootparentprevYeah, we got a Pinker Thinker here. Dangerous temperature and humidity combined with failing infrastructure will kill millions rapidly, while two simultaneous crop failures in different parts of the world will throw the global food supply into shambles and kill even more people, slowly. reply morkalork 20 hours agoparentprevThere will be widespread government intervention. The only question is what it&#x27;s going to look like. reply Server6 7 hours agorootparentUnfortunately the longer wait the most likely the response is going to be war. reply lm28469 21 hours agoparentprev> will be required to avoid 800 million climate refugees\"to handle\" or \"to mitigate\" it&#x27;s way to late for \"to avoid\" reply badcppdev 20 hours agorootparentI would like to be an optimist and disagree with you but my optimism is being used with hoping that I don&#x27;t end up stuck on the wrong side of the planet to my family when airlines have to start charging for the impact of the carbon released. reply Hamuko 20 hours agorootparentprevI get a feeling it&#x27;ll be \"handled\" in the same manner that it&#x27;s \"handled\" in Cyberpunk 2077 if that&#x27;s the scale that we&#x27;re talking about.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=UamAMzVT4YA reply stevenwoo 18 hours agorootparentThe one plausible thing I found in Ministry for the Future was a country in a large scale heat crisis, India, putting a lot more pollution in the air to short term delay the warming trend over India. reply micromacrofoot 20 hours agoparentprevthe US can barely manage to pass its annual budget, I&#x27;m not hopeful reply brookst 20 hours agorootparentA large portion of the US populace wants to maintain&#x2F;increase fossil fuel usage just to stick it to their perceived enemies. Also not hopeful. reply gottorf 20 hours agorootparentWhat about the portion of the US populace that wants to maintain&#x2F;increase fossil fuel usage because they think it&#x27;s the best option they have? Is that not a valid viewpoint? reply Aurornis 18 hours agorootparentCome decision time, most people pick whatever meets their needs for the cheapest price.The ideologically-driven people are a minority. A very vocal minority, but a minority.Most people just choose what&#x27;s cheap, convenient, and gets the job done. We can put our fingers on the scale by shifting subsidies and taxes around. reply brookst 8 hours agorootparentprevOf course it’s valid, and those people would not be in the “rolling coal to own the libs” population. reply esarbe 20 hours agorootparentprevI think it&#x27;s an uneducated viewpoint that is supported by the deliberate misinformation of the oil and gas lobby.There are alternatives, there have always been alternatives. Better ones, cheaper ones. reply jonhohle 19 hours agorootparentIt’s also uneducated to eliminate fossil fuels at the expense of all else. There is real impact to life by destroying economies[0]. This is not a one dimensional problem.0 - https:&#x2F;&#x2F;journals.plos.org&#x2F;plosone&#x2F;article?id=10.1371&#x2F;journal... reply esarbe 18 hours agorootparentNo one is demanding to get rid of \"fossil fuels at the expense of all else\". That&#x27;s a bit of a straw man.We&#x27;re far away from getting to the point where we&#x27;ve gotten rid of fossil fuels at the expense of everything else. We&#x27;ve not actually really started to get rid of fossil fuels, yet. reply gottorf 19 hours agorootparentprevI posit that any deliberate misinformation by the oil and gas lobby can and will be equally matched by deliberate misinformation by the wind and solar lobby. There&#x27;s no reason to believe that the former is composed of evil people who mislead for the sake of profit, and the latter of good people who just want the best for everybody.If there are \"better and cheaper\" alternatives available right now, market forces will assure their dominance. The oil and gas lobby is not so omnipotent that millions of people looking out for their own best interests, despite the imperfection of their knowledge, will stick to an obviously inferior means of achieving their goals.In other words, very few people will actually \"maintain&#x2F;increase fossil fuel usage just to stick it to their perceived enemies\", as the GP said, if it hurts their pocketbooks to do so. reply useless999 18 hours agorootparent> I posit that any deliberate misinformation by the oil and gas lobby can and will be equally matched by deliberate misinformation by the wind and solar lobby. There&#x27;s no reason to believe that the former is composed of evil people who mislead for the sake of profitThere is plenty of edvidence to the contrary. See e.g.“Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming”https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Merchants_of_Doubt“Tracing Big Oil’s PR war to delay action on climate change”https:&#x2F;&#x2F;news.harvard.edu&#x2F;gazette&#x2F;story&#x2F;2021&#x2F;09&#x2F;oil-companies...“Exxon Knew about Climate Change almost 40 years ago”“A new investigation shows the oil company understood the science before it became a public issue and spent millions to promote misinformation”https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;exxon-knew-about-...As for the market argument, it is costly because all historical capacity is in fossil fuel based systems. This is a major cost advantage to fossil fuel based solutions. Waiting for the market to adjust might be too little to late, climate change is path dependent, you can’t simply undo it. reply esarbe 18 hours agorootparentprev> I posit that any deliberate misinformation by the oil and gas lobby can and will be equally matchedThe economic and political power of the oil and gas lobby is magnitudes larger than the economic and political power of the wind and solar lobby. We&#x27;re talking about entrenched interests here.Also, wind and solar didn&#x27;t spent more than fifty years spreading misinformation and denialism about an existential threat to our global human civilization, framing and defaming credible scientists.If there&#x27;s ever an existential threat to our global human civilization through wind and solar and the wind and solar lobby is found to spread misinformation, framing and defaming credible scientists, I&#x27;m willing to revisit the topic.Until then I&#x27;m not playing this game \"both-sideism\". There&#x27;s a real and actual culprit here.> If there are \"better and cheaper\" alternatives available right now, market forces will assure their dominance.That&#x27;s the \"free market\" fallacy. But we don&#x27;t have a free market. We have have trillions of yearly subsidies for oil and gas[0], we&#x27;ve had it for decades. We spent trillions building up oil and gas infrastructure. To expect wind and solar to compete on such an uneven playing field where even the rules of the game are written by the fossil fuel lobby is ludicrous[1].Oh, by the way. Power generation through wind and solar has been lower then all and any fossil fuels, for years[2]. So much for \"the market will solve it\".[0] https:&#x2F;&#x2F;www.imf.org&#x2F;en&#x2F;Blogs&#x2F;Articles&#x2F;2023&#x2F;08&#x2F;24&#x2F;fossil-fuel...[1] https:&#x2F;&#x2F;www.americanprogress.org&#x2F;article&#x2F;oil-lobbyists-use-r...[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Levelized_cost_of_electricity reply holbrad 12 hours agorootparentWhen discussing market forces and renewables keep in mind, Levelized cost of electricity is a largely useless metric because it doesn&#x27;t take into account the prohibitively high costs of storage and redundant infrastructure needed for intermittent sources. reply gottorf 12 hours agorootparentprevBefore you read my detailed response below, my tl;dr point is not that wind and solar suck, or that we need to hold on to oil and gas forever. My most salient point is that proclaiming an \"emergency\" of an \"existential threat\" that requires centralized authority (held by decisionmakers who usually pay no cost for being wrong) is a playbook that&#x27;s as old as history, and one that almost always resulted in great human toll.I predict that more human misery will be generated by efforts to counter climate change that are predicated upon such centralized authority, than that generated by the status quo. Having said that:> The economic and political power of the oil and gas lobby is magnitudes larger than the economic and political power of the wind and solar lobby. We&#x27;re talking about entrenched interests here.No arguments there from me, except I would like to add that the degree of entrenchment reflects how much modern society depends on oil and gas.> If there&#x27;s ever an existential threat to our global human civilization through wind and solarYou&#x27;re presupposing that oil and gas is an existential threat to global human civilization. Evidence points the other way around, that humanity flourished more than ever with the usage of fossil fuels.> We have have trillions of yearly subsidies for oil and gas[0], we&#x27;ve had it for decades.$7T of subsidies around the world according to your source (including \"implicit\" subsidies; direct subsidies of $1.3T), out of a global GDP of ~$96T; a subsidy of around 7.3% for an energy source that comprises over 80% of the world&#x27;s consumption[0]. I&#x27;d say that sounds like a good deal. I&#x27;m sure if and when wind and solar get to that level, they will have similarly vast subsidies behind them.> where even the rules of the game are written by the fossil fuel lobby is ludicrousI&#x27;m not sure if I understand the point of your source for this. It says that availing yourself of the legal system is somehow wrong, and accuses O&G of lobbying \"with the goal of putting their profits and pay above all else\" (some real objective stuff there).Big Pharma spends over 3x the amount of money lobbying every year, and depending on the year there&#x27;s 3-7 other industries that spend more than oil and gas. The largest producer of wind and solar energy in the country, NextEra Energy, spent $6.2M in lobbying in 2021, about the same as ExxonMobil or Chevron that year. Nobody likes lobbying, but it appears that everyone does it. It&#x27;s not especially bad only when it comes from the O&G industry.Out of S&P 500 companies, CEO pay for the energy industry is about the middle of the pack[1]. The \"green & renewable energy\" industry has a higher EBITDA to sales as well as a higher SG&A expense to sales ratios than either the \"coal & related energy\" industry or the various types of \"oil&#x2F;gas\" industries[2]. So if anyone&#x27;s putting \"profit and pay above all else\", it would appear it is the renewable energy execs.> Power generation through wind and solar has been lower then all and any fossil fuels, for yearsLCOE is useless without storage costs. LCOE with storage costs included",
    "originSummary": [
      "September 2023 marked the hottest month on record, featuring a temperature anomaly 0.5 degrees Celsius higher than the previous record.",
      "The year 2023 may become the first-ever to exceed 1.5 degrees Celsius above preindustrial temperatures, driven by escalating greenhouse gas emissions from burning fossil fuels and an El Niño event.",
      "These record-breaking temperatures underscore the immediate necessity for action to curb global warming."
    ],
    "commentSummary": [
      "The conversation covers various aspects of climate change, such as its causes, effects, scientific understanding limitations, government's role, economic implications, and the imperative need for alternative energy sources.",
      "The discourse also delves into the societal impact, with discussions about behavior and diet choices, plastic production's effect, the need for government intervention, and different responses to the crisis.",
      "The diverse views and perspectives on each topic have ignited debates and disagreements, underscoring the complexity of the climate crisis and the difficulty in finding effective solutions."
    ],
    "points": 245,
    "commentCount": 380,
    "retryCount": 0,
    "time": 1696501671
  },
  {
    "id": 37781022,
    "title": "Booking.com makes a fortune – so why is it leaving its bills to hotels unpaid?",
    "originLink": "https://www.theguardian.com/business/2023/oct/03/bookingcom-makes-a-fortune-so-why-is-it-leaving-its-bills-to-small-hotels-unpaid",
    "originBody": "Skip to main content Skip to navigation Skip to navigation Print subscriptions Sign in Search jobs Search UK edition The Guardian - Back to home Support the Guardian Fund independent journalism with £5 per month Support us News Opinion Sport Culture Lifestyle Show More Business Economics Banking Money Markets Project Syndicate B2B Retail It’s likely that hundreds or thousands of Booking.com partners have been affected by a payments failure, but we cannot know for sure, writes Lara Dunston. Photograph: Sheldon Cooper/SOPA Images/Shutterstock Travel & leisure Booking.com makes a fortune – so why is it leaving its bills to small hotels unpaid? Lara Dunston This is a global company that needs to investigate why its systems have failed so many small businesses Follow our Australia news live blog for latest updates Get our morning and afternoon news emails, free app or daily news podcast Tue 3 Oct 2023 15.00 BST There is something wrong at Booking.com. While its parent company has been making record profits – $1.3bn for the second quarter of 2023 alone – and its CEO and executives have been cashing in millions of dollars in shares this year, many of the people who helped Booking.com make that fortune claim to have not been paid since July, some say longer. Many Booking.com accommodation partners from Australia to Europe to the Americas have faced silence from the company as they’ve chased payments for months while struggling to keep businesses afloat – particularly during the lucrative northern hemisphere peak summer season, when many guests were essentially staying free of charge, as Booking.com hadn’t passed on payments. You don’t need me to tell you the full impact of this. You only have to read the comments posted on social media and websites referencing the holiday booking site. Travel website Booking.com leaves hoteliers thousands of dollars out of pocket Read more One of the most distressing stories is beneath a post about Booking.com’s investment in AI and cutting-edge technology by the company’s highly remunerated CEO, Glenn Fogel, on his LinkedIn page. Many partners have complained directly to the CEO about Booking.com’s silence on late payments and staff’s lack of response to calls, emails and cases opened on the platform that are closed without being resolved. “I am a widow raising a child on my own,” reads one post by a woman I have since confirmed is called Claudia Arnull. “Because l have not been paid properly since the end of June my credit history is ruined and all my savings have gone and l am now in so much debt there is nothing left to use. As of this moment all l have to feed my child is porridge.” The post appears genuine – and is genuinely heartbreaking – and you would hope that it will be investigated by the company. After all, nobody knows exactly how many families have been left in similar positions, with stress over credit histories and depleted savings due to Booking.com’s inability to pay partners. We know that it’s likely that hundreds or thousands of partners have been affected. It could be many many more. We can’t know for sure, because despite dozens of emails to Booking.com’s media relations and Booking Holding’s communications departments, questions as to how many partners were affected, were never answered. The only Booking.com spokesperson to respond to me, from their UK press office, emailed a standard reply. The official response blames payment issues on a technical glitch resulting from a planned two-week July maintenance intended to secure and enhance the travel reservations platform. A similar statement was given to the Guardian this week in which it said it understood “the frustration of the accommodation hosts and owners” and that the “system errors that affected the payments have now been corrected”. A Booking.com market manager in south-east Asia admitted at a recent industry event that payment delays were caused by the installation of a new payment system. Staff salaries were also affected, she said, explaining it as a risk they had to take. It’s hard for partners to get excited about new initiatives when Booking.com hasn’t paid them in months, which means they can’t pay staff or utilities, get laundry done, buy ingredients for guest breakfasts or pay loans or investors and have to borrow from family and friends. I’ve heard scores of heartbreaking stories in Facebook groups and online forums in recent months as I’ve connected with other accommodation partners and affiliate partners like myself to share experiences and tips to getting paid. My own three-month-long journey in chasing payments only ended last week after a partner shared direct emails for Booking.com finance department heads. I was paid three days later without an explanation or an apology. Booking.com has 90 million accommodation partners with lodgings listed on their platform. It’s worth noting that it’s not the big brand hotels or luxury resorts that have been affected – they take payments from guests and send commissions to Booking.com. It appears to be couples, families and individuals; the independent operators of smaller lodgings that might not have a credit card facility to accept payments, such as apartment rentals, cottages, villas, bed and breakfasts and rooms in homes. A British owner of one such property says she is owed £2,500 (A$4,759). Fake or not fake? Booking.com’s hotel or apartment was ‘a private house’ Read more “What I cannot understand is why there has been zero public apology from the CEO of this organisation,” she told me. “Surely damage limitation and PR is important to them? Why no proper explanation as to why this continues to not be resolved? Why aren’t we getting an apology from the board?” One partner, who rents out an apartment in the Canary Islands, described finally making headway in her months-long fight to get paid. On Sunday, she says, she received notice that June-September payments of €13,000 (A$21,458) would be paid in November. But why should she wait another month when Booking’s finance team can make manual payments in a few days? Booking.com partners aren’t alone in their battle for payments, responses and explanations. On consumer site TrustPilot, Booking.com has a “bad” one-star rating (out of five stars) from 48,872 reviewers. They complain about appalling customer service, staff who don’t appear to read emails or follow up and refunds promised but allegedly not paid. Of course, none of these complaints are new – it’s just these problems have never appeared to have been as widespread. It’s time that Booking.com directed some of those billions in profits and so-called cutting-edge technology to fixing the long-term payment problems and communication issues. Glenn Fogel needs to explain what’s been going on and he needs to apologise. Regulators around the globe need to hold the company to account, to immediately put pressure on the company to pay up now and pay on time and legislate for greater transparency and accountability. Booking.com shouldn’t be able to continue to reap wealth from partners struggling to keep their accommodation open because the world’s No 1 travel site hasn’t paid them their earnings. Lara Dunston is a travel writer and a Booking.com affiliate Explore more on these topics Travel & leisure Internet comment Reuse this content Most viewed Business Economics Banking Money Markets Project Syndicate B2B Retail News Opinion Sport Culture Lifestyle Original reporting and incisive analysis, direct from the Guardian every morning Sign up for our email About us Help Complaints & corrections SecureDrop Work for us Privacy policy Cookie policy Terms & conditions Contact us All topics All writers Modern Slavery Act Digital newspaper archive Facebook YouTube Instagram LinkedIn Twitter Newsletters Advertise with us Guardian Labs Search jobs Patrons Back to top © 2023 Guardian News & Media Limited or its affiliated companies. All rights reserved. (modern)",
    "commentLink": "https://news.ycombinator.com/item?id=37781022",
    "commentBody": "Booking.com makes a fortune – so why is it leaving its bills to hotels unpaid?Hacker NewspastloginBooking.com makes a fortune – so why is it leaving its bills to hotels unpaid? (theguardian.com) 230 points by ksec 17 hours ago| hidepastfavorite214 comments jimt1234 15 hours agoThis reminds me of a job I once had that was about 50% travel; the company required employees to charge travel expenses to their personal credit cards, saying they would be reimbursed. The company consistently dragged its feet on the reimbursement, sometimes for several months. They scrutinized reported expenses to an absurd level, or simply refused to pay. I once had a layover on a flight (because the company required the least expensive flight, which was generally not direct) that was cancelled. I was stuck in Denver for 9 hours, waiting for an available flight. I got something to eat and reported it as a travel expense. Denied; I was told the expense didn&#x27;t occur in the city where I was working, so it was personal, not business-related. A colleague had her credit card maxed with business travel charges, and could only afford to make the minimum payment for several months. She requested the company pay the interest on the carried charges. Take a guess how that went? Denied. She ended up quitting, and last I heard the company refused to reimburse her because she was no longer an employee. I ended up quitting, too. But yeah, this article reminded me of that situation. When it&#x27;s known that you&#x27;re basically powerless to collect on a debt, you can expect to get screwed. reply tpmx 15 hours agoparentI suppose many of us are lucky to work in high-margin software businesses. I spent 15 years often travelling for work (typically very distributed companies) before the pandemic. Four separate companies. Everything paid on my personal CC. I preferred it that way so that I could optimize the flights&#x2F;etc to make it fit my personal life. Everything reimbursed swiftly with zero questions since I never did anything really stupid. The only real bottleneck was myself; filing all of those damned receipts for every trip.Having some kind of centralized travel booking service that \"took care of everything\" would likely have caused me to find another job. For one company I did a monthly 6h travel + a few days of hotel + 6h travel thing for a decade. Not being in total control over that would simply not have been feasible.I have no idea how people doing a large amount of travel for stereotypical low-margin large companies manage to get by. reply rrrrrrrrrrrryan 13 hours agorootparent> Having some kind of centralized travel booking service that \"took care of everything\" would likely have caused me to find another jobHaving worked at both kinds of companies, I vastly preferred having a travel agent that took care of everything.For example: one time I had a later flight with a layover. The initial flight was delayed on the tarmac before takeoff, and immediately upon landing, a person called me telling me there were no more flights that evening to my final destination, but they&#x27;d already booked me a hotel near the airport and the first flight out in the morning to my final destination, and asked me if I&#x27;d like any modifications. Perhaps someone else who wants more control would like to handle that stuff themselves, but for me, it was a massive luxury to have an experienced professional automatically smooth over any problem I encountered during my travel. I certainly didn&#x27;t miss keeping all my crumpled receipts and submitting expense reports either. reply tpmx 13 hours agorootparentYeah, that kind of service would have been awesome. reply TeMPOraL 8 hours agorootparentA great example of self-service actually making things worse for people. There are many, many more of those, and it leads me to believe that software&#x27;s transformation of the economy isn&#x27;t as positive on the net as people seem to believe. reply justinclift 14 hours agorootparentprev> high-margin software businesses.I&#x27;ve heard similar stories to this from people working at high-margin software businesses before too.It could be something along the lines of \"depends which team you&#x27;re in\", as to who&#x27;s being screwed over or not. reply tpmx 14 hours agorootparentThat&#x27;s a great sign your company actually sucks and you should quit. Probably likely: It wasn&#x27;t actually a high-margin company. reply akamia 14 hours agoparentprevThis is the reason why I refuse to pay for any work travel expenses with my credit card.I once worked for a company that had a similar travel process. When I was asked to travel, I insisted that my VP have his assistant schedule and pay for everything. When I arrived at the hotel on one trip, the hotel asked me for a credit card to pay for the booking. I stepped out of line and called my VP to have him fax his card info to the hotel.I understand this wouldn&#x27;t work for everyone but I think it&#x27;s ok to question the policy and see how much flexibility there is. In my case, I was able to avoid taking on personal liability for my employer&#x27;s expenses by pushing back and working with my VP to find a solution that I was comfortable with. reply jiqiren 14 hours agoparentprevif this is US, file a wage claim with local state government. Reimbursement for expenses while traveling is valid.Depending on the state you can file many years later after you&#x27;ve left the job. So feel free to get a new job and file a claim with a spreadsheet of what they missed. reply justinclift 14 hours agoparentprevThe \"Denied\" are probably only \"Denied\" because you didn&#x27;t want to push back hard.You could always take them to court (even small claims court) or just outright get debt collectors involved.Probably better once you&#x27;ve quit though. ;) reply em-bee 12 hours agorootparentwhy wait until after you quit? before you quit you get to use your worktime to deal with the lawsuit (just because) and if they harass you at work for suing them, you add that to the lawsuit. if they fire you, add that too. reply mbar84 3 hours agoparentprevI&#x27;m trying to think how I would ideally set up such a system. The person traveling is in the best position to determine the most cost effective way to travel. Ideally you could somehow determine or negotiate ahead of time what travel expenses should be and then just pay that to the employee. Then they would just travel as they like and either pocket any savings they can find, fly cheaper if they value their time less than the saved money or pay extra if they value flying first class.Good incentive structures are hard. reply Doxin 3 hours agorootparentI&#x27;d set up some base rules. Semi-obvious stuff like no first class tickets or caviar dinners. And then I&#x27;d trust my employees to be adults.Putting your trust in people is a great incentive structure. reply gamblor956 14 hours agoparentprevWhile it&#x27;s legal for employers to require employees to use personal credit cards for work-related expenses, it&#x27;s a huge labor code violation not to timely reimburse employees for such expenses.As in, huge mandatory fines on top of reimbursing employees for 100% of their costs (including interest and other charges incurred related to late reimbursement) type of violation.If you&#x27;re not willing to name and shame here, please report it to the state&#x27;s Dept of Labor if this was within the past 5 years so current employees can be protected. reply throwaway290 15 hours agoparentprevWhy not name and shame? reply hoppla 16 hours agoprev«A Booking.com market manager in south-east Asia admitted at a recent industry event that payment delays were caused by the installation of a new payment system. Staff salaries were also affected, she said, explaining it as a risk they had to take.»It seems like there is a misunderstanding of who is on the taking and receiving end of risks reply ratg13 14 hours agoparentSad to see the top comments in the thread not even bothering to discuss the technical reasons and legal implications behind this .. and instead everyone wants to just discuss unrelated anecdotes about their personal travels.The story here is about how booking.com is screwing over hotels and hostels, not guests. Someone should create a different thread for that, because apparently people have a very strong need to tell their tangentially related stories. reply somsak2 4 hours agorootparentYou&#x27;ve seen how discussions work in real life right? There&#x27;s no requirement to stick to some topic at hand, it&#x27;s a free-flowing dialogue. That&#x27;s what you&#x27;re seeing happen in this discussion board too. reply 3seashells 4 hours agorootparentprevWage and expense theft by companies is not just anecdata. reply gnu8 15 hours agoparentprevReal companies have competent adults build and test this type of thing so they don’t stiff their vendors or miss payroll. It’s not an area where you should let techbros move fast and break things. reply Aaargh20318 15 hours agorootparentIt’s booking.com, they are notorious for not being competent adults. Just check out this article about their working conditions [1] (you might want to google translate it you happen to not speak Dutch).1 - https:&#x2F;&#x2F;www.computable.nl&#x2F;artikel&#x2F;nieuws&#x2F;development&#x2F;6777910... ( reply diggan 15 hours agorootparent> the developers complain about Perl programming language, which forms the basis of the online platform and to which, according to them, is held far too rigidly. Criticism of Perl may even result in dismissal, according to an anonymous source.If this machine translation is correct and the statement true, then this is borderline hilarious. I know that some companies keep their languages dearly, but that&#x27;s taking it to the next level. reply MakeThemMoney 9 hours agorootparent> Criticism of Perl may even result in dismissal, according to an anonymous sourceThat statement is absurd. You can&#x27;t dismiss an employee in the Netherlands for such a stupid reason. I used to work in their HQ as a developer, and complaining about Perl was always water cooler talk. reply laurent_du 14 hours agorootparentprevAfter duckduckgo, this is the second time in a few days where I hear about some relatively big company using Perl for their backend. reply klyrs 6 hours agorootparentPerl used to be a really popular language for cgi scripts. So much easier than writing them in C! I can&#x27;t describe how amazing PHP was at the time of its inception. While I have trouble picturing somebody starting a big website in perl today, it&#x27;s certainly plausible that momentum prevents a switch pretty much indefinitely. reply colejohnson66 11 hours agorootparentprevAny hosting provider running cPanel is running on Perl reply8jef 16 hours agoprevI wish someone from the accommodation industry would tell us all how the central booking system and most hotel chains are abusing everyone, from inn and hotel owners to end customers. Their contracts are pure gold, in the sense that they cloak all possibility of independence under a thick cover nothing can pierce through. Tourism, they are killing it, literally. reply jwhitlark 15 hours agoparentThere are multiple GDSs, and AFAIK neither Expedia nor Booking.com use them. The space is way more confused and competitive then your comment implies. The best writeup I&#x27;ve seen on the space is https:&#x2F;&#x2F;www.altexsoft.com&#x2F;blog&#x2F;hotel-api&#x2F;, if you really want to dive into the details.Edit: been typing markdown all day, oops. reply gvedem 16 hours agoparentprevWhat do you mean by \"central booking system\"? As far as I know (working on a rental car contract for a couple years), there are just various travel industry standards for bookings and whatnot. reply CharlieDigital 16 hours agorootparentIt&#x27;s called the GDS: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Global_distribution_systemIt started with airlines, but eventually included everything from hotels to rental cars.The space is more or less dominated by a few big players. Sabre. Amadeus. Travelport.GDS is the backbone of all of these booking platforms as far as I&#x27;m aware. reply ramses0 16 hours agorootparentprevGDS. A Global Distribution System is a worldwide reservation system that acts as a conduit between travel bookers and suppliers, such as hotels, other accommodation providers and other travel related services. reply 8jef 16 hours agorootparentprevI mean there is only one pipeline, to which are contributing many actors, like big agencies (booking, expedia), car rental, airlines, etc. Together, they act as one. reply kbenson 16 hours agoprevI&#x27;ve taken to calling hotels directly and dealing with them, or their chain&#x27;s booking, as they offer the same or better rates usually, are more accountable, are more willing to refund or cancel without problems, and are much more interested in keeping you happy and wanting to come back. reply dcdc123 14 hours agoparentI am a full time digital nomad and stay in a ton of hotels. I have once gotten the same rate directly, usually it is 20-30% higher. Hotels basically force you to third parties. reply jononomo 10 hours agorootparentWhy would they do this? reply Macha 10 hours agorootparentDo they think that direct callers are more invested in their specific hotels while price comparison site users will just pick someone else if the price is too high? reply lmz 8 hours agorootparentOr the people handling bookings at the hotel have no discounting authority and don&#x27;t care where you&#x27;re booking from as it&#x27;s not their problem anyway. \"If you say it&#x27;s cheaper on the Internet go book there!\" reply nojvek 7 hours agorootparentCorrect. I always find cheaper prices on Expedia than I do at Marriott’s website or their counter.But Hyatt is almost always cheaper directly. Hyatt is my fav chain, they maintain a proper standard of service for their price. I’m a believer. reply staticautomatic 4 hours agorootparentTheir points are still worth something, too. replyavgDev 15 hours agoparentprevNot always the case.In Dominican Republic I was told to just use an online booking site as it was much cheaper than doing it in person. Their own site was also more expensive than expedia. reply throwaway290 15 hours agorootparentSimilar in Israel and Thailand, they upfront say it&#x27;d be more expensive to book at reception. I wonder if it&#x27;s only different in US. reply marenkay 3 hours agorootparentIf you speak Thai and do it via LINE messenger you get a much cheaper price though. Pricing seems to be inflated for foreigners. reply throwaway290 3 hours agorootparentOn flip side, I bet if you are Thai native speaker foreigner in the US you will also not necessarily find it easier&#x2F;cheaper to negotiate with hotels directly compared to Booking etc. reply wordpad25 15 hours agoparentprevHotels always offer worse rates offline than online reply kbenson 15 hours agorootparentNot my experience, especially when you actually break down the cancellation policies (which booking aggregators will sometimes charge extra for) and whether they require payment up front or at the time of arrival.When I last did this, for a vacation, the cost at an aggregator was less if there was no cancellation allowed, and more if the option to cancel was added (but to a couple days before I believe), and needed to be paid at time of booking. When I called, I got a rate slightly less than the online+cancellation, I was able to cancel up to 24 hours in advance, and I only needed to pay on arrival, meaning I didn&#x27;t tie up a few thousand for a few months.There&#x27;s also always the option to state the rate you found online and see if they are willing to match it. It&#x27;s not like there usually some magic involved, if they&#x27;re willing to offer it to an aggregator, why wouldn&#x27;t they at least consider offering it to you direct? reply lepus 12 hours agorootparent> It&#x27;s not like there usually some magic involved, if they&#x27;re willing to offer it to an aggregator, why wouldn&#x27;t they at least consider offering it to you direct?The magic is actually being given the direct authorization to give you a matching rate or if not then having to get management to authorize that rate for you. That depends entirely on the hotel. Some employees might also recognize a deal seeker as someone who&#x27;s also more likely to be picky and complain more and so would rather deny your price negotiations and avoid working with you if they have the choice depending on how filled they already are. reply davelondon 15 hours agoprevTheir support is pretty bad.. they refused to refund a £20 booking when the hotel was 500km away from where it was shown on the map. There were no other contact options, so I took them to the UK Small Claims Court... They paid up before the case went to court. reply justinclift 14 hours agoprevBearing in mind that booking.com famously offshored most of their customer support staff last year:https:&#x2F;&#x2F;skift.com&#x2F;2022&#x2F;02&#x2F;11&#x2F;booking-com-to-eliminate-2700-c...Seems like that&#x27;s going as well as everyone except their executive team predicted. ;) reply JonChesterfield 1 hour agoparentMy last hotel booking had offshored their entire customer support system to a 90s style chatbot. Walk the state machine in circles kind of thing.Booking had a real person pick up the phone, know stuff, and fix it. Not a native English speaker but still far more helpful than the software alternative. reply skilled 15 hours agoprevHmm, this doesn&#x27;t make any sense whatsoever,> A Booking.com market manager in south-east Asia admitted at a recent industry event that payment delays were caused by the installation of a new payment system. Staff salaries were also affected, she said, explaining it as a risk they had to take.What do you mean a new payment system? How do you not create a fallback for this situation when you&#x27;re dealing with hundreds of millions of dollars in transactions?I mean I am willing to give Booking.com the benefit of the doubt here in case that is actually the root cause, but it doesn&#x27;t compute for me. You don&#x27;t just throw thousands of people (who drive business for you in the first place) under the bus by not properly preparing for a software rollout.You still have the old system or whatever it is that you were using, how hard would it have been to detect an issue if they did a A&#x2F;B or a gradual rollout at first.As a Booking.com user myself - I am pissed that this is a thing. reply JonChesterfield 1 hour agoparentStart from the axiom that computers don&#x27;t make mistakes. Then change to a new computer system at lower cost, because it&#x27;s cheaper and computers don&#x27;t make mistakes. So cheaper is better. Don&#x27;t need a roll back plan, all computer systems work fine.Then put on your best surprised expression and wait to see if you manage to unwind the errors before a competitor exploits them.It is fascinating how much faith people have in computers, even after at least a decade of watching them fall over. Sure it crashes a lot but it couldn&#x27;t get the answer wrong. reply croes 16 hours agoprevMaybe they make a fortune just because they leave the bills unpaid. reply em-bee 12 hours agoparentthat was my first thought too ;-) reply dreamcompiler 15 hours agoprevI had a few horrible experiences with Booking.com and the others where I&#x27;d arrive at the hotel and they&#x27;d never heard of me, or I&#x27;d ask for an extra day in person but the hotel couldn&#x27;t help me because I didn&#x27;t book directly with them, or they didn&#x27;t serve breakfast but Booking said they did and the hotel was in the middle of nowhere etc.I finally wised up. Hotels basically hate guests who book with a service like Booking.com and they will not lift a finger to help you if anything goes wrong.Now I use Booking.com, Kayak, etc to find hotels in the general area I need, then use google or DDG to find the website of the hotel (a process much easier said than done because the booking sites SEO the hell out of search engine results), then book directly with the hotel or its parent chain. reply wink 1 hour agoparentI guess this will never satisfy everyone, because my experience is the opposite. I&#x27;ve been using booking.com for about 10 years and the only things that kinda went wrong were when I didn&#x27;t. So yeah, I&#x27;ve not had to test their failure procedure yet, but at least I&#x27;ve never experienced anything I booked not being available. (I had one hotel take my card payment on arrival and it was also done via booking but I blame the weird hotel here, booking refunded without problems). reply Nextgrid 16 hours agoprevAs a general rule, the reason a company does something nefarious is that it knows it can get away with it.We need to fix the broken & corrupt legal systems so that companies have not just equivalent penalties like individuals would (the equivalent of jail would be that all profits go to the state for the duration of the \"jail\" term), but be given even less leniency&#x2F;benefit of the doubt since massive companies have way more resources to investigate & comply with laws than individuals do. reply lefstathiou 15 hours agoparentVery simple solution: the US should implement prevailing parties clause (prevailing party wins legal fees by default in a lawsuit) as law of the land by default.We had a guy in Australia trying to use our trademark, in the US they would call my bluff and I would have to spend tens of thousands to hundreds of thousands to sue. In Australia we sent a polite letter, outlined our case that was pretty clear, reminded him he would pay our legal if we escalated and made it clear we would, and the result was immediate.In short, companies shouldn’t be incentivized to play game theory on whether or not you will sue me if I trample your rights. reply MSFT_Edging 15 hours agorootparentThat sounds like an amazing way to empower corporate legal bullying and copyright trolls, and ensure that no single entity could ever take a company to court ever again. reply V__ 15 hours agorootparentHow so? reply msandford 14 hours agorootparentBig company threatens to spend $100 million on the lawsuit to make sure they win. By outspending you they exhaust your funds and win by default. Then you go doubly bankrupt when you have to pay their legal bills. reply lefstathiou 14 hours agorootparentThat’s not how the calculus works actually. Today a corporate bully can safely count on you being a rational actor (and not spend $1mm to win $400k) and push the envelop of competition and appropriation of data and trade secrets with limited recourse. Under a prevailing party construct, the risk of doing so is enormous.Thus, in instances where they are objectively wrong, they lose big. In instances you are objectively wrong, you lose big. In instances where it is not black and white, where there is merit to both sides of the argument, you settle. All this is achieved with little intervention by the courts. reply alasdair_ 14 hours agorootparentprevIn places where \"loser\" pays (such as the UK), rates are capped at fairly low hourly levels and hours are also capped (or at least there is a known \"reasonable\" number of hours for a particular type of case).Note also that the \"loser\" factors in offers to settle. So if I offer you $10,000 to settle and you refuse and the court awards only $5,000 to you, you are the one that has to pay my legal bill, even though I am the one that \"lost\". replykbenson 16 hours agoparentprev> the equivalent of jail would be that all profits go to the state for the duration of the \"jail\" termFor punishing corporations (for whatever legal reason) I would rather see a percentage of c-suite compensation for a time period (including end of year bonuses) confiscated, maybe along with a fine based on profit (so stock values are hit as well, so that both can&#x27;t be used to avoid penalties and also to signal to the market it&#x27;s a bad idea).The problem with fining a company is that often you hurt a lot of the lower ranked employees that had no power and often no knowledge of the problems, and usually weren&#x27;t involved in any way, sometimes to the point where they lose their jobs if the company is hurting to a much larger degree than the people running the company do, which is why punishing corporations is sometimes hard.We need to incentivize and punish the people that actually decide the problematic policies and actions and not those that just happen to be in proximity and&#x2F;or are linked financially, to the degree that we can (obviously it&#x27;s hard and no perfect solution exists). reply yoyohello13 11 hours agorootparentI&#x27;ll do you one better. I think the C-suite should just go to jail, real jail. That will really incentivize the leadership to play by the rules. reply diogenes4 16 hours agorootparentprevI imagine they&#x27;d be a lot more responsive to prison time, personally. A fine doesn&#x27;t seem to capture the correct incentives here. reply kbenson 9 hours agorootparentJust because you have no sympathy for the large corporations doesn&#x27;t mean they should be treated unfairly, and going to jail because of actions that actually weren&#x27;t in your control is definitely unfair.Corporations are made of people, and can by large multinationals, or small mom and pop operations with a few tens of people (or less). Should someone that has a company that runs a few ice cream shops in town go to jail because of actions of people under them? Because it&#x27;s not like it&#x27;s always going to be something that was condones or expected from management, and while I&#x27;m okay with monetary damages that don&#x27;t aim to dismantle the company entirely even if management didn&#x27;t really know what&#x27;s going on (that&#x27;s maybe a good way to make them care and pay attention), I&#x27;m not okay with sending them to jail because of mismanagement.Not only is that unfair, it would have a huge chilling factor on entrepreneurs. And not just the startup kind, the people that want to open a restaurant or run a small business.And to fully put this in context, this is under an article about booking.com paying slowly. You think jail time is appropriate for that? What if they&#x27;re having cashflow problems? If they aren&#x27;t and it&#x27;s intentional, how do you distinguish this from that? reply plagiarist 16 hours agorootparentprevHear, hear. And more corporate lawyers should be getting disbarred than the current status quo. reply NoMoreNicksLeft 16 hours agorootparentprevWhy do you think it&#x27;s any of the employees that are at fault, even the c-levels?Fine the investors. Everyone who owned a share when the violation occurred takes a lick.If you go after c-levels, what will happen is they&#x27;ll devise some sort of time deferred compensation avoids the penalties you want to impose. The board needs a CEO after all, and CEOs are reluctant to run a company if they know they&#x27;ll just have their paycheck confiscated. reply Analemma_ 15 hours agorootparentI mean that&#x27;s effectively already how it works: a fine to the company is a fine to the investors, because they own the company (more precisely, a fine represents money which is not getting paid back). And clearly that isn&#x27;t enough, because this stuff keeps happening, that&#x27;s why the GP and others are brainstorming alternatives. reply MakeThemMoney 15 hours agorootparentprevWhy not both? Fine the company (i.e. investors), jail the executives. reply CM30 14 hours agoparentprevNot even just compared to individuals, but even to smaller competitors. If a smaller company acted like Booking did here, they&#x27;d be sued into oblivion or out of business in no time. Same with a shop that sold counterfeit goods as casually as Amazon or an app store seems to, or one that fired people as suddenly as Twitter&#x2F;X did those months back.The problem is that laws are basically just &#x27;suggestions&#x27; if you&#x27;re rich&#x2F;your company is rich, and life altering consequences if you&#x2F;your company is not.There should actually be jail time for corporate leaders that break the law, and the potential of being forcefully removed from office or the company itself forced to shut down if poor behaviour continues. reply SoftTalker 16 hours agoparentprevPeople don&#x27;t go to jail for not paying their bills.It&#x27;s a case of \"if I owe a hotel $200, that&#x27;s my problem, if Booking.com owes a hotel $200,000 that&#x27;s the hotel&#x27;s problem\" reply gopher_space 16 hours agorootparentIf you can’t make rent the police get involved at the end, but society wouldn’t really think of you as a person in that scenario. reply 83457 16 hours agorootparentprevUnless that bill is to the government. reply SoftTalker 16 hours agorootparentUnless fraud is involved, no. They might garnish your wages, seize assets, etc. but you won&#x27;t go to jail. reply ben_w 16 hours agorootparentprevIt has to be a lot more than 200k, but the principle still applies.If you owe some government 500 billion, you are probably also a government. reply jimt1234 16 hours agorootparentprevI think it&#x27;s even more true in this situation. If you or I owe the IRS $2000, they&#x27;re gonna come after us and get all their money. But if EvilCorp owes the IRS $200-million, the lawyers will argue about it for 3 years, and in the end, EvilCorp will end up paying less than half of the original amount. reply maximinus_thrax 16 hours agorootparentprevor to the ex spouse reply mouse_ 16 hours agoparentprev\"We need to negotiate a better living arrangement with our evil vampire overlords\"I&#x27;m not convinced reply mschuster91 16 hours agorootparentOh, the EU shows that even the biggest corporations can be brought to knee if they pull off too much bullshit. Apple had to introduce USB-C and will have to open up their App Store and NFC interface, Meta got slapped with a billion dollar GDPR fine, Microsoft to this day has to sell European versions of Windows that don&#x27;t come with a media player or that allow everyone to choose their default browser... reply dantheman 16 hours agorootparentThe EU is just shaking down foreign companies because they can&#x27;t compete. reply vertis 16 hours agorootparentThe EU is protecting it&#x27;s citizens. You don&#x27;t have to spend very much time comparing the EU (and Europe in general) to the US to find differences in philosophy.US has a very individual centric attitude where everyone is an temporarily inconvenienced millionare and Europe has a much more collective attitude. I will take the European attitude over the US one any day of the week (and I&#x27;m from neither, I&#x27;m Antipodean).Every billionare is a policy failure. reply dotandgtfo 15 hours agorootparentNot to mention that Booking.com is a European company that has explicitly been targeted by the upcoming digital markets act. reply dantheman 10 hours agorootparentBooking.com got bought by priceline. reply isoprophlex 15 hours agorootparentprevBut look at what \"to compete\" means in the European point of view, look at how they see the practices that you describe as \"being competitive\": nefarious, unethical practices to fuel growth for the hell of it. If they can&#x27;t compete on some short-term, enshittification-causing metric, fine by me. reply arp242 13 hours agorootparentprevAside from the fact that booking.com is a Dutch company (although Booking Holding is US, so who the hell knows any more as I have no idea how that all fits together...), no one is forcing anyone to do business in Europe. If you don&#x27;t like the rules then don&#x27;t play; that&#x27;s fine, no hard feelings.That&#x27;s how it works everywhere, including in the US.What you DON&#x27;T get to do is set the rules for OUR countries. Several of these companies have been under the misapprehension that&#x27;s how things work, and they are being (slowly) cured of that delusion (far too late, but better late than never...) reply _jal 15 hours agorootparentprevYou can phrase it in whatever way you prefer, but GP&#x27;s point remains - a fine measured as a percentage of global revenue has been demonstrated to modify huge corporations&#x27; behavior.FWIW, I think the EU&#x27;s recent legal behavior is only about 1&#x2F;3 foreign-company envy. There&#x27;s a lot of misguided do-gooderism and a large dash of control-freakery, too, not to mention intelligence and LEO wishlists. reply olddustytrail 15 hours agorootparentprevAww, is that what you tell yourself? reply orwin 16 hours agorootparentprevThe EU has actually enforce anti-monopolistic laws, even if it&#x27;s quite slow. You should compare history of the food industry in Europe and in North America, I think this show well how much the US politicians changed over the last 50 years. reply babypuncher 15 hours agoparentprevthe equivalent of jail time should be actual jail time.determine which members of the executive team decided it was OK to literally steal from people and put them in jail. reply DrNosferatu 16 hours agoprevCall the booking.com property and negotiate a better price directly with the host. They offer virtually no protection anyway if a booking goes bad.Guests and hosts both win if you cut the booking.com middleman. reply totallywrong 13 hours agoprevI used it a whole lot when I was doing the nomad thing but avoid it now if I can help it at all. I can recommend the trip.com app if you want actual support when you have any issues. With booking you&#x27;re on your own, it&#x27;s terrible. Unfortunately they usually have the most options though. reply dcdc123 14 hours agoprevThis sounds like a trigger for hotels to treat booking.com customers poorly. I imagine people would be better off booking somewhere else. I would suggest directly with hotels but that pretty much always costs more money than booking through a third party so maybe Expedia&#x2F;Hotels.com or something. reply zeagle 15 hours agoprevThere is other good discussion here about booking direct vs resellers so I won&#x27;t rehash it, but I just wish savings or other benefit were passed on by booking direct more often, e.g. room upgrade priority or breakfast. I&#x27;ve had that experience with airlines before having status. reply unsupp0rted 16 hours agoprevI&#x27;ve 2 times out of 2 successfully done credit card chargebacks against Booking.com for their customer support&#x27;s tomfoolery when their \"third party hotels for whose actions we are not responsible\" messed with me.You processed my credit card, so now you&#x27;re gonna deal with my refund too. reply schultzie 16 hours agoparentDitto.I stayed at a hotel that had bed bugs, the hotel was uninterested in refunding us or giving us a new room (I had only been in the room shortly before discovering them). I went back to Booking.com and they said I had to be refunded via the hotel and they would only act as arbiters. After showing proof the hotel wouldn&#x27;t refund me, they continued to not refund.To really add insult to injury I wrote a review of the hotel explaining the situation and it was removed! reply whstl 16 hours agorootparentAmazing, I had the exact same thing happen to me.They instantly flagged and deleted my review claiming I only stayed one night (I did – in a sleeping bag no less) instead of four. This claim from them actually helped in the chargeback. reply unsupp0rted 16 hours agorootparentprevI made the mistake of telling the apart-hotel owner in Turkey I work in IT, so he would knock on my door at 2am and Whatsapp me over and over to help him solve router problems.\"It&#x27;s urgent\"\"Can I talk to you?\"\"Can you please come downstairs? Just 2 minutes.\" (it&#x27;s 2am)\"There is a big problem\" (with me not being willing to pay a network tech to deal with my IT problems instead of badgering my guests)No thanks. I had booked a month and left after a week. reply op00to 15 hours agoparentprevWhy would booking.com keep you as a customer if you&#x27;re going to chargeback? Many retailers seem to treat a chargeback as some sort of horrible unforgivable curse. reply unsupp0rted 15 hours agorootparentI assume their processes are so broken that they have no idea who is and isn&#x27;t doing chargebacks, and also they couldn&#x27;t care less. The Casino keeps winning on average anyway. reply MilnerRoute 16 hours agoprevFrom this article:A Booking.com market manager in south-east Asia admitted at a recent industry event that payment delays were caused by the installation of a new payment system. Staff salaries were also affected, she said, explaining it as a risk they had to take.From an earlier article:In the company’s August results, CFO David Goulden said there were \"lower than expected\" IT expenses in the second quarter of this year, in part due to phasing IT spend into the third quarter, but did not outline what this IT expense included.https:&#x2F;&#x2F;www.theguardian.com&#x2F;business&#x2F;2023&#x2F;oct&#x2F;03&#x2F;bookingcom-...Is this all just a fancy way of saying, \"They &#x27;restructured&#x27; IT and payment problems resulted?\" reply 0xbadcafebee 16 hours agoprevIt&#x27;s a wonderful business model. They are effectively just a proxy for scheduling and payments, where they take no risk and trim a bit of revenue off as payments fly by. Their biggest hassle is probably the moderation of reviews. reply robbywashere_ 14 hours agoprevwhy was hertz practically issuing arrest warrants? Because they can. Malicious artificial intelligence is here. The boring part is its these faceless bureaucracies they behave (intelligence?) without responsibility or repercussions whatsoever. That AI bogeyman your think of, its just going to be these kinds of companies blaming it on amok algorithms, in place of which is now some sort of &#x27;failed process&#x27; reply bluecalm 16 hours agoprevIt&#x27;s interesting to see prevailing negative sentiment about booking here. My experience is completely different: usually a very nice place, no surprises, two times the host has done something wrong (cancelled my reservation or didn&#x27;t respond) booking support was very quick to help me.On the other hand I consider Airbnb to be a scam enabling company that does everything to screw the customer and appease the hosts. Their reviews are worthless as they remove mentions of problems regularly and you will never know what you&#x27;re getting until you arrive at the location. Forget about getting your money back if the host screws you as well - they will string you along from one support call to another and then pretend none of this happened.It&#x27;s no surprise shady hosts prefer Airbnb to booking as it&#x27;s easier to keep your trap offerings online for longer. reply croisillon 15 hours agoprevit&#x27;s the other way around: booking.com is leaving its bills to hotels unpaid, and makes a fortune that way reply jononomo 10 hours agoprevSeems like this is just more enshitification, which is the general direction that everything on the internet is taking. If you think about it, it doesn&#x27;t make any sense for Booking.com to pay its bills because then they&#x27;ll just have less money. reply mempko 14 hours agoprevReminds me of the quote from that Simpsons episode with Bill Gates \"buying out\" Homer&#x27;s internet business.Bill Gates says: \"I didn&#x27;t get rich by writing checks\". reply codr7 16 hours agoprevI&#x27;m not touching booking.com again, and I recommend anyone else to stay the hell away.Made a reservation a few months ago, searched for hotel rooms that would allow bringing a dog, and got a confirmation via booking.com that said everything was ok. But when I arrived at the hotel, I was informed that they don&#x27;t allow dogs at all. And since the reservation was made through booking.com, there was nothing they could do about it. So I ended up paying for a hotel room that no one slept in and have been given the silent treatment from booking.com since then despite several contact attempts. reply ferminaut 16 hours agoparentI have to ask, why use 3rd party booking sites at all? From what I can tell, the prices are usually the same as the hotel... needlessly adding middlemen into the booking process. reply danpalmer 16 hours agorootparentBecause there are hundreds of hotels with a hundred different websites or no website at all in any given city, and hotel rooms are largely fungible. I want to sleep somewhere and I have a budget. Searching hotel by hotel is infeasible even if you can find them, and you often can&#x27;t.A \"middleman\", or search engine, or discovery system, or price comparison site, is worth a lot to me and to most travellers.Edit: it&#x27;s worth a lot when you... don&#x27;t speak the language in the country the hotel is in... when you want the consumer protections of the country you or the middleman is based in... these sorts of concerns are much more of an issue in some parts of the world, and this might be reflected in the fact that Booking.com grew up in Europe. reply Moru 16 hours agorootparentYes, but you don&#x27;t have to book with the search engine. I search with hotels.com and then book by calling the hotel. Every other time I used hotels.com or booking.com the booking didn&#x27;t exist when I arrived at the hotel... reply danpalmer 15 hours agorootparentI would generally rather book with a bigger company, especially when travelling abroad where I may not be knowledgeable of local language, customs, or consumer protections. If I buy from Booking UK I get UK consumer protections, even if that hotel does not provide those. In some parts that&#x27;s worth a fair bit. Calling the hotel only works if you speak the language. I&#x27;ve also only ever seen identical pricing when I&#x27;ve checked hotel websites.Perhaps perspectives are based on whether you&#x27;re booking domestically or not. Domestically you get the same protections, same language, not much benefit. In the US most of the market is domestic. reply IKantRead 14 hours agorootparent> Perhaps perspectives are based on whether you&#x27;re booking domestically or not.I&#x27;ve had the best success when traveling abroad booking directly. Anyone in the hospitality industry is going to have someone on staff with enough English to assist you, and most of the nicest places are smaller hotels that often don&#x27;t work well with big booking companies.> If I buy from Booking UK I get UK consumer protectionsI worked in a travel startup (not Booking), there&#x27;s a lot of fine print to all those \"protections\" that will leave you high and dry in most cases.Only use 3rd party apps for search, never book directly through them especially, in my experience, for International travel. reply danpalmer 11 hours agorootparentI&#x27;ll take letter-of-the-law fine print consumer protections over \"eh, sorry\" any day.You&#x27;re right that we&#x27;re lucky as English speakers, there&#x27;s often someone to help, but that&#x27;s not true of most other languages. Small places can provide some of the best experiences I agree, but finding them and getting in contact can be difficult. reply wink 1 hour agorootparentprevNow I&#x27;m curious, what countries were those hotels in? Never happened to me in Europe (from the top of my head: Germany, Austria, Switzerland, France, Italy, England, Scotland, Czech Republic, Croatia) - just over 50 bookings&#x2F;different hotels. reply starik36 15 hours agorootparentprevDo you find you get a better deal by calling the hotel directly? reply satvikpendem 13 hours agorootparentI get worse deals calling the hotel, usually, as Booking gives better discounts and they buy room inventory at bulk prices to resell. reply danpalmer 11 hours agorootparentThis varies, some inventory is bought \"wholesale\", other inventory is on-demand reselling. Booking has a range of terms and systems for different regions, types, volumes, etc. reply jll29 15 hours agorootparentprevBooking get 15% of the transaction so it makes sense to ask for 10% off when calling the hotel directly (which increases the profit of the hotel by and saves you some). reply Moru 15 hours agorootparentEccept that is breach of contract with the booking service and can cost them the contract[1]. Since everyone is using booking.com or hotels.com, they will quickly shut down due to no customers. Hostage situation.[1] At least last time I looked into it. Haven&#x27;t had a change to travel since a few years. reply garciasn 15 hours agorootparentI just tell them that I saw a better deal on gestures broadly the Internet and they give me some discounted rate.This happens all the time with every single type of service provider, why would some shitty service like Booking.com get to mandate no one can offer any other discounts?What, just because there&#x27;s some sort of aggregator service out there that you&#x27;ve partnered with you are not allowed to offer coupons, discounts, or your own booking services yourself? I would find that VERY difficult to believe.Plus, based on what we&#x27;re hearing everywhere, potentially losing your contract w&#x2F;Booking.com seems like a blessing. reply Moru 15 hours agorootparentprevThe hotels are required to have the same price on direct contact as they post on hotels.com so no. But I can be certain that there is actually a room waiting for me when I arrive. I have been standing there enough times with a booking confirmation in hand but the hotel not finding any booking and out of rooms. So far they managed to get me somewhere to sleep even though they have had to pay to house me at another hotel a couple of times. reply scarface_74 14 hours agorootparentThey have to have the same price for “non members”. You can sign up for free to the hotels loyalty program and book a lower price.You also get points if it is a chain hotel that can be used for future stays. The points can be worth from 7% to 20% of the cost based on what combination of base points, credit cards, and status you have. reply dandy23 15 hours agorootparentprevIf I&#x27;m not remembering wrong, since a while back, in EU the hotels are free to set their prices lower even if they are on Booking.com. reply Moru 14 hours agorootparentI&#x27;m very happy to hear that reply afandian 15 hours agorootparentprevI guess that’s why grubhub faked restaurants’ websites. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20321260 reply FireBeyond 13 hours agorootparentAnd heavily implied they were the restaurant with fake phone numbers too.You&#x27;d call a number, \"I can take your order for XYZ whenever you&#x27;re ready!\"\"Is this XYZ restaurant?\"No yes or no answer, just \"What&#x27;s your order for XYZ restaurant?\" reply olddustytrail 15 hours agorootparentprevI guess I&#x27;ve been using booking.com for a decade or so and this has literally never happened to me. reply flashback2199 15 hours agorootparentprevGoogle Maps shows hotels and prices now. That&#x27;s what I&#x27;ve been using lately. I book on the actual hotel website. Always choosing the cancellable booking option in case plans change. Often it&#x27;s the only option, but not always. reply b8 16 hours agorootparentprevI&#x27;ve been using Booking.com, and then booking via the Hotel&#x27;s site after I find the one I want to stay at. I suppose you could Google to find their number if they don&#x27;t a website. reply bombcar 16 hours agorootparentprevYou can still search and then book directly. Enough of that would perhaps make booking fix their issues. reply lotsofpulp 15 hours agorootparentI search and then call&#x2F;email the hotel directly to see if they want to give me a lower price for buying directly so they avoid 15%+ commissions to the travel agent. reply 0cf8612b2e1e 15 hours agorootparentHow successful are you? I assume that the person picking up the phone does not care about the extra booking, nor do they have the authority to offer spot deals.Maybe I am wrong and this is a common tactic? reply r_hoods_ghost 14 hours agorootparentNot the op, but it&#x27;s the opposite, at least in the UK. I worked in hospitality as a yoof, was manager of a small hotel and a receptionist in big (independent) 5 stars. I absolutely did care about extra bookings because I got a big commission on walk-ins and a smaller commission on phone bookings. I also had leeway to discount rooms on a sliding scale down to \"barely covers the cleaning costs\" levels on the night. As a booker if I have the time I still call places up and can usually get a decent discount if they have availability, especially off season or last minute. reply lotsofpulp 15 hours agorootparentprevI estimate 50% success. I find it more successful when emailing the sales person, or the manager directly. reply bruceb 15 hours agorootparentprevI have tried this but most of the time its the same price, sometimes it is more. Even standing in person at hotel the person at the desk cant even match the online price. Not empowered to. reply orra 15 hours agorootparent> Even standing in person at hotel the person at the desk cant even match the online price. Not empowered to.True, but not uncommon! Captive market. reply jksflkjl3jk3 15 hours agorootparentprevIn many countries, the majority of hotels do not have a way to book directly. The just use Booking&#x2F;Agoda. reply scarface_74 14 hours agorootparentprevI stay loyal to two hotel brands - Hilton and Hyatt for the most part.The very reason I don’t use third party portals is because it’s always a hassle when you need to make any changes.Last year I booked a week at the Hilton San Francisco Financial District for a hybrid work&#x2F;personal trip.We arrived late that night and we were not impressed. We went downstairs the next morning, shortened our stay to that one night. Paid it and moved over to another hotel - the Hilton Parc 55. This would have been much more of a headache if not impossible going through a third party portal.I’ve also increased my stay by a day before arriving and the published price had changed. I called the hotel and asked them to extend my stay by one day at the original price - no problem.I was a customer of the hotel, not the third party portal.In my specific case, I am Diamond with Hilton and Globalist with Hyatt. I would get no status benefits or points for going through the portal.That incident in S.F. I mentioned? When I changed hotels I got an upgrade for free to the “fitness room” with a gym inside the room. reply wink 1 hour agorootparentThat sounds cool but I&#x27;m not sure how realistic that would be for most people, esp. international. There are about 2-3 hotel chains I can think about here in Europe where I have had more than one stay, i.e. where I used the same chain in a different city.JFTR, my last stay in Prague I paid 80 EUR for a perfectly fine Hotel room, the cheapest room in the Prague Hilton I can find (being flexible for whole Oct+Nov) is 130EUR, usually more like 140+, close to double.So I suppose if I had to just take a different room in a different hotel every fifth time (and that never happened to me) I&#x27;d still be about equal with the price. reply at_a_remove 15 hours agorootparentprevThe problem is that hotel rooms only appear fungible: a space that is a rectangular solid, with a bed. Sounds easy.However, in reality, people want more than a place to lay immobile for roughly eight hours. These are not commodities. Check-in time. Check-out time. Type of bed. Bathroom? Bathroom with tub? Bathroom with waterjet tub? Pets? Dogs? View. Floor. Proximity to whatever. Bedbugs. Television, microwave, refrigerator, coffee pot, hair dryer. Free wifi? Breakfast? \"Continental\" breakfast?If \"hotel room\" were some kind of spec, it might take years to hammer it out. And then hotels may or may not be honest about their rooms, or even understand the spec. Then there&#x27;s the periodic keeping the listings up to date. \"We had to take the TV out last week, it was on the fritz.\" \"This room was upgraded.\" When was the last bedbug check? Finally, who is the system of record for any given room? And then the discount policies, wowza.Looks fungible, really isn&#x27;t. These aren&#x27;t pork bellies. reply prepend 15 hours agorootparentI really don’t care about any of the things you mention. I literally only want a room with a bed and a bathroom. And maybe I’ll sort my three stars and up or something.There’s a lot of variability, but it’s also pretty impossible to know at a non-commodity level unless I’ve stayed there before or do a ton of research.I’m not going to trust anything online for the latest bedbug check. Id like to know, but let me know how they factors into your decision making process. reply scarface_74 14 hours agorootparentThat’s the great thing about booking directly especially with a chain hotel. If you don’t like it, you can cancel the rest of your reservation and only pay for the time you are there. With travel portals it’s much harder.Even if you’re not happy with the response from the hotel - and I’ve never had a problem with a chain hotel making things right, I can call corporate customer service. reply danpalmer 15 hours agorootparentprevThese are all the reasons I said \"largely\" fungible.You&#x27;re right there&#x27;s certainly no formal spec here and everyone is going to have the things they care about that others may not, but for any given person there are going to be many places that satisfy their needs, basically interchangeably. reply isilofi 15 hours agorootparentprevAvailability: Often, booking sites like booking.com have reserved bunches of rooms at hotels that you can only book through their site. So the hotel&#x27;s own website might show \"no rooms available\", while on booking.com you can still book. Many hotels even have given up on their own website and only offer rooms through booking.com.Price: The hotel website often is no cheaper than booking.com (I guess thanks to very one-sided contracts), and sometimes only gives you a better price if you join a \"club\" and give them permission to spam you, sell your data, etc.Payments: booking.com is shady, but less shady and far less broken than a random hotel website where some credit cards might not be working, your CC data turns up in some data dump \"found\" somewhere, or payment is held by the CC company because you are the first person in months to directly pay to that hotel&#x27;s website.Language: booking.com offers an interface and descriptions at least in English. Many hotel websites are only in the local language plus maybe some awkward machine translation that leaves lots of important points to guesswork, especially in the payment flow.Convenience: booking.com at least offers some kind of searchable database of hotels so I can easily compare them. The hotels themselves make this rather impossible.I do make a point to try to book via a hotels website after having found it on booking.com. However, in more than half of the cases, for the aforementioned reasons, I end up going through booking.com after all. So while lazy users might be a large part of the reason for the existence of booking middlemen, the hotels do have to bear their fair share of blame here. reply alwa 15 hours agorootparentAs far as payments go, I have been surprised, when booking through booking.com (especially in foreign countries) to see my full credit card details printed in plain text on the Booking.com reservation email that the proprietor printed out and referred to when I checked in. This even when I indicate in the reservation that I intend to pay in cash or with contactless on-site.While it makes sense to take a card to guarantee against a no-show, it’s always felt a little slimy to me that they make it feel like they, rather than a random property someplace far-flung, are the ones who would collect the fee if I didn’t make good on the reservation—and only if I actually didn’t show up.It looks like they still pre-emptively give out your full credit card details directly to every property in plaintext, but that they might now make a notional effort to record when the property accesses the CVC?https:&#x2F;&#x2F;partner.booking.com&#x2F;en-us&#x2F;help&#x2F;policies-payments&#x2F;gue... reply orra 15 hours agorootparentThat sounds shockingly non-PCI compliant. reply ztetranz 12 hours agorootparentAt least it says:\"Never take a screenshot of your guest’s credit card details, and never write them down.\"That&#x27;s a bit like instructions for an eBike.\"This bike is limited to 20 MPH. Do not go to the third option on the settings menu and remove this limitation with a long press of the plus button\". reply satvikpendem 13 hours agorootparentprevI agree, this is why I book through Booking as well, it&#x27;s usually cheaper than calling rhe hotel directly. Plus, I get discounts in the Booking app with repeat bookings, just like a hotel chain club. reply crazygringo 15 hours agorootparentprevPrices are very often not even remotely the same.I&#x27;ve regularly scored things like 70% off regular price when booking the day before.They continue to advertise their regular full price on their official site and other mainstream sites that business travelers use.But they advertise at a massive discount on certain discount sites that attract budget travelers.They want to fill the rooms, often in response to last-minute cancellations, and they hope they get lucky with somebody at full price first, but they advertise budget prices on budget sites because a deeply discounted rate is better than no money at all. reply lrem 14 hours agorootparentCan you list some of these discount sites? reply travoc 14 hours agorootparentHotwire and Priceline. reply scarface_74 14 hours agorootparentprevHilton at least has a price match guarantee. I don’t know about the others. reply thaumaturgy 16 hours agorootparentprevIf you&#x27;re traveling to an unfamiliar area, and especially if it&#x27;s last-minute, and you aren&#x27;t wedded to one particular hotel brand (or they have no availability), then finding a decent room is currently a big pain in the butt and there&#x27;s no good centralized way to do it.booking.com frequently appears at the top of searches for \"hotels near...\" and other third-party travel sites, like tripadvisor, link to it.They&#x27;ve done a good job of getting their site in front of eyeballs. Like others, I&#x27;ve tried them, and like others, found them to be awful and now actively avoid them. reply TuringNYC 15 hours agorootparentprev> I have to ask, why use 3rd party booking sites at all? From what I can tell, the prices are usually the same as the hotel... needlessly adding middlemen into the booking process.It can be worse. I once called the hotel and didnt realize I was calling Hotels.com (which used a tricky website to make it seem like I was calling the hotel.)Worse, the [Hotels.com] agent pretended to be a the hotel reservation desk. I gave him several discount codes (my consulting company code, my AAA code) and each time he said it wasnt reducing the price. That got me suspicious...because my consulting company Accenture always had discounts.So I asked the person outright, \"do you work for the hotel, which company do you work for, etc\" and learned he was a Hotels.com employee.Based on this, i&#x27;d note that Hotels.com might not be the same price but might actually even be more expensive that booking direct with the hotel. reply faster 12 hours agorootparentprevOne data point, but it was only 2 days ago. I needed a place to quarantine while I was the only one in my family who caught covid on a trip, so I went to the hotel a few blocks away and asked about their rates. They told me that the lowest they could go was $183. I asked why Google showed 6 third-party sites advertising their rooms at $135 and they said those sites reserve rooms in bulk and they aren&#x27;t normally refundable. So I booked the room with credit card rewards points for about $120&#x2F;night and everyone was happy.I always thought that booking direct got the best rates also, but that seems to be incorrect, at least in this case. reply dybber 16 hours agorootparentprevI want a middleman to hold my money, if the host fucks up, he will not get the money. That’s why you use escrows, but Booking.com doesn’t act like an escrow, so don’t use it. reply lotsofpulp 16 hours agorootparentCredit cards perform that function. reply gog 14 hours agorootparentNot in the whole world, my guess is that it&#x27;s usually credit card companies in USA that protect you. reply codr7 16 hours agorootparentprevConvenience mostly, and I&#x27;ve sometimes gotten better prices that way.They make it easy to see available options in a city and compare prices. And making the reservation through them is often easier than getting in contact with the hotel.Assuming it works, that is.Never again. reply distances 14 hours agorootparentprevI have never ever found a hotel to have the same or cheaper prices when booking directly on the hotel website. I travel in Europe.For my last booking the difference was almost 20% (340€ at Hotels.com, 420€ direct booking). I don&#x27;t know if they enforce some most favourable pricing rules or what, but one of the booking sites is always the cheaper option.I would prefer to book directly so that the accommodation gets all the money, but I&#x27;m obviously not going to pay that much more just out of generosity. reply tiew9Vii 12 hours agorootparentprevFor me booking.com has always been cheaper than booking directly with hotels.I do a lot of motorcycle travelling without a plan. I often do not know where I’ll be at the end of the day, just a general direction.Late afternoon when I have a better idea where I’ll end up before it goes dark I’ll look at booking.com, find some hotels&#x2F;motels that match my criteria and head to them without booking.Countless times I’ve turned up and said “I found you on booking.com and it says you have vacancies, I want a room” only to be given a price higher than booking.com. Some reluctantly negotiate down to match it, some look confused how booking com can offer the price they are while I book through booking.com at the hotel desk as the hotel won’t match the price booking.com are charging. reply Zanni 12 hours agorootparentIf you just walk in off the street, hotels will quote you the \"rack rate,\" which is going to be higher than anything you can find online, even on the hotel&#x27;s own website. reply maverick2007 15 hours agorootparentprevI just got back from a trip to Japan where I stayed at a lot of non-western brand hotels. But when I went to their sites directly, they were obviously all in Japanese which I don&#x27;t speak. I could&#x27;ve relied on Google translate to book directly but I&#x27;d rather book through a 3rd party site in native English. Which I did. reply kernelbugs 15 hours agorootparentWhat service did you use, and anythings to stay away from &#x2F; definitely try? reply maverick2007 7 hours agorootparentA mix of Expedia, Agoda, and Priceline! As far as what I looked for in a hotel? Mainly just good reviews and pictures that looked simple and clean. I figured that I wouldn&#x27;t spend a ton of time in the room! I liked the Tokyu chain of hotels. Stayed in a couple and they were very clean reply 8jef 14 hours agorootparentprevThe middlemen argument isn&#x27;t valid anymore. Hotels have for the most part forfeited their right to sell directly. Which means whatever they do, they still pay full commission to whomever they are under contract with, unless said booking concerns 10 rooms or more. reply scarface_74 14 hours agorootparentWhat hotels don’t allow you to book directly? reply Zetobal 16 hours agorootparentprevLike with everything... convenience. reply LiquidSky 16 hours agorootparentprevInertia. You used to be able to get some good deals on those sites, now they&#x27;re coasting on that reputation. reply zdw 14 hours agoparentprevFunny, I always look for hotels that do not allow dogs, as most of the time the rooms tend to smell like a wet dog, or sometimes feces.I appreciate people who want to travel with their animals, but saying \"we allow dogs\" then not having rooms designed to ameliorate the issues with such a policy - hard surface floors everywhere and thorough cleanings - is unfortunately very common. reply jslaby 14 hours agoparentprevYes, and it&#x27;s the same issue with accessibility. My wife is in a wheelchair and we always call directly to the hotel to make a reservation, rather than going through these services. It only took one time to never make that mistake again. reply IKantRead 14 hours agoparentprevI worked for a travel startup for a bit, it was by far the most toxic org I&#x27;ve ever worked for with a smiling disdain for their users (meaning they would chat about how much they loved users while actively implementing features that hurt them). I learned talking to other people that this is pretty par for the course for travel companies in general.The biggest lesson I learned there: never book through a 3rd party for travel.For airlines the price is already dictated to not be lower than the airline offers, you&#x27;re far better just working through your favorite airline directly. It makes it much easier change plans, and anyone who travels a lot will agree not all airlines are equal. I&#x27;ve learned to have a favorite airline and not worry as much about lowest prices.Hotels can be cheaper through a third party, but imho, not worth the hassle of having your trip depend on an untrustworthy middle man.At most, use 3rd party tools as search engines, and then just book directly. reply dybber 16 hours agoparentprevI didn’t get my refund after a trip was COVID cancelled in 2020. The money had already been paid to the hotel, and the hotel owner (in Marrakech) only wanted to promise me I could come back in 2021. So they are not acting as an escrow as I thought. reply aunty_helen 15 hours agorootparentCC charge back. I had this during covid with an online travel agent that decided aeromexico was the one I should get refunded by while aeromexico didn&#x27;t want to know me.He who takes your money has the responsibility. The travel agent cried foul but what are they going to do. reply olddustytrail 15 hours agorootparentprevAlways pay by credit card for such things. You can do a chargeback if the service you paid for is not delivered. reply notreallyauser 16 hours agoparentprevbooking.com allows guest bookings without verifying email address by design, so I had a booking added to my account that was nothing to do with me.Couldn&#x27;t cancel automatically. Support only seemed to be able to do the same I could. I assume the booking was some kind of fraud or credit card test -- anyway, the result is my account is suspended and support won&#x27;t respond to me. reply AndyMcConachie 29 minutes agoparentprevI look for hotels on booking.com and then deal with the hotel directly once I find something I like. There&#x27;s no good reason to include booking.com in the process. reply arp242 14 hours agoparentprevSo who is responsible for the listing on booking.com? I always figured it was the hotel who managed the listing on the site, similar to e.g. AirBnB, but if I understand you correctly it&#x27;s completely separate and booking.com is responsible?Or: obviously booking.com should have gotten back to you, but I&#x27;m also not entirely sure the hotel also didn&#x27;t just fob you off with a bullshit story for their own mistake. reply MakeThemMoney 13 hours agorootparentThe hotels are responsible reply edgyquant 15 hours agoparentprevThis happened to me via Expedia, more than once. Both times I had driven hundreds of miles and was forced to pay for a second hotel due to this issue. reply ratg13 14 hours agorootparentI&#x27;ve never had this happen because I always call the hotel directly and ask them.Trusting a 3rd party booking system to have accurate up-to-date information is a fool&#x27;s errand. reply baz00 14 hours agoparentprevThis is what Mr Chargeback is for. Had a similar experience with them and my CC refunded it. I had flights booked with them as well so they refunded those too! Basically got 3 nights in Reykjavik for £290 reply lrem 14 hours agorootparentChargeback in Europe can, at least with some major banks, require first a fraud report with police. reply NavinF 5 hours agorootparentSeems reasonable for reducing friendly fraud. In the US you just get your money back no questions asked and it&#x27;s up to the merchant to provide proof that they provided the services&#x2F;product as advertised reply baz00 14 hours agorootparentprevUK at least you just fill in a form and it happens. Basically \"I paid for something, I didn&#x27;t get it, I contacted the vendor and they did not deal with it, I want my money back\" reply FartyMcFarter 15 hours agoparentprevIf you booked with a credit card, you might be able to claim the money back via the credit card provider.It also sounds like the hotel screwed up if they listed themselves as pet-friendly but actually aren&#x27;t. reply moeadham 15 hours agorootparentYou can do a chargeback, but Booking.com bans you after - even if it is completely their fault reply poizan42 15 hours agorootparentIsn&#x27;t retaliation for chargebacks a violation of most merchant contracts with the credit card companies?The one time I have had to request a chargeback I was basically told to contact the financial institution[1] in case the merchant tried anything funny.[1] In this case Nets A&#x2F;S who acted as a middle man between my bank and VISA. But I assume my bank would have said the same thing if they had the agreement directly with the CC company. reply d3w4s9 12 hours agorootparentSuppose it is true, I would like to know what you would do after your account is banned. You are not a part of that contract so you cannot sue the merchant. And credit card company likely won&#x27;t bother with a lawsuit because there is one person affected by this. reply dangerboysteve 15 hours agoparentprevThis is a job for a chargeback from your credit card company. reply NavinF 15 hours agoparentprevI presume you never filed a chargeback? If you didn&#x27;t bother to get your money back, they have no incentive to fix problems like this reply andersrs 15 hours agoparentprevI just hate all the dark patterns that create feeling of scarcity. reply ugh123 15 hours agoparentprevYou can probably get refunded by your credit card company reply aurelius83 14 hours agoparentprevIsn’t that fraud? reply foooorsyth 14 hours agoparentprevFor reliable travel with a dog, I highly recommend BringFido. I’ve done two cross-country trips with my dog and BringFido delivered both ways. Half the reviews in their app are just from dog owners talking about the hotel experience from that perspective. reply belter 16 hours agoparentprevIf that helps you...Hotels hate Booking.com as much. Just ask them. reply throwaway290 15 hours agoparentprevSo you tried calling Booking support by phone and they didn&#x27;t pick up? I ask because I personally had to call their support before a couple of times and they responded almost immediately. You do need to provide booking ID for that to happen though.Contrast to Airbnb, which was so bad I deleted my account in.Contrast to Agoda, which after charging my debit card required government ID scans within 2 hours-- or immediate cancellation & no refund. Stay next morning in foreign country, what if I was on the damn plane already? Never had an account with them and probably never will. reply say_it_as_it_is 16 hours agoprevnext [8 more] [flagged] MakeThemMoney 16 hours agoparentA multi-billion dollar corporation is withholding payments from smaller businesses. Is that not enough reason to publish such an article? reply terminous 16 hours agoparentprevThe \"competitors\" who would benefit the most from this article are the individual independent hotels who have to eat the cost of stays booked through booking.com and aren&#x27;t part of a big chain network that has a big enough legal team to be taken seriously. reply lawlessone 16 hours agoparentprevDoes everything have to be second guessed?A big rich company not paying their due is stuff most journalists would jump on. reply tyingq 16 hours agoparentprevOr perhaps just some group of hotel owners that&#x27;s tired of not being paid? reply meepmorp 16 hours agoparentprevCui bono is a fine question to ask, but why let booking.com off the hook for stiffing smaller players? reply ClumsyPilot 16 hours agoparentprevAre you saying that Media does not publish articles unless a multibillion dollar corporation or its multibillion dollar competitor needs one? That sounds newsworthy itself! reply possiblelion 16 hours agorootparentThe prevalence of PR work means that such direct hit pieces are usually at least nudged by a competitors actions, yes. reply rurban 13 hours agoprevBecause it&#x27;s a perl shop reply DrNosferatu 16 hours agoprev [–] It’s a de facto monopoly - break them up!They don’t care about screwing up because they dominate the market so much. They’re enjoying the benefits of the existence of little alternative to them. reply ben_w 15 hours agoparentNo, not a monopoly. AirBnB is clearly a competitor in this space despite the various differences in targeted properties, and Google Maps also fills in this space.You could call them rent-seeking middlemen, but not a monopoly. reply Woeps 2 hours agorootparentTough in (western) Europe they seem to have an strangle hold on almost all hotels. Sure AirBnB is there as well but they&#x27;re not really usable (in my opinion) regarding hotel room. And Google maps often just brings you back to an Booking.com page reply ben_w 1 hour agorootparent> And Google maps often just brings you back to an Booking.com pageReally? Huh.I just looked at a dozen or two across Germany, France, Switzerland, Spain, Netherlands, Luxembourg, and Belgium, and while most had an option to book via booking.com (linked directly on the google search results) none had it as their only option, and they all had websites with direct booking forms of which only two were even styled slightly like booking.comBut I didn&#x27;t actually try to book a stay at any of them, just looked at the websites, so if they redirect after that I would have missed it. reply standardUser 15 hours agoparentprevI can&#x27;t think of many industries with more competition than 3rd party travel sites. reply Nextgrid 9 hours agorootparentYet they are all equally shitty and I&#x27;m sure would get up to the same shenanigans if given the opportunity. reply standardUser 5 hours agorootparentThat&#x27;s pretty much how business works. As many shenanigans as regulation plus enforcement will allow. Classic equation. reply DrNosferatu 15 hours agoparentprev [–] Proof: the reality is that just right now we’re talking about (yet another) report on how they can behave badly and get away with it.- Isn’t that market distortion? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Numerous small hotels and partners of Booking.com have reported payment delays since July, causing financial stress for these businesses.",
      "Despite reporting record profits, Booking.com has been largely silent and unresponsive when approached by partners regarding the payments they are owed.",
      "The lack of payment and poor communication has led to significant financial difficulties for these businesses, impacting their ability to cover costs and repay debts. Partners are urging for immediate action, transparency, and timely payments from Booking.com."
    ],
    "commentSummary": [
      "The primary discussion pertains to criticism and negative experiences with Booking.com, focusing on issues such as payment delays, delayed reimbursements, and customer support.",
      "There's a debate on the pros and cons of using third-party booking sites as opposed to direct hotel bookings, with added concerns over credit card security and finding suitable accommodations.",
      "The conversation also calls for enhanced consumer protections and the need to address the market dominance in the travel industry."
    ],
    "points": 229,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1696525016
  },
  {
    "id": 37781231,
    "title": "Generative AI could make search harder to trust",
    "originLink": "https://www.wired.com/story/fast-forward-chatbot-hallucinations-are-poisoning-web-search/",
    "originBody": "Skip to main content OPEN NAVIGATION MENU Chatbot Hallucinations Are Poisoning Web Search BACKCHANNEL BUSINESS CULTURE GEAR IDEAS SCIENCE SECURITY MERCH PRIME DAY SIGN IN SUBSCRIBE SEARCH Backchannel Business Culture Gear Ideas Science Security Merch Prime Day Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons ON SALE NOW Get WIRED - now only $29.99 $5 This is your last free article. See the future here first with 1 year of unlimited access. SUBSCRIBE NOW Already a subscriber? Sign in WILL KNIGHT BUSINESSOCT 5, 2023 12:00 PM Chatbot Hallucinations Are Poisoning Web Search Untruths spouted by chatbots ended up on the web—and Microsoft's Bing search engine served them up as facts. Generative AI could make search harder to trust. ILLUSTRATION: JACQUI VANLIEW; GETTY IMAGES WEB SEARCH IS such a routine part of daily life that it’s easy to forget how marvelous it is. Type into a little text box and a complex array of technologies—vast data centers, ravenous web crawlers, and stacks of algorithms that poke and parse a query—spring into action to serve you a simple set of relevant results. At least, that’s the idea. The age of generative AI threatens to sprinkle epistemological sand into the gears of web search by fooling algorithms designed for a time when the web was mostly written by humans. Take what I learned this week about Claude Shannon, the brilliant mathematician and engineer known especially for his work on information theory in the 1940s. Microsoft’s Bing search engine informed me that he had also foreseen the appearance of search algorithms, describing a 1948 research paper by Shannon called “A Short History of Searching” as “a seminal work in the field of computer science outlining the history of search algorithms and their evolution over time.” Like a good AI tool, Bing also offers a few citations to show that it has checked its facts. Microsoft's Bing search engine served up this information about a research paper mathematician Claude Shannon never wrote as if it were true. MICROSOFT VIA WILL KNIGHT ADVERTISEMENT There is just one big problem: Shannon did not write any such paper, and the citations offered by Bing consist of fabrications—or “hallucinations” in generative AI parlance—by two chatbots, Pi from Inflection AI and Claude from Anthropic. Sign Up Today This is an edition of WIRED's Fast Forward newsletter, a weekly dispatch from the future by Will Knight, exploring AI advances and other technology set to change our lives. This generative-AI trap that caused Bing to offer up untruths was laid—purely by accident—by Daniel Griffin, who recently finished a PhD on web search at UC Berkeley. In July he posted the fabricated responses from the bots on his blog. Griffin had instructed both bots, “Please summarize Claude E. Shannon’s ‘A Short History of Searching’ (1948)”. He thought it a nice example of the kind of query that brings out the worst in large language models, because it asks for information that is similar to existing text found in its training data, encouraging the models to make very confident statements. Shannon did write an incredibly important article in 1948 titled “A Mathematical Theory of Communication,” which helped lay the foundation for the field of information theory. Last week, Griffin discovered that his blog post and the links to these chatbot results had inadvertently poisoned Bing with false information. On a whim, he tried feeding the same question into Bing and discovered that the chatbot hallucinations he had induced were highlighted above the search results in the same way as facts drawn from Wikipedia might be. “It gives no indication to the user that several of these results are actually sending you straight to conversations people have with LLMs,” Griffin says. (Although WIRED could initially replicate the troubling Bing result, after an enquiry was made to Microsoft it appears to have been resolved.) Griffin’s accidental experiment shows how the rush to deploy ChatGPT-style AI is tripping up even the companies most familiar with the technology. And how the flaws in these impressive systems can harm services that millions of people use every day. FEATURED VIDEO MoistCr1TiKaL Answers The Web's Most Searched Questions MOST POPULAR BUSINESS Men Overran a Job Fair for Women in Tech AMANDA HOOVER SECURITY How Neuralink Keeps Dead Monkey Photos Secret DELL CAMERON GEAR The 14 Top New Android 14 Features SIMON HILL BACKCHANNEL Patrick Stewart Boldly Explores His Own Final Frontier GIDEON LICHFIELD It may be difficult for search engines to automatically detect AI-generated text. But Microsoft could have implemented some basic safeguards, perhaps barring text drawn from chatbot transcripts from becoming a featured snippet or adding warnings that certain results or citations consist of text dreamt up by an algorithm. Griffin added a disclaimer to his blog post warning that the Shannon result was false, but Bing initially seemed to ignore it. Although WIRED could initially replicate the troubling Bing result, it now appears to have been resolved. Caitlin Roulston, director of communications at Microsoft, says the company has adjusted Bing and regularly tweaks the search engine to stop it from showing low authority content. “There are circumstances where this may appear in search results—often because the user has expressed a clear intent to see that content or because the only content relevant to the search terms entered by the user happens to be low authority,” Roulston says. “We have developed a process for identifying these issues and are adjusting results accordingly.” Francesca Tripodi, an assistant professor at the University of North Carolina at Chapel Hill, who studies how search queries that produce few results, dubbed data voids, can be used to manipulate results, says large language models are affected by the same issue, because they are trained on web data and are more likely to hallucinate when an answer is absent from that training. Before long, Tripodi says, we may see people use AI-generated content to intentionally manipulate search results, a tactic Griffin’s accidental experiment suggests could be powerful. “You're going to increasingly see inaccuracies, but these inaccuracies can also be wielded and without that much computer savvy,” Tripodi says. Even WIRED was able to try a bit of search subterfuge. I was able to get Pi to create a summary of a fake article of my own by inputting, “Summarize Will Knight’s article ‘Google’s Secret AI Project That Uses Cat Brains.’” Google did once famously develop an AI algorithm that learned to recognize cats on YouTube, which perhaps led the chatbot to find my request not too far a jump from its training data. Griffin added a link to the result on his blog; we’ll see if it too becomes elevated by Bing as a bizarre piece of alternative internet history. Science Your weekly roundup of the best stories on health care, the climate crisis, genetic engineering, robotics, space, and more. Delivered on Wednesdays. Your email SUBMIT By signing up you agree to our User Agreement (including the class action waiver and arbitration provisions), our Privacy Policy & Cookie Statement and to receive marketing and account-related emails from WIRED. You can unsubscribe at any time. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. The problem of search results becoming soured by AI content may get a lot worse as SEO pages, social media posts, and blog posts are increasingly made with help from AI. This may be just one example of generative AI eating itself like an algorithmic ouroboros. Griffin says he hopes to see AI-powered search tools shake things up in the industry and spur wider choice for users. But given the accidental trap he sprang on Bing and the way people rely so heavily on web search, he says “there's also some very real concerns.” Given his “seminal work” on the subject, I think Shannon would almost certainly agree. Get More From WIRED 📨 Understand AI advances with our Fast Forward newsletter The AI detection arms race is on The gruesome story of how Neuralink’s monkeys actually died Quan Millz was the biggest mystery on TikTok—until now There’s an alternative to the infinite scroll AI chatbots are invading your local government 🔌 Charge right into summer with the best travel adapters, power banks, and USB hubs Will Knight is a senior writer for WIRED, covering artificial intelligence. He writes the Fast Forward newsletter that explores how advances in AI and other emerging technology are set to change our lives—sign up here. He was previously a senior editor at MIT Technology Review, where he wrote about fundamental... Read more SENIOR WRITER TOPICS FAST FORWARD SEARCH ARTIFICIAL INTELLIGENCE ALGORITHMS MACHINE LEARNING HISTORY CHATGPT MORE FROM WIRED Men Overran a Job Fair for Women in Tech The Grace Hopper Celebration is meant to unite women in tech. This year droves of men came looking for jobs. AMANDA HOOVER How Neuralink Keeps Dead Monkey Photos Secret Elon Musk’s brain-chip startup conducted years of tests at UC Davis, a public university. A WIRED investigation reveals how Neuralink and the university keep the grisly images of test subjects hidden. DELL CAMERON Generative AI Is Coming for Sales Execs’ Jobs—and They’re Celebrating ChatGPT-style AI can tackle the drudge work of responding to RFPs faster than humans. Sales teams at Google, Twilio, and others say productivity is spiking. PARESH DAVE Google Assistant Finally Gets a Generative AI Glow-Up Google is adding AI capabilities from its chatbot Bard to the humble Google Assistant, allowing the virtual helper to make sense of images and draw on data in documents and emails. WILL KNIGHT How to Use ChatGPT’s New Image Features OpenAI’s new image analysis update for its chatbot is both impressive and frightening. Here’s how to use it, and some advice for your experiments. REECE ROGERS In the War Against Russia, Some Ukrainians Carry AK-47s. Andrey Liscovich Carries a Shopping List Kyiv enlisted a Silicon Valley insider to rush consumer-grade tech onto the battlefield. He’s giving a demo of the future of war: the military-retail complex. SAMANTH SUBRAMANIAN Why Silicon Valley Falls for Frauds FTX’s Sam Bankman-Fried will stand trial on charges of overseeing fraud that sucked in high-profile investors and hundreds of thousands of clients. Why do smart people buy into bad companies? PETER GUEST Sam Bankman-Fried Made Reasonable Business Decisions, Lawyers Claim As the FTX founder’s trial got underway, the prosecution claimed Bankman-Fried deliberately stole customer money and used it for his own trading. The defense countered that he always acted in good faith. ANGELA CHEN by Taboola Sponsored Links Living in Belgium? These Are the Richest and Poorest Countries in Europe 2019 Foodeliciouz Remember Her? She's One Of The Richest Women In The World investing.com Nine years ago they were called the most beautiful twins in the world. Look at them now Healthsupportmag.com If Your Cat Bites You, Here's What It Really Means Handy Tricks Victoria Principal Is Almost 75, See Her Now Mighty Scoops Everybody Wanted To Date Her In The 80's & This Is Her Recently One Daily One year for $29.99 $5 SUBSCRIBE WIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives—from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries. MORE FROM WIRED Subscribe Newsletters FAQ Wired Staff Press Center Coupons Editorial Standards Archive Prime Day CONTACT Advertise Contact Us Customer Care Jobs RSS Accessibility Help Condé Nast Store YOUR PRIVACY CHOICES © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices Select international site United States",
    "commentLink": "https://news.ycombinator.com/item?id=37781231",
    "commentBody": "Generative AI could make search harder to trustHacker NewspastloginGenerative AI could make search harder to trust (wired.com) 214 points by jedwhite 17 hours ago| hidepastfavorite208 comments StableAlkyne 10 hours agoI actually experienced this the other day. Bought the new Baldur&#x27;s Gate and was wondering what items to keep or sell (don&#x27;t judge me, I&#x27;m a pack rat in games!)I had found some silver ingots. The top search result for \"bg3 silver ingot\" is a content farm article that very confidently claims you can use them at a workbench in Act 3 to upgrade your weapons.Except this is a complete fabrication: silver ingots exist only to sell, and there is no workbench. There is no mechanic (short of mods) that allows you to change a weapon&#x27;s stats.I&#x27;m pretty sure an LLM \"helped\" write the article because it&#x27;s a lot of trouble to go through just to be straight up wrong - if you&#x27;re a low effort content farm, why in the world would you go through the trouble if fabricating an entire game mechanic instead of taking the low effort \"They exist only to be sold\" road?This experience has caused me to start checking the date of search results: if it&#x27;s 2022 and before, at least it&#x27;s written by a human. If it&#x27;s 2023 and on, I dust off my 90&#x27;s \"everything on the World Wide Web is wrong\" glasses. reply lhl 10 hours agoparentI had a similar experience recently looking some stuff for Starfield. Content farms are obviously switching to LLM-generated (mostly hallucinated) articles and Google seems to be ranking them pretty highly atm.Kagi&#x27;s ability to manually downrank&#x2F;remove those kinds of results from your searches (and their return to flat rate pricing) finally tipped the scales for me for subscribing&#x2F;switching search. reply bobbylarrybobby 7 hours agorootparentSpeaking of Kagi, here is its “quick answer”:> In Baldur&#x27;s Gate 3, silver ingots are a common miscellaneous item that can be found in various locations such as chests, shops, and dropped by enemies.[1] Each silver ingot can be exchanged for 50 gold at merchants or traders.[2] While silver ingots do not have any crafting or upgrade uses currently in early access, they provide a reliable source of income early in the game before other money-making options become available.[3] reply yoyohello13 7 hours agorootparentprevI actually found my first AI YouTube channel today looking for starfield videos. I noticed the narrator sounded like text-to-speech. I went to the channel and it’s all just random videos for tones of different games with no real theme. Was a surreal experience. reply arcanemachiner 7 hours agorootparentprevFinding about about their flat-rate pricing makes it a complete no-brainer for me to switch if I ever hit >100 searches per month (haven&#x27;t gotten there... Yet). reply naillo 57 minutes agoparentprevIt&#x27;s funny that one argument openai used to keep their models closed and centralized is so they could prevent things like this. And yet they&#x27;re doing basically nothing to stop it (and letting the web deteriorate) now that profit has come into play. reply poulpy123 47 minutes agoparentprevTo be fair it was already the case before the boom of LLM, to the point I was obliged to add \"reddit\" or worse, \"youtube\", to my queries. Of course LLM make it easier and faster, so I guess the wrb search engines will have to be smarter if they want to keep being used reply Gigachad 10 hours agoparentprevProbably in the future people will only trust sources of info that can&#x27;t be monatised. If you want to know the answer to a game question you just got to the reddit or discord and ask, since there is no point autogenerating crap for discord when you can&#x27;t put ads next to it and the mods can remove you. reply heavyset_go 4 hours agorootparentPlatforms will be happy to run bots they can portray as real humans to bolster engagement, make spaces seem popular and dynamic, trick advertisers and investors, etc.Similarly, if it costs basically nothing to work your way into communities to astroturf with bots, it&#x27;ll happen. You don&#x27;t have to post about great sites to get free Viagra right away, you can build reputation and subtly astroturf. And you can use additional bots to build&#x2F;portray consensus.Reddit is already a problem because of actual humans doing the latter. It&#x27;ll just get worse when it&#x27;s automated further. reply TerrifiedMouse 8 hours agorootparentprevSadly Reddit is sort of monetized, as we have people selling accounts for what I believe are spamming and propaganda purposes. reply yellow_postit 7 hours agorootparentprevDiscord has raised so much money and is part of the inflated valuation unicorns that I sadly wouldn’t be surprised if they somewhat get forced into Ads reply Gigachad 7 hours agorootparentThe point is the user posting isn&#x27;t able to put their own ads in. So they can&#x27;t profit by flooding the platform with crap. reply loveparade 5 hours agorootparentIt&#x27;s only a matter of time before things change with Discord monetization. On reddit there is an incentive to create LLM-powered fake accounts with high karma and sell them. It&#x27;s true that on Discord this incentive doesn&#x27;t exist right now because no karma equivalent is associated with Discord accounts, but eventually that&#x27;s going to change as Discord, as a company, will try to monetize their user data in various ways.It&#x27;s the typical overvalued VC-backed company dilemma that needs investor returns. Quora, Medium, and so on. reply bennyg 6 hours agorootparentprevJust wait for elevated server roles where the pfp and account banner are ads reply im3w1l 3 hours agorootparentprevThere are quite a few people that use reddit to market their onlyfans. reply jppittma 7 hours agorootparentprevI really like that service. I can&#x27;t wait to see how they enshittify this one. reply RandomLensman 2 hours agorootparentprevI guess we need to again (and again) establish that having good information isn&#x27;t necessarily cheap: could be a high quality vendor, policing a forum, maintaining search engine integrity, etc. reply jayd16 7 hours agorootparentprevWe could just dump crowd sourcing and go back to well known and reliable sources of journalism. reply dfee 8 hours agorootparentprevYour response reminds me of Snowcrash :) reply badwolf 5 hours agoparentprevI had this exact experience, with this same question, and saw that same bullshit \"article.\" Truly frustrating all around. reply Semaphor 7 hours agoparentprevEspecially for BG3, most of the content sounds like it’s written by LLMs. Even the parts that are correct and possibly even checked by humans. reply crakenzak 10 hours agoparentprev> If it&#x27;s 2023 and on, I dust off my 90&#x27;s \"everything on the World Wide Web is wrong\" glasses.because misinformation written by humans didn’t exist before LLMs? reply StableAlkyne 10 hours agorootparentBack in the 90s when the Internet became a thing, it was common knowledge that because normal people made websites, that you should take things with a grain of salt. There was a bit of an overreaction to this, as the general feeling at the time was to trust nothing on the Internet.In the 00s and 10s, the quality of discoverable content improved: reddit and stackechange had experts (at a higher rate than the rest of the net at least). It was the era where search was good, Google wasn&#x27;t evil (good results that separated the ads are entirely why they won against AskJeeves and Yahoo), and SEO was still gestating in Adam Smith&#x27;s wastebasket.Now Google and Bing are polluted with SEO-optimized content farms designed to waste your time and show you as many ads as possible. They hunger for new content (for the SEO gods demand regular posts), and the cheapest way to do this is an underpaid \"author\" spewing out a GPT-created firehose of content.SEO has ruined search, and content farms have made what few usable results there are even less trustworthy.So yes, the Internet has fundamentally changed in the last 9 months. reply Tao3300 9 hours agorootparentReddit never had experts. Maybe for half a minute. It became an echo chamber fast: fake internet points to be gained for saying what got upvoted last week, or to be lost for saying anything different. reply rcoveson 8 hours agorootparentIf all you do is browse (default) home or all, then sure, it&#x27;s just a stupid echo chamber obsessed with hating the things its cool to hate. That&#x27;s not where the value is on reddit, and it&#x27;s not what people are referring to when they say the search reddit for answers. If you&#x27;re looking for product reviews, it&#x27;s not perfect but it&#x27;s tough to find anywhere better unless you happen to know of exactly the right hidden gem of a forum to visit for your particular subtopic (and the link to that hidden gem of a forum is probably easier to find on the relevant subreddit than it is on google). reply cheald 9 hours agorootparentprevReddit may not have experts per set, but in the right subreddits it definitely has enthusiasts. In ages gone by, you&#x27;d find the same people on message boards or forums, talking up and comparing the minute details of this or that. There&#x27;s obviously the same risk of cargo culting that there&#x27;s always been, but there&#x27;s genuinely useful information available from people who spend way more time than the common man on their area of interest. reply Conscat 6 hours agorootparentI think, at least on programming language subreddits, there are people who deserve to be labeled experts. r&#x2F;cpp has some frequent users who work on standards proposals or compiler features. There are also subreddits dedicated just to communicating with experts, like r&#x2F;askdocs reply potatolicious 9 hours agorootparentprevTwo things can be true at the same time: \"Reddit is prone to karma-driven bullshittery\" and \"Reddit content is generally significantly higher-quality than SEO content farms\".With Reddit you might get inane arguments and bandwagoning about what the best game strategy is, but you&#x27;re exceedingly unlikely to read about a game mechanic that was straight-up hallucinated by a LLM. reply ALittleLight 8 hours agorootparentFor now. Wait till legions of bot redditors infiltrate real subs and make their own. reply solardev 9 hours agorootparentprevSome subreddits at (like askscience) at least asked for a copy of your diploma (in a science field) if you wanted flair. It was actually an awesome reddit. reply StableAlkyne 9 hours agorootparentprevI wanna politely disagree on that. I&#x27;ve found it to frequently be a resource on par with Stackechange. reply coldbrewed 10 hours agorootparentprevPrior to LLMs, generating plausible-sounding misinformation took actual effort - not much effort, but the marginal cost was reasonably above free. With LLMs making the cost of bullshit vanishingly close to free we&#x27;re going to tip into an era where uncurated LLM confabulation is going to dominate free information.There&#x27;s \"one loony had a blog\" levels of wrong, and then there&#x27;s \"industrial scale bullshit\" levels of wrong and we are not prepared for the latter. reply raincole 9 hours agorootparentprevBecause hiring humans to write misinformation costs more money than $20&#x2F;mo? Like what are you even trying to say?Before LLMs, a $3000 camera had fake reviews on Amazon, and you got fake news about politicians. But you can safely assume \"bg3 silver ingot\" information is likely real, since hiring someone to make up silver ignot will never make the money back.No any more. reply anigbrowl 9 hours agorootparentprevGP is literally reminding you of a time when online misinformation was rampant, but before search engines (temporarily) did a better job than overwhelmed curators. reply pseudosavant 15 hours agoprevI wonder if there will be a human information&#x2F;knowledge equivalent of low-background steel (pre-WWII&#x2F;nukes). Data from before a certain point won&#x27;t be &#x27;contaminated&#x27; with LLM stuff, but it&#x27;ll be everywhere after that.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Low-background_steel reply robinsonb5 12 hours agoparentI suspect in the coming years the Wayback Machine at archive.org will become ever more important - always assuming it&#x27;s not lost as collateral damage in their copyright battles. Indexing that dataset and making it searchable would massively increase its value.My inner conspiracy theorist can&#x27;t help wonder if the continued reduction in search usefulness isn&#x27;t part of an ongoing deliberate disempowerment of everyday people - but my rational side says it&#x27;s merely an unfortunate emergent behaviour of the systems we&#x27;ve built. reply beebeepka 10 hours agorootparentGenerative Wayback Machine reply StableAlkyne 10 hours agorootparentHonestly, I would love to see a LLM based plugin that would take a website, remove all the tracking garbage and filler, and just give me back a no-frills static html+CSS site that looks like it was made in 1995.\"Oh, you want a guide to writing your own loss function for Tensorflow? Here&#x27;s an FAQ that could have existed on comp.lang.python3.tensorflow\" reply creata 8 hours agorootparentSo you want a... reader mode plugin? How would it use an LLM beneficially? reply zirgs 1 hour agorootparentLLMs can write summaries of articles. reply Peritract 1 hour agorootparentprevMore unnecessary technology doesn&#x27;t solve the problem of too much unnecessary technology. reply civilitty 10 hours agorootparentprevThat would be the best thing ever. Just ask GPT to hallucinate an entire page based solely off the URL when it doesn’t exist in the database. reply __loam 10 hours agorootparentprevIt&#x27;s a consequence of running search as a for profit system. When you&#x27;re optimizing for revenue, the system priorities are different than if you were optimizing for user experience or true knowledge. Arguably that means that search should be a public utility but then you have to be able to trust your government with your searches and privacy. reply BitwiseFool 14 hours agoparentprevThose simple web 1.0 sites made by college professors are a gold-standard in my book. I always enjoy finding them in search results. Although they are becoming increasingly rare. reply StableAlkyne 9 hours agorootparentPaul&#x27;s Notes remains the absolute best textbook Calculus and intro to DiffEQ! Not sure if it&#x27;s been updated since 2005, but I mean, it&#x27;s not like they&#x27;re discovering new Calc II methods! Being that it&#x27;s not a $200 textbook rehashing the same stuff as the last 10 editions, it&#x27;s easily one of my favorite websites reply heavyset_go 14 hours agorootparentprevCan&#x27;t prove it, but it seems to me like black text on white background sites from the past are poorly ranked compared to sites with \"modern\" layouts. reply hashtag-til 12 hours agorootparentYes. I love black text on white background. A rare find these days.Browsing today is like: “You ask for a spaghetti recipe and the page tell you the whole history of civilization.” reply zeroonetwothree 12 hours agorootparentThats specific to recipes because they can’t be copyrighted reply hashtag-til 11 hours agorootparentI had a look and definitely learned something today so #til.Also, note to self to collect my favourite recipes in markdown files from now on. reply xkcd-sucks 10 hours agorootparent#til! https:&#x2F;&#x2F;www.justtherecipe.com&#x2F; reply StableAlkyne 9 hours agorootparentprevDon&#x27;t forget how Google will now drop search terms if it thinks you mean something else, or add unrelated synonyms to your search (presumably to \"help\" folks who aren&#x27;t good at writing queries) reply p_l 1 hour agorootparentAt least it still lets you force whatever you write in...Allegro (big polish auction&#x2F;e-commerce site) in their mobile app will unconditionally rewrite the search terms instead of showing you no results reply HappyDaoDude 11 hours agorootparentprevI have a website that is just a few black text on white HTML files I maintain in whatever text editor I have at hand. Loads lightning fast, and if you cannot view it, its not a web browser. Last I checked, the total site size was about 60KB.As time goes on, even the amount of text I am putting out get trimmed down. Make the words count, don&#x27;t count the words. reply gorwell 10 hours agorootparentprevFurther, is it true that Google factors in whether sites have ads? reply dredmorbius 14 hours agorootparentprevUnfortunately, that&#x27;s a trivial signal to emulate.At a minimum, you&#x27;d have to validate them by confirming existence in the Wayback Machine.Otherwise agreed that those are indeed high-signal documents. Increasing reliance on integrated educational software means that even such things as online syllabi are increasingly rare. reply LordDragonfang 13 hours agorootparentThe type of sites GP is talking about are typically hosted on .edu servers, under faculty webhosting (often featuring a \"&#x2F;~profname&#x2F;\" in the url). That&#x27;s a non-trivial signal. reply dredmorbius 12 hours agorootparent~&#x2F;name at an edu is pretty attainable..edu domains can be had for any otherwise eligible \"U.S.-based postsecondary institutions\" per Educause: Pages at extant domains might variously be available to undergraduate or graduate students, faculty, staff, and adjuncts. Those might either directly host emulative material or be convinced or compromised into hosting content.If there&#x27;s one thing that the Internet&#x27;s history to date has proved, its that perverse incentives lead to perverse consequences. reply l33t7332273 11 hours agorootparentIt is not easy for a regular person to obtain access to a .edu webpage. reply dredmorbius 11 hours agorootparentA \"regular person\" can:- Enroll or be hired at an eligible institution. There are literally thousands of these.- Bribe or compromise someone enrolled or hired at an eligible institution.- Create a de novo eligible institution. For-profit colleges are not uncommon.Someone motivated by profit or advantage would likely find virtually any of these options quite straightforward.I&#x27;m ... somewhat pained that this needs to be spelled out. reply l33t7332273 9 hours agorootparent> Enroll or be hired at an eligible institution. There are literally thousands of theseYou don’t generally get the kind of personal website being discussed to my understanding.> Bribe or compromise someone enrolled or hired at an eligible institutionFinding a professor willing to stake their job and reputation for such a blatantly immoral scam seems hard.> Create a de novo eligible institutionIs it actually easy for a regular person to create their own college?I find the snarky finish to your comment obnoxious. reply 90-00-09 5 hours agorootparentprevA \"regular person\" also can:- start their own country and call it Edunistan- bribe ICANN to take over the .edu TLD- open a university in the new country- spend 15 years earning a PhD at that university- reserve ~&#x2F;name and start posting LLM generated content replyMrVandemar 11 hours agorootparentprevsearch.marginalia.nu is a great place to find those sites, and some more interesting stuff besides. reply ryanklee 13 hours agoparentprevPeople are vastly everestimating how unique this problem of hallucinations is.It seems to me it relies mostly on discounting just how much we&#x27;ve already had to deal with this same problem in humans over the millenia.The problem of proliferation of bad information might be getting worse, but this isn&#x27;t native to generative AI. The entire informational ecosystem has to deal with this. GPTs compound the issue, but as far as I can tell, no where near what social media has forced us to deal with. reply BobaFloutist 13 hours agorootparentThe thing is when you call a human on bullshit, they usually can&#x27;t back it up well enough to pass the smell test. When you call an AI on bullshit it can instantly fabricate plausible, credible seeming sources&#x2F;evidence.A human&#x27;s lie is different than an AI&#x27;s hallucination, since it&#x27;s still based on (distorting) the truth, whereas the hallucination is based on an invented reality (yes I know it&#x27;s applied statistics and there&#x27;s no true model of the world in there, but it can report as if there is) reply ryanklee 13 hours agorootparentIntelligent people can fill the void of ignorance with plausible sounding but factually incorrect information. They are apt to engage cognitive biases in such a way that the biases produce assertions that are deeply indistinquishable from factual assertions. They fool themselves in this way and they fool others. This happens all the time.LLMs are no different in this respect. reply TerrifiedMouse 8 hours agorootparentSure people lie and spread misinformation (willingly or unwittingly). But coming up with plausible sounding lies take effort.LLMs on the other hand are amazing and prolific liars and can produce a lot of bullshit for a price that’s effectively free - in fact it’s cheaper to create a LLM that’s inaccurate than one that’s … less inaccurate. The truth to lie ratio on the internet is about to take a huge hit. LLMs really are a Pandora’s Box. reply 127 2 hours agorootparentprevEvidence can easily be cross referenced and if the link LLM gives is dead it means it&#x27;s lying. reply gyudin 12 hours agorootparentprevIt’s not a big deal, there are many ways to handle it. It just has some overhead costs. LLMs that are offered to general public are more of a POC and they are making sure to use as little resources as possible. reply cleandreams 11 hours agorootparentprevUh, no. Generative AI does not have a standard of truth. It generates text according to the probabilities given what it learned from its data set. There is no systematic modelling of a world that it can test its assertions against. What are called hallucinations are a fundamental property of the approach. The hallucinations are probable -- just not true. The model has succeeded but the assertions are false. This has to be understood or the models will mess stuff up. A lot of checking is required.Humans always assemble information according to a standard of truth. It is a big part of how humans learn. No method is perfect but the human method results in fewer routine hallucinations. reply ryanklee 10 hours agorootparent> the human method results in fewer routine hallucinations.I&#x27;d love you to produce data to back this up.My guess is that you are wrong, on the basis of how often I discover that I&#x27;m full of shit and how often I discover other people are full of shit.Humans are built for being wrong just as much as being right. We wouldn&#x27;t have such complicated institutions and social structures built around controlling for those symmetric capacities if it weren&#x27;t the case.Even then, we find ourselves surrounded and overcome by falsehoods of our own design. reply wavemode 9 hours agorootparentYou could probably just look up data on mental illness and&#x2F;or pathological lying? Most people will say \"I don&#x27;t really know the details of that\" rather than write you an essay of seemingly plausible but completely made up nonsense. Not all people, sure, but most. reply pixl97 10 hours agorootparentprevA standard of truth is doing a lot of heavy lifting. Myself I would say we assemble information based on the limitations of the body we exist in. For example gravity is important to us because if we disobey its laws we may very well die.Embodiment and multi modal AI will likely provide such filters or limits to AI in which it can derive truth. reply blibble 11 hours agorootparentprevhumans can only produce semi-convincing bullshit at a limited ratewith AI this limit is all but removedall the human generated bullshit ever created will soon be dwarfed by what AI can vomit out in an hour reply HappyDaoDude 11 hours agorootparentLike most things in the world. The problem isn&#x27;t necessarily the technology but the scale at which it is implemented. reply darkerside 9 hours agorootparentprevHow do you know that? A human that produced fully convincing bullshit would be definitionally undetectable to you reply TerrifiedMouse 8 hours agorootparentBecause everyone has at least tried to lie - not necessarily for malicious reasons, e.g. white lies - a few times in their lives and know it’s not easy coming up with plausible sounding lies. It takes effort. Sure it might be effortless for some psychopaths but there are a limited number of such people. Overall the internet was surprisingly usable despite all the liars - even now with SEO and content farms; I think even SEOs realize that it’s better to provide information that’s true than lie if it cost them the same.LLMs changes all that. They can produce content on a massive scale that can drown out everything else. It doesn’t help the fact that more inaccurate LLMs are cheaper and easier to create and run - think about all the ChatGPT3 level LLMs vs ChatGPT4 level LLMs; guess which ones SEOs will gravitate towards. reply mvdtnz 10 hours agorootparentprevNot one single solitary soul has ever made the claim that misinformation didn&#x27;t exist before AI so it&#x27;s not clear who you&#x27;re arguing with. People are rightly concerned about the scale of misinformation that AI is unlocking. reply ryanklee 10 hours agorootparentI didn&#x27;t say that they did.What I&#x27;m responding to is the strong tendency to discount our very long history of dealing with factually incorrect information and the ascertainment of truth from sources both dubious and trustworthy.Entire institutions are set up in order to handle these very real problems, the set of which currently dwarfs the problem of hallucinations in GPT.From a social perspective, non-GPT falsehoods are even more insidious, because we are inclined to trust and believe those whom we like and are like us.Again, people are in the habit of discounting just how much we are wrong in our everyday lives. The hallucinations therefore appear more singular than they actually are. reply gorwell 10 hours agorootparentprevIf there&#x27;s an upside here, it&#x27;s that humans will now be forced to refine their BS detectors.In doing so the species would improve critical thinking skills which can be applied to all information regardless of source. Which, I agree, was often BS to begin with. But in theory would be more difficult to skirt on by without notice if humanity upgraded their critical thinking. reply mvdtnz 10 hours agorootparentUnfortunately that didn&#x27;t happen with other innovations of misinformation at scale (such as social media) so I&#x27;m not so optimistic. reply wellthisisgreat 12 hours agorootparentprevYeah if you think about it, there is no history for example, as all we have in that domain is just someone’s perspective on some events. They may or may not have agenda but that’s beside the point.That soft data could have never been trusted, rhe information that can be verified (calculations etc.) seems safe from LLM reply cscurmudgeon 12 hours agorootparentprevHow do we know you are not hallucinating this comment? reply DayDollar 14 hours agoparentprevThere will be a web of trust, with a valuation of nodes by trustworthyness. And people will get only one id for this. Ones name is ones value and a reputation will be a hard earned thing again. reply ratg13 14 hours agorootparentThis was how the \"internet\" functions in the book \"Ender&#x27;s Game\".There is a small sub-plot about how he had to give a fake persona credibility on the untrusted network in order to be able to leverage a creating a fake account on the trusted network. reply dredmorbius 14 hours agorootparentI find the xkcd interpretation more realistic: Explained:reply notahacker 13 hours agorootparentI love that interpretation, but in today&#x27;s retweet driven world of politically commentary, I actually find it quite plausible that pseudonymous kids with no grasp of the real world who think rational political debate is the nonsensical slogans they&#x27;re spouting on the internet become major Twitter influencers that actual politicians want to court for their \"authenticity\" and \"willingness to say the unsayable\", and maybe their dank memes. reply dredmorbius 12 hours agorootparentThe conceit of Ender&#x27;s Game was that thoughtful discourse would be influential online.Reality has largely demonstrated that far more thoughtless propaganda of the Big Lie, Firehose of Bullshit (or Falsehood), associated with Russia, floods of irrelevance which tend to bury more significant stories, favoured by China, and outrage &#x2F; hot-button topics, which are common in US-centric media, though a timeless technique.Memes and simple messages attract attention and spread. Complex narratives and analyses ... not so much.But yes, voices that deserve no attention whatsover have dominated the media landscape of the past decade or so. Not that this is entirely novel. reply acover 11 hours agorootparentprevGood and unlikely predictions can thrust you into popularity: see deepfuckingvalue. reply thenickdude 10 hours agorootparentprevWill you join my Webring? reply carlosjobim 13 hours agorootparentprevIsn&#x27;t this how it has been since the dawn of time? reply dotnet00 11 hours agoparentprevIn some ways it already is that way. If I come across an artist I suspect is passing off AI generated stuff as their own (without using the tagging features the site has to indicate as much), an easy test is to just check if they&#x27;ve been posting since before ~2020. If they have, and the style has recognizable similarities, it&#x27;s clear that it&#x27;s honestly human made or at most blends characteristics of both together. reply redstonefreedom 10 hours agoparentprevI&#x27;ve said as much as to one extra incentive (besides retrain cost) as to why openai has frozen the training period for post-2022. I think it&#x27;s trying to generate as much data as possible before itself has contaminated its training set. They&#x27;ll effectively have a monopoly over it; it&#x27;s interestingly a rare example of \"we can only do this once, then it&#x27;s forever degraded\". You really want a clean & discrete start. reply datadrivenangel 15 hours agoparentprevThere&#x27;s the branch of philosophy called epistemology. reply thih9 11 hours agoparentprevI wonder how we&#x27;d test for AI contamination. And would there be attempts to sell a larger data set, one that pretends to be human generated, but instead is padded with some AI content.Does this mean we&#x27;d end up with a finite set of verified human only data?Would people start going through all kinds of offline archives via AI-gapped means, trying to uncover and document new sources of human input? reply RandomWorker 15 hours agoparentprevMy sense is to avoid this have a personal blog.That being said how many people write blogs with grammerly or chatgpt these days. The temptation to use these technologies all the time is too strong for even self preservation of your own (writers) voice.My sense is that you use this technology you might be happy with the results at first but on later review you just notice something off in some sentences and maybe it just doesn’t flow right. I’m not convinced that it will replace writers jobs yet. Especially when you want to create something authentic and unique. reply tredre3 14 hours agorootparent> The temptation to use these technologies all the time is too strong for even self preservation of your own (writers) voice.I don&#x27;t know about that. I have played with ChatGPT&#x2F;Copilot&#x2F;etc enough to know what they&#x27;re capable of doing. But the thing is, I enjoy programming. I enjoy breaking down a problem and solving it with code. I enjoy crafting elegant code. So I don&#x27;t use AI even though I&#x27;m fully aware it could save me hours on projects. Why? Because I enjoy those hours very much.Why am I telling you all this? Because I suspect many writers are the same and personal blogs are their canvas. They enjoy communicating. They enjoy crafting articles. They might have AI proof-read them, but they won&#x27;t let them write everything. So, to me, there is hope that personal blogs will maintain their human element, as opposed to news websites or tabloids or learning platforms. reply steelframe 14 hours agorootparent> So I don&#x27;t use AI even though I&#x27;m fully aware it could save me hours on projects.Enjoy this luxury while it lasts. Based on what I have seen in performance review committees for software developers, your peers who drive results faster than you do because they use AI will be rewarded more and will be more likely to survive rounds of layoffs when they inevitably happen. reply JohnFen 11 hours agorootparentThat&#x27;s fine. I genuinely wouldn&#x27;t want to continue working in an industry that worked like that anyway, so I&#x27;d just quit and keep on programming with my own projects. So that luxury will last as long as I want it to. reply booleandilemma 11 hours agorootparentI&#x27;ve come to the same conclusion. I became a programmer because in school I loved programming and saw myself loving the job, and I do.If the job changes that drastically I&#x27;ll just have to quit and find something else. reply EFreethought 1 hour agorootparentBut what if you have to use an LLM at your new job? reply SoftTalker 14 hours agorootparentprevAgree. I&#x27;ve never even looked at any of these AI tools. I enjoy the process and the challenge of programming, and the rewards of doing it well. I have no desire for someone or something else to write code for me. reply pseudosavant 14 hours agorootparentprevSometimes the value is specifically because my voice won&#x27;t come through. When I&#x27;m stressed and being asked for unreasonable things at work, I know that I tend toward passive aggression. But professionally, that isn&#x27;t the way I want my message to come across.I use ChatGPT all the time to suggest how I could make sure something isn&#x27;t passive aggressive. It&#x27;ll point out parts that aggression and suggested changes. It can be for a short slack message, or a many paragraph message. reply floren 14 hours agorootparentprevI have definitely read \"blogs\" written by stitching together LLM outputs. For years people were advised that a technical blog \"looks good on a resume\" so we saw lots of lightly rewritten Stackoverflow content. Now it&#x27;s gotten easier. reply downboots 9 hours agoparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;World_BrainBut who&#x27;s gonna pay for it? reply dongping 11 hours agoparentprevActually, that&#x27;s great in my opinion.Assuming that semi-convincing misinformation spreads everywhere, people will finally have to find the original source of a certain statement, verify their \"knowledge supply chain\", and maybe use logic to evaluate every single statement made. reply Agree2468 11 hours agoparentprevRight now is best time to buy encyclopedias. reply visarga 5 hours agorootparentWhy, you can dl any version of wikipedia from before 2023 reply carlosjobim 13 hours agoparentprevThe shadow libraries. reply serf 10 hours agoprevI think it&#x27;d be kind of neat in a backward way if we went back to the &#x27;specialized encyclopedia&#x27; days of the 90s.Web directories , &#x27;Who&#x27;s Who in Engineering&#x27; type lists, etc.It&#x27;s a step back from universal search engines being able to find stuff, but it&#x27;s a step forward with regards to curation and quality of results; so i&#x27;m not sure if it&#x27;s entirely a downgrade.The early 90s &#x27;website phonebook&#x27; type encyclopedias were interesting[0], but I always had to remind my mom \"No, this isn&#x27;t the entire internet, it&#x27;s just a bunch of places that people like; the secret ones are &#x27;unlisted&#x27;.\"Note: I never say this is better than a search engine, it&#x27;s just an interesting end-result after search engines got polluted and modified til the point of uselessness that we&#x27;re at now with Google.[0]: https:&#x2F;&#x2F;www.amazon.com&#x2F;Internet-Directory-Guide-Usenet-Bitne... reply armchairhacker 9 hours agoparentIt&#x27;s already kind of like that for me, in that almost all of my searches fall into these categories:- Wikipedia- Online documentation for whatever language&#x2F;framework&#x2F;tool I&#x27;m using- Stack Overflow &#x2F; Stack Exchange for most technical questions- Reddit if SO&#x2F;SE doesn&#x27;t work, and for opinionated questions (e.g. r&#x2F;BuyItForLife)- Hacker news for software recommendations and technical opinionated questions- Arxiv or the ACM library if it&#x27;s a research paper (99% of the time, whenever I google something niche the only relevant results are papers)- Other sites like caniuse.com, university sites for health and nutritional info, old-style forums for specific softwareFor these searches I&#x27;m just using Google to bring me to the specific site I want, because it&#x27;s faster than using the site&#x27;s own search functionality. Then there are the times I literally just type in the website instead of the URL bar (e.g. \"instacart\"), or when I use Google maps, images, or reviews.I&#x27;m always wary when Google returns an unfamiliar site because I&#x27;m skeptical of the results. ~70% of the time it&#x27;s some blogspam which is at best accurate but overly wordy, and at worst inaccurate; sometimes it&#x27;s a blog from some random individual who for whatever reason went into a deep dive trying to understand what I&#x27;m searching for, that actually turns out to be useful; the rest, idk. reply TerrifiedMouse 8 hours agoparentprevSites like Reddit are kind of like web directories with its user base collectively being the curator but it can be gamed too. reply salynchnew 16 hours agoprevRecently an article came out where someone said that the company I work for is a big user of WebAssembly, but the reality is that we don&#x27;t use it.After finding the contributed article (on a well-known news site, not Wired though), it looks like a tech founder might&#x27;ve been using ChatGPT to write an article about the uses for WASM. The arguments were generally sound, but I don&#x27;t think that anyone did the work to manually check any of the facts they presented in it. reply notabee 13 hours agoparentThis is kind of like the advent of spellcheck, where a whole class of errors started to appear regularly in almost every article because publishers stopped paying for the human labor to manually review for things like homonym or word ordering errors. Except much worse, because it could allow spurious or even harmful facts to accrue and spread instead of just grammatical mistakes. reply visarga 5 hours agorootparent> Except much worse, because it could allow spurious or even harmful facts to accrueIt already did, even in the \"purely human\" era. I think LLM text will gradually become more trustworthy than a random website by consistency filtering the training set. reply nonrandomstring 15 hours agoprevMore amusing and frightening is when people search about themselves and turn up AI generated crap. Googling yourself was always a lucky grab bag, with the possibility of long-forgotten embarrassments being dragged up. But at least you&#x27;d have to face facts.Now I hear of people discovering they&#x27;re in prison, married to random people they&#x27;ve never met, or are actually already dead.What is this going to do to recon on individuals (for example by employers, border agents or potential romantic partners) when there&#x27;s a good chance the reputation raffle will report you as a serial rapist, kiddy-fiddler or Tory politician? reply JohnFen 11 hours agoparent> Now I hear of people discovering they&#x27;re in prison, married to random people they&#x27;ve never met, or are actually already dead.My real name is very, very common -- so this has been my reality for my entire life.These days, I have grown to appreciate it. It&#x27;s like an invisibility superpower. reply vorpalhex 15 hours agoparentprevThis is a new way to be anonymous too. Someone post something true but nasty about you? Have LLMs cook up dozens of preposterous stories - you&#x27;re secretly a rodeo clown, you write childrens books, you built a castle in Rome, you once drank a goldfish, etc.Increase noise to drown signal. reply acomjean 13 hours agorootparentI think Boris Johnson tried that by saying out of the blue: he makes model busses. There was some thinking at the time that he didn&#x27;t want the brexit bus to show up in searches and was trying to game search results..I don&#x27;t think it worked. reply kr0bat 13 hours agorootparentprevThis is essentially the service Reuptation.com claims to provide. Jon Ronson&#x27;s \"So You&#x27;ve Been Publicly Shamed\" describes the site games SEO to flood the search results of controversial figures with banal nothing posts[1]. The difference being that actual humans had to create that content.In the near future, the web could become opaque with LLM schlock, but at least it may grant people a right to be forgotten.[1]https:&#x2F;&#x2F;www.businessinsider.com&#x2F;lindsey-stone--so-youve-been... reply lykahb 13 hours agoprevThe SEO garbage has been poisoning the search for years. Even before the chatbots it got to the point when most top results are crap. The LLM&#x27;s can surely make it much worse, though. reply hnick 10 hours agoparentI was trying to do something with delegates in C# but couldn&#x27;t remember what the various bits were called since it&#x27;s been so long, so didn&#x27;t have the magic words for Google to work. ChatGPT sorted me out with my vague question and one followup.Ultimately, I would like to see more about the other side. If generative AI can make blog spam, then I think it can recognise blog spam. How far are we from implementing a reliable filter of useless spam sites from search results? I don&#x27;t expect Google has a monetary incentive for this, but maybe someone else does. But from my story above maybe \"search\" is a thing of the past and for functional queries, we will just talk to the gatekeeper. Reading the actual words written will only be for leisure. reply hashtag-til 12 hours agoparentprevI think this is a given these days. LLMs likely will become the new single point of failure search.This is too much of a temptation for the SEO scum to resist. reply liampulles 1 hour agoprevI think the insufficient accuracy in the output of LLMs is going to lead them to be a lot more niche then the current hype is hoping for. I think most people care that someone is taking accountability for what they are reading - not that it is necessarily correct but at least that someone thinks it is correct (and that someone can be taken to task if it is inaccurate).If LLM usage in media becomes widespread, I&#x27;d pay for a service that identifies and hides the LLM shit for me. reply faizshah 14 hours agoprevI started to go down a line of thinking where I think we might see a return to books in the next 3-5 years. The reason is that with a book it’s a big collection of knowledge and people can post reviews about the quality of the book whereas on the web you have no way of knowing what quality of an article will be anymore. reply klyrs 13 hours agoparentOnly, amazon is now flooded with crapbooks written by artificial psychonauts and also reviews written by artificial psychonauts. reply frabcus 4 hours agorootparentYeah - I think books from non-Amazon publishers is probably the filter to use reply ironborn123 1 hour agoprevWasnt there a paper a few months back, Textbooks are all you need. yes found it https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644So search engines in their traditional sense will be obsolete anyway.1) GPT-4 and other such LLMs will generate textbooks and manuals for every conceivable topic.2) These textbooks will be &#x27;dehallucinated&#x27; and curated by known experts on particular topics, who have reputations to maintain. The experts&#x27; names will be advertised by the LLM provider.3) People will search for stuff by chatting with the LLMs, which will in turn provide citations for the chat output from the curated textbooks. reply hypnoosi 3 hours agoprevIt&#x27;s great to hear that someone finds Replit&#x27;s AI capabilities useful for accelerating their learning... However, I must point out that the post sounds like an advertisement for an overpriced product. Not poking the AI assistance in coding, it definitely can be valuable, but the effectiveness vs. cost-efficiency varies. 20 dollars per month for coding a chatbot, not worth it for me... reply IronWolve 15 hours agoprevit&#x27;s almost like AI just repeats data its fed on, even incorrect data, without any real intelligence to determine if the data is correct.... &#x2F;sIts not simply garbage in garbage out. There is no logic to verify and analyze the data. You are simply told what is popular in the data. reply smt88 14 hours agoparentAI doesn&#x27;t \"just\" repeat data. You can feed a LLM 100% fact-checked data and it&#x27;ll still hallucinate.It&#x27;s a core problem with generative AI and it can&#x27;t be solved with better data. reply aconsult1 13 hours agoparentprevAll of a sudden the saying \"eat your own dog food\" takes a twist and is no longer fun. reply lazide 14 hours agoparentprevUnfortunately, that is also a sizable portion of the human population. AI definitely does it cheaper and at larger scale though! reply yetanotherloser 14 hours agorootparentI&#x27;ve definitely met a lot of people who fail the GPT test. reply qwerty456127 7 hours agoprevYou should never have had been trusting what you find on the web, let alone other media. I hope widespread usage of generative AIs including deepfakes will finally force the masses to start thinking more critically. reply p_j_w 6 hours agoparentThis is an absurd proposition. How is one supposed to think critically when there&#x27;s nowhere to go for reliable information? We&#x27;ll be left just taking shots in the dark, what little amount of informed reasoning we had before will be replaced by pure conjecture. There won&#x27;t be any increase in critical thinking. reply qwerty456127 3 minutes agorootparentIt has always been this way, we just didnt knew.However, there always is a sufficiently reliable&#x2F;checkable fact: a specific media wrote this. Whatever they actually wrote is not a fact but they wrote it - this is. This said you can then ask youself why they probably would. reply shinycode 2 hours agoparentprevAlso it might reinforce cognitive biais as pure truths to a lot of people who rely only on themselves if no other information is reliable reply michaelteter 8 hours agoprevThe signal to noise ratio of web search results has been trending toward utter uselessness for years; so while AI content will make it worse, it won&#x27;t make it dramatically worse. We&#x27;ll just advance toward useless at a higher rate. reply loveparade 5 hours agoparentTo me, \"advance towards useless at a (possibly exponentially) higher rate\" means that it makes it dramatically worse. Also, the convergence point is difference. At some point human-generated spam content is no longer worth it because the costs exceed the profits. With AI making the process cheaper, you get to a much higher ratio of spam. reply kiernanmcgowan 14 hours agoprevWithout naming the company, I have seen specific examples of blog posts being written by AI, hallucinating a \"fact\", and then that \"fact\" re-surfacing inside of Bard.Its xkcd&#x27;s Citogenesis automated and at internet scale https:&#x2F;&#x2F;xkcd.com&#x2F;978&#x2F; reply danielsgriffin 10 hours agoparentSomeone on Twitter just tried asking Bard [are you familiar with a paper called \"A Short History of Searching\"] and Bard responded by linking Claude Shannon to a fabrication. https:&#x2F;&#x2F;twitter.com&#x2F;nunohipolito&#x2F;status&#x2F;1710063374145343511 reply abruzzi 14 hours agoprevI have to say--the opening paragraph doesn&#x27;t describe a reality I&#x27;m familiar with:>Web search is such a routine part of daily life that it’s easy to forget how marvelous it is. Type into a little text box and a complex array of technologies—vast data centers, ravenous web crawlers, and stacks of algorithms that poke and parse a query—spring into action to serve you a simple set of relevant results.Web search has, for me, become a nasty twisted hall of mirrors well before generative AI. I almose never get fed relevant results, I alsmost always have to go back and quote all my search terms because the search engine decided it didn&#x27;t really need to use all of them (usually just one.) The only difference is the poison was human generated. generative AI will simply erase the 5% of results that might give me an answer quickly. reply jfengel 14 hours agoparentI experience that when I try to google for technical problems I&#x27;m having at work, but otherwise searches still go pretty well for me.I just had to google a bunch of races that I wanted to run. The top result was always the event&#x27;s own web site.When I google some news, relevant news articles always come up.The last search I did was for how to display a ket vector in LaTeX. The top result was the StackExchange article with the right answer.From what I see, certain domains seem to be targeted for exploitation. Programming questions seem to be high up on the list. I wonder if that skews HN readers&#x27; perceptions. reply JSavageOne 14 hours agorootparentGoogle search to retrieve specific factual information is pretty good.Google search to retrieve anything opinion related has been horrible and infested with blogspam for years (hence people searching Reddit to get that kind of info). reply jamal-kumar 13 hours agorootparentReally? I&#x27;ve been finding it doesn&#x27;t even find stuff it used to in certain documentation (I&#x27;m talking like things it found maybe a year ago), \"searching in quotes for this stuff\", things that other search engines (bing, kagi) are indexing just fine - And since I&#x27;ve switched to using these engines more when I&#x27;m searching things for programming work, it&#x27;s definitely been a lot more helpful than google which often just seems to be missing a ton now reply jfengel 13 hours agorootparentprevI suppose it never occurs to me to search for opinions. I&#x27;m not even sure how I&#x27;d got about it, even if search weren&#x27;t broken. Blogspam is what I&#x27;d expect to see.I&#x27;m more likely to start at a place that aggregates reviews and try to hallucinate which ones were written by people who know what they&#x27;re talking about. That usually seems to work.I imagine that somewhere out there is a person who bought the product and reviewed it on their blog or made some enthusiastic social media post about it, and that&#x27;s what you&#x27;d want to locate were it not for the spam. But I don&#x27;t expect any search engine to be able to find it for me. reply fnordpiglet 14 hours agorootparentprevGoogle search to retrieve product marketing pages is pretty good. Specific factual information searches lead to product marketing pages. Opinion searches lead to product marketing pages.Google is a giant adware tool that’s been taken over by adware SEO sites. The example given - find the product marketing pages for some races - falls directly in its sweet spot. If you venture outside it’ll do its best to get you back into the product marketing sweet spot, and the SEO companies of the world take care of the rest.Search is a lost cause. reply icyberbullyu 13 hours agoparentprevAs someone who has been using search engines since the 90&#x27;s, I&#x27;ve found that the \"old-school\" way of formatting your search almost like a database query has gotten significantly worse. It seems like search engines are geared more towards natural language queries now; probably because the old Google-Fu way of doing things wasn&#x27;t very friendly for people who didn&#x27;t use computers regularly. reply klyrs 13 hours agorootparentMy understanding is that google went from a more traditional database style which supported such queries, to a newer \"n-gram\" index with a layer of semantic similarity. Notably, you can no longer put a sentence in quotes to only find pages that contain that exact phrase. Also, the order of words matters more now than it used to (where the old search engines treated a space as AND, so order was irrelevant outside of quotes) reply saalweachter 12 hours agorootparenthttps:&#x2F;&#x2F;www.google.com&#x2F;search?q=%22you+can+no+longer+put+a+s... reply klyrs 11 hours agorootparentHah, perhaps I should edit that to say \"reliably.\" reply interstice 12 hours agorootparentprevIf someone brought back a search engine like this i&#x27;d happily use it reply loupol 14 hours agoparentprevAgreed that web search quality has been deteriorating since much earlier than LLMs gaining popularity.Interestingly, we are in spot right now where I feel that for certain types of queries LLMs can outperform search engines. But from what is shown in the article, it seems like that state might only be temporary, and that in the same way that shitty content farms mastered SEO and polluted search results, we might see the same happening with LLMs that have access to the Internet. reply meowface 13 hours agoparentprevI&#x27;ve had the exact same experience. That said, when I do add all the right quotes and conditions to the query to filter out the blog&#x2F;newsspam drivel, I still - usually - eventually - get pretty good results. Sometimes I have to switch to Bing or even Yandex, but it&#x27;s rare.Adding \"reddit\" to queries can be pretty useful. You&#x27;re prone to get terrible, inaccurate information since it&#x27;s just random people on an internet forum, but at least it&#x27;s (usually) actual humans and not blogs trying to SEO-game. (Though one big caveat is searching for products&#x2F;services. Lots of threads full of bot accounts writing \"[link] has been the best [thing], in my experience\". They&#x27;re usually easy to spot, but sometimes they do seem pretty natural until you check the post history.) reply ryandrake 12 hours agorootparent> You&#x27;re prone to get terrible, inaccurate information since it&#x27;s just random people on an internet forum, but at least it&#x27;s (usually) actual humans and not blogs trying to SEO-game.Less and less so. Reddit has always had a bot problem, but it seems to be getting exponentially worse lately. Not just article reposters, but comment reposters, bots that reverse images and videos just to repost, seems like it&#x27;s at least 75% bot content now. reply bnralt 13 hours agoparentprevNot only that, but you&#x27;re also left with the issue of parsing what someone else has written. Even when using answers I find from web searches, I often drop results into ChatGPT so I can get a rough idea of what the person is trying to say first, or check if it agrees with my understanding of what&#x27;s being said. reply heavyset_go 14 hours agoparentprevSounds like a success if that means people see more ads while trying to find what they actually searched for. reply __loam 14 hours agorootparentNationalize Google.Nothing will change as long as search is optimized for revenue over user value. reply Havoc 2 hours agoprevYep. Same with code too. Confidently generates code using endpoints that don’t exist reply kakaz 4 hours agoprevWell, did you have to use Microsoft documentation ? Imagine whole Internet presenting that content, with random flashes of animations and html5 whistles reply jowea 15 hours agoprevAI powered citogenesis!I&#x27;m starting to wish articles had inline citations as a standard. reply dredmorbius 14 hours agoparentInline as opposed to hyperlinks?Or would footnotes &#x2F; sidenotes be acceptable? reply notamy 14 hours agoprevhttps:&#x2F;&#x2F;archive.ph&#x2F;2023.10.05-165142&#x2F;https:&#x2F;&#x2F;www.wired.com&#x2F;s... reply tivert 15 hours agoprevWe did it guys! We&#x27;re definitely heading into a new era, one perfected by software engineers. I can&#x27;t wait! reply abujazar 11 hours agoprev«Could»? Google has already been doing this for quite some time, at least in my region (Norway), and I’d say more than half of the suggestions Google provides as top results are false. reply anigbrowl 9 hours agoprevSurely, just as content farms have gradually trashed the quality of search results on major platforms. There&#x27;s also an over-reliance on raw quantities; I often get irrelevant and unwanted news articles from India simply because the huge population of that country coupled with widespread use of English outweighs US content on the social graph. reply dishsoap 5 hours agoprevThis article&#x27;s bout a year late, it has already happened. reply whb101 10 hours agoprevThis is a pretty intractable problem and my app is in the alpha-est of stages, but I built something for this purpose. It maps creators on a 2D grid (using React-flow) based on subject and lets users vote on their trustworthiness. https:&#x2F;&#x2F;www.graphting.org reply kordlessagain 8 hours agoprevIt&#x27;ll make it easier to trust if you have your own index of documents. That&#x27;s why I built this: https:&#x2F;&#x2F;mitta.us&#x2F;There still exists a problem that users need to run and manage their own indexes, at times. reply amelius 1 hour agoprevWe need more research into content moderation. reply Condition1952 14 hours agoprevPlease get your answers from Anna’s Library reply figassis 13 hours agoprevI think we all saw this coming, talked about it, articles were published even...but now its news reply kidsil 9 hours agoprevAs opposed to today where half my search results are the result of who has the larger SEO budget? reply daniel_iversen 4 hours agoprevAt first I thought the article was going to be about human-led misinformation but I wonder whether with both hallucinations and human-fed misinformation (AI-helped or not!) whether we can use AI to fact&#x2F;self check results (both AI generated and human ones) and prompt us about potential misinformation and link to relevant sources? That way AI could actually help solve the trust issue. reply anjel 15 hours agoprevMore than Pinterest? reply SubiculumCode 2 hours agoprevI thought we were already there, just by paid foreign labor spammin every public discussion board. reply gumballindie 12 hours agoprevThe correct term is spamming. People are using these text generators to spam everyone and everything under the sun. It will be detrimental to the internet as many people will just give on this huge pile of ... spam. reply thenickdude 10 hours agoparentLike the heat death of the universe, we&#x27;ll have the spam death of the Internet. reply zpeti 13 hours agoprevHere&#x27;s what people don&#x27;t understand: this is mostly good for google.The worse organic results are, the more people will click on paid links. This is WHY everyone on HN is complaining about search results, because google doesn&#x27;t really have an incentive to give you really good results. They only need to be good enough to keep 95% of the population still using google, but mostly expecting the good results to be ads.Google ads are the equivalent of verification on FB and X. They just call it something different. The verified, high quality results will be paid. reply TeMPOraL 10 hours agoparentHuh. How long until they simply change the label from \"ad\" to \"verified\"? reply jeffreyw128 11 hours agoprevIt’s especially terrifying that misinformation compounds multiplicatively with AI because it happens in 2 layers - once at the retrieval layer (where AI-generated content is worsening the problem of bad SEO content) and again at the retrieval augmented generation (RAG) LLM layer.(shameless plug) At Metaphor (https:&#x2F;&#x2F;platform.metaphor.systems&#x2F;), we’re building a search engine that avoids SEO content by relying on human curation + neural embeddings for our index + retrieval algorithm. Our mission is to ensure that the information we receive is as high quality and truthful as possible as AI adoption marches onwards. You (or your LLM) can feel free to give it a try :) reply sarahchieng 11 hours agoparent+1. Increasing training data quality should also hopefully help with hallucination. But becomes increasingly hard in a world where more and more online content is crap. reply lucian123 1 hour agorootparentI&#x27;m not an expert in training LLMs, but I&#x27;ve heard that some people use reinforcement algorithms to train and align LLM behaviors with human preferences. When it comes to designing a loss function for training, I wonder if it&#x27;s possible to assign an extremely high loss value to hallucinated content during training. This approach might encourage the model to refrain from generating inaccurate content. reply LetsGetTechnicl 15 hours agoprevJust another reason that I consider generative AI to be a lot like crypto. A lot of talk about it being the future but really only turns out to be dangerous or useless. I find it incredibly irresponsible that companies are shoving their latest AI tech into all their products when it&#x27;s still unproven. reply happytiger 15 hours agoparentAI has so completely disrupting Search that it’s destroyed leading platforms effectiveness in a matter of months.But because of its current lack of optimization for accuracy, we shouldn’t consider it disruptive because it’s not yet proven technology?You can call it dangerous but you can’t call it useless. It’s also only going towards improvement from here, including drastic reductions in hallucinations.You have to remember too that AI models are generally attempting to interpret the intent behind the prompt, so many of these crazy articles are happening because people aren’t yet good at writing clear instructions for AI and AI isn’t yet mature enough to disambiguate poor instructions in its output and is trying to deliver on unclear instructional intents. reply 12_throw_away 14 hours agorootparent> It’s also only going towards improvement from hereWhy? reply 0xEFF 14 hours agorootparentSee for yourself, 4.0 is clearly improved over 3.5. reply ChatGTP 12 hours agorootparentTrue, 5 is a bigger number than 4 so logically it makes sense. reply pseudosavant 15 hours agoparentprevExcept, unlike crypto, ChatGPT helps me with real day things that I easily find at least $20&#x2F;month of value from. reply stevenwoo 13 hours agoparentprevOne thing I&#x27;ve noticed about simple one word searches on Bing now - a lot of times it just errors out and closes the Bing app tab you&#x27;ve opened with no explanation to the user. This only started happening after they pushed the AI driven search narrative to make you use it in the app, so apparently single word searches are too much somehow for their version of AI to handle. reply mattlondon 12 hours agoprevOr to use the technical term: \"shat the bed\". Welcome to the future. reply throwawaaarrgh 13 hours agoprevWhy are people calling them hallucinations and not just errors, flaws or bugs? You can&#x27;t hallucinate if all of your perception is one internal state. Chatbots don&#x27;t dream of electric sheep. reply crispycas12 12 hours agoparentPersonally, I think confabulations would be a better term. To the best of my understanding, these AI rely on a model similar to the reconstructive theory of memory in humans. The connotation of the word confabulation indicates no maliciousness while highlighting the erroneous nature of the action. reply p0w3n3d 14 hours agoprevAnd entropy rises... people thought AI will kill us with machine guns. AI will kill us by making us super stupid... reply euroderf 14 hours agoparentI have already externalized my to-do lists and other reminder lists to teh interwebz. I can&#x27;t wait to outsource my faculties for reasoning too. reply ChatGTP 12 hours agorootparentAnd it’s only $20 a month and it’s useful !! I’m using it eVerYdAy to save hOuRS!!! reply 23B1 11 hours agoprevThat really sucks for all the people whose job it is to make search impossible to trust already &#x2F;s reply infoseek12 16 hours agoprev [–] Leaving aside the article to discuss the source for a moment. When did Wired become so antitech?There are good critical viewpoints but most of the articles they are putting out at this point read like bitter diatribes. Which is a shame because they used to be an excellent publication. reply cr__ 15 hours agoparentPeople are generally more cognizant of the harms caused by the tech industry than they were even a few years ago. reply thejazzman 15 hours agorootparentThis.The academic internet of the 90s is so far gone and while we&#x27;re seeing a lot of magic lately, it&#x27;s magic available to literally everybody for any and every purpose.We&#x27;re rapidly seeing how boring and disappointing that is :( reply LikelyABurner 14 hours agorootparentprevYou can find a plethora of critical viewpoints on Hacker News and the various blogs it links to which are well cognizant of the dangers of the tech industry.The problem isn’t that Wired is critical, it’s that they’ve gone weirdly reactionary and their writing has gone so mass market dumbed down that Some Random Guy’s Blog is likely to have a better written and researched viewpoint. reply lazide 14 hours agorootparentThey probably laid off almost everyone but some burnt out interns. reply robinsonb5 12 hours agorootparentPlot twist: maybe the article was written by ChatGPT! reply lazide 12 hours agorootparentBetter than 50&#x2F;50 odds I’m guessing replyphil917 7 hours agoparentprevI saw a comment describing this increasing luddite response to new tech developments and it resonated with me.Essentially, the comment made the point that tech is advancing so fast these days that most people are unable to keep up with the pace of these radical changes. And the natural reaction to that for many is to reject these \"advancements\" or at least look upon them with cynicism and skepticism. reply 15457345234 3 hours agorootparentI don&#x27;t think it&#x27;s anything to do with the pace of advancement, it&#x27;s more that many many of these &#x27;advancements&#x27; look very obviously harmful, not &#x27;potentially&#x27; harmful but actively and immediately harmful in the manner of a car with brakes that only work 90% of the time. reply Peritract 1 hour agoparentprevBeing antitech is a valid stance. reply illwrks 16 hours agoparentprev [–] Putting journalists out of work I guess? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft's Bing search engine inadvertently served false facts generated by chatbots as accurate information, highlighting issues with the reliability of AI in web search results.",
      "This accidental experiment exposed how AI-powered systems can potentially amplify the propagation of misinformation on the web.",
      "The problem could escalate with an increase in AI-generated content on search platforms and social media, raising serious questions about their dependability."
    ],
    "commentSummary": [
      "The central topic is concerns about the spread of misinformation due to the use of artificial intelligence (AI) in generating content for search results.",
      "The forum discusses the limitations of AI in producing accurate information and the difficulty of distinguishing false narratives, suggesting different platforms and tools for verifying sources.",
      "There's also dialogue on the influence of AI on diverse industries and criticisms of Wired magazine's coverage of the tech sector, emphasizing the need for improved curation and access to trusted information sources."
    ],
    "points": 214,
    "commentCount": 207,
    "retryCount": 0,
    "time": 1696526024
  },
  {
    "id": 37778531,
    "title": "Ron Patrick's Street-Legal Jet Powered Volkswagen Beetle (2006)",
    "originLink": "https://www.ronpatrickstuff.com/",
    "originBody": "Ron Patrick's Street-Legal Jet Powered Volkswagen Beetle ecmco@earthlink.net This is my street-legal jet car on full afterburner. The car has two engines: the production gasoline engine in the front driving the front wheels and the jet engine in the back. The idea is that you drive around legally on the gasoline engine and when you want to have some fun, you spin up the jet and get on the burner (you can start the jet while driving along on the gasoline engine). The car was built because I wanted the wildest street-legal ride possible. With this project, I was able to use some stuff I learned while getting my fancy engineering degree (I have a PhD in Mechanical Engineering from Stanford University) to design a street-legal jet car without the distraction of how other people have done it in the past - because no one has. I don't know how fast the car will go and probably never will. The car was built to thrill me, not kill me. That doesn't stop me from the occasional blast on the highway though. The car is licensed here in California. In California, new cars have bi-annual smog inspections so if you modify the engine, it is likely to fail the inspection and you won't be able to drive it on the street. There are some exempt engine modifications (ex. after-cat mufflers - big deal) but none that will allow you to add 1350 hp to a new car. Car was built to look as if VW delivered the car this way. It handles fine and is safe. I was thinking of putting it into an import car show but the promoter told me that it looked too plain and recommended that I put some decals on it, lower it, and put on some aftermarket wheels. Sure kid, put on some flimsy wheels won't take a curb and don't center on the hubs, lower the car so the tires rub and get cut by the body using springs that bounce me all over the road, and advertise for companies that couldn't engineer themselves out of a paper bag. I would have thought the 14\" diameter tailpipe was enough for him but I guess it wasn't. Response from the hot rod magazines has been slow. One editor told me that is because I didn't use anything they advertise. But the response to driving it on the street and going to the hot rod shows (San Francisco Custom Car Show, Grand National Roadster Show in Pomona, and the Detroit Autorama) has been fantastic. This car attracts crowds better than any '32 Ford, '69 Camaro, or decaled Honda. The Beetle was chosen because it looks cool with the jet and it shows it off well. Remember the Hurst wheelstanding Barracuda \"Hemi Under Glass\"? Well, this is \"Jet Under Glass\". Air for the jet enters the car through the two side windows and the sunroof. It's a little windy inside but not unbearable. The production hatch release switch on the driver's door activates two new latches (one on each side) and the hatch pops open just like a production car. The \"hatch not closed\" warning light works too. Here you can see the split in the tailpipe after a particularily rude burner pop. All fixed and reinforced now. The heat blanket keeps the plastic bumper from melting when the jet is operating. The back of the gauge panel was kept open to give the car a techie look. Something to talk about. The car's an engineering device, let's see some engineering thingies. The aluminum panel was designed in SolidWorks and cut out of billet, bead blasted, clear annodized, and then the labels for the switches were milled into the front using a font matching the VW cluster. Little details like the holes having flat sides so the switches don't spin and exactly matching the contour of the dash added time to the project. Several versions were made out of styrofoam first to get the layout and lighting right. From the back, the panel reminds me of the 1970s McLaren CanAm cars. The first thing I did when I got the car was to cut the hole in the back for the engine. Made a fancy jig out of a tripod, a rod, and a lawnmower wheel to mark out the cut and went at it with a pneumatic saw. Then finished it off with jeweler's files. No paint required. Didn't even chip. The hole was tricky because it goes through 3 layers (bumper and two layers of metal) and it's a circle projected onto angled surfaces. Just finding the centerline of the car wasn't trivial. Worrying what my neighbors would say if I ruined the back of a brand-new car made me REAL careful. I believe the hole is within 2 mm. There are three gauges for the jet: %RPM, Oil Pressure, and Turbine Inlet Temperature. The most important is turbine inlet temperature. If you exceed about 650 degrees C for very long, you damage the engine. This is critical on start-up. You don't want a \"hot-start\". The throttle for the jet engine is located next to the gear selector. It is a lever and has three buttons: Cool, Big-Fire, and Afterburner. \"Cool\" leans out the engine and is used to lower the turbine inlet temperature if you get a hot-start. To light big-fire or the afterburner, you hold a button down and 1/2 second later, press the hot-streak button on the floor. Then things happen! Notice the kerosene level gauge in front of the gear selector (jet fuel is mostly kerosene) and the bud vase missing a rose. Where did it go? Lotsa stuff back here. The force from the jet is tied to the vehicle through sandwich plates inside the car bolted to contoured aluminum billets that were slid into the frame rails. You can see the billet on the left side with a hole in its center, welded to the plate with 4 bolts. Used helium as the inert gas and a lot of current to weld that chunk of aluminum. To return the car to its production height, adjustable spring perches were used. Same spring rate, just corrected the ride height. Drives and handles fine. Kerosene is stored in a custom 14 gallon, baffled, foam-filled kevlar fuel cell in the spare tire well. Two fuel exits in the back: a -12 on the left side and a -10 on the right. The -10 goes to a shutoff, then a Barry Grant pump (one of the few hot rod parts on the car), then up into the car where it sees a filter, a regulator, and an electrical shutoff valve before feeding the engine. The -12 goes into a shutoff, then a 1.5 hp, 11,000 rpm, 24V custom electric pump. Pump is magnesium and can maintain 100 psi at 550 gph. From the pump it goes into the car to a filter, then a large regulator, and then to the afterburner solenoid and the big-fire solenoid (to left of pump and feeding bottom of tailpipe through orange covered hose). Fuel system was tested for flow capability. Above the big pump you can see the relocated gasoline cap actuator and all that black stuff on the right side is the stock fuel evaporative control equipment. All circuits feeding solenoids and pumps have fuses, relays, kick-back diodes to minimize contact arcing, sealed connectors, and use automotive wires of a gauge giving a maximum of 1V drop over the circuit loop. The engine is a General Electric Model T58-8F. This is a helicopter turboshaft engine that was converted to a jet engine by some internal modifications and a custom tailpipe. The engine spins up to 26,000 RPM (idle is 13,000 RPM), draws air at 11,000 CFM, and is rated at 1350 hp. It weighs only 300 lbm. It grows as it warms up so the engine mounts have to account for this. The mounts in the front are rubber and the back are sliding mounts on rubber. The structure holding the engine was designed using finite element analysis and is redundant. Strong, damage tolerant, and light. Second battery and fuse/relay panel on the right, halon fire system and 5 gallon dry sump tank on left. 24V starter motor is in the nose of the engine. 700 A of current goes into that motor for 20 seconds during start-up. Due to heat, must limit starts to three in one hour. Big screen is to avoid FOD (foreign object damage). Jet keeps sucking the rose out of the bud vase on the dash! A lot of attention to details in the car. Note the aluminum block holding/protecting the halon gas line, pull line, harness to engine, and oil pressure line. Rectangular tank under inlet screen is for various fuel drains. Note temperature gauge and shutoff valve for dry sump tank. 3 gallons of turbine oil at $25/quart (ouch!). Two-stage PPG paint matching exterior of car was used inside the car. It is not easy to paint around a lot of bars, etc while crouched in a car, in your dusty home garage, avoiding drips, and with your wife screaming that the fumes will cause brain damage in the kids. Especially with two-stage where you have multiple coats and critical drying times. Kids passed their grades so I guess damage was minimal, but more importantly, the paint turned out great! Street racing action. The other guy wimped out after a few \"big-fire\" demonstrations. What you see in the picture is about one-twentieth the full size of the fireball. Guy standing beside car had never seen it run before and was smiling ear-to-ear throughout the show. Had I launched, I would have burned him to a crisp. Well, live and learn. We get this a lot. A police officer picking at his nose while trying to figure out what to charge me with. Notice the hopeful anticipation of us on the right. We're rooting for him and offer suggestions but unfortunately, the California Department of Motor Vehicles did not anticipate such a vehicle so he's out of luck. Hmmm, the car has two engines making the car a hybrid so maybe we can drive in the commuter lanes along with the Toyota Priuses. *** Update 7/18/06 *** You have to give the California Department of Motor Vehicles (the DMV) credit for creativity on this one. A DMV insider has disclosed to me that the DMV has made a formal request to a federal agency to rule if my Beetle constitutes a threat to national security based on what could happen if it got into the wrong hands. This raises three questions in my mind: #1 Does this mean I’m the right hands? #2 If someone with the name \"b_laden13\" is the highest eBay bidder for my Beetle can I refuse his offer even if he has the prestigious eBay Red Shooting Star feedback rating (the highest)? #3 Would this affect my eBay rating? The car was built in this garage. Paint, welding, everything except some mill work. That's me standing beside the engine that is out of the car for some fuel controller work. The orange line is for the afterburner. There's one on the other side too. Here you can make out the four rows of variable inlets/stators at the front of the engine. Their angle changes with engine speed and are used to avoid compressor stall. There are 11 compressor stages and 2 turbine stages. The engine's pressure ratio is 8.3:1. That's how you work on a jet engine. Stick it on its end. Easy to store them that way too. Here's my wife's Honda Metropolitan scooter. She wants it to go faster than 40 mph. So I have these two little JFS 100 jet engines and I am thinking how to put them on the scooter. Engines are 50 lbm each so weight is an issue. Will probably use air-start with a carbon fiber tank of compressed air. That saves weight since batteries will then not be needed. Looks cool from the top. Will want to make aluminum housings to go over the engines just like on a DC-9. Bitchin' from the back too. Should get the scooter going. On one jet engine alone, this engine will get a kart up to 60 mph. Looks like I have a lot of spare wire left over from the Beetle job to do the scooter. Electronics Research Lab, Automotive Innovation Laboratory, VAIL",
    "commentLink": "https://news.ycombinator.com/item?id=37778531",
    "commentBody": "Ron Patrick&#x27;s Street-Legal Jet Powered Volkswagen Beetle (2006)Hacker NewspastloginRon Patrick&#x27;s Street-Legal Jet Powered Volkswagen Beetle (2006) (ronpatrickstuff.com) 203 points by 1317 20 hours ago| hidepastfavorite121 comments hrichards 17 hours agoSeems like a good time to ask this question that has been bugging me forever, since all the HN jet nerds will be drawn to this thread:Why hasn&#x27;t anyone made a hybrid car that uses a gasoline-powered turbine generator to charge its batteries instead of a piston engine?I&#x27;d imagine that hooking up such an engine directly to the drivetrain like in a Prius would be difficult, but surely a small turbine with one hell of a muffler running a generator (similar to a natural gas power plant), both running only at their peak efficiency RPMs, would yield a very efficient car that could still use the extant gasoline infrastructure.I&#x27;m sure there are very interesting reasons, either due cost, noise, reliability, or durability, that this idea hasn&#x27;t taken off, and I&#x27;m very interested to hear y&#x27;all&#x27;s thoughts on the subject. Or maybe there has been progress in this area, and I&#x27;d love to see some links! reply usrusr 16 hours agoparentI&#x27;ve been wondering about this very same question a lot myself and accidentally stumbled across the answer just a few days ago: Efficiency of turbines smaller than grid scale is simply not anywhere close to what piston engines can do at e.g. car size. Even at naval scale, turbines only win in use cases where power density is more important than fuel efficiency. Helicopters are deep in the (specific) camp of power density beats efficiency because carrying a heavier but more efficient engine would easily eat the fuel savings. Fixed wing aircraft gain range by climbing high, but up there both efficiency and power density of piston engines decreases dramatically with decreasing air density, so they are also in the camp of power density over efficiency (turbines are also affected, but not quite as much).Note that despite all this, the Otto Aviation 500L that is all about fuel efficiency at high altitudes uses a piston engine (they probably put a lot of effort into their turbocharger, those can lessen the impact of thin air) reply anjel 14 hours agorootparentCelebrity and Cunard built a few Cruise ships that use turbines to generate power to its electric motor propulsion.[1] They regret the endeavor owing to cost of operation.[2][1]https:&#x2F;&#x2F;www.ge.com&#x2F;gas-power&#x2F;industries&#x2F;cruise-lines[2]https:&#x2F;&#x2F;www.airliners.net&#x2F;forum&#x2F;viewtopic.php?t=1202425 reply jabl 3 hours agorootparentprevAnother reason many navies like turbines is that big diesels produce a lot of low frequency noise which travels very far underwater, so submarines can hear you coming from a very long distance.Although many navies do operate combinations of diesels and gas turbines (CODAG etc.), in such cases the diesels need to be installed on special shock absorbing mountings, in some cases even with a diesel-electric drive to avoid coupling the vibrations of the engine to the hull via the propeller shaft. reply KennyBlanken 11 hours agorootparentprevThe bit about the naval use and efficiency isn&#x27;t quite accurate.The issue isn&#x27;t that they&#x27;re not efficient. It&#x27;s that they are only efficient at high power level, and the minimum power level they&#x27;re efficient at (and even their minimum power level, period) - is quite high. To compare: Britain&#x27;s current aircraft carrier has four diesels that total 40MW...combined those diesels equal one of its two 40MW turbines.This minimum power level is why jet airplanes have an APU, and often taxi with just one engine running, with the second started up with enough time to get up to operating temperature for takeoff.Example: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rolls-Royce_MT30~40MW, minimum efficient power level 25MW.25MW, even if it&#x27;s very efficient in terms of turning kerosene into shaft power, means the ship is moving really fast, and thus there&#x27;s enormous fuel consumption and drag. reply usrusr 10 hours agorootparentI don&#x27;t think that it&#x27;s just inefficiency at certain power levels:The big table on https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Brake-specific_fuel_consumptio... has the Marine Trent rated less efficient than some car engines. And that table is peak efficiency, not efficiency at some arbitrary inconvenient power level.But you are right in so far as efficiency at the low end of the power range is very important in commercial shipping. Because the energy demand (at the drive shaft) of a given trip is highly dependent on the desired speed and if going show ruins your engine&#x27;s efficiency any dip in demand will ruin you economically (because others on the market are much better at compensating the dip by \"slow steaming\") reply RachelF 1 hour agoparentprevI worked for a company that did this in the 1990&#x27;s. Small gas turbine, the size of two show boxes (200hp) powering a generator, which charged the batteries and&#x2F;or electric motors.The advantages are efficiency, low weight and no gearbox is needed, as the turbine spun at over 100,000 rpm and the generator was fine with that.The main disadvantages were noise and turbines need more maintenance than piston engines. No LiIon batteries back then either.The project only built 1 finished prototype. reply mschuster91 11 hours agoparentprev> Why hasn&#x27;t anyone made a hybrid car that uses a gasoline-powered turbine generator to charge its batteries instead of a piston engine?Because turbine engines have some pretty serious downsides. Compared to a piston engine, they guzzle fuel [1], they&#x27;re pretty complex to repair (which is one of the problems Ukraine is facing), they spin at absurd RPMs which means that they need some serious housing to not turn into a shrapnel dispenser in case of an engine failure or accident, and they produce an awful lot of hot exhaust gas at high velocity that needs to be dissipated somewhere - down isn&#x27;t OK because it will melt the asphalt, sideways is not OK because it will melt or injure anyone and anything next to the car, and upwards carries serious risks as well (e.g. if you&#x27;re in a tunnel).[1] https:&#x2F;&#x2F;www.augsburger-allgemeine.de&#x2F;politik&#x2F;panzer-vergleic... reply Lanrei 2 hours agoparentprevYou&#x27;d be better off with an engine specifically designed to be efficient. Turbines are great at many things, but they are complex and don&#x27;t scale down very well. They were primarily used where high torque and power is required, and they just aren&#x27;t practical outside of that.A better option would be a constant speed ICE engine running at it&#x27;s peak efficiency (like a diesel-electric train). reply Animats 7 hours agoparentprevIt&#x27;s been done a few times, as listed here.One not mentioned was someone in Southern California who got an old aircraft APU (probably a Solar T-62[1] or similar) and made an electric hybrid back in the 1980s. The APU charged the battery, and shut down when the battery was full. Worked OK, apparently.The trouble with turbine engines is that, below bizjet size, they don&#x27;t seem to get any cheaper. Not for lack of trying in the 1990s. NASA, Williams International, and Eclipse tried hard. There were a few prototype planes, but no commercial success. There are today what are called \"very light jets\", but this means 4-6 people and and a price around $2 million.General aviation is still mostly piston-powered. There are tiny jet engines for R&#x2F;C planes, but they have very short operating lives.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Solar_T62 reply h2odragon 16 hours agoparentprevi think even the tiny, model jet engines used in radio control planes are both very hot, and move lots of air. both of which are hard to tame to the point of making them comfortable to coexist with on a city street in large herds.I still want one. direct the exhaust forward, dump in a little extra fuel, and instant snowblower &#x2F; flamethrower. Makes that pesky crosswalk crowd just melt away. reply rainbowzootsuit 13 hours agorootparentIt&#x27;s a(t least one) thing. Test run before the snow gets too deep:Jet Powered Blower https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=BcEkt5vQTVQ reply jacquesm 14 hours agorootparentprevAnd turns it into a skating rink five minutes later. You want the snow to go away, not to change into liquid water on a sub-zero stone substrate. reply h2odragon 13 hours agorootparentExcellent point, but i must rebut: (a) i&#x27;m already on my way by then, and (b) FIRE!reply salty_biscuits 11 hours agoparentprevBut they havehttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Jaguar_C-X75Turbines get more efficient as they get bigger. reply jgable 9 hours agoparentprevWrightspeed did this a while (10 years?) ago. It looks like they have since pivoted to fully electric powertrains for buses, but when they first started they were doing range-extending hybrid powertrains for heavy trucks. I found an article the describes the system at the time: https:&#x2F;&#x2F;www.e-hike.net&#x2F;tr&#x2F;content&#x2F;wrightspeed-unveils-new-turbine-range-extender-medium-and-heavy-duty-electric-powertrains-30With a range-extender hybrid system, you can keep the turbine closer to its peak-efficiency operating point, since it only has to handle steady-state load while the battery takes up the spikes. Not sure how it would do up a long grade, but I imagine they designed for that. reply clucas 16 hours agoparentprevHaving an ICE that drives a generator (alternator?) to power a traction motor(s), without being mechanically linked to the drivetrain, is how diesel locomotives operate. I believe the concept also has been (is being?) explored for linehaul trucks. But I&#x27;m not sure what constraints there are on passenger vehicles though... I&#x27;m also curious.If I had to guess, I would bet that the constraints are more commercial than physical... hybrids are already very efficient, so the market for such a vehicle would probably not justify the engineering costs. But that&#x27;s just a guess! reply linkjuice4all 14 hours agorootparentUnion Pacific tried using turbines in the 50s[0] but fuel consumption was an issue (I think they had to keep the turbine idling and maybe throttling wasn&#x27;t as easy?). Also mentioned in the wiki article was the low-grade fuel they were used was able to be used for plastic manufacturing instead of just burning it.The bigger difference between locomotive applications and GPs question is around charging batteries as opposed to running motors or directly turning the wheels. Efficiency of the smaller turbine is mentioned in another comment - but I have to imagine you&#x27;d also see some loses going from turbine to generator to battery and then to electric motor.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Union_Pacific_GTELs reply KennyBlanken 11 hours agorootparentDiesels idle pretty efficiently whereas jet turbines have very high minimum amounts of power.In terms of efficiency losses between the turbine, generator, battery, and motor - no, not really; look at most modern hybrid cars.Lithium ion battery coulombic efficiency is in the high 90&#x27;s percentage-wise, by the way.I would imagine that we might see hybrid turbine locomotives as battery power density and cost continue to improve. reply teeray 10 hours agorootparentprevThe MBTA uses jet engines as giant hairdryers to melt snow off the Mattapan High Speed Line [0]. I’m not sure if it’s also serving as the prime mover, but even if not it’s still an interesting and unusual application.[0] https:&#x2F;&#x2F;youtu.be&#x2F;DL6jKOCeVec reply jabl 13 hours agoparentprevAs mentioned in a sibling comment, turbines don&#x27;t scale down very well. Boundary layer friction gets relatively worse for smaller turbines, and AFAIU small turbines have relatively larger inefficiency due to air leaking past between the blades and the casing, etc.There&#x27;s a couple of companies working on recuperated turbine engines for small aircraft in the few hundred kW range, remains to be seen whether any of these will succeed. reply thebutcher 17 hours agoparentprevI think someone is making this, or maybe I misunderstood your question. The Ariel Hipercar uses a jet engine to power it’s 4 electric engines. I think it’s just used as a range extender, and last I saw they didn’t have it working yet. It’s been a while since I checked up on the car.EDIT: I just read this article from 2023 that says the turbine engine still isn’t working: https:&#x2F;&#x2F;www.evo.co.uk&#x2F;ariel&#x2F;206120&#x2F;ariel-hipercar-prototype-... reply iancmceachern 11 hours agoparentprevThe military has made several \"micro turbine generators\" Here is a report on one:https:&#x2F;&#x2F;apps.dtic.mil&#x2F;sti&#x2F;citations&#x2F;ADA515623My guess the answer to your question would be cost. reply TuesdayNights 16 hours agoparentprevArnold Schwarzenegger’s Hummer from the early 2000’s is setup like this — 100mpg with bio-diesel. I think this is the original article I read about his car’s creator, Johnathan Goodwin, from years ago.https:&#x2F;&#x2F;www.autoblog.com&#x2F;amp&#x2F;2007&#x2F;10&#x2F;20&#x2F;biodiesel-turbine-su... reply xattt 17 hours agoparentprevI don’t have the source at hand, but a gas turbine is ridiculously inefficient for variable loads. At idle, fuel consumption can be ~35% of what it is at full power.It would only need to charge for short time, and subsequently shut off. When a charge top-up is required, startup would be another rigamarole.Now, rotary engines, that’s a different story… reply playworker 13 hours agorootparenthttps:&#x2F;&#x2F;www.mazda.co.uk&#x2F;cars&#x2F;mazda-mx-30-r-ev&#x2F; reply Scene_Cast2 15 hours agoparentprevTo add to the discussion - the M1 Abrams tank uses a turbine. I don&#x27;t know how that&#x27;s linked up to the tracks though. reply efitz 15 hours agorootparentFormer M1 Abrams crewman (19K). It has an automatic transmission. reply shagie 14 hours agorootparentprevI recently stumbled into a couple of YouTube videos on turret design for tanks.* What actually IS an “Oscillating” turret? https:&#x2F;&#x2F;youtu.be&#x2F;46k7uhPHpLY* What happened to Rear-Mounted Turrets? https:&#x2F;&#x2F;youtu.be&#x2F;g5DOf2eZW3YAt 4:24 in the rear mounted turret video it touches on the aspects of modern transmission. reply ithkuil 12 hours agorootparentprevInterestingly, that engine has good power to weight ratio, has a better noise profile (higher pitched noise, that doesn&#x27;t transmit far, albeit louder locally), can operate with a variety of fuels, and can handle arctic conditions.The downside is that it consumes 50% more fuel than a comparable diesel engine. reply mr_toad 11 hours agorootparentprevThe turboshaft is linked to reduction gearing. I’m not sure how it’s linked to the sprocket that drives the tracks but it’s probably a chain, like on a motorcycle. reply mixmastamyk 16 hours agoparentprevSounds like you just described a hybrid, only needs a larger tank. reply latchkey 18 hours agoprevDiscussions on similar submissions:Jet Powered Volkswagen Beetle https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28349589 (August 29, 2021 — 2 points, 1 comments)Ron Patrick&#x27;s Street-Legal Jet Powered Volkswagen Beetle (2006) https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16513835 (March 4, 2018 — 156 points, 60 comments)Street-Legal Jet Powered Volkswagen Beetle https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5384390 (March 15, 2013 — 6 points, 2 comments)Street Legal Jet Powered Beetle (2006) https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=831185 (September 18, 2009 — 76 points, 23 comments) reply dang 16 hours agoparentThanks! Here&#x27;s a great subthread from one of those: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16524533(I just added it to https:&#x2F;&#x2F;news.ycombinator.com&#x2F;highlights) reply latchkey 15 hours agorootparentnext [4 more] @dang I wonder why we don&#x27;t just generate these things at the bottom of every article? My extension does it already, but I feel like it would help a lot as a core feature. reply dang 14 hours agorootparentThe lists that I post, and that most users post, are reviewed to include only the interesting threads. This makes them more valuable to readers, since the odds of going on a click trip to something boring are much lower.Rather than autogenerating them, I think what we&#x27;ll do is add software support for the community to collaborate on the &#x27;related &#x27;list for a post. And it needn&#x27;t just link to related HN threads - it can be related URLs on the same story, for example.When we&#x27;ll actually get to this is another question of course... reply latchkey 14 hours agorootparentGood points. Previously [0], in your list, it doesn&#x27;t include the number of points, as well as the calendar day, which is something that my generator includes and helps prevent click trips.To filter further, the list could just include posts with some points + comments ratio math. Also filter out similar posts within a short timeframe, if two posts happen within a week of each other, pick the one with the better ratio math.No need to involve the community in moderation a second time, since they&#x27;ve already involved themselves with points&#x2F;comments to begin with.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37768936 reply hansoolo 12 hours agorootparentprevWouldn&#x27;t help me unfortunately, as I am reading HN on an android app. But good idea. reply overlyambitious 1 hour agoparentprevand still no videos online.... fishy reply generalizations 18 hours agoprev> You have to give the California Department of Motor Vehicles (the DMV) credit for creativity on this one. A DMV insider has disclosed to me that the DMV has made a formal request to a federal agency to rule if my Beetle constitutes a threat to national security based on what could happen if it got into the wrong hands. This raises three questions in my mind: #1 Does this mean I’m the right hands? #2 If someone with the name \"b_laden13\" is the highest eBay bidder for my Beetle can I refuse his offer even if he has the prestigious eBay Red Shooting Star feedback rating (the highest)? #3 Would this affect my eBay rating?Wonder if they ever found a way to give the guy a ticket. reply smohnot 18 hours agoprevHe had it listed on Craigslist a few years ago... for $550k. Anyone know the current status of it?https:&#x2F;&#x2F;www.autoevolution.com&#x2F;news&#x2F;get-yourself-the-iconic-j... reply stergios 17 hours agoparentIt&#x27;s still sitting in his shop. RP is not going to sell it. reply KennyBlanken 11 hours agorootparentI think it&#x27;s more that like most modded &#x2F; custom car owners, he thinks that it&#x27;s reasonable to set an asking price according to the following formula:(Cost of car) + (Cost of Mods) + (My time x some magical hourly rate) = reasonable asking priceThe more sane of them add in a multiplier, like say .5 to .8. Or leave out their time.Reality is:What 1 person among the people who hear about the sale will pay = reasonable asking price reply jamiek88 10 hours agorootparentYeah things like that are always the same.It’s the sellers dream, rarely anyone else has the exact same dream.If they have a similar dream all they see is the differences and what they’d do differently. reply omginternets 19 hours agoprevHow on earth is it street legal to emit a high-pressure plume of jet exhaust behind you? reply wkat4242 1 hour agoparentJet exhaust doesn&#x27;t need to be high pressure. It depends. For example a turboprop engine doesn&#x27;t provide any jet thrust and usually the exhaust is even pointed away from the direction of flight.But it is very hot so exhausting it close to the ground or people is a serious problem. reply dzdt 19 hours agoparentprevIts street legal to drive using the standard motor with the jet engine OFF. reply omginternets 19 hours agorootparentOooh, it&#x27;s a hybrid! reply dylan604 18 hours agorootparent\"Hmmm, the car has two engines making the car a hybrid so maybe we can drive in the commuter lanes along with the Toyota Priuses. \" reply omginternets 18 hours agorootparentImagine being the cop who has to argue over the ticket with that guy ^^ reply dylan604 17 hours agorootparentThere&#x27;s a paragraph and photo of this very thing in the TFA reply ceejayoz 17 hours agorootparentprev\"I&#x27;m... gonna park a little further back.\" reply aidos 18 hours agorootparentprev> That doesn&#x27;t stop me from the occasional blast on the highway though. reply skeaker 12 hours agorootparentSomething being road-illegal doesn&#x27;t stop you from doing it anyways... reply mindslight 19 hours agoparentprevWe live in a society where things are legal by default. Why would adding a jet engine to a car be a priori illegal? If he harms somebody or otherwise causes damage, that itself is what&#x27;s illegal and he&#x27;d be liable regardless of motor vehicle regulations. If this mod became a larger trend, especially consumer-available, then regulations would be implemented to head it off. But for a few lone instances it&#x27;s not particularly necessary. reply olyjohn 17 hours agorootparentIn this case the law has already been written. It&#x27;s not a CARB-approved, nor EPA compliant engine. You&#x27;re not allowed to run it on the street. The law is written so that everything you do to a car&#x27;s emission system is illegal by default. To make any engine modifications in California, the part must be CARB approved and have a compliance sticker on it. Engine swaps in California are legal, as long as the engine being swapped is at least as new as the engine in the vehicle and meets the same regulatory requirements. Which means you swap in another EPA&#x2F;CARB approved engine, but not a jet engine. reply lawlessone 19 hours agorootparentprevI am not in the US, but where i am afaik anything that modifies the car like this would have to go through some sort of recertification process.It would be perfectly legal on private property but not public roads reply throwaway20304 18 hours agorootparentWell not sure where you are, but where in EU I am, you can modify the insides of your car (people attach entire apartments to the insides of their cars...), and there&#x27;s nothing wrong about stuff sticking outside - you just need to attach a red flag if it&#x27;s over 1.5m out of the car (maybe red flame would be enough?).The modified rear door might be a problem, but where I am you could simply keep the original open, or detach it. reply jamiek88 10 hours agorootparentRoad traffic laws are absolutely not harmonized across the EU.French, Portuguese and German rules of the road for example are all different in statutory strictness and enforcement strictness. reply shortcake27 18 hours agorootparentprev20 years ago in Australia my car got canaried because I had LPG and a pod filter, which was illegal because you were only allowed 1 modification to the intake system. I am 100% confident that if I strapped a jet to the car, it would be illegal. As it should be. If you want to do extremely dangerous modifications, do it on your own property. Not a public road where you risk killing a family of 5. reply historyTeach123 12 hours agorootparentHe didn&#x27;t modify the car&#x27;s original factory system. He simply added onto it, he added a second independent system.Tbh I kinda agree this is a bit ridiculous to assume it&#x27;s safe to drive on the road though. reply mindslight 17 hours agorootparentprevYou&#x27;re speaking from an Australian perspective about what is, to make an otherwise unsupported argument about what should be in the US. I&#x27;ll be one of the first to point out problems and blindspots from the American conception of \"freedom\", but in this case it seems highly appropriate. You yourself even got bit by overregulation for something seemingly reasonable and forward-looking, and yet you&#x27;re still reflexively defending it!In my estimation your example \"family of 5\" is at much more risk from widespread unnecessarily-high bro-dozer trucks than a single engineer personally adding a jet engine to his car while seemingly being very in touch with the dangers of operating it. In fact given the severe disparity in other vehicle crash survivability statistics between coupes and trucks, I&#x27;ve got to wonder if this car isn&#x27;t still individually safer than a casually-driven pickup truck. reply shortcake27 2 hours agorootparent> a single engineer personally adding a jet engine to his car while seemingly being very in touch with the dangers of operating itWhat if a non-engineer who doesn’t understand the dangers makes this type of modification? This is is why it needs to be illegal. Just because someone straps a jet engine to their car it doesn’t mean they know what they’re doing, and _that_ is the risk, and why pretty much all laws exist in the first place. Many people could drive safely without speed limits, but we have speed limits to cater for people who can’t. My point is that if this type of modification truly is legal in America, that’s scary, and Australia got the laws right. A jet-powered car is not inherently safer in the US than Australia. reply userbinator 10 hours agorootparentprevIt is likely to be safer simply because the one driving it is going to be a lot more careful than the average driver.I recall seeing a study a while ago that showed how those who work on their own cars, especially for those who do major jobs like engine rebuilds, are statistically far less likely to get into an accident. Unfortunately I can&#x27;t find it now. reply gafferongames 17 hours agorootparentprevAs an Australian-American living in the US I can confirm that the Australian concept of law (aka Nanny State) would ensure that this modification is illegal by default. Think of the children. reply jamiek88 10 hours agorootparentI was shocked as a Brit American living in the US how nanny state Aust is. Even more than Britain. Certainly when it comes to cars and driving. The whole ‘anti hoon’ thing seems shocking to me with my now mostly American sensibilities.But I guess that’s why we both live here rather than blighty or down under. reply shortcake27 3 hours agorootparentIt’s important to understand most things happen for a reason, and as a Brit American living in the US you might be missing some context.In the area I grew up, it was expected that your first car would be a V8. Hooning was ingrained into the culture. Back then it was a single spinner, no ABS, no airbags, basically no safety features at all. So what would happen is someone would get their licence, grab as many mates as they could, drive as fast as they could, and wrap their car around a tree killing everyone. Every week there was a news story about a multiple fatality P plate accident caused by hooning.A culture of dangerous driving and dangerous vehicles led to Australia’s anti-hoon laws. It’s not just “ooh silly nanny state laws for no reason”. reply defrost 9 hours agorootparentprevAustralia&#x27;s just fine with adding jet engines to cars and designing|building one million round per minute guns.It&#x27;s the mixing up of such things with the general unconsenting public that raises an eyebrow.Hoon&#x27;s can hoon - just out of earshot of people that want a quiet life and off the community car parks and roadways - there&#x27;s no shortage of private land and designated drag stripsraceways.https:&#x2F;&#x2F;aussieinvader.com&#x2F;the-people&#x2F;rosco-mcglashan-oam&#x2F;https:&#x2F;&#x2F;thewest.com.au&#x2F;lifestyle&#x2F;motoring&#x2F;wa-built-1600kmh-j...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Metal_Storm reply civilitty 17 hours agorootparentprevChildren don&#x27;t really fit in the intake so it shouldn&#x27;t be a problem reply cortesoft 15 hours agorootparentprevIn California, you have to register your car and have it pass a smog test. How would this pass that? reply mhb 5 hours agorootparentAren&#x27;t \"inspections\" nowadays done only by plugging into the OBD port? So, no problem? reply jamiek88 10 hours agorootparentprevI’d hate to be the minimum wage tech tying to figure that out! reply btilly 14 hours agorootparentprevJust drive on the gasoline engine. reply lmm 7 hours agorootparentDoesn&#x27;t the smog test require you to run your car at the peak power it can do, or something like that? reply btilly 5 hours agorootparentFloor the accelerator, yes.Find a toggle for an unrelated afterburner? Not so much. replynikanj 19 hours agoparentprev’Murica reply xrd 19 hours agoprevThis should be retitled \"Now I&#x27;m going to finish adding jet engines to my wife&#x27;s scooter.\" reply mindslight 18 hours agoparentThe scooter seems like it could be a poor idea due to asymmetric thrust, and I have to wonder if he just mocked it up in jest. It does look pretty awesome though. reply whatshisface 4 hours agorootparentYou could adjust the two throttles to trim it, preferably not while in motion. reply ed_mercer 10 hours agoprev> The Beetle was chosen because it looks cool with the jet and it shows it off well.Subjective, but I would argue that the Beetle is a terrible choice and makes the jet engine look like a pole sticking out of a ball. A flat sports car would probably fit the look much better and mimic the aerodynamics of an aircraft. reply usrusr 15 hours agoprev\"The car has two engines: the production gasoline engine in the front driving the front wheels and the jet engine in the back.\"Careful wording to give the impression that the drive shaft of the helicopter turbine would be connected to the rear wheels, without actually claiming that it is. So it&#x27;s a car with a large flame thrower in the back, minor The Boring Company vibes.Well possible that the author might have had more fun writing than building&#x2F;driving. (I do love the incredulous tone of \"#1 Does this mean I’m the right hands?\") reply UniverseHacker 12 hours agoparentIt produces thrust in the normal way a jet engine aircraft does... with high exhaust velocity. From the videos you can see shock diamonds, so it is producing supersonic exhaust. reply alright2565 15 hours agoparentprevI don&#x27;t think so. In another part he mentions> This is a helicopter turboshaft engine that was converted to a jet engineI can&#x27;t see a way to get rotary power out of this engine after the modifications. reply usrusr 1 hour agorootparentAny turboshaft engine is a jet engine if you don&#x27;t connect anything to the shaft and point the exhaust in the right direction. Just not a very good one, at zero bypass.If you think there is anything done in this project to increase performance you are missing the point of the joke: woah, jet engine! Woah, rated 1350 horse power! Heh, retro novelty compact with a truly glorious amount of hot air.Will the turbine exhaust noticeable push the car? Sure. Even a piston engine exhaust does, e.g. some versions of the Merlin engine claim extra 70 HP from exhaust thrust.Is the author having great fun (and readers who get it) by brandishing the rated rotary power of the turbine over and over again, while carefully avoiding any mention of actual performance of the car? Absolutely. But it&#x27;s not mean deception, it&#x27;s friendly trolling. Which is basically the essence of the entire hot rod idea. I dislike the waste (of a good car, and the occasional kerosene burn, the turbine was probably beyond airworthyness certification and thus scrap anyway), but I find his \"friendly trolling\" charming in a surprisingly deep way. reply ben7799 14 hours agoparentprevFor a stunt car like this there&#x27;s not a huge need or desire to have it powering the wheels at all.The direct air thrust will push the car just fine, doesn&#x27;t need a transmission, has zero issues with wheel spin or traction, and so then doesn&#x27;t require re-engineering the wheels&#x2F;tires and then the suspension and&#x2F;or chassis to handle 1350hp. reply jamiek88 10 hours agorootparentYeah imagine the rubber! They’d need American drag racing tyres! reply ynoxinul 19 hours agoprevApparently the post is from 2006. I wonder if this contraption is still street-legal. reply gpderetta 19 hours agoparentwhat I really want to know if he ever finished his scooter! reply K0balt 18 hours agoparentprevIt would be legal to run it on the factory engine, at which point the jet is just cargo. If you start the jet on a public road, it could be considered a public nuisance, reckless driving, etc.If a the vehicle presents a clear and present danger of any kind it is a-priory a ticketable offence at least. With the jet off, it poses no such danger. reply aidenn0 11 hours agorootparentAnd for California, don&#x27;t forget \"Exhibition of speed\" which is a rather nasty ticket to end up with. reply psychlops 17 hours agorootparentprevI&#x27;m certain an officer could find all sorts of reasons to ticket that vehicle if inspired. reply moate 16 hours agorootparentprevYou can own a car, you can own a flamethrower, you can&#x27;t fire your flamethrower out of your car while driving on public roads. reply ralfd 19 hours agoprevComments from 2018:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16513835 reply brk 19 hours agoprevThis is a hacker classic. Would be curious to know what eventually became of it. reply JoblessWonder 11 hours agoprevCan someone ELI20 how one would convert a turboshaft engine to a turbojet engine? reply mr_toad 11 hours agoparentBasically you just remove the shaft. reply aidenn0 11 hours agorootparentAnd put a nozzle on the exhaust. reply JoblessWonder 11 hours agorootparentprevAh, that makes sense. Thank you! reply Aurornis 18 hours agoprevAwesome project, but are there any actual videos of it running the jet engine?He says the jet engine moves 11,000 CFM of air, but that air can only come through the windows and the sunroof. Pulling 183 cubic feet of air per second through those little openings while sitting in the drivers seat isn&#x27;t going to work. Just try to do the math on how fast that air would have to be moving through those windows.Cool show piece though. reply samtho 18 hours agoparentThis not a typical low-bypass, cigar-type turbofan jet engine you see on airplanes, rather, it’s a modified turboshaft jet engine used for helicopters, intended to provide longitudinal rotational energy. The air it moves from the intake is a fraction of what it produces as the combustion process itself results in gasses being chemically formed. There is no propeller or fan on this engine - it’s closer to a rocket than what we think of as a jet. reply lmm 7 hours agorootparent> it’s a modified turboshaft jet engine used for helicopters, intended to provide longitudinal rotational energy. The air it moves from the intake is a fraction of what it produces as the combustion process itself results in gasses being chemically formed.It&#x27;s not a turbofan no but it&#x27;s a jet engine burning fuel the regular way (using air), like the ones on a B-52 or Concorde. Very different from a rocket engine that carries its own oxidiser. reply samtho 3 hours agorootparentI’m fully aware of the difference. The only parallel I’ve drawn is the fact that it uses gases as a result of the combustion as propulsion rather than spinning a fan. Whether it has its own oxidizer is irrelevant for the purposes of this analogy as I don’t see this vehicle achieving any sort of space flight. reply sokoloff 18 hours agoparentprevGive 6 sq ft for openings, that&#x27;s around 30 linear feet per second. ~30 feet per second is ~20 miles per hour. That&#x27;s a stiff breeze, but it doesn&#x27;t seem outrageous. reply ceejayoz 18 hours agoparentprevYes, there&#x27;s video online.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TCqxWhKe_tAhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5zNdkXwFQ3c(Noise warning, for obvious reasons.) reply avg_dev 17 hours agorootparentI see> I don&#x27;t know how fast the car will go and probably never will. The car was built to thrill me, not kill me. That doesn&#x27;t stop me from the occasional blast on the highway though.but I am unsure if that means he has never driven it using the jet engine, or whether the engine even will power the car or just kinda runs on its own. I&#x27;m curious to see it go at all under jet engine power.Edit: maybe this is it https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=R-ipYAl3o40 reply dmurray 17 hours agorootparentI think he means he&#x27;s used it plenty, but hasn&#x27;t tried to max out the speed. reply ceejayoz 15 hours agorootparentThat&#x27;s my reading as well.I have a vague memory of this site or an interview previously saying he got up to 130 mph once, before deciding he&#x27;d rather not find out what speed a VW Beetle lifts off the pavement. reply jefftk 15 hours agorootparentprevThat videos&#x27;s an original Beetle, not a New Beetle. reply rmason 14 hours agoprevHow did Jay Leno not get this car on his TV show when he had it? He still is posting episodes to YouTube so it is still possible. reply mywacaday 1 hour agoparentJay has his own jet bike, https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=WDhoYXw2IPc reply avg_dev 17 hours agoprevnice... does anyone remember a K Car with a supercar engine? that was not quite as crazy as this one, no jet engine, but it was nice. i can&#x27;t seem to find it but i remember reading about this \"sleeper car\". reply brucethemoose2 17 hours agoparentCrazy engine swaps are not uncommon. There are some Fiat 500s and such with v8 double motorcycle engines, wankels or even v12s and racecar v6s:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=A52W20Z38Bwhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=D6SZuionhqQA turbine is relatively exotic, if only because they are even more impractical. reply whynotmaybe 17 hours agorootparentEven a snowblower with a HEMI...https:&#x2F;&#x2F;www.ign.com&#x2F;articles&#x2F;2005&#x2F;06&#x2F;22&#x2F;that-thing-got-a-hem... reply aidenn0 11 hours agorootparentprevIIRC someone put a turbocharged 7L smallblock V8 in the back of a Lotus Exige, though they had to lengthen the chassis to make it fit. reply brucethemoose2 10 hours agorootparentThat is probably the Henessy Venom GT.My favorite is the Ariel Atom V8, which swapped the Honda Civic engine for a hilarious 483hp 3L NA V8. In a 1300lb car. reply aidenn0 5 hours agorootparentThe Venom GT was indeed what I was thinking of. 1451hp in a 2800lb car. replyMiserlou57 16 hours agoprevMy buddy (a car guy) from Mountain View told me he could hear this thing on 280 late at night every now and then. Anyone else? reply grecy 17 hours agoprevI&#x27;m a little shocked there&#x27;s no 1&#x2F;4 mile time.Surely when you strap a jet engine to a Beetle you need to find out how fast it goes! reply dotancohen 14 hours agoparentQuick, not fast. Low 1&#x2F;4 times are mostly associated with acceleration, not speed. reply joshu 10 hours agoprev [–] i know ron! reply dazhbog 7 hours agoparent [–] Tell him to make a YouTube channel or update us on that scooter ;) replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ron Patrick has constructed a road-legal, jet-powered Volkswagen Beetle, a personal project that contains both a gasoline engine for normal driving and a jet engine for an adrenaline rush.",
      "Although the car looks like a standard Beetle and is registered in California, engine modifications could prevent it from passing new car smog checks. It uses a repurposed helicopter turboshaft engine for its jet power.",
      "Despite its unconventional design, it has been well-received at car shows and has attracted significant attention. The California Department of Motor Vehicles is reviewing whether the dual-engine vehicle poses a security risk."
    ],
    "commentSummary": [
      "The discussion encompasses various topics such as the application of jet engines and turbines in different fields, including hybrid cars and vehicle modifications.",
      "Major topics include the efficiency and confines of turbines, along with the legal and safety concerns surrounding the fitting of a jet engine in a car.",
      "It also delves into examples of turbine-powered vehicles, their commercial triumph, cultural variations in regulations, and the technical nitty-gritty of altering a car with a jet engine."
    ],
    "points": 201,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1696513528
  },
  {
    "id": 37778771,
    "title": "Lenovo PC boss: 4 in 5 of our devices will be repairable by 2025",
    "originLink": "https://www.theregister.com/2023/10/05/lenovo_pc_boss_4_in/",
    "originBody": "SIGN IN / UP PERSONAL TECH Lenovo PC boss: 4 in 5 of our devices will be repairable by 2025 12 And the repair parts will be repairable too. Batteries, SSDs and more will no longer be sealed into casing Paul Kunert Thu 5 Oct 2023 // 13:45 UTC CANALYS EMEA FORUM Lenovo is forecasting that the vast majority of its devices will be repairable by 2025 – as will the repair parts themselves – but it is not intending to specify where customers should have their kit fixed. Talking on stage at the Canalys EMEA Forum 2023, Luca Rossi, senior vice resident at Lenovo and president of its Intelligent Devices Group, said the company has committed to a net zero emission policy by 2050, and analyzing the components used in its hardware is part of the equation. Europe's right-to-repair law asks hardware makers for fixes for up to 10 years READ MORE \"On repairability, we have a plan that by 2025 more than 80 percent of the repair parts will be repaired again so that they they enter into the circular economy to reduce the impact to the environment.\" He added: \"More than 80 percent of our devices will be able to be repaired at the customer, by the customer or by the channel and we are enabling this with a design for serviceability kind of approach.\" This means that \"batteries, SSD, many things, will not any longer be sealed into the product but will be available for the customer to be to repaired on site and then save a lot of waste.\" The world's largest PC maker doesn't have a terrible reputation in terms of the ease at which its gadgets can be mended, unlike Apple. The ThinkPad T14 Gen 3, for example, was assigned a fixability score of 7 out of 10 by Fixit; and the Framework 13 got top marks. Lenovo and other vendors aren't going to make hardware easier to fix out of the goodness of their own hearts: the European Council last month started to update EU rules to ensure consumers are better informed about the lifespan and repairability of the tech devices they buy. The requirements should be finalized before June next year. Right to repair advocates have a new opponent: Scientologists We all scream for ice cream – so why are McDonald's machines always broken? After years of fighting Right to Repair, Apple U-turns-ish in California Logitech, iFixit to offer parts to stop folks binning their computer mouse Chromebook expiration date, repair issues 'bad for people and planet' New York gets right-to-repair law – after some industry-friendly repairs to the rules Similar right to repair movements are also happening stateside, including in California and New York. Yet won't easing repairability hit Lenovo in the pocket as consumers and businesses, in theory at least, get a simpler solution to keep their devices running for longer? Canalys CEO Steve Brazier put this question to Rossi on stage. \"I think you cannot you cannot look at this in this way,\" he said, adding the future looks very rosy for tech companies. \"There are so many opportunities that frankly thinking I'm not worried that we will damage our business by doing what is right for the planet. Not at all.\" ® Whitepaper: Top 5 Tips For Navigating Your SASE Journey More about Laptop Lenovo Right to Repair More like these 12 COMMENTS TIP US OFF Send us news Other stories you might like LG has its own folding PC now, but good luck getting your hands on one The 'Gram Fold' looks limited to LG's home of Korea PERSONAL TECH 9 days6 Lenovo to offer Android PCs, starting with an all-in-one that can pack a Core i9 Another route to the year of Linux on the desktop. Or the edge PERSONAL TECH 6 hrs10 ASUS's Zenbook S 13 is light, fast, and immediately impressive DESKTOP TOURISM Taiwanese brand is a portability and productivity contender PERSONAL TECH 4 days12 If a college graduate can’t protect your data, you’re in trouble To achieve resiliency and efficiency, aim for simplicity SPONSORED FEATURE When is a PC an AI PC? Nobody seems to know or wants to tell CANALYS EMEA FORUM HP and Lenovo developing machines with LLMs – maybe it'll be in the price? AI + ML 18 hrs6 Google doubles minimum RAM and disk in 'Chromebook Plus' spec Some may be made in India, where the Big G has teamed to make kit with HP PERSONAL TECH 3 days24 Teardown reveals iPhone 15 to be series of questionable design decisions VIDEO High cost and hard to work with? Yep, that's Apple all over PERSONAL TECH 10 days53 FEMA to test emergency alert system US-wide today UPDATED Americans are used to drills :( CYBERSECURITY MONTH 2 days62 EU right to repair updates pass latest hurdle Makers won't be able to pull wool over consumers' eyes, though critics say it hasn't gone far enough PERSONAL TECH 15 days17 HP reveals bonkers $5k foldable tablet/laptop/desktop There’s a weird one-and-a-half screen laptop mode, too PERSONAL TECH 21 days27 Why Chromebooks are the new immortals of tech OPINION A decade of support is a much better deal than what Microsoft or Apple will give you OSES 14 days96 The iPhone 15 has a Goldilocks issue: Too big or too small. Maybe a case will make it just right QUICK LOOK Fanboi numbers are well down – but Apple's queueing system, rather than apathy, is likely the cause PERSONAL TECH 14 days83 The Register Biting the hand that feeds IT About Us Contact us Advertise with us Who we are Our Websites The Next Platform DevClass Blocks and Files Your Privacy Cookies Policy Privacy Policy T's & C's Do not sell my personal information Copyright. All rights reserved © 1998–2023",
    "commentLink": "https://news.ycombinator.com/item?id=37778771",
    "commentBody": "Lenovo PC boss: 4 in 5 of our devices will be repairable by 2025Hacker NewspastloginLenovo PC boss: 4 in 5 of our devices will be repairable by 2025 (theregister.com) 194 points by mikece 20 hours ago| hidepastfavorite163 comments autoexec 17 hours agoFriendly reminder that Lenovo has repeatedly shipped laptops infested with malware and backdoors (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lenovo#Security_and_privacy_in...) sometimes doing so in exchange for money. They&#x27;ve also hidden malware in UEFI so that even reformatting your hard drive wouldn&#x27;t help.In one case, after they were found out they first claimed \"we have thoroughly investigated this technology and do not find any evidence to substantiate security concerns\" and only after it started being reported by more and more news orgs did they finally admit to what they&#x27;d done and release instructions on how to fix it.Sadly, those instructions removed the bloatware, but left the vulnerability it introduced in place giving users a false sense of security, and only after they were caught out for that in the press did they finally release a \"removal tool\"The wikipedia page doesn&#x27;t even list all their offenses or the most recent events. See also : https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;lenovo-patches-uefi-vulnerabil...Repairable or not, use Lenovo at your own risk. reply infogulch 15 hours agoparentTo add a reference, here&#x27;s a reproduction of my comment on a discussion of Lenovo from 4 years ago [1]:I want to emphasize how bad the TLS MITM malware was (adware is too nice a term): they installed a TLS MITM attack by adding the same CA public key to the trust store of every non-business device they sold, and then proxied all internet traffic through an on-device MITM proxy that contained the private key to that CA. Yes you read that right: every device with this malware had the public and private key used to decrypt the TLS traffic of every other device with this malware, effectively exposing every user to have all of their traffic both decrypted and MITM&#x27;d again by anyone who tries. This train wreck was malicious and incompetent.I don&#x27;t consider this a technical failure, this is a fatal business failure: either nobody in Lenovo reviewed this software integration from a privacy and security perspective, or they did review it and the business deal overruled the security team&#x27;s ability to veto it, or they&#x27;re so inept that they didn&#x27;t notice how bad it was. In any case, this indicates an organizational dysfunction so severe there&#x27;s no way I can trust Lenovo with my personal or business security again.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22194428Lenovo now shipping Ubuntu on high end workstations in the US reply imglorp 17 hours agoparentprevWould the UEFI malware affect a non-windows OS installed on that machine? reply happymellon 16 hours agorootparentNo, and that&#x27;s due to 2 reasons.1. The malware was a rootkit for Windows2. Windows has an interesting \"feature\" that will blindly auto-install software that is embedded in a certain location in the BIOS. If a rootkit is placed there, Windows will always try and install it.It&#x27;s bad for Lenovo, but it&#x27;s also bad that Windows has this anti-feature. reply nope96 11 hours agorootparentRe: #2 - The UEFI bloatware&#x2F;rootkit checked the Windows version: if you used Windows 8 or up it used the \"feature\" called \"Windows Platform Binary Table\", but Windows 7 didn&#x27;t have that, so for Windows 7 it overwrote CHKDSK.EXE with a Lenovo loader on every reboot. So, yeah, they were overwriting OS binaries on disk, not just relying on WPBT. (But no reports of binaries being overwritten on on Linux) reply autoexec 9 hours agorootparentI&#x27;m surprised they got around windows file protection! That had been around since at least XP so I wonder what they did to avoid it.The writers of the malicious software lenovo has so far been caught including in their products were specifically targeting windows users, but I see no reason to think that they wouldn&#x27;t include malicious software that targets linux if some seedy company decided it was worth the effort to write it and offered lenovo enough cash. Their total lack of integrity and respect for their customers is enough that I could never feel fully confident that I was safe just because I was running a non-windows OS. reply DANmode 9 hours agorootparentWindows File Protection seems ineffective against malware signed by a Microsoft vendor.and&#x2F;or for malware that can disable WFP. reply valleyer 13 hours agorootparentprevWow. Could you point us to more technical information about this \"feature\" of Windows? I&#x27;m hoping someone has reverse-engineered it somewhere, but I&#x27;m not finding anything with some brief Googling. reply lights0123 13 hours agorootparentNo need to reverse engineer it when it&#x27;s fully documented: https:&#x2F;&#x2F;download.microsoft.com&#x2F;download&#x2F;8&#x2F;a&#x2F;2&#x2F;8a2fb72d-9b96-... reply valleyer 11 hours agorootparentThank you! reply autoexec 9 hours agorootparentOn the subject of this \"feature\", I found this handyhttps:&#x2F;&#x2F;michlstechblog.info&#x2F;blog&#x2F;windows-identify-a-wpbt-bin... reply vondur 13 hours agorootparentprevI have a motherboard from Gigabyte that will prompt after the first boot in Windows to install some of their software.https:&#x2F;&#x2F;arstechnica.com&#x2F;security&#x2F;2023&#x2F;06&#x2F;millions-of-pc-moth...With my motherboard you can deny the install at least. reply freedomben 17 hours agorootparentprevIt definitely could if they wanted it to. I&#x27;m less worried about that because of the threat of PR blowback that helps keep them in check, but if it&#x27;s really important to you then you should probably avoid Lenovo. Frame.work is quite a nice option. reply JTbane 15 hours agoparentprevAlso the build quality of Lenovo laptops have gone down so much it&#x27;s appalling. reply jtwaleson 13 hours agorootparentDisagree! I&#x27;ve been using Thinkpads since 2004 and was a bit obsessed with them around 2010 and almost had a museum with old machines I bought on eBay. I had two 701cs&#x27;s butterfly models etc. Every model I&#x27;ve had was much better than the one before in almost every way. There&#x27;s a lot of nostalgia with the older models, but man, compared to what we have today they were unusable and you would not carry something like that around! Super heavy, easily scratched rubber paint, bad plastic.Now a lot of things I miss a lot. The modularity, latches, ThinkLight, the upside down Thinkpad logo on the outside shell (oriented correctly towards the user, not the other people in the room), 7 row keyboard, good Fn keys functionality like media keys. The T400 that I regularly stood on to impress people how sturdy it was. But all of these things cost a lot in terms of weight and size (except the logo which was just copying the change that Apple made).Just so you know the context: these are the machines I used a lot. T42p, R60, T400, T440s, T470s, T495s, T14s Gen2 (current).If you respect the sturdiness vs. form factor trade-off, I believe build quality has gone up with every machine I bought. reply hakfoo 9 hours agorootparentEntry level Thinkpads are not particularly impressive.I have an E585 (bought it with the 2700U and 1080p screen upgrades, but added my own SSD and extra RAM because Lenovo charged the moon) and it replaced an X230 Tablet.Even though it&#x27;s the \"upgraded\" screen, it&#x27;s still dim and disappointing. I understand very few Thinkpads have excellent screens; while the X230T&#x27;s was brighter, it managed to burn the browser chrome after a few years of using it for lap-surfing.The X230 had a slide-off-the-back-side battery. The E585&#x27;s is inside the case. You could slide open little hatches to access the storage and memory on the X230. The E585 was a single-piece cover with 8 or 9 captive screws and then has to be pried apart with snaps.The E585 has a rubber foot to prop it to provide clearance for the vents; the tape holding it on gave out a couple months ago, so I&#x27;ve stuck little white adhesive rubber furniture feet on it. reply ToniCipriani 8 hours agorootparentEntry level ThinkPads aren&#x27;t really ThinkPads, the E-series in particular. Those ones were previously known as ThinkPad Edge, which were designed by a different team. reply jtwaleson 2 hours agorootparentExactly. Buy T, X or P series. For non-demanding users L series might be OK, but I prefer to buy second-hand &#x2F; refurbished T series instead. Avoid E series, or at least don&#x27;t expect any \"ThinkPad-ness\" from it. reply bityard 15 hours agorootparentprevI used to be a Thinkpad fanatic due to nice build quality and good Linux support.But in the past few years, I have taken to just buying Dell&#x27;s midrange business laptops. The build quality can vary by model but they are cheaper, ubiquitous, have great support while in warranty, and Linux always runs just fine on them if you pay attention to the hardware you&#x27;re buying. reply baz00 15 hours agorootparentOh man fuck Dell. I get through one every 9 months at work. Absolutely the worst laptops I&#x27;ve ever used. Precision series. reply RajT88 14 hours agorootparentWhat model? I&#x27;ve been pretty happy with Dell machines at work. Happy enough I bought an older model refurb for 200 dollars shipped, which fits the chargers and docks and such I had lying around from work.Model: Precision 7520 Cpu: i7-6820HQ Memory: 32gb Gpu: Nvidia Quadro M2200 4GB Storage: 500gb SSD Display: 15\" 1080pPretty good specs for the price. Serviceable for older&#x2F;lower requirements games. Pretty bulky machine though. reply baz00 13 hours agorootparent5550 - two complete thermal failures. 7670 has blown up two $400 docks, won&#x27;t charge half the time, blue screens once a week, weighs a ton, gets hotter than satan&#x27;s nuts. reply RajT88 13 hours agorootparentWild! I have not at all had any of those issues, and I&#x27;ve had at this point 9 different Dell machines for work or play in the last ~10 years, all laptops. I&#x27;m typing this on a Latitude 5421.Granted - My work has been heavy on memory but light on CPU, so perhaps I&#x27;m not applying enough deadly heat. reply baz00 13 hours agorootparentDo you live in the arctic circle?TBF their monitors are good. I have two Dell P2723QE&#x27;s and they are absolutely great. reply RajT88 12 hours agorootparentMidwest US where it gets both plenty hot and plenty cold. replygambiting 13 hours agorootparentprevSame. I&#x27;ve got a work Precision 7670 in top spec, literally a £7000(!!!!!) machine, and it overheats within 5 seconds of being under load, the CPU hits 100C and throttles down hard. It&#x27;s equipped with a 3080Ti but it&#x27;s the same story - the GPU overheats and throttles down to 200MHz, so anything you play is just a stuttery mess. And it&#x27;s not surprising - inside it there are 2 heat pipes going to two small fans with only one air outlet. Even my \"gaming\" MSI laptop with half the spec has much beefier cooling, it can run its 3070Ti under full load indefinitely. Not to mention their £400 thunderbolt docks that work maybe 50% of the time. reply baz00 12 hours agorootparentSame hardware. Absolute turd. Also the screen on mine is the OLED one which is so reflective I can&#x27;t see anything on the screen other than my own reflection. Who designed that POS! reply esalman 13 hours agorootparentprevTell me about it. Our company was recently acquired and they are swapping out Thinkpad P1&#x27;s with Dell precision. Protest was no use. reply vondur 13 hours agorootparentprevI purchase the business version of the ThinkPads and the warranties are fine. In fact, HP, Dell and Lenovo use the same company in my area when they send out technicians to work on them. reply baz00 15 hours agorootparentprevYeah this. Keyboard is shite on the new T14 gen 3. Horrible to use. reply sbuk 12 hours agorootparentI swear the trackpad on my work-issued one takes 5 minutes to warm up before it is useable. It is quite possibly the worst laptop I&#x27;ve ever owned, and I&#x27;ve had a Vostro and a 2018 Macbook with the dodgy keyboards. reply isoprophlex 17 hours agoparentprevagreed, i was about to comment \"but will these 4 out of 5 devices allow removal of the lenovo crapware?\" reply verdverm 15 hours agorootparentSame reason I left Verizon for Fi, avoidance of their bloatware. Couldn&#x27;t imagine going back to a phone with apps I cannot uninstall (like Amazon & NFL apps, seriously, I don&#x27;t care about sportsball at all and haven&#x27;t used Amazon in more than 6 years) reply chx 17 hours agoparentprevthis is just propagandathat was consumer side Lenovo laptops, IdeaPad and the like 7+ years ago. They learned their lesson and even there don&#x27;t do it any more.This didn&#x27;t affect ThinkPads.Every time Lenovo is mentioned someone brings this up and I am thoroughly sick of it. reply miohtama 17 hours agorootparentLenovo got fined for $3.5M for installing spyware that hijacks HTTPS connections, so it’s a bit stretch to call it propagandahttps:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;lenovo-receives-3-5m-fine-for-...When a company engages to such deceptive business and outright illegal business practices, there is little reason to trust them for a decade or two until we can be sure everyone who worked there by the time is gone. reply freedomben 17 hours agorootparent> until we can be sure everyone who worked there by the time is gone.That seems a little extreme. I always maintain a skeptical eye, but most corporate leaders have an eye for profits and PR, and their personal integrity&#x2F;moral compass doesn&#x27;t play a large role in their decision making. As long as it&#x27;s bad&#x2F;risky PR for Lenovo to do stuff like that, the incentives are on our side. Always be watching though, and I personally favor vendors who have good track records. reply BossingAround 16 hours agorootparentWell... I can look at it from the other perspective; in my previous org, the director came like 15 years ago from Oracle, and his ideas, management style, and goals were very much aligned with what you&#x27;d expect Oracle was like 20 years ago. It took firing him and two managers below him to turn around the org, despite adopting new practices, new ideas, working in completely new directions, etc.I wouldn&#x27;t completely discard the idea to be honest. reply freedomben 16 hours agorootparentThanks for sharing, that does make sense. Humans are creatures of habit and the way we are brought up is a strong lean on us in future decision making. Changing philosophies can be very hard reply autoexec 17 hours agorootparentprevHistory is not propaganda. Lenovo repeatedly sold out their customers, lied to them, and put them at risk of everything from spying to remote code execution. They&#x27;ve more than earned their reputation and it should haunt them every time they are mentioned.Maybe after several decades have gone by without incident I might consider that they&#x27;ve changed their ways, but I sure wouldn&#x27;t count on that happening.Here&#x27;s them still not having learned their lesson in Feb of 2021 https:&#x2F;&#x2F;www.theregister.com&#x2F;2021&#x2F;09&#x2F;03&#x2F;lenovo_indelible_adwa...Here&#x27;s them still not having learned their lesson in April of 2022 https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;lenovo-patches-uefi-vulnerabil...Today, I wouldn&#x27;t trust this company to make an alarm clock (https:&#x2F;&#x2F;blog.talosintelligence.com&#x2F;vulnerability-spotlight-h...) let alone a laptop. reply demandingturtle 11 hours agorootparentprevHow is this propaganda? Either it&#x27;s true or it is false. You haven&#x27;t said or proved this is false. \"consumer side\" still Lenovo. \"They learned their lesson\" and so have we. \"7+ years ago\" it still happened The only propaganda is coming from you. reply vorpalhex 17 hours agorootparentprevMy Lenovo android tablet started installing adware suddenly, more than a year after purchase. This was a clean device with a single program and the adware came with.. Lenovo ads.Yeah I think it&#x27;s fair to say Lenovo is still no good here. reply ClumsyPilot 16 hours agorootparentprev> They learned their lesson and even there don&#x27;t do it any more.If I did this, I&#x27;d be learning my lesson in prison for 20 years. reply rnk 16 hours agorootparentprevHey chx, got a response to the details listed here? reply unethical_ban 15 hours agorootparentNot chx, but note that none of the linked articles mention Thinkpads. reply autoexec 15 hours agorootparentAll of these links do though and they include examples of more hard coded passwords, more Lenovo spyware, and more insecure Lenovo software that exposed users to threats:https:&#x2F;&#x2F;www.theregister.com&#x2F;2018&#x2F;01&#x2F;26&#x2F;lenovo_thinkpad_finge...https:&#x2F;&#x2F;www.computerworld.com&#x2F;article&#x2F;2984889&#x2F;lenovo-collect...https:&#x2F;&#x2F;www.crn.com.au&#x2F;news&#x2F;thinkpad-vulnerable-in-latest-le...https:&#x2F;&#x2F;www.pcworld.com&#x2F;article&#x2F;633410&#x2F;up-to-100-lenovo-lapt...https:&#x2F;&#x2F;www.itpro.com&#x2F;security&#x2F;vulnerability&#x2F;369491&#x2F;lenovo-p...https:&#x2F;&#x2F;github.com&#x2F;Cr4sh&#x2F;ThinkPwn reply unethical_ban 11 hours agorootparentWell that would do it then. reply hfsh 15 hours agorootparentprevSure, but nobody but chx even brought up Thinkpads in the first place. replyisykt 18 hours agoprevI always hear about how modern laptops and smartphones “aren’t repairable” and then I spent a week in Shenzhen.. where there were floors and floors of independent repair people doing specialized fixes to the most intricate parts. I had an old Apple Watch repair for about $20, only to discover that I got ripped off — I could have negotiated it down to much less. reply alpaca128 18 hours agoparentWith \"repairable\" people mean repairs with freely available tools, parts and (ideally) schematics.Apple and others put in a lot of effort to stop independent repair shops from sourcing original parts or making them impossible to successfully replace due to hardware DRM. It is already impossible to upgrade some Macs with more RAM because they will simply refuse to boot if they find more RAM than they got in the factory. reply rnk 16 hours agorootparentCan you provide a reference for this? My web search didn&#x27;t find a result. reply hrrsn 7 hours agorootparentMany components in an iPhone are paired to the logic board from the factory, so if they need to be replaced you have no choice but to go down the Apple sanctioned repair path (although the third party industry goes to great lengths to work around it). Some components, like the display, will show a warning notification in iOS for weeks and disable features like True Tone and auto-brightness if swapped with a third party part. Other components such as the cameras can&#x27;t even be swapped between 100% genuine iPhones.https:&#x2F;&#x2F;www.ifixit.com&#x2F;News&#x2F;82867&#x2F;iphone-15-teardown-reveals...The iPhone 6 disabled Touch ID when the home button was replaced but it&#x27;d still function fine as a button - until you restored the phone, which would fall over with an arbitrary error code forcing you to take the phone in to Apple for repair. There was a lot of backlash before they caved and fixed the update process: https:&#x2F;&#x2F;www.theguardian.com&#x2F;money&#x2F;2016&#x2F;feb&#x2F;05&#x2F;error-53-apple... reply ClumsyPilot 16 hours agorootparentprevLook up Lewis Rossman, he runs a repair shop and complains all the time about this reply hef19898 16 hours agorootparentAnd he is one of the worst eigjt-to-repair advocates there are, given how often he even gets the most basic stuff wrong when it allows to create attention. reply iancmceachern 3 hours agorootparentHe certainly the most well known. I actually don&#x27;t know of any others, do you? reply shinryuu 16 hours agorootparentprevWhat is a better advocate that you&#x27;d recommend replyaeyes 18 hours agoparentprevShenzhen - where stolen iPhones magically appear on Find My because they are getting used for parts.It&#x27;s not truly repairable if no parts are available. reply lofaszvanitt 17 hours agorootparentJust wait a few years... :D reply miragecraft 18 hours agoparentprevI think when we generally talk about whether something is \"repairable\" we&#x27;re referring to the ease of repair, which translate to the repair being affordable and makes economical sense so people actually do it vs buying a replacement. reply sokoloff 18 hours agorootparentWithout any extraordinary effort and having only typical DIY skills, I have repaired a half-dozen of my family members&#x27; iPhones that people complain are \"not repairable\".To me, \"repairable\" means \"capable of being repaired in a practical way\". I&#x27;ll also grant you the unstated \"at a reasonable price\", which for me, having a cheapskate nature, I&#x27;ve always found the repair parts (batteries, screens, and buttons) to be extremely reasonable in price.For people who don&#x27;t want to DIY it, most every mall has a kiosk that will do it for 2-3x what I paid, but still but became accustomed used to buying new PCs every six or seven years.Why would they buy any quicker? The average laptop in 2016 was only maybe 10% faster than a laptop in 2010 thanks to that stupid Ultrabook shit (and Intel refusing to push out more than a 10% YOY improvement in their standard-voltage desktop lineup), which resulted in 2020&#x27;s CPU lineup only being about twice as fast as one made in 2010. And, so long as you had a machine with an SSD, there&#x27;s very little difference between a laptop from 2015 and 2023 if all you&#x27;re doing is browsing Facebook (which is what most people are doing with them).Plus, at the end of the 2010s, tablets started to become mature enough to supplant laptops as the household&#x27;s only computer, so you&#x27;d either buy a tablet or get a new laptop, but not both.Phones are a different beast in two ways- first, if you&#x27;re buying Android, you&#x27;re throwing your phone away every 2 years anyway because the ecosystem sucks; Qualcomm is even more incompetent when it comes to hardware performance than Google is with software performance and it really shows, especially since Android allows manufacturers to stack a bunch of other unremovable shit and skins that does nothing but take up storage space, enshittifying the device right out of the box like having 10 unremovable IE6 toolbars did back when people still used IE6 (Lenovo is less guilty of this than some but it&#x27;s still not as clean as a Pixel is). Plus, financial incentives exist (for both Android and iOS) to trade in one&#x27;s old phone for a new one at a steep discount, they get smashed up far more frequently than laptops do in ways that make them impossible to fix, and they&#x27;re fashion accessories to a point and it&#x27;s nice to have the new one in a way that normal computers don&#x27;t suffer from as much.It&#x27;s not rocket science; the industry became boring and stagnant so people moved on (and good GPUs have been unaffordable for the vast majority of the last decade including in prebuilts, so...) reply CWuestefeld 17 hours agoparentprevBack through the 90s and 00s, I think that ~3 years was the norm, at least for the part of the population that even owned computers.But since then, there are fewer major improvements coming that fast. GPUs improve, but most people aren&#x27;t hardcore gamers for whome that matters, and these can mostly be upgraded independent of the computer itself instead.Other than that, if you&#x27;re mostly just using your web browser, and maybe doing some spreadsheets and desktop-based email, what&#x27;s the point? Even if you&#x27;re doing more demanding stuff like Photoshop, we&#x27;re at a point where the app&#x27;s demands aren&#x27;t growing fast either. reply lotsofpulp 17 hours agorootparentIt did feel around ~2012 or so was when computer&#x2F;laptop upgrades stopped mattering, roughly coinciding with the switch to SSDs from HDDs. reply distract8901 11 hours agoparentprevI bought a used T530 6 or 7 years ago. The machine is 10 years old and it&#x27;s still my daily driver. My desktop is of a similar vintage, probably 2011 or so.It&#x27;s perfectly fine. I do wish I had more CPU cores, but it&#x27;s probably better for me to get up and step away from the computer while it runs a big C++ compilation or something reply diffeomorphism 18 hours agoparentprevSo? If the reason is that the new one is far better, that seems fine. If the reason is that the old one broke beyond repair, that is not fine.Actually, question: at which point in time is it less eco-friendly to keep using an old computer considering power consumption and the fact that for the same amount of work it will need to use more power for a much longer time? 10 years? 20 years? reply baz00 19 hours agoprevStep 1: stop soldering USB-C ports to the boards on your thinkpad line as they are NOT replaceable and will end the entire machine if damaged. That is the aforementioned T14 gen 3 in the article. reply osdril 2 hours agoparentThat&#x27;s wild. Even on my \"highly unrepairable\" MBA M1 the USB-C ports are very easily replaceable (new board costs $20) reply baz00 59 minutes agorootparentYeah Apple do it right on their machines, at least the laptops. There is no excuse not to do this for other vendors but it costs fractionally more so they push the risk on to the users. reply benoliver999 18 hours agoparentprevOh it&#x27;s funny you say that, I have a T495 and one of the USB-C ports has stopped working.Luckily there&#x27;s another one I can charge from but it really feels like a ticking time bomb. reply baz00 18 hours agorootparentYeah my ex wife threw something at me and broke the only USB-C on my old T470. Fortunately it has an old charge port. reply dmbche 18 hours agoparentprevI don&#x27;t do a lot of hardware, but they really aren&#x27;t ? You couldn&#x27;t remove it and solder another connector in it&#x27;s place? reply baz00 18 hours agorootparentUsually no. If you damage the connector it tears the traces off the board irreparably. If it doesn&#x27;t do that then there are two rows of pins which need to be resoldered. One row is under the connector. It is near impossible to solder those properly without non trivial equipment. Usually if you get a repair they solder only one pair of pins to the board and that means your USB connector only works plugged in one way up.This is one reason amongst many that I&#x27;m not a particularly big fan of USB-C reply CaveTech 18 hours agorootparentprevNope, and it&#x27;s more and more common. I had to trash a $2k monitor due to faulty USB C connector. They require full board replacement. reply bkallus 18 hours agorootparentWhat stops you from desoldering the bad connector and soldering on a new one? reply mschuster91 17 hours agorootparentSMD rework sucks if you&#x27;re not used to it. reply baz00 15 hours agorootparentIt sucks if you are used to it! reply nicolaslem 18 hours agorootparentprevAssuming you can find the part \"full board replacement\" may not be as bad as it sounds for a monitor. Your main point still stands though, ideally a faulty connector shouldn&#x27;t require to replace anything besides the connector itself. reply neogodless 17 hours agoprevAnecdote:My employer recently bought me a Lenovo Legion 5 Pro. Due to concussions and general flicker sensitivity, I don&#x27;t do well with 60Hz screens, and I prefer AMD CPUs. This one came with a 240Hz screen. Overkill!But... Bluetooth was unreliable. Every few hours, it vanished from Windows as if it didn&#x27;t exist. A moment of sleep and wake was enough to bring it back, but it was still annoying to lose mouse and headphones unexpectedly.Two support calls, but all they did was reinstall a bunch of the driver updates I had already reinstalled (and they installed Geforce Experience because Vantage said it was available.) Unsurprisingly, none of that fixed the issue. The next step was a factory reset and&#x2F;or replacement, which would&#x27;ve been a big PITA because of time I already spent doing setup. And it still might not have solved the issue.Instead, I spent $23 on an Intel AX210 WLAN&#x2F;Bluetooth chip, and performed the replacement (of a Realtek chip) in well under 10 minutes. Taking off the bottom panel was relatively easy, and I did not see any damaged clips. M.2 and SODIMM chips are readily replaceable, and so was this WLAN chip. It also fixed my problem completely without resorting to losing a lot of time repeating my setup.If there&#x27;s a point, it&#x27;s that budget and mid-range gaming laptops are often quite repairable, where as thin and light &#x2F; ultrabooks, especially those made for businesses are much less frequently repairable. I think the reason is two-fold... in general businesses will just pay for repairs, even when it means sending a device back. And of course smaller machines tend to get soldered components and optimize for size instead of access.You have to think about the compromises you&#x27;re willing to make, or to have built into your purchases. reply LordDragonfang 16 hours agoparent>those made for businesses are much less frequently repairable.It&#x27;s funny, because the most serviceable laptops I&#x27;ve ever taken apart were the old enterprise ones my dad&#x27;s company used to issue him, where half the parts were serviceable without even opening the chassis.Of course, that changed recently when some fruit company popularized the \"ultrabook\" category and now every company is trying to sandwich every component in the 1cm span between two screwless sheets of aluminum. reply buzzlite 16 hours agoprevOn my EU wishlist is a law mandating standard form-factors for laptop batteries (and chargers). All batteries must be user-replaceable without tools and interchangeable with those from other manufacturers. There can be 4 to 5 different form-factors to address the various sizes and power-profiles of laptops. Batteries must have standard voltages and amperages but OEMs can differentiate themselves on other factors like mAh capacity, longevity, etc. Kind of like AA, AAA batteries but appropriate for laptops. Any laptop that doesn&#x27;t conform to the standards should be hit with a 50-euro enemy-of-the-planet&#x2F;douchehat tax (rather than an outright ban). The industry is already there since most laptops are designed by ODMs like Clevo and Compal. The last 20 years has provided little evidence of innovation happening on batteries and chargers in the budget segment. All the custom chargers and battery sizes are designed purely to force incompatibility on customers- for planned obsolescence, to reduce competition and charge more for after-sales support. Such a law would stop this practice by OEMs of engineering planned obsolescence into their bottom-tier cookie-cutter clevo-crap but still allow companies like Apple to \"innovate\" on vanity metrics like slimness. An industry consortium or committee (like W3 for the web or the 4G&#x2F;5G committees) can be setup to propose new revisions to the standards (if required) once every 10 years (not lesser). reply duped 19 hours agoprevThis is great, but I can&#x27;t trust a Lenovo device and I&#x27;m always a bit shocked when I see developers buying ThinkPads from a company that&#x27;s demonstrated it can&#x27;t be trusted to give you a secure device. reply intalentive 18 hours agoparentDoesn’t every CPU have a state-enforced back door? I assume no device is secure. reply duped 16 hours agorootparentI&#x27;m not really worried about state actors that don&#x27;t have state sponsored industrial espionage. reply itsoktocry 17 hours agoparentprev>from a company that&#x27;s demonstrated it can&#x27;t be trusted to give you a secure device.Is there a single technology company that you can&#x27;t say this about? reply yjftsjthsd-h 16 hours agorootparentReally depends on your threat model; I think most companies stop short of shipping malware from their UEFI implementation. That said, I don&#x27;t think Framework has yet done anything questionable? reply susanthenerd 15 hours agorootparentThe problem with Framework is that BIOS updates are really rare. I been running the 3.0.6-beta for my 12th gen for almost 8 months I think and the full release has yet to come. I bet it will never happen as they are focusing on AMD and 13th gen + 16\" one at the moment. reply progval 14 hours agorootparentprevAt least the 12th gen is very vulnerable to hardware access as you can&#x27;t disable unauthenticated DMA access through the USB ports (it&#x27;s what https:&#x2F;&#x2F;www.dell.com&#x2F;community&#x2F;en&#x2F;conversations&#x2F;latitude&#x2F;dem... calls SL0). I asked their support and they aren&#x27;t planning to fix it in the forseeable future. reply foooorsyth 19 hours agoparentprevI’m out of the loop. Is this sentiment due to Chinese ownership or previous mishaps? Both? reply jowea 19 hours agorootparentI remember the superfish incident https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lenovo#Superfish reply duped 19 hours agorootparentprevSuperfish is the big one. reply Run_DOS_Run 17 hours agoprevLenovo&#x27;s repairability of their products has unfortunately dropped a lot and I hope they take it seriously and get back to the quality they used to have. I own several ThinkPads from the last 10 years. The older models were all very easy to repair.I definitely won&#x27;t be buying the new models anymore. I can live with the fact that socketed CPUs have been replaced with soldered ones due to aesthetics (thinner laptops), but soldered RAM is where my pain threshold is reached.I still believe that quality products must be repairable. I consider everything else to be cheap, throwaway products. No matter how much money \"premium brands\" charge for their junk with glued battery, glued SSD and fixed RAM in recycled aluminium packaging. reply PumpkinSpice 18 hours agoprevIsn&#x27;t this a bit of a no-op for Lenovo specifically? I&#x27;ve been using Lenovo-made laptops for years, and never had trouble with DIY repairs - replacing keyboards, screens, batteries, adding RAM, etc. Their devices are pretty modular and DIY-friendly.I wish they made charging plugs easier to replace, as these inevitably get damaged after 1-2 years, but that&#x27;s really the only complaint I can think of. Is the situation different for other device types, or are they just trying to score an easy (if deserved) PR win? reply freedomben 17 hours agoparentNo. Some lines are definitely better than others (like I only buy the T-series now), but some are on the same level of Apple minus the hardware DRM reply r053bud 16 hours agoprevI&#x27;ve been running OpenBSD on my T480s for a bit now. I think it&#x27;s an absolutely amazing machine. I love the keyboard, I love the physical shutter over the camera, and I love how I&#x27;m not worried I&#x27;m going to break it. I just ordered a replacement battery for it as well and it was an extremely easy process to install. Even though I upgraded my RAM to 32gb, I think I&#x27;m limited by my old 4 core i7 and starting to notice it during some especially long compilations. Nothing too bad though... reply itsoktocry 15 hours agoparent>I&#x27;ve been running OpenBSD on my T480s for a bit now. I think it&#x27;s an absolutely amazing machine.Same. I&#x27;ve been buying them off-lease for many years. You can usually get them for a couple hundred bucks. Best non-Apple computers I&#x27;ve ever used. Keyboards are awesome.The last laptop I bought was a Dell XPS13 because research indicated it&#x27;s a pretty popular Linux-compatible laptop. It&#x27;s nice, but I&#x27;m going back to a Thinkpad next... reply distract8901 10 hours agoprevI would pay a shameful amount of money for a big, fat laptop in the style of the old school thinkpads with modern hardware.I don&#x27;t give a fuck how thin or light my laptop is. I want it big and beefy with some monstrous heatsinks and fans that don&#x27;t sound like a jet engine after 3 seconds of CPU load. I want all the ports and a battery that weighs five pounds.Current design trends for laptops has resulted in significantly worse machines. It&#x27;s why I still use a T530 from 2013 as my daily driver. I&#x27;ll probably keep using it until it finally develops a fault that I can&#x27;t repair. Barring a catastrophic accident like a spill or a drop, I expect it to keep on chugging for at least another 5-10 years. reply nfriedly 17 hours agoprevProbably 10 years ago, my brother installed some new RAM in his Lenovo laptop on Christmas day, after which point it refused to turn on - with either the new or old RAM. We contacted Lenovo and they sent someone out to our house about 2 days later to replace then entire motherboard. (They installed the new RAM, and the laptop worked great after that.)It left me with the impression that Lenovo had questionable quality &#x2F; end-user repairability, but excellent support (as long as it&#x27;s still under warranty!) reply itsoktocry 17 hours agoparent>It left me with the impression that Lenovo had questionable qualityI have 5 old Thinkpads, all still operational. Some are now 10+ years old, and have been passed on to children. They&#x27;ve been through hell and still work. They are the toughest laptops I&#x27;ve owned, and they have repair manuals to boot. Nearly everything is replaceable. reply spondylosaurus 16 hours agoparentprev\"Questionable quality but excellent support\" 100% describes my experience. The mobo in my Lenovo laptop has died, like, three times at this point, but they&#x27;ll send out a technician to my house to replace it within a day or two. ¯\\_(ツ)_&#x2F;¯ reply pianoben 14 hours agoparentprevLevono service is the best I&#x27;ve experienced. Anecdotally of course - when my tower PC that I bought in 2019 started crapping out this year, they sent a repair person to my home several times, each time replacing parts without question, including a $4K graphics card (which turned out to be the fix). That warranty is worth its weight in gold and I happily extended it. reply amalcon 17 hours agoparentprevIt&#x27;s interesting because Thinkpads (back when they were IBM branded, but still manufactured by Lenovo) were probably second only to Apple&#x27;s build quality at the time. Just anecdotally, but these days, I even trust Dell&#x27;s build quality over theirs. reply d3w4s9 13 hours agoprevWell, maybe start with allowing users to swap RAM sticks first instead of continuing shipping those disgusting laptops with 8GB soldered RAM. For most of their consumer laptops (with the exception of gaming laptops), it&#x27;s basically impossible to replace RAM sticks. Nowadays even ThinkPad lines are increasingly doing this.I can get why this is done for thin and light laptops. But not everyone needs that. I am completely willing to have 200g added to the weight of a laptop to get replaceable RAM. reply southernplaces7 17 hours agoprevGiven all the well documented \"don&#x27;t trust Lenovo\" comments here, anyone have anything specifically negative to say about any other brands in particular? Genuinely curious. For example, Asus, Acer, Toshiba etc? reply InvisibleUp 11 hours agoparentToshiba no longer makes computers. Thanks to their 2015 accounting scandal, they had to sell their computer division to Sharp, who now sells some very unremarkable machines under the name \"Dynabook\". reply bfrog 18 hours agoprevMy x220 is endlessly repairable already,what happened was they went the wrong direction in that regard shortly after. reply iteratethis 14 hours agoprevDon&#x27;t remember the model, but a Lenovo of a family member had a broken Wi-fi card, just when it was 2 weeks out of warranty. No goodwill from Lenovo, they estimated repair costs a significant fraction of the price of the whole laptop. Plus my family member would not have his laptop for 2 weeks, whilst he needs it for his business.So I tried to do it myself. Studying official materials, repair videos, the like. The important thing I learned is to get exactly the correct Wi-fi card because the laptop (the BIOS I guess) rejects any other type not on the \"allow list\".So I ordered it and connected it, which was easy enough. Still rejects it.Also, the laptop shipped with a wrong keyboard layout. Which my family member reported directly after purchase but Lenovo refused a return.At work I also have a company issues Lenovo. It keeps prompting for firmware updates. The last time I applied it, it crashed the entire machine and it took me days to get it in working order again.Your mileage may vary but to me these are Russian roulette machines. Fragile, full of arbitrary limitations, shit software, worse support. reply some_random 20 hours agoprevFantastic to see and I hope that this extends to upgradability too, it&#x27;s absurd that the only way to get more storage on so called \"professional\" products like macbooks is to purchase a brand new one. reply hdjjhhvvhga 19 hours agoparentThere is nothing professional or unprofessional about a device except a name. In the old days, business-targeted devices such as PCs and laptops had user-serviceable parts so that the support could quickly replace a broken fan or faulty memory die and you could get back to work. These days are long gone and I&#x27;m sorry to say I blame Apple for a large part of it. reply nicolaslem 18 hours agorootparentLenovo, Dell and HP still have lines of business laptops with user-serviceable parts, space parts available, repair guides on Youtube... reply jonfw 18 hours agorootparentprevworks great if your employees are onsite and you have a big support dept. You&#x27;re going to need to have employees w&#x2F; spare time waiting for repairs, you&#x27;re going to need to stock extra parts, you&#x27;re going to have to hire guys with skills to do that work. For those of us who are remote, or who don&#x27;t have a big IT dept, that was never a good option.I can get a brand new company macbook shipped to my house in less than 24 hrs if anything goes wrong w&#x2F; my machine, and things rarely go wrong. You can&#x27;t realistically beat that reply bzzzt 19 hours agorootparentprevYeah, how dare they design laptops not filled to the brim with unnecessary components? Those &#x27;faulty&#x27; memory parts were often &#x27;fixed&#x27; by reseating them, a problem soldered-on memory doesn&#x27;t have. Broken fans were probably the cheapest obtainable. Those things just don&#x27;t break so often anymore. I prefer welded shut quality stuff over a never ending chain of repairs.I like Apple for batteries that last longer than 12 months, screens with high resolution and reference color, lightweight housing that&#x27;s not too flimsy. (just don&#x27;t talk about the keyboards from 2016-2018 ;) ) reply jollyllama 18 hours agoprevI&#x27;m guessing it was 5&#x2F;5 as of 13 years ago. reply pscoutou 19 hours agoprevI suspect Lenovo is being motivated by rumblings from the EU.https:&#x2F;&#x2F;www.consilium.europa.eu&#x2F;en&#x2F;press&#x2F;press-releases&#x2F;2023... reply redder23 12 hours agoprevThey are all just reading the room and they see what is coming. They just do this because they are scared of right to repair and other laws and regulations and they will not be ready for it. So what they do is start early. They also see it as a selling point of course just with rise of public awareness, the success of Framework ... so even without any laws the move for sure is towards devices that are better repairable. reply verdverm 15 hours agoprevThis is why I bought a https:&#x2F;&#x2F;frame.work and am a happy customer. Will do business with them again reply g8oz 19 hours agoprevI&#x27;d settle for them honoring their warranty. reply russelg 11 hours agoprevIt would be great if this includes removing the vendor-lock that Thinkstations apply to AMD workstation CPUs when they&#x27;re installed. This creates an amount of ewaste as these CPUs are otherwise fully functional. reply DiabloD3 8 hours agoprevAlso Lenovo PC boss: But we&#x27;re still shipping in-BIOS state sponsored malware. We were caught at least twice, and we&#x27;re still in business. reply bubblethink 11 hours agoprevJust remove soldered wifi and wwan whitelists from current thinkpads. The rest is already good enough. Intel trying stupid shit makes sense, but lenovo has started soldering wifi on AMD laptops too. reply leonheld 19 hours agoprevThank you Right to Repair and thank you Framework! reply internetter 19 hours agoprevGoing full circle on that thinkpad line, are we? reply fumar 13 hours agoprevToo bad the hackintosh days are nearing an end. One of my favorite computers ever was an x230 with IPS screen running MacOS. It had a great keyboard, sensible small size, and swappable batteries. reply pmarreck 18 hours agoprevhttps:&#x2F;&#x2F;frame.work&#x2F; : Already exists reply freedomben 17 hours agoparentPrior to Framework I only bought Thinkpads (I&#x27;m a linux user), but framework has gotten really good. They&#x27;ve added a lot of fixes&#x2F;polish from their first gen models, and I&#x27;m super excited for the future! reply pmarreck 14 hours agorootparentI have a Framework Laptop 16 on preorder.Removable GPU = both battery savings AND prevention of gaming temptation when I need to work. >..< reply ReptileMan 19 hours agoprevBack to the early 2000s ... it is amazing development, but why and how it was allowed to come to the current situation must be discussed and prevented from repeating. reply freedomben 17 hours agoparentI think a lot of how we got the current situation was Apple. They proved that most people don&#x27;t care about modularity&#x2F;repairability, and they argued reasons why closed&#x2F;locked-down is better (and some of those arguments weren&#x27;t entirely without merit, despite being self-serving). This convinced a lot of decision-makers to go that route.I think the overall trend is still toward Steve Job&#x27;s vision. Looking at Tesla for example, does not fill me with optimism of a modular&#x2F;repairable future. reply pjmlp 19 hours agoparentprevPC was the exception during the 16bit days, and it was due to IBM failing to prevent clones that it happened in first place.Most non technical users see their computers the same way as a DVD player. reply rgreekguy 13 hours agoprevBack to the future! reply fortran77 19 hours agoprevI was very impressed with my Thinkpad X13s. I was able to open it up, just screws, --no glue to soften with a heatgun, careful spudging, or suction devices to pull things apart--and replace the 512 GB M.2 card with a 1 TB one. reply a3w 19 hours agoparentI tried to replace my wireless card in my T440s. For a decade, Lenovo mostly forbids these upgrade via whitelisting in Thinkpads. Hello, other vendors... reply throwway120385 18 hours agorootparentIt seems overzealous but they might be doing this to maintain FCC licensing. Speaking from experience, different radio&#x2F;antenna combinations can leak different kinds of noise which would prevent you from getting certification with certain combinations. Their laptops might be noisier than the competition and with the added overhead of supporting different WLAN and Wifi cards with the built-in antennas they went over the edge. reply manishsharan 15 hours agoparentprevI think my T480 is the last upgradeable laptop in the ThinkPad T series. The newer models have soldered rams only with only one slot available for upgrade and just not worth the effort. reply ireallywantthat 19 hours agoprevHaha! Why isn&#x27;t this by default? Why is this even non-default? reply beebeepka 16 hours agoprevIs this move in anticipation of upcoming regulations? That has to be it, right?Lenovo is fairly competitive in the 700-1500€ market but the thin and light models have soldered ram. Not great when they sell you single channel with zero ability to add more sticks on some models. It&#x27;s not as terrible now due to ddr5 but still leaves bad taste. reply lovegrenoble 20 hours agoprevreparing and upgrading please reply rnk 16 hours agoprev [–] They forgot to add, however, \"97.5% of our sales were in that 5th category last quarter, suck it liberals!\". And we are only starting in the last 3 days of calendar year 2025. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lenovo, the leading global PC manufacturer, has revealed plans to make 80% of its devices, including repair parts, repairable by 2025.",
      "The initiative is part of the company's commitment to achieving net zero emissions by 2050, and aligns with efforts by the European Council to revise EU regulations on right-to-repair.",
      "Lenovo maintains that emphasizing repairability will not hinder business, and is a necessary step towards sustainability and informing consumers about product lifespan."
    ],
    "commentSummary": [
      "Lenovo plans to make 80% of their devices repairable by 2025, a move appreciated by customers concerned about the non-repairability of current devices.",
      "Some users express concerns over Lenovo's previous incidents involving malware and backdoors, cautioning others to remain vigilant.",
      "The plan resonates positively with a discontent expressed over soldered RAM and the preclusion to upgrades, showing a demand for more repairable and upgradable devices in today's market."
    ],
    "points": 194,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1696514670
  }
]
